<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260215.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GSM-GS: Geometry-Constrained Single and Multi-view Gaussian Splatting for Surface Reconstruction", "author": "Xiao Ren and Yu Liu and Ning An and Jian Cheng and Xin Qiao and He Kong", "abstract": "Recently, 3D Gaussian Splatting has emerged as a prominent research direction owing to its ultrarapid training speed and high-fidelity rendering capabilities. However, the unstructured and irregular nature of Gaussian point clouds poses challenges to reconstruction accuracy. This limitation frequently causes high-frequency detail loss in complex surface microstructures when relying solely on routine strategies. To address this limitation, we propose GSM-GS: a synergistic optimization framework integrating single-view adaptive sub-region weighting constraints and multi-view spatial structure refinement. For single-view optimization, we leverage image gradient features to partition scenes into texture-rich and texture-less sub-regions. The reconstruction quality is enhanced through adaptive filtering mechanisms guided by depth discrepancy features. This preserves high-weight regions while implementing a dual-branch constraint strategy tailored to regional texture variations, thereby improving geometric detail characterization. For multi-view optimization, we introduce a geometry-guided cross-view point cloud association method combined with a dynamic weight sampling strategy. This constructs 3D structural normal constraints across adjacent point cloud frames, effectively reinforcing multi-view consistency and reconstruction fidelity. Extensive experiments on public datasets demonstrate that our method achieves both competitive rendering quality and geometric reconstruction. See our interactive project page", "link": "http://arxiv.org/abs/2602.12796v1", "date": "2026-02-13", "relevancy": 3.6281, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7437}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7184}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSM-GS%3A%20Geometry-Constrained%20Single%20and%20Multi-view%20Gaussian%20Splatting%20for%20Surface%20Reconstruction&body=Title%3A%20GSM-GS%3A%20Geometry-Constrained%20Single%20and%20Multi-view%20Gaussian%20Splatting%20for%20Surface%20Reconstruction%0AAuthor%3A%20Xiao%20Ren%20and%20Yu%20Liu%20and%20Ning%20An%20and%20Jian%20Cheng%20and%20Xin%20Qiao%20and%20He%20Kong%0AAbstract%3A%20Recently%2C%203D%20Gaussian%20Splatting%20has%20emerged%20as%20a%20prominent%20research%20direction%20owing%20to%20its%20ultrarapid%20training%20speed%20and%20high-fidelity%20rendering%20capabilities.%20However%2C%20the%20unstructured%20and%20irregular%20nature%20of%20Gaussian%20point%20clouds%20poses%20challenges%20to%20reconstruction%20accuracy.%20This%20limitation%20frequently%20causes%20high-frequency%20detail%20loss%20in%20complex%20surface%20microstructures%20when%20relying%20solely%20on%20routine%20strategies.%20To%20address%20this%20limitation%2C%20we%20propose%20GSM-GS%3A%20a%20synergistic%20optimization%20framework%20integrating%20single-view%20adaptive%20sub-region%20weighting%20constraints%20and%20multi-view%20spatial%20structure%20refinement.%20For%20single-view%20optimization%2C%20we%20leverage%20image%20gradient%20features%20to%20partition%20scenes%20into%20texture-rich%20and%20texture-less%20sub-regions.%20The%20reconstruction%20quality%20is%20enhanced%20through%20adaptive%20filtering%20mechanisms%20guided%20by%20depth%20discrepancy%20features.%20This%20preserves%20high-weight%20regions%20while%20implementing%20a%20dual-branch%20constraint%20strategy%20tailored%20to%20regional%20texture%20variations%2C%20thereby%20improving%20geometric%20detail%20characterization.%20For%20multi-view%20optimization%2C%20we%20introduce%20a%20geometry-guided%20cross-view%20point%20cloud%20association%20method%20combined%20with%20a%20dynamic%20weight%20sampling%20strategy.%20This%20constructs%203D%20structural%20normal%20constraints%20across%20adjacent%20point%20cloud%20frames%2C%20effectively%20reinforcing%20multi-view%20consistency%20and%20reconstruction%20fidelity.%20Extensive%20experiments%20on%20public%20datasets%20demonstrate%20that%20our%20method%20achieves%20both%20competitive%20rendering%20quality%20and%20geometric%20reconstruction.%20See%20our%20interactive%20project%20page%0ALink%3A%20http%3A//arxiv.org/abs/2602.12796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSM-GS%253A%2520Geometry-Constrained%2520Single%2520and%2520Multi-view%2520Gaussian%2520Splatting%2520for%2520Surface%2520Reconstruction%26entry.906535625%3DXiao%2520Ren%2520and%2520Yu%2520Liu%2520and%2520Ning%2520An%2520and%2520Jian%2520Cheng%2520and%2520Xin%2520Qiao%2520and%2520He%2520Kong%26entry.1292438233%3DRecently%252C%25203D%2520Gaussian%2520Splatting%2520has%2520emerged%2520as%2520a%2520prominent%2520research%2520direction%2520owing%2520to%2520its%2520ultrarapid%2520training%2520speed%2520and%2520high-fidelity%2520rendering%2520capabilities.%2520However%252C%2520the%2520unstructured%2520and%2520irregular%2520nature%2520of%2520Gaussian%2520point%2520clouds%2520poses%2520challenges%2520to%2520reconstruction%2520accuracy.%2520This%2520limitation%2520frequently%2520causes%2520high-frequency%2520detail%2520loss%2520in%2520complex%2520surface%2520microstructures%2520when%2520relying%2520solely%2520on%2520routine%2520strategies.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520GSM-GS%253A%2520a%2520synergistic%2520optimization%2520framework%2520integrating%2520single-view%2520adaptive%2520sub-region%2520weighting%2520constraints%2520and%2520multi-view%2520spatial%2520structure%2520refinement.%2520For%2520single-view%2520optimization%252C%2520we%2520leverage%2520image%2520gradient%2520features%2520to%2520partition%2520scenes%2520into%2520texture-rich%2520and%2520texture-less%2520sub-regions.%2520The%2520reconstruction%2520quality%2520is%2520enhanced%2520through%2520adaptive%2520filtering%2520mechanisms%2520guided%2520by%2520depth%2520discrepancy%2520features.%2520This%2520preserves%2520high-weight%2520regions%2520while%2520implementing%2520a%2520dual-branch%2520constraint%2520strategy%2520tailored%2520to%2520regional%2520texture%2520variations%252C%2520thereby%2520improving%2520geometric%2520detail%2520characterization.%2520For%2520multi-view%2520optimization%252C%2520we%2520introduce%2520a%2520geometry-guided%2520cross-view%2520point%2520cloud%2520association%2520method%2520combined%2520with%2520a%2520dynamic%2520weight%2520sampling%2520strategy.%2520This%2520constructs%25203D%2520structural%2520normal%2520constraints%2520across%2520adjacent%2520point%2520cloud%2520frames%252C%2520effectively%2520reinforcing%2520multi-view%2520consistency%2520and%2520reconstruction%2520fidelity.%2520Extensive%2520experiments%2520on%2520public%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520both%2520competitive%2520rendering%2520quality%2520and%2520geometric%2520reconstruction.%2520See%2520our%2520interactive%2520project%2520page%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSM-GS%3A%20Geometry-Constrained%20Single%20and%20Multi-view%20Gaussian%20Splatting%20for%20Surface%20Reconstruction&entry.906535625=Xiao%20Ren%20and%20Yu%20Liu%20and%20Ning%20An%20and%20Jian%20Cheng%20and%20Xin%20Qiao%20and%20He%20Kong&entry.1292438233=Recently%2C%203D%20Gaussian%20Splatting%20has%20emerged%20as%20a%20prominent%20research%20direction%20owing%20to%20its%20ultrarapid%20training%20speed%20and%20high-fidelity%20rendering%20capabilities.%20However%2C%20the%20unstructured%20and%20irregular%20nature%20of%20Gaussian%20point%20clouds%20poses%20challenges%20to%20reconstruction%20accuracy.%20This%20limitation%20frequently%20causes%20high-frequency%20detail%20loss%20in%20complex%20surface%20microstructures%20when%20relying%20solely%20on%20routine%20strategies.%20To%20address%20this%20limitation%2C%20we%20propose%20GSM-GS%3A%20a%20synergistic%20optimization%20framework%20integrating%20single-view%20adaptive%20sub-region%20weighting%20constraints%20and%20multi-view%20spatial%20structure%20refinement.%20For%20single-view%20optimization%2C%20we%20leverage%20image%20gradient%20features%20to%20partition%20scenes%20into%20texture-rich%20and%20texture-less%20sub-regions.%20The%20reconstruction%20quality%20is%20enhanced%20through%20adaptive%20filtering%20mechanisms%20guided%20by%20depth%20discrepancy%20features.%20This%20preserves%20high-weight%20regions%20while%20implementing%20a%20dual-branch%20constraint%20strategy%20tailored%20to%20regional%20texture%20variations%2C%20thereby%20improving%20geometric%20detail%20characterization.%20For%20multi-view%20optimization%2C%20we%20introduce%20a%20geometry-guided%20cross-view%20point%20cloud%20association%20method%20combined%20with%20a%20dynamic%20weight%20sampling%20strategy.%20This%20constructs%203D%20structural%20normal%20constraints%20across%20adjacent%20point%20cloud%20frames%2C%20effectively%20reinforcing%20multi-view%20consistency%20and%20reconstruction%20fidelity.%20Extensive%20experiments%20on%20public%20datasets%20demonstrate%20that%20our%20method%20achieves%20both%20competitive%20rendering%20quality%20and%20geometric%20reconstruction.%20See%20our%20interactive%20project%20page&entry.1838667208=http%3A//arxiv.org/abs/2602.12796v1&entry.124074799=Read"},
{"title": "A Step to Decouple Optimization in 3DGS", "author": "Renjie Ding and Yaonan Wang and Min Liu and Jialin Zhu and Jiazheng Wang and Jiahao Zhao and Wenting Shen and Feixiang He and Xiang Chen", "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time novel view synthesis. As an explicit representation optimized through gradient propagation among primitives, optimization widely accepted in deep neural networks (DNNs) is actually adopted in 3DGS, such as synchronous weight updating and Adam with the adaptive gradient. However, considering the physical significance and specific design in 3DGS, there are two overlooked details in the optimization of 3DGS: (i) update step coupling, which induces optimizer state rescaling and costly attribute updates outside the viewpoints, and (ii) gradient coupling in the moment, which may lead to under- or over-effective regularization. Nevertheless, such a complex coupling is under-explored. After revisiting the optimization of 3DGS, we take a step to decouple it and recompose the process into: Sparse Adam, Re-State Regularization and Decoupled Attribute Regularization. Taking a large number of experiments under the 3DGS and 3DGS-MCMC frameworks, our work provides a deeper understanding of these components. Finally, based on the empirical analysis, we re-design the optimization and propose AdamW-GS by re-coupling the beneficial components, under which better optimization efficiency and representation effectiveness are achieved simultaneously.", "link": "http://arxiv.org/abs/2601.16736v3", "date": "2026-02-13", "relevancy": 3.016, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6103}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6067}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Step%20to%20Decouple%20Optimization%20in%203DGS&body=Title%3A%20A%20Step%20to%20Decouple%20Optimization%20in%203DGS%0AAuthor%3A%20Renjie%20Ding%20and%20Yaonan%20Wang%20and%20Min%20Liu%20and%20Jialin%20Zhu%20and%20Jiazheng%20Wang%20and%20Jiahao%20Zhao%20and%20Wenting%20Shen%20and%20Feixiang%20He%20and%20Xiang%20Chen%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20powerful%20technique%20for%20real-time%20novel%20view%20synthesis.%20As%20an%20explicit%20representation%20optimized%20through%20gradient%20propagation%20among%20primitives%2C%20optimization%20widely%20accepted%20in%20deep%20neural%20networks%20%28DNNs%29%20is%20actually%20adopted%20in%203DGS%2C%20such%20as%20synchronous%20weight%20updating%20and%20Adam%20with%20the%20adaptive%20gradient.%20However%2C%20considering%20the%20physical%20significance%20and%20specific%20design%20in%203DGS%2C%20there%20are%20two%20overlooked%20details%20in%20the%20optimization%20of%203DGS%3A%20%28i%29%20update%20step%20coupling%2C%20which%20induces%20optimizer%20state%20rescaling%20and%20costly%20attribute%20updates%20outside%20the%20viewpoints%2C%20and%20%28ii%29%20gradient%20coupling%20in%20the%20moment%2C%20which%20may%20lead%20to%20under-%20or%20over-effective%20regularization.%20Nevertheless%2C%20such%20a%20complex%20coupling%20is%20under-explored.%20After%20revisiting%20the%20optimization%20of%203DGS%2C%20we%20take%20a%20step%20to%20decouple%20it%20and%20recompose%20the%20process%20into%3A%20Sparse%20Adam%2C%20Re-State%20Regularization%20and%20Decoupled%20Attribute%20Regularization.%20Taking%20a%20large%20number%20of%20experiments%20under%20the%203DGS%20and%203DGS-MCMC%20frameworks%2C%20our%20work%20provides%20a%20deeper%20understanding%20of%20these%20components.%20Finally%2C%20based%20on%20the%20empirical%20analysis%2C%20we%20re-design%20the%20optimization%20and%20propose%20AdamW-GS%20by%20re-coupling%20the%20beneficial%20components%2C%20under%20which%20better%20optimization%20efficiency%20and%20representation%20effectiveness%20are%20achieved%20simultaneously.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16736v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Step%2520to%2520Decouple%2520Optimization%2520in%25203DGS%26entry.906535625%3DRenjie%2520Ding%2520and%2520Yaonan%2520Wang%2520and%2520Min%2520Liu%2520and%2520Jialin%2520Zhu%2520and%2520Jiazheng%2520Wang%2520and%2520Jiahao%2520Zhao%2520and%2520Wenting%2520Shen%2520and%2520Feixiang%2520He%2520and%2520Xiang%2520Chen%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520technique%2520for%2520real-time%2520novel%2520view%2520synthesis.%2520As%2520an%2520explicit%2520representation%2520optimized%2520through%2520gradient%2520propagation%2520among%2520primitives%252C%2520optimization%2520widely%2520accepted%2520in%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520is%2520actually%2520adopted%2520in%25203DGS%252C%2520such%2520as%2520synchronous%2520weight%2520updating%2520and%2520Adam%2520with%2520the%2520adaptive%2520gradient.%2520However%252C%2520considering%2520the%2520physical%2520significance%2520and%2520specific%2520design%2520in%25203DGS%252C%2520there%2520are%2520two%2520overlooked%2520details%2520in%2520the%2520optimization%2520of%25203DGS%253A%2520%2528i%2529%2520update%2520step%2520coupling%252C%2520which%2520induces%2520optimizer%2520state%2520rescaling%2520and%2520costly%2520attribute%2520updates%2520outside%2520the%2520viewpoints%252C%2520and%2520%2528ii%2529%2520gradient%2520coupling%2520in%2520the%2520moment%252C%2520which%2520may%2520lead%2520to%2520under-%2520or%2520over-effective%2520regularization.%2520Nevertheless%252C%2520such%2520a%2520complex%2520coupling%2520is%2520under-explored.%2520After%2520revisiting%2520the%2520optimization%2520of%25203DGS%252C%2520we%2520take%2520a%2520step%2520to%2520decouple%2520it%2520and%2520recompose%2520the%2520process%2520into%253A%2520Sparse%2520Adam%252C%2520Re-State%2520Regularization%2520and%2520Decoupled%2520Attribute%2520Regularization.%2520Taking%2520a%2520large%2520number%2520of%2520experiments%2520under%2520the%25203DGS%2520and%25203DGS-MCMC%2520frameworks%252C%2520our%2520work%2520provides%2520a%2520deeper%2520understanding%2520of%2520these%2520components.%2520Finally%252C%2520based%2520on%2520the%2520empirical%2520analysis%252C%2520we%2520re-design%2520the%2520optimization%2520and%2520propose%2520AdamW-GS%2520by%2520re-coupling%2520the%2520beneficial%2520components%252C%2520under%2520which%2520better%2520optimization%2520efficiency%2520and%2520representation%2520effectiveness%2520are%2520achieved%2520simultaneously.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16736v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Step%20to%20Decouple%20Optimization%20in%203DGS&entry.906535625=Renjie%20Ding%20and%20Yaonan%20Wang%20and%20Min%20Liu%20and%20Jialin%20Zhu%20and%20Jiazheng%20Wang%20and%20Jiahao%20Zhao%20and%20Wenting%20Shen%20and%20Feixiang%20He%20and%20Xiang%20Chen&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20powerful%20technique%20for%20real-time%20novel%20view%20synthesis.%20As%20an%20explicit%20representation%20optimized%20through%20gradient%20propagation%20among%20primitives%2C%20optimization%20widely%20accepted%20in%20deep%20neural%20networks%20%28DNNs%29%20is%20actually%20adopted%20in%203DGS%2C%20such%20as%20synchronous%20weight%20updating%20and%20Adam%20with%20the%20adaptive%20gradient.%20However%2C%20considering%20the%20physical%20significance%20and%20specific%20design%20in%203DGS%2C%20there%20are%20two%20overlooked%20details%20in%20the%20optimization%20of%203DGS%3A%20%28i%29%20update%20step%20coupling%2C%20which%20induces%20optimizer%20state%20rescaling%20and%20costly%20attribute%20updates%20outside%20the%20viewpoints%2C%20and%20%28ii%29%20gradient%20coupling%20in%20the%20moment%2C%20which%20may%20lead%20to%20under-%20or%20over-effective%20regularization.%20Nevertheless%2C%20such%20a%20complex%20coupling%20is%20under-explored.%20After%20revisiting%20the%20optimization%20of%203DGS%2C%20we%20take%20a%20step%20to%20decouple%20it%20and%20recompose%20the%20process%20into%3A%20Sparse%20Adam%2C%20Re-State%20Regularization%20and%20Decoupled%20Attribute%20Regularization.%20Taking%20a%20large%20number%20of%20experiments%20under%20the%203DGS%20and%203DGS-MCMC%20frameworks%2C%20our%20work%20provides%20a%20deeper%20understanding%20of%20these%20components.%20Finally%2C%20based%20on%20the%20empirical%20analysis%2C%20we%20re-design%20the%20optimization%20and%20propose%20AdamW-GS%20by%20re-coupling%20the%20beneficial%20components%2C%20under%20which%20better%20optimization%20efficiency%20and%20representation%20effectiveness%20are%20achieved%20simultaneously.&entry.1838667208=http%3A//arxiv.org/abs/2601.16736v3&entry.124074799=Read"},
{"title": "Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions", "author": "Yunheng Li and Hengrui Zhang and Meng-Hao Guo and Wenzhao Gao and Shaoyong Jia and Shaohui Jiao and Qibin Hou and Ming-Ming Cheng", "abstract": "Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.", "link": "http://arxiv.org/abs/2602.13013v1", "date": "2026-02-13", "relevancy": 2.9825, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6066}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Universal%20Video%20MLLMs%20with%20Attribute-Structured%20and%20Quality-Verified%20Instructions&body=Title%3A%20Towards%20Universal%20Video%20MLLMs%20with%20Attribute-Structured%20and%20Quality-Verified%20Instructions%0AAuthor%3A%20Yunheng%20Li%20and%20Hengrui%20Zhang%20and%20Meng-Hao%20Guo%20and%20Wenzhao%20Gao%20and%20Shaoyong%20Jia%20and%20Shaohui%20Jiao%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng%0AAbstract%3A%20Universal%20video%20understanding%20requires%20modeling%20fine-grained%20visual%20and%20audio%20information%20over%20time%20in%20diverse%20real-world%20scenarios.%20However%2C%20the%20performance%20of%20existing%20models%20is%20primarily%20constrained%20by%20video-instruction%20data%20that%20represents%20complex%20audiovisual%20content%20as%20single%2C%20incomplete%20descriptions%2C%20lacking%20fine-grained%20organization%20and%20reliable%20annotation.%20To%20address%20this%2C%20we%20introduce%3A%20%28i%29%20ASID-1M%2C%20an%20open-source%20collection%20of%20one%20million%20structured%2C%20fine-grained%20audiovisual%20instruction%20annotations%20with%20single-%20and%20multi-attribute%20supervision%3B%20%28ii%29%20ASID-Verify%2C%20a%20scalable%20data%20curation%20pipeline%20for%20annotation%2C%20with%20automatic%20verification%20and%20refinement%20that%20enforces%20semantic%20and%20temporal%20consistency%20between%20descriptions%20and%20the%20corresponding%20audiovisual%20content%3B%20and%20%28iii%29%20ASID-Captioner%2C%20a%20video%20understanding%20model%20trained%20via%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20the%20ASID-1M.%20Experiments%20across%20seven%20benchmarks%20covering%20audiovisual%20captioning%2C%20attribute-wise%20captioning%2C%20caption-based%20QA%2C%20and%20caption-based%20temporal%20grounding%20show%20that%20ASID-Captioner%20improves%20fine-grained%20caption%20quality%20while%20reducing%20hallucinations%20and%20improving%20instruction%20following.%20It%20achieves%20state-of-the-art%20performance%20among%20open-source%20models%20and%20is%20competitive%20with%20Gemini-3-Pro.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Universal%2520Video%2520MLLMs%2520with%2520Attribute-Structured%2520and%2520Quality-Verified%2520Instructions%26entry.906535625%3DYunheng%2520Li%2520and%2520Hengrui%2520Zhang%2520and%2520Meng-Hao%2520Guo%2520and%2520Wenzhao%2520Gao%2520and%2520Shaoyong%2520Jia%2520and%2520Shaohui%2520Jiao%2520and%2520Qibin%2520Hou%2520and%2520Ming-Ming%2520Cheng%26entry.1292438233%3DUniversal%2520video%2520understanding%2520requires%2520modeling%2520fine-grained%2520visual%2520and%2520audio%2520information%2520over%2520time%2520in%2520diverse%2520real-world%2520scenarios.%2520However%252C%2520the%2520performance%2520of%2520existing%2520models%2520is%2520primarily%2520constrained%2520by%2520video-instruction%2520data%2520that%2520represents%2520complex%2520audiovisual%2520content%2520as%2520single%252C%2520incomplete%2520descriptions%252C%2520lacking%2520fine-grained%2520organization%2520and%2520reliable%2520annotation.%2520To%2520address%2520this%252C%2520we%2520introduce%253A%2520%2528i%2529%2520ASID-1M%252C%2520an%2520open-source%2520collection%2520of%2520one%2520million%2520structured%252C%2520fine-grained%2520audiovisual%2520instruction%2520annotations%2520with%2520single-%2520and%2520multi-attribute%2520supervision%253B%2520%2528ii%2529%2520ASID-Verify%252C%2520a%2520scalable%2520data%2520curation%2520pipeline%2520for%2520annotation%252C%2520with%2520automatic%2520verification%2520and%2520refinement%2520that%2520enforces%2520semantic%2520and%2520temporal%2520consistency%2520between%2520descriptions%2520and%2520the%2520corresponding%2520audiovisual%2520content%253B%2520and%2520%2528iii%2529%2520ASID-Captioner%252C%2520a%2520video%2520understanding%2520model%2520trained%2520via%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520on%2520the%2520ASID-1M.%2520Experiments%2520across%2520seven%2520benchmarks%2520covering%2520audiovisual%2520captioning%252C%2520attribute-wise%2520captioning%252C%2520caption-based%2520QA%252C%2520and%2520caption-based%2520temporal%2520grounding%2520show%2520that%2520ASID-Captioner%2520improves%2520fine-grained%2520caption%2520quality%2520while%2520reducing%2520hallucinations%2520and%2520improving%2520instruction%2520following.%2520It%2520achieves%2520state-of-the-art%2520performance%2520among%2520open-source%2520models%2520and%2520is%2520competitive%2520with%2520Gemini-3-Pro.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Universal%20Video%20MLLMs%20with%20Attribute-Structured%20and%20Quality-Verified%20Instructions&entry.906535625=Yunheng%20Li%20and%20Hengrui%20Zhang%20and%20Meng-Hao%20Guo%20and%20Wenzhao%20Gao%20and%20Shaoyong%20Jia%20and%20Shaohui%20Jiao%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng&entry.1292438233=Universal%20video%20understanding%20requires%20modeling%20fine-grained%20visual%20and%20audio%20information%20over%20time%20in%20diverse%20real-world%20scenarios.%20However%2C%20the%20performance%20of%20existing%20models%20is%20primarily%20constrained%20by%20video-instruction%20data%20that%20represents%20complex%20audiovisual%20content%20as%20single%2C%20incomplete%20descriptions%2C%20lacking%20fine-grained%20organization%20and%20reliable%20annotation.%20To%20address%20this%2C%20we%20introduce%3A%20%28i%29%20ASID-1M%2C%20an%20open-source%20collection%20of%20one%20million%20structured%2C%20fine-grained%20audiovisual%20instruction%20annotations%20with%20single-%20and%20multi-attribute%20supervision%3B%20%28ii%29%20ASID-Verify%2C%20a%20scalable%20data%20curation%20pipeline%20for%20annotation%2C%20with%20automatic%20verification%20and%20refinement%20that%20enforces%20semantic%20and%20temporal%20consistency%20between%20descriptions%20and%20the%20corresponding%20audiovisual%20content%3B%20and%20%28iii%29%20ASID-Captioner%2C%20a%20video%20understanding%20model%20trained%20via%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20the%20ASID-1M.%20Experiments%20across%20seven%20benchmarks%20covering%20audiovisual%20captioning%2C%20attribute-wise%20captioning%2C%20caption-based%20QA%2C%20and%20caption-based%20temporal%20grounding%20show%20that%20ASID-Captioner%20improves%20fine-grained%20caption%20quality%20while%20reducing%20hallucinations%20and%20improving%20instruction%20following.%20It%20achieves%20state-of-the-art%20performance%20among%20open-source%20models%20and%20is%20competitive%20with%20Gemini-3-Pro.&entry.1838667208=http%3A//arxiv.org/abs/2602.13013v1&entry.124074799=Read"},
{"title": "DynaGuide: A Generalizable Dynamic Guidance Framework for Unsupervised Semantic Segmentation", "author": "Boujemaa Guermazi and Riadh Ksantini and Naimul Khan", "abstract": "Unsupervised image segmentation is a critical task in computer vision. It enables dense scene understanding without human annotations, which is especially valuable in domains where labelled data is scarce. However, existing methods often struggle to reconcile global semantic structure with fine-grained boundary accuracy. This paper introduces DynaGuide, an adaptive segmentation framework that addresses these challenges through a novel dual-guidance strategy and dynamic loss optimization. Building on our previous work, DynaSeg, DynaGuide combines global pseudo-labels from zero-shot models such as DiffSeg or SegFormer with local boundary refinement using a lightweight CNN trained from scratch. This synergy allows the model to correct coarse or noisy global predictions and produce high-precision segmentations. At the heart of DynaGuide is a multi-component loss that dynamically balances feature similarity, Huber-smoothed spatial continuity, including diagonal relationships, and semantic alignment with the global pseudo-labels. Unlike prior approaches, DynaGuide trains entirely without ground-truth labels in the target domain and supports plug-and-play integration of diverse guidance sources. Extensive experiments on BSD500, PASCAL VOC2012, and COCO demonstrate that DynaGuide achieves state-of-the-art performance, improving mIoU by 17.5% on BSD500, 3.1% on PASCAL VOC2012, and 11.66% on COCO. With its modular design, strong generalization, and minimal computational footprint, DynaGuide offers a scalable and practical solution for unsupervised segmentation in real-world settings. Code available at: https://github.com/RyersonMultimediaLab/DynaGuide", "link": "http://arxiv.org/abs/2602.13020v1", "date": "2026-02-13", "relevancy": 2.9173, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5952}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5888}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaGuide%3A%20A%20Generalizable%20Dynamic%20Guidance%20Framework%20for%20Unsupervised%20Semantic%20Segmentation&body=Title%3A%20DynaGuide%3A%20A%20Generalizable%20Dynamic%20Guidance%20Framework%20for%20Unsupervised%20Semantic%20Segmentation%0AAuthor%3A%20Boujemaa%20Guermazi%20and%20Riadh%20Ksantini%20and%20Naimul%20Khan%0AAbstract%3A%20Unsupervised%20image%20segmentation%20is%20a%20critical%20task%20in%20computer%20vision.%20It%20enables%20dense%20scene%20understanding%20without%20human%20annotations%2C%20which%20is%20especially%20valuable%20in%20domains%20where%20labelled%20data%20is%20scarce.%20However%2C%20existing%20methods%20often%20struggle%20to%20reconcile%20global%20semantic%20structure%20with%20fine-grained%20boundary%20accuracy.%20This%20paper%20introduces%20DynaGuide%2C%20an%20adaptive%20segmentation%20framework%20that%20addresses%20these%20challenges%20through%20a%20novel%20dual-guidance%20strategy%20and%20dynamic%20loss%20optimization.%20Building%20on%20our%20previous%20work%2C%20DynaSeg%2C%20DynaGuide%20combines%20global%20pseudo-labels%20from%20zero-shot%20models%20such%20as%20DiffSeg%20or%20SegFormer%20with%20local%20boundary%20refinement%20using%20a%20lightweight%20CNN%20trained%20from%20scratch.%20This%20synergy%20allows%20the%20model%20to%20correct%20coarse%20or%20noisy%20global%20predictions%20and%20produce%20high-precision%20segmentations.%20At%20the%20heart%20of%20DynaGuide%20is%20a%20multi-component%20loss%20that%20dynamically%20balances%20feature%20similarity%2C%20Huber-smoothed%20spatial%20continuity%2C%20including%20diagonal%20relationships%2C%20and%20semantic%20alignment%20with%20the%20global%20pseudo-labels.%20Unlike%20prior%20approaches%2C%20DynaGuide%20trains%20entirely%20without%20ground-truth%20labels%20in%20the%20target%20domain%20and%20supports%20plug-and-play%20integration%20of%20diverse%20guidance%20sources.%20Extensive%20experiments%20on%20BSD500%2C%20PASCAL%20VOC2012%2C%20and%20COCO%20demonstrate%20that%20DynaGuide%20achieves%20state-of-the-art%20performance%2C%20improving%20mIoU%20by%2017.5%25%20on%20BSD500%2C%203.1%25%20on%20PASCAL%20VOC2012%2C%20and%2011.66%25%20on%20COCO.%20With%20its%20modular%20design%2C%20strong%20generalization%2C%20and%20minimal%20computational%20footprint%2C%20DynaGuide%20offers%20a%20scalable%20and%20practical%20solution%20for%20unsupervised%20segmentation%20in%20real-world%20settings.%20Code%20available%20at%3A%20https%3A//github.com/RyersonMultimediaLab/DynaGuide%0ALink%3A%20http%3A//arxiv.org/abs/2602.13020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaGuide%253A%2520A%2520Generalizable%2520Dynamic%2520Guidance%2520Framework%2520for%2520Unsupervised%2520Semantic%2520Segmentation%26entry.906535625%3DBoujemaa%2520Guermazi%2520and%2520Riadh%2520Ksantini%2520and%2520Naimul%2520Khan%26entry.1292438233%3DUnsupervised%2520image%2520segmentation%2520is%2520a%2520critical%2520task%2520in%2520computer%2520vision.%2520It%2520enables%2520dense%2520scene%2520understanding%2520without%2520human%2520annotations%252C%2520which%2520is%2520especially%2520valuable%2520in%2520domains%2520where%2520labelled%2520data%2520is%2520scarce.%2520However%252C%2520existing%2520methods%2520often%2520struggle%2520to%2520reconcile%2520global%2520semantic%2520structure%2520with%2520fine-grained%2520boundary%2520accuracy.%2520This%2520paper%2520introduces%2520DynaGuide%252C%2520an%2520adaptive%2520segmentation%2520framework%2520that%2520addresses%2520these%2520challenges%2520through%2520a%2520novel%2520dual-guidance%2520strategy%2520and%2520dynamic%2520loss%2520optimization.%2520Building%2520on%2520our%2520previous%2520work%252C%2520DynaSeg%252C%2520DynaGuide%2520combines%2520global%2520pseudo-labels%2520from%2520zero-shot%2520models%2520such%2520as%2520DiffSeg%2520or%2520SegFormer%2520with%2520local%2520boundary%2520refinement%2520using%2520a%2520lightweight%2520CNN%2520trained%2520from%2520scratch.%2520This%2520synergy%2520allows%2520the%2520model%2520to%2520correct%2520coarse%2520or%2520noisy%2520global%2520predictions%2520and%2520produce%2520high-precision%2520segmentations.%2520At%2520the%2520heart%2520of%2520DynaGuide%2520is%2520a%2520multi-component%2520loss%2520that%2520dynamically%2520balances%2520feature%2520similarity%252C%2520Huber-smoothed%2520spatial%2520continuity%252C%2520including%2520diagonal%2520relationships%252C%2520and%2520semantic%2520alignment%2520with%2520the%2520global%2520pseudo-labels.%2520Unlike%2520prior%2520approaches%252C%2520DynaGuide%2520trains%2520entirely%2520without%2520ground-truth%2520labels%2520in%2520the%2520target%2520domain%2520and%2520supports%2520plug-and-play%2520integration%2520of%2520diverse%2520guidance%2520sources.%2520Extensive%2520experiments%2520on%2520BSD500%252C%2520PASCAL%2520VOC2012%252C%2520and%2520COCO%2520demonstrate%2520that%2520DynaGuide%2520achieves%2520state-of-the-art%2520performance%252C%2520improving%2520mIoU%2520by%252017.5%2525%2520on%2520BSD500%252C%25203.1%2525%2520on%2520PASCAL%2520VOC2012%252C%2520and%252011.66%2525%2520on%2520COCO.%2520With%2520its%2520modular%2520design%252C%2520strong%2520generalization%252C%2520and%2520minimal%2520computational%2520footprint%252C%2520DynaGuide%2520offers%2520a%2520scalable%2520and%2520practical%2520solution%2520for%2520unsupervised%2520segmentation%2520in%2520real-world%2520settings.%2520Code%2520available%2520at%253A%2520https%253A//github.com/RyersonMultimediaLab/DynaGuide%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaGuide%3A%20A%20Generalizable%20Dynamic%20Guidance%20Framework%20for%20Unsupervised%20Semantic%20Segmentation&entry.906535625=Boujemaa%20Guermazi%20and%20Riadh%20Ksantini%20and%20Naimul%20Khan&entry.1292438233=Unsupervised%20image%20segmentation%20is%20a%20critical%20task%20in%20computer%20vision.%20It%20enables%20dense%20scene%20understanding%20without%20human%20annotations%2C%20which%20is%20especially%20valuable%20in%20domains%20where%20labelled%20data%20is%20scarce.%20However%2C%20existing%20methods%20often%20struggle%20to%20reconcile%20global%20semantic%20structure%20with%20fine-grained%20boundary%20accuracy.%20This%20paper%20introduces%20DynaGuide%2C%20an%20adaptive%20segmentation%20framework%20that%20addresses%20these%20challenges%20through%20a%20novel%20dual-guidance%20strategy%20and%20dynamic%20loss%20optimization.%20Building%20on%20our%20previous%20work%2C%20DynaSeg%2C%20DynaGuide%20combines%20global%20pseudo-labels%20from%20zero-shot%20models%20such%20as%20DiffSeg%20or%20SegFormer%20with%20local%20boundary%20refinement%20using%20a%20lightweight%20CNN%20trained%20from%20scratch.%20This%20synergy%20allows%20the%20model%20to%20correct%20coarse%20or%20noisy%20global%20predictions%20and%20produce%20high-precision%20segmentations.%20At%20the%20heart%20of%20DynaGuide%20is%20a%20multi-component%20loss%20that%20dynamically%20balances%20feature%20similarity%2C%20Huber-smoothed%20spatial%20continuity%2C%20including%20diagonal%20relationships%2C%20and%20semantic%20alignment%20with%20the%20global%20pseudo-labels.%20Unlike%20prior%20approaches%2C%20DynaGuide%20trains%20entirely%20without%20ground-truth%20labels%20in%20the%20target%20domain%20and%20supports%20plug-and-play%20integration%20of%20diverse%20guidance%20sources.%20Extensive%20experiments%20on%20BSD500%2C%20PASCAL%20VOC2012%2C%20and%20COCO%20demonstrate%20that%20DynaGuide%20achieves%20state-of-the-art%20performance%2C%20improving%20mIoU%20by%2017.5%25%20on%20BSD500%2C%203.1%25%20on%20PASCAL%20VOC2012%2C%20and%2011.66%25%20on%20COCO.%20With%20its%20modular%20design%2C%20strong%20generalization%2C%20and%20minimal%20computational%20footprint%2C%20DynaGuide%20offers%20a%20scalable%20and%20practical%20solution%20for%20unsupervised%20segmentation%20in%20real-world%20settings.%20Code%20available%20at%3A%20https%3A//github.com/RyersonMultimediaLab/DynaGuide&entry.1838667208=http%3A//arxiv.org/abs/2602.13020v1&entry.124074799=Read"},
{"title": "Unleashing MLLMs on the Edge: A Unified Framework for Cross-Modal ReID via Adaptive SVD Distillation", "author": "Hongbo Jiang and Jie Li and Xinqi Cai and Tianyu Xie and Yunhang Shen and Pingyang Dai and Liujuan Cao", "abstract": "Practical cloud-edge deployment of Cross-Modal Re-identification (CM-ReID) faces challenges due to maintaining a fragmented ecosystem of specialized cloud models for diverse modalities. While Multi-Modal Large Language Models (MLLMs) offer strong unification potential, existing approaches fail to adapt them into a single end-to-end backbone and lack effective knowledge distillation strategies for edge deployment. To address these limitations, we propose MLLMEmbed-ReID, a unified framework based on a powerful cloud-edge architecture. First, we adapt a foundational MLLM into a state-of-the-art cloud model. We leverage instruction-based prompting to guide the MLLM in generating a unified embedding space across RGB, infrared, sketch, and text modalities. This model is then trained efficiently with a hierarchical Low-Rank Adaptation finetuning (LoRA-SFT) strategy, optimized under a holistic cross-modal alignment objective. Second, to deploy its knowledge onto an edge-native student, we introduce a novel distillation strategy motivated by the low-rank property in the teacher's feature space. To prioritize essential information, this method employs a Principal Component Mapping loss, while relational structures are preserved via a Feature Relation loss. Our lightweight edge-based model achieves state-of-the-art performance on multiple visual CM-ReID benchmarks, while its cloud-based counterpart excels across all CM-ReID benchmarks. The MLLMEmbed-ReID framework thus presents a complete and effective solution for deploying unified MLLM-level intelligence on resource-constrained devices. The code and models will be open-sourced soon.", "link": "http://arxiv.org/abs/2602.12936v1", "date": "2026-02-13", "relevancy": 2.8871, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5778}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5777}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20MLLMs%20on%20the%20Edge%3A%20A%20Unified%20Framework%20for%20Cross-Modal%20ReID%20via%20Adaptive%20SVD%20Distillation&body=Title%3A%20Unleashing%20MLLMs%20on%20the%20Edge%3A%20A%20Unified%20Framework%20for%20Cross-Modal%20ReID%20via%20Adaptive%20SVD%20Distillation%0AAuthor%3A%20Hongbo%20Jiang%20and%20Jie%20Li%20and%20Xinqi%20Cai%20and%20Tianyu%20Xie%20and%20Yunhang%20Shen%20and%20Pingyang%20Dai%20and%20Liujuan%20Cao%0AAbstract%3A%20Practical%20cloud-edge%20deployment%20of%20Cross-Modal%20Re-identification%20%28CM-ReID%29%20faces%20challenges%20due%20to%20maintaining%20a%20fragmented%20ecosystem%20of%20specialized%20cloud%20models%20for%20diverse%20modalities.%20While%20Multi-Modal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20strong%20unification%20potential%2C%20existing%20approaches%20fail%20to%20adapt%20them%20into%20a%20single%20end-to-end%20backbone%20and%20lack%20effective%20knowledge%20distillation%20strategies%20for%20edge%20deployment.%20To%20address%20these%20limitations%2C%20we%20propose%20MLLMEmbed-ReID%2C%20a%20unified%20framework%20based%20on%20a%20powerful%20cloud-edge%20architecture.%20First%2C%20we%20adapt%20a%20foundational%20MLLM%20into%20a%20state-of-the-art%20cloud%20model.%20We%20leverage%20instruction-based%20prompting%20to%20guide%20the%20MLLM%20in%20generating%20a%20unified%20embedding%20space%20across%20RGB%2C%20infrared%2C%20sketch%2C%20and%20text%20modalities.%20This%20model%20is%20then%20trained%20efficiently%20with%20a%20hierarchical%20Low-Rank%20Adaptation%20finetuning%20%28LoRA-SFT%29%20strategy%2C%20optimized%20under%20a%20holistic%20cross-modal%20alignment%20objective.%20Second%2C%20to%20deploy%20its%20knowledge%20onto%20an%20edge-native%20student%2C%20we%20introduce%20a%20novel%20distillation%20strategy%20motivated%20by%20the%20low-rank%20property%20in%20the%20teacher%27s%20feature%20space.%20To%20prioritize%20essential%20information%2C%20this%20method%20employs%20a%20Principal%20Component%20Mapping%20loss%2C%20while%20relational%20structures%20are%20preserved%20via%20a%20Feature%20Relation%20loss.%20Our%20lightweight%20edge-based%20model%20achieves%20state-of-the-art%20performance%20on%20multiple%20visual%20CM-ReID%20benchmarks%2C%20while%20its%20cloud-based%20counterpart%20excels%20across%20all%20CM-ReID%20benchmarks.%20The%20MLLMEmbed-ReID%20framework%20thus%20presents%20a%20complete%20and%20effective%20solution%20for%20deploying%20unified%20MLLM-level%20intelligence%20on%20resource-constrained%20devices.%20The%20code%20and%20models%20will%20be%20open-sourced%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520MLLMs%2520on%2520the%2520Edge%253A%2520A%2520Unified%2520Framework%2520for%2520Cross-Modal%2520ReID%2520via%2520Adaptive%2520SVD%2520Distillation%26entry.906535625%3DHongbo%2520Jiang%2520and%2520Jie%2520Li%2520and%2520Xinqi%2520Cai%2520and%2520Tianyu%2520Xie%2520and%2520Yunhang%2520Shen%2520and%2520Pingyang%2520Dai%2520and%2520Liujuan%2520Cao%26entry.1292438233%3DPractical%2520cloud-edge%2520deployment%2520of%2520Cross-Modal%2520Re-identification%2520%2528CM-ReID%2529%2520faces%2520challenges%2520due%2520to%2520maintaining%2520a%2520fragmented%2520ecosystem%2520of%2520specialized%2520cloud%2520models%2520for%2520diverse%2520modalities.%2520While%2520Multi-Modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520offer%2520strong%2520unification%2520potential%252C%2520existing%2520approaches%2520fail%2520to%2520adapt%2520them%2520into%2520a%2520single%2520end-to-end%2520backbone%2520and%2520lack%2520effective%2520knowledge%2520distillation%2520strategies%2520for%2520edge%2520deployment.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520MLLMEmbed-ReID%252C%2520a%2520unified%2520framework%2520based%2520on%2520a%2520powerful%2520cloud-edge%2520architecture.%2520First%252C%2520we%2520adapt%2520a%2520foundational%2520MLLM%2520into%2520a%2520state-of-the-art%2520cloud%2520model.%2520We%2520leverage%2520instruction-based%2520prompting%2520to%2520guide%2520the%2520MLLM%2520in%2520generating%2520a%2520unified%2520embedding%2520space%2520across%2520RGB%252C%2520infrared%252C%2520sketch%252C%2520and%2520text%2520modalities.%2520This%2520model%2520is%2520then%2520trained%2520efficiently%2520with%2520a%2520hierarchical%2520Low-Rank%2520Adaptation%2520finetuning%2520%2528LoRA-SFT%2529%2520strategy%252C%2520optimized%2520under%2520a%2520holistic%2520cross-modal%2520alignment%2520objective.%2520Second%252C%2520to%2520deploy%2520its%2520knowledge%2520onto%2520an%2520edge-native%2520student%252C%2520we%2520introduce%2520a%2520novel%2520distillation%2520strategy%2520motivated%2520by%2520the%2520low-rank%2520property%2520in%2520the%2520teacher%2527s%2520feature%2520space.%2520To%2520prioritize%2520essential%2520information%252C%2520this%2520method%2520employs%2520a%2520Principal%2520Component%2520Mapping%2520loss%252C%2520while%2520relational%2520structures%2520are%2520preserved%2520via%2520a%2520Feature%2520Relation%2520loss.%2520Our%2520lightweight%2520edge-based%2520model%2520achieves%2520state-of-the-art%2520performance%2520on%2520multiple%2520visual%2520CM-ReID%2520benchmarks%252C%2520while%2520its%2520cloud-based%2520counterpart%2520excels%2520across%2520all%2520CM-ReID%2520benchmarks.%2520The%2520MLLMEmbed-ReID%2520framework%2520thus%2520presents%2520a%2520complete%2520and%2520effective%2520solution%2520for%2520deploying%2520unified%2520MLLM-level%2520intelligence%2520on%2520resource-constrained%2520devices.%2520The%2520code%2520and%2520models%2520will%2520be%2520open-sourced%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20MLLMs%20on%20the%20Edge%3A%20A%20Unified%20Framework%20for%20Cross-Modal%20ReID%20via%20Adaptive%20SVD%20Distillation&entry.906535625=Hongbo%20Jiang%20and%20Jie%20Li%20and%20Xinqi%20Cai%20and%20Tianyu%20Xie%20and%20Yunhang%20Shen%20and%20Pingyang%20Dai%20and%20Liujuan%20Cao&entry.1292438233=Practical%20cloud-edge%20deployment%20of%20Cross-Modal%20Re-identification%20%28CM-ReID%29%20faces%20challenges%20due%20to%20maintaining%20a%20fragmented%20ecosystem%20of%20specialized%20cloud%20models%20for%20diverse%20modalities.%20While%20Multi-Modal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20strong%20unification%20potential%2C%20existing%20approaches%20fail%20to%20adapt%20them%20into%20a%20single%20end-to-end%20backbone%20and%20lack%20effective%20knowledge%20distillation%20strategies%20for%20edge%20deployment.%20To%20address%20these%20limitations%2C%20we%20propose%20MLLMEmbed-ReID%2C%20a%20unified%20framework%20based%20on%20a%20powerful%20cloud-edge%20architecture.%20First%2C%20we%20adapt%20a%20foundational%20MLLM%20into%20a%20state-of-the-art%20cloud%20model.%20We%20leverage%20instruction-based%20prompting%20to%20guide%20the%20MLLM%20in%20generating%20a%20unified%20embedding%20space%20across%20RGB%2C%20infrared%2C%20sketch%2C%20and%20text%20modalities.%20This%20model%20is%20then%20trained%20efficiently%20with%20a%20hierarchical%20Low-Rank%20Adaptation%20finetuning%20%28LoRA-SFT%29%20strategy%2C%20optimized%20under%20a%20holistic%20cross-modal%20alignment%20objective.%20Second%2C%20to%20deploy%20its%20knowledge%20onto%20an%20edge-native%20student%2C%20we%20introduce%20a%20novel%20distillation%20strategy%20motivated%20by%20the%20low-rank%20property%20in%20the%20teacher%27s%20feature%20space.%20To%20prioritize%20essential%20information%2C%20this%20method%20employs%20a%20Principal%20Component%20Mapping%20loss%2C%20while%20relational%20structures%20are%20preserved%20via%20a%20Feature%20Relation%20loss.%20Our%20lightweight%20edge-based%20model%20achieves%20state-of-the-art%20performance%20on%20multiple%20visual%20CM-ReID%20benchmarks%2C%20while%20its%20cloud-based%20counterpart%20excels%20across%20all%20CM-ReID%20benchmarks.%20The%20MLLMEmbed-ReID%20framework%20thus%20presents%20a%20complete%20and%20effective%20solution%20for%20deploying%20unified%20MLLM-level%20intelligence%20on%20resource-constrained%20devices.%20The%20code%20and%20models%20will%20be%20open-sourced%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2602.12936v1&entry.124074799=Read"},
{"title": "Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation", "author": "Yichen Zhao and Zelin Peng and Piao Yang and Xiaokang Yang and Wei Shen", "abstract": "Radiological diagnosis is a perceptual process in which careful visual inspection and language reasoning are repeatedly interleaved. Most medical large vision language models (LVLMs) perform visual inspection only once and then rely on text-only chain-of-thought (CoT) reasoning, which operates purely in the linguistic space and is prone to hallucination. Recent methods attempt to mitigate this issue by introducing visually related coordinates, such as bounding boxes. However, these remain a pseudo-visual solution: coordinates are still text and fail to preserve rich visual details like texture and density. Motivated by the interleaved nature of radiological diagnosis, we introduce MMRad-IVL-22K, the first large-scale dataset designed for natively interleaved visual language reasoning in chest X-ray interpretation. MMRad-IVL-22K reflects a repeated cycle of reasoning and visual inspection workflow of radiologists, in which visual rationales complement textual descriptions and ground each step of the reasoning process. MMRad-IVL-22K comprises 21,994 diagnostic traces, enabling systematic scanning across 35 anatomical regions. Experimental results on advanced closed-source LVLMs demonstrate that report generation guided by multimodal CoT significantly outperforms that guided by text-only CoT in clinical accuracy and report quality (e.g., 6\\% increase in the RadGraph metric), confirming that high-fidelity interleaved vision language evidence is a non-substitutable component of reliable medical AI. Furthermore, benchmarking across seven state-of-the-art open-source LVLMs demonstrates that models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared with both general-purpose and medical-specific LVLMs. The project page is available at https://github.com/qiuzyc/thinking_like_a_radiologist.", "link": "http://arxiv.org/abs/2602.12843v1", "date": "2026-02-13", "relevancy": 2.8271, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5791}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5791}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20Like%20a%20Radiologist%3A%20A%20Dataset%20for%20Anatomy-Guided%20Interleaved%20Vision%20Language%20Reasoning%20in%20Chest%20X-ray%20Interpretation&body=Title%3A%20Thinking%20Like%20a%20Radiologist%3A%20A%20Dataset%20for%20Anatomy-Guided%20Interleaved%20Vision%20Language%20Reasoning%20in%20Chest%20X-ray%20Interpretation%0AAuthor%3A%20Yichen%20Zhao%20and%20Zelin%20Peng%20and%20Piao%20Yang%20and%20Xiaokang%20Yang%20and%20Wei%20Shen%0AAbstract%3A%20Radiological%20diagnosis%20is%20a%20perceptual%20process%20in%20which%20careful%20visual%20inspection%20and%20language%20reasoning%20are%20repeatedly%20interleaved.%20Most%20medical%20large%20vision%20language%20models%20%28LVLMs%29%20perform%20visual%20inspection%20only%20once%20and%20then%20rely%20on%20text-only%20chain-of-thought%20%28CoT%29%20reasoning%2C%20which%20operates%20purely%20in%20the%20linguistic%20space%20and%20is%20prone%20to%20hallucination.%20Recent%20methods%20attempt%20to%20mitigate%20this%20issue%20by%20introducing%20visually%20related%20coordinates%2C%20such%20as%20bounding%20boxes.%20However%2C%20these%20remain%20a%20pseudo-visual%20solution%3A%20coordinates%20are%20still%20text%20and%20fail%20to%20preserve%20rich%20visual%20details%20like%20texture%20and%20density.%20Motivated%20by%20the%20interleaved%20nature%20of%20radiological%20diagnosis%2C%20we%20introduce%20MMRad-IVL-22K%2C%20the%20first%20large-scale%20dataset%20designed%20for%20natively%20interleaved%20visual%20language%20reasoning%20in%20chest%20X-ray%20interpretation.%20MMRad-IVL-22K%20reflects%20a%20repeated%20cycle%20of%20reasoning%20and%20visual%20inspection%20workflow%20of%20radiologists%2C%20in%20which%20visual%20rationales%20complement%20textual%20descriptions%20and%20ground%20each%20step%20of%20the%20reasoning%20process.%20MMRad-IVL-22K%20comprises%2021%2C994%20diagnostic%20traces%2C%20enabling%20systematic%20scanning%20across%2035%20anatomical%20regions.%20Experimental%20results%20on%20advanced%20closed-source%20LVLMs%20demonstrate%20that%20report%20generation%20guided%20by%20multimodal%20CoT%20significantly%20outperforms%20that%20guided%20by%20text-only%20CoT%20in%20clinical%20accuracy%20and%20report%20quality%20%28e.g.%2C%206%5C%25%20increase%20in%20the%20RadGraph%20metric%29%2C%20confirming%20that%20high-fidelity%20interleaved%20vision%20language%20evidence%20is%20a%20non-substitutable%20component%20of%20reliable%20medical%20AI.%20Furthermore%2C%20benchmarking%20across%20seven%20state-of-the-art%20open-source%20LVLMs%20demonstrates%20that%20models%20fine-tuned%20on%20MMRad-IVL-22K%20achieve%20superior%20reasoning%20consistency%20and%20report%20quality%20compared%20with%20both%20general-purpose%20and%20medical-specific%20LVLMs.%20The%20project%20page%20is%20available%20at%20https%3A//github.com/qiuzyc/thinking_like_a_radiologist.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520Like%2520a%2520Radiologist%253A%2520A%2520Dataset%2520for%2520Anatomy-Guided%2520Interleaved%2520Vision%2520Language%2520Reasoning%2520in%2520Chest%2520X-ray%2520Interpretation%26entry.906535625%3DYichen%2520Zhao%2520and%2520Zelin%2520Peng%2520and%2520Piao%2520Yang%2520and%2520Xiaokang%2520Yang%2520and%2520Wei%2520Shen%26entry.1292438233%3DRadiological%2520diagnosis%2520is%2520a%2520perceptual%2520process%2520in%2520which%2520careful%2520visual%2520inspection%2520and%2520language%2520reasoning%2520are%2520repeatedly%2520interleaved.%2520Most%2520medical%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529%2520perform%2520visual%2520inspection%2520only%2520once%2520and%2520then%2520rely%2520on%2520text-only%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%252C%2520which%2520operates%2520purely%2520in%2520the%2520linguistic%2520space%2520and%2520is%2520prone%2520to%2520hallucination.%2520Recent%2520methods%2520attempt%2520to%2520mitigate%2520this%2520issue%2520by%2520introducing%2520visually%2520related%2520coordinates%252C%2520such%2520as%2520bounding%2520boxes.%2520However%252C%2520these%2520remain%2520a%2520pseudo-visual%2520solution%253A%2520coordinates%2520are%2520still%2520text%2520and%2520fail%2520to%2520preserve%2520rich%2520visual%2520details%2520like%2520texture%2520and%2520density.%2520Motivated%2520by%2520the%2520interleaved%2520nature%2520of%2520radiological%2520diagnosis%252C%2520we%2520introduce%2520MMRad-IVL-22K%252C%2520the%2520first%2520large-scale%2520dataset%2520designed%2520for%2520natively%2520interleaved%2520visual%2520language%2520reasoning%2520in%2520chest%2520X-ray%2520interpretation.%2520MMRad-IVL-22K%2520reflects%2520a%2520repeated%2520cycle%2520of%2520reasoning%2520and%2520visual%2520inspection%2520workflow%2520of%2520radiologists%252C%2520in%2520which%2520visual%2520rationales%2520complement%2520textual%2520descriptions%2520and%2520ground%2520each%2520step%2520of%2520the%2520reasoning%2520process.%2520MMRad-IVL-22K%2520comprises%252021%252C994%2520diagnostic%2520traces%252C%2520enabling%2520systematic%2520scanning%2520across%252035%2520anatomical%2520regions.%2520Experimental%2520results%2520on%2520advanced%2520closed-source%2520LVLMs%2520demonstrate%2520that%2520report%2520generation%2520guided%2520by%2520multimodal%2520CoT%2520significantly%2520outperforms%2520that%2520guided%2520by%2520text-only%2520CoT%2520in%2520clinical%2520accuracy%2520and%2520report%2520quality%2520%2528e.g.%252C%25206%255C%2525%2520increase%2520in%2520the%2520RadGraph%2520metric%2529%252C%2520confirming%2520that%2520high-fidelity%2520interleaved%2520vision%2520language%2520evidence%2520is%2520a%2520non-substitutable%2520component%2520of%2520reliable%2520medical%2520AI.%2520Furthermore%252C%2520benchmarking%2520across%2520seven%2520state-of-the-art%2520open-source%2520LVLMs%2520demonstrates%2520that%2520models%2520fine-tuned%2520on%2520MMRad-IVL-22K%2520achieve%2520superior%2520reasoning%2520consistency%2520and%2520report%2520quality%2520compared%2520with%2520both%2520general-purpose%2520and%2520medical-specific%2520LVLMs.%2520The%2520project%2520page%2520is%2520available%2520at%2520https%253A//github.com/qiuzyc/thinking_like_a_radiologist.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20Like%20a%20Radiologist%3A%20A%20Dataset%20for%20Anatomy-Guided%20Interleaved%20Vision%20Language%20Reasoning%20in%20Chest%20X-ray%20Interpretation&entry.906535625=Yichen%20Zhao%20and%20Zelin%20Peng%20and%20Piao%20Yang%20and%20Xiaokang%20Yang%20and%20Wei%20Shen&entry.1292438233=Radiological%20diagnosis%20is%20a%20perceptual%20process%20in%20which%20careful%20visual%20inspection%20and%20language%20reasoning%20are%20repeatedly%20interleaved.%20Most%20medical%20large%20vision%20language%20models%20%28LVLMs%29%20perform%20visual%20inspection%20only%20once%20and%20then%20rely%20on%20text-only%20chain-of-thought%20%28CoT%29%20reasoning%2C%20which%20operates%20purely%20in%20the%20linguistic%20space%20and%20is%20prone%20to%20hallucination.%20Recent%20methods%20attempt%20to%20mitigate%20this%20issue%20by%20introducing%20visually%20related%20coordinates%2C%20such%20as%20bounding%20boxes.%20However%2C%20these%20remain%20a%20pseudo-visual%20solution%3A%20coordinates%20are%20still%20text%20and%20fail%20to%20preserve%20rich%20visual%20details%20like%20texture%20and%20density.%20Motivated%20by%20the%20interleaved%20nature%20of%20radiological%20diagnosis%2C%20we%20introduce%20MMRad-IVL-22K%2C%20the%20first%20large-scale%20dataset%20designed%20for%20natively%20interleaved%20visual%20language%20reasoning%20in%20chest%20X-ray%20interpretation.%20MMRad-IVL-22K%20reflects%20a%20repeated%20cycle%20of%20reasoning%20and%20visual%20inspection%20workflow%20of%20radiologists%2C%20in%20which%20visual%20rationales%20complement%20textual%20descriptions%20and%20ground%20each%20step%20of%20the%20reasoning%20process.%20MMRad-IVL-22K%20comprises%2021%2C994%20diagnostic%20traces%2C%20enabling%20systematic%20scanning%20across%2035%20anatomical%20regions.%20Experimental%20results%20on%20advanced%20closed-source%20LVLMs%20demonstrate%20that%20report%20generation%20guided%20by%20multimodal%20CoT%20significantly%20outperforms%20that%20guided%20by%20text-only%20CoT%20in%20clinical%20accuracy%20and%20report%20quality%20%28e.g.%2C%206%5C%25%20increase%20in%20the%20RadGraph%20metric%29%2C%20confirming%20that%20high-fidelity%20interleaved%20vision%20language%20evidence%20is%20a%20non-substitutable%20component%20of%20reliable%20medical%20AI.%20Furthermore%2C%20benchmarking%20across%20seven%20state-of-the-art%20open-source%20LVLMs%20demonstrates%20that%20models%20fine-tuned%20on%20MMRad-IVL-22K%20achieve%20superior%20reasoning%20consistency%20and%20report%20quality%20compared%20with%20both%20general-purpose%20and%20medical-specific%20LVLMs.%20The%20project%20page%20is%20available%20at%20https%3A//github.com/qiuzyc/thinking_like_a_radiologist.&entry.1838667208=http%3A//arxiv.org/abs/2602.12843v1&entry.124074799=Read"},
{"title": "A Plug-and-Play Method for Guided Multi-contrast MRI Reconstruction based on Content/Style Modeling", "author": "Chinmay Rao and Matthias van Osch and Nicola Pezzotti and Jeroen de Bresser and Mark van Buchem and Laurens Beljaards and Jakob Meineke and Elwin de Weerdt and Huangling Lu and Mariya Doneva and Marius Staring", "abstract": "Since multiple MRI contrasts of the same anatomy contain redundant information, one contrast can guide the reconstruction of an undersampled subsequent contrast. To this end, several end-to-end learning-based guided reconstruction methods have been proposed. However, a key challenge is the requirement of large paired training datasets comprising raw data and aligned reference images. We propose a modular two-stage approach that does not require any k-space training data, relying solely on image-domain datasets, a large part of which can be unpaired. Additionally, our approach provides an explanatory framework for the multi-contrast problem based on the shared and non-shared generative factors underlying two given contrasts. A content/style model of two-contrast image data is learned from a largely unpaired image-domain dataset and is subsequently applied as a plug-and-play operator in iterative reconstruction. The disentanglement of content and style allows explicit representation of contrast-independent and contrast-specific factors. Consequently, incorporating prior information into the reconstruction reduces to a simple replacement of the aliased content of the reconstruction iterate with high-quality content derived from the reference scan. Combining this component with a data consistency step and introducing a general corrective process for the content yields an iterative scheme. We name this novel approach PnP-CoSMo. Various aspects like interpretability and convergence are explored via simulations. Furthermore, its practicality is demonstrated on the public NYU fastMRI DICOM dataset, showing improved generalizability compared to end-to-end methods, and on two in-house multi-coil raw datasets, offering up to 32.6\\% more acceleration over learning-based non-guided reconstruction for a given SSIM.", "link": "http://arxiv.org/abs/2409.13477v5", "date": "2026-02-13", "relevancy": 2.8233, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5724}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5724}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Plug-and-Play%20Method%20for%20Guided%20Multi-contrast%20MRI%20Reconstruction%20based%20on%20Content/Style%20Modeling&body=Title%3A%20A%20Plug-and-Play%20Method%20for%20Guided%20Multi-contrast%20MRI%20Reconstruction%20based%20on%20Content/Style%20Modeling%0AAuthor%3A%20Chinmay%20Rao%20and%20Matthias%20van%20Osch%20and%20Nicola%20Pezzotti%20and%20Jeroen%20de%20Bresser%20and%20Mark%20van%20Buchem%20and%20Laurens%20Beljaards%20and%20Jakob%20Meineke%20and%20Elwin%20de%20Weerdt%20and%20Huangling%20Lu%20and%20Mariya%20Doneva%20and%20Marius%20Staring%0AAbstract%3A%20Since%20multiple%20MRI%20contrasts%20of%20the%20same%20anatomy%20contain%20redundant%20information%2C%20one%20contrast%20can%20guide%20the%20reconstruction%20of%20an%20undersampled%20subsequent%20contrast.%20To%20this%20end%2C%20several%20end-to-end%20learning-based%20guided%20reconstruction%20methods%20have%20been%20proposed.%20However%2C%20a%20key%20challenge%20is%20the%20requirement%20of%20large%20paired%20training%20datasets%20comprising%20raw%20data%20and%20aligned%20reference%20images.%20We%20propose%20a%20modular%20two-stage%20approach%20that%20does%20not%20require%20any%20k-space%20training%20data%2C%20relying%20solely%20on%20image-domain%20datasets%2C%20a%20large%20part%20of%20which%20can%20be%20unpaired.%20Additionally%2C%20our%20approach%20provides%20an%20explanatory%20framework%20for%20the%20multi-contrast%20problem%20based%20on%20the%20shared%20and%20non-shared%20generative%20factors%20underlying%20two%20given%20contrasts.%20A%20content/style%20model%20of%20two-contrast%20image%20data%20is%20learned%20from%20a%20largely%20unpaired%20image-domain%20dataset%20and%20is%20subsequently%20applied%20as%20a%20plug-and-play%20operator%20in%20iterative%20reconstruction.%20The%20disentanglement%20of%20content%20and%20style%20allows%20explicit%20representation%20of%20contrast-independent%20and%20contrast-specific%20factors.%20Consequently%2C%20incorporating%20prior%20information%20into%20the%20reconstruction%20reduces%20to%20a%20simple%20replacement%20of%20the%20aliased%20content%20of%20the%20reconstruction%20iterate%20with%20high-quality%20content%20derived%20from%20the%20reference%20scan.%20Combining%20this%20component%20with%20a%20data%20consistency%20step%20and%20introducing%20a%20general%20corrective%20process%20for%20the%20content%20yields%20an%20iterative%20scheme.%20We%20name%20this%20novel%20approach%20PnP-CoSMo.%20Various%20aspects%20like%20interpretability%20and%20convergence%20are%20explored%20via%20simulations.%20Furthermore%2C%20its%20practicality%20is%20demonstrated%20on%20the%20public%20NYU%20fastMRI%20DICOM%20dataset%2C%20showing%20improved%20generalizability%20compared%20to%20end-to-end%20methods%2C%20and%20on%20two%20in-house%20multi-coil%20raw%20datasets%2C%20offering%20up%20to%2032.6%5C%25%20more%20acceleration%20over%20learning-based%20non-guided%20reconstruction%20for%20a%20given%20SSIM.%0ALink%3A%20http%3A//arxiv.org/abs/2409.13477v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Plug-and-Play%2520Method%2520for%2520Guided%2520Multi-contrast%2520MRI%2520Reconstruction%2520based%2520on%2520Content/Style%2520Modeling%26entry.906535625%3DChinmay%2520Rao%2520and%2520Matthias%2520van%2520Osch%2520and%2520Nicola%2520Pezzotti%2520and%2520Jeroen%2520de%2520Bresser%2520and%2520Mark%2520van%2520Buchem%2520and%2520Laurens%2520Beljaards%2520and%2520Jakob%2520Meineke%2520and%2520Elwin%2520de%2520Weerdt%2520and%2520Huangling%2520Lu%2520and%2520Mariya%2520Doneva%2520and%2520Marius%2520Staring%26entry.1292438233%3DSince%2520multiple%2520MRI%2520contrasts%2520of%2520the%2520same%2520anatomy%2520contain%2520redundant%2520information%252C%2520one%2520contrast%2520can%2520guide%2520the%2520reconstruction%2520of%2520an%2520undersampled%2520subsequent%2520contrast.%2520To%2520this%2520end%252C%2520several%2520end-to-end%2520learning-based%2520guided%2520reconstruction%2520methods%2520have%2520been%2520proposed.%2520However%252C%2520a%2520key%2520challenge%2520is%2520the%2520requirement%2520of%2520large%2520paired%2520training%2520datasets%2520comprising%2520raw%2520data%2520and%2520aligned%2520reference%2520images.%2520We%2520propose%2520a%2520modular%2520two-stage%2520approach%2520that%2520does%2520not%2520require%2520any%2520k-space%2520training%2520data%252C%2520relying%2520solely%2520on%2520image-domain%2520datasets%252C%2520a%2520large%2520part%2520of%2520which%2520can%2520be%2520unpaired.%2520Additionally%252C%2520our%2520approach%2520provides%2520an%2520explanatory%2520framework%2520for%2520the%2520multi-contrast%2520problem%2520based%2520on%2520the%2520shared%2520and%2520non-shared%2520generative%2520factors%2520underlying%2520two%2520given%2520contrasts.%2520A%2520content/style%2520model%2520of%2520two-contrast%2520image%2520data%2520is%2520learned%2520from%2520a%2520largely%2520unpaired%2520image-domain%2520dataset%2520and%2520is%2520subsequently%2520applied%2520as%2520a%2520plug-and-play%2520operator%2520in%2520iterative%2520reconstruction.%2520The%2520disentanglement%2520of%2520content%2520and%2520style%2520allows%2520explicit%2520representation%2520of%2520contrast-independent%2520and%2520contrast-specific%2520factors.%2520Consequently%252C%2520incorporating%2520prior%2520information%2520into%2520the%2520reconstruction%2520reduces%2520to%2520a%2520simple%2520replacement%2520of%2520the%2520aliased%2520content%2520of%2520the%2520reconstruction%2520iterate%2520with%2520high-quality%2520content%2520derived%2520from%2520the%2520reference%2520scan.%2520Combining%2520this%2520component%2520with%2520a%2520data%2520consistency%2520step%2520and%2520introducing%2520a%2520general%2520corrective%2520process%2520for%2520the%2520content%2520yields%2520an%2520iterative%2520scheme.%2520We%2520name%2520this%2520novel%2520approach%2520PnP-CoSMo.%2520Various%2520aspects%2520like%2520interpretability%2520and%2520convergence%2520are%2520explored%2520via%2520simulations.%2520Furthermore%252C%2520its%2520practicality%2520is%2520demonstrated%2520on%2520the%2520public%2520NYU%2520fastMRI%2520DICOM%2520dataset%252C%2520showing%2520improved%2520generalizability%2520compared%2520to%2520end-to-end%2520methods%252C%2520and%2520on%2520two%2520in-house%2520multi-coil%2520raw%2520datasets%252C%2520offering%2520up%2520to%252032.6%255C%2525%2520more%2520acceleration%2520over%2520learning-based%2520non-guided%2520reconstruction%2520for%2520a%2520given%2520SSIM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13477v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Plug-and-Play%20Method%20for%20Guided%20Multi-contrast%20MRI%20Reconstruction%20based%20on%20Content/Style%20Modeling&entry.906535625=Chinmay%20Rao%20and%20Matthias%20van%20Osch%20and%20Nicola%20Pezzotti%20and%20Jeroen%20de%20Bresser%20and%20Mark%20van%20Buchem%20and%20Laurens%20Beljaards%20and%20Jakob%20Meineke%20and%20Elwin%20de%20Weerdt%20and%20Huangling%20Lu%20and%20Mariya%20Doneva%20and%20Marius%20Staring&entry.1292438233=Since%20multiple%20MRI%20contrasts%20of%20the%20same%20anatomy%20contain%20redundant%20information%2C%20one%20contrast%20can%20guide%20the%20reconstruction%20of%20an%20undersampled%20subsequent%20contrast.%20To%20this%20end%2C%20several%20end-to-end%20learning-based%20guided%20reconstruction%20methods%20have%20been%20proposed.%20However%2C%20a%20key%20challenge%20is%20the%20requirement%20of%20large%20paired%20training%20datasets%20comprising%20raw%20data%20and%20aligned%20reference%20images.%20We%20propose%20a%20modular%20two-stage%20approach%20that%20does%20not%20require%20any%20k-space%20training%20data%2C%20relying%20solely%20on%20image-domain%20datasets%2C%20a%20large%20part%20of%20which%20can%20be%20unpaired.%20Additionally%2C%20our%20approach%20provides%20an%20explanatory%20framework%20for%20the%20multi-contrast%20problem%20based%20on%20the%20shared%20and%20non-shared%20generative%20factors%20underlying%20two%20given%20contrasts.%20A%20content/style%20model%20of%20two-contrast%20image%20data%20is%20learned%20from%20a%20largely%20unpaired%20image-domain%20dataset%20and%20is%20subsequently%20applied%20as%20a%20plug-and-play%20operator%20in%20iterative%20reconstruction.%20The%20disentanglement%20of%20content%20and%20style%20allows%20explicit%20representation%20of%20contrast-independent%20and%20contrast-specific%20factors.%20Consequently%2C%20incorporating%20prior%20information%20into%20the%20reconstruction%20reduces%20to%20a%20simple%20replacement%20of%20the%20aliased%20content%20of%20the%20reconstruction%20iterate%20with%20high-quality%20content%20derived%20from%20the%20reference%20scan.%20Combining%20this%20component%20with%20a%20data%20consistency%20step%20and%20introducing%20a%20general%20corrective%20process%20for%20the%20content%20yields%20an%20iterative%20scheme.%20We%20name%20this%20novel%20approach%20PnP-CoSMo.%20Various%20aspects%20like%20interpretability%20and%20convergence%20are%20explored%20via%20simulations.%20Furthermore%2C%20its%20practicality%20is%20demonstrated%20on%20the%20public%20NYU%20fastMRI%20DICOM%20dataset%2C%20showing%20improved%20generalizability%20compared%20to%20end-to-end%20methods%2C%20and%20on%20two%20in-house%20multi-coil%20raw%20datasets%2C%20offering%20up%20to%2032.6%5C%25%20more%20acceleration%20over%20learning-based%20non-guided%20reconstruction%20for%20a%20given%20SSIM.&entry.1838667208=http%3A//arxiv.org/abs/2409.13477v5&entry.124074799=Read"},
{"title": "RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training", "author": "Yunshuang Nie and Bingqian Lin and Minzhe Niu and Kun Xiang and Jianhua Han and Guowei Huang and Xingyue Quan and Hang Xu and Bokui Chen and Xiaodan Liang", "abstract": "Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bottlenecks. Current evaluation primarily relies on testing after supervised fine-tuning, which introduces laborious additional training and autoregressive decoding costs. Meanwhile, common pre-training metrics cannot quantify a model's perception and reasoning abilities in a disentangled manner. Furthermore, existing evaluation benchmarks are typically limited in scale or misaligned with pre-training objectives. Thus, we propose RADAR, an efficient ability-centric evaluation framework for Revealing Asymmetric Development of Abilities in MLLM pRe-training. RADAR involves two key components: (1) Soft Discrimination Score, a novel metric for robustly tracking ability development without fine-tuning, based on quantifying nuanced gradations of the model preference for the correct answer over distractors; and (2) Multi-Modal Mixture Benchmark, a new 15K+ sample benchmark for comprehensively evaluating pre-trained MLLMs' perception and reasoning abilities in a 0-shot manner, where we unify authoritative benchmark datasets and carefully collect new datasets, extending the evaluation scope and addressing the critical gaps in current benchmarks. With RADAR, we comprehensively reveal the asymmetric development of perceptual and reasoning capabilities in pretrained MLLMs across diverse factors, including data volume, model size, and pretraining strategy. Our RADAR underscores the need for a decomposed perspective on pre-training ability bottlenecks, informing targeted interventions to advance MLLMs efficiently. Our code is publicly available at https://github.com/Nieysh/RADAR.", "link": "http://arxiv.org/abs/2602.12892v1", "date": "2026-02-13", "relevancy": 2.8083, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RADAR%3A%20Revealing%20Asymmetric%20Development%20of%20Abilities%20in%20MLLM%20Pre-training&body=Title%3A%20RADAR%3A%20Revealing%20Asymmetric%20Development%20of%20Abilities%20in%20MLLM%20Pre-training%0AAuthor%3A%20Yunshuang%20Nie%20and%20Bingqian%20Lin%20and%20Minzhe%20Niu%20and%20Kun%20Xiang%20and%20Jianhua%20Han%20and%20Guowei%20Huang%20and%20Xingyue%20Quan%20and%20Hang%20Xu%20and%20Bokui%20Chen%20and%20Xiaodan%20Liang%0AAbstract%3A%20Pre-trained%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20provide%20a%20knowledge-rich%20foundation%20for%20post-training%20by%20leveraging%20their%20inherent%20perception%20and%20reasoning%20capabilities%20to%20solve%20complex%20tasks.%20However%2C%20the%20lack%20of%20an%20efficient%20evaluation%20framework%20impedes%20the%20diagnosis%20of%20their%20performance%20bottlenecks.%20Current%20evaluation%20primarily%20relies%20on%20testing%20after%20supervised%20fine-tuning%2C%20which%20introduces%20laborious%20additional%20training%20and%20autoregressive%20decoding%20costs.%20Meanwhile%2C%20common%20pre-training%20metrics%20cannot%20quantify%20a%20model%27s%20perception%20and%20reasoning%20abilities%20in%20a%20disentangled%20manner.%20Furthermore%2C%20existing%20evaluation%20benchmarks%20are%20typically%20limited%20in%20scale%20or%20misaligned%20with%20pre-training%20objectives.%20Thus%2C%20we%20propose%20RADAR%2C%20an%20efficient%20ability-centric%20evaluation%20framework%20for%20Revealing%20Asymmetric%20Development%20of%20Abilities%20in%20MLLM%20pRe-training.%20RADAR%20involves%20two%20key%20components%3A%20%281%29%20Soft%20Discrimination%20Score%2C%20a%20novel%20metric%20for%20robustly%20tracking%20ability%20development%20without%20fine-tuning%2C%20based%20on%20quantifying%20nuanced%20gradations%20of%20the%20model%20preference%20for%20the%20correct%20answer%20over%20distractors%3B%20and%20%282%29%20Multi-Modal%20Mixture%20Benchmark%2C%20a%20new%2015K%2B%20sample%20benchmark%20for%20comprehensively%20evaluating%20pre-trained%20MLLMs%27%20perception%20and%20reasoning%20abilities%20in%20a%200-shot%20manner%2C%20where%20we%20unify%20authoritative%20benchmark%20datasets%20and%20carefully%20collect%20new%20datasets%2C%20extending%20the%20evaluation%20scope%20and%20addressing%20the%20critical%20gaps%20in%20current%20benchmarks.%20With%20RADAR%2C%20we%20comprehensively%20reveal%20the%20asymmetric%20development%20of%20perceptual%20and%20reasoning%20capabilities%20in%20pretrained%20MLLMs%20across%20diverse%20factors%2C%20including%20data%20volume%2C%20model%20size%2C%20and%20pretraining%20strategy.%20Our%20RADAR%20underscores%20the%20need%20for%20a%20decomposed%20perspective%20on%20pre-training%20ability%20bottlenecks%2C%20informing%20targeted%20interventions%20to%20advance%20MLLMs%20efficiently.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/Nieysh/RADAR.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRADAR%253A%2520Revealing%2520Asymmetric%2520Development%2520of%2520Abilities%2520in%2520MLLM%2520Pre-training%26entry.906535625%3DYunshuang%2520Nie%2520and%2520Bingqian%2520Lin%2520and%2520Minzhe%2520Niu%2520and%2520Kun%2520Xiang%2520and%2520Jianhua%2520Han%2520and%2520Guowei%2520Huang%2520and%2520Xingyue%2520Quan%2520and%2520Hang%2520Xu%2520and%2520Bokui%2520Chen%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3DPre-trained%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520provide%2520a%2520knowledge-rich%2520foundation%2520for%2520post-training%2520by%2520leveraging%2520their%2520inherent%2520perception%2520and%2520reasoning%2520capabilities%2520to%2520solve%2520complex%2520tasks.%2520However%252C%2520the%2520lack%2520of%2520an%2520efficient%2520evaluation%2520framework%2520impedes%2520the%2520diagnosis%2520of%2520their%2520performance%2520bottlenecks.%2520Current%2520evaluation%2520primarily%2520relies%2520on%2520testing%2520after%2520supervised%2520fine-tuning%252C%2520which%2520introduces%2520laborious%2520additional%2520training%2520and%2520autoregressive%2520decoding%2520costs.%2520Meanwhile%252C%2520common%2520pre-training%2520metrics%2520cannot%2520quantify%2520a%2520model%2527s%2520perception%2520and%2520reasoning%2520abilities%2520in%2520a%2520disentangled%2520manner.%2520Furthermore%252C%2520existing%2520evaluation%2520benchmarks%2520are%2520typically%2520limited%2520in%2520scale%2520or%2520misaligned%2520with%2520pre-training%2520objectives.%2520Thus%252C%2520we%2520propose%2520RADAR%252C%2520an%2520efficient%2520ability-centric%2520evaluation%2520framework%2520for%2520Revealing%2520Asymmetric%2520Development%2520of%2520Abilities%2520in%2520MLLM%2520pRe-training.%2520RADAR%2520involves%2520two%2520key%2520components%253A%2520%25281%2529%2520Soft%2520Discrimination%2520Score%252C%2520a%2520novel%2520metric%2520for%2520robustly%2520tracking%2520ability%2520development%2520without%2520fine-tuning%252C%2520based%2520on%2520quantifying%2520nuanced%2520gradations%2520of%2520the%2520model%2520preference%2520for%2520the%2520correct%2520answer%2520over%2520distractors%253B%2520and%2520%25282%2529%2520Multi-Modal%2520Mixture%2520Benchmark%252C%2520a%2520new%252015K%252B%2520sample%2520benchmark%2520for%2520comprehensively%2520evaluating%2520pre-trained%2520MLLMs%2527%2520perception%2520and%2520reasoning%2520abilities%2520in%2520a%25200-shot%2520manner%252C%2520where%2520we%2520unify%2520authoritative%2520benchmark%2520datasets%2520and%2520carefully%2520collect%2520new%2520datasets%252C%2520extending%2520the%2520evaluation%2520scope%2520and%2520addressing%2520the%2520critical%2520gaps%2520in%2520current%2520benchmarks.%2520With%2520RADAR%252C%2520we%2520comprehensively%2520reveal%2520the%2520asymmetric%2520development%2520of%2520perceptual%2520and%2520reasoning%2520capabilities%2520in%2520pretrained%2520MLLMs%2520across%2520diverse%2520factors%252C%2520including%2520data%2520volume%252C%2520model%2520size%252C%2520and%2520pretraining%2520strategy.%2520Our%2520RADAR%2520underscores%2520the%2520need%2520for%2520a%2520decomposed%2520perspective%2520on%2520pre-training%2520ability%2520bottlenecks%252C%2520informing%2520targeted%2520interventions%2520to%2520advance%2520MLLMs%2520efficiently.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Nieysh/RADAR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RADAR%3A%20Revealing%20Asymmetric%20Development%20of%20Abilities%20in%20MLLM%20Pre-training&entry.906535625=Yunshuang%20Nie%20and%20Bingqian%20Lin%20and%20Minzhe%20Niu%20and%20Kun%20Xiang%20and%20Jianhua%20Han%20and%20Guowei%20Huang%20and%20Xingyue%20Quan%20and%20Hang%20Xu%20and%20Bokui%20Chen%20and%20Xiaodan%20Liang&entry.1292438233=Pre-trained%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20provide%20a%20knowledge-rich%20foundation%20for%20post-training%20by%20leveraging%20their%20inherent%20perception%20and%20reasoning%20capabilities%20to%20solve%20complex%20tasks.%20However%2C%20the%20lack%20of%20an%20efficient%20evaluation%20framework%20impedes%20the%20diagnosis%20of%20their%20performance%20bottlenecks.%20Current%20evaluation%20primarily%20relies%20on%20testing%20after%20supervised%20fine-tuning%2C%20which%20introduces%20laborious%20additional%20training%20and%20autoregressive%20decoding%20costs.%20Meanwhile%2C%20common%20pre-training%20metrics%20cannot%20quantify%20a%20model%27s%20perception%20and%20reasoning%20abilities%20in%20a%20disentangled%20manner.%20Furthermore%2C%20existing%20evaluation%20benchmarks%20are%20typically%20limited%20in%20scale%20or%20misaligned%20with%20pre-training%20objectives.%20Thus%2C%20we%20propose%20RADAR%2C%20an%20efficient%20ability-centric%20evaluation%20framework%20for%20Revealing%20Asymmetric%20Development%20of%20Abilities%20in%20MLLM%20pRe-training.%20RADAR%20involves%20two%20key%20components%3A%20%281%29%20Soft%20Discrimination%20Score%2C%20a%20novel%20metric%20for%20robustly%20tracking%20ability%20development%20without%20fine-tuning%2C%20based%20on%20quantifying%20nuanced%20gradations%20of%20the%20model%20preference%20for%20the%20correct%20answer%20over%20distractors%3B%20and%20%282%29%20Multi-Modal%20Mixture%20Benchmark%2C%20a%20new%2015K%2B%20sample%20benchmark%20for%20comprehensively%20evaluating%20pre-trained%20MLLMs%27%20perception%20and%20reasoning%20abilities%20in%20a%200-shot%20manner%2C%20where%20we%20unify%20authoritative%20benchmark%20datasets%20and%20carefully%20collect%20new%20datasets%2C%20extending%20the%20evaluation%20scope%20and%20addressing%20the%20critical%20gaps%20in%20current%20benchmarks.%20With%20RADAR%2C%20we%20comprehensively%20reveal%20the%20asymmetric%20development%20of%20perceptual%20and%20reasoning%20capabilities%20in%20pretrained%20MLLMs%20across%20diverse%20factors%2C%20including%20data%20volume%2C%20model%20size%2C%20and%20pretraining%20strategy.%20Our%20RADAR%20underscores%20the%20need%20for%20a%20decomposed%20perspective%20on%20pre-training%20ability%20bottlenecks%2C%20informing%20targeted%20interventions%20to%20advance%20MLLMs%20efficiently.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/Nieysh/RADAR.&entry.1838667208=http%3A//arxiv.org/abs/2602.12892v1&entry.124074799=Read"},
{"title": "Structured Spectral Graph Representation Learning for Multi-label Abnormality Analysis from 3D CT Scans", "author": "Theo Di Piazza and Carole Lazarus and Olivier Nempont and Loic Boussel", "abstract": "With the growing volume of CT examinations, there is an increasing demand for automated tools such as organ segmentation, abnormality detection, and report generation to support radiologists in managing their clinical workload. Multi-label classification of 3D Chest CT scans remains a critical yet challenging problem due to the complex spatial relationships inherent in volumetric data and the wide variability of abnormalities. Existing methods based on 3D convolutional neural networks struggle to capture long-range dependencies, while Vision Transformers often require extensive pre-training on large-scale, domain-specific datasets to perform competitively. In this work of academic research, we propose a 2.5D alternative by introducing a new graph-based framework that represents 3D CT volumes as structured graphs, where axial slice triplets serve as nodes processed through spectral graph convolution, enabling the model to reason over inter-slice dependencies while maintaining complexity compatible with clinical deployment. Our method, trained and evaluated on 3 datasets from independent institutions, achieves strong cross-dataset generalization, and shows competitive performance compared to state-of-the-art visual encoders. We further conduct comprehensive ablation studies to evaluate the impact of various aggregation strategies, edge-weighting schemes, and graph connectivity patterns. Additionally, we demonstrate the broader applicability of our approach through transfer experiments on automated radiology report generation and abdominal CT data.", "link": "http://arxiv.org/abs/2510.10779v3", "date": "2026-02-13", "relevancy": 2.8047, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5815}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Spectral%20Graph%20Representation%20Learning%20for%20Multi-label%20Abnormality%20Analysis%20from%203D%20CT%20Scans&body=Title%3A%20Structured%20Spectral%20Graph%20Representation%20Learning%20for%20Multi-label%20Abnormality%20Analysis%20from%203D%20CT%20Scans%0AAuthor%3A%20Theo%20Di%20Piazza%20and%20Carole%20Lazarus%20and%20Olivier%20Nempont%20and%20Loic%20Boussel%0AAbstract%3A%20With%20the%20growing%20volume%20of%20CT%20examinations%2C%20there%20is%20an%20increasing%20demand%20for%20automated%20tools%20such%20as%20organ%20segmentation%2C%20abnormality%20detection%2C%20and%20report%20generation%20to%20support%20radiologists%20in%20managing%20their%20clinical%20workload.%20Multi-label%20classification%20of%203D%20Chest%20CT%20scans%20remains%20a%20critical%20yet%20challenging%20problem%20due%20to%20the%20complex%20spatial%20relationships%20inherent%20in%20volumetric%20data%20and%20the%20wide%20variability%20of%20abnormalities.%20Existing%20methods%20based%20on%203D%20convolutional%20neural%20networks%20struggle%20to%20capture%20long-range%20dependencies%2C%20while%20Vision%20Transformers%20often%20require%20extensive%20pre-training%20on%20large-scale%2C%20domain-specific%20datasets%20to%20perform%20competitively.%20In%20this%20work%20of%20academic%20research%2C%20we%20propose%20a%202.5D%20alternative%20by%20introducing%20a%20new%20graph-based%20framework%20that%20represents%203D%20CT%20volumes%20as%20structured%20graphs%2C%20where%20axial%20slice%20triplets%20serve%20as%20nodes%20processed%20through%20spectral%20graph%20convolution%2C%20enabling%20the%20model%20to%20reason%20over%20inter-slice%20dependencies%20while%20maintaining%20complexity%20compatible%20with%20clinical%20deployment.%20Our%20method%2C%20trained%20and%20evaluated%20on%203%20datasets%20from%20independent%20institutions%2C%20achieves%20strong%20cross-dataset%20generalization%2C%20and%20shows%20competitive%20performance%20compared%20to%20state-of-the-art%20visual%20encoders.%20We%20further%20conduct%20comprehensive%20ablation%20studies%20to%20evaluate%20the%20impact%20of%20various%20aggregation%20strategies%2C%20edge-weighting%20schemes%2C%20and%20graph%20connectivity%20patterns.%20Additionally%2C%20we%20demonstrate%20the%20broader%20applicability%20of%20our%20approach%20through%20transfer%20experiments%20on%20automated%20radiology%20report%20generation%20and%20abdominal%20CT%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10779v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Spectral%2520Graph%2520Representation%2520Learning%2520for%2520Multi-label%2520Abnormality%2520Analysis%2520from%25203D%2520CT%2520Scans%26entry.906535625%3DTheo%2520Di%2520Piazza%2520and%2520Carole%2520Lazarus%2520and%2520Olivier%2520Nempont%2520and%2520Loic%2520Boussel%26entry.1292438233%3DWith%2520the%2520growing%2520volume%2520of%2520CT%2520examinations%252C%2520there%2520is%2520an%2520increasing%2520demand%2520for%2520automated%2520tools%2520such%2520as%2520organ%2520segmentation%252C%2520abnormality%2520detection%252C%2520and%2520report%2520generation%2520to%2520support%2520radiologists%2520in%2520managing%2520their%2520clinical%2520workload.%2520Multi-label%2520classification%2520of%25203D%2520Chest%2520CT%2520scans%2520remains%2520a%2520critical%2520yet%2520challenging%2520problem%2520due%2520to%2520the%2520complex%2520spatial%2520relationships%2520inherent%2520in%2520volumetric%2520data%2520and%2520the%2520wide%2520variability%2520of%2520abnormalities.%2520Existing%2520methods%2520based%2520on%25203D%2520convolutional%2520neural%2520networks%2520struggle%2520to%2520capture%2520long-range%2520dependencies%252C%2520while%2520Vision%2520Transformers%2520often%2520require%2520extensive%2520pre-training%2520on%2520large-scale%252C%2520domain-specific%2520datasets%2520to%2520perform%2520competitively.%2520In%2520this%2520work%2520of%2520academic%2520research%252C%2520we%2520propose%2520a%25202.5D%2520alternative%2520by%2520introducing%2520a%2520new%2520graph-based%2520framework%2520that%2520represents%25203D%2520CT%2520volumes%2520as%2520structured%2520graphs%252C%2520where%2520axial%2520slice%2520triplets%2520serve%2520as%2520nodes%2520processed%2520through%2520spectral%2520graph%2520convolution%252C%2520enabling%2520the%2520model%2520to%2520reason%2520over%2520inter-slice%2520dependencies%2520while%2520maintaining%2520complexity%2520compatible%2520with%2520clinical%2520deployment.%2520Our%2520method%252C%2520trained%2520and%2520evaluated%2520on%25203%2520datasets%2520from%2520independent%2520institutions%252C%2520achieves%2520strong%2520cross-dataset%2520generalization%252C%2520and%2520shows%2520competitive%2520performance%2520compared%2520to%2520state-of-the-art%2520visual%2520encoders.%2520We%2520further%2520conduct%2520comprehensive%2520ablation%2520studies%2520to%2520evaluate%2520the%2520impact%2520of%2520various%2520aggregation%2520strategies%252C%2520edge-weighting%2520schemes%252C%2520and%2520graph%2520connectivity%2520patterns.%2520Additionally%252C%2520we%2520demonstrate%2520the%2520broader%2520applicability%2520of%2520our%2520approach%2520through%2520transfer%2520experiments%2520on%2520automated%2520radiology%2520report%2520generation%2520and%2520abdominal%2520CT%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10779v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Spectral%20Graph%20Representation%20Learning%20for%20Multi-label%20Abnormality%20Analysis%20from%203D%20CT%20Scans&entry.906535625=Theo%20Di%20Piazza%20and%20Carole%20Lazarus%20and%20Olivier%20Nempont%20and%20Loic%20Boussel&entry.1292438233=With%20the%20growing%20volume%20of%20CT%20examinations%2C%20there%20is%20an%20increasing%20demand%20for%20automated%20tools%20such%20as%20organ%20segmentation%2C%20abnormality%20detection%2C%20and%20report%20generation%20to%20support%20radiologists%20in%20managing%20their%20clinical%20workload.%20Multi-label%20classification%20of%203D%20Chest%20CT%20scans%20remains%20a%20critical%20yet%20challenging%20problem%20due%20to%20the%20complex%20spatial%20relationships%20inherent%20in%20volumetric%20data%20and%20the%20wide%20variability%20of%20abnormalities.%20Existing%20methods%20based%20on%203D%20convolutional%20neural%20networks%20struggle%20to%20capture%20long-range%20dependencies%2C%20while%20Vision%20Transformers%20often%20require%20extensive%20pre-training%20on%20large-scale%2C%20domain-specific%20datasets%20to%20perform%20competitively.%20In%20this%20work%20of%20academic%20research%2C%20we%20propose%20a%202.5D%20alternative%20by%20introducing%20a%20new%20graph-based%20framework%20that%20represents%203D%20CT%20volumes%20as%20structured%20graphs%2C%20where%20axial%20slice%20triplets%20serve%20as%20nodes%20processed%20through%20spectral%20graph%20convolution%2C%20enabling%20the%20model%20to%20reason%20over%20inter-slice%20dependencies%20while%20maintaining%20complexity%20compatible%20with%20clinical%20deployment.%20Our%20method%2C%20trained%20and%20evaluated%20on%203%20datasets%20from%20independent%20institutions%2C%20achieves%20strong%20cross-dataset%20generalization%2C%20and%20shows%20competitive%20performance%20compared%20to%20state-of-the-art%20visual%20encoders.%20We%20further%20conduct%20comprehensive%20ablation%20studies%20to%20evaluate%20the%20impact%20of%20various%20aggregation%20strategies%2C%20edge-weighting%20schemes%2C%20and%20graph%20connectivity%20patterns.%20Additionally%2C%20we%20demonstrate%20the%20broader%20applicability%20of%20our%20approach%20through%20transfer%20experiments%20on%20automated%20radiology%20report%20generation%20and%20abdominal%20CT%20data.&entry.1838667208=http%3A//arxiv.org/abs/2510.10779v3&entry.124074799=Read"},
{"title": "Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images", "author": "Yuhao Chen and Gautham Vinod and Siddeshwar Raghavan and Talha Ibn Mahmud and Bruce Coburn and Jinge Ma and Fengqing Zhu and Jiangpeng He", "abstract": "We present Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images, a benchmark dataset designed to advance geometry-based food portion estimation in realistic dining scenarios. Existing dietary assessment methods largely rely on single-image analysis or appearance-based inference, including recent vision-language models, which lack explicit geometric reasoning and are sensitive to scale ambiguity. This benchmark reframes food portion estimation as an implicit-scale 3D reconstruction problem under monocular observations. To reflect real-world conditions, explicit physical references and metric annotations are removed; instead, contextual objects such as plates and utensils are provided, requiring algorithms to infer scale from implicit cues and prior knowledge. The dataset emphasizes multi-food scenes with diverse object geometries, frequent occlusions, and complex spatial arrangements. The benchmark was adopted as a challenge at the MetaFood 2025 Workshop, where multiple teams proposed reconstruction-based solutions. Experimental results show that while strong vision--language baselines achieve competitive performance, geometry-based reconstruction methods provide both improved accuracy and greater robustness, with the top-performing approach achieving 0.21 MAPE in volume estimation and 5.7 L1 Chamfer Distance in geometric accuracy.", "link": "http://arxiv.org/abs/2602.13041v1", "date": "2026-02-13", "relevancy": 2.7775, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5645}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5577}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit-Scale%203D%20Reconstruction%20for%20Multi-Food%20Volume%20Estimation%20from%20Monocular%20Images&body=Title%3A%20Implicit-Scale%203D%20Reconstruction%20for%20Multi-Food%20Volume%20Estimation%20from%20Monocular%20Images%0AAuthor%3A%20Yuhao%20Chen%20and%20Gautham%20Vinod%20and%20Siddeshwar%20Raghavan%20and%20Talha%20Ibn%20Mahmud%20and%20Bruce%20Coburn%20and%20Jinge%20Ma%20and%20Fengqing%20Zhu%20and%20Jiangpeng%20He%0AAbstract%3A%20We%20present%20Implicit-Scale%203D%20Reconstruction%20from%20Monocular%20Multi-Food%20Images%2C%20a%20benchmark%20dataset%20designed%20to%20advance%20geometry-based%20food%20portion%20estimation%20in%20realistic%20dining%20scenarios.%20Existing%20dietary%20assessment%20methods%20largely%20rely%20on%20single-image%20analysis%20or%20appearance-based%20inference%2C%20including%20recent%20vision-language%20models%2C%20which%20lack%20explicit%20geometric%20reasoning%20and%20are%20sensitive%20to%20scale%20ambiguity.%20This%20benchmark%20reframes%20food%20portion%20estimation%20as%20an%20implicit-scale%203D%20reconstruction%20problem%20under%20monocular%20observations.%20To%20reflect%20real-world%20conditions%2C%20explicit%20physical%20references%20and%20metric%20annotations%20are%20removed%3B%20instead%2C%20contextual%20objects%20such%20as%20plates%20and%20utensils%20are%20provided%2C%20requiring%20algorithms%20to%20infer%20scale%20from%20implicit%20cues%20and%20prior%20knowledge.%20The%20dataset%20emphasizes%20multi-food%20scenes%20with%20diverse%20object%20geometries%2C%20frequent%20occlusions%2C%20and%20complex%20spatial%20arrangements.%20The%20benchmark%20was%20adopted%20as%20a%20challenge%20at%20the%20MetaFood%202025%20Workshop%2C%20where%20multiple%20teams%20proposed%20reconstruction-based%20solutions.%20Experimental%20results%20show%20that%20while%20strong%20vision--language%20baselines%20achieve%20competitive%20performance%2C%20geometry-based%20reconstruction%20methods%20provide%20both%20improved%20accuracy%20and%20greater%20robustness%2C%20with%20the%20top-performing%20approach%20achieving%200.21%20MAPE%20in%20volume%20estimation%20and%205.7%20L1%20Chamfer%20Distance%20in%20geometric%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit-Scale%25203D%2520Reconstruction%2520for%2520Multi-Food%2520Volume%2520Estimation%2520from%2520Monocular%2520Images%26entry.906535625%3DYuhao%2520Chen%2520and%2520Gautham%2520Vinod%2520and%2520Siddeshwar%2520Raghavan%2520and%2520Talha%2520Ibn%2520Mahmud%2520and%2520Bruce%2520Coburn%2520and%2520Jinge%2520Ma%2520and%2520Fengqing%2520Zhu%2520and%2520Jiangpeng%2520He%26entry.1292438233%3DWe%2520present%2520Implicit-Scale%25203D%2520Reconstruction%2520from%2520Monocular%2520Multi-Food%2520Images%252C%2520a%2520benchmark%2520dataset%2520designed%2520to%2520advance%2520geometry-based%2520food%2520portion%2520estimation%2520in%2520realistic%2520dining%2520scenarios.%2520Existing%2520dietary%2520assessment%2520methods%2520largely%2520rely%2520on%2520single-image%2520analysis%2520or%2520appearance-based%2520inference%252C%2520including%2520recent%2520vision-language%2520models%252C%2520which%2520lack%2520explicit%2520geometric%2520reasoning%2520and%2520are%2520sensitive%2520to%2520scale%2520ambiguity.%2520This%2520benchmark%2520reframes%2520food%2520portion%2520estimation%2520as%2520an%2520implicit-scale%25203D%2520reconstruction%2520problem%2520under%2520monocular%2520observations.%2520To%2520reflect%2520real-world%2520conditions%252C%2520explicit%2520physical%2520references%2520and%2520metric%2520annotations%2520are%2520removed%253B%2520instead%252C%2520contextual%2520objects%2520such%2520as%2520plates%2520and%2520utensils%2520are%2520provided%252C%2520requiring%2520algorithms%2520to%2520infer%2520scale%2520from%2520implicit%2520cues%2520and%2520prior%2520knowledge.%2520The%2520dataset%2520emphasizes%2520multi-food%2520scenes%2520with%2520diverse%2520object%2520geometries%252C%2520frequent%2520occlusions%252C%2520and%2520complex%2520spatial%2520arrangements.%2520The%2520benchmark%2520was%2520adopted%2520as%2520a%2520challenge%2520at%2520the%2520MetaFood%25202025%2520Workshop%252C%2520where%2520multiple%2520teams%2520proposed%2520reconstruction-based%2520solutions.%2520Experimental%2520results%2520show%2520that%2520while%2520strong%2520vision--language%2520baselines%2520achieve%2520competitive%2520performance%252C%2520geometry-based%2520reconstruction%2520methods%2520provide%2520both%2520improved%2520accuracy%2520and%2520greater%2520robustness%252C%2520with%2520the%2520top-performing%2520approach%2520achieving%25200.21%2520MAPE%2520in%2520volume%2520estimation%2520and%25205.7%2520L1%2520Chamfer%2520Distance%2520in%2520geometric%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit-Scale%203D%20Reconstruction%20for%20Multi-Food%20Volume%20Estimation%20from%20Monocular%20Images&entry.906535625=Yuhao%20Chen%20and%20Gautham%20Vinod%20and%20Siddeshwar%20Raghavan%20and%20Talha%20Ibn%20Mahmud%20and%20Bruce%20Coburn%20and%20Jinge%20Ma%20and%20Fengqing%20Zhu%20and%20Jiangpeng%20He&entry.1292438233=We%20present%20Implicit-Scale%203D%20Reconstruction%20from%20Monocular%20Multi-Food%20Images%2C%20a%20benchmark%20dataset%20designed%20to%20advance%20geometry-based%20food%20portion%20estimation%20in%20realistic%20dining%20scenarios.%20Existing%20dietary%20assessment%20methods%20largely%20rely%20on%20single-image%20analysis%20or%20appearance-based%20inference%2C%20including%20recent%20vision-language%20models%2C%20which%20lack%20explicit%20geometric%20reasoning%20and%20are%20sensitive%20to%20scale%20ambiguity.%20This%20benchmark%20reframes%20food%20portion%20estimation%20as%20an%20implicit-scale%203D%20reconstruction%20problem%20under%20monocular%20observations.%20To%20reflect%20real-world%20conditions%2C%20explicit%20physical%20references%20and%20metric%20annotations%20are%20removed%3B%20instead%2C%20contextual%20objects%20such%20as%20plates%20and%20utensils%20are%20provided%2C%20requiring%20algorithms%20to%20infer%20scale%20from%20implicit%20cues%20and%20prior%20knowledge.%20The%20dataset%20emphasizes%20multi-food%20scenes%20with%20diverse%20object%20geometries%2C%20frequent%20occlusions%2C%20and%20complex%20spatial%20arrangements.%20The%20benchmark%20was%20adopted%20as%20a%20challenge%20at%20the%20MetaFood%202025%20Workshop%2C%20where%20multiple%20teams%20proposed%20reconstruction-based%20solutions.%20Experimental%20results%20show%20that%20while%20strong%20vision--language%20baselines%20achieve%20competitive%20performance%2C%20geometry-based%20reconstruction%20methods%20provide%20both%20improved%20accuracy%20and%20greater%20robustness%2C%20with%20the%20top-performing%20approach%20achieving%200.21%20MAPE%20in%20volume%20estimation%20and%205.7%20L1%20Chamfer%20Distance%20in%20geometric%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2602.13041v1&entry.124074799=Read"},
{"title": "Geometric Manifold Rectification for Imbalanced Learning", "author": "Xubin Wang and Qing Li and Weijia Jia", "abstract": "Imbalanced classification presents a formidable challenge in machine learning, particularly when tabular datasets are plagued by noise and overlapping class boundaries. From a geometric perspective, the core difficulty lies in the topological intrusion of the majority class into the minority manifold, which obscures the true decision boundary. Traditional undersampling techniques, such as Edited Nearest Neighbours (ENN), typically employ symmetric cleaning rules and uniform voting, failing to capture the local manifold structure and often inadvertently removing informative minority samples. In this paper, we propose GMR (Geometric Manifold Rectification), a novel framework designed to robustly handle imbalanced structured data by exploiting local geometric priors. GMR makes two contributions: (1) Geometric confidence estimation that uses inverse-distance weighted kNN voting with an adaptive distance metric to capture local reliability; and (2) asymmetric cleaning that is strict on majority samples while conservatively protecting minority samples via a safe-guarding cap on minority removal. Extensive experiments on multiple benchmark datasets show that GMR is competitive with strong sampling baselines.", "link": "http://arxiv.org/abs/2602.13045v1", "date": "2026-02-13", "relevancy": 2.7333, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5749}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5405}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Manifold%20Rectification%20for%20Imbalanced%20Learning&body=Title%3A%20Geometric%20Manifold%20Rectification%20for%20Imbalanced%20Learning%0AAuthor%3A%20Xubin%20Wang%20and%20Qing%20Li%20and%20Weijia%20Jia%0AAbstract%3A%20Imbalanced%20classification%20presents%20a%20formidable%20challenge%20in%20machine%20learning%2C%20particularly%20when%20tabular%20datasets%20are%20plagued%20by%20noise%20and%20overlapping%20class%20boundaries.%20From%20a%20geometric%20perspective%2C%20the%20core%20difficulty%20lies%20in%20the%20topological%20intrusion%20of%20the%20majority%20class%20into%20the%20minority%20manifold%2C%20which%20obscures%20the%20true%20decision%20boundary.%20Traditional%20undersampling%20techniques%2C%20such%20as%20Edited%20Nearest%20Neighbours%20%28ENN%29%2C%20typically%20employ%20symmetric%20cleaning%20rules%20and%20uniform%20voting%2C%20failing%20to%20capture%20the%20local%20manifold%20structure%20and%20often%20inadvertently%20removing%20informative%20minority%20samples.%20In%20this%20paper%2C%20we%20propose%20GMR%20%28Geometric%20Manifold%20Rectification%29%2C%20a%20novel%20framework%20designed%20to%20robustly%20handle%20imbalanced%20structured%20data%20by%20exploiting%20local%20geometric%20priors.%20GMR%20makes%20two%20contributions%3A%20%281%29%20Geometric%20confidence%20estimation%20that%20uses%20inverse-distance%20weighted%20kNN%20voting%20with%20an%20adaptive%20distance%20metric%20to%20capture%20local%20reliability%3B%20and%20%282%29%20asymmetric%20cleaning%20that%20is%20strict%20on%20majority%20samples%20while%20conservatively%20protecting%20minority%20samples%20via%20a%20safe-guarding%20cap%20on%20minority%20removal.%20Extensive%20experiments%20on%20multiple%20benchmark%20datasets%20show%20that%20GMR%20is%20competitive%20with%20strong%20sampling%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Manifold%2520Rectification%2520for%2520Imbalanced%2520Learning%26entry.906535625%3DXubin%2520Wang%2520and%2520Qing%2520Li%2520and%2520Weijia%2520Jia%26entry.1292438233%3DImbalanced%2520classification%2520presents%2520a%2520formidable%2520challenge%2520in%2520machine%2520learning%252C%2520particularly%2520when%2520tabular%2520datasets%2520are%2520plagued%2520by%2520noise%2520and%2520overlapping%2520class%2520boundaries.%2520From%2520a%2520geometric%2520perspective%252C%2520the%2520core%2520difficulty%2520lies%2520in%2520the%2520topological%2520intrusion%2520of%2520the%2520majority%2520class%2520into%2520the%2520minority%2520manifold%252C%2520which%2520obscures%2520the%2520true%2520decision%2520boundary.%2520Traditional%2520undersampling%2520techniques%252C%2520such%2520as%2520Edited%2520Nearest%2520Neighbours%2520%2528ENN%2529%252C%2520typically%2520employ%2520symmetric%2520cleaning%2520rules%2520and%2520uniform%2520voting%252C%2520failing%2520to%2520capture%2520the%2520local%2520manifold%2520structure%2520and%2520often%2520inadvertently%2520removing%2520informative%2520minority%2520samples.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GMR%2520%2528Geometric%2520Manifold%2520Rectification%2529%252C%2520a%2520novel%2520framework%2520designed%2520to%2520robustly%2520handle%2520imbalanced%2520structured%2520data%2520by%2520exploiting%2520local%2520geometric%2520priors.%2520GMR%2520makes%2520two%2520contributions%253A%2520%25281%2529%2520Geometric%2520confidence%2520estimation%2520that%2520uses%2520inverse-distance%2520weighted%2520kNN%2520voting%2520with%2520an%2520adaptive%2520distance%2520metric%2520to%2520capture%2520local%2520reliability%253B%2520and%2520%25282%2529%2520asymmetric%2520cleaning%2520that%2520is%2520strict%2520on%2520majority%2520samples%2520while%2520conservatively%2520protecting%2520minority%2520samples%2520via%2520a%2520safe-guarding%2520cap%2520on%2520minority%2520removal.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmark%2520datasets%2520show%2520that%2520GMR%2520is%2520competitive%2520with%2520strong%2520sampling%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Manifold%20Rectification%20for%20Imbalanced%20Learning&entry.906535625=Xubin%20Wang%20and%20Qing%20Li%20and%20Weijia%20Jia&entry.1292438233=Imbalanced%20classification%20presents%20a%20formidable%20challenge%20in%20machine%20learning%2C%20particularly%20when%20tabular%20datasets%20are%20plagued%20by%20noise%20and%20overlapping%20class%20boundaries.%20From%20a%20geometric%20perspective%2C%20the%20core%20difficulty%20lies%20in%20the%20topological%20intrusion%20of%20the%20majority%20class%20into%20the%20minority%20manifold%2C%20which%20obscures%20the%20true%20decision%20boundary.%20Traditional%20undersampling%20techniques%2C%20such%20as%20Edited%20Nearest%20Neighbours%20%28ENN%29%2C%20typically%20employ%20symmetric%20cleaning%20rules%20and%20uniform%20voting%2C%20failing%20to%20capture%20the%20local%20manifold%20structure%20and%20often%20inadvertently%20removing%20informative%20minority%20samples.%20In%20this%20paper%2C%20we%20propose%20GMR%20%28Geometric%20Manifold%20Rectification%29%2C%20a%20novel%20framework%20designed%20to%20robustly%20handle%20imbalanced%20structured%20data%20by%20exploiting%20local%20geometric%20priors.%20GMR%20makes%20two%20contributions%3A%20%281%29%20Geometric%20confidence%20estimation%20that%20uses%20inverse-distance%20weighted%20kNN%20voting%20with%20an%20adaptive%20distance%20metric%20to%20capture%20local%20reliability%3B%20and%20%282%29%20asymmetric%20cleaning%20that%20is%20strict%20on%20majority%20samples%20while%20conservatively%20protecting%20minority%20samples%20via%20a%20safe-guarding%20cap%20on%20minority%20removal.%20Extensive%20experiments%20on%20multiple%20benchmark%20datasets%20show%20that%20GMR%20is%20competitive%20with%20strong%20sampling%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2602.13045v1&entry.124074799=Read"},
{"title": "MissionHD: Hyperdimensional Refinement of Distribution-Deficient Reasoning Graphs for Video Anomaly Detection", "author": "Sanggeon Yun and Raheeb Hassan and Ryozo Masukawa and Nathaniel D. Bastian and Mohsen Imani", "abstract": "LLM-generated reasoning graphs, referred to as mission-specific graphs (MSGs), are increasingly used for video anomaly detection (VAD) and recognition (VAR). However, they are typically treated as fixed despite being generic and distribution-deficient. Conventional graph structure refinement (GSR) methods are ill-suited to this setting, as they rely on learning structural distributions that are absent in LLM-generated graphs. We propose HDC-constrained Graph Structure Refinement (HDC-GSR), a new paradigm that directly optimizes a decodable, task-aligned graph representation in a single hyperdimensional space without distribution modeling. Leveraging Hyperdimensional Computing (HDC), our framework encodes graphs via binding and bundling operations, aligns the resulting graph code with downstream loss, and decodes edge contributions to refine the structure. We instantiate this approach as MissionHD for weakly supervised VAD/VAR and demonstrate consistent performance gains on benchmark datasets.", "link": "http://arxiv.org/abs/2508.14746v4", "date": "2026-02-13", "relevancy": 2.6758, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5498}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MissionHD%3A%20Hyperdimensional%20Refinement%20of%20Distribution-Deficient%20Reasoning%20Graphs%20for%20Video%20Anomaly%20Detection&body=Title%3A%20MissionHD%3A%20Hyperdimensional%20Refinement%20of%20Distribution-Deficient%20Reasoning%20Graphs%20for%20Video%20Anomaly%20Detection%0AAuthor%3A%20Sanggeon%20Yun%20and%20Raheeb%20Hassan%20and%20Ryozo%20Masukawa%20and%20Nathaniel%20D.%20Bastian%20and%20Mohsen%20Imani%0AAbstract%3A%20LLM-generated%20reasoning%20graphs%2C%20referred%20to%20as%20mission-specific%20graphs%20%28MSGs%29%2C%20are%20increasingly%20used%20for%20video%20anomaly%20detection%20%28VAD%29%20and%20recognition%20%28VAR%29.%20However%2C%20they%20are%20typically%20treated%20as%20fixed%20despite%20being%20generic%20and%20distribution-deficient.%20Conventional%20graph%20structure%20refinement%20%28GSR%29%20methods%20are%20ill-suited%20to%20this%20setting%2C%20as%20they%20rely%20on%20learning%20structural%20distributions%20that%20are%20absent%20in%20LLM-generated%20graphs.%20We%20propose%20HDC-constrained%20Graph%20Structure%20Refinement%20%28HDC-GSR%29%2C%20a%20new%20paradigm%20that%20directly%20optimizes%20a%20decodable%2C%20task-aligned%20graph%20representation%20in%20a%20single%20hyperdimensional%20space%20without%20distribution%20modeling.%20Leveraging%20Hyperdimensional%20Computing%20%28HDC%29%2C%20our%20framework%20encodes%20graphs%20via%20binding%20and%20bundling%20operations%2C%20aligns%20the%20resulting%20graph%20code%20with%20downstream%20loss%2C%20and%20decodes%20edge%20contributions%20to%20refine%20the%20structure.%20We%20instantiate%20this%20approach%20as%20MissionHD%20for%20weakly%20supervised%20VAD/VAR%20and%20demonstrate%20consistent%20performance%20gains%20on%20benchmark%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2508.14746v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMissionHD%253A%2520Hyperdimensional%2520Refinement%2520of%2520Distribution-Deficient%2520Reasoning%2520Graphs%2520for%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DSanggeon%2520Yun%2520and%2520Raheeb%2520Hassan%2520and%2520Ryozo%2520Masukawa%2520and%2520Nathaniel%2520D.%2520Bastian%2520and%2520Mohsen%2520Imani%26entry.1292438233%3DLLM-generated%2520reasoning%2520graphs%252C%2520referred%2520to%2520as%2520mission-specific%2520graphs%2520%2528MSGs%2529%252C%2520are%2520increasingly%2520used%2520for%2520video%2520anomaly%2520detection%2520%2528VAD%2529%2520and%2520recognition%2520%2528VAR%2529.%2520However%252C%2520they%2520are%2520typically%2520treated%2520as%2520fixed%2520despite%2520being%2520generic%2520and%2520distribution-deficient.%2520Conventional%2520graph%2520structure%2520refinement%2520%2528GSR%2529%2520methods%2520are%2520ill-suited%2520to%2520this%2520setting%252C%2520as%2520they%2520rely%2520on%2520learning%2520structural%2520distributions%2520that%2520are%2520absent%2520in%2520LLM-generated%2520graphs.%2520We%2520propose%2520HDC-constrained%2520Graph%2520Structure%2520Refinement%2520%2528HDC-GSR%2529%252C%2520a%2520new%2520paradigm%2520that%2520directly%2520optimizes%2520a%2520decodable%252C%2520task-aligned%2520graph%2520representation%2520in%2520a%2520single%2520hyperdimensional%2520space%2520without%2520distribution%2520modeling.%2520Leveraging%2520Hyperdimensional%2520Computing%2520%2528HDC%2529%252C%2520our%2520framework%2520encodes%2520graphs%2520via%2520binding%2520and%2520bundling%2520operations%252C%2520aligns%2520the%2520resulting%2520graph%2520code%2520with%2520downstream%2520loss%252C%2520and%2520decodes%2520edge%2520contributions%2520to%2520refine%2520the%2520structure.%2520We%2520instantiate%2520this%2520approach%2520as%2520MissionHD%2520for%2520weakly%2520supervised%2520VAD/VAR%2520and%2520demonstrate%2520consistent%2520performance%2520gains%2520on%2520benchmark%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14746v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MissionHD%3A%20Hyperdimensional%20Refinement%20of%20Distribution-Deficient%20Reasoning%20Graphs%20for%20Video%20Anomaly%20Detection&entry.906535625=Sanggeon%20Yun%20and%20Raheeb%20Hassan%20and%20Ryozo%20Masukawa%20and%20Nathaniel%20D.%20Bastian%20and%20Mohsen%20Imani&entry.1292438233=LLM-generated%20reasoning%20graphs%2C%20referred%20to%20as%20mission-specific%20graphs%20%28MSGs%29%2C%20are%20increasingly%20used%20for%20video%20anomaly%20detection%20%28VAD%29%20and%20recognition%20%28VAR%29.%20However%2C%20they%20are%20typically%20treated%20as%20fixed%20despite%20being%20generic%20and%20distribution-deficient.%20Conventional%20graph%20structure%20refinement%20%28GSR%29%20methods%20are%20ill-suited%20to%20this%20setting%2C%20as%20they%20rely%20on%20learning%20structural%20distributions%20that%20are%20absent%20in%20LLM-generated%20graphs.%20We%20propose%20HDC-constrained%20Graph%20Structure%20Refinement%20%28HDC-GSR%29%2C%20a%20new%20paradigm%20that%20directly%20optimizes%20a%20decodable%2C%20task-aligned%20graph%20representation%20in%20a%20single%20hyperdimensional%20space%20without%20distribution%20modeling.%20Leveraging%20Hyperdimensional%20Computing%20%28HDC%29%2C%20our%20framework%20encodes%20graphs%20via%20binding%20and%20bundling%20operations%2C%20aligns%20the%20resulting%20graph%20code%20with%20downstream%20loss%2C%20and%20decodes%20edge%20contributions%20to%20refine%20the%20structure.%20We%20instantiate%20this%20approach%20as%20MissionHD%20for%20weakly%20supervised%20VAD/VAR%20and%20demonstrate%20consistent%20performance%20gains%20on%20benchmark%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2508.14746v4&entry.124074799=Read"},
{"title": "EPRBench: A High-Quality Benchmark Dataset for Event Stream Based Visual Place Recognition", "author": "Xiao Wang and Xingxing Xiong and Jinfeng Gao and Xufeng Lou and Bo Jiang and Si-bao Chen and Yaowei Wang and Yonghong Tian", "abstract": "Event stream-based Visual Place Recognition (VPR) is an emerging research direction that offers a compelling solution to the instability of conventional visible-light cameras under challenging conditions such as low illumination, overexposure, and high-speed motion. Recognizing the current scarcity of dedicated datasets in this domain, we introduce EPRBench, a high-quality benchmark specifically designed for event stream-based VPR. EPRBench comprises 10K event sequences and 65K event frames, collected using both handheld and vehicle-mounted setups to comprehensively capture real-world challenges across diverse viewpoints, weather conditions, and lighting scenarios. To support semantic-aware and language-integrated VPR research, we provide LLM-generated scene descriptions, subsequently refined through human annotation, establishing a solid foundation for integrating LLMs into event-based perception pipelines. To facilitate systematic evaluation, we implement and benchmark 15 state-of-the-art VPR algorithms on EPRBench, offering a strong baseline for future algorithmic comparisons. Furthermore, we propose a novel multi-modal fusion paradigm for VPR: leveraging LLMs to generate textual scene descriptions from raw event streams, which then guide spatially attentive token selection, cross-modal feature fusion, and multi-scale representation learning. This framework not only achieves highly accurate place recognition but also produces interpretable reasoning processes alongside its predictions, significantly enhancing model transparency and explainability. The dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID", "link": "http://arxiv.org/abs/2602.12919v1", "date": "2026-02-13", "relevancy": 2.6406, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EPRBench%3A%20A%20High-Quality%20Benchmark%20Dataset%20for%20Event%20Stream%20Based%20Visual%20Place%20Recognition&body=Title%3A%20EPRBench%3A%20A%20High-Quality%20Benchmark%20Dataset%20for%20Event%20Stream%20Based%20Visual%20Place%20Recognition%0AAuthor%3A%20Xiao%20Wang%20and%20Xingxing%20Xiong%20and%20Jinfeng%20Gao%20and%20Xufeng%20Lou%20and%20Bo%20Jiang%20and%20Si-bao%20Chen%20and%20Yaowei%20Wang%20and%20Yonghong%20Tian%0AAbstract%3A%20Event%20stream-based%20Visual%20Place%20Recognition%20%28VPR%29%20is%20an%20emerging%20research%20direction%20that%20offers%20a%20compelling%20solution%20to%20the%20instability%20of%20conventional%20visible-light%20cameras%20under%20challenging%20conditions%20such%20as%20low%20illumination%2C%20overexposure%2C%20and%20high-speed%20motion.%20Recognizing%20the%20current%20scarcity%20of%20dedicated%20datasets%20in%20this%20domain%2C%20we%20introduce%20EPRBench%2C%20a%20high-quality%20benchmark%20specifically%20designed%20for%20event%20stream-based%20VPR.%20EPRBench%20comprises%2010K%20event%20sequences%20and%2065K%20event%20frames%2C%20collected%20using%20both%20handheld%20and%20vehicle-mounted%20setups%20to%20comprehensively%20capture%20real-world%20challenges%20across%20diverse%20viewpoints%2C%20weather%20conditions%2C%20and%20lighting%20scenarios.%20To%20support%20semantic-aware%20and%20language-integrated%20VPR%20research%2C%20we%20provide%20LLM-generated%20scene%20descriptions%2C%20subsequently%20refined%20through%20human%20annotation%2C%20establishing%20a%20solid%20foundation%20for%20integrating%20LLMs%20into%20event-based%20perception%20pipelines.%20To%20facilitate%20systematic%20evaluation%2C%20we%20implement%20and%20benchmark%2015%20state-of-the-art%20VPR%20algorithms%20on%20EPRBench%2C%20offering%20a%20strong%20baseline%20for%20future%20algorithmic%20comparisons.%20Furthermore%2C%20we%20propose%20a%20novel%20multi-modal%20fusion%20paradigm%20for%20VPR%3A%20leveraging%20LLMs%20to%20generate%20textual%20scene%20descriptions%20from%20raw%20event%20streams%2C%20which%20then%20guide%20spatially%20attentive%20token%20selection%2C%20cross-modal%20feature%20fusion%2C%20and%20multi-scale%20representation%20learning.%20This%20framework%20not%20only%20achieves%20highly%20accurate%20place%20recognition%20but%20also%20produces%20interpretable%20reasoning%20processes%20alongside%20its%20predictions%2C%20significantly%20enhancing%20model%20transparency%20and%20explainability.%20The%20dataset%20and%20source%20code%20will%20be%20released%20on%20https%3A//github.com/Event-AHU/Neuromorphic_ReID%0ALink%3A%20http%3A//arxiv.org/abs/2602.12919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEPRBench%253A%2520A%2520High-Quality%2520Benchmark%2520Dataset%2520for%2520Event%2520Stream%2520Based%2520Visual%2520Place%2520Recognition%26entry.906535625%3DXiao%2520Wang%2520and%2520Xingxing%2520Xiong%2520and%2520Jinfeng%2520Gao%2520and%2520Xufeng%2520Lou%2520and%2520Bo%2520Jiang%2520and%2520Si-bao%2520Chen%2520and%2520Yaowei%2520Wang%2520and%2520Yonghong%2520Tian%26entry.1292438233%3DEvent%2520stream-based%2520Visual%2520Place%2520Recognition%2520%2528VPR%2529%2520is%2520an%2520emerging%2520research%2520direction%2520that%2520offers%2520a%2520compelling%2520solution%2520to%2520the%2520instability%2520of%2520conventional%2520visible-light%2520cameras%2520under%2520challenging%2520conditions%2520such%2520as%2520low%2520illumination%252C%2520overexposure%252C%2520and%2520high-speed%2520motion.%2520Recognizing%2520the%2520current%2520scarcity%2520of%2520dedicated%2520datasets%2520in%2520this%2520domain%252C%2520we%2520introduce%2520EPRBench%252C%2520a%2520high-quality%2520benchmark%2520specifically%2520designed%2520for%2520event%2520stream-based%2520VPR.%2520EPRBench%2520comprises%252010K%2520event%2520sequences%2520and%252065K%2520event%2520frames%252C%2520collected%2520using%2520both%2520handheld%2520and%2520vehicle-mounted%2520setups%2520to%2520comprehensively%2520capture%2520real-world%2520challenges%2520across%2520diverse%2520viewpoints%252C%2520weather%2520conditions%252C%2520and%2520lighting%2520scenarios.%2520To%2520support%2520semantic-aware%2520and%2520language-integrated%2520VPR%2520research%252C%2520we%2520provide%2520LLM-generated%2520scene%2520descriptions%252C%2520subsequently%2520refined%2520through%2520human%2520annotation%252C%2520establishing%2520a%2520solid%2520foundation%2520for%2520integrating%2520LLMs%2520into%2520event-based%2520perception%2520pipelines.%2520To%2520facilitate%2520systematic%2520evaluation%252C%2520we%2520implement%2520and%2520benchmark%252015%2520state-of-the-art%2520VPR%2520algorithms%2520on%2520EPRBench%252C%2520offering%2520a%2520strong%2520baseline%2520for%2520future%2520algorithmic%2520comparisons.%2520Furthermore%252C%2520we%2520propose%2520a%2520novel%2520multi-modal%2520fusion%2520paradigm%2520for%2520VPR%253A%2520leveraging%2520LLMs%2520to%2520generate%2520textual%2520scene%2520descriptions%2520from%2520raw%2520event%2520streams%252C%2520which%2520then%2520guide%2520spatially%2520attentive%2520token%2520selection%252C%2520cross-modal%2520feature%2520fusion%252C%2520and%2520multi-scale%2520representation%2520learning.%2520This%2520framework%2520not%2520only%2520achieves%2520highly%2520accurate%2520place%2520recognition%2520but%2520also%2520produces%2520interpretable%2520reasoning%2520processes%2520alongside%2520its%2520predictions%252C%2520significantly%2520enhancing%2520model%2520transparency%2520and%2520explainability.%2520The%2520dataset%2520and%2520source%2520code%2520will%2520be%2520released%2520on%2520https%253A//github.com/Event-AHU/Neuromorphic_ReID%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EPRBench%3A%20A%20High-Quality%20Benchmark%20Dataset%20for%20Event%20Stream%20Based%20Visual%20Place%20Recognition&entry.906535625=Xiao%20Wang%20and%20Xingxing%20Xiong%20and%20Jinfeng%20Gao%20and%20Xufeng%20Lou%20and%20Bo%20Jiang%20and%20Si-bao%20Chen%20and%20Yaowei%20Wang%20and%20Yonghong%20Tian&entry.1292438233=Event%20stream-based%20Visual%20Place%20Recognition%20%28VPR%29%20is%20an%20emerging%20research%20direction%20that%20offers%20a%20compelling%20solution%20to%20the%20instability%20of%20conventional%20visible-light%20cameras%20under%20challenging%20conditions%20such%20as%20low%20illumination%2C%20overexposure%2C%20and%20high-speed%20motion.%20Recognizing%20the%20current%20scarcity%20of%20dedicated%20datasets%20in%20this%20domain%2C%20we%20introduce%20EPRBench%2C%20a%20high-quality%20benchmark%20specifically%20designed%20for%20event%20stream-based%20VPR.%20EPRBench%20comprises%2010K%20event%20sequences%20and%2065K%20event%20frames%2C%20collected%20using%20both%20handheld%20and%20vehicle-mounted%20setups%20to%20comprehensively%20capture%20real-world%20challenges%20across%20diverse%20viewpoints%2C%20weather%20conditions%2C%20and%20lighting%20scenarios.%20To%20support%20semantic-aware%20and%20language-integrated%20VPR%20research%2C%20we%20provide%20LLM-generated%20scene%20descriptions%2C%20subsequently%20refined%20through%20human%20annotation%2C%20establishing%20a%20solid%20foundation%20for%20integrating%20LLMs%20into%20event-based%20perception%20pipelines.%20To%20facilitate%20systematic%20evaluation%2C%20we%20implement%20and%20benchmark%2015%20state-of-the-art%20VPR%20algorithms%20on%20EPRBench%2C%20offering%20a%20strong%20baseline%20for%20future%20algorithmic%20comparisons.%20Furthermore%2C%20we%20propose%20a%20novel%20multi-modal%20fusion%20paradigm%20for%20VPR%3A%20leveraging%20LLMs%20to%20generate%20textual%20scene%20descriptions%20from%20raw%20event%20streams%2C%20which%20then%20guide%20spatially%20attentive%20token%20selection%2C%20cross-modal%20feature%20fusion%2C%20and%20multi-scale%20representation%20learning.%20This%20framework%20not%20only%20achieves%20highly%20accurate%20place%20recognition%20but%20also%20produces%20interpretable%20reasoning%20processes%20alongside%20its%20predictions%2C%20significantly%20enhancing%20model%20transparency%20and%20explainability.%20The%20dataset%20and%20source%20code%20will%20be%20released%20on%20https%3A//github.com/Event-AHU/Neuromorphic_ReID&entry.1838667208=http%3A//arxiv.org/abs/2602.12919v1&entry.124074799=Read"},
{"title": "PixelRush: Ultra-Fast, Training-Free High-Resolution Image Generation via One-step Diffusion", "author": "Hong-Phuc Lai and Phong Nguyen and Anh Tran", "abstract": "Pre-trained diffusion models excel at generating high-quality images but remain inherently limited by their native training resolution. Recent training-free approaches have attempted to overcome this constraint by introducing interventions during the denoising process; however, these methods incur substantial computational overhead, often requiring more than five minutes to produce a single 4K image. In this paper, we present PixelRush, the first tuning-free framework for practical high-resolution text-to-image generation. Our method builds upon the established patch-based inference paradigm but eliminates the need for multiple inversion and regeneration cycles. Instead, PixelRush enables efficient patch-based denoising within a low-step regime. To address artifacts introduced by patch blending in few-step generation, we propose a seamless blending strategy. Furthermore, we mitigate over-smoothing effects through a noise injection mechanism. PixelRush delivers exceptional efficiency, generating 4K images in approximately 20 seconds representing a 10$\\times$ to 35$\\times$ speedup over state-of-the-art methods while maintaining superior visual fidelity. Extensive experiments validate both the performance gains and the quality of outputs achieved by our approach.", "link": "http://arxiv.org/abs/2602.12769v1", "date": "2026-02-13", "relevancy": 2.5935, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6868}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.656}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PixelRush%3A%20Ultra-Fast%2C%20Training-Free%20High-Resolution%20Image%20Generation%20via%20One-step%20Diffusion&body=Title%3A%20PixelRush%3A%20Ultra-Fast%2C%20Training-Free%20High-Resolution%20Image%20Generation%20via%20One-step%20Diffusion%0AAuthor%3A%20Hong-Phuc%20Lai%20and%20Phong%20Nguyen%20and%20Anh%20Tran%0AAbstract%3A%20Pre-trained%20diffusion%20models%20excel%20at%20generating%20high-quality%20images%20but%20remain%20inherently%20limited%20by%20their%20native%20training%20resolution.%20Recent%20training-free%20approaches%20have%20attempted%20to%20overcome%20this%20constraint%20by%20introducing%20interventions%20during%20the%20denoising%20process%3B%20however%2C%20these%20methods%20incur%20substantial%20computational%20overhead%2C%20often%20requiring%20more%20than%20five%20minutes%20to%20produce%20a%20single%204K%20image.%20In%20this%20paper%2C%20we%20present%20PixelRush%2C%20the%20first%20tuning-free%20framework%20for%20practical%20high-resolution%20text-to-image%20generation.%20Our%20method%20builds%20upon%20the%20established%20patch-based%20inference%20paradigm%20but%20eliminates%20the%20need%20for%20multiple%20inversion%20and%20regeneration%20cycles.%20Instead%2C%20PixelRush%20enables%20efficient%20patch-based%20denoising%20within%20a%20low-step%20regime.%20To%20address%20artifacts%20introduced%20by%20patch%20blending%20in%20few-step%20generation%2C%20we%20propose%20a%20seamless%20blending%20strategy.%20Furthermore%2C%20we%20mitigate%20over-smoothing%20effects%20through%20a%20noise%20injection%20mechanism.%20PixelRush%20delivers%20exceptional%20efficiency%2C%20generating%204K%20images%20in%20approximately%2020%20seconds%20representing%20a%2010%24%5Ctimes%24%20to%2035%24%5Ctimes%24%20speedup%20over%20state-of-the-art%20methods%20while%20maintaining%20superior%20visual%20fidelity.%20Extensive%20experiments%20validate%20both%20the%20performance%20gains%20and%20the%20quality%20of%20outputs%20achieved%20by%20our%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixelRush%253A%2520Ultra-Fast%252C%2520Training-Free%2520High-Resolution%2520Image%2520Generation%2520via%2520One-step%2520Diffusion%26entry.906535625%3DHong-Phuc%2520Lai%2520and%2520Phong%2520Nguyen%2520and%2520Anh%2520Tran%26entry.1292438233%3DPre-trained%2520diffusion%2520models%2520excel%2520at%2520generating%2520high-quality%2520images%2520but%2520remain%2520inherently%2520limited%2520by%2520their%2520native%2520training%2520resolution.%2520Recent%2520training-free%2520approaches%2520have%2520attempted%2520to%2520overcome%2520this%2520constraint%2520by%2520introducing%2520interventions%2520during%2520the%2520denoising%2520process%253B%2520however%252C%2520these%2520methods%2520incur%2520substantial%2520computational%2520overhead%252C%2520often%2520requiring%2520more%2520than%2520five%2520minutes%2520to%2520produce%2520a%2520single%25204K%2520image.%2520In%2520this%2520paper%252C%2520we%2520present%2520PixelRush%252C%2520the%2520first%2520tuning-free%2520framework%2520for%2520practical%2520high-resolution%2520text-to-image%2520generation.%2520Our%2520method%2520builds%2520upon%2520the%2520established%2520patch-based%2520inference%2520paradigm%2520but%2520eliminates%2520the%2520need%2520for%2520multiple%2520inversion%2520and%2520regeneration%2520cycles.%2520Instead%252C%2520PixelRush%2520enables%2520efficient%2520patch-based%2520denoising%2520within%2520a%2520low-step%2520regime.%2520To%2520address%2520artifacts%2520introduced%2520by%2520patch%2520blending%2520in%2520few-step%2520generation%252C%2520we%2520propose%2520a%2520seamless%2520blending%2520strategy.%2520Furthermore%252C%2520we%2520mitigate%2520over-smoothing%2520effects%2520through%2520a%2520noise%2520injection%2520mechanism.%2520PixelRush%2520delivers%2520exceptional%2520efficiency%252C%2520generating%25204K%2520images%2520in%2520approximately%252020%2520seconds%2520representing%2520a%252010%2524%255Ctimes%2524%2520to%252035%2524%255Ctimes%2524%2520speedup%2520over%2520state-of-the-art%2520methods%2520while%2520maintaining%2520superior%2520visual%2520fidelity.%2520Extensive%2520experiments%2520validate%2520both%2520the%2520performance%2520gains%2520and%2520the%2520quality%2520of%2520outputs%2520achieved%2520by%2520our%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PixelRush%3A%20Ultra-Fast%2C%20Training-Free%20High-Resolution%20Image%20Generation%20via%20One-step%20Diffusion&entry.906535625=Hong-Phuc%20Lai%20and%20Phong%20Nguyen%20and%20Anh%20Tran&entry.1292438233=Pre-trained%20diffusion%20models%20excel%20at%20generating%20high-quality%20images%20but%20remain%20inherently%20limited%20by%20their%20native%20training%20resolution.%20Recent%20training-free%20approaches%20have%20attempted%20to%20overcome%20this%20constraint%20by%20introducing%20interventions%20during%20the%20denoising%20process%3B%20however%2C%20these%20methods%20incur%20substantial%20computational%20overhead%2C%20often%20requiring%20more%20than%20five%20minutes%20to%20produce%20a%20single%204K%20image.%20In%20this%20paper%2C%20we%20present%20PixelRush%2C%20the%20first%20tuning-free%20framework%20for%20practical%20high-resolution%20text-to-image%20generation.%20Our%20method%20builds%20upon%20the%20established%20patch-based%20inference%20paradigm%20but%20eliminates%20the%20need%20for%20multiple%20inversion%20and%20regeneration%20cycles.%20Instead%2C%20PixelRush%20enables%20efficient%20patch-based%20denoising%20within%20a%20low-step%20regime.%20To%20address%20artifacts%20introduced%20by%20patch%20blending%20in%20few-step%20generation%2C%20we%20propose%20a%20seamless%20blending%20strategy.%20Furthermore%2C%20we%20mitigate%20over-smoothing%20effects%20through%20a%20noise%20injection%20mechanism.%20PixelRush%20delivers%20exceptional%20efficiency%2C%20generating%204K%20images%20in%20approximately%2020%20seconds%20representing%20a%2010%24%5Ctimes%24%20to%2035%24%5Ctimes%24%20speedup%20over%20state-of-the-art%20methods%20while%20maintaining%20superior%20visual%20fidelity.%20Extensive%20experiments%20validate%20both%20the%20performance%20gains%20and%20the%20quality%20of%20outputs%20achieved%20by%20our%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2602.12769v1&entry.124074799=Read"},
{"title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing", "author": "Dianyi Wang and Ruihang Li and Feng Han and Chaofan Ma and Wei Song and Siyuan Wang and Yibin Wang and Yi Xin and Hongjian Liu and Zhixiong Zhang and Shengyuan Ding and Tianhang Wang and Zhenglin Cheng and Tao Lin and Cheng Jin and Kaicheng Yu and Jingjing Chen and Wenjie Wang and Zhongyu Wei and Jiaqi Wang", "abstract": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.", "link": "http://arxiv.org/abs/2602.12205v2", "date": "2026-02-13", "relevancy": 2.5536, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6443}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6405}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepGen%201.0%3A%20A%20Lightweight%20Unified%20Multimodal%20Model%20for%20Advancing%20Image%20Generation%20and%20Editing&body=Title%3A%20DeepGen%201.0%3A%20A%20Lightweight%20Unified%20Multimodal%20Model%20for%20Advancing%20Image%20Generation%20and%20Editing%0AAuthor%3A%20Dianyi%20Wang%20and%20Ruihang%20Li%20and%20Feng%20Han%20and%20Chaofan%20Ma%20and%20Wei%20Song%20and%20Siyuan%20Wang%20and%20Yibin%20Wang%20and%20Yi%20Xin%20and%20Hongjian%20Liu%20and%20Zhixiong%20Zhang%20and%20Shengyuan%20Ding%20and%20Tianhang%20Wang%20and%20Zhenglin%20Cheng%20and%20Tao%20Lin%20and%20Cheng%20Jin%20and%20Kaicheng%20Yu%20and%20Jingjing%20Chen%20and%20Wenjie%20Wang%20and%20Zhongyu%20Wei%20and%20Jiaqi%20Wang%0AAbstract%3A%20Current%20unified%20multimodal%20models%20for%20image%20generation%20and%20editing%20typically%20rely%20on%20massive%20parameter%20scales%20%28e.g.%2C%20%3E10B%29%2C%20entailing%20prohibitive%20training%20costs%20and%20deployment%20footprints.%20In%20this%20work%2C%20we%20present%20DeepGen%201.0%2C%20a%20lightweight%205B%20unified%20model%20that%20achieves%20comprehensive%20capabilities%20competitive%20with%20or%20surpassing%20much%20larger%20counterparts.%20To%20overcome%20the%20limitations%20of%20compact%20models%20in%20semantic%20understanding%20and%20fine-grained%20control%2C%20we%20introduce%20Stacked%20Channel%20Bridging%20%28SCB%29%2C%20a%20deep%20alignment%20framework%20that%20extracts%20hierarchical%20features%20from%20multiple%20VLM%20layers%20and%20fuses%20them%20with%20learnable%20%27think%20tokens%27%20to%20provide%20the%20generative%20backbone%20with%20structured%2C%20reasoning-rich%20guidance.%20We%20further%20design%20a%20data-centric%20training%20strategy%20spanning%20three%20progressive%20stages%3A%20%281%29%20Alignment%20Pre-training%20on%20large-scale%20image-text%20pairs%20and%20editing%20triplets%20to%20synchronize%20VLM%20and%20DiT%20representations%2C%20%282%29%20Joint%20Supervised%20Fine-tuning%20on%20a%20high-quality%20mixture%20of%20generation%2C%20editing%2C%20and%20reasoning%20tasks%20to%20foster%20omni-capabilities%2C%20and%20%283%29%20Reinforcement%20Learning%20with%20MR-GRPO%2C%20which%20leverages%20a%20mixture%20of%20reward%20functions%20and%20supervision%20signals%2C%20resulting%20in%20substantial%20gains%20in%20generation%20quality%20and%20alignment%20with%20human%20preferences%2C%20while%20maintaining%20stable%20training%20progress%20and%20avoiding%20visual%20artifacts.%20Despite%20being%20trained%20on%20only%20~50M%20samples%2C%20DeepGen%201.0%20achieves%20leading%20performance%20across%20diverse%20benchmarks%2C%20surpassing%20the%2080B%20HunyuanImage%20by%2028%25%20on%20WISE%20and%20the%2027B%20Qwen-Image-Edit%20by%2037%25%20on%20UniREditBench.%20By%20open-sourcing%20our%20training%20code%2C%20weights%2C%20and%20datasets%2C%20we%20provide%20an%20efficient%2C%20high-performance%20alternative%20to%20democratize%20unified%20multimodal%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepGen%25201.0%253A%2520A%2520Lightweight%2520Unified%2520Multimodal%2520Model%2520for%2520Advancing%2520Image%2520Generation%2520and%2520Editing%26entry.906535625%3DDianyi%2520Wang%2520and%2520Ruihang%2520Li%2520and%2520Feng%2520Han%2520and%2520Chaofan%2520Ma%2520and%2520Wei%2520Song%2520and%2520Siyuan%2520Wang%2520and%2520Yibin%2520Wang%2520and%2520Yi%2520Xin%2520and%2520Hongjian%2520Liu%2520and%2520Zhixiong%2520Zhang%2520and%2520Shengyuan%2520Ding%2520and%2520Tianhang%2520Wang%2520and%2520Zhenglin%2520Cheng%2520and%2520Tao%2520Lin%2520and%2520Cheng%2520Jin%2520and%2520Kaicheng%2520Yu%2520and%2520Jingjing%2520Chen%2520and%2520Wenjie%2520Wang%2520and%2520Zhongyu%2520Wei%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3DCurrent%2520unified%2520multimodal%2520models%2520for%2520image%2520generation%2520and%2520editing%2520typically%2520rely%2520on%2520massive%2520parameter%2520scales%2520%2528e.g.%252C%2520%253E10B%2529%252C%2520entailing%2520prohibitive%2520training%2520costs%2520and%2520deployment%2520footprints.%2520In%2520this%2520work%252C%2520we%2520present%2520DeepGen%25201.0%252C%2520a%2520lightweight%25205B%2520unified%2520model%2520that%2520achieves%2520comprehensive%2520capabilities%2520competitive%2520with%2520or%2520surpassing%2520much%2520larger%2520counterparts.%2520To%2520overcome%2520the%2520limitations%2520of%2520compact%2520models%2520in%2520semantic%2520understanding%2520and%2520fine-grained%2520control%252C%2520we%2520introduce%2520Stacked%2520Channel%2520Bridging%2520%2528SCB%2529%252C%2520a%2520deep%2520alignment%2520framework%2520that%2520extracts%2520hierarchical%2520features%2520from%2520multiple%2520VLM%2520layers%2520and%2520fuses%2520them%2520with%2520learnable%2520%2527think%2520tokens%2527%2520to%2520provide%2520the%2520generative%2520backbone%2520with%2520structured%252C%2520reasoning-rich%2520guidance.%2520We%2520further%2520design%2520a%2520data-centric%2520training%2520strategy%2520spanning%2520three%2520progressive%2520stages%253A%2520%25281%2529%2520Alignment%2520Pre-training%2520on%2520large-scale%2520image-text%2520pairs%2520and%2520editing%2520triplets%2520to%2520synchronize%2520VLM%2520and%2520DiT%2520representations%252C%2520%25282%2529%2520Joint%2520Supervised%2520Fine-tuning%2520on%2520a%2520high-quality%2520mixture%2520of%2520generation%252C%2520editing%252C%2520and%2520reasoning%2520tasks%2520to%2520foster%2520omni-capabilities%252C%2520and%2520%25283%2529%2520Reinforcement%2520Learning%2520with%2520MR-GRPO%252C%2520which%2520leverages%2520a%2520mixture%2520of%2520reward%2520functions%2520and%2520supervision%2520signals%252C%2520resulting%2520in%2520substantial%2520gains%2520in%2520generation%2520quality%2520and%2520alignment%2520with%2520human%2520preferences%252C%2520while%2520maintaining%2520stable%2520training%2520progress%2520and%2520avoiding%2520visual%2520artifacts.%2520Despite%2520being%2520trained%2520on%2520only%2520~50M%2520samples%252C%2520DeepGen%25201.0%2520achieves%2520leading%2520performance%2520across%2520diverse%2520benchmarks%252C%2520surpassing%2520the%252080B%2520HunyuanImage%2520by%252028%2525%2520on%2520WISE%2520and%2520the%252027B%2520Qwen-Image-Edit%2520by%252037%2525%2520on%2520UniREditBench.%2520By%2520open-sourcing%2520our%2520training%2520code%252C%2520weights%252C%2520and%2520datasets%252C%2520we%2520provide%2520an%2520efficient%252C%2520high-performance%2520alternative%2520to%2520democratize%2520unified%2520multimodal%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepGen%201.0%3A%20A%20Lightweight%20Unified%20Multimodal%20Model%20for%20Advancing%20Image%20Generation%20and%20Editing&entry.906535625=Dianyi%20Wang%20and%20Ruihang%20Li%20and%20Feng%20Han%20and%20Chaofan%20Ma%20and%20Wei%20Song%20and%20Siyuan%20Wang%20and%20Yibin%20Wang%20and%20Yi%20Xin%20and%20Hongjian%20Liu%20and%20Zhixiong%20Zhang%20and%20Shengyuan%20Ding%20and%20Tianhang%20Wang%20and%20Zhenglin%20Cheng%20and%20Tao%20Lin%20and%20Cheng%20Jin%20and%20Kaicheng%20Yu%20and%20Jingjing%20Chen%20and%20Wenjie%20Wang%20and%20Zhongyu%20Wei%20and%20Jiaqi%20Wang&entry.1292438233=Current%20unified%20multimodal%20models%20for%20image%20generation%20and%20editing%20typically%20rely%20on%20massive%20parameter%20scales%20%28e.g.%2C%20%3E10B%29%2C%20entailing%20prohibitive%20training%20costs%20and%20deployment%20footprints.%20In%20this%20work%2C%20we%20present%20DeepGen%201.0%2C%20a%20lightweight%205B%20unified%20model%20that%20achieves%20comprehensive%20capabilities%20competitive%20with%20or%20surpassing%20much%20larger%20counterparts.%20To%20overcome%20the%20limitations%20of%20compact%20models%20in%20semantic%20understanding%20and%20fine-grained%20control%2C%20we%20introduce%20Stacked%20Channel%20Bridging%20%28SCB%29%2C%20a%20deep%20alignment%20framework%20that%20extracts%20hierarchical%20features%20from%20multiple%20VLM%20layers%20and%20fuses%20them%20with%20learnable%20%27think%20tokens%27%20to%20provide%20the%20generative%20backbone%20with%20structured%2C%20reasoning-rich%20guidance.%20We%20further%20design%20a%20data-centric%20training%20strategy%20spanning%20three%20progressive%20stages%3A%20%281%29%20Alignment%20Pre-training%20on%20large-scale%20image-text%20pairs%20and%20editing%20triplets%20to%20synchronize%20VLM%20and%20DiT%20representations%2C%20%282%29%20Joint%20Supervised%20Fine-tuning%20on%20a%20high-quality%20mixture%20of%20generation%2C%20editing%2C%20and%20reasoning%20tasks%20to%20foster%20omni-capabilities%2C%20and%20%283%29%20Reinforcement%20Learning%20with%20MR-GRPO%2C%20which%20leverages%20a%20mixture%20of%20reward%20functions%20and%20supervision%20signals%2C%20resulting%20in%20substantial%20gains%20in%20generation%20quality%20and%20alignment%20with%20human%20preferences%2C%20while%20maintaining%20stable%20training%20progress%20and%20avoiding%20visual%20artifacts.%20Despite%20being%20trained%20on%20only%20~50M%20samples%2C%20DeepGen%201.0%20achieves%20leading%20performance%20across%20diverse%20benchmarks%2C%20surpassing%20the%2080B%20HunyuanImage%20by%2028%25%20on%20WISE%20and%20the%2027B%20Qwen-Image-Edit%20by%2037%25%20on%20UniREditBench.%20By%20open-sourcing%20our%20training%20code%2C%20weights%2C%20and%20datasets%2C%20we%20provide%20an%20efficient%2C%20high-performance%20alternative%20to%20democratize%20unified%20multimodal%20research.&entry.1838667208=http%3A//arxiv.org/abs/2602.12205v2&entry.124074799=Read"},
{"title": "FedHENet: A Frugal Federated Learning Framework for Heterogeneous Environments", "author": "Alejandro Dopico-Castro and Oscar Fontenla-Romero and Bertha Guijarro-Berdi\u00f1as and Amparo Alonso-Betanzos and Iv\u00e1n P\u00e9rez Dig\u00f3n", "abstract": "Federated Learning (FL) enables collaborative training without centralizing data, essential for privacy compliance in real-world scenarios involving sensitive visual information. Most FL approaches rely on expensive, iterative deep network optimization, which still risks privacy via shared gradients. In this work, we propose FedHENet, extending the FedHEONN framework to image classification. By using a fixed, pre-trained feature extractor and learning only a single output layer, we avoid costly local fine-tuning. This layer is learned by analytically aggregating client knowledge in a single round of communication using homomorphic encryption (HE). Experiments show that FedHENet achieves competitive accuracy compared to iterative FL baselines while demonstrating superior stability performance and up to 70\\% better energy efficiency. Crucially, our method is hyperparameter-free, removing the carbon footprint associated with hyperparameter tuning in standard FL. Code available in https://github.com/AlejandroDopico2/FedHENet/", "link": "http://arxiv.org/abs/2602.13024v1", "date": "2026-02-13", "relevancy": 2.5493, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5337}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5268}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedHENet%3A%20A%20Frugal%20Federated%20Learning%20Framework%20for%20Heterogeneous%20Environments&body=Title%3A%20FedHENet%3A%20A%20Frugal%20Federated%20Learning%20Framework%20for%20Heterogeneous%20Environments%0AAuthor%3A%20Alejandro%20Dopico-Castro%20and%20Oscar%20Fontenla-Romero%20and%20Bertha%20Guijarro-Berdi%C3%B1as%20and%20Amparo%20Alonso-Betanzos%20and%20Iv%C3%A1n%20P%C3%A9rez%20Dig%C3%B3n%0AAbstract%3A%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20training%20without%20centralizing%20data%2C%20essential%20for%20privacy%20compliance%20in%20real-world%20scenarios%20involving%20sensitive%20visual%20information.%20Most%20FL%20approaches%20rely%20on%20expensive%2C%20iterative%20deep%20network%20optimization%2C%20which%20still%20risks%20privacy%20via%20shared%20gradients.%20In%20this%20work%2C%20we%20propose%20FedHENet%2C%20extending%20the%20FedHEONN%20framework%20to%20image%20classification.%20By%20using%20a%20fixed%2C%20pre-trained%20feature%20extractor%20and%20learning%20only%20a%20single%20output%20layer%2C%20we%20avoid%20costly%20local%20fine-tuning.%20This%20layer%20is%20learned%20by%20analytically%20aggregating%20client%20knowledge%20in%20a%20single%20round%20of%20communication%20using%20homomorphic%20encryption%20%28HE%29.%20Experiments%20show%20that%20FedHENet%20achieves%20competitive%20accuracy%20compared%20to%20iterative%20FL%20baselines%20while%20demonstrating%20superior%20stability%20performance%20and%20up%20to%2070%5C%25%20better%20energy%20efficiency.%20Crucially%2C%20our%20method%20is%20hyperparameter-free%2C%20removing%20the%20carbon%20footprint%20associated%20with%20hyperparameter%20tuning%20in%20standard%20FL.%20Code%20available%20in%20https%3A//github.com/AlejandroDopico2/FedHENet/%0ALink%3A%20http%3A//arxiv.org/abs/2602.13024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedHENet%253A%2520A%2520Frugal%2520Federated%2520Learning%2520Framework%2520for%2520Heterogeneous%2520Environments%26entry.906535625%3DAlejandro%2520Dopico-Castro%2520and%2520Oscar%2520Fontenla-Romero%2520and%2520Bertha%2520Guijarro-Berdi%25C3%25B1as%2520and%2520Amparo%2520Alonso-Betanzos%2520and%2520Iv%25C3%25A1n%2520P%25C3%25A9rez%2520Dig%25C3%25B3n%26entry.1292438233%3DFederated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520training%2520without%2520centralizing%2520data%252C%2520essential%2520for%2520privacy%2520compliance%2520in%2520real-world%2520scenarios%2520involving%2520sensitive%2520visual%2520information.%2520Most%2520FL%2520approaches%2520rely%2520on%2520expensive%252C%2520iterative%2520deep%2520network%2520optimization%252C%2520which%2520still%2520risks%2520privacy%2520via%2520shared%2520gradients.%2520In%2520this%2520work%252C%2520we%2520propose%2520FedHENet%252C%2520extending%2520the%2520FedHEONN%2520framework%2520to%2520image%2520classification.%2520By%2520using%2520a%2520fixed%252C%2520pre-trained%2520feature%2520extractor%2520and%2520learning%2520only%2520a%2520single%2520output%2520layer%252C%2520we%2520avoid%2520costly%2520local%2520fine-tuning.%2520This%2520layer%2520is%2520learned%2520by%2520analytically%2520aggregating%2520client%2520knowledge%2520in%2520a%2520single%2520round%2520of%2520communication%2520using%2520homomorphic%2520encryption%2520%2528HE%2529.%2520Experiments%2520show%2520that%2520FedHENet%2520achieves%2520competitive%2520accuracy%2520compared%2520to%2520iterative%2520FL%2520baselines%2520while%2520demonstrating%2520superior%2520stability%2520performance%2520and%2520up%2520to%252070%255C%2525%2520better%2520energy%2520efficiency.%2520Crucially%252C%2520our%2520method%2520is%2520hyperparameter-free%252C%2520removing%2520the%2520carbon%2520footprint%2520associated%2520with%2520hyperparameter%2520tuning%2520in%2520standard%2520FL.%2520Code%2520available%2520in%2520https%253A//github.com/AlejandroDopico2/FedHENet/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedHENet%3A%20A%20Frugal%20Federated%20Learning%20Framework%20for%20Heterogeneous%20Environments&entry.906535625=Alejandro%20Dopico-Castro%20and%20Oscar%20Fontenla-Romero%20and%20Bertha%20Guijarro-Berdi%C3%B1as%20and%20Amparo%20Alonso-Betanzos%20and%20Iv%C3%A1n%20P%C3%A9rez%20Dig%C3%B3n&entry.1292438233=Federated%20Learning%20%28FL%29%20enables%20collaborative%20training%20without%20centralizing%20data%2C%20essential%20for%20privacy%20compliance%20in%20real-world%20scenarios%20involving%20sensitive%20visual%20information.%20Most%20FL%20approaches%20rely%20on%20expensive%2C%20iterative%20deep%20network%20optimization%2C%20which%20still%20risks%20privacy%20via%20shared%20gradients.%20In%20this%20work%2C%20we%20propose%20FedHENet%2C%20extending%20the%20FedHEONN%20framework%20to%20image%20classification.%20By%20using%20a%20fixed%2C%20pre-trained%20feature%20extractor%20and%20learning%20only%20a%20single%20output%20layer%2C%20we%20avoid%20costly%20local%20fine-tuning.%20This%20layer%20is%20learned%20by%20analytically%20aggregating%20client%20knowledge%20in%20a%20single%20round%20of%20communication%20using%20homomorphic%20encryption%20%28HE%29.%20Experiments%20show%20that%20FedHENet%20achieves%20competitive%20accuracy%20compared%20to%20iterative%20FL%20baselines%20while%20demonstrating%20superior%20stability%20performance%20and%20up%20to%2070%5C%25%20better%20energy%20efficiency.%20Crucially%2C%20our%20method%20is%20hyperparameter-free%2C%20removing%20the%20carbon%20footprint%20associated%20with%20hyperparameter%20tuning%20in%20standard%20FL.%20Code%20available%20in%20https%3A//github.com/AlejandroDopico2/FedHENet/&entry.1838667208=http%3A//arxiv.org/abs/2602.13024v1&entry.124074799=Read"},
{"title": "Gauss-Newton Natural Gradient Descent for Shape Learning", "author": "James King and Arturs Berzins and Siddhartha Mishra and Marius Zeinhofer", "abstract": "We explore the use of the Gauss-Newton method for optimization in shape learning, including implicit neural surfaces and geometry-informed neural networks. The method addresses key challenges in shape learning, such as the ill-conditioning of the underlying differential constraints and the mismatch between the optimization problem in parameter space and the function space where the problem is naturally posed. This leads to significantly faster and more stable convergence than standard first-order methods, while also requiring far fewer iterations. Experiments across benchmark shape optimization tasks demonstrate that the Gauss-Newton method consistently improves both training speed and final solution accuracy.", "link": "http://arxiv.org/abs/2602.00099v2", "date": "2026-02-13", "relevancy": 2.5229, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5371}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4895}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gauss-Newton%20Natural%20Gradient%20Descent%20for%20Shape%20Learning&body=Title%3A%20Gauss-Newton%20Natural%20Gradient%20Descent%20for%20Shape%20Learning%0AAuthor%3A%20James%20King%20and%20Arturs%20Berzins%20and%20Siddhartha%20Mishra%20and%20Marius%20Zeinhofer%0AAbstract%3A%20We%20explore%20the%20use%20of%20the%20Gauss-Newton%20method%20for%20optimization%20in%20shape%20learning%2C%20including%20implicit%20neural%20surfaces%20and%20geometry-informed%20neural%20networks.%20The%20method%20addresses%20key%20challenges%20in%20shape%20learning%2C%20such%20as%20the%20ill-conditioning%20of%20the%20underlying%20differential%20constraints%20and%20the%20mismatch%20between%20the%20optimization%20problem%20in%20parameter%20space%20and%20the%20function%20space%20where%20the%20problem%20is%20naturally%20posed.%20This%20leads%20to%20significantly%20faster%20and%20more%20stable%20convergence%20than%20standard%20first-order%20methods%2C%20while%20also%20requiring%20far%20fewer%20iterations.%20Experiments%20across%20benchmark%20shape%20optimization%20tasks%20demonstrate%20that%20the%20Gauss-Newton%20method%20consistently%20improves%20both%20training%20speed%20and%20final%20solution%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2602.00099v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGauss-Newton%2520Natural%2520Gradient%2520Descent%2520for%2520Shape%2520Learning%26entry.906535625%3DJames%2520King%2520and%2520Arturs%2520Berzins%2520and%2520Siddhartha%2520Mishra%2520and%2520Marius%2520Zeinhofer%26entry.1292438233%3DWe%2520explore%2520the%2520use%2520of%2520the%2520Gauss-Newton%2520method%2520for%2520optimization%2520in%2520shape%2520learning%252C%2520including%2520implicit%2520neural%2520surfaces%2520and%2520geometry-informed%2520neural%2520networks.%2520The%2520method%2520addresses%2520key%2520challenges%2520in%2520shape%2520learning%252C%2520such%2520as%2520the%2520ill-conditioning%2520of%2520the%2520underlying%2520differential%2520constraints%2520and%2520the%2520mismatch%2520between%2520the%2520optimization%2520problem%2520in%2520parameter%2520space%2520and%2520the%2520function%2520space%2520where%2520the%2520problem%2520is%2520naturally%2520posed.%2520This%2520leads%2520to%2520significantly%2520faster%2520and%2520more%2520stable%2520convergence%2520than%2520standard%2520first-order%2520methods%252C%2520while%2520also%2520requiring%2520far%2520fewer%2520iterations.%2520Experiments%2520across%2520benchmark%2520shape%2520optimization%2520tasks%2520demonstrate%2520that%2520the%2520Gauss-Newton%2520method%2520consistently%2520improves%2520both%2520training%2520speed%2520and%2520final%2520solution%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.00099v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gauss-Newton%20Natural%20Gradient%20Descent%20for%20Shape%20Learning&entry.906535625=James%20King%20and%20Arturs%20Berzins%20and%20Siddhartha%20Mishra%20and%20Marius%20Zeinhofer&entry.1292438233=We%20explore%20the%20use%20of%20the%20Gauss-Newton%20method%20for%20optimization%20in%20shape%20learning%2C%20including%20implicit%20neural%20surfaces%20and%20geometry-informed%20neural%20networks.%20The%20method%20addresses%20key%20challenges%20in%20shape%20learning%2C%20such%20as%20the%20ill-conditioning%20of%20the%20underlying%20differential%20constraints%20and%20the%20mismatch%20between%20the%20optimization%20problem%20in%20parameter%20space%20and%20the%20function%20space%20where%20the%20problem%20is%20naturally%20posed.%20This%20leads%20to%20significantly%20faster%20and%20more%20stable%20convergence%20than%20standard%20first-order%20methods%2C%20while%20also%20requiring%20far%20fewer%20iterations.%20Experiments%20across%20benchmark%20shape%20optimization%20tasks%20demonstrate%20that%20the%20Gauss-Newton%20method%20consistently%20improves%20both%20training%20speed%20and%20final%20solution%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2602.00099v2&entry.124074799=Read"},
{"title": "Kairos: Toward Adaptive and Parameter-Efficient Time Series Foundation Models", "author": "Kun Feng and Shaocheng Lan and Yuchen Fang and Wenchao He and Lintao Ma and Xingyu Lu and Kan Ren", "abstract": "Inherent temporal heterogeneity, such as varying sampling densities and periodic structures, has posed substantial challenges in zero-shot generalization for Time Series Foundation Models (TSFMs). Existing TSFMs predominantly rely on massive parameterization to absorb such heterogeneity, as their static tokenization and positional encoding schemes entangle diverse temporal patterns into a fixed representation space, encouraging memorization rather than adaptation. To address this limitation, we propose Kairos, a flexible and parameter-efficient TSFM that decouples temporal heterogeneity from model capacity through a novel tokenization perspective. Kairos introduces a dynamic patching tokenizer and a mixture-of-size encoding that adapt observational granularity to local information density, enabling fine-grained temporal abstraction without increasing model width or depth. In addition, we design a multi-granularity positional embedding based on dynamic rotary encodings, which conditions on instance-level spectral features and temporal structure induced by dynamic patching tokenization, allowing robust modeling of diverse temporal dependencies. Trained on a novel Predictability-Stratified Time-Series (PreSTS) corpus, Kairos achieves superior zero-shot performance with substantially fewer parameters on two mainstream benchmarks, GIFT-Eval and Time-Series-Library. The project page is at https://foundation-model-research.github.io/Kairos .", "link": "http://arxiv.org/abs/2509.25826v2", "date": "2026-02-13", "relevancy": 2.5115, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.514}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5002}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kairos%3A%20Toward%20Adaptive%20and%20Parameter-Efficient%20Time%20Series%20Foundation%20Models&body=Title%3A%20Kairos%3A%20Toward%20Adaptive%20and%20Parameter-Efficient%20Time%20Series%20Foundation%20Models%0AAuthor%3A%20Kun%20Feng%20and%20Shaocheng%20Lan%20and%20Yuchen%20Fang%20and%20Wenchao%20He%20and%20Lintao%20Ma%20and%20Xingyu%20Lu%20and%20Kan%20Ren%0AAbstract%3A%20Inherent%20temporal%20heterogeneity%2C%20such%20as%20varying%20sampling%20densities%20and%20periodic%20structures%2C%20has%20posed%20substantial%20challenges%20in%20zero-shot%20generalization%20for%20Time%20Series%20Foundation%20Models%20%28TSFMs%29.%20Existing%20TSFMs%20predominantly%20rely%20on%20massive%20parameterization%20to%20absorb%20such%20heterogeneity%2C%20as%20their%20static%20tokenization%20and%20positional%20encoding%20schemes%20entangle%20diverse%20temporal%20patterns%20into%20a%20fixed%20representation%20space%2C%20encouraging%20memorization%20rather%20than%20adaptation.%20To%20address%20this%20limitation%2C%20we%20propose%20Kairos%2C%20a%20flexible%20and%20parameter-efficient%20TSFM%20that%20decouples%20temporal%20heterogeneity%20from%20model%20capacity%20through%20a%20novel%20tokenization%20perspective.%20Kairos%20introduces%20a%20dynamic%20patching%20tokenizer%20and%20a%20mixture-of-size%20encoding%20that%20adapt%20observational%20granularity%20to%20local%20information%20density%2C%20enabling%20fine-grained%20temporal%20abstraction%20without%20increasing%20model%20width%20or%20depth.%20In%20addition%2C%20we%20design%20a%20multi-granularity%20positional%20embedding%20based%20on%20dynamic%20rotary%20encodings%2C%20which%20conditions%20on%20instance-level%20spectral%20features%20and%20temporal%20structure%20induced%20by%20dynamic%20patching%20tokenization%2C%20allowing%20robust%20modeling%20of%20diverse%20temporal%20dependencies.%20Trained%20on%20a%20novel%20Predictability-Stratified%20Time-Series%20%28PreSTS%29%20corpus%2C%20Kairos%20achieves%20superior%20zero-shot%20performance%20with%20substantially%20fewer%20parameters%20on%20two%20mainstream%20benchmarks%2C%20GIFT-Eval%20and%20Time-Series-Library.%20The%20project%20page%20is%20at%20https%3A//foundation-model-research.github.io/Kairos%20.%0ALink%3A%20http%3A//arxiv.org/abs/2509.25826v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKairos%253A%2520Toward%2520Adaptive%2520and%2520Parameter-Efficient%2520Time%2520Series%2520Foundation%2520Models%26entry.906535625%3DKun%2520Feng%2520and%2520Shaocheng%2520Lan%2520and%2520Yuchen%2520Fang%2520and%2520Wenchao%2520He%2520and%2520Lintao%2520Ma%2520and%2520Xingyu%2520Lu%2520and%2520Kan%2520Ren%26entry.1292438233%3DInherent%2520temporal%2520heterogeneity%252C%2520such%2520as%2520varying%2520sampling%2520densities%2520and%2520periodic%2520structures%252C%2520has%2520posed%2520substantial%2520challenges%2520in%2520zero-shot%2520generalization%2520for%2520Time%2520Series%2520Foundation%2520Models%2520%2528TSFMs%2529.%2520Existing%2520TSFMs%2520predominantly%2520rely%2520on%2520massive%2520parameterization%2520to%2520absorb%2520such%2520heterogeneity%252C%2520as%2520their%2520static%2520tokenization%2520and%2520positional%2520encoding%2520schemes%2520entangle%2520diverse%2520temporal%2520patterns%2520into%2520a%2520fixed%2520representation%2520space%252C%2520encouraging%2520memorization%2520rather%2520than%2520adaptation.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Kairos%252C%2520a%2520flexible%2520and%2520parameter-efficient%2520TSFM%2520that%2520decouples%2520temporal%2520heterogeneity%2520from%2520model%2520capacity%2520through%2520a%2520novel%2520tokenization%2520perspective.%2520Kairos%2520introduces%2520a%2520dynamic%2520patching%2520tokenizer%2520and%2520a%2520mixture-of-size%2520encoding%2520that%2520adapt%2520observational%2520granularity%2520to%2520local%2520information%2520density%252C%2520enabling%2520fine-grained%2520temporal%2520abstraction%2520without%2520increasing%2520model%2520width%2520or%2520depth.%2520In%2520addition%252C%2520we%2520design%2520a%2520multi-granularity%2520positional%2520embedding%2520based%2520on%2520dynamic%2520rotary%2520encodings%252C%2520which%2520conditions%2520on%2520instance-level%2520spectral%2520features%2520and%2520temporal%2520structure%2520induced%2520by%2520dynamic%2520patching%2520tokenization%252C%2520allowing%2520robust%2520modeling%2520of%2520diverse%2520temporal%2520dependencies.%2520Trained%2520on%2520a%2520novel%2520Predictability-Stratified%2520Time-Series%2520%2528PreSTS%2529%2520corpus%252C%2520Kairos%2520achieves%2520superior%2520zero-shot%2520performance%2520with%2520substantially%2520fewer%2520parameters%2520on%2520two%2520mainstream%2520benchmarks%252C%2520GIFT-Eval%2520and%2520Time-Series-Library.%2520The%2520project%2520page%2520is%2520at%2520https%253A//foundation-model-research.github.io/Kairos%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25826v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kairos%3A%20Toward%20Adaptive%20and%20Parameter-Efficient%20Time%20Series%20Foundation%20Models&entry.906535625=Kun%20Feng%20and%20Shaocheng%20Lan%20and%20Yuchen%20Fang%20and%20Wenchao%20He%20and%20Lintao%20Ma%20and%20Xingyu%20Lu%20and%20Kan%20Ren&entry.1292438233=Inherent%20temporal%20heterogeneity%2C%20such%20as%20varying%20sampling%20densities%20and%20periodic%20structures%2C%20has%20posed%20substantial%20challenges%20in%20zero-shot%20generalization%20for%20Time%20Series%20Foundation%20Models%20%28TSFMs%29.%20Existing%20TSFMs%20predominantly%20rely%20on%20massive%20parameterization%20to%20absorb%20such%20heterogeneity%2C%20as%20their%20static%20tokenization%20and%20positional%20encoding%20schemes%20entangle%20diverse%20temporal%20patterns%20into%20a%20fixed%20representation%20space%2C%20encouraging%20memorization%20rather%20than%20adaptation.%20To%20address%20this%20limitation%2C%20we%20propose%20Kairos%2C%20a%20flexible%20and%20parameter-efficient%20TSFM%20that%20decouples%20temporal%20heterogeneity%20from%20model%20capacity%20through%20a%20novel%20tokenization%20perspective.%20Kairos%20introduces%20a%20dynamic%20patching%20tokenizer%20and%20a%20mixture-of-size%20encoding%20that%20adapt%20observational%20granularity%20to%20local%20information%20density%2C%20enabling%20fine-grained%20temporal%20abstraction%20without%20increasing%20model%20width%20or%20depth.%20In%20addition%2C%20we%20design%20a%20multi-granularity%20positional%20embedding%20based%20on%20dynamic%20rotary%20encodings%2C%20which%20conditions%20on%20instance-level%20spectral%20features%20and%20temporal%20structure%20induced%20by%20dynamic%20patching%20tokenization%2C%20allowing%20robust%20modeling%20of%20diverse%20temporal%20dependencies.%20Trained%20on%20a%20novel%20Predictability-Stratified%20Time-Series%20%28PreSTS%29%20corpus%2C%20Kairos%20achieves%20superior%20zero-shot%20performance%20with%20substantially%20fewer%20parameters%20on%20two%20mainstream%20benchmarks%2C%20GIFT-Eval%20and%20Time-Series-Library.%20The%20project%20page%20is%20at%20https%3A//foundation-model-research.github.io/Kairos%20.&entry.1838667208=http%3A//arxiv.org/abs/2509.25826v2&entry.124074799=Read"},
{"title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data", "author": "Chengsong Huang and Wenhao Yu and Xiaoyang Wang and Hongming Zhang and Zongxia Li and Ruosen Li and Jiaxin Huang and Haitao Mi and Dong Yu", "abstract": "Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.", "link": "http://arxiv.org/abs/2508.05004v4", "date": "2026-02-13", "relevancy": 2.5042, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R-Zero%3A%20Self-Evolving%20Reasoning%20LLM%20from%20Zero%20Data&body=Title%3A%20R-Zero%3A%20Self-Evolving%20Reasoning%20LLM%20from%20Zero%20Data%0AAuthor%3A%20Chengsong%20Huang%20and%20Wenhao%20Yu%20and%20Xiaoyang%20Wang%20and%20Hongming%20Zhang%20and%20Zongxia%20Li%20and%20Ruosen%20Li%20and%20Jiaxin%20Huang%20and%20Haitao%20Mi%20and%20Dong%20Yu%0AAbstract%3A%20Self-evolving%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20scalable%20path%20toward%20super-intelligence%20by%20autonomously%20generating%2C%20refining%2C%20and%20learning%20from%20their%20own%20experiences.%20However%2C%20existing%20methods%20for%20training%20such%20models%20still%20rely%20heavily%20on%20vast%20human-curated%20tasks%20and%20labels%2C%20typically%20via%20fine-tuning%20or%20reinforcement%20learning%2C%20which%20poses%20a%20fundamental%20bottleneck%20to%20advancing%20AI%20systems%20toward%20capabilities%20beyond%20human%20intelligence.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20R-Zero%2C%20a%20fully%20autonomous%20framework%20that%20generates%20its%20own%20training%20data%20from%20scratch.%20Starting%20from%20a%20single%20base%20LLM%2C%20R-Zero%20initializes%20two%20independent%20models%20with%20distinct%20roles%2C%20a%20Challenger%20and%20a%20Solver.%20These%20models%20are%20optimized%20separately%20and%20co-evolve%20through%20interaction%3A%20the%20Challenger%20is%20rewarded%20for%20proposing%20tasks%20near%20the%20edge%20of%20the%20Solver%20capability%2C%20and%20the%20Solver%20is%20rewarded%20for%20solving%20increasingly%20challenging%20tasks%20posed%20by%20the%20Challenger.%20This%20process%20yields%20a%20targeted%2C%20self-improving%20curriculum%20without%20any%20pre-existing%20tasks%20and%20labels.%20Empirically%2C%20R-Zero%20substantially%20improves%20reasoning%20capability%20across%20different%20backbone%20LLMs%2C%20e.g.%2C%20boosting%20the%20Qwen3-4B-Base%20by%20%2B6.49%20on%20math-reasoning%20benchmarks%20and%20%2B7.54%20on%20general-domain%20reasoning%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2508.05004v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR-Zero%253A%2520Self-Evolving%2520Reasoning%2520LLM%2520from%2520Zero%2520Data%26entry.906535625%3DChengsong%2520Huang%2520and%2520Wenhao%2520Yu%2520and%2520Xiaoyang%2520Wang%2520and%2520Hongming%2520Zhang%2520and%2520Zongxia%2520Li%2520and%2520Ruosen%2520Li%2520and%2520Jiaxin%2520Huang%2520and%2520Haitao%2520Mi%2520and%2520Dong%2520Yu%26entry.1292438233%3DSelf-evolving%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520a%2520scalable%2520path%2520toward%2520super-intelligence%2520by%2520autonomously%2520generating%252C%2520refining%252C%2520and%2520learning%2520from%2520their%2520own%2520experiences.%2520However%252C%2520existing%2520methods%2520for%2520training%2520such%2520models%2520still%2520rely%2520heavily%2520on%2520vast%2520human-curated%2520tasks%2520and%2520labels%252C%2520typically%2520via%2520fine-tuning%2520or%2520reinforcement%2520learning%252C%2520which%2520poses%2520a%2520fundamental%2520bottleneck%2520to%2520advancing%2520AI%2520systems%2520toward%2520capabilities%2520beyond%2520human%2520intelligence.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520R-Zero%252C%2520a%2520fully%2520autonomous%2520framework%2520that%2520generates%2520its%2520own%2520training%2520data%2520from%2520scratch.%2520Starting%2520from%2520a%2520single%2520base%2520LLM%252C%2520R-Zero%2520initializes%2520two%2520independent%2520models%2520with%2520distinct%2520roles%252C%2520a%2520Challenger%2520and%2520a%2520Solver.%2520These%2520models%2520are%2520optimized%2520separately%2520and%2520co-evolve%2520through%2520interaction%253A%2520the%2520Challenger%2520is%2520rewarded%2520for%2520proposing%2520tasks%2520near%2520the%2520edge%2520of%2520the%2520Solver%2520capability%252C%2520and%2520the%2520Solver%2520is%2520rewarded%2520for%2520solving%2520increasingly%2520challenging%2520tasks%2520posed%2520by%2520the%2520Challenger.%2520This%2520process%2520yields%2520a%2520targeted%252C%2520self-improving%2520curriculum%2520without%2520any%2520pre-existing%2520tasks%2520and%2520labels.%2520Empirically%252C%2520R-Zero%2520substantially%2520improves%2520reasoning%2520capability%2520across%2520different%2520backbone%2520LLMs%252C%2520e.g.%252C%2520boosting%2520the%2520Qwen3-4B-Base%2520by%2520%252B6.49%2520on%2520math-reasoning%2520benchmarks%2520and%2520%252B7.54%2520on%2520general-domain%2520reasoning%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05004v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R-Zero%3A%20Self-Evolving%20Reasoning%20LLM%20from%20Zero%20Data&entry.906535625=Chengsong%20Huang%20and%20Wenhao%20Yu%20and%20Xiaoyang%20Wang%20and%20Hongming%20Zhang%20and%20Zongxia%20Li%20and%20Ruosen%20Li%20and%20Jiaxin%20Huang%20and%20Haitao%20Mi%20and%20Dong%20Yu&entry.1292438233=Self-evolving%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20scalable%20path%20toward%20super-intelligence%20by%20autonomously%20generating%2C%20refining%2C%20and%20learning%20from%20their%20own%20experiences.%20However%2C%20existing%20methods%20for%20training%20such%20models%20still%20rely%20heavily%20on%20vast%20human-curated%20tasks%20and%20labels%2C%20typically%20via%20fine-tuning%20or%20reinforcement%20learning%2C%20which%20poses%20a%20fundamental%20bottleneck%20to%20advancing%20AI%20systems%20toward%20capabilities%20beyond%20human%20intelligence.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20R-Zero%2C%20a%20fully%20autonomous%20framework%20that%20generates%20its%20own%20training%20data%20from%20scratch.%20Starting%20from%20a%20single%20base%20LLM%2C%20R-Zero%20initializes%20two%20independent%20models%20with%20distinct%20roles%2C%20a%20Challenger%20and%20a%20Solver.%20These%20models%20are%20optimized%20separately%20and%20co-evolve%20through%20interaction%3A%20the%20Challenger%20is%20rewarded%20for%20proposing%20tasks%20near%20the%20edge%20of%20the%20Solver%20capability%2C%20and%20the%20Solver%20is%20rewarded%20for%20solving%20increasingly%20challenging%20tasks%20posed%20by%20the%20Challenger.%20This%20process%20yields%20a%20targeted%2C%20self-improving%20curriculum%20without%20any%20pre-existing%20tasks%20and%20labels.%20Empirically%2C%20R-Zero%20substantially%20improves%20reasoning%20capability%20across%20different%20backbone%20LLMs%2C%20e.g.%2C%20boosting%20the%20Qwen3-4B-Base%20by%20%2B6.49%20on%20math-reasoning%20benchmarks%20and%20%2B7.54%20on%20general-domain%20reasoning%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2508.05004v4&entry.124074799=Read"},
{"title": "Learning to Approximate Uniform Facility Location via Graph Neural Networks", "author": "Chendi Qian and Christopher Morris and Stefanie Jegelka and Christian Sohler", "abstract": "There has been a growing interest in using neural networks, especially message-passing neural networks (MPNNs), to solve hard combinatorial optimization problems heuristically. However, existing learning-based approaches for hard combinatorial optimization tasks often rely on supervised training data, reinforcement learning, or gradient estimators, leading to significant computational overhead, unstable training, or a lack of provable performance guarantees. In contrast, classical approximation algorithms offer such performance guarantees under worst-case inputs but are non-differentiable and unable to adaptively exploit structural regularities in natural input distributions. We address this dichotomy with the fundamental example of Uniform Facility Location (UniFL), a variant of the combinatorial facility location problem with applications in clustering, data summarization, logistics, and supply chain design. We develop a fully differentiable MPNN model that embeds approximation-algorithmic principles while avoiding the need for solver supervision or discrete relaxations. Our approach admits provable approximation and size generalization guarantees to much larger instances than seen during training. Empirically, we show that our approach outperforms standard non-learned approximation algorithms in terms of solution quality, closing the gap with computationally intensive integer linear programming approaches. Overall, this work provides a step toward bridging learning-based methods and approximation algorithms for discrete optimization.", "link": "http://arxiv.org/abs/2602.13155v1", "date": "2026-02-13", "relevancy": 2.5034, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5144}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4961}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Approximate%20Uniform%20Facility%20Location%20via%20Graph%20Neural%20Networks&body=Title%3A%20Learning%20to%20Approximate%20Uniform%20Facility%20Location%20via%20Graph%20Neural%20Networks%0AAuthor%3A%20Chendi%20Qian%20and%20Christopher%20Morris%20and%20Stefanie%20Jegelka%20and%20Christian%20Sohler%0AAbstract%3A%20There%20has%20been%20a%20growing%20interest%20in%20using%20neural%20networks%2C%20especially%20message-passing%20neural%20networks%20%28MPNNs%29%2C%20to%20solve%20hard%20combinatorial%20optimization%20problems%20heuristically.%20However%2C%20existing%20learning-based%20approaches%20for%20hard%20combinatorial%20optimization%20tasks%20often%20rely%20on%20supervised%20training%20data%2C%20reinforcement%20learning%2C%20or%20gradient%20estimators%2C%20leading%20to%20significant%20computational%20overhead%2C%20unstable%20training%2C%20or%20a%20lack%20of%20provable%20performance%20guarantees.%20In%20contrast%2C%20classical%20approximation%20algorithms%20offer%20such%20performance%20guarantees%20under%20worst-case%20inputs%20but%20are%20non-differentiable%20and%20unable%20to%20adaptively%20exploit%20structural%20regularities%20in%20natural%20input%20distributions.%20We%20address%20this%20dichotomy%20with%20the%20fundamental%20example%20of%20Uniform%20Facility%20Location%20%28UniFL%29%2C%20a%20variant%20of%20the%20combinatorial%20facility%20location%20problem%20with%20applications%20in%20clustering%2C%20data%20summarization%2C%20logistics%2C%20and%20supply%20chain%20design.%20We%20develop%20a%20fully%20differentiable%20MPNN%20model%20that%20embeds%20approximation-algorithmic%20principles%20while%20avoiding%20the%20need%20for%20solver%20supervision%20or%20discrete%20relaxations.%20Our%20approach%20admits%20provable%20approximation%20and%20size%20generalization%20guarantees%20to%20much%20larger%20instances%20than%20seen%20during%20training.%20Empirically%2C%20we%20show%20that%20our%20approach%20outperforms%20standard%20non-learned%20approximation%20algorithms%20in%20terms%20of%20solution%20quality%2C%20closing%20the%20gap%20with%20computationally%20intensive%20integer%20linear%20programming%20approaches.%20Overall%2C%20this%20work%20provides%20a%20step%20toward%20bridging%20learning-based%20methods%20and%20approximation%20algorithms%20for%20discrete%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Approximate%2520Uniform%2520Facility%2520Location%2520via%2520Graph%2520Neural%2520Networks%26entry.906535625%3DChendi%2520Qian%2520and%2520Christopher%2520Morris%2520and%2520Stefanie%2520Jegelka%2520and%2520Christian%2520Sohler%26entry.1292438233%3DThere%2520has%2520been%2520a%2520growing%2520interest%2520in%2520using%2520neural%2520networks%252C%2520especially%2520message-passing%2520neural%2520networks%2520%2528MPNNs%2529%252C%2520to%2520solve%2520hard%2520combinatorial%2520optimization%2520problems%2520heuristically.%2520However%252C%2520existing%2520learning-based%2520approaches%2520for%2520hard%2520combinatorial%2520optimization%2520tasks%2520often%2520rely%2520on%2520supervised%2520training%2520data%252C%2520reinforcement%2520learning%252C%2520or%2520gradient%2520estimators%252C%2520leading%2520to%2520significant%2520computational%2520overhead%252C%2520unstable%2520training%252C%2520or%2520a%2520lack%2520of%2520provable%2520performance%2520guarantees.%2520In%2520contrast%252C%2520classical%2520approximation%2520algorithms%2520offer%2520such%2520performance%2520guarantees%2520under%2520worst-case%2520inputs%2520but%2520are%2520non-differentiable%2520and%2520unable%2520to%2520adaptively%2520exploit%2520structural%2520regularities%2520in%2520natural%2520input%2520distributions.%2520We%2520address%2520this%2520dichotomy%2520with%2520the%2520fundamental%2520example%2520of%2520Uniform%2520Facility%2520Location%2520%2528UniFL%2529%252C%2520a%2520variant%2520of%2520the%2520combinatorial%2520facility%2520location%2520problem%2520with%2520applications%2520in%2520clustering%252C%2520data%2520summarization%252C%2520logistics%252C%2520and%2520supply%2520chain%2520design.%2520We%2520develop%2520a%2520fully%2520differentiable%2520MPNN%2520model%2520that%2520embeds%2520approximation-algorithmic%2520principles%2520while%2520avoiding%2520the%2520need%2520for%2520solver%2520supervision%2520or%2520discrete%2520relaxations.%2520Our%2520approach%2520admits%2520provable%2520approximation%2520and%2520size%2520generalization%2520guarantees%2520to%2520much%2520larger%2520instances%2520than%2520seen%2520during%2520training.%2520Empirically%252C%2520we%2520show%2520that%2520our%2520approach%2520outperforms%2520standard%2520non-learned%2520approximation%2520algorithms%2520in%2520terms%2520of%2520solution%2520quality%252C%2520closing%2520the%2520gap%2520with%2520computationally%2520intensive%2520integer%2520linear%2520programming%2520approaches.%2520Overall%252C%2520this%2520work%2520provides%2520a%2520step%2520toward%2520bridging%2520learning-based%2520methods%2520and%2520approximation%2520algorithms%2520for%2520discrete%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Approximate%20Uniform%20Facility%20Location%20via%20Graph%20Neural%20Networks&entry.906535625=Chendi%20Qian%20and%20Christopher%20Morris%20and%20Stefanie%20Jegelka%20and%20Christian%20Sohler&entry.1292438233=There%20has%20been%20a%20growing%20interest%20in%20using%20neural%20networks%2C%20especially%20message-passing%20neural%20networks%20%28MPNNs%29%2C%20to%20solve%20hard%20combinatorial%20optimization%20problems%20heuristically.%20However%2C%20existing%20learning-based%20approaches%20for%20hard%20combinatorial%20optimization%20tasks%20often%20rely%20on%20supervised%20training%20data%2C%20reinforcement%20learning%2C%20or%20gradient%20estimators%2C%20leading%20to%20significant%20computational%20overhead%2C%20unstable%20training%2C%20or%20a%20lack%20of%20provable%20performance%20guarantees.%20In%20contrast%2C%20classical%20approximation%20algorithms%20offer%20such%20performance%20guarantees%20under%20worst-case%20inputs%20but%20are%20non-differentiable%20and%20unable%20to%20adaptively%20exploit%20structural%20regularities%20in%20natural%20input%20distributions.%20We%20address%20this%20dichotomy%20with%20the%20fundamental%20example%20of%20Uniform%20Facility%20Location%20%28UniFL%29%2C%20a%20variant%20of%20the%20combinatorial%20facility%20location%20problem%20with%20applications%20in%20clustering%2C%20data%20summarization%2C%20logistics%2C%20and%20supply%20chain%20design.%20We%20develop%20a%20fully%20differentiable%20MPNN%20model%20that%20embeds%20approximation-algorithmic%20principles%20while%20avoiding%20the%20need%20for%20solver%20supervision%20or%20discrete%20relaxations.%20Our%20approach%20admits%20provable%20approximation%20and%20size%20generalization%20guarantees%20to%20much%20larger%20instances%20than%20seen%20during%20training.%20Empirically%2C%20we%20show%20that%20our%20approach%20outperforms%20standard%20non-learned%20approximation%20algorithms%20in%20terms%20of%20solution%20quality%2C%20closing%20the%20gap%20with%20computationally%20intensive%20integer%20linear%20programming%20approaches.%20Overall%2C%20this%20work%20provides%20a%20step%20toward%20bridging%20learning-based%20methods%20and%20approximation%20algorithms%20for%20discrete%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2602.13155v1&entry.124074799=Read"},
{"title": "LongStream: Long-Sequence Streaming Autoregressive Visual Geometry", "author": "Chong Cheng and Xianda Chen and Tao Xie and Wei Yin and Weiqiang Ren and Qian Zhang and Xiaoyuang Guo and Hao Wang", "abstract": "Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/", "link": "http://arxiv.org/abs/2602.13172v1", "date": "2026-02-13", "relevancy": 2.4865, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6329}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6181}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongStream%3A%20Long-Sequence%20Streaming%20Autoregressive%20Visual%20Geometry&body=Title%3A%20LongStream%3A%20Long-Sequence%20Streaming%20Autoregressive%20Visual%20Geometry%0AAuthor%3A%20Chong%20Cheng%20and%20Xianda%20Chen%20and%20Tao%20Xie%20and%20Wei%20Yin%20and%20Weiqiang%20Ren%20and%20Qian%20Zhang%20and%20Xiaoyuang%20Guo%20and%20Hao%20Wang%0AAbstract%3A%20Long-sequence%20streaming%203D%20reconstruction%20remains%20a%20significant%20open%20challenge.%20Existing%20autoregressive%20models%20often%20fail%20when%20processing%20long%20sequences.%20They%20typically%20anchor%20poses%20to%20the%20first%20frame%2C%20which%20leads%20to%20attention%20decay%2C%20scale%20drift%2C%20and%20extrapolation%20errors.%20We%20introduce%20LongStream%2C%20a%20novel%20gauge-decoupled%20streaming%20visual%20geometry%20model%20for%20metric-scale%20scene%20reconstruction%20across%20thousands%20of%20frames.%20Our%20approach%20is%20threefold.%20First%2C%20we%20discard%20the%20first-frame%20anchor%20and%20predict%20keyframe-relative%20poses.%20This%20reformulates%20long-range%20extrapolation%20into%20a%20constant-difficulty%20local%20task.%20Second%2C%20we%20introduce%20orthogonal%20scale%20learning.%20This%20method%20fully%20disentangles%20geometry%20from%20scale%20estimation%20to%20suppress%20drift.%20Finally%2C%20we%20solve%20Transformer%20cache%20issues%20such%20as%20attention-sink%20reliance%20and%20long-term%20KV-cache%20contamination.%20We%20propose%20cache-consistent%20training%20combined%20with%20periodic%20cache%20refresh.%20This%20approach%20suppresses%20attention%20degradation%20over%20ultra-long%20sequences%20and%20reduces%20the%20gap%20between%20training%20and%20inference.%20Experiments%20show%20LongStream%20achieves%20state-of-the-art%20performance.%20It%20delivers%20stable%2C%20metric-scale%20reconstruction%20over%20kilometer-scale%20sequences%20at%2018%20FPS.%20Project%20Page%3A%20https%3A//3dagentworld.github.io/longstream/%0ALink%3A%20http%3A//arxiv.org/abs/2602.13172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongStream%253A%2520Long-Sequence%2520Streaming%2520Autoregressive%2520Visual%2520Geometry%26entry.906535625%3DChong%2520Cheng%2520and%2520Xianda%2520Chen%2520and%2520Tao%2520Xie%2520and%2520Wei%2520Yin%2520and%2520Weiqiang%2520Ren%2520and%2520Qian%2520Zhang%2520and%2520Xiaoyuang%2520Guo%2520and%2520Hao%2520Wang%26entry.1292438233%3DLong-sequence%2520streaming%25203D%2520reconstruction%2520remains%2520a%2520significant%2520open%2520challenge.%2520Existing%2520autoregressive%2520models%2520often%2520fail%2520when%2520processing%2520long%2520sequences.%2520They%2520typically%2520anchor%2520poses%2520to%2520the%2520first%2520frame%252C%2520which%2520leads%2520to%2520attention%2520decay%252C%2520scale%2520drift%252C%2520and%2520extrapolation%2520errors.%2520We%2520introduce%2520LongStream%252C%2520a%2520novel%2520gauge-decoupled%2520streaming%2520visual%2520geometry%2520model%2520for%2520metric-scale%2520scene%2520reconstruction%2520across%2520thousands%2520of%2520frames.%2520Our%2520approach%2520is%2520threefold.%2520First%252C%2520we%2520discard%2520the%2520first-frame%2520anchor%2520and%2520predict%2520keyframe-relative%2520poses.%2520This%2520reformulates%2520long-range%2520extrapolation%2520into%2520a%2520constant-difficulty%2520local%2520task.%2520Second%252C%2520we%2520introduce%2520orthogonal%2520scale%2520learning.%2520This%2520method%2520fully%2520disentangles%2520geometry%2520from%2520scale%2520estimation%2520to%2520suppress%2520drift.%2520Finally%252C%2520we%2520solve%2520Transformer%2520cache%2520issues%2520such%2520as%2520attention-sink%2520reliance%2520and%2520long-term%2520KV-cache%2520contamination.%2520We%2520propose%2520cache-consistent%2520training%2520combined%2520with%2520periodic%2520cache%2520refresh.%2520This%2520approach%2520suppresses%2520attention%2520degradation%2520over%2520ultra-long%2520sequences%2520and%2520reduces%2520the%2520gap%2520between%2520training%2520and%2520inference.%2520Experiments%2520show%2520LongStream%2520achieves%2520state-of-the-art%2520performance.%2520It%2520delivers%2520stable%252C%2520metric-scale%2520reconstruction%2520over%2520kilometer-scale%2520sequences%2520at%252018%2520FPS.%2520Project%2520Page%253A%2520https%253A//3dagentworld.github.io/longstream/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongStream%3A%20Long-Sequence%20Streaming%20Autoregressive%20Visual%20Geometry&entry.906535625=Chong%20Cheng%20and%20Xianda%20Chen%20and%20Tao%20Xie%20and%20Wei%20Yin%20and%20Weiqiang%20Ren%20and%20Qian%20Zhang%20and%20Xiaoyuang%20Guo%20and%20Hao%20Wang&entry.1292438233=Long-sequence%20streaming%203D%20reconstruction%20remains%20a%20significant%20open%20challenge.%20Existing%20autoregressive%20models%20often%20fail%20when%20processing%20long%20sequences.%20They%20typically%20anchor%20poses%20to%20the%20first%20frame%2C%20which%20leads%20to%20attention%20decay%2C%20scale%20drift%2C%20and%20extrapolation%20errors.%20We%20introduce%20LongStream%2C%20a%20novel%20gauge-decoupled%20streaming%20visual%20geometry%20model%20for%20metric-scale%20scene%20reconstruction%20across%20thousands%20of%20frames.%20Our%20approach%20is%20threefold.%20First%2C%20we%20discard%20the%20first-frame%20anchor%20and%20predict%20keyframe-relative%20poses.%20This%20reformulates%20long-range%20extrapolation%20into%20a%20constant-difficulty%20local%20task.%20Second%2C%20we%20introduce%20orthogonal%20scale%20learning.%20This%20method%20fully%20disentangles%20geometry%20from%20scale%20estimation%20to%20suppress%20drift.%20Finally%2C%20we%20solve%20Transformer%20cache%20issues%20such%20as%20attention-sink%20reliance%20and%20long-term%20KV-cache%20contamination.%20We%20propose%20cache-consistent%20training%20combined%20with%20periodic%20cache%20refresh.%20This%20approach%20suppresses%20attention%20degradation%20over%20ultra-long%20sequences%20and%20reduces%20the%20gap%20between%20training%20and%20inference.%20Experiments%20show%20LongStream%20achieves%20state-of-the-art%20performance.%20It%20delivers%20stable%2C%20metric-scale%20reconstruction%20over%20kilometer-scale%20sequences%20at%2018%20FPS.%20Project%20Page%3A%20https%3A//3dagentworld.github.io/longstream/&entry.1838667208=http%3A//arxiv.org/abs/2602.13172v1&entry.124074799=Read"},
{"title": "Reliable Thinking with Images", "author": "Haobin Li and Yutong Yang and Yijie Lin and Dai Xiang and Mouxing Yang and Xi Peng", "abstract": "As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another'', erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.", "link": "http://arxiv.org/abs/2602.12916v1", "date": "2026-02-13", "relevancy": 2.481, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.514}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5048}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20Thinking%20with%20Images&body=Title%3A%20Reliable%20Thinking%20with%20Images%0AAuthor%3A%20Haobin%20Li%20and%20Yutong%20Yang%20and%20Yijie%20Lin%20and%20Dai%20Xiang%20and%20Mouxing%20Yang%20and%20Xi%20Peng%0AAbstract%3A%20As%20a%20multimodal%20extension%20of%20Chain-of-Thought%20%28CoT%29%2C%20Thinking%20with%20Images%20%28TWI%29%20has%20recently%20emerged%20as%20a%20promising%20avenue%20to%20enhance%20the%20reasoning%20capability%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%2C%20which%20generates%20interleaved%20CoT%20by%20incorporating%20visual%20cues%20into%20the%20textual%20reasoning%20process.%20However%2C%20the%20success%20of%20existing%20TWI%20methods%20heavily%20relies%20on%20the%20assumption%20that%20interleaved%20image-text%20CoTs%20are%20faultless%2C%20which%20is%20easily%20violated%20in%20real-world%20scenarios%20due%20to%20the%20complexity%20of%20multimodal%20understanding.%20In%20this%20paper%2C%20we%20reveal%20and%20study%20a%20highly-practical%20yet%20under-explored%20problem%20in%20TWI%2C%20termed%20Noisy%20Thinking%20%28NT%29.%20Specifically%2C%20NT%20refers%20to%20the%20imperfect%20visual%20cues%20mining%20and%20answer%20reasoning%20process.%20As%20the%20saying%20goes%2C%20%60%60One%20mistake%20leads%20to%20another%27%27%2C%20erroneous%20interleaved%20CoT%20would%20cause%20error%20accumulation%2C%20thus%20significantly%20degrading%20the%20performance%20of%20MLLMs.%20To%20solve%20the%20NT%20problem%2C%20we%20propose%20a%20novel%20method%20dubbed%20Reliable%20Thinking%20with%20Images%20%28RTWI%29.%20In%20brief%2C%20RTWI%20estimates%20the%20reliability%20of%20visual%20cues%20and%20textual%20CoT%20in%20a%20unified%20text-centric%20manner%20and%20accordingly%20employs%20robust%20filtering%20and%20voting%20modules%20to%20prevent%20NT%20from%20contaminating%20the%20final%20answer.%20Extensive%20experiments%20on%20seven%20benchmarks%20verify%20the%20effectiveness%20of%20RTWI%20against%20NT.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520Thinking%2520with%2520Images%26entry.906535625%3DHaobin%2520Li%2520and%2520Yutong%2520Yang%2520and%2520Yijie%2520Lin%2520and%2520Dai%2520Xiang%2520and%2520Mouxing%2520Yang%2520and%2520Xi%2520Peng%26entry.1292438233%3DAs%2520a%2520multimodal%2520extension%2520of%2520Chain-of-Thought%2520%2528CoT%2529%252C%2520Thinking%2520with%2520Images%2520%2528TWI%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520avenue%2520to%2520enhance%2520the%2520reasoning%2520capability%2520of%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520which%2520generates%2520interleaved%2520CoT%2520by%2520incorporating%2520visual%2520cues%2520into%2520the%2520textual%2520reasoning%2520process.%2520However%252C%2520the%2520success%2520of%2520existing%2520TWI%2520methods%2520heavily%2520relies%2520on%2520the%2520assumption%2520that%2520interleaved%2520image-text%2520CoTs%2520are%2520faultless%252C%2520which%2520is%2520easily%2520violated%2520in%2520real-world%2520scenarios%2520due%2520to%2520the%2520complexity%2520of%2520multimodal%2520understanding.%2520In%2520this%2520paper%252C%2520we%2520reveal%2520and%2520study%2520a%2520highly-practical%2520yet%2520under-explored%2520problem%2520in%2520TWI%252C%2520termed%2520Noisy%2520Thinking%2520%2528NT%2529.%2520Specifically%252C%2520NT%2520refers%2520to%2520the%2520imperfect%2520visual%2520cues%2520mining%2520and%2520answer%2520reasoning%2520process.%2520As%2520the%2520saying%2520goes%252C%2520%2560%2560One%2520mistake%2520leads%2520to%2520another%2527%2527%252C%2520erroneous%2520interleaved%2520CoT%2520would%2520cause%2520error%2520accumulation%252C%2520thus%2520significantly%2520degrading%2520the%2520performance%2520of%2520MLLMs.%2520To%2520solve%2520the%2520NT%2520problem%252C%2520we%2520propose%2520a%2520novel%2520method%2520dubbed%2520Reliable%2520Thinking%2520with%2520Images%2520%2528RTWI%2529.%2520In%2520brief%252C%2520RTWI%2520estimates%2520the%2520reliability%2520of%2520visual%2520cues%2520and%2520textual%2520CoT%2520in%2520a%2520unified%2520text-centric%2520manner%2520and%2520accordingly%2520employs%2520robust%2520filtering%2520and%2520voting%2520modules%2520to%2520prevent%2520NT%2520from%2520contaminating%2520the%2520final%2520answer.%2520Extensive%2520experiments%2520on%2520seven%2520benchmarks%2520verify%2520the%2520effectiveness%2520of%2520RTWI%2520against%2520NT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20Thinking%20with%20Images&entry.906535625=Haobin%20Li%20and%20Yutong%20Yang%20and%20Yijie%20Lin%20and%20Dai%20Xiang%20and%20Mouxing%20Yang%20and%20Xi%20Peng&entry.1292438233=As%20a%20multimodal%20extension%20of%20Chain-of-Thought%20%28CoT%29%2C%20Thinking%20with%20Images%20%28TWI%29%20has%20recently%20emerged%20as%20a%20promising%20avenue%20to%20enhance%20the%20reasoning%20capability%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%2C%20which%20generates%20interleaved%20CoT%20by%20incorporating%20visual%20cues%20into%20the%20textual%20reasoning%20process.%20However%2C%20the%20success%20of%20existing%20TWI%20methods%20heavily%20relies%20on%20the%20assumption%20that%20interleaved%20image-text%20CoTs%20are%20faultless%2C%20which%20is%20easily%20violated%20in%20real-world%20scenarios%20due%20to%20the%20complexity%20of%20multimodal%20understanding.%20In%20this%20paper%2C%20we%20reveal%20and%20study%20a%20highly-practical%20yet%20under-explored%20problem%20in%20TWI%2C%20termed%20Noisy%20Thinking%20%28NT%29.%20Specifically%2C%20NT%20refers%20to%20the%20imperfect%20visual%20cues%20mining%20and%20answer%20reasoning%20process.%20As%20the%20saying%20goes%2C%20%60%60One%20mistake%20leads%20to%20another%27%27%2C%20erroneous%20interleaved%20CoT%20would%20cause%20error%20accumulation%2C%20thus%20significantly%20degrading%20the%20performance%20of%20MLLMs.%20To%20solve%20the%20NT%20problem%2C%20we%20propose%20a%20novel%20method%20dubbed%20Reliable%20Thinking%20with%20Images%20%28RTWI%29.%20In%20brief%2C%20RTWI%20estimates%20the%20reliability%20of%20visual%20cues%20and%20textual%20CoT%20in%20a%20unified%20text-centric%20manner%20and%20accordingly%20employs%20robust%20filtering%20and%20voting%20modules%20to%20prevent%20NT%20from%20contaminating%20the%20final%20answer.%20Extensive%20experiments%20on%20seven%20benchmarks%20verify%20the%20effectiveness%20of%20RTWI%20against%20NT.&entry.1838667208=http%3A//arxiv.org/abs/2602.12916v1&entry.124074799=Read"},
{"title": "Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining", "author": "Rupert Mitchell and Kristian Kersting", "abstract": "Pretraining transformers on long sequences (entire code repositories, collections of related documents) is bottlenecked by quadratic attention costs. We present Multipole Semantic Attention (MuSe), which accelerates 64k-context pretraining by 36% while matching baseline loss, requiring no architectural changes. MuSe clusters queries and keys separately in representation space. This yields query-specific summaries that substantially outperform spatial blocking at matched sparsity, while also enabling drop-in compatibility with existing pretrained models; we validate on Llama 3.1-8B and 3.2-1B without retraining. We pretrain language models up to 1B parameters at 64k context on code and scientific documents, confirming that MuSe preserves quality and long-context utilization during training.", "link": "http://arxiv.org/abs/2509.10406v3", "date": "2026-02-13", "relevancy": 2.4805, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.507}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4985}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multipole%20Semantic%20Attention%3A%20A%20Fast%20Approximation%20of%20Softmax%20Attention%20for%20Pretraining&body=Title%3A%20Multipole%20Semantic%20Attention%3A%20A%20Fast%20Approximation%20of%20Softmax%20Attention%20for%20Pretraining%0AAuthor%3A%20Rupert%20Mitchell%20and%20Kristian%20Kersting%0AAbstract%3A%20Pretraining%20transformers%20on%20long%20sequences%20%28entire%20code%20repositories%2C%20collections%20of%20related%20documents%29%20is%20bottlenecked%20by%20quadratic%20attention%20costs.%20We%20present%20Multipole%20Semantic%20Attention%20%28MuSe%29%2C%20which%20accelerates%2064k-context%20pretraining%20by%2036%25%20while%20matching%20baseline%20loss%2C%20requiring%20no%20architectural%20changes.%20MuSe%20clusters%20queries%20and%20keys%20separately%20in%20representation%20space.%20This%20yields%20query-specific%20summaries%20that%20substantially%20outperform%20spatial%20blocking%20at%20matched%20sparsity%2C%20while%20also%20enabling%20drop-in%20compatibility%20with%20existing%20pretrained%20models%3B%20we%20validate%20on%20Llama%203.1-8B%20and%203.2-1B%20without%20retraining.%20We%20pretrain%20language%20models%20up%20to%201B%20parameters%20at%2064k%20context%20on%20code%20and%20scientific%20documents%2C%20confirming%20that%20MuSe%20preserves%20quality%20and%20long-context%20utilization%20during%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2509.10406v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultipole%2520Semantic%2520Attention%253A%2520A%2520Fast%2520Approximation%2520of%2520Softmax%2520Attention%2520for%2520Pretraining%26entry.906535625%3DRupert%2520Mitchell%2520and%2520Kristian%2520Kersting%26entry.1292438233%3DPretraining%2520transformers%2520on%2520long%2520sequences%2520%2528entire%2520code%2520repositories%252C%2520collections%2520of%2520related%2520documents%2529%2520is%2520bottlenecked%2520by%2520quadratic%2520attention%2520costs.%2520We%2520present%2520Multipole%2520Semantic%2520Attention%2520%2528MuSe%2529%252C%2520which%2520accelerates%252064k-context%2520pretraining%2520by%252036%2525%2520while%2520matching%2520baseline%2520loss%252C%2520requiring%2520no%2520architectural%2520changes.%2520MuSe%2520clusters%2520queries%2520and%2520keys%2520separately%2520in%2520representation%2520space.%2520This%2520yields%2520query-specific%2520summaries%2520that%2520substantially%2520outperform%2520spatial%2520blocking%2520at%2520matched%2520sparsity%252C%2520while%2520also%2520enabling%2520drop-in%2520compatibility%2520with%2520existing%2520pretrained%2520models%253B%2520we%2520validate%2520on%2520Llama%25203.1-8B%2520and%25203.2-1B%2520without%2520retraining.%2520We%2520pretrain%2520language%2520models%2520up%2520to%25201B%2520parameters%2520at%252064k%2520context%2520on%2520code%2520and%2520scientific%2520documents%252C%2520confirming%2520that%2520MuSe%2520preserves%2520quality%2520and%2520long-context%2520utilization%2520during%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10406v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multipole%20Semantic%20Attention%3A%20A%20Fast%20Approximation%20of%20Softmax%20Attention%20for%20Pretraining&entry.906535625=Rupert%20Mitchell%20and%20Kristian%20Kersting&entry.1292438233=Pretraining%20transformers%20on%20long%20sequences%20%28entire%20code%20repositories%2C%20collections%20of%20related%20documents%29%20is%20bottlenecked%20by%20quadratic%20attention%20costs.%20We%20present%20Multipole%20Semantic%20Attention%20%28MuSe%29%2C%20which%20accelerates%2064k-context%20pretraining%20by%2036%25%20while%20matching%20baseline%20loss%2C%20requiring%20no%20architectural%20changes.%20MuSe%20clusters%20queries%20and%20keys%20separately%20in%20representation%20space.%20This%20yields%20query-specific%20summaries%20that%20substantially%20outperform%20spatial%20blocking%20at%20matched%20sparsity%2C%20while%20also%20enabling%20drop-in%20compatibility%20with%20existing%20pretrained%20models%3B%20we%20validate%20on%20Llama%203.1-8B%20and%203.2-1B%20without%20retraining.%20We%20pretrain%20language%20models%20up%20to%201B%20parameters%20at%2064k%20context%20on%20code%20and%20scientific%20documents%2C%20confirming%20that%20MuSe%20preserves%20quality%20and%20long-context%20utilization%20during%20training.&entry.1838667208=http%3A//arxiv.org/abs/2509.10406v3&entry.124074799=Read"},
{"title": "EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models", "author": "Wei Xiong and Jiangtong Li and Jie Li and Kun Zhu and Changjun Jiang", "abstract": "Electroencephalography foundation models (EEG-FMs) have advanced brain signal analysis, but the lack of standardized evaluation benchmarks impedes model comparison and scientific progress. Current evaluations rely on inconsistent protocols that render cross-model comparisons unreliable, while a lack of diagnostic analyses obscures the internal mechanisms driving transfer efficiency and scaling behaviors. To address this, we introduce \\textbf{EEG-FM-Bench}, a unified system for the standardized evaluation of EEG-FMs. The benchmark integrates 14 datasets across 10 paradigms and incorporates diverse experimental settings, including multiple fine-tuning strategies, task organizations, and classifier configurations, supported by tools for gradient and representation analysis. Our experiments and analysis reveal several critical insights: (1) multi-task learning acts as a critical regularizer to mitigate overfitting in data-scarce EEG contexts; (2) pre-training efficiency is currently limited by gradient conflicts between reconstruction objectives and downstream tasks; (3) model scaling deviates from typical laws, as compact architectures with domain-specific inductive biases consistently outperform significantly larger models. This benchmark enables fair comparison and reproducible analysis, shifting the field from fragmented results to interpretable advances. Code is available at https://github.com/xw1216/EEG-FM-Bench.", "link": "http://arxiv.org/abs/2508.17742v2", "date": "2026-02-13", "relevancy": 2.477, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG-FM-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20the%20Systematic%20Evaluation%20of%20EEG%20Foundation%20Models&body=Title%3A%20EEG-FM-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20the%20Systematic%20Evaluation%20of%20EEG%20Foundation%20Models%0AAuthor%3A%20Wei%20Xiong%20and%20Jiangtong%20Li%20and%20Jie%20Li%20and%20Kun%20Zhu%20and%20Changjun%20Jiang%0AAbstract%3A%20Electroencephalography%20foundation%20models%20%28EEG-FMs%29%20have%20advanced%20brain%20signal%20analysis%2C%20but%20the%20lack%20of%20standardized%20evaluation%20benchmarks%20impedes%20model%20comparison%20and%20scientific%20progress.%20Current%20evaluations%20rely%20on%20inconsistent%20protocols%20that%20render%20cross-model%20comparisons%20unreliable%2C%20while%20a%20lack%20of%20diagnostic%20analyses%20obscures%20the%20internal%20mechanisms%20driving%20transfer%20efficiency%20and%20scaling%20behaviors.%20To%20address%20this%2C%20we%20introduce%20%5Ctextbf%7BEEG-FM-Bench%7D%2C%20a%20unified%20system%20for%20the%20standardized%20evaluation%20of%20EEG-FMs.%20The%20benchmark%20integrates%2014%20datasets%20across%2010%20paradigms%20and%20incorporates%20diverse%20experimental%20settings%2C%20including%20multiple%20fine-tuning%20strategies%2C%20task%20organizations%2C%20and%20classifier%20configurations%2C%20supported%20by%20tools%20for%20gradient%20and%20representation%20analysis.%20Our%20experiments%20and%20analysis%20reveal%20several%20critical%20insights%3A%20%281%29%20multi-task%20learning%20acts%20as%20a%20critical%20regularizer%20to%20mitigate%20overfitting%20in%20data-scarce%20EEG%20contexts%3B%20%282%29%20pre-training%20efficiency%20is%20currently%20limited%20by%20gradient%20conflicts%20between%20reconstruction%20objectives%20and%20downstream%20tasks%3B%20%283%29%20model%20scaling%20deviates%20from%20typical%20laws%2C%20as%20compact%20architectures%20with%20domain-specific%20inductive%20biases%20consistently%20outperform%20significantly%20larger%20models.%20This%20benchmark%20enables%20fair%20comparison%20and%20reproducible%20analysis%2C%20shifting%20the%20field%20from%20fragmented%20results%20to%20interpretable%20advances.%20Code%20is%20available%20at%20https%3A//github.com/xw1216/EEG-FM-Bench.%0ALink%3A%20http%3A//arxiv.org/abs/2508.17742v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG-FM-Bench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520the%2520Systematic%2520Evaluation%2520of%2520EEG%2520Foundation%2520Models%26entry.906535625%3DWei%2520Xiong%2520and%2520Jiangtong%2520Li%2520and%2520Jie%2520Li%2520and%2520Kun%2520Zhu%2520and%2520Changjun%2520Jiang%26entry.1292438233%3DElectroencephalography%2520foundation%2520models%2520%2528EEG-FMs%2529%2520have%2520advanced%2520brain%2520signal%2520analysis%252C%2520but%2520the%2520lack%2520of%2520standardized%2520evaluation%2520benchmarks%2520impedes%2520model%2520comparison%2520and%2520scientific%2520progress.%2520Current%2520evaluations%2520rely%2520on%2520inconsistent%2520protocols%2520that%2520render%2520cross-model%2520comparisons%2520unreliable%252C%2520while%2520a%2520lack%2520of%2520diagnostic%2520analyses%2520obscures%2520the%2520internal%2520mechanisms%2520driving%2520transfer%2520efficiency%2520and%2520scaling%2520behaviors.%2520To%2520address%2520this%252C%2520we%2520introduce%2520%255Ctextbf%257BEEG-FM-Bench%257D%252C%2520a%2520unified%2520system%2520for%2520the%2520standardized%2520evaluation%2520of%2520EEG-FMs.%2520The%2520benchmark%2520integrates%252014%2520datasets%2520across%252010%2520paradigms%2520and%2520incorporates%2520diverse%2520experimental%2520settings%252C%2520including%2520multiple%2520fine-tuning%2520strategies%252C%2520task%2520organizations%252C%2520and%2520classifier%2520configurations%252C%2520supported%2520by%2520tools%2520for%2520gradient%2520and%2520representation%2520analysis.%2520Our%2520experiments%2520and%2520analysis%2520reveal%2520several%2520critical%2520insights%253A%2520%25281%2529%2520multi-task%2520learning%2520acts%2520as%2520a%2520critical%2520regularizer%2520to%2520mitigate%2520overfitting%2520in%2520data-scarce%2520EEG%2520contexts%253B%2520%25282%2529%2520pre-training%2520efficiency%2520is%2520currently%2520limited%2520by%2520gradient%2520conflicts%2520between%2520reconstruction%2520objectives%2520and%2520downstream%2520tasks%253B%2520%25283%2529%2520model%2520scaling%2520deviates%2520from%2520typical%2520laws%252C%2520as%2520compact%2520architectures%2520with%2520domain-specific%2520inductive%2520biases%2520consistently%2520outperform%2520significantly%2520larger%2520models.%2520This%2520benchmark%2520enables%2520fair%2520comparison%2520and%2520reproducible%2520analysis%252C%2520shifting%2520the%2520field%2520from%2520fragmented%2520results%2520to%2520interpretable%2520advances.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/xw1216/EEG-FM-Bench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17742v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG-FM-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20the%20Systematic%20Evaluation%20of%20EEG%20Foundation%20Models&entry.906535625=Wei%20Xiong%20and%20Jiangtong%20Li%20and%20Jie%20Li%20and%20Kun%20Zhu%20and%20Changjun%20Jiang&entry.1292438233=Electroencephalography%20foundation%20models%20%28EEG-FMs%29%20have%20advanced%20brain%20signal%20analysis%2C%20but%20the%20lack%20of%20standardized%20evaluation%20benchmarks%20impedes%20model%20comparison%20and%20scientific%20progress.%20Current%20evaluations%20rely%20on%20inconsistent%20protocols%20that%20render%20cross-model%20comparisons%20unreliable%2C%20while%20a%20lack%20of%20diagnostic%20analyses%20obscures%20the%20internal%20mechanisms%20driving%20transfer%20efficiency%20and%20scaling%20behaviors.%20To%20address%20this%2C%20we%20introduce%20%5Ctextbf%7BEEG-FM-Bench%7D%2C%20a%20unified%20system%20for%20the%20standardized%20evaluation%20of%20EEG-FMs.%20The%20benchmark%20integrates%2014%20datasets%20across%2010%20paradigms%20and%20incorporates%20diverse%20experimental%20settings%2C%20including%20multiple%20fine-tuning%20strategies%2C%20task%20organizations%2C%20and%20classifier%20configurations%2C%20supported%20by%20tools%20for%20gradient%20and%20representation%20analysis.%20Our%20experiments%20and%20analysis%20reveal%20several%20critical%20insights%3A%20%281%29%20multi-task%20learning%20acts%20as%20a%20critical%20regularizer%20to%20mitigate%20overfitting%20in%20data-scarce%20EEG%20contexts%3B%20%282%29%20pre-training%20efficiency%20is%20currently%20limited%20by%20gradient%20conflicts%20between%20reconstruction%20objectives%20and%20downstream%20tasks%3B%20%283%29%20model%20scaling%20deviates%20from%20typical%20laws%2C%20as%20compact%20architectures%20with%20domain-specific%20inductive%20biases%20consistently%20outperform%20significantly%20larger%20models.%20This%20benchmark%20enables%20fair%20comparison%20and%20reproducible%20analysis%2C%20shifting%20the%20field%20from%20fragmented%20results%20to%20interpretable%20advances.%20Code%20is%20available%20at%20https%3A//github.com/xw1216/EEG-FM-Bench.&entry.1838667208=http%3A//arxiv.org/abs/2508.17742v2&entry.124074799=Read"},
{"title": "Unified Multi-Domain Graph Pre-training for Homogeneous and Heterogeneous Graphs via Domain-Specific Expert Encoding", "author": "Chundong Liang and Yongqi Huang and Dongxiao He and Peiyuan Li and Yawen Li and Di Jin and Weixiong Zhang", "abstract": "Graph pre-training has achieved remarkable success in recent years, delivering transferable representations for downstream adaptation. However, most existing methods are designed for either homogeneous or heterogeneous graphs, thereby hindering unified graph modeling across diverse graph types. This separation contradicts real-world applications, where mixed homogeneous and heterogeneous graphs are ubiquitous, and distribution shifts between upstream pre-training and downstream deployment are common. In this paper, we empirically demonstrate that a balanced mixture of homogeneous and heterogeneous graph pre-training benefits downstream tasks and propose a unified multi-domain \\textbf{G}raph \\textbf{P}re-training method across \\textbf{H}omogeneous and \\textbf{H}eterogeneous graphs ($\\mathbf{GPH^{2}}$). To address the lack of a unified encoder for homogeneous and heterogeneous graphs, we propose a Unified Multi-View Graph Construction that simultaneously encodes both without explicit graph-type-specific designs. To cope with the increased cross-domain distribution discrepancies arising from mixed graphs, we introduce domain-specific expert encoding. Each expert is independently pre-trained on a single graph to capture domain-specific knowledge, thereby shielding the pre-training encoder from the adverse effects of cross-domain discrepancies. For downstream tasks, we further design a Task-oriented Expert Fusion Strategy that adaptively integrates multiple experts based on their discriminative strengths. Extensive experiments on mixed graphs demonstrate that $\\text{GPH}^{2}$ enables stable transfer across graph types and domains, significantly outperforming existing graph pre-training methods.", "link": "http://arxiv.org/abs/2602.13075v1", "date": "2026-02-13", "relevancy": 2.4627, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5005}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4943}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Multi-Domain%20Graph%20Pre-training%20for%20Homogeneous%20and%20Heterogeneous%20Graphs%20via%20Domain-Specific%20Expert%20Encoding&body=Title%3A%20Unified%20Multi-Domain%20Graph%20Pre-training%20for%20Homogeneous%20and%20Heterogeneous%20Graphs%20via%20Domain-Specific%20Expert%20Encoding%0AAuthor%3A%20Chundong%20Liang%20and%20Yongqi%20Huang%20and%20Dongxiao%20He%20and%20Peiyuan%20Li%20and%20Yawen%20Li%20and%20Di%20Jin%20and%20Weixiong%20Zhang%0AAbstract%3A%20Graph%20pre-training%20has%20achieved%20remarkable%20success%20in%20recent%20years%2C%20delivering%20transferable%20representations%20for%20downstream%20adaptation.%20However%2C%20most%20existing%20methods%20are%20designed%20for%20either%20homogeneous%20or%20heterogeneous%20graphs%2C%20thereby%20hindering%20unified%20graph%20modeling%20across%20diverse%20graph%20types.%20This%20separation%20contradicts%20real-world%20applications%2C%20where%20mixed%20homogeneous%20and%20heterogeneous%20graphs%20are%20ubiquitous%2C%20and%20distribution%20shifts%20between%20upstream%20pre-training%20and%20downstream%20deployment%20are%20common.%20In%20this%20paper%2C%20we%20empirically%20demonstrate%20that%20a%20balanced%20mixture%20of%20homogeneous%20and%20heterogeneous%20graph%20pre-training%20benefits%20downstream%20tasks%20and%20propose%20a%20unified%20multi-domain%20%5Ctextbf%7BG%7Draph%20%5Ctextbf%7BP%7Dre-training%20method%20across%20%5Ctextbf%7BH%7Domogeneous%20and%20%5Ctextbf%7BH%7Deterogeneous%20graphs%20%28%24%5Cmathbf%7BGPH%5E%7B2%7D%7D%24%29.%20To%20address%20the%20lack%20of%20a%20unified%20encoder%20for%20homogeneous%20and%20heterogeneous%20graphs%2C%20we%20propose%20a%20Unified%20Multi-View%20Graph%20Construction%20that%20simultaneously%20encodes%20both%20without%20explicit%20graph-type-specific%20designs.%20To%20cope%20with%20the%20increased%20cross-domain%20distribution%20discrepancies%20arising%20from%20mixed%20graphs%2C%20we%20introduce%20domain-specific%20expert%20encoding.%20Each%20expert%20is%20independently%20pre-trained%20on%20a%20single%20graph%20to%20capture%20domain-specific%20knowledge%2C%20thereby%20shielding%20the%20pre-training%20encoder%20from%20the%20adverse%20effects%20of%20cross-domain%20discrepancies.%20For%20downstream%20tasks%2C%20we%20further%20design%20a%20Task-oriented%20Expert%20Fusion%20Strategy%20that%20adaptively%20integrates%20multiple%20experts%20based%20on%20their%20discriminative%20strengths.%20Extensive%20experiments%20on%20mixed%20graphs%20demonstrate%20that%20%24%5Ctext%7BGPH%7D%5E%7B2%7D%24%20enables%20stable%20transfer%20across%20graph%20types%20and%20domains%2C%20significantly%20outperforming%20existing%20graph%20pre-training%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Multi-Domain%2520Graph%2520Pre-training%2520for%2520Homogeneous%2520and%2520Heterogeneous%2520Graphs%2520via%2520Domain-Specific%2520Expert%2520Encoding%26entry.906535625%3DChundong%2520Liang%2520and%2520Yongqi%2520Huang%2520and%2520Dongxiao%2520He%2520and%2520Peiyuan%2520Li%2520and%2520Yawen%2520Li%2520and%2520Di%2520Jin%2520and%2520Weixiong%2520Zhang%26entry.1292438233%3DGraph%2520pre-training%2520has%2520achieved%2520remarkable%2520success%2520in%2520recent%2520years%252C%2520delivering%2520transferable%2520representations%2520for%2520downstream%2520adaptation.%2520However%252C%2520most%2520existing%2520methods%2520are%2520designed%2520for%2520either%2520homogeneous%2520or%2520heterogeneous%2520graphs%252C%2520thereby%2520hindering%2520unified%2520graph%2520modeling%2520across%2520diverse%2520graph%2520types.%2520This%2520separation%2520contradicts%2520real-world%2520applications%252C%2520where%2520mixed%2520homogeneous%2520and%2520heterogeneous%2520graphs%2520are%2520ubiquitous%252C%2520and%2520distribution%2520shifts%2520between%2520upstream%2520pre-training%2520and%2520downstream%2520deployment%2520are%2520common.%2520In%2520this%2520paper%252C%2520we%2520empirically%2520demonstrate%2520that%2520a%2520balanced%2520mixture%2520of%2520homogeneous%2520and%2520heterogeneous%2520graph%2520pre-training%2520benefits%2520downstream%2520tasks%2520and%2520propose%2520a%2520unified%2520multi-domain%2520%255Ctextbf%257BG%257Draph%2520%255Ctextbf%257BP%257Dre-training%2520method%2520across%2520%255Ctextbf%257BH%257Domogeneous%2520and%2520%255Ctextbf%257BH%257Deterogeneous%2520graphs%2520%2528%2524%255Cmathbf%257BGPH%255E%257B2%257D%257D%2524%2529.%2520To%2520address%2520the%2520lack%2520of%2520a%2520unified%2520encoder%2520for%2520homogeneous%2520and%2520heterogeneous%2520graphs%252C%2520we%2520propose%2520a%2520Unified%2520Multi-View%2520Graph%2520Construction%2520that%2520simultaneously%2520encodes%2520both%2520without%2520explicit%2520graph-type-specific%2520designs.%2520To%2520cope%2520with%2520the%2520increased%2520cross-domain%2520distribution%2520discrepancies%2520arising%2520from%2520mixed%2520graphs%252C%2520we%2520introduce%2520domain-specific%2520expert%2520encoding.%2520Each%2520expert%2520is%2520independently%2520pre-trained%2520on%2520a%2520single%2520graph%2520to%2520capture%2520domain-specific%2520knowledge%252C%2520thereby%2520shielding%2520the%2520pre-training%2520encoder%2520from%2520the%2520adverse%2520effects%2520of%2520cross-domain%2520discrepancies.%2520For%2520downstream%2520tasks%252C%2520we%2520further%2520design%2520a%2520Task-oriented%2520Expert%2520Fusion%2520Strategy%2520that%2520adaptively%2520integrates%2520multiple%2520experts%2520based%2520on%2520their%2520discriminative%2520strengths.%2520Extensive%2520experiments%2520on%2520mixed%2520graphs%2520demonstrate%2520that%2520%2524%255Ctext%257BGPH%257D%255E%257B2%257D%2524%2520enables%2520stable%2520transfer%2520across%2520graph%2520types%2520and%2520domains%252C%2520significantly%2520outperforming%2520existing%2520graph%2520pre-training%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Multi-Domain%20Graph%20Pre-training%20for%20Homogeneous%20and%20Heterogeneous%20Graphs%20via%20Domain-Specific%20Expert%20Encoding&entry.906535625=Chundong%20Liang%20and%20Yongqi%20Huang%20and%20Dongxiao%20He%20and%20Peiyuan%20Li%20and%20Yawen%20Li%20and%20Di%20Jin%20and%20Weixiong%20Zhang&entry.1292438233=Graph%20pre-training%20has%20achieved%20remarkable%20success%20in%20recent%20years%2C%20delivering%20transferable%20representations%20for%20downstream%20adaptation.%20However%2C%20most%20existing%20methods%20are%20designed%20for%20either%20homogeneous%20or%20heterogeneous%20graphs%2C%20thereby%20hindering%20unified%20graph%20modeling%20across%20diverse%20graph%20types.%20This%20separation%20contradicts%20real-world%20applications%2C%20where%20mixed%20homogeneous%20and%20heterogeneous%20graphs%20are%20ubiquitous%2C%20and%20distribution%20shifts%20between%20upstream%20pre-training%20and%20downstream%20deployment%20are%20common.%20In%20this%20paper%2C%20we%20empirically%20demonstrate%20that%20a%20balanced%20mixture%20of%20homogeneous%20and%20heterogeneous%20graph%20pre-training%20benefits%20downstream%20tasks%20and%20propose%20a%20unified%20multi-domain%20%5Ctextbf%7BG%7Draph%20%5Ctextbf%7BP%7Dre-training%20method%20across%20%5Ctextbf%7BH%7Domogeneous%20and%20%5Ctextbf%7BH%7Deterogeneous%20graphs%20%28%24%5Cmathbf%7BGPH%5E%7B2%7D%7D%24%29.%20To%20address%20the%20lack%20of%20a%20unified%20encoder%20for%20homogeneous%20and%20heterogeneous%20graphs%2C%20we%20propose%20a%20Unified%20Multi-View%20Graph%20Construction%20that%20simultaneously%20encodes%20both%20without%20explicit%20graph-type-specific%20designs.%20To%20cope%20with%20the%20increased%20cross-domain%20distribution%20discrepancies%20arising%20from%20mixed%20graphs%2C%20we%20introduce%20domain-specific%20expert%20encoding.%20Each%20expert%20is%20independently%20pre-trained%20on%20a%20single%20graph%20to%20capture%20domain-specific%20knowledge%2C%20thereby%20shielding%20the%20pre-training%20encoder%20from%20the%20adverse%20effects%20of%20cross-domain%20discrepancies.%20For%20downstream%20tasks%2C%20we%20further%20design%20a%20Task-oriented%20Expert%20Fusion%20Strategy%20that%20adaptively%20integrates%20multiple%20experts%20based%20on%20their%20discriminative%20strengths.%20Extensive%20experiments%20on%20mixed%20graphs%20demonstrate%20that%20%24%5Ctext%7BGPH%7D%5E%7B2%7D%24%20enables%20stable%20transfer%20across%20graph%20types%20and%20domains%2C%20significantly%20outperforming%20existing%20graph%20pre-training%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2602.13075v1&entry.124074799=Read"},
{"title": "FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control", "author": "Mingzhi Sheng and Zekai Gu and Peng Li and Cheng Lin and Hao-Xiang Guo and Ying-Cong Chen and Yuan Liu", "abstract": "Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of \"appearance\" and \"motion\" provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.", "link": "http://arxiv.org/abs/2602.13185v1", "date": "2026-02-13", "relevancy": 2.457, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6294}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6246}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexAM%3A%20Flexible%20Appearance-Motion%20Decomposition%20for%20Versatile%20Video%20Generation%20Control&body=Title%3A%20FlexAM%3A%20Flexible%20Appearance-Motion%20Decomposition%20for%20Versatile%20Video%20Generation%20Control%0AAuthor%3A%20Mingzhi%20Sheng%20and%20Zekai%20Gu%20and%20Peng%20Li%20and%20Cheng%20Lin%20and%20Hao-Xiang%20Guo%20and%20Ying-Cong%20Chen%20and%20Yuan%20Liu%0AAbstract%3A%20Effective%20and%20generalizable%20control%20in%20video%20generation%20remains%20a%20significant%20challenge.%20While%20many%20methods%20rely%20on%20ambiguous%20or%20task-specific%20signals%2C%20we%20argue%20that%20a%20fundamental%20disentanglement%20of%20%22appearance%22%20and%20%22motion%22%20provides%20a%20more%20robust%20and%20scalable%20pathway.%20We%20propose%20FlexAM%2C%20a%20unified%20framework%20built%20upon%20a%20novel%203D%20control%20signal.%20This%20signal%20represents%20video%20dynamics%20as%20a%20point%20cloud%2C%20introducing%20three%20key%20enhancements%3A%20multi-frequency%20positional%20encoding%20to%20distinguish%20fine-grained%20motion%2C%20depth-aware%20positional%20encoding%2C%20and%20a%20flexible%20control%20signal%20for%20balancing%20precision%20and%20generative%20quality.%20This%20representation%20allows%20FlexAM%20to%20effectively%20disentangle%20appearance%20and%20motion%2C%20enabling%20a%20wide%20range%20of%20tasks%20including%20I2V/V2V%20editing%2C%20camera%20control%2C%20and%20spatial%20object%20editing.%20Extensive%20experiments%20demonstrate%20that%20FlexAM%20achieves%20superior%20performance%20across%20all%20evaluated%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexAM%253A%2520Flexible%2520Appearance-Motion%2520Decomposition%2520for%2520Versatile%2520Video%2520Generation%2520Control%26entry.906535625%3DMingzhi%2520Sheng%2520and%2520Zekai%2520Gu%2520and%2520Peng%2520Li%2520and%2520Cheng%2520Lin%2520and%2520Hao-Xiang%2520Guo%2520and%2520Ying-Cong%2520Chen%2520and%2520Yuan%2520Liu%26entry.1292438233%3DEffective%2520and%2520generalizable%2520control%2520in%2520video%2520generation%2520remains%2520a%2520significant%2520challenge.%2520While%2520many%2520methods%2520rely%2520on%2520ambiguous%2520or%2520task-specific%2520signals%252C%2520we%2520argue%2520that%2520a%2520fundamental%2520disentanglement%2520of%2520%2522appearance%2522%2520and%2520%2522motion%2522%2520provides%2520a%2520more%2520robust%2520and%2520scalable%2520pathway.%2520We%2520propose%2520FlexAM%252C%2520a%2520unified%2520framework%2520built%2520upon%2520a%2520novel%25203D%2520control%2520signal.%2520This%2520signal%2520represents%2520video%2520dynamics%2520as%2520a%2520point%2520cloud%252C%2520introducing%2520three%2520key%2520enhancements%253A%2520multi-frequency%2520positional%2520encoding%2520to%2520distinguish%2520fine-grained%2520motion%252C%2520depth-aware%2520positional%2520encoding%252C%2520and%2520a%2520flexible%2520control%2520signal%2520for%2520balancing%2520precision%2520and%2520generative%2520quality.%2520This%2520representation%2520allows%2520FlexAM%2520to%2520effectively%2520disentangle%2520appearance%2520and%2520motion%252C%2520enabling%2520a%2520wide%2520range%2520of%2520tasks%2520including%2520I2V/V2V%2520editing%252C%2520camera%2520control%252C%2520and%2520spatial%2520object%2520editing.%2520Extensive%2520experiments%2520demonstrate%2520that%2520FlexAM%2520achieves%2520superior%2520performance%2520across%2520all%2520evaluated%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexAM%3A%20Flexible%20Appearance-Motion%20Decomposition%20for%20Versatile%20Video%20Generation%20Control&entry.906535625=Mingzhi%20Sheng%20and%20Zekai%20Gu%20and%20Peng%20Li%20and%20Cheng%20Lin%20and%20Hao-Xiang%20Guo%20and%20Ying-Cong%20Chen%20and%20Yuan%20Liu&entry.1292438233=Effective%20and%20generalizable%20control%20in%20video%20generation%20remains%20a%20significant%20challenge.%20While%20many%20methods%20rely%20on%20ambiguous%20or%20task-specific%20signals%2C%20we%20argue%20that%20a%20fundamental%20disentanglement%20of%20%22appearance%22%20and%20%22motion%22%20provides%20a%20more%20robust%20and%20scalable%20pathway.%20We%20propose%20FlexAM%2C%20a%20unified%20framework%20built%20upon%20a%20novel%203D%20control%20signal.%20This%20signal%20represents%20video%20dynamics%20as%20a%20point%20cloud%2C%20introducing%20three%20key%20enhancements%3A%20multi-frequency%20positional%20encoding%20to%20distinguish%20fine-grained%20motion%2C%20depth-aware%20positional%20encoding%2C%20and%20a%20flexible%20control%20signal%20for%20balancing%20precision%20and%20generative%20quality.%20This%20representation%20allows%20FlexAM%20to%20effectively%20disentangle%20appearance%20and%20motion%2C%20enabling%20a%20wide%20range%20of%20tasks%20including%20I2V/V2V%20editing%2C%20camera%20control%2C%20and%20spatial%20object%20editing.%20Extensive%20experiments%20demonstrate%20that%20FlexAM%20achieves%20superior%20performance%20across%20all%20evaluated%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.13185v1&entry.124074799=Read"},
{"title": "Generating Physical Dynamics under Priors", "author": "Zihan Zhou and Xiaoxue Wang and Tianshu Yu", "abstract": "Generating physically feasible dynamics in a data-driven context is challenging, especially when adhering to physical priors expressed in specific equations or formulas. Existing methodologies often overlook the integration of physical priors, resulting in violation of basic physical laws and suboptimal performance. In this paper, we introduce a novel framework that seamlessly incorporates physical priors into diffusion-based generative models to address this limitation. Our approach leverages two categories of priors: 1) distributional priors, such as roto-translational invariance, and 2) physical feasibility priors, including energy and momentum conservation laws and PDE constraints. By embedding these priors into the generative process, our method can efficiently generate physically realistic dynamics, encompassing trajectories and flows. Empirical evaluations demonstrate that our method produces high-quality dynamics across a diverse array of physical phenomena with remarkable robustness, underscoring its potential to advance data-driven studies in AI4Physics. Our contributions signify a substantial advancement in the field of generative modeling, offering a robust solution to generate accurate and physically consistent dynamics.", "link": "http://arxiv.org/abs/2409.00730v4", "date": "2026-02-13", "relevancy": 2.4533, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6317}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6158}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Physical%20Dynamics%20under%20Priors&body=Title%3A%20Generating%20Physical%20Dynamics%20under%20Priors%0AAuthor%3A%20Zihan%20Zhou%20and%20Xiaoxue%20Wang%20and%20Tianshu%20Yu%0AAbstract%3A%20Generating%20physically%20feasible%20dynamics%20in%20a%20data-driven%20context%20is%20challenging%2C%20especially%20when%20adhering%20to%20physical%20priors%20expressed%20in%20specific%20equations%20or%20formulas.%20Existing%20methodologies%20often%20overlook%20the%20integration%20of%20physical%20priors%2C%20resulting%20in%20violation%20of%20basic%20physical%20laws%20and%20suboptimal%20performance.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%20that%20seamlessly%20incorporates%20physical%20priors%20into%20diffusion-based%20generative%20models%20to%20address%20this%20limitation.%20Our%20approach%20leverages%20two%20categories%20of%20priors%3A%201%29%20distributional%20priors%2C%20such%20as%20roto-translational%20invariance%2C%20and%202%29%20physical%20feasibility%20priors%2C%20including%20energy%20and%20momentum%20conservation%20laws%20and%20PDE%20constraints.%20By%20embedding%20these%20priors%20into%20the%20generative%20process%2C%20our%20method%20can%20efficiently%20generate%20physically%20realistic%20dynamics%2C%20encompassing%20trajectories%20and%20flows.%20Empirical%20evaluations%20demonstrate%20that%20our%20method%20produces%20high-quality%20dynamics%20across%20a%20diverse%20array%20of%20physical%20phenomena%20with%20remarkable%20robustness%2C%20underscoring%20its%20potential%20to%20advance%20data-driven%20studies%20in%20AI4Physics.%20Our%20contributions%20signify%20a%20substantial%20advancement%20in%20the%20field%20of%20generative%20modeling%2C%20offering%20a%20robust%20solution%20to%20generate%20accurate%20and%20physically%20consistent%20dynamics.%0ALink%3A%20http%3A//arxiv.org/abs/2409.00730v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Physical%2520Dynamics%2520under%2520Priors%26entry.906535625%3DZihan%2520Zhou%2520and%2520Xiaoxue%2520Wang%2520and%2520Tianshu%2520Yu%26entry.1292438233%3DGenerating%2520physically%2520feasible%2520dynamics%2520in%2520a%2520data-driven%2520context%2520is%2520challenging%252C%2520especially%2520when%2520adhering%2520to%2520physical%2520priors%2520expressed%2520in%2520specific%2520equations%2520or%2520formulas.%2520Existing%2520methodologies%2520often%2520overlook%2520the%2520integration%2520of%2520physical%2520priors%252C%2520resulting%2520in%2520violation%2520of%2520basic%2520physical%2520laws%2520and%2520suboptimal%2520performance.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520that%2520seamlessly%2520incorporates%2520physical%2520priors%2520into%2520diffusion-based%2520generative%2520models%2520to%2520address%2520this%2520limitation.%2520Our%2520approach%2520leverages%2520two%2520categories%2520of%2520priors%253A%25201%2529%2520distributional%2520priors%252C%2520such%2520as%2520roto-translational%2520invariance%252C%2520and%25202%2529%2520physical%2520feasibility%2520priors%252C%2520including%2520energy%2520and%2520momentum%2520conservation%2520laws%2520and%2520PDE%2520constraints.%2520By%2520embedding%2520these%2520priors%2520into%2520the%2520generative%2520process%252C%2520our%2520method%2520can%2520efficiently%2520generate%2520physically%2520realistic%2520dynamics%252C%2520encompassing%2520trajectories%2520and%2520flows.%2520Empirical%2520evaluations%2520demonstrate%2520that%2520our%2520method%2520produces%2520high-quality%2520dynamics%2520across%2520a%2520diverse%2520array%2520of%2520physical%2520phenomena%2520with%2520remarkable%2520robustness%252C%2520underscoring%2520its%2520potential%2520to%2520advance%2520data-driven%2520studies%2520in%2520AI4Physics.%2520Our%2520contributions%2520signify%2520a%2520substantial%2520advancement%2520in%2520the%2520field%2520of%2520generative%2520modeling%252C%2520offering%2520a%2520robust%2520solution%2520to%2520generate%2520accurate%2520and%2520physically%2520consistent%2520dynamics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00730v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Physical%20Dynamics%20under%20Priors&entry.906535625=Zihan%20Zhou%20and%20Xiaoxue%20Wang%20and%20Tianshu%20Yu&entry.1292438233=Generating%20physically%20feasible%20dynamics%20in%20a%20data-driven%20context%20is%20challenging%2C%20especially%20when%20adhering%20to%20physical%20priors%20expressed%20in%20specific%20equations%20or%20formulas.%20Existing%20methodologies%20often%20overlook%20the%20integration%20of%20physical%20priors%2C%20resulting%20in%20violation%20of%20basic%20physical%20laws%20and%20suboptimal%20performance.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%20that%20seamlessly%20incorporates%20physical%20priors%20into%20diffusion-based%20generative%20models%20to%20address%20this%20limitation.%20Our%20approach%20leverages%20two%20categories%20of%20priors%3A%201%29%20distributional%20priors%2C%20such%20as%20roto-translational%20invariance%2C%20and%202%29%20physical%20feasibility%20priors%2C%20including%20energy%20and%20momentum%20conservation%20laws%20and%20PDE%20constraints.%20By%20embedding%20these%20priors%20into%20the%20generative%20process%2C%20our%20method%20can%20efficiently%20generate%20physically%20realistic%20dynamics%2C%20encompassing%20trajectories%20and%20flows.%20Empirical%20evaluations%20demonstrate%20that%20our%20method%20produces%20high-quality%20dynamics%20across%20a%20diverse%20array%20of%20physical%20phenomena%20with%20remarkable%20robustness%2C%20underscoring%20its%20potential%20to%20advance%20data-driven%20studies%20in%20AI4Physics.%20Our%20contributions%20signify%20a%20substantial%20advancement%20in%20the%20field%20of%20generative%20modeling%2C%20offering%20a%20robust%20solution%20to%20generate%20accurate%20and%20physically%20consistent%20dynamics.&entry.1838667208=http%3A//arxiv.org/abs/2409.00730v4&entry.124074799=Read"},
{"title": "Improved Regret Guarantees for Online Mirror Descent using a Portfolio of Mirror Maps", "author": "Swati Gupta and Jai Moondra and Mohit Singh", "abstract": "OMD and its variants give a flexible framework for OCO where the performance depends crucially on the choice of the mirror map. While the geometries underlying OPGD and OEG, both special cases of OMD, are well understood, it remains a challenging open question on how to construct an optimal mirror map for any given constrained set and a general family of loss functions, e.g., sparse losses. Motivated by parameterizing a near-optimal set of mirror maps, we consider a simpler question: is it even possible to obtain polynomial gains in regret by using mirror maps for geometries that interpolate between $L_1$ and $L_2$, which may not be possible by restricting to only OEG ($L_1$) or OPGD ($L_2$).\n  Our main result answers this question positively. We show that mirror maps based on block norms adapt better to the sparsity of loss functions, compared to previous $L_p$ (for $p \\in [1, 2]$) interpolations. In particular, we construct a family of online convex optimization instances in $\\mathbb{R}^d$, where block norm-based mirror maps achieve a provable polynomial (in $d$) improvement in regret over OEG and OPGD for sparse loss functions. We then turn to the setting in which the sparsity level of the loss functions is unknown. In this case, the choice of geometry itself becomes an online decision problem. We first show that naively switching between OEG and OPGD can incur linear regret, highlighting the intrinsic difficulty of geometry selection. To overcome this issue, we propose a meta-algorithm based on multiplicative weights that dynamically selects among a family of uniform block norms. We show that this approach effectively tunes OMD to the sparsity of the losses, yielding adaptive regret guarantees. Overall, our results demonstrate that online mirror-map selection can significantly enhance the ability of OMD to exploit sparsity in online convex optimization.", "link": "http://arxiv.org/abs/2602.13177v1", "date": "2026-02-13", "relevancy": 2.4446, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5051}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4857}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Regret%20Guarantees%20for%20Online%20Mirror%20Descent%20using%20a%20Portfolio%20of%20Mirror%20Maps&body=Title%3A%20Improved%20Regret%20Guarantees%20for%20Online%20Mirror%20Descent%20using%20a%20Portfolio%20of%20Mirror%20Maps%0AAuthor%3A%20Swati%20Gupta%20and%20Jai%20Moondra%20and%20Mohit%20Singh%0AAbstract%3A%20OMD%20and%20its%20variants%20give%20a%20flexible%20framework%20for%20OCO%20where%20the%20performance%20depends%20crucially%20on%20the%20choice%20of%20the%20mirror%20map.%20While%20the%20geometries%20underlying%20OPGD%20and%20OEG%2C%20both%20special%20cases%20of%20OMD%2C%20are%20well%20understood%2C%20it%20remains%20a%20challenging%20open%20question%20on%20how%20to%20construct%20an%20optimal%20mirror%20map%20for%20any%20given%20constrained%20set%20and%20a%20general%20family%20of%20loss%20functions%2C%20e.g.%2C%20sparse%20losses.%20Motivated%20by%20parameterizing%20a%20near-optimal%20set%20of%20mirror%20maps%2C%20we%20consider%20a%20simpler%20question%3A%20is%20it%20even%20possible%20to%20obtain%20polynomial%20gains%20in%20regret%20by%20using%20mirror%20maps%20for%20geometries%20that%20interpolate%20between%20%24L_1%24%20and%20%24L_2%24%2C%20which%20may%20not%20be%20possible%20by%20restricting%20to%20only%20OEG%20%28%24L_1%24%29%20or%20OPGD%20%28%24L_2%24%29.%0A%20%20Our%20main%20result%20answers%20this%20question%20positively.%20We%20show%20that%20mirror%20maps%20based%20on%20block%20norms%20adapt%20better%20to%20the%20sparsity%20of%20loss%20functions%2C%20compared%20to%20previous%20%24L_p%24%20%28for%20%24p%20%5Cin%20%5B1%2C%202%5D%24%29%20interpolations.%20In%20particular%2C%20we%20construct%20a%20family%20of%20online%20convex%20optimization%20instances%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20where%20block%20norm-based%20mirror%20maps%20achieve%20a%20provable%20polynomial%20%28in%20%24d%24%29%20improvement%20in%20regret%20over%20OEG%20and%20OPGD%20for%20sparse%20loss%20functions.%20We%20then%20turn%20to%20the%20setting%20in%20which%20the%20sparsity%20level%20of%20the%20loss%20functions%20is%20unknown.%20In%20this%20case%2C%20the%20choice%20of%20geometry%20itself%20becomes%20an%20online%20decision%20problem.%20We%20first%20show%20that%20naively%20switching%20between%20OEG%20and%20OPGD%20can%20incur%20linear%20regret%2C%20highlighting%20the%20intrinsic%20difficulty%20of%20geometry%20selection.%20To%20overcome%20this%20issue%2C%20we%20propose%20a%20meta-algorithm%20based%20on%20multiplicative%20weights%20that%20dynamically%20selects%20among%20a%20family%20of%20uniform%20block%20norms.%20We%20show%20that%20this%20approach%20effectively%20tunes%20OMD%20to%20the%20sparsity%20of%20the%20losses%2C%20yielding%20adaptive%20regret%20guarantees.%20Overall%2C%20our%20results%20demonstrate%20that%20online%20mirror-map%20selection%20can%20significantly%20enhance%20the%20ability%20of%20OMD%20to%20exploit%20sparsity%20in%20online%20convex%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Regret%2520Guarantees%2520for%2520Online%2520Mirror%2520Descent%2520using%2520a%2520Portfolio%2520of%2520Mirror%2520Maps%26entry.906535625%3DSwati%2520Gupta%2520and%2520Jai%2520Moondra%2520and%2520Mohit%2520Singh%26entry.1292438233%3DOMD%2520and%2520its%2520variants%2520give%2520a%2520flexible%2520framework%2520for%2520OCO%2520where%2520the%2520performance%2520depends%2520crucially%2520on%2520the%2520choice%2520of%2520the%2520mirror%2520map.%2520While%2520the%2520geometries%2520underlying%2520OPGD%2520and%2520OEG%252C%2520both%2520special%2520cases%2520of%2520OMD%252C%2520are%2520well%2520understood%252C%2520it%2520remains%2520a%2520challenging%2520open%2520question%2520on%2520how%2520to%2520construct%2520an%2520optimal%2520mirror%2520map%2520for%2520any%2520given%2520constrained%2520set%2520and%2520a%2520general%2520family%2520of%2520loss%2520functions%252C%2520e.g.%252C%2520sparse%2520losses.%2520Motivated%2520by%2520parameterizing%2520a%2520near-optimal%2520set%2520of%2520mirror%2520maps%252C%2520we%2520consider%2520a%2520simpler%2520question%253A%2520is%2520it%2520even%2520possible%2520to%2520obtain%2520polynomial%2520gains%2520in%2520regret%2520by%2520using%2520mirror%2520maps%2520for%2520geometries%2520that%2520interpolate%2520between%2520%2524L_1%2524%2520and%2520%2524L_2%2524%252C%2520which%2520may%2520not%2520be%2520possible%2520by%2520restricting%2520to%2520only%2520OEG%2520%2528%2524L_1%2524%2529%2520or%2520OPGD%2520%2528%2524L_2%2524%2529.%250A%2520%2520Our%2520main%2520result%2520answers%2520this%2520question%2520positively.%2520We%2520show%2520that%2520mirror%2520maps%2520based%2520on%2520block%2520norms%2520adapt%2520better%2520to%2520the%2520sparsity%2520of%2520loss%2520functions%252C%2520compared%2520to%2520previous%2520%2524L_p%2524%2520%2528for%2520%2524p%2520%255Cin%2520%255B1%252C%25202%255D%2524%2529%2520interpolations.%2520In%2520particular%252C%2520we%2520construct%2520a%2520family%2520of%2520online%2520convex%2520optimization%2520instances%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520where%2520block%2520norm-based%2520mirror%2520maps%2520achieve%2520a%2520provable%2520polynomial%2520%2528in%2520%2524d%2524%2529%2520improvement%2520in%2520regret%2520over%2520OEG%2520and%2520OPGD%2520for%2520sparse%2520loss%2520functions.%2520We%2520then%2520turn%2520to%2520the%2520setting%2520in%2520which%2520the%2520sparsity%2520level%2520of%2520the%2520loss%2520functions%2520is%2520unknown.%2520In%2520this%2520case%252C%2520the%2520choice%2520of%2520geometry%2520itself%2520becomes%2520an%2520online%2520decision%2520problem.%2520We%2520first%2520show%2520that%2520naively%2520switching%2520between%2520OEG%2520and%2520OPGD%2520can%2520incur%2520linear%2520regret%252C%2520highlighting%2520the%2520intrinsic%2520difficulty%2520of%2520geometry%2520selection.%2520To%2520overcome%2520this%2520issue%252C%2520we%2520propose%2520a%2520meta-algorithm%2520based%2520on%2520multiplicative%2520weights%2520that%2520dynamically%2520selects%2520among%2520a%2520family%2520of%2520uniform%2520block%2520norms.%2520We%2520show%2520that%2520this%2520approach%2520effectively%2520tunes%2520OMD%2520to%2520the%2520sparsity%2520of%2520the%2520losses%252C%2520yielding%2520adaptive%2520regret%2520guarantees.%2520Overall%252C%2520our%2520results%2520demonstrate%2520that%2520online%2520mirror-map%2520selection%2520can%2520significantly%2520enhance%2520the%2520ability%2520of%2520OMD%2520to%2520exploit%2520sparsity%2520in%2520online%2520convex%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Regret%20Guarantees%20for%20Online%20Mirror%20Descent%20using%20a%20Portfolio%20of%20Mirror%20Maps&entry.906535625=Swati%20Gupta%20and%20Jai%20Moondra%20and%20Mohit%20Singh&entry.1292438233=OMD%20and%20its%20variants%20give%20a%20flexible%20framework%20for%20OCO%20where%20the%20performance%20depends%20crucially%20on%20the%20choice%20of%20the%20mirror%20map.%20While%20the%20geometries%20underlying%20OPGD%20and%20OEG%2C%20both%20special%20cases%20of%20OMD%2C%20are%20well%20understood%2C%20it%20remains%20a%20challenging%20open%20question%20on%20how%20to%20construct%20an%20optimal%20mirror%20map%20for%20any%20given%20constrained%20set%20and%20a%20general%20family%20of%20loss%20functions%2C%20e.g.%2C%20sparse%20losses.%20Motivated%20by%20parameterizing%20a%20near-optimal%20set%20of%20mirror%20maps%2C%20we%20consider%20a%20simpler%20question%3A%20is%20it%20even%20possible%20to%20obtain%20polynomial%20gains%20in%20regret%20by%20using%20mirror%20maps%20for%20geometries%20that%20interpolate%20between%20%24L_1%24%20and%20%24L_2%24%2C%20which%20may%20not%20be%20possible%20by%20restricting%20to%20only%20OEG%20%28%24L_1%24%29%20or%20OPGD%20%28%24L_2%24%29.%0A%20%20Our%20main%20result%20answers%20this%20question%20positively.%20We%20show%20that%20mirror%20maps%20based%20on%20block%20norms%20adapt%20better%20to%20the%20sparsity%20of%20loss%20functions%2C%20compared%20to%20previous%20%24L_p%24%20%28for%20%24p%20%5Cin%20%5B1%2C%202%5D%24%29%20interpolations.%20In%20particular%2C%20we%20construct%20a%20family%20of%20online%20convex%20optimization%20instances%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20where%20block%20norm-based%20mirror%20maps%20achieve%20a%20provable%20polynomial%20%28in%20%24d%24%29%20improvement%20in%20regret%20over%20OEG%20and%20OPGD%20for%20sparse%20loss%20functions.%20We%20then%20turn%20to%20the%20setting%20in%20which%20the%20sparsity%20level%20of%20the%20loss%20functions%20is%20unknown.%20In%20this%20case%2C%20the%20choice%20of%20geometry%20itself%20becomes%20an%20online%20decision%20problem.%20We%20first%20show%20that%20naively%20switching%20between%20OEG%20and%20OPGD%20can%20incur%20linear%20regret%2C%20highlighting%20the%20intrinsic%20difficulty%20of%20geometry%20selection.%20To%20overcome%20this%20issue%2C%20we%20propose%20a%20meta-algorithm%20based%20on%20multiplicative%20weights%20that%20dynamically%20selects%20among%20a%20family%20of%20uniform%20block%20norms.%20We%20show%20that%20this%20approach%20effectively%20tunes%20OMD%20to%20the%20sparsity%20of%20the%20losses%2C%20yielding%20adaptive%20regret%20guarantees.%20Overall%2C%20our%20results%20demonstrate%20that%20online%20mirror-map%20selection%20can%20significantly%20enhance%20the%20ability%20of%20OMD%20to%20exploit%20sparsity%20in%20online%20convex%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2602.13177v1&entry.124074799=Read"},
{"title": "Post-hoc Probabilistic Vision-Language Models", "author": "Anton Baumann and Rui Li and Marcus Klasson and Santeri Mentu and Shyamgopal Karthik and Zeynep Akata and Arno Solin and Martin Trapp", "abstract": "Vision-language models (VLMs), such as CLIP and SigLIP, have found remarkable success in classification, retrieval, and generative tasks. For this, VLMs deterministically map images and text descriptions to a joint latent space in which their similarity is assessed using the cosine similarity. However, a deterministic mapping of inputs fails to capture uncertainties over concepts arising from domain shifts when used in downstream tasks. In this work, we propose post-hoc uncertainty estimation in VLMs that does not require additional training. Our method leverages a Bayesian posterior approximation over the last layers in VLMs and analytically quantifies uncertainties over cosine similarities. We demonstrate its effectiveness for uncertainty quantification and support set selection in active learning. Compared to baselines, we obtain improved and well-calibrated predictive uncertainties, interpretable uncertainty estimates, and sample-efficient active learning. Our results show promise for safety-critical applications of large-scale models.", "link": "http://arxiv.org/abs/2412.06014v5", "date": "2026-02-13", "relevancy": 2.4404, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6158}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.6129}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post-hoc%20Probabilistic%20Vision-Language%20Models&body=Title%3A%20Post-hoc%20Probabilistic%20Vision-Language%20Models%0AAuthor%3A%20Anton%20Baumann%20and%20Rui%20Li%20and%20Marcus%20Klasson%20and%20Santeri%20Mentu%20and%20Shyamgopal%20Karthik%20and%20Zeynep%20Akata%20and%20Arno%20Solin%20and%20Martin%20Trapp%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%20and%20SigLIP%2C%20have%20found%20remarkable%20success%20in%20classification%2C%20retrieval%2C%20and%20generative%20tasks.%20For%20this%2C%20VLMs%20deterministically%20map%20images%20and%20text%20descriptions%20to%20a%20joint%20latent%20space%20in%20which%20their%20similarity%20is%20assessed%20using%20the%20cosine%20similarity.%20However%2C%20a%20deterministic%20mapping%20of%20inputs%20fails%20to%20capture%20uncertainties%20over%20concepts%20arising%20from%20domain%20shifts%20when%20used%20in%20downstream%20tasks.%20In%20this%20work%2C%20we%20propose%20post-hoc%20uncertainty%20estimation%20in%20VLMs%20that%20does%20not%20require%20additional%20training.%20Our%20method%20leverages%20a%20Bayesian%20posterior%20approximation%20over%20the%20last%20layers%20in%20VLMs%20and%20analytically%20quantifies%20uncertainties%20over%20cosine%20similarities.%20We%20demonstrate%20its%20effectiveness%20for%20uncertainty%20quantification%20and%20support%20set%20selection%20in%20active%20learning.%20Compared%20to%20baselines%2C%20we%20obtain%20improved%20and%20well-calibrated%20predictive%20uncertainties%2C%20interpretable%20uncertainty%20estimates%2C%20and%20sample-efficient%20active%20learning.%20Our%20results%20show%20promise%20for%20safety-critical%20applications%20of%20large-scale%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2412.06014v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost-hoc%2520Probabilistic%2520Vision-Language%2520Models%26entry.906535625%3DAnton%2520Baumann%2520and%2520Rui%2520Li%2520and%2520Marcus%2520Klasson%2520and%2520Santeri%2520Mentu%2520and%2520Shyamgopal%2520Karthik%2520and%2520Zeynep%2520Akata%2520and%2520Arno%2520Solin%2520and%2520Martin%2520Trapp%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%2520and%2520SigLIP%252C%2520have%2520found%2520remarkable%2520success%2520in%2520classification%252C%2520retrieval%252C%2520and%2520generative%2520tasks.%2520For%2520this%252C%2520VLMs%2520deterministically%2520map%2520images%2520and%2520text%2520descriptions%2520to%2520a%2520joint%2520latent%2520space%2520in%2520which%2520their%2520similarity%2520is%2520assessed%2520using%2520the%2520cosine%2520similarity.%2520However%252C%2520a%2520deterministic%2520mapping%2520of%2520inputs%2520fails%2520to%2520capture%2520uncertainties%2520over%2520concepts%2520arising%2520from%2520domain%2520shifts%2520when%2520used%2520in%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520post-hoc%2520uncertainty%2520estimation%2520in%2520VLMs%2520that%2520does%2520not%2520require%2520additional%2520training.%2520Our%2520method%2520leverages%2520a%2520Bayesian%2520posterior%2520approximation%2520over%2520the%2520last%2520layers%2520in%2520VLMs%2520and%2520analytically%2520quantifies%2520uncertainties%2520over%2520cosine%2520similarities.%2520We%2520demonstrate%2520its%2520effectiveness%2520for%2520uncertainty%2520quantification%2520and%2520support%2520set%2520selection%2520in%2520active%2520learning.%2520Compared%2520to%2520baselines%252C%2520we%2520obtain%2520improved%2520and%2520well-calibrated%2520predictive%2520uncertainties%252C%2520interpretable%2520uncertainty%2520estimates%252C%2520and%2520sample-efficient%2520active%2520learning.%2520Our%2520results%2520show%2520promise%2520for%2520safety-critical%2520applications%2520of%2520large-scale%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06014v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-hoc%20Probabilistic%20Vision-Language%20Models&entry.906535625=Anton%20Baumann%20and%20Rui%20Li%20and%20Marcus%20Klasson%20and%20Santeri%20Mentu%20and%20Shyamgopal%20Karthik%20and%20Zeynep%20Akata%20and%20Arno%20Solin%20and%20Martin%20Trapp&entry.1292438233=Vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%20and%20SigLIP%2C%20have%20found%20remarkable%20success%20in%20classification%2C%20retrieval%2C%20and%20generative%20tasks.%20For%20this%2C%20VLMs%20deterministically%20map%20images%20and%20text%20descriptions%20to%20a%20joint%20latent%20space%20in%20which%20their%20similarity%20is%20assessed%20using%20the%20cosine%20similarity.%20However%2C%20a%20deterministic%20mapping%20of%20inputs%20fails%20to%20capture%20uncertainties%20over%20concepts%20arising%20from%20domain%20shifts%20when%20used%20in%20downstream%20tasks.%20In%20this%20work%2C%20we%20propose%20post-hoc%20uncertainty%20estimation%20in%20VLMs%20that%20does%20not%20require%20additional%20training.%20Our%20method%20leverages%20a%20Bayesian%20posterior%20approximation%20over%20the%20last%20layers%20in%20VLMs%20and%20analytically%20quantifies%20uncertainties%20over%20cosine%20similarities.%20We%20demonstrate%20its%20effectiveness%20for%20uncertainty%20quantification%20and%20support%20set%20selection%20in%20active%20learning.%20Compared%20to%20baselines%2C%20we%20obtain%20improved%20and%20well-calibrated%20predictive%20uncertainties%2C%20interpretable%20uncertainty%20estimates%2C%20and%20sample-efficient%20active%20learning.%20Our%20results%20show%20promise%20for%20safety-critical%20applications%20of%20large-scale%20models.&entry.1838667208=http%3A//arxiv.org/abs/2412.06014v5&entry.124074799=Read"},
{"title": "AdaGrad-Diff: A New Version of the Adaptive Gradient Algorithm", "author": "Matia Bojovic and Saverio Salzo and Massimiliano Pontil", "abstract": "Vanilla gradient methods are often highly sensitive to the choice of stepsize, which typically requires manual tuning. Adaptive methods alleviate this issue and have therefore become widely used. Among them, AdaGrad has been particularly influential. In this paper, we propose an AdaGrad-style adaptive method in which the adaptation is driven by the cumulative squared norms of successive gradient differences rather than gradient norms themselves. The key idea is that when gradients vary little across iterations, the stepsize is not unnecessarily reduced, while significant gradient fluctuations, reflecting curvature or instability, lead to automatic stepsize damping. Numerical experiments demonstrate that the proposed method is more robust than AdaGrad in several practically relevant settings.", "link": "http://arxiv.org/abs/2602.13112v1", "date": "2026-02-13", "relevancy": 2.4403, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4971}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4899}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaGrad-Diff%3A%20A%20New%20Version%20of%20the%20Adaptive%20Gradient%20Algorithm&body=Title%3A%20AdaGrad-Diff%3A%20A%20New%20Version%20of%20the%20Adaptive%20Gradient%20Algorithm%0AAuthor%3A%20Matia%20Bojovic%20and%20Saverio%20Salzo%20and%20Massimiliano%20Pontil%0AAbstract%3A%20Vanilla%20gradient%20methods%20are%20often%20highly%20sensitive%20to%20the%20choice%20of%20stepsize%2C%20which%20typically%20requires%20manual%20tuning.%20Adaptive%20methods%20alleviate%20this%20issue%20and%20have%20therefore%20become%20widely%20used.%20Among%20them%2C%20AdaGrad%20has%20been%20particularly%20influential.%20In%20this%20paper%2C%20we%20propose%20an%20AdaGrad-style%20adaptive%20method%20in%20which%20the%20adaptation%20is%20driven%20by%20the%20cumulative%20squared%20norms%20of%20successive%20gradient%20differences%20rather%20than%20gradient%20norms%20themselves.%20The%20key%20idea%20is%20that%20when%20gradients%20vary%20little%20across%20iterations%2C%20the%20stepsize%20is%20not%20unnecessarily%20reduced%2C%20while%20significant%20gradient%20fluctuations%2C%20reflecting%20curvature%20or%20instability%2C%20lead%20to%20automatic%20stepsize%20damping.%20Numerical%20experiments%20demonstrate%20that%20the%20proposed%20method%20is%20more%20robust%20than%20AdaGrad%20in%20several%20practically%20relevant%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaGrad-Diff%253A%2520A%2520New%2520Version%2520of%2520the%2520Adaptive%2520Gradient%2520Algorithm%26entry.906535625%3DMatia%2520Bojovic%2520and%2520Saverio%2520Salzo%2520and%2520Massimiliano%2520Pontil%26entry.1292438233%3DVanilla%2520gradient%2520methods%2520are%2520often%2520highly%2520sensitive%2520to%2520the%2520choice%2520of%2520stepsize%252C%2520which%2520typically%2520requires%2520manual%2520tuning.%2520Adaptive%2520methods%2520alleviate%2520this%2520issue%2520and%2520have%2520therefore%2520become%2520widely%2520used.%2520Among%2520them%252C%2520AdaGrad%2520has%2520been%2520particularly%2520influential.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520AdaGrad-style%2520adaptive%2520method%2520in%2520which%2520the%2520adaptation%2520is%2520driven%2520by%2520the%2520cumulative%2520squared%2520norms%2520of%2520successive%2520gradient%2520differences%2520rather%2520than%2520gradient%2520norms%2520themselves.%2520The%2520key%2520idea%2520is%2520that%2520when%2520gradients%2520vary%2520little%2520across%2520iterations%252C%2520the%2520stepsize%2520is%2520not%2520unnecessarily%2520reduced%252C%2520while%2520significant%2520gradient%2520fluctuations%252C%2520reflecting%2520curvature%2520or%2520instability%252C%2520lead%2520to%2520automatic%2520stepsize%2520damping.%2520Numerical%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520method%2520is%2520more%2520robust%2520than%2520AdaGrad%2520in%2520several%2520practically%2520relevant%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaGrad-Diff%3A%20A%20New%20Version%20of%20the%20Adaptive%20Gradient%20Algorithm&entry.906535625=Matia%20Bojovic%20and%20Saverio%20Salzo%20and%20Massimiliano%20Pontil&entry.1292438233=Vanilla%20gradient%20methods%20are%20often%20highly%20sensitive%20to%20the%20choice%20of%20stepsize%2C%20which%20typically%20requires%20manual%20tuning.%20Adaptive%20methods%20alleviate%20this%20issue%20and%20have%20therefore%20become%20widely%20used.%20Among%20them%2C%20AdaGrad%20has%20been%20particularly%20influential.%20In%20this%20paper%2C%20we%20propose%20an%20AdaGrad-style%20adaptive%20method%20in%20which%20the%20adaptation%20is%20driven%20by%20the%20cumulative%20squared%20norms%20of%20successive%20gradient%20differences%20rather%20than%20gradient%20norms%20themselves.%20The%20key%20idea%20is%20that%20when%20gradients%20vary%20little%20across%20iterations%2C%20the%20stepsize%20is%20not%20unnecessarily%20reduced%2C%20while%20significant%20gradient%20fluctuations%2C%20reflecting%20curvature%20or%20instability%2C%20lead%20to%20automatic%20stepsize%20damping.%20Numerical%20experiments%20demonstrate%20that%20the%20proposed%20method%20is%20more%20robust%20than%20AdaGrad%20in%20several%20practically%20relevant%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2602.13112v1&entry.124074799=Read"},
{"title": "tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models", "author": "Kevin Li and Dibyadeep Saha and Avni Kanodia and Fan Lai", "abstract": "As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone. While recent advances enable batching (co-locating) multiple adapters during serving, efficient training-time co-location of heterogeneous LoRA adapters presents unique challenges. Jobs often differ in adapter rank, batch size, and resource allocation, and na\u00efve batching can introduce synchronization stalls, communication overheads, and per-job slowdowns that are worse than executing independently. We introduce tLoRA, a framework that enables efficient batch training of multiple LoRA jobs. tLoRA fuses adapters that share the same base model into an elastic shared super-model, exploiting existing distributed training frameworks to derive parallelism plans that share resources effectively. At the kernel level, tLoRA employs a fused LoRA kernel that adaptively reconstructs low-rank computation tiles and schedules rank-aware nano-batches to maximize overlap between computation and communication across adapters. At the scheduling layer, tLoRA incorporates an online, residual-capacity-aware scheduler that adaptively groups jobs to maximize collective throughput. Evaluations using real-world cluster traces demonstrate that tLoRA improves training throughput by 1.2--1.8x, job training completion time by 2.3--5.4x, and GPU utilization by 37%.", "link": "http://arxiv.org/abs/2602.07263v2", "date": "2026-02-13", "relevancy": 2.4202, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.468}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20tLoRA%3A%20Efficient%20Multi-LoRA%20Training%20with%20Elastic%20Shared%20Super-Models&body=Title%3A%20tLoRA%3A%20Efficient%20Multi-LoRA%20Training%20with%20Elastic%20Shared%20Super-Models%0AAuthor%3A%20Kevin%20Li%20and%20Dibyadeep%20Saha%20and%20Avni%20Kanodia%20and%20Fan%20Lai%0AAbstract%3A%20As%20Low-Rank%20Adaptation%20%28LoRA%29%20becomes%20the%20standard%20approach%20for%20efficiently%20fine-tuning%20large%20language%20models%20%28LLMs%29%2C%20shared%20clusters%20increasingly%20execute%20many%20concurrent%20LoRA%20training%20jobs%20over%20the%20same%20frozen%20backbone.%20While%20recent%20advances%20enable%20batching%20%28co-locating%29%20multiple%20adapters%20during%20serving%2C%20efficient%20training-time%20co-location%20of%20heterogeneous%20LoRA%20adapters%20presents%20unique%20challenges.%20Jobs%20often%20differ%20in%20adapter%20rank%2C%20batch%20size%2C%20and%20resource%20allocation%2C%20and%20na%C3%AFve%20batching%20can%20introduce%20synchronization%20stalls%2C%20communication%20overheads%2C%20and%20per-job%20slowdowns%20that%20are%20worse%20than%20executing%20independently.%20We%20introduce%20tLoRA%2C%20a%20framework%20that%20enables%20efficient%20batch%20training%20of%20multiple%20LoRA%20jobs.%20tLoRA%20fuses%20adapters%20that%20share%20the%20same%20base%20model%20into%20an%20elastic%20shared%20super-model%2C%20exploiting%20existing%20distributed%20training%20frameworks%20to%20derive%20parallelism%20plans%20that%20share%20resources%20effectively.%20At%20the%20kernel%20level%2C%20tLoRA%20employs%20a%20fused%20LoRA%20kernel%20that%20adaptively%20reconstructs%20low-rank%20computation%20tiles%20and%20schedules%20rank-aware%20nano-batches%20to%20maximize%20overlap%20between%20computation%20and%20communication%20across%20adapters.%20At%20the%20scheduling%20layer%2C%20tLoRA%20incorporates%20an%20online%2C%20residual-capacity-aware%20scheduler%20that%20adaptively%20groups%20jobs%20to%20maximize%20collective%20throughput.%20Evaluations%20using%20real-world%20cluster%20traces%20demonstrate%20that%20tLoRA%20improves%20training%20throughput%20by%201.2--1.8x%2C%20job%20training%20completion%20time%20by%202.3--5.4x%2C%20and%20GPU%20utilization%20by%2037%25.%0ALink%3A%20http%3A//arxiv.org/abs/2602.07263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DtLoRA%253A%2520Efficient%2520Multi-LoRA%2520Training%2520with%2520Elastic%2520Shared%2520Super-Models%26entry.906535625%3DKevin%2520Li%2520and%2520Dibyadeep%2520Saha%2520and%2520Avni%2520Kanodia%2520and%2520Fan%2520Lai%26entry.1292438233%3DAs%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520becomes%2520the%2520standard%2520approach%2520for%2520efficiently%2520fine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520shared%2520clusters%2520increasingly%2520execute%2520many%2520concurrent%2520LoRA%2520training%2520jobs%2520over%2520the%2520same%2520frozen%2520backbone.%2520While%2520recent%2520advances%2520enable%2520batching%2520%2528co-locating%2529%2520multiple%2520adapters%2520during%2520serving%252C%2520efficient%2520training-time%2520co-location%2520of%2520heterogeneous%2520LoRA%2520adapters%2520presents%2520unique%2520challenges.%2520Jobs%2520often%2520differ%2520in%2520adapter%2520rank%252C%2520batch%2520size%252C%2520and%2520resource%2520allocation%252C%2520and%2520na%25C3%25AFve%2520batching%2520can%2520introduce%2520synchronization%2520stalls%252C%2520communication%2520overheads%252C%2520and%2520per-job%2520slowdowns%2520that%2520are%2520worse%2520than%2520executing%2520independently.%2520We%2520introduce%2520tLoRA%252C%2520a%2520framework%2520that%2520enables%2520efficient%2520batch%2520training%2520of%2520multiple%2520LoRA%2520jobs.%2520tLoRA%2520fuses%2520adapters%2520that%2520share%2520the%2520same%2520base%2520model%2520into%2520an%2520elastic%2520shared%2520super-model%252C%2520exploiting%2520existing%2520distributed%2520training%2520frameworks%2520to%2520derive%2520parallelism%2520plans%2520that%2520share%2520resources%2520effectively.%2520At%2520the%2520kernel%2520level%252C%2520tLoRA%2520employs%2520a%2520fused%2520LoRA%2520kernel%2520that%2520adaptively%2520reconstructs%2520low-rank%2520computation%2520tiles%2520and%2520schedules%2520rank-aware%2520nano-batches%2520to%2520maximize%2520overlap%2520between%2520computation%2520and%2520communication%2520across%2520adapters.%2520At%2520the%2520scheduling%2520layer%252C%2520tLoRA%2520incorporates%2520an%2520online%252C%2520residual-capacity-aware%2520scheduler%2520that%2520adaptively%2520groups%2520jobs%2520to%2520maximize%2520collective%2520throughput.%2520Evaluations%2520using%2520real-world%2520cluster%2520traces%2520demonstrate%2520that%2520tLoRA%2520improves%2520training%2520throughput%2520by%25201.2--1.8x%252C%2520job%2520training%2520completion%2520time%2520by%25202.3--5.4x%252C%2520and%2520GPU%2520utilization%2520by%252037%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.07263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=tLoRA%3A%20Efficient%20Multi-LoRA%20Training%20with%20Elastic%20Shared%20Super-Models&entry.906535625=Kevin%20Li%20and%20Dibyadeep%20Saha%20and%20Avni%20Kanodia%20and%20Fan%20Lai&entry.1292438233=As%20Low-Rank%20Adaptation%20%28LoRA%29%20becomes%20the%20standard%20approach%20for%20efficiently%20fine-tuning%20large%20language%20models%20%28LLMs%29%2C%20shared%20clusters%20increasingly%20execute%20many%20concurrent%20LoRA%20training%20jobs%20over%20the%20same%20frozen%20backbone.%20While%20recent%20advances%20enable%20batching%20%28co-locating%29%20multiple%20adapters%20during%20serving%2C%20efficient%20training-time%20co-location%20of%20heterogeneous%20LoRA%20adapters%20presents%20unique%20challenges.%20Jobs%20often%20differ%20in%20adapter%20rank%2C%20batch%20size%2C%20and%20resource%20allocation%2C%20and%20na%C3%AFve%20batching%20can%20introduce%20synchronization%20stalls%2C%20communication%20overheads%2C%20and%20per-job%20slowdowns%20that%20are%20worse%20than%20executing%20independently.%20We%20introduce%20tLoRA%2C%20a%20framework%20that%20enables%20efficient%20batch%20training%20of%20multiple%20LoRA%20jobs.%20tLoRA%20fuses%20adapters%20that%20share%20the%20same%20base%20model%20into%20an%20elastic%20shared%20super-model%2C%20exploiting%20existing%20distributed%20training%20frameworks%20to%20derive%20parallelism%20plans%20that%20share%20resources%20effectively.%20At%20the%20kernel%20level%2C%20tLoRA%20employs%20a%20fused%20LoRA%20kernel%20that%20adaptively%20reconstructs%20low-rank%20computation%20tiles%20and%20schedules%20rank-aware%20nano-batches%20to%20maximize%20overlap%20between%20computation%20and%20communication%20across%20adapters.%20At%20the%20scheduling%20layer%2C%20tLoRA%20incorporates%20an%20online%2C%20residual-capacity-aware%20scheduler%20that%20adaptively%20groups%20jobs%20to%20maximize%20collective%20throughput.%20Evaluations%20using%20real-world%20cluster%20traces%20demonstrate%20that%20tLoRA%20improves%20training%20throughput%20by%201.2--1.8x%2C%20job%20training%20completion%20time%20by%202.3--5.4x%2C%20and%20GPU%20utilization%20by%2037%25.&entry.1838667208=http%3A//arxiv.org/abs/2602.07263v2&entry.124074799=Read"},
{"title": "Learnable Chernoff Baselines for Inference-Time Alignment", "author": "Sunil Madhow and Yuchen Liang and Ness Shroff and Yingbin Liang and Yu-Xiang Wang", "abstract": "We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.", "link": "http://arxiv.org/abs/2602.07738v2", "date": "2026-02-13", "relevancy": 2.4201, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5098}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4747}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnable%20Chernoff%20Baselines%20for%20Inference-Time%20Alignment&body=Title%3A%20Learnable%20Chernoff%20Baselines%20for%20Inference-Time%20Alignment%0AAuthor%3A%20Sunil%20Madhow%20and%20Yuchen%20Liang%20and%20Ness%20Shroff%20and%20Yingbin%20Liang%20and%20Yu-Xiang%20Wang%0AAbstract%3A%20We%20study%20inference-time%20reward-guided%20alignment%20for%20generative%20models.%20Existing%20methods%20often%20rely%20on%20either%20architecture-specific%20adaptations%20or%20computationally%20costly%20inference%20procedures.%20We%20introduce%20Learnable%20Chernoff%20Baselines%20%28LCBs%29%20as%20a%20method%20for%20efficiently%20and%20approximately%20sampling%20from%20the%20exponentially%20tilted%20kernels%20that%20arise%20from%20KL-regularized%20reward%20alignment.%20Using%20only%20black-box%20sampling%20access%20to%20the%20pretrained%20model%2C%20LCBs%20implement%20a%20form%20of%20rejection%20sampling%20with%20adaptively%20selected%20acceptance%20probabilities%2C%20which%20allows%20fine-grained%20control%20over%20inference-compute%20scaling.%20We%20establish%20total-variation%20guarantees%20to%20the%20ideal%20aligned%20model%2C%20and%20demonstrate%20in%20both%20continuous%20and%20discrete%20diffusion%20settings%20that%20LCB%20sampling%20closely%20matches%20ideal%20rejection%20sampling%20while%20using%20substantially%20fewer%20queries%20to%20the%20pretrained%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2602.07738v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnable%2520Chernoff%2520Baselines%2520for%2520Inference-Time%2520Alignment%26entry.906535625%3DSunil%2520Madhow%2520and%2520Yuchen%2520Liang%2520and%2520Ness%2520Shroff%2520and%2520Yingbin%2520Liang%2520and%2520Yu-Xiang%2520Wang%26entry.1292438233%3DWe%2520study%2520inference-time%2520reward-guided%2520alignment%2520for%2520generative%2520models.%2520Existing%2520methods%2520often%2520rely%2520on%2520either%2520architecture-specific%2520adaptations%2520or%2520computationally%2520costly%2520inference%2520procedures.%2520We%2520introduce%2520Learnable%2520Chernoff%2520Baselines%2520%2528LCBs%2529%2520as%2520a%2520method%2520for%2520efficiently%2520and%2520approximately%2520sampling%2520from%2520the%2520exponentially%2520tilted%2520kernels%2520that%2520arise%2520from%2520KL-regularized%2520reward%2520alignment.%2520Using%2520only%2520black-box%2520sampling%2520access%2520to%2520the%2520pretrained%2520model%252C%2520LCBs%2520implement%2520a%2520form%2520of%2520rejection%2520sampling%2520with%2520adaptively%2520selected%2520acceptance%2520probabilities%252C%2520which%2520allows%2520fine-grained%2520control%2520over%2520inference-compute%2520scaling.%2520We%2520establish%2520total-variation%2520guarantees%2520to%2520the%2520ideal%2520aligned%2520model%252C%2520and%2520demonstrate%2520in%2520both%2520continuous%2520and%2520discrete%2520diffusion%2520settings%2520that%2520LCB%2520sampling%2520closely%2520matches%2520ideal%2520rejection%2520sampling%2520while%2520using%2520substantially%2520fewer%2520queries%2520to%2520the%2520pretrained%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.07738v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20Chernoff%20Baselines%20for%20Inference-Time%20Alignment&entry.906535625=Sunil%20Madhow%20and%20Yuchen%20Liang%20and%20Ness%20Shroff%20and%20Yingbin%20Liang%20and%20Yu-Xiang%20Wang&entry.1292438233=We%20study%20inference-time%20reward-guided%20alignment%20for%20generative%20models.%20Existing%20methods%20often%20rely%20on%20either%20architecture-specific%20adaptations%20or%20computationally%20costly%20inference%20procedures.%20We%20introduce%20Learnable%20Chernoff%20Baselines%20%28LCBs%29%20as%20a%20method%20for%20efficiently%20and%20approximately%20sampling%20from%20the%20exponentially%20tilted%20kernels%20that%20arise%20from%20KL-regularized%20reward%20alignment.%20Using%20only%20black-box%20sampling%20access%20to%20the%20pretrained%20model%2C%20LCBs%20implement%20a%20form%20of%20rejection%20sampling%20with%20adaptively%20selected%20acceptance%20probabilities%2C%20which%20allows%20fine-grained%20control%20over%20inference-compute%20scaling.%20We%20establish%20total-variation%20guarantees%20to%20the%20ideal%20aligned%20model%2C%20and%20demonstrate%20in%20both%20continuous%20and%20discrete%20diffusion%20settings%20that%20LCB%20sampling%20closely%20matches%20ideal%20rejection%20sampling%20while%20using%20substantially%20fewer%20queries%20to%20the%20pretrained%20model.&entry.1838667208=http%3A//arxiv.org/abs/2602.07738v2&entry.124074799=Read"},
{"title": "LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection", "author": "Youssef Attia El Hili and Albert Thomas and Malik Tiomoko and Abdelhakim Benechehab and Corentin L\u00e9ger and Corinne Ancourt and Bal\u00e1zs K\u00e9gl", "abstract": "Model and hyperparameter selection are critical but challenging in machine learning, typically requiring expert intuition or expensive automated search. We investigate whether large language models (LLMs) can act as in-context meta-learners for this task. By converting each dataset into interpretable metadata, we prompt an LLM to recommend both model families and hyperparameters. We study two prompting strategies: (1) a zero-shot mode relying solely on pretrained knowledge, and (2) a meta-informed mode augmented with examples of models and their performance on past tasks. Across synthetic and real-world benchmarks, we show that LLMs can exploit dataset metadata to recommend competitive models and hyperparameters without search, and that improvements from meta-informed prompting demonstrate their capacity for in-context meta-learning. These results highlight a promising new role for LLMs as lightweight, general-purpose assistants for model selection and hyperparameter optimization.", "link": "http://arxiv.org/abs/2510.26510v3", "date": "2026-02-13", "relevancy": 2.4194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4885}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20as%20In-Context%20Meta-Learners%20for%20Model%20and%20Hyperparameter%20Selection&body=Title%3A%20LLMs%20as%20In-Context%20Meta-Learners%20for%20Model%20and%20Hyperparameter%20Selection%0AAuthor%3A%20Youssef%20Attia%20El%20Hili%20and%20Albert%20Thomas%20and%20Malik%20Tiomoko%20and%20Abdelhakim%20Benechehab%20and%20Corentin%20L%C3%A9ger%20and%20Corinne%20Ancourt%20and%20Bal%C3%A1zs%20K%C3%A9gl%0AAbstract%3A%20Model%20and%20hyperparameter%20selection%20are%20critical%20but%20challenging%20in%20machine%20learning%2C%20typically%20requiring%20expert%20intuition%20or%20expensive%20automated%20search.%20We%20investigate%20whether%20large%20language%20models%20%28LLMs%29%20can%20act%20as%20in-context%20meta-learners%20for%20this%20task.%20By%20converting%20each%20dataset%20into%20interpretable%20metadata%2C%20we%20prompt%20an%20LLM%20to%20recommend%20both%20model%20families%20and%20hyperparameters.%20We%20study%20two%20prompting%20strategies%3A%20%281%29%20a%20zero-shot%20mode%20relying%20solely%20on%20pretrained%20knowledge%2C%20and%20%282%29%20a%20meta-informed%20mode%20augmented%20with%20examples%20of%20models%20and%20their%20performance%20on%20past%20tasks.%20Across%20synthetic%20and%20real-world%20benchmarks%2C%20we%20show%20that%20LLMs%20can%20exploit%20dataset%20metadata%20to%20recommend%20competitive%20models%20and%20hyperparameters%20without%20search%2C%20and%20that%20improvements%20from%20meta-informed%20prompting%20demonstrate%20their%20capacity%20for%20in-context%20meta-learning.%20These%20results%20highlight%20a%20promising%20new%20role%20for%20LLMs%20as%20lightweight%2C%20general-purpose%20assistants%20for%20model%20selection%20and%20hyperparameter%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2510.26510v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520as%2520In-Context%2520Meta-Learners%2520for%2520Model%2520and%2520Hyperparameter%2520Selection%26entry.906535625%3DYoussef%2520Attia%2520El%2520Hili%2520and%2520Albert%2520Thomas%2520and%2520Malik%2520Tiomoko%2520and%2520Abdelhakim%2520Benechehab%2520and%2520Corentin%2520L%25C3%25A9ger%2520and%2520Corinne%2520Ancourt%2520and%2520Bal%25C3%25A1zs%2520K%25C3%25A9gl%26entry.1292438233%3DModel%2520and%2520hyperparameter%2520selection%2520are%2520critical%2520but%2520challenging%2520in%2520machine%2520learning%252C%2520typically%2520requiring%2520expert%2520intuition%2520or%2520expensive%2520automated%2520search.%2520We%2520investigate%2520whether%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520act%2520as%2520in-context%2520meta-learners%2520for%2520this%2520task.%2520By%2520converting%2520each%2520dataset%2520into%2520interpretable%2520metadata%252C%2520we%2520prompt%2520an%2520LLM%2520to%2520recommend%2520both%2520model%2520families%2520and%2520hyperparameters.%2520We%2520study%2520two%2520prompting%2520strategies%253A%2520%25281%2529%2520a%2520zero-shot%2520mode%2520relying%2520solely%2520on%2520pretrained%2520knowledge%252C%2520and%2520%25282%2529%2520a%2520meta-informed%2520mode%2520augmented%2520with%2520examples%2520of%2520models%2520and%2520their%2520performance%2520on%2520past%2520tasks.%2520Across%2520synthetic%2520and%2520real-world%2520benchmarks%252C%2520we%2520show%2520that%2520LLMs%2520can%2520exploit%2520dataset%2520metadata%2520to%2520recommend%2520competitive%2520models%2520and%2520hyperparameters%2520without%2520search%252C%2520and%2520that%2520improvements%2520from%2520meta-informed%2520prompting%2520demonstrate%2520their%2520capacity%2520for%2520in-context%2520meta-learning.%2520These%2520results%2520highlight%2520a%2520promising%2520new%2520role%2520for%2520LLMs%2520as%2520lightweight%252C%2520general-purpose%2520assistants%2520for%2520model%2520selection%2520and%2520hyperparameter%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26510v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20as%20In-Context%20Meta-Learners%20for%20Model%20and%20Hyperparameter%20Selection&entry.906535625=Youssef%20Attia%20El%20Hili%20and%20Albert%20Thomas%20and%20Malik%20Tiomoko%20and%20Abdelhakim%20Benechehab%20and%20Corentin%20L%C3%A9ger%20and%20Corinne%20Ancourt%20and%20Bal%C3%A1zs%20K%C3%A9gl&entry.1292438233=Model%20and%20hyperparameter%20selection%20are%20critical%20but%20challenging%20in%20machine%20learning%2C%20typically%20requiring%20expert%20intuition%20or%20expensive%20automated%20search.%20We%20investigate%20whether%20large%20language%20models%20%28LLMs%29%20can%20act%20as%20in-context%20meta-learners%20for%20this%20task.%20By%20converting%20each%20dataset%20into%20interpretable%20metadata%2C%20we%20prompt%20an%20LLM%20to%20recommend%20both%20model%20families%20and%20hyperparameters.%20We%20study%20two%20prompting%20strategies%3A%20%281%29%20a%20zero-shot%20mode%20relying%20solely%20on%20pretrained%20knowledge%2C%20and%20%282%29%20a%20meta-informed%20mode%20augmented%20with%20examples%20of%20models%20and%20their%20performance%20on%20past%20tasks.%20Across%20synthetic%20and%20real-world%20benchmarks%2C%20we%20show%20that%20LLMs%20can%20exploit%20dataset%20metadata%20to%20recommend%20competitive%20models%20and%20hyperparameters%20without%20search%2C%20and%20that%20improvements%20from%20meta-informed%20prompting%20demonstrate%20their%20capacity%20for%20in-context%20meta-learning.%20These%20results%20highlight%20a%20promising%20new%20role%20for%20LLMs%20as%20lightweight%2C%20general-purpose%20assistants%20for%20model%20selection%20and%20hyperparameter%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2510.26510v3&entry.124074799=Read"},
{"title": "Optimal Take-off under Fuzzy Clearances", "author": "Hugo Henry and Arthur Tsai and Kelly Cohen", "abstract": "This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.", "link": "http://arxiv.org/abs/2602.13166v1", "date": "2026-02-13", "relevancy": 2.3823, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4895}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4732}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Take-off%20under%20Fuzzy%20Clearances&body=Title%3A%20Optimal%20Take-off%20under%20Fuzzy%20Clearances%0AAuthor%3A%20Hugo%20Henry%20and%20Arthur%20Tsai%20and%20Kelly%20Cohen%0AAbstract%3A%20This%20paper%20presents%20a%20hybrid%20obstacle%20avoidance%20architecture%20that%20integrates%20Optimal%20Control%20under%20clearance%20with%20a%20Fuzzy%20Rule%20Based%20System%20%28FRBS%29%20to%20enable%20adaptive%20constraint%20handling%20for%20unmanned%20aircraft.%20Motivated%20by%20the%20limitations%20of%20classical%20optimal%20control%20under%20uncertainty%20and%20the%20need%20for%20interpretable%20decision%20making%20in%20safety%20critical%20aviation%20systems%2C%20we%20design%20a%20three%20stage%20Takagi%20Sugeno%20Kang%20fuzzy%20layer%20that%20modulates%20constraint%20radii%2C%20urgency%20levels%2C%20and%20activation%20decisions%20based%20on%20regulatory%20separation%20minima%20and%20airworthiness%20guidelines%20from%20FAA%20and%20EASA.%20These%20fuzzy-derived%20clearances%20are%20then%20incorporated%20as%20soft%20constraints%20into%20an%20optimal%20control%20problem%20solved%20using%20the%20FALCON%20toolbox%20and%20IPOPT.%20The%20framework%20aims%20to%20reduce%20unnecessary%20recomputations%20by%20selectively%20activating%20obstacle%20avoidance%20updates%20while%20maintaining%20compliance%20with%20aviation%20procedures.%20A%20proof%20of%20concept%20implementation%20using%20a%20simplified%20aircraft%20model%20demonstrates%20that%20the%20approach%20can%20generate%20optimal%20trajectories%20with%20computation%20times%20of%202%2C3%20seconds%20per%20iteration%20in%20a%20single%20threaded%20MATLAB%20environment%2C%20suggesting%20feasibility%20for%20near%20real%20time%20applications.%20However%2C%20our%20experiments%20revealed%20a%20critical%20software%20incompatibility%20in%20the%20latest%20versions%20of%20FALCON%20and%20IPOPT%2C%20in%20which%20the%20Lagrangian%20penalty%20term%20remained%20identically%20zero%2C%20preventing%20proper%20constraint%20enforcement.%20This%20behavior%20was%20consistent%20across%20scenarios%20and%20indicates%20a%20solver%20toolbox%20regression%20rather%20than%20a%20modeling%20flaw.%20Future%20work%20includes%20validating%20this%20effect%20by%20reverting%20to%20earlier%20software%20versions%2C%20optimizing%20the%20fuzzy%20membership%20functions%20using%20evolutionary%20methods%2C%20and%20extending%20the%20system%20to%20higher%20fidelity%20aircraft%20models%20and%20stochastic%20obstacle%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Take-off%2520under%2520Fuzzy%2520Clearances%26entry.906535625%3DHugo%2520Henry%2520and%2520Arthur%2520Tsai%2520and%2520Kelly%2520Cohen%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520hybrid%2520obstacle%2520avoidance%2520architecture%2520that%2520integrates%2520Optimal%2520Control%2520under%2520clearance%2520with%2520a%2520Fuzzy%2520Rule%2520Based%2520System%2520%2528FRBS%2529%2520to%2520enable%2520adaptive%2520constraint%2520handling%2520for%2520unmanned%2520aircraft.%2520Motivated%2520by%2520the%2520limitations%2520of%2520classical%2520optimal%2520control%2520under%2520uncertainty%2520and%2520the%2520need%2520for%2520interpretable%2520decision%2520making%2520in%2520safety%2520critical%2520aviation%2520systems%252C%2520we%2520design%2520a%2520three%2520stage%2520Takagi%2520Sugeno%2520Kang%2520fuzzy%2520layer%2520that%2520modulates%2520constraint%2520radii%252C%2520urgency%2520levels%252C%2520and%2520activation%2520decisions%2520based%2520on%2520regulatory%2520separation%2520minima%2520and%2520airworthiness%2520guidelines%2520from%2520FAA%2520and%2520EASA.%2520These%2520fuzzy-derived%2520clearances%2520are%2520then%2520incorporated%2520as%2520soft%2520constraints%2520into%2520an%2520optimal%2520control%2520problem%2520solved%2520using%2520the%2520FALCON%2520toolbox%2520and%2520IPOPT.%2520The%2520framework%2520aims%2520to%2520reduce%2520unnecessary%2520recomputations%2520by%2520selectively%2520activating%2520obstacle%2520avoidance%2520updates%2520while%2520maintaining%2520compliance%2520with%2520aviation%2520procedures.%2520A%2520proof%2520of%2520concept%2520implementation%2520using%2520a%2520simplified%2520aircraft%2520model%2520demonstrates%2520that%2520the%2520approach%2520can%2520generate%2520optimal%2520trajectories%2520with%2520computation%2520times%2520of%25202%252C3%2520seconds%2520per%2520iteration%2520in%2520a%2520single%2520threaded%2520MATLAB%2520environment%252C%2520suggesting%2520feasibility%2520for%2520near%2520real%2520time%2520applications.%2520However%252C%2520our%2520experiments%2520revealed%2520a%2520critical%2520software%2520incompatibility%2520in%2520the%2520latest%2520versions%2520of%2520FALCON%2520and%2520IPOPT%252C%2520in%2520which%2520the%2520Lagrangian%2520penalty%2520term%2520remained%2520identically%2520zero%252C%2520preventing%2520proper%2520constraint%2520enforcement.%2520This%2520behavior%2520was%2520consistent%2520across%2520scenarios%2520and%2520indicates%2520a%2520solver%2520toolbox%2520regression%2520rather%2520than%2520a%2520modeling%2520flaw.%2520Future%2520work%2520includes%2520validating%2520this%2520effect%2520by%2520reverting%2520to%2520earlier%2520software%2520versions%252C%2520optimizing%2520the%2520fuzzy%2520membership%2520functions%2520using%2520evolutionary%2520methods%252C%2520and%2520extending%2520the%2520system%2520to%2520higher%2520fidelity%2520aircraft%2520models%2520and%2520stochastic%2520obstacle%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Take-off%20under%20Fuzzy%20Clearances&entry.906535625=Hugo%20Henry%20and%20Arthur%20Tsai%20and%20Kelly%20Cohen&entry.1292438233=This%20paper%20presents%20a%20hybrid%20obstacle%20avoidance%20architecture%20that%20integrates%20Optimal%20Control%20under%20clearance%20with%20a%20Fuzzy%20Rule%20Based%20System%20%28FRBS%29%20to%20enable%20adaptive%20constraint%20handling%20for%20unmanned%20aircraft.%20Motivated%20by%20the%20limitations%20of%20classical%20optimal%20control%20under%20uncertainty%20and%20the%20need%20for%20interpretable%20decision%20making%20in%20safety%20critical%20aviation%20systems%2C%20we%20design%20a%20three%20stage%20Takagi%20Sugeno%20Kang%20fuzzy%20layer%20that%20modulates%20constraint%20radii%2C%20urgency%20levels%2C%20and%20activation%20decisions%20based%20on%20regulatory%20separation%20minima%20and%20airworthiness%20guidelines%20from%20FAA%20and%20EASA.%20These%20fuzzy-derived%20clearances%20are%20then%20incorporated%20as%20soft%20constraints%20into%20an%20optimal%20control%20problem%20solved%20using%20the%20FALCON%20toolbox%20and%20IPOPT.%20The%20framework%20aims%20to%20reduce%20unnecessary%20recomputations%20by%20selectively%20activating%20obstacle%20avoidance%20updates%20while%20maintaining%20compliance%20with%20aviation%20procedures.%20A%20proof%20of%20concept%20implementation%20using%20a%20simplified%20aircraft%20model%20demonstrates%20that%20the%20approach%20can%20generate%20optimal%20trajectories%20with%20computation%20times%20of%202%2C3%20seconds%20per%20iteration%20in%20a%20single%20threaded%20MATLAB%20environment%2C%20suggesting%20feasibility%20for%20near%20real%20time%20applications.%20However%2C%20our%20experiments%20revealed%20a%20critical%20software%20incompatibility%20in%20the%20latest%20versions%20of%20FALCON%20and%20IPOPT%2C%20in%20which%20the%20Lagrangian%20penalty%20term%20remained%20identically%20zero%2C%20preventing%20proper%20constraint%20enforcement.%20This%20behavior%20was%20consistent%20across%20scenarios%20and%20indicates%20a%20solver%20toolbox%20regression%20rather%20than%20a%20modeling%20flaw.%20Future%20work%20includes%20validating%20this%20effect%20by%20reverting%20to%20earlier%20software%20versions%2C%20optimizing%20the%20fuzzy%20membership%20functions%20using%20evolutionary%20methods%2C%20and%20extending%20the%20system%20to%20higher%20fidelity%20aircraft%20models%20and%20stochastic%20obstacle%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2602.13166v1&entry.124074799=Read"},
{"title": "Left-right asymmetry in predicting brain activity from LLMs' representations emerges with their formal linguistic competence", "author": "Laurent Bonnasse-Gahot and Christophe Pallier", "abstract": "When humans and large language models (LLMs) process the same text, activations in the LLMs correlate with brain activity measured, e.g., with functional magnetic resonance imaging (fMRI). Moreover, it has been shown that, as the training of an LLM progresses, the performance in predicting brain activity from its internal activations improves more in the left hemisphere than in the right one. The aim of the present work is to understand which kind of competence acquired by the LLMs underlies the emergence of this left-right asymmetry. Using the OLMo-2 7B language model at various training checkpoints and fMRI data from English participants, we compare the evolution of the left-right asymmetry in brain scores alongside performance on several benchmarks. We observe that the asymmetry co-emerges with the formal linguistic abilities of the LLM. These abilities are demonstrated in two ways: by the model's capacity to assign a higher probability to an acceptable sentence than to a grammatically unacceptable one within a minimal contrasting pair, or its ability to produce well-formed text. On the opposite, the left-right asymmetry does not correlate with the performance on arithmetic or Dyck language tasks; nor with text-based tasks involving world knowledge and reasoning. We generalize these results to another family of LLMs (Pythia) and another language, namely French. Our observations indicate that the left-right asymmetry in brain predictivity matches the progress in formal linguistic competence (knowledge of linguistic patterns).", "link": "http://arxiv.org/abs/2602.12811v1", "date": "2026-02-13", "relevancy": 2.371, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4815}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Left-right%20asymmetry%20in%20predicting%20brain%20activity%20from%20LLMs%27%20representations%20emerges%20with%20their%20formal%20linguistic%20competence&body=Title%3A%20Left-right%20asymmetry%20in%20predicting%20brain%20activity%20from%20LLMs%27%20representations%20emerges%20with%20their%20formal%20linguistic%20competence%0AAuthor%3A%20Laurent%20Bonnasse-Gahot%20and%20Christophe%20Pallier%0AAbstract%3A%20When%20humans%20and%20large%20language%20models%20%28LLMs%29%20process%20the%20same%20text%2C%20activations%20in%20the%20LLMs%20correlate%20with%20brain%20activity%20measured%2C%20e.g.%2C%20with%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29.%20Moreover%2C%20it%20has%20been%20shown%20that%2C%20as%20the%20training%20of%20an%20LLM%20progresses%2C%20the%20performance%20in%20predicting%20brain%20activity%20from%20its%20internal%20activations%20improves%20more%20in%20the%20left%20hemisphere%20than%20in%20the%20right%20one.%20The%20aim%20of%20the%20present%20work%20is%20to%20understand%20which%20kind%20of%20competence%20acquired%20by%20the%20LLMs%20underlies%20the%20emergence%20of%20this%20left-right%20asymmetry.%20Using%20the%20OLMo-2%207B%20language%20model%20at%20various%20training%20checkpoints%20and%20fMRI%20data%20from%20English%20participants%2C%20we%20compare%20the%20evolution%20of%20the%20left-right%20asymmetry%20in%20brain%20scores%20alongside%20performance%20on%20several%20benchmarks.%20We%20observe%20that%20the%20asymmetry%20co-emerges%20with%20the%20formal%20linguistic%20abilities%20of%20the%20LLM.%20These%20abilities%20are%20demonstrated%20in%20two%20ways%3A%20by%20the%20model%27s%20capacity%20to%20assign%20a%20higher%20probability%20to%20an%20acceptable%20sentence%20than%20to%20a%20grammatically%20unacceptable%20one%20within%20a%20minimal%20contrasting%20pair%2C%20or%20its%20ability%20to%20produce%20well-formed%20text.%20On%20the%20opposite%2C%20the%20left-right%20asymmetry%20does%20not%20correlate%20with%20the%20performance%20on%20arithmetic%20or%20Dyck%20language%20tasks%3B%20nor%20with%20text-based%20tasks%20involving%20world%20knowledge%20and%20reasoning.%20We%20generalize%20these%20results%20to%20another%20family%20of%20LLMs%20%28Pythia%29%20and%20another%20language%2C%20namely%20French.%20Our%20observations%20indicate%20that%20the%20left-right%20asymmetry%20in%20brain%20predictivity%20matches%20the%20progress%20in%20formal%20linguistic%20competence%20%28knowledge%20of%20linguistic%20patterns%29.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeft-right%2520asymmetry%2520in%2520predicting%2520brain%2520activity%2520from%2520LLMs%2527%2520representations%2520emerges%2520with%2520their%2520formal%2520linguistic%2520competence%26entry.906535625%3DLaurent%2520Bonnasse-Gahot%2520and%2520Christophe%2520Pallier%26entry.1292438233%3DWhen%2520humans%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%2520process%2520the%2520same%2520text%252C%2520activations%2520in%2520the%2520LLMs%2520correlate%2520with%2520brain%2520activity%2520measured%252C%2520e.g.%252C%2520with%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529.%2520Moreover%252C%2520it%2520has%2520been%2520shown%2520that%252C%2520as%2520the%2520training%2520of%2520an%2520LLM%2520progresses%252C%2520the%2520performance%2520in%2520predicting%2520brain%2520activity%2520from%2520its%2520internal%2520activations%2520improves%2520more%2520in%2520the%2520left%2520hemisphere%2520than%2520in%2520the%2520right%2520one.%2520The%2520aim%2520of%2520the%2520present%2520work%2520is%2520to%2520understand%2520which%2520kind%2520of%2520competence%2520acquired%2520by%2520the%2520LLMs%2520underlies%2520the%2520emergence%2520of%2520this%2520left-right%2520asymmetry.%2520Using%2520the%2520OLMo-2%25207B%2520language%2520model%2520at%2520various%2520training%2520checkpoints%2520and%2520fMRI%2520data%2520from%2520English%2520participants%252C%2520we%2520compare%2520the%2520evolution%2520of%2520the%2520left-right%2520asymmetry%2520in%2520brain%2520scores%2520alongside%2520performance%2520on%2520several%2520benchmarks.%2520We%2520observe%2520that%2520the%2520asymmetry%2520co-emerges%2520with%2520the%2520formal%2520linguistic%2520abilities%2520of%2520the%2520LLM.%2520These%2520abilities%2520are%2520demonstrated%2520in%2520two%2520ways%253A%2520by%2520the%2520model%2527s%2520capacity%2520to%2520assign%2520a%2520higher%2520probability%2520to%2520an%2520acceptable%2520sentence%2520than%2520to%2520a%2520grammatically%2520unacceptable%2520one%2520within%2520a%2520minimal%2520contrasting%2520pair%252C%2520or%2520its%2520ability%2520to%2520produce%2520well-formed%2520text.%2520On%2520the%2520opposite%252C%2520the%2520left-right%2520asymmetry%2520does%2520not%2520correlate%2520with%2520the%2520performance%2520on%2520arithmetic%2520or%2520Dyck%2520language%2520tasks%253B%2520nor%2520with%2520text-based%2520tasks%2520involving%2520world%2520knowledge%2520and%2520reasoning.%2520We%2520generalize%2520these%2520results%2520to%2520another%2520family%2520of%2520LLMs%2520%2528Pythia%2529%2520and%2520another%2520language%252C%2520namely%2520French.%2520Our%2520observations%2520indicate%2520that%2520the%2520left-right%2520asymmetry%2520in%2520brain%2520predictivity%2520matches%2520the%2520progress%2520in%2520formal%2520linguistic%2520competence%2520%2528knowledge%2520of%2520linguistic%2520patterns%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Left-right%20asymmetry%20in%20predicting%20brain%20activity%20from%20LLMs%27%20representations%20emerges%20with%20their%20formal%20linguistic%20competence&entry.906535625=Laurent%20Bonnasse-Gahot%20and%20Christophe%20Pallier&entry.1292438233=When%20humans%20and%20large%20language%20models%20%28LLMs%29%20process%20the%20same%20text%2C%20activations%20in%20the%20LLMs%20correlate%20with%20brain%20activity%20measured%2C%20e.g.%2C%20with%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29.%20Moreover%2C%20it%20has%20been%20shown%20that%2C%20as%20the%20training%20of%20an%20LLM%20progresses%2C%20the%20performance%20in%20predicting%20brain%20activity%20from%20its%20internal%20activations%20improves%20more%20in%20the%20left%20hemisphere%20than%20in%20the%20right%20one.%20The%20aim%20of%20the%20present%20work%20is%20to%20understand%20which%20kind%20of%20competence%20acquired%20by%20the%20LLMs%20underlies%20the%20emergence%20of%20this%20left-right%20asymmetry.%20Using%20the%20OLMo-2%207B%20language%20model%20at%20various%20training%20checkpoints%20and%20fMRI%20data%20from%20English%20participants%2C%20we%20compare%20the%20evolution%20of%20the%20left-right%20asymmetry%20in%20brain%20scores%20alongside%20performance%20on%20several%20benchmarks.%20We%20observe%20that%20the%20asymmetry%20co-emerges%20with%20the%20formal%20linguistic%20abilities%20of%20the%20LLM.%20These%20abilities%20are%20demonstrated%20in%20two%20ways%3A%20by%20the%20model%27s%20capacity%20to%20assign%20a%20higher%20probability%20to%20an%20acceptable%20sentence%20than%20to%20a%20grammatically%20unacceptable%20one%20within%20a%20minimal%20contrasting%20pair%2C%20or%20its%20ability%20to%20produce%20well-formed%20text.%20On%20the%20opposite%2C%20the%20left-right%20asymmetry%20does%20not%20correlate%20with%20the%20performance%20on%20arithmetic%20or%20Dyck%20language%20tasks%3B%20nor%20with%20text-based%20tasks%20involving%20world%20knowledge%20and%20reasoning.%20We%20generalize%20these%20results%20to%20another%20family%20of%20LLMs%20%28Pythia%29%20and%20another%20language%2C%20namely%20French.%20Our%20observations%20indicate%20that%20the%20left-right%20asymmetry%20in%20brain%20predictivity%20matches%20the%20progress%20in%20formal%20linguistic%20competence%20%28knowledge%20of%20linguistic%20patterns%29.&entry.1838667208=http%3A//arxiv.org/abs/2602.12811v1&entry.124074799=Read"},
{"title": "R3DPA: Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation", "author": "Nicolas Sereyjol-Garros and Ellington Kirby and Victor Besnier and Nermin Samet", "abstract": "LiDAR scene synthesis is an emerging solution to scarcity in 3D data for robotic tasks such as autonomous driving. Recent approaches employ diffusion or flow matching models to generate realistic scenes, but 3D data remains limited compared to RGB datasets with millions of samples. We introduce R3DPA, the first LiDAR scene generation method to unlock image-pretrained priors for LiDAR point clouds, and leverage self-supervised 3D representations for state-of-the-art results. Specifically, we (i) align intermediate features of our generative model with self-supervised 3D features, which substantially improves generation quality; (ii) transfer knowledge from large-scale image-pretrained generative models to LiDAR generation, mitigating limited LiDAR datasets; and (iii) enable point cloud control at inference for object inpainting and scene mixing with solely an unconditional model. On the KITTI-360 benchmark R3DPA achieves state of the art performance. Code and pretrained models are available at https://github.com/valeoai/R3DPA.", "link": "http://arxiv.org/abs/2601.07692v2", "date": "2026-02-13", "relevancy": 2.3687, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5976}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5915}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R3DPA%3A%20Leveraging%203D%20Representation%20Alignment%20and%20RGB%20Pretrained%20Priors%20for%20LiDAR%20Scene%20Generation&body=Title%3A%20R3DPA%3A%20Leveraging%203D%20Representation%20Alignment%20and%20RGB%20Pretrained%20Priors%20for%20LiDAR%20Scene%20Generation%0AAuthor%3A%20Nicolas%20Sereyjol-Garros%20and%20Ellington%20Kirby%20and%20Victor%20Besnier%20and%20Nermin%20Samet%0AAbstract%3A%20LiDAR%20scene%20synthesis%20is%20an%20emerging%20solution%20to%20scarcity%20in%203D%20data%20for%20robotic%20tasks%20such%20as%20autonomous%20driving.%20Recent%20approaches%20employ%20diffusion%20or%20flow%20matching%20models%20to%20generate%20realistic%20scenes%2C%20but%203D%20data%20remains%20limited%20compared%20to%20RGB%20datasets%20with%20millions%20of%20samples.%20We%20introduce%20R3DPA%2C%20the%20first%20LiDAR%20scene%20generation%20method%20to%20unlock%20image-pretrained%20priors%20for%20LiDAR%20point%20clouds%2C%20and%20leverage%20self-supervised%203D%20representations%20for%20state-of-the-art%20results.%20Specifically%2C%20we%20%28i%29%20align%20intermediate%20features%20of%20our%20generative%20model%20with%20self-supervised%203D%20features%2C%20which%20substantially%20improves%20generation%20quality%3B%20%28ii%29%20transfer%20knowledge%20from%20large-scale%20image-pretrained%20generative%20models%20to%20LiDAR%20generation%2C%20mitigating%20limited%20LiDAR%20datasets%3B%20and%20%28iii%29%20enable%20point%20cloud%20control%20at%20inference%20for%20object%20inpainting%20and%20scene%20mixing%20with%20solely%20an%20unconditional%20model.%20On%20the%20KITTI-360%20benchmark%20R3DPA%20achieves%20state%20of%20the%20art%20performance.%20Code%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/valeoai/R3DPA.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07692v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR3DPA%253A%2520Leveraging%25203D%2520Representation%2520Alignment%2520and%2520RGB%2520Pretrained%2520Priors%2520for%2520LiDAR%2520Scene%2520Generation%26entry.906535625%3DNicolas%2520Sereyjol-Garros%2520and%2520Ellington%2520Kirby%2520and%2520Victor%2520Besnier%2520and%2520Nermin%2520Samet%26entry.1292438233%3DLiDAR%2520scene%2520synthesis%2520is%2520an%2520emerging%2520solution%2520to%2520scarcity%2520in%25203D%2520data%2520for%2520robotic%2520tasks%2520such%2520as%2520autonomous%2520driving.%2520Recent%2520approaches%2520employ%2520diffusion%2520or%2520flow%2520matching%2520models%2520to%2520generate%2520realistic%2520scenes%252C%2520but%25203D%2520data%2520remains%2520limited%2520compared%2520to%2520RGB%2520datasets%2520with%2520millions%2520of%2520samples.%2520We%2520introduce%2520R3DPA%252C%2520the%2520first%2520LiDAR%2520scene%2520generation%2520method%2520to%2520unlock%2520image-pretrained%2520priors%2520for%2520LiDAR%2520point%2520clouds%252C%2520and%2520leverage%2520self-supervised%25203D%2520representations%2520for%2520state-of-the-art%2520results.%2520Specifically%252C%2520we%2520%2528i%2529%2520align%2520intermediate%2520features%2520of%2520our%2520generative%2520model%2520with%2520self-supervised%25203D%2520features%252C%2520which%2520substantially%2520improves%2520generation%2520quality%253B%2520%2528ii%2529%2520transfer%2520knowledge%2520from%2520large-scale%2520image-pretrained%2520generative%2520models%2520to%2520LiDAR%2520generation%252C%2520mitigating%2520limited%2520LiDAR%2520datasets%253B%2520and%2520%2528iii%2529%2520enable%2520point%2520cloud%2520control%2520at%2520inference%2520for%2520object%2520inpainting%2520and%2520scene%2520mixing%2520with%2520solely%2520an%2520unconditional%2520model.%2520On%2520the%2520KITTI-360%2520benchmark%2520R3DPA%2520achieves%2520state%2520of%2520the%2520art%2520performance.%2520Code%2520and%2520pretrained%2520models%2520are%2520available%2520at%2520https%253A//github.com/valeoai/R3DPA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07692v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R3DPA%3A%20Leveraging%203D%20Representation%20Alignment%20and%20RGB%20Pretrained%20Priors%20for%20LiDAR%20Scene%20Generation&entry.906535625=Nicolas%20Sereyjol-Garros%20and%20Ellington%20Kirby%20and%20Victor%20Besnier%20and%20Nermin%20Samet&entry.1292438233=LiDAR%20scene%20synthesis%20is%20an%20emerging%20solution%20to%20scarcity%20in%203D%20data%20for%20robotic%20tasks%20such%20as%20autonomous%20driving.%20Recent%20approaches%20employ%20diffusion%20or%20flow%20matching%20models%20to%20generate%20realistic%20scenes%2C%20but%203D%20data%20remains%20limited%20compared%20to%20RGB%20datasets%20with%20millions%20of%20samples.%20We%20introduce%20R3DPA%2C%20the%20first%20LiDAR%20scene%20generation%20method%20to%20unlock%20image-pretrained%20priors%20for%20LiDAR%20point%20clouds%2C%20and%20leverage%20self-supervised%203D%20representations%20for%20state-of-the-art%20results.%20Specifically%2C%20we%20%28i%29%20align%20intermediate%20features%20of%20our%20generative%20model%20with%20self-supervised%203D%20features%2C%20which%20substantially%20improves%20generation%20quality%3B%20%28ii%29%20transfer%20knowledge%20from%20large-scale%20image-pretrained%20generative%20models%20to%20LiDAR%20generation%2C%20mitigating%20limited%20LiDAR%20datasets%3B%20and%20%28iii%29%20enable%20point%20cloud%20control%20at%20inference%20for%20object%20inpainting%20and%20scene%20mixing%20with%20solely%20an%20unconditional%20model.%20On%20the%20KITTI-360%20benchmark%20R3DPA%20achieves%20state%20of%20the%20art%20performance.%20Code%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/valeoai/R3DPA.&entry.1838667208=http%3A//arxiv.org/abs/2601.07692v2&entry.124074799=Read"},
{"title": "Hierarchical Retrieval at Scale: Bridging Transparency and Efficiency", "author": "Shubham Gupta and Zichao Li and Tianyi Chen and Cem Subakan and Siva Reddy and Perouz Taslakian and Valentina Zantedeschi", "abstract": "Information retrieval is a core component of many intelligent systems as it enables conditioning of outputs on new and large-scale datasets. While effective, the standard practice of encoding data into high-dimensional representations for similarity search entails large memory and compute footprints, and also makes it hard to inspect the inner workings of the system. Hierarchical retrieval methods offer an interpretable alternative by organizing data at multiple granular levels, yet do not match the efficiency and performance of flat retrieval approaches. In this paper, we propose Retreever, a tree-based method that makes hierarchical retrieval viable at scale by directly optimizing its structure for retrieval performance while naturally providing transparency through meaningful semantic groupings. Our method offers the flexibility to balance cost and utility by indexing data using representations from any tree level. We show that Retreever delivers strong coarse (intermediate levels) and fine representations (terminal level), while achieving the highest retrieval accuracy at the lowest latency among hierarchical methods. These results demonstrate that this family of techniques is viable in practical applications.", "link": "http://arxiv.org/abs/2502.07971v2", "date": "2026-02-13", "relevancy": 2.345, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4715}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Retrieval%20at%20Scale%3A%20Bridging%20Transparency%20and%20Efficiency&body=Title%3A%20Hierarchical%20Retrieval%20at%20Scale%3A%20Bridging%20Transparency%20and%20Efficiency%0AAuthor%3A%20Shubham%20Gupta%20and%20Zichao%20Li%20and%20Tianyi%20Chen%20and%20Cem%20Subakan%20and%20Siva%20Reddy%20and%20Perouz%20Taslakian%20and%20Valentina%20Zantedeschi%0AAbstract%3A%20Information%20retrieval%20is%20a%20core%20component%20of%20many%20intelligent%20systems%20as%20it%20enables%20conditioning%20of%20outputs%20on%20new%20and%20large-scale%20datasets.%20While%20effective%2C%20the%20standard%20practice%20of%20encoding%20data%20into%20high-dimensional%20representations%20for%20similarity%20search%20entails%20large%20memory%20and%20compute%20footprints%2C%20and%20also%20makes%20it%20hard%20to%20inspect%20the%20inner%20workings%20of%20the%20system.%20Hierarchical%20retrieval%20methods%20offer%20an%20interpretable%20alternative%20by%20organizing%20data%20at%20multiple%20granular%20levels%2C%20yet%20do%20not%20match%20the%20efficiency%20and%20performance%20of%20flat%20retrieval%20approaches.%20In%20this%20paper%2C%20we%20propose%20Retreever%2C%20a%20tree-based%20method%20that%20makes%20hierarchical%20retrieval%20viable%20at%20scale%20by%20directly%20optimizing%20its%20structure%20for%20retrieval%20performance%20while%20naturally%20providing%20transparency%20through%20meaningful%20semantic%20groupings.%20Our%20method%20offers%20the%20flexibility%20to%20balance%20cost%20and%20utility%20by%20indexing%20data%20using%20representations%20from%20any%20tree%20level.%20We%20show%20that%20Retreever%20delivers%20strong%20coarse%20%28intermediate%20levels%29%20and%20fine%20representations%20%28terminal%20level%29%2C%20while%20achieving%20the%20highest%20retrieval%20accuracy%20at%20the%20lowest%20latency%20among%20hierarchical%20methods.%20These%20results%20demonstrate%20that%20this%20family%20of%20techniques%20is%20viable%20in%20practical%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2502.07971v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Retrieval%2520at%2520Scale%253A%2520Bridging%2520Transparency%2520and%2520Efficiency%26entry.906535625%3DShubham%2520Gupta%2520and%2520Zichao%2520Li%2520and%2520Tianyi%2520Chen%2520and%2520Cem%2520Subakan%2520and%2520Siva%2520Reddy%2520and%2520Perouz%2520Taslakian%2520and%2520Valentina%2520Zantedeschi%26entry.1292438233%3DInformation%2520retrieval%2520is%2520a%2520core%2520component%2520of%2520many%2520intelligent%2520systems%2520as%2520it%2520enables%2520conditioning%2520of%2520outputs%2520on%2520new%2520and%2520large-scale%2520datasets.%2520While%2520effective%252C%2520the%2520standard%2520practice%2520of%2520encoding%2520data%2520into%2520high-dimensional%2520representations%2520for%2520similarity%2520search%2520entails%2520large%2520memory%2520and%2520compute%2520footprints%252C%2520and%2520also%2520makes%2520it%2520hard%2520to%2520inspect%2520the%2520inner%2520workings%2520of%2520the%2520system.%2520Hierarchical%2520retrieval%2520methods%2520offer%2520an%2520interpretable%2520alternative%2520by%2520organizing%2520data%2520at%2520multiple%2520granular%2520levels%252C%2520yet%2520do%2520not%2520match%2520the%2520efficiency%2520and%2520performance%2520of%2520flat%2520retrieval%2520approaches.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Retreever%252C%2520a%2520tree-based%2520method%2520that%2520makes%2520hierarchical%2520retrieval%2520viable%2520at%2520scale%2520by%2520directly%2520optimizing%2520its%2520structure%2520for%2520retrieval%2520performance%2520while%2520naturally%2520providing%2520transparency%2520through%2520meaningful%2520semantic%2520groupings.%2520Our%2520method%2520offers%2520the%2520flexibility%2520to%2520balance%2520cost%2520and%2520utility%2520by%2520indexing%2520data%2520using%2520representations%2520from%2520any%2520tree%2520level.%2520We%2520show%2520that%2520Retreever%2520delivers%2520strong%2520coarse%2520%2528intermediate%2520levels%2529%2520and%2520fine%2520representations%2520%2528terminal%2520level%2529%252C%2520while%2520achieving%2520the%2520highest%2520retrieval%2520accuracy%2520at%2520the%2520lowest%2520latency%2520among%2520hierarchical%2520methods.%2520These%2520results%2520demonstrate%2520that%2520this%2520family%2520of%2520techniques%2520is%2520viable%2520in%2520practical%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07971v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Retrieval%20at%20Scale%3A%20Bridging%20Transparency%20and%20Efficiency&entry.906535625=Shubham%20Gupta%20and%20Zichao%20Li%20and%20Tianyi%20Chen%20and%20Cem%20Subakan%20and%20Siva%20Reddy%20and%20Perouz%20Taslakian%20and%20Valentina%20Zantedeschi&entry.1292438233=Information%20retrieval%20is%20a%20core%20component%20of%20many%20intelligent%20systems%20as%20it%20enables%20conditioning%20of%20outputs%20on%20new%20and%20large-scale%20datasets.%20While%20effective%2C%20the%20standard%20practice%20of%20encoding%20data%20into%20high-dimensional%20representations%20for%20similarity%20search%20entails%20large%20memory%20and%20compute%20footprints%2C%20and%20also%20makes%20it%20hard%20to%20inspect%20the%20inner%20workings%20of%20the%20system.%20Hierarchical%20retrieval%20methods%20offer%20an%20interpretable%20alternative%20by%20organizing%20data%20at%20multiple%20granular%20levels%2C%20yet%20do%20not%20match%20the%20efficiency%20and%20performance%20of%20flat%20retrieval%20approaches.%20In%20this%20paper%2C%20we%20propose%20Retreever%2C%20a%20tree-based%20method%20that%20makes%20hierarchical%20retrieval%20viable%20at%20scale%20by%20directly%20optimizing%20its%20structure%20for%20retrieval%20performance%20while%20naturally%20providing%20transparency%20through%20meaningful%20semantic%20groupings.%20Our%20method%20offers%20the%20flexibility%20to%20balance%20cost%20and%20utility%20by%20indexing%20data%20using%20representations%20from%20any%20tree%20level.%20We%20show%20that%20Retreever%20delivers%20strong%20coarse%20%28intermediate%20levels%29%20and%20fine%20representations%20%28terminal%20level%29%2C%20while%20achieving%20the%20highest%20retrieval%20accuracy%20at%20the%20lowest%20latency%20among%20hierarchical%20methods.%20These%20results%20demonstrate%20that%20this%20family%20of%20techniques%20is%20viable%20in%20practical%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2502.07971v2&entry.124074799=Read"},
{"title": "DRL-Based Beam Positioning for LEO Satellite Constellations with Weighted Least Squares", "author": "Po-Heng Chou and Chiapin Wang and Kuan-Hao Chen and Wei-Chen Hsiao", "abstract": "In this paper, we propose a reinforcement learning based beam weighting framework that couples a policy network with an augmented weighted least squares (WLS) estimator for accurate and low-complexity positioning in multi-beam LEO constellations. Unlike conventional geometry or CSI-dependent approaches, the policy learns directly from uplink pilot responses and geometry features, enabling robust localization without explicit CSI estimation. An augmented WLS jointly estimates position and receiver clock bias, improving numerical stability under dynamic beam geometry. Across representative scenarios, the proposed method reduces the mean positioning error by 99.3% compared with the geometry-based baseline, achieving 0.395 m RMSE with near real-time inference.", "link": "http://arxiv.org/abs/2511.08852v2", "date": "2026-02-13", "relevancy": 2.3378, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4899}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4583}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRL-Based%20Beam%20Positioning%20for%20LEO%20Satellite%20Constellations%20with%20Weighted%20Least%20Squares&body=Title%3A%20DRL-Based%20Beam%20Positioning%20for%20LEO%20Satellite%20Constellations%20with%20Weighted%20Least%20Squares%0AAuthor%3A%20Po-Heng%20Chou%20and%20Chiapin%20Wang%20and%20Kuan-Hao%20Chen%20and%20Wei-Chen%20Hsiao%0AAbstract%3A%20In%20this%20paper%2C%20we%20propose%20a%20reinforcement%20learning%20based%20beam%20weighting%20framework%20that%20couples%20a%20policy%20network%20with%20an%20augmented%20weighted%20least%20squares%20%28WLS%29%20estimator%20for%20accurate%20and%20low-complexity%20positioning%20in%20multi-beam%20LEO%20constellations.%20Unlike%20conventional%20geometry%20or%20CSI-dependent%20approaches%2C%20the%20policy%20learns%20directly%20from%20uplink%20pilot%20responses%20and%20geometry%20features%2C%20enabling%20robust%20localization%20without%20explicit%20CSI%20estimation.%20An%20augmented%20WLS%20jointly%20estimates%20position%20and%20receiver%20clock%20bias%2C%20improving%20numerical%20stability%20under%20dynamic%20beam%20geometry.%20Across%20representative%20scenarios%2C%20the%20proposed%20method%20reduces%20the%20mean%20positioning%20error%20by%2099.3%25%20compared%20with%20the%20geometry-based%20baseline%2C%20achieving%200.395%20m%20RMSE%20with%20near%20real-time%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08852v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRL-Based%2520Beam%2520Positioning%2520for%2520LEO%2520Satellite%2520Constellations%2520with%2520Weighted%2520Least%2520Squares%26entry.906535625%3DPo-Heng%2520Chou%2520and%2520Chiapin%2520Wang%2520and%2520Kuan-Hao%2520Chen%2520and%2520Wei-Chen%2520Hsiao%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520reinforcement%2520learning%2520based%2520beam%2520weighting%2520framework%2520that%2520couples%2520a%2520policy%2520network%2520with%2520an%2520augmented%2520weighted%2520least%2520squares%2520%2528WLS%2529%2520estimator%2520for%2520accurate%2520and%2520low-complexity%2520positioning%2520in%2520multi-beam%2520LEO%2520constellations.%2520Unlike%2520conventional%2520geometry%2520or%2520CSI-dependent%2520approaches%252C%2520the%2520policy%2520learns%2520directly%2520from%2520uplink%2520pilot%2520responses%2520and%2520geometry%2520features%252C%2520enabling%2520robust%2520localization%2520without%2520explicit%2520CSI%2520estimation.%2520An%2520augmented%2520WLS%2520jointly%2520estimates%2520position%2520and%2520receiver%2520clock%2520bias%252C%2520improving%2520numerical%2520stability%2520under%2520dynamic%2520beam%2520geometry.%2520Across%2520representative%2520scenarios%252C%2520the%2520proposed%2520method%2520reduces%2520the%2520mean%2520positioning%2520error%2520by%252099.3%2525%2520compared%2520with%2520the%2520geometry-based%2520baseline%252C%2520achieving%25200.395%2520m%2520RMSE%2520with%2520near%2520real-time%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08852v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRL-Based%20Beam%20Positioning%20for%20LEO%20Satellite%20Constellations%20with%20Weighted%20Least%20Squares&entry.906535625=Po-Heng%20Chou%20and%20Chiapin%20Wang%20and%20Kuan-Hao%20Chen%20and%20Wei-Chen%20Hsiao&entry.1292438233=In%20this%20paper%2C%20we%20propose%20a%20reinforcement%20learning%20based%20beam%20weighting%20framework%20that%20couples%20a%20policy%20network%20with%20an%20augmented%20weighted%20least%20squares%20%28WLS%29%20estimator%20for%20accurate%20and%20low-complexity%20positioning%20in%20multi-beam%20LEO%20constellations.%20Unlike%20conventional%20geometry%20or%20CSI-dependent%20approaches%2C%20the%20policy%20learns%20directly%20from%20uplink%20pilot%20responses%20and%20geometry%20features%2C%20enabling%20robust%20localization%20without%20explicit%20CSI%20estimation.%20An%20augmented%20WLS%20jointly%20estimates%20position%20and%20receiver%20clock%20bias%2C%20improving%20numerical%20stability%20under%20dynamic%20beam%20geometry.%20Across%20representative%20scenarios%2C%20the%20proposed%20method%20reduces%20the%20mean%20positioning%20error%20by%2099.3%25%20compared%20with%20the%20geometry-based%20baseline%2C%20achieving%200.395%20m%20RMSE%20with%20near%20real-time%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2511.08852v2&entry.124074799=Read"},
{"title": "PA-MPPI: Perception-Aware Model Predictive Path Integral Control for Quadrotor Navigation in Unknown Environments", "author": "Yifan Zhai and Rudolf Reiter and Davide Scaramuzza", "abstract": "Quadrotor navigation in unknown environments is critical for practical missions such as search-and-rescue. Solving this problem requires addressing three key challenges: path planning in non-convex free space due to obstacles, satisfying quadrotor-specific dynamics and objectives, and exploring unknown regions to expand the map. Recently, the Model Predictive Path Integral (MPPI) method has emerged as a promising solution to the first two challenges. By leveraging sampling-based optimization, it can effectively handle non-convex free space while directly optimizing over the full quadrotor dynamics, enabling the inclusion of quadrotor-specific costs such as energy consumption. However, MPPI has been limited to tracking control that optimizes trajectories only within a small neighborhood around a reference trajectory, as it lacks the ability to explore unknown regions and plan alternative paths when blocked by large obstacles. To address this limitation, we introduce Perception-Aware MPPI (PA-MPPI). In this approach, perception-awareness is characterized by planning and adapting the trajectory online based on perception objectives. Specifically, when the goal is occluded, PA-MPPI incorporates a perception cost that biases trajectories toward those that can observe unknown regions. This expands the mapped traversable space and increases the likelihood of finding alternative paths to the goal. Through hardware experiments, we demonstrate that PA-MPPI, running at 50 Hz, performs on par with the state-of-the-art quadrotor navigation planner for unknown environments in challenging test scenarios. Furthermore, we show that PA-MPPI can serve as a safe and robust action policy for navigation foundation models, which often provide goal poses that are not directly reachable.", "link": "http://arxiv.org/abs/2509.14978v3", "date": "2026-02-13", "relevancy": 2.3331, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5928}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5908}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PA-MPPI%3A%20Perception-Aware%20Model%20Predictive%20Path%20Integral%20Control%20for%20Quadrotor%20Navigation%20in%20Unknown%20Environments&body=Title%3A%20PA-MPPI%3A%20Perception-Aware%20Model%20Predictive%20Path%20Integral%20Control%20for%20Quadrotor%20Navigation%20in%20Unknown%20Environments%0AAuthor%3A%20Yifan%20Zhai%20and%20Rudolf%20Reiter%20and%20Davide%20Scaramuzza%0AAbstract%3A%20Quadrotor%20navigation%20in%20unknown%20environments%20is%20critical%20for%20practical%20missions%20such%20as%20search-and-rescue.%20Solving%20this%20problem%20requires%20addressing%20three%20key%20challenges%3A%20path%20planning%20in%20non-convex%20free%20space%20due%20to%20obstacles%2C%20satisfying%20quadrotor-specific%20dynamics%20and%20objectives%2C%20and%20exploring%20unknown%20regions%20to%20expand%20the%20map.%20Recently%2C%20the%20Model%20Predictive%20Path%20Integral%20%28MPPI%29%20method%20has%20emerged%20as%20a%20promising%20solution%20to%20the%20first%20two%20challenges.%20By%20leveraging%20sampling-based%20optimization%2C%20it%20can%20effectively%20handle%20non-convex%20free%20space%20while%20directly%20optimizing%20over%20the%20full%20quadrotor%20dynamics%2C%20enabling%20the%20inclusion%20of%20quadrotor-specific%20costs%20such%20as%20energy%20consumption.%20However%2C%20MPPI%20has%20been%20limited%20to%20tracking%20control%20that%20optimizes%20trajectories%20only%20within%20a%20small%20neighborhood%20around%20a%20reference%20trajectory%2C%20as%20it%20lacks%20the%20ability%20to%20explore%20unknown%20regions%20and%20plan%20alternative%20paths%20when%20blocked%20by%20large%20obstacles.%20To%20address%20this%20limitation%2C%20we%20introduce%20Perception-Aware%20MPPI%20%28PA-MPPI%29.%20In%20this%20approach%2C%20perception-awareness%20is%20characterized%20by%20planning%20and%20adapting%20the%20trajectory%20online%20based%20on%20perception%20objectives.%20Specifically%2C%20when%20the%20goal%20is%20occluded%2C%20PA-MPPI%20incorporates%20a%20perception%20cost%20that%20biases%20trajectories%20toward%20those%20that%20can%20observe%20unknown%20regions.%20This%20expands%20the%20mapped%20traversable%20space%20and%20increases%20the%20likelihood%20of%20finding%20alternative%20paths%20to%20the%20goal.%20Through%20hardware%20experiments%2C%20we%20demonstrate%20that%20PA-MPPI%2C%20running%20at%2050%20Hz%2C%20performs%20on%20par%20with%20the%20state-of-the-art%20quadrotor%20navigation%20planner%20for%20unknown%20environments%20in%20challenging%20test%20scenarios.%20Furthermore%2C%20we%20show%20that%20PA-MPPI%20can%20serve%20as%20a%20safe%20and%20robust%20action%20policy%20for%20navigation%20foundation%20models%2C%20which%20often%20provide%20goal%20poses%20that%20are%20not%20directly%20reachable.%0ALink%3A%20http%3A//arxiv.org/abs/2509.14978v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPA-MPPI%253A%2520Perception-Aware%2520Model%2520Predictive%2520Path%2520Integral%2520Control%2520for%2520Quadrotor%2520Navigation%2520in%2520Unknown%2520Environments%26entry.906535625%3DYifan%2520Zhai%2520and%2520Rudolf%2520Reiter%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3DQuadrotor%2520navigation%2520in%2520unknown%2520environments%2520is%2520critical%2520for%2520practical%2520missions%2520such%2520as%2520search-and-rescue.%2520Solving%2520this%2520problem%2520requires%2520addressing%2520three%2520key%2520challenges%253A%2520path%2520planning%2520in%2520non-convex%2520free%2520space%2520due%2520to%2520obstacles%252C%2520satisfying%2520quadrotor-specific%2520dynamics%2520and%2520objectives%252C%2520and%2520exploring%2520unknown%2520regions%2520to%2520expand%2520the%2520map.%2520Recently%252C%2520the%2520Model%2520Predictive%2520Path%2520Integral%2520%2528MPPI%2529%2520method%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520to%2520the%2520first%2520two%2520challenges.%2520By%2520leveraging%2520sampling-based%2520optimization%252C%2520it%2520can%2520effectively%2520handle%2520non-convex%2520free%2520space%2520while%2520directly%2520optimizing%2520over%2520the%2520full%2520quadrotor%2520dynamics%252C%2520enabling%2520the%2520inclusion%2520of%2520quadrotor-specific%2520costs%2520such%2520as%2520energy%2520consumption.%2520However%252C%2520MPPI%2520has%2520been%2520limited%2520to%2520tracking%2520control%2520that%2520optimizes%2520trajectories%2520only%2520within%2520a%2520small%2520neighborhood%2520around%2520a%2520reference%2520trajectory%252C%2520as%2520it%2520lacks%2520the%2520ability%2520to%2520explore%2520unknown%2520regions%2520and%2520plan%2520alternative%2520paths%2520when%2520blocked%2520by%2520large%2520obstacles.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520Perception-Aware%2520MPPI%2520%2528PA-MPPI%2529.%2520In%2520this%2520approach%252C%2520perception-awareness%2520is%2520characterized%2520by%2520planning%2520and%2520adapting%2520the%2520trajectory%2520online%2520based%2520on%2520perception%2520objectives.%2520Specifically%252C%2520when%2520the%2520goal%2520is%2520occluded%252C%2520PA-MPPI%2520incorporates%2520a%2520perception%2520cost%2520that%2520biases%2520trajectories%2520toward%2520those%2520that%2520can%2520observe%2520unknown%2520regions.%2520This%2520expands%2520the%2520mapped%2520traversable%2520space%2520and%2520increases%2520the%2520likelihood%2520of%2520finding%2520alternative%2520paths%2520to%2520the%2520goal.%2520Through%2520hardware%2520experiments%252C%2520we%2520demonstrate%2520that%2520PA-MPPI%252C%2520running%2520at%252050%2520Hz%252C%2520performs%2520on%2520par%2520with%2520the%2520state-of-the-art%2520quadrotor%2520navigation%2520planner%2520for%2520unknown%2520environments%2520in%2520challenging%2520test%2520scenarios.%2520Furthermore%252C%2520we%2520show%2520that%2520PA-MPPI%2520can%2520serve%2520as%2520a%2520safe%2520and%2520robust%2520action%2520policy%2520for%2520navigation%2520foundation%2520models%252C%2520which%2520often%2520provide%2520goal%2520poses%2520that%2520are%2520not%2520directly%2520reachable.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14978v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PA-MPPI%3A%20Perception-Aware%20Model%20Predictive%20Path%20Integral%20Control%20for%20Quadrotor%20Navigation%20in%20Unknown%20Environments&entry.906535625=Yifan%20Zhai%20and%20Rudolf%20Reiter%20and%20Davide%20Scaramuzza&entry.1292438233=Quadrotor%20navigation%20in%20unknown%20environments%20is%20critical%20for%20practical%20missions%20such%20as%20search-and-rescue.%20Solving%20this%20problem%20requires%20addressing%20three%20key%20challenges%3A%20path%20planning%20in%20non-convex%20free%20space%20due%20to%20obstacles%2C%20satisfying%20quadrotor-specific%20dynamics%20and%20objectives%2C%20and%20exploring%20unknown%20regions%20to%20expand%20the%20map.%20Recently%2C%20the%20Model%20Predictive%20Path%20Integral%20%28MPPI%29%20method%20has%20emerged%20as%20a%20promising%20solution%20to%20the%20first%20two%20challenges.%20By%20leveraging%20sampling-based%20optimization%2C%20it%20can%20effectively%20handle%20non-convex%20free%20space%20while%20directly%20optimizing%20over%20the%20full%20quadrotor%20dynamics%2C%20enabling%20the%20inclusion%20of%20quadrotor-specific%20costs%20such%20as%20energy%20consumption.%20However%2C%20MPPI%20has%20been%20limited%20to%20tracking%20control%20that%20optimizes%20trajectories%20only%20within%20a%20small%20neighborhood%20around%20a%20reference%20trajectory%2C%20as%20it%20lacks%20the%20ability%20to%20explore%20unknown%20regions%20and%20plan%20alternative%20paths%20when%20blocked%20by%20large%20obstacles.%20To%20address%20this%20limitation%2C%20we%20introduce%20Perception-Aware%20MPPI%20%28PA-MPPI%29.%20In%20this%20approach%2C%20perception-awareness%20is%20characterized%20by%20planning%20and%20adapting%20the%20trajectory%20online%20based%20on%20perception%20objectives.%20Specifically%2C%20when%20the%20goal%20is%20occluded%2C%20PA-MPPI%20incorporates%20a%20perception%20cost%20that%20biases%20trajectories%20toward%20those%20that%20can%20observe%20unknown%20regions.%20This%20expands%20the%20mapped%20traversable%20space%20and%20increases%20the%20likelihood%20of%20finding%20alternative%20paths%20to%20the%20goal.%20Through%20hardware%20experiments%2C%20we%20demonstrate%20that%20PA-MPPI%2C%20running%20at%2050%20Hz%2C%20performs%20on%20par%20with%20the%20state-of-the-art%20quadrotor%20navigation%20planner%20for%20unknown%20environments%20in%20challenging%20test%20scenarios.%20Furthermore%2C%20we%20show%20that%20PA-MPPI%20can%20serve%20as%20a%20safe%20and%20robust%20action%20policy%20for%20navigation%20foundation%20models%2C%20which%20often%20provide%20goal%20poses%20that%20are%20not%20directly%20reachable.&entry.1838667208=http%3A//arxiv.org/abs/2509.14978v3&entry.124074799=Read"},
{"title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents", "author": "Huanyao Zhang and Jiepeng Zhou and Bo Li and Bowen Zhou and Yanzhe Dan and Haishan Lu and Zhiyong Cao and Jiaoyang Chen and Yuqian Han and Zinan Sheng and Zhengwei Tao and Hao Liang and Jialong Wu and Yang Shi and Yuanpeng He and Jiaye Lin and Qintong Zhang and Guochen Yan and Runhao Zhao and Zhengpin Li and Xiaohan Yu and Lang Mei and Chong Chen and Wentao Zhang and Bin Cui", "abstract": "Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.", "link": "http://arxiv.org/abs/2602.12876v1", "date": "2026-02-13", "relevancy": 2.3179, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5868}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrowseComp-%24V%5E3%24%3A%20A%20Visual%2C%20Vertical%2C%20and%20Verifiable%20Benchmark%20for%20Multimodal%20Browsing%20Agents&body=Title%3A%20BrowseComp-%24V%5E3%24%3A%20A%20Visual%2C%20Vertical%2C%20and%20Verifiable%20Benchmark%20for%20Multimodal%20Browsing%20Agents%0AAuthor%3A%20Huanyao%20Zhang%20and%20Jiepeng%20Zhou%20and%20Bo%20Li%20and%20Bowen%20Zhou%20and%20Yanzhe%20Dan%20and%20Haishan%20Lu%20and%20Zhiyong%20Cao%20and%20Jiaoyang%20Chen%20and%20Yuqian%20Han%20and%20Zinan%20Sheng%20and%20Zhengwei%20Tao%20and%20Hao%20Liang%20and%20Jialong%20Wu%20and%20Yang%20Shi%20and%20Yuanpeng%20He%20and%20Jiaye%20Lin%20and%20Qintong%20Zhang%20and%20Guochen%20Yan%20and%20Runhao%20Zhao%20and%20Zhengpin%20Li%20and%20Xiaohan%20Yu%20and%20Lang%20Mei%20and%20Chong%20Chen%20and%20Wentao%20Zhang%20and%20Bin%20Cui%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%2C%20equipped%20with%20increasingly%20advanced%20planning%20and%20tool-use%20capabilities%2C%20are%20evolving%20into%20autonomous%20agents%20capable%20of%20performing%20multimodal%20web%20browsing%20and%20deep%20search%20in%20open-world%20environments.%20However%2C%20existing%20benchmarks%20for%20multimodal%20browsing%20remain%20limited%20in%20task%20complexity%2C%20evidence%20accessibility%2C%20and%20evaluation%20granularity%2C%20hindering%20comprehensive%20and%20reproducible%20assessments%20of%20deep%20search%20capabilities.%20To%20address%20these%20limitations%2C%20we%20introduce%20BrowseComp-%24V%5E3%24%2C%20a%20novel%20benchmark%20consisting%20of%20300%20carefully%20curated%20and%20challenging%20questions%20spanning%20diverse%20domains.%20The%20benchmark%20emphasizes%20deep%2C%20multi-level%2C%20and%20cross-modal%20multi-hop%20reasoning%2C%20where%20critical%20evidence%20is%20interleaved%20across%20textual%20and%20visual%20modalities%20within%20and%20across%20web%20pages.%20All%20supporting%20evidence%20is%20strictly%20required%20to%20be%20publicly%20searchable%2C%20ensuring%20fairness%20and%20reproducibility.%20Beyond%20final-answer%20accuracy%2C%20we%20incorporate%20an%20expert-validated%2C%20subgoal-driven%20process%20evaluation%20mechanism%20that%20enables%20fine-grained%20analysis%20of%20intermediate%20reasoning%20behaviors%20and%20systematic%20characterization%20of%20capability%20boundaries.%20In%20addition%2C%20we%20propose%20OmniSeeker%2C%20a%20unified%20multimodal%20browsing%20agent%20framework%20integrating%20diverse%20web%20search%20and%20visual%20perception%20tools.%20Comprehensive%20experiments%20demonstrate%20that%20even%20state-of-the-art%20models%20achieve%20only%2036%25%20accuracy%20on%20our%20benchmark%2C%20revealing%20critical%20bottlenecks%20in%20multimodal%20information%20integration%20and%20fine-grained%20perception.%20Our%20results%20highlight%20a%20fundamental%20gap%20between%20current%20model%20capabilities%20and%20robust%20multimodal%20deep%20search%20in%20real-world%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrowseComp-%2524V%255E3%2524%253A%2520A%2520Visual%252C%2520Vertical%252C%2520and%2520Verifiable%2520Benchmark%2520for%2520Multimodal%2520Browsing%2520Agents%26entry.906535625%3DHuanyao%2520Zhang%2520and%2520Jiepeng%2520Zhou%2520and%2520Bo%2520Li%2520and%2520Bowen%2520Zhou%2520and%2520Yanzhe%2520Dan%2520and%2520Haishan%2520Lu%2520and%2520Zhiyong%2520Cao%2520and%2520Jiaoyang%2520Chen%2520and%2520Yuqian%2520Han%2520and%2520Zinan%2520Sheng%2520and%2520Zhengwei%2520Tao%2520and%2520Hao%2520Liang%2520and%2520Jialong%2520Wu%2520and%2520Yang%2520Shi%2520and%2520Yuanpeng%2520He%2520and%2520Jiaye%2520Lin%2520and%2520Qintong%2520Zhang%2520and%2520Guochen%2520Yan%2520and%2520Runhao%2520Zhao%2520and%2520Zhengpin%2520Li%2520and%2520Xiaohan%2520Yu%2520and%2520Lang%2520Mei%2520and%2520Chong%2520Chen%2520and%2520Wentao%2520Zhang%2520and%2520Bin%2520Cui%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520equipped%2520with%2520increasingly%2520advanced%2520planning%2520and%2520tool-use%2520capabilities%252C%2520are%2520evolving%2520into%2520autonomous%2520agents%2520capable%2520of%2520performing%2520multimodal%2520web%2520browsing%2520and%2520deep%2520search%2520in%2520open-world%2520environments.%2520However%252C%2520existing%2520benchmarks%2520for%2520multimodal%2520browsing%2520remain%2520limited%2520in%2520task%2520complexity%252C%2520evidence%2520accessibility%252C%2520and%2520evaluation%2520granularity%252C%2520hindering%2520comprehensive%2520and%2520reproducible%2520assessments%2520of%2520deep%2520search%2520capabilities.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520BrowseComp-%2524V%255E3%2524%252C%2520a%2520novel%2520benchmark%2520consisting%2520of%2520300%2520carefully%2520curated%2520and%2520challenging%2520questions%2520spanning%2520diverse%2520domains.%2520The%2520benchmark%2520emphasizes%2520deep%252C%2520multi-level%252C%2520and%2520cross-modal%2520multi-hop%2520reasoning%252C%2520where%2520critical%2520evidence%2520is%2520interleaved%2520across%2520textual%2520and%2520visual%2520modalities%2520within%2520and%2520across%2520web%2520pages.%2520All%2520supporting%2520evidence%2520is%2520strictly%2520required%2520to%2520be%2520publicly%2520searchable%252C%2520ensuring%2520fairness%2520and%2520reproducibility.%2520Beyond%2520final-answer%2520accuracy%252C%2520we%2520incorporate%2520an%2520expert-validated%252C%2520subgoal-driven%2520process%2520evaluation%2520mechanism%2520that%2520enables%2520fine-grained%2520analysis%2520of%2520intermediate%2520reasoning%2520behaviors%2520and%2520systematic%2520characterization%2520of%2520capability%2520boundaries.%2520In%2520addition%252C%2520we%2520propose%2520OmniSeeker%252C%2520a%2520unified%2520multimodal%2520browsing%2520agent%2520framework%2520integrating%2520diverse%2520web%2520search%2520and%2520visual%2520perception%2520tools.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520even%2520state-of-the-art%2520models%2520achieve%2520only%252036%2525%2520accuracy%2520on%2520our%2520benchmark%252C%2520revealing%2520critical%2520bottlenecks%2520in%2520multimodal%2520information%2520integration%2520and%2520fine-grained%2520perception.%2520Our%2520results%2520highlight%2520a%2520fundamental%2520gap%2520between%2520current%2520model%2520capabilities%2520and%2520robust%2520multimodal%2520deep%2520search%2520in%2520real-world%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrowseComp-%24V%5E3%24%3A%20A%20Visual%2C%20Vertical%2C%20and%20Verifiable%20Benchmark%20for%20Multimodal%20Browsing%20Agents&entry.906535625=Huanyao%20Zhang%20and%20Jiepeng%20Zhou%20and%20Bo%20Li%20and%20Bowen%20Zhou%20and%20Yanzhe%20Dan%20and%20Haishan%20Lu%20and%20Zhiyong%20Cao%20and%20Jiaoyang%20Chen%20and%20Yuqian%20Han%20and%20Zinan%20Sheng%20and%20Zhengwei%20Tao%20and%20Hao%20Liang%20and%20Jialong%20Wu%20and%20Yang%20Shi%20and%20Yuanpeng%20He%20and%20Jiaye%20Lin%20and%20Qintong%20Zhang%20and%20Guochen%20Yan%20and%20Runhao%20Zhao%20and%20Zhengpin%20Li%20and%20Xiaohan%20Yu%20and%20Lang%20Mei%20and%20Chong%20Chen%20and%20Wentao%20Zhang%20and%20Bin%20Cui&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%2C%20equipped%20with%20increasingly%20advanced%20planning%20and%20tool-use%20capabilities%2C%20are%20evolving%20into%20autonomous%20agents%20capable%20of%20performing%20multimodal%20web%20browsing%20and%20deep%20search%20in%20open-world%20environments.%20However%2C%20existing%20benchmarks%20for%20multimodal%20browsing%20remain%20limited%20in%20task%20complexity%2C%20evidence%20accessibility%2C%20and%20evaluation%20granularity%2C%20hindering%20comprehensive%20and%20reproducible%20assessments%20of%20deep%20search%20capabilities.%20To%20address%20these%20limitations%2C%20we%20introduce%20BrowseComp-%24V%5E3%24%2C%20a%20novel%20benchmark%20consisting%20of%20300%20carefully%20curated%20and%20challenging%20questions%20spanning%20diverse%20domains.%20The%20benchmark%20emphasizes%20deep%2C%20multi-level%2C%20and%20cross-modal%20multi-hop%20reasoning%2C%20where%20critical%20evidence%20is%20interleaved%20across%20textual%20and%20visual%20modalities%20within%20and%20across%20web%20pages.%20All%20supporting%20evidence%20is%20strictly%20required%20to%20be%20publicly%20searchable%2C%20ensuring%20fairness%20and%20reproducibility.%20Beyond%20final-answer%20accuracy%2C%20we%20incorporate%20an%20expert-validated%2C%20subgoal-driven%20process%20evaluation%20mechanism%20that%20enables%20fine-grained%20analysis%20of%20intermediate%20reasoning%20behaviors%20and%20systematic%20characterization%20of%20capability%20boundaries.%20In%20addition%2C%20we%20propose%20OmniSeeker%2C%20a%20unified%20multimodal%20browsing%20agent%20framework%20integrating%20diverse%20web%20search%20and%20visual%20perception%20tools.%20Comprehensive%20experiments%20demonstrate%20that%20even%20state-of-the-art%20models%20achieve%20only%2036%25%20accuracy%20on%20our%20benchmark%2C%20revealing%20critical%20bottlenecks%20in%20multimodal%20information%20integration%20and%20fine-grained%20perception.%20Our%20results%20highlight%20a%20fundamental%20gap%20between%20current%20model%20capabilities%20and%20robust%20multimodal%20deep%20search%20in%20real-world%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2602.12876v1&entry.124074799=Read"},
{"title": "Backdoor Attacks on Contrastive Continual Learning for IoT Systems", "author": "Alfous Tim and Kuniyilh Simi D", "abstract": "The Internet of Things (IoT) systems increasingly depend on continual learning to adapt to non-stationary environments. These environments can include factors such as sensor drift, changing user behavior, device aging, and adversarial dynamics. Contrastive continual learning (CCL) combines contrastive representation learning with incremental adaptation, enabling robust feature reuse across tasks and domains. However, the geometric nature of contrastive objectives, when paired with replay-based rehearsal and stability-preserving regularization, introduces new security vulnerabilities. Notably, backdoor attacks can exploit embedding alignment and replay reinforcement, enabling the implantation of persistent malicious behaviors that endure through updates and deployment cycles. This paper provides a comprehensive analysis of backdoor attacks on CCL within IoT systems. We formalize the objectives of embedding-level attacks, examine persistence mechanisms unique to IoT deployments, and develop a layered taxonomy tailored to IoT. Additionally, we compare vulnerabilities across various learning paradigms and evaluate defense strategies under IoT constraints, including limited memory, edge computing, and federated aggregation. Our findings indicate that while CCL is effective for enhancing adaptive IoT intelligence, it may also elevate long-lived representation-level threats if not adequately secured.", "link": "http://arxiv.org/abs/2602.13062v1", "date": "2026-02-13", "relevancy": 2.2859, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4625}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.457}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backdoor%20Attacks%20on%20Contrastive%20Continual%20Learning%20for%20IoT%20Systems&body=Title%3A%20Backdoor%20Attacks%20on%20Contrastive%20Continual%20Learning%20for%20IoT%20Systems%0AAuthor%3A%20Alfous%20Tim%20and%20Kuniyilh%20Simi%20D%0AAbstract%3A%20The%20Internet%20of%20Things%20%28IoT%29%20systems%20increasingly%20depend%20on%20continual%20learning%20to%20adapt%20to%20non-stationary%20environments.%20These%20environments%20can%20include%20factors%20such%20as%20sensor%20drift%2C%20changing%20user%20behavior%2C%20device%20aging%2C%20and%20adversarial%20dynamics.%20Contrastive%20continual%20learning%20%28CCL%29%20combines%20contrastive%20representation%20learning%20with%20incremental%20adaptation%2C%20enabling%20robust%20feature%20reuse%20across%20tasks%20and%20domains.%20However%2C%20the%20geometric%20nature%20of%20contrastive%20objectives%2C%20when%20paired%20with%20replay-based%20rehearsal%20and%20stability-preserving%20regularization%2C%20introduces%20new%20security%20vulnerabilities.%20Notably%2C%20backdoor%20attacks%20can%20exploit%20embedding%20alignment%20and%20replay%20reinforcement%2C%20enabling%20the%20implantation%20of%20persistent%20malicious%20behaviors%20that%20endure%20through%20updates%20and%20deployment%20cycles.%20This%20paper%20provides%20a%20comprehensive%20analysis%20of%20backdoor%20attacks%20on%20CCL%20within%20IoT%20systems.%20We%20formalize%20the%20objectives%20of%20embedding-level%20attacks%2C%20examine%20persistence%20mechanisms%20unique%20to%20IoT%20deployments%2C%20and%20develop%20a%20layered%20taxonomy%20tailored%20to%20IoT.%20Additionally%2C%20we%20compare%20vulnerabilities%20across%20various%20learning%20paradigms%20and%20evaluate%20defense%20strategies%20under%20IoT%20constraints%2C%20including%20limited%20memory%2C%20edge%20computing%2C%20and%20federated%20aggregation.%20Our%20findings%20indicate%20that%20while%20CCL%20is%20effective%20for%20enhancing%20adaptive%20IoT%20intelligence%2C%20it%20may%20also%20elevate%20long-lived%20representation-level%20threats%20if%20not%20adequately%20secured.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackdoor%2520Attacks%2520on%2520Contrastive%2520Continual%2520Learning%2520for%2520IoT%2520Systems%26entry.906535625%3DAlfous%2520Tim%2520and%2520Kuniyilh%2520Simi%2520D%26entry.1292438233%3DThe%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520systems%2520increasingly%2520depend%2520on%2520continual%2520learning%2520to%2520adapt%2520to%2520non-stationary%2520environments.%2520These%2520environments%2520can%2520include%2520factors%2520such%2520as%2520sensor%2520drift%252C%2520changing%2520user%2520behavior%252C%2520device%2520aging%252C%2520and%2520adversarial%2520dynamics.%2520Contrastive%2520continual%2520learning%2520%2528CCL%2529%2520combines%2520contrastive%2520representation%2520learning%2520with%2520incremental%2520adaptation%252C%2520enabling%2520robust%2520feature%2520reuse%2520across%2520tasks%2520and%2520domains.%2520However%252C%2520the%2520geometric%2520nature%2520of%2520contrastive%2520objectives%252C%2520when%2520paired%2520with%2520replay-based%2520rehearsal%2520and%2520stability-preserving%2520regularization%252C%2520introduces%2520new%2520security%2520vulnerabilities.%2520Notably%252C%2520backdoor%2520attacks%2520can%2520exploit%2520embedding%2520alignment%2520and%2520replay%2520reinforcement%252C%2520enabling%2520the%2520implantation%2520of%2520persistent%2520malicious%2520behaviors%2520that%2520endure%2520through%2520updates%2520and%2520deployment%2520cycles.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520analysis%2520of%2520backdoor%2520attacks%2520on%2520CCL%2520within%2520IoT%2520systems.%2520We%2520formalize%2520the%2520objectives%2520of%2520embedding-level%2520attacks%252C%2520examine%2520persistence%2520mechanisms%2520unique%2520to%2520IoT%2520deployments%252C%2520and%2520develop%2520a%2520layered%2520taxonomy%2520tailored%2520to%2520IoT.%2520Additionally%252C%2520we%2520compare%2520vulnerabilities%2520across%2520various%2520learning%2520paradigms%2520and%2520evaluate%2520defense%2520strategies%2520under%2520IoT%2520constraints%252C%2520including%2520limited%2520memory%252C%2520edge%2520computing%252C%2520and%2520federated%2520aggregation.%2520Our%2520findings%2520indicate%2520that%2520while%2520CCL%2520is%2520effective%2520for%2520enhancing%2520adaptive%2520IoT%2520intelligence%252C%2520it%2520may%2520also%2520elevate%2520long-lived%2520representation-level%2520threats%2520if%2520not%2520adequately%2520secured.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backdoor%20Attacks%20on%20Contrastive%20Continual%20Learning%20for%20IoT%20Systems&entry.906535625=Alfous%20Tim%20and%20Kuniyilh%20Simi%20D&entry.1292438233=The%20Internet%20of%20Things%20%28IoT%29%20systems%20increasingly%20depend%20on%20continual%20learning%20to%20adapt%20to%20non-stationary%20environments.%20These%20environments%20can%20include%20factors%20such%20as%20sensor%20drift%2C%20changing%20user%20behavior%2C%20device%20aging%2C%20and%20adversarial%20dynamics.%20Contrastive%20continual%20learning%20%28CCL%29%20combines%20contrastive%20representation%20learning%20with%20incremental%20adaptation%2C%20enabling%20robust%20feature%20reuse%20across%20tasks%20and%20domains.%20However%2C%20the%20geometric%20nature%20of%20contrastive%20objectives%2C%20when%20paired%20with%20replay-based%20rehearsal%20and%20stability-preserving%20regularization%2C%20introduces%20new%20security%20vulnerabilities.%20Notably%2C%20backdoor%20attacks%20can%20exploit%20embedding%20alignment%20and%20replay%20reinforcement%2C%20enabling%20the%20implantation%20of%20persistent%20malicious%20behaviors%20that%20endure%20through%20updates%20and%20deployment%20cycles.%20This%20paper%20provides%20a%20comprehensive%20analysis%20of%20backdoor%20attacks%20on%20CCL%20within%20IoT%20systems.%20We%20formalize%20the%20objectives%20of%20embedding-level%20attacks%2C%20examine%20persistence%20mechanisms%20unique%20to%20IoT%20deployments%2C%20and%20develop%20a%20layered%20taxonomy%20tailored%20to%20IoT.%20Additionally%2C%20we%20compare%20vulnerabilities%20across%20various%20learning%20paradigms%20and%20evaluate%20defense%20strategies%20under%20IoT%20constraints%2C%20including%20limited%20memory%2C%20edge%20computing%2C%20and%20federated%20aggregation.%20Our%20findings%20indicate%20that%20while%20CCL%20is%20effective%20for%20enhancing%20adaptive%20IoT%20intelligence%2C%20it%20may%20also%20elevate%20long-lived%20representation-level%20threats%20if%20not%20adequately%20secured.&entry.1838667208=http%3A//arxiv.org/abs/2602.13062v1&entry.124074799=Read"},
{"title": "From Prompt to Product: A Human-Centered Benchmark of Agentic App Generation Systems", "author": "Marcos Ortiz and Justin Hill and Collin Overbay and Ingrida Semenec and Frederic Sauve-Hoover and Jim Schwoebel and Joel Shor", "abstract": "Agentic AI systems capable of generating full-stack web applications from natural language prompts (\"prompt- to-app\") represent a significant shift in software development. However, evaluating these systems remains challenging, as visual polish, functional correctness, and user trust are often misaligned. As a result, it is unclear how existing prompt-to-app tools compare under realistic, human-centered evaluation criteria. In this paper, we introduce a human-centered benchmark for evaluating prompt-to-app systems and conduct a large-scale comparative study of three widely used platforms: Replit, Bolt, and Firebase Studio. Using a diverse set of 96 prompts spanning common web application tasks, we generate 288 unique application artifacts. We evaluate these systems through a large-scale human-rater study involving 205 participants and 1,071 quality-filtered pairwise comparisons, assessing task-based ease of use, visual appeal, perceived completeness, and user trust. Our results show that these systems are not interchangeable: Firebase Studio consistently outperforms competing platforms across all human-evaluated dimensions, achieving the highest win rates for ease of use, trust, visual appeal, and visual appropriateness. Bolt performs competitively on visual appeal but trails Firebase on usability and trust, while Replit underperforms relative to both across most metrics. These findings highlight a persistent gap between visual polish and functional reliability in prompt-to-app systems and demonstrate the necessity of interactive, task-based evaluation. We release our benchmark framework, prompt set, and generated artifacts to support reproducible evaluation and future research in agentic application generation.", "link": "http://arxiv.org/abs/2512.18080v2", "date": "2026-02-13", "relevancy": 2.2687, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4601}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4583}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Prompt%20to%20Product%3A%20A%20Human-Centered%20Benchmark%20of%20Agentic%20App%20Generation%20Systems&body=Title%3A%20From%20Prompt%20to%20Product%3A%20A%20Human-Centered%20Benchmark%20of%20Agentic%20App%20Generation%20Systems%0AAuthor%3A%20Marcos%20Ortiz%20and%20Justin%20Hill%20and%20Collin%20Overbay%20and%20Ingrida%20Semenec%20and%20Frederic%20Sauve-Hoover%20and%20Jim%20Schwoebel%20and%20Joel%20Shor%0AAbstract%3A%20Agentic%20AI%20systems%20capable%20of%20generating%20full-stack%20web%20applications%20from%20natural%20language%20prompts%20%28%22prompt-%20to-app%22%29%20represent%20a%20significant%20shift%20in%20software%20development.%20However%2C%20evaluating%20these%20systems%20remains%20challenging%2C%20as%20visual%20polish%2C%20functional%20correctness%2C%20and%20user%20trust%20are%20often%20misaligned.%20As%20a%20result%2C%20it%20is%20unclear%20how%20existing%20prompt-to-app%20tools%20compare%20under%20realistic%2C%20human-centered%20evaluation%20criteria.%20In%20this%20paper%2C%20we%20introduce%20a%20human-centered%20benchmark%20for%20evaluating%20prompt-to-app%20systems%20and%20conduct%20a%20large-scale%20comparative%20study%20of%20three%20widely%20used%20platforms%3A%20Replit%2C%20Bolt%2C%20and%20Firebase%20Studio.%20Using%20a%20diverse%20set%20of%2096%20prompts%20spanning%20common%20web%20application%20tasks%2C%20we%20generate%20288%20unique%20application%20artifacts.%20We%20evaluate%20these%20systems%20through%20a%20large-scale%20human-rater%20study%20involving%20205%20participants%20and%201%2C071%20quality-filtered%20pairwise%20comparisons%2C%20assessing%20task-based%20ease%20of%20use%2C%20visual%20appeal%2C%20perceived%20completeness%2C%20and%20user%20trust.%20Our%20results%20show%20that%20these%20systems%20are%20not%20interchangeable%3A%20Firebase%20Studio%20consistently%20outperforms%20competing%20platforms%20across%20all%20human-evaluated%20dimensions%2C%20achieving%20the%20highest%20win%20rates%20for%20ease%20of%20use%2C%20trust%2C%20visual%20appeal%2C%20and%20visual%20appropriateness.%20Bolt%20performs%20competitively%20on%20visual%20appeal%20but%20trails%20Firebase%20on%20usability%20and%20trust%2C%20while%20Replit%20underperforms%20relative%20to%20both%20across%20most%20metrics.%20These%20findings%20highlight%20a%20persistent%20gap%20between%20visual%20polish%20and%20functional%20reliability%20in%20prompt-to-app%20systems%20and%20demonstrate%20the%20necessity%20of%20interactive%2C%20task-based%20evaluation.%20We%20release%20our%20benchmark%20framework%2C%20prompt%20set%2C%20and%20generated%20artifacts%20to%20support%20reproducible%20evaluation%20and%20future%20research%20in%20agentic%20application%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.18080v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Prompt%2520to%2520Product%253A%2520A%2520Human-Centered%2520Benchmark%2520of%2520Agentic%2520App%2520Generation%2520Systems%26entry.906535625%3DMarcos%2520Ortiz%2520and%2520Justin%2520Hill%2520and%2520Collin%2520Overbay%2520and%2520Ingrida%2520Semenec%2520and%2520Frederic%2520Sauve-Hoover%2520and%2520Jim%2520Schwoebel%2520and%2520Joel%2520Shor%26entry.1292438233%3DAgentic%2520AI%2520systems%2520capable%2520of%2520generating%2520full-stack%2520web%2520applications%2520from%2520natural%2520language%2520prompts%2520%2528%2522prompt-%2520to-app%2522%2529%2520represent%2520a%2520significant%2520shift%2520in%2520software%2520development.%2520However%252C%2520evaluating%2520these%2520systems%2520remains%2520challenging%252C%2520as%2520visual%2520polish%252C%2520functional%2520correctness%252C%2520and%2520user%2520trust%2520are%2520often%2520misaligned.%2520As%2520a%2520result%252C%2520it%2520is%2520unclear%2520how%2520existing%2520prompt-to-app%2520tools%2520compare%2520under%2520realistic%252C%2520human-centered%2520evaluation%2520criteria.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520human-centered%2520benchmark%2520for%2520evaluating%2520prompt-to-app%2520systems%2520and%2520conduct%2520a%2520large-scale%2520comparative%2520study%2520of%2520three%2520widely%2520used%2520platforms%253A%2520Replit%252C%2520Bolt%252C%2520and%2520Firebase%2520Studio.%2520Using%2520a%2520diverse%2520set%2520of%252096%2520prompts%2520spanning%2520common%2520web%2520application%2520tasks%252C%2520we%2520generate%2520288%2520unique%2520application%2520artifacts.%2520We%2520evaluate%2520these%2520systems%2520through%2520a%2520large-scale%2520human-rater%2520study%2520involving%2520205%2520participants%2520and%25201%252C071%2520quality-filtered%2520pairwise%2520comparisons%252C%2520assessing%2520task-based%2520ease%2520of%2520use%252C%2520visual%2520appeal%252C%2520perceived%2520completeness%252C%2520and%2520user%2520trust.%2520Our%2520results%2520show%2520that%2520these%2520systems%2520are%2520not%2520interchangeable%253A%2520Firebase%2520Studio%2520consistently%2520outperforms%2520competing%2520platforms%2520across%2520all%2520human-evaluated%2520dimensions%252C%2520achieving%2520the%2520highest%2520win%2520rates%2520for%2520ease%2520of%2520use%252C%2520trust%252C%2520visual%2520appeal%252C%2520and%2520visual%2520appropriateness.%2520Bolt%2520performs%2520competitively%2520on%2520visual%2520appeal%2520but%2520trails%2520Firebase%2520on%2520usability%2520and%2520trust%252C%2520while%2520Replit%2520underperforms%2520relative%2520to%2520both%2520across%2520most%2520metrics.%2520These%2520findings%2520highlight%2520a%2520persistent%2520gap%2520between%2520visual%2520polish%2520and%2520functional%2520reliability%2520in%2520prompt-to-app%2520systems%2520and%2520demonstrate%2520the%2520necessity%2520of%2520interactive%252C%2520task-based%2520evaluation.%2520We%2520release%2520our%2520benchmark%2520framework%252C%2520prompt%2520set%252C%2520and%2520generated%2520artifacts%2520to%2520support%2520reproducible%2520evaluation%2520and%2520future%2520research%2520in%2520agentic%2520application%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.18080v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Prompt%20to%20Product%3A%20A%20Human-Centered%20Benchmark%20of%20Agentic%20App%20Generation%20Systems&entry.906535625=Marcos%20Ortiz%20and%20Justin%20Hill%20and%20Collin%20Overbay%20and%20Ingrida%20Semenec%20and%20Frederic%20Sauve-Hoover%20and%20Jim%20Schwoebel%20and%20Joel%20Shor&entry.1292438233=Agentic%20AI%20systems%20capable%20of%20generating%20full-stack%20web%20applications%20from%20natural%20language%20prompts%20%28%22prompt-%20to-app%22%29%20represent%20a%20significant%20shift%20in%20software%20development.%20However%2C%20evaluating%20these%20systems%20remains%20challenging%2C%20as%20visual%20polish%2C%20functional%20correctness%2C%20and%20user%20trust%20are%20often%20misaligned.%20As%20a%20result%2C%20it%20is%20unclear%20how%20existing%20prompt-to-app%20tools%20compare%20under%20realistic%2C%20human-centered%20evaluation%20criteria.%20In%20this%20paper%2C%20we%20introduce%20a%20human-centered%20benchmark%20for%20evaluating%20prompt-to-app%20systems%20and%20conduct%20a%20large-scale%20comparative%20study%20of%20three%20widely%20used%20platforms%3A%20Replit%2C%20Bolt%2C%20and%20Firebase%20Studio.%20Using%20a%20diverse%20set%20of%2096%20prompts%20spanning%20common%20web%20application%20tasks%2C%20we%20generate%20288%20unique%20application%20artifacts.%20We%20evaluate%20these%20systems%20through%20a%20large-scale%20human-rater%20study%20involving%20205%20participants%20and%201%2C071%20quality-filtered%20pairwise%20comparisons%2C%20assessing%20task-based%20ease%20of%20use%2C%20visual%20appeal%2C%20perceived%20completeness%2C%20and%20user%20trust.%20Our%20results%20show%20that%20these%20systems%20are%20not%20interchangeable%3A%20Firebase%20Studio%20consistently%20outperforms%20competing%20platforms%20across%20all%20human-evaluated%20dimensions%2C%20achieving%20the%20highest%20win%20rates%20for%20ease%20of%20use%2C%20trust%2C%20visual%20appeal%2C%20and%20visual%20appropriateness.%20Bolt%20performs%20competitively%20on%20visual%20appeal%20but%20trails%20Firebase%20on%20usability%20and%20trust%2C%20while%20Replit%20underperforms%20relative%20to%20both%20across%20most%20metrics.%20These%20findings%20highlight%20a%20persistent%20gap%20between%20visual%20polish%20and%20functional%20reliability%20in%20prompt-to-app%20systems%20and%20demonstrate%20the%20necessity%20of%20interactive%2C%20task-based%20evaluation.%20We%20release%20our%20benchmark%20framework%2C%20prompt%20set%2C%20and%20generated%20artifacts%20to%20support%20reproducible%20evaluation%20and%20future%20research%20in%20agentic%20application%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.18080v2&entry.124074799=Read"},
{"title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models", "author": "Sayan Deb Sarkar and R\u00e9mi Pautrat and Ondrej Miksik and Marc Pollefeys and Iro Armeni and Mahdi Rad and Mihai Dusmanu", "abstract": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\\%$ and token usage by up to $93\\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.", "link": "http://arxiv.org/abs/2602.13191v1", "date": "2026-02-13", "relevancy": 2.2668, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5705}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoPE-VideoLM%3A%20Codec%20Primitives%20For%20Efficient%20Video%20Language%20Models&body=Title%3A%20CoPE-VideoLM%3A%20Codec%20Primitives%20For%20Efficient%20Video%20Language%20Models%0AAuthor%3A%20Sayan%20Deb%20Sarkar%20and%20R%C3%A9mi%20Pautrat%20and%20Ondrej%20Miksik%20and%20Marc%20Pollefeys%20and%20Iro%20Armeni%20and%20Mahdi%20Rad%20and%20Mihai%20Dusmanu%0AAbstract%3A%20Video%20Language%20Models%20%28VideoLMs%29%20empower%20AI%20systems%20to%20understand%20temporal%20dynamics%20in%20videos.%20To%20fit%20to%20the%20maximum%20context%20window%20constraint%2C%20current%20methods%20use%20keyframe%20sampling%20which%20can%20miss%20both%20macro-level%20events%20and%20micro-level%20details%20due%20to%20the%20sparse%20temporal%20coverage.%20Furthermore%2C%20processing%20full%20images%20and%20their%20tokens%20for%20each%20frame%20incurs%20substantial%20computational%20overhead.%20To%20address%20these%20limitations%2C%20we%20propose%20to%20leverage%20video%20codec%20primitives%20%28specifically%20motion%20vectors%20and%20residuals%29%20which%20natively%20encode%20video%20redundancy%20and%20sparsity%20without%20requiring%20expensive%20full-image%20encoding%20for%20most%20frames.%20To%20this%20end%2C%20we%20introduce%20lightweight%20transformer-based%20encoders%20that%20aggregate%20codec%20primitives%20and%20align%20their%20representations%20with%20image%20encoder%20embeddings%20through%20a%20pre-training%20strategy%20that%20accelerates%20convergence%20during%20end-to-end%20fine-tuning.%20Our%20approach%20reduces%20the%20time-to-first-token%20by%20up%20to%20%2486%5C%25%24%20and%20token%20usage%20by%20up%20to%20%2493%5C%25%24%20compared%20to%20standard%20VideoLMs.%20Moreover%2C%20by%20varying%20the%20keyframe%20and%20codec%20primitive%20densities%20we%20are%20able%20to%20maintain%20or%20exceed%20performance%20on%20%2414%24%20diverse%20video%20understanding%20benchmarks%20spanning%20general%20question%20answering%2C%20temporal%20reasoning%2C%20long-form%20understanding%2C%20and%20spatial%20scene%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoPE-VideoLM%253A%2520Codec%2520Primitives%2520For%2520Efficient%2520Video%2520Language%2520Models%26entry.906535625%3DSayan%2520Deb%2520Sarkar%2520and%2520R%25C3%25A9mi%2520Pautrat%2520and%2520Ondrej%2520Miksik%2520and%2520Marc%2520Pollefeys%2520and%2520Iro%2520Armeni%2520and%2520Mahdi%2520Rad%2520and%2520Mihai%2520Dusmanu%26entry.1292438233%3DVideo%2520Language%2520Models%2520%2528VideoLMs%2529%2520empower%2520AI%2520systems%2520to%2520understand%2520temporal%2520dynamics%2520in%2520videos.%2520To%2520fit%2520to%2520the%2520maximum%2520context%2520window%2520constraint%252C%2520current%2520methods%2520use%2520keyframe%2520sampling%2520which%2520can%2520miss%2520both%2520macro-level%2520events%2520and%2520micro-level%2520details%2520due%2520to%2520the%2520sparse%2520temporal%2520coverage.%2520Furthermore%252C%2520processing%2520full%2520images%2520and%2520their%2520tokens%2520for%2520each%2520frame%2520incurs%2520substantial%2520computational%2520overhead.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520to%2520leverage%2520video%2520codec%2520primitives%2520%2528specifically%2520motion%2520vectors%2520and%2520residuals%2529%2520which%2520natively%2520encode%2520video%2520redundancy%2520and%2520sparsity%2520without%2520requiring%2520expensive%2520full-image%2520encoding%2520for%2520most%2520frames.%2520To%2520this%2520end%252C%2520we%2520introduce%2520lightweight%2520transformer-based%2520encoders%2520that%2520aggregate%2520codec%2520primitives%2520and%2520align%2520their%2520representations%2520with%2520image%2520encoder%2520embeddings%2520through%2520a%2520pre-training%2520strategy%2520that%2520accelerates%2520convergence%2520during%2520end-to-end%2520fine-tuning.%2520Our%2520approach%2520reduces%2520the%2520time-to-first-token%2520by%2520up%2520to%2520%252486%255C%2525%2524%2520and%2520token%2520usage%2520by%2520up%2520to%2520%252493%255C%2525%2524%2520compared%2520to%2520standard%2520VideoLMs.%2520Moreover%252C%2520by%2520varying%2520the%2520keyframe%2520and%2520codec%2520primitive%2520densities%2520we%2520are%2520able%2520to%2520maintain%2520or%2520exceed%2520performance%2520on%2520%252414%2524%2520diverse%2520video%2520understanding%2520benchmarks%2520spanning%2520general%2520question%2520answering%252C%2520temporal%2520reasoning%252C%2520long-form%2520understanding%252C%2520and%2520spatial%2520scene%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoPE-VideoLM%3A%20Codec%20Primitives%20For%20Efficient%20Video%20Language%20Models&entry.906535625=Sayan%20Deb%20Sarkar%20and%20R%C3%A9mi%20Pautrat%20and%20Ondrej%20Miksik%20and%20Marc%20Pollefeys%20and%20Iro%20Armeni%20and%20Mahdi%20Rad%20and%20Mihai%20Dusmanu&entry.1292438233=Video%20Language%20Models%20%28VideoLMs%29%20empower%20AI%20systems%20to%20understand%20temporal%20dynamics%20in%20videos.%20To%20fit%20to%20the%20maximum%20context%20window%20constraint%2C%20current%20methods%20use%20keyframe%20sampling%20which%20can%20miss%20both%20macro-level%20events%20and%20micro-level%20details%20due%20to%20the%20sparse%20temporal%20coverage.%20Furthermore%2C%20processing%20full%20images%20and%20their%20tokens%20for%20each%20frame%20incurs%20substantial%20computational%20overhead.%20To%20address%20these%20limitations%2C%20we%20propose%20to%20leverage%20video%20codec%20primitives%20%28specifically%20motion%20vectors%20and%20residuals%29%20which%20natively%20encode%20video%20redundancy%20and%20sparsity%20without%20requiring%20expensive%20full-image%20encoding%20for%20most%20frames.%20To%20this%20end%2C%20we%20introduce%20lightweight%20transformer-based%20encoders%20that%20aggregate%20codec%20primitives%20and%20align%20their%20representations%20with%20image%20encoder%20embeddings%20through%20a%20pre-training%20strategy%20that%20accelerates%20convergence%20during%20end-to-end%20fine-tuning.%20Our%20approach%20reduces%20the%20time-to-first-token%20by%20up%20to%20%2486%5C%25%24%20and%20token%20usage%20by%20up%20to%20%2493%5C%25%24%20compared%20to%20standard%20VideoLMs.%20Moreover%2C%20by%20varying%20the%20keyframe%20and%20codec%20primitive%20densities%20we%20are%20able%20to%20maintain%20or%20exceed%20performance%20on%20%2414%24%20diverse%20video%20understanding%20benchmarks%20spanning%20general%20question%20answering%2C%20temporal%20reasoning%2C%20long-form%20understanding%2C%20and%20spatial%20scene%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2602.13191v1&entry.124074799=Read"},
{"title": "TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation", "author": "Daiye Miao and Yufang Liu and Jie Wang and Changzhi Sun and Yunke Zhang and Demei Yan and Shaokang Dong and Qi Zhang and Yuanbin Wu", "abstract": "LoRA has become one of the most widely used parameter-efficient fine-tuning methods due to its simplicity and effectiveness. However, numerous studies have shown that LoRA often introduces substantial parameter redundancy, which not only increases the number of trainable parameters but also hinders the effectiveness of fine-tuning. Since identifying redundant parameters in LoRA is inherently difficult, how to eliminate them efficiently and accurately remains a challenging problem. In this paper, we propose TASO, a redundancy reduction method that leverages importance information from the pretrained model's weights to mitigate LoRA redundancy. Specifically, we estimate parameter importance on downstream tasks and identify task-specific core regions based on the distribution of importance scores. The location information of these core regions is then used to determine the sparse structure of LoRA modules, enabling redundancy removal before fine-tuning. Our approach significantly reduces the number of trainable parameters required for task adaptation, while providing a novel task-aligned perspective for LoRA redundancy reduction. Experimental results demonstrate that, with a parameter budget comparable to LoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across multiple tasks, achieving strong fine-tuning performance while effectively eliminating redundant parameters.", "link": "http://arxiv.org/abs/2509.17688v2", "date": "2026-02-13", "relevancy": 2.2567, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4754}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4438}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TASO%3A%20Task-Aligned%20Sparse%20Optimization%20for%20Parameter-Efficient%20Model%20Adaptation&body=Title%3A%20TASO%3A%20Task-Aligned%20Sparse%20Optimization%20for%20Parameter-Efficient%20Model%20Adaptation%0AAuthor%3A%20Daiye%20Miao%20and%20Yufang%20Liu%20and%20Jie%20Wang%20and%20Changzhi%20Sun%20and%20Yunke%20Zhang%20and%20Demei%20Yan%20and%20Shaokang%20Dong%20and%20Qi%20Zhang%20and%20Yuanbin%20Wu%0AAbstract%3A%20LoRA%20has%20become%20one%20of%20the%20most%20widely%20used%20parameter-efficient%20fine-tuning%20methods%20due%20to%20its%20simplicity%20and%20effectiveness.%20However%2C%20numerous%20studies%20have%20shown%20that%20LoRA%20often%20introduces%20substantial%20parameter%20redundancy%2C%20which%20not%20only%20increases%20the%20number%20of%20trainable%20parameters%20but%20also%20hinders%20the%20effectiveness%20of%20fine-tuning.%20Since%20identifying%20redundant%20parameters%20in%20LoRA%20is%20inherently%20difficult%2C%20how%20to%20eliminate%20them%20efficiently%20and%20accurately%20remains%20a%20challenging%20problem.%20In%20this%20paper%2C%20we%20propose%20TASO%2C%20a%20redundancy%20reduction%20method%20that%20leverages%20importance%20information%20from%20the%20pretrained%20model%27s%20weights%20to%20mitigate%20LoRA%20redundancy.%20Specifically%2C%20we%20estimate%20parameter%20importance%20on%20downstream%20tasks%20and%20identify%20task-specific%20core%20regions%20based%20on%20the%20distribution%20of%20importance%20scores.%20The%20location%20information%20of%20these%20core%20regions%20is%20then%20used%20to%20determine%20the%20sparse%20structure%20of%20LoRA%20modules%2C%20enabling%20redundancy%20removal%20before%20fine-tuning.%20Our%20approach%20significantly%20reduces%20the%20number%20of%20trainable%20parameters%20required%20for%20task%20adaptation%2C%20while%20providing%20a%20novel%20task-aligned%20perspective%20for%20LoRA%20redundancy%20reduction.%20Experimental%20results%20demonstrate%20that%2C%20with%20a%20parameter%20budget%20comparable%20to%20LoRA%20with%20rank%20%24r%20%3D%201%24%2C%20TASO%20consistently%20outperforms%20standard%20LoRA%20across%20multiple%20tasks%2C%20achieving%20strong%20fine-tuning%20performance%20while%20effectively%20eliminating%20redundant%20parameters.%0ALink%3A%20http%3A//arxiv.org/abs/2509.17688v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTASO%253A%2520Task-Aligned%2520Sparse%2520Optimization%2520for%2520Parameter-Efficient%2520Model%2520Adaptation%26entry.906535625%3DDaiye%2520Miao%2520and%2520Yufang%2520Liu%2520and%2520Jie%2520Wang%2520and%2520Changzhi%2520Sun%2520and%2520Yunke%2520Zhang%2520and%2520Demei%2520Yan%2520and%2520Shaokang%2520Dong%2520and%2520Qi%2520Zhang%2520and%2520Yuanbin%2520Wu%26entry.1292438233%3DLoRA%2520has%2520become%2520one%2520of%2520the%2520most%2520widely%2520used%2520parameter-efficient%2520fine-tuning%2520methods%2520due%2520to%2520its%2520simplicity%2520and%2520effectiveness.%2520However%252C%2520numerous%2520studies%2520have%2520shown%2520that%2520LoRA%2520often%2520introduces%2520substantial%2520parameter%2520redundancy%252C%2520which%2520not%2520only%2520increases%2520the%2520number%2520of%2520trainable%2520parameters%2520but%2520also%2520hinders%2520the%2520effectiveness%2520of%2520fine-tuning.%2520Since%2520identifying%2520redundant%2520parameters%2520in%2520LoRA%2520is%2520inherently%2520difficult%252C%2520how%2520to%2520eliminate%2520them%2520efficiently%2520and%2520accurately%2520remains%2520a%2520challenging%2520problem.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TASO%252C%2520a%2520redundancy%2520reduction%2520method%2520that%2520leverages%2520importance%2520information%2520from%2520the%2520pretrained%2520model%2527s%2520weights%2520to%2520mitigate%2520LoRA%2520redundancy.%2520Specifically%252C%2520we%2520estimate%2520parameter%2520importance%2520on%2520downstream%2520tasks%2520and%2520identify%2520task-specific%2520core%2520regions%2520based%2520on%2520the%2520distribution%2520of%2520importance%2520scores.%2520The%2520location%2520information%2520of%2520these%2520core%2520regions%2520is%2520then%2520used%2520to%2520determine%2520the%2520sparse%2520structure%2520of%2520LoRA%2520modules%252C%2520enabling%2520redundancy%2520removal%2520before%2520fine-tuning.%2520Our%2520approach%2520significantly%2520reduces%2520the%2520number%2520of%2520trainable%2520parameters%2520required%2520for%2520task%2520adaptation%252C%2520while%2520providing%2520a%2520novel%2520task-aligned%2520perspective%2520for%2520LoRA%2520redundancy%2520reduction.%2520Experimental%2520results%2520demonstrate%2520that%252C%2520with%2520a%2520parameter%2520budget%2520comparable%2520to%2520LoRA%2520with%2520rank%2520%2524r%2520%253D%25201%2524%252C%2520TASO%2520consistently%2520outperforms%2520standard%2520LoRA%2520across%2520multiple%2520tasks%252C%2520achieving%2520strong%2520fine-tuning%2520performance%2520while%2520effectively%2520eliminating%2520redundant%2520parameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17688v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TASO%3A%20Task-Aligned%20Sparse%20Optimization%20for%20Parameter-Efficient%20Model%20Adaptation&entry.906535625=Daiye%20Miao%20and%20Yufang%20Liu%20and%20Jie%20Wang%20and%20Changzhi%20Sun%20and%20Yunke%20Zhang%20and%20Demei%20Yan%20and%20Shaokang%20Dong%20and%20Qi%20Zhang%20and%20Yuanbin%20Wu&entry.1292438233=LoRA%20has%20become%20one%20of%20the%20most%20widely%20used%20parameter-efficient%20fine-tuning%20methods%20due%20to%20its%20simplicity%20and%20effectiveness.%20However%2C%20numerous%20studies%20have%20shown%20that%20LoRA%20often%20introduces%20substantial%20parameter%20redundancy%2C%20which%20not%20only%20increases%20the%20number%20of%20trainable%20parameters%20but%20also%20hinders%20the%20effectiveness%20of%20fine-tuning.%20Since%20identifying%20redundant%20parameters%20in%20LoRA%20is%20inherently%20difficult%2C%20how%20to%20eliminate%20them%20efficiently%20and%20accurately%20remains%20a%20challenging%20problem.%20In%20this%20paper%2C%20we%20propose%20TASO%2C%20a%20redundancy%20reduction%20method%20that%20leverages%20importance%20information%20from%20the%20pretrained%20model%27s%20weights%20to%20mitigate%20LoRA%20redundancy.%20Specifically%2C%20we%20estimate%20parameter%20importance%20on%20downstream%20tasks%20and%20identify%20task-specific%20core%20regions%20based%20on%20the%20distribution%20of%20importance%20scores.%20The%20location%20information%20of%20these%20core%20regions%20is%20then%20used%20to%20determine%20the%20sparse%20structure%20of%20LoRA%20modules%2C%20enabling%20redundancy%20removal%20before%20fine-tuning.%20Our%20approach%20significantly%20reduces%20the%20number%20of%20trainable%20parameters%20required%20for%20task%20adaptation%2C%20while%20providing%20a%20novel%20task-aligned%20perspective%20for%20LoRA%20redundancy%20reduction.%20Experimental%20results%20demonstrate%20that%2C%20with%20a%20parameter%20budget%20comparable%20to%20LoRA%20with%20rank%20%24r%20%3D%201%24%2C%20TASO%20consistently%20outperforms%20standard%20LoRA%20across%20multiple%20tasks%2C%20achieving%20strong%20fine-tuning%20performance%20while%20effectively%20eliminating%20redundant%20parameters.&entry.1838667208=http%3A//arxiv.org/abs/2509.17688v2&entry.124074799=Read"},
{"title": "Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding", "author": "Wenhui Liao and Hongliang Li and Pengyu Xie and Xinyu Cai and Yufan Shen and Yi Xin and Qi Qin and Shenglong Ye and Tianbin Li and Ming Hu and Junjun He and Yihao Liu and Wenhai Wang and Min Dou and Bin Fu and Botian Shi and Yu Qiao and Lianwen Jin", "abstract": "Document parsing is a fundamental task in multimodal understanding, supporting a wide range of downstream applications such as information extraction and intelligent document analysis. Benefiting from strong semantic modeling and robust generalization, VLM-based end-to-end approaches have emerged as the mainstream paradigm in recent years. However, these models often suffer from substantial inference latency, as they must auto-regressively generate long token sequences when processing long-form documents. In this work, motivated by the extremely long outputs and complex layout structures commonly found in document parsing, we propose a training-free and highly efficient acceleration method. Inspired by speculative decoding, we employ a lightweight document parsing pipeline as a draft model to predict batches of future tokens, while the more accurate VLM verifies these draft predictions in parallel. Moreover, we further exploit the layout-structured nature of documents by partitioning each page into independent regions, enabling parallel decoding of each region using the same draft-verify strategy. The final predictions are then assembled according to the natural reading order. Experimental results demonstrate the effectiveness of our approach: on the general-purpose OmniDocBench, our method provides a 2.42x lossless acceleration for the dots.ocr model, and achieves up to 4.89x acceleration on long-document parsing tasks. We will release our code to facilitate reproducibility and future research.", "link": "http://arxiv.org/abs/2602.12957v1", "date": "2026-02-13", "relevancy": 2.2096, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.558}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Acceleration%20for%20Document%20Parsing%20Vision-Language%20Model%20with%20Hierarchical%20Speculative%20Decoding&body=Title%3A%20Training-Free%20Acceleration%20for%20Document%20Parsing%20Vision-Language%20Model%20with%20Hierarchical%20Speculative%20Decoding%0AAuthor%3A%20Wenhui%20Liao%20and%20Hongliang%20Li%20and%20Pengyu%20Xie%20and%20Xinyu%20Cai%20and%20Yufan%20Shen%20and%20Yi%20Xin%20and%20Qi%20Qin%20and%20Shenglong%20Ye%20and%20Tianbin%20Li%20and%20Ming%20Hu%20and%20Junjun%20He%20and%20Yihao%20Liu%20and%20Wenhai%20Wang%20and%20Min%20Dou%20and%20Bin%20Fu%20and%20Botian%20Shi%20and%20Yu%20Qiao%20and%20Lianwen%20Jin%0AAbstract%3A%20Document%20parsing%20is%20a%20fundamental%20task%20in%20multimodal%20understanding%2C%20supporting%20a%20wide%20range%20of%20downstream%20applications%20such%20as%20information%20extraction%20and%20intelligent%20document%20analysis.%20Benefiting%20from%20strong%20semantic%20modeling%20and%20robust%20generalization%2C%20VLM-based%20end-to-end%20approaches%20have%20emerged%20as%20the%20mainstream%20paradigm%20in%20recent%20years.%20However%2C%20these%20models%20often%20suffer%20from%20substantial%20inference%20latency%2C%20as%20they%20must%20auto-regressively%20generate%20long%20token%20sequences%20when%20processing%20long-form%20documents.%20In%20this%20work%2C%20motivated%20by%20the%20extremely%20long%20outputs%20and%20complex%20layout%20structures%20commonly%20found%20in%20document%20parsing%2C%20we%20propose%20a%20training-free%20and%20highly%20efficient%20acceleration%20method.%20Inspired%20by%20speculative%20decoding%2C%20we%20employ%20a%20lightweight%20document%20parsing%20pipeline%20as%20a%20draft%20model%20to%20predict%20batches%20of%20future%20tokens%2C%20while%20the%20more%20accurate%20VLM%20verifies%20these%20draft%20predictions%20in%20parallel.%20Moreover%2C%20we%20further%20exploit%20the%20layout-structured%20nature%20of%20documents%20by%20partitioning%20each%20page%20into%20independent%20regions%2C%20enabling%20parallel%20decoding%20of%20each%20region%20using%20the%20same%20draft-verify%20strategy.%20The%20final%20predictions%20are%20then%20assembled%20according%20to%20the%20natural%20reading%20order.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%3A%20on%20the%20general-purpose%20OmniDocBench%2C%20our%20method%20provides%20a%202.42x%20lossless%20acceleration%20for%20the%20dots.ocr%20model%2C%20and%20achieves%20up%20to%204.89x%20acceleration%20on%20long-document%20parsing%20tasks.%20We%20will%20release%20our%20code%20to%20facilitate%20reproducibility%20and%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Acceleration%2520for%2520Document%2520Parsing%2520Vision-Language%2520Model%2520with%2520Hierarchical%2520Speculative%2520Decoding%26entry.906535625%3DWenhui%2520Liao%2520and%2520Hongliang%2520Li%2520and%2520Pengyu%2520Xie%2520and%2520Xinyu%2520Cai%2520and%2520Yufan%2520Shen%2520and%2520Yi%2520Xin%2520and%2520Qi%2520Qin%2520and%2520Shenglong%2520Ye%2520and%2520Tianbin%2520Li%2520and%2520Ming%2520Hu%2520and%2520Junjun%2520He%2520and%2520Yihao%2520Liu%2520and%2520Wenhai%2520Wang%2520and%2520Min%2520Dou%2520and%2520Bin%2520Fu%2520and%2520Botian%2520Shi%2520and%2520Yu%2520Qiao%2520and%2520Lianwen%2520Jin%26entry.1292438233%3DDocument%2520parsing%2520is%2520a%2520fundamental%2520task%2520in%2520multimodal%2520understanding%252C%2520supporting%2520a%2520wide%2520range%2520of%2520downstream%2520applications%2520such%2520as%2520information%2520extraction%2520and%2520intelligent%2520document%2520analysis.%2520Benefiting%2520from%2520strong%2520semantic%2520modeling%2520and%2520robust%2520generalization%252C%2520VLM-based%2520end-to-end%2520approaches%2520have%2520emerged%2520as%2520the%2520mainstream%2520paradigm%2520in%2520recent%2520years.%2520However%252C%2520these%2520models%2520often%2520suffer%2520from%2520substantial%2520inference%2520latency%252C%2520as%2520they%2520must%2520auto-regressively%2520generate%2520long%2520token%2520sequences%2520when%2520processing%2520long-form%2520documents.%2520In%2520this%2520work%252C%2520motivated%2520by%2520the%2520extremely%2520long%2520outputs%2520and%2520complex%2520layout%2520structures%2520commonly%2520found%2520in%2520document%2520parsing%252C%2520we%2520propose%2520a%2520training-free%2520and%2520highly%2520efficient%2520acceleration%2520method.%2520Inspired%2520by%2520speculative%2520decoding%252C%2520we%2520employ%2520a%2520lightweight%2520document%2520parsing%2520pipeline%2520as%2520a%2520draft%2520model%2520to%2520predict%2520batches%2520of%2520future%2520tokens%252C%2520while%2520the%2520more%2520accurate%2520VLM%2520verifies%2520these%2520draft%2520predictions%2520in%2520parallel.%2520Moreover%252C%2520we%2520further%2520exploit%2520the%2520layout-structured%2520nature%2520of%2520documents%2520by%2520partitioning%2520each%2520page%2520into%2520independent%2520regions%252C%2520enabling%2520parallel%2520decoding%2520of%2520each%2520region%2520using%2520the%2520same%2520draft-verify%2520strategy.%2520The%2520final%2520predictions%2520are%2520then%2520assembled%2520according%2520to%2520the%2520natural%2520reading%2520order.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%253A%2520on%2520the%2520general-purpose%2520OmniDocBench%252C%2520our%2520method%2520provides%2520a%25202.42x%2520lossless%2520acceleration%2520for%2520the%2520dots.ocr%2520model%252C%2520and%2520achieves%2520up%2520to%25204.89x%2520acceleration%2520on%2520long-document%2520parsing%2520tasks.%2520We%2520will%2520release%2520our%2520code%2520to%2520facilitate%2520reproducibility%2520and%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Acceleration%20for%20Document%20Parsing%20Vision-Language%20Model%20with%20Hierarchical%20Speculative%20Decoding&entry.906535625=Wenhui%20Liao%20and%20Hongliang%20Li%20and%20Pengyu%20Xie%20and%20Xinyu%20Cai%20and%20Yufan%20Shen%20and%20Yi%20Xin%20and%20Qi%20Qin%20and%20Shenglong%20Ye%20and%20Tianbin%20Li%20and%20Ming%20Hu%20and%20Junjun%20He%20and%20Yihao%20Liu%20and%20Wenhai%20Wang%20and%20Min%20Dou%20and%20Bin%20Fu%20and%20Botian%20Shi%20and%20Yu%20Qiao%20and%20Lianwen%20Jin&entry.1292438233=Document%20parsing%20is%20a%20fundamental%20task%20in%20multimodal%20understanding%2C%20supporting%20a%20wide%20range%20of%20downstream%20applications%20such%20as%20information%20extraction%20and%20intelligent%20document%20analysis.%20Benefiting%20from%20strong%20semantic%20modeling%20and%20robust%20generalization%2C%20VLM-based%20end-to-end%20approaches%20have%20emerged%20as%20the%20mainstream%20paradigm%20in%20recent%20years.%20However%2C%20these%20models%20often%20suffer%20from%20substantial%20inference%20latency%2C%20as%20they%20must%20auto-regressively%20generate%20long%20token%20sequences%20when%20processing%20long-form%20documents.%20In%20this%20work%2C%20motivated%20by%20the%20extremely%20long%20outputs%20and%20complex%20layout%20structures%20commonly%20found%20in%20document%20parsing%2C%20we%20propose%20a%20training-free%20and%20highly%20efficient%20acceleration%20method.%20Inspired%20by%20speculative%20decoding%2C%20we%20employ%20a%20lightweight%20document%20parsing%20pipeline%20as%20a%20draft%20model%20to%20predict%20batches%20of%20future%20tokens%2C%20while%20the%20more%20accurate%20VLM%20verifies%20these%20draft%20predictions%20in%20parallel.%20Moreover%2C%20we%20further%20exploit%20the%20layout-structured%20nature%20of%20documents%20by%20partitioning%20each%20page%20into%20independent%20regions%2C%20enabling%20parallel%20decoding%20of%20each%20region%20using%20the%20same%20draft-verify%20strategy.%20The%20final%20predictions%20are%20then%20assembled%20according%20to%20the%20natural%20reading%20order.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%3A%20on%20the%20general-purpose%20OmniDocBench%2C%20our%20method%20provides%20a%202.42x%20lossless%20acceleration%20for%20the%20dots.ocr%20model%2C%20and%20achieves%20up%20to%204.89x%20acceleration%20on%20long-document%20parsing%20tasks.%20We%20will%20release%20our%20code%20to%20facilitate%20reproducibility%20and%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2602.12957v1&entry.124074799=Read"},
{"title": "Multi-Dimensional Visual Data Recovery: Scale-Aware Tensor Modeling and Accelerated Randomized Computation", "author": "Wenjin Qin and Hailin Wang and Jiangjun Peng and Jianjun Wang and Tingwen Huang", "abstract": "The recently proposed fully-connected tensor network (FCTN) decomposition has demonstrated significant advantages in correlation characterization and transpositional invariance, and has achieved notable achievements in multi-dimensional data processing and analysis. However, existing multi-dimensional data recovery methods leveraging FCTN decomposition still have room for further enhancement, particularly in computational efficiency and modeling capability. To address these issues, we first propose a FCTN-based generalized nonconvex regularization paradigm from the perspective of gradient mapping. Then, reliable and scalable multi-dimensional data recovery models are investigated, where the model formulation is shifted from unquantized observations to coarse-grained quantized observations. Based on the alternating direction method of multipliers (ADMM) framework, we derive efficient optimization algorithms with convergence guarantees to solve the formulated models. To alleviate the computational bottleneck encountered when processing large-scale multi-dimensional data, fast and efficient randomized compression algorithms are devised in virtue of sketching techniques in numerical linear algebra. These dimensionality-reduction techniques serve as the computational acceleration core of our proposed algorithm framework. Theoretical results on approximation error upper bounds and convergence analysis for the proposed method are derived. Extensive numerical experiments illustrate the effectiveness and superiority of the proposed algorithm over other state-of-the-art methods in terms of quantitative metrics, visual quality, and running time.", "link": "http://arxiv.org/abs/2602.12982v1", "date": "2026-02-13", "relevancy": 2.2049, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.568}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5558}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Dimensional%20Visual%20Data%20Recovery%3A%20Scale-Aware%20Tensor%20Modeling%20and%20Accelerated%20Randomized%20Computation&body=Title%3A%20Multi-Dimensional%20Visual%20Data%20Recovery%3A%20Scale-Aware%20Tensor%20Modeling%20and%20Accelerated%20Randomized%20Computation%0AAuthor%3A%20Wenjin%20Qin%20and%20Hailin%20Wang%20and%20Jiangjun%20Peng%20and%20Jianjun%20Wang%20and%20Tingwen%20Huang%0AAbstract%3A%20The%20recently%20proposed%20fully-connected%20tensor%20network%20%28FCTN%29%20decomposition%20has%20demonstrated%20significant%20advantages%20in%20correlation%20characterization%20and%20transpositional%20invariance%2C%20and%20has%20achieved%20notable%20achievements%20in%20multi-dimensional%20data%20processing%20and%20analysis.%20However%2C%20existing%20multi-dimensional%20data%20recovery%20methods%20leveraging%20FCTN%20decomposition%20still%20have%20room%20for%20further%20enhancement%2C%20particularly%20in%20computational%20efficiency%20and%20modeling%20capability.%20To%20address%20these%20issues%2C%20we%20first%20propose%20a%20FCTN-based%20generalized%20nonconvex%20regularization%20paradigm%20from%20the%20perspective%20of%20gradient%20mapping.%20Then%2C%20reliable%20and%20scalable%20multi-dimensional%20data%20recovery%20models%20are%20investigated%2C%20where%20the%20model%20formulation%20is%20shifted%20from%20unquantized%20observations%20to%20coarse-grained%20quantized%20observations.%20Based%20on%20the%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20framework%2C%20we%20derive%20efficient%20optimization%20algorithms%20with%20convergence%20guarantees%20to%20solve%20the%20formulated%20models.%20To%20alleviate%20the%20computational%20bottleneck%20encountered%20when%20processing%20large-scale%20multi-dimensional%20data%2C%20fast%20and%20efficient%20randomized%20compression%20algorithms%20are%20devised%20in%20virtue%20of%20sketching%20techniques%20in%20numerical%20linear%20algebra.%20These%20dimensionality-reduction%20techniques%20serve%20as%20the%20computational%20acceleration%20core%20of%20our%20proposed%20algorithm%20framework.%20Theoretical%20results%20on%20approximation%20error%20upper%20bounds%20and%20convergence%20analysis%20for%20the%20proposed%20method%20are%20derived.%20Extensive%20numerical%20experiments%20illustrate%20the%20effectiveness%20and%20superiority%20of%20the%20proposed%20algorithm%20over%20other%20state-of-the-art%20methods%20in%20terms%20of%20quantitative%20metrics%2C%20visual%20quality%2C%20and%20running%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Dimensional%2520Visual%2520Data%2520Recovery%253A%2520Scale-Aware%2520Tensor%2520Modeling%2520and%2520Accelerated%2520Randomized%2520Computation%26entry.906535625%3DWenjin%2520Qin%2520and%2520Hailin%2520Wang%2520and%2520Jiangjun%2520Peng%2520and%2520Jianjun%2520Wang%2520and%2520Tingwen%2520Huang%26entry.1292438233%3DThe%2520recently%2520proposed%2520fully-connected%2520tensor%2520network%2520%2528FCTN%2529%2520decomposition%2520has%2520demonstrated%2520significant%2520advantages%2520in%2520correlation%2520characterization%2520and%2520transpositional%2520invariance%252C%2520and%2520has%2520achieved%2520notable%2520achievements%2520in%2520multi-dimensional%2520data%2520processing%2520and%2520analysis.%2520However%252C%2520existing%2520multi-dimensional%2520data%2520recovery%2520methods%2520leveraging%2520FCTN%2520decomposition%2520still%2520have%2520room%2520for%2520further%2520enhancement%252C%2520particularly%2520in%2520computational%2520efficiency%2520and%2520modeling%2520capability.%2520To%2520address%2520these%2520issues%252C%2520we%2520first%2520propose%2520a%2520FCTN-based%2520generalized%2520nonconvex%2520regularization%2520paradigm%2520from%2520the%2520perspective%2520of%2520gradient%2520mapping.%2520Then%252C%2520reliable%2520and%2520scalable%2520multi-dimensional%2520data%2520recovery%2520models%2520are%2520investigated%252C%2520where%2520the%2520model%2520formulation%2520is%2520shifted%2520from%2520unquantized%2520observations%2520to%2520coarse-grained%2520quantized%2520observations.%2520Based%2520on%2520the%2520alternating%2520direction%2520method%2520of%2520multipliers%2520%2528ADMM%2529%2520framework%252C%2520we%2520derive%2520efficient%2520optimization%2520algorithms%2520with%2520convergence%2520guarantees%2520to%2520solve%2520the%2520formulated%2520models.%2520To%2520alleviate%2520the%2520computational%2520bottleneck%2520encountered%2520when%2520processing%2520large-scale%2520multi-dimensional%2520data%252C%2520fast%2520and%2520efficient%2520randomized%2520compression%2520algorithms%2520are%2520devised%2520in%2520virtue%2520of%2520sketching%2520techniques%2520in%2520numerical%2520linear%2520algebra.%2520These%2520dimensionality-reduction%2520techniques%2520serve%2520as%2520the%2520computational%2520acceleration%2520core%2520of%2520our%2520proposed%2520algorithm%2520framework.%2520Theoretical%2520results%2520on%2520approximation%2520error%2520upper%2520bounds%2520and%2520convergence%2520analysis%2520for%2520the%2520proposed%2520method%2520are%2520derived.%2520Extensive%2520numerical%2520experiments%2520illustrate%2520the%2520effectiveness%2520and%2520superiority%2520of%2520the%2520proposed%2520algorithm%2520over%2520other%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520quantitative%2520metrics%252C%2520visual%2520quality%252C%2520and%2520running%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Dimensional%20Visual%20Data%20Recovery%3A%20Scale-Aware%20Tensor%20Modeling%20and%20Accelerated%20Randomized%20Computation&entry.906535625=Wenjin%20Qin%20and%20Hailin%20Wang%20and%20Jiangjun%20Peng%20and%20Jianjun%20Wang%20and%20Tingwen%20Huang&entry.1292438233=The%20recently%20proposed%20fully-connected%20tensor%20network%20%28FCTN%29%20decomposition%20has%20demonstrated%20significant%20advantages%20in%20correlation%20characterization%20and%20transpositional%20invariance%2C%20and%20has%20achieved%20notable%20achievements%20in%20multi-dimensional%20data%20processing%20and%20analysis.%20However%2C%20existing%20multi-dimensional%20data%20recovery%20methods%20leveraging%20FCTN%20decomposition%20still%20have%20room%20for%20further%20enhancement%2C%20particularly%20in%20computational%20efficiency%20and%20modeling%20capability.%20To%20address%20these%20issues%2C%20we%20first%20propose%20a%20FCTN-based%20generalized%20nonconvex%20regularization%20paradigm%20from%20the%20perspective%20of%20gradient%20mapping.%20Then%2C%20reliable%20and%20scalable%20multi-dimensional%20data%20recovery%20models%20are%20investigated%2C%20where%20the%20model%20formulation%20is%20shifted%20from%20unquantized%20observations%20to%20coarse-grained%20quantized%20observations.%20Based%20on%20the%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20framework%2C%20we%20derive%20efficient%20optimization%20algorithms%20with%20convergence%20guarantees%20to%20solve%20the%20formulated%20models.%20To%20alleviate%20the%20computational%20bottleneck%20encountered%20when%20processing%20large-scale%20multi-dimensional%20data%2C%20fast%20and%20efficient%20randomized%20compression%20algorithms%20are%20devised%20in%20virtue%20of%20sketching%20techniques%20in%20numerical%20linear%20algebra.%20These%20dimensionality-reduction%20techniques%20serve%20as%20the%20computational%20acceleration%20core%20of%20our%20proposed%20algorithm%20framework.%20Theoretical%20results%20on%20approximation%20error%20upper%20bounds%20and%20convergence%20analysis%20for%20the%20proposed%20method%20are%20derived.%20Extensive%20numerical%20experiments%20illustrate%20the%20effectiveness%20and%20superiority%20of%20the%20proposed%20algorithm%20over%20other%20state-of-the-art%20methods%20in%20terms%20of%20quantitative%20metrics%2C%20visual%20quality%2C%20and%20running%20time.&entry.1838667208=http%3A//arxiv.org/abs/2602.12982v1&entry.124074799=Read"},
{"title": "N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion", "author": "Caleb Chin and Aashish Khubchandani and Harshvardhan Maskara and Kyuseong Choi and Jacob Feitelberg and Albert Gong and Manit Paul and Tathagata Sadhukhan and Anish Agarwal and Raaz Dwivedi", "abstract": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix completion, offering strong empirical performance and recent theoretical guarantees, including entry-wise error bounds, confidence intervals, and minimax optimality. Despite their simplicity, recent work has shown that NN approaches are robust to a range of missingness patterns and effective across diverse applications. This paper introduces N$^2$, a unified Python package and testbed that consolidates a broad class of NN-based methods through a modular, extensible interface. Built for both researchers and practitioners, N$^2$ supports rapid experimentation and benchmarking. Using this framework, we introduce a new NN variant that achieves state-of-the-art results in several settings. We also release a benchmark suite of real-world datasets, from healthcare and recommender systems to causal inference and LLM evaluation, designed to stress-test matrix completion methods beyond synthetic scenarios. Our experiments demonstrate that while classical methods excel on idealized data, NN-based techniques consistently outperform them in real-world settings.", "link": "http://arxiv.org/abs/2506.04166v2", "date": "2026-02-13", "relevancy": 2.2001, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4498}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4411}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20N%24%5E2%24%3A%20A%20Unified%20Python%20Package%20and%20Test%20Bench%20for%20Nearest%20Neighbor-Based%20Matrix%20Completion&body=Title%3A%20N%24%5E2%24%3A%20A%20Unified%20Python%20Package%20and%20Test%20Bench%20for%20Nearest%20Neighbor-Based%20Matrix%20Completion%0AAuthor%3A%20Caleb%20Chin%20and%20Aashish%20Khubchandani%20and%20Harshvardhan%20Maskara%20and%20Kyuseong%20Choi%20and%20Jacob%20Feitelberg%20and%20Albert%20Gong%20and%20Manit%20Paul%20and%20Tathagata%20Sadhukhan%20and%20Anish%20Agarwal%20and%20Raaz%20Dwivedi%0AAbstract%3A%20Nearest%20neighbor%20%28NN%29%20methods%20have%20re-emerged%20as%20competitive%20tools%20for%20matrix%20completion%2C%20offering%20strong%20empirical%20performance%20and%20recent%20theoretical%20guarantees%2C%20including%20entry-wise%20error%20bounds%2C%20confidence%20intervals%2C%20and%20minimax%20optimality.%20Despite%20their%20simplicity%2C%20recent%20work%20has%20shown%20that%20NN%20approaches%20are%20robust%20to%20a%20range%20of%20missingness%20patterns%20and%20effective%20across%20diverse%20applications.%20This%20paper%20introduces%20N%24%5E2%24%2C%20a%20unified%20Python%20package%20and%20testbed%20that%20consolidates%20a%20broad%20class%20of%20NN-based%20methods%20through%20a%20modular%2C%20extensible%20interface.%20Built%20for%20both%20researchers%20and%20practitioners%2C%20N%24%5E2%24%20supports%20rapid%20experimentation%20and%20benchmarking.%20Using%20this%20framework%2C%20we%20introduce%20a%20new%20NN%20variant%20that%20achieves%20state-of-the-art%20results%20in%20several%20settings.%20We%20also%20release%20a%20benchmark%20suite%20of%20real-world%20datasets%2C%20from%20healthcare%20and%20recommender%20systems%20to%20causal%20inference%20and%20LLM%20evaluation%2C%20designed%20to%20stress-test%20matrix%20completion%20methods%20beyond%20synthetic%20scenarios.%20Our%20experiments%20demonstrate%20that%20while%20classical%20methods%20excel%20on%20idealized%20data%2C%20NN-based%20techniques%20consistently%20outperform%20them%20in%20real-world%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2506.04166v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DN%2524%255E2%2524%253A%2520A%2520Unified%2520Python%2520Package%2520and%2520Test%2520Bench%2520for%2520Nearest%2520Neighbor-Based%2520Matrix%2520Completion%26entry.906535625%3DCaleb%2520Chin%2520and%2520Aashish%2520Khubchandani%2520and%2520Harshvardhan%2520Maskara%2520and%2520Kyuseong%2520Choi%2520and%2520Jacob%2520Feitelberg%2520and%2520Albert%2520Gong%2520and%2520Manit%2520Paul%2520and%2520Tathagata%2520Sadhukhan%2520and%2520Anish%2520Agarwal%2520and%2520Raaz%2520Dwivedi%26entry.1292438233%3DNearest%2520neighbor%2520%2528NN%2529%2520methods%2520have%2520re-emerged%2520as%2520competitive%2520tools%2520for%2520matrix%2520completion%252C%2520offering%2520strong%2520empirical%2520performance%2520and%2520recent%2520theoretical%2520guarantees%252C%2520including%2520entry-wise%2520error%2520bounds%252C%2520confidence%2520intervals%252C%2520and%2520minimax%2520optimality.%2520Despite%2520their%2520simplicity%252C%2520recent%2520work%2520has%2520shown%2520that%2520NN%2520approaches%2520are%2520robust%2520to%2520a%2520range%2520of%2520missingness%2520patterns%2520and%2520effective%2520across%2520diverse%2520applications.%2520This%2520paper%2520introduces%2520N%2524%255E2%2524%252C%2520a%2520unified%2520Python%2520package%2520and%2520testbed%2520that%2520consolidates%2520a%2520broad%2520class%2520of%2520NN-based%2520methods%2520through%2520a%2520modular%252C%2520extensible%2520interface.%2520Built%2520for%2520both%2520researchers%2520and%2520practitioners%252C%2520N%2524%255E2%2524%2520supports%2520rapid%2520experimentation%2520and%2520benchmarking.%2520Using%2520this%2520framework%252C%2520we%2520introduce%2520a%2520new%2520NN%2520variant%2520that%2520achieves%2520state-of-the-art%2520results%2520in%2520several%2520settings.%2520We%2520also%2520release%2520a%2520benchmark%2520suite%2520of%2520real-world%2520datasets%252C%2520from%2520healthcare%2520and%2520recommender%2520systems%2520to%2520causal%2520inference%2520and%2520LLM%2520evaluation%252C%2520designed%2520to%2520stress-test%2520matrix%2520completion%2520methods%2520beyond%2520synthetic%2520scenarios.%2520Our%2520experiments%2520demonstrate%2520that%2520while%2520classical%2520methods%2520excel%2520on%2520idealized%2520data%252C%2520NN-based%2520techniques%2520consistently%2520outperform%2520them%2520in%2520real-world%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04166v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=N%24%5E2%24%3A%20A%20Unified%20Python%20Package%20and%20Test%20Bench%20for%20Nearest%20Neighbor-Based%20Matrix%20Completion&entry.906535625=Caleb%20Chin%20and%20Aashish%20Khubchandani%20and%20Harshvardhan%20Maskara%20and%20Kyuseong%20Choi%20and%20Jacob%20Feitelberg%20and%20Albert%20Gong%20and%20Manit%20Paul%20and%20Tathagata%20Sadhukhan%20and%20Anish%20Agarwal%20and%20Raaz%20Dwivedi&entry.1292438233=Nearest%20neighbor%20%28NN%29%20methods%20have%20re-emerged%20as%20competitive%20tools%20for%20matrix%20completion%2C%20offering%20strong%20empirical%20performance%20and%20recent%20theoretical%20guarantees%2C%20including%20entry-wise%20error%20bounds%2C%20confidence%20intervals%2C%20and%20minimax%20optimality.%20Despite%20their%20simplicity%2C%20recent%20work%20has%20shown%20that%20NN%20approaches%20are%20robust%20to%20a%20range%20of%20missingness%20patterns%20and%20effective%20across%20diverse%20applications.%20This%20paper%20introduces%20N%24%5E2%24%2C%20a%20unified%20Python%20package%20and%20testbed%20that%20consolidates%20a%20broad%20class%20of%20NN-based%20methods%20through%20a%20modular%2C%20extensible%20interface.%20Built%20for%20both%20researchers%20and%20practitioners%2C%20N%24%5E2%24%20supports%20rapid%20experimentation%20and%20benchmarking.%20Using%20this%20framework%2C%20we%20introduce%20a%20new%20NN%20variant%20that%20achieves%20state-of-the-art%20results%20in%20several%20settings.%20We%20also%20release%20a%20benchmark%20suite%20of%20real-world%20datasets%2C%20from%20healthcare%20and%20recommender%20systems%20to%20causal%20inference%20and%20LLM%20evaluation%2C%20designed%20to%20stress-test%20matrix%20completion%20methods%20beyond%20synthetic%20scenarios.%20Our%20experiments%20demonstrate%20that%20while%20classical%20methods%20excel%20on%20idealized%20data%2C%20NN-based%20techniques%20consistently%20outperform%20them%20in%20real-world%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2506.04166v2&entry.124074799=Read"},
{"title": "Resource-Efficient Gesture Recognition through Convexified Attention", "author": "Daniel Schwartz and Dario Salvucci and Yusuf Osmanlioglu and Richard Vallett and Genevieve Dion and Ali Shokoufandeh", "abstract": "Wearable e-textile interfaces require gesture recognition capabilities but face severe constraints in power consumption, computational capacity, and form factor that make traditional deep learning impractical. While lightweight architectures like MobileNet improve efficiency, they still demand thousands of parameters, limiting deployment on textile-integrated platforms. We introduce a convexified attention mechanism for wearable applications that dynamically weights features while preserving convexity through nonexpansive simplex projection and convex loss functions. Unlike conventional attention mechanisms using non-convex softmax operations, our approach employs Euclidean projection onto the probability simplex combined with multi-class hinge loss, ensuring global convergence guarantees. Implemented on a textile-based capacitive sensor with four connection points, our approach achieves 100.00\\% accuracy on tap gestures and 100.00\\% on swipe gestures -- consistent across 10-fold cross-validation and held-out test evaluation -- while requiring only 120--360 parameters, a 97\\% reduction compared to conventional approaches. With sub-millisecond inference times (290--296$\u03bc$s) and minimal storage requirements ($<$7KB), our method enables gesture interfaces directly within e-textiles without external processing. Our evaluation, conducted in controlled laboratory conditions with a single-user dataset, demonstrates feasibility for basic gesture interactions. Real-world deployment would require validation across multiple users, environmental conditions, and more complex gesture vocabularies. These results demonstrate how convex optimization can enable efficient on-device machine learning for textile interfaces.", "link": "http://arxiv.org/abs/2602.13030v1", "date": "2026-02-13", "relevancy": 2.1971, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.551}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5488}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resource-Efficient%20Gesture%20Recognition%20through%20Convexified%20Attention&body=Title%3A%20Resource-Efficient%20Gesture%20Recognition%20through%20Convexified%20Attention%0AAuthor%3A%20Daniel%20Schwartz%20and%20Dario%20Salvucci%20and%20Yusuf%20Osmanlioglu%20and%20Richard%20Vallett%20and%20Genevieve%20Dion%20and%20Ali%20Shokoufandeh%0AAbstract%3A%20Wearable%20e-textile%20interfaces%20require%20gesture%20recognition%20capabilities%20but%20face%20severe%20constraints%20in%20power%20consumption%2C%20computational%20capacity%2C%20and%20form%20factor%20that%20make%20traditional%20deep%20learning%20impractical.%20While%20lightweight%20architectures%20like%20MobileNet%20improve%20efficiency%2C%20they%20still%20demand%20thousands%20of%20parameters%2C%20limiting%20deployment%20on%20textile-integrated%20platforms.%20We%20introduce%20a%20convexified%20attention%20mechanism%20for%20wearable%20applications%20that%20dynamically%20weights%20features%20while%20preserving%20convexity%20through%20nonexpansive%20simplex%20projection%20and%20convex%20loss%20functions.%20Unlike%20conventional%20attention%20mechanisms%20using%20non-convex%20softmax%20operations%2C%20our%20approach%20employs%20Euclidean%20projection%20onto%20the%20probability%20simplex%20combined%20with%20multi-class%20hinge%20loss%2C%20ensuring%20global%20convergence%20guarantees.%20Implemented%20on%20a%20textile-based%20capacitive%20sensor%20with%20four%20connection%20points%2C%20our%20approach%20achieves%20100.00%5C%25%20accuracy%20on%20tap%20gestures%20and%20100.00%5C%25%20on%20swipe%20gestures%20--%20consistent%20across%2010-fold%20cross-validation%20and%20held-out%20test%20evaluation%20--%20while%20requiring%20only%20120--360%20parameters%2C%20a%2097%5C%25%20reduction%20compared%20to%20conventional%20approaches.%20With%20sub-millisecond%20inference%20times%20%28290--296%24%CE%BC%24s%29%20and%20minimal%20storage%20requirements%20%28%24%3C%247KB%29%2C%20our%20method%20enables%20gesture%20interfaces%20directly%20within%20e-textiles%20without%20external%20processing.%20Our%20evaluation%2C%20conducted%20in%20controlled%20laboratory%20conditions%20with%20a%20single-user%20dataset%2C%20demonstrates%20feasibility%20for%20basic%20gesture%20interactions.%20Real-world%20deployment%20would%20require%20validation%20across%20multiple%20users%2C%20environmental%20conditions%2C%20and%20more%20complex%20gesture%20vocabularies.%20These%20results%20demonstrate%20how%20convex%20optimization%20can%20enable%20efficient%20on-device%20machine%20learning%20for%20textile%20interfaces.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResource-Efficient%2520Gesture%2520Recognition%2520through%2520Convexified%2520Attention%26entry.906535625%3DDaniel%2520Schwartz%2520and%2520Dario%2520Salvucci%2520and%2520Yusuf%2520Osmanlioglu%2520and%2520Richard%2520Vallett%2520and%2520Genevieve%2520Dion%2520and%2520Ali%2520Shokoufandeh%26entry.1292438233%3DWearable%2520e-textile%2520interfaces%2520require%2520gesture%2520recognition%2520capabilities%2520but%2520face%2520severe%2520constraints%2520in%2520power%2520consumption%252C%2520computational%2520capacity%252C%2520and%2520form%2520factor%2520that%2520make%2520traditional%2520deep%2520learning%2520impractical.%2520While%2520lightweight%2520architectures%2520like%2520MobileNet%2520improve%2520efficiency%252C%2520they%2520still%2520demand%2520thousands%2520of%2520parameters%252C%2520limiting%2520deployment%2520on%2520textile-integrated%2520platforms.%2520We%2520introduce%2520a%2520convexified%2520attention%2520mechanism%2520for%2520wearable%2520applications%2520that%2520dynamically%2520weights%2520features%2520while%2520preserving%2520convexity%2520through%2520nonexpansive%2520simplex%2520projection%2520and%2520convex%2520loss%2520functions.%2520Unlike%2520conventional%2520attention%2520mechanisms%2520using%2520non-convex%2520softmax%2520operations%252C%2520our%2520approach%2520employs%2520Euclidean%2520projection%2520onto%2520the%2520probability%2520simplex%2520combined%2520with%2520multi-class%2520hinge%2520loss%252C%2520ensuring%2520global%2520convergence%2520guarantees.%2520Implemented%2520on%2520a%2520textile-based%2520capacitive%2520sensor%2520with%2520four%2520connection%2520points%252C%2520our%2520approach%2520achieves%2520100.00%255C%2525%2520accuracy%2520on%2520tap%2520gestures%2520and%2520100.00%255C%2525%2520on%2520swipe%2520gestures%2520--%2520consistent%2520across%252010-fold%2520cross-validation%2520and%2520held-out%2520test%2520evaluation%2520--%2520while%2520requiring%2520only%2520120--360%2520parameters%252C%2520a%252097%255C%2525%2520reduction%2520compared%2520to%2520conventional%2520approaches.%2520With%2520sub-millisecond%2520inference%2520times%2520%2528290--296%2524%25CE%25BC%2524s%2529%2520and%2520minimal%2520storage%2520requirements%2520%2528%2524%253C%25247KB%2529%252C%2520our%2520method%2520enables%2520gesture%2520interfaces%2520directly%2520within%2520e-textiles%2520without%2520external%2520processing.%2520Our%2520evaluation%252C%2520conducted%2520in%2520controlled%2520laboratory%2520conditions%2520with%2520a%2520single-user%2520dataset%252C%2520demonstrates%2520feasibility%2520for%2520basic%2520gesture%2520interactions.%2520Real-world%2520deployment%2520would%2520require%2520validation%2520across%2520multiple%2520users%252C%2520environmental%2520conditions%252C%2520and%2520more%2520complex%2520gesture%2520vocabularies.%2520These%2520results%2520demonstrate%2520how%2520convex%2520optimization%2520can%2520enable%2520efficient%2520on-device%2520machine%2520learning%2520for%2520textile%2520interfaces.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resource-Efficient%20Gesture%20Recognition%20through%20Convexified%20Attention&entry.906535625=Daniel%20Schwartz%20and%20Dario%20Salvucci%20and%20Yusuf%20Osmanlioglu%20and%20Richard%20Vallett%20and%20Genevieve%20Dion%20and%20Ali%20Shokoufandeh&entry.1292438233=Wearable%20e-textile%20interfaces%20require%20gesture%20recognition%20capabilities%20but%20face%20severe%20constraints%20in%20power%20consumption%2C%20computational%20capacity%2C%20and%20form%20factor%20that%20make%20traditional%20deep%20learning%20impractical.%20While%20lightweight%20architectures%20like%20MobileNet%20improve%20efficiency%2C%20they%20still%20demand%20thousands%20of%20parameters%2C%20limiting%20deployment%20on%20textile-integrated%20platforms.%20We%20introduce%20a%20convexified%20attention%20mechanism%20for%20wearable%20applications%20that%20dynamically%20weights%20features%20while%20preserving%20convexity%20through%20nonexpansive%20simplex%20projection%20and%20convex%20loss%20functions.%20Unlike%20conventional%20attention%20mechanisms%20using%20non-convex%20softmax%20operations%2C%20our%20approach%20employs%20Euclidean%20projection%20onto%20the%20probability%20simplex%20combined%20with%20multi-class%20hinge%20loss%2C%20ensuring%20global%20convergence%20guarantees.%20Implemented%20on%20a%20textile-based%20capacitive%20sensor%20with%20four%20connection%20points%2C%20our%20approach%20achieves%20100.00%5C%25%20accuracy%20on%20tap%20gestures%20and%20100.00%5C%25%20on%20swipe%20gestures%20--%20consistent%20across%2010-fold%20cross-validation%20and%20held-out%20test%20evaluation%20--%20while%20requiring%20only%20120--360%20parameters%2C%20a%2097%5C%25%20reduction%20compared%20to%20conventional%20approaches.%20With%20sub-millisecond%20inference%20times%20%28290--296%24%CE%BC%24s%29%20and%20minimal%20storage%20requirements%20%28%24%3C%247KB%29%2C%20our%20method%20enables%20gesture%20interfaces%20directly%20within%20e-textiles%20without%20external%20processing.%20Our%20evaluation%2C%20conducted%20in%20controlled%20laboratory%20conditions%20with%20a%20single-user%20dataset%2C%20demonstrates%20feasibility%20for%20basic%20gesture%20interactions.%20Real-world%20deployment%20would%20require%20validation%20across%20multiple%20users%2C%20environmental%20conditions%2C%20and%20more%20complex%20gesture%20vocabularies.%20These%20results%20demonstrate%20how%20convex%20optimization%20can%20enable%20efficient%20on-device%20machine%20learning%20for%20textile%20interfaces.&entry.1838667208=http%3A//arxiv.org/abs/2602.13030v1&entry.124074799=Read"},
{"title": "PromptDepthAnything++: Accurate 4K Metric Depth Estimation via Pattern-Agnostic Prompting", "author": "Haotong Lin and Sida Peng and Qinglin Yang and Peishan Yang and Jiaming Sun and Ruizhen Hu and Kai Xu and Hujun Bao and Bingyi Kang and Xiaowei Zhou", "abstract": "Prompts play a critical role in unleashing the power of language and vision foundation models for specific tasks. For the first time, we introduce prompting into depth foundation models, creating a new paradigm for metric depth estimation termed Prompt Depth Anything. Specifically, we use a low-cost LiDAR as the prompt to guide the Depth Anything model for accurate metric depth output, achieving up to 4K resolution. Our approach centers on a concise prompt fusion design that integrates the LiDAR at multiple scales within the depth decoder. To address training challenges posed by limited datasets containing both LiDAR depth and precise GT depth, we propose a scalable data pipeline that includes synthetic data LiDAR simulation and real data pseudo GT depth generation. To further extend our method to work with any prompt depth points, we propose a new prompting mechanism, which serializes the input depth points into tokens and uses self-attention to enhance image tokens from depth foundation models. Our approach sets new state-of-the-arts on 8 zero-shot depth benchmarks and benefits downstream applications, including 3D reconstruction and generalized robotic grasping. The code is available at https://github.com/DepthAnything/PromptDA .", "link": "http://arxiv.org/abs/2412.14015v3", "date": "2026-02-13", "relevancy": 2.1878, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.552}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PromptDepthAnything%2B%2B%3A%20Accurate%204K%20Metric%20Depth%20Estimation%20via%20Pattern-Agnostic%20Prompting&body=Title%3A%20PromptDepthAnything%2B%2B%3A%20Accurate%204K%20Metric%20Depth%20Estimation%20via%20Pattern-Agnostic%20Prompting%0AAuthor%3A%20Haotong%20Lin%20and%20Sida%20Peng%20and%20Qinglin%20Yang%20and%20Peishan%20Yang%20and%20Jiaming%20Sun%20and%20Ruizhen%20Hu%20and%20Kai%20Xu%20and%20Hujun%20Bao%20and%20Bingyi%20Kang%20and%20Xiaowei%20Zhou%0AAbstract%3A%20Prompts%20play%20a%20critical%20role%20in%20unleashing%20the%20power%20of%20language%20and%20vision%20foundation%20models%20for%20specific%20tasks.%20For%20the%20first%20time%2C%20we%20introduce%20prompting%20into%20depth%20foundation%20models%2C%20creating%20a%20new%20paradigm%20for%20metric%20depth%20estimation%20termed%20Prompt%20Depth%20Anything.%20Specifically%2C%20we%20use%20a%20low-cost%20LiDAR%20as%20the%20prompt%20to%20guide%20the%20Depth%20Anything%20model%20for%20accurate%20metric%20depth%20output%2C%20achieving%20up%20to%204K%20resolution.%20Our%20approach%20centers%20on%20a%20concise%20prompt%20fusion%20design%20that%20integrates%20the%20LiDAR%20at%20multiple%20scales%20within%20the%20depth%20decoder.%20To%20address%20training%20challenges%20posed%20by%20limited%20datasets%20containing%20both%20LiDAR%20depth%20and%20precise%20GT%20depth%2C%20we%20propose%20a%20scalable%20data%20pipeline%20that%20includes%20synthetic%20data%20LiDAR%20simulation%20and%20real%20data%20pseudo%20GT%20depth%20generation.%20To%20further%20extend%20our%20method%20to%20work%20with%20any%20prompt%20depth%20points%2C%20we%20propose%20a%20new%20prompting%20mechanism%2C%20which%20serializes%20the%20input%20depth%20points%20into%20tokens%20and%20uses%20self-attention%20to%20enhance%20image%20tokens%20from%20depth%20foundation%20models.%20Our%20approach%20sets%20new%20state-of-the-arts%20on%208%20zero-shot%20depth%20benchmarks%20and%20benefits%20downstream%20applications%2C%20including%203D%20reconstruction%20and%20generalized%20robotic%20grasping.%20The%20code%20is%20available%20at%20https%3A//github.com/DepthAnything/PromptDA%20.%0ALink%3A%20http%3A//arxiv.org/abs/2412.14015v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPromptDepthAnything%252B%252B%253A%2520Accurate%25204K%2520Metric%2520Depth%2520Estimation%2520via%2520Pattern-Agnostic%2520Prompting%26entry.906535625%3DHaotong%2520Lin%2520and%2520Sida%2520Peng%2520and%2520Qinglin%2520Yang%2520and%2520Peishan%2520Yang%2520and%2520Jiaming%2520Sun%2520and%2520Ruizhen%2520Hu%2520and%2520Kai%2520Xu%2520and%2520Hujun%2520Bao%2520and%2520Bingyi%2520Kang%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3DPrompts%2520play%2520a%2520critical%2520role%2520in%2520unleashing%2520the%2520power%2520of%2520language%2520and%2520vision%2520foundation%2520models%2520for%2520specific%2520tasks.%2520For%2520the%2520first%2520time%252C%2520we%2520introduce%2520prompting%2520into%2520depth%2520foundation%2520models%252C%2520creating%2520a%2520new%2520paradigm%2520for%2520metric%2520depth%2520estimation%2520termed%2520Prompt%2520Depth%2520Anything.%2520Specifically%252C%2520we%2520use%2520a%2520low-cost%2520LiDAR%2520as%2520the%2520prompt%2520to%2520guide%2520the%2520Depth%2520Anything%2520model%2520for%2520accurate%2520metric%2520depth%2520output%252C%2520achieving%2520up%2520to%25204K%2520resolution.%2520Our%2520approach%2520centers%2520on%2520a%2520concise%2520prompt%2520fusion%2520design%2520that%2520integrates%2520the%2520LiDAR%2520at%2520multiple%2520scales%2520within%2520the%2520depth%2520decoder.%2520To%2520address%2520training%2520challenges%2520posed%2520by%2520limited%2520datasets%2520containing%2520both%2520LiDAR%2520depth%2520and%2520precise%2520GT%2520depth%252C%2520we%2520propose%2520a%2520scalable%2520data%2520pipeline%2520that%2520includes%2520synthetic%2520data%2520LiDAR%2520simulation%2520and%2520real%2520data%2520pseudo%2520GT%2520depth%2520generation.%2520To%2520further%2520extend%2520our%2520method%2520to%2520work%2520with%2520any%2520prompt%2520depth%2520points%252C%2520we%2520propose%2520a%2520new%2520prompting%2520mechanism%252C%2520which%2520serializes%2520the%2520input%2520depth%2520points%2520into%2520tokens%2520and%2520uses%2520self-attention%2520to%2520enhance%2520image%2520tokens%2520from%2520depth%2520foundation%2520models.%2520Our%2520approach%2520sets%2520new%2520state-of-the-arts%2520on%25208%2520zero-shot%2520depth%2520benchmarks%2520and%2520benefits%2520downstream%2520applications%252C%2520including%25203D%2520reconstruction%2520and%2520generalized%2520robotic%2520grasping.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/DepthAnything/PromptDA%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14015v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PromptDepthAnything%2B%2B%3A%20Accurate%204K%20Metric%20Depth%20Estimation%20via%20Pattern-Agnostic%20Prompting&entry.906535625=Haotong%20Lin%20and%20Sida%20Peng%20and%20Qinglin%20Yang%20and%20Peishan%20Yang%20and%20Jiaming%20Sun%20and%20Ruizhen%20Hu%20and%20Kai%20Xu%20and%20Hujun%20Bao%20and%20Bingyi%20Kang%20and%20Xiaowei%20Zhou&entry.1292438233=Prompts%20play%20a%20critical%20role%20in%20unleashing%20the%20power%20of%20language%20and%20vision%20foundation%20models%20for%20specific%20tasks.%20For%20the%20first%20time%2C%20we%20introduce%20prompting%20into%20depth%20foundation%20models%2C%20creating%20a%20new%20paradigm%20for%20metric%20depth%20estimation%20termed%20Prompt%20Depth%20Anything.%20Specifically%2C%20we%20use%20a%20low-cost%20LiDAR%20as%20the%20prompt%20to%20guide%20the%20Depth%20Anything%20model%20for%20accurate%20metric%20depth%20output%2C%20achieving%20up%20to%204K%20resolution.%20Our%20approach%20centers%20on%20a%20concise%20prompt%20fusion%20design%20that%20integrates%20the%20LiDAR%20at%20multiple%20scales%20within%20the%20depth%20decoder.%20To%20address%20training%20challenges%20posed%20by%20limited%20datasets%20containing%20both%20LiDAR%20depth%20and%20precise%20GT%20depth%2C%20we%20propose%20a%20scalable%20data%20pipeline%20that%20includes%20synthetic%20data%20LiDAR%20simulation%20and%20real%20data%20pseudo%20GT%20depth%20generation.%20To%20further%20extend%20our%20method%20to%20work%20with%20any%20prompt%20depth%20points%2C%20we%20propose%20a%20new%20prompting%20mechanism%2C%20which%20serializes%20the%20input%20depth%20points%20into%20tokens%20and%20uses%20self-attention%20to%20enhance%20image%20tokens%20from%20depth%20foundation%20models.%20Our%20approach%20sets%20new%20state-of-the-arts%20on%208%20zero-shot%20depth%20benchmarks%20and%20benefits%20downstream%20applications%2C%20including%203D%20reconstruction%20and%20generalized%20robotic%20grasping.%20The%20code%20is%20available%20at%20https%3A//github.com/DepthAnything/PromptDA%20.&entry.1838667208=http%3A//arxiv.org/abs/2412.14015v3&entry.124074799=Read"},
{"title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising", "author": "Yichao Liu and Hengzhi Xue and YueYang Teng and Junwen Guo", "abstract": "Low-dose computed tomography (LDCT) and positron emission tomography (PET) have emerged as safer alternatives to conventional imaging modalities by significantly reducing radiation exposure. However, current approaches often face a trade$-$off between training stability and computational efficiency. In this study, we propose a novel Hybrid Swin Attention Network (HSANet), which incorporates Efficient Global Attention (EGA) modules and a hybrid upsampling module to address these limitations. The EGA modules enhance both spatial and channel-wise interaction, improving the network's capacity to capture relevant features, while the hybrid upsampling module mitigates the risk of overfitting to noise. We validate the proposed approach using a publicly available LDCT/PET dataset. Experimental results demonstrate that HSANet achieves superior denoising performance compared to state of the art methods, while maintaining a lightweight model size suitable for deployment on GPUs with standard memory configurations. Thus, our approach demonstrates significant potential for practical, real-world clinical applications.", "link": "http://arxiv.org/abs/2509.06591v6", "date": "2026-02-13", "relevancy": 2.1703, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5546}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5373}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%20Denoising&body=Title%3A%20Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%20Denoising%0AAuthor%3A%20Yichao%20Liu%20and%20Hengzhi%20Xue%20and%20YueYang%20Teng%20and%20Junwen%20Guo%0AAbstract%3A%20Low-dose%20computed%20tomography%20%28LDCT%29%20and%20positron%20emission%20tomography%20%28PET%29%20have%20emerged%20as%20safer%20alternatives%20to%20conventional%20imaging%20modalities%20by%20significantly%20reducing%20radiation%20exposure.%20However%2C%20current%20approaches%20often%20face%20a%20trade%24-%24off%20between%20training%20stability%20and%20computational%20efficiency.%20In%20this%20study%2C%20we%20propose%20a%20novel%20Hybrid%20Swin%20Attention%20Network%20%28HSANet%29%2C%20which%20incorporates%20Efficient%20Global%20Attention%20%28EGA%29%20modules%20and%20a%20hybrid%20upsampling%20module%20to%20address%20these%20limitations.%20The%20EGA%20modules%20enhance%20both%20spatial%20and%20channel-wise%20interaction%2C%20improving%20the%20network%27s%20capacity%20to%20capture%20relevant%20features%2C%20while%20the%20hybrid%20upsampling%20module%20mitigates%20the%20risk%20of%20overfitting%20to%20noise.%20We%20validate%20the%20proposed%20approach%20using%20a%20publicly%20available%20LDCT/PET%20dataset.%20Experimental%20results%20demonstrate%20that%20HSANet%20achieves%20superior%20denoising%20performance%20compared%20to%20state%20of%20the%20art%20methods%2C%20while%20maintaining%20a%20lightweight%20model%20size%20suitable%20for%20deployment%20on%20GPUs%20with%20standard%20memory%20configurations.%20Thus%2C%20our%20approach%20demonstrates%20significant%20potential%20for%20practical%2C%20real-world%20clinical%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2509.06591v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Swin%2520Attention%2520Networks%2520for%2520Simultaneously%2520Low-Dose%2520PET%2520and%2520CT%2520Denoising%26entry.906535625%3DYichao%2520Liu%2520and%2520Hengzhi%2520Xue%2520and%2520YueYang%2520Teng%2520and%2520Junwen%2520Guo%26entry.1292438233%3DLow-dose%2520computed%2520tomography%2520%2528LDCT%2529%2520and%2520positron%2520emission%2520tomography%2520%2528PET%2529%2520have%2520emerged%2520as%2520safer%2520alternatives%2520to%2520conventional%2520imaging%2520modalities%2520by%2520significantly%2520reducing%2520radiation%2520exposure.%2520However%252C%2520current%2520approaches%2520often%2520face%2520a%2520trade%2524-%2524off%2520between%2520training%2520stability%2520and%2520computational%2520efficiency.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520Hybrid%2520Swin%2520Attention%2520Network%2520%2528HSANet%2529%252C%2520which%2520incorporates%2520Efficient%2520Global%2520Attention%2520%2528EGA%2529%2520modules%2520and%2520a%2520hybrid%2520upsampling%2520module%2520to%2520address%2520these%2520limitations.%2520The%2520EGA%2520modules%2520enhance%2520both%2520spatial%2520and%2520channel-wise%2520interaction%252C%2520improving%2520the%2520network%2527s%2520capacity%2520to%2520capture%2520relevant%2520features%252C%2520while%2520the%2520hybrid%2520upsampling%2520module%2520mitigates%2520the%2520risk%2520of%2520overfitting%2520to%2520noise.%2520We%2520validate%2520the%2520proposed%2520approach%2520using%2520a%2520publicly%2520available%2520LDCT/PET%2520dataset.%2520Experimental%2520results%2520demonstrate%2520that%2520HSANet%2520achieves%2520superior%2520denoising%2520performance%2520compared%2520to%2520state%2520of%2520the%2520art%2520methods%252C%2520while%2520maintaining%2520a%2520lightweight%2520model%2520size%2520suitable%2520for%2520deployment%2520on%2520GPUs%2520with%2520standard%2520memory%2520configurations.%2520Thus%252C%2520our%2520approach%2520demonstrates%2520significant%2520potential%2520for%2520practical%252C%2520real-world%2520clinical%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06591v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%20Denoising&entry.906535625=Yichao%20Liu%20and%20Hengzhi%20Xue%20and%20YueYang%20Teng%20and%20Junwen%20Guo&entry.1292438233=Low-dose%20computed%20tomography%20%28LDCT%29%20and%20positron%20emission%20tomography%20%28PET%29%20have%20emerged%20as%20safer%20alternatives%20to%20conventional%20imaging%20modalities%20by%20significantly%20reducing%20radiation%20exposure.%20However%2C%20current%20approaches%20often%20face%20a%20trade%24-%24off%20between%20training%20stability%20and%20computational%20efficiency.%20In%20this%20study%2C%20we%20propose%20a%20novel%20Hybrid%20Swin%20Attention%20Network%20%28HSANet%29%2C%20which%20incorporates%20Efficient%20Global%20Attention%20%28EGA%29%20modules%20and%20a%20hybrid%20upsampling%20module%20to%20address%20these%20limitations.%20The%20EGA%20modules%20enhance%20both%20spatial%20and%20channel-wise%20interaction%2C%20improving%20the%20network%27s%20capacity%20to%20capture%20relevant%20features%2C%20while%20the%20hybrid%20upsampling%20module%20mitigates%20the%20risk%20of%20overfitting%20to%20noise.%20We%20validate%20the%20proposed%20approach%20using%20a%20publicly%20available%20LDCT/PET%20dataset.%20Experimental%20results%20demonstrate%20that%20HSANet%20achieves%20superior%20denoising%20performance%20compared%20to%20state%20of%20the%20art%20methods%2C%20while%20maintaining%20a%20lightweight%20model%20size%20suitable%20for%20deployment%20on%20GPUs%20with%20standard%20memory%20configurations.%20Thus%2C%20our%20approach%20demonstrates%20significant%20potential%20for%20practical%2C%20real-world%20clinical%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2509.06591v6&entry.124074799=Read"},
{"title": "Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision", "author": "Aadarsh Sahoo and Georgia Gkioxari", "abstract": "Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., \"left-most apple\") and overlooks functional and physical reasoning (e.g., \"where can I safely store the knife?\"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/", "link": "http://arxiv.org/abs/2602.13195v1", "date": "2026-02-13", "relevancy": 2.1646, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conversational%20Image%20Segmentation%3A%20Grounding%20Abstract%20Concepts%20with%20Scalable%20Supervision&body=Title%3A%20Conversational%20Image%20Segmentation%3A%20Grounding%20Abstract%20Concepts%20with%20Scalable%20Supervision%0AAuthor%3A%20Aadarsh%20Sahoo%20and%20Georgia%20Gkioxari%0AAbstract%3A%20Conversational%20image%20segmentation%20grounds%20abstract%2C%20intent-driven%20concepts%20into%20pixel-accurate%20masks.%20Prior%20work%20on%20referring%20image%20grounding%20focuses%20on%20categorical%20and%20spatial%20queries%20%28e.g.%2C%20%22left-most%20apple%22%29%20and%20overlooks%20functional%20and%20physical%20reasoning%20%28e.g.%2C%20%22where%20can%20I%20safely%20store%20the%20knife%3F%22%29.%20We%20address%20this%20gap%20and%20introduce%20Conversational%20Image%20Segmentation%20%28CIS%29%20and%20ConverSeg%2C%20a%20benchmark%20spanning%20entities%2C%20spatial%20relations%2C%20intent%2C%20affordances%2C%20functions%2C%20safety%2C%20and%20physical%20reasoning.%20We%20also%20present%20ConverSeg-Net%2C%20which%20fuses%20strong%20segmentation%20priors%20with%20language%20understanding%2C%20and%20an%20AI-powered%20data%20engine%20that%20generates%20prompt-mask%20pairs%20without%20human%20supervision.%20We%20show%20that%20current%20language-guided%20segmentation%20models%20are%20inadequate%20for%20CIS%2C%20while%20ConverSeg-Net%20trained%20on%20our%20data%20engine%20achieves%20significant%20gains%20on%20ConverSeg%20and%20maintains%20strong%20performance%20on%20existing%20language-guided%20segmentation%20benchmarks.%20Project%20webpage%3A%20https%3A//glab-caltech.github.io/converseg/%0ALink%3A%20http%3A//arxiv.org/abs/2602.13195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConversational%2520Image%2520Segmentation%253A%2520Grounding%2520Abstract%2520Concepts%2520with%2520Scalable%2520Supervision%26entry.906535625%3DAadarsh%2520Sahoo%2520and%2520Georgia%2520Gkioxari%26entry.1292438233%3DConversational%2520image%2520segmentation%2520grounds%2520abstract%252C%2520intent-driven%2520concepts%2520into%2520pixel-accurate%2520masks.%2520Prior%2520work%2520on%2520referring%2520image%2520grounding%2520focuses%2520on%2520categorical%2520and%2520spatial%2520queries%2520%2528e.g.%252C%2520%2522left-most%2520apple%2522%2529%2520and%2520overlooks%2520functional%2520and%2520physical%2520reasoning%2520%2528e.g.%252C%2520%2522where%2520can%2520I%2520safely%2520store%2520the%2520knife%253F%2522%2529.%2520We%2520address%2520this%2520gap%2520and%2520introduce%2520Conversational%2520Image%2520Segmentation%2520%2528CIS%2529%2520and%2520ConverSeg%252C%2520a%2520benchmark%2520spanning%2520entities%252C%2520spatial%2520relations%252C%2520intent%252C%2520affordances%252C%2520functions%252C%2520safety%252C%2520and%2520physical%2520reasoning.%2520We%2520also%2520present%2520ConverSeg-Net%252C%2520which%2520fuses%2520strong%2520segmentation%2520priors%2520with%2520language%2520understanding%252C%2520and%2520an%2520AI-powered%2520data%2520engine%2520that%2520generates%2520prompt-mask%2520pairs%2520without%2520human%2520supervision.%2520We%2520show%2520that%2520current%2520language-guided%2520segmentation%2520models%2520are%2520inadequate%2520for%2520CIS%252C%2520while%2520ConverSeg-Net%2520trained%2520on%2520our%2520data%2520engine%2520achieves%2520significant%2520gains%2520on%2520ConverSeg%2520and%2520maintains%2520strong%2520performance%2520on%2520existing%2520language-guided%2520segmentation%2520benchmarks.%2520Project%2520webpage%253A%2520https%253A//glab-caltech.github.io/converseg/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conversational%20Image%20Segmentation%3A%20Grounding%20Abstract%20Concepts%20with%20Scalable%20Supervision&entry.906535625=Aadarsh%20Sahoo%20and%20Georgia%20Gkioxari&entry.1292438233=Conversational%20image%20segmentation%20grounds%20abstract%2C%20intent-driven%20concepts%20into%20pixel-accurate%20masks.%20Prior%20work%20on%20referring%20image%20grounding%20focuses%20on%20categorical%20and%20spatial%20queries%20%28e.g.%2C%20%22left-most%20apple%22%29%20and%20overlooks%20functional%20and%20physical%20reasoning%20%28e.g.%2C%20%22where%20can%20I%20safely%20store%20the%20knife%3F%22%29.%20We%20address%20this%20gap%20and%20introduce%20Conversational%20Image%20Segmentation%20%28CIS%29%20and%20ConverSeg%2C%20a%20benchmark%20spanning%20entities%2C%20spatial%20relations%2C%20intent%2C%20affordances%2C%20functions%2C%20safety%2C%20and%20physical%20reasoning.%20We%20also%20present%20ConverSeg-Net%2C%20which%20fuses%20strong%20segmentation%20priors%20with%20language%20understanding%2C%20and%20an%20AI-powered%20data%20engine%20that%20generates%20prompt-mask%20pairs%20without%20human%20supervision.%20We%20show%20that%20current%20language-guided%20segmentation%20models%20are%20inadequate%20for%20CIS%2C%20while%20ConverSeg-Net%20trained%20on%20our%20data%20engine%20achieves%20significant%20gains%20on%20ConverSeg%20and%20maintains%20strong%20performance%20on%20existing%20language-guided%20segmentation%20benchmarks.%20Project%20webpage%3A%20https%3A//glab-caltech.github.io/converseg/&entry.1838667208=http%3A//arxiv.org/abs/2602.13195v1&entry.124074799=Read"},
{"title": "Batch-CAM: Introduction to better reasoning in convolutional deep learning models", "author": "Giacomo Ignesti and Davide Moroni and Massimo Martinelli", "abstract": "Deep learning opacity often impedes deployment in high-stakes domains. We propose a training framework that aligns model focus with class-representative features without requiring pixel-level annotations. To this end, we introduce Batch-CAM, a vectorised implementation of Gradient-weighted Class Activation Mapping that integrates directly into the training loop with minimal computational overhead. We propose two regularisation terms: a Prototype Loss, which aligns individual-sample attention with the global class average, and a Batch-CAM Loss, which enforces consistency within a training batch. These are evaluated using L1, L2, and SSIM metrics. Validated on MNIST and Fashion-MNIST using ResNet18 and ConvNeXt-V2, our method generates significantly more coherent and human-interpretable saliency maps compared to baselines. While maintaining competitive classification accuracy, the framework successfully suppresses spurious feature activation, as evidenced by qualitative reconstruction analysis. Batch-CAM appears to offer a scalable pathway for training intrinsically interpretable models by leveraging batch-level statistics to guide feature extraction, effectively bridging the gap between predictive performance and explainability.", "link": "http://arxiv.org/abs/2510.00664v2", "date": "2026-02-13", "relevancy": 2.1598, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.57}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5344}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Batch-CAM%3A%20Introduction%20to%20better%20reasoning%20in%20convolutional%20deep%20learning%20models&body=Title%3A%20Batch-CAM%3A%20Introduction%20to%20better%20reasoning%20in%20convolutional%20deep%20learning%20models%0AAuthor%3A%20Giacomo%20Ignesti%20and%20Davide%20Moroni%20and%20Massimo%20Martinelli%0AAbstract%3A%20Deep%20learning%20opacity%20often%20impedes%20deployment%20in%20high-stakes%20domains.%20We%20propose%20a%20training%20framework%20that%20aligns%20model%20focus%20with%20class-representative%20features%20without%20requiring%20pixel-level%20annotations.%20To%20this%20end%2C%20we%20introduce%20Batch-CAM%2C%20a%20vectorised%20implementation%20of%20Gradient-weighted%20Class%20Activation%20Mapping%20that%20integrates%20directly%20into%20the%20training%20loop%20with%20minimal%20computational%20overhead.%20We%20propose%20two%20regularisation%20terms%3A%20a%20Prototype%20Loss%2C%20which%20aligns%20individual-sample%20attention%20with%20the%20global%20class%20average%2C%20and%20a%20Batch-CAM%20Loss%2C%20which%20enforces%20consistency%20within%20a%20training%20batch.%20These%20are%20evaluated%20using%20L1%2C%20L2%2C%20and%20SSIM%20metrics.%20Validated%20on%20MNIST%20and%20Fashion-MNIST%20using%20ResNet18%20and%20ConvNeXt-V2%2C%20our%20method%20generates%20significantly%20more%20coherent%20and%20human-interpretable%20saliency%20maps%20compared%20to%20baselines.%20While%20maintaining%20competitive%20classification%20accuracy%2C%20the%20framework%20successfully%20suppresses%20spurious%20feature%20activation%2C%20as%20evidenced%20by%20qualitative%20reconstruction%20analysis.%20Batch-CAM%20appears%20to%20offer%20a%20scalable%20pathway%20for%20training%20intrinsically%20interpretable%20models%20by%20leveraging%20batch-level%20statistics%20to%20guide%20feature%20extraction%2C%20effectively%20bridging%20the%20gap%20between%20predictive%20performance%20and%20explainability.%0ALink%3A%20http%3A//arxiv.org/abs/2510.00664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBatch-CAM%253A%2520Introduction%2520to%2520better%2520reasoning%2520in%2520convolutional%2520deep%2520learning%2520models%26entry.906535625%3DGiacomo%2520Ignesti%2520and%2520Davide%2520Moroni%2520and%2520Massimo%2520Martinelli%26entry.1292438233%3DDeep%2520learning%2520opacity%2520often%2520impedes%2520deployment%2520in%2520high-stakes%2520domains.%2520We%2520propose%2520a%2520training%2520framework%2520that%2520aligns%2520model%2520focus%2520with%2520class-representative%2520features%2520without%2520requiring%2520pixel-level%2520annotations.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Batch-CAM%252C%2520a%2520vectorised%2520implementation%2520of%2520Gradient-weighted%2520Class%2520Activation%2520Mapping%2520that%2520integrates%2520directly%2520into%2520the%2520training%2520loop%2520with%2520minimal%2520computational%2520overhead.%2520We%2520propose%2520two%2520regularisation%2520terms%253A%2520a%2520Prototype%2520Loss%252C%2520which%2520aligns%2520individual-sample%2520attention%2520with%2520the%2520global%2520class%2520average%252C%2520and%2520a%2520Batch-CAM%2520Loss%252C%2520which%2520enforces%2520consistency%2520within%2520a%2520training%2520batch.%2520These%2520are%2520evaluated%2520using%2520L1%252C%2520L2%252C%2520and%2520SSIM%2520metrics.%2520Validated%2520on%2520MNIST%2520and%2520Fashion-MNIST%2520using%2520ResNet18%2520and%2520ConvNeXt-V2%252C%2520our%2520method%2520generates%2520significantly%2520more%2520coherent%2520and%2520human-interpretable%2520saliency%2520maps%2520compared%2520to%2520baselines.%2520While%2520maintaining%2520competitive%2520classification%2520accuracy%252C%2520the%2520framework%2520successfully%2520suppresses%2520spurious%2520feature%2520activation%252C%2520as%2520evidenced%2520by%2520qualitative%2520reconstruction%2520analysis.%2520Batch-CAM%2520appears%2520to%2520offer%2520a%2520scalable%2520pathway%2520for%2520training%2520intrinsically%2520interpretable%2520models%2520by%2520leveraging%2520batch-level%2520statistics%2520to%2520guide%2520feature%2520extraction%252C%2520effectively%2520bridging%2520the%2520gap%2520between%2520predictive%2520performance%2520and%2520explainability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Batch-CAM%3A%20Introduction%20to%20better%20reasoning%20in%20convolutional%20deep%20learning%20models&entry.906535625=Giacomo%20Ignesti%20and%20Davide%20Moroni%20and%20Massimo%20Martinelli&entry.1292438233=Deep%20learning%20opacity%20often%20impedes%20deployment%20in%20high-stakes%20domains.%20We%20propose%20a%20training%20framework%20that%20aligns%20model%20focus%20with%20class-representative%20features%20without%20requiring%20pixel-level%20annotations.%20To%20this%20end%2C%20we%20introduce%20Batch-CAM%2C%20a%20vectorised%20implementation%20of%20Gradient-weighted%20Class%20Activation%20Mapping%20that%20integrates%20directly%20into%20the%20training%20loop%20with%20minimal%20computational%20overhead.%20We%20propose%20two%20regularisation%20terms%3A%20a%20Prototype%20Loss%2C%20which%20aligns%20individual-sample%20attention%20with%20the%20global%20class%20average%2C%20and%20a%20Batch-CAM%20Loss%2C%20which%20enforces%20consistency%20within%20a%20training%20batch.%20These%20are%20evaluated%20using%20L1%2C%20L2%2C%20and%20SSIM%20metrics.%20Validated%20on%20MNIST%20and%20Fashion-MNIST%20using%20ResNet18%20and%20ConvNeXt-V2%2C%20our%20method%20generates%20significantly%20more%20coherent%20and%20human-interpretable%20saliency%20maps%20compared%20to%20baselines.%20While%20maintaining%20competitive%20classification%20accuracy%2C%20the%20framework%20successfully%20suppresses%20spurious%20feature%20activation%2C%20as%20evidenced%20by%20qualitative%20reconstruction%20analysis.%20Batch-CAM%20appears%20to%20offer%20a%20scalable%20pathway%20for%20training%20intrinsically%20interpretable%20models%20by%20leveraging%20batch-level%20statistics%20to%20guide%20feature%20extraction%2C%20effectively%20bridging%20the%20gap%20between%20predictive%20performance%20and%20explainability.&entry.1838667208=http%3A//arxiv.org/abs/2510.00664v2&entry.124074799=Read"},
{"title": "Universal Transformation of One-Class Classifiers for Unsupervised Anomaly Detection", "author": "Declan McIntosh and Alexandra Branzan Albu", "abstract": "Detecting anomalies in images and video is an essential task for multiple real-world problems, including industrial inspection, computer-assisted diagnosis, and environmental monitoring. Anomaly detection is typically formulated as a one-class classification problem, where the training data consists solely of nominal values, leaving methods built on this assumption susceptible to training label noise. We present a dataset folding method that transforms an arbitrary one-class classifier-based anomaly detector into a fully unsupervised method. This is achieved by making a set of key weak assumptions: that anomalies are uncommon in the training dataset and generally heterogeneous. These assumptions enable us to utilize multiple independently trained instances of a one-class classifier to filter the training dataset for anomalies. This transformation requires no modifications to the underlying anomaly detector; the only changes are algorithmically selected data subsets used for training. We demonstrate that our method can transform a wide variety of one-class classifier anomaly detectors for both images and videos into unsupervised ones. Our method creates the first unsupervised logical anomaly detectors by transforming existing methods. We also demonstrate that our method achieves state-of-the-art performance for unsupervised anomaly detection on the MVTec AD, ViSA, and MVTec Loco AD datasets. As improvements to one-class classifiers are made, our method directly transfers those improvements to the unsupervised domain, linking the domains.", "link": "http://arxiv.org/abs/2602.13091v1", "date": "2026-02-13", "relevancy": 2.1379, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5548}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5202}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Transformation%20of%20One-Class%20Classifiers%20for%20Unsupervised%20Anomaly%20Detection&body=Title%3A%20Universal%20Transformation%20of%20One-Class%20Classifiers%20for%20Unsupervised%20Anomaly%20Detection%0AAuthor%3A%20Declan%20McIntosh%20and%20Alexandra%20Branzan%20Albu%0AAbstract%3A%20Detecting%20anomalies%20in%20images%20and%20video%20is%20an%20essential%20task%20for%20multiple%20real-world%20problems%2C%20including%20industrial%20inspection%2C%20computer-assisted%20diagnosis%2C%20and%20environmental%20monitoring.%20Anomaly%20detection%20is%20typically%20formulated%20as%20a%20one-class%20classification%20problem%2C%20where%20the%20training%20data%20consists%20solely%20of%20nominal%20values%2C%20leaving%20methods%20built%20on%20this%20assumption%20susceptible%20to%20training%20label%20noise.%20We%20present%20a%20dataset%20folding%20method%20that%20transforms%20an%20arbitrary%20one-class%20classifier-based%20anomaly%20detector%20into%20a%20fully%20unsupervised%20method.%20This%20is%20achieved%20by%20making%20a%20set%20of%20key%20weak%20assumptions%3A%20that%20anomalies%20are%20uncommon%20in%20the%20training%20dataset%20and%20generally%20heterogeneous.%20These%20assumptions%20enable%20us%20to%20utilize%20multiple%20independently%20trained%20instances%20of%20a%20one-class%20classifier%20to%20filter%20the%20training%20dataset%20for%20anomalies.%20This%20transformation%20requires%20no%20modifications%20to%20the%20underlying%20anomaly%20detector%3B%20the%20only%20changes%20are%20algorithmically%20selected%20data%20subsets%20used%20for%20training.%20We%20demonstrate%20that%20our%20method%20can%20transform%20a%20wide%20variety%20of%20one-class%20classifier%20anomaly%20detectors%20for%20both%20images%20and%20videos%20into%20unsupervised%20ones.%20Our%20method%20creates%20the%20first%20unsupervised%20logical%20anomaly%20detectors%20by%20transforming%20existing%20methods.%20We%20also%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20for%20unsupervised%20anomaly%20detection%20on%20the%20MVTec%20AD%2C%20ViSA%2C%20and%20MVTec%20Loco%20AD%20datasets.%20As%20improvements%20to%20one-class%20classifiers%20are%20made%2C%20our%20method%20directly%20transfers%20those%20improvements%20to%20the%20unsupervised%20domain%2C%20linking%20the%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Transformation%2520of%2520One-Class%2520Classifiers%2520for%2520Unsupervised%2520Anomaly%2520Detection%26entry.906535625%3DDeclan%2520McIntosh%2520and%2520Alexandra%2520Branzan%2520Albu%26entry.1292438233%3DDetecting%2520anomalies%2520in%2520images%2520and%2520video%2520is%2520an%2520essential%2520task%2520for%2520multiple%2520real-world%2520problems%252C%2520including%2520industrial%2520inspection%252C%2520computer-assisted%2520diagnosis%252C%2520and%2520environmental%2520monitoring.%2520Anomaly%2520detection%2520is%2520typically%2520formulated%2520as%2520a%2520one-class%2520classification%2520problem%252C%2520where%2520the%2520training%2520data%2520consists%2520solely%2520of%2520nominal%2520values%252C%2520leaving%2520methods%2520built%2520on%2520this%2520assumption%2520susceptible%2520to%2520training%2520label%2520noise.%2520We%2520present%2520a%2520dataset%2520folding%2520method%2520that%2520transforms%2520an%2520arbitrary%2520one-class%2520classifier-based%2520anomaly%2520detector%2520into%2520a%2520fully%2520unsupervised%2520method.%2520This%2520is%2520achieved%2520by%2520making%2520a%2520set%2520of%2520key%2520weak%2520assumptions%253A%2520that%2520anomalies%2520are%2520uncommon%2520in%2520the%2520training%2520dataset%2520and%2520generally%2520heterogeneous.%2520These%2520assumptions%2520enable%2520us%2520to%2520utilize%2520multiple%2520independently%2520trained%2520instances%2520of%2520a%2520one-class%2520classifier%2520to%2520filter%2520the%2520training%2520dataset%2520for%2520anomalies.%2520This%2520transformation%2520requires%2520no%2520modifications%2520to%2520the%2520underlying%2520anomaly%2520detector%253B%2520the%2520only%2520changes%2520are%2520algorithmically%2520selected%2520data%2520subsets%2520used%2520for%2520training.%2520We%2520demonstrate%2520that%2520our%2520method%2520can%2520transform%2520a%2520wide%2520variety%2520of%2520one-class%2520classifier%2520anomaly%2520detectors%2520for%2520both%2520images%2520and%2520videos%2520into%2520unsupervised%2520ones.%2520Our%2520method%2520creates%2520the%2520first%2520unsupervised%2520logical%2520anomaly%2520detectors%2520by%2520transforming%2520existing%2520methods.%2520We%2520also%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520for%2520unsupervised%2520anomaly%2520detection%2520on%2520the%2520MVTec%2520AD%252C%2520ViSA%252C%2520and%2520MVTec%2520Loco%2520AD%2520datasets.%2520As%2520improvements%2520to%2520one-class%2520classifiers%2520are%2520made%252C%2520our%2520method%2520directly%2520transfers%2520those%2520improvements%2520to%2520the%2520unsupervised%2520domain%252C%2520linking%2520the%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Transformation%20of%20One-Class%20Classifiers%20for%20Unsupervised%20Anomaly%20Detection&entry.906535625=Declan%20McIntosh%20and%20Alexandra%20Branzan%20Albu&entry.1292438233=Detecting%20anomalies%20in%20images%20and%20video%20is%20an%20essential%20task%20for%20multiple%20real-world%20problems%2C%20including%20industrial%20inspection%2C%20computer-assisted%20diagnosis%2C%20and%20environmental%20monitoring.%20Anomaly%20detection%20is%20typically%20formulated%20as%20a%20one-class%20classification%20problem%2C%20where%20the%20training%20data%20consists%20solely%20of%20nominal%20values%2C%20leaving%20methods%20built%20on%20this%20assumption%20susceptible%20to%20training%20label%20noise.%20We%20present%20a%20dataset%20folding%20method%20that%20transforms%20an%20arbitrary%20one-class%20classifier-based%20anomaly%20detector%20into%20a%20fully%20unsupervised%20method.%20This%20is%20achieved%20by%20making%20a%20set%20of%20key%20weak%20assumptions%3A%20that%20anomalies%20are%20uncommon%20in%20the%20training%20dataset%20and%20generally%20heterogeneous.%20These%20assumptions%20enable%20us%20to%20utilize%20multiple%20independently%20trained%20instances%20of%20a%20one-class%20classifier%20to%20filter%20the%20training%20dataset%20for%20anomalies.%20This%20transformation%20requires%20no%20modifications%20to%20the%20underlying%20anomaly%20detector%3B%20the%20only%20changes%20are%20algorithmically%20selected%20data%20subsets%20used%20for%20training.%20We%20demonstrate%20that%20our%20method%20can%20transform%20a%20wide%20variety%20of%20one-class%20classifier%20anomaly%20detectors%20for%20both%20images%20and%20videos%20into%20unsupervised%20ones.%20Our%20method%20creates%20the%20first%20unsupervised%20logical%20anomaly%20detectors%20by%20transforming%20existing%20methods.%20We%20also%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20for%20unsupervised%20anomaly%20detection%20on%20the%20MVTec%20AD%2C%20ViSA%2C%20and%20MVTec%20Loco%20AD%20datasets.%20As%20improvements%20to%20one-class%20classifiers%20are%20made%2C%20our%20method%20directly%20transfers%20those%20improvements%20to%20the%20unsupervised%20domain%2C%20linking%20the%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2602.13091v1&entry.124074799=Read"},
{"title": "Diffusion-Pretrained Dense and Contextual Embeddings", "author": "Sedigheh Eslami and Maksim Gaiduk and Markus Krimmel and Louis Milliken and Bo Wang and Denis Bykov", "abstract": "In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunking strategy to better preserve global context across long documents. We release two model types: pplx-embed-v1 for standard retrieval, and pplx-embed-context-v1 for contextualized embeddings that incorporate global document context into passage representations. pplx-embed-v1 achieves competitive performance on the MTEB(Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet retrieval benchmarks, while pplx-embed-context-v1 sets new records on the ConTEB benchmark. Beyond public benchmarks, pplx-embed-v1 demonstrates strong performance on our internal evaluation suite, focusing on real-world, large-scale search scenarios constructed from 1B production web pages. These results validate the models' effectiveness in production environments where retrieval quality and efficiency are critical at scale.", "link": "http://arxiv.org/abs/2602.11151v2", "date": "2026-02-13", "relevancy": 2.1372, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-Pretrained%20Dense%20and%20Contextual%20Embeddings&body=Title%3A%20Diffusion-Pretrained%20Dense%20and%20Contextual%20Embeddings%0AAuthor%3A%20Sedigheh%20Eslami%20and%20Maksim%20Gaiduk%20and%20Markus%20Krimmel%20and%20Louis%20Milliken%20and%20Bo%20Wang%20and%20Denis%20Bykov%0AAbstract%3A%20In%20this%20report%2C%20we%20introduce%20pplx-embed%2C%20a%20family%20of%20multilingual%20embedding%20models%20that%20employ%20multi-stage%20contrastive%20learning%20on%20a%20diffusion-pretrained%20language%20model%20backbone%20for%20web-scale%20retrieval.%20By%20leveraging%20bidirectional%20attention%20through%20diffusion-based%20pretraining%2C%20our%20models%20capture%20comprehensive%20bidirectional%20context%20within%20passages%2C%20enabling%20the%20use%20of%20mean%20pooling%20and%20a%20late%20chunking%20strategy%20to%20better%20preserve%20global%20context%20across%20long%20documents.%20We%20release%20two%20model%20types%3A%20pplx-embed-v1%20for%20standard%20retrieval%2C%20and%20pplx-embed-context-v1%20for%20contextualized%20embeddings%20that%20incorporate%20global%20document%20context%20into%20passage%20representations.%20pplx-embed-v1%20achieves%20competitive%20performance%20on%20the%20MTEB%28Multilingual%2C%20v2%29%2C%20MTEB%28Code%29%2C%20MIRACL%2C%20BERGEN%2C%20and%20ToolRet%20retrieval%20benchmarks%2C%20while%20pplx-embed-context-v1%20sets%20new%20records%20on%20the%20ConTEB%20benchmark.%20Beyond%20public%20benchmarks%2C%20pplx-embed-v1%20demonstrates%20strong%20performance%20on%20our%20internal%20evaluation%20suite%2C%20focusing%20on%20real-world%2C%20large-scale%20search%20scenarios%20constructed%20from%201B%20production%20web%20pages.%20These%20results%20validate%20the%20models%27%20effectiveness%20in%20production%20environments%20where%20retrieval%20quality%20and%20efficiency%20are%20critical%20at%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-Pretrained%2520Dense%2520and%2520Contextual%2520Embeddings%26entry.906535625%3DSedigheh%2520Eslami%2520and%2520Maksim%2520Gaiduk%2520and%2520Markus%2520Krimmel%2520and%2520Louis%2520Milliken%2520and%2520Bo%2520Wang%2520and%2520Denis%2520Bykov%26entry.1292438233%3DIn%2520this%2520report%252C%2520we%2520introduce%2520pplx-embed%252C%2520a%2520family%2520of%2520multilingual%2520embedding%2520models%2520that%2520employ%2520multi-stage%2520contrastive%2520learning%2520on%2520a%2520diffusion-pretrained%2520language%2520model%2520backbone%2520for%2520web-scale%2520retrieval.%2520By%2520leveraging%2520bidirectional%2520attention%2520through%2520diffusion-based%2520pretraining%252C%2520our%2520models%2520capture%2520comprehensive%2520bidirectional%2520context%2520within%2520passages%252C%2520enabling%2520the%2520use%2520of%2520mean%2520pooling%2520and%2520a%2520late%2520chunking%2520strategy%2520to%2520better%2520preserve%2520global%2520context%2520across%2520long%2520documents.%2520We%2520release%2520two%2520model%2520types%253A%2520pplx-embed-v1%2520for%2520standard%2520retrieval%252C%2520and%2520pplx-embed-context-v1%2520for%2520contextualized%2520embeddings%2520that%2520incorporate%2520global%2520document%2520context%2520into%2520passage%2520representations.%2520pplx-embed-v1%2520achieves%2520competitive%2520performance%2520on%2520the%2520MTEB%2528Multilingual%252C%2520v2%2529%252C%2520MTEB%2528Code%2529%252C%2520MIRACL%252C%2520BERGEN%252C%2520and%2520ToolRet%2520retrieval%2520benchmarks%252C%2520while%2520pplx-embed-context-v1%2520sets%2520new%2520records%2520on%2520the%2520ConTEB%2520benchmark.%2520Beyond%2520public%2520benchmarks%252C%2520pplx-embed-v1%2520demonstrates%2520strong%2520performance%2520on%2520our%2520internal%2520evaluation%2520suite%252C%2520focusing%2520on%2520real-world%252C%2520large-scale%2520search%2520scenarios%2520constructed%2520from%25201B%2520production%2520web%2520pages.%2520These%2520results%2520validate%2520the%2520models%2527%2520effectiveness%2520in%2520production%2520environments%2520where%2520retrieval%2520quality%2520and%2520efficiency%2520are%2520critical%2520at%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Pretrained%20Dense%20and%20Contextual%20Embeddings&entry.906535625=Sedigheh%20Eslami%20and%20Maksim%20Gaiduk%20and%20Markus%20Krimmel%20and%20Louis%20Milliken%20and%20Bo%20Wang%20and%20Denis%20Bykov&entry.1292438233=In%20this%20report%2C%20we%20introduce%20pplx-embed%2C%20a%20family%20of%20multilingual%20embedding%20models%20that%20employ%20multi-stage%20contrastive%20learning%20on%20a%20diffusion-pretrained%20language%20model%20backbone%20for%20web-scale%20retrieval.%20By%20leveraging%20bidirectional%20attention%20through%20diffusion-based%20pretraining%2C%20our%20models%20capture%20comprehensive%20bidirectional%20context%20within%20passages%2C%20enabling%20the%20use%20of%20mean%20pooling%20and%20a%20late%20chunking%20strategy%20to%20better%20preserve%20global%20context%20across%20long%20documents.%20We%20release%20two%20model%20types%3A%20pplx-embed-v1%20for%20standard%20retrieval%2C%20and%20pplx-embed-context-v1%20for%20contextualized%20embeddings%20that%20incorporate%20global%20document%20context%20into%20passage%20representations.%20pplx-embed-v1%20achieves%20competitive%20performance%20on%20the%20MTEB%28Multilingual%2C%20v2%29%2C%20MTEB%28Code%29%2C%20MIRACL%2C%20BERGEN%2C%20and%20ToolRet%20retrieval%20benchmarks%2C%20while%20pplx-embed-context-v1%20sets%20new%20records%20on%20the%20ConTEB%20benchmark.%20Beyond%20public%20benchmarks%2C%20pplx-embed-v1%20demonstrates%20strong%20performance%20on%20our%20internal%20evaluation%20suite%2C%20focusing%20on%20real-world%2C%20large-scale%20search%20scenarios%20constructed%20from%201B%20production%20web%20pages.%20These%20results%20validate%20the%20models%27%20effectiveness%20in%20production%20environments%20where%20retrieval%20quality%20and%20efficiency%20are%20critical%20at%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2602.11151v2&entry.124074799=Read"},
{"title": "MDAFNet: Multiscale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection", "author": "Shuying Li and Qiang Ma and San Zhang and Wuwei Wang and Chuang Yang", "abstract": "Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network's capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.", "link": "http://arxiv.org/abs/2601.16434v2", "date": "2026-02-13", "relevancy": 2.1096, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5276}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5275}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDAFNet%3A%20Multiscale%20Differential%20Edge%20and%20Adaptive%20Frequency%20Guided%20Network%20for%20Infrared%20Small%20Target%20Detection&body=Title%3A%20MDAFNet%3A%20Multiscale%20Differential%20Edge%20and%20Adaptive%20Frequency%20Guided%20Network%20for%20Infrared%20Small%20Target%20Detection%0AAuthor%3A%20Shuying%20Li%20and%20Qiang%20Ma%20and%20San%20Zhang%20and%20Wuwei%20Wang%20and%20Chuang%20Yang%0AAbstract%3A%20Infrared%20small%20target%20detection%20%28IRSTD%29%20plays%20a%20crucial%20role%20in%20numerous%20military%20and%20civilian%20applications.%20However%2C%20existing%20methods%20often%20face%20the%20gradual%20degradation%20of%20target%20edge%20pixels%20as%20the%20number%20of%20network%20layers%20increases%2C%20and%20traditional%20convolution%20struggles%20to%20differentiate%20between%20frequency%20components%20during%20feature%20extraction%2C%20leading%20to%20low-frequency%20backgrounds%20interfering%20with%20high-frequency%20targets%20and%20high-frequency%20noise%20triggering%20false%20detections.%20To%20address%20these%20limitations%2C%20we%20propose%20MDAFNet%20%28Multi-scale%20Differential%20Edge%20and%20Adaptive%20Frequency%20Guided%20Network%20for%20Infrared%20Small%20Target%20Detection%29%2C%20which%20integrates%20the%20Multi-Scale%20Differential%20Edge%20%28MSDE%29%20module%20and%20Dual-Domain%20Adaptive%20Feature%20Enhancement%20%28DAFE%29%20module.%20The%20MSDE%20module%2C%20through%20a%20multi-scale%20edge%20extraction%20and%20enhancement%20mechanism%2C%20effectively%20compensates%20for%20the%20cumulative%20loss%20of%20target%20edge%20information%20during%20downsampling.%20The%20DAFE%20module%20combines%20frequency%20domain%20processing%20mechanisms%20with%20simulated%20frequency%20decomposition%20and%20fusion%20mechanisms%20in%20the%20spatial%20domain%20to%20effectively%20improve%20the%20network%27s%20capability%20to%20adaptively%20enhance%20high-frequency%20targets%20and%20selectively%20suppress%20high-frequency%20noise.%20Experimental%20results%20on%20multiple%20datasets%20demonstrate%20the%20superior%20detection%20performance%20of%20MDAFNet.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDAFNet%253A%2520Multiscale%2520Differential%2520Edge%2520and%2520Adaptive%2520Frequency%2520Guided%2520Network%2520for%2520Infrared%2520Small%2520Target%2520Detection%26entry.906535625%3DShuying%2520Li%2520and%2520Qiang%2520Ma%2520and%2520San%2520Zhang%2520and%2520Wuwei%2520Wang%2520and%2520Chuang%2520Yang%26entry.1292438233%3DInfrared%2520small%2520target%2520detection%2520%2528IRSTD%2529%2520plays%2520a%2520crucial%2520role%2520in%2520numerous%2520military%2520and%2520civilian%2520applications.%2520However%252C%2520existing%2520methods%2520often%2520face%2520the%2520gradual%2520degradation%2520of%2520target%2520edge%2520pixels%2520as%2520the%2520number%2520of%2520network%2520layers%2520increases%252C%2520and%2520traditional%2520convolution%2520struggles%2520to%2520differentiate%2520between%2520frequency%2520components%2520during%2520feature%2520extraction%252C%2520leading%2520to%2520low-frequency%2520backgrounds%2520interfering%2520with%2520high-frequency%2520targets%2520and%2520high-frequency%2520noise%2520triggering%2520false%2520detections.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520MDAFNet%2520%2528Multi-scale%2520Differential%2520Edge%2520and%2520Adaptive%2520Frequency%2520Guided%2520Network%2520for%2520Infrared%2520Small%2520Target%2520Detection%2529%252C%2520which%2520integrates%2520the%2520Multi-Scale%2520Differential%2520Edge%2520%2528MSDE%2529%2520module%2520and%2520Dual-Domain%2520Adaptive%2520Feature%2520Enhancement%2520%2528DAFE%2529%2520module.%2520The%2520MSDE%2520module%252C%2520through%2520a%2520multi-scale%2520edge%2520extraction%2520and%2520enhancement%2520mechanism%252C%2520effectively%2520compensates%2520for%2520the%2520cumulative%2520loss%2520of%2520target%2520edge%2520information%2520during%2520downsampling.%2520The%2520DAFE%2520module%2520combines%2520frequency%2520domain%2520processing%2520mechanisms%2520with%2520simulated%2520frequency%2520decomposition%2520and%2520fusion%2520mechanisms%2520in%2520the%2520spatial%2520domain%2520to%2520effectively%2520improve%2520the%2520network%2527s%2520capability%2520to%2520adaptively%2520enhance%2520high-frequency%2520targets%2520and%2520selectively%2520suppress%2520high-frequency%2520noise.%2520Experimental%2520results%2520on%2520multiple%2520datasets%2520demonstrate%2520the%2520superior%2520detection%2520performance%2520of%2520MDAFNet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDAFNet%3A%20Multiscale%20Differential%20Edge%20and%20Adaptive%20Frequency%20Guided%20Network%20for%20Infrared%20Small%20Target%20Detection&entry.906535625=Shuying%20Li%20and%20Qiang%20Ma%20and%20San%20Zhang%20and%20Wuwei%20Wang%20and%20Chuang%20Yang&entry.1292438233=Infrared%20small%20target%20detection%20%28IRSTD%29%20plays%20a%20crucial%20role%20in%20numerous%20military%20and%20civilian%20applications.%20However%2C%20existing%20methods%20often%20face%20the%20gradual%20degradation%20of%20target%20edge%20pixels%20as%20the%20number%20of%20network%20layers%20increases%2C%20and%20traditional%20convolution%20struggles%20to%20differentiate%20between%20frequency%20components%20during%20feature%20extraction%2C%20leading%20to%20low-frequency%20backgrounds%20interfering%20with%20high-frequency%20targets%20and%20high-frequency%20noise%20triggering%20false%20detections.%20To%20address%20these%20limitations%2C%20we%20propose%20MDAFNet%20%28Multi-scale%20Differential%20Edge%20and%20Adaptive%20Frequency%20Guided%20Network%20for%20Infrared%20Small%20Target%20Detection%29%2C%20which%20integrates%20the%20Multi-Scale%20Differential%20Edge%20%28MSDE%29%20module%20and%20Dual-Domain%20Adaptive%20Feature%20Enhancement%20%28DAFE%29%20module.%20The%20MSDE%20module%2C%20through%20a%20multi-scale%20edge%20extraction%20and%20enhancement%20mechanism%2C%20effectively%20compensates%20for%20the%20cumulative%20loss%20of%20target%20edge%20information%20during%20downsampling.%20The%20DAFE%20module%20combines%20frequency%20domain%20processing%20mechanisms%20with%20simulated%20frequency%20decomposition%20and%20fusion%20mechanisms%20in%20the%20spatial%20domain%20to%20effectively%20improve%20the%20network%27s%20capability%20to%20adaptively%20enhance%20high-frequency%20targets%20and%20selectively%20suppress%20high-frequency%20noise.%20Experimental%20results%20on%20multiple%20datasets%20demonstrate%20the%20superior%20detection%20performance%20of%20MDAFNet.&entry.1838667208=http%3A//arxiv.org/abs/2601.16434v2&entry.124074799=Read"},
{"title": "3DLAND: 3D Lesion Abdominal Anomaly Localization Dataset", "author": "Mehran Advand and Zahra Dehghanian and Navid Faraji and Reza Barati and Seyed Amir Ahmad Safavi-Naini and Hamid R. Rabiee", "abstract": "Existing medical imaging datasets for abdominal CT often lack three-dimensional annotations, multi-organ coverage, or precise lesion-to-organ associations, hindering robust representation learning and clinical applications. To address this gap, we introduce 3DLAND, a large-scale benchmark dataset comprising over 6,000 contrast-enhanced CT volumes with over 20,000 high-fidelity 3D lesion annotations linked to seven abdominal organs: liver, kidneys, pancreas, spleen, stomach, and gallbladder. Our streamlined three-phase pipeline integrates automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores exceeding 0.75. By providing diverse lesion types and patient demographics, 3DLAND enables scalable evaluation of anomaly detection, localization, and cross-organ transfer learning for medical AI. Our dataset establishes a new benchmark for evaluating organ-aware 3D segmentation models, paving the way for advancements in healthcare-oriented AI. To facilitate reproducibility and further research, the 3DLAND dataset and implementation code are publicly available at https://mehrn79.github.io/3DLAND.", "link": "http://arxiv.org/abs/2602.12820v1", "date": "2026-02-13", "relevancy": 2.1071, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5246}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DLAND%3A%203D%20Lesion%20Abdominal%20Anomaly%20Localization%20Dataset&body=Title%3A%203DLAND%3A%203D%20Lesion%20Abdominal%20Anomaly%20Localization%20Dataset%0AAuthor%3A%20Mehran%20Advand%20and%20Zahra%20Dehghanian%20and%20Navid%20Faraji%20and%20Reza%20Barati%20and%20Seyed%20Amir%20Ahmad%20Safavi-Naini%20and%20Hamid%20R.%20Rabiee%0AAbstract%3A%20Existing%20medical%20imaging%20datasets%20for%20abdominal%20CT%20often%20lack%20three-dimensional%20annotations%2C%20multi-organ%20coverage%2C%20or%20precise%20lesion-to-organ%20associations%2C%20hindering%20robust%20representation%20learning%20and%20clinical%20applications.%20To%20address%20this%20gap%2C%20we%20introduce%203DLAND%2C%20a%20large-scale%20benchmark%20dataset%20comprising%20over%206%2C000%20contrast-enhanced%20CT%20volumes%20with%20over%2020%2C000%20high-fidelity%203D%20lesion%20annotations%20linked%20to%20seven%20abdominal%20organs%3A%20liver%2C%20kidneys%2C%20pancreas%2C%20spleen%2C%20stomach%2C%20and%20gallbladder.%20Our%20streamlined%20three-phase%20pipeline%20integrates%20automated%20spatial%20reasoning%2C%20prompt-optimized%202D%20segmentation%2C%20and%20memory-guided%203D%20propagation%2C%20validated%20by%20expert%20radiologists%20with%20surface%20dice%20scores%20exceeding%200.75.%20By%20providing%20diverse%20lesion%20types%20and%20patient%20demographics%2C%203DLAND%20enables%20scalable%20evaluation%20of%20anomaly%20detection%2C%20localization%2C%20and%20cross-organ%20transfer%20learning%20for%20medical%20AI.%20Our%20dataset%20establishes%20a%20new%20benchmark%20for%20evaluating%20organ-aware%203D%20segmentation%20models%2C%20paving%20the%20way%20for%20advancements%20in%20healthcare-oriented%20AI.%20To%20facilitate%20reproducibility%20and%20further%20research%2C%20the%203DLAND%20dataset%20and%20implementation%20code%20are%20publicly%20available%20at%20https%3A//mehrn79.github.io/3DLAND.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DLAND%253A%25203D%2520Lesion%2520Abdominal%2520Anomaly%2520Localization%2520Dataset%26entry.906535625%3DMehran%2520Advand%2520and%2520Zahra%2520Dehghanian%2520and%2520Navid%2520Faraji%2520and%2520Reza%2520Barati%2520and%2520Seyed%2520Amir%2520Ahmad%2520Safavi-Naini%2520and%2520Hamid%2520R.%2520Rabiee%26entry.1292438233%3DExisting%2520medical%2520imaging%2520datasets%2520for%2520abdominal%2520CT%2520often%2520lack%2520three-dimensional%2520annotations%252C%2520multi-organ%2520coverage%252C%2520or%2520precise%2520lesion-to-organ%2520associations%252C%2520hindering%2520robust%2520representation%2520learning%2520and%2520clinical%2520applications.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%25203DLAND%252C%2520a%2520large-scale%2520benchmark%2520dataset%2520comprising%2520over%25206%252C000%2520contrast-enhanced%2520CT%2520volumes%2520with%2520over%252020%252C000%2520high-fidelity%25203D%2520lesion%2520annotations%2520linked%2520to%2520seven%2520abdominal%2520organs%253A%2520liver%252C%2520kidneys%252C%2520pancreas%252C%2520spleen%252C%2520stomach%252C%2520and%2520gallbladder.%2520Our%2520streamlined%2520three-phase%2520pipeline%2520integrates%2520automated%2520spatial%2520reasoning%252C%2520prompt-optimized%25202D%2520segmentation%252C%2520and%2520memory-guided%25203D%2520propagation%252C%2520validated%2520by%2520expert%2520radiologists%2520with%2520surface%2520dice%2520scores%2520exceeding%25200.75.%2520By%2520providing%2520diverse%2520lesion%2520types%2520and%2520patient%2520demographics%252C%25203DLAND%2520enables%2520scalable%2520evaluation%2520of%2520anomaly%2520detection%252C%2520localization%252C%2520and%2520cross-organ%2520transfer%2520learning%2520for%2520medical%2520AI.%2520Our%2520dataset%2520establishes%2520a%2520new%2520benchmark%2520for%2520evaluating%2520organ-aware%25203D%2520segmentation%2520models%252C%2520paving%2520the%2520way%2520for%2520advancements%2520in%2520healthcare-oriented%2520AI.%2520To%2520facilitate%2520reproducibility%2520and%2520further%2520research%252C%2520the%25203DLAND%2520dataset%2520and%2520implementation%2520code%2520are%2520publicly%2520available%2520at%2520https%253A//mehrn79.github.io/3DLAND.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DLAND%3A%203D%20Lesion%20Abdominal%20Anomaly%20Localization%20Dataset&entry.906535625=Mehran%20Advand%20and%20Zahra%20Dehghanian%20and%20Navid%20Faraji%20and%20Reza%20Barati%20and%20Seyed%20Amir%20Ahmad%20Safavi-Naini%20and%20Hamid%20R.%20Rabiee&entry.1292438233=Existing%20medical%20imaging%20datasets%20for%20abdominal%20CT%20often%20lack%20three-dimensional%20annotations%2C%20multi-organ%20coverage%2C%20or%20precise%20lesion-to-organ%20associations%2C%20hindering%20robust%20representation%20learning%20and%20clinical%20applications.%20To%20address%20this%20gap%2C%20we%20introduce%203DLAND%2C%20a%20large-scale%20benchmark%20dataset%20comprising%20over%206%2C000%20contrast-enhanced%20CT%20volumes%20with%20over%2020%2C000%20high-fidelity%203D%20lesion%20annotations%20linked%20to%20seven%20abdominal%20organs%3A%20liver%2C%20kidneys%2C%20pancreas%2C%20spleen%2C%20stomach%2C%20and%20gallbladder.%20Our%20streamlined%20three-phase%20pipeline%20integrates%20automated%20spatial%20reasoning%2C%20prompt-optimized%202D%20segmentation%2C%20and%20memory-guided%203D%20propagation%2C%20validated%20by%20expert%20radiologists%20with%20surface%20dice%20scores%20exceeding%200.75.%20By%20providing%20diverse%20lesion%20types%20and%20patient%20demographics%2C%203DLAND%20enables%20scalable%20evaluation%20of%20anomaly%20detection%2C%20localization%2C%20and%20cross-organ%20transfer%20learning%20for%20medical%20AI.%20Our%20dataset%20establishes%20a%20new%20benchmark%20for%20evaluating%20organ-aware%203D%20segmentation%20models%2C%20paving%20the%20way%20for%20advancements%20in%20healthcare-oriented%20AI.%20To%20facilitate%20reproducibility%20and%20further%20research%2C%20the%203DLAND%20dataset%20and%20implementation%20code%20are%20publicly%20available%20at%20https%3A//mehrn79.github.io/3DLAND.&entry.1838667208=http%3A//arxiv.org/abs/2602.12820v1&entry.124074799=Read"},
{"title": "Machine Learning-Based Classification of Jhana Advanced Concentrative Absorption Meditation (ACAM-J) using 7T fMRI", "author": "Puneet Kumar and Winson F. Z. Yang and Alakhsimar Singh and Xiaobai Li and Matthew D. Sacchet", "abstract": "Jhana advanced concentration absorption meditation (ACAM-J) is related to profound changes in consciousness and cognitive processing, making the study of their neural correlates vital for insights into consciousness and well-being. This study evaluates whether functional MRI-derived regional homogeneity (ReHo) can be used to classify ACAM-J using machine-learning approaches. We collected group-level fMRI data from 20 advanced meditators to train the classifiers, and intensive single-case data from an advanced practitioner performing ACAM-J and control tasks to evaluate generalization. ReHo maps were computed, and features were extracted from predefined brain regions of interest. We trained multiple machine learning classifiers using stratified cross-validation to evaluate whether ReHo patterns distinguish ACAM-J from non-meditative states. Ensemble models achieved 66.82% (p < 0.05) accuracy in distinguishing ACAM-J from control conditions. Feature-importance analysis indicated that prefrontal and anterior cingulate areas contributed most to model decisions, aligning with established involvement of these regions in attentional regulation and metacognitive processes. Moreover, moderate agreement reflected in Cohen's kappa supports the feasibility of using machine learning to distinguish ACAM-J from non-meditative states. These findings advocate machine-learning's feasibility in classifying advanced meditation states, future research on neuromodulation and mechanistic models of advanced meditation.", "link": "http://arxiv.org/abs/2602.13008v1", "date": "2026-02-13", "relevancy": 2.0993, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4437}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4112}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning-Based%20Classification%20of%20Jhana%20Advanced%20Concentrative%20Absorption%20Meditation%20%28ACAM-J%29%20using%207T%20fMRI&body=Title%3A%20Machine%20Learning-Based%20Classification%20of%20Jhana%20Advanced%20Concentrative%20Absorption%20Meditation%20%28ACAM-J%29%20using%207T%20fMRI%0AAuthor%3A%20Puneet%20Kumar%20and%20Winson%20F.%20Z.%20Yang%20and%20Alakhsimar%20Singh%20and%20Xiaobai%20Li%20and%20Matthew%20D.%20Sacchet%0AAbstract%3A%20Jhana%20advanced%20concentration%20absorption%20meditation%20%28ACAM-J%29%20is%20related%20to%20profound%20changes%20in%20consciousness%20and%20cognitive%20processing%2C%20making%20the%20study%20of%20their%20neural%20correlates%20vital%20for%20insights%20into%20consciousness%20and%20well-being.%20This%20study%20evaluates%20whether%20functional%20MRI-derived%20regional%20homogeneity%20%28ReHo%29%20can%20be%20used%20to%20classify%20ACAM-J%20using%20machine-learning%20approaches.%20We%20collected%20group-level%20fMRI%20data%20from%2020%20advanced%20meditators%20to%20train%20the%20classifiers%2C%20and%20intensive%20single-case%20data%20from%20an%20advanced%20practitioner%20performing%20ACAM-J%20and%20control%20tasks%20to%20evaluate%20generalization.%20ReHo%20maps%20were%20computed%2C%20and%20features%20were%20extracted%20from%20predefined%20brain%20regions%20of%20interest.%20We%20trained%20multiple%20machine%20learning%20classifiers%20using%20stratified%20cross-validation%20to%20evaluate%20whether%20ReHo%20patterns%20distinguish%20ACAM-J%20from%20non-meditative%20states.%20Ensemble%20models%20achieved%2066.82%25%20%28p%20%3C%200.05%29%20accuracy%20in%20distinguishing%20ACAM-J%20from%20control%20conditions.%20Feature-importance%20analysis%20indicated%20that%20prefrontal%20and%20anterior%20cingulate%20areas%20contributed%20most%20to%20model%20decisions%2C%20aligning%20with%20established%20involvement%20of%20these%20regions%20in%20attentional%20regulation%20and%20metacognitive%20processes.%20Moreover%2C%20moderate%20agreement%20reflected%20in%20Cohen%27s%20kappa%20supports%20the%20feasibility%20of%20using%20machine%20learning%20to%20distinguish%20ACAM-J%20from%20non-meditative%20states.%20These%20findings%20advocate%20machine-learning%27s%20feasibility%20in%20classifying%20advanced%20meditation%20states%2C%20future%20research%20on%20neuromodulation%20and%20mechanistic%20models%20of%20advanced%20meditation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning-Based%2520Classification%2520of%2520Jhana%2520Advanced%2520Concentrative%2520Absorption%2520Meditation%2520%2528ACAM-J%2529%2520using%25207T%2520fMRI%26entry.906535625%3DPuneet%2520Kumar%2520and%2520Winson%2520F.%2520Z.%2520Yang%2520and%2520Alakhsimar%2520Singh%2520and%2520Xiaobai%2520Li%2520and%2520Matthew%2520D.%2520Sacchet%26entry.1292438233%3DJhana%2520advanced%2520concentration%2520absorption%2520meditation%2520%2528ACAM-J%2529%2520is%2520related%2520to%2520profound%2520changes%2520in%2520consciousness%2520and%2520cognitive%2520processing%252C%2520making%2520the%2520study%2520of%2520their%2520neural%2520correlates%2520vital%2520for%2520insights%2520into%2520consciousness%2520and%2520well-being.%2520This%2520study%2520evaluates%2520whether%2520functional%2520MRI-derived%2520regional%2520homogeneity%2520%2528ReHo%2529%2520can%2520be%2520used%2520to%2520classify%2520ACAM-J%2520using%2520machine-learning%2520approaches.%2520We%2520collected%2520group-level%2520fMRI%2520data%2520from%252020%2520advanced%2520meditators%2520to%2520train%2520the%2520classifiers%252C%2520and%2520intensive%2520single-case%2520data%2520from%2520an%2520advanced%2520practitioner%2520performing%2520ACAM-J%2520and%2520control%2520tasks%2520to%2520evaluate%2520generalization.%2520ReHo%2520maps%2520were%2520computed%252C%2520and%2520features%2520were%2520extracted%2520from%2520predefined%2520brain%2520regions%2520of%2520interest.%2520We%2520trained%2520multiple%2520machine%2520learning%2520classifiers%2520using%2520stratified%2520cross-validation%2520to%2520evaluate%2520whether%2520ReHo%2520patterns%2520distinguish%2520ACAM-J%2520from%2520non-meditative%2520states.%2520Ensemble%2520models%2520achieved%252066.82%2525%2520%2528p%2520%253C%25200.05%2529%2520accuracy%2520in%2520distinguishing%2520ACAM-J%2520from%2520control%2520conditions.%2520Feature-importance%2520analysis%2520indicated%2520that%2520prefrontal%2520and%2520anterior%2520cingulate%2520areas%2520contributed%2520most%2520to%2520model%2520decisions%252C%2520aligning%2520with%2520established%2520involvement%2520of%2520these%2520regions%2520in%2520attentional%2520regulation%2520and%2520metacognitive%2520processes.%2520Moreover%252C%2520moderate%2520agreement%2520reflected%2520in%2520Cohen%2527s%2520kappa%2520supports%2520the%2520feasibility%2520of%2520using%2520machine%2520learning%2520to%2520distinguish%2520ACAM-J%2520from%2520non-meditative%2520states.%2520These%2520findings%2520advocate%2520machine-learning%2527s%2520feasibility%2520in%2520classifying%2520advanced%2520meditation%2520states%252C%2520future%2520research%2520on%2520neuromodulation%2520and%2520mechanistic%2520models%2520of%2520advanced%2520meditation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning-Based%20Classification%20of%20Jhana%20Advanced%20Concentrative%20Absorption%20Meditation%20%28ACAM-J%29%20using%207T%20fMRI&entry.906535625=Puneet%20Kumar%20and%20Winson%20F.%20Z.%20Yang%20and%20Alakhsimar%20Singh%20and%20Xiaobai%20Li%20and%20Matthew%20D.%20Sacchet&entry.1292438233=Jhana%20advanced%20concentration%20absorption%20meditation%20%28ACAM-J%29%20is%20related%20to%20profound%20changes%20in%20consciousness%20and%20cognitive%20processing%2C%20making%20the%20study%20of%20their%20neural%20correlates%20vital%20for%20insights%20into%20consciousness%20and%20well-being.%20This%20study%20evaluates%20whether%20functional%20MRI-derived%20regional%20homogeneity%20%28ReHo%29%20can%20be%20used%20to%20classify%20ACAM-J%20using%20machine-learning%20approaches.%20We%20collected%20group-level%20fMRI%20data%20from%2020%20advanced%20meditators%20to%20train%20the%20classifiers%2C%20and%20intensive%20single-case%20data%20from%20an%20advanced%20practitioner%20performing%20ACAM-J%20and%20control%20tasks%20to%20evaluate%20generalization.%20ReHo%20maps%20were%20computed%2C%20and%20features%20were%20extracted%20from%20predefined%20brain%20regions%20of%20interest.%20We%20trained%20multiple%20machine%20learning%20classifiers%20using%20stratified%20cross-validation%20to%20evaluate%20whether%20ReHo%20patterns%20distinguish%20ACAM-J%20from%20non-meditative%20states.%20Ensemble%20models%20achieved%2066.82%25%20%28p%20%3C%200.05%29%20accuracy%20in%20distinguishing%20ACAM-J%20from%20control%20conditions.%20Feature-importance%20analysis%20indicated%20that%20prefrontal%20and%20anterior%20cingulate%20areas%20contributed%20most%20to%20model%20decisions%2C%20aligning%20with%20established%20involvement%20of%20these%20regions%20in%20attentional%20regulation%20and%20metacognitive%20processes.%20Moreover%2C%20moderate%20agreement%20reflected%20in%20Cohen%27s%20kappa%20supports%20the%20feasibility%20of%20using%20machine%20learning%20to%20distinguish%20ACAM-J%20from%20non-meditative%20states.%20These%20findings%20advocate%20machine-learning%27s%20feasibility%20in%20classifying%20advanced%20meditation%20states%2C%20future%20research%20on%20neuromodulation%20and%20mechanistic%20models%20of%20advanced%20meditation.&entry.1838667208=http%3A//arxiv.org/abs/2602.13008v1&entry.124074799=Read"},
{"title": "Monocular Markerless Motion Capture Enables Quantitative Assessment of Upper Extremity Reachable Workspace", "author": "Seth Donahue and J. D. Peiffer and R. Tyler Richardson and Yishan Zhong and Shaun Q. Y. Tan and Benoit Marteau and Stephanie R. Russo and May D. Wang and R. James Cotton and Ross Chafetz", "abstract": "To validate a clinically accessible approach for quantifying the Upper Extremity Reachable Workspace (UERW) using a single (monocular) camera and Artificial Intelligence (AI)-driven Markerless Motion Capture (MMC) for biomechanical analysis. Objective assessment and validation of these techniques for specific clinically oriented tasks are crucial for their adoption in clinical motion analysis. AI-driven monocular MMC reduces the barriers to adoption in the clinic and has the potential to reduce the overhead for analysis of this common clinical assessment. Nine adult participants with no impairments performed the standardized UERW task, which entails reaching targets distributed across a virtual sphere centered on the torso, with targets displayed in a VR headset. Movements were simultaneously captured using a marker-based motion capture system and a set of eight FLIR cameras. We performed monocular video analysis on two of these video camera views to compare a frontal and offset camera configurations. The frontal camera orientation demonstrated strong agreement with the marker-based reference, exhibiting a minimal mean bias of $0.61 \\pm 0.12$ \\% reachspace reached per octanct (mean $\\pm$ standard deviation). In contrast, the offset camera view underestimated the percent workspace reached ($-5.66 \\pm 0.45$ \\% reachspace reached). Conclusion: The findings support the feasibility of a frontal monocular camera configuration for UERW assessment, particularly for anterior workspace evaluation where agreement with marker-based motion capture was highest. The overall performance demonstrates clinical potential for practical, single-camera assessments. This study provides the first validation of monocular MMC system for the assessment of the UERW task. By reducing technical complexity, this approach enables broader implementation of quantitative upper extremity mobility assessment.", "link": "http://arxiv.org/abs/2602.13176v1", "date": "2026-02-13", "relevancy": 2.0989, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5608}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5006}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monocular%20Markerless%20Motion%20Capture%20Enables%20Quantitative%20Assessment%20of%20Upper%20Extremity%20Reachable%20Workspace&body=Title%3A%20Monocular%20Markerless%20Motion%20Capture%20Enables%20Quantitative%20Assessment%20of%20Upper%20Extremity%20Reachable%20Workspace%0AAuthor%3A%20Seth%20Donahue%20and%20J.%20D.%20Peiffer%20and%20R.%20Tyler%20Richardson%20and%20Yishan%20Zhong%20and%20Shaun%20Q.%20Y.%20Tan%20and%20Benoit%20Marteau%20and%20Stephanie%20R.%20Russo%20and%20May%20D.%20Wang%20and%20R.%20James%20Cotton%20and%20Ross%20Chafetz%0AAbstract%3A%20To%20validate%20a%20clinically%20accessible%20approach%20for%20quantifying%20the%20Upper%20Extremity%20Reachable%20Workspace%20%28UERW%29%20using%20a%20single%20%28monocular%29%20camera%20and%20Artificial%20Intelligence%20%28AI%29-driven%20Markerless%20Motion%20Capture%20%28MMC%29%20for%20biomechanical%20analysis.%20Objective%20assessment%20and%20validation%20of%20these%20techniques%20for%20specific%20clinically%20oriented%20tasks%20are%20crucial%20for%20their%20adoption%20in%20clinical%20motion%20analysis.%20AI-driven%20monocular%20MMC%20reduces%20the%20barriers%20to%20adoption%20in%20the%20clinic%20and%20has%20the%20potential%20to%20reduce%20the%20overhead%20for%20analysis%20of%20this%20common%20clinical%20assessment.%20Nine%20adult%20participants%20with%20no%20impairments%20performed%20the%20standardized%20UERW%20task%2C%20which%20entails%20reaching%20targets%20distributed%20across%20a%20virtual%20sphere%20centered%20on%20the%20torso%2C%20with%20targets%20displayed%20in%20a%20VR%20headset.%20Movements%20were%20simultaneously%20captured%20using%20a%20marker-based%20motion%20capture%20system%20and%20a%20set%20of%20eight%20FLIR%20cameras.%20We%20performed%20monocular%20video%20analysis%20on%20two%20of%20these%20video%20camera%20views%20to%20compare%20a%20frontal%20and%20offset%20camera%20configurations.%20The%20frontal%20camera%20orientation%20demonstrated%20strong%20agreement%20with%20the%20marker-based%20reference%2C%20exhibiting%20a%20minimal%20mean%20bias%20of%20%240.61%20%5Cpm%200.12%24%20%5C%25%20reachspace%20reached%20per%20octanct%20%28mean%20%24%5Cpm%24%20standard%20deviation%29.%20In%20contrast%2C%20the%20offset%20camera%20view%20underestimated%20the%20percent%20workspace%20reached%20%28%24-5.66%20%5Cpm%200.45%24%20%5C%25%20reachspace%20reached%29.%20Conclusion%3A%20The%20findings%20support%20the%20feasibility%20of%20a%20frontal%20monocular%20camera%20configuration%20for%20UERW%20assessment%2C%20particularly%20for%20anterior%20workspace%20evaluation%20where%20agreement%20with%20marker-based%20motion%20capture%20was%20highest.%20The%20overall%20performance%20demonstrates%20clinical%20potential%20for%20practical%2C%20single-camera%20assessments.%20This%20study%20provides%20the%20first%20validation%20of%20monocular%20MMC%20system%20for%20the%20assessment%20of%20the%20UERW%20task.%20By%20reducing%20technical%20complexity%2C%20this%20approach%20enables%20broader%20implementation%20of%20quantitative%20upper%20extremity%20mobility%20assessment.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonocular%2520Markerless%2520Motion%2520Capture%2520Enables%2520Quantitative%2520Assessment%2520of%2520Upper%2520Extremity%2520Reachable%2520Workspace%26entry.906535625%3DSeth%2520Donahue%2520and%2520J.%2520D.%2520Peiffer%2520and%2520R.%2520Tyler%2520Richardson%2520and%2520Yishan%2520Zhong%2520and%2520Shaun%2520Q.%2520Y.%2520Tan%2520and%2520Benoit%2520Marteau%2520and%2520Stephanie%2520R.%2520Russo%2520and%2520May%2520D.%2520Wang%2520and%2520R.%2520James%2520Cotton%2520and%2520Ross%2520Chafetz%26entry.1292438233%3DTo%2520validate%2520a%2520clinically%2520accessible%2520approach%2520for%2520quantifying%2520the%2520Upper%2520Extremity%2520Reachable%2520Workspace%2520%2528UERW%2529%2520using%2520a%2520single%2520%2528monocular%2529%2520camera%2520and%2520Artificial%2520Intelligence%2520%2528AI%2529-driven%2520Markerless%2520Motion%2520Capture%2520%2528MMC%2529%2520for%2520biomechanical%2520analysis.%2520Objective%2520assessment%2520and%2520validation%2520of%2520these%2520techniques%2520for%2520specific%2520clinically%2520oriented%2520tasks%2520are%2520crucial%2520for%2520their%2520adoption%2520in%2520clinical%2520motion%2520analysis.%2520AI-driven%2520monocular%2520MMC%2520reduces%2520the%2520barriers%2520to%2520adoption%2520in%2520the%2520clinic%2520and%2520has%2520the%2520potential%2520to%2520reduce%2520the%2520overhead%2520for%2520analysis%2520of%2520this%2520common%2520clinical%2520assessment.%2520Nine%2520adult%2520participants%2520with%2520no%2520impairments%2520performed%2520the%2520standardized%2520UERW%2520task%252C%2520which%2520entails%2520reaching%2520targets%2520distributed%2520across%2520a%2520virtual%2520sphere%2520centered%2520on%2520the%2520torso%252C%2520with%2520targets%2520displayed%2520in%2520a%2520VR%2520headset.%2520Movements%2520were%2520simultaneously%2520captured%2520using%2520a%2520marker-based%2520motion%2520capture%2520system%2520and%2520a%2520set%2520of%2520eight%2520FLIR%2520cameras.%2520We%2520performed%2520monocular%2520video%2520analysis%2520on%2520two%2520of%2520these%2520video%2520camera%2520views%2520to%2520compare%2520a%2520frontal%2520and%2520offset%2520camera%2520configurations.%2520The%2520frontal%2520camera%2520orientation%2520demonstrated%2520strong%2520agreement%2520with%2520the%2520marker-based%2520reference%252C%2520exhibiting%2520a%2520minimal%2520mean%2520bias%2520of%2520%25240.61%2520%255Cpm%25200.12%2524%2520%255C%2525%2520reachspace%2520reached%2520per%2520octanct%2520%2528mean%2520%2524%255Cpm%2524%2520standard%2520deviation%2529.%2520In%2520contrast%252C%2520the%2520offset%2520camera%2520view%2520underestimated%2520the%2520percent%2520workspace%2520reached%2520%2528%2524-5.66%2520%255Cpm%25200.45%2524%2520%255C%2525%2520reachspace%2520reached%2529.%2520Conclusion%253A%2520The%2520findings%2520support%2520the%2520feasibility%2520of%2520a%2520frontal%2520monocular%2520camera%2520configuration%2520for%2520UERW%2520assessment%252C%2520particularly%2520for%2520anterior%2520workspace%2520evaluation%2520where%2520agreement%2520with%2520marker-based%2520motion%2520capture%2520was%2520highest.%2520The%2520overall%2520performance%2520demonstrates%2520clinical%2520potential%2520for%2520practical%252C%2520single-camera%2520assessments.%2520This%2520study%2520provides%2520the%2520first%2520validation%2520of%2520monocular%2520MMC%2520system%2520for%2520the%2520assessment%2520of%2520the%2520UERW%2520task.%2520By%2520reducing%2520technical%2520complexity%252C%2520this%2520approach%2520enables%2520broader%2520implementation%2520of%2520quantitative%2520upper%2520extremity%2520mobility%2520assessment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%20Markerless%20Motion%20Capture%20Enables%20Quantitative%20Assessment%20of%20Upper%20Extremity%20Reachable%20Workspace&entry.906535625=Seth%20Donahue%20and%20J.%20D.%20Peiffer%20and%20R.%20Tyler%20Richardson%20and%20Yishan%20Zhong%20and%20Shaun%20Q.%20Y.%20Tan%20and%20Benoit%20Marteau%20and%20Stephanie%20R.%20Russo%20and%20May%20D.%20Wang%20and%20R.%20James%20Cotton%20and%20Ross%20Chafetz&entry.1292438233=To%20validate%20a%20clinically%20accessible%20approach%20for%20quantifying%20the%20Upper%20Extremity%20Reachable%20Workspace%20%28UERW%29%20using%20a%20single%20%28monocular%29%20camera%20and%20Artificial%20Intelligence%20%28AI%29-driven%20Markerless%20Motion%20Capture%20%28MMC%29%20for%20biomechanical%20analysis.%20Objective%20assessment%20and%20validation%20of%20these%20techniques%20for%20specific%20clinically%20oriented%20tasks%20are%20crucial%20for%20their%20adoption%20in%20clinical%20motion%20analysis.%20AI-driven%20monocular%20MMC%20reduces%20the%20barriers%20to%20adoption%20in%20the%20clinic%20and%20has%20the%20potential%20to%20reduce%20the%20overhead%20for%20analysis%20of%20this%20common%20clinical%20assessment.%20Nine%20adult%20participants%20with%20no%20impairments%20performed%20the%20standardized%20UERW%20task%2C%20which%20entails%20reaching%20targets%20distributed%20across%20a%20virtual%20sphere%20centered%20on%20the%20torso%2C%20with%20targets%20displayed%20in%20a%20VR%20headset.%20Movements%20were%20simultaneously%20captured%20using%20a%20marker-based%20motion%20capture%20system%20and%20a%20set%20of%20eight%20FLIR%20cameras.%20We%20performed%20monocular%20video%20analysis%20on%20two%20of%20these%20video%20camera%20views%20to%20compare%20a%20frontal%20and%20offset%20camera%20configurations.%20The%20frontal%20camera%20orientation%20demonstrated%20strong%20agreement%20with%20the%20marker-based%20reference%2C%20exhibiting%20a%20minimal%20mean%20bias%20of%20%240.61%20%5Cpm%200.12%24%20%5C%25%20reachspace%20reached%20per%20octanct%20%28mean%20%24%5Cpm%24%20standard%20deviation%29.%20In%20contrast%2C%20the%20offset%20camera%20view%20underestimated%20the%20percent%20workspace%20reached%20%28%24-5.66%20%5Cpm%200.45%24%20%5C%25%20reachspace%20reached%29.%20Conclusion%3A%20The%20findings%20support%20the%20feasibility%20of%20a%20frontal%20monocular%20camera%20configuration%20for%20UERW%20assessment%2C%20particularly%20for%20anterior%20workspace%20evaluation%20where%20agreement%20with%20marker-based%20motion%20capture%20was%20highest.%20The%20overall%20performance%20demonstrates%20clinical%20potential%20for%20practical%2C%20single-camera%20assessments.%20This%20study%20provides%20the%20first%20validation%20of%20monocular%20MMC%20system%20for%20the%20assessment%20of%20the%20UERW%20task.%20By%20reducing%20technical%20complexity%2C%20this%20approach%20enables%20broader%20implementation%20of%20quantitative%20upper%20extremity%20mobility%20assessment.&entry.1838667208=http%3A//arxiv.org/abs/2602.13176v1&entry.124074799=Read"},
{"title": "WISE: A Multimodal Search Engine for Visual Scenes, Audio, Objects, Faces, Speech, and Metadata", "author": "Prasanna Sridhar and Horace Lee and David M. S. Pinto and Andrew Zisserman and Abhishek Dutta", "abstract": "In this paper, we present WISE, an open-source audiovisual search engine which integrates a range of multimodal retrieval capabilities into a single, practical tool accessible to users without machine learning expertise. WISE supports natural-language and reverse-image queries at both the scene level (e.g. empty street) and object level (e.g. horse) across images and videos; face-based search for specific individuals; audio retrieval of acoustic events using text (e.g. wood creak) or an audio file; search over automatically transcribed speech; and filtering by user-provided metadata. Rich insights can be obtained by combining queries across modalities -- for example, retrieving German trains from a historical archive by applying the object query \"train\" and the metadata query \"Germany\", or searching for a face in a place. By employing vector search techniques, WISE can scale to support efficient retrieval over millions of images or thousands of hours of video. Its modular architecture facilitates the integration of new models. WISE can be deployed locally for private or sensitive collections, and has been applied to various real-world use cases. Our code is open-source and available at https://gitlab.com/vgg/wise/wise.", "link": "http://arxiv.org/abs/2602.12819v1", "date": "2026-02-13", "relevancy": 2.085, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5248}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5205}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WISE%3A%20A%20Multimodal%20Search%20Engine%20for%20Visual%20Scenes%2C%20Audio%2C%20Objects%2C%20Faces%2C%20Speech%2C%20and%20Metadata&body=Title%3A%20WISE%3A%20A%20Multimodal%20Search%20Engine%20for%20Visual%20Scenes%2C%20Audio%2C%20Objects%2C%20Faces%2C%20Speech%2C%20and%20Metadata%0AAuthor%3A%20Prasanna%20Sridhar%20and%20Horace%20Lee%20and%20David%20M.%20S.%20Pinto%20and%20Andrew%20Zisserman%20and%20Abhishek%20Dutta%0AAbstract%3A%20In%20this%20paper%2C%20we%20present%20WISE%2C%20an%20open-source%20audiovisual%20search%20engine%20which%20integrates%20a%20range%20of%20multimodal%20retrieval%20capabilities%20into%20a%20single%2C%20practical%20tool%20accessible%20to%20users%20without%20machine%20learning%20expertise.%20WISE%20supports%20natural-language%20and%20reverse-image%20queries%20at%20both%20the%20scene%20level%20%28e.g.%20empty%20street%29%20and%20object%20level%20%28e.g.%20horse%29%20across%20images%20and%20videos%3B%20face-based%20search%20for%20specific%20individuals%3B%20audio%20retrieval%20of%20acoustic%20events%20using%20text%20%28e.g.%20wood%20creak%29%20or%20an%20audio%20file%3B%20search%20over%20automatically%20transcribed%20speech%3B%20and%20filtering%20by%20user-provided%20metadata.%20Rich%20insights%20can%20be%20obtained%20by%20combining%20queries%20across%20modalities%20--%20for%20example%2C%20retrieving%20German%20trains%20from%20a%20historical%20archive%20by%20applying%20the%20object%20query%20%22train%22%20and%20the%20metadata%20query%20%22Germany%22%2C%20or%20searching%20for%20a%20face%20in%20a%20place.%20By%20employing%20vector%20search%20techniques%2C%20WISE%20can%20scale%20to%20support%20efficient%20retrieval%20over%20millions%20of%20images%20or%20thousands%20of%20hours%20of%20video.%20Its%20modular%20architecture%20facilitates%20the%20integration%20of%20new%20models.%20WISE%20can%20be%20deployed%20locally%20for%20private%20or%20sensitive%20collections%2C%20and%20has%20been%20applied%20to%20various%20real-world%20use%20cases.%20Our%20code%20is%20open-source%20and%20available%20at%20https%3A//gitlab.com/vgg/wise/wise.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWISE%253A%2520A%2520Multimodal%2520Search%2520Engine%2520for%2520Visual%2520Scenes%252C%2520Audio%252C%2520Objects%252C%2520Faces%252C%2520Speech%252C%2520and%2520Metadata%26entry.906535625%3DPrasanna%2520Sridhar%2520and%2520Horace%2520Lee%2520and%2520David%2520M.%2520S.%2520Pinto%2520and%2520Andrew%2520Zisserman%2520and%2520Abhishek%2520Dutta%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520present%2520WISE%252C%2520an%2520open-source%2520audiovisual%2520search%2520engine%2520which%2520integrates%2520a%2520range%2520of%2520multimodal%2520retrieval%2520capabilities%2520into%2520a%2520single%252C%2520practical%2520tool%2520accessible%2520to%2520users%2520without%2520machine%2520learning%2520expertise.%2520WISE%2520supports%2520natural-language%2520and%2520reverse-image%2520queries%2520at%2520both%2520the%2520scene%2520level%2520%2528e.g.%2520empty%2520street%2529%2520and%2520object%2520level%2520%2528e.g.%2520horse%2529%2520across%2520images%2520and%2520videos%253B%2520face-based%2520search%2520for%2520specific%2520individuals%253B%2520audio%2520retrieval%2520of%2520acoustic%2520events%2520using%2520text%2520%2528e.g.%2520wood%2520creak%2529%2520or%2520an%2520audio%2520file%253B%2520search%2520over%2520automatically%2520transcribed%2520speech%253B%2520and%2520filtering%2520by%2520user-provided%2520metadata.%2520Rich%2520insights%2520can%2520be%2520obtained%2520by%2520combining%2520queries%2520across%2520modalities%2520--%2520for%2520example%252C%2520retrieving%2520German%2520trains%2520from%2520a%2520historical%2520archive%2520by%2520applying%2520the%2520object%2520query%2520%2522train%2522%2520and%2520the%2520metadata%2520query%2520%2522Germany%2522%252C%2520or%2520searching%2520for%2520a%2520face%2520in%2520a%2520place.%2520By%2520employing%2520vector%2520search%2520techniques%252C%2520WISE%2520can%2520scale%2520to%2520support%2520efficient%2520retrieval%2520over%2520millions%2520of%2520images%2520or%2520thousands%2520of%2520hours%2520of%2520video.%2520Its%2520modular%2520architecture%2520facilitates%2520the%2520integration%2520of%2520new%2520models.%2520WISE%2520can%2520be%2520deployed%2520locally%2520for%2520private%2520or%2520sensitive%2520collections%252C%2520and%2520has%2520been%2520applied%2520to%2520various%2520real-world%2520use%2520cases.%2520Our%2520code%2520is%2520open-source%2520and%2520available%2520at%2520https%253A//gitlab.com/vgg/wise/wise.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WISE%3A%20A%20Multimodal%20Search%20Engine%20for%20Visual%20Scenes%2C%20Audio%2C%20Objects%2C%20Faces%2C%20Speech%2C%20and%20Metadata&entry.906535625=Prasanna%20Sridhar%20and%20Horace%20Lee%20and%20David%20M.%20S.%20Pinto%20and%20Andrew%20Zisserman%20and%20Abhishek%20Dutta&entry.1292438233=In%20this%20paper%2C%20we%20present%20WISE%2C%20an%20open-source%20audiovisual%20search%20engine%20which%20integrates%20a%20range%20of%20multimodal%20retrieval%20capabilities%20into%20a%20single%2C%20practical%20tool%20accessible%20to%20users%20without%20machine%20learning%20expertise.%20WISE%20supports%20natural-language%20and%20reverse-image%20queries%20at%20both%20the%20scene%20level%20%28e.g.%20empty%20street%29%20and%20object%20level%20%28e.g.%20horse%29%20across%20images%20and%20videos%3B%20face-based%20search%20for%20specific%20individuals%3B%20audio%20retrieval%20of%20acoustic%20events%20using%20text%20%28e.g.%20wood%20creak%29%20or%20an%20audio%20file%3B%20search%20over%20automatically%20transcribed%20speech%3B%20and%20filtering%20by%20user-provided%20metadata.%20Rich%20insights%20can%20be%20obtained%20by%20combining%20queries%20across%20modalities%20--%20for%20example%2C%20retrieving%20German%20trains%20from%20a%20historical%20archive%20by%20applying%20the%20object%20query%20%22train%22%20and%20the%20metadata%20query%20%22Germany%22%2C%20or%20searching%20for%20a%20face%20in%20a%20place.%20By%20employing%20vector%20search%20techniques%2C%20WISE%20can%20scale%20to%20support%20efficient%20retrieval%20over%20millions%20of%20images%20or%20thousands%20of%20hours%20of%20video.%20Its%20modular%20architecture%20facilitates%20the%20integration%20of%20new%20models.%20WISE%20can%20be%20deployed%20locally%20for%20private%20or%20sensitive%20collections%2C%20and%20has%20been%20applied%20to%20various%20real-world%20use%20cases.%20Our%20code%20is%20open-source%20and%20available%20at%20https%3A//gitlab.com/vgg/wise/wise.&entry.1838667208=http%3A//arxiv.org/abs/2602.12819v1&entry.124074799=Read"},
{"title": "Detecting Object Tracking Failure via Sequential Hypothesis Testing", "author": "Alejandro Monroy Mu\u00f1oz and Rajeev Verma and Alexander Timans", "abstract": "Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.", "link": "http://arxiv.org/abs/2602.12983v1", "date": "2026-02-13", "relevancy": 2.082, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5417}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5266}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Object%20Tracking%20Failure%20via%20Sequential%20Hypothesis%20Testing&body=Title%3A%20Detecting%20Object%20Tracking%20Failure%20via%20Sequential%20Hypothesis%20Testing%0AAuthor%3A%20Alejandro%20Monroy%20Mu%C3%B1oz%20and%20Rajeev%20Verma%20and%20Alexander%20Timans%0AAbstract%3A%20Real-time%20online%20object%20tracking%20in%20videos%20constitutes%20a%20core%20task%20in%20computer%20vision%2C%20with%20wide-ranging%20applications%20including%20video%20surveillance%2C%20motion%20capture%2C%20and%20robotics.%20Deployed%20tracking%20systems%20usually%20lack%20formal%20safety%20assurances%20to%20convey%20when%20tracking%20is%20reliable%20and%20when%20it%20may%20fail%2C%20at%20best%20relying%20on%20heuristic%20measures%20of%20model%20confidence%20to%20raise%20alerts.%20To%20obtain%20such%20assurances%20we%20propose%20interpreting%20object%20tracking%20as%20a%20sequential%20hypothesis%20test%2C%20wherein%20evidence%20for%20or%20against%20tracking%20failures%20is%20gradually%20accumulated%20over%20time.%20Leveraging%20recent%20advancements%20in%20the%20field%2C%20our%20sequential%20test%20%28formalized%20as%20an%20e-process%29%20quickly%20identifies%20when%20tracking%20failures%20set%20in%20whilst%20provably%20containing%20false%20alerts%20at%20a%20desired%20rate%2C%20and%20thus%20limiting%20potentially%20costly%20re-calibration%20or%20intervention%20steps.%20The%20approach%20is%20computationally%20light-weight%2C%20requires%20no%20extra%20training%20or%20fine-tuning%2C%20and%20is%20in%20principle%20model-agnostic.%20We%20propose%20both%20supervised%20and%20unsupervised%20variants%20by%20leveraging%20either%20ground-truth%20or%20solely%20internal%20tracking%20information%2C%20and%20demonstrate%20its%20effectiveness%20for%20two%20established%20tracking%20models%20across%20four%20video%20benchmarks.%20As%20such%2C%20sequential%20testing%20can%20offer%20a%20statistically%20grounded%20and%20efficient%20mechanism%20to%20incorporate%20safety%20assurances%20into%20real-time%20tracking%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Object%2520Tracking%2520Failure%2520via%2520Sequential%2520Hypothesis%2520Testing%26entry.906535625%3DAlejandro%2520Monroy%2520Mu%25C3%25B1oz%2520and%2520Rajeev%2520Verma%2520and%2520Alexander%2520Timans%26entry.1292438233%3DReal-time%2520online%2520object%2520tracking%2520in%2520videos%2520constitutes%2520a%2520core%2520task%2520in%2520computer%2520vision%252C%2520with%2520wide-ranging%2520applications%2520including%2520video%2520surveillance%252C%2520motion%2520capture%252C%2520and%2520robotics.%2520Deployed%2520tracking%2520systems%2520usually%2520lack%2520formal%2520safety%2520assurances%2520to%2520convey%2520when%2520tracking%2520is%2520reliable%2520and%2520when%2520it%2520may%2520fail%252C%2520at%2520best%2520relying%2520on%2520heuristic%2520measures%2520of%2520model%2520confidence%2520to%2520raise%2520alerts.%2520To%2520obtain%2520such%2520assurances%2520we%2520propose%2520interpreting%2520object%2520tracking%2520as%2520a%2520sequential%2520hypothesis%2520test%252C%2520wherein%2520evidence%2520for%2520or%2520against%2520tracking%2520failures%2520is%2520gradually%2520accumulated%2520over%2520time.%2520Leveraging%2520recent%2520advancements%2520in%2520the%2520field%252C%2520our%2520sequential%2520test%2520%2528formalized%2520as%2520an%2520e-process%2529%2520quickly%2520identifies%2520when%2520tracking%2520failures%2520set%2520in%2520whilst%2520provably%2520containing%2520false%2520alerts%2520at%2520a%2520desired%2520rate%252C%2520and%2520thus%2520limiting%2520potentially%2520costly%2520re-calibration%2520or%2520intervention%2520steps.%2520The%2520approach%2520is%2520computationally%2520light-weight%252C%2520requires%2520no%2520extra%2520training%2520or%2520fine-tuning%252C%2520and%2520is%2520in%2520principle%2520model-agnostic.%2520We%2520propose%2520both%2520supervised%2520and%2520unsupervised%2520variants%2520by%2520leveraging%2520either%2520ground-truth%2520or%2520solely%2520internal%2520tracking%2520information%252C%2520and%2520demonstrate%2520its%2520effectiveness%2520for%2520two%2520established%2520tracking%2520models%2520across%2520four%2520video%2520benchmarks.%2520As%2520such%252C%2520sequential%2520testing%2520can%2520offer%2520a%2520statistically%2520grounded%2520and%2520efficient%2520mechanism%2520to%2520incorporate%2520safety%2520assurances%2520into%2520real-time%2520tracking%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Object%20Tracking%20Failure%20via%20Sequential%20Hypothesis%20Testing&entry.906535625=Alejandro%20Monroy%20Mu%C3%B1oz%20and%20Rajeev%20Verma%20and%20Alexander%20Timans&entry.1292438233=Real-time%20online%20object%20tracking%20in%20videos%20constitutes%20a%20core%20task%20in%20computer%20vision%2C%20with%20wide-ranging%20applications%20including%20video%20surveillance%2C%20motion%20capture%2C%20and%20robotics.%20Deployed%20tracking%20systems%20usually%20lack%20formal%20safety%20assurances%20to%20convey%20when%20tracking%20is%20reliable%20and%20when%20it%20may%20fail%2C%20at%20best%20relying%20on%20heuristic%20measures%20of%20model%20confidence%20to%20raise%20alerts.%20To%20obtain%20such%20assurances%20we%20propose%20interpreting%20object%20tracking%20as%20a%20sequential%20hypothesis%20test%2C%20wherein%20evidence%20for%20or%20against%20tracking%20failures%20is%20gradually%20accumulated%20over%20time.%20Leveraging%20recent%20advancements%20in%20the%20field%2C%20our%20sequential%20test%20%28formalized%20as%20an%20e-process%29%20quickly%20identifies%20when%20tracking%20failures%20set%20in%20whilst%20provably%20containing%20false%20alerts%20at%20a%20desired%20rate%2C%20and%20thus%20limiting%20potentially%20costly%20re-calibration%20or%20intervention%20steps.%20The%20approach%20is%20computationally%20light-weight%2C%20requires%20no%20extra%20training%20or%20fine-tuning%2C%20and%20is%20in%20principle%20model-agnostic.%20We%20propose%20both%20supervised%20and%20unsupervised%20variants%20by%20leveraging%20either%20ground-truth%20or%20solely%20internal%20tracking%20information%2C%20and%20demonstrate%20its%20effectiveness%20for%20two%20established%20tracking%20models%20across%20four%20video%20benchmarks.%20As%20such%2C%20sequential%20testing%20can%20offer%20a%20statistically%20grounded%20and%20efficient%20mechanism%20to%20incorporate%20safety%20assurances%20into%20real-time%20tracking%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2602.12983v1&entry.124074799=Read"},
{"title": "Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels", "author": "Julius Pesonen and Stefan Rua and Josef Taher and Niko Koivum\u00e4ki and Xiaowei Yu and Eija Honkavaara", "abstract": "Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task.", "link": "http://arxiv.org/abs/2602.13022v1", "date": "2026-02-13", "relevancy": 2.0794, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.528}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5151}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Image-based%20Tree%20Crown%20Segmentation%20from%20Enhanced%20Lidar-based%20Pseudo-labels&body=Title%3A%20Learning%20Image-based%20Tree%20Crown%20Segmentation%20from%20Enhanced%20Lidar-based%20Pseudo-labels%0AAuthor%3A%20Julius%20Pesonen%20and%20Stefan%20Rua%20and%20Josef%20Taher%20and%20Niko%20Koivum%C3%A4ki%20and%20Xiaowei%20Yu%20and%20Eija%20Honkavaara%0AAbstract%3A%20Mapping%20individual%20tree%20crowns%20is%20essential%20for%20tasks%20such%20as%20maintaining%20urban%20tree%20inventories%20and%20monitoring%20forest%20health%2C%20which%20help%20us%20understand%20and%20care%20for%20our%20environment.%20However%2C%20automatically%20separating%20the%20crowns%20from%20each%20other%20in%20aerial%20imagery%20is%20challenging%20due%20to%20factors%20such%20as%20the%20texture%20and%20partial%20tree%20crown%20overlaps.%20In%20this%20study%2C%20we%20present%20a%20method%20to%20train%20deep%20learning%20models%20that%20segment%20and%20separate%20individual%20trees%20from%20RGB%20and%20multispectral%20images%2C%20using%20pseudo-labels%20derived%20from%20aerial%20laser%20scanning%20%28ALS%29%20data.%20Our%20study%20shows%20that%20the%20ALS-derived%20pseudo-labels%20can%20be%20enhanced%20using%20a%20zero-shot%20instance%20segmentation%20model%2C%20Segment%20Anything%20Model%202%20%28SAM%202%29.%20Our%20method%20offers%20a%20way%20to%20obtain%20domain-specific%20training%20annotations%20for%20optical%20image-based%20models%20without%20any%20manual%20annotation%20cost%2C%20leading%20to%20segmentation%20models%20which%20outperform%20any%20available%20models%20which%20have%20been%20targeted%20for%20general%20domain%20deployment%20on%20the%20same%20task.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Image-based%2520Tree%2520Crown%2520Segmentation%2520from%2520Enhanced%2520Lidar-based%2520Pseudo-labels%26entry.906535625%3DJulius%2520Pesonen%2520and%2520Stefan%2520Rua%2520and%2520Josef%2520Taher%2520and%2520Niko%2520Koivum%25C3%25A4ki%2520and%2520Xiaowei%2520Yu%2520and%2520Eija%2520Honkavaara%26entry.1292438233%3DMapping%2520individual%2520tree%2520crowns%2520is%2520essential%2520for%2520tasks%2520such%2520as%2520maintaining%2520urban%2520tree%2520inventories%2520and%2520monitoring%2520forest%2520health%252C%2520which%2520help%2520us%2520understand%2520and%2520care%2520for%2520our%2520environment.%2520However%252C%2520automatically%2520separating%2520the%2520crowns%2520from%2520each%2520other%2520in%2520aerial%2520imagery%2520is%2520challenging%2520due%2520to%2520factors%2520such%2520as%2520the%2520texture%2520and%2520partial%2520tree%2520crown%2520overlaps.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520method%2520to%2520train%2520deep%2520learning%2520models%2520that%2520segment%2520and%2520separate%2520individual%2520trees%2520from%2520RGB%2520and%2520multispectral%2520images%252C%2520using%2520pseudo-labels%2520derived%2520from%2520aerial%2520laser%2520scanning%2520%2528ALS%2529%2520data.%2520Our%2520study%2520shows%2520that%2520the%2520ALS-derived%2520pseudo-labels%2520can%2520be%2520enhanced%2520using%2520a%2520zero-shot%2520instance%2520segmentation%2520model%252C%2520Segment%2520Anything%2520Model%25202%2520%2528SAM%25202%2529.%2520Our%2520method%2520offers%2520a%2520way%2520to%2520obtain%2520domain-specific%2520training%2520annotations%2520for%2520optical%2520image-based%2520models%2520without%2520any%2520manual%2520annotation%2520cost%252C%2520leading%2520to%2520segmentation%2520models%2520which%2520outperform%2520any%2520available%2520models%2520which%2520have%2520been%2520targeted%2520for%2520general%2520domain%2520deployment%2520on%2520the%2520same%2520task.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Image-based%20Tree%20Crown%20Segmentation%20from%20Enhanced%20Lidar-based%20Pseudo-labels&entry.906535625=Julius%20Pesonen%20and%20Stefan%20Rua%20and%20Josef%20Taher%20and%20Niko%20Koivum%C3%A4ki%20and%20Xiaowei%20Yu%20and%20Eija%20Honkavaara&entry.1292438233=Mapping%20individual%20tree%20crowns%20is%20essential%20for%20tasks%20such%20as%20maintaining%20urban%20tree%20inventories%20and%20monitoring%20forest%20health%2C%20which%20help%20us%20understand%20and%20care%20for%20our%20environment.%20However%2C%20automatically%20separating%20the%20crowns%20from%20each%20other%20in%20aerial%20imagery%20is%20challenging%20due%20to%20factors%20such%20as%20the%20texture%20and%20partial%20tree%20crown%20overlaps.%20In%20this%20study%2C%20we%20present%20a%20method%20to%20train%20deep%20learning%20models%20that%20segment%20and%20separate%20individual%20trees%20from%20RGB%20and%20multispectral%20images%2C%20using%20pseudo-labels%20derived%20from%20aerial%20laser%20scanning%20%28ALS%29%20data.%20Our%20study%20shows%20that%20the%20ALS-derived%20pseudo-labels%20can%20be%20enhanced%20using%20a%20zero-shot%20instance%20segmentation%20model%2C%20Segment%20Anything%20Model%202%20%28SAM%202%29.%20Our%20method%20offers%20a%20way%20to%20obtain%20domain-specific%20training%20annotations%20for%20optical%20image-based%20models%20without%20any%20manual%20annotation%20cost%2C%20leading%20to%20segmentation%20models%20which%20outperform%20any%20available%20models%20which%20have%20been%20targeted%20for%20general%20domain%20deployment%20on%20the%20same%20task.&entry.1838667208=http%3A//arxiv.org/abs/2602.13022v1&entry.124074799=Read"},
{"title": "SIEFormer: Spectral-Interpretable and -Enhanced Transformer for Generalized Category Discovery", "author": "Chunming Li and Shidong Wang and Tong Xin and Haofeng Zhang", "abstract": "This paper presents a novel approach, Spectral-Interpretable and -Enhanced Transformer (SIEFormer), which leverages spectral analysis to reinterpret the attention mechanism within Vision Transformer (ViT) and enhance feature adaptability, with particular emphasis on challenging Generalized Category Discovery (GCD) tasks. The proposed SIEFormer is composed of two main branches, each corresponding to an implicit and explicit spectral perspective of the ViT, enabling joint optimization. The implicit branch realizes the use of different types of graph Laplacians to model the local structure correlations of tokens, along with a novel Band-adaptive Filter (BaF) layer that can flexibly perform both band-pass and band-reject filtering. The explicit branch, on the other hand, introduces a Maneuverable Filtering Layer (MFL) that learns global dependencies among tokens by applying the Fourier transform to the input ``value\" features, modulating the transformed signal with a set of learnable parameters in the frequency domain, and then performing an inverse Fourier transform to obtain the enhanced features. Extensive experiments reveal state-of-the-art performance on multiple image recognition datasets, reaffirming the superiority of our approach through ablation studies and visualizations.", "link": "http://arxiv.org/abs/2602.13067v1", "date": "2026-02-13", "relevancy": 2.0758, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5447}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5277}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIEFormer%3A%20Spectral-Interpretable%20and%20-Enhanced%20Transformer%20for%20Generalized%20Category%20Discovery&body=Title%3A%20SIEFormer%3A%20Spectral-Interpretable%20and%20-Enhanced%20Transformer%20for%20Generalized%20Category%20Discovery%0AAuthor%3A%20Chunming%20Li%20and%20Shidong%20Wang%20and%20Tong%20Xin%20and%20Haofeng%20Zhang%0AAbstract%3A%20This%20paper%20presents%20a%20novel%20approach%2C%20Spectral-Interpretable%20and%20-Enhanced%20Transformer%20%28SIEFormer%29%2C%20which%20leverages%20spectral%20analysis%20to%20reinterpret%20the%20attention%20mechanism%20within%20Vision%20Transformer%20%28ViT%29%20and%20enhance%20feature%20adaptability%2C%20with%20particular%20emphasis%20on%20challenging%20Generalized%20Category%20Discovery%20%28GCD%29%20tasks.%20The%20proposed%20SIEFormer%20is%20composed%20of%20two%20main%20branches%2C%20each%20corresponding%20to%20an%20implicit%20and%20explicit%20spectral%20perspective%20of%20the%20ViT%2C%20enabling%20joint%20optimization.%20The%20implicit%20branch%20realizes%20the%20use%20of%20different%20types%20of%20graph%20Laplacians%20to%20model%20the%20local%20structure%20correlations%20of%20tokens%2C%20along%20with%20a%20novel%20Band-adaptive%20Filter%20%28BaF%29%20layer%20that%20can%20flexibly%20perform%20both%20band-pass%20and%20band-reject%20filtering.%20The%20explicit%20branch%2C%20on%20the%20other%20hand%2C%20introduces%20a%20Maneuverable%20Filtering%20Layer%20%28MFL%29%20that%20learns%20global%20dependencies%20among%20tokens%20by%20applying%20the%20Fourier%20transform%20to%20the%20input%20%60%60value%22%20features%2C%20modulating%20the%20transformed%20signal%20with%20a%20set%20of%20learnable%20parameters%20in%20the%20frequency%20domain%2C%20and%20then%20performing%20an%20inverse%20Fourier%20transform%20to%20obtain%20the%20enhanced%20features.%20Extensive%20experiments%20reveal%20state-of-the-art%20performance%20on%20multiple%20image%20recognition%20datasets%2C%20reaffirming%20the%20superiority%20of%20our%20approach%20through%20ablation%20studies%20and%20visualizations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIEFormer%253A%2520Spectral-Interpretable%2520and%2520-Enhanced%2520Transformer%2520for%2520Generalized%2520Category%2520Discovery%26entry.906535625%3DChunming%2520Li%2520and%2520Shidong%2520Wang%2520and%2520Tong%2520Xin%2520and%2520Haofeng%2520Zhang%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520novel%2520approach%252C%2520Spectral-Interpretable%2520and%2520-Enhanced%2520Transformer%2520%2528SIEFormer%2529%252C%2520which%2520leverages%2520spectral%2520analysis%2520to%2520reinterpret%2520the%2520attention%2520mechanism%2520within%2520Vision%2520Transformer%2520%2528ViT%2529%2520and%2520enhance%2520feature%2520adaptability%252C%2520with%2520particular%2520emphasis%2520on%2520challenging%2520Generalized%2520Category%2520Discovery%2520%2528GCD%2529%2520tasks.%2520The%2520proposed%2520SIEFormer%2520is%2520composed%2520of%2520two%2520main%2520branches%252C%2520each%2520corresponding%2520to%2520an%2520implicit%2520and%2520explicit%2520spectral%2520perspective%2520of%2520the%2520ViT%252C%2520enabling%2520joint%2520optimization.%2520The%2520implicit%2520branch%2520realizes%2520the%2520use%2520of%2520different%2520types%2520of%2520graph%2520Laplacians%2520to%2520model%2520the%2520local%2520structure%2520correlations%2520of%2520tokens%252C%2520along%2520with%2520a%2520novel%2520Band-adaptive%2520Filter%2520%2528BaF%2529%2520layer%2520that%2520can%2520flexibly%2520perform%2520both%2520band-pass%2520and%2520band-reject%2520filtering.%2520The%2520explicit%2520branch%252C%2520on%2520the%2520other%2520hand%252C%2520introduces%2520a%2520Maneuverable%2520Filtering%2520Layer%2520%2528MFL%2529%2520that%2520learns%2520global%2520dependencies%2520among%2520tokens%2520by%2520applying%2520the%2520Fourier%2520transform%2520to%2520the%2520input%2520%2560%2560value%2522%2520features%252C%2520modulating%2520the%2520transformed%2520signal%2520with%2520a%2520set%2520of%2520learnable%2520parameters%2520in%2520the%2520frequency%2520domain%252C%2520and%2520then%2520performing%2520an%2520inverse%2520Fourier%2520transform%2520to%2520obtain%2520the%2520enhanced%2520features.%2520Extensive%2520experiments%2520reveal%2520state-of-the-art%2520performance%2520on%2520multiple%2520image%2520recognition%2520datasets%252C%2520reaffirming%2520the%2520superiority%2520of%2520our%2520approach%2520through%2520ablation%2520studies%2520and%2520visualizations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIEFormer%3A%20Spectral-Interpretable%20and%20-Enhanced%20Transformer%20for%20Generalized%20Category%20Discovery&entry.906535625=Chunming%20Li%20and%20Shidong%20Wang%20and%20Tong%20Xin%20and%20Haofeng%20Zhang&entry.1292438233=This%20paper%20presents%20a%20novel%20approach%2C%20Spectral-Interpretable%20and%20-Enhanced%20Transformer%20%28SIEFormer%29%2C%20which%20leverages%20spectral%20analysis%20to%20reinterpret%20the%20attention%20mechanism%20within%20Vision%20Transformer%20%28ViT%29%20and%20enhance%20feature%20adaptability%2C%20with%20particular%20emphasis%20on%20challenging%20Generalized%20Category%20Discovery%20%28GCD%29%20tasks.%20The%20proposed%20SIEFormer%20is%20composed%20of%20two%20main%20branches%2C%20each%20corresponding%20to%20an%20implicit%20and%20explicit%20spectral%20perspective%20of%20the%20ViT%2C%20enabling%20joint%20optimization.%20The%20implicit%20branch%20realizes%20the%20use%20of%20different%20types%20of%20graph%20Laplacians%20to%20model%20the%20local%20structure%20correlations%20of%20tokens%2C%20along%20with%20a%20novel%20Band-adaptive%20Filter%20%28BaF%29%20layer%20that%20can%20flexibly%20perform%20both%20band-pass%20and%20band-reject%20filtering.%20The%20explicit%20branch%2C%20on%20the%20other%20hand%2C%20introduces%20a%20Maneuverable%20Filtering%20Layer%20%28MFL%29%20that%20learns%20global%20dependencies%20among%20tokens%20by%20applying%20the%20Fourier%20transform%20to%20the%20input%20%60%60value%22%20features%2C%20modulating%20the%20transformed%20signal%20with%20a%20set%20of%20learnable%20parameters%20in%20the%20frequency%20domain%2C%20and%20then%20performing%20an%20inverse%20Fourier%20transform%20to%20obtain%20the%20enhanced%20features.%20Extensive%20experiments%20reveal%20state-of-the-art%20performance%20on%20multiple%20image%20recognition%20datasets%2C%20reaffirming%20the%20superiority%20of%20our%20approach%20through%20ablation%20studies%20and%20visualizations.&entry.1838667208=http%3A//arxiv.org/abs/2602.13067v1&entry.124074799=Read"},
{"title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise", "author": "Yuejie Li and Ke Yang and Yueying Hua and Berlin Chen and Jianhao Nie and Yueping He and Caixin Kang", "abstract": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.", "link": "http://arxiv.org/abs/2602.12783v1", "date": "2026-02-13", "relevancy": 2.0729, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4189}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4189}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SQuTR%3A%20A%20Robustness%20Benchmark%20for%20Spoken%20Query%20to%20Text%20Retrieval%20under%20Acoustic%20Noise&body=Title%3A%20SQuTR%3A%20A%20Robustness%20Benchmark%20for%20Spoken%20Query%20to%20Text%20Retrieval%20under%20Acoustic%20Noise%0AAuthor%3A%20Yuejie%20Li%20and%20Ke%20Yang%20and%20Yueying%20Hua%20and%20Berlin%20Chen%20and%20Jianhao%20Nie%20and%20Yueping%20He%20and%20Caixin%20Kang%0AAbstract%3A%20Spoken%20query%20retrieval%20is%20an%20important%20interaction%20mode%20in%20modern%20information%20retrieval.%20However%2C%20existing%20evaluation%20datasets%20are%20often%20limited%20to%20simple%20queries%20under%20constrained%20noise%20conditions%2C%20making%20them%20inadequate%20for%20assessing%20the%20robustness%20of%20spoken%20query%20retrieval%20systems%20under%20complex%20acoustic%20perturbations.%20To%20address%20this%20limitation%2C%20we%20present%20SQuTR%2C%20a%20robustness%20benchmark%20for%20spoken%20query%20retrieval%20that%20includes%20a%20large-scale%20dataset%20and%20a%20unified%20evaluation%20protocol.%20SQuTR%20aggregates%2037%2C317%20unique%20queries%20from%20six%20commonly%20used%20English%20and%20Chinese%20text%20retrieval%20datasets%2C%20spanning%20multiple%20domains%20and%20diverse%20query%20types.%20We%20synthesize%20speech%20using%20voice%20profiles%20from%20200%20real%20speakers%20and%20mix%2017%20categories%20of%20real-world%20environmental%20noise%20under%20controlled%20SNR%20levels%2C%20enabling%20reproducible%20robustness%20evaluation%20from%20quiet%20to%20highly%20noisy%20conditions.%20Under%20the%20unified%20protocol%2C%20we%20conduct%20large-scale%20evaluations%20on%20representative%20cascaded%20and%20end-to-end%20retrieval%20systems.%20Experimental%20results%20show%20that%20retrieval%20performance%20decreases%20as%20noise%20increases%2C%20with%20substantially%20different%20drops%20across%20systems.%20Even%20large-scale%20retrieval%20models%20struggle%20under%20extreme%20noise%2C%20indicating%20that%20robustness%20remains%20a%20critical%20bottleneck.%20Overall%2C%20SQuTR%20provides%20a%20reproducible%20testbed%20for%20benchmarking%20and%20diagnostic%20analysis%2C%20and%20facilitates%20future%20research%20on%20robustness%20in%20spoken%20query%20to%20text%20retrieval.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSQuTR%253A%2520A%2520Robustness%2520Benchmark%2520for%2520Spoken%2520Query%2520to%2520Text%2520Retrieval%2520under%2520Acoustic%2520Noise%26entry.906535625%3DYuejie%2520Li%2520and%2520Ke%2520Yang%2520and%2520Yueying%2520Hua%2520and%2520Berlin%2520Chen%2520and%2520Jianhao%2520Nie%2520and%2520Yueping%2520He%2520and%2520Caixin%2520Kang%26entry.1292438233%3DSpoken%2520query%2520retrieval%2520is%2520an%2520important%2520interaction%2520mode%2520in%2520modern%2520information%2520retrieval.%2520However%252C%2520existing%2520evaluation%2520datasets%2520are%2520often%2520limited%2520to%2520simple%2520queries%2520under%2520constrained%2520noise%2520conditions%252C%2520making%2520them%2520inadequate%2520for%2520assessing%2520the%2520robustness%2520of%2520spoken%2520query%2520retrieval%2520systems%2520under%2520complex%2520acoustic%2520perturbations.%2520To%2520address%2520this%2520limitation%252C%2520we%2520present%2520SQuTR%252C%2520a%2520robustness%2520benchmark%2520for%2520spoken%2520query%2520retrieval%2520that%2520includes%2520a%2520large-scale%2520dataset%2520and%2520a%2520unified%2520evaluation%2520protocol.%2520SQuTR%2520aggregates%252037%252C317%2520unique%2520queries%2520from%2520six%2520commonly%2520used%2520English%2520and%2520Chinese%2520text%2520retrieval%2520datasets%252C%2520spanning%2520multiple%2520domains%2520and%2520diverse%2520query%2520types.%2520We%2520synthesize%2520speech%2520using%2520voice%2520profiles%2520from%2520200%2520real%2520speakers%2520and%2520mix%252017%2520categories%2520of%2520real-world%2520environmental%2520noise%2520under%2520controlled%2520SNR%2520levels%252C%2520enabling%2520reproducible%2520robustness%2520evaluation%2520from%2520quiet%2520to%2520highly%2520noisy%2520conditions.%2520Under%2520the%2520unified%2520protocol%252C%2520we%2520conduct%2520large-scale%2520evaluations%2520on%2520representative%2520cascaded%2520and%2520end-to-end%2520retrieval%2520systems.%2520Experimental%2520results%2520show%2520that%2520retrieval%2520performance%2520decreases%2520as%2520noise%2520increases%252C%2520with%2520substantially%2520different%2520drops%2520across%2520systems.%2520Even%2520large-scale%2520retrieval%2520models%2520struggle%2520under%2520extreme%2520noise%252C%2520indicating%2520that%2520robustness%2520remains%2520a%2520critical%2520bottleneck.%2520Overall%252C%2520SQuTR%2520provides%2520a%2520reproducible%2520testbed%2520for%2520benchmarking%2520and%2520diagnostic%2520analysis%252C%2520and%2520facilitates%2520future%2520research%2520on%2520robustness%2520in%2520spoken%2520query%2520to%2520text%2520retrieval.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SQuTR%3A%20A%20Robustness%20Benchmark%20for%20Spoken%20Query%20to%20Text%20Retrieval%20under%20Acoustic%20Noise&entry.906535625=Yuejie%20Li%20and%20Ke%20Yang%20and%20Yueying%20Hua%20and%20Berlin%20Chen%20and%20Jianhao%20Nie%20and%20Yueping%20He%20and%20Caixin%20Kang&entry.1292438233=Spoken%20query%20retrieval%20is%20an%20important%20interaction%20mode%20in%20modern%20information%20retrieval.%20However%2C%20existing%20evaluation%20datasets%20are%20often%20limited%20to%20simple%20queries%20under%20constrained%20noise%20conditions%2C%20making%20them%20inadequate%20for%20assessing%20the%20robustness%20of%20spoken%20query%20retrieval%20systems%20under%20complex%20acoustic%20perturbations.%20To%20address%20this%20limitation%2C%20we%20present%20SQuTR%2C%20a%20robustness%20benchmark%20for%20spoken%20query%20retrieval%20that%20includes%20a%20large-scale%20dataset%20and%20a%20unified%20evaluation%20protocol.%20SQuTR%20aggregates%2037%2C317%20unique%20queries%20from%20six%20commonly%20used%20English%20and%20Chinese%20text%20retrieval%20datasets%2C%20spanning%20multiple%20domains%20and%20diverse%20query%20types.%20We%20synthesize%20speech%20using%20voice%20profiles%20from%20200%20real%20speakers%20and%20mix%2017%20categories%20of%20real-world%20environmental%20noise%20under%20controlled%20SNR%20levels%2C%20enabling%20reproducible%20robustness%20evaluation%20from%20quiet%20to%20highly%20noisy%20conditions.%20Under%20the%20unified%20protocol%2C%20we%20conduct%20large-scale%20evaluations%20on%20representative%20cascaded%20and%20end-to-end%20retrieval%20systems.%20Experimental%20results%20show%20that%20retrieval%20performance%20decreases%20as%20noise%20increases%2C%20with%20substantially%20different%20drops%20across%20systems.%20Even%20large-scale%20retrieval%20models%20struggle%20under%20extreme%20noise%2C%20indicating%20that%20robustness%20remains%20a%20critical%20bottleneck.%20Overall%2C%20SQuTR%20provides%20a%20reproducible%20testbed%20for%20benchmarking%20and%20diagnostic%20analysis%2C%20and%20facilitates%20future%20research%20on%20robustness%20in%20spoken%20query%20to%20text%20retrieval.&entry.1838667208=http%3A//arxiv.org/abs/2602.12783v1&entry.124074799=Read"},
{"title": "FlashSchNet: Fast and Accurate Coarse-Grained Neural Network Molecular Dynamics", "author": "Pingzhi Li and Hongxuan Li and Zirui Liu and Xingcheng Lin and Tianlong Chen", "abstract": "Graph neural network (GNN) potentials such as SchNet improve the accuracy and transferability of molecular dynamics (MD) simulation by learning many-body interactions, but remain slower than classical force fields due to fragmented kernels and memory-bound pipelines that underutilize GPUs. We show that a missing principle is making GNN-MD IO-aware, carefully accounting for reads and writes between GPU high-bandwidth memory (HBM) and on-chip SRAM. We present FlashSchNet, an efficient and accurate IO-aware SchNet-style GNN-MD framework built on four techniques: (1) flash radial basis, which fuses pairwise distance computation, Gaussian basis expansion, and cosine envelope into a single tiled pass, computing each distance once and reusing it across all basis functions; (2) flash message passing, which fuses cutoff, neighbor gather, filter multiplication, and reduction to avoid materializing edge tensors in HBM; (3) flash aggregation, which reformulates scatter-add via CSR segment reduce, reducing atomic writes by a factor of feature dimension and enabling contention-free accumulation in both forward and backward passes; (4) channel-wise 16-bit quantization that exploits the low per-channel dynamic range in SchNet MLP weights to further improve throughput with negligible accuracy loss. On a single NVIDIA RTX PRO 6000, FlashSchNet achieves 1000 ns/day aggregate simulation throughput over 64 parallel replicas on coarse-grained (CG) protein containing 269 beads (6.5x faster than CGSchNet baseline with 80% reduction of peak memory), surpassing classical force fields (e.g. MARTINI) while retaining SchNet-level accuracy and transferability.", "link": "http://arxiv.org/abs/2602.13140v1", "date": "2026-02-13", "relevancy": 2.0706, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5263}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5127}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlashSchNet%3A%20Fast%20and%20Accurate%20Coarse-Grained%20Neural%20Network%20Molecular%20Dynamics&body=Title%3A%20FlashSchNet%3A%20Fast%20and%20Accurate%20Coarse-Grained%20Neural%20Network%20Molecular%20Dynamics%0AAuthor%3A%20Pingzhi%20Li%20and%20Hongxuan%20Li%20and%20Zirui%20Liu%20and%20Xingcheng%20Lin%20and%20Tianlong%20Chen%0AAbstract%3A%20Graph%20neural%20network%20%28GNN%29%20potentials%20such%20as%20SchNet%20improve%20the%20accuracy%20and%20transferability%20of%20molecular%20dynamics%20%28MD%29%20simulation%20by%20learning%20many-body%20interactions%2C%20but%20remain%20slower%20than%20classical%20force%20fields%20due%20to%20fragmented%20kernels%20and%20memory-bound%20pipelines%20that%20underutilize%20GPUs.%20We%20show%20that%20a%20missing%20principle%20is%20making%20GNN-MD%20IO-aware%2C%20carefully%20accounting%20for%20reads%20and%20writes%20between%20GPU%20high-bandwidth%20memory%20%28HBM%29%20and%20on-chip%20SRAM.%20We%20present%20FlashSchNet%2C%20an%20efficient%20and%20accurate%20IO-aware%20SchNet-style%20GNN-MD%20framework%20built%20on%20four%20techniques%3A%20%281%29%20flash%20radial%20basis%2C%20which%20fuses%20pairwise%20distance%20computation%2C%20Gaussian%20basis%20expansion%2C%20and%20cosine%20envelope%20into%20a%20single%20tiled%20pass%2C%20computing%20each%20distance%20once%20and%20reusing%20it%20across%20all%20basis%20functions%3B%20%282%29%20flash%20message%20passing%2C%20which%20fuses%20cutoff%2C%20neighbor%20gather%2C%20filter%20multiplication%2C%20and%20reduction%20to%20avoid%20materializing%20edge%20tensors%20in%20HBM%3B%20%283%29%20flash%20aggregation%2C%20which%20reformulates%20scatter-add%20via%20CSR%20segment%20reduce%2C%20reducing%20atomic%20writes%20by%20a%20factor%20of%20feature%20dimension%20and%20enabling%20contention-free%20accumulation%20in%20both%20forward%20and%20backward%20passes%3B%20%284%29%20channel-wise%2016-bit%20quantization%20that%20exploits%20the%20low%20per-channel%20dynamic%20range%20in%20SchNet%20MLP%20weights%20to%20further%20improve%20throughput%20with%20negligible%20accuracy%20loss.%20On%20a%20single%20NVIDIA%20RTX%20PRO%206000%2C%20FlashSchNet%20achieves%201000%20ns/day%20aggregate%20simulation%20throughput%20over%2064%20parallel%20replicas%20on%20coarse-grained%20%28CG%29%20protein%20containing%20269%20beads%20%286.5x%20faster%20than%20CGSchNet%20baseline%20with%2080%25%20reduction%20of%20peak%20memory%29%2C%20surpassing%20classical%20force%20fields%20%28e.g.%20MARTINI%29%20while%20retaining%20SchNet-level%20accuracy%20and%20transferability.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlashSchNet%253A%2520Fast%2520and%2520Accurate%2520Coarse-Grained%2520Neural%2520Network%2520Molecular%2520Dynamics%26entry.906535625%3DPingzhi%2520Li%2520and%2520Hongxuan%2520Li%2520and%2520Zirui%2520Liu%2520and%2520Xingcheng%2520Lin%2520and%2520Tianlong%2520Chen%26entry.1292438233%3DGraph%2520neural%2520network%2520%2528GNN%2529%2520potentials%2520such%2520as%2520SchNet%2520improve%2520the%2520accuracy%2520and%2520transferability%2520of%2520molecular%2520dynamics%2520%2528MD%2529%2520simulation%2520by%2520learning%2520many-body%2520interactions%252C%2520but%2520remain%2520slower%2520than%2520classical%2520force%2520fields%2520due%2520to%2520fragmented%2520kernels%2520and%2520memory-bound%2520pipelines%2520that%2520underutilize%2520GPUs.%2520We%2520show%2520that%2520a%2520missing%2520principle%2520is%2520making%2520GNN-MD%2520IO-aware%252C%2520carefully%2520accounting%2520for%2520reads%2520and%2520writes%2520between%2520GPU%2520high-bandwidth%2520memory%2520%2528HBM%2529%2520and%2520on-chip%2520SRAM.%2520We%2520present%2520FlashSchNet%252C%2520an%2520efficient%2520and%2520accurate%2520IO-aware%2520SchNet-style%2520GNN-MD%2520framework%2520built%2520on%2520four%2520techniques%253A%2520%25281%2529%2520flash%2520radial%2520basis%252C%2520which%2520fuses%2520pairwise%2520distance%2520computation%252C%2520Gaussian%2520basis%2520expansion%252C%2520and%2520cosine%2520envelope%2520into%2520a%2520single%2520tiled%2520pass%252C%2520computing%2520each%2520distance%2520once%2520and%2520reusing%2520it%2520across%2520all%2520basis%2520functions%253B%2520%25282%2529%2520flash%2520message%2520passing%252C%2520which%2520fuses%2520cutoff%252C%2520neighbor%2520gather%252C%2520filter%2520multiplication%252C%2520and%2520reduction%2520to%2520avoid%2520materializing%2520edge%2520tensors%2520in%2520HBM%253B%2520%25283%2529%2520flash%2520aggregation%252C%2520which%2520reformulates%2520scatter-add%2520via%2520CSR%2520segment%2520reduce%252C%2520reducing%2520atomic%2520writes%2520by%2520a%2520factor%2520of%2520feature%2520dimension%2520and%2520enabling%2520contention-free%2520accumulation%2520in%2520both%2520forward%2520and%2520backward%2520passes%253B%2520%25284%2529%2520channel-wise%252016-bit%2520quantization%2520that%2520exploits%2520the%2520low%2520per-channel%2520dynamic%2520range%2520in%2520SchNet%2520MLP%2520weights%2520to%2520further%2520improve%2520throughput%2520with%2520negligible%2520accuracy%2520loss.%2520On%2520a%2520single%2520NVIDIA%2520RTX%2520PRO%25206000%252C%2520FlashSchNet%2520achieves%25201000%2520ns/day%2520aggregate%2520simulation%2520throughput%2520over%252064%2520parallel%2520replicas%2520on%2520coarse-grained%2520%2528CG%2529%2520protein%2520containing%2520269%2520beads%2520%25286.5x%2520faster%2520than%2520CGSchNet%2520baseline%2520with%252080%2525%2520reduction%2520of%2520peak%2520memory%2529%252C%2520surpassing%2520classical%2520force%2520fields%2520%2528e.g.%2520MARTINI%2529%2520while%2520retaining%2520SchNet-level%2520accuracy%2520and%2520transferability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlashSchNet%3A%20Fast%20and%20Accurate%20Coarse-Grained%20Neural%20Network%20Molecular%20Dynamics&entry.906535625=Pingzhi%20Li%20and%20Hongxuan%20Li%20and%20Zirui%20Liu%20and%20Xingcheng%20Lin%20and%20Tianlong%20Chen&entry.1292438233=Graph%20neural%20network%20%28GNN%29%20potentials%20such%20as%20SchNet%20improve%20the%20accuracy%20and%20transferability%20of%20molecular%20dynamics%20%28MD%29%20simulation%20by%20learning%20many-body%20interactions%2C%20but%20remain%20slower%20than%20classical%20force%20fields%20due%20to%20fragmented%20kernels%20and%20memory-bound%20pipelines%20that%20underutilize%20GPUs.%20We%20show%20that%20a%20missing%20principle%20is%20making%20GNN-MD%20IO-aware%2C%20carefully%20accounting%20for%20reads%20and%20writes%20between%20GPU%20high-bandwidth%20memory%20%28HBM%29%20and%20on-chip%20SRAM.%20We%20present%20FlashSchNet%2C%20an%20efficient%20and%20accurate%20IO-aware%20SchNet-style%20GNN-MD%20framework%20built%20on%20four%20techniques%3A%20%281%29%20flash%20radial%20basis%2C%20which%20fuses%20pairwise%20distance%20computation%2C%20Gaussian%20basis%20expansion%2C%20and%20cosine%20envelope%20into%20a%20single%20tiled%20pass%2C%20computing%20each%20distance%20once%20and%20reusing%20it%20across%20all%20basis%20functions%3B%20%282%29%20flash%20message%20passing%2C%20which%20fuses%20cutoff%2C%20neighbor%20gather%2C%20filter%20multiplication%2C%20and%20reduction%20to%20avoid%20materializing%20edge%20tensors%20in%20HBM%3B%20%283%29%20flash%20aggregation%2C%20which%20reformulates%20scatter-add%20via%20CSR%20segment%20reduce%2C%20reducing%20atomic%20writes%20by%20a%20factor%20of%20feature%20dimension%20and%20enabling%20contention-free%20accumulation%20in%20both%20forward%20and%20backward%20passes%3B%20%284%29%20channel-wise%2016-bit%20quantization%20that%20exploits%20the%20low%20per-channel%20dynamic%20range%20in%20SchNet%20MLP%20weights%20to%20further%20improve%20throughput%20with%20negligible%20accuracy%20loss.%20On%20a%20single%20NVIDIA%20RTX%20PRO%206000%2C%20FlashSchNet%20achieves%201000%20ns/day%20aggregate%20simulation%20throughput%20over%2064%20parallel%20replicas%20on%20coarse-grained%20%28CG%29%20protein%20containing%20269%20beads%20%286.5x%20faster%20than%20CGSchNet%20baseline%20with%2080%25%20reduction%20of%20peak%20memory%29%2C%20surpassing%20classical%20force%20fields%20%28e.g.%20MARTINI%29%20while%20retaining%20SchNet-level%20accuracy%20and%20transferability.&entry.1838667208=http%3A//arxiv.org/abs/2602.13140v1&entry.124074799=Read"},
{"title": "GPTZero: Robust Detection of LLM-Generated Texts", "author": "George Alexandru Adam and Alexander Cui and Edwin Thomas and Emily Napier and Nazar Shmatko and Jacob Schnell and Jacob Junqi Tian and Alekhya Dronavalli and Edward Tian and Dongwon Lee", "abstract": "While historical considerations surrounding text authenticity revolved primarily around plagiarism, the advent of large language models (LLMs) has introduced a new challenge: distinguishing human-authored from AI-generated text. This shift raises significant concerns, including the undermining of skill evaluations, the mass-production of low-quality content, and the proliferation of misinformation. Addressing these issues, we introduce GPTZero a state-of-the-art industrial AI detection solution, offering reliable discernment between human and LLM-generated text. Our key contributions include: introducing a hierarchical, multi-task architecture enabling a flexible taxonomy of human and AI texts, demonstrating state-of-the-art accuracy on a variety of domains with granular predictions, and achieving superior robustness to adversarial attacks and paraphrasing via multi-tiered automated red teaming. GPTZero offers accurate and explainable detection, and educates users on its responsible use, ensuring fair and transparent assessment of text.", "link": "http://arxiv.org/abs/2602.13042v1", "date": "2026-02-13", "relevancy": 2.0647, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5241}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.522}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPTZero%3A%20Robust%20Detection%20of%20LLM-Generated%20Texts&body=Title%3A%20GPTZero%3A%20Robust%20Detection%20of%20LLM-Generated%20Texts%0AAuthor%3A%20George%20Alexandru%20Adam%20and%20Alexander%20Cui%20and%20Edwin%20Thomas%20and%20Emily%20Napier%20and%20Nazar%20Shmatko%20and%20Jacob%20Schnell%20and%20Jacob%20Junqi%20Tian%20and%20Alekhya%20Dronavalli%20and%20Edward%20Tian%20and%20Dongwon%20Lee%0AAbstract%3A%20While%20historical%20considerations%20surrounding%20text%20authenticity%20revolved%20primarily%20around%20plagiarism%2C%20the%20advent%20of%20large%20language%20models%20%28LLMs%29%20has%20introduced%20a%20new%20challenge%3A%20distinguishing%20human-authored%20from%20AI-generated%20text.%20This%20shift%20raises%20significant%20concerns%2C%20including%20the%20undermining%20of%20skill%20evaluations%2C%20the%20mass-production%20of%20low-quality%20content%2C%20and%20the%20proliferation%20of%20misinformation.%20Addressing%20these%20issues%2C%20we%20introduce%20GPTZero%20a%20state-of-the-art%20industrial%20AI%20detection%20solution%2C%20offering%20reliable%20discernment%20between%20human%20and%20LLM-generated%20text.%20Our%20key%20contributions%20include%3A%20introducing%20a%20hierarchical%2C%20multi-task%20architecture%20enabling%20a%20flexible%20taxonomy%20of%20human%20and%20AI%20texts%2C%20demonstrating%20state-of-the-art%20accuracy%20on%20a%20variety%20of%20domains%20with%20granular%20predictions%2C%20and%20achieving%20superior%20robustness%20to%20adversarial%20attacks%20and%20paraphrasing%20via%20multi-tiered%20automated%20red%20teaming.%20GPTZero%20offers%20accurate%20and%20explainable%20detection%2C%20and%20educates%20users%20on%20its%20responsible%20use%2C%20ensuring%20fair%20and%20transparent%20assessment%20of%20text.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPTZero%253A%2520Robust%2520Detection%2520of%2520LLM-Generated%2520Texts%26entry.906535625%3DGeorge%2520Alexandru%2520Adam%2520and%2520Alexander%2520Cui%2520and%2520Edwin%2520Thomas%2520and%2520Emily%2520Napier%2520and%2520Nazar%2520Shmatko%2520and%2520Jacob%2520Schnell%2520and%2520Jacob%2520Junqi%2520Tian%2520and%2520Alekhya%2520Dronavalli%2520and%2520Edward%2520Tian%2520and%2520Dongwon%2520Lee%26entry.1292438233%3DWhile%2520historical%2520considerations%2520surrounding%2520text%2520authenticity%2520revolved%2520primarily%2520around%2520plagiarism%252C%2520the%2520advent%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520introduced%2520a%2520new%2520challenge%253A%2520distinguishing%2520human-authored%2520from%2520AI-generated%2520text.%2520This%2520shift%2520raises%2520significant%2520concerns%252C%2520including%2520the%2520undermining%2520of%2520skill%2520evaluations%252C%2520the%2520mass-production%2520of%2520low-quality%2520content%252C%2520and%2520the%2520proliferation%2520of%2520misinformation.%2520Addressing%2520these%2520issues%252C%2520we%2520introduce%2520GPTZero%2520a%2520state-of-the-art%2520industrial%2520AI%2520detection%2520solution%252C%2520offering%2520reliable%2520discernment%2520between%2520human%2520and%2520LLM-generated%2520text.%2520Our%2520key%2520contributions%2520include%253A%2520introducing%2520a%2520hierarchical%252C%2520multi-task%2520architecture%2520enabling%2520a%2520flexible%2520taxonomy%2520of%2520human%2520and%2520AI%2520texts%252C%2520demonstrating%2520state-of-the-art%2520accuracy%2520on%2520a%2520variety%2520of%2520domains%2520with%2520granular%2520predictions%252C%2520and%2520achieving%2520superior%2520robustness%2520to%2520adversarial%2520attacks%2520and%2520paraphrasing%2520via%2520multi-tiered%2520automated%2520red%2520teaming.%2520GPTZero%2520offers%2520accurate%2520and%2520explainable%2520detection%252C%2520and%2520educates%2520users%2520on%2520its%2520responsible%2520use%252C%2520ensuring%2520fair%2520and%2520transparent%2520assessment%2520of%2520text.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPTZero%3A%20Robust%20Detection%20of%20LLM-Generated%20Texts&entry.906535625=George%20Alexandru%20Adam%20and%20Alexander%20Cui%20and%20Edwin%20Thomas%20and%20Emily%20Napier%20and%20Nazar%20Shmatko%20and%20Jacob%20Schnell%20and%20Jacob%20Junqi%20Tian%20and%20Alekhya%20Dronavalli%20and%20Edward%20Tian%20and%20Dongwon%20Lee&entry.1292438233=While%20historical%20considerations%20surrounding%20text%20authenticity%20revolved%20primarily%20around%20plagiarism%2C%20the%20advent%20of%20large%20language%20models%20%28LLMs%29%20has%20introduced%20a%20new%20challenge%3A%20distinguishing%20human-authored%20from%20AI-generated%20text.%20This%20shift%20raises%20significant%20concerns%2C%20including%20the%20undermining%20of%20skill%20evaluations%2C%20the%20mass-production%20of%20low-quality%20content%2C%20and%20the%20proliferation%20of%20misinformation.%20Addressing%20these%20issues%2C%20we%20introduce%20GPTZero%20a%20state-of-the-art%20industrial%20AI%20detection%20solution%2C%20offering%20reliable%20discernment%20between%20human%20and%20LLM-generated%20text.%20Our%20key%20contributions%20include%3A%20introducing%20a%20hierarchical%2C%20multi-task%20architecture%20enabling%20a%20flexible%20taxonomy%20of%20human%20and%20AI%20texts%2C%20demonstrating%20state-of-the-art%20accuracy%20on%20a%20variety%20of%20domains%20with%20granular%20predictions%2C%20and%20achieving%20superior%20robustness%20to%20adversarial%20attacks%20and%20paraphrasing%20via%20multi-tiered%20automated%20red%20teaming.%20GPTZero%20offers%20accurate%20and%20explainable%20detection%2C%20and%20educates%20users%20on%20its%20responsible%20use%2C%20ensuring%20fair%20and%20transparent%20assessment%20of%20text.&entry.1838667208=http%3A//arxiv.org/abs/2602.13042v1&entry.124074799=Read"},
{"title": "MAUNet-Light: A Concise MAUNet Architecture for Bias Correction and Downscaling of Precipitation Estimates", "author": "Sumanta Chandra Mishra Sharma and Adway Mitra and Auroop Ratan Ganguly", "abstract": "Satellite-derived data products and climate model simulations of geophysical variables like precipitation, often exhibit systematic biases compared to in-situ measurements. Bias correction and spatial downscaling are fundamental components to develop operational weather forecast systems, as they seek to improve the consistency between coarse-resolution climate model simulations or satellite-based estimates and ground-based observations. In recent years, deep learning-based models have been increasingly replaced traditional statistical methods to generate high-resolution, bias free projections of climate variables. For example, Max-Average U-Net (MAUNet) architecture has been demonstrated for its ability to downscale precipitation estimates. The versatility and adaptability of these neural models make them highly effective across a range of applications, though this often come at the cost of high computational and memory requirements. The aim of this research is to develop light-weight neural network architectures for both bias correction and downscaling of precipitation, for which the teacher-student based learning paradigm is explored. This research demonstrates the adaptability of MAUNet to the task of bias correction, and further introduces a compact, lightweight neural network architecture termed MAUNet-Light.The proposed MAUNet-Light model is developed by transferring knowledge from the trained MAUNet, and it is designed to perform both downscaling and bias correction with reduced computational requirements without any significant loss in accuracy compared to state-of-the-art.", "link": "http://arxiv.org/abs/2602.12980v1", "date": "2026-02-13", "relevancy": 2.0626, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5306}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5176}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAUNet-Light%3A%20A%20Concise%20MAUNet%20Architecture%20for%20Bias%20Correction%20and%20Downscaling%20of%20Precipitation%20Estimates&body=Title%3A%20MAUNet-Light%3A%20A%20Concise%20MAUNet%20Architecture%20for%20Bias%20Correction%20and%20Downscaling%20of%20Precipitation%20Estimates%0AAuthor%3A%20Sumanta%20Chandra%20Mishra%20Sharma%20and%20Adway%20Mitra%20and%20Auroop%20Ratan%20Ganguly%0AAbstract%3A%20Satellite-derived%20data%20products%20and%20climate%20model%20simulations%20of%20geophysical%20variables%20like%20precipitation%2C%20often%20exhibit%20systematic%20biases%20compared%20to%20in-situ%20measurements.%20Bias%20correction%20and%20spatial%20downscaling%20are%20fundamental%20components%20to%20develop%20operational%20weather%20forecast%20systems%2C%20as%20they%20seek%20to%20improve%20the%20consistency%20between%20coarse-resolution%20climate%20model%20simulations%20or%20satellite-based%20estimates%20and%20ground-based%20observations.%20In%20recent%20years%2C%20deep%20learning-based%20models%20have%20been%20increasingly%20replaced%20traditional%20statistical%20methods%20to%20generate%20high-resolution%2C%20bias%20free%20projections%20of%20climate%20variables.%20For%20example%2C%20Max-Average%20U-Net%20%28MAUNet%29%20architecture%20has%20been%20demonstrated%20for%20its%20ability%20to%20downscale%20precipitation%20estimates.%20The%20versatility%20and%20adaptability%20of%20these%20neural%20models%20make%20them%20highly%20effective%20across%20a%20range%20of%20applications%2C%20though%20this%20often%20come%20at%20the%20cost%20of%20high%20computational%20and%20memory%20requirements.%20The%20aim%20of%20this%20research%20is%20to%20develop%20light-weight%20neural%20network%20architectures%20for%20both%20bias%20correction%20and%20downscaling%20of%20precipitation%2C%20for%20which%20the%20teacher-student%20based%20learning%20paradigm%20is%20explored.%20This%20research%20demonstrates%20the%20adaptability%20of%20MAUNet%20to%20the%20task%20of%20bias%20correction%2C%20and%20further%20introduces%20a%20compact%2C%20lightweight%20neural%20network%20architecture%20termed%20MAUNet-Light.The%20proposed%20MAUNet-Light%20model%20is%20developed%20by%20transferring%20knowledge%20from%20the%20trained%20MAUNet%2C%20and%20it%20is%20designed%20to%20perform%20both%20downscaling%20and%20bias%20correction%20with%20reduced%20computational%20requirements%20without%20any%20significant%20loss%20in%20accuracy%20compared%20to%20state-of-the-art.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAUNet-Light%253A%2520A%2520Concise%2520MAUNet%2520Architecture%2520for%2520Bias%2520Correction%2520and%2520Downscaling%2520of%2520Precipitation%2520Estimates%26entry.906535625%3DSumanta%2520Chandra%2520Mishra%2520Sharma%2520and%2520Adway%2520Mitra%2520and%2520Auroop%2520Ratan%2520Ganguly%26entry.1292438233%3DSatellite-derived%2520data%2520products%2520and%2520climate%2520model%2520simulations%2520of%2520geophysical%2520variables%2520like%2520precipitation%252C%2520often%2520exhibit%2520systematic%2520biases%2520compared%2520to%2520in-situ%2520measurements.%2520Bias%2520correction%2520and%2520spatial%2520downscaling%2520are%2520fundamental%2520components%2520to%2520develop%2520operational%2520weather%2520forecast%2520systems%252C%2520as%2520they%2520seek%2520to%2520improve%2520the%2520consistency%2520between%2520coarse-resolution%2520climate%2520model%2520simulations%2520or%2520satellite-based%2520estimates%2520and%2520ground-based%2520observations.%2520In%2520recent%2520years%252C%2520deep%2520learning-based%2520models%2520have%2520been%2520increasingly%2520replaced%2520traditional%2520statistical%2520methods%2520to%2520generate%2520high-resolution%252C%2520bias%2520free%2520projections%2520of%2520climate%2520variables.%2520For%2520example%252C%2520Max-Average%2520U-Net%2520%2528MAUNet%2529%2520architecture%2520has%2520been%2520demonstrated%2520for%2520its%2520ability%2520to%2520downscale%2520precipitation%2520estimates.%2520The%2520versatility%2520and%2520adaptability%2520of%2520these%2520neural%2520models%2520make%2520them%2520highly%2520effective%2520across%2520a%2520range%2520of%2520applications%252C%2520though%2520this%2520often%2520come%2520at%2520the%2520cost%2520of%2520high%2520computational%2520and%2520memory%2520requirements.%2520The%2520aim%2520of%2520this%2520research%2520is%2520to%2520develop%2520light-weight%2520neural%2520network%2520architectures%2520for%2520both%2520bias%2520correction%2520and%2520downscaling%2520of%2520precipitation%252C%2520for%2520which%2520the%2520teacher-student%2520based%2520learning%2520paradigm%2520is%2520explored.%2520This%2520research%2520demonstrates%2520the%2520adaptability%2520of%2520MAUNet%2520to%2520the%2520task%2520of%2520bias%2520correction%252C%2520and%2520further%2520introduces%2520a%2520compact%252C%2520lightweight%2520neural%2520network%2520architecture%2520termed%2520MAUNet-Light.The%2520proposed%2520MAUNet-Light%2520model%2520is%2520developed%2520by%2520transferring%2520knowledge%2520from%2520the%2520trained%2520MAUNet%252C%2520and%2520it%2520is%2520designed%2520to%2520perform%2520both%2520downscaling%2520and%2520bias%2520correction%2520with%2520reduced%2520computational%2520requirements%2520without%2520any%2520significant%2520loss%2520in%2520accuracy%2520compared%2520to%2520state-of-the-art.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAUNet-Light%3A%20A%20Concise%20MAUNet%20Architecture%20for%20Bias%20Correction%20and%20Downscaling%20of%20Precipitation%20Estimates&entry.906535625=Sumanta%20Chandra%20Mishra%20Sharma%20and%20Adway%20Mitra%20and%20Auroop%20Ratan%20Ganguly&entry.1292438233=Satellite-derived%20data%20products%20and%20climate%20model%20simulations%20of%20geophysical%20variables%20like%20precipitation%2C%20often%20exhibit%20systematic%20biases%20compared%20to%20in-situ%20measurements.%20Bias%20correction%20and%20spatial%20downscaling%20are%20fundamental%20components%20to%20develop%20operational%20weather%20forecast%20systems%2C%20as%20they%20seek%20to%20improve%20the%20consistency%20between%20coarse-resolution%20climate%20model%20simulations%20or%20satellite-based%20estimates%20and%20ground-based%20observations.%20In%20recent%20years%2C%20deep%20learning-based%20models%20have%20been%20increasingly%20replaced%20traditional%20statistical%20methods%20to%20generate%20high-resolution%2C%20bias%20free%20projections%20of%20climate%20variables.%20For%20example%2C%20Max-Average%20U-Net%20%28MAUNet%29%20architecture%20has%20been%20demonstrated%20for%20its%20ability%20to%20downscale%20precipitation%20estimates.%20The%20versatility%20and%20adaptability%20of%20these%20neural%20models%20make%20them%20highly%20effective%20across%20a%20range%20of%20applications%2C%20though%20this%20often%20come%20at%20the%20cost%20of%20high%20computational%20and%20memory%20requirements.%20The%20aim%20of%20this%20research%20is%20to%20develop%20light-weight%20neural%20network%20architectures%20for%20both%20bias%20correction%20and%20downscaling%20of%20precipitation%2C%20for%20which%20the%20teacher-student%20based%20learning%20paradigm%20is%20explored.%20This%20research%20demonstrates%20the%20adaptability%20of%20MAUNet%20to%20the%20task%20of%20bias%20correction%2C%20and%20further%20introduces%20a%20compact%2C%20lightweight%20neural%20network%20architecture%20termed%20MAUNet-Light.The%20proposed%20MAUNet-Light%20model%20is%20developed%20by%20transferring%20knowledge%20from%20the%20trained%20MAUNet%2C%20and%20it%20is%20designed%20to%20perform%20both%20downscaling%20and%20bias%20correction%20with%20reduced%20computational%20requirements%20without%20any%20significant%20loss%20in%20accuracy%20compared%20to%20state-of-the-art.&entry.1838667208=http%3A//arxiv.org/abs/2602.12980v1&entry.124074799=Read"},
{"title": "Deep-Learning Atlas Registration for Melanoma Brain Metastases: Preserving Pathology While Enabling Cohort-Level Analyses", "author": "Nanna E. Wielenberg and Ilinca Popp and Oliver Blanck and Lucas Zander and Jan C. Peeken and Stephanie E. Combs and Anca-Ligia Grosu and Dimos Baltas and Tobias Fechter", "abstract": "Melanoma brain metastases (MBM) are common and spatially heterogeneous lesions, complicating cohort-level analyses due to anatomical variability and differing MRI protocols. We propose a fully differentiable, deep-learning-based deformable registration framework that aligns individual pathological brains to a common atlas while preserving metastatic tissue without requiring lesion masks or preprocessing.\n  Missing anatomical correspondences caused by metastases are handled through a forward-model similarity metric based on distance-transformed anatomical labels, combined with a volume-preserving regularization term to ensure deformation plausibility. Registration performance was evaluated using Dice coefficient (DSC), Hausdorff distance (HD), average symmetric surface distance (ASSD), and Jacobian-based measures. The method was applied to 209 MBM patients from three centres, enabling standardized mapping of metastases to anatomical, arterial, and perfusion atlases.\n  The framework achieved high registration accuracy across datasets (DSC 0.89-0.92, HD 6.79-7.60 mm, ASSD 0.63-0.77 mm) while preserving metastatic volumes. Spatial analysis demonstrated significant over-representation of MBM in the cerebral cortex and putamen, under-representation in white matter, and consistent localization near the gray-white matter junction. No arterial territory showed increased metastasis frequency after volume correction.\n  This approach enables robust atlas registration of pathological brain MRI without lesion masks and supports reproducible multi-centre analyses. Applied to MBM, it confirms and refines known spatial predilections, particularly preferential seeding near the gray-white matter junction and cortical regions. The publicly available implementation facilitates reproducible research and extension to other brain tumours and neurological pathologies.", "link": "http://arxiv.org/abs/2602.12933v1", "date": "2026-02-13", "relevancy": 2.0611, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5276}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5093}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep-Learning%20Atlas%20Registration%20for%20Melanoma%20Brain%20Metastases%3A%20Preserving%20Pathology%20While%20Enabling%20Cohort-Level%20Analyses&body=Title%3A%20Deep-Learning%20Atlas%20Registration%20for%20Melanoma%20Brain%20Metastases%3A%20Preserving%20Pathology%20While%20Enabling%20Cohort-Level%20Analyses%0AAuthor%3A%20Nanna%20E.%20Wielenberg%20and%20Ilinca%20Popp%20and%20Oliver%20Blanck%20and%20Lucas%20Zander%20and%20Jan%20C.%20Peeken%20and%20Stephanie%20E.%20Combs%20and%20Anca-Ligia%20Grosu%20and%20Dimos%20Baltas%20and%20Tobias%20Fechter%0AAbstract%3A%20Melanoma%20brain%20metastases%20%28MBM%29%20are%20common%20and%20spatially%20heterogeneous%20lesions%2C%20complicating%20cohort-level%20analyses%20due%20to%20anatomical%20variability%20and%20differing%20MRI%20protocols.%20We%20propose%20a%20fully%20differentiable%2C%20deep-learning-based%20deformable%20registration%20framework%20that%20aligns%20individual%20pathological%20brains%20to%20a%20common%20atlas%20while%20preserving%20metastatic%20tissue%20without%20requiring%20lesion%20masks%20or%20preprocessing.%0A%20%20Missing%20anatomical%20correspondences%20caused%20by%20metastases%20are%20handled%20through%20a%20forward-model%20similarity%20metric%20based%20on%20distance-transformed%20anatomical%20labels%2C%20combined%20with%20a%20volume-preserving%20regularization%20term%20to%20ensure%20deformation%20plausibility.%20Registration%20performance%20was%20evaluated%20using%20Dice%20coefficient%20%28DSC%29%2C%20Hausdorff%20distance%20%28HD%29%2C%20average%20symmetric%20surface%20distance%20%28ASSD%29%2C%20and%20Jacobian-based%20measures.%20The%20method%20was%20applied%20to%20209%20MBM%20patients%20from%20three%20centres%2C%20enabling%20standardized%20mapping%20of%20metastases%20to%20anatomical%2C%20arterial%2C%20and%20perfusion%20atlases.%0A%20%20The%20framework%20achieved%20high%20registration%20accuracy%20across%20datasets%20%28DSC%200.89-0.92%2C%20HD%206.79-7.60%20mm%2C%20ASSD%200.63-0.77%20mm%29%20while%20preserving%20metastatic%20volumes.%20Spatial%20analysis%20demonstrated%20significant%20over-representation%20of%20MBM%20in%20the%20cerebral%20cortex%20and%20putamen%2C%20under-representation%20in%20white%20matter%2C%20and%20consistent%20localization%20near%20the%20gray-white%20matter%20junction.%20No%20arterial%20territory%20showed%20increased%20metastasis%20frequency%20after%20volume%20correction.%0A%20%20This%20approach%20enables%20robust%20atlas%20registration%20of%20pathological%20brain%20MRI%20without%20lesion%20masks%20and%20supports%20reproducible%20multi-centre%20analyses.%20Applied%20to%20MBM%2C%20it%20confirms%20and%20refines%20known%20spatial%20predilections%2C%20particularly%20preferential%20seeding%20near%20the%20gray-white%20matter%20junction%20and%20cortical%20regions.%20The%20publicly%20available%20implementation%20facilitates%20reproducible%20research%20and%20extension%20to%20other%20brain%20tumours%20and%20neurological%20pathologies.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep-Learning%2520Atlas%2520Registration%2520for%2520Melanoma%2520Brain%2520Metastases%253A%2520Preserving%2520Pathology%2520While%2520Enabling%2520Cohort-Level%2520Analyses%26entry.906535625%3DNanna%2520E.%2520Wielenberg%2520and%2520Ilinca%2520Popp%2520and%2520Oliver%2520Blanck%2520and%2520Lucas%2520Zander%2520and%2520Jan%2520C.%2520Peeken%2520and%2520Stephanie%2520E.%2520Combs%2520and%2520Anca-Ligia%2520Grosu%2520and%2520Dimos%2520Baltas%2520and%2520Tobias%2520Fechter%26entry.1292438233%3DMelanoma%2520brain%2520metastases%2520%2528MBM%2529%2520are%2520common%2520and%2520spatially%2520heterogeneous%2520lesions%252C%2520complicating%2520cohort-level%2520analyses%2520due%2520to%2520anatomical%2520variability%2520and%2520differing%2520MRI%2520protocols.%2520We%2520propose%2520a%2520fully%2520differentiable%252C%2520deep-learning-based%2520deformable%2520registration%2520framework%2520that%2520aligns%2520individual%2520pathological%2520brains%2520to%2520a%2520common%2520atlas%2520while%2520preserving%2520metastatic%2520tissue%2520without%2520requiring%2520lesion%2520masks%2520or%2520preprocessing.%250A%2520%2520Missing%2520anatomical%2520correspondences%2520caused%2520by%2520metastases%2520are%2520handled%2520through%2520a%2520forward-model%2520similarity%2520metric%2520based%2520on%2520distance-transformed%2520anatomical%2520labels%252C%2520combined%2520with%2520a%2520volume-preserving%2520regularization%2520term%2520to%2520ensure%2520deformation%2520plausibility.%2520Registration%2520performance%2520was%2520evaluated%2520using%2520Dice%2520coefficient%2520%2528DSC%2529%252C%2520Hausdorff%2520distance%2520%2528HD%2529%252C%2520average%2520symmetric%2520surface%2520distance%2520%2528ASSD%2529%252C%2520and%2520Jacobian-based%2520measures.%2520The%2520method%2520was%2520applied%2520to%2520209%2520MBM%2520patients%2520from%2520three%2520centres%252C%2520enabling%2520standardized%2520mapping%2520of%2520metastases%2520to%2520anatomical%252C%2520arterial%252C%2520and%2520perfusion%2520atlases.%250A%2520%2520The%2520framework%2520achieved%2520high%2520registration%2520accuracy%2520across%2520datasets%2520%2528DSC%25200.89-0.92%252C%2520HD%25206.79-7.60%2520mm%252C%2520ASSD%25200.63-0.77%2520mm%2529%2520while%2520preserving%2520metastatic%2520volumes.%2520Spatial%2520analysis%2520demonstrated%2520significant%2520over-representation%2520of%2520MBM%2520in%2520the%2520cerebral%2520cortex%2520and%2520putamen%252C%2520under-representation%2520in%2520white%2520matter%252C%2520and%2520consistent%2520localization%2520near%2520the%2520gray-white%2520matter%2520junction.%2520No%2520arterial%2520territory%2520showed%2520increased%2520metastasis%2520frequency%2520after%2520volume%2520correction.%250A%2520%2520This%2520approach%2520enables%2520robust%2520atlas%2520registration%2520of%2520pathological%2520brain%2520MRI%2520without%2520lesion%2520masks%2520and%2520supports%2520reproducible%2520multi-centre%2520analyses.%2520Applied%2520to%2520MBM%252C%2520it%2520confirms%2520and%2520refines%2520known%2520spatial%2520predilections%252C%2520particularly%2520preferential%2520seeding%2520near%2520the%2520gray-white%2520matter%2520junction%2520and%2520cortical%2520regions.%2520The%2520publicly%2520available%2520implementation%2520facilitates%2520reproducible%2520research%2520and%2520extension%2520to%2520other%2520brain%2520tumours%2520and%2520neurological%2520pathologies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-Learning%20Atlas%20Registration%20for%20Melanoma%20Brain%20Metastases%3A%20Preserving%20Pathology%20While%20Enabling%20Cohort-Level%20Analyses&entry.906535625=Nanna%20E.%20Wielenberg%20and%20Ilinca%20Popp%20and%20Oliver%20Blanck%20and%20Lucas%20Zander%20and%20Jan%20C.%20Peeken%20and%20Stephanie%20E.%20Combs%20and%20Anca-Ligia%20Grosu%20and%20Dimos%20Baltas%20and%20Tobias%20Fechter&entry.1292438233=Melanoma%20brain%20metastases%20%28MBM%29%20are%20common%20and%20spatially%20heterogeneous%20lesions%2C%20complicating%20cohort-level%20analyses%20due%20to%20anatomical%20variability%20and%20differing%20MRI%20protocols.%20We%20propose%20a%20fully%20differentiable%2C%20deep-learning-based%20deformable%20registration%20framework%20that%20aligns%20individual%20pathological%20brains%20to%20a%20common%20atlas%20while%20preserving%20metastatic%20tissue%20without%20requiring%20lesion%20masks%20or%20preprocessing.%0A%20%20Missing%20anatomical%20correspondences%20caused%20by%20metastases%20are%20handled%20through%20a%20forward-model%20similarity%20metric%20based%20on%20distance-transformed%20anatomical%20labels%2C%20combined%20with%20a%20volume-preserving%20regularization%20term%20to%20ensure%20deformation%20plausibility.%20Registration%20performance%20was%20evaluated%20using%20Dice%20coefficient%20%28DSC%29%2C%20Hausdorff%20distance%20%28HD%29%2C%20average%20symmetric%20surface%20distance%20%28ASSD%29%2C%20and%20Jacobian-based%20measures.%20The%20method%20was%20applied%20to%20209%20MBM%20patients%20from%20three%20centres%2C%20enabling%20standardized%20mapping%20of%20metastases%20to%20anatomical%2C%20arterial%2C%20and%20perfusion%20atlases.%0A%20%20The%20framework%20achieved%20high%20registration%20accuracy%20across%20datasets%20%28DSC%200.89-0.92%2C%20HD%206.79-7.60%20mm%2C%20ASSD%200.63-0.77%20mm%29%20while%20preserving%20metastatic%20volumes.%20Spatial%20analysis%20demonstrated%20significant%20over-representation%20of%20MBM%20in%20the%20cerebral%20cortex%20and%20putamen%2C%20under-representation%20in%20white%20matter%2C%20and%20consistent%20localization%20near%20the%20gray-white%20matter%20junction.%20No%20arterial%20territory%20showed%20increased%20metastasis%20frequency%20after%20volume%20correction.%0A%20%20This%20approach%20enables%20robust%20atlas%20registration%20of%20pathological%20brain%20MRI%20without%20lesion%20masks%20and%20supports%20reproducible%20multi-centre%20analyses.%20Applied%20to%20MBM%2C%20it%20confirms%20and%20refines%20known%20spatial%20predilections%2C%20particularly%20preferential%20seeding%20near%20the%20gray-white%20matter%20junction%20and%20cortical%20regions.%20The%20publicly%20available%20implementation%20facilitates%20reproducible%20research%20and%20extension%20to%20other%20brain%20tumours%20and%20neurological%20pathologies.&entry.1838667208=http%3A//arxiv.org/abs/2602.12933v1&entry.124074799=Read"},
{"title": "Ultrasound-Guided Real-Time Spinal Motion Visualization for Spinal Instability Assessment", "author": "Feng Li and Yuan Bi and Tianyu Song and Zhongliang Jiang and Nassir Navab", "abstract": "Purpose: Spinal instability is a widespread condition that causes pain, fatigue, and restricted mobility, profoundly affecting patients' quality of life. In clinical practice, the gold standard for diagnosis is dynamic X-ray imaging. However, X-ray provides only 2D motion information, while 3D modalities such as computed tomography (CT) or cone beam computed tomography (CBCT) cannot efficiently capture motion. Therefore, there is a need for a system capable of visualizing real-time 3D spinal motion while minimizing radiation exposure.\n  Methods: We propose ultrasound as an auxiliary modality for 3D spine visualization. Due to acoustic limitations, ultrasound captures only the superficial spinal surface. Therefore, the partially compounded ultrasound volume is registered to preoperative 3D imaging. In this study, CBCT provides the neutral spine configuration, while robotic ultrasound acquisition is performed at maximal spinal bending. A kinematic model is applied to the CBCT-derived spine model for coarse registration, followed by ICP for fine registration, with kinematic parameters optimized based on the registration results. Real-time ultrasound motion tracking is then used to estimate continuous 3D spinal motion by interpolating between the neutral and maximally bent states.\n  Results: The pipeline was evaluated on a bendable 3D-printed lumbar spine phantom. The registration error was $1.941 \\pm 0.199$ mm and the interpolated spinal motion error was $2.01 \\pm 0.309$ mm (median).\n  Conclusion: The proposed robotic ultrasound framework enables radiation-reduced, real-time 3D visualization of spinal motion, offering a promising 3D alternative to conventional dynamic X-ray imaging for assessing spinal instability.", "link": "http://arxiv.org/abs/2602.12917v1", "date": "2026-02-13", "relevancy": 2.0611, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5331}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.504}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultrasound-Guided%20Real-Time%20Spinal%20Motion%20Visualization%20for%20Spinal%20Instability%20Assessment&body=Title%3A%20Ultrasound-Guided%20Real-Time%20Spinal%20Motion%20Visualization%20for%20Spinal%20Instability%20Assessment%0AAuthor%3A%20Feng%20Li%20and%20Yuan%20Bi%20and%20Tianyu%20Song%20and%20Zhongliang%20Jiang%20and%20Nassir%20Navab%0AAbstract%3A%20Purpose%3A%20Spinal%20instability%20is%20a%20widespread%20condition%20that%20causes%20pain%2C%20fatigue%2C%20and%20restricted%20mobility%2C%20profoundly%20affecting%20patients%27%20quality%20of%20life.%20In%20clinical%20practice%2C%20the%20gold%20standard%20for%20diagnosis%20is%20dynamic%20X-ray%20imaging.%20However%2C%20X-ray%20provides%20only%202D%20motion%20information%2C%20while%203D%20modalities%20such%20as%20computed%20tomography%20%28CT%29%20or%20cone%20beam%20computed%20tomography%20%28CBCT%29%20cannot%20efficiently%20capture%20motion.%20Therefore%2C%20there%20is%20a%20need%20for%20a%20system%20capable%20of%20visualizing%20real-time%203D%20spinal%20motion%20while%20minimizing%20radiation%20exposure.%0A%20%20Methods%3A%20We%20propose%20ultrasound%20as%20an%20auxiliary%20modality%20for%203D%20spine%20visualization.%20Due%20to%20acoustic%20limitations%2C%20ultrasound%20captures%20only%20the%20superficial%20spinal%20surface.%20Therefore%2C%20the%20partially%20compounded%20ultrasound%20volume%20is%20registered%20to%20preoperative%203D%20imaging.%20In%20this%20study%2C%20CBCT%20provides%20the%20neutral%20spine%20configuration%2C%20while%20robotic%20ultrasound%20acquisition%20is%20performed%20at%20maximal%20spinal%20bending.%20A%20kinematic%20model%20is%20applied%20to%20the%20CBCT-derived%20spine%20model%20for%20coarse%20registration%2C%20followed%20by%20ICP%20for%20fine%20registration%2C%20with%20kinematic%20parameters%20optimized%20based%20on%20the%20registration%20results.%20Real-time%20ultrasound%20motion%20tracking%20is%20then%20used%20to%20estimate%20continuous%203D%20spinal%20motion%20by%20interpolating%20between%20the%20neutral%20and%20maximally%20bent%20states.%0A%20%20Results%3A%20The%20pipeline%20was%20evaluated%20on%20a%20bendable%203D-printed%20lumbar%20spine%20phantom.%20The%20registration%20error%20was%20%241.941%20%5Cpm%200.199%24%20mm%20and%20the%20interpolated%20spinal%20motion%20error%20was%20%242.01%20%5Cpm%200.309%24%20mm%20%28median%29.%0A%20%20Conclusion%3A%20The%20proposed%20robotic%20ultrasound%20framework%20enables%20radiation-reduced%2C%20real-time%203D%20visualization%20of%20spinal%20motion%2C%20offering%20a%20promising%203D%20alternative%20to%20conventional%20dynamic%20X-ray%20imaging%20for%20assessing%20spinal%20instability.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltrasound-Guided%2520Real-Time%2520Spinal%2520Motion%2520Visualization%2520for%2520Spinal%2520Instability%2520Assessment%26entry.906535625%3DFeng%2520Li%2520and%2520Yuan%2520Bi%2520and%2520Tianyu%2520Song%2520and%2520Zhongliang%2520Jiang%2520and%2520Nassir%2520Navab%26entry.1292438233%3DPurpose%253A%2520Spinal%2520instability%2520is%2520a%2520widespread%2520condition%2520that%2520causes%2520pain%252C%2520fatigue%252C%2520and%2520restricted%2520mobility%252C%2520profoundly%2520affecting%2520patients%2527%2520quality%2520of%2520life.%2520In%2520clinical%2520practice%252C%2520the%2520gold%2520standard%2520for%2520diagnosis%2520is%2520dynamic%2520X-ray%2520imaging.%2520However%252C%2520X-ray%2520provides%2520only%25202D%2520motion%2520information%252C%2520while%25203D%2520modalities%2520such%2520as%2520computed%2520tomography%2520%2528CT%2529%2520or%2520cone%2520beam%2520computed%2520tomography%2520%2528CBCT%2529%2520cannot%2520efficiently%2520capture%2520motion.%2520Therefore%252C%2520there%2520is%2520a%2520need%2520for%2520a%2520system%2520capable%2520of%2520visualizing%2520real-time%25203D%2520spinal%2520motion%2520while%2520minimizing%2520radiation%2520exposure.%250A%2520%2520Methods%253A%2520We%2520propose%2520ultrasound%2520as%2520an%2520auxiliary%2520modality%2520for%25203D%2520spine%2520visualization.%2520Due%2520to%2520acoustic%2520limitations%252C%2520ultrasound%2520captures%2520only%2520the%2520superficial%2520spinal%2520surface.%2520Therefore%252C%2520the%2520partially%2520compounded%2520ultrasound%2520volume%2520is%2520registered%2520to%2520preoperative%25203D%2520imaging.%2520In%2520this%2520study%252C%2520CBCT%2520provides%2520the%2520neutral%2520spine%2520configuration%252C%2520while%2520robotic%2520ultrasound%2520acquisition%2520is%2520performed%2520at%2520maximal%2520spinal%2520bending.%2520A%2520kinematic%2520model%2520is%2520applied%2520to%2520the%2520CBCT-derived%2520spine%2520model%2520for%2520coarse%2520registration%252C%2520followed%2520by%2520ICP%2520for%2520fine%2520registration%252C%2520with%2520kinematic%2520parameters%2520optimized%2520based%2520on%2520the%2520registration%2520results.%2520Real-time%2520ultrasound%2520motion%2520tracking%2520is%2520then%2520used%2520to%2520estimate%2520continuous%25203D%2520spinal%2520motion%2520by%2520interpolating%2520between%2520the%2520neutral%2520and%2520maximally%2520bent%2520states.%250A%2520%2520Results%253A%2520The%2520pipeline%2520was%2520evaluated%2520on%2520a%2520bendable%25203D-printed%2520lumbar%2520spine%2520phantom.%2520The%2520registration%2520error%2520was%2520%25241.941%2520%255Cpm%25200.199%2524%2520mm%2520and%2520the%2520interpolated%2520spinal%2520motion%2520error%2520was%2520%25242.01%2520%255Cpm%25200.309%2524%2520mm%2520%2528median%2529.%250A%2520%2520Conclusion%253A%2520The%2520proposed%2520robotic%2520ultrasound%2520framework%2520enables%2520radiation-reduced%252C%2520real-time%25203D%2520visualization%2520of%2520spinal%2520motion%252C%2520offering%2520a%2520promising%25203D%2520alternative%2520to%2520conventional%2520dynamic%2520X-ray%2520imaging%2520for%2520assessing%2520spinal%2520instability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultrasound-Guided%20Real-Time%20Spinal%20Motion%20Visualization%20for%20Spinal%20Instability%20Assessment&entry.906535625=Feng%20Li%20and%20Yuan%20Bi%20and%20Tianyu%20Song%20and%20Zhongliang%20Jiang%20and%20Nassir%20Navab&entry.1292438233=Purpose%3A%20Spinal%20instability%20is%20a%20widespread%20condition%20that%20causes%20pain%2C%20fatigue%2C%20and%20restricted%20mobility%2C%20profoundly%20affecting%20patients%27%20quality%20of%20life.%20In%20clinical%20practice%2C%20the%20gold%20standard%20for%20diagnosis%20is%20dynamic%20X-ray%20imaging.%20However%2C%20X-ray%20provides%20only%202D%20motion%20information%2C%20while%203D%20modalities%20such%20as%20computed%20tomography%20%28CT%29%20or%20cone%20beam%20computed%20tomography%20%28CBCT%29%20cannot%20efficiently%20capture%20motion.%20Therefore%2C%20there%20is%20a%20need%20for%20a%20system%20capable%20of%20visualizing%20real-time%203D%20spinal%20motion%20while%20minimizing%20radiation%20exposure.%0A%20%20Methods%3A%20We%20propose%20ultrasound%20as%20an%20auxiliary%20modality%20for%203D%20spine%20visualization.%20Due%20to%20acoustic%20limitations%2C%20ultrasound%20captures%20only%20the%20superficial%20spinal%20surface.%20Therefore%2C%20the%20partially%20compounded%20ultrasound%20volume%20is%20registered%20to%20preoperative%203D%20imaging.%20In%20this%20study%2C%20CBCT%20provides%20the%20neutral%20spine%20configuration%2C%20while%20robotic%20ultrasound%20acquisition%20is%20performed%20at%20maximal%20spinal%20bending.%20A%20kinematic%20model%20is%20applied%20to%20the%20CBCT-derived%20spine%20model%20for%20coarse%20registration%2C%20followed%20by%20ICP%20for%20fine%20registration%2C%20with%20kinematic%20parameters%20optimized%20based%20on%20the%20registration%20results.%20Real-time%20ultrasound%20motion%20tracking%20is%20then%20used%20to%20estimate%20continuous%203D%20spinal%20motion%20by%20interpolating%20between%20the%20neutral%20and%20maximally%20bent%20states.%0A%20%20Results%3A%20The%20pipeline%20was%20evaluated%20on%20a%20bendable%203D-printed%20lumbar%20spine%20phantom.%20The%20registration%20error%20was%20%241.941%20%5Cpm%200.199%24%20mm%20and%20the%20interpolated%20spinal%20motion%20error%20was%20%242.01%20%5Cpm%200.309%24%20mm%20%28median%29.%0A%20%20Conclusion%3A%20The%20proposed%20robotic%20ultrasound%20framework%20enables%20radiation-reduced%2C%20real-time%203D%20visualization%20of%20spinal%20motion%2C%20offering%20a%20promising%203D%20alternative%20to%20conventional%20dynamic%20X-ray%20imaging%20for%20assessing%20spinal%20instability.&entry.1838667208=http%3A//arxiv.org/abs/2602.12917v1&entry.124074799=Read"},
{"title": "Bootstrapping MLLM for Weakly-Supervised Class-Agnostic Object Counting", "author": "Xiaowen Zhang and Zijie Yue and Yong Luo and Cairong Zhao and Qijun Chen and Miaojing Shi", "abstract": "Object counting is a fundamental task in computer vision, with broad applicability in many real-world scenarios. Fully-supervised counting methods require costly point-level annotations per object. Few weakly-supervised methods leverage only image-level object counts as supervision and achieve fairly promising results. They are, however, often limited to counting a single category, e.g. person. In this paper, we propose WS-COC, the first MLLM-driven weakly-supervised framework for class-agnostic object counting. Instead of directly fine-tuning MLLMs to predict object counts, which can be challenging due to the modality gap, we incorporate three simple yet effective strategies to bootstrap the counting paradigm in both training and testing: First, a divide-and-discern dialogue tuning strategy is proposed to guide the MLLM to determine whether the object count falls within a specific range and progressively break down the range through multi-round dialogue. Second, a compare-and-rank count optimization strategy is introduced to train the MLLM to optimize the relative ranking of multiple images according to their object counts. Third, a global-and-local counting enhancement strategy aggregates and fuses local and global count predictions to improve counting performance in dense scenes. Extensive experiments on FSC-147, CARPK, PUCPR+, and ShanghaiTech show that WS-COC matches or even surpasses many state-of-art fully-supervised methods while significantly reducing annotation costs. Code is available at https://github.com/viscom-tongji/WS-COC.", "link": "http://arxiv.org/abs/2602.12774v1", "date": "2026-02-13", "relevancy": 2.0588, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5196}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5146}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrapping%20MLLM%20for%20Weakly-Supervised%20Class-Agnostic%20Object%20Counting&body=Title%3A%20Bootstrapping%20MLLM%20for%20Weakly-Supervised%20Class-Agnostic%20Object%20Counting%0AAuthor%3A%20Xiaowen%20Zhang%20and%20Zijie%20Yue%20and%20Yong%20Luo%20and%20Cairong%20Zhao%20and%20Qijun%20Chen%20and%20Miaojing%20Shi%0AAbstract%3A%20Object%20counting%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%20with%20broad%20applicability%20in%20many%20real-world%20scenarios.%20Fully-supervised%20counting%20methods%20require%20costly%20point-level%20annotations%20per%20object.%20Few%20weakly-supervised%20methods%20leverage%20only%20image-level%20object%20counts%20as%20supervision%20and%20achieve%20fairly%20promising%20results.%20They%20are%2C%20however%2C%20often%20limited%20to%20counting%20a%20single%20category%2C%20e.g.%20person.%20In%20this%20paper%2C%20we%20propose%20WS-COC%2C%20the%20first%20MLLM-driven%20weakly-supervised%20framework%20for%20class-agnostic%20object%20counting.%20Instead%20of%20directly%20fine-tuning%20MLLMs%20to%20predict%20object%20counts%2C%20which%20can%20be%20challenging%20due%20to%20the%20modality%20gap%2C%20we%20incorporate%20three%20simple%20yet%20effective%20strategies%20to%20bootstrap%20the%20counting%20paradigm%20in%20both%20training%20and%20testing%3A%20First%2C%20a%20divide-and-discern%20dialogue%20tuning%20strategy%20is%20proposed%20to%20guide%20the%20MLLM%20to%20determine%20whether%20the%20object%20count%20falls%20within%20a%20specific%20range%20and%20progressively%20break%20down%20the%20range%20through%20multi-round%20dialogue.%20Second%2C%20a%20compare-and-rank%20count%20optimization%20strategy%20is%20introduced%20to%20train%20the%20MLLM%20to%20optimize%20the%20relative%20ranking%20of%20multiple%20images%20according%20to%20their%20object%20counts.%20Third%2C%20a%20global-and-local%20counting%20enhancement%20strategy%20aggregates%20and%20fuses%20local%20and%20global%20count%20predictions%20to%20improve%20counting%20performance%20in%20dense%20scenes.%20Extensive%20experiments%20on%20FSC-147%2C%20CARPK%2C%20PUCPR%2B%2C%20and%20ShanghaiTech%20show%20that%20WS-COC%20matches%20or%20even%20surpasses%20many%20state-of-art%20fully-supervised%20methods%20while%20significantly%20reducing%20annotation%20costs.%20Code%20is%20available%20at%20https%3A//github.com/viscom-tongji/WS-COC.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrapping%2520MLLM%2520for%2520Weakly-Supervised%2520Class-Agnostic%2520Object%2520Counting%26entry.906535625%3DXiaowen%2520Zhang%2520and%2520Zijie%2520Yue%2520and%2520Yong%2520Luo%2520and%2520Cairong%2520Zhao%2520and%2520Qijun%2520Chen%2520and%2520Miaojing%2520Shi%26entry.1292438233%3DObject%2520counting%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%252C%2520with%2520broad%2520applicability%2520in%2520many%2520real-world%2520scenarios.%2520Fully-supervised%2520counting%2520methods%2520require%2520costly%2520point-level%2520annotations%2520per%2520object.%2520Few%2520weakly-supervised%2520methods%2520leverage%2520only%2520image-level%2520object%2520counts%2520as%2520supervision%2520and%2520achieve%2520fairly%2520promising%2520results.%2520They%2520are%252C%2520however%252C%2520often%2520limited%2520to%2520counting%2520a%2520single%2520category%252C%2520e.g.%2520person.%2520In%2520this%2520paper%252C%2520we%2520propose%2520WS-COC%252C%2520the%2520first%2520MLLM-driven%2520weakly-supervised%2520framework%2520for%2520class-agnostic%2520object%2520counting.%2520Instead%2520of%2520directly%2520fine-tuning%2520MLLMs%2520to%2520predict%2520object%2520counts%252C%2520which%2520can%2520be%2520challenging%2520due%2520to%2520the%2520modality%2520gap%252C%2520we%2520incorporate%2520three%2520simple%2520yet%2520effective%2520strategies%2520to%2520bootstrap%2520the%2520counting%2520paradigm%2520in%2520both%2520training%2520and%2520testing%253A%2520First%252C%2520a%2520divide-and-discern%2520dialogue%2520tuning%2520strategy%2520is%2520proposed%2520to%2520guide%2520the%2520MLLM%2520to%2520determine%2520whether%2520the%2520object%2520count%2520falls%2520within%2520a%2520specific%2520range%2520and%2520progressively%2520break%2520down%2520the%2520range%2520through%2520multi-round%2520dialogue.%2520Second%252C%2520a%2520compare-and-rank%2520count%2520optimization%2520strategy%2520is%2520introduced%2520to%2520train%2520the%2520MLLM%2520to%2520optimize%2520the%2520relative%2520ranking%2520of%2520multiple%2520images%2520according%2520to%2520their%2520object%2520counts.%2520Third%252C%2520a%2520global-and-local%2520counting%2520enhancement%2520strategy%2520aggregates%2520and%2520fuses%2520local%2520and%2520global%2520count%2520predictions%2520to%2520improve%2520counting%2520performance%2520in%2520dense%2520scenes.%2520Extensive%2520experiments%2520on%2520FSC-147%252C%2520CARPK%252C%2520PUCPR%252B%252C%2520and%2520ShanghaiTech%2520show%2520that%2520WS-COC%2520matches%2520or%2520even%2520surpasses%2520many%2520state-of-art%2520fully-supervised%2520methods%2520while%2520significantly%2520reducing%2520annotation%2520costs.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/viscom-tongji/WS-COC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrapping%20MLLM%20for%20Weakly-Supervised%20Class-Agnostic%20Object%20Counting&entry.906535625=Xiaowen%20Zhang%20and%20Zijie%20Yue%20and%20Yong%20Luo%20and%20Cairong%20Zhao%20and%20Qijun%20Chen%20and%20Miaojing%20Shi&entry.1292438233=Object%20counting%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%20with%20broad%20applicability%20in%20many%20real-world%20scenarios.%20Fully-supervised%20counting%20methods%20require%20costly%20point-level%20annotations%20per%20object.%20Few%20weakly-supervised%20methods%20leverage%20only%20image-level%20object%20counts%20as%20supervision%20and%20achieve%20fairly%20promising%20results.%20They%20are%2C%20however%2C%20often%20limited%20to%20counting%20a%20single%20category%2C%20e.g.%20person.%20In%20this%20paper%2C%20we%20propose%20WS-COC%2C%20the%20first%20MLLM-driven%20weakly-supervised%20framework%20for%20class-agnostic%20object%20counting.%20Instead%20of%20directly%20fine-tuning%20MLLMs%20to%20predict%20object%20counts%2C%20which%20can%20be%20challenging%20due%20to%20the%20modality%20gap%2C%20we%20incorporate%20three%20simple%20yet%20effective%20strategies%20to%20bootstrap%20the%20counting%20paradigm%20in%20both%20training%20and%20testing%3A%20First%2C%20a%20divide-and-discern%20dialogue%20tuning%20strategy%20is%20proposed%20to%20guide%20the%20MLLM%20to%20determine%20whether%20the%20object%20count%20falls%20within%20a%20specific%20range%20and%20progressively%20break%20down%20the%20range%20through%20multi-round%20dialogue.%20Second%2C%20a%20compare-and-rank%20count%20optimization%20strategy%20is%20introduced%20to%20train%20the%20MLLM%20to%20optimize%20the%20relative%20ranking%20of%20multiple%20images%20according%20to%20their%20object%20counts.%20Third%2C%20a%20global-and-local%20counting%20enhancement%20strategy%20aggregates%20and%20fuses%20local%20and%20global%20count%20predictions%20to%20improve%20counting%20performance%20in%20dense%20scenes.%20Extensive%20experiments%20on%20FSC-147%2C%20CARPK%2C%20PUCPR%2B%2C%20and%20ShanghaiTech%20show%20that%20WS-COC%20matches%20or%20even%20surpasses%20many%20state-of-art%20fully-supervised%20methods%20while%20significantly%20reducing%20annotation%20costs.%20Code%20is%20available%20at%20https%3A//github.com/viscom-tongji/WS-COC.&entry.1838667208=http%3A//arxiv.org/abs/2602.12774v1&entry.124074799=Read"},
{"title": "X-VORTEX: Spatio-Temporal Contrastive Learning for Wake Vortex Trajectory Forecasting", "author": "Zhan Qu and Michael F\u00e4rber", "abstract": "Wake vortices are strong, coherent air turbulences created by aircraft, and they pose a major safety and capacity challenge for air traffic management. Tracking how vortices move, weaken, and dissipate over time from LiDAR measurements is still difficult because scans are sparse, vortex signatures fade as the flow breaks down under atmospheric turbulence and instabilities, and point-wise annotation is prohibitively expensive. Existing approaches largely treat each scan as an independent, fully supervised segmentation problem, which overlooks temporal structure and does not scale to the vast unlabeled archives collected in practice. We present X-VORTEX, a spatio-temporal contrastive learning framework grounded in Augmentation Overlap Theory that learns physics-aware representations from unlabeled LiDAR point cloud sequences. X-VORTEX addresses two core challenges: sensor sparsity and time-varying vortex dynamics. It constructs paired inputs from the same underlying flight event by combining a weakly perturbed sequence with a strongly augmented counterpart produced via temporal subsampling and spatial masking, encouraging the model to align representations across missing frames and partial observations. Architecturally, a time-distributed geometric encoder extracts per-scan features and a sequential aggregator models the evolving vortex state across variable-length sequences. We evaluate on a real-world dataset of over one million LiDAR scans. X-VORTEX achieves superior vortex center localization while using only 1% of the labeled data required by supervised baselines, and the learned representations support accurate trajectory forecasting.", "link": "http://arxiv.org/abs/2602.12869v1", "date": "2026-02-13", "relevancy": 2.0556, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5233}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5184}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-VORTEX%3A%20Spatio-Temporal%20Contrastive%20Learning%20for%20Wake%20Vortex%20Trajectory%20Forecasting&body=Title%3A%20X-VORTEX%3A%20Spatio-Temporal%20Contrastive%20Learning%20for%20Wake%20Vortex%20Trajectory%20Forecasting%0AAuthor%3A%20Zhan%20Qu%20and%20Michael%20F%C3%A4rber%0AAbstract%3A%20Wake%20vortices%20are%20strong%2C%20coherent%20air%20turbulences%20created%20by%20aircraft%2C%20and%20they%20pose%20a%20major%20safety%20and%20capacity%20challenge%20for%20air%20traffic%20management.%20Tracking%20how%20vortices%20move%2C%20weaken%2C%20and%20dissipate%20over%20time%20from%20LiDAR%20measurements%20is%20still%20difficult%20because%20scans%20are%20sparse%2C%20vortex%20signatures%20fade%20as%20the%20flow%20breaks%20down%20under%20atmospheric%20turbulence%20and%20instabilities%2C%20and%20point-wise%20annotation%20is%20prohibitively%20expensive.%20Existing%20approaches%20largely%20treat%20each%20scan%20as%20an%20independent%2C%20fully%20supervised%20segmentation%20problem%2C%20which%20overlooks%20temporal%20structure%20and%20does%20not%20scale%20to%20the%20vast%20unlabeled%20archives%20collected%20in%20practice.%20We%20present%20X-VORTEX%2C%20a%20spatio-temporal%20contrastive%20learning%20framework%20grounded%20in%20Augmentation%20Overlap%20Theory%20that%20learns%20physics-aware%20representations%20from%20unlabeled%20LiDAR%20point%20cloud%20sequences.%20X-VORTEX%20addresses%20two%20core%20challenges%3A%20sensor%20sparsity%20and%20time-varying%20vortex%20dynamics.%20It%20constructs%20paired%20inputs%20from%20the%20same%20underlying%20flight%20event%20by%20combining%20a%20weakly%20perturbed%20sequence%20with%20a%20strongly%20augmented%20counterpart%20produced%20via%20temporal%20subsampling%20and%20spatial%20masking%2C%20encouraging%20the%20model%20to%20align%20representations%20across%20missing%20frames%20and%20partial%20observations.%20Architecturally%2C%20a%20time-distributed%20geometric%20encoder%20extracts%20per-scan%20features%20and%20a%20sequential%20aggregator%20models%20the%20evolving%20vortex%20state%20across%20variable-length%20sequences.%20We%20evaluate%20on%20a%20real-world%20dataset%20of%20over%20one%20million%20LiDAR%20scans.%20X-VORTEX%20achieves%20superior%20vortex%20center%20localization%20while%20using%20only%201%25%20of%20the%20labeled%20data%20required%20by%20supervised%20baselines%2C%20and%20the%20learned%20representations%20support%20accurate%20trajectory%20forecasting.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-VORTEX%253A%2520Spatio-Temporal%2520Contrastive%2520Learning%2520for%2520Wake%2520Vortex%2520Trajectory%2520Forecasting%26entry.906535625%3DZhan%2520Qu%2520and%2520Michael%2520F%25C3%25A4rber%26entry.1292438233%3DWake%2520vortices%2520are%2520strong%252C%2520coherent%2520air%2520turbulences%2520created%2520by%2520aircraft%252C%2520and%2520they%2520pose%2520a%2520major%2520safety%2520and%2520capacity%2520challenge%2520for%2520air%2520traffic%2520management.%2520Tracking%2520how%2520vortices%2520move%252C%2520weaken%252C%2520and%2520dissipate%2520over%2520time%2520from%2520LiDAR%2520measurements%2520is%2520still%2520difficult%2520because%2520scans%2520are%2520sparse%252C%2520vortex%2520signatures%2520fade%2520as%2520the%2520flow%2520breaks%2520down%2520under%2520atmospheric%2520turbulence%2520and%2520instabilities%252C%2520and%2520point-wise%2520annotation%2520is%2520prohibitively%2520expensive.%2520Existing%2520approaches%2520largely%2520treat%2520each%2520scan%2520as%2520an%2520independent%252C%2520fully%2520supervised%2520segmentation%2520problem%252C%2520which%2520overlooks%2520temporal%2520structure%2520and%2520does%2520not%2520scale%2520to%2520the%2520vast%2520unlabeled%2520archives%2520collected%2520in%2520practice.%2520We%2520present%2520X-VORTEX%252C%2520a%2520spatio-temporal%2520contrastive%2520learning%2520framework%2520grounded%2520in%2520Augmentation%2520Overlap%2520Theory%2520that%2520learns%2520physics-aware%2520representations%2520from%2520unlabeled%2520LiDAR%2520point%2520cloud%2520sequences.%2520X-VORTEX%2520addresses%2520two%2520core%2520challenges%253A%2520sensor%2520sparsity%2520and%2520time-varying%2520vortex%2520dynamics.%2520It%2520constructs%2520paired%2520inputs%2520from%2520the%2520same%2520underlying%2520flight%2520event%2520by%2520combining%2520a%2520weakly%2520perturbed%2520sequence%2520with%2520a%2520strongly%2520augmented%2520counterpart%2520produced%2520via%2520temporal%2520subsampling%2520and%2520spatial%2520masking%252C%2520encouraging%2520the%2520model%2520to%2520align%2520representations%2520across%2520missing%2520frames%2520and%2520partial%2520observations.%2520Architecturally%252C%2520a%2520time-distributed%2520geometric%2520encoder%2520extracts%2520per-scan%2520features%2520and%2520a%2520sequential%2520aggregator%2520models%2520the%2520evolving%2520vortex%2520state%2520across%2520variable-length%2520sequences.%2520We%2520evaluate%2520on%2520a%2520real-world%2520dataset%2520of%2520over%2520one%2520million%2520LiDAR%2520scans.%2520X-VORTEX%2520achieves%2520superior%2520vortex%2520center%2520localization%2520while%2520using%2520only%25201%2525%2520of%2520the%2520labeled%2520data%2520required%2520by%2520supervised%2520baselines%252C%2520and%2520the%2520learned%2520representations%2520support%2520accurate%2520trajectory%2520forecasting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-VORTEX%3A%20Spatio-Temporal%20Contrastive%20Learning%20for%20Wake%20Vortex%20Trajectory%20Forecasting&entry.906535625=Zhan%20Qu%20and%20Michael%20F%C3%A4rber&entry.1292438233=Wake%20vortices%20are%20strong%2C%20coherent%20air%20turbulences%20created%20by%20aircraft%2C%20and%20they%20pose%20a%20major%20safety%20and%20capacity%20challenge%20for%20air%20traffic%20management.%20Tracking%20how%20vortices%20move%2C%20weaken%2C%20and%20dissipate%20over%20time%20from%20LiDAR%20measurements%20is%20still%20difficult%20because%20scans%20are%20sparse%2C%20vortex%20signatures%20fade%20as%20the%20flow%20breaks%20down%20under%20atmospheric%20turbulence%20and%20instabilities%2C%20and%20point-wise%20annotation%20is%20prohibitively%20expensive.%20Existing%20approaches%20largely%20treat%20each%20scan%20as%20an%20independent%2C%20fully%20supervised%20segmentation%20problem%2C%20which%20overlooks%20temporal%20structure%20and%20does%20not%20scale%20to%20the%20vast%20unlabeled%20archives%20collected%20in%20practice.%20We%20present%20X-VORTEX%2C%20a%20spatio-temporal%20contrastive%20learning%20framework%20grounded%20in%20Augmentation%20Overlap%20Theory%20that%20learns%20physics-aware%20representations%20from%20unlabeled%20LiDAR%20point%20cloud%20sequences.%20X-VORTEX%20addresses%20two%20core%20challenges%3A%20sensor%20sparsity%20and%20time-varying%20vortex%20dynamics.%20It%20constructs%20paired%20inputs%20from%20the%20same%20underlying%20flight%20event%20by%20combining%20a%20weakly%20perturbed%20sequence%20with%20a%20strongly%20augmented%20counterpart%20produced%20via%20temporal%20subsampling%20and%20spatial%20masking%2C%20encouraging%20the%20model%20to%20align%20representations%20across%20missing%20frames%20and%20partial%20observations.%20Architecturally%2C%20a%20time-distributed%20geometric%20encoder%20extracts%20per-scan%20features%20and%20a%20sequential%20aggregator%20models%20the%20evolving%20vortex%20state%20across%20variable-length%20sequences.%20We%20evaluate%20on%20a%20real-world%20dataset%20of%20over%20one%20million%20LiDAR%20scans.%20X-VORTEX%20achieves%20superior%20vortex%20center%20localization%20while%20using%20only%201%25%20of%20the%20labeled%20data%20required%20by%20supervised%20baselines%2C%20and%20the%20learned%20representations%20support%20accurate%20trajectory%20forecasting.&entry.1838667208=http%3A//arxiv.org/abs/2602.12869v1&entry.124074799=Read"},
{"title": "TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion", "author": "Xinyu Gao", "abstract": "Knowledge Graphs have become fundamental infrastructure for applications such as intelligent question answering and recommender systems due to their expressive representation. Nevertheless, real-world knowledge is heterogeneous, leading to a pronounced long-tailed distribution over relations. Previous studies mainly based on metric matching or meta learning. However, they often overlook the distributional characteristics of positive and negative triple samples. In this paper, we propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show significant advantages of our methods.", "link": "http://arxiv.org/abs/2512.12182v2", "date": "2026-02-13", "relevancy": 2.0408, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5302}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4985}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TA-KAND%3A%20Two-stage%20Attention%20Triple%20Enhancement%20and%20U-KAN%20based%20Diffusion%20For%20Few-shot%20Knowledge%20Graph%20Completion&body=Title%3A%20TA-KAND%3A%20Two-stage%20Attention%20Triple%20Enhancement%20and%20U-KAN%20based%20Diffusion%20For%20Few-shot%20Knowledge%20Graph%20Completion%0AAuthor%3A%20Xinyu%20Gao%0AAbstract%3A%20Knowledge%20Graphs%20have%20become%20fundamental%20infrastructure%20for%20applications%20such%20as%20intelligent%20question%20answering%20and%20recommender%20systems%20due%20to%20their%20expressive%20representation.%20Nevertheless%2C%20real-world%20knowledge%20is%20heterogeneous%2C%20leading%20to%20a%20pronounced%20long-tailed%20distribution%20over%20relations.%20Previous%20studies%20mainly%20based%20on%20metric%20matching%20or%20meta%20learning.%20However%2C%20they%20often%20overlook%20the%20distributional%20characteristics%20of%20positive%20and%20negative%20triple%20samples.%20In%20this%20paper%2C%20we%20propose%20a%20few-shot%20knowledge%20graph%20completion%20framework%20that%20integrates%20two-stage%20attention%20triple%20enhancer%20with%20U-KAN%20based%20diffusion%20model.%20Extensive%20experiments%20on%20two%20public%20datasets%20show%20significant%20advantages%20of%20our%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12182v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTA-KAND%253A%2520Two-stage%2520Attention%2520Triple%2520Enhancement%2520and%2520U-KAN%2520based%2520Diffusion%2520For%2520Few-shot%2520Knowledge%2520Graph%2520Completion%26entry.906535625%3DXinyu%2520Gao%26entry.1292438233%3DKnowledge%2520Graphs%2520have%2520become%2520fundamental%2520infrastructure%2520for%2520applications%2520such%2520as%2520intelligent%2520question%2520answering%2520and%2520recommender%2520systems%2520due%2520to%2520their%2520expressive%2520representation.%2520Nevertheless%252C%2520real-world%2520knowledge%2520is%2520heterogeneous%252C%2520leading%2520to%2520a%2520pronounced%2520long-tailed%2520distribution%2520over%2520relations.%2520Previous%2520studies%2520mainly%2520based%2520on%2520metric%2520matching%2520or%2520meta%2520learning.%2520However%252C%2520they%2520often%2520overlook%2520the%2520distributional%2520characteristics%2520of%2520positive%2520and%2520negative%2520triple%2520samples.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520few-shot%2520knowledge%2520graph%2520completion%2520framework%2520that%2520integrates%2520two-stage%2520attention%2520triple%2520enhancer%2520with%2520U-KAN%2520based%2520diffusion%2520model.%2520Extensive%2520experiments%2520on%2520two%2520public%2520datasets%2520show%2520significant%2520advantages%2520of%2520our%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12182v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TA-KAND%3A%20Two-stage%20Attention%20Triple%20Enhancement%20and%20U-KAN%20based%20Diffusion%20For%20Few-shot%20Knowledge%20Graph%20Completion&entry.906535625=Xinyu%20Gao&entry.1292438233=Knowledge%20Graphs%20have%20become%20fundamental%20infrastructure%20for%20applications%20such%20as%20intelligent%20question%20answering%20and%20recommender%20systems%20due%20to%20their%20expressive%20representation.%20Nevertheless%2C%20real-world%20knowledge%20is%20heterogeneous%2C%20leading%20to%20a%20pronounced%20long-tailed%20distribution%20over%20relations.%20Previous%20studies%20mainly%20based%20on%20metric%20matching%20or%20meta%20learning.%20However%2C%20they%20often%20overlook%20the%20distributional%20characteristics%20of%20positive%20and%20negative%20triple%20samples.%20In%20this%20paper%2C%20we%20propose%20a%20few-shot%20knowledge%20graph%20completion%20framework%20that%20integrates%20two-stage%20attention%20triple%20enhancer%20with%20U-KAN%20based%20diffusion%20model.%20Extensive%20experiments%20on%20two%20public%20datasets%20show%20significant%20advantages%20of%20our%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.12182v2&entry.124074799=Read"},
{"title": "Knowledge-Based Design Requirements for Generative Social Robots in Higher Education", "author": "Stephan Vonschallen and Dominique Oberle and Theresa Schmiedel and Friederike Eyssel", "abstract": "Generative social robots (GSRs) powered by large language models enable adaptive, conversational tutoring but also introduce risks such as hallucina-tions, overreliance, and privacy violations. Existing frameworks for educa-tional technologies and responsible AI primarily define desired behaviors, yet they rarely specify the knowledge prerequisites that enable generative systems to express these behaviors reliably. To address this gap, we adopt a knowledge-based design perspective and investigate what information tutor-ing-oriented GSRs require to function responsibly and effectively in higher education. Based on twelve semi-structured interviews with university stu-dents and lecturers, we identify twelve design requirements across three knowledge types: self-knowledge (assertive, conscientious and friendly per-sonality with customizable role), user-knowledge (personalized information about student learning goals, learning progress, motivation type, emotional state and background), and context-knowledge (learning materials, educa-tional strategies, course-related information, and physical learning environ-ment). By identifying these knowledge requirements, this work provides a structured foundation for the design of tutoring GSRs and future evaluations, aligning generative system capabilities with pedagogical and ethical expecta-tions.", "link": "http://arxiv.org/abs/2602.12873v1", "date": "2026-02-13", "relevancy": 2.0342, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5201}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5127}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge-Based%20Design%20Requirements%20for%20Generative%20Social%20Robots%20in%20Higher%20Education&body=Title%3A%20Knowledge-Based%20Design%20Requirements%20for%20Generative%20Social%20Robots%20in%20Higher%20Education%0AAuthor%3A%20Stephan%20Vonschallen%20and%20Dominique%20Oberle%20and%20Theresa%20Schmiedel%20and%20Friederike%20Eyssel%0AAbstract%3A%20Generative%20social%20robots%20%28GSRs%29%20powered%20by%20large%20language%20models%20enable%20adaptive%2C%20conversational%20tutoring%20but%20also%20introduce%20risks%20such%20as%20hallucina-tions%2C%20overreliance%2C%20and%20privacy%20violations.%20Existing%20frameworks%20for%20educa-tional%20technologies%20and%20responsible%20AI%20primarily%20define%20desired%20behaviors%2C%20yet%20they%20rarely%20specify%20the%20knowledge%20prerequisites%20that%20enable%20generative%20systems%20to%20express%20these%20behaviors%20reliably.%20To%20address%20this%20gap%2C%20we%20adopt%20a%20knowledge-based%20design%20perspective%20and%20investigate%20what%20information%20tutor-ing-oriented%20GSRs%20require%20to%20function%20responsibly%20and%20effectively%20in%20higher%20education.%20Based%20on%20twelve%20semi-structured%20interviews%20with%20university%20stu-dents%20and%20lecturers%2C%20we%20identify%20twelve%20design%20requirements%20across%20three%20knowledge%20types%3A%20self-knowledge%20%28assertive%2C%20conscientious%20and%20friendly%20per-sonality%20with%20customizable%20role%29%2C%20user-knowledge%20%28personalized%20information%20about%20student%20learning%20goals%2C%20learning%20progress%2C%20motivation%20type%2C%20emotional%20state%20and%20background%29%2C%20and%20context-knowledge%20%28learning%20materials%2C%20educa-tional%20strategies%2C%20course-related%20information%2C%20and%20physical%20learning%20environ-ment%29.%20By%20identifying%20these%20knowledge%20requirements%2C%20this%20work%20provides%20a%20structured%20foundation%20for%20the%20design%20of%20tutoring%20GSRs%20and%20future%20evaluations%2C%20aligning%20generative%20system%20capabilities%20with%20pedagogical%20and%20ethical%20expecta-tions.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge-Based%2520Design%2520Requirements%2520for%2520Generative%2520Social%2520Robots%2520in%2520Higher%2520Education%26entry.906535625%3DStephan%2520Vonschallen%2520and%2520Dominique%2520Oberle%2520and%2520Theresa%2520Schmiedel%2520and%2520Friederike%2520Eyssel%26entry.1292438233%3DGenerative%2520social%2520robots%2520%2528GSRs%2529%2520powered%2520by%2520large%2520language%2520models%2520enable%2520adaptive%252C%2520conversational%2520tutoring%2520but%2520also%2520introduce%2520risks%2520such%2520as%2520hallucina-tions%252C%2520overreliance%252C%2520and%2520privacy%2520violations.%2520Existing%2520frameworks%2520for%2520educa-tional%2520technologies%2520and%2520responsible%2520AI%2520primarily%2520define%2520desired%2520behaviors%252C%2520yet%2520they%2520rarely%2520specify%2520the%2520knowledge%2520prerequisites%2520that%2520enable%2520generative%2520systems%2520to%2520express%2520these%2520behaviors%2520reliably.%2520To%2520address%2520this%2520gap%252C%2520we%2520adopt%2520a%2520knowledge-based%2520design%2520perspective%2520and%2520investigate%2520what%2520information%2520tutor-ing-oriented%2520GSRs%2520require%2520to%2520function%2520responsibly%2520and%2520effectively%2520in%2520higher%2520education.%2520Based%2520on%2520twelve%2520semi-structured%2520interviews%2520with%2520university%2520stu-dents%2520and%2520lecturers%252C%2520we%2520identify%2520twelve%2520design%2520requirements%2520across%2520three%2520knowledge%2520types%253A%2520self-knowledge%2520%2528assertive%252C%2520conscientious%2520and%2520friendly%2520per-sonality%2520with%2520customizable%2520role%2529%252C%2520user-knowledge%2520%2528personalized%2520information%2520about%2520student%2520learning%2520goals%252C%2520learning%2520progress%252C%2520motivation%2520type%252C%2520emotional%2520state%2520and%2520background%2529%252C%2520and%2520context-knowledge%2520%2528learning%2520materials%252C%2520educa-tional%2520strategies%252C%2520course-related%2520information%252C%2520and%2520physical%2520learning%2520environ-ment%2529.%2520By%2520identifying%2520these%2520knowledge%2520requirements%252C%2520this%2520work%2520provides%2520a%2520structured%2520foundation%2520for%2520the%2520design%2520of%2520tutoring%2520GSRs%2520and%2520future%2520evaluations%252C%2520aligning%2520generative%2520system%2520capabilities%2520with%2520pedagogical%2520and%2520ethical%2520expecta-tions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-Based%20Design%20Requirements%20for%20Generative%20Social%20Robots%20in%20Higher%20Education&entry.906535625=Stephan%20Vonschallen%20and%20Dominique%20Oberle%20and%20Theresa%20Schmiedel%20and%20Friederike%20Eyssel&entry.1292438233=Generative%20social%20robots%20%28GSRs%29%20powered%20by%20large%20language%20models%20enable%20adaptive%2C%20conversational%20tutoring%20but%20also%20introduce%20risks%20such%20as%20hallucina-tions%2C%20overreliance%2C%20and%20privacy%20violations.%20Existing%20frameworks%20for%20educa-tional%20technologies%20and%20responsible%20AI%20primarily%20define%20desired%20behaviors%2C%20yet%20they%20rarely%20specify%20the%20knowledge%20prerequisites%20that%20enable%20generative%20systems%20to%20express%20these%20behaviors%20reliably.%20To%20address%20this%20gap%2C%20we%20adopt%20a%20knowledge-based%20design%20perspective%20and%20investigate%20what%20information%20tutor-ing-oriented%20GSRs%20require%20to%20function%20responsibly%20and%20effectively%20in%20higher%20education.%20Based%20on%20twelve%20semi-structured%20interviews%20with%20university%20stu-dents%20and%20lecturers%2C%20we%20identify%20twelve%20design%20requirements%20across%20three%20knowledge%20types%3A%20self-knowledge%20%28assertive%2C%20conscientious%20and%20friendly%20per-sonality%20with%20customizable%20role%29%2C%20user-knowledge%20%28personalized%20information%20about%20student%20learning%20goals%2C%20learning%20progress%2C%20motivation%20type%2C%20emotional%20state%20and%20background%29%2C%20and%20context-knowledge%20%28learning%20materials%2C%20educa-tional%20strategies%2C%20course-related%20information%2C%20and%20physical%20learning%20environ-ment%29.%20By%20identifying%20these%20knowledge%20requirements%2C%20this%20work%20provides%20a%20structured%20foundation%20for%20the%20design%20of%20tutoring%20GSRs%20and%20future%20evaluations%2C%20aligning%20generative%20system%20capabilities%20with%20pedagogical%20and%20ethical%20expecta-tions.&entry.1838667208=http%3A//arxiv.org/abs/2602.12873v1&entry.124074799=Read"},
{"title": "Realistic Face Reconstruction from Facial Embeddings via Diffusion Models", "author": "Dong Han and Yong Li and Joachim Denzler", "abstract": "With the advancement of face recognition (FR) systems, privacy-preserving face recognition (PPFR) systems have gained popularity for their accurate recognition, enhanced facial privacy protection, and robustness to various attacks. However, there are limited studies to further verify privacy risks by reconstructing realistic high-resolution face images from embeddings of these systems, especially for PPFR. In this work, we propose the face embedding mapping (FEM), a general framework that explores Kolmogorov-Arnold Network (KAN) for conducting the embedding-to-face attack by leveraging pre-trained Identity-Preserving diffusion model against state-of-the-art (SOTA) FR and PPFR systems. Based on extensive experiments, we verify that reconstructed faces can be used for accessing other real-word FR systems. Besides, the proposed method shows the robustness in reconstructing faces from the partial and protected face embeddings. Moreover, FEM can be utilized as a tool for evaluating safety of FR and PPFR systems in terms of privacy leakage. All images used in this work are from public datasets.", "link": "http://arxiv.org/abs/2602.13168v1", "date": "2026-02-13", "relevancy": 2.0338, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5204}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5107}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20Face%20Reconstruction%20from%20Facial%20Embeddings%20via%20Diffusion%20Models&body=Title%3A%20Realistic%20Face%20Reconstruction%20from%20Facial%20Embeddings%20via%20Diffusion%20Models%0AAuthor%3A%20Dong%20Han%20and%20Yong%20Li%20and%20Joachim%20Denzler%0AAbstract%3A%20With%20the%20advancement%20of%20face%20recognition%20%28FR%29%20systems%2C%20privacy-preserving%20face%20recognition%20%28PPFR%29%20systems%20have%20gained%20popularity%20for%20their%20accurate%20recognition%2C%20enhanced%20facial%20privacy%20protection%2C%20and%20robustness%20to%20various%20attacks.%20However%2C%20there%20are%20limited%20studies%20to%20further%20verify%20privacy%20risks%20by%20reconstructing%20realistic%20high-resolution%20face%20images%20from%20embeddings%20of%20these%20systems%2C%20especially%20for%20PPFR.%20In%20this%20work%2C%20we%20propose%20the%20face%20embedding%20mapping%20%28FEM%29%2C%20a%20general%20framework%20that%20explores%20Kolmogorov-Arnold%20Network%20%28KAN%29%20for%20conducting%20the%20embedding-to-face%20attack%20by%20leveraging%20pre-trained%20Identity-Preserving%20diffusion%20model%20against%20state-of-the-art%20%28SOTA%29%20FR%20and%20PPFR%20systems.%20Based%20on%20extensive%20experiments%2C%20we%20verify%20that%20reconstructed%20faces%20can%20be%20used%20for%20accessing%20other%20real-word%20FR%20systems.%20Besides%2C%20the%20proposed%20method%20shows%20the%20robustness%20in%20reconstructing%20faces%20from%20the%20partial%20and%20protected%20face%20embeddings.%20Moreover%2C%20FEM%20can%20be%20utilized%20as%20a%20tool%20for%20evaluating%20safety%20of%20FR%20and%20PPFR%20systems%20in%20terms%20of%20privacy%20leakage.%20All%20images%20used%20in%20this%20work%20are%20from%20public%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520Face%2520Reconstruction%2520from%2520Facial%2520Embeddings%2520via%2520Diffusion%2520Models%26entry.906535625%3DDong%2520Han%2520and%2520Yong%2520Li%2520and%2520Joachim%2520Denzler%26entry.1292438233%3DWith%2520the%2520advancement%2520of%2520face%2520recognition%2520%2528FR%2529%2520systems%252C%2520privacy-preserving%2520face%2520recognition%2520%2528PPFR%2529%2520systems%2520have%2520gained%2520popularity%2520for%2520their%2520accurate%2520recognition%252C%2520enhanced%2520facial%2520privacy%2520protection%252C%2520and%2520robustness%2520to%2520various%2520attacks.%2520However%252C%2520there%2520are%2520limited%2520studies%2520to%2520further%2520verify%2520privacy%2520risks%2520by%2520reconstructing%2520realistic%2520high-resolution%2520face%2520images%2520from%2520embeddings%2520of%2520these%2520systems%252C%2520especially%2520for%2520PPFR.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520face%2520embedding%2520mapping%2520%2528FEM%2529%252C%2520a%2520general%2520framework%2520that%2520explores%2520Kolmogorov-Arnold%2520Network%2520%2528KAN%2529%2520for%2520conducting%2520the%2520embedding-to-face%2520attack%2520by%2520leveraging%2520pre-trained%2520Identity-Preserving%2520diffusion%2520model%2520against%2520state-of-the-art%2520%2528SOTA%2529%2520FR%2520and%2520PPFR%2520systems.%2520Based%2520on%2520extensive%2520experiments%252C%2520we%2520verify%2520that%2520reconstructed%2520faces%2520can%2520be%2520used%2520for%2520accessing%2520other%2520real-word%2520FR%2520systems.%2520Besides%252C%2520the%2520proposed%2520method%2520shows%2520the%2520robustness%2520in%2520reconstructing%2520faces%2520from%2520the%2520partial%2520and%2520protected%2520face%2520embeddings.%2520Moreover%252C%2520FEM%2520can%2520be%2520utilized%2520as%2520a%2520tool%2520for%2520evaluating%2520safety%2520of%2520FR%2520and%2520PPFR%2520systems%2520in%2520terms%2520of%2520privacy%2520leakage.%2520All%2520images%2520used%2520in%2520this%2520work%2520are%2520from%2520public%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20Face%20Reconstruction%20from%20Facial%20Embeddings%20via%20Diffusion%20Models&entry.906535625=Dong%20Han%20and%20Yong%20Li%20and%20Joachim%20Denzler&entry.1292438233=With%20the%20advancement%20of%20face%20recognition%20%28FR%29%20systems%2C%20privacy-preserving%20face%20recognition%20%28PPFR%29%20systems%20have%20gained%20popularity%20for%20their%20accurate%20recognition%2C%20enhanced%20facial%20privacy%20protection%2C%20and%20robustness%20to%20various%20attacks.%20However%2C%20there%20are%20limited%20studies%20to%20further%20verify%20privacy%20risks%20by%20reconstructing%20realistic%20high-resolution%20face%20images%20from%20embeddings%20of%20these%20systems%2C%20especially%20for%20PPFR.%20In%20this%20work%2C%20we%20propose%20the%20face%20embedding%20mapping%20%28FEM%29%2C%20a%20general%20framework%20that%20explores%20Kolmogorov-Arnold%20Network%20%28KAN%29%20for%20conducting%20the%20embedding-to-face%20attack%20by%20leveraging%20pre-trained%20Identity-Preserving%20diffusion%20model%20against%20state-of-the-art%20%28SOTA%29%20FR%20and%20PPFR%20systems.%20Based%20on%20extensive%20experiments%2C%20we%20verify%20that%20reconstructed%20faces%20can%20be%20used%20for%20accessing%20other%20real-word%20FR%20systems.%20Besides%2C%20the%20proposed%20method%20shows%20the%20robustness%20in%20reconstructing%20faces%20from%20the%20partial%20and%20protected%20face%20embeddings.%20Moreover%2C%20FEM%20can%20be%20utilized%20as%20a%20tool%20for%20evaluating%20safety%20of%20FR%20and%20PPFR%20systems%20in%20terms%20of%20privacy%20leakage.%20All%20images%20used%20in%20this%20work%20are%20from%20public%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2602.13168v1&entry.124074799=Read"},
{"title": "Represent Micro-Doppler Signature in Orders", "author": "Weicheng Gao", "abstract": "Non-line-of-sight sensing of human activities in complex environments is enabled by multiple-input multiple-output through-the-wall radar (TWR). However, the distinctiveness of micro-Doppler signature between similar indoor human activities such as gun carrying and normal walking is minimal, while the large scale of input images required for effective identification utilizing time-frequency spectrograms creates challenges for model training and inference efficiency. To address this issue, the Chebyshev-time map is proposed in this paper, which is a method characterizing micro-Doppler signature using polynomial orders. The parametric kinematic models for human motion and the TWR echo model are first established. Then, a time-frequency feature representation method based on orthogonal Chebyshev polynomial decomposition is proposed. The kinematic envelopes of the torso and limbs are extracted, and the time-frequency spectrum slices are mapped into a robust Chebyshev-time coefficient space, preserving the multi-order morphological detail information of time-frequency spectrum. Numerical simulations and experiments are conducted to verify the effectiveness of the proposed method, which demonstrates the capability to characterize armed and unarmed indoor human activities while effectively compressing the scale of the time-frequency spectrum to achieve a balance between recognition accuracy and input data dimensions. The open-source code of this paper can be found in: https://github.com/JoeyBGOfficial/Represent-Micro-Doppler-Signature-in-Orders.", "link": "http://arxiv.org/abs/2602.12985v1", "date": "2026-02-13", "relevancy": 2.0305, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5123}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5047}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Represent%20Micro-Doppler%20Signature%20in%20Orders&body=Title%3A%20Represent%20Micro-Doppler%20Signature%20in%20Orders%0AAuthor%3A%20Weicheng%20Gao%0AAbstract%3A%20Non-line-of-sight%20sensing%20of%20human%20activities%20in%20complex%20environments%20is%20enabled%20by%20multiple-input%20multiple-output%20through-the-wall%20radar%20%28TWR%29.%20However%2C%20the%20distinctiveness%20of%20micro-Doppler%20signature%20between%20similar%20indoor%20human%20activities%20such%20as%20gun%20carrying%20and%20normal%20walking%20is%20minimal%2C%20while%20the%20large%20scale%20of%20input%20images%20required%20for%20effective%20identification%20utilizing%20time-frequency%20spectrograms%20creates%20challenges%20for%20model%20training%20and%20inference%20efficiency.%20To%20address%20this%20issue%2C%20the%20Chebyshev-time%20map%20is%20proposed%20in%20this%20paper%2C%20which%20is%20a%20method%20characterizing%20micro-Doppler%20signature%20using%20polynomial%20orders.%20The%20parametric%20kinematic%20models%20for%20human%20motion%20and%20the%20TWR%20echo%20model%20are%20first%20established.%20Then%2C%20a%20time-frequency%20feature%20representation%20method%20based%20on%20orthogonal%20Chebyshev%20polynomial%20decomposition%20is%20proposed.%20The%20kinematic%20envelopes%20of%20the%20torso%20and%20limbs%20are%20extracted%2C%20and%20the%20time-frequency%20spectrum%20slices%20are%20mapped%20into%20a%20robust%20Chebyshev-time%20coefficient%20space%2C%20preserving%20the%20multi-order%20morphological%20detail%20information%20of%20time-frequency%20spectrum.%20Numerical%20simulations%20and%20experiments%20are%20conducted%20to%20verify%20the%20effectiveness%20of%20the%20proposed%20method%2C%20which%20demonstrates%20the%20capability%20to%20characterize%20armed%20and%20unarmed%20indoor%20human%20activities%20while%20effectively%20compressing%20the%20scale%20of%20the%20time-frequency%20spectrum%20to%20achieve%20a%20balance%20between%20recognition%20accuracy%20and%20input%20data%20dimensions.%20The%20open-source%20code%20of%20this%20paper%20can%20be%20found%20in%3A%20https%3A//github.com/JoeyBGOfficial/Represent-Micro-Doppler-Signature-in-Orders.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresent%2520Micro-Doppler%2520Signature%2520in%2520Orders%26entry.906535625%3DWeicheng%2520Gao%26entry.1292438233%3DNon-line-of-sight%2520sensing%2520of%2520human%2520activities%2520in%2520complex%2520environments%2520is%2520enabled%2520by%2520multiple-input%2520multiple-output%2520through-the-wall%2520radar%2520%2528TWR%2529.%2520However%252C%2520the%2520distinctiveness%2520of%2520micro-Doppler%2520signature%2520between%2520similar%2520indoor%2520human%2520activities%2520such%2520as%2520gun%2520carrying%2520and%2520normal%2520walking%2520is%2520minimal%252C%2520while%2520the%2520large%2520scale%2520of%2520input%2520images%2520required%2520for%2520effective%2520identification%2520utilizing%2520time-frequency%2520spectrograms%2520creates%2520challenges%2520for%2520model%2520training%2520and%2520inference%2520efficiency.%2520To%2520address%2520this%2520issue%252C%2520the%2520Chebyshev-time%2520map%2520is%2520proposed%2520in%2520this%2520paper%252C%2520which%2520is%2520a%2520method%2520characterizing%2520micro-Doppler%2520signature%2520using%2520polynomial%2520orders.%2520The%2520parametric%2520kinematic%2520models%2520for%2520human%2520motion%2520and%2520the%2520TWR%2520echo%2520model%2520are%2520first%2520established.%2520Then%252C%2520a%2520time-frequency%2520feature%2520representation%2520method%2520based%2520on%2520orthogonal%2520Chebyshev%2520polynomial%2520decomposition%2520is%2520proposed.%2520The%2520kinematic%2520envelopes%2520of%2520the%2520torso%2520and%2520limbs%2520are%2520extracted%252C%2520and%2520the%2520time-frequency%2520spectrum%2520slices%2520are%2520mapped%2520into%2520a%2520robust%2520Chebyshev-time%2520coefficient%2520space%252C%2520preserving%2520the%2520multi-order%2520morphological%2520detail%2520information%2520of%2520time-frequency%2520spectrum.%2520Numerical%2520simulations%2520and%2520experiments%2520are%2520conducted%2520to%2520verify%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520which%2520demonstrates%2520the%2520capability%2520to%2520characterize%2520armed%2520and%2520unarmed%2520indoor%2520human%2520activities%2520while%2520effectively%2520compressing%2520the%2520scale%2520of%2520the%2520time-frequency%2520spectrum%2520to%2520achieve%2520a%2520balance%2520between%2520recognition%2520accuracy%2520and%2520input%2520data%2520dimensions.%2520The%2520open-source%2520code%2520of%2520this%2520paper%2520can%2520be%2520found%2520in%253A%2520https%253A//github.com/JoeyBGOfficial/Represent-Micro-Doppler-Signature-in-Orders.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Represent%20Micro-Doppler%20Signature%20in%20Orders&entry.906535625=Weicheng%20Gao&entry.1292438233=Non-line-of-sight%20sensing%20of%20human%20activities%20in%20complex%20environments%20is%20enabled%20by%20multiple-input%20multiple-output%20through-the-wall%20radar%20%28TWR%29.%20However%2C%20the%20distinctiveness%20of%20micro-Doppler%20signature%20between%20similar%20indoor%20human%20activities%20such%20as%20gun%20carrying%20and%20normal%20walking%20is%20minimal%2C%20while%20the%20large%20scale%20of%20input%20images%20required%20for%20effective%20identification%20utilizing%20time-frequency%20spectrograms%20creates%20challenges%20for%20model%20training%20and%20inference%20efficiency.%20To%20address%20this%20issue%2C%20the%20Chebyshev-time%20map%20is%20proposed%20in%20this%20paper%2C%20which%20is%20a%20method%20characterizing%20micro-Doppler%20signature%20using%20polynomial%20orders.%20The%20parametric%20kinematic%20models%20for%20human%20motion%20and%20the%20TWR%20echo%20model%20are%20first%20established.%20Then%2C%20a%20time-frequency%20feature%20representation%20method%20based%20on%20orthogonal%20Chebyshev%20polynomial%20decomposition%20is%20proposed.%20The%20kinematic%20envelopes%20of%20the%20torso%20and%20limbs%20are%20extracted%2C%20and%20the%20time-frequency%20spectrum%20slices%20are%20mapped%20into%20a%20robust%20Chebyshev-time%20coefficient%20space%2C%20preserving%20the%20multi-order%20morphological%20detail%20information%20of%20time-frequency%20spectrum.%20Numerical%20simulations%20and%20experiments%20are%20conducted%20to%20verify%20the%20effectiveness%20of%20the%20proposed%20method%2C%20which%20demonstrates%20the%20capability%20to%20characterize%20armed%20and%20unarmed%20indoor%20human%20activities%20while%20effectively%20compressing%20the%20scale%20of%20the%20time-frequency%20spectrum%20to%20achieve%20a%20balance%20between%20recognition%20accuracy%20and%20input%20data%20dimensions.%20The%20open-source%20code%20of%20this%20paper%20can%20be%20found%20in%3A%20https%3A//github.com/JoeyBGOfficial/Represent-Micro-Doppler-Signature-in-Orders.&entry.1838667208=http%3A//arxiv.org/abs/2602.12985v1&entry.124074799=Read"},
{"title": "EXCODER: EXplainable Classification Of DiscretE time series Representations", "author": "Yannik Hahn and Antonin K\u00f6nigsfeld and Hasan Tercan and Tobias Meisen", "abstract": "Deep learning has significantly improved time series classification, yet the lack of explainability in these models remains a major challenge. While Explainable AI (XAI) techniques aim to make model decisions more transparent, their effectiveness is often hindered by the high dimensionality and noise present in raw time series data. In this work, we investigate whether transforming time series into discrete latent representations-using methods such as Vector Quantized Variational Autoencoders (VQ-VAE) and Discrete Variational Autoencoders (DVAE)-not only preserves but enhances explainability by reducing redundancy and focusing on the most informative patterns. We show that applying XAI methods to these compressed representations leads to concise and structured explanations that maintain faithfulness without sacrificing classification performance. Additionally, we propose Similar Subsequence Accuracy (SSA), a novel metric that quantitatively assesses the alignment between XAI-identified salient subsequences and the label distribution in the training data. SSA provides a systematic way to validate whether the features highlighted by XAI methods are truly representative of the learned classification patterns. Our findings demonstrate that discrete latent representations not only retain the essential characteristics needed for classification but also offer a pathway to more compact, interpretable, and computationally efficient explanations in time series analysis.", "link": "http://arxiv.org/abs/2602.13087v1", "date": "2026-02-13", "relevancy": 2.0298, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5269}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5011}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EXCODER%3A%20EXplainable%20Classification%20Of%20DiscretE%20time%20series%20Representations&body=Title%3A%20EXCODER%3A%20EXplainable%20Classification%20Of%20DiscretE%20time%20series%20Representations%0AAuthor%3A%20Yannik%20Hahn%20and%20Antonin%20K%C3%B6nigsfeld%20and%20Hasan%20Tercan%20and%20Tobias%20Meisen%0AAbstract%3A%20Deep%20learning%20has%20significantly%20improved%20time%20series%20classification%2C%20yet%20the%20lack%20of%20explainability%20in%20these%20models%20remains%20a%20major%20challenge.%20While%20Explainable%20AI%20%28XAI%29%20techniques%20aim%20to%20make%20model%20decisions%20more%20transparent%2C%20their%20effectiveness%20is%20often%20hindered%20by%20the%20high%20dimensionality%20and%20noise%20present%20in%20raw%20time%20series%20data.%20In%20this%20work%2C%20we%20investigate%20whether%20transforming%20time%20series%20into%20discrete%20latent%20representations-using%20methods%20such%20as%20Vector%20Quantized%20Variational%20Autoencoders%20%28VQ-VAE%29%20and%20Discrete%20Variational%20Autoencoders%20%28DVAE%29-not%20only%20preserves%20but%20enhances%20explainability%20by%20reducing%20redundancy%20and%20focusing%20on%20the%20most%20informative%20patterns.%20We%20show%20that%20applying%20XAI%20methods%20to%20these%20compressed%20representations%20leads%20to%20concise%20and%20structured%20explanations%20that%20maintain%20faithfulness%20without%20sacrificing%20classification%20performance.%20Additionally%2C%20we%20propose%20Similar%20Subsequence%20Accuracy%20%28SSA%29%2C%20a%20novel%20metric%20that%20quantitatively%20assesses%20the%20alignment%20between%20XAI-identified%20salient%20subsequences%20and%20the%20label%20distribution%20in%20the%20training%20data.%20SSA%20provides%20a%20systematic%20way%20to%20validate%20whether%20the%20features%20highlighted%20by%20XAI%20methods%20are%20truly%20representative%20of%20the%20learned%20classification%20patterns.%20Our%20findings%20demonstrate%20that%20discrete%20latent%20representations%20not%20only%20retain%20the%20essential%20characteristics%20needed%20for%20classification%20but%20also%20offer%20a%20pathway%20to%20more%20compact%2C%20interpretable%2C%20and%20computationally%20efficient%20explanations%20in%20time%20series%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEXCODER%253A%2520EXplainable%2520Classification%2520Of%2520DiscretE%2520time%2520series%2520Representations%26entry.906535625%3DYannik%2520Hahn%2520and%2520Antonin%2520K%25C3%25B6nigsfeld%2520and%2520Hasan%2520Tercan%2520and%2520Tobias%2520Meisen%26entry.1292438233%3DDeep%2520learning%2520has%2520significantly%2520improved%2520time%2520series%2520classification%252C%2520yet%2520the%2520lack%2520of%2520explainability%2520in%2520these%2520models%2520remains%2520a%2520major%2520challenge.%2520While%2520Explainable%2520AI%2520%2528XAI%2529%2520techniques%2520aim%2520to%2520make%2520model%2520decisions%2520more%2520transparent%252C%2520their%2520effectiveness%2520is%2520often%2520hindered%2520by%2520the%2520high%2520dimensionality%2520and%2520noise%2520present%2520in%2520raw%2520time%2520series%2520data.%2520In%2520this%2520work%252C%2520we%2520investigate%2520whether%2520transforming%2520time%2520series%2520into%2520discrete%2520latent%2520representations-using%2520methods%2520such%2520as%2520Vector%2520Quantized%2520Variational%2520Autoencoders%2520%2528VQ-VAE%2529%2520and%2520Discrete%2520Variational%2520Autoencoders%2520%2528DVAE%2529-not%2520only%2520preserves%2520but%2520enhances%2520explainability%2520by%2520reducing%2520redundancy%2520and%2520focusing%2520on%2520the%2520most%2520informative%2520patterns.%2520We%2520show%2520that%2520applying%2520XAI%2520methods%2520to%2520these%2520compressed%2520representations%2520leads%2520to%2520concise%2520and%2520structured%2520explanations%2520that%2520maintain%2520faithfulness%2520without%2520sacrificing%2520classification%2520performance.%2520Additionally%252C%2520we%2520propose%2520Similar%2520Subsequence%2520Accuracy%2520%2528SSA%2529%252C%2520a%2520novel%2520metric%2520that%2520quantitatively%2520assesses%2520the%2520alignment%2520between%2520XAI-identified%2520salient%2520subsequences%2520and%2520the%2520label%2520distribution%2520in%2520the%2520training%2520data.%2520SSA%2520provides%2520a%2520systematic%2520way%2520to%2520validate%2520whether%2520the%2520features%2520highlighted%2520by%2520XAI%2520methods%2520are%2520truly%2520representative%2520of%2520the%2520learned%2520classification%2520patterns.%2520Our%2520findings%2520demonstrate%2520that%2520discrete%2520latent%2520representations%2520not%2520only%2520retain%2520the%2520essential%2520characteristics%2520needed%2520for%2520classification%2520but%2520also%2520offer%2520a%2520pathway%2520to%2520more%2520compact%252C%2520interpretable%252C%2520and%2520computationally%2520efficient%2520explanations%2520in%2520time%2520series%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EXCODER%3A%20EXplainable%20Classification%20Of%20DiscretE%20time%20series%20Representations&entry.906535625=Yannik%20Hahn%20and%20Antonin%20K%C3%B6nigsfeld%20and%20Hasan%20Tercan%20and%20Tobias%20Meisen&entry.1292438233=Deep%20learning%20has%20significantly%20improved%20time%20series%20classification%2C%20yet%20the%20lack%20of%20explainability%20in%20these%20models%20remains%20a%20major%20challenge.%20While%20Explainable%20AI%20%28XAI%29%20techniques%20aim%20to%20make%20model%20decisions%20more%20transparent%2C%20their%20effectiveness%20is%20often%20hindered%20by%20the%20high%20dimensionality%20and%20noise%20present%20in%20raw%20time%20series%20data.%20In%20this%20work%2C%20we%20investigate%20whether%20transforming%20time%20series%20into%20discrete%20latent%20representations-using%20methods%20such%20as%20Vector%20Quantized%20Variational%20Autoencoders%20%28VQ-VAE%29%20and%20Discrete%20Variational%20Autoencoders%20%28DVAE%29-not%20only%20preserves%20but%20enhances%20explainability%20by%20reducing%20redundancy%20and%20focusing%20on%20the%20most%20informative%20patterns.%20We%20show%20that%20applying%20XAI%20methods%20to%20these%20compressed%20representations%20leads%20to%20concise%20and%20structured%20explanations%20that%20maintain%20faithfulness%20without%20sacrificing%20classification%20performance.%20Additionally%2C%20we%20propose%20Similar%20Subsequence%20Accuracy%20%28SSA%29%2C%20a%20novel%20metric%20that%20quantitatively%20assesses%20the%20alignment%20between%20XAI-identified%20salient%20subsequences%20and%20the%20label%20distribution%20in%20the%20training%20data.%20SSA%20provides%20a%20systematic%20way%20to%20validate%20whether%20the%20features%20highlighted%20by%20XAI%20methods%20are%20truly%20representative%20of%20the%20learned%20classification%20patterns.%20Our%20findings%20demonstrate%20that%20discrete%20latent%20representations%20not%20only%20retain%20the%20essential%20characteristics%20needed%20for%20classification%20but%20also%20offer%20a%20pathway%20to%20more%20compact%2C%20interpretable%2C%20and%20computationally%20efficient%20explanations%20in%20time%20series%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2602.13087v1&entry.124074799=Read"},
{"title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning", "author": "Zelai Xu and Zhexuan Xu and Ruize Zhang and Chunyang Zhu and Shi Yu and Weilin Liu and Quanlu Zhang and Wenbo Ding and Chao Yu and Yu Wang", "abstract": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.", "link": "http://arxiv.org/abs/2602.04634v2", "date": "2026-02-13", "relevancy": 2.027, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5073}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WideSeek-R1%3A%20Exploring%20Width%20Scaling%20for%20Broad%20Information%20Seeking%20via%20Multi-Agent%20Reinforcement%20Learning&body=Title%3A%20WideSeek-R1%3A%20Exploring%20Width%20Scaling%20for%20Broad%20Information%20Seeking%20via%20Multi-Agent%20Reinforcement%20Learning%0AAuthor%3A%20Zelai%20Xu%20and%20Zhexuan%20Xu%20and%20Ruize%20Zhang%20and%20Chunyang%20Zhu%20and%20Shi%20Yu%20and%20Weilin%20Liu%20and%20Quanlu%20Zhang%20and%20Wenbo%20Ding%20and%20Chao%20Yu%20and%20Yu%20Wang%0AAbstract%3A%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20largely%20focused%20on%20depth%20scaling%2C%20where%20a%20single%20agent%20solves%20long-horizon%20problems%20with%20multi-turn%20reasoning%20and%20tool%20use.%20However%2C%20as%20tasks%20grow%20broader%2C%20the%20key%20bottleneck%20shifts%20from%20individual%20competence%20to%20organizational%20capability.%20In%20this%20work%2C%20we%20explore%20a%20complementary%20dimension%20of%20width%20scaling%20with%20multi-agent%20systems%20to%20address%20broad%20information%20seeking.%20Existing%20multi-agent%20systems%20often%20rely%20on%20hand-crafted%20workflows%20and%20turn-taking%20interactions%20that%20fail%20to%20parallelize%20work%20effectively.%20To%20bridge%20this%20gap%2C%20we%20propose%20WideSeek-R1%2C%20a%20lead-agent-subagent%20framework%20trained%20via%20multi-agent%20reinforcement%20learning%20%28MARL%29%20to%20synergize%20scalable%20orchestration%20and%20parallel%20execution.%20By%20utilizing%20a%20shared%20LLM%20with%20isolated%20contexts%20and%20specialized%20tools%2C%20WideSeek-R1%20jointly%20optimizes%20the%20lead%20agent%20and%20parallel%20subagents%20on%20a%20curated%20dataset%20of%2020k%20broad%20information-seeking%20tasks.%20Extensive%20experiments%20show%20that%20WideSeek-R1-4B%20achieves%20an%20item%20F1%20score%20of%2040.0%25%20on%20the%20WideSearch%20benchmark%2C%20which%20is%20comparable%20to%20the%20performance%20of%20single-agent%20DeepSeek-R1-671B.%20Furthermore%2C%20WideSeek-R1-4B%20exhibits%20consistent%20performance%20gains%20as%20the%20number%20of%20parallel%20subagents%20increases%2C%20highlighting%20the%20effectiveness%20of%20width%20scaling.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWideSeek-R1%253A%2520Exploring%2520Width%2520Scaling%2520for%2520Broad%2520Information%2520Seeking%2520via%2520Multi-Agent%2520Reinforcement%2520Learning%26entry.906535625%3DZelai%2520Xu%2520and%2520Zhexuan%2520Xu%2520and%2520Ruize%2520Zhang%2520and%2520Chunyang%2520Zhu%2520and%2520Shi%2520Yu%2520and%2520Weilin%2520Liu%2520and%2520Quanlu%2520Zhang%2520and%2520Wenbo%2520Ding%2520and%2520Chao%2520Yu%2520and%2520Yu%2520Wang%26entry.1292438233%3DRecent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520largely%2520focused%2520on%2520depth%2520scaling%252C%2520where%2520a%2520single%2520agent%2520solves%2520long-horizon%2520problems%2520with%2520multi-turn%2520reasoning%2520and%2520tool%2520use.%2520However%252C%2520as%2520tasks%2520grow%2520broader%252C%2520the%2520key%2520bottleneck%2520shifts%2520from%2520individual%2520competence%2520to%2520organizational%2520capability.%2520In%2520this%2520work%252C%2520we%2520explore%2520a%2520complementary%2520dimension%2520of%2520width%2520scaling%2520with%2520multi-agent%2520systems%2520to%2520address%2520broad%2520information%2520seeking.%2520Existing%2520multi-agent%2520systems%2520often%2520rely%2520on%2520hand-crafted%2520workflows%2520and%2520turn-taking%2520interactions%2520that%2520fail%2520to%2520parallelize%2520work%2520effectively.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520WideSeek-R1%252C%2520a%2520lead-agent-subagent%2520framework%2520trained%2520via%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%2520to%2520synergize%2520scalable%2520orchestration%2520and%2520parallel%2520execution.%2520By%2520utilizing%2520a%2520shared%2520LLM%2520with%2520isolated%2520contexts%2520and%2520specialized%2520tools%252C%2520WideSeek-R1%2520jointly%2520optimizes%2520the%2520lead%2520agent%2520and%2520parallel%2520subagents%2520on%2520a%2520curated%2520dataset%2520of%252020k%2520broad%2520information-seeking%2520tasks.%2520Extensive%2520experiments%2520show%2520that%2520WideSeek-R1-4B%2520achieves%2520an%2520item%2520F1%2520score%2520of%252040.0%2525%2520on%2520the%2520WideSearch%2520benchmark%252C%2520which%2520is%2520comparable%2520to%2520the%2520performance%2520of%2520single-agent%2520DeepSeek-R1-671B.%2520Furthermore%252C%2520WideSeek-R1-4B%2520exhibits%2520consistent%2520performance%2520gains%2520as%2520the%2520number%2520of%2520parallel%2520subagents%2520increases%252C%2520highlighting%2520the%2520effectiveness%2520of%2520width%2520scaling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WideSeek-R1%3A%20Exploring%20Width%20Scaling%20for%20Broad%20Information%20Seeking%20via%20Multi-Agent%20Reinforcement%20Learning&entry.906535625=Zelai%20Xu%20and%20Zhexuan%20Xu%20and%20Ruize%20Zhang%20and%20Chunyang%20Zhu%20and%20Shi%20Yu%20and%20Weilin%20Liu%20and%20Quanlu%20Zhang%20and%20Wenbo%20Ding%20and%20Chao%20Yu%20and%20Yu%20Wang&entry.1292438233=Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20largely%20focused%20on%20depth%20scaling%2C%20where%20a%20single%20agent%20solves%20long-horizon%20problems%20with%20multi-turn%20reasoning%20and%20tool%20use.%20However%2C%20as%20tasks%20grow%20broader%2C%20the%20key%20bottleneck%20shifts%20from%20individual%20competence%20to%20organizational%20capability.%20In%20this%20work%2C%20we%20explore%20a%20complementary%20dimension%20of%20width%20scaling%20with%20multi-agent%20systems%20to%20address%20broad%20information%20seeking.%20Existing%20multi-agent%20systems%20often%20rely%20on%20hand-crafted%20workflows%20and%20turn-taking%20interactions%20that%20fail%20to%20parallelize%20work%20effectively.%20To%20bridge%20this%20gap%2C%20we%20propose%20WideSeek-R1%2C%20a%20lead-agent-subagent%20framework%20trained%20via%20multi-agent%20reinforcement%20learning%20%28MARL%29%20to%20synergize%20scalable%20orchestration%20and%20parallel%20execution.%20By%20utilizing%20a%20shared%20LLM%20with%20isolated%20contexts%20and%20specialized%20tools%2C%20WideSeek-R1%20jointly%20optimizes%20the%20lead%20agent%20and%20parallel%20subagents%20on%20a%20curated%20dataset%20of%2020k%20broad%20information-seeking%20tasks.%20Extensive%20experiments%20show%20that%20WideSeek-R1-4B%20achieves%20an%20item%20F1%20score%20of%2040.0%25%20on%20the%20WideSearch%20benchmark%2C%20which%20is%20comparable%20to%20the%20performance%20of%20single-agent%20DeepSeek-R1-671B.%20Furthermore%2C%20WideSeek-R1-4B%20exhibits%20consistent%20performance%20gains%20as%20the%20number%20of%20parallel%20subagents%20increases%2C%20highlighting%20the%20effectiveness%20of%20width%20scaling.&entry.1838667208=http%3A//arxiv.org/abs/2602.04634v2&entry.124074799=Read"},
{"title": "RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads", "author": "Vijayasri Iyer and Maahin Rathinagiriswaran and Jyothikamalesh S", "abstract": "Understanding road scenes is essential for autonomous driving, as it enables systems to interpret visual surroundings to aid in effective decision-making. We present Roadscapes, a multitask multimodal dataset consisting of upto 9,000 images captured in diverse Indian driving environments, accompanied by manually verified bounding boxes. To facilitate scalable scene understanding, we employ rule-based heuristics to infer various scene attributes, which are subsequently used to generate question-answer (QA) pairs for tasks such as object grounding, reasoning, and scene understanding. The dataset includes a variety of scenes from urban and rural India, encompassing highways, service roads, village paths, and congested city streets, captured in both daytime and nighttime settings. Roadscapes has been curated to advance research on visual scene understanding in unstructured environments. In this paper, we describe the data collection and annotation process, present key dataset statistics, and provide initial baselines for image QA tasks using vision-language models.", "link": "http://arxiv.org/abs/2602.12877v1", "date": "2026-02-13", "relevancy": 2.0242, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5084}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5084}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoadscapesQA%3A%20A%20Multitask%2C%20Multimodal%20Dataset%20for%20Visual%20Question%20Answering%20on%20Indian%20Roads&body=Title%3A%20RoadscapesQA%3A%20A%20Multitask%2C%20Multimodal%20Dataset%20for%20Visual%20Question%20Answering%20on%20Indian%20Roads%0AAuthor%3A%20Vijayasri%20Iyer%20and%20Maahin%20Rathinagiriswaran%20and%20Jyothikamalesh%20S%0AAbstract%3A%20Understanding%20road%20scenes%20is%20essential%20for%20autonomous%20driving%2C%20as%20it%20enables%20systems%20to%20interpret%20visual%20surroundings%20to%20aid%20in%20effective%20decision-making.%20We%20present%20Roadscapes%2C%20a%20multitask%20multimodal%20dataset%20consisting%20of%20upto%209%2C000%20images%20captured%20in%20diverse%20Indian%20driving%20environments%2C%20accompanied%20by%20manually%20verified%20bounding%20boxes.%20To%20facilitate%20scalable%20scene%20understanding%2C%20we%20employ%20rule-based%20heuristics%20to%20infer%20various%20scene%20attributes%2C%20which%20are%20subsequently%20used%20to%20generate%20question-answer%20%28QA%29%20pairs%20for%20tasks%20such%20as%20object%20grounding%2C%20reasoning%2C%20and%20scene%20understanding.%20The%20dataset%20includes%20a%20variety%20of%20scenes%20from%20urban%20and%20rural%20India%2C%20encompassing%20highways%2C%20service%20roads%2C%20village%20paths%2C%20and%20congested%20city%20streets%2C%20captured%20in%20both%20daytime%20and%20nighttime%20settings.%20Roadscapes%20has%20been%20curated%20to%20advance%20research%20on%20visual%20scene%20understanding%20in%20unstructured%20environments.%20In%20this%20paper%2C%20we%20describe%20the%20data%20collection%20and%20annotation%20process%2C%20present%20key%20dataset%20statistics%2C%20and%20provide%20initial%20baselines%20for%20image%20QA%20tasks%20using%20vision-language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoadscapesQA%253A%2520A%2520Multitask%252C%2520Multimodal%2520Dataset%2520for%2520Visual%2520Question%2520Answering%2520on%2520Indian%2520Roads%26entry.906535625%3DVijayasri%2520Iyer%2520and%2520Maahin%2520Rathinagiriswaran%2520and%2520Jyothikamalesh%2520S%26entry.1292438233%3DUnderstanding%2520road%2520scenes%2520is%2520essential%2520for%2520autonomous%2520driving%252C%2520as%2520it%2520enables%2520systems%2520to%2520interpret%2520visual%2520surroundings%2520to%2520aid%2520in%2520effective%2520decision-making.%2520We%2520present%2520Roadscapes%252C%2520a%2520multitask%2520multimodal%2520dataset%2520consisting%2520of%2520upto%25209%252C000%2520images%2520captured%2520in%2520diverse%2520Indian%2520driving%2520environments%252C%2520accompanied%2520by%2520manually%2520verified%2520bounding%2520boxes.%2520To%2520facilitate%2520scalable%2520scene%2520understanding%252C%2520we%2520employ%2520rule-based%2520heuristics%2520to%2520infer%2520various%2520scene%2520attributes%252C%2520which%2520are%2520subsequently%2520used%2520to%2520generate%2520question-answer%2520%2528QA%2529%2520pairs%2520for%2520tasks%2520such%2520as%2520object%2520grounding%252C%2520reasoning%252C%2520and%2520scene%2520understanding.%2520The%2520dataset%2520includes%2520a%2520variety%2520of%2520scenes%2520from%2520urban%2520and%2520rural%2520India%252C%2520encompassing%2520highways%252C%2520service%2520roads%252C%2520village%2520paths%252C%2520and%2520congested%2520city%2520streets%252C%2520captured%2520in%2520both%2520daytime%2520and%2520nighttime%2520settings.%2520Roadscapes%2520has%2520been%2520curated%2520to%2520advance%2520research%2520on%2520visual%2520scene%2520understanding%2520in%2520unstructured%2520environments.%2520In%2520this%2520paper%252C%2520we%2520describe%2520the%2520data%2520collection%2520and%2520annotation%2520process%252C%2520present%2520key%2520dataset%2520statistics%252C%2520and%2520provide%2520initial%2520baselines%2520for%2520image%2520QA%2520tasks%2520using%2520vision-language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoadscapesQA%3A%20A%20Multitask%2C%20Multimodal%20Dataset%20for%20Visual%20Question%20Answering%20on%20Indian%20Roads&entry.906535625=Vijayasri%20Iyer%20and%20Maahin%20Rathinagiriswaran%20and%20Jyothikamalesh%20S&entry.1292438233=Understanding%20road%20scenes%20is%20essential%20for%20autonomous%20driving%2C%20as%20it%20enables%20systems%20to%20interpret%20visual%20surroundings%20to%20aid%20in%20effective%20decision-making.%20We%20present%20Roadscapes%2C%20a%20multitask%20multimodal%20dataset%20consisting%20of%20upto%209%2C000%20images%20captured%20in%20diverse%20Indian%20driving%20environments%2C%20accompanied%20by%20manually%20verified%20bounding%20boxes.%20To%20facilitate%20scalable%20scene%20understanding%2C%20we%20employ%20rule-based%20heuristics%20to%20infer%20various%20scene%20attributes%2C%20which%20are%20subsequently%20used%20to%20generate%20question-answer%20%28QA%29%20pairs%20for%20tasks%20such%20as%20object%20grounding%2C%20reasoning%2C%20and%20scene%20understanding.%20The%20dataset%20includes%20a%20variety%20of%20scenes%20from%20urban%20and%20rural%20India%2C%20encompassing%20highways%2C%20service%20roads%2C%20village%20paths%2C%20and%20congested%20city%20streets%2C%20captured%20in%20both%20daytime%20and%20nighttime%20settings.%20Roadscapes%20has%20been%20curated%20to%20advance%20research%20on%20visual%20scene%20understanding%20in%20unstructured%20environments.%20In%20this%20paper%2C%20we%20describe%20the%20data%20collection%20and%20annotation%20process%2C%20present%20key%20dataset%20statistics%2C%20and%20provide%20initial%20baselines%20for%20image%20QA%20tasks%20using%20vision-language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2602.12877v1&entry.124074799=Read"},
{"title": "How to Train Your LLM Web Agent: A Statistical Diagnosis", "author": "Dheeraj Vattikonda and Santhoshi Ravichandran and Emiliano Penaloza and Hadi Nekoei and Megh Thakkar and Thibault Le Sellier de Chezelles and Nicolas Gontier and Miguel Mu\u00f1oz-M\u00e1rmol and Sahar Omidi Shayegan and Stefania Raimondo and Xue Liu and Alexandre Drouin and Laurent Charlin and Alexandre Pich\u00e9 and Alexandre Lacoste and Massimo Caccia", "abstract": "LLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systems, widening the gap with open-source alternatives. Progress has been held back by two key challenges: first, a narrow focus on single-step tasks that overlooks the complexity of multi-step web interactions; and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices, making exhaustive sweeps impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy requires only 55% of the compute to match the peak performance of pure SFT on MiniWob++, effectively pushing the compute-performance Pareto frontier, and is the only strategy that can close the gap with closed-source models.", "link": "http://arxiv.org/abs/2507.04103v4", "date": "2026-02-13", "relevancy": 2.0219, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5557}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4994}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Train%20Your%20LLM%20Web%20Agent%3A%20A%20Statistical%20Diagnosis&body=Title%3A%20How%20to%20Train%20Your%20LLM%20Web%20Agent%3A%20A%20Statistical%20Diagnosis%0AAuthor%3A%20Dheeraj%20Vattikonda%20and%20Santhoshi%20Ravichandran%20and%20Emiliano%20Penaloza%20and%20Hadi%20Nekoei%20and%20Megh%20Thakkar%20and%20Thibault%20Le%20Sellier%20de%20Chezelles%20and%20Nicolas%20Gontier%20and%20Miguel%20Mu%C3%B1oz-M%C3%A1rmol%20and%20Sahar%20Omidi%20Shayegan%20and%20Stefania%20Raimondo%20and%20Xue%20Liu%20and%20Alexandre%20Drouin%20and%20Laurent%20Charlin%20and%20Alexandre%20Pich%C3%A9%20and%20Alexandre%20Lacoste%20and%20Massimo%20Caccia%0AAbstract%3A%20LLM-based%20web%20agents%20have%20recently%20made%20significant%20progress%2C%20but%20much%20of%20it%20has%20occurred%20in%20closed-source%20systems%2C%20widening%20the%20gap%20with%20open-source%20alternatives.%20Progress%20has%20been%20held%20back%20by%20two%20key%20challenges%3A%20first%2C%20a%20narrow%20focus%20on%20single-step%20tasks%20that%20overlooks%20the%20complexity%20of%20multi-step%20web%20interactions%3B%20and%20second%2C%20the%20high%20compute%20costs%20required%20to%20post-train%20LLM-based%20web%20agents.%20To%20address%20this%2C%20we%20present%20the%20first%20statistically%20grounded%20study%20on%20compute%20allocation%20for%20LLM%20web-agent%20post-training.%20Our%20approach%20uses%20a%20two-stage%20pipeline%2C%20training%20a%20Llama%203.1%208B%20student%20to%20imitate%20a%20Llama%203.3%2070B%20teacher%20via%20supervised%20fine-tuning%20%28SFT%29%2C%20followed%20by%20on-policy%20reinforcement%20learning.%20We%20find%20this%20process%20highly%20sensitive%20to%20hyperparameter%20choices%2C%20making%20exhaustive%20sweeps%20impractical.%20To%20spare%20others%20from%20expensive%20trial-and-error%2C%20we%20sample%201%2C370%20configurations%20and%20use%20bootstrapping%20to%20estimate%20effective%20hyperparameters.%20Our%20results%20show%20that%20combining%20SFT%20with%20on-policy%20RL%20consistently%20outperforms%20either%20approach%20alone%20on%20both%20WorkArena%20and%20MiniWob%2B%2B.%20Further%2C%20this%20strategy%20requires%20only%2055%25%20of%20the%20compute%20to%20match%20the%20peak%20performance%20of%20pure%20SFT%20on%20MiniWob%2B%2B%2C%20effectively%20pushing%20the%20compute-performance%20Pareto%20frontier%2C%20and%20is%20the%20only%20strategy%20that%20can%20close%20the%20gap%20with%20closed-source%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2507.04103v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Train%2520Your%2520LLM%2520Web%2520Agent%253A%2520A%2520Statistical%2520Diagnosis%26entry.906535625%3DDheeraj%2520Vattikonda%2520and%2520Santhoshi%2520Ravichandran%2520and%2520Emiliano%2520Penaloza%2520and%2520Hadi%2520Nekoei%2520and%2520Megh%2520Thakkar%2520and%2520Thibault%2520Le%2520Sellier%2520de%2520Chezelles%2520and%2520Nicolas%2520Gontier%2520and%2520Miguel%2520Mu%25C3%25B1oz-M%25C3%25A1rmol%2520and%2520Sahar%2520Omidi%2520Shayegan%2520and%2520Stefania%2520Raimondo%2520and%2520Xue%2520Liu%2520and%2520Alexandre%2520Drouin%2520and%2520Laurent%2520Charlin%2520and%2520Alexandre%2520Pich%25C3%25A9%2520and%2520Alexandre%2520Lacoste%2520and%2520Massimo%2520Caccia%26entry.1292438233%3DLLM-based%2520web%2520agents%2520have%2520recently%2520made%2520significant%2520progress%252C%2520but%2520much%2520of%2520it%2520has%2520occurred%2520in%2520closed-source%2520systems%252C%2520widening%2520the%2520gap%2520with%2520open-source%2520alternatives.%2520Progress%2520has%2520been%2520held%2520back%2520by%2520two%2520key%2520challenges%253A%2520first%252C%2520a%2520narrow%2520focus%2520on%2520single-step%2520tasks%2520that%2520overlooks%2520the%2520complexity%2520of%2520multi-step%2520web%2520interactions%253B%2520and%2520second%252C%2520the%2520high%2520compute%2520costs%2520required%2520to%2520post-train%2520LLM-based%2520web%2520agents.%2520To%2520address%2520this%252C%2520we%2520present%2520the%2520first%2520statistically%2520grounded%2520study%2520on%2520compute%2520allocation%2520for%2520LLM%2520web-agent%2520post-training.%2520Our%2520approach%2520uses%2520a%2520two-stage%2520pipeline%252C%2520training%2520a%2520Llama%25203.1%25208B%2520student%2520to%2520imitate%2520a%2520Llama%25203.3%252070B%2520teacher%2520via%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520followed%2520by%2520on-policy%2520reinforcement%2520learning.%2520We%2520find%2520this%2520process%2520highly%2520sensitive%2520to%2520hyperparameter%2520choices%252C%2520making%2520exhaustive%2520sweeps%2520impractical.%2520To%2520spare%2520others%2520from%2520expensive%2520trial-and-error%252C%2520we%2520sample%25201%252C370%2520configurations%2520and%2520use%2520bootstrapping%2520to%2520estimate%2520effective%2520hyperparameters.%2520Our%2520results%2520show%2520that%2520combining%2520SFT%2520with%2520on-policy%2520RL%2520consistently%2520outperforms%2520either%2520approach%2520alone%2520on%2520both%2520WorkArena%2520and%2520MiniWob%252B%252B.%2520Further%252C%2520this%2520strategy%2520requires%2520only%252055%2525%2520of%2520the%2520compute%2520to%2520match%2520the%2520peak%2520performance%2520of%2520pure%2520SFT%2520on%2520MiniWob%252B%252B%252C%2520effectively%2520pushing%2520the%2520compute-performance%2520Pareto%2520frontier%252C%2520and%2520is%2520the%2520only%2520strategy%2520that%2520can%2520close%2520the%2520gap%2520with%2520closed-source%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04103v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Train%20Your%20LLM%20Web%20Agent%3A%20A%20Statistical%20Diagnosis&entry.906535625=Dheeraj%20Vattikonda%20and%20Santhoshi%20Ravichandran%20and%20Emiliano%20Penaloza%20and%20Hadi%20Nekoei%20and%20Megh%20Thakkar%20and%20Thibault%20Le%20Sellier%20de%20Chezelles%20and%20Nicolas%20Gontier%20and%20Miguel%20Mu%C3%B1oz-M%C3%A1rmol%20and%20Sahar%20Omidi%20Shayegan%20and%20Stefania%20Raimondo%20and%20Xue%20Liu%20and%20Alexandre%20Drouin%20and%20Laurent%20Charlin%20and%20Alexandre%20Pich%C3%A9%20and%20Alexandre%20Lacoste%20and%20Massimo%20Caccia&entry.1292438233=LLM-based%20web%20agents%20have%20recently%20made%20significant%20progress%2C%20but%20much%20of%20it%20has%20occurred%20in%20closed-source%20systems%2C%20widening%20the%20gap%20with%20open-source%20alternatives.%20Progress%20has%20been%20held%20back%20by%20two%20key%20challenges%3A%20first%2C%20a%20narrow%20focus%20on%20single-step%20tasks%20that%20overlooks%20the%20complexity%20of%20multi-step%20web%20interactions%3B%20and%20second%2C%20the%20high%20compute%20costs%20required%20to%20post-train%20LLM-based%20web%20agents.%20To%20address%20this%2C%20we%20present%20the%20first%20statistically%20grounded%20study%20on%20compute%20allocation%20for%20LLM%20web-agent%20post-training.%20Our%20approach%20uses%20a%20two-stage%20pipeline%2C%20training%20a%20Llama%203.1%208B%20student%20to%20imitate%20a%20Llama%203.3%2070B%20teacher%20via%20supervised%20fine-tuning%20%28SFT%29%2C%20followed%20by%20on-policy%20reinforcement%20learning.%20We%20find%20this%20process%20highly%20sensitive%20to%20hyperparameter%20choices%2C%20making%20exhaustive%20sweeps%20impractical.%20To%20spare%20others%20from%20expensive%20trial-and-error%2C%20we%20sample%201%2C370%20configurations%20and%20use%20bootstrapping%20to%20estimate%20effective%20hyperparameters.%20Our%20results%20show%20that%20combining%20SFT%20with%20on-policy%20RL%20consistently%20outperforms%20either%20approach%20alone%20on%20both%20WorkArena%20and%20MiniWob%2B%2B.%20Further%2C%20this%20strategy%20requires%20only%2055%25%20of%20the%20compute%20to%20match%20the%20peak%20performance%20of%20pure%20SFT%20on%20MiniWob%2B%2B%2C%20effectively%20pushing%20the%20compute-performance%20Pareto%20frontier%2C%20and%20is%20the%20only%20strategy%20that%20can%20close%20the%20gap%20with%20closed-source%20models.&entry.1838667208=http%3A//arxiv.org/abs/2507.04103v4&entry.124074799=Read"},
{"title": "Reasoning about Intent for Ambiguous Requests", "author": "Irina Saparina and Mirella Lapata", "abstract": "Large language models often respond to ambiguous requests by implicitly committing to one interpretation. Intent misunderstandings can frustrate users and create safety risks. To address this, we propose generating multiple interpretation-answer pairs in a single structured response to ambiguous requests. Our models are trained with reinforcement learning and customized reward functions using multiple valid answers as supervision. Experiments on conversational question answering and semantic parsing demonstrate that our method achieves higher coverage of valid answers than baseline approaches. Human evaluation confirms that predicted interpretations are highly aligned with their answers. Our approach promotes transparency with explicit interpretations, achieves efficiency by requiring only one generation step, and supports downstream applications through its structured output format.", "link": "http://arxiv.org/abs/2511.10453v2", "date": "2026-02-13", "relevancy": 2.0205, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5224}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20about%20Intent%20for%20Ambiguous%20Requests&body=Title%3A%20Reasoning%20about%20Intent%20for%20Ambiguous%20Requests%0AAuthor%3A%20Irina%20Saparina%20and%20Mirella%20Lapata%0AAbstract%3A%20Large%20language%20models%20often%20respond%20to%20ambiguous%20requests%20by%20implicitly%20committing%20to%20one%20interpretation.%20Intent%20misunderstandings%20can%20frustrate%20users%20and%20create%20safety%20risks.%20To%20address%20this%2C%20we%20propose%20generating%20multiple%20interpretation-answer%20pairs%20in%20a%20single%20structured%20response%20to%20ambiguous%20requests.%20Our%20models%20are%20trained%20with%20reinforcement%20learning%20and%20customized%20reward%20functions%20using%20multiple%20valid%20answers%20as%20supervision.%20Experiments%20on%20conversational%20question%20answering%20and%20semantic%20parsing%20demonstrate%20that%20our%20method%20achieves%20higher%20coverage%20of%20valid%20answers%20than%20baseline%20approaches.%20Human%20evaluation%20confirms%20that%20predicted%20interpretations%20are%20highly%20aligned%20with%20their%20answers.%20Our%20approach%20promotes%20transparency%20with%20explicit%20interpretations%2C%20achieves%20efficiency%20by%20requiring%20only%20one%20generation%20step%2C%20and%20supports%20downstream%20applications%20through%20its%20structured%20output%20format.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10453v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520about%2520Intent%2520for%2520Ambiguous%2520Requests%26entry.906535625%3DIrina%2520Saparina%2520and%2520Mirella%2520Lapata%26entry.1292438233%3DLarge%2520language%2520models%2520often%2520respond%2520to%2520ambiguous%2520requests%2520by%2520implicitly%2520committing%2520to%2520one%2520interpretation.%2520Intent%2520misunderstandings%2520can%2520frustrate%2520users%2520and%2520create%2520safety%2520risks.%2520To%2520address%2520this%252C%2520we%2520propose%2520generating%2520multiple%2520interpretation-answer%2520pairs%2520in%2520a%2520single%2520structured%2520response%2520to%2520ambiguous%2520requests.%2520Our%2520models%2520are%2520trained%2520with%2520reinforcement%2520learning%2520and%2520customized%2520reward%2520functions%2520using%2520multiple%2520valid%2520answers%2520as%2520supervision.%2520Experiments%2520on%2520conversational%2520question%2520answering%2520and%2520semantic%2520parsing%2520demonstrate%2520that%2520our%2520method%2520achieves%2520higher%2520coverage%2520of%2520valid%2520answers%2520than%2520baseline%2520approaches.%2520Human%2520evaluation%2520confirms%2520that%2520predicted%2520interpretations%2520are%2520highly%2520aligned%2520with%2520their%2520answers.%2520Our%2520approach%2520promotes%2520transparency%2520with%2520explicit%2520interpretations%252C%2520achieves%2520efficiency%2520by%2520requiring%2520only%2520one%2520generation%2520step%252C%2520and%2520supports%2520downstream%2520applications%2520through%2520its%2520structured%2520output%2520format.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10453v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20about%20Intent%20for%20Ambiguous%20Requests&entry.906535625=Irina%20Saparina%20and%20Mirella%20Lapata&entry.1292438233=Large%20language%20models%20often%20respond%20to%20ambiguous%20requests%20by%20implicitly%20committing%20to%20one%20interpretation.%20Intent%20misunderstandings%20can%20frustrate%20users%20and%20create%20safety%20risks.%20To%20address%20this%2C%20we%20propose%20generating%20multiple%20interpretation-answer%20pairs%20in%20a%20single%20structured%20response%20to%20ambiguous%20requests.%20Our%20models%20are%20trained%20with%20reinforcement%20learning%20and%20customized%20reward%20functions%20using%20multiple%20valid%20answers%20as%20supervision.%20Experiments%20on%20conversational%20question%20answering%20and%20semantic%20parsing%20demonstrate%20that%20our%20method%20achieves%20higher%20coverage%20of%20valid%20answers%20than%20baseline%20approaches.%20Human%20evaluation%20confirms%20that%20predicted%20interpretations%20are%20highly%20aligned%20with%20their%20answers.%20Our%20approach%20promotes%20transparency%20with%20explicit%20interpretations%2C%20achieves%20efficiency%20by%20requiring%20only%20one%20generation%20step%2C%20and%20supports%20downstream%20applications%20through%20its%20structured%20output%20format.&entry.1838667208=http%3A//arxiv.org/abs/2511.10453v2&entry.124074799=Read"},
{"title": "Drift-Aware Variational Autoencoder-based Anomaly Detection with Two-level Ensembling", "author": "Jin Li and Kleanthis Malialis and Christos G. Panayiotou and Marios M. Polycarpou", "abstract": "In today's digital world, the generation of vast amounts of streaming data in various domains has become ubiquitous. However, many of these data are unlabeled, making it challenging to identify events, particularly anomalies. This task becomes even more formidable in nonstationary environments where model performance can deteriorate over time due to concept drift. To address these challenges, this paper presents a novel method, VAE++ESDD, which employs incremental learning and two-level ensembling: an ensemble of Variational AutoEncoder(VAEs) for anomaly prediction, along with an ensemble of concept drift detectors. Each drift detector utilizes a statistical-based concept drift mechanism. To evaluate the effectiveness of VAE++ESDD, we conduct a comprehensive experimental study using real-world and synthetic datasets characterized by severely or extremely low anomalous rates and various drift characteristics. Our study reveals that the proposed method significantly outperforms both strong baselines and state-of-the-art methods.", "link": "http://arxiv.org/abs/2602.12976v1", "date": "2026-02-13", "relevancy": 2.0194, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5131}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5052}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drift-Aware%20Variational%20Autoencoder-based%20Anomaly%20Detection%20with%20Two-level%20Ensembling&body=Title%3A%20Drift-Aware%20Variational%20Autoencoder-based%20Anomaly%20Detection%20with%20Two-level%20Ensembling%0AAuthor%3A%20Jin%20Li%20and%20Kleanthis%20Malialis%20and%20Christos%20G.%20Panayiotou%20and%20Marios%20M.%20Polycarpou%0AAbstract%3A%20In%20today%27s%20digital%20world%2C%20the%20generation%20of%20vast%20amounts%20of%20streaming%20data%20in%20various%20domains%20has%20become%20ubiquitous.%20However%2C%20many%20of%20these%20data%20are%20unlabeled%2C%20making%20it%20challenging%20to%20identify%20events%2C%20particularly%20anomalies.%20This%20task%20becomes%20even%20more%20formidable%20in%20nonstationary%20environments%20where%20model%20performance%20can%20deteriorate%20over%20time%20due%20to%20concept%20drift.%20To%20address%20these%20challenges%2C%20this%20paper%20presents%20a%20novel%20method%2C%20VAE%2B%2BESDD%2C%20which%20employs%20incremental%20learning%20and%20two-level%20ensembling%3A%20an%20ensemble%20of%20Variational%20AutoEncoder%28VAEs%29%20for%20anomaly%20prediction%2C%20along%20with%20an%20ensemble%20of%20concept%20drift%20detectors.%20Each%20drift%20detector%20utilizes%20a%20statistical-based%20concept%20drift%20mechanism.%20To%20evaluate%20the%20effectiveness%20of%20VAE%2B%2BESDD%2C%20we%20conduct%20a%20comprehensive%20experimental%20study%20using%20real-world%20and%20synthetic%20datasets%20characterized%20by%20severely%20or%20extremely%20low%20anomalous%20rates%20and%20various%20drift%20characteristics.%20Our%20study%20reveals%20that%20the%20proposed%20method%20significantly%20outperforms%20both%20strong%20baselines%20and%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrift-Aware%2520Variational%2520Autoencoder-based%2520Anomaly%2520Detection%2520with%2520Two-level%2520Ensembling%26entry.906535625%3DJin%2520Li%2520and%2520Kleanthis%2520Malialis%2520and%2520Christos%2520G.%2520Panayiotou%2520and%2520Marios%2520M.%2520Polycarpou%26entry.1292438233%3DIn%2520today%2527s%2520digital%2520world%252C%2520the%2520generation%2520of%2520vast%2520amounts%2520of%2520streaming%2520data%2520in%2520various%2520domains%2520has%2520become%2520ubiquitous.%2520However%252C%2520many%2520of%2520these%2520data%2520are%2520unlabeled%252C%2520making%2520it%2520challenging%2520to%2520identify%2520events%252C%2520particularly%2520anomalies.%2520This%2520task%2520becomes%2520even%2520more%2520formidable%2520in%2520nonstationary%2520environments%2520where%2520model%2520performance%2520can%2520deteriorate%2520over%2520time%2520due%2520to%2520concept%2520drift.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520presents%2520a%2520novel%2520method%252C%2520VAE%252B%252BESDD%252C%2520which%2520employs%2520incremental%2520learning%2520and%2520two-level%2520ensembling%253A%2520an%2520ensemble%2520of%2520Variational%2520AutoEncoder%2528VAEs%2529%2520for%2520anomaly%2520prediction%252C%2520along%2520with%2520an%2520ensemble%2520of%2520concept%2520drift%2520detectors.%2520Each%2520drift%2520detector%2520utilizes%2520a%2520statistical-based%2520concept%2520drift%2520mechanism.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520VAE%252B%252BESDD%252C%2520we%2520conduct%2520a%2520comprehensive%2520experimental%2520study%2520using%2520real-world%2520and%2520synthetic%2520datasets%2520characterized%2520by%2520severely%2520or%2520extremely%2520low%2520anomalous%2520rates%2520and%2520various%2520drift%2520characteristics.%2520Our%2520study%2520reveals%2520that%2520the%2520proposed%2520method%2520significantly%2520outperforms%2520both%2520strong%2520baselines%2520and%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drift-Aware%20Variational%20Autoencoder-based%20Anomaly%20Detection%20with%20Two-level%20Ensembling&entry.906535625=Jin%20Li%20and%20Kleanthis%20Malialis%20and%20Christos%20G.%20Panayiotou%20and%20Marios%20M.%20Polycarpou&entry.1292438233=In%20today%27s%20digital%20world%2C%20the%20generation%20of%20vast%20amounts%20of%20streaming%20data%20in%20various%20domains%20has%20become%20ubiquitous.%20However%2C%20many%20of%20these%20data%20are%20unlabeled%2C%20making%20it%20challenging%20to%20identify%20events%2C%20particularly%20anomalies.%20This%20task%20becomes%20even%20more%20formidable%20in%20nonstationary%20environments%20where%20model%20performance%20can%20deteriorate%20over%20time%20due%20to%20concept%20drift.%20To%20address%20these%20challenges%2C%20this%20paper%20presents%20a%20novel%20method%2C%20VAE%2B%2BESDD%2C%20which%20employs%20incremental%20learning%20and%20two-level%20ensembling%3A%20an%20ensemble%20of%20Variational%20AutoEncoder%28VAEs%29%20for%20anomaly%20prediction%2C%20along%20with%20an%20ensemble%20of%20concept%20drift%20detectors.%20Each%20drift%20detector%20utilizes%20a%20statistical-based%20concept%20drift%20mechanism.%20To%20evaluate%20the%20effectiveness%20of%20VAE%2B%2BESDD%2C%20we%20conduct%20a%20comprehensive%20experimental%20study%20using%20real-world%20and%20synthetic%20datasets%20characterized%20by%20severely%20or%20extremely%20low%20anomalous%20rates%20and%20various%20drift%20characteristics.%20Our%20study%20reveals%20that%20the%20proposed%20method%20significantly%20outperforms%20both%20strong%20baselines%20and%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2602.12976v1&entry.124074799=Read"},
{"title": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models", "author": "Yang Yang and Hua XU and Zhangyi Hu and Yutao Yue", "abstract": "Large Language Models (LLMs) can propose rules in natural language, sidestepping the need for a predefined predicate space in traditional rule learning. Yet many LLM-based approaches ignore interactions among rules, and the opportunity to couple LLMs with probabilistic rule learning for robust inference remains underexplored. We present RLIE, a unified framework that integrates LLMs with probabilistic modeling to learn a set of weighted rules. RLIE has four stages: (1) Rule generation, where an LLM proposes and filters candidates; (2) Logistic regression, which learns probabilistic weights for global selection and calibration; (3) Iterative refinement, which updates the rule set using prediction errors; and (4) Evaluation, which compares the weighted rule set as a direct classifier with methods that inject rules into an LLM. We evaluate multiple inference strategies on real-world datasets. Applying rules directly with their learned weights yields superior performance, whereas prompting LLMs with the rules, weights, and logistic-model outputs surprisingly degrades accuracy. This supports the view that LLMs excel at semantic generation and interpretation but are less reliable for precise probabilistic integration. RLIE clarifies the potential and limitations of LLMs for inductive reasoning and couples them with classic probabilistic rule combination methods to enable more reliable neuro-symbolic reasoning.", "link": "http://arxiv.org/abs/2510.19698v2", "date": "2026-02-13", "relevancy": 2.0099, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5228}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RLIE%3A%20Rule%20Generation%20with%20Logistic%20Regression%2C%20Iterative%20Refinement%2C%20and%20Evaluation%20for%20Large%20Language%20Models&body=Title%3A%20RLIE%3A%20Rule%20Generation%20with%20Logistic%20Regression%2C%20Iterative%20Refinement%2C%20and%20Evaluation%20for%20Large%20Language%20Models%0AAuthor%3A%20Yang%20Yang%20and%20Hua%20XU%20and%20Zhangyi%20Hu%20and%20Yutao%20Yue%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20can%20propose%20rules%20in%20natural%20language%2C%20sidestepping%20the%20need%20for%20a%20predefined%20predicate%20space%20in%20traditional%20rule%20learning.%20Yet%20many%20LLM-based%20approaches%20ignore%20interactions%20among%20rules%2C%20and%20the%20opportunity%20to%20couple%20LLMs%20with%20probabilistic%20rule%20learning%20for%20robust%20inference%20remains%20underexplored.%20We%20present%20RLIE%2C%20a%20unified%20framework%20that%20integrates%20LLMs%20with%20probabilistic%20modeling%20to%20learn%20a%20set%20of%20weighted%20rules.%20RLIE%20has%20four%20stages%3A%20%281%29%20Rule%20generation%2C%20where%20an%20LLM%20proposes%20and%20filters%20candidates%3B%20%282%29%20Logistic%20regression%2C%20which%20learns%20probabilistic%20weights%20for%20global%20selection%20and%20calibration%3B%20%283%29%20Iterative%20refinement%2C%20which%20updates%20the%20rule%20set%20using%20prediction%20errors%3B%20and%20%284%29%20Evaluation%2C%20which%20compares%20the%20weighted%20rule%20set%20as%20a%20direct%20classifier%20with%20methods%20that%20inject%20rules%20into%20an%20LLM.%20We%20evaluate%20multiple%20inference%20strategies%20on%20real-world%20datasets.%20Applying%20rules%20directly%20with%20their%20learned%20weights%20yields%20superior%20performance%2C%20whereas%20prompting%20LLMs%20with%20the%20rules%2C%20weights%2C%20and%20logistic-model%20outputs%20surprisingly%20degrades%20accuracy.%20This%20supports%20the%20view%20that%20LLMs%20excel%20at%20semantic%20generation%20and%20interpretation%20but%20are%20less%20reliable%20for%20precise%20probabilistic%20integration.%20RLIE%20clarifies%20the%20potential%20and%20limitations%20of%20LLMs%20for%20inductive%20reasoning%20and%20couples%20them%20with%20classic%20probabilistic%20rule%20combination%20methods%20to%20enable%20more%20reliable%20neuro-symbolic%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2510.19698v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRLIE%253A%2520Rule%2520Generation%2520with%2520Logistic%2520Regression%252C%2520Iterative%2520Refinement%252C%2520and%2520Evaluation%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DYang%2520Yang%2520and%2520Hua%2520XU%2520and%2520Zhangyi%2520Hu%2520and%2520Yutao%2520Yue%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520propose%2520rules%2520in%2520natural%2520language%252C%2520sidestepping%2520the%2520need%2520for%2520a%2520predefined%2520predicate%2520space%2520in%2520traditional%2520rule%2520learning.%2520Yet%2520many%2520LLM-based%2520approaches%2520ignore%2520interactions%2520among%2520rules%252C%2520and%2520the%2520opportunity%2520to%2520couple%2520LLMs%2520with%2520probabilistic%2520rule%2520learning%2520for%2520robust%2520inference%2520remains%2520underexplored.%2520We%2520present%2520RLIE%252C%2520a%2520unified%2520framework%2520that%2520integrates%2520LLMs%2520with%2520probabilistic%2520modeling%2520to%2520learn%2520a%2520set%2520of%2520weighted%2520rules.%2520RLIE%2520has%2520four%2520stages%253A%2520%25281%2529%2520Rule%2520generation%252C%2520where%2520an%2520LLM%2520proposes%2520and%2520filters%2520candidates%253B%2520%25282%2529%2520Logistic%2520regression%252C%2520which%2520learns%2520probabilistic%2520weights%2520for%2520global%2520selection%2520and%2520calibration%253B%2520%25283%2529%2520Iterative%2520refinement%252C%2520which%2520updates%2520the%2520rule%2520set%2520using%2520prediction%2520errors%253B%2520and%2520%25284%2529%2520Evaluation%252C%2520which%2520compares%2520the%2520weighted%2520rule%2520set%2520as%2520a%2520direct%2520classifier%2520with%2520methods%2520that%2520inject%2520rules%2520into%2520an%2520LLM.%2520We%2520evaluate%2520multiple%2520inference%2520strategies%2520on%2520real-world%2520datasets.%2520Applying%2520rules%2520directly%2520with%2520their%2520learned%2520weights%2520yields%2520superior%2520performance%252C%2520whereas%2520prompting%2520LLMs%2520with%2520the%2520rules%252C%2520weights%252C%2520and%2520logistic-model%2520outputs%2520surprisingly%2520degrades%2520accuracy.%2520This%2520supports%2520the%2520view%2520that%2520LLMs%2520excel%2520at%2520semantic%2520generation%2520and%2520interpretation%2520but%2520are%2520less%2520reliable%2520for%2520precise%2520probabilistic%2520integration.%2520RLIE%2520clarifies%2520the%2520potential%2520and%2520limitations%2520of%2520LLMs%2520for%2520inductive%2520reasoning%2520and%2520couples%2520them%2520with%2520classic%2520probabilistic%2520rule%2520combination%2520methods%2520to%2520enable%2520more%2520reliable%2520neuro-symbolic%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19698v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLIE%3A%20Rule%20Generation%20with%20Logistic%20Regression%2C%20Iterative%20Refinement%2C%20and%20Evaluation%20for%20Large%20Language%20Models&entry.906535625=Yang%20Yang%20and%20Hua%20XU%20and%20Zhangyi%20Hu%20and%20Yutao%20Yue&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20can%20propose%20rules%20in%20natural%20language%2C%20sidestepping%20the%20need%20for%20a%20predefined%20predicate%20space%20in%20traditional%20rule%20learning.%20Yet%20many%20LLM-based%20approaches%20ignore%20interactions%20among%20rules%2C%20and%20the%20opportunity%20to%20couple%20LLMs%20with%20probabilistic%20rule%20learning%20for%20robust%20inference%20remains%20underexplored.%20We%20present%20RLIE%2C%20a%20unified%20framework%20that%20integrates%20LLMs%20with%20probabilistic%20modeling%20to%20learn%20a%20set%20of%20weighted%20rules.%20RLIE%20has%20four%20stages%3A%20%281%29%20Rule%20generation%2C%20where%20an%20LLM%20proposes%20and%20filters%20candidates%3B%20%282%29%20Logistic%20regression%2C%20which%20learns%20probabilistic%20weights%20for%20global%20selection%20and%20calibration%3B%20%283%29%20Iterative%20refinement%2C%20which%20updates%20the%20rule%20set%20using%20prediction%20errors%3B%20and%20%284%29%20Evaluation%2C%20which%20compares%20the%20weighted%20rule%20set%20as%20a%20direct%20classifier%20with%20methods%20that%20inject%20rules%20into%20an%20LLM.%20We%20evaluate%20multiple%20inference%20strategies%20on%20real-world%20datasets.%20Applying%20rules%20directly%20with%20their%20learned%20weights%20yields%20superior%20performance%2C%20whereas%20prompting%20LLMs%20with%20the%20rules%2C%20weights%2C%20and%20logistic-model%20outputs%20surprisingly%20degrades%20accuracy.%20This%20supports%20the%20view%20that%20LLMs%20excel%20at%20semantic%20generation%20and%20interpretation%20but%20are%20less%20reliable%20for%20precise%20probabilistic%20integration.%20RLIE%20clarifies%20the%20potential%20and%20limitations%20of%20LLMs%20for%20inductive%20reasoning%20and%20couples%20them%20with%20classic%20probabilistic%20rule%20combination%20methods%20to%20enable%20more%20reliable%20neuro-symbolic%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2510.19698v2&entry.124074799=Read"},
{"title": "Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models", "author": "Hao Chen and Ye He and Yuchun Fan and Yukun Yan and Zhenghao Liu and Qingfu Zhu and Maosong Sun and Wanxiang Che", "abstract": "Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that lead to overconfident errors or uncertain truths. To bridge this gap, we propose a novel meta-cognitive framework for reliable knowledge augmentation via differentiated intervention and alignment. Our approach leverages internal cognitive signals to partition the knowledge space into mastered, confused, and missing regions, guiding targeted knowledge expansion. Furthermore, we introduce a cognitive consistency mechanism to synchronize subjective certainty with objective accuracy, ensuring calibrated knowledge boundaries. Extensive experiments demonstrate the our framework consistently outperforms strong baselines, validating its rationality in not only enhancing knowledge capabilities but also fostering cognitive behaviors that better distinguish knowns from unknowns.", "link": "http://arxiv.org/abs/2602.12996v1", "date": "2026-02-13", "relevancy": 1.9963, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Know%20More%2C%20Know%20Clearer%3A%20A%20Meta-Cognitive%20Framework%20for%20Knowledge%20Augmentation%20in%20Large%20Language%20Models&body=Title%3A%20Know%20More%2C%20Know%20Clearer%3A%20A%20Meta-Cognitive%20Framework%20for%20Knowledge%20Augmentation%20in%20Large%20Language%20Models%0AAuthor%3A%20Hao%20Chen%20and%20Ye%20He%20and%20Yuchun%20Fan%20and%20Yukun%20Yan%20and%20Zhenghao%20Liu%20and%20Qingfu%20Zhu%20and%20Maosong%20Sun%20and%20Wanxiang%20Che%0AAbstract%3A%20Knowledge%20augmentation%20has%20significantly%20enhanced%20the%20performance%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20knowledge-intensive%20tasks.%20However%2C%20existing%20methods%20typically%20operate%20on%20the%20simplistic%20premise%20that%20model%20performance%20equates%20with%20internal%20knowledge%2C%20overlooking%20the%20knowledge-confidence%20gaps%20that%20lead%20to%20overconfident%20errors%20or%20uncertain%20truths.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20novel%20meta-cognitive%20framework%20for%20reliable%20knowledge%20augmentation%20via%20differentiated%20intervention%20and%20alignment.%20Our%20approach%20leverages%20internal%20cognitive%20signals%20to%20partition%20the%20knowledge%20space%20into%20mastered%2C%20confused%2C%20and%20missing%20regions%2C%20guiding%20targeted%20knowledge%20expansion.%20Furthermore%2C%20we%20introduce%20a%20cognitive%20consistency%20mechanism%20to%20synchronize%20subjective%20certainty%20with%20objective%20accuracy%2C%20ensuring%20calibrated%20knowledge%20boundaries.%20Extensive%20experiments%20demonstrate%20the%20our%20framework%20consistently%20outperforms%20strong%20baselines%2C%20validating%20its%20rationality%20in%20not%20only%20enhancing%20knowledge%20capabilities%20but%20also%20fostering%20cognitive%20behaviors%20that%20better%20distinguish%20knowns%20from%20unknowns.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnow%2520More%252C%2520Know%2520Clearer%253A%2520A%2520Meta-Cognitive%2520Framework%2520for%2520Knowledge%2520Augmentation%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DHao%2520Chen%2520and%2520Ye%2520He%2520and%2520Yuchun%2520Fan%2520and%2520Yukun%2520Yan%2520and%2520Zhenghao%2520Liu%2520and%2520Qingfu%2520Zhu%2520and%2520Maosong%2520Sun%2520and%2520Wanxiang%2520Che%26entry.1292438233%3DKnowledge%2520augmentation%2520has%2520significantly%2520enhanced%2520the%2520performance%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520knowledge-intensive%2520tasks.%2520However%252C%2520existing%2520methods%2520typically%2520operate%2520on%2520the%2520simplistic%2520premise%2520that%2520model%2520performance%2520equates%2520with%2520internal%2520knowledge%252C%2520overlooking%2520the%2520knowledge-confidence%2520gaps%2520that%2520lead%2520to%2520overconfident%2520errors%2520or%2520uncertain%2520truths.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520novel%2520meta-cognitive%2520framework%2520for%2520reliable%2520knowledge%2520augmentation%2520via%2520differentiated%2520intervention%2520and%2520alignment.%2520Our%2520approach%2520leverages%2520internal%2520cognitive%2520signals%2520to%2520partition%2520the%2520knowledge%2520space%2520into%2520mastered%252C%2520confused%252C%2520and%2520missing%2520regions%252C%2520guiding%2520targeted%2520knowledge%2520expansion.%2520Furthermore%252C%2520we%2520introduce%2520a%2520cognitive%2520consistency%2520mechanism%2520to%2520synchronize%2520subjective%2520certainty%2520with%2520objective%2520accuracy%252C%2520ensuring%2520calibrated%2520knowledge%2520boundaries.%2520Extensive%2520experiments%2520demonstrate%2520the%2520our%2520framework%2520consistently%2520outperforms%2520strong%2520baselines%252C%2520validating%2520its%2520rationality%2520in%2520not%2520only%2520enhancing%2520knowledge%2520capabilities%2520but%2520also%2520fostering%2520cognitive%2520behaviors%2520that%2520better%2520distinguish%2520knowns%2520from%2520unknowns.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Know%20More%2C%20Know%20Clearer%3A%20A%20Meta-Cognitive%20Framework%20for%20Knowledge%20Augmentation%20in%20Large%20Language%20Models&entry.906535625=Hao%20Chen%20and%20Ye%20He%20and%20Yuchun%20Fan%20and%20Yukun%20Yan%20and%20Zhenghao%20Liu%20and%20Qingfu%20Zhu%20and%20Maosong%20Sun%20and%20Wanxiang%20Che&entry.1292438233=Knowledge%20augmentation%20has%20significantly%20enhanced%20the%20performance%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20knowledge-intensive%20tasks.%20However%2C%20existing%20methods%20typically%20operate%20on%20the%20simplistic%20premise%20that%20model%20performance%20equates%20with%20internal%20knowledge%2C%20overlooking%20the%20knowledge-confidence%20gaps%20that%20lead%20to%20overconfident%20errors%20or%20uncertain%20truths.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20novel%20meta-cognitive%20framework%20for%20reliable%20knowledge%20augmentation%20via%20differentiated%20intervention%20and%20alignment.%20Our%20approach%20leverages%20internal%20cognitive%20signals%20to%20partition%20the%20knowledge%20space%20into%20mastered%2C%20confused%2C%20and%20missing%20regions%2C%20guiding%20targeted%20knowledge%20expansion.%20Furthermore%2C%20we%20introduce%20a%20cognitive%20consistency%20mechanism%20to%20synchronize%20subjective%20certainty%20with%20objective%20accuracy%2C%20ensuring%20calibrated%20knowledge%20boundaries.%20Extensive%20experiments%20demonstrate%20the%20our%20framework%20consistently%20outperforms%20strong%20baselines%2C%20validating%20its%20rationality%20in%20not%20only%20enhancing%20knowledge%20capabilities%20but%20also%20fostering%20cognitive%20behaviors%20that%20better%20distinguish%20knowns%20from%20unknowns.&entry.1838667208=http%3A//arxiv.org/abs/2602.12996v1&entry.124074799=Read"},
{"title": "Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching", "author": "Chenguang Wang and Zihan Zhou and Lei Bai and Tianshu Yu", "abstract": "Template-free retrosynthesis methods treat the task as black-box sequence generation, limiting learning efficiency, while semi-template approaches rely on rigid reaction libraries that constrain generalization. We address this gap with a key insight: atom ordering in neural representations matters. Building on this insight, we propose a structure-aware template-free framework that encodes the two-stage nature of chemical reactions as a positional inductive bias. By placing reaction center atoms at the sequence head, our method transforms implicit chemical knowledge into explicit positional patterns that the model can readily capture. The proposed RetroDiT backbone, a graph transformer with rotary position embeddings, exploits this ordering to prioritize chemically critical regions. Combined with discrete flow matching, our approach decouples training from sampling and enables generation in 20--50 steps versus 500 for prior diffusion methods. Our method achieves state-of-the-art performance on both USPTO-50k (61.2% top-1) and the large-scale USPTO-Full (51.3% top-1) with predicted reaction centers. With oracle centers, performance reaches 71.1% and 63.4% respectively, surpassing foundation models trained on 10 billion reactions while using orders of magnitude less data. Ablation studies further reveal that structural priors outperform brute-force scaling: a 280K-parameter model with proper ordering matches a 65M-parameter model without it.", "link": "http://arxiv.org/abs/2602.13136v1", "date": "2026-02-13", "relevancy": 1.9949, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5321}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4938}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Order%20Matters%20in%20Retrosynthesis%3A%20Structure-aware%20Generation%20via%20Reaction-Center-Guided%20Discrete%20Flow%20Matching&body=Title%3A%20Order%20Matters%20in%20Retrosynthesis%3A%20Structure-aware%20Generation%20via%20Reaction-Center-Guided%20Discrete%20Flow%20Matching%0AAuthor%3A%20Chenguang%20Wang%20and%20Zihan%20Zhou%20and%20Lei%20Bai%20and%20Tianshu%20Yu%0AAbstract%3A%20Template-free%20retrosynthesis%20methods%20treat%20the%20task%20as%20black-box%20sequence%20generation%2C%20limiting%20learning%20efficiency%2C%20while%20semi-template%20approaches%20rely%20on%20rigid%20reaction%20libraries%20that%20constrain%20generalization.%20We%20address%20this%20gap%20with%20a%20key%20insight%3A%20atom%20ordering%20in%20neural%20representations%20matters.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20structure-aware%20template-free%20framework%20that%20encodes%20the%20two-stage%20nature%20of%20chemical%20reactions%20as%20a%20positional%20inductive%20bias.%20By%20placing%20reaction%20center%20atoms%20at%20the%20sequence%20head%2C%20our%20method%20transforms%20implicit%20chemical%20knowledge%20into%20explicit%20positional%20patterns%20that%20the%20model%20can%20readily%20capture.%20The%20proposed%20RetroDiT%20backbone%2C%20a%20graph%20transformer%20with%20rotary%20position%20embeddings%2C%20exploits%20this%20ordering%20to%20prioritize%20chemically%20critical%20regions.%20Combined%20with%20discrete%20flow%20matching%2C%20our%20approach%20decouples%20training%20from%20sampling%20and%20enables%20generation%20in%2020--50%20steps%20versus%20500%20for%20prior%20diffusion%20methods.%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20both%20USPTO-50k%20%2861.2%25%20top-1%29%20and%20the%20large-scale%20USPTO-Full%20%2851.3%25%20top-1%29%20with%20predicted%20reaction%20centers.%20With%20oracle%20centers%2C%20performance%20reaches%2071.1%25%20and%2063.4%25%20respectively%2C%20surpassing%20foundation%20models%20trained%20on%2010%20billion%20reactions%20while%20using%20orders%20of%20magnitude%20less%20data.%20Ablation%20studies%20further%20reveal%20that%20structural%20priors%20outperform%20brute-force%20scaling%3A%20a%20280K-parameter%20model%20with%20proper%20ordering%20matches%20a%2065M-parameter%20model%20without%20it.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrder%2520Matters%2520in%2520Retrosynthesis%253A%2520Structure-aware%2520Generation%2520via%2520Reaction-Center-Guided%2520Discrete%2520Flow%2520Matching%26entry.906535625%3DChenguang%2520Wang%2520and%2520Zihan%2520Zhou%2520and%2520Lei%2520Bai%2520and%2520Tianshu%2520Yu%26entry.1292438233%3DTemplate-free%2520retrosynthesis%2520methods%2520treat%2520the%2520task%2520as%2520black-box%2520sequence%2520generation%252C%2520limiting%2520learning%2520efficiency%252C%2520while%2520semi-template%2520approaches%2520rely%2520on%2520rigid%2520reaction%2520libraries%2520that%2520constrain%2520generalization.%2520We%2520address%2520this%2520gap%2520with%2520a%2520key%2520insight%253A%2520atom%2520ordering%2520in%2520neural%2520representations%2520matters.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520a%2520structure-aware%2520template-free%2520framework%2520that%2520encodes%2520the%2520two-stage%2520nature%2520of%2520chemical%2520reactions%2520as%2520a%2520positional%2520inductive%2520bias.%2520By%2520placing%2520reaction%2520center%2520atoms%2520at%2520the%2520sequence%2520head%252C%2520our%2520method%2520transforms%2520implicit%2520chemical%2520knowledge%2520into%2520explicit%2520positional%2520patterns%2520that%2520the%2520model%2520can%2520readily%2520capture.%2520The%2520proposed%2520RetroDiT%2520backbone%252C%2520a%2520graph%2520transformer%2520with%2520rotary%2520position%2520embeddings%252C%2520exploits%2520this%2520ordering%2520to%2520prioritize%2520chemically%2520critical%2520regions.%2520Combined%2520with%2520discrete%2520flow%2520matching%252C%2520our%2520approach%2520decouples%2520training%2520from%2520sampling%2520and%2520enables%2520generation%2520in%252020--50%2520steps%2520versus%2520500%2520for%2520prior%2520diffusion%2520methods.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520both%2520USPTO-50k%2520%252861.2%2525%2520top-1%2529%2520and%2520the%2520large-scale%2520USPTO-Full%2520%252851.3%2525%2520top-1%2529%2520with%2520predicted%2520reaction%2520centers.%2520With%2520oracle%2520centers%252C%2520performance%2520reaches%252071.1%2525%2520and%252063.4%2525%2520respectively%252C%2520surpassing%2520foundation%2520models%2520trained%2520on%252010%2520billion%2520reactions%2520while%2520using%2520orders%2520of%2520magnitude%2520less%2520data.%2520Ablation%2520studies%2520further%2520reveal%2520that%2520structural%2520priors%2520outperform%2520brute-force%2520scaling%253A%2520a%2520280K-parameter%2520model%2520with%2520proper%2520ordering%2520matches%2520a%252065M-parameter%2520model%2520without%2520it.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Order%20Matters%20in%20Retrosynthesis%3A%20Structure-aware%20Generation%20via%20Reaction-Center-Guided%20Discrete%20Flow%20Matching&entry.906535625=Chenguang%20Wang%20and%20Zihan%20Zhou%20and%20Lei%20Bai%20and%20Tianshu%20Yu&entry.1292438233=Template-free%20retrosynthesis%20methods%20treat%20the%20task%20as%20black-box%20sequence%20generation%2C%20limiting%20learning%20efficiency%2C%20while%20semi-template%20approaches%20rely%20on%20rigid%20reaction%20libraries%20that%20constrain%20generalization.%20We%20address%20this%20gap%20with%20a%20key%20insight%3A%20atom%20ordering%20in%20neural%20representations%20matters.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20structure-aware%20template-free%20framework%20that%20encodes%20the%20two-stage%20nature%20of%20chemical%20reactions%20as%20a%20positional%20inductive%20bias.%20By%20placing%20reaction%20center%20atoms%20at%20the%20sequence%20head%2C%20our%20method%20transforms%20implicit%20chemical%20knowledge%20into%20explicit%20positional%20patterns%20that%20the%20model%20can%20readily%20capture.%20The%20proposed%20RetroDiT%20backbone%2C%20a%20graph%20transformer%20with%20rotary%20position%20embeddings%2C%20exploits%20this%20ordering%20to%20prioritize%20chemically%20critical%20regions.%20Combined%20with%20discrete%20flow%20matching%2C%20our%20approach%20decouples%20training%20from%20sampling%20and%20enables%20generation%20in%2020--50%20steps%20versus%20500%20for%20prior%20diffusion%20methods.%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20both%20USPTO-50k%20%2861.2%25%20top-1%29%20and%20the%20large-scale%20USPTO-Full%20%2851.3%25%20top-1%29%20with%20predicted%20reaction%20centers.%20With%20oracle%20centers%2C%20performance%20reaches%2071.1%25%20and%2063.4%25%20respectively%2C%20surpassing%20foundation%20models%20trained%20on%2010%20billion%20reactions%20while%20using%20orders%20of%20magnitude%20less%20data.%20Ablation%20studies%20further%20reveal%20that%20structural%20priors%20outperform%20brute-force%20scaling%3A%20a%20280K-parameter%20model%20with%20proper%20ordering%20matches%20a%2065M-parameter%20model%20without%20it.&entry.1838667208=http%3A//arxiv.org/abs/2602.13136v1&entry.124074799=Read"},
{"title": "Transporting Task Vectors across Different Architectures without Training", "author": "Filippo Rinaldi and Aniello Panariello and Giacomo Salici and Angelo Porrello and Simone Calderara", "abstract": "Adapting large pre-trained models to downstream tasks often produces task-specific parameter updates that are expensive to relearn for every model variant. While recent work has shown that such updates can be transferred between models with identical architectures, transferring them across models of different widths remains largely unexplored. In this work, we introduce Theseus, a training-free method for transporting task-specific updates across heterogeneous models. Rather than matching parameters directly, we characterize a task update by the functional effect it induces on intermediate representations. We formalize task-vector transport as a functional matching problem on observed activations and show that, after aligning representation spaces via orthogonal Procrustes analysis, it admits a stable closed-form solution that preserves the geometry of the update. We evaluate Theseus on vision and language models across different widths, showing consistent improvements over strong baselines without additional training or backpropagation. Our results show that task updates can be meaningfully transferred across architectures when task identity is defined functionally rather than parametrically.", "link": "http://arxiv.org/abs/2602.12952v1", "date": "2026-02-13", "relevancy": 1.9877, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5166}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4976}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transporting%20Task%20Vectors%20across%20Different%20Architectures%20without%20Training&body=Title%3A%20Transporting%20Task%20Vectors%20across%20Different%20Architectures%20without%20Training%0AAuthor%3A%20Filippo%20Rinaldi%20and%20Aniello%20Panariello%20and%20Giacomo%20Salici%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%0AAbstract%3A%20Adapting%20large%20pre-trained%20models%20to%20downstream%20tasks%20often%20produces%20task-specific%20parameter%20updates%20that%20are%20expensive%20to%20relearn%20for%20every%20model%20variant.%20While%20recent%20work%20has%20shown%20that%20such%20updates%20can%20be%20transferred%20between%20models%20with%20identical%20architectures%2C%20transferring%20them%20across%20models%20of%20different%20widths%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20introduce%20Theseus%2C%20a%20training-free%20method%20for%20transporting%20task-specific%20updates%20across%20heterogeneous%20models.%20Rather%20than%20matching%20parameters%20directly%2C%20we%20characterize%20a%20task%20update%20by%20the%20functional%20effect%20it%20induces%20on%20intermediate%20representations.%20We%20formalize%20task-vector%20transport%20as%20a%20functional%20matching%20problem%20on%20observed%20activations%20and%20show%20that%2C%20after%20aligning%20representation%20spaces%20via%20orthogonal%20Procrustes%20analysis%2C%20it%20admits%20a%20stable%20closed-form%20solution%20that%20preserves%20the%20geometry%20of%20the%20update.%20We%20evaluate%20Theseus%20on%20vision%20and%20language%20models%20across%20different%20widths%2C%20showing%20consistent%20improvements%20over%20strong%20baselines%20without%20additional%20training%20or%20backpropagation.%20Our%20results%20show%20that%20task%20updates%20can%20be%20meaningfully%20transferred%20across%20architectures%20when%20task%20identity%20is%20defined%20functionally%20rather%20than%20parametrically.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransporting%2520Task%2520Vectors%2520across%2520Different%2520Architectures%2520without%2520Training%26entry.906535625%3DFilippo%2520Rinaldi%2520and%2520Aniello%2520Panariello%2520and%2520Giacomo%2520Salici%2520and%2520Angelo%2520Porrello%2520and%2520Simone%2520Calderara%26entry.1292438233%3DAdapting%2520large%2520pre-trained%2520models%2520to%2520downstream%2520tasks%2520often%2520produces%2520task-specific%2520parameter%2520updates%2520that%2520are%2520expensive%2520to%2520relearn%2520for%2520every%2520model%2520variant.%2520While%2520recent%2520work%2520has%2520shown%2520that%2520such%2520updates%2520can%2520be%2520transferred%2520between%2520models%2520with%2520identical%2520architectures%252C%2520transferring%2520them%2520across%2520models%2520of%2520different%2520widths%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Theseus%252C%2520a%2520training-free%2520method%2520for%2520transporting%2520task-specific%2520updates%2520across%2520heterogeneous%2520models.%2520Rather%2520than%2520matching%2520parameters%2520directly%252C%2520we%2520characterize%2520a%2520task%2520update%2520by%2520the%2520functional%2520effect%2520it%2520induces%2520on%2520intermediate%2520representations.%2520We%2520formalize%2520task-vector%2520transport%2520as%2520a%2520functional%2520matching%2520problem%2520on%2520observed%2520activations%2520and%2520show%2520that%252C%2520after%2520aligning%2520representation%2520spaces%2520via%2520orthogonal%2520Procrustes%2520analysis%252C%2520it%2520admits%2520a%2520stable%2520closed-form%2520solution%2520that%2520preserves%2520the%2520geometry%2520of%2520the%2520update.%2520We%2520evaluate%2520Theseus%2520on%2520vision%2520and%2520language%2520models%2520across%2520different%2520widths%252C%2520showing%2520consistent%2520improvements%2520over%2520strong%2520baselines%2520without%2520additional%2520training%2520or%2520backpropagation.%2520Our%2520results%2520show%2520that%2520task%2520updates%2520can%2520be%2520meaningfully%2520transferred%2520across%2520architectures%2520when%2520task%2520identity%2520is%2520defined%2520functionally%2520rather%2520than%2520parametrically.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transporting%20Task%20Vectors%20across%20Different%20Architectures%20without%20Training&entry.906535625=Filippo%20Rinaldi%20and%20Aniello%20Panariello%20and%20Giacomo%20Salici%20and%20Angelo%20Porrello%20and%20Simone%20Calderara&entry.1292438233=Adapting%20large%20pre-trained%20models%20to%20downstream%20tasks%20often%20produces%20task-specific%20parameter%20updates%20that%20are%20expensive%20to%20relearn%20for%20every%20model%20variant.%20While%20recent%20work%20has%20shown%20that%20such%20updates%20can%20be%20transferred%20between%20models%20with%20identical%20architectures%2C%20transferring%20them%20across%20models%20of%20different%20widths%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20introduce%20Theseus%2C%20a%20training-free%20method%20for%20transporting%20task-specific%20updates%20across%20heterogeneous%20models.%20Rather%20than%20matching%20parameters%20directly%2C%20we%20characterize%20a%20task%20update%20by%20the%20functional%20effect%20it%20induces%20on%20intermediate%20representations.%20We%20formalize%20task-vector%20transport%20as%20a%20functional%20matching%20problem%20on%20observed%20activations%20and%20show%20that%2C%20after%20aligning%20representation%20spaces%20via%20orthogonal%20Procrustes%20analysis%2C%20it%20admits%20a%20stable%20closed-form%20solution%20that%20preserves%20the%20geometry%20of%20the%20update.%20We%20evaluate%20Theseus%20on%20vision%20and%20language%20models%20across%20different%20widths%2C%20showing%20consistent%20improvements%20over%20strong%20baselines%20without%20additional%20training%20or%20backpropagation.%20Our%20results%20show%20that%20task%20updates%20can%20be%20meaningfully%20transferred%20across%20architectures%20when%20task%20identity%20is%20defined%20functionally%20rather%20than%20parametrically.&entry.1838667208=http%3A//arxiv.org/abs/2602.12952v1&entry.124074799=Read"},
{"title": "DuoCast: Duo-Probabilistic Diffusion for Precipitation Nowcasting", "author": "Penghui Wen and Mengwei He and Patrick Filippi and Na Zhao and Feng Zhang and Thomas Francis Bishop and Zhiyong Wang and Kun Hu", "abstract": "Accurate short-term precipitation forecasting is critical for weather-sensitive decision-making in agriculture, transportation, and disaster response. Existing deep learning approaches often struggle to balance global structural consistency with local detail preservation, especially under complex meteorological conditions. We propose DuoCast, a dual-diffusion framework that decomposes precipitation forecasting into low- and high-frequency components modeled in orthogonal latent subspaces. We theoretically prove that this frequency decomposition reduces prediction error compared to conventional single branch U-Net diffusion models. In DuoCast, the low-frequency model captures large-scale trends via convolutional encoders conditioned on weather front dynamics, while the high-frequency model refines fine-scale variability using a self-attention-based architecture. Experiments on four benchmark radar datasets show that DuoCast consistently outperforms state-of-the-art baselines, achieving superior accuracy in both spatial detail and temporal evolution.", "link": "http://arxiv.org/abs/2412.01091v4", "date": "2026-02-13", "relevancy": 1.987, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5587}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.49}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DuoCast%3A%20Duo-Probabilistic%20Diffusion%20for%20Precipitation%20Nowcasting&body=Title%3A%20DuoCast%3A%20Duo-Probabilistic%20Diffusion%20for%20Precipitation%20Nowcasting%0AAuthor%3A%20Penghui%20Wen%20and%20Mengwei%20He%20and%20Patrick%20Filippi%20and%20Na%20Zhao%20and%20Feng%20Zhang%20and%20Thomas%20Francis%20Bishop%20and%20Zhiyong%20Wang%20and%20Kun%20Hu%0AAbstract%3A%20Accurate%20short-term%20precipitation%20forecasting%20is%20critical%20for%20weather-sensitive%20decision-making%20in%20agriculture%2C%20transportation%2C%20and%20disaster%20response.%20Existing%20deep%20learning%20approaches%20often%20struggle%20to%20balance%20global%20structural%20consistency%20with%20local%20detail%20preservation%2C%20especially%20under%20complex%20meteorological%20conditions.%20We%20propose%20DuoCast%2C%20a%20dual-diffusion%20framework%20that%20decomposes%20precipitation%20forecasting%20into%20low-%20and%20high-frequency%20components%20modeled%20in%20orthogonal%20latent%20subspaces.%20We%20theoretically%20prove%20that%20this%20frequency%20decomposition%20reduces%20prediction%20error%20compared%20to%20conventional%20single%20branch%20U-Net%20diffusion%20models.%20In%20DuoCast%2C%20the%20low-frequency%20model%20captures%20large-scale%20trends%20via%20convolutional%20encoders%20conditioned%20on%20weather%20front%20dynamics%2C%20while%20the%20high-frequency%20model%20refines%20fine-scale%20variability%20using%20a%20self-attention-based%20architecture.%20Experiments%20on%20four%20benchmark%20radar%20datasets%20show%20that%20DuoCast%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20achieving%20superior%20accuracy%20in%20both%20spatial%20detail%20and%20temporal%20evolution.%0ALink%3A%20http%3A//arxiv.org/abs/2412.01091v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDuoCast%253A%2520Duo-Probabilistic%2520Diffusion%2520for%2520Precipitation%2520Nowcasting%26entry.906535625%3DPenghui%2520Wen%2520and%2520Mengwei%2520He%2520and%2520Patrick%2520Filippi%2520and%2520Na%2520Zhao%2520and%2520Feng%2520Zhang%2520and%2520Thomas%2520Francis%2520Bishop%2520and%2520Zhiyong%2520Wang%2520and%2520Kun%2520Hu%26entry.1292438233%3DAccurate%2520short-term%2520precipitation%2520forecasting%2520is%2520critical%2520for%2520weather-sensitive%2520decision-making%2520in%2520agriculture%252C%2520transportation%252C%2520and%2520disaster%2520response.%2520Existing%2520deep%2520learning%2520approaches%2520often%2520struggle%2520to%2520balance%2520global%2520structural%2520consistency%2520with%2520local%2520detail%2520preservation%252C%2520especially%2520under%2520complex%2520meteorological%2520conditions.%2520We%2520propose%2520DuoCast%252C%2520a%2520dual-diffusion%2520framework%2520that%2520decomposes%2520precipitation%2520forecasting%2520into%2520low-%2520and%2520high-frequency%2520components%2520modeled%2520in%2520orthogonal%2520latent%2520subspaces.%2520We%2520theoretically%2520prove%2520that%2520this%2520frequency%2520decomposition%2520reduces%2520prediction%2520error%2520compared%2520to%2520conventional%2520single%2520branch%2520U-Net%2520diffusion%2520models.%2520In%2520DuoCast%252C%2520the%2520low-frequency%2520model%2520captures%2520large-scale%2520trends%2520via%2520convolutional%2520encoders%2520conditioned%2520on%2520weather%2520front%2520dynamics%252C%2520while%2520the%2520high-frequency%2520model%2520refines%2520fine-scale%2520variability%2520using%2520a%2520self-attention-based%2520architecture.%2520Experiments%2520on%2520four%2520benchmark%2520radar%2520datasets%2520show%2520that%2520DuoCast%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%252C%2520achieving%2520superior%2520accuracy%2520in%2520both%2520spatial%2520detail%2520and%2520temporal%2520evolution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01091v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DuoCast%3A%20Duo-Probabilistic%20Diffusion%20for%20Precipitation%20Nowcasting&entry.906535625=Penghui%20Wen%20and%20Mengwei%20He%20and%20Patrick%20Filippi%20and%20Na%20Zhao%20and%20Feng%20Zhang%20and%20Thomas%20Francis%20Bishop%20and%20Zhiyong%20Wang%20and%20Kun%20Hu&entry.1292438233=Accurate%20short-term%20precipitation%20forecasting%20is%20critical%20for%20weather-sensitive%20decision-making%20in%20agriculture%2C%20transportation%2C%20and%20disaster%20response.%20Existing%20deep%20learning%20approaches%20often%20struggle%20to%20balance%20global%20structural%20consistency%20with%20local%20detail%20preservation%2C%20especially%20under%20complex%20meteorological%20conditions.%20We%20propose%20DuoCast%2C%20a%20dual-diffusion%20framework%20that%20decomposes%20precipitation%20forecasting%20into%20low-%20and%20high-frequency%20components%20modeled%20in%20orthogonal%20latent%20subspaces.%20We%20theoretically%20prove%20that%20this%20frequency%20decomposition%20reduces%20prediction%20error%20compared%20to%20conventional%20single%20branch%20U-Net%20diffusion%20models.%20In%20DuoCast%2C%20the%20low-frequency%20model%20captures%20large-scale%20trends%20via%20convolutional%20encoders%20conditioned%20on%20weather%20front%20dynamics%2C%20while%20the%20high-frequency%20model%20refines%20fine-scale%20variability%20using%20a%20self-attention-based%20architecture.%20Experiments%20on%20four%20benchmark%20radar%20datasets%20show%20that%20DuoCast%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20achieving%20superior%20accuracy%20in%20both%20spatial%20detail%20and%20temporal%20evolution.&entry.1838667208=http%3A//arxiv.org/abs/2412.01091v4&entry.124074799=Read"},
{"title": "TCRL: Temporal-Coupled Adversarial Training for Robust Constrained Reinforcement Learning in Worst-Case Scenarios", "author": "Wentao Xu and Zhongming Yao and Weihao Li and Zhenghang Song and Yumeng Song and Tianyi Li and Yushuai Li", "abstract": "Constrained Reinforcement Learning (CRL) aims to optimize decision-making policies under constraint conditions, making it highly applicable to safety-critical domains such as autonomous driving, robotics, and power grid management. However, existing robust CRL approaches predominantly focus on single-step perturbations and temporally independent adversarial models, lacking explicit modeling of robustness against temporally coupled perturbations. To tackle these challenges, we propose TCRL, a novel temporal-coupled adversarial training framework for robust constrained reinforcement learning (TCRL) in worst-case scenarios. First, TCRL introduces a worst-case-perceived cost constraint function that estimates safety costs under temporally coupled perturbations without the need to explicitly model adversarial attackers. Second, TCRL establishes a dual-constraint defense mechanism on the reward to counter temporally coupled adversaries while maintaining reward unpredictability. Experimental results demonstrate that TCRL consistently outperforms existing methods in terms of robustness against temporally coupled perturbation attacks across a variety of CRL tasks.", "link": "http://arxiv.org/abs/2602.13040v1", "date": "2026-02-13", "relevancy": 1.9858, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5094}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4875}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TCRL%3A%20Temporal-Coupled%20Adversarial%20Training%20for%20Robust%20Constrained%20Reinforcement%20Learning%20in%20Worst-Case%20Scenarios&body=Title%3A%20TCRL%3A%20Temporal-Coupled%20Adversarial%20Training%20for%20Robust%20Constrained%20Reinforcement%20Learning%20in%20Worst-Case%20Scenarios%0AAuthor%3A%20Wentao%20Xu%20and%20Zhongming%20Yao%20and%20Weihao%20Li%20and%20Zhenghang%20Song%20and%20Yumeng%20Song%20and%20Tianyi%20Li%20and%20Yushuai%20Li%0AAbstract%3A%20Constrained%20Reinforcement%20Learning%20%28CRL%29%20aims%20to%20optimize%20decision-making%20policies%20under%20constraint%20conditions%2C%20making%20it%20highly%20applicable%20to%20safety-critical%20domains%20such%20as%20autonomous%20driving%2C%20robotics%2C%20and%20power%20grid%20management.%20However%2C%20existing%20robust%20CRL%20approaches%20predominantly%20focus%20on%20single-step%20perturbations%20and%20temporally%20independent%20adversarial%20models%2C%20lacking%20explicit%20modeling%20of%20robustness%20against%20temporally%20coupled%20perturbations.%20To%20tackle%20these%20challenges%2C%20we%20propose%20TCRL%2C%20a%20novel%20temporal-coupled%20adversarial%20training%20framework%20for%20robust%20constrained%20reinforcement%20learning%20%28TCRL%29%20in%20worst-case%20scenarios.%20First%2C%20TCRL%20introduces%20a%20worst-case-perceived%20cost%20constraint%20function%20that%20estimates%20safety%20costs%20under%20temporally%20coupled%20perturbations%20without%20the%20need%20to%20explicitly%20model%20adversarial%20attackers.%20Second%2C%20TCRL%20establishes%20a%20dual-constraint%20defense%20mechanism%20on%20the%20reward%20to%20counter%20temporally%20coupled%20adversaries%20while%20maintaining%20reward%20unpredictability.%20Experimental%20results%20demonstrate%20that%20TCRL%20consistently%20outperforms%20existing%20methods%20in%20terms%20of%20robustness%20against%20temporally%20coupled%20perturbation%20attacks%20across%20a%20variety%20of%20CRL%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTCRL%253A%2520Temporal-Coupled%2520Adversarial%2520Training%2520for%2520Robust%2520Constrained%2520Reinforcement%2520Learning%2520in%2520Worst-Case%2520Scenarios%26entry.906535625%3DWentao%2520Xu%2520and%2520Zhongming%2520Yao%2520and%2520Weihao%2520Li%2520and%2520Zhenghang%2520Song%2520and%2520Yumeng%2520Song%2520and%2520Tianyi%2520Li%2520and%2520Yushuai%2520Li%26entry.1292438233%3DConstrained%2520Reinforcement%2520Learning%2520%2528CRL%2529%2520aims%2520to%2520optimize%2520decision-making%2520policies%2520under%2520constraint%2520conditions%252C%2520making%2520it%2520highly%2520applicable%2520to%2520safety-critical%2520domains%2520such%2520as%2520autonomous%2520driving%252C%2520robotics%252C%2520and%2520power%2520grid%2520management.%2520However%252C%2520existing%2520robust%2520CRL%2520approaches%2520predominantly%2520focus%2520on%2520single-step%2520perturbations%2520and%2520temporally%2520independent%2520adversarial%2520models%252C%2520lacking%2520explicit%2520modeling%2520of%2520robustness%2520against%2520temporally%2520coupled%2520perturbations.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520TCRL%252C%2520a%2520novel%2520temporal-coupled%2520adversarial%2520training%2520framework%2520for%2520robust%2520constrained%2520reinforcement%2520learning%2520%2528TCRL%2529%2520in%2520worst-case%2520scenarios.%2520First%252C%2520TCRL%2520introduces%2520a%2520worst-case-perceived%2520cost%2520constraint%2520function%2520that%2520estimates%2520safety%2520costs%2520under%2520temporally%2520coupled%2520perturbations%2520without%2520the%2520need%2520to%2520explicitly%2520model%2520adversarial%2520attackers.%2520Second%252C%2520TCRL%2520establishes%2520a%2520dual-constraint%2520defense%2520mechanism%2520on%2520the%2520reward%2520to%2520counter%2520temporally%2520coupled%2520adversaries%2520while%2520maintaining%2520reward%2520unpredictability.%2520Experimental%2520results%2520demonstrate%2520that%2520TCRL%2520consistently%2520outperforms%2520existing%2520methods%2520in%2520terms%2520of%2520robustness%2520against%2520temporally%2520coupled%2520perturbation%2520attacks%2520across%2520a%2520variety%2520of%2520CRL%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TCRL%3A%20Temporal-Coupled%20Adversarial%20Training%20for%20Robust%20Constrained%20Reinforcement%20Learning%20in%20Worst-Case%20Scenarios&entry.906535625=Wentao%20Xu%20and%20Zhongming%20Yao%20and%20Weihao%20Li%20and%20Zhenghang%20Song%20and%20Yumeng%20Song%20and%20Tianyi%20Li%20and%20Yushuai%20Li&entry.1292438233=Constrained%20Reinforcement%20Learning%20%28CRL%29%20aims%20to%20optimize%20decision-making%20policies%20under%20constraint%20conditions%2C%20making%20it%20highly%20applicable%20to%20safety-critical%20domains%20such%20as%20autonomous%20driving%2C%20robotics%2C%20and%20power%20grid%20management.%20However%2C%20existing%20robust%20CRL%20approaches%20predominantly%20focus%20on%20single-step%20perturbations%20and%20temporally%20independent%20adversarial%20models%2C%20lacking%20explicit%20modeling%20of%20robustness%20against%20temporally%20coupled%20perturbations.%20To%20tackle%20these%20challenges%2C%20we%20propose%20TCRL%2C%20a%20novel%20temporal-coupled%20adversarial%20training%20framework%20for%20robust%20constrained%20reinforcement%20learning%20%28TCRL%29%20in%20worst-case%20scenarios.%20First%2C%20TCRL%20introduces%20a%20worst-case-perceived%20cost%20constraint%20function%20that%20estimates%20safety%20costs%20under%20temporally%20coupled%20perturbations%20without%20the%20need%20to%20explicitly%20model%20adversarial%20attackers.%20Second%2C%20TCRL%20establishes%20a%20dual-constraint%20defense%20mechanism%20on%20the%20reward%20to%20counter%20temporally%20coupled%20adversaries%20while%20maintaining%20reward%20unpredictability.%20Experimental%20results%20demonstrate%20that%20TCRL%20consistently%20outperforms%20existing%20methods%20in%20terms%20of%20robustness%20against%20temporally%20coupled%20perturbation%20attacks%20across%20a%20variety%20of%20CRL%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.13040v1&entry.124074799=Read"},
{"title": "A DMD-Based Adaptive Modulation Method for High Dynamic Range Imaging in High-Glare Environments", "author": "Banglei Guan and Jing Tao and Liang Xu and Dongcai Tan and Pengju Sun and Jianbing Liu and Yang Shang and Qifeng Yu", "abstract": "Background The accuracy of photomechanics measurements critically relies on image quality,particularly under extreme illumination conditions such as welding arc monitoring and polished metallic surface analysis. High dynamic range (HDR) imaging above 120 dB is essential in these contexts. Conventional CCD/CMOS sensors, with dynamic ranges typically below 70 dB, are highly susceptible to saturation under glare, resulting in irreversible loss of detail and significant errors in digital image correlation (DIC). Methods This paper presents an HDR imaging system that leverages the spatial modulation capability of a digital micromirror device (DMD). The system architecture enables autonomous regional segmentation and adaptive exposure control for high-dynamic-range scenes through an integrated framework comprising two synergistic subsystems: a DMD-based optical modulation unit and an adaptive computational imaging pipeline. Results The system achieves a measurable dynamic range of 127 dB, effectively eliminating satu ration artifacts under high glare. Experimental results demonstrate a 78% reduction in strain error and improved DIC positioning accuracy, confirming reliable performance across extreme intensity variations. Conclusion The DMD-based system provides high fidelity adaptive HDR imaging, overcoming key limitations of conventional sensors. It exhibits strong potential for optical metrology and stress analysis in high-glare environments where traditional methods are inadequate.", "link": "http://arxiv.org/abs/2602.12044v2", "date": "2026-02-13", "relevancy": 1.9857, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5063}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4971}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20DMD-Based%20Adaptive%20Modulation%20Method%20for%20High%20Dynamic%20Range%20Imaging%20in%20High-Glare%20Environments&body=Title%3A%20A%20DMD-Based%20Adaptive%20Modulation%20Method%20for%20High%20Dynamic%20Range%20Imaging%20in%20High-Glare%20Environments%0AAuthor%3A%20Banglei%20Guan%20and%20Jing%20Tao%20and%20Liang%20Xu%20and%20Dongcai%20Tan%20and%20Pengju%20Sun%20and%20Jianbing%20Liu%20and%20Yang%20Shang%20and%20Qifeng%20Yu%0AAbstract%3A%20Background%20The%20accuracy%20of%20photomechanics%20measurements%20critically%20relies%20on%20image%20quality%2Cparticularly%20under%20extreme%20illumination%20conditions%20such%20as%20welding%20arc%20monitoring%20and%20polished%20metallic%20surface%20analysis.%20High%20dynamic%20range%20%28HDR%29%20imaging%20above%20120%20dB%20is%20essential%20in%20these%20contexts.%20Conventional%20CCD/CMOS%20sensors%2C%20with%20dynamic%20ranges%20typically%20below%2070%20dB%2C%20are%20highly%20susceptible%20to%20saturation%20under%20glare%2C%20resulting%20in%20irreversible%20loss%20of%20detail%20and%20significant%20errors%20in%20digital%20image%20correlation%20%28DIC%29.%20Methods%20This%20paper%20presents%20an%20HDR%20imaging%20system%20that%20leverages%20the%20spatial%20modulation%20capability%20of%20a%20digital%20micromirror%20device%20%28DMD%29.%20The%20system%20architecture%20enables%20autonomous%20regional%20segmentation%20and%20adaptive%20exposure%20control%20for%20high-dynamic-range%20scenes%20through%20an%20integrated%20framework%20comprising%20two%20synergistic%20subsystems%3A%20a%20DMD-based%20optical%20modulation%20unit%20and%20an%20adaptive%20computational%20imaging%20pipeline.%20Results%20The%20system%20achieves%20a%20measurable%20dynamic%20range%20of%20127%20dB%2C%20effectively%20eliminating%20satu%20ration%20artifacts%20under%20high%20glare.%20Experimental%20results%20demonstrate%20a%2078%25%20reduction%20in%20strain%20error%20and%20improved%20DIC%20positioning%20accuracy%2C%20confirming%20reliable%20performance%20across%20extreme%20intensity%20variations.%20Conclusion%20The%20DMD-based%20system%20provides%20high%20fidelity%20adaptive%20HDR%20imaging%2C%20overcoming%20key%20limitations%20of%20conventional%20sensors.%20It%20exhibits%20strong%20potential%20for%20optical%20metrology%20and%20stress%20analysis%20in%20high-glare%20environments%20where%20traditional%20methods%20are%20inadequate.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12044v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520DMD-Based%2520Adaptive%2520Modulation%2520Method%2520for%2520High%2520Dynamic%2520Range%2520Imaging%2520in%2520High-Glare%2520Environments%26entry.906535625%3DBanglei%2520Guan%2520and%2520Jing%2520Tao%2520and%2520Liang%2520Xu%2520and%2520Dongcai%2520Tan%2520and%2520Pengju%2520Sun%2520and%2520Jianbing%2520Liu%2520and%2520Yang%2520Shang%2520and%2520Qifeng%2520Yu%26entry.1292438233%3DBackground%2520The%2520accuracy%2520of%2520photomechanics%2520measurements%2520critically%2520relies%2520on%2520image%2520quality%252Cparticularly%2520under%2520extreme%2520illumination%2520conditions%2520such%2520as%2520welding%2520arc%2520monitoring%2520and%2520polished%2520metallic%2520surface%2520analysis.%2520High%2520dynamic%2520range%2520%2528HDR%2529%2520imaging%2520above%2520120%2520dB%2520is%2520essential%2520in%2520these%2520contexts.%2520Conventional%2520CCD/CMOS%2520sensors%252C%2520with%2520dynamic%2520ranges%2520typically%2520below%252070%2520dB%252C%2520are%2520highly%2520susceptible%2520to%2520saturation%2520under%2520glare%252C%2520resulting%2520in%2520irreversible%2520loss%2520of%2520detail%2520and%2520significant%2520errors%2520in%2520digital%2520image%2520correlation%2520%2528DIC%2529.%2520Methods%2520This%2520paper%2520presents%2520an%2520HDR%2520imaging%2520system%2520that%2520leverages%2520the%2520spatial%2520modulation%2520capability%2520of%2520a%2520digital%2520micromirror%2520device%2520%2528DMD%2529.%2520The%2520system%2520architecture%2520enables%2520autonomous%2520regional%2520segmentation%2520and%2520adaptive%2520exposure%2520control%2520for%2520high-dynamic-range%2520scenes%2520through%2520an%2520integrated%2520framework%2520comprising%2520two%2520synergistic%2520subsystems%253A%2520a%2520DMD-based%2520optical%2520modulation%2520unit%2520and%2520an%2520adaptive%2520computational%2520imaging%2520pipeline.%2520Results%2520The%2520system%2520achieves%2520a%2520measurable%2520dynamic%2520range%2520of%2520127%2520dB%252C%2520effectively%2520eliminating%2520satu%2520ration%2520artifacts%2520under%2520high%2520glare.%2520Experimental%2520results%2520demonstrate%2520a%252078%2525%2520reduction%2520in%2520strain%2520error%2520and%2520improved%2520DIC%2520positioning%2520accuracy%252C%2520confirming%2520reliable%2520performance%2520across%2520extreme%2520intensity%2520variations.%2520Conclusion%2520The%2520DMD-based%2520system%2520provides%2520high%2520fidelity%2520adaptive%2520HDR%2520imaging%252C%2520overcoming%2520key%2520limitations%2520of%2520conventional%2520sensors.%2520It%2520exhibits%2520strong%2520potential%2520for%2520optical%2520metrology%2520and%2520stress%2520analysis%2520in%2520high-glare%2520environments%2520where%2520traditional%2520methods%2520are%2520inadequate.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12044v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20DMD-Based%20Adaptive%20Modulation%20Method%20for%20High%20Dynamic%20Range%20Imaging%20in%20High-Glare%20Environments&entry.906535625=Banglei%20Guan%20and%20Jing%20Tao%20and%20Liang%20Xu%20and%20Dongcai%20Tan%20and%20Pengju%20Sun%20and%20Jianbing%20Liu%20and%20Yang%20Shang%20and%20Qifeng%20Yu&entry.1292438233=Background%20The%20accuracy%20of%20photomechanics%20measurements%20critically%20relies%20on%20image%20quality%2Cparticularly%20under%20extreme%20illumination%20conditions%20such%20as%20welding%20arc%20monitoring%20and%20polished%20metallic%20surface%20analysis.%20High%20dynamic%20range%20%28HDR%29%20imaging%20above%20120%20dB%20is%20essential%20in%20these%20contexts.%20Conventional%20CCD/CMOS%20sensors%2C%20with%20dynamic%20ranges%20typically%20below%2070%20dB%2C%20are%20highly%20susceptible%20to%20saturation%20under%20glare%2C%20resulting%20in%20irreversible%20loss%20of%20detail%20and%20significant%20errors%20in%20digital%20image%20correlation%20%28DIC%29.%20Methods%20This%20paper%20presents%20an%20HDR%20imaging%20system%20that%20leverages%20the%20spatial%20modulation%20capability%20of%20a%20digital%20micromirror%20device%20%28DMD%29.%20The%20system%20architecture%20enables%20autonomous%20regional%20segmentation%20and%20adaptive%20exposure%20control%20for%20high-dynamic-range%20scenes%20through%20an%20integrated%20framework%20comprising%20two%20synergistic%20subsystems%3A%20a%20DMD-based%20optical%20modulation%20unit%20and%20an%20adaptive%20computational%20imaging%20pipeline.%20Results%20The%20system%20achieves%20a%20measurable%20dynamic%20range%20of%20127%20dB%2C%20effectively%20eliminating%20satu%20ration%20artifacts%20under%20high%20glare.%20Experimental%20results%20demonstrate%20a%2078%25%20reduction%20in%20strain%20error%20and%20improved%20DIC%20positioning%20accuracy%2C%20confirming%20reliable%20performance%20across%20extreme%20intensity%20variations.%20Conclusion%20The%20DMD-based%20system%20provides%20high%20fidelity%20adaptive%20HDR%20imaging%2C%20overcoming%20key%20limitations%20of%20conventional%20sensors.%20It%20exhibits%20strong%20potential%20for%20optical%20metrology%20and%20stress%20analysis%20in%20high-glare%20environments%20where%20traditional%20methods%20are%20inadequate.&entry.1838667208=http%3A//arxiv.org/abs/2602.12044v2&entry.124074799=Read"},
{"title": "SKYSURF: A Self-learning Framework for Persistent Surveillance using Cooperative Aerial Gliders", "author": "Houssem Eddine Mohamadi and Nadjia Kara", "abstract": "The success of surveillance applications involving small unmanned aerial vehicles (UAVs) depends on how long the limited on-board power would persist. To cope with this challenge, alternative renewable sources of lift are sought. One promising solution is to extract energy from rising masses of buoyant air. This paper proposes a local-global behavioral management and decision-making approach for the autonomous deployment of soaring-capable UAVs. The cooperative UAVs are modeled as non-deterministic finite state-based rational agents. In addition to a mission planning module for assigning tasks and issuing dynamic navigation waypoints for a new path planning scheme, in which the concepts of visibility and prediction are applied to avoid the collisions. Moreover, a delayed learning and tuning strategy is employed optimize the gains of the path tracking controller. Rigorous comparative analyses carried out with three benchmarking baselines and 15 evolutionary algorithms highlight the adequacy of the proposed approach for maintaining the surveillance persistency (staying aloft for longer periods without landing) and maximizing the detection of targets (two times better than non-cooperative and semi-cooperative approaches) with less power consumption (almost 6% of battery consumed in six hours).", "link": "http://arxiv.org/abs/2602.12838v1", "date": "2026-02-13", "relevancy": 1.9846, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5131}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4854}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SKYSURF%3A%20A%20Self-learning%20Framework%20for%20Persistent%20Surveillance%20using%20Cooperative%20Aerial%20Gliders&body=Title%3A%20SKYSURF%3A%20A%20Self-learning%20Framework%20for%20Persistent%20Surveillance%20using%20Cooperative%20Aerial%20Gliders%0AAuthor%3A%20Houssem%20Eddine%20Mohamadi%20and%20Nadjia%20Kara%0AAbstract%3A%20The%20success%20of%20surveillance%20applications%20involving%20small%20unmanned%20aerial%20vehicles%20%28UAVs%29%20depends%20on%20how%20long%20the%20limited%20on-board%20power%20would%20persist.%20To%20cope%20with%20this%20challenge%2C%20alternative%20renewable%20sources%20of%20lift%20are%20sought.%20One%20promising%20solution%20is%20to%20extract%20energy%20from%20rising%20masses%20of%20buoyant%20air.%20This%20paper%20proposes%20a%20local-global%20behavioral%20management%20and%20decision-making%20approach%20for%20the%20autonomous%20deployment%20of%20soaring-capable%20UAVs.%20The%20cooperative%20UAVs%20are%20modeled%20as%20non-deterministic%20finite%20state-based%20rational%20agents.%20In%20addition%20to%20a%20mission%20planning%20module%20for%20assigning%20tasks%20and%20issuing%20dynamic%20navigation%20waypoints%20for%20a%20new%20path%20planning%20scheme%2C%20in%20which%20the%20concepts%20of%20visibility%20and%20prediction%20are%20applied%20to%20avoid%20the%20collisions.%20Moreover%2C%20a%20delayed%20learning%20and%20tuning%20strategy%20is%20employed%20optimize%20the%20gains%20of%20the%20path%20tracking%20controller.%20Rigorous%20comparative%20analyses%20carried%20out%20with%20three%20benchmarking%20baselines%20and%2015%20evolutionary%20algorithms%20highlight%20the%20adequacy%20of%20the%20proposed%20approach%20for%20maintaining%20the%20surveillance%20persistency%20%28staying%20aloft%20for%20longer%20periods%20without%20landing%29%20and%20maximizing%20the%20detection%20of%20targets%20%28two%20times%20better%20than%20non-cooperative%20and%20semi-cooperative%20approaches%29%20with%20less%20power%20consumption%20%28almost%206%25%20of%20battery%20consumed%20in%20six%20hours%29.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSKYSURF%253A%2520A%2520Self-learning%2520Framework%2520for%2520Persistent%2520Surveillance%2520using%2520Cooperative%2520Aerial%2520Gliders%26entry.906535625%3DHoussem%2520Eddine%2520Mohamadi%2520and%2520Nadjia%2520Kara%26entry.1292438233%3DThe%2520success%2520of%2520surveillance%2520applications%2520involving%2520small%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520depends%2520on%2520how%2520long%2520the%2520limited%2520on-board%2520power%2520would%2520persist.%2520To%2520cope%2520with%2520this%2520challenge%252C%2520alternative%2520renewable%2520sources%2520of%2520lift%2520are%2520sought.%2520One%2520promising%2520solution%2520is%2520to%2520extract%2520energy%2520from%2520rising%2520masses%2520of%2520buoyant%2520air.%2520This%2520paper%2520proposes%2520a%2520local-global%2520behavioral%2520management%2520and%2520decision-making%2520approach%2520for%2520the%2520autonomous%2520deployment%2520of%2520soaring-capable%2520UAVs.%2520The%2520cooperative%2520UAVs%2520are%2520modeled%2520as%2520non-deterministic%2520finite%2520state-based%2520rational%2520agents.%2520In%2520addition%2520to%2520a%2520mission%2520planning%2520module%2520for%2520assigning%2520tasks%2520and%2520issuing%2520dynamic%2520navigation%2520waypoints%2520for%2520a%2520new%2520path%2520planning%2520scheme%252C%2520in%2520which%2520the%2520concepts%2520of%2520visibility%2520and%2520prediction%2520are%2520applied%2520to%2520avoid%2520the%2520collisions.%2520Moreover%252C%2520a%2520delayed%2520learning%2520and%2520tuning%2520strategy%2520is%2520employed%2520optimize%2520the%2520gains%2520of%2520the%2520path%2520tracking%2520controller.%2520Rigorous%2520comparative%2520analyses%2520carried%2520out%2520with%2520three%2520benchmarking%2520baselines%2520and%252015%2520evolutionary%2520algorithms%2520highlight%2520the%2520adequacy%2520of%2520the%2520proposed%2520approach%2520for%2520maintaining%2520the%2520surveillance%2520persistency%2520%2528staying%2520aloft%2520for%2520longer%2520periods%2520without%2520landing%2529%2520and%2520maximizing%2520the%2520detection%2520of%2520targets%2520%2528two%2520times%2520better%2520than%2520non-cooperative%2520and%2520semi-cooperative%2520approaches%2529%2520with%2520less%2520power%2520consumption%2520%2528almost%25206%2525%2520of%2520battery%2520consumed%2520in%2520six%2520hours%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SKYSURF%3A%20A%20Self-learning%20Framework%20for%20Persistent%20Surveillance%20using%20Cooperative%20Aerial%20Gliders&entry.906535625=Houssem%20Eddine%20Mohamadi%20and%20Nadjia%20Kara&entry.1292438233=The%20success%20of%20surveillance%20applications%20involving%20small%20unmanned%20aerial%20vehicles%20%28UAVs%29%20depends%20on%20how%20long%20the%20limited%20on-board%20power%20would%20persist.%20To%20cope%20with%20this%20challenge%2C%20alternative%20renewable%20sources%20of%20lift%20are%20sought.%20One%20promising%20solution%20is%20to%20extract%20energy%20from%20rising%20masses%20of%20buoyant%20air.%20This%20paper%20proposes%20a%20local-global%20behavioral%20management%20and%20decision-making%20approach%20for%20the%20autonomous%20deployment%20of%20soaring-capable%20UAVs.%20The%20cooperative%20UAVs%20are%20modeled%20as%20non-deterministic%20finite%20state-based%20rational%20agents.%20In%20addition%20to%20a%20mission%20planning%20module%20for%20assigning%20tasks%20and%20issuing%20dynamic%20navigation%20waypoints%20for%20a%20new%20path%20planning%20scheme%2C%20in%20which%20the%20concepts%20of%20visibility%20and%20prediction%20are%20applied%20to%20avoid%20the%20collisions.%20Moreover%2C%20a%20delayed%20learning%20and%20tuning%20strategy%20is%20employed%20optimize%20the%20gains%20of%20the%20path%20tracking%20controller.%20Rigorous%20comparative%20analyses%20carried%20out%20with%20three%20benchmarking%20baselines%20and%2015%20evolutionary%20algorithms%20highlight%20the%20adequacy%20of%20the%20proposed%20approach%20for%20maintaining%20the%20surveillance%20persistency%20%28staying%20aloft%20for%20longer%20periods%20without%20landing%29%20and%20maximizing%20the%20detection%20of%20targets%20%28two%20times%20better%20than%20non-cooperative%20and%20semi-cooperative%20approaches%29%20with%20less%20power%20consumption%20%28almost%206%25%20of%20battery%20consumed%20in%20six%20hours%29.&entry.1838667208=http%3A//arxiv.org/abs/2602.12838v1&entry.124074799=Read"},
{"title": "Dual-Phase Cross-Modal Contrastive Learning for CMR-Guided ECG Representations for Cardiovascular Disease Assessment", "author": "Laura Alvarez-Florez and Angel Bujalance-Gomez and Femke Raijmakers and Samuel Ruiperez-Campillo and Maarten Z. H. Kolk and Jesse Wiers and Julia Vogt and Erik J. Bekkers and Ivana I\u0161gum and Fleur V. Y. Tjong", "abstract": "Cardiac magnetic resonance imaging (CMR) offers detailed evaluation of cardiac structure and function, but its limited accessibility restricts use to selected patient populations. In contrast, the electrocardiogram (ECG) is ubiquitous and inexpensive, and provides rich information on cardiac electrical activity and rhythm, yet offers limited insight into underlying cardiac structure and mechanical function. To address this, we introduce a contrastive learning framework that improves the extraction of clinically relevant cardiac phenotypes from ECG by learning from paired ECG-CMR data. Our approach aligns ECG representations with 3D CMR volumes at end-diastole (ED) and end-systole (ES), with a dual-phase contrastive loss to anchor each ECG jointly with both cardiac phases in a shared latent space. Unlike prior methods limited to 2D CMR representations with or without a temporal component, our framework models 3D anatomy at both ED and ES phases as distinct latent representations, enabling flexible disentanglement of structural and functional cardiac properties. Using over 34,000 ECG-CMR pairs from the UK Biobank, we demonstrate improved extraction of image-derived phenotypes from ECG, particularly for functional parameters ($\\uparrow$ 9.2\\%), while improvements in clinical outcome prediction remained modest ($\\uparrow$ 0.7\\%). This strategy could enable scalable and cost-effective extraction of image-derived traits from ECG. The code for this research is publicly available.", "link": "http://arxiv.org/abs/2602.12883v1", "date": "2026-02-13", "relevancy": 1.9753, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5002}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.499}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Phase%20Cross-Modal%20Contrastive%20Learning%20for%20CMR-Guided%20ECG%20Representations%20for%20Cardiovascular%20Disease%20Assessment&body=Title%3A%20Dual-Phase%20Cross-Modal%20Contrastive%20Learning%20for%20CMR-Guided%20ECG%20Representations%20for%20Cardiovascular%20Disease%20Assessment%0AAuthor%3A%20Laura%20Alvarez-Florez%20and%20Angel%20Bujalance-Gomez%20and%20Femke%20Raijmakers%20and%20Samuel%20Ruiperez-Campillo%20and%20Maarten%20Z.%20H.%20Kolk%20and%20Jesse%20Wiers%20and%20Julia%20Vogt%20and%20Erik%20J.%20Bekkers%20and%20Ivana%20I%C5%A1gum%20and%20Fleur%20V.%20Y.%20Tjong%0AAbstract%3A%20Cardiac%20magnetic%20resonance%20imaging%20%28CMR%29%20offers%20detailed%20evaluation%20of%20cardiac%20structure%20and%20function%2C%20but%20its%20limited%20accessibility%20restricts%20use%20to%20selected%20patient%20populations.%20In%20contrast%2C%20the%20electrocardiogram%20%28ECG%29%20is%20ubiquitous%20and%20inexpensive%2C%20and%20provides%20rich%20information%20on%20cardiac%20electrical%20activity%20and%20rhythm%2C%20yet%20offers%20limited%20insight%20into%20underlying%20cardiac%20structure%20and%20mechanical%20function.%20To%20address%20this%2C%20we%20introduce%20a%20contrastive%20learning%20framework%20that%20improves%20the%20extraction%20of%20clinically%20relevant%20cardiac%20phenotypes%20from%20ECG%20by%20learning%20from%20paired%20ECG-CMR%20data.%20Our%20approach%20aligns%20ECG%20representations%20with%203D%20CMR%20volumes%20at%20end-diastole%20%28ED%29%20and%20end-systole%20%28ES%29%2C%20with%20a%20dual-phase%20contrastive%20loss%20to%20anchor%20each%20ECG%20jointly%20with%20both%20cardiac%20phases%20in%20a%20shared%20latent%20space.%20Unlike%20prior%20methods%20limited%20to%202D%20CMR%20representations%20with%20or%20without%20a%20temporal%20component%2C%20our%20framework%20models%203D%20anatomy%20at%20both%20ED%20and%20ES%20phases%20as%20distinct%20latent%20representations%2C%20enabling%20flexible%20disentanglement%20of%20structural%20and%20functional%20cardiac%20properties.%20Using%20over%2034%2C000%20ECG-CMR%20pairs%20from%20the%20UK%20Biobank%2C%20we%20demonstrate%20improved%20extraction%20of%20image-derived%20phenotypes%20from%20ECG%2C%20particularly%20for%20functional%20parameters%20%28%24%5Cuparrow%24%209.2%5C%25%29%2C%20while%20improvements%20in%20clinical%20outcome%20prediction%20remained%20modest%20%28%24%5Cuparrow%24%200.7%5C%25%29.%20This%20strategy%20could%20enable%20scalable%20and%20cost-effective%20extraction%20of%20image-derived%20traits%20from%20ECG.%20The%20code%20for%20this%20research%20is%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Phase%2520Cross-Modal%2520Contrastive%2520Learning%2520for%2520CMR-Guided%2520ECG%2520Representations%2520for%2520Cardiovascular%2520Disease%2520Assessment%26entry.906535625%3DLaura%2520Alvarez-Florez%2520and%2520Angel%2520Bujalance-Gomez%2520and%2520Femke%2520Raijmakers%2520and%2520Samuel%2520Ruiperez-Campillo%2520and%2520Maarten%2520Z.%2520H.%2520Kolk%2520and%2520Jesse%2520Wiers%2520and%2520Julia%2520Vogt%2520and%2520Erik%2520J.%2520Bekkers%2520and%2520Ivana%2520I%25C5%25A1gum%2520and%2520Fleur%2520V.%2520Y.%2520Tjong%26entry.1292438233%3DCardiac%2520magnetic%2520resonance%2520imaging%2520%2528CMR%2529%2520offers%2520detailed%2520evaluation%2520of%2520cardiac%2520structure%2520and%2520function%252C%2520but%2520its%2520limited%2520accessibility%2520restricts%2520use%2520to%2520selected%2520patient%2520populations.%2520In%2520contrast%252C%2520the%2520electrocardiogram%2520%2528ECG%2529%2520is%2520ubiquitous%2520and%2520inexpensive%252C%2520and%2520provides%2520rich%2520information%2520on%2520cardiac%2520electrical%2520activity%2520and%2520rhythm%252C%2520yet%2520offers%2520limited%2520insight%2520into%2520underlying%2520cardiac%2520structure%2520and%2520mechanical%2520function.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520contrastive%2520learning%2520framework%2520that%2520improves%2520the%2520extraction%2520of%2520clinically%2520relevant%2520cardiac%2520phenotypes%2520from%2520ECG%2520by%2520learning%2520from%2520paired%2520ECG-CMR%2520data.%2520Our%2520approach%2520aligns%2520ECG%2520representations%2520with%25203D%2520CMR%2520volumes%2520at%2520end-diastole%2520%2528ED%2529%2520and%2520end-systole%2520%2528ES%2529%252C%2520with%2520a%2520dual-phase%2520contrastive%2520loss%2520to%2520anchor%2520each%2520ECG%2520jointly%2520with%2520both%2520cardiac%2520phases%2520in%2520a%2520shared%2520latent%2520space.%2520Unlike%2520prior%2520methods%2520limited%2520to%25202D%2520CMR%2520representations%2520with%2520or%2520without%2520a%2520temporal%2520component%252C%2520our%2520framework%2520models%25203D%2520anatomy%2520at%2520both%2520ED%2520and%2520ES%2520phases%2520as%2520distinct%2520latent%2520representations%252C%2520enabling%2520flexible%2520disentanglement%2520of%2520structural%2520and%2520functional%2520cardiac%2520properties.%2520Using%2520over%252034%252C000%2520ECG-CMR%2520pairs%2520from%2520the%2520UK%2520Biobank%252C%2520we%2520demonstrate%2520improved%2520extraction%2520of%2520image-derived%2520phenotypes%2520from%2520ECG%252C%2520particularly%2520for%2520functional%2520parameters%2520%2528%2524%255Cuparrow%2524%25209.2%255C%2525%2529%252C%2520while%2520improvements%2520in%2520clinical%2520outcome%2520prediction%2520remained%2520modest%2520%2528%2524%255Cuparrow%2524%25200.7%255C%2525%2529.%2520This%2520strategy%2520could%2520enable%2520scalable%2520and%2520cost-effective%2520extraction%2520of%2520image-derived%2520traits%2520from%2520ECG.%2520The%2520code%2520for%2520this%2520research%2520is%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Phase%20Cross-Modal%20Contrastive%20Learning%20for%20CMR-Guided%20ECG%20Representations%20for%20Cardiovascular%20Disease%20Assessment&entry.906535625=Laura%20Alvarez-Florez%20and%20Angel%20Bujalance-Gomez%20and%20Femke%20Raijmakers%20and%20Samuel%20Ruiperez-Campillo%20and%20Maarten%20Z.%20H.%20Kolk%20and%20Jesse%20Wiers%20and%20Julia%20Vogt%20and%20Erik%20J.%20Bekkers%20and%20Ivana%20I%C5%A1gum%20and%20Fleur%20V.%20Y.%20Tjong&entry.1292438233=Cardiac%20magnetic%20resonance%20imaging%20%28CMR%29%20offers%20detailed%20evaluation%20of%20cardiac%20structure%20and%20function%2C%20but%20its%20limited%20accessibility%20restricts%20use%20to%20selected%20patient%20populations.%20In%20contrast%2C%20the%20electrocardiogram%20%28ECG%29%20is%20ubiquitous%20and%20inexpensive%2C%20and%20provides%20rich%20information%20on%20cardiac%20electrical%20activity%20and%20rhythm%2C%20yet%20offers%20limited%20insight%20into%20underlying%20cardiac%20structure%20and%20mechanical%20function.%20To%20address%20this%2C%20we%20introduce%20a%20contrastive%20learning%20framework%20that%20improves%20the%20extraction%20of%20clinically%20relevant%20cardiac%20phenotypes%20from%20ECG%20by%20learning%20from%20paired%20ECG-CMR%20data.%20Our%20approach%20aligns%20ECG%20representations%20with%203D%20CMR%20volumes%20at%20end-diastole%20%28ED%29%20and%20end-systole%20%28ES%29%2C%20with%20a%20dual-phase%20contrastive%20loss%20to%20anchor%20each%20ECG%20jointly%20with%20both%20cardiac%20phases%20in%20a%20shared%20latent%20space.%20Unlike%20prior%20methods%20limited%20to%202D%20CMR%20representations%20with%20or%20without%20a%20temporal%20component%2C%20our%20framework%20models%203D%20anatomy%20at%20both%20ED%20and%20ES%20phases%20as%20distinct%20latent%20representations%2C%20enabling%20flexible%20disentanglement%20of%20structural%20and%20functional%20cardiac%20properties.%20Using%20over%2034%2C000%20ECG-CMR%20pairs%20from%20the%20UK%20Biobank%2C%20we%20demonstrate%20improved%20extraction%20of%20image-derived%20phenotypes%20from%20ECG%2C%20particularly%20for%20functional%20parameters%20%28%24%5Cuparrow%24%209.2%5C%25%29%2C%20while%20improvements%20in%20clinical%20outcome%20prediction%20remained%20modest%20%28%24%5Cuparrow%24%200.7%5C%25%29.%20This%20strategy%20could%20enable%20scalable%20and%20cost-effective%20extraction%20of%20image-derived%20traits%20from%20ECG.%20The%20code%20for%20this%20research%20is%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2602.12883v1&entry.124074799=Read"},
{"title": "Low-Dimensional Execution Manifolds in Transformer Learning Dynamics: Evidence from Modular Arithmetic Tasks", "author": "Yongzhong Xu", "abstract": "We investigate the geometric structure of learning dynamics in overparameterized transformer models through carefully controlled modular arithmetic tasks. Our primary finding is that despite operating in high-dimensional parameter spaces ($d=128$), transformer training trajectories rapidly collapse onto low-dimensional execution manifolds of dimension $3$--$4$. This dimensional collapse is robust across random seeds and moderate task difficulties, though the orientation of the manifold in parameter space varies between runs. We demonstrate that this geometric structure underlies several empirically observed phenomena: (1) sharp attention concentration emerges as saturation along routing coordinates within the execution manifold, (2) SGD commutators are preferentially aligned with the execution subspace (up to $10\\times$ random baseline) early in training, with $>92\\%$ of non-commutativity confined to orthogonal staging directions and this alignment decreasing as training converges, and (3) sparse autoencoders capture auxiliary routing structure but fail to isolate execution itself, which remains distributed across the low-dimensional manifold. Our results suggest a unifying geometric framework for understanding transformer learning, where the vast majority of parameters serve to absorb optimization interference while core computation occurs in a dramatically reduced subspace. These findings have implications for interpretability, training curriculum design, and understanding the role of overparameterization in neural network learning.", "link": "http://arxiv.org/abs/2602.10496v2", "date": "2026-02-13", "relevancy": 1.9662, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5191}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4881}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Dimensional%20Execution%20Manifolds%20in%20Transformer%20Learning%20Dynamics%3A%20Evidence%20from%20Modular%20Arithmetic%20Tasks&body=Title%3A%20Low-Dimensional%20Execution%20Manifolds%20in%20Transformer%20Learning%20Dynamics%3A%20Evidence%20from%20Modular%20Arithmetic%20Tasks%0AAuthor%3A%20Yongzhong%20Xu%0AAbstract%3A%20We%20investigate%20the%20geometric%20structure%20of%20learning%20dynamics%20in%20overparameterized%20transformer%20models%20through%20carefully%20controlled%20modular%20arithmetic%20tasks.%20Our%20primary%20finding%20is%20that%20despite%20operating%20in%20high-dimensional%20parameter%20spaces%20%28%24d%3D128%24%29%2C%20transformer%20training%20trajectories%20rapidly%20collapse%20onto%20low-dimensional%20execution%20manifolds%20of%20dimension%20%243%24--%244%24.%20This%20dimensional%20collapse%20is%20robust%20across%20random%20seeds%20and%20moderate%20task%20difficulties%2C%20though%20the%20orientation%20of%20the%20manifold%20in%20parameter%20space%20varies%20between%20runs.%20We%20demonstrate%20that%20this%20geometric%20structure%20underlies%20several%20empirically%20observed%20phenomena%3A%20%281%29%20sharp%20attention%20concentration%20emerges%20as%20saturation%20along%20routing%20coordinates%20within%20the%20execution%20manifold%2C%20%282%29%20SGD%20commutators%20are%20preferentially%20aligned%20with%20the%20execution%20subspace%20%28up%20to%20%2410%5Ctimes%24%20random%20baseline%29%20early%20in%20training%2C%20with%20%24%3E92%5C%25%24%20of%20non-commutativity%20confined%20to%20orthogonal%20staging%20directions%20and%20this%20alignment%20decreasing%20as%20training%20converges%2C%20and%20%283%29%20sparse%20autoencoders%20capture%20auxiliary%20routing%20structure%20but%20fail%20to%20isolate%20execution%20itself%2C%20which%20remains%20distributed%20across%20the%20low-dimensional%20manifold.%20Our%20results%20suggest%20a%20unifying%20geometric%20framework%20for%20understanding%20transformer%20learning%2C%20where%20the%20vast%20majority%20of%20parameters%20serve%20to%20absorb%20optimization%20interference%20while%20core%20computation%20occurs%20in%20a%20dramatically%20reduced%20subspace.%20These%20findings%20have%20implications%20for%20interpretability%2C%20training%20curriculum%20design%2C%20and%20understanding%20the%20role%20of%20overparameterization%20in%20neural%20network%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10496v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Dimensional%2520Execution%2520Manifolds%2520in%2520Transformer%2520Learning%2520Dynamics%253A%2520Evidence%2520from%2520Modular%2520Arithmetic%2520Tasks%26entry.906535625%3DYongzhong%2520Xu%26entry.1292438233%3DWe%2520investigate%2520the%2520geometric%2520structure%2520of%2520learning%2520dynamics%2520in%2520overparameterized%2520transformer%2520models%2520through%2520carefully%2520controlled%2520modular%2520arithmetic%2520tasks.%2520Our%2520primary%2520finding%2520is%2520that%2520despite%2520operating%2520in%2520high-dimensional%2520parameter%2520spaces%2520%2528%2524d%253D128%2524%2529%252C%2520transformer%2520training%2520trajectories%2520rapidly%2520collapse%2520onto%2520low-dimensional%2520execution%2520manifolds%2520of%2520dimension%2520%25243%2524--%25244%2524.%2520This%2520dimensional%2520collapse%2520is%2520robust%2520across%2520random%2520seeds%2520and%2520moderate%2520task%2520difficulties%252C%2520though%2520the%2520orientation%2520of%2520the%2520manifold%2520in%2520parameter%2520space%2520varies%2520between%2520runs.%2520We%2520demonstrate%2520that%2520this%2520geometric%2520structure%2520underlies%2520several%2520empirically%2520observed%2520phenomena%253A%2520%25281%2529%2520sharp%2520attention%2520concentration%2520emerges%2520as%2520saturation%2520along%2520routing%2520coordinates%2520within%2520the%2520execution%2520manifold%252C%2520%25282%2529%2520SGD%2520commutators%2520are%2520preferentially%2520aligned%2520with%2520the%2520execution%2520subspace%2520%2528up%2520to%2520%252410%255Ctimes%2524%2520random%2520baseline%2529%2520early%2520in%2520training%252C%2520with%2520%2524%253E92%255C%2525%2524%2520of%2520non-commutativity%2520confined%2520to%2520orthogonal%2520staging%2520directions%2520and%2520this%2520alignment%2520decreasing%2520as%2520training%2520converges%252C%2520and%2520%25283%2529%2520sparse%2520autoencoders%2520capture%2520auxiliary%2520routing%2520structure%2520but%2520fail%2520to%2520isolate%2520execution%2520itself%252C%2520which%2520remains%2520distributed%2520across%2520the%2520low-dimensional%2520manifold.%2520Our%2520results%2520suggest%2520a%2520unifying%2520geometric%2520framework%2520for%2520understanding%2520transformer%2520learning%252C%2520where%2520the%2520vast%2520majority%2520of%2520parameters%2520serve%2520to%2520absorb%2520optimization%2520interference%2520while%2520core%2520computation%2520occurs%2520in%2520a%2520dramatically%2520reduced%2520subspace.%2520These%2520findings%2520have%2520implications%2520for%2520interpretability%252C%2520training%2520curriculum%2520design%252C%2520and%2520understanding%2520the%2520role%2520of%2520overparameterization%2520in%2520neural%2520network%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10496v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Dimensional%20Execution%20Manifolds%20in%20Transformer%20Learning%20Dynamics%3A%20Evidence%20from%20Modular%20Arithmetic%20Tasks&entry.906535625=Yongzhong%20Xu&entry.1292438233=We%20investigate%20the%20geometric%20structure%20of%20learning%20dynamics%20in%20overparameterized%20transformer%20models%20through%20carefully%20controlled%20modular%20arithmetic%20tasks.%20Our%20primary%20finding%20is%20that%20despite%20operating%20in%20high-dimensional%20parameter%20spaces%20%28%24d%3D128%24%29%2C%20transformer%20training%20trajectories%20rapidly%20collapse%20onto%20low-dimensional%20execution%20manifolds%20of%20dimension%20%243%24--%244%24.%20This%20dimensional%20collapse%20is%20robust%20across%20random%20seeds%20and%20moderate%20task%20difficulties%2C%20though%20the%20orientation%20of%20the%20manifold%20in%20parameter%20space%20varies%20between%20runs.%20We%20demonstrate%20that%20this%20geometric%20structure%20underlies%20several%20empirically%20observed%20phenomena%3A%20%281%29%20sharp%20attention%20concentration%20emerges%20as%20saturation%20along%20routing%20coordinates%20within%20the%20execution%20manifold%2C%20%282%29%20SGD%20commutators%20are%20preferentially%20aligned%20with%20the%20execution%20subspace%20%28up%20to%20%2410%5Ctimes%24%20random%20baseline%29%20early%20in%20training%2C%20with%20%24%3E92%5C%25%24%20of%20non-commutativity%20confined%20to%20orthogonal%20staging%20directions%20and%20this%20alignment%20decreasing%20as%20training%20converges%2C%20and%20%283%29%20sparse%20autoencoders%20capture%20auxiliary%20routing%20structure%20but%20fail%20to%20isolate%20execution%20itself%2C%20which%20remains%20distributed%20across%20the%20low-dimensional%20manifold.%20Our%20results%20suggest%20a%20unifying%20geometric%20framework%20for%20understanding%20transformer%20learning%2C%20where%20the%20vast%20majority%20of%20parameters%20serve%20to%20absorb%20optimization%20interference%20while%20core%20computation%20occurs%20in%20a%20dramatically%20reduced%20subspace.%20These%20findings%20have%20implications%20for%20interpretability%2C%20training%20curriculum%20design%2C%20and%20understanding%20the%20role%20of%20overparameterization%20in%20neural%20network%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2602.10496v2&entry.124074799=Read"},
{"title": "Ca-MCF: Category-level Multi-label Causal Feature selection", "author": "Wanfu Gao and Yanan Wang and Yonghao Li", "abstract": "Multi-label causal feature selection has attracted extensive attention in recent years. However, current methods primarily operate at the label level, treating each label variable as a monolithic entity and overlooking the fine-grained causal mechanisms unique to individual categories. To address this, we propose a Category-level Multi-label Causal Feature selection method named Ca-MCF. Ca-MCF utilizes label category flattening to decompose label variables into specific category nodes, enabling precise modeling of causal structures within the label space. Furthermore, we introduce an explanatory competition-based category-aware recovery mechanism that leverages the proposed Specific Category-Specific Mutual Information (SCSMI) and Distinct Category-Specific Mutual Information (DCSMI) to salvage causal features obscured by label correlations. The method also incorporates structural symmetry checks and cross-dimensional redundancy removal to ensure the robustness and compactness of the identified Markov Blankets. Extensive experiments across seven real-world datasets demonstrate that Ca-MCF significantly outperforms state-of-the-art benchmarks, achieving superior predictive accuracy with reduced feature dimensionality.", "link": "http://arxiv.org/abs/2602.12961v1", "date": "2026-02-13", "relevancy": 1.2857, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4403}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4365}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ca-MCF%3A%20Category-level%20Multi-label%20Causal%20Feature%20selection&body=Title%3A%20Ca-MCF%3A%20Category-level%20Multi-label%20Causal%20Feature%20selection%0AAuthor%3A%20Wanfu%20Gao%20and%20Yanan%20Wang%20and%20Yonghao%20Li%0AAbstract%3A%20Multi-label%20causal%20feature%20selection%20has%20attracted%20extensive%20attention%20in%20recent%20years.%20However%2C%20current%20methods%20primarily%20operate%20at%20the%20label%20level%2C%20treating%20each%20label%20variable%20as%20a%20monolithic%20entity%20and%20overlooking%20the%20fine-grained%20causal%20mechanisms%20unique%20to%20individual%20categories.%20To%20address%20this%2C%20we%20propose%20a%20Category-level%20Multi-label%20Causal%20Feature%20selection%20method%20named%20Ca-MCF.%20Ca-MCF%20utilizes%20label%20category%20flattening%20to%20decompose%20label%20variables%20into%20specific%20category%20nodes%2C%20enabling%20precise%20modeling%20of%20causal%20structures%20within%20the%20label%20space.%20Furthermore%2C%20we%20introduce%20an%20explanatory%20competition-based%20category-aware%20recovery%20mechanism%20that%20leverages%20the%20proposed%20Specific%20Category-Specific%20Mutual%20Information%20%28SCSMI%29%20and%20Distinct%20Category-Specific%20Mutual%20Information%20%28DCSMI%29%20to%20salvage%20causal%20features%20obscured%20by%20label%20correlations.%20The%20method%20also%20incorporates%20structural%20symmetry%20checks%20and%20cross-dimensional%20redundancy%20removal%20to%20ensure%20the%20robustness%20and%20compactness%20of%20the%20identified%20Markov%20Blankets.%20Extensive%20experiments%20across%20seven%20real-world%20datasets%20demonstrate%20that%20Ca-MCF%20significantly%20outperforms%20state-of-the-art%20benchmarks%2C%20achieving%20superior%20predictive%20accuracy%20with%20reduced%20feature%20dimensionality.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCa-MCF%253A%2520Category-level%2520Multi-label%2520Causal%2520Feature%2520selection%26entry.906535625%3DWanfu%2520Gao%2520and%2520Yanan%2520Wang%2520and%2520Yonghao%2520Li%26entry.1292438233%3DMulti-label%2520causal%2520feature%2520selection%2520has%2520attracted%2520extensive%2520attention%2520in%2520recent%2520years.%2520However%252C%2520current%2520methods%2520primarily%2520operate%2520at%2520the%2520label%2520level%252C%2520treating%2520each%2520label%2520variable%2520as%2520a%2520monolithic%2520entity%2520and%2520overlooking%2520the%2520fine-grained%2520causal%2520mechanisms%2520unique%2520to%2520individual%2520categories.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520Category-level%2520Multi-label%2520Causal%2520Feature%2520selection%2520method%2520named%2520Ca-MCF.%2520Ca-MCF%2520utilizes%2520label%2520category%2520flattening%2520to%2520decompose%2520label%2520variables%2520into%2520specific%2520category%2520nodes%252C%2520enabling%2520precise%2520modeling%2520of%2520causal%2520structures%2520within%2520the%2520label%2520space.%2520Furthermore%252C%2520we%2520introduce%2520an%2520explanatory%2520competition-based%2520category-aware%2520recovery%2520mechanism%2520that%2520leverages%2520the%2520proposed%2520Specific%2520Category-Specific%2520Mutual%2520Information%2520%2528SCSMI%2529%2520and%2520Distinct%2520Category-Specific%2520Mutual%2520Information%2520%2528DCSMI%2529%2520to%2520salvage%2520causal%2520features%2520obscured%2520by%2520label%2520correlations.%2520The%2520method%2520also%2520incorporates%2520structural%2520symmetry%2520checks%2520and%2520cross-dimensional%2520redundancy%2520removal%2520to%2520ensure%2520the%2520robustness%2520and%2520compactness%2520of%2520the%2520identified%2520Markov%2520Blankets.%2520Extensive%2520experiments%2520across%2520seven%2520real-world%2520datasets%2520demonstrate%2520that%2520Ca-MCF%2520significantly%2520outperforms%2520state-of-the-art%2520benchmarks%252C%2520achieving%2520superior%2520predictive%2520accuracy%2520with%2520reduced%2520feature%2520dimensionality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ca-MCF%3A%20Category-level%20Multi-label%20Causal%20Feature%20selection&entry.906535625=Wanfu%20Gao%20and%20Yanan%20Wang%20and%20Yonghao%20Li&entry.1292438233=Multi-label%20causal%20feature%20selection%20has%20attracted%20extensive%20attention%20in%20recent%20years.%20However%2C%20current%20methods%20primarily%20operate%20at%20the%20label%20level%2C%20treating%20each%20label%20variable%20as%20a%20monolithic%20entity%20and%20overlooking%20the%20fine-grained%20causal%20mechanisms%20unique%20to%20individual%20categories.%20To%20address%20this%2C%20we%20propose%20a%20Category-level%20Multi-label%20Causal%20Feature%20selection%20method%20named%20Ca-MCF.%20Ca-MCF%20utilizes%20label%20category%20flattening%20to%20decompose%20label%20variables%20into%20specific%20category%20nodes%2C%20enabling%20precise%20modeling%20of%20causal%20structures%20within%20the%20label%20space.%20Furthermore%2C%20we%20introduce%20an%20explanatory%20competition-based%20category-aware%20recovery%20mechanism%20that%20leverages%20the%20proposed%20Specific%20Category-Specific%20Mutual%20Information%20%28SCSMI%29%20and%20Distinct%20Category-Specific%20Mutual%20Information%20%28DCSMI%29%20to%20salvage%20causal%20features%20obscured%20by%20label%20correlations.%20The%20method%20also%20incorporates%20structural%20symmetry%20checks%20and%20cross-dimensional%20redundancy%20removal%20to%20ensure%20the%20robustness%20and%20compactness%20of%20the%20identified%20Markov%20Blankets.%20Extensive%20experiments%20across%20seven%20real-world%20datasets%20demonstrate%20that%20Ca-MCF%20significantly%20outperforms%20state-of-the-art%20benchmarks%2C%20achieving%20superior%20predictive%20accuracy%20with%20reduced%20feature%20dimensionality.&entry.1838667208=http%3A//arxiv.org/abs/2602.12961v1&entry.124074799=Read"},
{"title": "Eliminating stability hallucinations in llm-based tts models via attention guidance", "author": "ShiMing Wang and ZhiHao Du and Yang Xiang and TianYu Zhao and Han Zhao and Qian Chen and XianGang Li and HanJie Guo and ZhenHua Ling", "abstract": "This paper focuses on resolving stability hallucinations (e.g., repetitive or omitted speech) in LLM-based Text-to-Speech (TTS) models by improving and leveraging the attention mechanism. First, we analyzed the alignment mechanism between text tokens and speech tokens in LLMs. We then proposed a metric termed the Optimal Alignment Score (OAS), which employs the Viterbi algorithm to evaluate text-speech alignment quality. Subsequently, OAS was integrated into the training of CosyVoice2 to assist LLMs in learning continuous, stable alignment. Additionally, the pre-trained attention value is employed to guide the training of the student CosyVoice2 via chain-of-thought (CoT), which further reduces stability hallucinations in synthesized speech. Experiments on the Seed-TTS-Eval and CV3-Eval test sets demonstrate that the proposed methods can effectively reduce the stability hallucinations of CosyVoice2 without introducing additional negative effects. The appendix is available at https://wsmzzz.github.io/llm_attn.", "link": "http://arxiv.org/abs/2509.19852v2", "date": "2026-02-13", "relevancy": 1.8256, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4631}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4529}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eliminating%20stability%20hallucinations%20in%20llm-based%20tts%20models%20via%20attention%20guidance&body=Title%3A%20Eliminating%20stability%20hallucinations%20in%20llm-based%20tts%20models%20via%20attention%20guidance%0AAuthor%3A%20ShiMing%20Wang%20and%20ZhiHao%20Du%20and%20Yang%20Xiang%20and%20TianYu%20Zhao%20and%20Han%20Zhao%20and%20Qian%20Chen%20and%20XianGang%20Li%20and%20HanJie%20Guo%20and%20ZhenHua%20Ling%0AAbstract%3A%20This%20paper%20focuses%20on%20resolving%20stability%20hallucinations%20%28e.g.%2C%20repetitive%20or%20omitted%20speech%29%20in%20LLM-based%20Text-to-Speech%20%28TTS%29%20models%20by%20improving%20and%20leveraging%20the%20attention%20mechanism.%20First%2C%20we%20analyzed%20the%20alignment%20mechanism%20between%20text%20tokens%20and%20speech%20tokens%20in%20LLMs.%20We%20then%20proposed%20a%20metric%20termed%20the%20Optimal%20Alignment%20Score%20%28OAS%29%2C%20which%20employs%20the%20Viterbi%20algorithm%20to%20evaluate%20text-speech%20alignment%20quality.%20Subsequently%2C%20OAS%20was%20integrated%20into%20the%20training%20of%20CosyVoice2%20to%20assist%20LLMs%20in%20learning%20continuous%2C%20stable%20alignment.%20Additionally%2C%20the%20pre-trained%20attention%20value%20is%20employed%20to%20guide%20the%20training%20of%20the%20student%20CosyVoice2%20via%20chain-of-thought%20%28CoT%29%2C%20which%20further%20reduces%20stability%20hallucinations%20in%20synthesized%20speech.%20Experiments%20on%20the%20Seed-TTS-Eval%20and%20CV3-Eval%20test%20sets%20demonstrate%20that%20the%20proposed%20methods%20can%20effectively%20reduce%20the%20stability%20hallucinations%20of%20CosyVoice2%20without%20introducing%20additional%20negative%20effects.%20The%20appendix%20is%20available%20at%20https%3A//wsmzzz.github.io/llm_attn.%0ALink%3A%20http%3A//arxiv.org/abs/2509.19852v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEliminating%2520stability%2520hallucinations%2520in%2520llm-based%2520tts%2520models%2520via%2520attention%2520guidance%26entry.906535625%3DShiMing%2520Wang%2520and%2520ZhiHao%2520Du%2520and%2520Yang%2520Xiang%2520and%2520TianYu%2520Zhao%2520and%2520Han%2520Zhao%2520and%2520Qian%2520Chen%2520and%2520XianGang%2520Li%2520and%2520HanJie%2520Guo%2520and%2520ZhenHua%2520Ling%26entry.1292438233%3DThis%2520paper%2520focuses%2520on%2520resolving%2520stability%2520hallucinations%2520%2528e.g.%252C%2520repetitive%2520or%2520omitted%2520speech%2529%2520in%2520LLM-based%2520Text-to-Speech%2520%2528TTS%2529%2520models%2520by%2520improving%2520and%2520leveraging%2520the%2520attention%2520mechanism.%2520First%252C%2520we%2520analyzed%2520the%2520alignment%2520mechanism%2520between%2520text%2520tokens%2520and%2520speech%2520tokens%2520in%2520LLMs.%2520We%2520then%2520proposed%2520a%2520metric%2520termed%2520the%2520Optimal%2520Alignment%2520Score%2520%2528OAS%2529%252C%2520which%2520employs%2520the%2520Viterbi%2520algorithm%2520to%2520evaluate%2520text-speech%2520alignment%2520quality.%2520Subsequently%252C%2520OAS%2520was%2520integrated%2520into%2520the%2520training%2520of%2520CosyVoice2%2520to%2520assist%2520LLMs%2520in%2520learning%2520continuous%252C%2520stable%2520alignment.%2520Additionally%252C%2520the%2520pre-trained%2520attention%2520value%2520is%2520employed%2520to%2520guide%2520the%2520training%2520of%2520the%2520student%2520CosyVoice2%2520via%2520chain-of-thought%2520%2528CoT%2529%252C%2520which%2520further%2520reduces%2520stability%2520hallucinations%2520in%2520synthesized%2520speech.%2520Experiments%2520on%2520the%2520Seed-TTS-Eval%2520and%2520CV3-Eval%2520test%2520sets%2520demonstrate%2520that%2520the%2520proposed%2520methods%2520can%2520effectively%2520reduce%2520the%2520stability%2520hallucinations%2520of%2520CosyVoice2%2520without%2520introducing%2520additional%2520negative%2520effects.%2520The%2520appendix%2520is%2520available%2520at%2520https%253A//wsmzzz.github.io/llm_attn.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19852v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eliminating%20stability%20hallucinations%20in%20llm-based%20tts%20models%20via%20attention%20guidance&entry.906535625=ShiMing%20Wang%20and%20ZhiHao%20Du%20and%20Yang%20Xiang%20and%20TianYu%20Zhao%20and%20Han%20Zhao%20and%20Qian%20Chen%20and%20XianGang%20Li%20and%20HanJie%20Guo%20and%20ZhenHua%20Ling&entry.1292438233=This%20paper%20focuses%20on%20resolving%20stability%20hallucinations%20%28e.g.%2C%20repetitive%20or%20omitted%20speech%29%20in%20LLM-based%20Text-to-Speech%20%28TTS%29%20models%20by%20improving%20and%20leveraging%20the%20attention%20mechanism.%20First%2C%20we%20analyzed%20the%20alignment%20mechanism%20between%20text%20tokens%20and%20speech%20tokens%20in%20LLMs.%20We%20then%20proposed%20a%20metric%20termed%20the%20Optimal%20Alignment%20Score%20%28OAS%29%2C%20which%20employs%20the%20Viterbi%20algorithm%20to%20evaluate%20text-speech%20alignment%20quality.%20Subsequently%2C%20OAS%20was%20integrated%20into%20the%20training%20of%20CosyVoice2%20to%20assist%20LLMs%20in%20learning%20continuous%2C%20stable%20alignment.%20Additionally%2C%20the%20pre-trained%20attention%20value%20is%20employed%20to%20guide%20the%20training%20of%20the%20student%20CosyVoice2%20via%20chain-of-thought%20%28CoT%29%2C%20which%20further%20reduces%20stability%20hallucinations%20in%20synthesized%20speech.%20Experiments%20on%20the%20Seed-TTS-Eval%20and%20CV3-Eval%20test%20sets%20demonstrate%20that%20the%20proposed%20methods%20can%20effectively%20reduce%20the%20stability%20hallucinations%20of%20CosyVoice2%20without%20introducing%20additional%20negative%20effects.%20The%20appendix%20is%20available%20at%20https%3A//wsmzzz.github.io/llm_attn.&entry.1838667208=http%3A//arxiv.org/abs/2509.19852v2&entry.124074799=Read"},
{"title": "Synaptic Activation and Dual Liquid Dynamics for Interpretable Bio-Inspired Models", "author": "M\u00f3nika Farsang and Radu Grosu", "abstract": "In this paper, we present a unified framework for various bio-inspired models to better understand their structural and functional differences. We show that liquid-capacitance-extended models lead to interpretable behavior even in dense, all-to-all recurrent neural network (RNN) policies. We further demonstrate that incorporating chemical synapses improves interpretability and that combining chemical synapses with synaptic activation yields the most accurate and interpretable RNN models. To assess the accuracy and interpretability of these RNN policies, we consider the challenging lane-keeping control task and evaluate performance across multiple metrics, including turn-weighted validation loss, neural activity during driving, absolute correlation between neural activity and road trajectory, saliency maps of the networks' attention, and the robustness of their saliency maps measured by the structural similarity index.", "link": "http://arxiv.org/abs/2602.13017v1", "date": "2026-02-13", "relevancy": 1.5646, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5495}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5154}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synaptic%20Activation%20and%20Dual%20Liquid%20Dynamics%20for%20Interpretable%20Bio-Inspired%20Models&body=Title%3A%20Synaptic%20Activation%20and%20Dual%20Liquid%20Dynamics%20for%20Interpretable%20Bio-Inspired%20Models%0AAuthor%3A%20M%C3%B3nika%20Farsang%20and%20Radu%20Grosu%0AAbstract%3A%20In%20this%20paper%2C%20we%20present%20a%20unified%20framework%20for%20various%20bio-inspired%20models%20to%20better%20understand%20their%20structural%20and%20functional%20differences.%20We%20show%20that%20liquid-capacitance-extended%20models%20lead%20to%20interpretable%20behavior%20even%20in%20dense%2C%20all-to-all%20recurrent%20neural%20network%20%28RNN%29%20policies.%20We%20further%20demonstrate%20that%20incorporating%20chemical%20synapses%20improves%20interpretability%20and%20that%20combining%20chemical%20synapses%20with%20synaptic%20activation%20yields%20the%20most%20accurate%20and%20interpretable%20RNN%20models.%20To%20assess%20the%20accuracy%20and%20interpretability%20of%20these%20RNN%20policies%2C%20we%20consider%20the%20challenging%20lane-keeping%20control%20task%20and%20evaluate%20performance%20across%20multiple%20metrics%2C%20including%20turn-weighted%20validation%20loss%2C%20neural%20activity%20during%20driving%2C%20absolute%20correlation%20between%20neural%20activity%20and%20road%20trajectory%2C%20saliency%20maps%20of%20the%20networks%27%20attention%2C%20and%20the%20robustness%20of%20their%20saliency%20maps%20measured%20by%20the%20structural%20similarity%20index.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynaptic%2520Activation%2520and%2520Dual%2520Liquid%2520Dynamics%2520for%2520Interpretable%2520Bio-Inspired%2520Models%26entry.906535625%3DM%25C3%25B3nika%2520Farsang%2520and%2520Radu%2520Grosu%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520present%2520a%2520unified%2520framework%2520for%2520various%2520bio-inspired%2520models%2520to%2520better%2520understand%2520their%2520structural%2520and%2520functional%2520differences.%2520We%2520show%2520that%2520liquid-capacitance-extended%2520models%2520lead%2520to%2520interpretable%2520behavior%2520even%2520in%2520dense%252C%2520all-to-all%2520recurrent%2520neural%2520network%2520%2528RNN%2529%2520policies.%2520We%2520further%2520demonstrate%2520that%2520incorporating%2520chemical%2520synapses%2520improves%2520interpretability%2520and%2520that%2520combining%2520chemical%2520synapses%2520with%2520synaptic%2520activation%2520yields%2520the%2520most%2520accurate%2520and%2520interpretable%2520RNN%2520models.%2520To%2520assess%2520the%2520accuracy%2520and%2520interpretability%2520of%2520these%2520RNN%2520policies%252C%2520we%2520consider%2520the%2520challenging%2520lane-keeping%2520control%2520task%2520and%2520evaluate%2520performance%2520across%2520multiple%2520metrics%252C%2520including%2520turn-weighted%2520validation%2520loss%252C%2520neural%2520activity%2520during%2520driving%252C%2520absolute%2520correlation%2520between%2520neural%2520activity%2520and%2520road%2520trajectory%252C%2520saliency%2520maps%2520of%2520the%2520networks%2527%2520attention%252C%2520and%2520the%2520robustness%2520of%2520their%2520saliency%2520maps%2520measured%2520by%2520the%2520structural%2520similarity%2520index.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synaptic%20Activation%20and%20Dual%20Liquid%20Dynamics%20for%20Interpretable%20Bio-Inspired%20Models&entry.906535625=M%C3%B3nika%20Farsang%20and%20Radu%20Grosu&entry.1292438233=In%20this%20paper%2C%20we%20present%20a%20unified%20framework%20for%20various%20bio-inspired%20models%20to%20better%20understand%20their%20structural%20and%20functional%20differences.%20We%20show%20that%20liquid-capacitance-extended%20models%20lead%20to%20interpretable%20behavior%20even%20in%20dense%2C%20all-to-all%20recurrent%20neural%20network%20%28RNN%29%20policies.%20We%20further%20demonstrate%20that%20incorporating%20chemical%20synapses%20improves%20interpretability%20and%20that%20combining%20chemical%20synapses%20with%20synaptic%20activation%20yields%20the%20most%20accurate%20and%20interpretable%20RNN%20models.%20To%20assess%20the%20accuracy%20and%20interpretability%20of%20these%20RNN%20policies%2C%20we%20consider%20the%20challenging%20lane-keeping%20control%20task%20and%20evaluate%20performance%20across%20multiple%20metrics%2C%20including%20turn-weighted%20validation%20loss%2C%20neural%20activity%20during%20driving%2C%20absolute%20correlation%20between%20neural%20activity%20and%20road%20trajectory%2C%20saliency%20maps%20of%20the%20networks%27%20attention%2C%20and%20the%20robustness%20of%20their%20saliency%20maps%20measured%20by%20the%20structural%20similarity%20index.&entry.1838667208=http%3A//arxiv.org/abs/2602.13017v1&entry.124074799=Read"},
{"title": "Information-theoretic analysis of world models in optimal reward maximizers", "author": "Alfred Harwood and Jose Faustino and Alex Altair", "abstract": "An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \\log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \\log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the \"implicit world model'' necessary for optimality.", "link": "http://arxiv.org/abs/2602.12963v1", "date": "2026-02-13", "relevancy": 1.7171, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4406}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4276}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information-theoretic%20analysis%20of%20world%20models%20in%20optimal%20reward%20maximizers&body=Title%3A%20Information-theoretic%20analysis%20of%20world%20models%20in%20optimal%20reward%20maximizers%0AAuthor%3A%20Alfred%20Harwood%20and%20Jose%20Faustino%20and%20Alex%20Altair%0AAbstract%3A%20An%20important%20question%20in%20the%20field%20of%20AI%20is%20the%20extent%20to%20which%20successful%20behaviour%20requires%20an%20internal%20representation%20of%20the%20world.%20In%20this%20work%2C%20we%20quantify%20the%20amount%20of%20information%20an%20optimal%20policy%20provides%20about%20the%20underlying%20environment.%20We%20consider%20a%20Controlled%20Markov%20Process%20%28CMP%29%20with%20%24n%24%20states%20and%20%24m%24%20actions%2C%20assuming%20a%20uniform%20prior%20over%20the%20space%20of%20possible%20transition%20dynamics.%20We%20prove%20that%20observing%20a%20deterministic%20policy%20that%20is%20optimal%20for%20any%20non-constant%20reward%20function%20then%20conveys%20exactly%20%24n%20%5Clog%20m%24%20bits%20of%20information%20about%20the%20environment.%20Specifically%2C%20we%20show%20that%20the%20mutual%20information%20between%20the%20environment%20and%20the%20optimal%20policy%20is%20%24n%20%5Clog%20m%24%20bits.%20This%20bound%20holds%20across%20a%20broad%20class%20of%20objectives%2C%20including%20finite-horizon%2C%20infinite-horizon%20discounted%2C%20and%20time-averaged%20reward%20maximization.%20These%20findings%20provide%20a%20precise%20information-theoretic%20lower%20bound%20on%20the%20%22implicit%20world%20model%27%27%20necessary%20for%20optimality.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation-theoretic%2520analysis%2520of%2520world%2520models%2520in%2520optimal%2520reward%2520maximizers%26entry.906535625%3DAlfred%2520Harwood%2520and%2520Jose%2520Faustino%2520and%2520Alex%2520Altair%26entry.1292438233%3DAn%2520important%2520question%2520in%2520the%2520field%2520of%2520AI%2520is%2520the%2520extent%2520to%2520which%2520successful%2520behaviour%2520requires%2520an%2520internal%2520representation%2520of%2520the%2520world.%2520In%2520this%2520work%252C%2520we%2520quantify%2520the%2520amount%2520of%2520information%2520an%2520optimal%2520policy%2520provides%2520about%2520the%2520underlying%2520environment.%2520We%2520consider%2520a%2520Controlled%2520Markov%2520Process%2520%2528CMP%2529%2520with%2520%2524n%2524%2520states%2520and%2520%2524m%2524%2520actions%252C%2520assuming%2520a%2520uniform%2520prior%2520over%2520the%2520space%2520of%2520possible%2520transition%2520dynamics.%2520We%2520prove%2520that%2520observing%2520a%2520deterministic%2520policy%2520that%2520is%2520optimal%2520for%2520any%2520non-constant%2520reward%2520function%2520then%2520conveys%2520exactly%2520%2524n%2520%255Clog%2520m%2524%2520bits%2520of%2520information%2520about%2520the%2520environment.%2520Specifically%252C%2520we%2520show%2520that%2520the%2520mutual%2520information%2520between%2520the%2520environment%2520and%2520the%2520optimal%2520policy%2520is%2520%2524n%2520%255Clog%2520m%2524%2520bits.%2520This%2520bound%2520holds%2520across%2520a%2520broad%2520class%2520of%2520objectives%252C%2520including%2520finite-horizon%252C%2520infinite-horizon%2520discounted%252C%2520and%2520time-averaged%2520reward%2520maximization.%2520These%2520findings%2520provide%2520a%2520precise%2520information-theoretic%2520lower%2520bound%2520on%2520the%2520%2522implicit%2520world%2520model%2527%2527%2520necessary%2520for%2520optimality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information-theoretic%20analysis%20of%20world%20models%20in%20optimal%20reward%20maximizers&entry.906535625=Alfred%20Harwood%20and%20Jose%20Faustino%20and%20Alex%20Altair&entry.1292438233=An%20important%20question%20in%20the%20field%20of%20AI%20is%20the%20extent%20to%20which%20successful%20behaviour%20requires%20an%20internal%20representation%20of%20the%20world.%20In%20this%20work%2C%20we%20quantify%20the%20amount%20of%20information%20an%20optimal%20policy%20provides%20about%20the%20underlying%20environment.%20We%20consider%20a%20Controlled%20Markov%20Process%20%28CMP%29%20with%20%24n%24%20states%20and%20%24m%24%20actions%2C%20assuming%20a%20uniform%20prior%20over%20the%20space%20of%20possible%20transition%20dynamics.%20We%20prove%20that%20observing%20a%20deterministic%20policy%20that%20is%20optimal%20for%20any%20non-constant%20reward%20function%20then%20conveys%20exactly%20%24n%20%5Clog%20m%24%20bits%20of%20information%20about%20the%20environment.%20Specifically%2C%20we%20show%20that%20the%20mutual%20information%20between%20the%20environment%20and%20the%20optimal%20policy%20is%20%24n%20%5Clog%20m%24%20bits.%20This%20bound%20holds%20across%20a%20broad%20class%20of%20objectives%2C%20including%20finite-horizon%2C%20infinite-horizon%20discounted%2C%20and%20time-averaged%20reward%20maximization.%20These%20findings%20provide%20a%20precise%20information-theoretic%20lower%20bound%20on%20the%20%22implicit%20world%20model%27%27%20necessary%20for%20optimality.&entry.1838667208=http%3A//arxiv.org/abs/2602.12963v1&entry.124074799=Read"},
{"title": "Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off", "author": "Muhammad Faraz Ul Abrar and Nicol\u00f2 Michelusi", "abstract": "Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \\emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.", "link": "http://arxiv.org/abs/2510.26722v4", "date": "2026-02-13", "relevancy": 1.8979, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4978}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4743}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Convex%20Over-the-Air%20Heterogeneous%20Federated%20Learning%3A%20A%20Bias-Variance%20Trade-off&body=Title%3A%20Non-Convex%20Over-the-Air%20Heterogeneous%20Federated%20Learning%3A%20A%20Bias-Variance%20Trade-off%0AAuthor%3A%20Muhammad%20Faraz%20Ul%20Abrar%20and%20Nicol%C3%B2%20Michelusi%0AAbstract%3A%20Over-the-air%20%28OTA%29%20federated%20learning%20%28FL%29%20has%20been%20well%20recognized%20as%20a%20scalable%20paradigm%20that%20exploits%20the%20waveform%20superposition%20of%20the%20wireless%20multiple-access%20channel%20to%20aggregate%20model%20updates%20in%20a%20single%20use.%20Existing%20OTA-FL%20designs%20largely%20enforce%20zero-bias%20model%20updates%20by%20either%20assuming%20%5Cemph%7Bhomogeneous%7D%20wireless%20conditions%20%28equal%20path%20loss%20across%20devices%29%20or%20forcing%20zero-bias%20updates%20to%20guarantee%20convergence.%20Under%20%5Cemph%7Bheterogeneous%7D%20wireless%20scenarios%2C%20however%2C%20such%20designs%20are%20constrained%20by%20the%20weakest%20device%20and%20inflate%20the%20update%20variance.%20Moreover%2C%20prior%20analyses%20of%20biased%20OTA-FL%20largely%20address%20convex%20objectives%2C%20while%20most%20modern%20AI%20models%20are%20highly%20non-convex.%20Motivated%20by%20these%20gaps%2C%20we%20study%20OTA-FL%20with%20stochastic%20gradient%20descent%20%28SGD%29%20for%20general%20smooth%20non-convex%20objectives%20under%20wireless%20heterogeneity.%20We%20develop%20novel%20OTA-FL%20SGD%20updates%20that%20allow%20a%20structured%2C%20time-invariant%20model%20bias%20while%20facilitating%20reduced%20variance%20updates.%20We%20derive%20a%20finite-time%20stationarity%20bound%20%28expected%20time%20average%20squared%20gradient%20norm%29%20that%20explicitly%20reveals%20a%20bias-variance%20trade-off.%20To%20optimize%20this%20trade-off%2C%20we%20pose%20a%20non-convex%20joint%20OTA%20power-control%20design%20and%20develop%20an%20efficient%20successive%20convex%20approximation%20%28SCA%29%20algorithm%20that%20requires%20only%20statistical%20CSI%20at%20the%20base%20station.%20Experiments%20on%20a%20non-convex%20image%20classification%20task%20validate%20the%20approach%3A%20the%20SCA-based%20design%20accelerates%20convergence%20via%20an%20optimized%20bias%20and%20improves%20generalization%20over%20prior%20OTA-FL%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2510.26722v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Convex%2520Over-the-Air%2520Heterogeneous%2520Federated%2520Learning%253A%2520A%2520Bias-Variance%2520Trade-off%26entry.906535625%3DMuhammad%2520Faraz%2520Ul%2520Abrar%2520and%2520Nicol%25C3%25B2%2520Michelusi%26entry.1292438233%3DOver-the-air%2520%2528OTA%2529%2520federated%2520learning%2520%2528FL%2529%2520has%2520been%2520well%2520recognized%2520as%2520a%2520scalable%2520paradigm%2520that%2520exploits%2520the%2520waveform%2520superposition%2520of%2520the%2520wireless%2520multiple-access%2520channel%2520to%2520aggregate%2520model%2520updates%2520in%2520a%2520single%2520use.%2520Existing%2520OTA-FL%2520designs%2520largely%2520enforce%2520zero-bias%2520model%2520updates%2520by%2520either%2520assuming%2520%255Cemph%257Bhomogeneous%257D%2520wireless%2520conditions%2520%2528equal%2520path%2520loss%2520across%2520devices%2529%2520or%2520forcing%2520zero-bias%2520updates%2520to%2520guarantee%2520convergence.%2520Under%2520%255Cemph%257Bheterogeneous%257D%2520wireless%2520scenarios%252C%2520however%252C%2520such%2520designs%2520are%2520constrained%2520by%2520the%2520weakest%2520device%2520and%2520inflate%2520the%2520update%2520variance.%2520Moreover%252C%2520prior%2520analyses%2520of%2520biased%2520OTA-FL%2520largely%2520address%2520convex%2520objectives%252C%2520while%2520most%2520modern%2520AI%2520models%2520are%2520highly%2520non-convex.%2520Motivated%2520by%2520these%2520gaps%252C%2520we%2520study%2520OTA-FL%2520with%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520for%2520general%2520smooth%2520non-convex%2520objectives%2520under%2520wireless%2520heterogeneity.%2520We%2520develop%2520novel%2520OTA-FL%2520SGD%2520updates%2520that%2520allow%2520a%2520structured%252C%2520time-invariant%2520model%2520bias%2520while%2520facilitating%2520reduced%2520variance%2520updates.%2520We%2520derive%2520a%2520finite-time%2520stationarity%2520bound%2520%2528expected%2520time%2520average%2520squared%2520gradient%2520norm%2529%2520that%2520explicitly%2520reveals%2520a%2520bias-variance%2520trade-off.%2520To%2520optimize%2520this%2520trade-off%252C%2520we%2520pose%2520a%2520non-convex%2520joint%2520OTA%2520power-control%2520design%2520and%2520develop%2520an%2520efficient%2520successive%2520convex%2520approximation%2520%2528SCA%2529%2520algorithm%2520that%2520requires%2520only%2520statistical%2520CSI%2520at%2520the%2520base%2520station.%2520Experiments%2520on%2520a%2520non-convex%2520image%2520classification%2520task%2520validate%2520the%2520approach%253A%2520the%2520SCA-based%2520design%2520accelerates%2520convergence%2520via%2520an%2520optimized%2520bias%2520and%2520improves%2520generalization%2520over%2520prior%2520OTA-FL%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26722v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Convex%20Over-the-Air%20Heterogeneous%20Federated%20Learning%3A%20A%20Bias-Variance%20Trade-off&entry.906535625=Muhammad%20Faraz%20Ul%20Abrar%20and%20Nicol%C3%B2%20Michelusi&entry.1292438233=Over-the-air%20%28OTA%29%20federated%20learning%20%28FL%29%20has%20been%20well%20recognized%20as%20a%20scalable%20paradigm%20that%20exploits%20the%20waveform%20superposition%20of%20the%20wireless%20multiple-access%20channel%20to%20aggregate%20model%20updates%20in%20a%20single%20use.%20Existing%20OTA-FL%20designs%20largely%20enforce%20zero-bias%20model%20updates%20by%20either%20assuming%20%5Cemph%7Bhomogeneous%7D%20wireless%20conditions%20%28equal%20path%20loss%20across%20devices%29%20or%20forcing%20zero-bias%20updates%20to%20guarantee%20convergence.%20Under%20%5Cemph%7Bheterogeneous%7D%20wireless%20scenarios%2C%20however%2C%20such%20designs%20are%20constrained%20by%20the%20weakest%20device%20and%20inflate%20the%20update%20variance.%20Moreover%2C%20prior%20analyses%20of%20biased%20OTA-FL%20largely%20address%20convex%20objectives%2C%20while%20most%20modern%20AI%20models%20are%20highly%20non-convex.%20Motivated%20by%20these%20gaps%2C%20we%20study%20OTA-FL%20with%20stochastic%20gradient%20descent%20%28SGD%29%20for%20general%20smooth%20non-convex%20objectives%20under%20wireless%20heterogeneity.%20We%20develop%20novel%20OTA-FL%20SGD%20updates%20that%20allow%20a%20structured%2C%20time-invariant%20model%20bias%20while%20facilitating%20reduced%20variance%20updates.%20We%20derive%20a%20finite-time%20stationarity%20bound%20%28expected%20time%20average%20squared%20gradient%20norm%29%20that%20explicitly%20reveals%20a%20bias-variance%20trade-off.%20To%20optimize%20this%20trade-off%2C%20we%20pose%20a%20non-convex%20joint%20OTA%20power-control%20design%20and%20develop%20an%20efficient%20successive%20convex%20approximation%20%28SCA%29%20algorithm%20that%20requires%20only%20statistical%20CSI%20at%20the%20base%20station.%20Experiments%20on%20a%20non-convex%20image%20classification%20task%20validate%20the%20approach%3A%20the%20SCA-based%20design%20accelerates%20convergence%20via%20an%20optimized%20bias%20and%20improves%20generalization%20over%20prior%20OTA-FL%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2510.26722v4&entry.124074799=Read"},
{"title": "Characterizing Trainability of Instantaneous Quantum Polynomial Circuit Born Machines", "author": "Kevin Shen and Susanne Pielawa and Vedran Dunjko and Hao Wang", "abstract": "Instantaneous quantum polynomial quantum circuit Born machines (IQP-QCBMs) have been proposed as quantum generative models with a classically tractable training objective based on the maximum mean discrepancy (MMD) and a potential quantum advantage motivated by sampling-complexity arguments, making them an exciting model worth deeper investigation. While recent works have further proven the universality of a (slightly generalized) model, the next immediate question pertains to its trainability, i.e., whether it suffers from the exponentially vanishing loss gradients, known as the barren plateau issue, preventing effective use, and how regimes of trainability overlap with regimes of possible quantum advantage. Here, we provide significant strides in these directions. To study the trainability at initialization, we analytically derive closed-form expressions for the variances of the partial derivatives of the MMD loss function and provide general upper and lower bounds. With uniform initialization, we show that barren plateaus depend on the generator set and the spectrum of the chosen kernel. We identify regimes in which low-weight-biased kernels avoid exponential gradient suppression in structured topologies. Also, we prove that a small-variance Gaussian initialization ensures polynomial scaling for the gradient under mild conditions. As for the potential quantum advantage, we further argue, based on previous complexity-theoretic arguments, that sparse IQP families can output a probability distribution family that is classically intractable, and that this distribution remains trainable at initialization at least at lower-weight frequencies.", "link": "http://arxiv.org/abs/2602.11042v2", "date": "2026-02-13", "relevancy": 1.7244, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4381}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4261}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20Trainability%20of%20Instantaneous%20Quantum%20Polynomial%20Circuit%20Born%20Machines&body=Title%3A%20Characterizing%20Trainability%20of%20Instantaneous%20Quantum%20Polynomial%20Circuit%20Born%20Machines%0AAuthor%3A%20Kevin%20Shen%20and%20Susanne%20Pielawa%20and%20Vedran%20Dunjko%20and%20Hao%20Wang%0AAbstract%3A%20Instantaneous%20quantum%20polynomial%20quantum%20circuit%20Born%20machines%20%28IQP-QCBMs%29%20have%20been%20proposed%20as%20quantum%20generative%20models%20with%20a%20classically%20tractable%20training%20objective%20based%20on%20the%20maximum%20mean%20discrepancy%20%28MMD%29%20and%20a%20potential%20quantum%20advantage%20motivated%20by%20sampling-complexity%20arguments%2C%20making%20them%20an%20exciting%20model%20worth%20deeper%20investigation.%20While%20recent%20works%20have%20further%20proven%20the%20universality%20of%20a%20%28slightly%20generalized%29%20model%2C%20the%20next%20immediate%20question%20pertains%20to%20its%20trainability%2C%20i.e.%2C%20whether%20it%20suffers%20from%20the%20exponentially%20vanishing%20loss%20gradients%2C%20known%20as%20the%20barren%20plateau%20issue%2C%20preventing%20effective%20use%2C%20and%20how%20regimes%20of%20trainability%20overlap%20with%20regimes%20of%20possible%20quantum%20advantage.%20Here%2C%20we%20provide%20significant%20strides%20in%20these%20directions.%20To%20study%20the%20trainability%20at%20initialization%2C%20we%20analytically%20derive%20closed-form%20expressions%20for%20the%20variances%20of%20the%20partial%20derivatives%20of%20the%20MMD%20loss%20function%20and%20provide%20general%20upper%20and%20lower%20bounds.%20With%20uniform%20initialization%2C%20we%20show%20that%20barren%20plateaus%20depend%20on%20the%20generator%20set%20and%20the%20spectrum%20of%20the%20chosen%20kernel.%20We%20identify%20regimes%20in%20which%20low-weight-biased%20kernels%20avoid%20exponential%20gradient%20suppression%20in%20structured%20topologies.%20Also%2C%20we%20prove%20that%20a%20small-variance%20Gaussian%20initialization%20ensures%20polynomial%20scaling%20for%20the%20gradient%20under%20mild%20conditions.%20As%20for%20the%20potential%20quantum%20advantage%2C%20we%20further%20argue%2C%20based%20on%20previous%20complexity-theoretic%20arguments%2C%20that%20sparse%20IQP%20families%20can%20output%20a%20probability%20distribution%20family%20that%20is%20classically%20intractable%2C%20and%20that%20this%20distribution%20remains%20trainable%20at%20initialization%20at%20least%20at%20lower-weight%20frequencies.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11042v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520Trainability%2520of%2520Instantaneous%2520Quantum%2520Polynomial%2520Circuit%2520Born%2520Machines%26entry.906535625%3DKevin%2520Shen%2520and%2520Susanne%2520Pielawa%2520and%2520Vedran%2520Dunjko%2520and%2520Hao%2520Wang%26entry.1292438233%3DInstantaneous%2520quantum%2520polynomial%2520quantum%2520circuit%2520Born%2520machines%2520%2528IQP-QCBMs%2529%2520have%2520been%2520proposed%2520as%2520quantum%2520generative%2520models%2520with%2520a%2520classically%2520tractable%2520training%2520objective%2520based%2520on%2520the%2520maximum%2520mean%2520discrepancy%2520%2528MMD%2529%2520and%2520a%2520potential%2520quantum%2520advantage%2520motivated%2520by%2520sampling-complexity%2520arguments%252C%2520making%2520them%2520an%2520exciting%2520model%2520worth%2520deeper%2520investigation.%2520While%2520recent%2520works%2520have%2520further%2520proven%2520the%2520universality%2520of%2520a%2520%2528slightly%2520generalized%2529%2520model%252C%2520the%2520next%2520immediate%2520question%2520pertains%2520to%2520its%2520trainability%252C%2520i.e.%252C%2520whether%2520it%2520suffers%2520from%2520the%2520exponentially%2520vanishing%2520loss%2520gradients%252C%2520known%2520as%2520the%2520barren%2520plateau%2520issue%252C%2520preventing%2520effective%2520use%252C%2520and%2520how%2520regimes%2520of%2520trainability%2520overlap%2520with%2520regimes%2520of%2520possible%2520quantum%2520advantage.%2520Here%252C%2520we%2520provide%2520significant%2520strides%2520in%2520these%2520directions.%2520To%2520study%2520the%2520trainability%2520at%2520initialization%252C%2520we%2520analytically%2520derive%2520closed-form%2520expressions%2520for%2520the%2520variances%2520of%2520the%2520partial%2520derivatives%2520of%2520the%2520MMD%2520loss%2520function%2520and%2520provide%2520general%2520upper%2520and%2520lower%2520bounds.%2520With%2520uniform%2520initialization%252C%2520we%2520show%2520that%2520barren%2520plateaus%2520depend%2520on%2520the%2520generator%2520set%2520and%2520the%2520spectrum%2520of%2520the%2520chosen%2520kernel.%2520We%2520identify%2520regimes%2520in%2520which%2520low-weight-biased%2520kernels%2520avoid%2520exponential%2520gradient%2520suppression%2520in%2520structured%2520topologies.%2520Also%252C%2520we%2520prove%2520that%2520a%2520small-variance%2520Gaussian%2520initialization%2520ensures%2520polynomial%2520scaling%2520for%2520the%2520gradient%2520under%2520mild%2520conditions.%2520As%2520for%2520the%2520potential%2520quantum%2520advantage%252C%2520we%2520further%2520argue%252C%2520based%2520on%2520previous%2520complexity-theoretic%2520arguments%252C%2520that%2520sparse%2520IQP%2520families%2520can%2520output%2520a%2520probability%2520distribution%2520family%2520that%2520is%2520classically%2520intractable%252C%2520and%2520that%2520this%2520distribution%2520remains%2520trainable%2520at%2520initialization%2520at%2520least%2520at%2520lower-weight%2520frequencies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11042v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20Trainability%20of%20Instantaneous%20Quantum%20Polynomial%20Circuit%20Born%20Machines&entry.906535625=Kevin%20Shen%20and%20Susanne%20Pielawa%20and%20Vedran%20Dunjko%20and%20Hao%20Wang&entry.1292438233=Instantaneous%20quantum%20polynomial%20quantum%20circuit%20Born%20machines%20%28IQP-QCBMs%29%20have%20been%20proposed%20as%20quantum%20generative%20models%20with%20a%20classically%20tractable%20training%20objective%20based%20on%20the%20maximum%20mean%20discrepancy%20%28MMD%29%20and%20a%20potential%20quantum%20advantage%20motivated%20by%20sampling-complexity%20arguments%2C%20making%20them%20an%20exciting%20model%20worth%20deeper%20investigation.%20While%20recent%20works%20have%20further%20proven%20the%20universality%20of%20a%20%28slightly%20generalized%29%20model%2C%20the%20next%20immediate%20question%20pertains%20to%20its%20trainability%2C%20i.e.%2C%20whether%20it%20suffers%20from%20the%20exponentially%20vanishing%20loss%20gradients%2C%20known%20as%20the%20barren%20plateau%20issue%2C%20preventing%20effective%20use%2C%20and%20how%20regimes%20of%20trainability%20overlap%20with%20regimes%20of%20possible%20quantum%20advantage.%20Here%2C%20we%20provide%20significant%20strides%20in%20these%20directions.%20To%20study%20the%20trainability%20at%20initialization%2C%20we%20analytically%20derive%20closed-form%20expressions%20for%20the%20variances%20of%20the%20partial%20derivatives%20of%20the%20MMD%20loss%20function%20and%20provide%20general%20upper%20and%20lower%20bounds.%20With%20uniform%20initialization%2C%20we%20show%20that%20barren%20plateaus%20depend%20on%20the%20generator%20set%20and%20the%20spectrum%20of%20the%20chosen%20kernel.%20We%20identify%20regimes%20in%20which%20low-weight-biased%20kernels%20avoid%20exponential%20gradient%20suppression%20in%20structured%20topologies.%20Also%2C%20we%20prove%20that%20a%20small-variance%20Gaussian%20initialization%20ensures%20polynomial%20scaling%20for%20the%20gradient%20under%20mild%20conditions.%20As%20for%20the%20potential%20quantum%20advantage%2C%20we%20further%20argue%2C%20based%20on%20previous%20complexity-theoretic%20arguments%2C%20that%20sparse%20IQP%20families%20can%20output%20a%20probability%20distribution%20family%20that%20is%20classically%20intractable%2C%20and%20that%20this%20distribution%20remains%20trainable%20at%20initialization%20at%20least%20at%20lower-weight%20frequencies.&entry.1838667208=http%3A//arxiv.org/abs/2602.11042v2&entry.124074799=Read"},
{"title": "Constrained Assumption-Based Argumentation Frameworks", "author": "Emanuele De Angelis and Fabio Fioravanti and Maria Chiara Meo and Alberto Pettorossi and Maurizio Proietti and Francesca Toni", "abstract": "Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.", "link": "http://arxiv.org/abs/2602.13135v1", "date": "2026-02-13", "relevancy": 0.8055, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4253}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3925}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20Assumption-Based%20Argumentation%20Frameworks&body=Title%3A%20Constrained%20Assumption-Based%20Argumentation%20Frameworks%0AAuthor%3A%20Emanuele%20De%20Angelis%20and%20Fabio%20Fioravanti%20and%20Maria%20Chiara%20Meo%20and%20Alberto%20Pettorossi%20and%20Maurizio%20Proietti%20and%20Francesca%20Toni%0AAbstract%3A%20Assumption-based%20Argumentation%20%28ABA%29%20is%20a%20well-established%20form%20of%20structured%20argumentation.%20ABA%20frameworks%20with%20an%20underlying%20atomic%20language%20are%20widely%20studied%2C%20but%20their%20applicability%20is%20limited%20by%20a%20representational%20restriction%20to%20ground%20%28variable-free%29%20arguments%20and%20attacks%20built%20from%20propositional%20atoms.%20In%20this%20paper%2C%20we%20lift%20this%20restriction%20and%20propose%20a%20novel%20notion%20of%20constrained%20ABA%20%28CABA%29%2C%20whose%20components%2C%20as%20well%20as%20arguments%20built%20from%20them%2C%20may%20include%20constrained%20variables%2C%20ranging%20over%20possibly%20infinite%20domains.%20We%20define%20non-ground%20semantics%20for%20CABA%2C%20in%20terms%20of%20various%20notions%20of%20non-ground%20attacks.%20We%20show%20that%20the%20new%20semantics%20conservatively%20generalise%20standard%20ABA%20semantics.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520Assumption-Based%2520Argumentation%2520Frameworks%26entry.906535625%3DEmanuele%2520De%2520Angelis%2520and%2520Fabio%2520Fioravanti%2520and%2520Maria%2520Chiara%2520Meo%2520and%2520Alberto%2520Pettorossi%2520and%2520Maurizio%2520Proietti%2520and%2520Francesca%2520Toni%26entry.1292438233%3DAssumption-based%2520Argumentation%2520%2528ABA%2529%2520is%2520a%2520well-established%2520form%2520of%2520structured%2520argumentation.%2520ABA%2520frameworks%2520with%2520an%2520underlying%2520atomic%2520language%2520are%2520widely%2520studied%252C%2520but%2520their%2520applicability%2520is%2520limited%2520by%2520a%2520representational%2520restriction%2520to%2520ground%2520%2528variable-free%2529%2520arguments%2520and%2520attacks%2520built%2520from%2520propositional%2520atoms.%2520In%2520this%2520paper%252C%2520we%2520lift%2520this%2520restriction%2520and%2520propose%2520a%2520novel%2520notion%2520of%2520constrained%2520ABA%2520%2528CABA%2529%252C%2520whose%2520components%252C%2520as%2520well%2520as%2520arguments%2520built%2520from%2520them%252C%2520may%2520include%2520constrained%2520variables%252C%2520ranging%2520over%2520possibly%2520infinite%2520domains.%2520We%2520define%2520non-ground%2520semantics%2520for%2520CABA%252C%2520in%2520terms%2520of%2520various%2520notions%2520of%2520non-ground%2520attacks.%2520We%2520show%2520that%2520the%2520new%2520semantics%2520conservatively%2520generalise%2520standard%2520ABA%2520semantics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20Assumption-Based%20Argumentation%20Frameworks&entry.906535625=Emanuele%20De%20Angelis%20and%20Fabio%20Fioravanti%20and%20Maria%20Chiara%20Meo%20and%20Alberto%20Pettorossi%20and%20Maurizio%20Proietti%20and%20Francesca%20Toni&entry.1292438233=Assumption-based%20Argumentation%20%28ABA%29%20is%20a%20well-established%20form%20of%20structured%20argumentation.%20ABA%20frameworks%20with%20an%20underlying%20atomic%20language%20are%20widely%20studied%2C%20but%20their%20applicability%20is%20limited%20by%20a%20representational%20restriction%20to%20ground%20%28variable-free%29%20arguments%20and%20attacks%20built%20from%20propositional%20atoms.%20In%20this%20paper%2C%20we%20lift%20this%20restriction%20and%20propose%20a%20novel%20notion%20of%20constrained%20ABA%20%28CABA%29%2C%20whose%20components%2C%20as%20well%20as%20arguments%20built%20from%20them%2C%20may%20include%20constrained%20variables%2C%20ranging%20over%20possibly%20infinite%20domains.%20We%20define%20non-ground%20semantics%20for%20CABA%2C%20in%20terms%20of%20various%20notions%20of%20non-ground%20attacks.%20We%20show%20that%20the%20new%20semantics%20conservatively%20generalise%20standard%20ABA%20semantics.&entry.1838667208=http%3A//arxiv.org/abs/2602.13135v1&entry.124074799=Read"},
{"title": "Privacy-Preserving Federated Learning with Verifiable Fairness Guarantees", "author": "Mohammed Himayath Ali and Mohammed Aqib Abdullah and Syed Muneer Hussain and Mohammed Mudassir Uddin and Shahnawaz Alam", "abstract": "Federated learning enables collaborative model training across distributed institutions without centralizing sensitive data; however, ensuring algorithmic fairness across heterogeneous data distributions while preserving privacy remains fundamentally unresolved. This paper introduces CryptoFair-FL, a novel cryptographic framework providing the first verifiable fairness guarantees for federated learning systems under formal security definitions. The proposed approach combines additively homomorphic encryption with secure multi-party computation to enable privacy-preserving verification of demographic parity and equalized odds metrics without revealing protected attribute distributions or individual predictions. A novel batched verification protocol reduces computational complexity from BigO(n^2) to BigO(n \\log n) while maintaining (\\dparam, \\deltap)-differential privacy with dparam = 0.5 and deltap = 10^{-6}. Theoretical analysis establishes information-theoretic lower bounds on the privacy cost of fairness verification, demonstrating that the proposed protocol achieves near-optimal privacy-fairness tradeoffs. Comprehensive experiments across four benchmark datasets (MIMIC-IV healthcare records, Adult Income, CelebA, and a novel FedFair-100 benchmark) demonstrate that CryptoFair-FL reduces fairness violations from 0.231 to 0.031 demographic parity difference while incurring only 2.3 times computational overhead compared to standard federated averaging. The framework successfully defends against attribute inference attacks, maintaining adversarial success probability below 0.05 across all tested configurations. These results establish a practical pathway for deploying fairness-aware federated learning in regulated industries requiring both privacy protection and algorithmic accountability.", "link": "http://arxiv.org/abs/2601.12447v2", "date": "2026-02-13", "relevancy": 1.8026, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4727}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.468}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy-Preserving%20Federated%20Learning%20with%20Verifiable%20Fairness%20Guarantees&body=Title%3A%20Privacy-Preserving%20Federated%20Learning%20with%20Verifiable%20Fairness%20Guarantees%0AAuthor%3A%20Mohammed%20Himayath%20Ali%20and%20Mohammed%20Aqib%20Abdullah%20and%20Syed%20Muneer%20Hussain%20and%20Mohammed%20Mudassir%20Uddin%20and%20Shahnawaz%20Alam%0AAbstract%3A%20Federated%20learning%20enables%20collaborative%20model%20training%20across%20distributed%20institutions%20without%20centralizing%20sensitive%20data%3B%20however%2C%20ensuring%20algorithmic%20fairness%20across%20heterogeneous%20data%20distributions%20while%20preserving%20privacy%20remains%20fundamentally%20unresolved.%20This%20paper%20introduces%20CryptoFair-FL%2C%20a%20novel%20cryptographic%20framework%20providing%20the%20first%20verifiable%20fairness%20guarantees%20for%20federated%20learning%20systems%20under%20formal%20security%20definitions.%20The%20proposed%20approach%20combines%20additively%20homomorphic%20encryption%20with%20secure%20multi-party%20computation%20to%20enable%20privacy-preserving%20verification%20of%20demographic%20parity%20and%20equalized%20odds%20metrics%20without%20revealing%20protected%20attribute%20distributions%20or%20individual%20predictions.%20A%20novel%20batched%20verification%20protocol%20reduces%20computational%20complexity%20from%20BigO%28n%5E2%29%20to%20BigO%28n%20%5Clog%20n%29%20while%20maintaining%20%28%5Cdparam%2C%20%5Cdeltap%29-differential%20privacy%20with%20dparam%20%3D%200.5%20and%20deltap%20%3D%2010%5E%7B-6%7D.%20Theoretical%20analysis%20establishes%20information-theoretic%20lower%20bounds%20on%20the%20privacy%20cost%20of%20fairness%20verification%2C%20demonstrating%20that%20the%20proposed%20protocol%20achieves%20near-optimal%20privacy-fairness%20tradeoffs.%20Comprehensive%20experiments%20across%20four%20benchmark%20datasets%20%28MIMIC-IV%20healthcare%20records%2C%20Adult%20Income%2C%20CelebA%2C%20and%20a%20novel%20FedFair-100%20benchmark%29%20demonstrate%20that%20CryptoFair-FL%20reduces%20fairness%20violations%20from%200.231%20to%200.031%20demographic%20parity%20difference%20while%20incurring%20only%202.3%20times%20computational%20overhead%20compared%20to%20standard%20federated%20averaging.%20The%20framework%20successfully%20defends%20against%20attribute%20inference%20attacks%2C%20maintaining%20adversarial%20success%20probability%20below%200.05%20across%20all%20tested%20configurations.%20These%20results%20establish%20a%20practical%20pathway%20for%20deploying%20fairness-aware%20federated%20learning%20in%20regulated%20industries%20requiring%20both%20privacy%20protection%20and%20algorithmic%20accountability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.12447v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy-Preserving%2520Federated%2520Learning%2520with%2520Verifiable%2520Fairness%2520Guarantees%26entry.906535625%3DMohammed%2520Himayath%2520Ali%2520and%2520Mohammed%2520Aqib%2520Abdullah%2520and%2520Syed%2520Muneer%2520Hussain%2520and%2520Mohammed%2520Mudassir%2520Uddin%2520and%2520Shahnawaz%2520Alam%26entry.1292438233%3DFederated%2520learning%2520enables%2520collaborative%2520model%2520training%2520across%2520distributed%2520institutions%2520without%2520centralizing%2520sensitive%2520data%253B%2520however%252C%2520ensuring%2520algorithmic%2520fairness%2520across%2520heterogeneous%2520data%2520distributions%2520while%2520preserving%2520privacy%2520remains%2520fundamentally%2520unresolved.%2520This%2520paper%2520introduces%2520CryptoFair-FL%252C%2520a%2520novel%2520cryptographic%2520framework%2520providing%2520the%2520first%2520verifiable%2520fairness%2520guarantees%2520for%2520federated%2520learning%2520systems%2520under%2520formal%2520security%2520definitions.%2520The%2520proposed%2520approach%2520combines%2520additively%2520homomorphic%2520encryption%2520with%2520secure%2520multi-party%2520computation%2520to%2520enable%2520privacy-preserving%2520verification%2520of%2520demographic%2520parity%2520and%2520equalized%2520odds%2520metrics%2520without%2520revealing%2520protected%2520attribute%2520distributions%2520or%2520individual%2520predictions.%2520A%2520novel%2520batched%2520verification%2520protocol%2520reduces%2520computational%2520complexity%2520from%2520BigO%2528n%255E2%2529%2520to%2520BigO%2528n%2520%255Clog%2520n%2529%2520while%2520maintaining%2520%2528%255Cdparam%252C%2520%255Cdeltap%2529-differential%2520privacy%2520with%2520dparam%2520%253D%25200.5%2520and%2520deltap%2520%253D%252010%255E%257B-6%257D.%2520Theoretical%2520analysis%2520establishes%2520information-theoretic%2520lower%2520bounds%2520on%2520the%2520privacy%2520cost%2520of%2520fairness%2520verification%252C%2520demonstrating%2520that%2520the%2520proposed%2520protocol%2520achieves%2520near-optimal%2520privacy-fairness%2520tradeoffs.%2520Comprehensive%2520experiments%2520across%2520four%2520benchmark%2520datasets%2520%2528MIMIC-IV%2520healthcare%2520records%252C%2520Adult%2520Income%252C%2520CelebA%252C%2520and%2520a%2520novel%2520FedFair-100%2520benchmark%2529%2520demonstrate%2520that%2520CryptoFair-FL%2520reduces%2520fairness%2520violations%2520from%25200.231%2520to%25200.031%2520demographic%2520parity%2520difference%2520while%2520incurring%2520only%25202.3%2520times%2520computational%2520overhead%2520compared%2520to%2520standard%2520federated%2520averaging.%2520The%2520framework%2520successfully%2520defends%2520against%2520attribute%2520inference%2520attacks%252C%2520maintaining%2520adversarial%2520success%2520probability%2520below%25200.05%2520across%2520all%2520tested%2520configurations.%2520These%2520results%2520establish%2520a%2520practical%2520pathway%2520for%2520deploying%2520fairness-aware%2520federated%2520learning%2520in%2520regulated%2520industries%2520requiring%2520both%2520privacy%2520protection%2520and%2520algorithmic%2520accountability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.12447v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy-Preserving%20Federated%20Learning%20with%20Verifiable%20Fairness%20Guarantees&entry.906535625=Mohammed%20Himayath%20Ali%20and%20Mohammed%20Aqib%20Abdullah%20and%20Syed%20Muneer%20Hussain%20and%20Mohammed%20Mudassir%20Uddin%20and%20Shahnawaz%20Alam&entry.1292438233=Federated%20learning%20enables%20collaborative%20model%20training%20across%20distributed%20institutions%20without%20centralizing%20sensitive%20data%3B%20however%2C%20ensuring%20algorithmic%20fairness%20across%20heterogeneous%20data%20distributions%20while%20preserving%20privacy%20remains%20fundamentally%20unresolved.%20This%20paper%20introduces%20CryptoFair-FL%2C%20a%20novel%20cryptographic%20framework%20providing%20the%20first%20verifiable%20fairness%20guarantees%20for%20federated%20learning%20systems%20under%20formal%20security%20definitions.%20The%20proposed%20approach%20combines%20additively%20homomorphic%20encryption%20with%20secure%20multi-party%20computation%20to%20enable%20privacy-preserving%20verification%20of%20demographic%20parity%20and%20equalized%20odds%20metrics%20without%20revealing%20protected%20attribute%20distributions%20or%20individual%20predictions.%20A%20novel%20batched%20verification%20protocol%20reduces%20computational%20complexity%20from%20BigO%28n%5E2%29%20to%20BigO%28n%20%5Clog%20n%29%20while%20maintaining%20%28%5Cdparam%2C%20%5Cdeltap%29-differential%20privacy%20with%20dparam%20%3D%200.5%20and%20deltap%20%3D%2010%5E%7B-6%7D.%20Theoretical%20analysis%20establishes%20information-theoretic%20lower%20bounds%20on%20the%20privacy%20cost%20of%20fairness%20verification%2C%20demonstrating%20that%20the%20proposed%20protocol%20achieves%20near-optimal%20privacy-fairness%20tradeoffs.%20Comprehensive%20experiments%20across%20four%20benchmark%20datasets%20%28MIMIC-IV%20healthcare%20records%2C%20Adult%20Income%2C%20CelebA%2C%20and%20a%20novel%20FedFair-100%20benchmark%29%20demonstrate%20that%20CryptoFair-FL%20reduces%20fairness%20violations%20from%200.231%20to%200.031%20demographic%20parity%20difference%20while%20incurring%20only%202.3%20times%20computational%20overhead%20compared%20to%20standard%20federated%20averaging.%20The%20framework%20successfully%20defends%20against%20attribute%20inference%20attacks%2C%20maintaining%20adversarial%20success%20probability%20below%200.05%20across%20all%20tested%20configurations.%20These%20results%20establish%20a%20practical%20pathway%20for%20deploying%20fairness-aware%20federated%20learning%20in%20regulated%20industries%20requiring%20both%20privacy%20protection%20and%20algorithmic%20accountability.&entry.1838667208=http%3A//arxiv.org/abs/2601.12447v2&entry.124074799=Read"},
{"title": "Quantization-Aware Collaborative Inference for Large Embodied AI Models", "author": "Zhonghao Lyu and Ming Xiao and Mikael Skoglund and Merouane Debbah and H. Vincent Poor", "abstract": "Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.", "link": "http://arxiv.org/abs/2602.13052v1", "date": "2026-02-13", "relevancy": 1.0068, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5245}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4978}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantization-Aware%20Collaborative%20Inference%20for%20Large%20Embodied%20AI%20Models&body=Title%3A%20Quantization-Aware%20Collaborative%20Inference%20for%20Large%20Embodied%20AI%20Models%0AAuthor%3A%20Zhonghao%20Lyu%20and%20Ming%20Xiao%20and%20Mikael%20Skoglund%20and%20Merouane%20Debbah%20and%20H.%20Vincent%20Poor%0AAbstract%3A%20Large%20artificial%20intelligence%20models%20%28LAIMs%29%20are%20increasingly%20regarded%20as%20a%20core%20intelligence%20engine%20for%20embodied%20AI%20applications.%20However%2C%20the%20massive%20parameter%20scale%20and%20computational%20demands%20of%20LAIMs%20pose%20significant%20challenges%20for%20resource-limited%20embodied%20agents.%20To%20address%20this%20issue%2C%20we%20investigate%20quantization-aware%20collaborative%20inference%20%28co-inference%29%20for%20embodied%20AI%20systems.%20First%2C%20we%20develop%20a%20tractable%20approximation%20for%20quantization-induced%20inference%20distortion.%20Based%20on%20this%20approximation%2C%20we%20derive%20lower%20and%20upper%20bounds%20on%20the%20quantization%20rate-inference%20distortion%20function%2C%20characterizing%20its%20dependence%20on%20LAIM%20statistics%2C%20including%20the%20quantization%20bit-width.%20Next%2C%20we%20formulate%20a%20joint%20quantization%20bit-width%20and%20computation%20frequency%20design%20problem%20under%20delay%20and%20energy%20constraints%2C%20aiming%20to%20minimize%20the%20distortion%20upper%20bound%20while%20ensuring%20tightness%20through%20the%20corresponding%20lower%20bound.%20Extensive%20evaluations%20validate%20the%20proposed%20distortion%20approximation%2C%20the%20derived%20rate-distortion%20bounds%2C%20and%20the%20effectiveness%20of%20the%20proposed%20joint%20design.%20Particularly%2C%20simulations%20and%20real-world%20testbed%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20joint%20design%20in%20balancing%20inference%20quality%2C%20latency%2C%20and%20energy%20consumption%20in%20edge%20embodied%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantization-Aware%2520Collaborative%2520Inference%2520for%2520Large%2520Embodied%2520AI%2520Models%26entry.906535625%3DZhonghao%2520Lyu%2520and%2520Ming%2520Xiao%2520and%2520Mikael%2520Skoglund%2520and%2520Merouane%2520Debbah%2520and%2520H.%2520Vincent%2520Poor%26entry.1292438233%3DLarge%2520artificial%2520intelligence%2520models%2520%2528LAIMs%2529%2520are%2520increasingly%2520regarded%2520as%2520a%2520core%2520intelligence%2520engine%2520for%2520embodied%2520AI%2520applications.%2520However%252C%2520the%2520massive%2520parameter%2520scale%2520and%2520computational%2520demands%2520of%2520LAIMs%2520pose%2520significant%2520challenges%2520for%2520resource-limited%2520embodied%2520agents.%2520To%2520address%2520this%2520issue%252C%2520we%2520investigate%2520quantization-aware%2520collaborative%2520inference%2520%2528co-inference%2529%2520for%2520embodied%2520AI%2520systems.%2520First%252C%2520we%2520develop%2520a%2520tractable%2520approximation%2520for%2520quantization-induced%2520inference%2520distortion.%2520Based%2520on%2520this%2520approximation%252C%2520we%2520derive%2520lower%2520and%2520upper%2520bounds%2520on%2520the%2520quantization%2520rate-inference%2520distortion%2520function%252C%2520characterizing%2520its%2520dependence%2520on%2520LAIM%2520statistics%252C%2520including%2520the%2520quantization%2520bit-width.%2520Next%252C%2520we%2520formulate%2520a%2520joint%2520quantization%2520bit-width%2520and%2520computation%2520frequency%2520design%2520problem%2520under%2520delay%2520and%2520energy%2520constraints%252C%2520aiming%2520to%2520minimize%2520the%2520distortion%2520upper%2520bound%2520while%2520ensuring%2520tightness%2520through%2520the%2520corresponding%2520lower%2520bound.%2520Extensive%2520evaluations%2520validate%2520the%2520proposed%2520distortion%2520approximation%252C%2520the%2520derived%2520rate-distortion%2520bounds%252C%2520and%2520the%2520effectiveness%2520of%2520the%2520proposed%2520joint%2520design.%2520Particularly%252C%2520simulations%2520and%2520real-world%2520testbed%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520joint%2520design%2520in%2520balancing%2520inference%2520quality%252C%2520latency%252C%2520and%2520energy%2520consumption%2520in%2520edge%2520embodied%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantization-Aware%20Collaborative%20Inference%20for%20Large%20Embodied%20AI%20Models&entry.906535625=Zhonghao%20Lyu%20and%20Ming%20Xiao%20and%20Mikael%20Skoglund%20and%20Merouane%20Debbah%20and%20H.%20Vincent%20Poor&entry.1292438233=Large%20artificial%20intelligence%20models%20%28LAIMs%29%20are%20increasingly%20regarded%20as%20a%20core%20intelligence%20engine%20for%20embodied%20AI%20applications.%20However%2C%20the%20massive%20parameter%20scale%20and%20computational%20demands%20of%20LAIMs%20pose%20significant%20challenges%20for%20resource-limited%20embodied%20agents.%20To%20address%20this%20issue%2C%20we%20investigate%20quantization-aware%20collaborative%20inference%20%28co-inference%29%20for%20embodied%20AI%20systems.%20First%2C%20we%20develop%20a%20tractable%20approximation%20for%20quantization-induced%20inference%20distortion.%20Based%20on%20this%20approximation%2C%20we%20derive%20lower%20and%20upper%20bounds%20on%20the%20quantization%20rate-inference%20distortion%20function%2C%20characterizing%20its%20dependence%20on%20LAIM%20statistics%2C%20including%20the%20quantization%20bit-width.%20Next%2C%20we%20formulate%20a%20joint%20quantization%20bit-width%20and%20computation%20frequency%20design%20problem%20under%20delay%20and%20energy%20constraints%2C%20aiming%20to%20minimize%20the%20distortion%20upper%20bound%20while%20ensuring%20tightness%20through%20the%20corresponding%20lower%20bound.%20Extensive%20evaluations%20validate%20the%20proposed%20distortion%20approximation%2C%20the%20derived%20rate-distortion%20bounds%2C%20and%20the%20effectiveness%20of%20the%20proposed%20joint%20design.%20Particularly%2C%20simulations%20and%20real-world%20testbed%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20joint%20design%20in%20balancing%20inference%20quality%2C%20latency%2C%20and%20energy%20consumption%20in%20edge%20embodied%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2602.13052v1&entry.124074799=Read"},
{"title": "Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System", "author": "Zhenhua Zou and Sheng Guo and Qiuyang Zhan and Lepeng Zhao and Shuo Li and Qi Li and Ke Xu and Mingwei Xu and Zhuotao Liu", "abstract": "The evolution of Large Language Models (LLMs) has shifted mobile computing from App-centric interactions to system-level autonomous agents. Current implementations predominantly rely on a \"Screen-as-Interface\" paradigm, which inherits structural vulnerabilities and conflicts with the mobile ecosystem's economic foundations. In this paper, we conduct a systematic security analysis of state-of-the-art mobile agents using Doubao Mobile Assistant as a representative case. We decompose the threat landscape into four dimensions - Agent Identity, External Interface, Internal Reasoning, and Action Execution - revealing critical flaws such as fake App identity, visual spoofing, indirect prompt injection, and unauthorized privilege escalation stemming from a reliance on unstructured visual data.\n  To address these challenges, we propose Aura, an Agent Universal Runtime Architecture for a clean-slate secure agent OS. Aura replaces brittle GUI scraping with a structured, agent-native interaction model. It adopts a Hub-and-Spoke topology where a privileged System Agent orchestrates intent, sandboxed App Agents execute domain-specific tasks, and the Agent Kernel mediates all communication. The Agent Kernel enforces four defense pillars: (i) cryptographic identity binding via a Global Agent Registry; (ii) semantic input sanitization through a multilayer Semantic Firewall; (iii) cognitive integrity via taint-aware memory and plan-trajectory alignment; and (iv) granular access control with non-deniable auditing. Evaluation on MobileSafetyBench shows that, compared to Doubao, Aura improves low-risk Task Success Rate from roughly 75% to 94.3%, reduces high-risk Attack Success Rate from roughly 40% to 4.4%, and achieves near-order-of-magnitude latency gains. These results demonstrate Aura as a viable, secure alternative to the \"Screen-as-Interface\" paradigm.", "link": "http://arxiv.org/abs/2602.10915v3", "date": "2026-02-13", "relevancy": 1.3861, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4677}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4604}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blind%20Gods%20and%20Broken%20Screens%3A%20Architecting%20a%20Secure%2C%20Intent-Centric%20Mobile%20Agent%20Operating%20System&body=Title%3A%20Blind%20Gods%20and%20Broken%20Screens%3A%20Architecting%20a%20Secure%2C%20Intent-Centric%20Mobile%20Agent%20Operating%20System%0AAuthor%3A%20Zhenhua%20Zou%20and%20Sheng%20Guo%20and%20Qiuyang%20Zhan%20and%20Lepeng%20Zhao%20and%20Shuo%20Li%20and%20Qi%20Li%20and%20Ke%20Xu%20and%20Mingwei%20Xu%20and%20Zhuotao%20Liu%0AAbstract%3A%20The%20evolution%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20shifted%20mobile%20computing%20from%20App-centric%20interactions%20to%20system-level%20autonomous%20agents.%20Current%20implementations%20predominantly%20rely%20on%20a%20%22Screen-as-Interface%22%20paradigm%2C%20which%20inherits%20structural%20vulnerabilities%20and%20conflicts%20with%20the%20mobile%20ecosystem%27s%20economic%20foundations.%20In%20this%20paper%2C%20we%20conduct%20a%20systematic%20security%20analysis%20of%20state-of-the-art%20mobile%20agents%20using%20Doubao%20Mobile%20Assistant%20as%20a%20representative%20case.%20We%20decompose%20the%20threat%20landscape%20into%20four%20dimensions%20-%20Agent%20Identity%2C%20External%20Interface%2C%20Internal%20Reasoning%2C%20and%20Action%20Execution%20-%20revealing%20critical%20flaws%20such%20as%20fake%20App%20identity%2C%20visual%20spoofing%2C%20indirect%20prompt%20injection%2C%20and%20unauthorized%20privilege%20escalation%20stemming%20from%20a%20reliance%20on%20unstructured%20visual%20data.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20Aura%2C%20an%20Agent%20Universal%20Runtime%20Architecture%20for%20a%20clean-slate%20secure%20agent%20OS.%20Aura%20replaces%20brittle%20GUI%20scraping%20with%20a%20structured%2C%20agent-native%20interaction%20model.%20It%20adopts%20a%20Hub-and-Spoke%20topology%20where%20a%20privileged%20System%20Agent%20orchestrates%20intent%2C%20sandboxed%20App%20Agents%20execute%20domain-specific%20tasks%2C%20and%20the%20Agent%20Kernel%20mediates%20all%20communication.%20The%20Agent%20Kernel%20enforces%20four%20defense%20pillars%3A%20%28i%29%20cryptographic%20identity%20binding%20via%20a%20Global%20Agent%20Registry%3B%20%28ii%29%20semantic%20input%20sanitization%20through%20a%20multilayer%20Semantic%20Firewall%3B%20%28iii%29%20cognitive%20integrity%20via%20taint-aware%20memory%20and%20plan-trajectory%20alignment%3B%20and%20%28iv%29%20granular%20access%20control%20with%20non-deniable%20auditing.%20Evaluation%20on%20MobileSafetyBench%20shows%20that%2C%20compared%20to%20Doubao%2C%20Aura%20improves%20low-risk%20Task%20Success%20Rate%20from%20roughly%2075%25%20to%2094.3%25%2C%20reduces%20high-risk%20Attack%20Success%20Rate%20from%20roughly%2040%25%20to%204.4%25%2C%20and%20achieves%20near-order-of-magnitude%20latency%20gains.%20These%20results%20demonstrate%20Aura%20as%20a%20viable%2C%20secure%20alternative%20to%20the%20%22Screen-as-Interface%22%20paradigm.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10915v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlind%2520Gods%2520and%2520Broken%2520Screens%253A%2520Architecting%2520a%2520Secure%252C%2520Intent-Centric%2520Mobile%2520Agent%2520Operating%2520System%26entry.906535625%3DZhenhua%2520Zou%2520and%2520Sheng%2520Guo%2520and%2520Qiuyang%2520Zhan%2520and%2520Lepeng%2520Zhao%2520and%2520Shuo%2520Li%2520and%2520Qi%2520Li%2520and%2520Ke%2520Xu%2520and%2520Mingwei%2520Xu%2520and%2520Zhuotao%2520Liu%26entry.1292438233%3DThe%2520evolution%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520shifted%2520mobile%2520computing%2520from%2520App-centric%2520interactions%2520to%2520system-level%2520autonomous%2520agents.%2520Current%2520implementations%2520predominantly%2520rely%2520on%2520a%2520%2522Screen-as-Interface%2522%2520paradigm%252C%2520which%2520inherits%2520structural%2520vulnerabilities%2520and%2520conflicts%2520with%2520the%2520mobile%2520ecosystem%2527s%2520economic%2520foundations.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520systematic%2520security%2520analysis%2520of%2520state-of-the-art%2520mobile%2520agents%2520using%2520Doubao%2520Mobile%2520Assistant%2520as%2520a%2520representative%2520case.%2520We%2520decompose%2520the%2520threat%2520landscape%2520into%2520four%2520dimensions%2520-%2520Agent%2520Identity%252C%2520External%2520Interface%252C%2520Internal%2520Reasoning%252C%2520and%2520Action%2520Execution%2520-%2520revealing%2520critical%2520flaws%2520such%2520as%2520fake%2520App%2520identity%252C%2520visual%2520spoofing%252C%2520indirect%2520prompt%2520injection%252C%2520and%2520unauthorized%2520privilege%2520escalation%2520stemming%2520from%2520a%2520reliance%2520on%2520unstructured%2520visual%2520data.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Aura%252C%2520an%2520Agent%2520Universal%2520Runtime%2520Architecture%2520for%2520a%2520clean-slate%2520secure%2520agent%2520OS.%2520Aura%2520replaces%2520brittle%2520GUI%2520scraping%2520with%2520a%2520structured%252C%2520agent-native%2520interaction%2520model.%2520It%2520adopts%2520a%2520Hub-and-Spoke%2520topology%2520where%2520a%2520privileged%2520System%2520Agent%2520orchestrates%2520intent%252C%2520sandboxed%2520App%2520Agents%2520execute%2520domain-specific%2520tasks%252C%2520and%2520the%2520Agent%2520Kernel%2520mediates%2520all%2520communication.%2520The%2520Agent%2520Kernel%2520enforces%2520four%2520defense%2520pillars%253A%2520%2528i%2529%2520cryptographic%2520identity%2520binding%2520via%2520a%2520Global%2520Agent%2520Registry%253B%2520%2528ii%2529%2520semantic%2520input%2520sanitization%2520through%2520a%2520multilayer%2520Semantic%2520Firewall%253B%2520%2528iii%2529%2520cognitive%2520integrity%2520via%2520taint-aware%2520memory%2520and%2520plan-trajectory%2520alignment%253B%2520and%2520%2528iv%2529%2520granular%2520access%2520control%2520with%2520non-deniable%2520auditing.%2520Evaluation%2520on%2520MobileSafetyBench%2520shows%2520that%252C%2520compared%2520to%2520Doubao%252C%2520Aura%2520improves%2520low-risk%2520Task%2520Success%2520Rate%2520from%2520roughly%252075%2525%2520to%252094.3%2525%252C%2520reduces%2520high-risk%2520Attack%2520Success%2520Rate%2520from%2520roughly%252040%2525%2520to%25204.4%2525%252C%2520and%2520achieves%2520near-order-of-magnitude%2520latency%2520gains.%2520These%2520results%2520demonstrate%2520Aura%2520as%2520a%2520viable%252C%2520secure%2520alternative%2520to%2520the%2520%2522Screen-as-Interface%2522%2520paradigm.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10915v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blind%20Gods%20and%20Broken%20Screens%3A%20Architecting%20a%20Secure%2C%20Intent-Centric%20Mobile%20Agent%20Operating%20System&entry.906535625=Zhenhua%20Zou%20and%20Sheng%20Guo%20and%20Qiuyang%20Zhan%20and%20Lepeng%20Zhao%20and%20Shuo%20Li%20and%20Qi%20Li%20and%20Ke%20Xu%20and%20Mingwei%20Xu%20and%20Zhuotao%20Liu&entry.1292438233=The%20evolution%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20shifted%20mobile%20computing%20from%20App-centric%20interactions%20to%20system-level%20autonomous%20agents.%20Current%20implementations%20predominantly%20rely%20on%20a%20%22Screen-as-Interface%22%20paradigm%2C%20which%20inherits%20structural%20vulnerabilities%20and%20conflicts%20with%20the%20mobile%20ecosystem%27s%20economic%20foundations.%20In%20this%20paper%2C%20we%20conduct%20a%20systematic%20security%20analysis%20of%20state-of-the-art%20mobile%20agents%20using%20Doubao%20Mobile%20Assistant%20as%20a%20representative%20case.%20We%20decompose%20the%20threat%20landscape%20into%20four%20dimensions%20-%20Agent%20Identity%2C%20External%20Interface%2C%20Internal%20Reasoning%2C%20and%20Action%20Execution%20-%20revealing%20critical%20flaws%20such%20as%20fake%20App%20identity%2C%20visual%20spoofing%2C%20indirect%20prompt%20injection%2C%20and%20unauthorized%20privilege%20escalation%20stemming%20from%20a%20reliance%20on%20unstructured%20visual%20data.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20Aura%2C%20an%20Agent%20Universal%20Runtime%20Architecture%20for%20a%20clean-slate%20secure%20agent%20OS.%20Aura%20replaces%20brittle%20GUI%20scraping%20with%20a%20structured%2C%20agent-native%20interaction%20model.%20It%20adopts%20a%20Hub-and-Spoke%20topology%20where%20a%20privileged%20System%20Agent%20orchestrates%20intent%2C%20sandboxed%20App%20Agents%20execute%20domain-specific%20tasks%2C%20and%20the%20Agent%20Kernel%20mediates%20all%20communication.%20The%20Agent%20Kernel%20enforces%20four%20defense%20pillars%3A%20%28i%29%20cryptographic%20identity%20binding%20via%20a%20Global%20Agent%20Registry%3B%20%28ii%29%20semantic%20input%20sanitization%20through%20a%20multilayer%20Semantic%20Firewall%3B%20%28iii%29%20cognitive%20integrity%20via%20taint-aware%20memory%20and%20plan-trajectory%20alignment%3B%20and%20%28iv%29%20granular%20access%20control%20with%20non-deniable%20auditing.%20Evaluation%20on%20MobileSafetyBench%20shows%20that%2C%20compared%20to%20Doubao%2C%20Aura%20improves%20low-risk%20Task%20Success%20Rate%20from%20roughly%2075%25%20to%2094.3%25%2C%20reduces%20high-risk%20Attack%20Success%20Rate%20from%20roughly%2040%25%20to%204.4%25%2C%20and%20achieves%20near-order-of-magnitude%20latency%20gains.%20These%20results%20demonstrate%20Aura%20as%20a%20viable%2C%20secure%20alternative%20to%20the%20%22Screen-as-Interface%22%20paradigm.&entry.1838667208=http%3A//arxiv.org/abs/2602.10915v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


