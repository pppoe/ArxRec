<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240504.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "author": "Shengjie Ma and Yanlin Weng and Tianjia Shao and Kun Zhou", "abstract": "  We introduce 3D Gaussian blendshapes for modeling photorealistic head\navatars. Taking a monocular video as input, we learn a base head model of\nneutral expression, along with a group of expression blendshapes, each of which\ncorresponds to a basis expression in classical parametric face models. Both the\nneutral model and expression blendshapes are represented as 3D Gaussians, which\ncontain a few properties to depict the avatar appearance. The avatar model of\nan arbitrary expression can be effectively generated by combining the neutral\nmodel and expression blendshapes through linear blending of Gaussians with the\nexpression coefficients. High-fidelity head avatar animations can be\nsynthesized in real time using Gaussian splatting. Compared to state-of-the-art\nmethods, our Gaussian blendshape representation better captures high-frequency\ndetails exhibited in input video, and achieves superior rendering performance.\n", "link": "http://arxiv.org/abs/2404.19398v2", "date": "2024-05-02", "relevancy": 4.3328, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 1.0}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 1.0}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Gaussian%20Blendshapes%20for%20Head%20Avatar%20Animation&body=Title%3A%203D%20Gaussian%20Blendshapes%20for%20Head%20Avatar%20Animation%0AAuthor%3A%20Shengjie%20Ma%20and%20Yanlin%20Weng%20and%20Tianjia%20Shao%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20We%20introduce%203D%20Gaussian%20blendshapes%20for%20modeling%20photorealistic%20head%0Aavatars.%20Taking%20a%20monocular%20video%20as%20input%2C%20we%20learn%20a%20base%20head%20model%20of%0Aneutral%20expression%2C%20along%20with%20a%20group%20of%20expression%20blendshapes%2C%20each%20of%20which%0Acorresponds%20to%20a%20basis%20expression%20in%20classical%20parametric%20face%20models.%20Both%20the%0Aneutral%20model%20and%20expression%20blendshapes%20are%20represented%20as%203D%20Gaussians%2C%20which%0Acontain%20a%20few%20properties%20to%20depict%20the%20avatar%20appearance.%20The%20avatar%20model%20of%0Aan%20arbitrary%20expression%20can%20be%20effectively%20generated%20by%20combining%20the%20neutral%0Amodel%20and%20expression%20blendshapes%20through%20linear%20blending%20of%20Gaussians%20with%20the%0Aexpression%20coefficients.%20High-fidelity%20head%20avatar%20animations%20can%20be%0Asynthesized%20in%20real%20time%20using%20Gaussian%20splatting.%20Compared%20to%20state-of-the-art%0Amethods%2C%20our%20Gaussian%20blendshape%20representation%20better%20captures%20high-frequency%0Adetails%20exhibited%20in%20input%20video%2C%20and%20achieves%20superior%20rendering%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19398v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Gaussian%2520Blendshapes%2520for%2520Head%2520Avatar%2520Animation%26entry.906535625%3DShengjie%2520Ma%2520and%2520Yanlin%2520Weng%2520and%2520Tianjia%2520Shao%2520and%2520Kun%2520Zhou%26entry.1292438233%3D%2520%2520We%2520introduce%25203D%2520Gaussian%2520blendshapes%2520for%2520modeling%2520photorealistic%2520head%250Aavatars.%2520Taking%2520a%2520monocular%2520video%2520as%2520input%252C%2520we%2520learn%2520a%2520base%2520head%2520model%2520of%250Aneutral%2520expression%252C%2520along%2520with%2520a%2520group%2520of%2520expression%2520blendshapes%252C%2520each%2520of%2520which%250Acorresponds%2520to%2520a%2520basis%2520expression%2520in%2520classical%2520parametric%2520face%2520models.%2520Both%2520the%250Aneutral%2520model%2520and%2520expression%2520blendshapes%2520are%2520represented%2520as%25203D%2520Gaussians%252C%2520which%250Acontain%2520a%2520few%2520properties%2520to%2520depict%2520the%2520avatar%2520appearance.%2520The%2520avatar%2520model%2520of%250Aan%2520arbitrary%2520expression%2520can%2520be%2520effectively%2520generated%2520by%2520combining%2520the%2520neutral%250Amodel%2520and%2520expression%2520blendshapes%2520through%2520linear%2520blending%2520of%2520Gaussians%2520with%2520the%250Aexpression%2520coefficients.%2520High-fidelity%2520head%2520avatar%2520animations%2520can%2520be%250Asynthesized%2520in%2520real%2520time%2520using%2520Gaussian%2520splatting.%2520Compared%2520to%2520state-of-the-art%250Amethods%252C%2520our%2520Gaussian%2520blendshape%2520representation%2520better%2520captures%2520high-frequency%250Adetails%2520exhibited%2520in%2520input%2520video%252C%2520and%2520achieves%2520superior%2520rendering%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19398v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Gaussian%20Blendshapes%20for%20Head%20Avatar%20Animation&entry.906535625=Shengjie%20Ma%20and%20Yanlin%20Weng%20and%20Tianjia%20Shao%20and%20Kun%20Zhou&entry.1292438233=%20%20We%20introduce%203D%20Gaussian%20blendshapes%20for%20modeling%20photorealistic%20head%0Aavatars.%20Taking%20a%20monocular%20video%20as%20input%2C%20we%20learn%20a%20base%20head%20model%20of%0Aneutral%20expression%2C%20along%20with%20a%20group%20of%20expression%20blendshapes%2C%20each%20of%20which%0Acorresponds%20to%20a%20basis%20expression%20in%20classical%20parametric%20face%20models.%20Both%20the%0Aneutral%20model%20and%20expression%20blendshapes%20are%20represented%20as%203D%20Gaussians%2C%20which%0Acontain%20a%20few%20properties%20to%20depict%20the%20avatar%20appearance.%20The%20avatar%20model%20of%0Aan%20arbitrary%20expression%20can%20be%20effectively%20generated%20by%20combining%20the%20neutral%0Amodel%20and%20expression%20blendshapes%20through%20linear%20blending%20of%20Gaussians%20with%20the%0Aexpression%20coefficients.%20High-fidelity%20head%20avatar%20animations%20can%20be%0Asynthesized%20in%20real%20time%20using%20Gaussian%20splatting.%20Compared%20to%20state-of-the-art%0Amethods%2C%20our%20Gaussian%20blendshape%20representation%20better%20captures%20high-frequency%0Adetails%20exhibited%20in%20input%20video%2C%20and%20achieves%20superior%20rendering%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19398v2&entry.124074799=Read"},
{"title": "Compact 3D Scene Representation via Self-Organizing Gaussian Grids", "author": "Wieland Morgenstern and Florian Barthel and Anna Hilsmann and Peter Eisert", "abstract": "  3D Gaussian Splatting has recently emerged as a highly promising technique\nfor modeling of static 3D scenes. In contrast to Neural Radiance Fields, it\nutilizes efficient rasterization allowing for very fast rendering at\nhigh-quality. However, the storage size is significantly higher, which hinders\npractical deployment, e.g. on resource constrained devices. In this paper, we\nintroduce a compact scene representation organizing the parameters of 3D\nGaussian Splatting (3DGS) into a 2D grid with local homogeneity, ensuring a\ndrastic reduction in storage requirements without compromising visual quality\nduring rendering. Central to our idea is the explicit exploitation of\nperceptual redundancies present in natural scenes. In essence, the inherent\nnature of a scene allows for numerous permutations of Gaussian parameters to\nequivalently represent it. To this end, we propose a novel highly parallel\nalgorithm that regularly arranges the high-dimensional Gaussian parameters into\na 2D grid while preserving their neighborhood structure. During training, we\nfurther enforce local smoothness between the sorted parameters in the grid. The\nuncompressed Gaussians use the same structure as 3DGS, ensuring a seamless\nintegration with established renderers. Our method achieves a reduction factor\nof 17x to 42x in size for complex scenes with no increase in training time,\nmarking a substantial leap forward in the domain of 3D scene distribution and\nconsumption. Additional information can be found on our project page:\nhttps://fraunhoferhhi.github.io/Self-Organizing-Gaussians/\n", "link": "http://arxiv.org/abs/2312.13299v2", "date": "2024-05-02", "relevancy": 3.2235, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7224}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6589}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compact%203D%20Scene%20Representation%20via%20Self-Organizing%20Gaussian%20Grids&body=Title%3A%20Compact%203D%20Scene%20Representation%20via%20Self-Organizing%20Gaussian%20Grids%0AAuthor%3A%20Wieland%20Morgenstern%20and%20Florian%20Barthel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20recently%20emerged%20as%20a%20highly%20promising%20technique%0Afor%20modeling%20of%20static%203D%20scenes.%20In%20contrast%20to%20Neural%20Radiance%20Fields%2C%20it%0Autilizes%20efficient%20rasterization%20allowing%20for%20very%20fast%20rendering%20at%0Ahigh-quality.%20However%2C%20the%20storage%20size%20is%20significantly%20higher%2C%20which%20hinders%0Apractical%20deployment%2C%20e.g.%20on%20resource%20constrained%20devices.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20compact%20scene%20representation%20organizing%20the%20parameters%20of%203D%0AGaussian%20Splatting%20%283DGS%29%20into%20a%202D%20grid%20with%20local%20homogeneity%2C%20ensuring%20a%0Adrastic%20reduction%20in%20storage%20requirements%20without%20compromising%20visual%20quality%0Aduring%20rendering.%20Central%20to%20our%20idea%20is%20the%20explicit%20exploitation%20of%0Aperceptual%20redundancies%20present%20in%20natural%20scenes.%20In%20essence%2C%20the%20inherent%0Anature%20of%20a%20scene%20allows%20for%20numerous%20permutations%20of%20Gaussian%20parameters%20to%0Aequivalently%20represent%20it.%20To%20this%20end%2C%20we%20propose%20a%20novel%20highly%20parallel%0Aalgorithm%20that%20regularly%20arranges%20the%20high-dimensional%20Gaussian%20parameters%20into%0Aa%202D%20grid%20while%20preserving%20their%20neighborhood%20structure.%20During%20training%2C%20we%0Afurther%20enforce%20local%20smoothness%20between%20the%20sorted%20parameters%20in%20the%20grid.%20The%0Auncompressed%20Gaussians%20use%20the%20same%20structure%20as%203DGS%2C%20ensuring%20a%20seamless%0Aintegration%20with%20established%20renderers.%20Our%20method%20achieves%20a%20reduction%20factor%0Aof%2017x%20to%2042x%20in%20size%20for%20complex%20scenes%20with%20no%20increase%20in%20training%20time%2C%0Amarking%20a%20substantial%20leap%20forward%20in%20the%20domain%20of%203D%20scene%20distribution%20and%0Aconsumption.%20Additional%20information%20can%20be%20found%20on%20our%20project%20page%3A%0Ahttps%3A//fraunhoferhhi.github.io/Self-Organizing-Gaussians/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompact%25203D%2520Scene%2520Representation%2520via%2520Self-Organizing%2520Gaussian%2520Grids%26entry.906535625%3DWieland%2520Morgenstern%2520and%2520Florian%2520Barthel%2520and%2520Anna%2520Hilsmann%2520and%2520Peter%2520Eisert%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520recently%2520emerged%2520as%2520a%2520highly%2520promising%2520technique%250Afor%2520modeling%2520of%2520static%25203D%2520scenes.%2520In%2520contrast%2520to%2520Neural%2520Radiance%2520Fields%252C%2520it%250Autilizes%2520efficient%2520rasterization%2520allowing%2520for%2520very%2520fast%2520rendering%2520at%250Ahigh-quality.%2520However%252C%2520the%2520storage%2520size%2520is%2520significantly%2520higher%252C%2520which%2520hinders%250Apractical%2520deployment%252C%2520e.g.%2520on%2520resource%2520constrained%2520devices.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520compact%2520scene%2520representation%2520organizing%2520the%2520parameters%2520of%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520into%2520a%25202D%2520grid%2520with%2520local%2520homogeneity%252C%2520ensuring%2520a%250Adrastic%2520reduction%2520in%2520storage%2520requirements%2520without%2520compromising%2520visual%2520quality%250Aduring%2520rendering.%2520Central%2520to%2520our%2520idea%2520is%2520the%2520explicit%2520exploitation%2520of%250Aperceptual%2520redundancies%2520present%2520in%2520natural%2520scenes.%2520In%2520essence%252C%2520the%2520inherent%250Anature%2520of%2520a%2520scene%2520allows%2520for%2520numerous%2520permutations%2520of%2520Gaussian%2520parameters%2520to%250Aequivalently%2520represent%2520it.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520highly%2520parallel%250Aalgorithm%2520that%2520regularly%2520arranges%2520the%2520high-dimensional%2520Gaussian%2520parameters%2520into%250Aa%25202D%2520grid%2520while%2520preserving%2520their%2520neighborhood%2520structure.%2520During%2520training%252C%2520we%250Afurther%2520enforce%2520local%2520smoothness%2520between%2520the%2520sorted%2520parameters%2520in%2520the%2520grid.%2520The%250Auncompressed%2520Gaussians%2520use%2520the%2520same%2520structure%2520as%25203DGS%252C%2520ensuring%2520a%2520seamless%250Aintegration%2520with%2520established%2520renderers.%2520Our%2520method%2520achieves%2520a%2520reduction%2520factor%250Aof%252017x%2520to%252042x%2520in%2520size%2520for%2520complex%2520scenes%2520with%2520no%2520increase%2520in%2520training%2520time%252C%250Amarking%2520a%2520substantial%2520leap%2520forward%2520in%2520the%2520domain%2520of%25203D%2520scene%2520distribution%2520and%250Aconsumption.%2520Additional%2520information%2520can%2520be%2520found%2520on%2520our%2520project%2520page%253A%250Ahttps%253A//fraunhoferhhi.github.io/Self-Organizing-Gaussians/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compact%203D%20Scene%20Representation%20via%20Self-Organizing%20Gaussian%20Grids&entry.906535625=Wieland%20Morgenstern%20and%20Florian%20Barthel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20recently%20emerged%20as%20a%20highly%20promising%20technique%0Afor%20modeling%20of%20static%203D%20scenes.%20In%20contrast%20to%20Neural%20Radiance%20Fields%2C%20it%0Autilizes%20efficient%20rasterization%20allowing%20for%20very%20fast%20rendering%20at%0Ahigh-quality.%20However%2C%20the%20storage%20size%20is%20significantly%20higher%2C%20which%20hinders%0Apractical%20deployment%2C%20e.g.%20on%20resource%20constrained%20devices.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20compact%20scene%20representation%20organizing%20the%20parameters%20of%203D%0AGaussian%20Splatting%20%283DGS%29%20into%20a%202D%20grid%20with%20local%20homogeneity%2C%20ensuring%20a%0Adrastic%20reduction%20in%20storage%20requirements%20without%20compromising%20visual%20quality%0Aduring%20rendering.%20Central%20to%20our%20idea%20is%20the%20explicit%20exploitation%20of%0Aperceptual%20redundancies%20present%20in%20natural%20scenes.%20In%20essence%2C%20the%20inherent%0Anature%20of%20a%20scene%20allows%20for%20numerous%20permutations%20of%20Gaussian%20parameters%20to%0Aequivalently%20represent%20it.%20To%20this%20end%2C%20we%20propose%20a%20novel%20highly%20parallel%0Aalgorithm%20that%20regularly%20arranges%20the%20high-dimensional%20Gaussian%20parameters%20into%0Aa%202D%20grid%20while%20preserving%20their%20neighborhood%20structure.%20During%20training%2C%20we%0Afurther%20enforce%20local%20smoothness%20between%20the%20sorted%20parameters%20in%20the%20grid.%20The%0Auncompressed%20Gaussians%20use%20the%20same%20structure%20as%203DGS%2C%20ensuring%20a%20seamless%0Aintegration%20with%20established%20renderers.%20Our%20method%20achieves%20a%20reduction%20factor%0Aof%2017x%20to%2042x%20in%20size%20for%20complex%20scenes%20with%20no%20increase%20in%20training%20time%2C%0Amarking%20a%20substantial%20leap%20forward%20in%20the%20domain%20of%203D%20scene%20distribution%20and%0Aconsumption.%20Additional%20information%20can%20be%20found%20on%20our%20project%20page%3A%0Ahttps%3A//fraunhoferhhi.github.io/Self-Organizing-Gaussians/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13299v2&entry.124074799=Read"},
{"title": "Self-Supervised Learning for Interventional Image Analytics: Towards\n  Robust Device Trackers", "author": "Saahil Islam and Venkatesh N. Murthy and Dominik Neumann and Badhan Kumar Das and Puneet Sharma and Andreas Maier and Dorin Comaniciu and Florin C. Ghesu", "abstract": "  An accurate detection and tracking of devices such as guiding catheters in\nlive X-ray image acquisitions is an essential prerequisite for endovascular\ncardiac interventions. This information is leveraged for procedural guidance,\ne.g., directing stent placements. To ensure procedural safety and efficacy,\nthere is a need for high robustness no failures during tracking. To achieve\nthat, one needs to efficiently tackle challenges, such as: device obscuration\nby contrast agent or other external devices or wires, changes in field-of-view\nor acquisition angle, as well as the continuous movement due to cardiac and\nrespiratory motion. To overcome the aforementioned challenges, we propose a\nnovel approach to learn spatio-temporal features from a very large data cohort\nof over 16 million interventional X-ray frames using self-supervision for image\nsequence data. Our approach is based on a masked image modeling technique that\nleverages frame interpolation based reconstruction to learn fine inter-frame\ntemporal correspondences. The features encoded in the resulting model are\nfine-tuned downstream. Our approach achieves state-of-the-art performance and\nin particular robustness compared to ultra optimized reference solutions (that\nuse multi-stage feature fusion, multi-task and flow regularization). The\nexperiments show that our method achieves 66.31% reduction in maximum tracking\nerror against reference solutions (23.20% when flow regularization is used);\nachieving a success score of 97.95% at a 3x faster inference speed of 42\nframes-per-second (on GPU). The results encourage the use of our approach in\nvarious other tasks within interventional image analytics that require\neffective understanding of spatio-temporal semantics.\n", "link": "http://arxiv.org/abs/2405.01156v1", "date": "2024-05-02", "relevancy": 2.8214, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5792}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5697}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20for%20Interventional%20Image%20Analytics%3A%20Towards%0A%20%20Robust%20Device%20Trackers&body=Title%3A%20Self-Supervised%20Learning%20for%20Interventional%20Image%20Analytics%3A%20Towards%0A%20%20Robust%20Device%20Trackers%0AAuthor%3A%20Saahil%20Islam%20and%20Venkatesh%20N.%20Murthy%20and%20Dominik%20Neumann%20and%20Badhan%20Kumar%20Das%20and%20Puneet%20Sharma%20and%20Andreas%20Maier%20and%20Dorin%20Comaniciu%20and%20Florin%20C.%20Ghesu%0AAbstract%3A%20%20%20An%20accurate%20detection%20and%20tracking%20of%20devices%20such%20as%20guiding%20catheters%20in%0Alive%20X-ray%20image%20acquisitions%20is%20an%20essential%20prerequisite%20for%20endovascular%0Acardiac%20interventions.%20This%20information%20is%20leveraged%20for%20procedural%20guidance%2C%0Ae.g.%2C%20directing%20stent%20placements.%20To%20ensure%20procedural%20safety%20and%20efficacy%2C%0Athere%20is%20a%20need%20for%20high%20robustness%20no%20failures%20during%20tracking.%20To%20achieve%0Athat%2C%20one%20needs%20to%20efficiently%20tackle%20challenges%2C%20such%20as%3A%20device%20obscuration%0Aby%20contrast%20agent%20or%20other%20external%20devices%20or%20wires%2C%20changes%20in%20field-of-view%0Aor%20acquisition%20angle%2C%20as%20well%20as%20the%20continuous%20movement%20due%20to%20cardiac%20and%0Arespiratory%20motion.%20To%20overcome%20the%20aforementioned%20challenges%2C%20we%20propose%20a%0Anovel%20approach%20to%20learn%20spatio-temporal%20features%20from%20a%20very%20large%20data%20cohort%0Aof%20over%2016%20million%20interventional%20X-ray%20frames%20using%20self-supervision%20for%20image%0Asequence%20data.%20Our%20approach%20is%20based%20on%20a%20masked%20image%20modeling%20technique%20that%0Aleverages%20frame%20interpolation%20based%20reconstruction%20to%20learn%20fine%20inter-frame%0Atemporal%20correspondences.%20The%20features%20encoded%20in%20the%20resulting%20model%20are%0Afine-tuned%20downstream.%20Our%20approach%20achieves%20state-of-the-art%20performance%20and%0Ain%20particular%20robustness%20compared%20to%20ultra%20optimized%20reference%20solutions%20%28that%0Ause%20multi-stage%20feature%20fusion%2C%20multi-task%20and%20flow%20regularization%29.%20The%0Aexperiments%20show%20that%20our%20method%20achieves%2066.31%25%20reduction%20in%20maximum%20tracking%0Aerror%20against%20reference%20solutions%20%2823.20%25%20when%20flow%20regularization%20is%20used%29%3B%0Aachieving%20a%20success%20score%20of%2097.95%25%20at%20a%203x%20faster%20inference%20speed%20of%2042%0Aframes-per-second%20%28on%20GPU%29.%20The%20results%20encourage%20the%20use%20of%20our%20approach%20in%0Avarious%20other%20tasks%20within%20interventional%20image%20analytics%20that%20require%0Aeffective%20understanding%20of%20spatio-temporal%20semantics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520for%2520Interventional%2520Image%2520Analytics%253A%2520Towards%250A%2520%2520Robust%2520Device%2520Trackers%26entry.906535625%3DSaahil%2520Islam%2520and%2520Venkatesh%2520N.%2520Murthy%2520and%2520Dominik%2520Neumann%2520and%2520Badhan%2520Kumar%2520Das%2520and%2520Puneet%2520Sharma%2520and%2520Andreas%2520Maier%2520and%2520Dorin%2520Comaniciu%2520and%2520Florin%2520C.%2520Ghesu%26entry.1292438233%3D%2520%2520An%2520accurate%2520detection%2520and%2520tracking%2520of%2520devices%2520such%2520as%2520guiding%2520catheters%2520in%250Alive%2520X-ray%2520image%2520acquisitions%2520is%2520an%2520essential%2520prerequisite%2520for%2520endovascular%250Acardiac%2520interventions.%2520This%2520information%2520is%2520leveraged%2520for%2520procedural%2520guidance%252C%250Ae.g.%252C%2520directing%2520stent%2520placements.%2520To%2520ensure%2520procedural%2520safety%2520and%2520efficacy%252C%250Athere%2520is%2520a%2520need%2520for%2520high%2520robustness%2520no%2520failures%2520during%2520tracking.%2520To%2520achieve%250Athat%252C%2520one%2520needs%2520to%2520efficiently%2520tackle%2520challenges%252C%2520such%2520as%253A%2520device%2520obscuration%250Aby%2520contrast%2520agent%2520or%2520other%2520external%2520devices%2520or%2520wires%252C%2520changes%2520in%2520field-of-view%250Aor%2520acquisition%2520angle%252C%2520as%2520well%2520as%2520the%2520continuous%2520movement%2520due%2520to%2520cardiac%2520and%250Arespiratory%2520motion.%2520To%2520overcome%2520the%2520aforementioned%2520challenges%252C%2520we%2520propose%2520a%250Anovel%2520approach%2520to%2520learn%2520spatio-temporal%2520features%2520from%2520a%2520very%2520large%2520data%2520cohort%250Aof%2520over%252016%2520million%2520interventional%2520X-ray%2520frames%2520using%2520self-supervision%2520for%2520image%250Asequence%2520data.%2520Our%2520approach%2520is%2520based%2520on%2520a%2520masked%2520image%2520modeling%2520technique%2520that%250Aleverages%2520frame%2520interpolation%2520based%2520reconstruction%2520to%2520learn%2520fine%2520inter-frame%250Atemporal%2520correspondences.%2520The%2520features%2520encoded%2520in%2520the%2520resulting%2520model%2520are%250Afine-tuned%2520downstream.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520and%250Ain%2520particular%2520robustness%2520compared%2520to%2520ultra%2520optimized%2520reference%2520solutions%2520%2528that%250Ause%2520multi-stage%2520feature%2520fusion%252C%2520multi-task%2520and%2520flow%2520regularization%2529.%2520The%250Aexperiments%2520show%2520that%2520our%2520method%2520achieves%252066.31%2525%2520reduction%2520in%2520maximum%2520tracking%250Aerror%2520against%2520reference%2520solutions%2520%252823.20%2525%2520when%2520flow%2520regularization%2520is%2520used%2529%253B%250Aachieving%2520a%2520success%2520score%2520of%252097.95%2525%2520at%2520a%25203x%2520faster%2520inference%2520speed%2520of%252042%250Aframes-per-second%2520%2528on%2520GPU%2529.%2520The%2520results%2520encourage%2520the%2520use%2520of%2520our%2520approach%2520in%250Avarious%2520other%2520tasks%2520within%2520interventional%2520image%2520analytics%2520that%2520require%250Aeffective%2520understanding%2520of%2520spatio-temporal%2520semantics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20for%20Interventional%20Image%20Analytics%3A%20Towards%0A%20%20Robust%20Device%20Trackers&entry.906535625=Saahil%20Islam%20and%20Venkatesh%20N.%20Murthy%20and%20Dominik%20Neumann%20and%20Badhan%20Kumar%20Das%20and%20Puneet%20Sharma%20and%20Andreas%20Maier%20and%20Dorin%20Comaniciu%20and%20Florin%20C.%20Ghesu&entry.1292438233=%20%20An%20accurate%20detection%20and%20tracking%20of%20devices%20such%20as%20guiding%20catheters%20in%0Alive%20X-ray%20image%20acquisitions%20is%20an%20essential%20prerequisite%20for%20endovascular%0Acardiac%20interventions.%20This%20information%20is%20leveraged%20for%20procedural%20guidance%2C%0Ae.g.%2C%20directing%20stent%20placements.%20To%20ensure%20procedural%20safety%20and%20efficacy%2C%0Athere%20is%20a%20need%20for%20high%20robustness%20no%20failures%20during%20tracking.%20To%20achieve%0Athat%2C%20one%20needs%20to%20efficiently%20tackle%20challenges%2C%20such%20as%3A%20device%20obscuration%0Aby%20contrast%20agent%20or%20other%20external%20devices%20or%20wires%2C%20changes%20in%20field-of-view%0Aor%20acquisition%20angle%2C%20as%20well%20as%20the%20continuous%20movement%20due%20to%20cardiac%20and%0Arespiratory%20motion.%20To%20overcome%20the%20aforementioned%20challenges%2C%20we%20propose%20a%0Anovel%20approach%20to%20learn%20spatio-temporal%20features%20from%20a%20very%20large%20data%20cohort%0Aof%20over%2016%20million%20interventional%20X-ray%20frames%20using%20self-supervision%20for%20image%0Asequence%20data.%20Our%20approach%20is%20based%20on%20a%20masked%20image%20modeling%20technique%20that%0Aleverages%20frame%20interpolation%20based%20reconstruction%20to%20learn%20fine%20inter-frame%0Atemporal%20correspondences.%20The%20features%20encoded%20in%20the%20resulting%20model%20are%0Afine-tuned%20downstream.%20Our%20approach%20achieves%20state-of-the-art%20performance%20and%0Ain%20particular%20robustness%20compared%20to%20ultra%20optimized%20reference%20solutions%20%28that%0Ause%20multi-stage%20feature%20fusion%2C%20multi-task%20and%20flow%20regularization%29.%20The%0Aexperiments%20show%20that%20our%20method%20achieves%2066.31%25%20reduction%20in%20maximum%20tracking%0Aerror%20against%20reference%20solutions%20%2823.20%25%20when%20flow%20regularization%20is%20used%29%3B%0Aachieving%20a%20success%20score%20of%2097.95%25%20at%20a%203x%20faster%20inference%20speed%20of%2042%0Aframes-per-second%20%28on%20GPU%29.%20The%20results%20encourage%20the%20use%20of%20our%20approach%20in%0Avarious%20other%20tasks%20within%20interventional%20image%20analytics%20that%20require%0Aeffective%20understanding%20of%20spatio-temporal%20semantics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01156v1&entry.124074799=Read"},
{"title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey", "author": "Penghao Zhao and Hailin Zhang and Qinhan Yu and Zhengren Wang and Yunteng Geng and Fangcheng Fu and Ling Yang and Wentao Zhang and Jie Jiang and Bin Cui", "abstract": "  Advancements in model algorithms, the growth of foundational models, and\naccess to high-quality datasets have propelled the evolution of Artificial\nIntelligence Generated Content (AIGC). Despite its notable successes, AIGC\nstill faces hurdles such as updating knowledge, handling long-tail data,\nmitigating data leakage, and managing high training and inference costs.\nRetrieval-Augmented Generation (RAG) has recently emerged as a paradigm to\naddress such challenges. In particular, RAG introduces the information\nretrieval process, which enhances the generation process by retrieving relevant\nobjects from available data stores, leading to higher accuracy and better\nrobustness. In this paper, we comprehensively review existing efforts that\nintegrate RAG technique into AIGC scenarios. We first classify RAG foundations\naccording to how the retriever augments the generator, distilling the\nfundamental abstractions of the augmentation methodologies for various\nretrievers and generators. This unified perspective encompasses all RAG\nscenarios, illuminating advancements and pivotal technologies that help with\npotential future progress. We also summarize additional enhancements methods\nfor RAG, facilitating effective engineering and implementation of RAG systems.\nThen from another view, we survey on practical applications of RAG across\ndifferent modalities and tasks, offering valuable references for researchers\nand practitioners. Furthermore, we introduce the benchmarks for RAG, discuss\nthe limitations of current RAG systems, and suggest potential directions for\nfuture research. Github: https://github.com/PKU-DAIR/RAG-Survey.\n", "link": "http://arxiv.org/abs/2402.19473v4", "date": "2024-05-02", "relevancy": 2.7387, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5726}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5517}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval-Augmented%20Generation%20for%20AI-Generated%20Content%3A%20A%20Survey&body=Title%3A%20Retrieval-Augmented%20Generation%20for%20AI-Generated%20Content%3A%20A%20Survey%0AAuthor%3A%20Penghao%20Zhao%20and%20Hailin%20Zhang%20and%20Qinhan%20Yu%20and%20Zhengren%20Wang%20and%20Yunteng%20Geng%20and%20Fangcheng%20Fu%20and%20Ling%20Yang%20and%20Wentao%20Zhang%20and%20Jie%20Jiang%20and%20Bin%20Cui%0AAbstract%3A%20%20%20Advancements%20in%20model%20algorithms%2C%20the%20growth%20of%20foundational%20models%2C%20and%0Aaccess%20to%20high-quality%20datasets%20have%20propelled%20the%20evolution%20of%20Artificial%0AIntelligence%20Generated%20Content%20%28AIGC%29.%20Despite%20its%20notable%20successes%2C%20AIGC%0Astill%20faces%20hurdles%20such%20as%20updating%20knowledge%2C%20handling%20long-tail%20data%2C%0Amitigating%20data%20leakage%2C%20and%20managing%20high%20training%20and%20inference%20costs.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20has%20recently%20emerged%20as%20a%20paradigm%20to%0Aaddress%20such%20challenges.%20In%20particular%2C%20RAG%20introduces%20the%20information%0Aretrieval%20process%2C%20which%20enhances%20the%20generation%20process%20by%20retrieving%20relevant%0Aobjects%20from%20available%20data%20stores%2C%20leading%20to%20higher%20accuracy%20and%20better%0Arobustness.%20In%20this%20paper%2C%20we%20comprehensively%20review%20existing%20efforts%20that%0Aintegrate%20RAG%20technique%20into%20AIGC%20scenarios.%20We%20first%20classify%20RAG%20foundations%0Aaccording%20to%20how%20the%20retriever%20augments%20the%20generator%2C%20distilling%20the%0Afundamental%20abstractions%20of%20the%20augmentation%20methodologies%20for%20various%0Aretrievers%20and%20generators.%20This%20unified%20perspective%20encompasses%20all%20RAG%0Ascenarios%2C%20illuminating%20advancements%20and%20pivotal%20technologies%20that%20help%20with%0Apotential%20future%20progress.%20We%20also%20summarize%20additional%20enhancements%20methods%0Afor%20RAG%2C%20facilitating%20effective%20engineering%20and%20implementation%20of%20RAG%20systems.%0AThen%20from%20another%20view%2C%20we%20survey%20on%20practical%20applications%20of%20RAG%20across%0Adifferent%20modalities%20and%20tasks%2C%20offering%20valuable%20references%20for%20researchers%0Aand%20practitioners.%20Furthermore%2C%20we%20introduce%20the%20benchmarks%20for%20RAG%2C%20discuss%0Athe%20limitations%20of%20current%20RAG%20systems%2C%20and%20suggest%20potential%20directions%20for%0Afuture%20research.%20Github%3A%20https%3A//github.com/PKU-DAIR/RAG-Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19473v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval-Augmented%2520Generation%2520for%2520AI-Generated%2520Content%253A%2520A%2520Survey%26entry.906535625%3DPenghao%2520Zhao%2520and%2520Hailin%2520Zhang%2520and%2520Qinhan%2520Yu%2520and%2520Zhengren%2520Wang%2520and%2520Yunteng%2520Geng%2520and%2520Fangcheng%2520Fu%2520and%2520Ling%2520Yang%2520and%2520Wentao%2520Zhang%2520and%2520Jie%2520Jiang%2520and%2520Bin%2520Cui%26entry.1292438233%3D%2520%2520Advancements%2520in%2520model%2520algorithms%252C%2520the%2520growth%2520of%2520foundational%2520models%252C%2520and%250Aaccess%2520to%2520high-quality%2520datasets%2520have%2520propelled%2520the%2520evolution%2520of%2520Artificial%250AIntelligence%2520Generated%2520Content%2520%2528AIGC%2529.%2520Despite%2520its%2520notable%2520successes%252C%2520AIGC%250Astill%2520faces%2520hurdles%2520such%2520as%2520updating%2520knowledge%252C%2520handling%2520long-tail%2520data%252C%250Amitigating%2520data%2520leakage%252C%2520and%2520managing%2520high%2520training%2520and%2520inference%2520costs.%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520has%2520recently%2520emerged%2520as%2520a%2520paradigm%2520to%250Aaddress%2520such%2520challenges.%2520In%2520particular%252C%2520RAG%2520introduces%2520the%2520information%250Aretrieval%2520process%252C%2520which%2520enhances%2520the%2520generation%2520process%2520by%2520retrieving%2520relevant%250Aobjects%2520from%2520available%2520data%2520stores%252C%2520leading%2520to%2520higher%2520accuracy%2520and%2520better%250Arobustness.%2520In%2520this%2520paper%252C%2520we%2520comprehensively%2520review%2520existing%2520efforts%2520that%250Aintegrate%2520RAG%2520technique%2520into%2520AIGC%2520scenarios.%2520We%2520first%2520classify%2520RAG%2520foundations%250Aaccording%2520to%2520how%2520the%2520retriever%2520augments%2520the%2520generator%252C%2520distilling%2520the%250Afundamental%2520abstractions%2520of%2520the%2520augmentation%2520methodologies%2520for%2520various%250Aretrievers%2520and%2520generators.%2520This%2520unified%2520perspective%2520encompasses%2520all%2520RAG%250Ascenarios%252C%2520illuminating%2520advancements%2520and%2520pivotal%2520technologies%2520that%2520help%2520with%250Apotential%2520future%2520progress.%2520We%2520also%2520summarize%2520additional%2520enhancements%2520methods%250Afor%2520RAG%252C%2520facilitating%2520effective%2520engineering%2520and%2520implementation%2520of%2520RAG%2520systems.%250AThen%2520from%2520another%2520view%252C%2520we%2520survey%2520on%2520practical%2520applications%2520of%2520RAG%2520across%250Adifferent%2520modalities%2520and%2520tasks%252C%2520offering%2520valuable%2520references%2520for%2520researchers%250Aand%2520practitioners.%2520Furthermore%252C%2520we%2520introduce%2520the%2520benchmarks%2520for%2520RAG%252C%2520discuss%250Athe%2520limitations%2520of%2520current%2520RAG%2520systems%252C%2520and%2520suggest%2520potential%2520directions%2520for%250Afuture%2520research.%2520Github%253A%2520https%253A//github.com/PKU-DAIR/RAG-Survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19473v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-Augmented%20Generation%20for%20AI-Generated%20Content%3A%20A%20Survey&entry.906535625=Penghao%20Zhao%20and%20Hailin%20Zhang%20and%20Qinhan%20Yu%20and%20Zhengren%20Wang%20and%20Yunteng%20Geng%20and%20Fangcheng%20Fu%20and%20Ling%20Yang%20and%20Wentao%20Zhang%20and%20Jie%20Jiang%20and%20Bin%20Cui&entry.1292438233=%20%20Advancements%20in%20model%20algorithms%2C%20the%20growth%20of%20foundational%20models%2C%20and%0Aaccess%20to%20high-quality%20datasets%20have%20propelled%20the%20evolution%20of%20Artificial%0AIntelligence%20Generated%20Content%20%28AIGC%29.%20Despite%20its%20notable%20successes%2C%20AIGC%0Astill%20faces%20hurdles%20such%20as%20updating%20knowledge%2C%20handling%20long-tail%20data%2C%0Amitigating%20data%20leakage%2C%20and%20managing%20high%20training%20and%20inference%20costs.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20has%20recently%20emerged%20as%20a%20paradigm%20to%0Aaddress%20such%20challenges.%20In%20particular%2C%20RAG%20introduces%20the%20information%0Aretrieval%20process%2C%20which%20enhances%20the%20generation%20process%20by%20retrieving%20relevant%0Aobjects%20from%20available%20data%20stores%2C%20leading%20to%20higher%20accuracy%20and%20better%0Arobustness.%20In%20this%20paper%2C%20we%20comprehensively%20review%20existing%20efforts%20that%0Aintegrate%20RAG%20technique%20into%20AIGC%20scenarios.%20We%20first%20classify%20RAG%20foundations%0Aaccording%20to%20how%20the%20retriever%20augments%20the%20generator%2C%20distilling%20the%0Afundamental%20abstractions%20of%20the%20augmentation%20methodologies%20for%20various%0Aretrievers%20and%20generators.%20This%20unified%20perspective%20encompasses%20all%20RAG%0Ascenarios%2C%20illuminating%20advancements%20and%20pivotal%20technologies%20that%20help%20with%0Apotential%20future%20progress.%20We%20also%20summarize%20additional%20enhancements%20methods%0Afor%20RAG%2C%20facilitating%20effective%20engineering%20and%20implementation%20of%20RAG%20systems.%0AThen%20from%20another%20view%2C%20we%20survey%20on%20practical%20applications%20of%20RAG%20across%0Adifferent%20modalities%20and%20tasks%2C%20offering%20valuable%20references%20for%20researchers%0Aand%20practitioners.%20Furthermore%2C%20we%20introduce%20the%20benchmarks%20for%20RAG%2C%20discuss%0Athe%20limitations%20of%20current%20RAG%20systems%2C%20and%20suggest%20potential%20directions%20for%0Afuture%20research.%20Github%3A%20https%3A//github.com/PKU-DAIR/RAG-Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19473v4&entry.124074799=Read"},
{"title": "CLIP4STR: A Simple Baseline for Scene Text Recognition with Pre-trained\n  Vision-Language Model", "author": "Shuai Zhao and Ruijie Quan and Linchao Zhu and Yi Yang", "abstract": "  Pre-trained vision-language models~(VLMs) are the de-facto foundation models\nfor various downstream tasks. However, scene text recognition methods still\nprefer backbones pre-trained on a single modality, namely, the visual modality,\ndespite the potential of VLMs to serve as powerful scene text readers. For\nexample, CLIP can robustly identify regular (horizontal) and irregular\n(rotated, curved, blurred, or occluded) text in images. With such merits, we\ntransform CLIP into a scene text reader and introduce CLIP4STR, a simple yet\neffective STR method built upon image and text encoders of CLIP. It has two\nencoder-decoder branches: a visual branch and a cross-modal branch. The visual\nbranch provides an initial prediction based on the visual feature, and the\ncross-modal branch refines this prediction by addressing the discrepancy\nbetween the visual feature and text semantics. To fully leverage the\ncapabilities of both branches, we design a dual predict-and-refine decoding\nscheme for inference. We scale CLIP4STR in terms of the model size,\npre-training data, and training data, achieving state-of-the-art performance on\n11 STR benchmarks. Additionally, a comprehensive empirical study is provided to\nenhance the understanding of the adaptation of CLIP to STR. We believe our\nmethod establishes a simple yet strong baseline for future STR research with\nVLMs.\n", "link": "http://arxiv.org/abs/2305.14014v3", "date": "2024-05-02", "relevancy": 2.6735, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.604}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5072}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP4STR%3A%20A%20Simple%20Baseline%20for%20Scene%20Text%20Recognition%20with%20Pre-trained%0A%20%20Vision-Language%20Model&body=Title%3A%20CLIP4STR%3A%20A%20Simple%20Baseline%20for%20Scene%20Text%20Recognition%20with%20Pre-trained%0A%20%20Vision-Language%20Model%0AAuthor%3A%20Shuai%20Zhao%20and%20Ruijie%20Quan%20and%20Linchao%20Zhu%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models~%28VLMs%29%20are%20the%20de-facto%20foundation%20models%0Afor%20various%20downstream%20tasks.%20However%2C%20scene%20text%20recognition%20methods%20still%0Aprefer%20backbones%20pre-trained%20on%20a%20single%20modality%2C%20namely%2C%20the%20visual%20modality%2C%0Adespite%20the%20potential%20of%20VLMs%20to%20serve%20as%20powerful%20scene%20text%20readers.%20For%0Aexample%2C%20CLIP%20can%20robustly%20identify%20regular%20%28horizontal%29%20and%20irregular%0A%28rotated%2C%20curved%2C%20blurred%2C%20or%20occluded%29%20text%20in%20images.%20With%20such%20merits%2C%20we%0Atransform%20CLIP%20into%20a%20scene%20text%20reader%20and%20introduce%20CLIP4STR%2C%20a%20simple%20yet%0Aeffective%20STR%20method%20built%20upon%20image%20and%20text%20encoders%20of%20CLIP.%20It%20has%20two%0Aencoder-decoder%20branches%3A%20a%20visual%20branch%20and%20a%20cross-modal%20branch.%20The%20visual%0Abranch%20provides%20an%20initial%20prediction%20based%20on%20the%20visual%20feature%2C%20and%20the%0Across-modal%20branch%20refines%20this%20prediction%20by%20addressing%20the%20discrepancy%0Abetween%20the%20visual%20feature%20and%20text%20semantics.%20To%20fully%20leverage%20the%0Acapabilities%20of%20both%20branches%2C%20we%20design%20a%20dual%20predict-and-refine%20decoding%0Ascheme%20for%20inference.%20We%20scale%20CLIP4STR%20in%20terms%20of%20the%20model%20size%2C%0Apre-training%20data%2C%20and%20training%20data%2C%20achieving%20state-of-the-art%20performance%20on%0A11%20STR%20benchmarks.%20Additionally%2C%20a%20comprehensive%20empirical%20study%20is%20provided%20to%0Aenhance%20the%20understanding%20of%20the%20adaptation%20of%20CLIP%20to%20STR.%20We%20believe%20our%0Amethod%20establishes%20a%20simple%20yet%20strong%20baseline%20for%20future%20STR%20research%20with%0AVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.14014v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP4STR%253A%2520A%2520Simple%2520Baseline%2520for%2520Scene%2520Text%2520Recognition%2520with%2520Pre-trained%250A%2520%2520Vision-Language%2520Model%26entry.906535625%3DShuai%2520Zhao%2520and%2520Ruijie%2520Quan%2520and%2520Linchao%2520Zhu%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models~%2528VLMs%2529%2520are%2520the%2520de-facto%2520foundation%2520models%250Afor%2520various%2520downstream%2520tasks.%2520However%252C%2520scene%2520text%2520recognition%2520methods%2520still%250Aprefer%2520backbones%2520pre-trained%2520on%2520a%2520single%2520modality%252C%2520namely%252C%2520the%2520visual%2520modality%252C%250Adespite%2520the%2520potential%2520of%2520VLMs%2520to%2520serve%2520as%2520powerful%2520scene%2520text%2520readers.%2520For%250Aexample%252C%2520CLIP%2520can%2520robustly%2520identify%2520regular%2520%2528horizontal%2529%2520and%2520irregular%250A%2528rotated%252C%2520curved%252C%2520blurred%252C%2520or%2520occluded%2529%2520text%2520in%2520images.%2520With%2520such%2520merits%252C%2520we%250Atransform%2520CLIP%2520into%2520a%2520scene%2520text%2520reader%2520and%2520introduce%2520CLIP4STR%252C%2520a%2520simple%2520yet%250Aeffective%2520STR%2520method%2520built%2520upon%2520image%2520and%2520text%2520encoders%2520of%2520CLIP.%2520It%2520has%2520two%250Aencoder-decoder%2520branches%253A%2520a%2520visual%2520branch%2520and%2520a%2520cross-modal%2520branch.%2520The%2520visual%250Abranch%2520provides%2520an%2520initial%2520prediction%2520based%2520on%2520the%2520visual%2520feature%252C%2520and%2520the%250Across-modal%2520branch%2520refines%2520this%2520prediction%2520by%2520addressing%2520the%2520discrepancy%250Abetween%2520the%2520visual%2520feature%2520and%2520text%2520semantics.%2520To%2520fully%2520leverage%2520the%250Acapabilities%2520of%2520both%2520branches%252C%2520we%2520design%2520a%2520dual%2520predict-and-refine%2520decoding%250Ascheme%2520for%2520inference.%2520We%2520scale%2520CLIP4STR%2520in%2520terms%2520of%2520the%2520model%2520size%252C%250Apre-training%2520data%252C%2520and%2520training%2520data%252C%2520achieving%2520state-of-the-art%2520performance%2520on%250A11%2520STR%2520benchmarks.%2520Additionally%252C%2520a%2520comprehensive%2520empirical%2520study%2520is%2520provided%2520to%250Aenhance%2520the%2520understanding%2520of%2520the%2520adaptation%2520of%2520CLIP%2520to%2520STR.%2520We%2520believe%2520our%250Amethod%2520establishes%2520a%2520simple%2520yet%2520strong%2520baseline%2520for%2520future%2520STR%2520research%2520with%250AVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.14014v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP4STR%3A%20A%20Simple%20Baseline%20for%20Scene%20Text%20Recognition%20with%20Pre-trained%0A%20%20Vision-Language%20Model&entry.906535625=Shuai%20Zhao%20and%20Ruijie%20Quan%20and%20Linchao%20Zhu%20and%20Yi%20Yang&entry.1292438233=%20%20Pre-trained%20vision-language%20models~%28VLMs%29%20are%20the%20de-facto%20foundation%20models%0Afor%20various%20downstream%20tasks.%20However%2C%20scene%20text%20recognition%20methods%20still%0Aprefer%20backbones%20pre-trained%20on%20a%20single%20modality%2C%20namely%2C%20the%20visual%20modality%2C%0Adespite%20the%20potential%20of%20VLMs%20to%20serve%20as%20powerful%20scene%20text%20readers.%20For%0Aexample%2C%20CLIP%20can%20robustly%20identify%20regular%20%28horizontal%29%20and%20irregular%0A%28rotated%2C%20curved%2C%20blurred%2C%20or%20occluded%29%20text%20in%20images.%20With%20such%20merits%2C%20we%0Atransform%20CLIP%20into%20a%20scene%20text%20reader%20and%20introduce%20CLIP4STR%2C%20a%20simple%20yet%0Aeffective%20STR%20method%20built%20upon%20image%20and%20text%20encoders%20of%20CLIP.%20It%20has%20two%0Aencoder-decoder%20branches%3A%20a%20visual%20branch%20and%20a%20cross-modal%20branch.%20The%20visual%0Abranch%20provides%20an%20initial%20prediction%20based%20on%20the%20visual%20feature%2C%20and%20the%0Across-modal%20branch%20refines%20this%20prediction%20by%20addressing%20the%20discrepancy%0Abetween%20the%20visual%20feature%20and%20text%20semantics.%20To%20fully%20leverage%20the%0Acapabilities%20of%20both%20branches%2C%20we%20design%20a%20dual%20predict-and-refine%20decoding%0Ascheme%20for%20inference.%20We%20scale%20CLIP4STR%20in%20terms%20of%20the%20model%20size%2C%0Apre-training%20data%2C%20and%20training%20data%2C%20achieving%20state-of-the-art%20performance%20on%0A11%20STR%20benchmarks.%20Additionally%2C%20a%20comprehensive%20empirical%20study%20is%20provided%20to%0Aenhance%20the%20understanding%20of%20the%20adaptation%20of%20CLIP%20to%20STR.%20We%20believe%20our%0Amethod%20establishes%20a%20simple%20yet%20strong%20baseline%20for%20future%20STR%20research%20with%0AVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.14014v3&entry.124074799=Read"},
{"title": "Controllable Text Generation in the Instruction-Tuning Era", "author": "Dhananjay Ashok and Barnabas Poczos", "abstract": "  While most research on controllable text generation has focused on steering\nbase Language Models, the emerging instruction-tuning and prompting paradigm\noffers an alternate approach to controllability. We compile and release\nConGenBench, a testbed of 17 different controllable generation tasks, using a\nsubset of it to benchmark the performance of 9 different baselines and methods\non Instruction-tuned Language Models. To our surprise, we find that\nprompting-based approaches outperform controllable text generation methods on\nmost datasets and tasks, highlighting a need for research on controllable text\ngeneration with Instruction-tuned Language Models in specific. Prompt-based\napproaches match human performance on most stylistic tasks while lagging on\nstructural tasks, foregrounding a need to study more varied constraints and\nmore challenging stylistic tasks. To facilitate such research, we provide an\nalgorithm that uses only a task dataset and a Large Language Model with\nin-context capabilities to automatically generate a constraint dataset. This\nmethod eliminates the fields dependence on pre-curated constraint datasets,\nhence vastly expanding the range of constraints that can be studied in the\nfuture.\n", "link": "http://arxiv.org/abs/2405.01490v1", "date": "2024-05-02", "relevancy": 2.6568, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5692}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5418}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controllable%20Text%20Generation%20in%20the%20Instruction-Tuning%20Era&body=Title%3A%20Controllable%20Text%20Generation%20in%20the%20Instruction-Tuning%20Era%0AAuthor%3A%20Dhananjay%20Ashok%20and%20Barnabas%20Poczos%0AAbstract%3A%20%20%20While%20most%20research%20on%20controllable%20text%20generation%20has%20focused%20on%20steering%0Abase%20Language%20Models%2C%20the%20emerging%20instruction-tuning%20and%20prompting%20paradigm%0Aoffers%20an%20alternate%20approach%20to%20controllability.%20We%20compile%20and%20release%0AConGenBench%2C%20a%20testbed%20of%2017%20different%20controllable%20generation%20tasks%2C%20using%20a%0Asubset%20of%20it%20to%20benchmark%20the%20performance%20of%209%20different%20baselines%20and%20methods%0Aon%20Instruction-tuned%20Language%20Models.%20To%20our%20surprise%2C%20we%20find%20that%0Aprompting-based%20approaches%20outperform%20controllable%20text%20generation%20methods%20on%0Amost%20datasets%20and%20tasks%2C%20highlighting%20a%20need%20for%20research%20on%20controllable%20text%0Ageneration%20with%20Instruction-tuned%20Language%20Models%20in%20specific.%20Prompt-based%0Aapproaches%20match%20human%20performance%20on%20most%20stylistic%20tasks%20while%20lagging%20on%0Astructural%20tasks%2C%20foregrounding%20a%20need%20to%20study%20more%20varied%20constraints%20and%0Amore%20challenging%20stylistic%20tasks.%20To%20facilitate%20such%20research%2C%20we%20provide%20an%0Aalgorithm%20that%20uses%20only%20a%20task%20dataset%20and%20a%20Large%20Language%20Model%20with%0Ain-context%20capabilities%20to%20automatically%20generate%20a%20constraint%20dataset.%20This%0Amethod%20eliminates%20the%20fields%20dependence%20on%20pre-curated%20constraint%20datasets%2C%0Ahence%20vastly%20expanding%20the%20range%20of%20constraints%20that%20can%20be%20studied%20in%20the%0Afuture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControllable%2520Text%2520Generation%2520in%2520the%2520Instruction-Tuning%2520Era%26entry.906535625%3DDhananjay%2520Ashok%2520and%2520Barnabas%2520Poczos%26entry.1292438233%3D%2520%2520While%2520most%2520research%2520on%2520controllable%2520text%2520generation%2520has%2520focused%2520on%2520steering%250Abase%2520Language%2520Models%252C%2520the%2520emerging%2520instruction-tuning%2520and%2520prompting%2520paradigm%250Aoffers%2520an%2520alternate%2520approach%2520to%2520controllability.%2520We%2520compile%2520and%2520release%250AConGenBench%252C%2520a%2520testbed%2520of%252017%2520different%2520controllable%2520generation%2520tasks%252C%2520using%2520a%250Asubset%2520of%2520it%2520to%2520benchmark%2520the%2520performance%2520of%25209%2520different%2520baselines%2520and%2520methods%250Aon%2520Instruction-tuned%2520Language%2520Models.%2520To%2520our%2520surprise%252C%2520we%2520find%2520that%250Aprompting-based%2520approaches%2520outperform%2520controllable%2520text%2520generation%2520methods%2520on%250Amost%2520datasets%2520and%2520tasks%252C%2520highlighting%2520a%2520need%2520for%2520research%2520on%2520controllable%2520text%250Ageneration%2520with%2520Instruction-tuned%2520Language%2520Models%2520in%2520specific.%2520Prompt-based%250Aapproaches%2520match%2520human%2520performance%2520on%2520most%2520stylistic%2520tasks%2520while%2520lagging%2520on%250Astructural%2520tasks%252C%2520foregrounding%2520a%2520need%2520to%2520study%2520more%2520varied%2520constraints%2520and%250Amore%2520challenging%2520stylistic%2520tasks.%2520To%2520facilitate%2520such%2520research%252C%2520we%2520provide%2520an%250Aalgorithm%2520that%2520uses%2520only%2520a%2520task%2520dataset%2520and%2520a%2520Large%2520Language%2520Model%2520with%250Ain-context%2520capabilities%2520to%2520automatically%2520generate%2520a%2520constraint%2520dataset.%2520This%250Amethod%2520eliminates%2520the%2520fields%2520dependence%2520on%2520pre-curated%2520constraint%2520datasets%252C%250Ahence%2520vastly%2520expanding%2520the%2520range%2520of%2520constraints%2520that%2520can%2520be%2520studied%2520in%2520the%250Afuture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20Text%20Generation%20in%20the%20Instruction-Tuning%20Era&entry.906535625=Dhananjay%20Ashok%20and%20Barnabas%20Poczos&entry.1292438233=%20%20While%20most%20research%20on%20controllable%20text%20generation%20has%20focused%20on%20steering%0Abase%20Language%20Models%2C%20the%20emerging%20instruction-tuning%20and%20prompting%20paradigm%0Aoffers%20an%20alternate%20approach%20to%20controllability.%20We%20compile%20and%20release%0AConGenBench%2C%20a%20testbed%20of%2017%20different%20controllable%20generation%20tasks%2C%20using%20a%0Asubset%20of%20it%20to%20benchmark%20the%20performance%20of%209%20different%20baselines%20and%20methods%0Aon%20Instruction-tuned%20Language%20Models.%20To%20our%20surprise%2C%20we%20find%20that%0Aprompting-based%20approaches%20outperform%20controllable%20text%20generation%20methods%20on%0Amost%20datasets%20and%20tasks%2C%20highlighting%20a%20need%20for%20research%20on%20controllable%20text%0Ageneration%20with%20Instruction-tuned%20Language%20Models%20in%20specific.%20Prompt-based%0Aapproaches%20match%20human%20performance%20on%20most%20stylistic%20tasks%20while%20lagging%20on%0Astructural%20tasks%2C%20foregrounding%20a%20need%20to%20study%20more%20varied%20constraints%20and%0Amore%20challenging%20stylistic%20tasks.%20To%20facilitate%20such%20research%2C%20we%20provide%20an%0Aalgorithm%20that%20uses%20only%20a%20task%20dataset%20and%20a%20Large%20Language%20Model%20with%0Ain-context%20capabilities%20to%20automatically%20generate%20a%20constraint%20dataset.%20This%0Amethod%20eliminates%20the%20fields%20dependence%20on%20pre-curated%20constraint%20datasets%2C%0Ahence%20vastly%20expanding%20the%20range%20of%20constraints%20that%20can%20be%20studied%20in%20the%0Afuture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01490v1&entry.124074799=Read"},
{"title": "CromSS: Cross-modal pre-training with noisy labels for remote sensing\n  image segmentation", "author": "Chenying Liu and Conrad Albrecht and Yi Wang and Xiao Xiang Zhu", "abstract": "  We study the potential of noisy labels y to pretrain semantic segmentation\nmodels in a multi-modal learning framework for geospatial applications.\nSpecifically, we propose a novel Cross-modal Sample Selection method (CromSS)\nthat utilizes the class distributions P^{(d)}(x,c) over pixels x and classes c\nmodelled by multiple sensors/modalities d of a given geospatial scene.\nConsistency of predictions across sensors $d$ is jointly informed by the\nentropy of P^{(d)}(x,c). Noisy label sampling we determine by the confidence of\neach sensor d in the noisy class label, P^{(d)}(x,c=y(x)). To verify the\nperformance of our approach, we conduct experiments with Sentinel-1 (radar) and\nSentinel-2 (optical) satellite imagery from the globally-sampled SSL4EO-S12\ndataset. We pair those scenes with 9-class noisy labels sourced from the Google\nDynamic World project for pretraining. Transfer learning evaluations\n(downstream task) on the DFC2020 dataset confirm the effectiveness of the\nproposed method for remote sensing image segmentation.\n", "link": "http://arxiv.org/abs/2405.01217v1", "date": "2024-05-02", "relevancy": 2.6472, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.547}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5234}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CromSS%3A%20Cross-modal%20pre-training%20with%20noisy%20labels%20for%20remote%20sensing%0A%20%20image%20segmentation&body=Title%3A%20CromSS%3A%20Cross-modal%20pre-training%20with%20noisy%20labels%20for%20remote%20sensing%0A%20%20image%20segmentation%0AAuthor%3A%20Chenying%20Liu%20and%20Conrad%20Albrecht%20and%20Yi%20Wang%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%20We%20study%20the%20potential%20of%20noisy%20labels%20y%20to%20pretrain%20semantic%20segmentation%0Amodels%20in%20a%20multi-modal%20learning%20framework%20for%20geospatial%20applications.%0ASpecifically%2C%20we%20propose%20a%20novel%20Cross-modal%20Sample%20Selection%20method%20%28CromSS%29%0Athat%20utilizes%20the%20class%20distributions%20P%5E%7B%28d%29%7D%28x%2Cc%29%20over%20pixels%20x%20and%20classes%20c%0Amodelled%20by%20multiple%20sensors/modalities%20d%20of%20a%20given%20geospatial%20scene.%0AConsistency%20of%20predictions%20across%20sensors%20%24d%24%20is%20jointly%20informed%20by%20the%0Aentropy%20of%20P%5E%7B%28d%29%7D%28x%2Cc%29.%20Noisy%20label%20sampling%20we%20determine%20by%20the%20confidence%20of%0Aeach%20sensor%20d%20in%20the%20noisy%20class%20label%2C%20P%5E%7B%28d%29%7D%28x%2Cc%3Dy%28x%29%29.%20To%20verify%20the%0Aperformance%20of%20our%20approach%2C%20we%20conduct%20experiments%20with%20Sentinel-1%20%28radar%29%20and%0ASentinel-2%20%28optical%29%20satellite%20imagery%20from%20the%20globally-sampled%20SSL4EO-S12%0Adataset.%20We%20pair%20those%20scenes%20with%209-class%20noisy%20labels%20sourced%20from%20the%20Google%0ADynamic%20World%20project%20for%20pretraining.%20Transfer%20learning%20evaluations%0A%28downstream%20task%29%20on%20the%20DFC2020%20dataset%20confirm%20the%20effectiveness%20of%20the%0Aproposed%20method%20for%20remote%20sensing%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCromSS%253A%2520Cross-modal%2520pre-training%2520with%2520noisy%2520labels%2520for%2520remote%2520sensing%250A%2520%2520image%2520segmentation%26entry.906535625%3DChenying%2520Liu%2520and%2520Conrad%2520Albrecht%2520and%2520Yi%2520Wang%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520potential%2520of%2520noisy%2520labels%2520y%2520to%2520pretrain%2520semantic%2520segmentation%250Amodels%2520in%2520a%2520multi-modal%2520learning%2520framework%2520for%2520geospatial%2520applications.%250ASpecifically%252C%2520we%2520propose%2520a%2520novel%2520Cross-modal%2520Sample%2520Selection%2520method%2520%2528CromSS%2529%250Athat%2520utilizes%2520the%2520class%2520distributions%2520P%255E%257B%2528d%2529%257D%2528x%252Cc%2529%2520over%2520pixels%2520x%2520and%2520classes%2520c%250Amodelled%2520by%2520multiple%2520sensors/modalities%2520d%2520of%2520a%2520given%2520geospatial%2520scene.%250AConsistency%2520of%2520predictions%2520across%2520sensors%2520%2524d%2524%2520is%2520jointly%2520informed%2520by%2520the%250Aentropy%2520of%2520P%255E%257B%2528d%2529%257D%2528x%252Cc%2529.%2520Noisy%2520label%2520sampling%2520we%2520determine%2520by%2520the%2520confidence%2520of%250Aeach%2520sensor%2520d%2520in%2520the%2520noisy%2520class%2520label%252C%2520P%255E%257B%2528d%2529%257D%2528x%252Cc%253Dy%2528x%2529%2529.%2520To%2520verify%2520the%250Aperformance%2520of%2520our%2520approach%252C%2520we%2520conduct%2520experiments%2520with%2520Sentinel-1%2520%2528radar%2529%2520and%250ASentinel-2%2520%2528optical%2529%2520satellite%2520imagery%2520from%2520the%2520globally-sampled%2520SSL4EO-S12%250Adataset.%2520We%2520pair%2520those%2520scenes%2520with%25209-class%2520noisy%2520labels%2520sourced%2520from%2520the%2520Google%250ADynamic%2520World%2520project%2520for%2520pretraining.%2520Transfer%2520learning%2520evaluations%250A%2528downstream%2520task%2529%2520on%2520the%2520DFC2020%2520dataset%2520confirm%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method%2520for%2520remote%2520sensing%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CromSS%3A%20Cross-modal%20pre-training%20with%20noisy%20labels%20for%20remote%20sensing%0A%20%20image%20segmentation&entry.906535625=Chenying%20Liu%20and%20Conrad%20Albrecht%20and%20Yi%20Wang%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20We%20study%20the%20potential%20of%20noisy%20labels%20y%20to%20pretrain%20semantic%20segmentation%0Amodels%20in%20a%20multi-modal%20learning%20framework%20for%20geospatial%20applications.%0ASpecifically%2C%20we%20propose%20a%20novel%20Cross-modal%20Sample%20Selection%20method%20%28CromSS%29%0Athat%20utilizes%20the%20class%20distributions%20P%5E%7B%28d%29%7D%28x%2Cc%29%20over%20pixels%20x%20and%20classes%20c%0Amodelled%20by%20multiple%20sensors/modalities%20d%20of%20a%20given%20geospatial%20scene.%0AConsistency%20of%20predictions%20across%20sensors%20%24d%24%20is%20jointly%20informed%20by%20the%0Aentropy%20of%20P%5E%7B%28d%29%7D%28x%2Cc%29.%20Noisy%20label%20sampling%20we%20determine%20by%20the%20confidence%20of%0Aeach%20sensor%20d%20in%20the%20noisy%20class%20label%2C%20P%5E%7B%28d%29%7D%28x%2Cc%3Dy%28x%29%29.%20To%20verify%20the%0Aperformance%20of%20our%20approach%2C%20we%20conduct%20experiments%20with%20Sentinel-1%20%28radar%29%20and%0ASentinel-2%20%28optical%29%20satellite%20imagery%20from%20the%20globally-sampled%20SSL4EO-S12%0Adataset.%20We%20pair%20those%20scenes%20with%209-class%20noisy%20labels%20sourced%20from%20the%20Google%0ADynamic%20World%20project%20for%20pretraining.%20Transfer%20learning%20evaluations%0A%28downstream%20task%29%20on%20the%20DFC2020%20dataset%20confirm%20the%20effectiveness%20of%20the%0Aproposed%20method%20for%20remote%20sensing%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01217v1&entry.124074799=Read"},
{"title": "Perception and Localization of Macular Degeneration Applying\n  Convolutional Neural Network, ResNet and Grad-CAM", "author": "Tahmim Hossain and Sagor Chandro Bakchy", "abstract": "  A well-known retinal disease that sends blurry visions to the affected\npatients is Macular Degeneration. This research is based on classifying the\nhealthy and macular degeneration fundus by localizing the affected region of\nthe fundus. A CNN architecture and CNN with ResNet architecture (ResNet50,\nResNet50v2, ResNet101, ResNet101v2, ResNet152, ResNet152v2) as the backbone are\nused to classify the two types of fundus. The data are split into three\ncategories including (a) Training set is 90% and Testing set is 10% (b)\nTraining set is 80% and Testing set is 20%, (c) Training set is 50% and Testing\nset is 50%. After the training, the best model has been selected from the\nevaluation metrics. Among the models, CNN with a backbone of ResNet50 performs\nbest which gives the training accuracy of 98.7% for 90% train and 10% test data\nsplit. With this model, we have performed the Grad-CAM visualization to get the\nregion of the affected area of the fundus.\n", "link": "http://arxiv.org/abs/2404.15918v2", "date": "2024-05-02", "relevancy": 2.58, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.556}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5222}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%20and%20Localization%20of%20Macular%20Degeneration%20Applying%0A%20%20Convolutional%20Neural%20Network%2C%20ResNet%20and%20Grad-CAM&body=Title%3A%20Perception%20and%20Localization%20of%20Macular%20Degeneration%20Applying%0A%20%20Convolutional%20Neural%20Network%2C%20ResNet%20and%20Grad-CAM%0AAuthor%3A%20Tahmim%20Hossain%20and%20Sagor%20Chandro%20Bakchy%0AAbstract%3A%20%20%20A%20well-known%20retinal%20disease%20that%20sends%20blurry%20visions%20to%20the%20affected%0Apatients%20is%20Macular%20Degeneration.%20This%20research%20is%20based%20on%20classifying%20the%0Ahealthy%20and%20macular%20degeneration%20fundus%20by%20localizing%20the%20affected%20region%20of%0Athe%20fundus.%20A%20CNN%20architecture%20and%20CNN%20with%20ResNet%20architecture%20%28ResNet50%2C%0AResNet50v2%2C%20ResNet101%2C%20ResNet101v2%2C%20ResNet152%2C%20ResNet152v2%29%20as%20the%20backbone%20are%0Aused%20to%20classify%20the%20two%20types%20of%20fundus.%20The%20data%20are%20split%20into%20three%0Acategories%20including%20%28a%29%20Training%20set%20is%2090%25%20and%20Testing%20set%20is%2010%25%20%28b%29%0ATraining%20set%20is%2080%25%20and%20Testing%20set%20is%2020%25%2C%20%28c%29%20Training%20set%20is%2050%25%20and%20Testing%0Aset%20is%2050%25.%20After%20the%20training%2C%20the%20best%20model%20has%20been%20selected%20from%20the%0Aevaluation%20metrics.%20Among%20the%20models%2C%20CNN%20with%20a%20backbone%20of%20ResNet50%20performs%0Abest%20which%20gives%20the%20training%20accuracy%20of%2098.7%25%20for%2090%25%20train%20and%2010%25%20test%20data%0Asplit.%20With%20this%20model%2C%20we%20have%20performed%20the%20Grad-CAM%20visualization%20to%20get%20the%0Aregion%20of%20the%20affected%20area%20of%20the%20fundus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%2520and%2520Localization%2520of%2520Macular%2520Degeneration%2520Applying%250A%2520%2520Convolutional%2520Neural%2520Network%252C%2520ResNet%2520and%2520Grad-CAM%26entry.906535625%3DTahmim%2520Hossain%2520and%2520Sagor%2520Chandro%2520Bakchy%26entry.1292438233%3D%2520%2520A%2520well-known%2520retinal%2520disease%2520that%2520sends%2520blurry%2520visions%2520to%2520the%2520affected%250Apatients%2520is%2520Macular%2520Degeneration.%2520This%2520research%2520is%2520based%2520on%2520classifying%2520the%250Ahealthy%2520and%2520macular%2520degeneration%2520fundus%2520by%2520localizing%2520the%2520affected%2520region%2520of%250Athe%2520fundus.%2520A%2520CNN%2520architecture%2520and%2520CNN%2520with%2520ResNet%2520architecture%2520%2528ResNet50%252C%250AResNet50v2%252C%2520ResNet101%252C%2520ResNet101v2%252C%2520ResNet152%252C%2520ResNet152v2%2529%2520as%2520the%2520backbone%2520are%250Aused%2520to%2520classify%2520the%2520two%2520types%2520of%2520fundus.%2520The%2520data%2520are%2520split%2520into%2520three%250Acategories%2520including%2520%2528a%2529%2520Training%2520set%2520is%252090%2525%2520and%2520Testing%2520set%2520is%252010%2525%2520%2528b%2529%250ATraining%2520set%2520is%252080%2525%2520and%2520Testing%2520set%2520is%252020%2525%252C%2520%2528c%2529%2520Training%2520set%2520is%252050%2525%2520and%2520Testing%250Aset%2520is%252050%2525.%2520After%2520the%2520training%252C%2520the%2520best%2520model%2520has%2520been%2520selected%2520from%2520the%250Aevaluation%2520metrics.%2520Among%2520the%2520models%252C%2520CNN%2520with%2520a%2520backbone%2520of%2520ResNet50%2520performs%250Abest%2520which%2520gives%2520the%2520training%2520accuracy%2520of%252098.7%2525%2520for%252090%2525%2520train%2520and%252010%2525%2520test%2520data%250Asplit.%2520With%2520this%2520model%252C%2520we%2520have%2520performed%2520the%2520Grad-CAM%2520visualization%2520to%2520get%2520the%250Aregion%2520of%2520the%2520affected%2520area%2520of%2520the%2520fundus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%20and%20Localization%20of%20Macular%20Degeneration%20Applying%0A%20%20Convolutional%20Neural%20Network%2C%20ResNet%20and%20Grad-CAM&entry.906535625=Tahmim%20Hossain%20and%20Sagor%20Chandro%20Bakchy&entry.1292438233=%20%20A%20well-known%20retinal%20disease%20that%20sends%20blurry%20visions%20to%20the%20affected%0Apatients%20is%20Macular%20Degeneration.%20This%20research%20is%20based%20on%20classifying%20the%0Ahealthy%20and%20macular%20degeneration%20fundus%20by%20localizing%20the%20affected%20region%20of%0Athe%20fundus.%20A%20CNN%20architecture%20and%20CNN%20with%20ResNet%20architecture%20%28ResNet50%2C%0AResNet50v2%2C%20ResNet101%2C%20ResNet101v2%2C%20ResNet152%2C%20ResNet152v2%29%20as%20the%20backbone%20are%0Aused%20to%20classify%20the%20two%20types%20of%20fundus.%20The%20data%20are%20split%20into%20three%0Acategories%20including%20%28a%29%20Training%20set%20is%2090%25%20and%20Testing%20set%20is%2010%25%20%28b%29%0ATraining%20set%20is%2080%25%20and%20Testing%20set%20is%2020%25%2C%20%28c%29%20Training%20set%20is%2050%25%20and%20Testing%0Aset%20is%2050%25.%20After%20the%20training%2C%20the%20best%20model%20has%20been%20selected%20from%20the%0Aevaluation%20metrics.%20Among%20the%20models%2C%20CNN%20with%20a%20backbone%20of%20ResNet50%20performs%0Abest%20which%20gives%20the%20training%20accuracy%20of%2098.7%25%20for%2090%25%20train%20and%2010%25%20test%20data%0Asplit.%20With%20this%20model%2C%20we%20have%20performed%20the%20Grad-CAM%20visualization%20to%20get%20the%0Aregion%20of%20the%20affected%20area%20of%20the%20fundus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15918v2&entry.124074799=Read"},
{"title": "Goal-conditioned reinforcement learning for ultrasound navigation\n  guidance", "author": "Abdoul Aziz Amadou and Vivek Singh and Florin C. Ghesu and Young-Ho Kim and Laura Stanciulescu and Harshitha P. Sai and Puneet Sharma and Alistair Young and Ronak Rajani and Kawal Rhode", "abstract": "  Transesophageal echocardiography (TEE) plays a pivotal role in cardiology for\ndiagnostic and interventional procedures. However, using it effectively\nrequires extensive training due to the intricate nature of image acquisition\nand interpretation. To enhance the efficiency of novice sonographers and reduce\nvariability in scan acquisitions, we propose a novel ultrasound (US) navigation\nassistance method based on contrastive learning as goal-conditioned\nreinforcement learning (GCRL). We augment the previous framework using a novel\ncontrastive patient batching method (CPB) and a data-augmented contrastive\nloss, both of which we demonstrate are essential to ensure generalization to\nanatomical variations across patients. The proposed framework enables\nnavigation to both standard diagnostic as well as intricate interventional\nviews with a single model. Our method was developed with a large dataset of 789\npatients and obtained an average error of 6.56 mm in position and 9.36 degrees\nin angle on a testing dataset of 140 patients, which is competitive or superior\nto models trained on individual views. Furthermore, we quantitatively validate\nour method's ability to navigate to interventional views such as the Left\nAtrial Appendage (LAA) view used in LAA closure. Our approach holds promise in\nproviding valuable guidance during transesophageal ultrasound examinations,\ncontributing to the advancement of skill acquisition for cardiac ultrasound\npractitioners.\n", "link": "http://arxiv.org/abs/2405.01409v1", "date": "2024-05-02", "relevancy": 2.5106, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5062}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5014}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Goal-conditioned%20reinforcement%20learning%20for%20ultrasound%20navigation%0A%20%20guidance&body=Title%3A%20Goal-conditioned%20reinforcement%20learning%20for%20ultrasound%20navigation%0A%20%20guidance%0AAuthor%3A%20Abdoul%20Aziz%20Amadou%20and%20Vivek%20Singh%20and%20Florin%20C.%20Ghesu%20and%20Young-Ho%20Kim%20and%20Laura%20Stanciulescu%20and%20Harshitha%20P.%20Sai%20and%20Puneet%20Sharma%20and%20Alistair%20Young%20and%20Ronak%20Rajani%20and%20Kawal%20Rhode%0AAbstract%3A%20%20%20Transesophageal%20echocardiography%20%28TEE%29%20plays%20a%20pivotal%20role%20in%20cardiology%20for%0Adiagnostic%20and%20interventional%20procedures.%20However%2C%20using%20it%20effectively%0Arequires%20extensive%20training%20due%20to%20the%20intricate%20nature%20of%20image%20acquisition%0Aand%20interpretation.%20To%20enhance%20the%20efficiency%20of%20novice%20sonographers%20and%20reduce%0Avariability%20in%20scan%20acquisitions%2C%20we%20propose%20a%20novel%20ultrasound%20%28US%29%20navigation%0Aassistance%20method%20based%20on%20contrastive%20learning%20as%20goal-conditioned%0Areinforcement%20learning%20%28GCRL%29.%20We%20augment%20the%20previous%20framework%20using%20a%20novel%0Acontrastive%20patient%20batching%20method%20%28CPB%29%20and%20a%20data-augmented%20contrastive%0Aloss%2C%20both%20of%20which%20we%20demonstrate%20are%20essential%20to%20ensure%20generalization%20to%0Aanatomical%20variations%20across%20patients.%20The%20proposed%20framework%20enables%0Anavigation%20to%20both%20standard%20diagnostic%20as%20well%20as%20intricate%20interventional%0Aviews%20with%20a%20single%20model.%20Our%20method%20was%20developed%20with%20a%20large%20dataset%20of%20789%0Apatients%20and%20obtained%20an%20average%20error%20of%206.56%20mm%20in%20position%20and%209.36%20degrees%0Ain%20angle%20on%20a%20testing%20dataset%20of%20140%20patients%2C%20which%20is%20competitive%20or%20superior%0Ato%20models%20trained%20on%20individual%20views.%20Furthermore%2C%20we%20quantitatively%20validate%0Aour%20method%27s%20ability%20to%20navigate%20to%20interventional%20views%20such%20as%20the%20Left%0AAtrial%20Appendage%20%28LAA%29%20view%20used%20in%20LAA%20closure.%20Our%20approach%20holds%20promise%20in%0Aproviding%20valuable%20guidance%20during%20transesophageal%20ultrasound%20examinations%2C%0Acontributing%20to%20the%20advancement%20of%20skill%20acquisition%20for%20cardiac%20ultrasound%0Apractitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoal-conditioned%2520reinforcement%2520learning%2520for%2520ultrasound%2520navigation%250A%2520%2520guidance%26entry.906535625%3DAbdoul%2520Aziz%2520Amadou%2520and%2520Vivek%2520Singh%2520and%2520Florin%2520C.%2520Ghesu%2520and%2520Young-Ho%2520Kim%2520and%2520Laura%2520Stanciulescu%2520and%2520Harshitha%2520P.%2520Sai%2520and%2520Puneet%2520Sharma%2520and%2520Alistair%2520Young%2520and%2520Ronak%2520Rajani%2520and%2520Kawal%2520Rhode%26entry.1292438233%3D%2520%2520Transesophageal%2520echocardiography%2520%2528TEE%2529%2520plays%2520a%2520pivotal%2520role%2520in%2520cardiology%2520for%250Adiagnostic%2520and%2520interventional%2520procedures.%2520However%252C%2520using%2520it%2520effectively%250Arequires%2520extensive%2520training%2520due%2520to%2520the%2520intricate%2520nature%2520of%2520image%2520acquisition%250Aand%2520interpretation.%2520To%2520enhance%2520the%2520efficiency%2520of%2520novice%2520sonographers%2520and%2520reduce%250Avariability%2520in%2520scan%2520acquisitions%252C%2520we%2520propose%2520a%2520novel%2520ultrasound%2520%2528US%2529%2520navigation%250Aassistance%2520method%2520based%2520on%2520contrastive%2520learning%2520as%2520goal-conditioned%250Areinforcement%2520learning%2520%2528GCRL%2529.%2520We%2520augment%2520the%2520previous%2520framework%2520using%2520a%2520novel%250Acontrastive%2520patient%2520batching%2520method%2520%2528CPB%2529%2520and%2520a%2520data-augmented%2520contrastive%250Aloss%252C%2520both%2520of%2520which%2520we%2520demonstrate%2520are%2520essential%2520to%2520ensure%2520generalization%2520to%250Aanatomical%2520variations%2520across%2520patients.%2520The%2520proposed%2520framework%2520enables%250Anavigation%2520to%2520both%2520standard%2520diagnostic%2520as%2520well%2520as%2520intricate%2520interventional%250Aviews%2520with%2520a%2520single%2520model.%2520Our%2520method%2520was%2520developed%2520with%2520a%2520large%2520dataset%2520of%2520789%250Apatients%2520and%2520obtained%2520an%2520average%2520error%2520of%25206.56%2520mm%2520in%2520position%2520and%25209.36%2520degrees%250Ain%2520angle%2520on%2520a%2520testing%2520dataset%2520of%2520140%2520patients%252C%2520which%2520is%2520competitive%2520or%2520superior%250Ato%2520models%2520trained%2520on%2520individual%2520views.%2520Furthermore%252C%2520we%2520quantitatively%2520validate%250Aour%2520method%2527s%2520ability%2520to%2520navigate%2520to%2520interventional%2520views%2520such%2520as%2520the%2520Left%250AAtrial%2520Appendage%2520%2528LAA%2529%2520view%2520used%2520in%2520LAA%2520closure.%2520Our%2520approach%2520holds%2520promise%2520in%250Aproviding%2520valuable%2520guidance%2520during%2520transesophageal%2520ultrasound%2520examinations%252C%250Acontributing%2520to%2520the%2520advancement%2520of%2520skill%2520acquisition%2520for%2520cardiac%2520ultrasound%250Apractitioners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goal-conditioned%20reinforcement%20learning%20for%20ultrasound%20navigation%0A%20%20guidance&entry.906535625=Abdoul%20Aziz%20Amadou%20and%20Vivek%20Singh%20and%20Florin%20C.%20Ghesu%20and%20Young-Ho%20Kim%20and%20Laura%20Stanciulescu%20and%20Harshitha%20P.%20Sai%20and%20Puneet%20Sharma%20and%20Alistair%20Young%20and%20Ronak%20Rajani%20and%20Kawal%20Rhode&entry.1292438233=%20%20Transesophageal%20echocardiography%20%28TEE%29%20plays%20a%20pivotal%20role%20in%20cardiology%20for%0Adiagnostic%20and%20interventional%20procedures.%20However%2C%20using%20it%20effectively%0Arequires%20extensive%20training%20due%20to%20the%20intricate%20nature%20of%20image%20acquisition%0Aand%20interpretation.%20To%20enhance%20the%20efficiency%20of%20novice%20sonographers%20and%20reduce%0Avariability%20in%20scan%20acquisitions%2C%20we%20propose%20a%20novel%20ultrasound%20%28US%29%20navigation%0Aassistance%20method%20based%20on%20contrastive%20learning%20as%20goal-conditioned%0Areinforcement%20learning%20%28GCRL%29.%20We%20augment%20the%20previous%20framework%20using%20a%20novel%0Acontrastive%20patient%20batching%20method%20%28CPB%29%20and%20a%20data-augmented%20contrastive%0Aloss%2C%20both%20of%20which%20we%20demonstrate%20are%20essential%20to%20ensure%20generalization%20to%0Aanatomical%20variations%20across%20patients.%20The%20proposed%20framework%20enables%0Anavigation%20to%20both%20standard%20diagnostic%20as%20well%20as%20intricate%20interventional%0Aviews%20with%20a%20single%20model.%20Our%20method%20was%20developed%20with%20a%20large%20dataset%20of%20789%0Apatients%20and%20obtained%20an%20average%20error%20of%206.56%20mm%20in%20position%20and%209.36%20degrees%0Ain%20angle%20on%20a%20testing%20dataset%20of%20140%20patients%2C%20which%20is%20competitive%20or%20superior%0Ato%20models%20trained%20on%20individual%20views.%20Furthermore%2C%20we%20quantitatively%20validate%0Aour%20method%27s%20ability%20to%20navigate%20to%20interventional%20views%20such%20as%20the%20Left%0AAtrial%20Appendage%20%28LAA%29%20view%20used%20in%20LAA%20closure.%20Our%20approach%20holds%20promise%20in%0Aproviding%20valuable%20guidance%20during%20transesophageal%20ultrasound%20examinations%2C%0Acontributing%20to%20the%20advancement%20of%20skill%20acquisition%20for%20cardiac%20ultrasound%0Apractitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01409v1&entry.124074799=Read"},
{"title": "Sparse is Enough in Fine-tuning Pre-trained Large Language Models", "author": "Weixi Song and Zuchao Li and Lefei Zhang and Hai Zhao and Bo Du", "abstract": "  With the prevalence of pre-training-fine-tuning paradigm, how to efficiently\nadapt the pre-trained model to the downstream tasks has been an intriguing\nissue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for\nlow-cost adaptation. Although PEFT has demonstrated effectiveness and been\nwidely applied, the underlying principles are still unclear. In this paper, we\nadopt the PAC-Bayesian generalization error bound, viewing pre-training as a\nshift of prior distribution which leads to a tighter bound for generalization\nerror. We validate this shift from the perspectives of oscillations in the loss\nlandscape and the quasi-sparsity in gradient distribution. Based on this, we\npropose a gradient-based sparse fine-tuning algorithm, named Sparse Increment\nFine-Tuning (SIFT), and validate its effectiveness on a range of tasks\nincluding the GLUE Benchmark and Instruction-tuning. The code is accessible at\nhttps://github.com/song-wx/SIFT/.\n", "link": "http://arxiv.org/abs/2312.11875v2", "date": "2024-05-02", "relevancy": 2.4931, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5168}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4945}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20is%20Enough%20in%20Fine-tuning%20Pre-trained%20Large%20Language%20Models&body=Title%3A%20Sparse%20is%20Enough%20in%20Fine-tuning%20Pre-trained%20Large%20Language%20Models%0AAuthor%3A%20Weixi%20Song%20and%20Zuchao%20Li%20and%20Lefei%20Zhang%20and%20Hai%20Zhao%20and%20Bo%20Du%0AAbstract%3A%20%20%20With%20the%20prevalence%20of%20pre-training-fine-tuning%20paradigm%2C%20how%20to%20efficiently%0Aadapt%20the%20pre-trained%20model%20to%20the%20downstream%20tasks%20has%20been%20an%20intriguing%0Aissue.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20have%20been%20proposed%20for%0Alow-cost%20adaptation.%20Although%20PEFT%20has%20demonstrated%20effectiveness%20and%20been%0Awidely%20applied%2C%20the%20underlying%20principles%20are%20still%20unclear.%20In%20this%20paper%2C%20we%0Aadopt%20the%20PAC-Bayesian%20generalization%20error%20bound%2C%20viewing%20pre-training%20as%20a%0Ashift%20of%20prior%20distribution%20which%20leads%20to%20a%20tighter%20bound%20for%20generalization%0Aerror.%20We%20validate%20this%20shift%20from%20the%20perspectives%20of%20oscillations%20in%20the%20loss%0Alandscape%20and%20the%20quasi-sparsity%20in%20gradient%20distribution.%20Based%20on%20this%2C%20we%0Apropose%20a%20gradient-based%20sparse%20fine-tuning%20algorithm%2C%20named%20Sparse%20Increment%0AFine-Tuning%20%28SIFT%29%2C%20and%20validate%20its%20effectiveness%20on%20a%20range%20of%20tasks%0Aincluding%20the%20GLUE%20Benchmark%20and%20Instruction-tuning.%20The%20code%20is%20accessible%20at%0Ahttps%3A//github.com/song-wx/SIFT/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11875v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520is%2520Enough%2520in%2520Fine-tuning%2520Pre-trained%2520Large%2520Language%2520Models%26entry.906535625%3DWeixi%2520Song%2520and%2520Zuchao%2520Li%2520and%2520Lefei%2520Zhang%2520and%2520Hai%2520Zhao%2520and%2520Bo%2520Du%26entry.1292438233%3D%2520%2520With%2520the%2520prevalence%2520of%2520pre-training-fine-tuning%2520paradigm%252C%2520how%2520to%2520efficiently%250Aadapt%2520the%2520pre-trained%2520model%2520to%2520the%2520downstream%2520tasks%2520has%2520been%2520an%2520intriguing%250Aissue.%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520have%2520been%2520proposed%2520for%250Alow-cost%2520adaptation.%2520Although%2520PEFT%2520has%2520demonstrated%2520effectiveness%2520and%2520been%250Awidely%2520applied%252C%2520the%2520underlying%2520principles%2520are%2520still%2520unclear.%2520In%2520this%2520paper%252C%2520we%250Aadopt%2520the%2520PAC-Bayesian%2520generalization%2520error%2520bound%252C%2520viewing%2520pre-training%2520as%2520a%250Ashift%2520of%2520prior%2520distribution%2520which%2520leads%2520to%2520a%2520tighter%2520bound%2520for%2520generalization%250Aerror.%2520We%2520validate%2520this%2520shift%2520from%2520the%2520perspectives%2520of%2520oscillations%2520in%2520the%2520loss%250Alandscape%2520and%2520the%2520quasi-sparsity%2520in%2520gradient%2520distribution.%2520Based%2520on%2520this%252C%2520we%250Apropose%2520a%2520gradient-based%2520sparse%2520fine-tuning%2520algorithm%252C%2520named%2520Sparse%2520Increment%250AFine-Tuning%2520%2528SIFT%2529%252C%2520and%2520validate%2520its%2520effectiveness%2520on%2520a%2520range%2520of%2520tasks%250Aincluding%2520the%2520GLUE%2520Benchmark%2520and%2520Instruction-tuning.%2520The%2520code%2520is%2520accessible%2520at%250Ahttps%253A//github.com/song-wx/SIFT/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11875v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20is%20Enough%20in%20Fine-tuning%20Pre-trained%20Large%20Language%20Models&entry.906535625=Weixi%20Song%20and%20Zuchao%20Li%20and%20Lefei%20Zhang%20and%20Hai%20Zhao%20and%20Bo%20Du&entry.1292438233=%20%20With%20the%20prevalence%20of%20pre-training-fine-tuning%20paradigm%2C%20how%20to%20efficiently%0Aadapt%20the%20pre-trained%20model%20to%20the%20downstream%20tasks%20has%20been%20an%20intriguing%0Aissue.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20have%20been%20proposed%20for%0Alow-cost%20adaptation.%20Although%20PEFT%20has%20demonstrated%20effectiveness%20and%20been%0Awidely%20applied%2C%20the%20underlying%20principles%20are%20still%20unclear.%20In%20this%20paper%2C%20we%0Aadopt%20the%20PAC-Bayesian%20generalization%20error%20bound%2C%20viewing%20pre-training%20as%20a%0Ashift%20of%20prior%20distribution%20which%20leads%20to%20a%20tighter%20bound%20for%20generalization%0Aerror.%20We%20validate%20this%20shift%20from%20the%20perspectives%20of%20oscillations%20in%20the%20loss%0Alandscape%20and%20the%20quasi-sparsity%20in%20gradient%20distribution.%20Based%20on%20this%2C%20we%0Apropose%20a%20gradient-based%20sparse%20fine-tuning%20algorithm%2C%20named%20Sparse%20Increment%0AFine-Tuning%20%28SIFT%29%2C%20and%20validate%20its%20effectiveness%20on%20a%20range%20of%20tasks%0Aincluding%20the%20GLUE%20Benchmark%20and%20Instruction-tuning.%20The%20code%20is%20accessible%20at%0Ahttps%3A//github.com/song-wx/SIFT/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11875v2&entry.124074799=Read"},
{"title": "Evaluation of Video-Based rPPG in Challenging Environments: Artifact\n  Mitigation and Network Resilience", "author": "Nhi Nguyen and Le Nguyen and Honghan Li and Miguel Bordallo L\u00f3pez and Constantino \u00c1lvarez Casado", "abstract": "  Video-based remote photoplethysmography (rPPG) has emerged as a promising\ntechnology for non-contact vital sign monitoring, especially under controlled\nconditions. However, the accurate measurement of vital signs in real-world\nscenarios faces several challenges, including artifacts induced by videocodecs,\nlow-light noise, degradation, low dynamic range, occlusions, and hardware and\nnetwork constraints. In this article, we systematically investigate\ncomprehensive investigate these issues, measuring their detrimental effects on\nthe quality of rPPG measurements. Additionally, we propose practical strategies\nfor mitigating these challenges to improve the dependability and resilience of\nvideo-based rPPG systems. We detail methods for effective biosignal recovery in\nthe presence of network limitations and present denoising and inpainting\ntechniques aimed at preserving video frame integrity. Through extensive\nevaluations and direct comparisons, we demonstrate the effectiveness of the\napproaches in enhancing rPPG measurements under challenging environments,\ncontributing to the development of more reliable and effective remote vital\nsign monitoring technologies.\n", "link": "http://arxiv.org/abs/2405.01230v1", "date": "2024-05-02", "relevancy": 2.486, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5216}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Video-Based%20rPPG%20in%20Challenging%20Environments%3A%20Artifact%0A%20%20Mitigation%20and%20Network%20Resilience&body=Title%3A%20Evaluation%20of%20Video-Based%20rPPG%20in%20Challenging%20Environments%3A%20Artifact%0A%20%20Mitigation%20and%20Network%20Resilience%0AAuthor%3A%20Nhi%20Nguyen%20and%20Le%20Nguyen%20and%20Honghan%20Li%20and%20Miguel%20Bordallo%20L%C3%B3pez%20and%20Constantino%20%C3%81lvarez%20Casado%0AAbstract%3A%20%20%20Video-based%20remote%20photoplethysmography%20%28rPPG%29%20has%20emerged%20as%20a%20promising%0Atechnology%20for%20non-contact%20vital%20sign%20monitoring%2C%20especially%20under%20controlled%0Aconditions.%20However%2C%20the%20accurate%20measurement%20of%20vital%20signs%20in%20real-world%0Ascenarios%20faces%20several%20challenges%2C%20including%20artifacts%20induced%20by%20videocodecs%2C%0Alow-light%20noise%2C%20degradation%2C%20low%20dynamic%20range%2C%20occlusions%2C%20and%20hardware%20and%0Anetwork%20constraints.%20In%20this%20article%2C%20we%20systematically%20investigate%0Acomprehensive%20investigate%20these%20issues%2C%20measuring%20their%20detrimental%20effects%20on%0Athe%20quality%20of%20rPPG%20measurements.%20Additionally%2C%20we%20propose%20practical%20strategies%0Afor%20mitigating%20these%20challenges%20to%20improve%20the%20dependability%20and%20resilience%20of%0Avideo-based%20rPPG%20systems.%20We%20detail%20methods%20for%20effective%20biosignal%20recovery%20in%0Athe%20presence%20of%20network%20limitations%20and%20present%20denoising%20and%20inpainting%0Atechniques%20aimed%20at%20preserving%20video%20frame%20integrity.%20Through%20extensive%0Aevaluations%20and%20direct%20comparisons%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%0Aapproaches%20in%20enhancing%20rPPG%20measurements%20under%20challenging%20environments%2C%0Acontributing%20to%20the%20development%20of%20more%20reliable%20and%20effective%20remote%20vital%0Asign%20monitoring%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Video-Based%2520rPPG%2520in%2520Challenging%2520Environments%253A%2520Artifact%250A%2520%2520Mitigation%2520and%2520Network%2520Resilience%26entry.906535625%3DNhi%2520Nguyen%2520and%2520Le%2520Nguyen%2520and%2520Honghan%2520Li%2520and%2520Miguel%2520Bordallo%2520L%25C3%25B3pez%2520and%2520Constantino%2520%25C3%2581lvarez%2520Casado%26entry.1292438233%3D%2520%2520Video-based%2520remote%2520photoplethysmography%2520%2528rPPG%2529%2520has%2520emerged%2520as%2520a%2520promising%250Atechnology%2520for%2520non-contact%2520vital%2520sign%2520monitoring%252C%2520especially%2520under%2520controlled%250Aconditions.%2520However%252C%2520the%2520accurate%2520measurement%2520of%2520vital%2520signs%2520in%2520real-world%250Ascenarios%2520faces%2520several%2520challenges%252C%2520including%2520artifacts%2520induced%2520by%2520videocodecs%252C%250Alow-light%2520noise%252C%2520degradation%252C%2520low%2520dynamic%2520range%252C%2520occlusions%252C%2520and%2520hardware%2520and%250Anetwork%2520constraints.%2520In%2520this%2520article%252C%2520we%2520systematically%2520investigate%250Acomprehensive%2520investigate%2520these%2520issues%252C%2520measuring%2520their%2520detrimental%2520effects%2520on%250Athe%2520quality%2520of%2520rPPG%2520measurements.%2520Additionally%252C%2520we%2520propose%2520practical%2520strategies%250Afor%2520mitigating%2520these%2520challenges%2520to%2520improve%2520the%2520dependability%2520and%2520resilience%2520of%250Avideo-based%2520rPPG%2520systems.%2520We%2520detail%2520methods%2520for%2520effective%2520biosignal%2520recovery%2520in%250Athe%2520presence%2520of%2520network%2520limitations%2520and%2520present%2520denoising%2520and%2520inpainting%250Atechniques%2520aimed%2520at%2520preserving%2520video%2520frame%2520integrity.%2520Through%2520extensive%250Aevaluations%2520and%2520direct%2520comparisons%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aapproaches%2520in%2520enhancing%2520rPPG%2520measurements%2520under%2520challenging%2520environments%252C%250Acontributing%2520to%2520the%2520development%2520of%2520more%2520reliable%2520and%2520effective%2520remote%2520vital%250Asign%2520monitoring%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Video-Based%20rPPG%20in%20Challenging%20Environments%3A%20Artifact%0A%20%20Mitigation%20and%20Network%20Resilience&entry.906535625=Nhi%20Nguyen%20and%20Le%20Nguyen%20and%20Honghan%20Li%20and%20Miguel%20Bordallo%20L%C3%B3pez%20and%20Constantino%20%C3%81lvarez%20Casado&entry.1292438233=%20%20Video-based%20remote%20photoplethysmography%20%28rPPG%29%20has%20emerged%20as%20a%20promising%0Atechnology%20for%20non-contact%20vital%20sign%20monitoring%2C%20especially%20under%20controlled%0Aconditions.%20However%2C%20the%20accurate%20measurement%20of%20vital%20signs%20in%20real-world%0Ascenarios%20faces%20several%20challenges%2C%20including%20artifacts%20induced%20by%20videocodecs%2C%0Alow-light%20noise%2C%20degradation%2C%20low%20dynamic%20range%2C%20occlusions%2C%20and%20hardware%20and%0Anetwork%20constraints.%20In%20this%20article%2C%20we%20systematically%20investigate%0Acomprehensive%20investigate%20these%20issues%2C%20measuring%20their%20detrimental%20effects%20on%0Athe%20quality%20of%20rPPG%20measurements.%20Additionally%2C%20we%20propose%20practical%20strategies%0Afor%20mitigating%20these%20challenges%20to%20improve%20the%20dependability%20and%20resilience%20of%0Avideo-based%20rPPG%20systems.%20We%20detail%20methods%20for%20effective%20biosignal%20recovery%20in%0Athe%20presence%20of%20network%20limitations%20and%20present%20denoising%20and%20inpainting%0Atechniques%20aimed%20at%20preserving%20video%20frame%20integrity.%20Through%20extensive%0Aevaluations%20and%20direct%20comparisons%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%0Aapproaches%20in%20enhancing%20rPPG%20measurements%20under%20challenging%20environments%2C%0Acontributing%20to%20the%20development%20of%20more%20reliable%20and%20effective%20remote%20vital%0Asign%20monitoring%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01230v1&entry.124074799=Read"},
{"title": "NeuroLGP-SM: Scalable Surrogate-Assisted Neuroevolution for Deep Neural\n  Networks", "author": "Fergal Stapleton and Edgar Galv\u00e1n", "abstract": "  Evolutionary Algorithms (EAs) play a crucial role in the architectural\nconfiguration and training of Artificial Deep Neural Networks (DNNs), a process\nknown as neuroevolution. However, neuroevolution is hindered by its inherent\ncomputational expense, requiring multiple generations, a large population, and\nnumerous epochs. The most computationally intensive aspect lies in evaluating\nthe fitness function of a single candidate solution. To address this challenge,\nwe employ Surrogate-assisted EAs (SAEAs). While a few SAEAs approaches have\nbeen proposed in neuroevolution, none have been applied to truly large DNNs due\nto issues like intractable information usage. In this work, drawing inspiration\nfrom Genetic Programming semantics, we use phenotypic distance vectors,\noutputted from DNNs, alongside Kriging Partial Least Squares (KPLS), an\napproach that is effective in handling these large vectors, making them\nsuitable for search. Our proposed approach, named Neuro-Linear Genetic\nProgramming surrogate model (NeuroLGP-SM), efficiently and accurately estimates\nDNN fitness without the need for complete evaluations. NeuroLGP-SM demonstrates\ncompetitive or superior results compared to 12 other methods, including\nNeuroLGP without SM, convolutional neural networks, support vector machines,\nand autoencoders. Additionally, it is worth noting that NeuroLGP-SM is 25% more\nenergy-efficient than its NeuroLGP counterpart. This efficiency advantage adds\nto the overall appeal of our proposed NeuroLGP-SM in optimising the\nconfiguration of large DNNs.\n", "link": "http://arxiv.org/abs/2404.08786v3", "date": "2024-05-02", "relevancy": 2.4478, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4993}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4853}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroLGP-SM%3A%20Scalable%20Surrogate-Assisted%20Neuroevolution%20for%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20NeuroLGP-SM%3A%20Scalable%20Surrogate-Assisted%20Neuroevolution%20for%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Fergal%20Stapleton%20and%20Edgar%20Galv%C3%A1n%0AAbstract%3A%20%20%20Evolutionary%20Algorithms%20%28EAs%29%20play%20a%20crucial%20role%20in%20the%20architectural%0Aconfiguration%20and%20training%20of%20Artificial%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20a%20process%0Aknown%20as%20neuroevolution.%20However%2C%20neuroevolution%20is%20hindered%20by%20its%20inherent%0Acomputational%20expense%2C%20requiring%20multiple%20generations%2C%20a%20large%20population%2C%20and%0Anumerous%20epochs.%20The%20most%20computationally%20intensive%20aspect%20lies%20in%20evaluating%0Athe%20fitness%20function%20of%20a%20single%20candidate%20solution.%20To%20address%20this%20challenge%2C%0Awe%20employ%20Surrogate-assisted%20EAs%20%28SAEAs%29.%20While%20a%20few%20SAEAs%20approaches%20have%0Abeen%20proposed%20in%20neuroevolution%2C%20none%20have%20been%20applied%20to%20truly%20large%20DNNs%20due%0Ato%20issues%20like%20intractable%20information%20usage.%20In%20this%20work%2C%20drawing%20inspiration%0Afrom%20Genetic%20Programming%20semantics%2C%20we%20use%20phenotypic%20distance%20vectors%2C%0Aoutputted%20from%20DNNs%2C%20alongside%20Kriging%20Partial%20Least%20Squares%20%28KPLS%29%2C%20an%0Aapproach%20that%20is%20effective%20in%20handling%20these%20large%20vectors%2C%20making%20them%0Asuitable%20for%20search.%20Our%20proposed%20approach%2C%20named%20Neuro-Linear%20Genetic%0AProgramming%20surrogate%20model%20%28NeuroLGP-SM%29%2C%20efficiently%20and%20accurately%20estimates%0ADNN%20fitness%20without%20the%20need%20for%20complete%20evaluations.%20NeuroLGP-SM%20demonstrates%0Acompetitive%20or%20superior%20results%20compared%20to%2012%20other%20methods%2C%20including%0ANeuroLGP%20without%20SM%2C%20convolutional%20neural%20networks%2C%20support%20vector%20machines%2C%0Aand%20autoencoders.%20Additionally%2C%20it%20is%20worth%20noting%20that%20NeuroLGP-SM%20is%2025%25%20more%0Aenergy-efficient%20than%20its%20NeuroLGP%20counterpart.%20This%20efficiency%20advantage%20adds%0Ato%20the%20overall%20appeal%20of%20our%20proposed%20NeuroLGP-SM%20in%20optimising%20the%0Aconfiguration%20of%20large%20DNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08786v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroLGP-SM%253A%2520Scalable%2520Surrogate-Assisted%2520Neuroevolution%2520for%2520Deep%2520Neural%250A%2520%2520Networks%26entry.906535625%3DFergal%2520Stapleton%2520and%2520Edgar%2520Galv%25C3%25A1n%26entry.1292438233%3D%2520%2520Evolutionary%2520Algorithms%2520%2528EAs%2529%2520play%2520a%2520crucial%2520role%2520in%2520the%2520architectural%250Aconfiguration%2520and%2520training%2520of%2520Artificial%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%252C%2520a%2520process%250Aknown%2520as%2520neuroevolution.%2520However%252C%2520neuroevolution%2520is%2520hindered%2520by%2520its%2520inherent%250Acomputational%2520expense%252C%2520requiring%2520multiple%2520generations%252C%2520a%2520large%2520population%252C%2520and%250Anumerous%2520epochs.%2520The%2520most%2520computationally%2520intensive%2520aspect%2520lies%2520in%2520evaluating%250Athe%2520fitness%2520function%2520of%2520a%2520single%2520candidate%2520solution.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520employ%2520Surrogate-assisted%2520EAs%2520%2528SAEAs%2529.%2520While%2520a%2520few%2520SAEAs%2520approaches%2520have%250Abeen%2520proposed%2520in%2520neuroevolution%252C%2520none%2520have%2520been%2520applied%2520to%2520truly%2520large%2520DNNs%2520due%250Ato%2520issues%2520like%2520intractable%2520information%2520usage.%2520In%2520this%2520work%252C%2520drawing%2520inspiration%250Afrom%2520Genetic%2520Programming%2520semantics%252C%2520we%2520use%2520phenotypic%2520distance%2520vectors%252C%250Aoutputted%2520from%2520DNNs%252C%2520alongside%2520Kriging%2520Partial%2520Least%2520Squares%2520%2528KPLS%2529%252C%2520an%250Aapproach%2520that%2520is%2520effective%2520in%2520handling%2520these%2520large%2520vectors%252C%2520making%2520them%250Asuitable%2520for%2520search.%2520Our%2520proposed%2520approach%252C%2520named%2520Neuro-Linear%2520Genetic%250AProgramming%2520surrogate%2520model%2520%2528NeuroLGP-SM%2529%252C%2520efficiently%2520and%2520accurately%2520estimates%250ADNN%2520fitness%2520without%2520the%2520need%2520for%2520complete%2520evaluations.%2520NeuroLGP-SM%2520demonstrates%250Acompetitive%2520or%2520superior%2520results%2520compared%2520to%252012%2520other%2520methods%252C%2520including%250ANeuroLGP%2520without%2520SM%252C%2520convolutional%2520neural%2520networks%252C%2520support%2520vector%2520machines%252C%250Aand%2520autoencoders.%2520Additionally%252C%2520it%2520is%2520worth%2520noting%2520that%2520NeuroLGP-SM%2520is%252025%2525%2520more%250Aenergy-efficient%2520than%2520its%2520NeuroLGP%2520counterpart.%2520This%2520efficiency%2520advantage%2520adds%250Ato%2520the%2520overall%2520appeal%2520of%2520our%2520proposed%2520NeuroLGP-SM%2520in%2520optimising%2520the%250Aconfiguration%2520of%2520large%2520DNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08786v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroLGP-SM%3A%20Scalable%20Surrogate-Assisted%20Neuroevolution%20for%20Deep%20Neural%0A%20%20Networks&entry.906535625=Fergal%20Stapleton%20and%20Edgar%20Galv%C3%A1n&entry.1292438233=%20%20Evolutionary%20Algorithms%20%28EAs%29%20play%20a%20crucial%20role%20in%20the%20architectural%0Aconfiguration%20and%20training%20of%20Artificial%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20a%20process%0Aknown%20as%20neuroevolution.%20However%2C%20neuroevolution%20is%20hindered%20by%20its%20inherent%0Acomputational%20expense%2C%20requiring%20multiple%20generations%2C%20a%20large%20population%2C%20and%0Anumerous%20epochs.%20The%20most%20computationally%20intensive%20aspect%20lies%20in%20evaluating%0Athe%20fitness%20function%20of%20a%20single%20candidate%20solution.%20To%20address%20this%20challenge%2C%0Awe%20employ%20Surrogate-assisted%20EAs%20%28SAEAs%29.%20While%20a%20few%20SAEAs%20approaches%20have%0Abeen%20proposed%20in%20neuroevolution%2C%20none%20have%20been%20applied%20to%20truly%20large%20DNNs%20due%0Ato%20issues%20like%20intractable%20information%20usage.%20In%20this%20work%2C%20drawing%20inspiration%0Afrom%20Genetic%20Programming%20semantics%2C%20we%20use%20phenotypic%20distance%20vectors%2C%0Aoutputted%20from%20DNNs%2C%20alongside%20Kriging%20Partial%20Least%20Squares%20%28KPLS%29%2C%20an%0Aapproach%20that%20is%20effective%20in%20handling%20these%20large%20vectors%2C%20making%20them%0Asuitable%20for%20search.%20Our%20proposed%20approach%2C%20named%20Neuro-Linear%20Genetic%0AProgramming%20surrogate%20model%20%28NeuroLGP-SM%29%2C%20efficiently%20and%20accurately%20estimates%0ADNN%20fitness%20without%20the%20need%20for%20complete%20evaluations.%20NeuroLGP-SM%20demonstrates%0Acompetitive%20or%20superior%20results%20compared%20to%2012%20other%20methods%2C%20including%0ANeuroLGP%20without%20SM%2C%20convolutional%20neural%20networks%2C%20support%20vector%20machines%2C%0Aand%20autoencoders.%20Additionally%2C%20it%20is%20worth%20noting%20that%20NeuroLGP-SM%20is%2025%25%20more%0Aenergy-efficient%20than%20its%20NeuroLGP%20counterpart.%20This%20efficiency%20advantage%20adds%0Ato%20the%20overall%20appeal%20of%20our%20proposed%20NeuroLGP-SM%20in%20optimising%20the%0Aconfiguration%20of%20large%20DNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08786v3&entry.124074799=Read"},
{"title": "Evaluation and Optimization of Adaptive Cruise Control in Autonomous\n  Vehicles using the CARLA Simulator: A Study on Performance under Wet and Dry\n  Weather Conditions", "author": "Roza Al-Hindaw and Taqwa I. Alhadidi and Mohammad Adas", "abstract": "  Adaptive Cruise Control ACC can change the speed of the ego vehicle to\nmaintain a safe distance from the following vehicle automatically. The primary\npurpose of this research is to use cutting-edge computing approaches to locate\nand track vehicles in real time under various conditions to achieve a safe ACC.\nThe paper examines the extension of ACC employing depth cameras and radar\nsensors within Autonomous Vehicles AVs to respond in real time by changing\nweather conditions using the Car Learning to Act CARLA simulation platform at\nnoon. The ego vehicle controller's decision to accelerate or decelerate depends\non the speed of the leading ahead vehicle and the safe distance from that\nvehicle. Simulation results show that a Proportional Integral Derivative PID\ncontrol of autonomous vehicles using a depth camera and radar sensors reduces\nthe speed of the leading vehicle and the ego vehicle when it rains. In\naddition, longer travel time was observed for both vehicles in rainy conditions\nthan in dry conditions. Also, PID control prevents the leading vehicle from\nrear collisions\n", "link": "http://arxiv.org/abs/2405.01504v1", "date": "2024-05-02", "relevancy": 2.4474, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4976}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20and%20Optimization%20of%20Adaptive%20Cruise%20Control%20in%20Autonomous%0A%20%20Vehicles%20using%20the%20CARLA%20Simulator%3A%20A%20Study%20on%20Performance%20under%20Wet%20and%20Dry%0A%20%20Weather%20Conditions&body=Title%3A%20Evaluation%20and%20Optimization%20of%20Adaptive%20Cruise%20Control%20in%20Autonomous%0A%20%20Vehicles%20using%20the%20CARLA%20Simulator%3A%20A%20Study%20on%20Performance%20under%20Wet%20and%20Dry%0A%20%20Weather%20Conditions%0AAuthor%3A%20Roza%20Al-Hindaw%20and%20Taqwa%20I.%20Alhadidi%20and%20Mohammad%20Adas%0AAbstract%3A%20%20%20Adaptive%20Cruise%20Control%20ACC%20can%20change%20the%20speed%20of%20the%20ego%20vehicle%20to%0Amaintain%20a%20safe%20distance%20from%20the%20following%20vehicle%20automatically.%20The%20primary%0Apurpose%20of%20this%20research%20is%20to%20use%20cutting-edge%20computing%20approaches%20to%20locate%0Aand%20track%20vehicles%20in%20real%20time%20under%20various%20conditions%20to%20achieve%20a%20safe%20ACC.%0AThe%20paper%20examines%20the%20extension%20of%20ACC%20employing%20depth%20cameras%20and%20radar%0Asensors%20within%20Autonomous%20Vehicles%20AVs%20to%20respond%20in%20real%20time%20by%20changing%0Aweather%20conditions%20using%20the%20Car%20Learning%20to%20Act%20CARLA%20simulation%20platform%20at%0Anoon.%20The%20ego%20vehicle%20controller%27s%20decision%20to%20accelerate%20or%20decelerate%20depends%0Aon%20the%20speed%20of%20the%20leading%20ahead%20vehicle%20and%20the%20safe%20distance%20from%20that%0Avehicle.%20Simulation%20results%20show%20that%20a%20Proportional%20Integral%20Derivative%20PID%0Acontrol%20of%20autonomous%20vehicles%20using%20a%20depth%20camera%20and%20radar%20sensors%20reduces%0Athe%20speed%20of%20the%20leading%20vehicle%20and%20the%20ego%20vehicle%20when%20it%20rains.%20In%0Aaddition%2C%20longer%20travel%20time%20was%20observed%20for%20both%20vehicles%20in%20rainy%20conditions%0Athan%20in%20dry%20conditions.%20Also%2C%20PID%20control%20prevents%20the%20leading%20vehicle%20from%0Arear%20collisions%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520and%2520Optimization%2520of%2520Adaptive%2520Cruise%2520Control%2520in%2520Autonomous%250A%2520%2520Vehicles%2520using%2520the%2520CARLA%2520Simulator%253A%2520A%2520Study%2520on%2520Performance%2520under%2520Wet%2520and%2520Dry%250A%2520%2520Weather%2520Conditions%26entry.906535625%3DRoza%2520Al-Hindaw%2520and%2520Taqwa%2520I.%2520Alhadidi%2520and%2520Mohammad%2520Adas%26entry.1292438233%3D%2520%2520Adaptive%2520Cruise%2520Control%2520ACC%2520can%2520change%2520the%2520speed%2520of%2520the%2520ego%2520vehicle%2520to%250Amaintain%2520a%2520safe%2520distance%2520from%2520the%2520following%2520vehicle%2520automatically.%2520The%2520primary%250Apurpose%2520of%2520this%2520research%2520is%2520to%2520use%2520cutting-edge%2520computing%2520approaches%2520to%2520locate%250Aand%2520track%2520vehicles%2520in%2520real%2520time%2520under%2520various%2520conditions%2520to%2520achieve%2520a%2520safe%2520ACC.%250AThe%2520paper%2520examines%2520the%2520extension%2520of%2520ACC%2520employing%2520depth%2520cameras%2520and%2520radar%250Asensors%2520within%2520Autonomous%2520Vehicles%2520AVs%2520to%2520respond%2520in%2520real%2520time%2520by%2520changing%250Aweather%2520conditions%2520using%2520the%2520Car%2520Learning%2520to%2520Act%2520CARLA%2520simulation%2520platform%2520at%250Anoon.%2520The%2520ego%2520vehicle%2520controller%2527s%2520decision%2520to%2520accelerate%2520or%2520decelerate%2520depends%250Aon%2520the%2520speed%2520of%2520the%2520leading%2520ahead%2520vehicle%2520and%2520the%2520safe%2520distance%2520from%2520that%250Avehicle.%2520Simulation%2520results%2520show%2520that%2520a%2520Proportional%2520Integral%2520Derivative%2520PID%250Acontrol%2520of%2520autonomous%2520vehicles%2520using%2520a%2520depth%2520camera%2520and%2520radar%2520sensors%2520reduces%250Athe%2520speed%2520of%2520the%2520leading%2520vehicle%2520and%2520the%2520ego%2520vehicle%2520when%2520it%2520rains.%2520In%250Aaddition%252C%2520longer%2520travel%2520time%2520was%2520observed%2520for%2520both%2520vehicles%2520in%2520rainy%2520conditions%250Athan%2520in%2520dry%2520conditions.%2520Also%252C%2520PID%2520control%2520prevents%2520the%2520leading%2520vehicle%2520from%250Arear%2520collisions%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20and%20Optimization%20of%20Adaptive%20Cruise%20Control%20in%20Autonomous%0A%20%20Vehicles%20using%20the%20CARLA%20Simulator%3A%20A%20Study%20on%20Performance%20under%20Wet%20and%20Dry%0A%20%20Weather%20Conditions&entry.906535625=Roza%20Al-Hindaw%20and%20Taqwa%20I.%20Alhadidi%20and%20Mohammad%20Adas&entry.1292438233=%20%20Adaptive%20Cruise%20Control%20ACC%20can%20change%20the%20speed%20of%20the%20ego%20vehicle%20to%0Amaintain%20a%20safe%20distance%20from%20the%20following%20vehicle%20automatically.%20The%20primary%0Apurpose%20of%20this%20research%20is%20to%20use%20cutting-edge%20computing%20approaches%20to%20locate%0Aand%20track%20vehicles%20in%20real%20time%20under%20various%20conditions%20to%20achieve%20a%20safe%20ACC.%0AThe%20paper%20examines%20the%20extension%20of%20ACC%20employing%20depth%20cameras%20and%20radar%0Asensors%20within%20Autonomous%20Vehicles%20AVs%20to%20respond%20in%20real%20time%20by%20changing%0Aweather%20conditions%20using%20the%20Car%20Learning%20to%20Act%20CARLA%20simulation%20platform%20at%0Anoon.%20The%20ego%20vehicle%20controller%27s%20decision%20to%20accelerate%20or%20decelerate%20depends%0Aon%20the%20speed%20of%20the%20leading%20ahead%20vehicle%20and%20the%20safe%20distance%20from%20that%0Avehicle.%20Simulation%20results%20show%20that%20a%20Proportional%20Integral%20Derivative%20PID%0Acontrol%20of%20autonomous%20vehicles%20using%20a%20depth%20camera%20and%20radar%20sensors%20reduces%0Athe%20speed%20of%20the%20leading%20vehicle%20and%20the%20ego%20vehicle%20when%20it%20rains.%20In%0Aaddition%2C%20longer%20travel%20time%20was%20observed%20for%20both%20vehicles%20in%20rainy%20conditions%0Athan%20in%20dry%20conditions.%20Also%2C%20PID%20control%20prevents%20the%20leading%20vehicle%20from%0Arear%20collisions%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01504v1&entry.124074799=Read"},
{"title": "IntervenGen: Interventional Data Generation for Robust and\n  Data-Efficient Robot Imitation Learning", "author": "Ryan Hoque and Ajay Mandlekar and Caelan Garrett and Ken Goldberg and Dieter Fox", "abstract": "  Imitation learning is a promising paradigm for training robot control\npolicies, but these policies can suffer from distribution shift, where the\nconditions at evaluation time differ from those in the training data. A popular\napproach for increasing policy robustness to distribution shift is interactive\nimitation learning (i.e., DAgger and variants), where a human operator provides\ncorrective interventions during policy rollouts. However, collecting a\nsufficient amount of interventions to cover the distribution of policy mistakes\ncan be burdensome for human operators. We propose IntervenGen (I-Gen), a novel\ndata generation system that can autonomously produce a large set of corrective\ninterventions with rich coverage of the state space from a small number of\nhuman interventions. We apply I-Gen to 4 simulated environments and 1 physical\nenvironment with object pose estimation error and show that it can increase\npolicy robustness by up to 39x with only 10 human interventions. Videos and\nmore results are available at https://sites.google.com/view/intervengen2024.\n", "link": "http://arxiv.org/abs/2405.01472v1", "date": "2024-05-02", "relevancy": 2.4295, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6525}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.588}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IntervenGen%3A%20Interventional%20Data%20Generation%20for%20Robust%20and%0A%20%20Data-Efficient%20Robot%20Imitation%20Learning&body=Title%3A%20IntervenGen%3A%20Interventional%20Data%20Generation%20for%20Robust%20and%0A%20%20Data-Efficient%20Robot%20Imitation%20Learning%0AAuthor%3A%20Ryan%20Hoque%20and%20Ajay%20Mandlekar%20and%20Caelan%20Garrett%20and%20Ken%20Goldberg%20and%20Dieter%20Fox%0AAbstract%3A%20%20%20Imitation%20learning%20is%20a%20promising%20paradigm%20for%20training%20robot%20control%0Apolicies%2C%20but%20these%20policies%20can%20suffer%20from%20distribution%20shift%2C%20where%20the%0Aconditions%20at%20evaluation%20time%20differ%20from%20those%20in%20the%20training%20data.%20A%20popular%0Aapproach%20for%20increasing%20policy%20robustness%20to%20distribution%20shift%20is%20interactive%0Aimitation%20learning%20%28i.e.%2C%20DAgger%20and%20variants%29%2C%20where%20a%20human%20operator%20provides%0Acorrective%20interventions%20during%20policy%20rollouts.%20However%2C%20collecting%20a%0Asufficient%20amount%20of%20interventions%20to%20cover%20the%20distribution%20of%20policy%20mistakes%0Acan%20be%20burdensome%20for%20human%20operators.%20We%20propose%20IntervenGen%20%28I-Gen%29%2C%20a%20novel%0Adata%20generation%20system%20that%20can%20autonomously%20produce%20a%20large%20set%20of%20corrective%0Ainterventions%20with%20rich%20coverage%20of%20the%20state%20space%20from%20a%20small%20number%20of%0Ahuman%20interventions.%20We%20apply%20I-Gen%20to%204%20simulated%20environments%20and%201%20physical%0Aenvironment%20with%20object%20pose%20estimation%20error%20and%20show%20that%20it%20can%20increase%0Apolicy%20robustness%20by%20up%20to%2039x%20with%20only%2010%20human%20interventions.%20Videos%20and%0Amore%20results%20are%20available%20at%20https%3A//sites.google.com/view/intervengen2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntervenGen%253A%2520Interventional%2520Data%2520Generation%2520for%2520Robust%2520and%250A%2520%2520Data-Efficient%2520Robot%2520Imitation%2520Learning%26entry.906535625%3DRyan%2520Hoque%2520and%2520Ajay%2520Mandlekar%2520and%2520Caelan%2520Garrett%2520and%2520Ken%2520Goldberg%2520and%2520Dieter%2520Fox%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520is%2520a%2520promising%2520paradigm%2520for%2520training%2520robot%2520control%250Apolicies%252C%2520but%2520these%2520policies%2520can%2520suffer%2520from%2520distribution%2520shift%252C%2520where%2520the%250Aconditions%2520at%2520evaluation%2520time%2520differ%2520from%2520those%2520in%2520the%2520training%2520data.%2520A%2520popular%250Aapproach%2520for%2520increasing%2520policy%2520robustness%2520to%2520distribution%2520shift%2520is%2520interactive%250Aimitation%2520learning%2520%2528i.e.%252C%2520DAgger%2520and%2520variants%2529%252C%2520where%2520a%2520human%2520operator%2520provides%250Acorrective%2520interventions%2520during%2520policy%2520rollouts.%2520However%252C%2520collecting%2520a%250Asufficient%2520amount%2520of%2520interventions%2520to%2520cover%2520the%2520distribution%2520of%2520policy%2520mistakes%250Acan%2520be%2520burdensome%2520for%2520human%2520operators.%2520We%2520propose%2520IntervenGen%2520%2528I-Gen%2529%252C%2520a%2520novel%250Adata%2520generation%2520system%2520that%2520can%2520autonomously%2520produce%2520a%2520large%2520set%2520of%2520corrective%250Ainterventions%2520with%2520rich%2520coverage%2520of%2520the%2520state%2520space%2520from%2520a%2520small%2520number%2520of%250Ahuman%2520interventions.%2520We%2520apply%2520I-Gen%2520to%25204%2520simulated%2520environments%2520and%25201%2520physical%250Aenvironment%2520with%2520object%2520pose%2520estimation%2520error%2520and%2520show%2520that%2520it%2520can%2520increase%250Apolicy%2520robustness%2520by%2520up%2520to%252039x%2520with%2520only%252010%2520human%2520interventions.%2520Videos%2520and%250Amore%2520results%2520are%2520available%2520at%2520https%253A//sites.google.com/view/intervengen2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IntervenGen%3A%20Interventional%20Data%20Generation%20for%20Robust%20and%0A%20%20Data-Efficient%20Robot%20Imitation%20Learning&entry.906535625=Ryan%20Hoque%20and%20Ajay%20Mandlekar%20and%20Caelan%20Garrett%20and%20Ken%20Goldberg%20and%20Dieter%20Fox&entry.1292438233=%20%20Imitation%20learning%20is%20a%20promising%20paradigm%20for%20training%20robot%20control%0Apolicies%2C%20but%20these%20policies%20can%20suffer%20from%20distribution%20shift%2C%20where%20the%0Aconditions%20at%20evaluation%20time%20differ%20from%20those%20in%20the%20training%20data.%20A%20popular%0Aapproach%20for%20increasing%20policy%20robustness%20to%20distribution%20shift%20is%20interactive%0Aimitation%20learning%20%28i.e.%2C%20DAgger%20and%20variants%29%2C%20where%20a%20human%20operator%20provides%0Acorrective%20interventions%20during%20policy%20rollouts.%20However%2C%20collecting%20a%0Asufficient%20amount%20of%20interventions%20to%20cover%20the%20distribution%20of%20policy%20mistakes%0Acan%20be%20burdensome%20for%20human%20operators.%20We%20propose%20IntervenGen%20%28I-Gen%29%2C%20a%20novel%0Adata%20generation%20system%20that%20can%20autonomously%20produce%20a%20large%20set%20of%20corrective%0Ainterventions%20with%20rich%20coverage%20of%20the%20state%20space%20from%20a%20small%20number%20of%0Ahuman%20interventions.%20We%20apply%20I-Gen%20to%204%20simulated%20environments%20and%201%20physical%0Aenvironment%20with%20object%20pose%20estimation%20error%20and%20show%20that%20it%20can%20increase%0Apolicy%20robustness%20by%20up%20to%2039x%20with%20only%2010%20human%20interventions.%20Videos%20and%0Amore%20results%20are%20available%20at%20https%3A//sites.google.com/view/intervengen2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01472v1&entry.124074799=Read"},
{"title": "Learning to Embed Time Series Patches Independently", "author": "Seunghan Lee and Taeyoung Park and Kibok Lee", "abstract": "  Masked time series modeling has recently gained much attention as a\nself-supervised representation learning strategy for time series. Inspired by\nmasked image modeling in computer vision, recent works first patchify and\npartially mask out time series, and then train Transformers to capture the\ndependencies between patches by predicting masked patches from unmasked\npatches. However, we argue that capturing such patch dependencies might not be\nan optimal strategy for time series representation learning; rather, learning\nto embed patches independently results in better time series representations.\nSpecifically, we propose to use 1) the simple patch reconstruction task, which\nautoencode each patch without looking at other patches, and 2) the simple\npatch-wise MLP that embeds each patch independently. In addition, we introduce\ncomplementary contrastive learning to hierarchically capture adjacent time\nseries information efficiently. Our proposed method improves time series\nforecasting and classification performance compared to state-of-the-art\nTransformer-based models, while it is more efficient in terms of the number of\nparameters and training/inference time. Code is available at this repository:\nhttps://github.com/seunghan96/pits.\n", "link": "http://arxiv.org/abs/2312.16427v4", "date": "2024-05-02", "relevancy": 2.4288, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5065}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4816}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Embed%20Time%20Series%20Patches%20Independently&body=Title%3A%20Learning%20to%20Embed%20Time%20Series%20Patches%20Independently%0AAuthor%3A%20Seunghan%20Lee%20and%20Taeyoung%20Park%20and%20Kibok%20Lee%0AAbstract%3A%20%20%20Masked%20time%20series%20modeling%20has%20recently%20gained%20much%20attention%20as%20a%0Aself-supervised%20representation%20learning%20strategy%20for%20time%20series.%20Inspired%20by%0Amasked%20image%20modeling%20in%20computer%20vision%2C%20recent%20works%20first%20patchify%20and%0Apartially%20mask%20out%20time%20series%2C%20and%20then%20train%20Transformers%20to%20capture%20the%0Adependencies%20between%20patches%20by%20predicting%20masked%20patches%20from%20unmasked%0Apatches.%20However%2C%20we%20argue%20that%20capturing%20such%20patch%20dependencies%20might%20not%20be%0Aan%20optimal%20strategy%20for%20time%20series%20representation%20learning%3B%20rather%2C%20learning%0Ato%20embed%20patches%20independently%20results%20in%20better%20time%20series%20representations.%0ASpecifically%2C%20we%20propose%20to%20use%201%29%20the%20simple%20patch%20reconstruction%20task%2C%20which%0Aautoencode%20each%20patch%20without%20looking%20at%20other%20patches%2C%20and%202%29%20the%20simple%0Apatch-wise%20MLP%20that%20embeds%20each%20patch%20independently.%20In%20addition%2C%20we%20introduce%0Acomplementary%20contrastive%20learning%20to%20hierarchically%20capture%20adjacent%20time%0Aseries%20information%20efficiently.%20Our%20proposed%20method%20improves%20time%20series%0Aforecasting%20and%20classification%20performance%20compared%20to%20state-of-the-art%0ATransformer-based%20models%2C%20while%20it%20is%20more%20efficient%20in%20terms%20of%20the%20number%20of%0Aparameters%20and%20training/inference%20time.%20Code%20is%20available%20at%20this%20repository%3A%0Ahttps%3A//github.com/seunghan96/pits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16427v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Embed%2520Time%2520Series%2520Patches%2520Independently%26entry.906535625%3DSeunghan%2520Lee%2520and%2520Taeyoung%2520Park%2520and%2520Kibok%2520Lee%26entry.1292438233%3D%2520%2520Masked%2520time%2520series%2520modeling%2520has%2520recently%2520gained%2520much%2520attention%2520as%2520a%250Aself-supervised%2520representation%2520learning%2520strategy%2520for%2520time%2520series.%2520Inspired%2520by%250Amasked%2520image%2520modeling%2520in%2520computer%2520vision%252C%2520recent%2520works%2520first%2520patchify%2520and%250Apartially%2520mask%2520out%2520time%2520series%252C%2520and%2520then%2520train%2520Transformers%2520to%2520capture%2520the%250Adependencies%2520between%2520patches%2520by%2520predicting%2520masked%2520patches%2520from%2520unmasked%250Apatches.%2520However%252C%2520we%2520argue%2520that%2520capturing%2520such%2520patch%2520dependencies%2520might%2520not%2520be%250Aan%2520optimal%2520strategy%2520for%2520time%2520series%2520representation%2520learning%253B%2520rather%252C%2520learning%250Ato%2520embed%2520patches%2520independently%2520results%2520in%2520better%2520time%2520series%2520representations.%250ASpecifically%252C%2520we%2520propose%2520to%2520use%25201%2529%2520the%2520simple%2520patch%2520reconstruction%2520task%252C%2520which%250Aautoencode%2520each%2520patch%2520without%2520looking%2520at%2520other%2520patches%252C%2520and%25202%2529%2520the%2520simple%250Apatch-wise%2520MLP%2520that%2520embeds%2520each%2520patch%2520independently.%2520In%2520addition%252C%2520we%2520introduce%250Acomplementary%2520contrastive%2520learning%2520to%2520hierarchically%2520capture%2520adjacent%2520time%250Aseries%2520information%2520efficiently.%2520Our%2520proposed%2520method%2520improves%2520time%2520series%250Aforecasting%2520and%2520classification%2520performance%2520compared%2520to%2520state-of-the-art%250ATransformer-based%2520models%252C%2520while%2520it%2520is%2520more%2520efficient%2520in%2520terms%2520of%2520the%2520number%2520of%250Aparameters%2520and%2520training/inference%2520time.%2520Code%2520is%2520available%2520at%2520this%2520repository%253A%250Ahttps%253A//github.com/seunghan96/pits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.16427v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Embed%20Time%20Series%20Patches%20Independently&entry.906535625=Seunghan%20Lee%20and%20Taeyoung%20Park%20and%20Kibok%20Lee&entry.1292438233=%20%20Masked%20time%20series%20modeling%20has%20recently%20gained%20much%20attention%20as%20a%0Aself-supervised%20representation%20learning%20strategy%20for%20time%20series.%20Inspired%20by%0Amasked%20image%20modeling%20in%20computer%20vision%2C%20recent%20works%20first%20patchify%20and%0Apartially%20mask%20out%20time%20series%2C%20and%20then%20train%20Transformers%20to%20capture%20the%0Adependencies%20between%20patches%20by%20predicting%20masked%20patches%20from%20unmasked%0Apatches.%20However%2C%20we%20argue%20that%20capturing%20such%20patch%20dependencies%20might%20not%20be%0Aan%20optimal%20strategy%20for%20time%20series%20representation%20learning%3B%20rather%2C%20learning%0Ato%20embed%20patches%20independently%20results%20in%20better%20time%20series%20representations.%0ASpecifically%2C%20we%20propose%20to%20use%201%29%20the%20simple%20patch%20reconstruction%20task%2C%20which%0Aautoencode%20each%20patch%20without%20looking%20at%20other%20patches%2C%20and%202%29%20the%20simple%0Apatch-wise%20MLP%20that%20embeds%20each%20patch%20independently.%20In%20addition%2C%20we%20introduce%0Acomplementary%20contrastive%20learning%20to%20hierarchically%20capture%20adjacent%20time%0Aseries%20information%20efficiently.%20Our%20proposed%20method%20improves%20time%20series%0Aforecasting%20and%20classification%20performance%20compared%20to%20state-of-the-art%0ATransformer-based%20models%2C%20while%20it%20is%20more%20efficient%20in%20terms%20of%20the%20number%20of%0Aparameters%20and%20training/inference%20time.%20Code%20is%20available%20at%20this%20repository%3A%0Ahttps%3A//github.com/seunghan96/pits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16427v4&entry.124074799=Read"},
{"title": "Lying Graph Convolution: Learning to Lie for Node Classification Tasks", "author": "Daniele Castellana", "abstract": "  In the context of machine learning for graphs, many researchers have\nempirically observed that Deep Graph Networks (DGNs) perform favourably on node\nclassification tasks when the graph structure is homophilic (\\ie adjacent nodes\nare similar). In this paper, we introduce Lying-GCN, a new DGN inspired by\nopinion dynamics that can adaptively work in both the heterophilic and the\nhomophilic setting. At each layer, each agent (node) shares its own opinions\n(node embeddings) with its neighbours. Instead of sharing its opinion directly\nas in GCN, we introduce a mechanism which allows agents to lie. Such a\nmechanism is adaptive, thus the agents learn how and when to lie according to\nthe task that should be solved. We provide a characterisation of our proposal\nin terms of dynamical systems, by studying the spectral property of the\ncoefficient matrix of the system. While the steady state of the system\ncollapses to zero, we believe the lying mechanism is still usable to solve node\nclassification tasks. We empirically prove our belief on both synthetic and\nreal-world datasets, by showing that the lying mechanism allows to increase the\nperformances in the heterophilic setting without harming the results in the\nhomophilic one.\n", "link": "http://arxiv.org/abs/2405.01247v1", "date": "2024-05-02", "relevancy": 2.4184, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5109}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.471}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lying%20Graph%20Convolution%3A%20Learning%20to%20Lie%20for%20Node%20Classification%20Tasks&body=Title%3A%20Lying%20Graph%20Convolution%3A%20Learning%20to%20Lie%20for%20Node%20Classification%20Tasks%0AAuthor%3A%20Daniele%20Castellana%0AAbstract%3A%20%20%20In%20the%20context%20of%20machine%20learning%20for%20graphs%2C%20many%20researchers%20have%0Aempirically%20observed%20that%20Deep%20Graph%20Networks%20%28DGNs%29%20perform%20favourably%20on%20node%0Aclassification%20tasks%20when%20the%20graph%20structure%20is%20homophilic%20%28%5Cie%20adjacent%20nodes%0Aare%20similar%29.%20In%20this%20paper%2C%20we%20introduce%20Lying-GCN%2C%20a%20new%20DGN%20inspired%20by%0Aopinion%20dynamics%20that%20can%20adaptively%20work%20in%20both%20the%20heterophilic%20and%20the%0Ahomophilic%20setting.%20At%20each%20layer%2C%20each%20agent%20%28node%29%20shares%20its%20own%20opinions%0A%28node%20embeddings%29%20with%20its%20neighbours.%20Instead%20of%20sharing%20its%20opinion%20directly%0Aas%20in%20GCN%2C%20we%20introduce%20a%20mechanism%20which%20allows%20agents%20to%20lie.%20Such%20a%0Amechanism%20is%20adaptive%2C%20thus%20the%20agents%20learn%20how%20and%20when%20to%20lie%20according%20to%0Athe%20task%20that%20should%20be%20solved.%20We%20provide%20a%20characterisation%20of%20our%20proposal%0Ain%20terms%20of%20dynamical%20systems%2C%20by%20studying%20the%20spectral%20property%20of%20the%0Acoefficient%20matrix%20of%20the%20system.%20While%20the%20steady%20state%20of%20the%20system%0Acollapses%20to%20zero%2C%20we%20believe%20the%20lying%20mechanism%20is%20still%20usable%20to%20solve%20node%0Aclassification%20tasks.%20We%20empirically%20prove%20our%20belief%20on%20both%20synthetic%20and%0Areal-world%20datasets%2C%20by%20showing%20that%20the%20lying%20mechanism%20allows%20to%20increase%20the%0Aperformances%20in%20the%20heterophilic%20setting%20without%20harming%20the%20results%20in%20the%0Ahomophilic%20one.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLying%2520Graph%2520Convolution%253A%2520Learning%2520to%2520Lie%2520for%2520Node%2520Classification%2520Tasks%26entry.906535625%3DDaniele%2520Castellana%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520machine%2520learning%2520for%2520graphs%252C%2520many%2520researchers%2520have%250Aempirically%2520observed%2520that%2520Deep%2520Graph%2520Networks%2520%2528DGNs%2529%2520perform%2520favourably%2520on%2520node%250Aclassification%2520tasks%2520when%2520the%2520graph%2520structure%2520is%2520homophilic%2520%2528%255Cie%2520adjacent%2520nodes%250Aare%2520similar%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Lying-GCN%252C%2520a%2520new%2520DGN%2520inspired%2520by%250Aopinion%2520dynamics%2520that%2520can%2520adaptively%2520work%2520in%2520both%2520the%2520heterophilic%2520and%2520the%250Ahomophilic%2520setting.%2520At%2520each%2520layer%252C%2520each%2520agent%2520%2528node%2529%2520shares%2520its%2520own%2520opinions%250A%2528node%2520embeddings%2529%2520with%2520its%2520neighbours.%2520Instead%2520of%2520sharing%2520its%2520opinion%2520directly%250Aas%2520in%2520GCN%252C%2520we%2520introduce%2520a%2520mechanism%2520which%2520allows%2520agents%2520to%2520lie.%2520Such%2520a%250Amechanism%2520is%2520adaptive%252C%2520thus%2520the%2520agents%2520learn%2520how%2520and%2520when%2520to%2520lie%2520according%2520to%250Athe%2520task%2520that%2520should%2520be%2520solved.%2520We%2520provide%2520a%2520characterisation%2520of%2520our%2520proposal%250Ain%2520terms%2520of%2520dynamical%2520systems%252C%2520by%2520studying%2520the%2520spectral%2520property%2520of%2520the%250Acoefficient%2520matrix%2520of%2520the%2520system.%2520While%2520the%2520steady%2520state%2520of%2520the%2520system%250Acollapses%2520to%2520zero%252C%2520we%2520believe%2520the%2520lying%2520mechanism%2520is%2520still%2520usable%2520to%2520solve%2520node%250Aclassification%2520tasks.%2520We%2520empirically%2520prove%2520our%2520belief%2520on%2520both%2520synthetic%2520and%250Areal-world%2520datasets%252C%2520by%2520showing%2520that%2520the%2520lying%2520mechanism%2520allows%2520to%2520increase%2520the%250Aperformances%2520in%2520the%2520heterophilic%2520setting%2520without%2520harming%2520the%2520results%2520in%2520the%250Ahomophilic%2520one.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lying%20Graph%20Convolution%3A%20Learning%20to%20Lie%20for%20Node%20Classification%20Tasks&entry.906535625=Daniele%20Castellana&entry.1292438233=%20%20In%20the%20context%20of%20machine%20learning%20for%20graphs%2C%20many%20researchers%20have%0Aempirically%20observed%20that%20Deep%20Graph%20Networks%20%28DGNs%29%20perform%20favourably%20on%20node%0Aclassification%20tasks%20when%20the%20graph%20structure%20is%20homophilic%20%28%5Cie%20adjacent%20nodes%0Aare%20similar%29.%20In%20this%20paper%2C%20we%20introduce%20Lying-GCN%2C%20a%20new%20DGN%20inspired%20by%0Aopinion%20dynamics%20that%20can%20adaptively%20work%20in%20both%20the%20heterophilic%20and%20the%0Ahomophilic%20setting.%20At%20each%20layer%2C%20each%20agent%20%28node%29%20shares%20its%20own%20opinions%0A%28node%20embeddings%29%20with%20its%20neighbours.%20Instead%20of%20sharing%20its%20opinion%20directly%0Aas%20in%20GCN%2C%20we%20introduce%20a%20mechanism%20which%20allows%20agents%20to%20lie.%20Such%20a%0Amechanism%20is%20adaptive%2C%20thus%20the%20agents%20learn%20how%20and%20when%20to%20lie%20according%20to%0Athe%20task%20that%20should%20be%20solved.%20We%20provide%20a%20characterisation%20of%20our%20proposal%0Ain%20terms%20of%20dynamical%20systems%2C%20by%20studying%20the%20spectral%20property%20of%20the%0Acoefficient%20matrix%20of%20the%20system.%20While%20the%20steady%20state%20of%20the%20system%0Acollapses%20to%20zero%2C%20we%20believe%20the%20lying%20mechanism%20is%20still%20usable%20to%20solve%20node%0Aclassification%20tasks.%20We%20empirically%20prove%20our%20belief%20on%20both%20synthetic%20and%0Areal-world%20datasets%2C%20by%20showing%20that%20the%20lying%20mechanism%20allows%20to%20increase%20the%0Aperformances%20in%20the%20heterophilic%20setting%20without%20harming%20the%20results%20in%20the%0Ahomophilic%20one.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01247v1&entry.124074799=Read"},
{"title": "Learning-to-solve unit commitment based on few-shot physics-guided\n  spatial-temporal graph convolution network", "author": "Mei Yang and Gao Qiu andJunyong Liu and Kai Liu", "abstract": "  This letter proposes a few-shot physics-guided spatial temporal graph\nconvolutional network (FPG-STGCN) to fast solve unit commitment (UC). Firstly,\nSTGCN is tailored to parameterize UC. Then, few-shot physics-guided learning\nscheme is proposed. It exploits few typical UC solutions yielded via commercial\noptimizer to escape from local minimum, and leverages the augmented Lagrangian\nmethod for constraint satisfaction. To further enable both feasibility and\ncontinuous relaxation for integers in learning process, straight-through\nestimator for Tanh-Sign composition is proposed to fully differentiate the\nmixed integer solution space. Case study on the IEEE benchmark justifies that,\nour method bests mainstream learning ways on UC feasibility, and surpasses\ntraditional solver on efficiency.\n", "link": "http://arxiv.org/abs/2405.01200v1", "date": "2024-05-02", "relevancy": 2.4109, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4866}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4836}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-to-solve%20unit%20commitment%20based%20on%20few-shot%20physics-guided%0A%20%20spatial-temporal%20graph%20convolution%20network&body=Title%3A%20Learning-to-solve%20unit%20commitment%20based%20on%20few-shot%20physics-guided%0A%20%20spatial-temporal%20graph%20convolution%20network%0AAuthor%3A%20Mei%20Yang%20and%20Gao%20Qiu%20andJunyong%20Liu%20and%20Kai%20Liu%0AAbstract%3A%20%20%20This%20letter%20proposes%20a%20few-shot%20physics-guided%20spatial%20temporal%20graph%0Aconvolutional%20network%20%28FPG-STGCN%29%20to%20fast%20solve%20unit%20commitment%20%28UC%29.%20Firstly%2C%0ASTGCN%20is%20tailored%20to%20parameterize%20UC.%20Then%2C%20few-shot%20physics-guided%20learning%0Ascheme%20is%20proposed.%20It%20exploits%20few%20typical%20UC%20solutions%20yielded%20via%20commercial%0Aoptimizer%20to%20escape%20from%20local%20minimum%2C%20and%20leverages%20the%20augmented%20Lagrangian%0Amethod%20for%20constraint%20satisfaction.%20To%20further%20enable%20both%20feasibility%20and%0Acontinuous%20relaxation%20for%20integers%20in%20learning%20process%2C%20straight-through%0Aestimator%20for%20Tanh-Sign%20composition%20is%20proposed%20to%20fully%20differentiate%20the%0Amixed%20integer%20solution%20space.%20Case%20study%20on%20the%20IEEE%20benchmark%20justifies%20that%2C%0Aour%20method%20bests%20mainstream%20learning%20ways%20on%20UC%20feasibility%2C%20and%20surpasses%0Atraditional%20solver%20on%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-to-solve%2520unit%2520commitment%2520based%2520on%2520few-shot%2520physics-guided%250A%2520%2520spatial-temporal%2520graph%2520convolution%2520network%26entry.906535625%3DMei%2520Yang%2520and%2520Gao%2520Qiu%2520andJunyong%2520Liu%2520and%2520Kai%2520Liu%26entry.1292438233%3D%2520%2520This%2520letter%2520proposes%2520a%2520few-shot%2520physics-guided%2520spatial%2520temporal%2520graph%250Aconvolutional%2520network%2520%2528FPG-STGCN%2529%2520to%2520fast%2520solve%2520unit%2520commitment%2520%2528UC%2529.%2520Firstly%252C%250ASTGCN%2520is%2520tailored%2520to%2520parameterize%2520UC.%2520Then%252C%2520few-shot%2520physics-guided%2520learning%250Ascheme%2520is%2520proposed.%2520It%2520exploits%2520few%2520typical%2520UC%2520solutions%2520yielded%2520via%2520commercial%250Aoptimizer%2520to%2520escape%2520from%2520local%2520minimum%252C%2520and%2520leverages%2520the%2520augmented%2520Lagrangian%250Amethod%2520for%2520constraint%2520satisfaction.%2520To%2520further%2520enable%2520both%2520feasibility%2520and%250Acontinuous%2520relaxation%2520for%2520integers%2520in%2520learning%2520process%252C%2520straight-through%250Aestimator%2520for%2520Tanh-Sign%2520composition%2520is%2520proposed%2520to%2520fully%2520differentiate%2520the%250Amixed%2520integer%2520solution%2520space.%2520Case%2520study%2520on%2520the%2520IEEE%2520benchmark%2520justifies%2520that%252C%250Aour%2520method%2520bests%2520mainstream%2520learning%2520ways%2520on%2520UC%2520feasibility%252C%2520and%2520surpasses%250Atraditional%2520solver%2520on%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-to-solve%20unit%20commitment%20based%20on%20few-shot%20physics-guided%0A%20%20spatial-temporal%20graph%20convolution%20network&entry.906535625=Mei%20Yang%20and%20Gao%20Qiu%20andJunyong%20Liu%20and%20Kai%20Liu&entry.1292438233=%20%20This%20letter%20proposes%20a%20few-shot%20physics-guided%20spatial%20temporal%20graph%0Aconvolutional%20network%20%28FPG-STGCN%29%20to%20fast%20solve%20unit%20commitment%20%28UC%29.%20Firstly%2C%0ASTGCN%20is%20tailored%20to%20parameterize%20UC.%20Then%2C%20few-shot%20physics-guided%20learning%0Ascheme%20is%20proposed.%20It%20exploits%20few%20typical%20UC%20solutions%20yielded%20via%20commercial%0Aoptimizer%20to%20escape%20from%20local%20minimum%2C%20and%20leverages%20the%20augmented%20Lagrangian%0Amethod%20for%20constraint%20satisfaction.%20To%20further%20enable%20both%20feasibility%20and%0Acontinuous%20relaxation%20for%20integers%20in%20learning%20process%2C%20straight-through%0Aestimator%20for%20Tanh-Sign%20composition%20is%20proposed%20to%20fully%20differentiate%20the%0Amixed%20integer%20solution%20space.%20Case%20study%20on%20the%20IEEE%20benchmark%20justifies%20that%2C%0Aour%20method%20bests%20mainstream%20learning%20ways%20on%20UC%20feasibility%2C%20and%20surpasses%0Atraditional%20solver%20on%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01200v1&entry.124074799=Read"},
{"title": "Improving Subgraph-GNNs via Edge-Level Ego-Network Encodings", "author": "Nurudin Alvarez-Gonzalez and Andreas Kaltenbrunner and Vicen\u00e7 G\u00f3mez", "abstract": "  We present a novel edge-level ego-network encoding for learning on graphs\nthat can boost Message Passing Graph Neural Networks (MP-GNNs) by providing\nadditional node and edge features or extending message-passing formats. The\nproposed encoding is sufficient to distinguish Strongly Regular Graphs, a\nfamily of challenging 3-WL equivalent graphs. We show theoretically that such\nencoding is more expressive than node-based sub-graph MP-GNNs. In an empirical\nevaluation on four benchmarks with 10 graph datasets, our results match or\nimprove previous baselines on expressivity, graph classification, graph\nregression, and proximity tasks -- while reducing memory usage by 18.1x in\ncertain real-world settings.\n", "link": "http://arxiv.org/abs/2312.05905v2", "date": "2024-05-02", "relevancy": 2.4074, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5203}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4745}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Subgraph-GNNs%20via%20Edge-Level%20Ego-Network%20Encodings&body=Title%3A%20Improving%20Subgraph-GNNs%20via%20Edge-Level%20Ego-Network%20Encodings%0AAuthor%3A%20Nurudin%20Alvarez-Gonzalez%20and%20Andreas%20Kaltenbrunner%20and%20Vicen%C3%A7%20G%C3%B3mez%0AAbstract%3A%20%20%20We%20present%20a%20novel%20edge-level%20ego-network%20encoding%20for%20learning%20on%20graphs%0Athat%20can%20boost%20Message%20Passing%20Graph%20Neural%20Networks%20%28MP-GNNs%29%20by%20providing%0Aadditional%20node%20and%20edge%20features%20or%20extending%20message-passing%20formats.%20The%0Aproposed%20encoding%20is%20sufficient%20to%20distinguish%20Strongly%20Regular%20Graphs%2C%20a%0Afamily%20of%20challenging%203-WL%20equivalent%20graphs.%20We%20show%20theoretically%20that%20such%0Aencoding%20is%20more%20expressive%20than%20node-based%20sub-graph%20MP-GNNs.%20In%20an%20empirical%0Aevaluation%20on%20four%20benchmarks%20with%2010%20graph%20datasets%2C%20our%20results%20match%20or%0Aimprove%20previous%20baselines%20on%20expressivity%2C%20graph%20classification%2C%20graph%0Aregression%2C%20and%20proximity%20tasks%20--%20while%20reducing%20memory%20usage%20by%2018.1x%20in%0Acertain%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05905v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Subgraph-GNNs%2520via%2520Edge-Level%2520Ego-Network%2520Encodings%26entry.906535625%3DNurudin%2520Alvarez-Gonzalez%2520and%2520Andreas%2520Kaltenbrunner%2520and%2520Vicen%25C3%25A7%2520G%25C3%25B3mez%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520edge-level%2520ego-network%2520encoding%2520for%2520learning%2520on%2520graphs%250Athat%2520can%2520boost%2520Message%2520Passing%2520Graph%2520Neural%2520Networks%2520%2528MP-GNNs%2529%2520by%2520providing%250Aadditional%2520node%2520and%2520edge%2520features%2520or%2520extending%2520message-passing%2520formats.%2520The%250Aproposed%2520encoding%2520is%2520sufficient%2520to%2520distinguish%2520Strongly%2520Regular%2520Graphs%252C%2520a%250Afamily%2520of%2520challenging%25203-WL%2520equivalent%2520graphs.%2520We%2520show%2520theoretically%2520that%2520such%250Aencoding%2520is%2520more%2520expressive%2520than%2520node-based%2520sub-graph%2520MP-GNNs.%2520In%2520an%2520empirical%250Aevaluation%2520on%2520four%2520benchmarks%2520with%252010%2520graph%2520datasets%252C%2520our%2520results%2520match%2520or%250Aimprove%2520previous%2520baselines%2520on%2520expressivity%252C%2520graph%2520classification%252C%2520graph%250Aregression%252C%2520and%2520proximity%2520tasks%2520--%2520while%2520reducing%2520memory%2520usage%2520by%252018.1x%2520in%250Acertain%2520real-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05905v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Subgraph-GNNs%20via%20Edge-Level%20Ego-Network%20Encodings&entry.906535625=Nurudin%20Alvarez-Gonzalez%20and%20Andreas%20Kaltenbrunner%20and%20Vicen%C3%A7%20G%C3%B3mez&entry.1292438233=%20%20We%20present%20a%20novel%20edge-level%20ego-network%20encoding%20for%20learning%20on%20graphs%0Athat%20can%20boost%20Message%20Passing%20Graph%20Neural%20Networks%20%28MP-GNNs%29%20by%20providing%0Aadditional%20node%20and%20edge%20features%20or%20extending%20message-passing%20formats.%20The%0Aproposed%20encoding%20is%20sufficient%20to%20distinguish%20Strongly%20Regular%20Graphs%2C%20a%0Afamily%20of%20challenging%203-WL%20equivalent%20graphs.%20We%20show%20theoretically%20that%20such%0Aencoding%20is%20more%20expressive%20than%20node-based%20sub-graph%20MP-GNNs.%20In%20an%20empirical%0Aevaluation%20on%20four%20benchmarks%20with%2010%20graph%20datasets%2C%20our%20results%20match%20or%0Aimprove%20previous%20baselines%20on%20expressivity%2C%20graph%20classification%2C%20graph%0Aregression%2C%20and%20proximity%20tasks%20--%20while%20reducing%20memory%20usage%20by%2018.1x%20in%0Acertain%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05905v2&entry.124074799=Read"},
{"title": "ATOM: Attention Mixer for Efficient Dataset Distillation", "author": "Samir Khaki and Ahmad Sajedi and Kai Wang and Lucy Z. Liu and Yuri A. Lawryshyn and Konstantinos N. Plataniotis", "abstract": "  Recent works in dataset distillation seek to minimize training expenses by\ngenerating a condensed synthetic dataset that encapsulates the information\npresent in a larger real dataset. These approaches ultimately aim to attain\ntest accuracy levels akin to those achieved by models trained on the entirety\nof the original dataset. Previous studies in feature and distribution matching\nhave achieved significant results without incurring the costs of bi-level\noptimization in the distillation process. Despite their convincing efficiency,\nmany of these methods suffer from marginal downstream performance improvements,\nlimited distillation of contextual information, and subpar cross-architecture\ngeneralization. To address these challenges in dataset distillation, we propose\nthe ATtentiOn Mixer (ATOM) module to efficiently distill large datasets using a\nmixture of channel and spatial-wise attention in the feature matching process.\nSpatial-wise attention helps guide the learning process based on consistent\nlocalization of classes in their respective images, allowing for distillation\nfrom a broader receptive field. Meanwhile, channel-wise attention captures the\ncontextual information associated with the class itself, thus making the\nsynthetic image more informative for training. By integrating both types of\nattention, our ATOM module demonstrates superior performance across various\ncomputer vision datasets, including CIFAR10/100 and TinyImagenet. Notably, our\nmethod significantly improves performance in scenarios with a low number of\nimages per class, thereby enhancing its potential. Furthermore, we maintain the\nimprovement in cross-architectures and applications such as neural architecture\nsearch.\n", "link": "http://arxiv.org/abs/2405.01373v1", "date": "2024-05-02", "relevancy": 2.3872, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6546}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5723}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATOM%3A%20Attention%20Mixer%20for%20Efficient%20Dataset%20Distillation&body=Title%3A%20ATOM%3A%20Attention%20Mixer%20for%20Efficient%20Dataset%20Distillation%0AAuthor%3A%20Samir%20Khaki%20and%20Ahmad%20Sajedi%20and%20Kai%20Wang%20and%20Lucy%20Z.%20Liu%20and%20Yuri%20A.%20Lawryshyn%20and%20Konstantinos%20N.%20Plataniotis%0AAbstract%3A%20%20%20Recent%20works%20in%20dataset%20distillation%20seek%20to%20minimize%20training%20expenses%20by%0Agenerating%20a%20condensed%20synthetic%20dataset%20that%20encapsulates%20the%20information%0Apresent%20in%20a%20larger%20real%20dataset.%20These%20approaches%20ultimately%20aim%20to%20attain%0Atest%20accuracy%20levels%20akin%20to%20those%20achieved%20by%20models%20trained%20on%20the%20entirety%0Aof%20the%20original%20dataset.%20Previous%20studies%20in%20feature%20and%20distribution%20matching%0Ahave%20achieved%20significant%20results%20without%20incurring%20the%20costs%20of%20bi-level%0Aoptimization%20in%20the%20distillation%20process.%20Despite%20their%20convincing%20efficiency%2C%0Amany%20of%20these%20methods%20suffer%20from%20marginal%20downstream%20performance%20improvements%2C%0Alimited%20distillation%20of%20contextual%20information%2C%20and%20subpar%20cross-architecture%0Ageneralization.%20To%20address%20these%20challenges%20in%20dataset%20distillation%2C%20we%20propose%0Athe%20ATtentiOn%20Mixer%20%28ATOM%29%20module%20to%20efficiently%20distill%20large%20datasets%20using%20a%0Amixture%20of%20channel%20and%20spatial-wise%20attention%20in%20the%20feature%20matching%20process.%0ASpatial-wise%20attention%20helps%20guide%20the%20learning%20process%20based%20on%20consistent%0Alocalization%20of%20classes%20in%20their%20respective%20images%2C%20allowing%20for%20distillation%0Afrom%20a%20broader%20receptive%20field.%20Meanwhile%2C%20channel-wise%20attention%20captures%20the%0Acontextual%20information%20associated%20with%20the%20class%20itself%2C%20thus%20making%20the%0Asynthetic%20image%20more%20informative%20for%20training.%20By%20integrating%20both%20types%20of%0Aattention%2C%20our%20ATOM%20module%20demonstrates%20superior%20performance%20across%20various%0Acomputer%20vision%20datasets%2C%20including%20CIFAR10/100%20and%20TinyImagenet.%20Notably%2C%20our%0Amethod%20significantly%20improves%20performance%20in%20scenarios%20with%20a%20low%20number%20of%0Aimages%20per%20class%2C%20thereby%20enhancing%20its%20potential.%20Furthermore%2C%20we%20maintain%20the%0Aimprovement%20in%20cross-architectures%20and%20applications%20such%20as%20neural%20architecture%0Asearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATOM%253A%2520Attention%2520Mixer%2520for%2520Efficient%2520Dataset%2520Distillation%26entry.906535625%3DSamir%2520Khaki%2520and%2520Ahmad%2520Sajedi%2520and%2520Kai%2520Wang%2520and%2520Lucy%2520Z.%2520Liu%2520and%2520Yuri%2520A.%2520Lawryshyn%2520and%2520Konstantinos%2520N.%2520Plataniotis%26entry.1292438233%3D%2520%2520Recent%2520works%2520in%2520dataset%2520distillation%2520seek%2520to%2520minimize%2520training%2520expenses%2520by%250Agenerating%2520a%2520condensed%2520synthetic%2520dataset%2520that%2520encapsulates%2520the%2520information%250Apresent%2520in%2520a%2520larger%2520real%2520dataset.%2520These%2520approaches%2520ultimately%2520aim%2520to%2520attain%250Atest%2520accuracy%2520levels%2520akin%2520to%2520those%2520achieved%2520by%2520models%2520trained%2520on%2520the%2520entirety%250Aof%2520the%2520original%2520dataset.%2520Previous%2520studies%2520in%2520feature%2520and%2520distribution%2520matching%250Ahave%2520achieved%2520significant%2520results%2520without%2520incurring%2520the%2520costs%2520of%2520bi-level%250Aoptimization%2520in%2520the%2520distillation%2520process.%2520Despite%2520their%2520convincing%2520efficiency%252C%250Amany%2520of%2520these%2520methods%2520suffer%2520from%2520marginal%2520downstream%2520performance%2520improvements%252C%250Alimited%2520distillation%2520of%2520contextual%2520information%252C%2520and%2520subpar%2520cross-architecture%250Ageneralization.%2520To%2520address%2520these%2520challenges%2520in%2520dataset%2520distillation%252C%2520we%2520propose%250Athe%2520ATtentiOn%2520Mixer%2520%2528ATOM%2529%2520module%2520to%2520efficiently%2520distill%2520large%2520datasets%2520using%2520a%250Amixture%2520of%2520channel%2520and%2520spatial-wise%2520attention%2520in%2520the%2520feature%2520matching%2520process.%250ASpatial-wise%2520attention%2520helps%2520guide%2520the%2520learning%2520process%2520based%2520on%2520consistent%250Alocalization%2520of%2520classes%2520in%2520their%2520respective%2520images%252C%2520allowing%2520for%2520distillation%250Afrom%2520a%2520broader%2520receptive%2520field.%2520Meanwhile%252C%2520channel-wise%2520attention%2520captures%2520the%250Acontextual%2520information%2520associated%2520with%2520the%2520class%2520itself%252C%2520thus%2520making%2520the%250Asynthetic%2520image%2520more%2520informative%2520for%2520training.%2520By%2520integrating%2520both%2520types%2520of%250Aattention%252C%2520our%2520ATOM%2520module%2520demonstrates%2520superior%2520performance%2520across%2520various%250Acomputer%2520vision%2520datasets%252C%2520including%2520CIFAR10/100%2520and%2520TinyImagenet.%2520Notably%252C%2520our%250Amethod%2520significantly%2520improves%2520performance%2520in%2520scenarios%2520with%2520a%2520low%2520number%2520of%250Aimages%2520per%2520class%252C%2520thereby%2520enhancing%2520its%2520potential.%2520Furthermore%252C%2520we%2520maintain%2520the%250Aimprovement%2520in%2520cross-architectures%2520and%2520applications%2520such%2520as%2520neural%2520architecture%250Asearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATOM%3A%20Attention%20Mixer%20for%20Efficient%20Dataset%20Distillation&entry.906535625=Samir%20Khaki%20and%20Ahmad%20Sajedi%20and%20Kai%20Wang%20and%20Lucy%20Z.%20Liu%20and%20Yuri%20A.%20Lawryshyn%20and%20Konstantinos%20N.%20Plataniotis&entry.1292438233=%20%20Recent%20works%20in%20dataset%20distillation%20seek%20to%20minimize%20training%20expenses%20by%0Agenerating%20a%20condensed%20synthetic%20dataset%20that%20encapsulates%20the%20information%0Apresent%20in%20a%20larger%20real%20dataset.%20These%20approaches%20ultimately%20aim%20to%20attain%0Atest%20accuracy%20levels%20akin%20to%20those%20achieved%20by%20models%20trained%20on%20the%20entirety%0Aof%20the%20original%20dataset.%20Previous%20studies%20in%20feature%20and%20distribution%20matching%0Ahave%20achieved%20significant%20results%20without%20incurring%20the%20costs%20of%20bi-level%0Aoptimization%20in%20the%20distillation%20process.%20Despite%20their%20convincing%20efficiency%2C%0Amany%20of%20these%20methods%20suffer%20from%20marginal%20downstream%20performance%20improvements%2C%0Alimited%20distillation%20of%20contextual%20information%2C%20and%20subpar%20cross-architecture%0Ageneralization.%20To%20address%20these%20challenges%20in%20dataset%20distillation%2C%20we%20propose%0Athe%20ATtentiOn%20Mixer%20%28ATOM%29%20module%20to%20efficiently%20distill%20large%20datasets%20using%20a%0Amixture%20of%20channel%20and%20spatial-wise%20attention%20in%20the%20feature%20matching%20process.%0ASpatial-wise%20attention%20helps%20guide%20the%20learning%20process%20based%20on%20consistent%0Alocalization%20of%20classes%20in%20their%20respective%20images%2C%20allowing%20for%20distillation%0Afrom%20a%20broader%20receptive%20field.%20Meanwhile%2C%20channel-wise%20attention%20captures%20the%0Acontextual%20information%20associated%20with%20the%20class%20itself%2C%20thus%20making%20the%0Asynthetic%20image%20more%20informative%20for%20training.%20By%20integrating%20both%20types%20of%0Aattention%2C%20our%20ATOM%20module%20demonstrates%20superior%20performance%20across%20various%0Acomputer%20vision%20datasets%2C%20including%20CIFAR10/100%20and%20TinyImagenet.%20Notably%2C%20our%0Amethod%20significantly%20improves%20performance%20in%20scenarios%20with%20a%20low%20number%20of%0Aimages%20per%20class%2C%20thereby%20enhancing%20its%20potential.%20Furthermore%2C%20we%20maintain%20the%0Aimprovement%20in%20cross-architectures%20and%20applications%20such%20as%20neural%20architecture%0Asearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01373v1&entry.124074799=Read"},
{"title": "Towards Consistent Object Detection via LiDAR-Camera Synergy", "author": "Kai Luo and Hao Wu and Kefu Yi and Kailun Yang and Wei Hao and Rongdong Hu", "abstract": "  As human-machine interaction continues to evolve, the capacity for\nenvironmental perception is becoming increasingly crucial. Integrating the two\nmost common types of sensory data, images, and point clouds, can enhance\ndetection accuracy. However, currently, no model exists that can simultaneously\ndetect an object's position in both point clouds and images and ascertain their\ncorresponding relationship. This information is invaluable for human-machine\ninteractions, offering new possibilities for their enhancement. In light of\nthis, this paper introduces an end-to-end Consistency Object Detection (COD)\nalgorithm framework that requires only a single forward inference to\nsimultaneously obtain an object's position in both point clouds and images and\nestablish their correlation. Furthermore, to assess the accuracy of the object\ncorrelation between point clouds and images, this paper proposes a new\nevaluation metric, Consistency Precision (CP). To verify the effectiveness of\nthe proposed framework, an extensive set of experiments has been conducted on\nthe KITTI and DAIR-V2X datasets. The study also explored how the proposed\nconsistency detection method performs on images when the calibration parameters\nbetween images and point clouds are disturbed, compared to existing\npost-processing methods. The experimental results demonstrate that the proposed\nmethod exhibits excellent detection performance and robustness, achieving\nend-to-end consistency detection. The source code will be made publicly\navailable at https://github.com/xifen523/COD.\n", "link": "http://arxiv.org/abs/2405.01258v1", "date": "2024-05-02", "relevancy": 2.384, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6142}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5855}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Consistent%20Object%20Detection%20via%20LiDAR-Camera%20Synergy&body=Title%3A%20Towards%20Consistent%20Object%20Detection%20via%20LiDAR-Camera%20Synergy%0AAuthor%3A%20Kai%20Luo%20and%20Hao%20Wu%20and%20Kefu%20Yi%20and%20Kailun%20Yang%20and%20Wei%20Hao%20and%20Rongdong%20Hu%0AAbstract%3A%20%20%20As%20human-machine%20interaction%20continues%20to%20evolve%2C%20the%20capacity%20for%0Aenvironmental%20perception%20is%20becoming%20increasingly%20crucial.%20Integrating%20the%20two%0Amost%20common%20types%20of%20sensory%20data%2C%20images%2C%20and%20point%20clouds%2C%20can%20enhance%0Adetection%20accuracy.%20However%2C%20currently%2C%20no%20model%20exists%20that%20can%20simultaneously%0Adetect%20an%20object%27s%20position%20in%20both%20point%20clouds%20and%20images%20and%20ascertain%20their%0Acorresponding%20relationship.%20This%20information%20is%20invaluable%20for%20human-machine%0Ainteractions%2C%20offering%20new%20possibilities%20for%20their%20enhancement.%20In%20light%20of%0Athis%2C%20this%20paper%20introduces%20an%20end-to-end%20Consistency%20Object%20Detection%20%28COD%29%0Aalgorithm%20framework%20that%20requires%20only%20a%20single%20forward%20inference%20to%0Asimultaneously%20obtain%20an%20object%27s%20position%20in%20both%20point%20clouds%20and%20images%20and%0Aestablish%20their%20correlation.%20Furthermore%2C%20to%20assess%20the%20accuracy%20of%20the%20object%0Acorrelation%20between%20point%20clouds%20and%20images%2C%20this%20paper%20proposes%20a%20new%0Aevaluation%20metric%2C%20Consistency%20Precision%20%28CP%29.%20To%20verify%20the%20effectiveness%20of%0Athe%20proposed%20framework%2C%20an%20extensive%20set%20of%20experiments%20has%20been%20conducted%20on%0Athe%20KITTI%20and%20DAIR-V2X%20datasets.%20The%20study%20also%20explored%20how%20the%20proposed%0Aconsistency%20detection%20method%20performs%20on%20images%20when%20the%20calibration%20parameters%0Abetween%20images%20and%20point%20clouds%20are%20disturbed%2C%20compared%20to%20existing%0Apost-processing%20methods.%20The%20experimental%20results%20demonstrate%20that%20the%20proposed%0Amethod%20exhibits%20excellent%20detection%20performance%20and%20robustness%2C%20achieving%0Aend-to-end%20consistency%20detection.%20The%20source%20code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/xifen523/COD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Consistent%2520Object%2520Detection%2520via%2520LiDAR-Camera%2520Synergy%26entry.906535625%3DKai%2520Luo%2520and%2520Hao%2520Wu%2520and%2520Kefu%2520Yi%2520and%2520Kailun%2520Yang%2520and%2520Wei%2520Hao%2520and%2520Rongdong%2520Hu%26entry.1292438233%3D%2520%2520As%2520human-machine%2520interaction%2520continues%2520to%2520evolve%252C%2520the%2520capacity%2520for%250Aenvironmental%2520perception%2520is%2520becoming%2520increasingly%2520crucial.%2520Integrating%2520the%2520two%250Amost%2520common%2520types%2520of%2520sensory%2520data%252C%2520images%252C%2520and%2520point%2520clouds%252C%2520can%2520enhance%250Adetection%2520accuracy.%2520However%252C%2520currently%252C%2520no%2520model%2520exists%2520that%2520can%2520simultaneously%250Adetect%2520an%2520object%2527s%2520position%2520in%2520both%2520point%2520clouds%2520and%2520images%2520and%2520ascertain%2520their%250Acorresponding%2520relationship.%2520This%2520information%2520is%2520invaluable%2520for%2520human-machine%250Ainteractions%252C%2520offering%2520new%2520possibilities%2520for%2520their%2520enhancement.%2520In%2520light%2520of%250Athis%252C%2520this%2520paper%2520introduces%2520an%2520end-to-end%2520Consistency%2520Object%2520Detection%2520%2528COD%2529%250Aalgorithm%2520framework%2520that%2520requires%2520only%2520a%2520single%2520forward%2520inference%2520to%250Asimultaneously%2520obtain%2520an%2520object%2527s%2520position%2520in%2520both%2520point%2520clouds%2520and%2520images%2520and%250Aestablish%2520their%2520correlation.%2520Furthermore%252C%2520to%2520assess%2520the%2520accuracy%2520of%2520the%2520object%250Acorrelation%2520between%2520point%2520clouds%2520and%2520images%252C%2520this%2520paper%2520proposes%2520a%2520new%250Aevaluation%2520metric%252C%2520Consistency%2520Precision%2520%2528CP%2529.%2520To%2520verify%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520framework%252C%2520an%2520extensive%2520set%2520of%2520experiments%2520has%2520been%2520conducted%2520on%250Athe%2520KITTI%2520and%2520DAIR-V2X%2520datasets.%2520The%2520study%2520also%2520explored%2520how%2520the%2520proposed%250Aconsistency%2520detection%2520method%2520performs%2520on%2520images%2520when%2520the%2520calibration%2520parameters%250Abetween%2520images%2520and%2520point%2520clouds%2520are%2520disturbed%252C%2520compared%2520to%2520existing%250Apost-processing%2520methods.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%250Amethod%2520exhibits%2520excellent%2520detection%2520performance%2520and%2520robustness%252C%2520achieving%250Aend-to-end%2520consistency%2520detection.%2520The%2520source%2520code%2520will%2520be%2520made%2520publicly%250Aavailable%2520at%2520https%253A//github.com/xifen523/COD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Consistent%20Object%20Detection%20via%20LiDAR-Camera%20Synergy&entry.906535625=Kai%20Luo%20and%20Hao%20Wu%20and%20Kefu%20Yi%20and%20Kailun%20Yang%20and%20Wei%20Hao%20and%20Rongdong%20Hu&entry.1292438233=%20%20As%20human-machine%20interaction%20continues%20to%20evolve%2C%20the%20capacity%20for%0Aenvironmental%20perception%20is%20becoming%20increasingly%20crucial.%20Integrating%20the%20two%0Amost%20common%20types%20of%20sensory%20data%2C%20images%2C%20and%20point%20clouds%2C%20can%20enhance%0Adetection%20accuracy.%20However%2C%20currently%2C%20no%20model%20exists%20that%20can%20simultaneously%0Adetect%20an%20object%27s%20position%20in%20both%20point%20clouds%20and%20images%20and%20ascertain%20their%0Acorresponding%20relationship.%20This%20information%20is%20invaluable%20for%20human-machine%0Ainteractions%2C%20offering%20new%20possibilities%20for%20their%20enhancement.%20In%20light%20of%0Athis%2C%20this%20paper%20introduces%20an%20end-to-end%20Consistency%20Object%20Detection%20%28COD%29%0Aalgorithm%20framework%20that%20requires%20only%20a%20single%20forward%20inference%20to%0Asimultaneously%20obtain%20an%20object%27s%20position%20in%20both%20point%20clouds%20and%20images%20and%0Aestablish%20their%20correlation.%20Furthermore%2C%20to%20assess%20the%20accuracy%20of%20the%20object%0Acorrelation%20between%20point%20clouds%20and%20images%2C%20this%20paper%20proposes%20a%20new%0Aevaluation%20metric%2C%20Consistency%20Precision%20%28CP%29.%20To%20verify%20the%20effectiveness%20of%0Athe%20proposed%20framework%2C%20an%20extensive%20set%20of%20experiments%20has%20been%20conducted%20on%0Athe%20KITTI%20and%20DAIR-V2X%20datasets.%20The%20study%20also%20explored%20how%20the%20proposed%0Aconsistency%20detection%20method%20performs%20on%20images%20when%20the%20calibration%20parameters%0Abetween%20images%20and%20point%20clouds%20are%20disturbed%2C%20compared%20to%20existing%0Apost-processing%20methods.%20The%20experimental%20results%20demonstrate%20that%20the%20proposed%0Amethod%20exhibits%20excellent%20detection%20performance%20and%20robustness%2C%20achieving%0Aend-to-end%20consistency%20detection.%20The%20source%20code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/xifen523/COD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01258v1&entry.124074799=Read"},
{"title": "Boosting Jailbreak Attack with Momentum", "author": "Yihao Zhang and Zeming Wei", "abstract": "  Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, yet they remain vulnerable to adversarial attacks, notably the\nwell-documented \\textit{jailbreak} attack. Recently, the Greedy Coordinate\nGradient (GCG) attack has demonstrated efficacy in exploiting this\nvulnerability by optimizing adversarial prompts through a combination of\ngradient heuristics and greedy search. However, the efficiency of this attack\nhas become a bottleneck in the attacking process. To mitigate this limitation,\nin this paper we rethink the generation of adversarial prompts through an\noptimization lens, aiming to stabilize the optimization process and harness\nmore heuristic insights from previous iterations. Specifically, we introduce\nthe \\textbf{M}omentum \\textbf{A}ccelerated G\\textbf{C}G (\\textbf{MAC}) attack,\nwhich incorporates a momentum term into the gradient heuristic. Experimental\nresults showcase the notable enhancement achieved by MAP in gradient-based\nattacks on aligned language models. Our code is available at\nhttps://github.com/weizeming/momentum-attack-llm.\n", "link": "http://arxiv.org/abs/2405.01229v1", "date": "2024-05-02", "relevancy": 2.3735, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.488}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4698}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Jailbreak%20Attack%20with%20Momentum&body=Title%3A%20Boosting%20Jailbreak%20Attack%20with%20Momentum%0AAuthor%3A%20Yihao%20Zhang%20and%20Zeming%20Wei%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20across%20diverse%0Atasks%2C%20yet%20they%20remain%20vulnerable%20to%20adversarial%20attacks%2C%20notably%20the%0Awell-documented%20%5Ctextit%7Bjailbreak%7D%20attack.%20Recently%2C%20the%20Greedy%20Coordinate%0AGradient%20%28GCG%29%20attack%20has%20demonstrated%20efficacy%20in%20exploiting%20this%0Avulnerability%20by%20optimizing%20adversarial%20prompts%20through%20a%20combination%20of%0Agradient%20heuristics%20and%20greedy%20search.%20However%2C%20the%20efficiency%20of%20this%20attack%0Ahas%20become%20a%20bottleneck%20in%20the%20attacking%20process.%20To%20mitigate%20this%20limitation%2C%0Ain%20this%20paper%20we%20rethink%20the%20generation%20of%20adversarial%20prompts%20through%20an%0Aoptimization%20lens%2C%20aiming%20to%20stabilize%20the%20optimization%20process%20and%20harness%0Amore%20heuristic%20insights%20from%20previous%20iterations.%20Specifically%2C%20we%20introduce%0Athe%20%5Ctextbf%7BM%7Domentum%20%5Ctextbf%7BA%7Dccelerated%20G%5Ctextbf%7BC%7DG%20%28%5Ctextbf%7BMAC%7D%29%20attack%2C%0Awhich%20incorporates%20a%20momentum%20term%20into%20the%20gradient%20heuristic.%20Experimental%0Aresults%20showcase%20the%20notable%20enhancement%20achieved%20by%20MAP%20in%20gradient-based%0Aattacks%20on%20aligned%20language%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/weizeming/momentum-attack-llm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Jailbreak%2520Attack%2520with%2520Momentum%26entry.906535625%3DYihao%2520Zhang%2520and%2520Zeming%2520Wei%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%2520diverse%250Atasks%252C%2520yet%2520they%2520remain%2520vulnerable%2520to%2520adversarial%2520attacks%252C%2520notably%2520the%250Awell-documented%2520%255Ctextit%257Bjailbreak%257D%2520attack.%2520Recently%252C%2520the%2520Greedy%2520Coordinate%250AGradient%2520%2528GCG%2529%2520attack%2520has%2520demonstrated%2520efficacy%2520in%2520exploiting%2520this%250Avulnerability%2520by%2520optimizing%2520adversarial%2520prompts%2520through%2520a%2520combination%2520of%250Agradient%2520heuristics%2520and%2520greedy%2520search.%2520However%252C%2520the%2520efficiency%2520of%2520this%2520attack%250Ahas%2520become%2520a%2520bottleneck%2520in%2520the%2520attacking%2520process.%2520To%2520mitigate%2520this%2520limitation%252C%250Ain%2520this%2520paper%2520we%2520rethink%2520the%2520generation%2520of%2520adversarial%2520prompts%2520through%2520an%250Aoptimization%2520lens%252C%2520aiming%2520to%2520stabilize%2520the%2520optimization%2520process%2520and%2520harness%250Amore%2520heuristic%2520insights%2520from%2520previous%2520iterations.%2520Specifically%252C%2520we%2520introduce%250Athe%2520%255Ctextbf%257BM%257Domentum%2520%255Ctextbf%257BA%257Dccelerated%2520G%255Ctextbf%257BC%257DG%2520%2528%255Ctextbf%257BMAC%257D%2529%2520attack%252C%250Awhich%2520incorporates%2520a%2520momentum%2520term%2520into%2520the%2520gradient%2520heuristic.%2520Experimental%250Aresults%2520showcase%2520the%2520notable%2520enhancement%2520achieved%2520by%2520MAP%2520in%2520gradient-based%250Aattacks%2520on%2520aligned%2520language%2520models.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/weizeming/momentum-attack-llm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Jailbreak%20Attack%20with%20Momentum&entry.906535625=Yihao%20Zhang%20and%20Zeming%20Wei&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20across%20diverse%0Atasks%2C%20yet%20they%20remain%20vulnerable%20to%20adversarial%20attacks%2C%20notably%20the%0Awell-documented%20%5Ctextit%7Bjailbreak%7D%20attack.%20Recently%2C%20the%20Greedy%20Coordinate%0AGradient%20%28GCG%29%20attack%20has%20demonstrated%20efficacy%20in%20exploiting%20this%0Avulnerability%20by%20optimizing%20adversarial%20prompts%20through%20a%20combination%20of%0Agradient%20heuristics%20and%20greedy%20search.%20However%2C%20the%20efficiency%20of%20this%20attack%0Ahas%20become%20a%20bottleneck%20in%20the%20attacking%20process.%20To%20mitigate%20this%20limitation%2C%0Ain%20this%20paper%20we%20rethink%20the%20generation%20of%20adversarial%20prompts%20through%20an%0Aoptimization%20lens%2C%20aiming%20to%20stabilize%20the%20optimization%20process%20and%20harness%0Amore%20heuristic%20insights%20from%20previous%20iterations.%20Specifically%2C%20we%20introduce%0Athe%20%5Ctextbf%7BM%7Domentum%20%5Ctextbf%7BA%7Dccelerated%20G%5Ctextbf%7BC%7DG%20%28%5Ctextbf%7BMAC%7D%29%20attack%2C%0Awhich%20incorporates%20a%20momentum%20term%20into%20the%20gradient%20heuristic.%20Experimental%0Aresults%20showcase%20the%20notable%20enhancement%20achieved%20by%20MAP%20in%20gradient-based%0Aattacks%20on%20aligned%20language%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/weizeming/momentum-attack-llm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01229v1&entry.124074799=Read"},
{"title": "USC: Uncompromising Spatial Constraints for Safety-Oriented 3D Object\n  Detectors in Autonomous Driving", "author": "Brian Hsuan-Cheng Liao and Chih-Hong Cheng and Hasan Esen and Alois Knoll", "abstract": "  We consider the safety-oriented performance of 3D object detectors in\nautonomous driving contexts. Specifically, despite impressive results shown by\nthe mass literature, developers often find it hard to ensure the safe\ndeployment of these learning-based perception models. Attributing the challenge\nto the lack of safety-oriented metrics, we hereby present uncompromising\nspatial constraints (USC), which characterize a simple yet important\nlocalization requirement demanding the predictions to fully cover the objects\nwhen seen from the autonomous vehicle. The constraints, as we formulate using\nthe perspective and bird's-eye views, can be naturally reflected by\nquantitative measures, such that having an object detector with a higher score\nimplies a lower risk of collision. Finally, beyond model evaluation, we\nincorporate the quantitative measures into common loss functions to enable\nsafety-oriented fine-tuning for existing models. With experiments using the\nnuScenes dataset and a closed-loop simulation, our work demonstrates such\nconsiderations of safety notions at the perception level not only improve model\nperformances beyond accuracy but also allow for a more direct linkage to actual\nsystem safety.\n", "link": "http://arxiv.org/abs/2209.10368v4", "date": "2024-05-02", "relevancy": 2.3647, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.629}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5862}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20USC%3A%20Uncompromising%20Spatial%20Constraints%20for%20Safety-Oriented%203D%20Object%0A%20%20Detectors%20in%20Autonomous%20Driving&body=Title%3A%20USC%3A%20Uncompromising%20Spatial%20Constraints%20for%20Safety-Oriented%203D%20Object%0A%20%20Detectors%20in%20Autonomous%20Driving%0AAuthor%3A%20Brian%20Hsuan-Cheng%20Liao%20and%20Chih-Hong%20Cheng%20and%20Hasan%20Esen%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20We%20consider%20the%20safety-oriented%20performance%20of%203D%20object%20detectors%20in%0Aautonomous%20driving%20contexts.%20Specifically%2C%20despite%20impressive%20results%20shown%20by%0Athe%20mass%20literature%2C%20developers%20often%20find%20it%20hard%20to%20ensure%20the%20safe%0Adeployment%20of%20these%20learning-based%20perception%20models.%20Attributing%20the%20challenge%0Ato%20the%20lack%20of%20safety-oriented%20metrics%2C%20we%20hereby%20present%20uncompromising%0Aspatial%20constraints%20%28USC%29%2C%20which%20characterize%20a%20simple%20yet%20important%0Alocalization%20requirement%20demanding%20the%20predictions%20to%20fully%20cover%20the%20objects%0Awhen%20seen%20from%20the%20autonomous%20vehicle.%20The%20constraints%2C%20as%20we%20formulate%20using%0Athe%20perspective%20and%20bird%27s-eye%20views%2C%20can%20be%20naturally%20reflected%20by%0Aquantitative%20measures%2C%20such%20that%20having%20an%20object%20detector%20with%20a%20higher%20score%0Aimplies%20a%20lower%20risk%20of%20collision.%20Finally%2C%20beyond%20model%20evaluation%2C%20we%0Aincorporate%20the%20quantitative%20measures%20into%20common%20loss%20functions%20to%20enable%0Asafety-oriented%20fine-tuning%20for%20existing%20models.%20With%20experiments%20using%20the%0AnuScenes%20dataset%20and%20a%20closed-loop%20simulation%2C%20our%20work%20demonstrates%20such%0Aconsiderations%20of%20safety%20notions%20at%20the%20perception%20level%20not%20only%20improve%20model%0Aperformances%20beyond%20accuracy%20but%20also%20allow%20for%20a%20more%20direct%20linkage%20to%20actual%0Asystem%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.10368v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUSC%253A%2520Uncompromising%2520Spatial%2520Constraints%2520for%2520Safety-Oriented%25203D%2520Object%250A%2520%2520Detectors%2520in%2520Autonomous%2520Driving%26entry.906535625%3DBrian%2520Hsuan-Cheng%2520Liao%2520and%2520Chih-Hong%2520Cheng%2520and%2520Hasan%2520Esen%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520safety-oriented%2520performance%2520of%25203D%2520object%2520detectors%2520in%250Aautonomous%2520driving%2520contexts.%2520Specifically%252C%2520despite%2520impressive%2520results%2520shown%2520by%250Athe%2520mass%2520literature%252C%2520developers%2520often%2520find%2520it%2520hard%2520to%2520ensure%2520the%2520safe%250Adeployment%2520of%2520these%2520learning-based%2520perception%2520models.%2520Attributing%2520the%2520challenge%250Ato%2520the%2520lack%2520of%2520safety-oriented%2520metrics%252C%2520we%2520hereby%2520present%2520uncompromising%250Aspatial%2520constraints%2520%2528USC%2529%252C%2520which%2520characterize%2520a%2520simple%2520yet%2520important%250Alocalization%2520requirement%2520demanding%2520the%2520predictions%2520to%2520fully%2520cover%2520the%2520objects%250Awhen%2520seen%2520from%2520the%2520autonomous%2520vehicle.%2520The%2520constraints%252C%2520as%2520we%2520formulate%2520using%250Athe%2520perspective%2520and%2520bird%2527s-eye%2520views%252C%2520can%2520be%2520naturally%2520reflected%2520by%250Aquantitative%2520measures%252C%2520such%2520that%2520having%2520an%2520object%2520detector%2520with%2520a%2520higher%2520score%250Aimplies%2520a%2520lower%2520risk%2520of%2520collision.%2520Finally%252C%2520beyond%2520model%2520evaluation%252C%2520we%250Aincorporate%2520the%2520quantitative%2520measures%2520into%2520common%2520loss%2520functions%2520to%2520enable%250Asafety-oriented%2520fine-tuning%2520for%2520existing%2520models.%2520With%2520experiments%2520using%2520the%250AnuScenes%2520dataset%2520and%2520a%2520closed-loop%2520simulation%252C%2520our%2520work%2520demonstrates%2520such%250Aconsiderations%2520of%2520safety%2520notions%2520at%2520the%2520perception%2520level%2520not%2520only%2520improve%2520model%250Aperformances%2520beyond%2520accuracy%2520but%2520also%2520allow%2520for%2520a%2520more%2520direct%2520linkage%2520to%2520actual%250Asystem%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.10368v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USC%3A%20Uncompromising%20Spatial%20Constraints%20for%20Safety-Oriented%203D%20Object%0A%20%20Detectors%20in%20Autonomous%20Driving&entry.906535625=Brian%20Hsuan-Cheng%20Liao%20and%20Chih-Hong%20Cheng%20and%20Hasan%20Esen%20and%20Alois%20Knoll&entry.1292438233=%20%20We%20consider%20the%20safety-oriented%20performance%20of%203D%20object%20detectors%20in%0Aautonomous%20driving%20contexts.%20Specifically%2C%20despite%20impressive%20results%20shown%20by%0Athe%20mass%20literature%2C%20developers%20often%20find%20it%20hard%20to%20ensure%20the%20safe%0Adeployment%20of%20these%20learning-based%20perception%20models.%20Attributing%20the%20challenge%0Ato%20the%20lack%20of%20safety-oriented%20metrics%2C%20we%20hereby%20present%20uncompromising%0Aspatial%20constraints%20%28USC%29%2C%20which%20characterize%20a%20simple%20yet%20important%0Alocalization%20requirement%20demanding%20the%20predictions%20to%20fully%20cover%20the%20objects%0Awhen%20seen%20from%20the%20autonomous%20vehicle.%20The%20constraints%2C%20as%20we%20formulate%20using%0Athe%20perspective%20and%20bird%27s-eye%20views%2C%20can%20be%20naturally%20reflected%20by%0Aquantitative%20measures%2C%20such%20that%20having%20an%20object%20detector%20with%20a%20higher%20score%0Aimplies%20a%20lower%20risk%20of%20collision.%20Finally%2C%20beyond%20model%20evaluation%2C%20we%0Aincorporate%20the%20quantitative%20measures%20into%20common%20loss%20functions%20to%20enable%0Asafety-oriented%20fine-tuning%20for%20existing%20models.%20With%20experiments%20using%20the%0AnuScenes%20dataset%20and%20a%20closed-loop%20simulation%2C%20our%20work%20demonstrates%20such%0Aconsiderations%20of%20safety%20notions%20at%20the%20perception%20level%20not%20only%20improve%20model%0Aperformances%20beyond%20accuracy%20but%20also%20allow%20for%20a%20more%20direct%20linkage%20to%20actual%0Asystem%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.10368v4&entry.124074799=Read"},
{"title": "Low-resource speech recognition and dialect identification of Irish in a\n  multi-task framework", "author": "Liam Lonergan and Mengjie Qian and Neasa N\u00ed Chiar\u00e1in and Christer Gobl and Ailbhe N\u00ed Chasaide", "abstract": "  This paper explores the use of Hybrid CTC/Attention encoder-decoder models\ntrained with Intermediate CTC (InterCTC) for Irish (Gaelic) low-resource speech\nrecognition (ASR) and dialect identification (DID). Results are compared to the\ncurrent best performing models trained for ASR (TDNN-HMM) and DID (ECAPA-TDNN).\nAn optimal InterCTC setting is initially established using a Conformer encoder.\nThis setting is then used to train a model with an E-branchformer encoder and\nthe performance of both architectures are compared. A multi-task fine-tuning\napproach is adopted for language model (LM) shallow fusion. The experiments\nyielded an improvement in DID accuracy of 10.8% relative to a baseline\nECAPA-TDNN, and WER performance approaching the TDNN-HMM model. This multi-task\napproach emerges as a promising strategy for Irish low-resource ASR and DID.\n", "link": "http://arxiv.org/abs/2405.01293v1", "date": "2024-05-02", "relevancy": 2.3626, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4896}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4793}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-resource%20speech%20recognition%20and%20dialect%20identification%20of%20Irish%20in%20a%0A%20%20multi-task%20framework&body=Title%3A%20Low-resource%20speech%20recognition%20and%20dialect%20identification%20of%20Irish%20in%20a%0A%20%20multi-task%20framework%0AAuthor%3A%20Liam%20Lonergan%20and%20Mengjie%20Qian%20and%20Neasa%20N%C3%AD%20Chiar%C3%A1in%20and%20Christer%20Gobl%20and%20Ailbhe%20N%C3%AD%20Chasaide%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20use%20of%20Hybrid%20CTC/Attention%20encoder-decoder%20models%0Atrained%20with%20Intermediate%20CTC%20%28InterCTC%29%20for%20Irish%20%28Gaelic%29%20low-resource%20speech%0Arecognition%20%28ASR%29%20and%20dialect%20identification%20%28DID%29.%20Results%20are%20compared%20to%20the%0Acurrent%20best%20performing%20models%20trained%20for%20ASR%20%28TDNN-HMM%29%20and%20DID%20%28ECAPA-TDNN%29.%0AAn%20optimal%20InterCTC%20setting%20is%20initially%20established%20using%20a%20Conformer%20encoder.%0AThis%20setting%20is%20then%20used%20to%20train%20a%20model%20with%20an%20E-branchformer%20encoder%20and%0Athe%20performance%20of%20both%20architectures%20are%20compared.%20A%20multi-task%20fine-tuning%0Aapproach%20is%20adopted%20for%20language%20model%20%28LM%29%20shallow%20fusion.%20The%20experiments%0Ayielded%20an%20improvement%20in%20DID%20accuracy%20of%2010.8%25%20relative%20to%20a%20baseline%0AECAPA-TDNN%2C%20and%20WER%20performance%20approaching%20the%20TDNN-HMM%20model.%20This%20multi-task%0Aapproach%20emerges%20as%20a%20promising%20strategy%20for%20Irish%20low-resource%20ASR%20and%20DID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-resource%2520speech%2520recognition%2520and%2520dialect%2520identification%2520of%2520Irish%2520in%2520a%250A%2520%2520multi-task%2520framework%26entry.906535625%3DLiam%2520Lonergan%2520and%2520Mengjie%2520Qian%2520and%2520Neasa%2520N%25C3%25AD%2520Chiar%25C3%25A1in%2520and%2520Christer%2520Gobl%2520and%2520Ailbhe%2520N%25C3%25AD%2520Chasaide%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520use%2520of%2520Hybrid%2520CTC/Attention%2520encoder-decoder%2520models%250Atrained%2520with%2520Intermediate%2520CTC%2520%2528InterCTC%2529%2520for%2520Irish%2520%2528Gaelic%2529%2520low-resource%2520speech%250Arecognition%2520%2528ASR%2529%2520and%2520dialect%2520identification%2520%2528DID%2529.%2520Results%2520are%2520compared%2520to%2520the%250Acurrent%2520best%2520performing%2520models%2520trained%2520for%2520ASR%2520%2528TDNN-HMM%2529%2520and%2520DID%2520%2528ECAPA-TDNN%2529.%250AAn%2520optimal%2520InterCTC%2520setting%2520is%2520initially%2520established%2520using%2520a%2520Conformer%2520encoder.%250AThis%2520setting%2520is%2520then%2520used%2520to%2520train%2520a%2520model%2520with%2520an%2520E-branchformer%2520encoder%2520and%250Athe%2520performance%2520of%2520both%2520architectures%2520are%2520compared.%2520A%2520multi-task%2520fine-tuning%250Aapproach%2520is%2520adopted%2520for%2520language%2520model%2520%2528LM%2529%2520shallow%2520fusion.%2520The%2520experiments%250Ayielded%2520an%2520improvement%2520in%2520DID%2520accuracy%2520of%252010.8%2525%2520relative%2520to%2520a%2520baseline%250AECAPA-TDNN%252C%2520and%2520WER%2520performance%2520approaching%2520the%2520TDNN-HMM%2520model.%2520This%2520multi-task%250Aapproach%2520emerges%2520as%2520a%2520promising%2520strategy%2520for%2520Irish%2520low-resource%2520ASR%2520and%2520DID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-resource%20speech%20recognition%20and%20dialect%20identification%20of%20Irish%20in%20a%0A%20%20multi-task%20framework&entry.906535625=Liam%20Lonergan%20and%20Mengjie%20Qian%20and%20Neasa%20N%C3%AD%20Chiar%C3%A1in%20and%20Christer%20Gobl%20and%20Ailbhe%20N%C3%AD%20Chasaide&entry.1292438233=%20%20This%20paper%20explores%20the%20use%20of%20Hybrid%20CTC/Attention%20encoder-decoder%20models%0Atrained%20with%20Intermediate%20CTC%20%28InterCTC%29%20for%20Irish%20%28Gaelic%29%20low-resource%20speech%0Arecognition%20%28ASR%29%20and%20dialect%20identification%20%28DID%29.%20Results%20are%20compared%20to%20the%0Acurrent%20best%20performing%20models%20trained%20for%20ASR%20%28TDNN-HMM%29%20and%20DID%20%28ECAPA-TDNN%29.%0AAn%20optimal%20InterCTC%20setting%20is%20initially%20established%20using%20a%20Conformer%20encoder.%0AThis%20setting%20is%20then%20used%20to%20train%20a%20model%20with%20an%20E-branchformer%20encoder%20and%0Athe%20performance%20of%20both%20architectures%20are%20compared.%20A%20multi-task%20fine-tuning%0Aapproach%20is%20adopted%20for%20language%20model%20%28LM%29%20shallow%20fusion.%20The%20experiments%0Ayielded%20an%20improvement%20in%20DID%20accuracy%20of%2010.8%25%20relative%20to%20a%20baseline%0AECAPA-TDNN%2C%20and%20WER%20performance%20approaching%20the%20TDNN-HMM%20model.%20This%20multi-task%0Aapproach%20emerges%20as%20a%20promising%20strategy%20for%20Irish%20low-resource%20ASR%20and%20DID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01293v1&entry.124074799=Read"},
{"title": "Multi-Space Alignments Towards Universal LiDAR Segmentation", "author": "Youquan Liu and Lingdong Kong and Xiaoyang Wu and Runnan Chen and Xin Li and Liang Pan and Ziwei Liu and Yuexin Ma", "abstract": "  A unified and versatile LiDAR segmentation model with strong robustness and\ngeneralizability is desirable for safe autonomous driving perception. This work\npresents M3Net, a one-of-a-kind framework for fulfilling multi-task,\nmulti-dataset, multi-modality LiDAR segmentation in a universal manner using\njust a single set of parameters. To better exploit data volume and diversity,\nwe first combine large-scale driving datasets acquired by different types of\nsensors from diverse scenes and then conduct alignments in three spaces, namely\ndata, feature, and label spaces, during the training. As a result, M3Net is\ncapable of taming heterogeneous data for training state-of-the-art LiDAR\nsegmentation models. Extensive experiments on twelve LiDAR segmentation\ndatasets verify our effectiveness. Notably, using a shared set of parameters,\nM3Net achieves 75.1%, 83.1%, and 72.4% mIoU scores, respectively, on the\nofficial benchmarks of SemanticKITTI, nuScenes, and Waymo Open.\n", "link": "http://arxiv.org/abs/2405.01538v1", "date": "2024-05-02", "relevancy": 2.3607, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5946}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5912}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Space%20Alignments%20Towards%20Universal%20LiDAR%20Segmentation&body=Title%3A%20Multi-Space%20Alignments%20Towards%20Universal%20LiDAR%20Segmentation%0AAuthor%3A%20Youquan%20Liu%20and%20Lingdong%20Kong%20and%20Xiaoyang%20Wu%20and%20Runnan%20Chen%20and%20Xin%20Li%20and%20Liang%20Pan%20and%20Ziwei%20Liu%20and%20Yuexin%20Ma%0AAbstract%3A%20%20%20A%20unified%20and%20versatile%20LiDAR%20segmentation%20model%20with%20strong%20robustness%20and%0Ageneralizability%20is%20desirable%20for%20safe%20autonomous%20driving%20perception.%20This%20work%0Apresents%20M3Net%2C%20a%20one-of-a-kind%20framework%20for%20fulfilling%20multi-task%2C%0Amulti-dataset%2C%20multi-modality%20LiDAR%20segmentation%20in%20a%20universal%20manner%20using%0Ajust%20a%20single%20set%20of%20parameters.%20To%20better%20exploit%20data%20volume%20and%20diversity%2C%0Awe%20first%20combine%20large-scale%20driving%20datasets%20acquired%20by%20different%20types%20of%0Asensors%20from%20diverse%20scenes%20and%20then%20conduct%20alignments%20in%20three%20spaces%2C%20namely%0Adata%2C%20feature%2C%20and%20label%20spaces%2C%20during%20the%20training.%20As%20a%20result%2C%20M3Net%20is%0Acapable%20of%20taming%20heterogeneous%20data%20for%20training%20state-of-the-art%20LiDAR%0Asegmentation%20models.%20Extensive%20experiments%20on%20twelve%20LiDAR%20segmentation%0Adatasets%20verify%20our%20effectiveness.%20Notably%2C%20using%20a%20shared%20set%20of%20parameters%2C%0AM3Net%20achieves%2075.1%25%2C%2083.1%25%2C%20and%2072.4%25%20mIoU%20scores%2C%20respectively%2C%20on%20the%0Aofficial%20benchmarks%20of%20SemanticKITTI%2C%20nuScenes%2C%20and%20Waymo%20Open.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Space%2520Alignments%2520Towards%2520Universal%2520LiDAR%2520Segmentation%26entry.906535625%3DYouquan%2520Liu%2520and%2520Lingdong%2520Kong%2520and%2520Xiaoyang%2520Wu%2520and%2520Runnan%2520Chen%2520and%2520Xin%2520Li%2520and%2520Liang%2520Pan%2520and%2520Ziwei%2520Liu%2520and%2520Yuexin%2520Ma%26entry.1292438233%3D%2520%2520A%2520unified%2520and%2520versatile%2520LiDAR%2520segmentation%2520model%2520with%2520strong%2520robustness%2520and%250Ageneralizability%2520is%2520desirable%2520for%2520safe%2520autonomous%2520driving%2520perception.%2520This%2520work%250Apresents%2520M3Net%252C%2520a%2520one-of-a-kind%2520framework%2520for%2520fulfilling%2520multi-task%252C%250Amulti-dataset%252C%2520multi-modality%2520LiDAR%2520segmentation%2520in%2520a%2520universal%2520manner%2520using%250Ajust%2520a%2520single%2520set%2520of%2520parameters.%2520To%2520better%2520exploit%2520data%2520volume%2520and%2520diversity%252C%250Awe%2520first%2520combine%2520large-scale%2520driving%2520datasets%2520acquired%2520by%2520different%2520types%2520of%250Asensors%2520from%2520diverse%2520scenes%2520and%2520then%2520conduct%2520alignments%2520in%2520three%2520spaces%252C%2520namely%250Adata%252C%2520feature%252C%2520and%2520label%2520spaces%252C%2520during%2520the%2520training.%2520As%2520a%2520result%252C%2520M3Net%2520is%250Acapable%2520of%2520taming%2520heterogeneous%2520data%2520for%2520training%2520state-of-the-art%2520LiDAR%250Asegmentation%2520models.%2520Extensive%2520experiments%2520on%2520twelve%2520LiDAR%2520segmentation%250Adatasets%2520verify%2520our%2520effectiveness.%2520Notably%252C%2520using%2520a%2520shared%2520set%2520of%2520parameters%252C%250AM3Net%2520achieves%252075.1%2525%252C%252083.1%2525%252C%2520and%252072.4%2525%2520mIoU%2520scores%252C%2520respectively%252C%2520on%2520the%250Aofficial%2520benchmarks%2520of%2520SemanticKITTI%252C%2520nuScenes%252C%2520and%2520Waymo%2520Open.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Space%20Alignments%20Towards%20Universal%20LiDAR%20Segmentation&entry.906535625=Youquan%20Liu%20and%20Lingdong%20Kong%20and%20Xiaoyang%20Wu%20and%20Runnan%20Chen%20and%20Xin%20Li%20and%20Liang%20Pan%20and%20Ziwei%20Liu%20and%20Yuexin%20Ma&entry.1292438233=%20%20A%20unified%20and%20versatile%20LiDAR%20segmentation%20model%20with%20strong%20robustness%20and%0Ageneralizability%20is%20desirable%20for%20safe%20autonomous%20driving%20perception.%20This%20work%0Apresents%20M3Net%2C%20a%20one-of-a-kind%20framework%20for%20fulfilling%20multi-task%2C%0Amulti-dataset%2C%20multi-modality%20LiDAR%20segmentation%20in%20a%20universal%20manner%20using%0Ajust%20a%20single%20set%20of%20parameters.%20To%20better%20exploit%20data%20volume%20and%20diversity%2C%0Awe%20first%20combine%20large-scale%20driving%20datasets%20acquired%20by%20different%20types%20of%0Asensors%20from%20diverse%20scenes%20and%20then%20conduct%20alignments%20in%20three%20spaces%2C%20namely%0Adata%2C%20feature%2C%20and%20label%20spaces%2C%20during%20the%20training.%20As%20a%20result%2C%20M3Net%20is%0Acapable%20of%20taming%20heterogeneous%20data%20for%20training%20state-of-the-art%20LiDAR%0Asegmentation%20models.%20Extensive%20experiments%20on%20twelve%20LiDAR%20segmentation%0Adatasets%20verify%20our%20effectiveness.%20Notably%2C%20using%20a%20shared%20set%20of%20parameters%2C%0AM3Net%20achieves%2075.1%25%2C%2083.1%25%2C%20and%2072.4%25%20mIoU%20scores%2C%20respectively%2C%20on%20the%0Aofficial%20benchmarks%20of%20SemanticKITTI%2C%20nuScenes%2C%20and%20Waymo%20Open.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01538v1&entry.124074799=Read"},
{"title": "MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language\n  Models using 2D Priors", "author": "Yuan Tang and Xu Han and Xianzhi Li and Qiao Yu and Yixue Hao and Long Hu and Min Chen", "abstract": "  Large 2D vision-language models (2D-LLMs) have gained significant attention\nby bridging Large Language Models (LLMs) with images using a simple projector.\nInspired by their success, large 3D point cloud-language models (3D-LLMs) also\nintegrate point clouds into LLMs. However, directly aligning point clouds with\nLLM requires expensive training costs, typically in hundreds of GPU-hours on\nA100, which hinders the development of 3D-LLMs. In this paper, we introduce\nMiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTA\nresults while training for only 27 hours on one RTX 3090. Specifically, we\npropose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, which\ncan leverage the similarity between 2D and 3D visual information. We introduce\na novel four-stage training strategy for modality alignment in a cascaded way,\nand a mixture of query experts module to adaptively aggregate features with\nhigh efficiency. Moreover, we utilize parameter-efficient fine-tuning methods\nLoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, which\nis up to 260x fewer than existing methods. Extensive experiments show that\nMiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, with\nsignificantly cheaper training costs. Notably, MiniGPT-3D gains an 8.12\nincrease on GPT-4 evaluation score for the challenging object captioning task\ncompared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800.\nWe are the first to explore the efficient 3D-LLM, offering new insights to the\ncommunity. Code and weights are available at\nhttps://github.com/TangYuan96/MiniGPT-3D.\n", "link": "http://arxiv.org/abs/2405.01413v1", "date": "2024-05-02", "relevancy": 2.3549, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6034}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5864}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiniGPT-3D%3A%20Efficiently%20Aligning%203D%20Point%20Clouds%20with%20Large%20Language%0A%20%20Models%20using%202D%20Priors&body=Title%3A%20MiniGPT-3D%3A%20Efficiently%20Aligning%203D%20Point%20Clouds%20with%20Large%20Language%0A%20%20Models%20using%202D%20Priors%0AAuthor%3A%20Yuan%20Tang%20and%20Xu%20Han%20and%20Xianzhi%20Li%20and%20Qiao%20Yu%20and%20Yixue%20Hao%20and%20Long%20Hu%20and%20Min%20Chen%0AAbstract%3A%20%20%20Large%202D%20vision-language%20models%20%282D-LLMs%29%20have%20gained%20significant%20attention%0Aby%20bridging%20Large%20Language%20Models%20%28LLMs%29%20with%20images%20using%20a%20simple%20projector.%0AInspired%20by%20their%20success%2C%20large%203D%20point%20cloud-language%20models%20%283D-LLMs%29%20also%0Aintegrate%20point%20clouds%20into%20LLMs.%20However%2C%20directly%20aligning%20point%20clouds%20with%0ALLM%20requires%20expensive%20training%20costs%2C%20typically%20in%20hundreds%20of%20GPU-hours%20on%0AA100%2C%20which%20hinders%20the%20development%20of%203D-LLMs.%20In%20this%20paper%2C%20we%20introduce%0AMiniGPT-3D%2C%20an%20efficient%20and%20powerful%203D-LLM%20that%20achieves%20multiple%20SOTA%0Aresults%20while%20training%20for%20only%2027%20hours%20on%20one%20RTX%203090.%20Specifically%2C%20we%0Apropose%20to%20align%203D%20point%20clouds%20with%20LLMs%20using%202D%20priors%20from%202D-LLMs%2C%20which%0Acan%20leverage%20the%20similarity%20between%202D%20and%203D%20visual%20information.%20We%20introduce%0Aa%20novel%20four-stage%20training%20strategy%20for%20modality%20alignment%20in%20a%20cascaded%20way%2C%0Aand%20a%20mixture%20of%20query%20experts%20module%20to%20adaptively%20aggregate%20features%20with%0Ahigh%20efficiency.%20Moreover%2C%20we%20utilize%20parameter-efficient%20fine-tuning%20methods%0ALoRA%20and%20Norm%20fine-tuning%2C%20resulting%20in%20only%2047.8M%20learnable%20parameters%2C%20which%0Ais%20up%20to%20260x%20fewer%20than%20existing%20methods.%20Extensive%20experiments%20show%20that%0AMiniGPT-3D%20achieves%20SOTA%20on%203D%20object%20classification%20and%20captioning%20tasks%2C%20with%0Asignificantly%20cheaper%20training%20costs.%20Notably%2C%20MiniGPT-3D%20gains%20an%208.12%0Aincrease%20on%20GPT-4%20evaluation%20score%20for%20the%20challenging%20object%20captioning%20task%0Acompared%20to%20ShapeLLM-13B%2C%20while%20the%20latter%20costs%20160%20total%20GPU-hours%20on%208%20A800.%0AWe%20are%20the%20first%20to%20explore%20the%20efficient%203D-LLM%2C%20offering%20new%20insights%20to%20the%0Acommunity.%20Code%20and%20weights%20are%20available%20at%0Ahttps%3A//github.com/TangYuan96/MiniGPT-3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiniGPT-3D%253A%2520Efficiently%2520Aligning%25203D%2520Point%2520Clouds%2520with%2520Large%2520Language%250A%2520%2520Models%2520using%25202D%2520Priors%26entry.906535625%3DYuan%2520Tang%2520and%2520Xu%2520Han%2520and%2520Xianzhi%2520Li%2520and%2520Qiao%2520Yu%2520and%2520Yixue%2520Hao%2520and%2520Long%2520Hu%2520and%2520Min%2520Chen%26entry.1292438233%3D%2520%2520Large%25202D%2520vision-language%2520models%2520%25282D-LLMs%2529%2520have%2520gained%2520significant%2520attention%250Aby%2520bridging%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520images%2520using%2520a%2520simple%2520projector.%250AInspired%2520by%2520their%2520success%252C%2520large%25203D%2520point%2520cloud-language%2520models%2520%25283D-LLMs%2529%2520also%250Aintegrate%2520point%2520clouds%2520into%2520LLMs.%2520However%252C%2520directly%2520aligning%2520point%2520clouds%2520with%250ALLM%2520requires%2520expensive%2520training%2520costs%252C%2520typically%2520in%2520hundreds%2520of%2520GPU-hours%2520on%250AA100%252C%2520which%2520hinders%2520the%2520development%2520of%25203D-LLMs.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AMiniGPT-3D%252C%2520an%2520efficient%2520and%2520powerful%25203D-LLM%2520that%2520achieves%2520multiple%2520SOTA%250Aresults%2520while%2520training%2520for%2520only%252027%2520hours%2520on%2520one%2520RTX%25203090.%2520Specifically%252C%2520we%250Apropose%2520to%2520align%25203D%2520point%2520clouds%2520with%2520LLMs%2520using%25202D%2520priors%2520from%25202D-LLMs%252C%2520which%250Acan%2520leverage%2520the%2520similarity%2520between%25202D%2520and%25203D%2520visual%2520information.%2520We%2520introduce%250Aa%2520novel%2520four-stage%2520training%2520strategy%2520for%2520modality%2520alignment%2520in%2520a%2520cascaded%2520way%252C%250Aand%2520a%2520mixture%2520of%2520query%2520experts%2520module%2520to%2520adaptively%2520aggregate%2520features%2520with%250Ahigh%2520efficiency.%2520Moreover%252C%2520we%2520utilize%2520parameter-efficient%2520fine-tuning%2520methods%250ALoRA%2520and%2520Norm%2520fine-tuning%252C%2520resulting%2520in%2520only%252047.8M%2520learnable%2520parameters%252C%2520which%250Ais%2520up%2520to%2520260x%2520fewer%2520than%2520existing%2520methods.%2520Extensive%2520experiments%2520show%2520that%250AMiniGPT-3D%2520achieves%2520SOTA%2520on%25203D%2520object%2520classification%2520and%2520captioning%2520tasks%252C%2520with%250Asignificantly%2520cheaper%2520training%2520costs.%2520Notably%252C%2520MiniGPT-3D%2520gains%2520an%25208.12%250Aincrease%2520on%2520GPT-4%2520evaluation%2520score%2520for%2520the%2520challenging%2520object%2520captioning%2520task%250Acompared%2520to%2520ShapeLLM-13B%252C%2520while%2520the%2520latter%2520costs%2520160%2520total%2520GPU-hours%2520on%25208%2520A800.%250AWe%2520are%2520the%2520first%2520to%2520explore%2520the%2520efficient%25203D-LLM%252C%2520offering%2520new%2520insights%2520to%2520the%250Acommunity.%2520Code%2520and%2520weights%2520are%2520available%2520at%250Ahttps%253A//github.com/TangYuan96/MiniGPT-3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiniGPT-3D%3A%20Efficiently%20Aligning%203D%20Point%20Clouds%20with%20Large%20Language%0A%20%20Models%20using%202D%20Priors&entry.906535625=Yuan%20Tang%20and%20Xu%20Han%20and%20Xianzhi%20Li%20and%20Qiao%20Yu%20and%20Yixue%20Hao%20and%20Long%20Hu%20and%20Min%20Chen&entry.1292438233=%20%20Large%202D%20vision-language%20models%20%282D-LLMs%29%20have%20gained%20significant%20attention%0Aby%20bridging%20Large%20Language%20Models%20%28LLMs%29%20with%20images%20using%20a%20simple%20projector.%0AInspired%20by%20their%20success%2C%20large%203D%20point%20cloud-language%20models%20%283D-LLMs%29%20also%0Aintegrate%20point%20clouds%20into%20LLMs.%20However%2C%20directly%20aligning%20point%20clouds%20with%0ALLM%20requires%20expensive%20training%20costs%2C%20typically%20in%20hundreds%20of%20GPU-hours%20on%0AA100%2C%20which%20hinders%20the%20development%20of%203D-LLMs.%20In%20this%20paper%2C%20we%20introduce%0AMiniGPT-3D%2C%20an%20efficient%20and%20powerful%203D-LLM%20that%20achieves%20multiple%20SOTA%0Aresults%20while%20training%20for%20only%2027%20hours%20on%20one%20RTX%203090.%20Specifically%2C%20we%0Apropose%20to%20align%203D%20point%20clouds%20with%20LLMs%20using%202D%20priors%20from%202D-LLMs%2C%20which%0Acan%20leverage%20the%20similarity%20between%202D%20and%203D%20visual%20information.%20We%20introduce%0Aa%20novel%20four-stage%20training%20strategy%20for%20modality%20alignment%20in%20a%20cascaded%20way%2C%0Aand%20a%20mixture%20of%20query%20experts%20module%20to%20adaptively%20aggregate%20features%20with%0Ahigh%20efficiency.%20Moreover%2C%20we%20utilize%20parameter-efficient%20fine-tuning%20methods%0ALoRA%20and%20Norm%20fine-tuning%2C%20resulting%20in%20only%2047.8M%20learnable%20parameters%2C%20which%0Ais%20up%20to%20260x%20fewer%20than%20existing%20methods.%20Extensive%20experiments%20show%20that%0AMiniGPT-3D%20achieves%20SOTA%20on%203D%20object%20classification%20and%20captioning%20tasks%2C%20with%0Asignificantly%20cheaper%20training%20costs.%20Notably%2C%20MiniGPT-3D%20gains%20an%208.12%0Aincrease%20on%20GPT-4%20evaluation%20score%20for%20the%20challenging%20object%20captioning%20task%0Acompared%20to%20ShapeLLM-13B%2C%20while%20the%20latter%20costs%20160%20total%20GPU-hours%20on%208%20A800.%0AWe%20are%20the%20first%20to%20explore%20the%20efficient%203D-LLM%2C%20offering%20new%20insights%20to%20the%0Acommunity.%20Code%20and%20weights%20are%20available%20at%0Ahttps%3A//github.com/TangYuan96/MiniGPT-3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01413v1&entry.124074799=Read"},
{"title": "Community-Invariant Graph Contrastive Learning", "author": "Shiyin Tan and Dongyuan Li and Renhe Jiang and Ying Zhang and Manabu Okumura", "abstract": "  Graph augmentation has received great attention in recent years for graph\ncontrastive learning (GCL) to learn well-generalized node/graph\nrepresentations. However, mainstream GCL methods often favor randomly\ndisrupting graphs for augmentation, which shows limited generalization and\ninevitably leads to the corruption of high-level graph information, i.e., the\ngraph community. Moreover, current knowledge-based graph augmentation methods\ncan only focus on either topology or node features, causing the model to lack\nrobustness against various types of noise. To address these limitations, this\nresearch investigated the role of the graph community in graph augmentation and\nfigured out its crucial advantage for learnable graph augmentation. Based on\nour observations, we propose a community-invariant GCL framework to maintain\ngraph community structure during learnable graph augmentation. By maximizing\nthe spectral changes, this framework unifies the constraints of both topology\nand feature augmentation, enhancing the model's robustness. Empirical evidence\non 21 benchmark datasets demonstrates the exclusive merits of our framework.\nCode is released on Github (https://github.com/ShiyinTan/CI-GCL.git).\n", "link": "http://arxiv.org/abs/2405.01350v1", "date": "2024-05-02", "relevancy": 2.3453, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4918}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4634}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Community-Invariant%20Graph%20Contrastive%20Learning&body=Title%3A%20Community-Invariant%20Graph%20Contrastive%20Learning%0AAuthor%3A%20Shiyin%20Tan%20and%20Dongyuan%20Li%20and%20Renhe%20Jiang%20and%20Ying%20Zhang%20and%20Manabu%20Okumura%0AAbstract%3A%20%20%20Graph%20augmentation%20has%20received%20great%20attention%20in%20recent%20years%20for%20graph%0Acontrastive%20learning%20%28GCL%29%20to%20learn%20well-generalized%20node/graph%0Arepresentations.%20However%2C%20mainstream%20GCL%20methods%20often%20favor%20randomly%0Adisrupting%20graphs%20for%20augmentation%2C%20which%20shows%20limited%20generalization%20and%0Ainevitably%20leads%20to%20the%20corruption%20of%20high-level%20graph%20information%2C%20i.e.%2C%20the%0Agraph%20community.%20Moreover%2C%20current%20knowledge-based%20graph%20augmentation%20methods%0Acan%20only%20focus%20on%20either%20topology%20or%20node%20features%2C%20causing%20the%20model%20to%20lack%0Arobustness%20against%20various%20types%20of%20noise.%20To%20address%20these%20limitations%2C%20this%0Aresearch%20investigated%20the%20role%20of%20the%20graph%20community%20in%20graph%20augmentation%20and%0Afigured%20out%20its%20crucial%20advantage%20for%20learnable%20graph%20augmentation.%20Based%20on%0Aour%20observations%2C%20we%20propose%20a%20community-invariant%20GCL%20framework%20to%20maintain%0Agraph%20community%20structure%20during%20learnable%20graph%20augmentation.%20By%20maximizing%0Athe%20spectral%20changes%2C%20this%20framework%20unifies%20the%20constraints%20of%20both%20topology%0Aand%20feature%20augmentation%2C%20enhancing%20the%20model%27s%20robustness.%20Empirical%20evidence%0Aon%2021%20benchmark%20datasets%20demonstrates%20the%20exclusive%20merits%20of%20our%20framework.%0ACode%20is%20released%20on%20Github%20%28https%3A//github.com/ShiyinTan/CI-GCL.git%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunity-Invariant%2520Graph%2520Contrastive%2520Learning%26entry.906535625%3DShiyin%2520Tan%2520and%2520Dongyuan%2520Li%2520and%2520Renhe%2520Jiang%2520and%2520Ying%2520Zhang%2520and%2520Manabu%2520Okumura%26entry.1292438233%3D%2520%2520Graph%2520augmentation%2520has%2520received%2520great%2520attention%2520in%2520recent%2520years%2520for%2520graph%250Acontrastive%2520learning%2520%2528GCL%2529%2520to%2520learn%2520well-generalized%2520node/graph%250Arepresentations.%2520However%252C%2520mainstream%2520GCL%2520methods%2520often%2520favor%2520randomly%250Adisrupting%2520graphs%2520for%2520augmentation%252C%2520which%2520shows%2520limited%2520generalization%2520and%250Ainevitably%2520leads%2520to%2520the%2520corruption%2520of%2520high-level%2520graph%2520information%252C%2520i.e.%252C%2520the%250Agraph%2520community.%2520Moreover%252C%2520current%2520knowledge-based%2520graph%2520augmentation%2520methods%250Acan%2520only%2520focus%2520on%2520either%2520topology%2520or%2520node%2520features%252C%2520causing%2520the%2520model%2520to%2520lack%250Arobustness%2520against%2520various%2520types%2520of%2520noise.%2520To%2520address%2520these%2520limitations%252C%2520this%250Aresearch%2520investigated%2520the%2520role%2520of%2520the%2520graph%2520community%2520in%2520graph%2520augmentation%2520and%250Afigured%2520out%2520its%2520crucial%2520advantage%2520for%2520learnable%2520graph%2520augmentation.%2520Based%2520on%250Aour%2520observations%252C%2520we%2520propose%2520a%2520community-invariant%2520GCL%2520framework%2520to%2520maintain%250Agraph%2520community%2520structure%2520during%2520learnable%2520graph%2520augmentation.%2520By%2520maximizing%250Athe%2520spectral%2520changes%252C%2520this%2520framework%2520unifies%2520the%2520constraints%2520of%2520both%2520topology%250Aand%2520feature%2520augmentation%252C%2520enhancing%2520the%2520model%2527s%2520robustness.%2520Empirical%2520evidence%250Aon%252021%2520benchmark%2520datasets%2520demonstrates%2520the%2520exclusive%2520merits%2520of%2520our%2520framework.%250ACode%2520is%2520released%2520on%2520Github%2520%2528https%253A//github.com/ShiyinTan/CI-GCL.git%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Community-Invariant%20Graph%20Contrastive%20Learning&entry.906535625=Shiyin%20Tan%20and%20Dongyuan%20Li%20and%20Renhe%20Jiang%20and%20Ying%20Zhang%20and%20Manabu%20Okumura&entry.1292438233=%20%20Graph%20augmentation%20has%20received%20great%20attention%20in%20recent%20years%20for%20graph%0Acontrastive%20learning%20%28GCL%29%20to%20learn%20well-generalized%20node/graph%0Arepresentations.%20However%2C%20mainstream%20GCL%20methods%20often%20favor%20randomly%0Adisrupting%20graphs%20for%20augmentation%2C%20which%20shows%20limited%20generalization%20and%0Ainevitably%20leads%20to%20the%20corruption%20of%20high-level%20graph%20information%2C%20i.e.%2C%20the%0Agraph%20community.%20Moreover%2C%20current%20knowledge-based%20graph%20augmentation%20methods%0Acan%20only%20focus%20on%20either%20topology%20or%20node%20features%2C%20causing%20the%20model%20to%20lack%0Arobustness%20against%20various%20types%20of%20noise.%20To%20address%20these%20limitations%2C%20this%0Aresearch%20investigated%20the%20role%20of%20the%20graph%20community%20in%20graph%20augmentation%20and%0Afigured%20out%20its%20crucial%20advantage%20for%20learnable%20graph%20augmentation.%20Based%20on%0Aour%20observations%2C%20we%20propose%20a%20community-invariant%20GCL%20framework%20to%20maintain%0Agraph%20community%20structure%20during%20learnable%20graph%20augmentation.%20By%20maximizing%0Athe%20spectral%20changes%2C%20this%20framework%20unifies%20the%20constraints%20of%20both%20topology%0Aand%20feature%20augmentation%2C%20enhancing%20the%20model%27s%20robustness.%20Empirical%20evidence%0Aon%2021%20benchmark%20datasets%20demonstrates%20the%20exclusive%20merits%20of%20our%20framework.%0ACode%20is%20released%20on%20Github%20%28https%3A//github.com/ShiyinTan/CI-GCL.git%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01350v1&entry.124074799=Read"},
{"title": "OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D\n  Perception, Reasoning and Planning", "author": "Shihao Wang and Zhiding Yu and Xiaohui Jiang and Shiyi Lan and Min Shi and Nadine Chang and Jan Kautz and Ying Li and Jose M. Alvarez", "abstract": "  The advances in multimodal large language models (MLLMs) have led to growing\ninterests in LLM-based autonomous driving agents to leverage their strong\nreasoning capabilities. However, capitalizing on MLLMs' strong reasoning\ncapabilities for improved planning behavior is challenging since planning\nrequires full 3D situational awareness beyond 2D reasoning. To address this\nchallenge, our work proposes a holistic framework for strong alignment between\nagent models and 3D driving tasks. Our framework starts with a novel 3D MLLM\narchitecture that uses sparse queries to lift and compress visual\nrepresentations into 3D before feeding them into an LLM. This query-based\nrepresentation allows us to jointly encode dynamic objects and static map\nelements (e.g., traffic lanes), providing a condensed world model for\nperception-action alignment in 3D. We further propose OmniDrive-nuScenes, a new\nvisual question-answering dataset challenging the true 3D situational awareness\nof a model with comprehensive visual question-answering (VQA) tasks, including\nscene description, traffic regulation, 3D grounding, counterfactual reasoning,\ndecision making and planning. Extensive studies show the effectiveness of the\nproposed architecture as well as the importance of the VQA tasks for reasoning\nand planning in complex 3D scenes.\n", "link": "http://arxiv.org/abs/2405.01533v1", "date": "2024-05-02", "relevancy": 2.3284, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5757}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniDrive%3A%20A%20Holistic%20LLM-Agent%20Framework%20for%20Autonomous%20Driving%20with%203D%0A%20%20Perception%2C%20Reasoning%20and%20Planning&body=Title%3A%20OmniDrive%3A%20A%20Holistic%20LLM-Agent%20Framework%20for%20Autonomous%20Driving%20with%203D%0A%20%20Perception%2C%20Reasoning%20and%20Planning%0AAuthor%3A%20Shihao%20Wang%20and%20Zhiding%20Yu%20and%20Xiaohui%20Jiang%20and%20Shiyi%20Lan%20and%20Min%20Shi%20and%20Nadine%20Chang%20and%20Jan%20Kautz%20and%20Ying%20Li%20and%20Jose%20M.%20Alvarez%0AAbstract%3A%20%20%20The%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20led%20to%20growing%0Ainterests%20in%20LLM-based%20autonomous%20driving%20agents%20to%20leverage%20their%20strong%0Areasoning%20capabilities.%20However%2C%20capitalizing%20on%20MLLMs%27%20strong%20reasoning%0Acapabilities%20for%20improved%20planning%20behavior%20is%20challenging%20since%20planning%0Arequires%20full%203D%20situational%20awareness%20beyond%202D%20reasoning.%20To%20address%20this%0Achallenge%2C%20our%20work%20proposes%20a%20holistic%20framework%20for%20strong%20alignment%20between%0Aagent%20models%20and%203D%20driving%20tasks.%20Our%20framework%20starts%20with%20a%20novel%203D%20MLLM%0Aarchitecture%20that%20uses%20sparse%20queries%20to%20lift%20and%20compress%20visual%0Arepresentations%20into%203D%20before%20feeding%20them%20into%20an%20LLM.%20This%20query-based%0Arepresentation%20allows%20us%20to%20jointly%20encode%20dynamic%20objects%20and%20static%20map%0Aelements%20%28e.g.%2C%20traffic%20lanes%29%2C%20providing%20a%20condensed%20world%20model%20for%0Aperception-action%20alignment%20in%203D.%20We%20further%20propose%20OmniDrive-nuScenes%2C%20a%20new%0Avisual%20question-answering%20dataset%20challenging%20the%20true%203D%20situational%20awareness%0Aof%20a%20model%20with%20comprehensive%20visual%20question-answering%20%28VQA%29%20tasks%2C%20including%0Ascene%20description%2C%20traffic%20regulation%2C%203D%20grounding%2C%20counterfactual%20reasoning%2C%0Adecision%20making%20and%20planning.%20Extensive%20studies%20show%20the%20effectiveness%20of%20the%0Aproposed%20architecture%20as%20well%20as%20the%20importance%20of%20the%20VQA%20tasks%20for%20reasoning%0Aand%20planning%20in%20complex%203D%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniDrive%253A%2520A%2520Holistic%2520LLM-Agent%2520Framework%2520for%2520Autonomous%2520Driving%2520with%25203D%250A%2520%2520Perception%252C%2520Reasoning%2520and%2520Planning%26entry.906535625%3DShihao%2520Wang%2520and%2520Zhiding%2520Yu%2520and%2520Xiaohui%2520Jiang%2520and%2520Shiyi%2520Lan%2520and%2520Min%2520Shi%2520and%2520Nadine%2520Chang%2520and%2520Jan%2520Kautz%2520and%2520Ying%2520Li%2520and%2520Jose%2520M.%2520Alvarez%26entry.1292438233%3D%2520%2520The%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520led%2520to%2520growing%250Ainterests%2520in%2520LLM-based%2520autonomous%2520driving%2520agents%2520to%2520leverage%2520their%2520strong%250Areasoning%2520capabilities.%2520However%252C%2520capitalizing%2520on%2520MLLMs%2527%2520strong%2520reasoning%250Acapabilities%2520for%2520improved%2520planning%2520behavior%2520is%2520challenging%2520since%2520planning%250Arequires%2520full%25203D%2520situational%2520awareness%2520beyond%25202D%2520reasoning.%2520To%2520address%2520this%250Achallenge%252C%2520our%2520work%2520proposes%2520a%2520holistic%2520framework%2520for%2520strong%2520alignment%2520between%250Aagent%2520models%2520and%25203D%2520driving%2520tasks.%2520Our%2520framework%2520starts%2520with%2520a%2520novel%25203D%2520MLLM%250Aarchitecture%2520that%2520uses%2520sparse%2520queries%2520to%2520lift%2520and%2520compress%2520visual%250Arepresentations%2520into%25203D%2520before%2520feeding%2520them%2520into%2520an%2520LLM.%2520This%2520query-based%250Arepresentation%2520allows%2520us%2520to%2520jointly%2520encode%2520dynamic%2520objects%2520and%2520static%2520map%250Aelements%2520%2528e.g.%252C%2520traffic%2520lanes%2529%252C%2520providing%2520a%2520condensed%2520world%2520model%2520for%250Aperception-action%2520alignment%2520in%25203D.%2520We%2520further%2520propose%2520OmniDrive-nuScenes%252C%2520a%2520new%250Avisual%2520question-answering%2520dataset%2520challenging%2520the%2520true%25203D%2520situational%2520awareness%250Aof%2520a%2520model%2520with%2520comprehensive%2520visual%2520question-answering%2520%2528VQA%2529%2520tasks%252C%2520including%250Ascene%2520description%252C%2520traffic%2520regulation%252C%25203D%2520grounding%252C%2520counterfactual%2520reasoning%252C%250Adecision%2520making%2520and%2520planning.%2520Extensive%2520studies%2520show%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520architecture%2520as%2520well%2520as%2520the%2520importance%2520of%2520the%2520VQA%2520tasks%2520for%2520reasoning%250Aand%2520planning%2520in%2520complex%25203D%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniDrive%3A%20A%20Holistic%20LLM-Agent%20Framework%20for%20Autonomous%20Driving%20with%203D%0A%20%20Perception%2C%20Reasoning%20and%20Planning&entry.906535625=Shihao%20Wang%20and%20Zhiding%20Yu%20and%20Xiaohui%20Jiang%20and%20Shiyi%20Lan%20and%20Min%20Shi%20and%20Nadine%20Chang%20and%20Jan%20Kautz%20and%20Ying%20Li%20and%20Jose%20M.%20Alvarez&entry.1292438233=%20%20The%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20led%20to%20growing%0Ainterests%20in%20LLM-based%20autonomous%20driving%20agents%20to%20leverage%20their%20strong%0Areasoning%20capabilities.%20However%2C%20capitalizing%20on%20MLLMs%27%20strong%20reasoning%0Acapabilities%20for%20improved%20planning%20behavior%20is%20challenging%20since%20planning%0Arequires%20full%203D%20situational%20awareness%20beyond%202D%20reasoning.%20To%20address%20this%0Achallenge%2C%20our%20work%20proposes%20a%20holistic%20framework%20for%20strong%20alignment%20between%0Aagent%20models%20and%203D%20driving%20tasks.%20Our%20framework%20starts%20with%20a%20novel%203D%20MLLM%0Aarchitecture%20that%20uses%20sparse%20queries%20to%20lift%20and%20compress%20visual%0Arepresentations%20into%203D%20before%20feeding%20them%20into%20an%20LLM.%20This%20query-based%0Arepresentation%20allows%20us%20to%20jointly%20encode%20dynamic%20objects%20and%20static%20map%0Aelements%20%28e.g.%2C%20traffic%20lanes%29%2C%20providing%20a%20condensed%20world%20model%20for%0Aperception-action%20alignment%20in%203D.%20We%20further%20propose%20OmniDrive-nuScenes%2C%20a%20new%0Avisual%20question-answering%20dataset%20challenging%20the%20true%203D%20situational%20awareness%0Aof%20a%20model%20with%20comprehensive%20visual%20question-answering%20%28VQA%29%20tasks%2C%20including%0Ascene%20description%2C%20traffic%20regulation%2C%203D%20grounding%2C%20counterfactual%20reasoning%2C%0Adecision%20making%20and%20planning.%20Extensive%20studies%20show%20the%20effectiveness%20of%20the%0Aproposed%20architecture%20as%20well%20as%20the%20importance%20of%20the%20VQA%20tasks%20for%20reasoning%0Aand%20planning%20in%20complex%203D%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01533v1&entry.124074799=Read"},
{"title": "Scalable network reconstruction in subquadratic time", "author": "Tiago P. Peixoto", "abstract": "  Network reconstruction consists in determining the unobserved pairwise\ncouplings between $N$ nodes given only observational data on the resulting\nbehavior that is conditioned on those couplings -- typically a time-series or\nindependent samples from a graphical model. A major obstacle to the scalability\nof algorithms proposed for this problem is a seemingly unavoidable quadratic\ncomplexity of $\\Omega(N^2)$, corresponding to the requirement of each possible\npairwise coupling being contemplated at least once, despite the fact that most\nnetworks of interest are sparse, with a number of non-zero couplings that is\nonly $O(N)$. Here we present a general algorithm applicable to a broad range of\nreconstruction problems that significantly outperforms this quadratic baseline.\nOur algorithm relies on a stochastic second neighbor search (Dong et al., 2011)\nthat produces the best edge candidates with high probability, thus bypassing an\nexhaustive quadratic search. If we rely on the conjecture that the\nsecond-neighbor search finishes in log-linear time (Baron & Darling, 2020;\n2022), we demonstrate theoretically that our algorithm finishes in subquadratic\ntime, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\\log\nN)$, but with a more typical log-linear complexity of $O(N\\log^2N)$. In\npractice, we show that our algorithm achieves a performance that is many orders\nof magnitude faster than the quadratic baseline -- in a manner consistent with\nour theoretical analysis -- allows for easy parallelization, and thus enables\nthe reconstruction of networks with hundreds of thousands and even millions of\nnodes and edges.\n", "link": "http://arxiv.org/abs/2401.01404v4", "date": "2024-05-02", "relevancy": 2.3264, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4832}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4723}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20network%20reconstruction%20in%20subquadratic%20time&body=Title%3A%20Scalable%20network%20reconstruction%20in%20subquadratic%20time%0AAuthor%3A%20Tiago%20P.%20Peixoto%0AAbstract%3A%20%20%20Network%20reconstruction%20consists%20in%20determining%20the%20unobserved%20pairwise%0Acouplings%20between%20%24N%24%20nodes%20given%20only%20observational%20data%20on%20the%20resulting%0Abehavior%20that%20is%20conditioned%20on%20those%20couplings%20--%20typically%20a%20time-series%20or%0Aindependent%20samples%20from%20a%20graphical%20model.%20A%20major%20obstacle%20to%20the%20scalability%0Aof%20algorithms%20proposed%20for%20this%20problem%20is%20a%20seemingly%20unavoidable%20quadratic%0Acomplexity%20of%20%24%5COmega%28N%5E2%29%24%2C%20corresponding%20to%20the%20requirement%20of%20each%20possible%0Apairwise%20coupling%20being%20contemplated%20at%20least%20once%2C%20despite%20the%20fact%20that%20most%0Anetworks%20of%20interest%20are%20sparse%2C%20with%20a%20number%20of%20non-zero%20couplings%20that%20is%0Aonly%20%24O%28N%29%24.%20Here%20we%20present%20a%20general%20algorithm%20applicable%20to%20a%20broad%20range%20of%0Areconstruction%20problems%20that%20significantly%20outperforms%20this%20quadratic%20baseline.%0AOur%20algorithm%20relies%20on%20a%20stochastic%20second%20neighbor%20search%20%28Dong%20et%20al.%2C%202011%29%0Athat%20produces%20the%20best%20edge%20candidates%20with%20high%20probability%2C%20thus%20bypassing%20an%0Aexhaustive%20quadratic%20search.%20If%20we%20rely%20on%20the%20conjecture%20that%20the%0Asecond-neighbor%20search%20finishes%20in%20log-linear%20time%20%28Baron%20%26%20Darling%2C%202020%3B%0A2022%29%2C%20we%20demonstrate%20theoretically%20that%20our%20algorithm%20finishes%20in%20subquadratic%0Atime%2C%20with%20a%20data-dependent%20complexity%20loosely%20upper%20bounded%20by%20%24O%28N%5E%7B3/2%7D%5Clog%0AN%29%24%2C%20but%20with%20a%20more%20typical%20log-linear%20complexity%20of%20%24O%28N%5Clog%5E2N%29%24.%20In%0Apractice%2C%20we%20show%20that%20our%20algorithm%20achieves%20a%20performance%20that%20is%20many%20orders%0Aof%20magnitude%20faster%20than%20the%20quadratic%20baseline%20--%20in%20a%20manner%20consistent%20with%0Aour%20theoretical%20analysis%20--%20allows%20for%20easy%20parallelization%2C%20and%20thus%20enables%0Athe%20reconstruction%20of%20networks%20with%20hundreds%20of%20thousands%20and%20even%20millions%20of%0Anodes%20and%20edges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01404v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520network%2520reconstruction%2520in%2520subquadratic%2520time%26entry.906535625%3DTiago%2520P.%2520Peixoto%26entry.1292438233%3D%2520%2520Network%2520reconstruction%2520consists%2520in%2520determining%2520the%2520unobserved%2520pairwise%250Acouplings%2520between%2520%2524N%2524%2520nodes%2520given%2520only%2520observational%2520data%2520on%2520the%2520resulting%250Abehavior%2520that%2520is%2520conditioned%2520on%2520those%2520couplings%2520--%2520typically%2520a%2520time-series%2520or%250Aindependent%2520samples%2520from%2520a%2520graphical%2520model.%2520A%2520major%2520obstacle%2520to%2520the%2520scalability%250Aof%2520algorithms%2520proposed%2520for%2520this%2520problem%2520is%2520a%2520seemingly%2520unavoidable%2520quadratic%250Acomplexity%2520of%2520%2524%255COmega%2528N%255E2%2529%2524%252C%2520corresponding%2520to%2520the%2520requirement%2520of%2520each%2520possible%250Apairwise%2520coupling%2520being%2520contemplated%2520at%2520least%2520once%252C%2520despite%2520the%2520fact%2520that%2520most%250Anetworks%2520of%2520interest%2520are%2520sparse%252C%2520with%2520a%2520number%2520of%2520non-zero%2520couplings%2520that%2520is%250Aonly%2520%2524O%2528N%2529%2524.%2520Here%2520we%2520present%2520a%2520general%2520algorithm%2520applicable%2520to%2520a%2520broad%2520range%2520of%250Areconstruction%2520problems%2520that%2520significantly%2520outperforms%2520this%2520quadratic%2520baseline.%250AOur%2520algorithm%2520relies%2520on%2520a%2520stochastic%2520second%2520neighbor%2520search%2520%2528Dong%2520et%2520al.%252C%25202011%2529%250Athat%2520produces%2520the%2520best%2520edge%2520candidates%2520with%2520high%2520probability%252C%2520thus%2520bypassing%2520an%250Aexhaustive%2520quadratic%2520search.%2520If%2520we%2520rely%2520on%2520the%2520conjecture%2520that%2520the%250Asecond-neighbor%2520search%2520finishes%2520in%2520log-linear%2520time%2520%2528Baron%2520%2526%2520Darling%252C%25202020%253B%250A2022%2529%252C%2520we%2520demonstrate%2520theoretically%2520that%2520our%2520algorithm%2520finishes%2520in%2520subquadratic%250Atime%252C%2520with%2520a%2520data-dependent%2520complexity%2520loosely%2520upper%2520bounded%2520by%2520%2524O%2528N%255E%257B3/2%257D%255Clog%250AN%2529%2524%252C%2520but%2520with%2520a%2520more%2520typical%2520log-linear%2520complexity%2520of%2520%2524O%2528N%255Clog%255E2N%2529%2524.%2520In%250Apractice%252C%2520we%2520show%2520that%2520our%2520algorithm%2520achieves%2520a%2520performance%2520that%2520is%2520many%2520orders%250Aof%2520magnitude%2520faster%2520than%2520the%2520quadratic%2520baseline%2520--%2520in%2520a%2520manner%2520consistent%2520with%250Aour%2520theoretical%2520analysis%2520--%2520allows%2520for%2520easy%2520parallelization%252C%2520and%2520thus%2520enables%250Athe%2520reconstruction%2520of%2520networks%2520with%2520hundreds%2520of%2520thousands%2520and%2520even%2520millions%2520of%250Anodes%2520and%2520edges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01404v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20network%20reconstruction%20in%20subquadratic%20time&entry.906535625=Tiago%20P.%20Peixoto&entry.1292438233=%20%20Network%20reconstruction%20consists%20in%20determining%20the%20unobserved%20pairwise%0Acouplings%20between%20%24N%24%20nodes%20given%20only%20observational%20data%20on%20the%20resulting%0Abehavior%20that%20is%20conditioned%20on%20those%20couplings%20--%20typically%20a%20time-series%20or%0Aindependent%20samples%20from%20a%20graphical%20model.%20A%20major%20obstacle%20to%20the%20scalability%0Aof%20algorithms%20proposed%20for%20this%20problem%20is%20a%20seemingly%20unavoidable%20quadratic%0Acomplexity%20of%20%24%5COmega%28N%5E2%29%24%2C%20corresponding%20to%20the%20requirement%20of%20each%20possible%0Apairwise%20coupling%20being%20contemplated%20at%20least%20once%2C%20despite%20the%20fact%20that%20most%0Anetworks%20of%20interest%20are%20sparse%2C%20with%20a%20number%20of%20non-zero%20couplings%20that%20is%0Aonly%20%24O%28N%29%24.%20Here%20we%20present%20a%20general%20algorithm%20applicable%20to%20a%20broad%20range%20of%0Areconstruction%20problems%20that%20significantly%20outperforms%20this%20quadratic%20baseline.%0AOur%20algorithm%20relies%20on%20a%20stochastic%20second%20neighbor%20search%20%28Dong%20et%20al.%2C%202011%29%0Athat%20produces%20the%20best%20edge%20candidates%20with%20high%20probability%2C%20thus%20bypassing%20an%0Aexhaustive%20quadratic%20search.%20If%20we%20rely%20on%20the%20conjecture%20that%20the%0Asecond-neighbor%20search%20finishes%20in%20log-linear%20time%20%28Baron%20%26%20Darling%2C%202020%3B%0A2022%29%2C%20we%20demonstrate%20theoretically%20that%20our%20algorithm%20finishes%20in%20subquadratic%0Atime%2C%20with%20a%20data-dependent%20complexity%20loosely%20upper%20bounded%20by%20%24O%28N%5E%7B3/2%7D%5Clog%0AN%29%24%2C%20but%20with%20a%20more%20typical%20log-linear%20complexity%20of%20%24O%28N%5Clog%5E2N%29%24.%20In%0Apractice%2C%20we%20show%20that%20our%20algorithm%20achieves%20a%20performance%20that%20is%20many%20orders%0Aof%20magnitude%20faster%20than%20the%20quadratic%20baseline%20--%20in%20a%20manner%20consistent%20with%0Aour%20theoretical%20analysis%20--%20allows%20for%20easy%20parallelization%2C%20and%20thus%20enables%0Athe%20reconstruction%20of%20networks%20with%20hundreds%20of%20thousands%20and%20even%20millions%20of%0Anodes%20and%20edges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01404v4&entry.124074799=Read"},
{"title": "Improving Subject-Driven Image Synthesis with Subject-Agnostic Guidance", "author": "Kelvin C. K. Chan and Yang Zhao and Xuhui Jia and Ming-Hsuan Yang and Huisheng Wang", "abstract": "  In subject-driven text-to-image synthesis, the synthesis process tends to be\nheavily influenced by the reference images provided by users, often overlooking\ncrucial attributes detailed in the text prompt. In this work, we propose\nSubject-Agnostic Guidance (SAG), a simple yet effective solution to remedy the\nproblem. We show that through constructing a subject-agnostic condition and\napplying our proposed dual classifier-free guidance, one could obtain outputs\nconsistent with both the given subject and input text prompts. We validate the\nefficacy of our approach through both optimization-based and encoder-based\nmethods. Additionally, we demonstrate its applicability in second-order\ncustomization methods, where an encoder-based model is fine-tuned with\nDreamBooth. Our approach is conceptually simple and requires only minimal code\nmodifications, but leads to substantial quality improvements, as evidenced by\nour evaluations and user studies.\n", "link": "http://arxiv.org/abs/2405.01356v1", "date": "2024-05-02", "relevancy": 2.2778, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5779}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5646}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Subject-Driven%20Image%20Synthesis%20with%20Subject-Agnostic%20Guidance&body=Title%3A%20Improving%20Subject-Driven%20Image%20Synthesis%20with%20Subject-Agnostic%20Guidance%0AAuthor%3A%20Kelvin%20C.%20K.%20Chan%20and%20Yang%20Zhao%20and%20Xuhui%20Jia%20and%20Ming-Hsuan%20Yang%20and%20Huisheng%20Wang%0AAbstract%3A%20%20%20In%20subject-driven%20text-to-image%20synthesis%2C%20the%20synthesis%20process%20tends%20to%20be%0Aheavily%20influenced%20by%20the%20reference%20images%20provided%20by%20users%2C%20often%20overlooking%0Acrucial%20attributes%20detailed%20in%20the%20text%20prompt.%20In%20this%20work%2C%20we%20propose%0ASubject-Agnostic%20Guidance%20%28SAG%29%2C%20a%20simple%20yet%20effective%20solution%20to%20remedy%20the%0Aproblem.%20We%20show%20that%20through%20constructing%20a%20subject-agnostic%20condition%20and%0Aapplying%20our%20proposed%20dual%20classifier-free%20guidance%2C%20one%20could%20obtain%20outputs%0Aconsistent%20with%20both%20the%20given%20subject%20and%20input%20text%20prompts.%20We%20validate%20the%0Aefficacy%20of%20our%20approach%20through%20both%20optimization-based%20and%20encoder-based%0Amethods.%20Additionally%2C%20we%20demonstrate%20its%20applicability%20in%20second-order%0Acustomization%20methods%2C%20where%20an%20encoder-based%20model%20is%20fine-tuned%20with%0ADreamBooth.%20Our%20approach%20is%20conceptually%20simple%20and%20requires%20only%20minimal%20code%0Amodifications%2C%20but%20leads%20to%20substantial%20quality%20improvements%2C%20as%20evidenced%20by%0Aour%20evaluations%20and%20user%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Subject-Driven%2520Image%2520Synthesis%2520with%2520Subject-Agnostic%2520Guidance%26entry.906535625%3DKelvin%2520C.%2520K.%2520Chan%2520and%2520Yang%2520Zhao%2520and%2520Xuhui%2520Jia%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Huisheng%2520Wang%26entry.1292438233%3D%2520%2520In%2520subject-driven%2520text-to-image%2520synthesis%252C%2520the%2520synthesis%2520process%2520tends%2520to%2520be%250Aheavily%2520influenced%2520by%2520the%2520reference%2520images%2520provided%2520by%2520users%252C%2520often%2520overlooking%250Acrucial%2520attributes%2520detailed%2520in%2520the%2520text%2520prompt.%2520In%2520this%2520work%252C%2520we%2520propose%250ASubject-Agnostic%2520Guidance%2520%2528SAG%2529%252C%2520a%2520simple%2520yet%2520effective%2520solution%2520to%2520remedy%2520the%250Aproblem.%2520We%2520show%2520that%2520through%2520constructing%2520a%2520subject-agnostic%2520condition%2520and%250Aapplying%2520our%2520proposed%2520dual%2520classifier-free%2520guidance%252C%2520one%2520could%2520obtain%2520outputs%250Aconsistent%2520with%2520both%2520the%2520given%2520subject%2520and%2520input%2520text%2520prompts.%2520We%2520validate%2520the%250Aefficacy%2520of%2520our%2520approach%2520through%2520both%2520optimization-based%2520and%2520encoder-based%250Amethods.%2520Additionally%252C%2520we%2520demonstrate%2520its%2520applicability%2520in%2520second-order%250Acustomization%2520methods%252C%2520where%2520an%2520encoder-based%2520model%2520is%2520fine-tuned%2520with%250ADreamBooth.%2520Our%2520approach%2520is%2520conceptually%2520simple%2520and%2520requires%2520only%2520minimal%2520code%250Amodifications%252C%2520but%2520leads%2520to%2520substantial%2520quality%2520improvements%252C%2520as%2520evidenced%2520by%250Aour%2520evaluations%2520and%2520user%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Subject-Driven%20Image%20Synthesis%20with%20Subject-Agnostic%20Guidance&entry.906535625=Kelvin%20C.%20K.%20Chan%20and%20Yang%20Zhao%20and%20Xuhui%20Jia%20and%20Ming-Hsuan%20Yang%20and%20Huisheng%20Wang&entry.1292438233=%20%20In%20subject-driven%20text-to-image%20synthesis%2C%20the%20synthesis%20process%20tends%20to%20be%0Aheavily%20influenced%20by%20the%20reference%20images%20provided%20by%20users%2C%20often%20overlooking%0Acrucial%20attributes%20detailed%20in%20the%20text%20prompt.%20In%20this%20work%2C%20we%20propose%0ASubject-Agnostic%20Guidance%20%28SAG%29%2C%20a%20simple%20yet%20effective%20solution%20to%20remedy%20the%0Aproblem.%20We%20show%20that%20through%20constructing%20a%20subject-agnostic%20condition%20and%0Aapplying%20our%20proposed%20dual%20classifier-free%20guidance%2C%20one%20could%20obtain%20outputs%0Aconsistent%20with%20both%20the%20given%20subject%20and%20input%20text%20prompts.%20We%20validate%20the%0Aefficacy%20of%20our%20approach%20through%20both%20optimization-based%20and%20encoder-based%0Amethods.%20Additionally%2C%20we%20demonstrate%20its%20applicability%20in%20second-order%0Acustomization%20methods%2C%20where%20an%20encoder-based%20model%20is%20fine-tuned%20with%0ADreamBooth.%20Our%20approach%20is%20conceptually%20simple%20and%20requires%20only%20minimal%20code%0Amodifications%2C%20but%20leads%20to%20substantial%20quality%20improvements%2C%20as%20evidenced%20by%0Aour%20evaluations%20and%20user%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01356v1&entry.124074799=Read"},
{"title": "Adaptive Federated Learning with Auto-Tuned Clients", "author": "Junhyung Lyle Kim and Mohammad Taha Toghani and C\u00e9sar A. Uribe and Anastasios Kyrillidis", "abstract": "  Federated learning (FL) is a distributed machine learning framework where the\nglobal model of a central server is trained via multiple collaborative steps by\nparticipating clients without sharing their data. While being a flexible\nframework, where the distribution of local data, participation rate, and\ncomputing power of each client can greatly vary, such flexibility gives rise to\nmany new challenges, especially in the hyperparameter tuning on the client\nside. We propose $\\Delta$-SGD, a simple step size rule for SGD that enables\neach client to use its own step size by adapting to the local smoothness of the\nfunction each client is optimizing. We provide theoretical and empirical\nresults where the benefit of the client adaptivity is shown in various FL\nscenarios.\n", "link": "http://arxiv.org/abs/2306.11201v3", "date": "2024-05-02", "relevancy": 2.2758, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4632}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4518}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Federated%20Learning%20with%20Auto-Tuned%20Clients&body=Title%3A%20Adaptive%20Federated%20Learning%20with%20Auto-Tuned%20Clients%0AAuthor%3A%20Junhyung%20Lyle%20Kim%20and%20Mohammad%20Taha%20Toghani%20and%20C%C3%A9sar%20A.%20Uribe%20and%20Anastasios%20Kyrillidis%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20framework%20where%20the%0Aglobal%20model%20of%20a%20central%20server%20is%20trained%20via%20multiple%20collaborative%20steps%20by%0Aparticipating%20clients%20without%20sharing%20their%20data.%20While%20being%20a%20flexible%0Aframework%2C%20where%20the%20distribution%20of%20local%20data%2C%20participation%20rate%2C%20and%0Acomputing%20power%20of%20each%20client%20can%20greatly%20vary%2C%20such%20flexibility%20gives%20rise%20to%0Amany%20new%20challenges%2C%20especially%20in%20the%20hyperparameter%20tuning%20on%20the%20client%0Aside.%20We%20propose%20%24%5CDelta%24-SGD%2C%20a%20simple%20step%20size%20rule%20for%20SGD%20that%20enables%0Aeach%20client%20to%20use%20its%20own%20step%20size%20by%20adapting%20to%20the%20local%20smoothness%20of%20the%0Afunction%20each%20client%20is%20optimizing.%20We%20provide%20theoretical%20and%20empirical%0Aresults%20where%20the%20benefit%20of%20the%20client%20adaptivity%20is%20shown%20in%20various%20FL%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11201v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Federated%2520Learning%2520with%2520Auto-Tuned%2520Clients%26entry.906535625%3DJunhyung%2520Lyle%2520Kim%2520and%2520Mohammad%2520Taha%2520Toghani%2520and%2520C%25C3%25A9sar%2520A.%2520Uribe%2520and%2520Anastasios%2520Kyrillidis%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520distributed%2520machine%2520learning%2520framework%2520where%2520the%250Aglobal%2520model%2520of%2520a%2520central%2520server%2520is%2520trained%2520via%2520multiple%2520collaborative%2520steps%2520by%250Aparticipating%2520clients%2520without%2520sharing%2520their%2520data.%2520While%2520being%2520a%2520flexible%250Aframework%252C%2520where%2520the%2520distribution%2520of%2520local%2520data%252C%2520participation%2520rate%252C%2520and%250Acomputing%2520power%2520of%2520each%2520client%2520can%2520greatly%2520vary%252C%2520such%2520flexibility%2520gives%2520rise%2520to%250Amany%2520new%2520challenges%252C%2520especially%2520in%2520the%2520hyperparameter%2520tuning%2520on%2520the%2520client%250Aside.%2520We%2520propose%2520%2524%255CDelta%2524-SGD%252C%2520a%2520simple%2520step%2520size%2520rule%2520for%2520SGD%2520that%2520enables%250Aeach%2520client%2520to%2520use%2520its%2520own%2520step%2520size%2520by%2520adapting%2520to%2520the%2520local%2520smoothness%2520of%2520the%250Afunction%2520each%2520client%2520is%2520optimizing.%2520We%2520provide%2520theoretical%2520and%2520empirical%250Aresults%2520where%2520the%2520benefit%2520of%2520the%2520client%2520adaptivity%2520is%2520shown%2520in%2520various%2520FL%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.11201v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Federated%20Learning%20with%20Auto-Tuned%20Clients&entry.906535625=Junhyung%20Lyle%20Kim%20and%20Mohammad%20Taha%20Toghani%20and%20C%C3%A9sar%20A.%20Uribe%20and%20Anastasios%20Kyrillidis&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20framework%20where%20the%0Aglobal%20model%20of%20a%20central%20server%20is%20trained%20via%20multiple%20collaborative%20steps%20by%0Aparticipating%20clients%20without%20sharing%20their%20data.%20While%20being%20a%20flexible%0Aframework%2C%20where%20the%20distribution%20of%20local%20data%2C%20participation%20rate%2C%20and%0Acomputing%20power%20of%20each%20client%20can%20greatly%20vary%2C%20such%20flexibility%20gives%20rise%20to%0Amany%20new%20challenges%2C%20especially%20in%20the%20hyperparameter%20tuning%20on%20the%20client%0Aside.%20We%20propose%20%24%5CDelta%24-SGD%2C%20a%20simple%20step%20size%20rule%20for%20SGD%20that%20enables%0Aeach%20client%20to%20use%20its%20own%20step%20size%20by%20adapting%20to%20the%20local%20smoothness%20of%20the%0Afunction%20each%20client%20is%20optimizing.%20We%20provide%20theoretical%20and%20empirical%0Aresults%20where%20the%20benefit%20of%20the%20client%20adaptivity%20is%20shown%20in%20various%20FL%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11201v3&entry.124074799=Read"},
{"title": "Error-Driven Uncertainty Aware Training", "author": "Pedro Mendes and Paolo Romano and David Garlan", "abstract": "  Neural networks are often overconfident about their predictions, which\nundermines their reliability and trustworthiness. In this work, we present a\nnovel technique, named Error-Driven Uncertainty Aware Training (EUAT), which\naims to enhance the ability of neural models to estimate their uncertainty\ncorrectly, namely to be highly uncertain when they output inaccurate\npredictions and low uncertain when their output is accurate. The EUAT approach\noperates during the model's training phase by selectively employing two loss\nfunctions depending on whether the training examples are correctly or\nincorrectly predicted by the model. This allows for pursuing the twofold goal\nof i) minimizing model uncertainty for correctly predicted inputs and ii)\nmaximizing uncertainty for mispredicted inputs, while preserving the model's\nmisprediction rate. We evaluate EUAT using diverse neural models and datasets\nin the image recognition domains considering both non-adversarial and\nadversarial settings. The results show that EUAT outperforms existing\napproaches for uncertainty estimation (including other uncertainty-aware\ntraining techniques, calibration, ensembles, and DEUP) by providing uncertainty\nestimates that not only have higher quality when evaluated via statistical\nmetrics (e.g., correlation with residuals) but also when employed to build\nbinary classifiers that decide whether the model's output can be trusted or not\nand under distributional data shifts.\n", "link": "http://arxiv.org/abs/2405.01205v1", "date": "2024-05-02", "relevancy": 2.2689, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6376}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5758}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Error-Driven%20Uncertainty%20Aware%20Training&body=Title%3A%20Error-Driven%20Uncertainty%20Aware%20Training%0AAuthor%3A%20Pedro%20Mendes%20and%20Paolo%20Romano%20and%20David%20Garlan%0AAbstract%3A%20%20%20Neural%20networks%20are%20often%20overconfident%20about%20their%20predictions%2C%20which%0Aundermines%20their%20reliability%20and%20trustworthiness.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20technique%2C%20named%20Error-Driven%20Uncertainty%20Aware%20Training%20%28EUAT%29%2C%20which%0Aaims%20to%20enhance%20the%20ability%20of%20neural%20models%20to%20estimate%20their%20uncertainty%0Acorrectly%2C%20namely%20to%20be%20highly%20uncertain%20when%20they%20output%20inaccurate%0Apredictions%20and%20low%20uncertain%20when%20their%20output%20is%20accurate.%20The%20EUAT%20approach%0Aoperates%20during%20the%20model%27s%20training%20phase%20by%20selectively%20employing%20two%20loss%0Afunctions%20depending%20on%20whether%20the%20training%20examples%20are%20correctly%20or%0Aincorrectly%20predicted%20by%20the%20model.%20This%20allows%20for%20pursuing%20the%20twofold%20goal%0Aof%20i%29%20minimizing%20model%20uncertainty%20for%20correctly%20predicted%20inputs%20and%20ii%29%0Amaximizing%20uncertainty%20for%20mispredicted%20inputs%2C%20while%20preserving%20the%20model%27s%0Amisprediction%20rate.%20We%20evaluate%20EUAT%20using%20diverse%20neural%20models%20and%20datasets%0Ain%20the%20image%20recognition%20domains%20considering%20both%20non-adversarial%20and%0Aadversarial%20settings.%20The%20results%20show%20that%20EUAT%20outperforms%20existing%0Aapproaches%20for%20uncertainty%20estimation%20%28including%20other%20uncertainty-aware%0Atraining%20techniques%2C%20calibration%2C%20ensembles%2C%20and%20DEUP%29%20by%20providing%20uncertainty%0Aestimates%20that%20not%20only%20have%20higher%20quality%20when%20evaluated%20via%20statistical%0Ametrics%20%28e.g.%2C%20correlation%20with%20residuals%29%20but%20also%20when%20employed%20to%20build%0Abinary%20classifiers%20that%20decide%20whether%20the%20model%27s%20output%20can%20be%20trusted%20or%20not%0Aand%20under%20distributional%20data%20shifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DError-Driven%2520Uncertainty%2520Aware%2520Training%26entry.906535625%3DPedro%2520Mendes%2520and%2520Paolo%2520Romano%2520and%2520David%2520Garlan%26entry.1292438233%3D%2520%2520Neural%2520networks%2520are%2520often%2520overconfident%2520about%2520their%2520predictions%252C%2520which%250Aundermines%2520their%2520reliability%2520and%2520trustworthiness.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Anovel%2520technique%252C%2520named%2520Error-Driven%2520Uncertainty%2520Aware%2520Training%2520%2528EUAT%2529%252C%2520which%250Aaims%2520to%2520enhance%2520the%2520ability%2520of%2520neural%2520models%2520to%2520estimate%2520their%2520uncertainty%250Acorrectly%252C%2520namely%2520to%2520be%2520highly%2520uncertain%2520when%2520they%2520output%2520inaccurate%250Apredictions%2520and%2520low%2520uncertain%2520when%2520their%2520output%2520is%2520accurate.%2520The%2520EUAT%2520approach%250Aoperates%2520during%2520the%2520model%2527s%2520training%2520phase%2520by%2520selectively%2520employing%2520two%2520loss%250Afunctions%2520depending%2520on%2520whether%2520the%2520training%2520examples%2520are%2520correctly%2520or%250Aincorrectly%2520predicted%2520by%2520the%2520model.%2520This%2520allows%2520for%2520pursuing%2520the%2520twofold%2520goal%250Aof%2520i%2529%2520minimizing%2520model%2520uncertainty%2520for%2520correctly%2520predicted%2520inputs%2520and%2520ii%2529%250Amaximizing%2520uncertainty%2520for%2520mispredicted%2520inputs%252C%2520while%2520preserving%2520the%2520model%2527s%250Amisprediction%2520rate.%2520We%2520evaluate%2520EUAT%2520using%2520diverse%2520neural%2520models%2520and%2520datasets%250Ain%2520the%2520image%2520recognition%2520domains%2520considering%2520both%2520non-adversarial%2520and%250Aadversarial%2520settings.%2520The%2520results%2520show%2520that%2520EUAT%2520outperforms%2520existing%250Aapproaches%2520for%2520uncertainty%2520estimation%2520%2528including%2520other%2520uncertainty-aware%250Atraining%2520techniques%252C%2520calibration%252C%2520ensembles%252C%2520and%2520DEUP%2529%2520by%2520providing%2520uncertainty%250Aestimates%2520that%2520not%2520only%2520have%2520higher%2520quality%2520when%2520evaluated%2520via%2520statistical%250Ametrics%2520%2528e.g.%252C%2520correlation%2520with%2520residuals%2529%2520but%2520also%2520when%2520employed%2520to%2520build%250Abinary%2520classifiers%2520that%2520decide%2520whether%2520the%2520model%2527s%2520output%2520can%2520be%2520trusted%2520or%2520not%250Aand%2520under%2520distributional%2520data%2520shifts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Error-Driven%20Uncertainty%20Aware%20Training&entry.906535625=Pedro%20Mendes%20and%20Paolo%20Romano%20and%20David%20Garlan&entry.1292438233=%20%20Neural%20networks%20are%20often%20overconfident%20about%20their%20predictions%2C%20which%0Aundermines%20their%20reliability%20and%20trustworthiness.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20technique%2C%20named%20Error-Driven%20Uncertainty%20Aware%20Training%20%28EUAT%29%2C%20which%0Aaims%20to%20enhance%20the%20ability%20of%20neural%20models%20to%20estimate%20their%20uncertainty%0Acorrectly%2C%20namely%20to%20be%20highly%20uncertain%20when%20they%20output%20inaccurate%0Apredictions%20and%20low%20uncertain%20when%20their%20output%20is%20accurate.%20The%20EUAT%20approach%0Aoperates%20during%20the%20model%27s%20training%20phase%20by%20selectively%20employing%20two%20loss%0Afunctions%20depending%20on%20whether%20the%20training%20examples%20are%20correctly%20or%0Aincorrectly%20predicted%20by%20the%20model.%20This%20allows%20for%20pursuing%20the%20twofold%20goal%0Aof%20i%29%20minimizing%20model%20uncertainty%20for%20correctly%20predicted%20inputs%20and%20ii%29%0Amaximizing%20uncertainty%20for%20mispredicted%20inputs%2C%20while%20preserving%20the%20model%27s%0Amisprediction%20rate.%20We%20evaluate%20EUAT%20using%20diverse%20neural%20models%20and%20datasets%0Ain%20the%20image%20recognition%20domains%20considering%20both%20non-adversarial%20and%0Aadversarial%20settings.%20The%20results%20show%20that%20EUAT%20outperforms%20existing%0Aapproaches%20for%20uncertainty%20estimation%20%28including%20other%20uncertainty-aware%0Atraining%20techniques%2C%20calibration%2C%20ensembles%2C%20and%20DEUP%29%20by%20providing%20uncertainty%0Aestimates%20that%20not%20only%20have%20higher%20quality%20when%20evaluated%20via%20statistical%0Ametrics%20%28e.g.%2C%20correlation%20with%20residuals%29%20but%20also%20when%20employed%20to%20build%0Abinary%20classifiers%20that%20decide%20whether%20the%20model%27s%20output%20can%20be%20trusted%20or%20not%0Aand%20under%20distributional%20data%20shifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01205v1&entry.124074799=Read"},
{"title": "PAM-UNet: Shifting Attention on Region of Interest in Medical Images", "author": "Abhijit Das and Debesh Jha and Vandan Gorade and Koushik Biswas and Hongyi Pan and Zheyuan Zhang and Daniela P. Ladner and Yury Velichko and Amir Borhani and Ulas Bagci", "abstract": "  Computer-aided segmentation methods can assist medical personnel in improving\ndiagnostic outcomes. While recent advancements like UNet and its variants have\nshown promise, they face a critical challenge: balancing accuracy with\ncomputational efficiency. Shallow encoder architectures in UNets often struggle\nto capture crucial spatial features, leading in inaccurate and sparse\nsegmentation. To address this limitation, we propose a novel\n\\underline{P}rogressive \\underline{A}ttention based \\underline{M}obile\n\\underline{UNet} (\\underline{PAM-UNet}) architecture. The inverted residual\n(IR) blocks in PAM-UNet help maintain a lightweight framework, while layerwise\n\\textit{Progressive Luong Attention} ($\\mathcal{PLA}$) promotes precise\nsegmentation by directing attention toward regions of interest during\nsynthesis. Our approach prioritizes both accuracy and speed, achieving a\ncommendable balance with a mean IoU of 74.65 and a dice score of 82.87, while\nrequiring only 1.32 floating-point operations per second (FLOPS) on the Liver\nTumor Segmentation Benchmark (LiTS) 2017 dataset. These results highlight the\nimportance of developing efficient segmentation models to accelerate the\nadoption of AI in clinical practice.\n", "link": "http://arxiv.org/abs/2405.01503v1", "date": "2024-05-02", "relevancy": 2.2233, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.593}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5487}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAM-UNet%3A%20Shifting%20Attention%20on%20Region%20of%20Interest%20in%20Medical%20Images&body=Title%3A%20PAM-UNet%3A%20Shifting%20Attention%20on%20Region%20of%20Interest%20in%20Medical%20Images%0AAuthor%3A%20Abhijit%20Das%20and%20Debesh%20Jha%20and%20Vandan%20Gorade%20and%20Koushik%20Biswas%20and%20Hongyi%20Pan%20and%20Zheyuan%20Zhang%20and%20Daniela%20P.%20Ladner%20and%20Yury%20Velichko%20and%20Amir%20Borhani%20and%20Ulas%20Bagci%0AAbstract%3A%20%20%20Computer-aided%20segmentation%20methods%20can%20assist%20medical%20personnel%20in%20improving%0Adiagnostic%20outcomes.%20While%20recent%20advancements%20like%20UNet%20and%20its%20variants%20have%0Ashown%20promise%2C%20they%20face%20a%20critical%20challenge%3A%20balancing%20accuracy%20with%0Acomputational%20efficiency.%20Shallow%20encoder%20architectures%20in%20UNets%20often%20struggle%0Ato%20capture%20crucial%20spatial%20features%2C%20leading%20in%20inaccurate%20and%20sparse%0Asegmentation.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%0A%5Cunderline%7BP%7Drogressive%20%5Cunderline%7BA%7Dttention%20based%20%5Cunderline%7BM%7Dobile%0A%5Cunderline%7BUNet%7D%20%28%5Cunderline%7BPAM-UNet%7D%29%20architecture.%20The%20inverted%20residual%0A%28IR%29%20blocks%20in%20PAM-UNet%20help%20maintain%20a%20lightweight%20framework%2C%20while%20layerwise%0A%5Ctextit%7BProgressive%20Luong%20Attention%7D%20%28%24%5Cmathcal%7BPLA%7D%24%29%20promotes%20precise%0Asegmentation%20by%20directing%20attention%20toward%20regions%20of%20interest%20during%0Asynthesis.%20Our%20approach%20prioritizes%20both%20accuracy%20and%20speed%2C%20achieving%20a%0Acommendable%20balance%20with%20a%20mean%20IoU%20of%2074.65%20and%20a%20dice%20score%20of%2082.87%2C%20while%0Arequiring%20only%201.32%20floating-point%20operations%20per%20second%20%28FLOPS%29%20on%20the%20Liver%0ATumor%20Segmentation%20Benchmark%20%28LiTS%29%202017%20dataset.%20These%20results%20highlight%20the%0Aimportance%20of%20developing%20efficient%20segmentation%20models%20to%20accelerate%20the%0Aadoption%20of%20AI%20in%20clinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAM-UNet%253A%2520Shifting%2520Attention%2520on%2520Region%2520of%2520Interest%2520in%2520Medical%2520Images%26entry.906535625%3DAbhijit%2520Das%2520and%2520Debesh%2520Jha%2520and%2520Vandan%2520Gorade%2520and%2520Koushik%2520Biswas%2520and%2520Hongyi%2520Pan%2520and%2520Zheyuan%2520Zhang%2520and%2520Daniela%2520P.%2520Ladner%2520and%2520Yury%2520Velichko%2520and%2520Amir%2520Borhani%2520and%2520Ulas%2520Bagci%26entry.1292438233%3D%2520%2520Computer-aided%2520segmentation%2520methods%2520can%2520assist%2520medical%2520personnel%2520in%2520improving%250Adiagnostic%2520outcomes.%2520While%2520recent%2520advancements%2520like%2520UNet%2520and%2520its%2520variants%2520have%250Ashown%2520promise%252C%2520they%2520face%2520a%2520critical%2520challenge%253A%2520balancing%2520accuracy%2520with%250Acomputational%2520efficiency.%2520Shallow%2520encoder%2520architectures%2520in%2520UNets%2520often%2520struggle%250Ato%2520capture%2520crucial%2520spatial%2520features%252C%2520leading%2520in%2520inaccurate%2520and%2520sparse%250Asegmentation.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%250A%255Cunderline%257BP%257Drogressive%2520%255Cunderline%257BA%257Dttention%2520based%2520%255Cunderline%257BM%257Dobile%250A%255Cunderline%257BUNet%257D%2520%2528%255Cunderline%257BPAM-UNet%257D%2529%2520architecture.%2520The%2520inverted%2520residual%250A%2528IR%2529%2520blocks%2520in%2520PAM-UNet%2520help%2520maintain%2520a%2520lightweight%2520framework%252C%2520while%2520layerwise%250A%255Ctextit%257BProgressive%2520Luong%2520Attention%257D%2520%2528%2524%255Cmathcal%257BPLA%257D%2524%2529%2520promotes%2520precise%250Asegmentation%2520by%2520directing%2520attention%2520toward%2520regions%2520of%2520interest%2520during%250Asynthesis.%2520Our%2520approach%2520prioritizes%2520both%2520accuracy%2520and%2520speed%252C%2520achieving%2520a%250Acommendable%2520balance%2520with%2520a%2520mean%2520IoU%2520of%252074.65%2520and%2520a%2520dice%2520score%2520of%252082.87%252C%2520while%250Arequiring%2520only%25201.32%2520floating-point%2520operations%2520per%2520second%2520%2528FLOPS%2529%2520on%2520the%2520Liver%250ATumor%2520Segmentation%2520Benchmark%2520%2528LiTS%2529%25202017%2520dataset.%2520These%2520results%2520highlight%2520the%250Aimportance%2520of%2520developing%2520efficient%2520segmentation%2520models%2520to%2520accelerate%2520the%250Aadoption%2520of%2520AI%2520in%2520clinical%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAM-UNet%3A%20Shifting%20Attention%20on%20Region%20of%20Interest%20in%20Medical%20Images&entry.906535625=Abhijit%20Das%20and%20Debesh%20Jha%20and%20Vandan%20Gorade%20and%20Koushik%20Biswas%20and%20Hongyi%20Pan%20and%20Zheyuan%20Zhang%20and%20Daniela%20P.%20Ladner%20and%20Yury%20Velichko%20and%20Amir%20Borhani%20and%20Ulas%20Bagci&entry.1292438233=%20%20Computer-aided%20segmentation%20methods%20can%20assist%20medical%20personnel%20in%20improving%0Adiagnostic%20outcomes.%20While%20recent%20advancements%20like%20UNet%20and%20its%20variants%20have%0Ashown%20promise%2C%20they%20face%20a%20critical%20challenge%3A%20balancing%20accuracy%20with%0Acomputational%20efficiency.%20Shallow%20encoder%20architectures%20in%20UNets%20often%20struggle%0Ato%20capture%20crucial%20spatial%20features%2C%20leading%20in%20inaccurate%20and%20sparse%0Asegmentation.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%0A%5Cunderline%7BP%7Drogressive%20%5Cunderline%7BA%7Dttention%20based%20%5Cunderline%7BM%7Dobile%0A%5Cunderline%7BUNet%7D%20%28%5Cunderline%7BPAM-UNet%7D%29%20architecture.%20The%20inverted%20residual%0A%28IR%29%20blocks%20in%20PAM-UNet%20help%20maintain%20a%20lightweight%20framework%2C%20while%20layerwise%0A%5Ctextit%7BProgressive%20Luong%20Attention%7D%20%28%24%5Cmathcal%7BPLA%7D%24%29%20promotes%20precise%0Asegmentation%20by%20directing%20attention%20toward%20regions%20of%20interest%20during%0Asynthesis.%20Our%20approach%20prioritizes%20both%20accuracy%20and%20speed%2C%20achieving%20a%0Acommendable%20balance%20with%20a%20mean%20IoU%20of%2074.65%20and%20a%20dice%20score%20of%2082.87%2C%20while%0Arequiring%20only%201.32%20floating-point%20operations%20per%20second%20%28FLOPS%29%20on%20the%20Liver%0ATumor%20Segmentation%20Benchmark%20%28LiTS%29%202017%20dataset.%20These%20results%20highlight%20the%0Aimportance%20of%20developing%20efficient%20segmentation%20models%20to%20accelerate%20the%0Aadoption%20of%20AI%20in%20clinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01503v1&entry.124074799=Read"},
{"title": "Sharp Bounds for Sequential Federated Learning on Heterogeneous Data", "author": "Yipeng Li and Xinchen Lyu", "abstract": "  There are two paradigms in Federated Learning (FL): parallel FL (PFL), where\nmodels are trained in a parallel manner across clients; and sequential FL\n(SFL), where models are trained in a sequential manner across clients. In\ncontrast to that of PFL, the convergence theory of SFL on heterogeneous data is\nstill lacking. To resolve the theoretical dilemma of SFL, we establish sharp\nconvergence guarantees for SFL on heterogeneous data with both upper and lower\nbounds. Specifically, we derive the upper bounds for strongly convex, general\nconvex and non-convex objective functions, and construct the matching lower\nbounds for the strongly convex and general convex objective functions. Then, we\ncompare the upper bounds of SFL with those of PFL, showing that SFL outperforms\nPFL (at least, when the level of heterogeneity is relatively high).\nExperimental results on quadratic functions and real data sets validate the\ncounterintuitive comparison result.\n", "link": "http://arxiv.org/abs/2405.01142v1", "date": "2024-05-02", "relevancy": 2.213, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4483}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4427}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharp%20Bounds%20for%20Sequential%20Federated%20Learning%20on%20Heterogeneous%20Data&body=Title%3A%20Sharp%20Bounds%20for%20Sequential%20Federated%20Learning%20on%20Heterogeneous%20Data%0AAuthor%3A%20Yipeng%20Li%20and%20Xinchen%20Lyu%0AAbstract%3A%20%20%20There%20are%20two%20paradigms%20in%20Federated%20Learning%20%28FL%29%3A%20parallel%20FL%20%28PFL%29%2C%20where%0Amodels%20are%20trained%20in%20a%20parallel%20manner%20across%20clients%3B%20and%20sequential%20FL%0A%28SFL%29%2C%20where%20models%20are%20trained%20in%20a%20sequential%20manner%20across%20clients.%20In%0Acontrast%20to%20that%20of%20PFL%2C%20the%20convergence%20theory%20of%20SFL%20on%20heterogeneous%20data%20is%0Astill%20lacking.%20To%20resolve%20the%20theoretical%20dilemma%20of%20SFL%2C%20we%20establish%20sharp%0Aconvergence%20guarantees%20for%20SFL%20on%20heterogeneous%20data%20with%20both%20upper%20and%20lower%0Abounds.%20Specifically%2C%20we%20derive%20the%20upper%20bounds%20for%20strongly%20convex%2C%20general%0Aconvex%20and%20non-convex%20objective%20functions%2C%20and%20construct%20the%20matching%20lower%0Abounds%20for%20the%20strongly%20convex%20and%20general%20convex%20objective%20functions.%20Then%2C%20we%0Acompare%20the%20upper%20bounds%20of%20SFL%20with%20those%20of%20PFL%2C%20showing%20that%20SFL%20outperforms%0APFL%20%28at%20least%2C%20when%20the%20level%20of%20heterogeneity%20is%20relatively%20high%29.%0AExperimental%20results%20on%20quadratic%20functions%20and%20real%20data%20sets%20validate%20the%0Acounterintuitive%20comparison%20result.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharp%2520Bounds%2520for%2520Sequential%2520Federated%2520Learning%2520on%2520Heterogeneous%2520Data%26entry.906535625%3DYipeng%2520Li%2520and%2520Xinchen%2520Lyu%26entry.1292438233%3D%2520%2520There%2520are%2520two%2520paradigms%2520in%2520Federated%2520Learning%2520%2528FL%2529%253A%2520parallel%2520FL%2520%2528PFL%2529%252C%2520where%250Amodels%2520are%2520trained%2520in%2520a%2520parallel%2520manner%2520across%2520clients%253B%2520and%2520sequential%2520FL%250A%2528SFL%2529%252C%2520where%2520models%2520are%2520trained%2520in%2520a%2520sequential%2520manner%2520across%2520clients.%2520In%250Acontrast%2520to%2520that%2520of%2520PFL%252C%2520the%2520convergence%2520theory%2520of%2520SFL%2520on%2520heterogeneous%2520data%2520is%250Astill%2520lacking.%2520To%2520resolve%2520the%2520theoretical%2520dilemma%2520of%2520SFL%252C%2520we%2520establish%2520sharp%250Aconvergence%2520guarantees%2520for%2520SFL%2520on%2520heterogeneous%2520data%2520with%2520both%2520upper%2520and%2520lower%250Abounds.%2520Specifically%252C%2520we%2520derive%2520the%2520upper%2520bounds%2520for%2520strongly%2520convex%252C%2520general%250Aconvex%2520and%2520non-convex%2520objective%2520functions%252C%2520and%2520construct%2520the%2520matching%2520lower%250Abounds%2520for%2520the%2520strongly%2520convex%2520and%2520general%2520convex%2520objective%2520functions.%2520Then%252C%2520we%250Acompare%2520the%2520upper%2520bounds%2520of%2520SFL%2520with%2520those%2520of%2520PFL%252C%2520showing%2520that%2520SFL%2520outperforms%250APFL%2520%2528at%2520least%252C%2520when%2520the%2520level%2520of%2520heterogeneity%2520is%2520relatively%2520high%2529.%250AExperimental%2520results%2520on%2520quadratic%2520functions%2520and%2520real%2520data%2520sets%2520validate%2520the%250Acounterintuitive%2520comparison%2520result.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharp%20Bounds%20for%20Sequential%20Federated%20Learning%20on%20Heterogeneous%20Data&entry.906535625=Yipeng%20Li%20and%20Xinchen%20Lyu&entry.1292438233=%20%20There%20are%20two%20paradigms%20in%20Federated%20Learning%20%28FL%29%3A%20parallel%20FL%20%28PFL%29%2C%20where%0Amodels%20are%20trained%20in%20a%20parallel%20manner%20across%20clients%3B%20and%20sequential%20FL%0A%28SFL%29%2C%20where%20models%20are%20trained%20in%20a%20sequential%20manner%20across%20clients.%20In%0Acontrast%20to%20that%20of%20PFL%2C%20the%20convergence%20theory%20of%20SFL%20on%20heterogeneous%20data%20is%0Astill%20lacking.%20To%20resolve%20the%20theoretical%20dilemma%20of%20SFL%2C%20we%20establish%20sharp%0Aconvergence%20guarantees%20for%20SFL%20on%20heterogeneous%20data%20with%20both%20upper%20and%20lower%0Abounds.%20Specifically%2C%20we%20derive%20the%20upper%20bounds%20for%20strongly%20convex%2C%20general%0Aconvex%20and%20non-convex%20objective%20functions%2C%20and%20construct%20the%20matching%20lower%0Abounds%20for%20the%20strongly%20convex%20and%20general%20convex%20objective%20functions.%20Then%2C%20we%0Acompare%20the%20upper%20bounds%20of%20SFL%20with%20those%20of%20PFL%2C%20showing%20that%20SFL%20outperforms%0APFL%20%28at%20least%2C%20when%20the%20level%20of%20heterogeneity%20is%20relatively%20high%29.%0AExperimental%20results%20on%20quadratic%20functions%20and%20real%20data%20sets%20validate%20the%0Acounterintuitive%20comparison%20result.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01142v1&entry.124074799=Read"},
{"title": "Uncertainty-aware self-training with expectation maximization basis\n  transformation", "author": "Zijia Wang and Wenbin Yang and Zhisong Liu and Zhen Jia", "abstract": "  Self-training is a powerful approach to deep learning. The key process is to\nfind a pseudo-label for modeling. However, previous self-training algorithms\nsuffer from the over-confidence issue brought by the hard labels, even some\nconfidence-related regularizers cannot comprehensively catch the uncertainty.\nTherefore, we propose a new self-training framework to combine uncertainty\ninformation of both model and dataset. Specifically, we propose to use\nExpectation-Maximization (EM) to smooth the labels and comprehensively estimate\nthe uncertainty information. We further design a basis extraction network to\nestimate the initial basis from the dataset. The obtained basis with\nuncertainty can be filtered based on uncertainty information. It can then be\ntransformed into the real hard label to iteratively update the model and basis\nin the retraining process. Experiments on image classification and semantic\nsegmentation show the advantages of our methods among confidence-aware\nself-training algorithms with 1-3 percentage improvement on different datasets.\n", "link": "http://arxiv.org/abs/2405.01175v1", "date": "2024-05-02", "relevancy": 2.1584, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6161}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5252}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-aware%20self-training%20with%20expectation%20maximization%20basis%0A%20%20transformation&body=Title%3A%20Uncertainty-aware%20self-training%20with%20expectation%20maximization%20basis%0A%20%20transformation%0AAuthor%3A%20Zijia%20Wang%20and%20Wenbin%20Yang%20and%20Zhisong%20Liu%20and%20Zhen%20Jia%0AAbstract%3A%20%20%20Self-training%20is%20a%20powerful%20approach%20to%20deep%20learning.%20The%20key%20process%20is%20to%0Afind%20a%20pseudo-label%20for%20modeling.%20However%2C%20previous%20self-training%20algorithms%0Asuffer%20from%20the%20over-confidence%20issue%20brought%20by%20the%20hard%20labels%2C%20even%20some%0Aconfidence-related%20regularizers%20cannot%20comprehensively%20catch%20the%20uncertainty.%0ATherefore%2C%20we%20propose%20a%20new%20self-training%20framework%20to%20combine%20uncertainty%0Ainformation%20of%20both%20model%20and%20dataset.%20Specifically%2C%20we%20propose%20to%20use%0AExpectation-Maximization%20%28EM%29%20to%20smooth%20the%20labels%20and%20comprehensively%20estimate%0Athe%20uncertainty%20information.%20We%20further%20design%20a%20basis%20extraction%20network%20to%0Aestimate%20the%20initial%20basis%20from%20the%20dataset.%20The%20obtained%20basis%20with%0Auncertainty%20can%20be%20filtered%20based%20on%20uncertainty%20information.%20It%20can%20then%20be%0Atransformed%20into%20the%20real%20hard%20label%20to%20iteratively%20update%20the%20model%20and%20basis%0Ain%20the%20retraining%20process.%20Experiments%20on%20image%20classification%20and%20semantic%0Asegmentation%20show%20the%20advantages%20of%20our%20methods%20among%20confidence-aware%0Aself-training%20algorithms%20with%201-3%20percentage%20improvement%20on%20different%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-aware%2520self-training%2520with%2520expectation%2520maximization%2520basis%250A%2520%2520transformation%26entry.906535625%3DZijia%2520Wang%2520and%2520Wenbin%2520Yang%2520and%2520Zhisong%2520Liu%2520and%2520Zhen%2520Jia%26entry.1292438233%3D%2520%2520Self-training%2520is%2520a%2520powerful%2520approach%2520to%2520deep%2520learning.%2520The%2520key%2520process%2520is%2520to%250Afind%2520a%2520pseudo-label%2520for%2520modeling.%2520However%252C%2520previous%2520self-training%2520algorithms%250Asuffer%2520from%2520the%2520over-confidence%2520issue%2520brought%2520by%2520the%2520hard%2520labels%252C%2520even%2520some%250Aconfidence-related%2520regularizers%2520cannot%2520comprehensively%2520catch%2520the%2520uncertainty.%250ATherefore%252C%2520we%2520propose%2520a%2520new%2520self-training%2520framework%2520to%2520combine%2520uncertainty%250Ainformation%2520of%2520both%2520model%2520and%2520dataset.%2520Specifically%252C%2520we%2520propose%2520to%2520use%250AExpectation-Maximization%2520%2528EM%2529%2520to%2520smooth%2520the%2520labels%2520and%2520comprehensively%2520estimate%250Athe%2520uncertainty%2520information.%2520We%2520further%2520design%2520a%2520basis%2520extraction%2520network%2520to%250Aestimate%2520the%2520initial%2520basis%2520from%2520the%2520dataset.%2520The%2520obtained%2520basis%2520with%250Auncertainty%2520can%2520be%2520filtered%2520based%2520on%2520uncertainty%2520information.%2520It%2520can%2520then%2520be%250Atransformed%2520into%2520the%2520real%2520hard%2520label%2520to%2520iteratively%2520update%2520the%2520model%2520and%2520basis%250Ain%2520the%2520retraining%2520process.%2520Experiments%2520on%2520image%2520classification%2520and%2520semantic%250Asegmentation%2520show%2520the%2520advantages%2520of%2520our%2520methods%2520among%2520confidence-aware%250Aself-training%2520algorithms%2520with%25201-3%2520percentage%2520improvement%2520on%2520different%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-aware%20self-training%20with%20expectation%20maximization%20basis%0A%20%20transformation&entry.906535625=Zijia%20Wang%20and%20Wenbin%20Yang%20and%20Zhisong%20Liu%20and%20Zhen%20Jia&entry.1292438233=%20%20Self-training%20is%20a%20powerful%20approach%20to%20deep%20learning.%20The%20key%20process%20is%20to%0Afind%20a%20pseudo-label%20for%20modeling.%20However%2C%20previous%20self-training%20algorithms%0Asuffer%20from%20the%20over-confidence%20issue%20brought%20by%20the%20hard%20labels%2C%20even%20some%0Aconfidence-related%20regularizers%20cannot%20comprehensively%20catch%20the%20uncertainty.%0ATherefore%2C%20we%20propose%20a%20new%20self-training%20framework%20to%20combine%20uncertainty%0Ainformation%20of%20both%20model%20and%20dataset.%20Specifically%2C%20we%20propose%20to%20use%0AExpectation-Maximization%20%28EM%29%20to%20smooth%20the%20labels%20and%20comprehensively%20estimate%0Athe%20uncertainty%20information.%20We%20further%20design%20a%20basis%20extraction%20network%20to%0Aestimate%20the%20initial%20basis%20from%20the%20dataset.%20The%20obtained%20basis%20with%0Auncertainty%20can%20be%20filtered%20based%20on%20uncertainty%20information.%20It%20can%20then%20be%0Atransformed%20into%20the%20real%20hard%20label%20to%20iteratively%20update%20the%20model%20and%20basis%0Ain%20the%20retraining%20process.%20Experiments%20on%20image%20classification%20and%20semantic%0Asegmentation%20show%20the%20advantages%20of%20our%20methods%20among%20confidence-aware%0Aself-training%20algorithms%20with%201-3%20percentage%20improvement%20on%20different%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01175v1&entry.124074799=Read"},
{"title": "Sparse multi-view hand-object reconstruction for unseen environments", "author": "Yik Lung Pang and Changjae Oh and Andrea Cavallaro", "abstract": "  Recent works in hand-object reconstruction mainly focus on the single-view\nand dense multi-view settings. On the one hand, single-view methods can\nleverage learned shape priors to generalise to unseen objects but are prone to\ninaccuracies due to occlusions. On the other hand, dense multi-view methods are\nvery accurate but cannot easily adapt to unseen objects without further data\ncollection. In contrast, sparse multi-view methods can take advantage of the\nadditional views to tackle occlusion, while keeping the computational cost low\ncompared to dense multi-view methods. In this paper, we consider the problem of\nhand-object reconstruction with unseen objects in the sparse multi-view\nsetting. Given multiple RGB images of the hand and object captured at the same\ntime, our model SVHO combines the predictions from each view into a unified\nreconstruction without optimisation across views. We train our model on a\nsynthetic hand-object dataset and evaluate directly on a real world recorded\nhand-object dataset with unseen objects. We show that while reconstruction of\nunseen hands and objects from RGB is challenging, additional views can help\nimprove the reconstruction quality.\n", "link": "http://arxiv.org/abs/2405.01353v1", "date": "2024-05-02", "relevancy": 2.1453, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5413}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.539}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20multi-view%20hand-object%20reconstruction%20for%20unseen%20environments&body=Title%3A%20Sparse%20multi-view%20hand-object%20reconstruction%20for%20unseen%20environments%0AAuthor%3A%20Yik%20Lung%20Pang%20and%20Changjae%20Oh%20and%20Andrea%20Cavallaro%0AAbstract%3A%20%20%20Recent%20works%20in%20hand-object%20reconstruction%20mainly%20focus%20on%20the%20single-view%0Aand%20dense%20multi-view%20settings.%20On%20the%20one%20hand%2C%20single-view%20methods%20can%0Aleverage%20learned%20shape%20priors%20to%20generalise%20to%20unseen%20objects%20but%20are%20prone%20to%0Ainaccuracies%20due%20to%20occlusions.%20On%20the%20other%20hand%2C%20dense%20multi-view%20methods%20are%0Avery%20accurate%20but%20cannot%20easily%20adapt%20to%20unseen%20objects%20without%20further%20data%0Acollection.%20In%20contrast%2C%20sparse%20multi-view%20methods%20can%20take%20advantage%20of%20the%0Aadditional%20views%20to%20tackle%20occlusion%2C%20while%20keeping%20the%20computational%20cost%20low%0Acompared%20to%20dense%20multi-view%20methods.%20In%20this%20paper%2C%20we%20consider%20the%20problem%20of%0Ahand-object%20reconstruction%20with%20unseen%20objects%20in%20the%20sparse%20multi-view%0Asetting.%20Given%20multiple%20RGB%20images%20of%20the%20hand%20and%20object%20captured%20at%20the%20same%0Atime%2C%20our%20model%20SVHO%20combines%20the%20predictions%20from%20each%20view%20into%20a%20unified%0Areconstruction%20without%20optimisation%20across%20views.%20We%20train%20our%20model%20on%20a%0Asynthetic%20hand-object%20dataset%20and%20evaluate%20directly%20on%20a%20real%20world%20recorded%0Ahand-object%20dataset%20with%20unseen%20objects.%20We%20show%20that%20while%20reconstruction%20of%0Aunseen%20hands%20and%20objects%20from%20RGB%20is%20challenging%2C%20additional%20views%20can%20help%0Aimprove%20the%20reconstruction%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520multi-view%2520hand-object%2520reconstruction%2520for%2520unseen%2520environments%26entry.906535625%3DYik%2520Lung%2520Pang%2520and%2520Changjae%2520Oh%2520and%2520Andrea%2520Cavallaro%26entry.1292438233%3D%2520%2520Recent%2520works%2520in%2520hand-object%2520reconstruction%2520mainly%2520focus%2520on%2520the%2520single-view%250Aand%2520dense%2520multi-view%2520settings.%2520On%2520the%2520one%2520hand%252C%2520single-view%2520methods%2520can%250Aleverage%2520learned%2520shape%2520priors%2520to%2520generalise%2520to%2520unseen%2520objects%2520but%2520are%2520prone%2520to%250Ainaccuracies%2520due%2520to%2520occlusions.%2520On%2520the%2520other%2520hand%252C%2520dense%2520multi-view%2520methods%2520are%250Avery%2520accurate%2520but%2520cannot%2520easily%2520adapt%2520to%2520unseen%2520objects%2520without%2520further%2520data%250Acollection.%2520In%2520contrast%252C%2520sparse%2520multi-view%2520methods%2520can%2520take%2520advantage%2520of%2520the%250Aadditional%2520views%2520to%2520tackle%2520occlusion%252C%2520while%2520keeping%2520the%2520computational%2520cost%2520low%250Acompared%2520to%2520dense%2520multi-view%2520methods.%2520In%2520this%2520paper%252C%2520we%2520consider%2520the%2520problem%2520of%250Ahand-object%2520reconstruction%2520with%2520unseen%2520objects%2520in%2520the%2520sparse%2520multi-view%250Asetting.%2520Given%2520multiple%2520RGB%2520images%2520of%2520the%2520hand%2520and%2520object%2520captured%2520at%2520the%2520same%250Atime%252C%2520our%2520model%2520SVHO%2520combines%2520the%2520predictions%2520from%2520each%2520view%2520into%2520a%2520unified%250Areconstruction%2520without%2520optimisation%2520across%2520views.%2520We%2520train%2520our%2520model%2520on%2520a%250Asynthetic%2520hand-object%2520dataset%2520and%2520evaluate%2520directly%2520on%2520a%2520real%2520world%2520recorded%250Ahand-object%2520dataset%2520with%2520unseen%2520objects.%2520We%2520show%2520that%2520while%2520reconstruction%2520of%250Aunseen%2520hands%2520and%2520objects%2520from%2520RGB%2520is%2520challenging%252C%2520additional%2520views%2520can%2520help%250Aimprove%2520the%2520reconstruction%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20multi-view%20hand-object%20reconstruction%20for%20unseen%20environments&entry.906535625=Yik%20Lung%20Pang%20and%20Changjae%20Oh%20and%20Andrea%20Cavallaro&entry.1292438233=%20%20Recent%20works%20in%20hand-object%20reconstruction%20mainly%20focus%20on%20the%20single-view%0Aand%20dense%20multi-view%20settings.%20On%20the%20one%20hand%2C%20single-view%20methods%20can%0Aleverage%20learned%20shape%20priors%20to%20generalise%20to%20unseen%20objects%20but%20are%20prone%20to%0Ainaccuracies%20due%20to%20occlusions.%20On%20the%20other%20hand%2C%20dense%20multi-view%20methods%20are%0Avery%20accurate%20but%20cannot%20easily%20adapt%20to%20unseen%20objects%20without%20further%20data%0Acollection.%20In%20contrast%2C%20sparse%20multi-view%20methods%20can%20take%20advantage%20of%20the%0Aadditional%20views%20to%20tackle%20occlusion%2C%20while%20keeping%20the%20computational%20cost%20low%0Acompared%20to%20dense%20multi-view%20methods.%20In%20this%20paper%2C%20we%20consider%20the%20problem%20of%0Ahand-object%20reconstruction%20with%20unseen%20objects%20in%20the%20sparse%20multi-view%0Asetting.%20Given%20multiple%20RGB%20images%20of%20the%20hand%20and%20object%20captured%20at%20the%20same%0Atime%2C%20our%20model%20SVHO%20combines%20the%20predictions%20from%20each%20view%20into%20a%20unified%0Areconstruction%20without%20optimisation%20across%20views.%20We%20train%20our%20model%20on%20a%0Asynthetic%20hand-object%20dataset%20and%20evaluate%20directly%20on%20a%20real%20world%20recorded%0Ahand-object%20dataset%20with%20unseen%20objects.%20We%20show%20that%20while%20reconstruction%20of%0Aunseen%20hands%20and%20objects%20from%20RGB%20is%20challenging%2C%20additional%20views%20can%20help%0Aimprove%20the%20reconstruction%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01353v1&entry.124074799=Read"},
{"title": "Bridging Dimensions: Confident Reachability for High-Dimensional\n  Controllers", "author": "Yuang Geng and Jake Brandon Baldauf and Souradeep Dutta and Chao Huang and Ivan Ruchkin", "abstract": "  Autonomous systems are increasingly implemented using end-to-end\nlearning-based controllers. Such controllers make decisions that are executed\non the real system, with images as one of the primary sensing modalities. Deep\nneural networks form a fundamental building block of such controllers.\nUnfortunately, the existing neural-network verification tools do not scale to\ninputs with thousands of dimensions -- especially when the individual inputs\n(such as pixels) are devoid of clear physical meaning. This paper takes a step\ntowards connecting exhaustive closed-loop verification with high-dimensional\ncontrollers. Our key insight is that the behavior of a high-dimensional\ncontroller can be approximated with several low-dimensional controllers. To\nbalance the approximation accuracy and verifiability of our low-dimensional\ncontrollers, we leverage the latest verification-aware knowledge distillation.\nThen, we inflate low-dimensional reachability results with statistical\napproximation errors, yielding a high-confidence reachability guarantee for the\nhigh-dimensional controller. We investigate two inflation techniques -- based\non trajectories and control actions -- both of which show convincing\nperformance in three OpenAI gym benchmarks.\n", "link": "http://arxiv.org/abs/2311.04843v4", "date": "2024-05-02", "relevancy": 2.1117, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5755}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5204}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Dimensions%3A%20Confident%20Reachability%20for%20High-Dimensional%0A%20%20Controllers&body=Title%3A%20Bridging%20Dimensions%3A%20Confident%20Reachability%20for%20High-Dimensional%0A%20%20Controllers%0AAuthor%3A%20Yuang%20Geng%20and%20Jake%20Brandon%20Baldauf%20and%20Souradeep%20Dutta%20and%20Chao%20Huang%20and%20Ivan%20Ruchkin%0AAbstract%3A%20%20%20Autonomous%20systems%20are%20increasingly%20implemented%20using%20end-to-end%0Alearning-based%20controllers.%20Such%20controllers%20make%20decisions%20that%20are%20executed%0Aon%20the%20real%20system%2C%20with%20images%20as%20one%20of%20the%20primary%20sensing%20modalities.%20Deep%0Aneural%20networks%20form%20a%20fundamental%20building%20block%20of%20such%20controllers.%0AUnfortunately%2C%20the%20existing%20neural-network%20verification%20tools%20do%20not%20scale%20to%0Ainputs%20with%20thousands%20of%20dimensions%20--%20especially%20when%20the%20individual%20inputs%0A%28such%20as%20pixels%29%20are%20devoid%20of%20clear%20physical%20meaning.%20This%20paper%20takes%20a%20step%0Atowards%20connecting%20exhaustive%20closed-loop%20verification%20with%20high-dimensional%0Acontrollers.%20Our%20key%20insight%20is%20that%20the%20behavior%20of%20a%20high-dimensional%0Acontroller%20can%20be%20approximated%20with%20several%20low-dimensional%20controllers.%20To%0Abalance%20the%20approximation%20accuracy%20and%20verifiability%20of%20our%20low-dimensional%0Acontrollers%2C%20we%20leverage%20the%20latest%20verification-aware%20knowledge%20distillation.%0AThen%2C%20we%20inflate%20low-dimensional%20reachability%20results%20with%20statistical%0Aapproximation%20errors%2C%20yielding%20a%20high-confidence%20reachability%20guarantee%20for%20the%0Ahigh-dimensional%20controller.%20We%20investigate%20two%20inflation%20techniques%20--%20based%0Aon%20trajectories%20and%20control%20actions%20--%20both%20of%20which%20show%20convincing%0Aperformance%20in%20three%20OpenAI%20gym%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04843v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Dimensions%253A%2520Confident%2520Reachability%2520for%2520High-Dimensional%250A%2520%2520Controllers%26entry.906535625%3DYuang%2520Geng%2520and%2520Jake%2520Brandon%2520Baldauf%2520and%2520Souradeep%2520Dutta%2520and%2520Chao%2520Huang%2520and%2520Ivan%2520Ruchkin%26entry.1292438233%3D%2520%2520Autonomous%2520systems%2520are%2520increasingly%2520implemented%2520using%2520end-to-end%250Alearning-based%2520controllers.%2520Such%2520controllers%2520make%2520decisions%2520that%2520are%2520executed%250Aon%2520the%2520real%2520system%252C%2520with%2520images%2520as%2520one%2520of%2520the%2520primary%2520sensing%2520modalities.%2520Deep%250Aneural%2520networks%2520form%2520a%2520fundamental%2520building%2520block%2520of%2520such%2520controllers.%250AUnfortunately%252C%2520the%2520existing%2520neural-network%2520verification%2520tools%2520do%2520not%2520scale%2520to%250Ainputs%2520with%2520thousands%2520of%2520dimensions%2520--%2520especially%2520when%2520the%2520individual%2520inputs%250A%2528such%2520as%2520pixels%2529%2520are%2520devoid%2520of%2520clear%2520physical%2520meaning.%2520This%2520paper%2520takes%2520a%2520step%250Atowards%2520connecting%2520exhaustive%2520closed-loop%2520verification%2520with%2520high-dimensional%250Acontrollers.%2520Our%2520key%2520insight%2520is%2520that%2520the%2520behavior%2520of%2520a%2520high-dimensional%250Acontroller%2520can%2520be%2520approximated%2520with%2520several%2520low-dimensional%2520controllers.%2520To%250Abalance%2520the%2520approximation%2520accuracy%2520and%2520verifiability%2520of%2520our%2520low-dimensional%250Acontrollers%252C%2520we%2520leverage%2520the%2520latest%2520verification-aware%2520knowledge%2520distillation.%250AThen%252C%2520we%2520inflate%2520low-dimensional%2520reachability%2520results%2520with%2520statistical%250Aapproximation%2520errors%252C%2520yielding%2520a%2520high-confidence%2520reachability%2520guarantee%2520for%2520the%250Ahigh-dimensional%2520controller.%2520We%2520investigate%2520two%2520inflation%2520techniques%2520--%2520based%250Aon%2520trajectories%2520and%2520control%2520actions%2520--%2520both%2520of%2520which%2520show%2520convincing%250Aperformance%2520in%2520three%2520OpenAI%2520gym%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04843v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Dimensions%3A%20Confident%20Reachability%20for%20High-Dimensional%0A%20%20Controllers&entry.906535625=Yuang%20Geng%20and%20Jake%20Brandon%20Baldauf%20and%20Souradeep%20Dutta%20and%20Chao%20Huang%20and%20Ivan%20Ruchkin&entry.1292438233=%20%20Autonomous%20systems%20are%20increasingly%20implemented%20using%20end-to-end%0Alearning-based%20controllers.%20Such%20controllers%20make%20decisions%20that%20are%20executed%0Aon%20the%20real%20system%2C%20with%20images%20as%20one%20of%20the%20primary%20sensing%20modalities.%20Deep%0Aneural%20networks%20form%20a%20fundamental%20building%20block%20of%20such%20controllers.%0AUnfortunately%2C%20the%20existing%20neural-network%20verification%20tools%20do%20not%20scale%20to%0Ainputs%20with%20thousands%20of%20dimensions%20--%20especially%20when%20the%20individual%20inputs%0A%28such%20as%20pixels%29%20are%20devoid%20of%20clear%20physical%20meaning.%20This%20paper%20takes%20a%20step%0Atowards%20connecting%20exhaustive%20closed-loop%20verification%20with%20high-dimensional%0Acontrollers.%20Our%20key%20insight%20is%20that%20the%20behavior%20of%20a%20high-dimensional%0Acontroller%20can%20be%20approximated%20with%20several%20low-dimensional%20controllers.%20To%0Abalance%20the%20approximation%20accuracy%20and%20verifiability%20of%20our%20low-dimensional%0Acontrollers%2C%20we%20leverage%20the%20latest%20verification-aware%20knowledge%20distillation.%0AThen%2C%20we%20inflate%20low-dimensional%20reachability%20results%20with%20statistical%0Aapproximation%20errors%2C%20yielding%20a%20high-confidence%20reachability%20guarantee%20for%20the%0Ahigh-dimensional%20controller.%20We%20investigate%20two%20inflation%20techniques%20--%20based%0Aon%20trajectories%20and%20control%20actions%20--%20both%20of%20which%20show%20convincing%0Aperformance%20in%20three%20OpenAI%20gym%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04843v4&entry.124074799=Read"},
{"title": "Decoupling Feature Extraction and Classification Layers for Calibrated\n  Neural Networks", "author": "Mikkel Jordahn and Pablo Olmos", "abstract": "  Deep Neural Networks (DNN) have shown great promise in many classification\napplications, yet are widely known to have poorly calibrated predictions when\nthey are over-parametrized. Improving DNN calibration without comprising on\nmodel accuracy is of extreme importance and interest in safety critical\napplications such as in the health-care sector. In this work, we show that\ndecoupling the training of feature extraction layers and classification layers\nin over-parametrized DNN architectures such as Wide Residual Networks (WRN) and\nVisual Transformers (ViT) significantly improves model calibration whilst\nretaining accuracy, and at a low training cost. In addition, we show that\nplacing a Gaussian prior on the last hidden layer outputs of a DNN, and\ntraining the model variationally in the classification training stage, even\nfurther improves calibration. We illustrate these methods improve calibration\nacross ViT and WRN architectures for several image classification benchmark\ndatasets.\n", "link": "http://arxiv.org/abs/2405.01196v1", "date": "2024-05-02", "relevancy": 2.1105, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5432}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5202}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Feature%20Extraction%20and%20Classification%20Layers%20for%20Calibrated%0A%20%20Neural%20Networks&body=Title%3A%20Decoupling%20Feature%20Extraction%20and%20Classification%20Layers%20for%20Calibrated%0A%20%20Neural%20Networks%0AAuthor%3A%20Mikkel%20Jordahn%20and%20Pablo%20Olmos%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNN%29%20have%20shown%20great%20promise%20in%20many%20classification%0Aapplications%2C%20yet%20are%20widely%20known%20to%20have%20poorly%20calibrated%20predictions%20when%0Athey%20are%20over-parametrized.%20Improving%20DNN%20calibration%20without%20comprising%20on%0Amodel%20accuracy%20is%20of%20extreme%20importance%20and%20interest%20in%20safety%20critical%0Aapplications%20such%20as%20in%20the%20health-care%20sector.%20In%20this%20work%2C%20we%20show%20that%0Adecoupling%20the%20training%20of%20feature%20extraction%20layers%20and%20classification%20layers%0Ain%20over-parametrized%20DNN%20architectures%20such%20as%20Wide%20Residual%20Networks%20%28WRN%29%20and%0AVisual%20Transformers%20%28ViT%29%20significantly%20improves%20model%20calibration%20whilst%0Aretaining%20accuracy%2C%20and%20at%20a%20low%20training%20cost.%20In%20addition%2C%20we%20show%20that%0Aplacing%20a%20Gaussian%20prior%20on%20the%20last%20hidden%20layer%20outputs%20of%20a%20DNN%2C%20and%0Atraining%20the%20model%20variationally%20in%20the%20classification%20training%20stage%2C%20even%0Afurther%20improves%20calibration.%20We%20illustrate%20these%20methods%20improve%20calibration%0Aacross%20ViT%20and%20WRN%20architectures%20for%20several%20image%20classification%20benchmark%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Feature%2520Extraction%2520and%2520Classification%2520Layers%2520for%2520Calibrated%250A%2520%2520Neural%2520Networks%26entry.906535625%3DMikkel%2520Jordahn%2520and%2520Pablo%2520Olmos%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520%2528DNN%2529%2520have%2520shown%2520great%2520promise%2520in%2520many%2520classification%250Aapplications%252C%2520yet%2520are%2520widely%2520known%2520to%2520have%2520poorly%2520calibrated%2520predictions%2520when%250Athey%2520are%2520over-parametrized.%2520Improving%2520DNN%2520calibration%2520without%2520comprising%2520on%250Amodel%2520accuracy%2520is%2520of%2520extreme%2520importance%2520and%2520interest%2520in%2520safety%2520critical%250Aapplications%2520such%2520as%2520in%2520the%2520health-care%2520sector.%2520In%2520this%2520work%252C%2520we%2520show%2520that%250Adecoupling%2520the%2520training%2520of%2520feature%2520extraction%2520layers%2520and%2520classification%2520layers%250Ain%2520over-parametrized%2520DNN%2520architectures%2520such%2520as%2520Wide%2520Residual%2520Networks%2520%2528WRN%2529%2520and%250AVisual%2520Transformers%2520%2528ViT%2529%2520significantly%2520improves%2520model%2520calibration%2520whilst%250Aretaining%2520accuracy%252C%2520and%2520at%2520a%2520low%2520training%2520cost.%2520In%2520addition%252C%2520we%2520show%2520that%250Aplacing%2520a%2520Gaussian%2520prior%2520on%2520the%2520last%2520hidden%2520layer%2520outputs%2520of%2520a%2520DNN%252C%2520and%250Atraining%2520the%2520model%2520variationally%2520in%2520the%2520classification%2520training%2520stage%252C%2520even%250Afurther%2520improves%2520calibration.%2520We%2520illustrate%2520these%2520methods%2520improve%2520calibration%250Aacross%2520ViT%2520and%2520WRN%2520architectures%2520for%2520several%2520image%2520classification%2520benchmark%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Feature%20Extraction%20and%20Classification%20Layers%20for%20Calibrated%0A%20%20Neural%20Networks&entry.906535625=Mikkel%20Jordahn%20and%20Pablo%20Olmos&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNN%29%20have%20shown%20great%20promise%20in%20many%20classification%0Aapplications%2C%20yet%20are%20widely%20known%20to%20have%20poorly%20calibrated%20predictions%20when%0Athey%20are%20over-parametrized.%20Improving%20DNN%20calibration%20without%20comprising%20on%0Amodel%20accuracy%20is%20of%20extreme%20importance%20and%20interest%20in%20safety%20critical%0Aapplications%20such%20as%20in%20the%20health-care%20sector.%20In%20this%20work%2C%20we%20show%20that%0Adecoupling%20the%20training%20of%20feature%20extraction%20layers%20and%20classification%20layers%0Ain%20over-parametrized%20DNN%20architectures%20such%20as%20Wide%20Residual%20Networks%20%28WRN%29%20and%0AVisual%20Transformers%20%28ViT%29%20significantly%20improves%20model%20calibration%20whilst%0Aretaining%20accuracy%2C%20and%20at%20a%20low%20training%20cost.%20In%20addition%2C%20we%20show%20that%0Aplacing%20a%20Gaussian%20prior%20on%20the%20last%20hidden%20layer%20outputs%20of%20a%20DNN%2C%20and%0Atraining%20the%20model%20variationally%20in%20the%20classification%20training%20stage%2C%20even%0Afurther%20improves%20calibration.%20We%20illustrate%20these%20methods%20improve%20calibration%0Aacross%20ViT%20and%20WRN%20architectures%20for%20several%20image%20classification%20benchmark%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01196v1&entry.124074799=Read"},
{"title": "RaffeSDG: Random Frequency Filtering enabled Single-source Domain\n  Generalization for Medical Image Segmentation", "author": "Heng Li and Haojin Li and Jianyu Chen and Zhongxi Qiu and Huazhu Fu and Lidai Wang and Yan Hu and Jiang Liu", "abstract": "  Deep learning models often encounter challenges in making accurate inferences\nwhen there are domain shifts between the source and target data. This issue is\nparticularly pronounced in clinical settings due to the scarcity of annotated\ndata resulting from the professional and private nature of medical data.\nDespite the existence of decent solutions, many of them are hindered in\nclinical settings due to limitations in data collection and computational\ncomplexity. To tackle domain shifts in data-scarce medical scenarios, we\npropose a Random frequency filtering enabled Single-source Domain\nGeneralization algorithm (RaffeSDG), which promises robust out-of-domain\ninference with segmentation models trained on a single-source domain. A\nfilter-based data augmentation strategy is first proposed to promote domain\nvariability within a single-source domain by introducing variations in\nfrequency space and blending homologous samples. Then Gaussian filter-based\nstructural saliency is also leveraged to learn robust representations across\naugmented samples, further facilitating the training of generalizable\nsegmentation models. To validate the effectiveness of RaffeSDG, we conducted\nextensive experiments involving out-of-domain inference on segmentation tasks\nfor three human tissues imaged by four diverse modalities. Through thorough\ninvestigations and comparisons, compelling evidence was observed in these\nexperiments, demonstrating the potential and generalizability of RaffeSDG. The\ncode is available at\nhttps://github.com/liamheng/Non-IID_Medical_Image_Segmentation.\n", "link": "http://arxiv.org/abs/2405.01228v1", "date": "2024-05-02", "relevancy": 2.1097, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5469}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5377}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaffeSDG%3A%20Random%20Frequency%20Filtering%20enabled%20Single-source%20Domain%0A%20%20Generalization%20for%20Medical%20Image%20Segmentation&body=Title%3A%20RaffeSDG%3A%20Random%20Frequency%20Filtering%20enabled%20Single-source%20Domain%0A%20%20Generalization%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Heng%20Li%20and%20Haojin%20Li%20and%20Jianyu%20Chen%20and%20Zhongxi%20Qiu%20and%20Huazhu%20Fu%20and%20Lidai%20Wang%20and%20Yan%20Hu%20and%20Jiang%20Liu%0AAbstract%3A%20%20%20Deep%20learning%20models%20often%20encounter%20challenges%20in%20making%20accurate%20inferences%0Awhen%20there%20are%20domain%20shifts%20between%20the%20source%20and%20target%20data.%20This%20issue%20is%0Aparticularly%20pronounced%20in%20clinical%20settings%20due%20to%20the%20scarcity%20of%20annotated%0Adata%20resulting%20from%20the%20professional%20and%20private%20nature%20of%20medical%20data.%0ADespite%20the%20existence%20of%20decent%20solutions%2C%20many%20of%20them%20are%20hindered%20in%0Aclinical%20settings%20due%20to%20limitations%20in%20data%20collection%20and%20computational%0Acomplexity.%20To%20tackle%20domain%20shifts%20in%20data-scarce%20medical%20scenarios%2C%20we%0Apropose%20a%20Random%20frequency%20filtering%20enabled%20Single-source%20Domain%0AGeneralization%20algorithm%20%28RaffeSDG%29%2C%20which%20promises%20robust%20out-of-domain%0Ainference%20with%20segmentation%20models%20trained%20on%20a%20single-source%20domain.%20A%0Afilter-based%20data%20augmentation%20strategy%20is%20first%20proposed%20to%20promote%20domain%0Avariability%20within%20a%20single-source%20domain%20by%20introducing%20variations%20in%0Afrequency%20space%20and%20blending%20homologous%20samples.%20Then%20Gaussian%20filter-based%0Astructural%20saliency%20is%20also%20leveraged%20to%20learn%20robust%20representations%20across%0Aaugmented%20samples%2C%20further%20facilitating%20the%20training%20of%20generalizable%0Asegmentation%20models.%20To%20validate%20the%20effectiveness%20of%20RaffeSDG%2C%20we%20conducted%0Aextensive%20experiments%20involving%20out-of-domain%20inference%20on%20segmentation%20tasks%0Afor%20three%20human%20tissues%20imaged%20by%20four%20diverse%20modalities.%20Through%20thorough%0Ainvestigations%20and%20comparisons%2C%20compelling%20evidence%20was%20observed%20in%20these%0Aexperiments%2C%20demonstrating%20the%20potential%20and%20generalizability%20of%20RaffeSDG.%20The%0Acode%20is%20available%20at%0Ahttps%3A//github.com/liamheng/Non-IID_Medical_Image_Segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaffeSDG%253A%2520Random%2520Frequency%2520Filtering%2520enabled%2520Single-source%2520Domain%250A%2520%2520Generalization%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DHeng%2520Li%2520and%2520Haojin%2520Li%2520and%2520Jianyu%2520Chen%2520and%2520Zhongxi%2520Qiu%2520and%2520Huazhu%2520Fu%2520and%2520Lidai%2520Wang%2520and%2520Yan%2520Hu%2520and%2520Jiang%2520Liu%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520often%2520encounter%2520challenges%2520in%2520making%2520accurate%2520inferences%250Awhen%2520there%2520are%2520domain%2520shifts%2520between%2520the%2520source%2520and%2520target%2520data.%2520This%2520issue%2520is%250Aparticularly%2520pronounced%2520in%2520clinical%2520settings%2520due%2520to%2520the%2520scarcity%2520of%2520annotated%250Adata%2520resulting%2520from%2520the%2520professional%2520and%2520private%2520nature%2520of%2520medical%2520data.%250ADespite%2520the%2520existence%2520of%2520decent%2520solutions%252C%2520many%2520of%2520them%2520are%2520hindered%2520in%250Aclinical%2520settings%2520due%2520to%2520limitations%2520in%2520data%2520collection%2520and%2520computational%250Acomplexity.%2520To%2520tackle%2520domain%2520shifts%2520in%2520data-scarce%2520medical%2520scenarios%252C%2520we%250Apropose%2520a%2520Random%2520frequency%2520filtering%2520enabled%2520Single-source%2520Domain%250AGeneralization%2520algorithm%2520%2528RaffeSDG%2529%252C%2520which%2520promises%2520robust%2520out-of-domain%250Ainference%2520with%2520segmentation%2520models%2520trained%2520on%2520a%2520single-source%2520domain.%2520A%250Afilter-based%2520data%2520augmentation%2520strategy%2520is%2520first%2520proposed%2520to%2520promote%2520domain%250Avariability%2520within%2520a%2520single-source%2520domain%2520by%2520introducing%2520variations%2520in%250Afrequency%2520space%2520and%2520blending%2520homologous%2520samples.%2520Then%2520Gaussian%2520filter-based%250Astructural%2520saliency%2520is%2520also%2520leveraged%2520to%2520learn%2520robust%2520representations%2520across%250Aaugmented%2520samples%252C%2520further%2520facilitating%2520the%2520training%2520of%2520generalizable%250Asegmentation%2520models.%2520To%2520validate%2520the%2520effectiveness%2520of%2520RaffeSDG%252C%2520we%2520conducted%250Aextensive%2520experiments%2520involving%2520out-of-domain%2520inference%2520on%2520segmentation%2520tasks%250Afor%2520three%2520human%2520tissues%2520imaged%2520by%2520four%2520diverse%2520modalities.%2520Through%2520thorough%250Ainvestigations%2520and%2520comparisons%252C%2520compelling%2520evidence%2520was%2520observed%2520in%2520these%250Aexperiments%252C%2520demonstrating%2520the%2520potential%2520and%2520generalizability%2520of%2520RaffeSDG.%2520The%250Acode%2520is%2520available%2520at%250Ahttps%253A//github.com/liamheng/Non-IID_Medical_Image_Segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaffeSDG%3A%20Random%20Frequency%20Filtering%20enabled%20Single-source%20Domain%0A%20%20Generalization%20for%20Medical%20Image%20Segmentation&entry.906535625=Heng%20Li%20and%20Haojin%20Li%20and%20Jianyu%20Chen%20and%20Zhongxi%20Qiu%20and%20Huazhu%20Fu%20and%20Lidai%20Wang%20and%20Yan%20Hu%20and%20Jiang%20Liu&entry.1292438233=%20%20Deep%20learning%20models%20often%20encounter%20challenges%20in%20making%20accurate%20inferences%0Awhen%20there%20are%20domain%20shifts%20between%20the%20source%20and%20target%20data.%20This%20issue%20is%0Aparticularly%20pronounced%20in%20clinical%20settings%20due%20to%20the%20scarcity%20of%20annotated%0Adata%20resulting%20from%20the%20professional%20and%20private%20nature%20of%20medical%20data.%0ADespite%20the%20existence%20of%20decent%20solutions%2C%20many%20of%20them%20are%20hindered%20in%0Aclinical%20settings%20due%20to%20limitations%20in%20data%20collection%20and%20computational%0Acomplexity.%20To%20tackle%20domain%20shifts%20in%20data-scarce%20medical%20scenarios%2C%20we%0Apropose%20a%20Random%20frequency%20filtering%20enabled%20Single-source%20Domain%0AGeneralization%20algorithm%20%28RaffeSDG%29%2C%20which%20promises%20robust%20out-of-domain%0Ainference%20with%20segmentation%20models%20trained%20on%20a%20single-source%20domain.%20A%0Afilter-based%20data%20augmentation%20strategy%20is%20first%20proposed%20to%20promote%20domain%0Avariability%20within%20a%20single-source%20domain%20by%20introducing%20variations%20in%0Afrequency%20space%20and%20blending%20homologous%20samples.%20Then%20Gaussian%20filter-based%0Astructural%20saliency%20is%20also%20leveraged%20to%20learn%20robust%20representations%20across%0Aaugmented%20samples%2C%20further%20facilitating%20the%20training%20of%20generalizable%0Asegmentation%20models.%20To%20validate%20the%20effectiveness%20of%20RaffeSDG%2C%20we%20conducted%0Aextensive%20experiments%20involving%20out-of-domain%20inference%20on%20segmentation%20tasks%0Afor%20three%20human%20tissues%20imaged%20by%20four%20diverse%20modalities.%20Through%20thorough%0Ainvestigations%20and%20comparisons%2C%20compelling%20evidence%20was%20observed%20in%20these%0Aexperiments%2C%20demonstrating%20the%20potential%20and%20generalizability%20of%20RaffeSDG.%20The%0Acode%20is%20available%20at%0Ahttps%3A//github.com/liamheng/Non-IID_Medical_Image_Segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01228v1&entry.124074799=Read"},
{"title": "Imagine the Unseen: Occluded Pedestrian Detection via Adversarial\n  Feature Completion", "author": "Shanshan Zhang and Mingqian Ji and Yang Li and Jian Yang", "abstract": "  Pedestrian detection has significantly progressed in recent years, thanks to\nthe development of DNNs. However, detection performance at occluded scenes is\nstill far from satisfactory, as occlusion increases the intra-class variance of\npedestrians, hindering the model from finding an accurate classification\nboundary between pedestrians and background clutters. From the perspective of\nreducing intra-class variance, we propose to complete features for occluded\nregions so as to align the features of pedestrians across different occlusion\npatterns. An important premise for feature completion is to locate occluded\nregions. From our analysis, channel features of different pedestrian proposals\nonly show high correlation values at visible parts and thus feature\ncorrelations can be used to model occlusion patterns. In order to narrow down\nthe gap between completed features and real fully visible ones, we propose an\nadversarial learning method, which completes occluded features with a generator\nsuch that they can hardly be distinguished by the discriminator from real fully\nvisible features. We report experimental results on the CityPersons, Caltech\nand CrowdHuman datasets. On CityPersons, we show significant improvements over\nfive different baseline detectors, especially on the heavy occlusion subset.\nFurthermore, we show that our proposed method FeatComp++ achieves\nstate-of-the-art results on all the above three datasets without relying on\nextra cues.\n", "link": "http://arxiv.org/abs/2405.01311v1", "date": "2024-05-02", "relevancy": 2.108, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5347}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5277}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imagine%20the%20Unseen%3A%20Occluded%20Pedestrian%20Detection%20via%20Adversarial%0A%20%20Feature%20Completion&body=Title%3A%20Imagine%20the%20Unseen%3A%20Occluded%20Pedestrian%20Detection%20via%20Adversarial%0A%20%20Feature%20Completion%0AAuthor%3A%20Shanshan%20Zhang%20and%20Mingqian%20Ji%20and%20Yang%20Li%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Pedestrian%20detection%20has%20significantly%20progressed%20in%20recent%20years%2C%20thanks%20to%0Athe%20development%20of%20DNNs.%20However%2C%20detection%20performance%20at%20occluded%20scenes%20is%0Astill%20far%20from%20satisfactory%2C%20as%20occlusion%20increases%20the%20intra-class%20variance%20of%0Apedestrians%2C%20hindering%20the%20model%20from%20finding%20an%20accurate%20classification%0Aboundary%20between%20pedestrians%20and%20background%20clutters.%20From%20the%20perspective%20of%0Areducing%20intra-class%20variance%2C%20we%20propose%20to%20complete%20features%20for%20occluded%0Aregions%20so%20as%20to%20align%20the%20features%20of%20pedestrians%20across%20different%20occlusion%0Apatterns.%20An%20important%20premise%20for%20feature%20completion%20is%20to%20locate%20occluded%0Aregions.%20From%20our%20analysis%2C%20channel%20features%20of%20different%20pedestrian%20proposals%0Aonly%20show%20high%20correlation%20values%20at%20visible%20parts%20and%20thus%20feature%0Acorrelations%20can%20be%20used%20to%20model%20occlusion%20patterns.%20In%20order%20to%20narrow%20down%0Athe%20gap%20between%20completed%20features%20and%20real%20fully%20visible%20ones%2C%20we%20propose%20an%0Aadversarial%20learning%20method%2C%20which%20completes%20occluded%20features%20with%20a%20generator%0Asuch%20that%20they%20can%20hardly%20be%20distinguished%20by%20the%20discriminator%20from%20real%20fully%0Avisible%20features.%20We%20report%20experimental%20results%20on%20the%20CityPersons%2C%20Caltech%0Aand%20CrowdHuman%20datasets.%20On%20CityPersons%2C%20we%20show%20significant%20improvements%20over%0Afive%20different%20baseline%20detectors%2C%20especially%20on%20the%20heavy%20occlusion%20subset.%0AFurthermore%2C%20we%20show%20that%20our%20proposed%20method%20FeatComp%2B%2B%20achieves%0Astate-of-the-art%20results%20on%20all%20the%20above%20three%20datasets%20without%20relying%20on%0Aextra%20cues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagine%2520the%2520Unseen%253A%2520Occluded%2520Pedestrian%2520Detection%2520via%2520Adversarial%250A%2520%2520Feature%2520Completion%26entry.906535625%3DShanshan%2520Zhang%2520and%2520Mingqian%2520Ji%2520and%2520Yang%2520Li%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Pedestrian%2520detection%2520has%2520significantly%2520progressed%2520in%2520recent%2520years%252C%2520thanks%2520to%250Athe%2520development%2520of%2520DNNs.%2520However%252C%2520detection%2520performance%2520at%2520occluded%2520scenes%2520is%250Astill%2520far%2520from%2520satisfactory%252C%2520as%2520occlusion%2520increases%2520the%2520intra-class%2520variance%2520of%250Apedestrians%252C%2520hindering%2520the%2520model%2520from%2520finding%2520an%2520accurate%2520classification%250Aboundary%2520between%2520pedestrians%2520and%2520background%2520clutters.%2520From%2520the%2520perspective%2520of%250Areducing%2520intra-class%2520variance%252C%2520we%2520propose%2520to%2520complete%2520features%2520for%2520occluded%250Aregions%2520so%2520as%2520to%2520align%2520the%2520features%2520of%2520pedestrians%2520across%2520different%2520occlusion%250Apatterns.%2520An%2520important%2520premise%2520for%2520feature%2520completion%2520is%2520to%2520locate%2520occluded%250Aregions.%2520From%2520our%2520analysis%252C%2520channel%2520features%2520of%2520different%2520pedestrian%2520proposals%250Aonly%2520show%2520high%2520correlation%2520values%2520at%2520visible%2520parts%2520and%2520thus%2520feature%250Acorrelations%2520can%2520be%2520used%2520to%2520model%2520occlusion%2520patterns.%2520In%2520order%2520to%2520narrow%2520down%250Athe%2520gap%2520between%2520completed%2520features%2520and%2520real%2520fully%2520visible%2520ones%252C%2520we%2520propose%2520an%250Aadversarial%2520learning%2520method%252C%2520which%2520completes%2520occluded%2520features%2520with%2520a%2520generator%250Asuch%2520that%2520they%2520can%2520hardly%2520be%2520distinguished%2520by%2520the%2520discriminator%2520from%2520real%2520fully%250Avisible%2520features.%2520We%2520report%2520experimental%2520results%2520on%2520the%2520CityPersons%252C%2520Caltech%250Aand%2520CrowdHuman%2520datasets.%2520On%2520CityPersons%252C%2520we%2520show%2520significant%2520improvements%2520over%250Afive%2520different%2520baseline%2520detectors%252C%2520especially%2520on%2520the%2520heavy%2520occlusion%2520subset.%250AFurthermore%252C%2520we%2520show%2520that%2520our%2520proposed%2520method%2520FeatComp%252B%252B%2520achieves%250Astate-of-the-art%2520results%2520on%2520all%2520the%2520above%2520three%2520datasets%2520without%2520relying%2520on%250Aextra%2520cues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imagine%20the%20Unseen%3A%20Occluded%20Pedestrian%20Detection%20via%20Adversarial%0A%20%20Feature%20Completion&entry.906535625=Shanshan%20Zhang%20and%20Mingqian%20Ji%20and%20Yang%20Li%20and%20Jian%20Yang&entry.1292438233=%20%20Pedestrian%20detection%20has%20significantly%20progressed%20in%20recent%20years%2C%20thanks%20to%0Athe%20development%20of%20DNNs.%20However%2C%20detection%20performance%20at%20occluded%20scenes%20is%0Astill%20far%20from%20satisfactory%2C%20as%20occlusion%20increases%20the%20intra-class%20variance%20of%0Apedestrians%2C%20hindering%20the%20model%20from%20finding%20an%20accurate%20classification%0Aboundary%20between%20pedestrians%20and%20background%20clutters.%20From%20the%20perspective%20of%0Areducing%20intra-class%20variance%2C%20we%20propose%20to%20complete%20features%20for%20occluded%0Aregions%20so%20as%20to%20align%20the%20features%20of%20pedestrians%20across%20different%20occlusion%0Apatterns.%20An%20important%20premise%20for%20feature%20completion%20is%20to%20locate%20occluded%0Aregions.%20From%20our%20analysis%2C%20channel%20features%20of%20different%20pedestrian%20proposals%0Aonly%20show%20high%20correlation%20values%20at%20visible%20parts%20and%20thus%20feature%0Acorrelations%20can%20be%20used%20to%20model%20occlusion%20patterns.%20In%20order%20to%20narrow%20down%0Athe%20gap%20between%20completed%20features%20and%20real%20fully%20visible%20ones%2C%20we%20propose%20an%0Aadversarial%20learning%20method%2C%20which%20completes%20occluded%20features%20with%20a%20generator%0Asuch%20that%20they%20can%20hardly%20be%20distinguished%20by%20the%20discriminator%20from%20real%20fully%0Avisible%20features.%20We%20report%20experimental%20results%20on%20the%20CityPersons%2C%20Caltech%0Aand%20CrowdHuman%20datasets.%20On%20CityPersons%2C%20we%20show%20significant%20improvements%20over%0Afive%20different%20baseline%20detectors%2C%20especially%20on%20the%20heavy%20occlusion%20subset.%0AFurthermore%2C%20we%20show%20that%20our%20proposed%20method%20FeatComp%2B%2B%20achieves%0Astate-of-the-art%20results%20on%20all%20the%20above%20three%20datasets%20without%20relying%20on%0Aextra%20cues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01311v1&entry.124074799=Read"},
{"title": "Improving Domain Generalization on Gaze Estimation via Branch-out\n  Auxiliary Regularization", "author": "Ruijie Zhao and Pinyan Tang and Sihui Luo", "abstract": "  Despite remarkable advancements, mainstream gaze estimation techniques,\nparticularly appearance-based methods, often suffer from performance\ndegradation in uncontrolled environments due to variations in illumination and\nindividual facial attributes. Existing domain adaptation strategies, limited by\ntheir need for target domain samples, may fall short in real-world\napplications. This letter introduces Branch-out Auxiliary Regularization (BAR),\nan innovative method designed to boost gaze estimation's generalization\ncapabilities without requiring direct access to target domain data.\nSpecifically, BAR integrates two auxiliary consistency regularization branches:\none that uses augmented samples to counteract environmental variations, and\nanother that aligns gaze directions with positive source domain samples to\nencourage the learning of consistent gaze features. These auxiliary pathways\nstrengthen the core network and are integrated in a smooth, plug-and-play\nmanner, facilitating easy adaptation to various other models. Comprehensive\nexperimental evaluations on four cross-dataset tasks demonstrate the\nsuperiority of our approach.\n", "link": "http://arxiv.org/abs/2405.01439v1", "date": "2024-05-02", "relevancy": 2.1037, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5318}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5249}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Domain%20Generalization%20on%20Gaze%20Estimation%20via%20Branch-out%0A%20%20Auxiliary%20Regularization&body=Title%3A%20Improving%20Domain%20Generalization%20on%20Gaze%20Estimation%20via%20Branch-out%0A%20%20Auxiliary%20Regularization%0AAuthor%3A%20Ruijie%20Zhao%20and%20Pinyan%20Tang%20and%20Sihui%20Luo%0AAbstract%3A%20%20%20Despite%20remarkable%20advancements%2C%20mainstream%20gaze%20estimation%20techniques%2C%0Aparticularly%20appearance-based%20methods%2C%20often%20suffer%20from%20performance%0Adegradation%20in%20uncontrolled%20environments%20due%20to%20variations%20in%20illumination%20and%0Aindividual%20facial%20attributes.%20Existing%20domain%20adaptation%20strategies%2C%20limited%20by%0Atheir%20need%20for%20target%20domain%20samples%2C%20may%20fall%20short%20in%20real-world%0Aapplications.%20This%20letter%20introduces%20Branch-out%20Auxiliary%20Regularization%20%28BAR%29%2C%0Aan%20innovative%20method%20designed%20to%20boost%20gaze%20estimation%27s%20generalization%0Acapabilities%20without%20requiring%20direct%20access%20to%20target%20domain%20data.%0ASpecifically%2C%20BAR%20integrates%20two%20auxiliary%20consistency%20regularization%20branches%3A%0Aone%20that%20uses%20augmented%20samples%20to%20counteract%20environmental%20variations%2C%20and%0Aanother%20that%20aligns%20gaze%20directions%20with%20positive%20source%20domain%20samples%20to%0Aencourage%20the%20learning%20of%20consistent%20gaze%20features.%20These%20auxiliary%20pathways%0Astrengthen%20the%20core%20network%20and%20are%20integrated%20in%20a%20smooth%2C%20plug-and-play%0Amanner%2C%20facilitating%20easy%20adaptation%20to%20various%20other%20models.%20Comprehensive%0Aexperimental%20evaluations%20on%20four%20cross-dataset%20tasks%20demonstrate%20the%0Asuperiority%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Domain%2520Generalization%2520on%2520Gaze%2520Estimation%2520via%2520Branch-out%250A%2520%2520Auxiliary%2520Regularization%26entry.906535625%3DRuijie%2520Zhao%2520and%2520Pinyan%2520Tang%2520and%2520Sihui%2520Luo%26entry.1292438233%3D%2520%2520Despite%2520remarkable%2520advancements%252C%2520mainstream%2520gaze%2520estimation%2520techniques%252C%250Aparticularly%2520appearance-based%2520methods%252C%2520often%2520suffer%2520from%2520performance%250Adegradation%2520in%2520uncontrolled%2520environments%2520due%2520to%2520variations%2520in%2520illumination%2520and%250Aindividual%2520facial%2520attributes.%2520Existing%2520domain%2520adaptation%2520strategies%252C%2520limited%2520by%250Atheir%2520need%2520for%2520target%2520domain%2520samples%252C%2520may%2520fall%2520short%2520in%2520real-world%250Aapplications.%2520This%2520letter%2520introduces%2520Branch-out%2520Auxiliary%2520Regularization%2520%2528BAR%2529%252C%250Aan%2520innovative%2520method%2520designed%2520to%2520boost%2520gaze%2520estimation%2527s%2520generalization%250Acapabilities%2520without%2520requiring%2520direct%2520access%2520to%2520target%2520domain%2520data.%250ASpecifically%252C%2520BAR%2520integrates%2520two%2520auxiliary%2520consistency%2520regularization%2520branches%253A%250Aone%2520that%2520uses%2520augmented%2520samples%2520to%2520counteract%2520environmental%2520variations%252C%2520and%250Aanother%2520that%2520aligns%2520gaze%2520directions%2520with%2520positive%2520source%2520domain%2520samples%2520to%250Aencourage%2520the%2520learning%2520of%2520consistent%2520gaze%2520features.%2520These%2520auxiliary%2520pathways%250Astrengthen%2520the%2520core%2520network%2520and%2520are%2520integrated%2520in%2520a%2520smooth%252C%2520plug-and-play%250Amanner%252C%2520facilitating%2520easy%2520adaptation%2520to%2520various%2520other%2520models.%2520Comprehensive%250Aexperimental%2520evaluations%2520on%2520four%2520cross-dataset%2520tasks%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Domain%20Generalization%20on%20Gaze%20Estimation%20via%20Branch-out%0A%20%20Auxiliary%20Regularization&entry.906535625=Ruijie%20Zhao%20and%20Pinyan%20Tang%20and%20Sihui%20Luo&entry.1292438233=%20%20Despite%20remarkable%20advancements%2C%20mainstream%20gaze%20estimation%20techniques%2C%0Aparticularly%20appearance-based%20methods%2C%20often%20suffer%20from%20performance%0Adegradation%20in%20uncontrolled%20environments%20due%20to%20variations%20in%20illumination%20and%0Aindividual%20facial%20attributes.%20Existing%20domain%20adaptation%20strategies%2C%20limited%20by%0Atheir%20need%20for%20target%20domain%20samples%2C%20may%20fall%20short%20in%20real-world%0Aapplications.%20This%20letter%20introduces%20Branch-out%20Auxiliary%20Regularization%20%28BAR%29%2C%0Aan%20innovative%20method%20designed%20to%20boost%20gaze%20estimation%27s%20generalization%0Acapabilities%20without%20requiring%20direct%20access%20to%20target%20domain%20data.%0ASpecifically%2C%20BAR%20integrates%20two%20auxiliary%20consistency%20regularization%20branches%3A%0Aone%20that%20uses%20augmented%20samples%20to%20counteract%20environmental%20variations%2C%20and%0Aanother%20that%20aligns%20gaze%20directions%20with%20positive%20source%20domain%20samples%20to%0Aencourage%20the%20learning%20of%20consistent%20gaze%20features.%20These%20auxiliary%20pathways%0Astrengthen%20the%20core%20network%20and%20are%20integrated%20in%20a%20smooth%2C%20plug-and-play%0Amanner%2C%20facilitating%20easy%20adaptation%20to%20various%20other%20models.%20Comprehensive%0Aexperimental%20evaluations%20on%20four%20cross-dataset%20tasks%20demonstrate%20the%0Asuperiority%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01439v1&entry.124074799=Read"},
{"title": "A Probabilistic Framework for Modular Continual Learning", "author": "Lazar Valkov and Akash Srivastava and Swarat Chaudhuri and Charles Sutton", "abstract": "  Modular approaches that use a different composition of modules for each\nproblem are a promising direction in continual learning (CL). However,\nsearching through the large, discrete space of module compositions is\nchallenging, especially because evaluating a composition's performance requires\na round of neural network training. We address this challenge through a modular\nCL framework, PICLE, that uses a probabilistic model to cheaply compute the\nfitness of each composition, allowing PICLE to achieve both perceptual,\nfew-shot and latent transfer. The model combines prior knowledge about good\nmodule compositions with dataset-specific information. We evaluate PICLE using\ntwo benchmark suites designed to assess different desiderata of CL techniques.\nComparing to a wide range of approaches, we show that PICLE is the first\nmodular CL algorithm to achieve perceptual, few-shot and latent transfer while\nscaling well to large search spaces, outperforming previous state-of-the-art\nmodular CL approaches on long problem sequences.\n", "link": "http://arxiv.org/abs/2306.06545v2", "date": "2024-05-02", "relevancy": 2.1034, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5557}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Probabilistic%20Framework%20for%20Modular%20Continual%20Learning&body=Title%3A%20A%20Probabilistic%20Framework%20for%20Modular%20Continual%20Learning%0AAuthor%3A%20Lazar%20Valkov%20and%20Akash%20Srivastava%20and%20Swarat%20Chaudhuri%20and%20Charles%20Sutton%0AAbstract%3A%20%20%20Modular%20approaches%20that%20use%20a%20different%20composition%20of%20modules%20for%20each%0Aproblem%20are%20a%20promising%20direction%20in%20continual%20learning%20%28CL%29.%20However%2C%0Asearching%20through%20the%20large%2C%20discrete%20space%20of%20module%20compositions%20is%0Achallenging%2C%20especially%20because%20evaluating%20a%20composition%27s%20performance%20requires%0Aa%20round%20of%20neural%20network%20training.%20We%20address%20this%20challenge%20through%20a%20modular%0ACL%20framework%2C%20PICLE%2C%20that%20uses%20a%20probabilistic%20model%20to%20cheaply%20compute%20the%0Afitness%20of%20each%20composition%2C%20allowing%20PICLE%20to%20achieve%20both%20perceptual%2C%0Afew-shot%20and%20latent%20transfer.%20The%20model%20combines%20prior%20knowledge%20about%20good%0Amodule%20compositions%20with%20dataset-specific%20information.%20We%20evaluate%20PICLE%20using%0Atwo%20benchmark%20suites%20designed%20to%20assess%20different%20desiderata%20of%20CL%20techniques.%0AComparing%20to%20a%20wide%20range%20of%20approaches%2C%20we%20show%20that%20PICLE%20is%20the%20first%0Amodular%20CL%20algorithm%20to%20achieve%20perceptual%2C%20few-shot%20and%20latent%20transfer%20while%0Ascaling%20well%20to%20large%20search%20spaces%2C%20outperforming%20previous%20state-of-the-art%0Amodular%20CL%20approaches%20on%20long%20problem%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Probabilistic%2520Framework%2520for%2520Modular%2520Continual%2520Learning%26entry.906535625%3DLazar%2520Valkov%2520and%2520Akash%2520Srivastava%2520and%2520Swarat%2520Chaudhuri%2520and%2520Charles%2520Sutton%26entry.1292438233%3D%2520%2520Modular%2520approaches%2520that%2520use%2520a%2520different%2520composition%2520of%2520modules%2520for%2520each%250Aproblem%2520are%2520a%2520promising%2520direction%2520in%2520continual%2520learning%2520%2528CL%2529.%2520However%252C%250Asearching%2520through%2520the%2520large%252C%2520discrete%2520space%2520of%2520module%2520compositions%2520is%250Achallenging%252C%2520especially%2520because%2520evaluating%2520a%2520composition%2527s%2520performance%2520requires%250Aa%2520round%2520of%2520neural%2520network%2520training.%2520We%2520address%2520this%2520challenge%2520through%2520a%2520modular%250ACL%2520framework%252C%2520PICLE%252C%2520that%2520uses%2520a%2520probabilistic%2520model%2520to%2520cheaply%2520compute%2520the%250Afitness%2520of%2520each%2520composition%252C%2520allowing%2520PICLE%2520to%2520achieve%2520both%2520perceptual%252C%250Afew-shot%2520and%2520latent%2520transfer.%2520The%2520model%2520combines%2520prior%2520knowledge%2520about%2520good%250Amodule%2520compositions%2520with%2520dataset-specific%2520information.%2520We%2520evaluate%2520PICLE%2520using%250Atwo%2520benchmark%2520suites%2520designed%2520to%2520assess%2520different%2520desiderata%2520of%2520CL%2520techniques.%250AComparing%2520to%2520a%2520wide%2520range%2520of%2520approaches%252C%2520we%2520show%2520that%2520PICLE%2520is%2520the%2520first%250Amodular%2520CL%2520algorithm%2520to%2520achieve%2520perceptual%252C%2520few-shot%2520and%2520latent%2520transfer%2520while%250Ascaling%2520well%2520to%2520large%2520search%2520spaces%252C%2520outperforming%2520previous%2520state-of-the-art%250Amodular%2520CL%2520approaches%2520on%2520long%2520problem%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Probabilistic%20Framework%20for%20Modular%20Continual%20Learning&entry.906535625=Lazar%20Valkov%20and%20Akash%20Srivastava%20and%20Swarat%20Chaudhuri%20and%20Charles%20Sutton&entry.1292438233=%20%20Modular%20approaches%20that%20use%20a%20different%20composition%20of%20modules%20for%20each%0Aproblem%20are%20a%20promising%20direction%20in%20continual%20learning%20%28CL%29.%20However%2C%0Asearching%20through%20the%20large%2C%20discrete%20space%20of%20module%20compositions%20is%0Achallenging%2C%20especially%20because%20evaluating%20a%20composition%27s%20performance%20requires%0Aa%20round%20of%20neural%20network%20training.%20We%20address%20this%20challenge%20through%20a%20modular%0ACL%20framework%2C%20PICLE%2C%20that%20uses%20a%20probabilistic%20model%20to%20cheaply%20compute%20the%0Afitness%20of%20each%20composition%2C%20allowing%20PICLE%20to%20achieve%20both%20perceptual%2C%0Afew-shot%20and%20latent%20transfer.%20The%20model%20combines%20prior%20knowledge%20about%20good%0Amodule%20compositions%20with%20dataset-specific%20information.%20We%20evaluate%20PICLE%20using%0Atwo%20benchmark%20suites%20designed%20to%20assess%20different%20desiderata%20of%20CL%20techniques.%0AComparing%20to%20a%20wide%20range%20of%20approaches%2C%20we%20show%20that%20PICLE%20is%20the%20first%0Amodular%20CL%20algorithm%20to%20achieve%20perceptual%2C%20few-shot%20and%20latent%20transfer%20while%0Ascaling%20well%20to%20large%20search%20spaces%2C%20outperforming%20previous%20state-of-the-art%0Amodular%20CL%20approaches%20on%20long%20problem%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06545v2&entry.124074799=Read"},
{"title": "Reinforcement Learning based Autonomous Multi-Rotor Landing on Moving\n  Platforms", "author": "Pascal Goldschmid and Aamir Ahmad", "abstract": "  Multi-rotor UAVs suffer from a restricted range and flight duration due to\nlimited battery capacity. Autonomous landing on a 2D moving platform offers the\npossibility to replenish batteries and offload data, thus increasing the\nutility of the vehicle. Classical approaches rely on accurate, complex and\ndifficult-to-derive models of the vehicle and the environment. Reinforcement\nlearning (RL) provides an attractive alternative due to its ability to learn a\nsuitable control policy exclusively from data during a training procedure.\nHowever, current methods require several hours to train, have limited success\nrates and depend on hyperparameters that need to be tuned by trial-and-error.\nWe address all these issues in this work. First, we decompose the landing\nprocedure into a sequence of simpler, but similar learning tasks. This is\nenabled by applying two instances of the same RL based controller trained for\n1D motion for controlling the multi-rotor's movement in both the longitudinal\nand the lateral directions. Second, we introduce a powerful state space\ndiscretization technique that is based on i) kinematic modeling of the moving\nplatform to derive information about the state space topology and ii)\nstructuring the training as a sequential curriculum using transfer learning.\nThird, we leverage the kinematics model of the moving platform to also derive\ninterpretable hyperparameters for the training process that ensure sufficient\nmaneuverability of the multi-rotor vehicle. The training is performed using the\ntabular RL method Double Q-Learning. Through extensive simulations we show that\nthe presented method significantly increases the rate of successful landings,\nwhile requiring less training time compared to other deep RL approaches.\nFinally, we deploy and demonstrate our algorithm on real hardware. For all\nevaluation scenarios we provide statistics on the agent's performance.\n", "link": "http://arxiv.org/abs/2302.13192v2", "date": "2024-05-02", "relevancy": 2.0842, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5814}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.516}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20based%20Autonomous%20Multi-Rotor%20Landing%20on%20Moving%0A%20%20Platforms&body=Title%3A%20Reinforcement%20Learning%20based%20Autonomous%20Multi-Rotor%20Landing%20on%20Moving%0A%20%20Platforms%0AAuthor%3A%20Pascal%20Goldschmid%20and%20Aamir%20Ahmad%0AAbstract%3A%20%20%20Multi-rotor%20UAVs%20suffer%20from%20a%20restricted%20range%20and%20flight%20duration%20due%20to%0Alimited%20battery%20capacity.%20Autonomous%20landing%20on%20a%202D%20moving%20platform%20offers%20the%0Apossibility%20to%20replenish%20batteries%20and%20offload%20data%2C%20thus%20increasing%20the%0Autility%20of%20the%20vehicle.%20Classical%20approaches%20rely%20on%20accurate%2C%20complex%20and%0Adifficult-to-derive%20models%20of%20the%20vehicle%20and%20the%20environment.%20Reinforcement%0Alearning%20%28RL%29%20provides%20an%20attractive%20alternative%20due%20to%20its%20ability%20to%20learn%20a%0Asuitable%20control%20policy%20exclusively%20from%20data%20during%20a%20training%20procedure.%0AHowever%2C%20current%20methods%20require%20several%20hours%20to%20train%2C%20have%20limited%20success%0Arates%20and%20depend%20on%20hyperparameters%20that%20need%20to%20be%20tuned%20by%20trial-and-error.%0AWe%20address%20all%20these%20issues%20in%20this%20work.%20First%2C%20we%20decompose%20the%20landing%0Aprocedure%20into%20a%20sequence%20of%20simpler%2C%20but%20similar%20learning%20tasks.%20This%20is%0Aenabled%20by%20applying%20two%20instances%20of%20the%20same%20RL%20based%20controller%20trained%20for%0A1D%20motion%20for%20controlling%20the%20multi-rotor%27s%20movement%20in%20both%20the%20longitudinal%0Aand%20the%20lateral%20directions.%20Second%2C%20we%20introduce%20a%20powerful%20state%20space%0Adiscretization%20technique%20that%20is%20based%20on%20i%29%20kinematic%20modeling%20of%20the%20moving%0Aplatform%20to%20derive%20information%20about%20the%20state%20space%20topology%20and%20ii%29%0Astructuring%20the%20training%20as%20a%20sequential%20curriculum%20using%20transfer%20learning.%0AThird%2C%20we%20leverage%20the%20kinematics%20model%20of%20the%20moving%20platform%20to%20also%20derive%0Ainterpretable%20hyperparameters%20for%20the%20training%20process%20that%20ensure%20sufficient%0Amaneuverability%20of%20the%20multi-rotor%20vehicle.%20The%20training%20is%20performed%20using%20the%0Atabular%20RL%20method%20Double%20Q-Learning.%20Through%20extensive%20simulations%20we%20show%20that%0Athe%20presented%20method%20significantly%20increases%20the%20rate%20of%20successful%20landings%2C%0Awhile%20requiring%20less%20training%20time%20compared%20to%20other%20deep%20RL%20approaches.%0AFinally%2C%20we%20deploy%20and%20demonstrate%20our%20algorithm%20on%20real%20hardware.%20For%20all%0Aevaluation%20scenarios%20we%20provide%20statistics%20on%20the%20agent%27s%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.13192v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520based%2520Autonomous%2520Multi-Rotor%2520Landing%2520on%2520Moving%250A%2520%2520Platforms%26entry.906535625%3DPascal%2520Goldschmid%2520and%2520Aamir%2520Ahmad%26entry.1292438233%3D%2520%2520Multi-rotor%2520UAVs%2520suffer%2520from%2520a%2520restricted%2520range%2520and%2520flight%2520duration%2520due%2520to%250Alimited%2520battery%2520capacity.%2520Autonomous%2520landing%2520on%2520a%25202D%2520moving%2520platform%2520offers%2520the%250Apossibility%2520to%2520replenish%2520batteries%2520and%2520offload%2520data%252C%2520thus%2520increasing%2520the%250Autility%2520of%2520the%2520vehicle.%2520Classical%2520approaches%2520rely%2520on%2520accurate%252C%2520complex%2520and%250Adifficult-to-derive%2520models%2520of%2520the%2520vehicle%2520and%2520the%2520environment.%2520Reinforcement%250Alearning%2520%2528RL%2529%2520provides%2520an%2520attractive%2520alternative%2520due%2520to%2520its%2520ability%2520to%2520learn%2520a%250Asuitable%2520control%2520policy%2520exclusively%2520from%2520data%2520during%2520a%2520training%2520procedure.%250AHowever%252C%2520current%2520methods%2520require%2520several%2520hours%2520to%2520train%252C%2520have%2520limited%2520success%250Arates%2520and%2520depend%2520on%2520hyperparameters%2520that%2520need%2520to%2520be%2520tuned%2520by%2520trial-and-error.%250AWe%2520address%2520all%2520these%2520issues%2520in%2520this%2520work.%2520First%252C%2520we%2520decompose%2520the%2520landing%250Aprocedure%2520into%2520a%2520sequence%2520of%2520simpler%252C%2520but%2520similar%2520learning%2520tasks.%2520This%2520is%250Aenabled%2520by%2520applying%2520two%2520instances%2520of%2520the%2520same%2520RL%2520based%2520controller%2520trained%2520for%250A1D%2520motion%2520for%2520controlling%2520the%2520multi-rotor%2527s%2520movement%2520in%2520both%2520the%2520longitudinal%250Aand%2520the%2520lateral%2520directions.%2520Second%252C%2520we%2520introduce%2520a%2520powerful%2520state%2520space%250Adiscretization%2520technique%2520that%2520is%2520based%2520on%2520i%2529%2520kinematic%2520modeling%2520of%2520the%2520moving%250Aplatform%2520to%2520derive%2520information%2520about%2520the%2520state%2520space%2520topology%2520and%2520ii%2529%250Astructuring%2520the%2520training%2520as%2520a%2520sequential%2520curriculum%2520using%2520transfer%2520learning.%250AThird%252C%2520we%2520leverage%2520the%2520kinematics%2520model%2520of%2520the%2520moving%2520platform%2520to%2520also%2520derive%250Ainterpretable%2520hyperparameters%2520for%2520the%2520training%2520process%2520that%2520ensure%2520sufficient%250Amaneuverability%2520of%2520the%2520multi-rotor%2520vehicle.%2520The%2520training%2520is%2520performed%2520using%2520the%250Atabular%2520RL%2520method%2520Double%2520Q-Learning.%2520Through%2520extensive%2520simulations%2520we%2520show%2520that%250Athe%2520presented%2520method%2520significantly%2520increases%2520the%2520rate%2520of%2520successful%2520landings%252C%250Awhile%2520requiring%2520less%2520training%2520time%2520compared%2520to%2520other%2520deep%2520RL%2520approaches.%250AFinally%252C%2520we%2520deploy%2520and%2520demonstrate%2520our%2520algorithm%2520on%2520real%2520hardware.%2520For%2520all%250Aevaluation%2520scenarios%2520we%2520provide%2520statistics%2520on%2520the%2520agent%2527s%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.13192v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20based%20Autonomous%20Multi-Rotor%20Landing%20on%20Moving%0A%20%20Platforms&entry.906535625=Pascal%20Goldschmid%20and%20Aamir%20Ahmad&entry.1292438233=%20%20Multi-rotor%20UAVs%20suffer%20from%20a%20restricted%20range%20and%20flight%20duration%20due%20to%0Alimited%20battery%20capacity.%20Autonomous%20landing%20on%20a%202D%20moving%20platform%20offers%20the%0Apossibility%20to%20replenish%20batteries%20and%20offload%20data%2C%20thus%20increasing%20the%0Autility%20of%20the%20vehicle.%20Classical%20approaches%20rely%20on%20accurate%2C%20complex%20and%0Adifficult-to-derive%20models%20of%20the%20vehicle%20and%20the%20environment.%20Reinforcement%0Alearning%20%28RL%29%20provides%20an%20attractive%20alternative%20due%20to%20its%20ability%20to%20learn%20a%0Asuitable%20control%20policy%20exclusively%20from%20data%20during%20a%20training%20procedure.%0AHowever%2C%20current%20methods%20require%20several%20hours%20to%20train%2C%20have%20limited%20success%0Arates%20and%20depend%20on%20hyperparameters%20that%20need%20to%20be%20tuned%20by%20trial-and-error.%0AWe%20address%20all%20these%20issues%20in%20this%20work.%20First%2C%20we%20decompose%20the%20landing%0Aprocedure%20into%20a%20sequence%20of%20simpler%2C%20but%20similar%20learning%20tasks.%20This%20is%0Aenabled%20by%20applying%20two%20instances%20of%20the%20same%20RL%20based%20controller%20trained%20for%0A1D%20motion%20for%20controlling%20the%20multi-rotor%27s%20movement%20in%20both%20the%20longitudinal%0Aand%20the%20lateral%20directions.%20Second%2C%20we%20introduce%20a%20powerful%20state%20space%0Adiscretization%20technique%20that%20is%20based%20on%20i%29%20kinematic%20modeling%20of%20the%20moving%0Aplatform%20to%20derive%20information%20about%20the%20state%20space%20topology%20and%20ii%29%0Astructuring%20the%20training%20as%20a%20sequential%20curriculum%20using%20transfer%20learning.%0AThird%2C%20we%20leverage%20the%20kinematics%20model%20of%20the%20moving%20platform%20to%20also%20derive%0Ainterpretable%20hyperparameters%20for%20the%20training%20process%20that%20ensure%20sufficient%0Amaneuverability%20of%20the%20multi-rotor%20vehicle.%20The%20training%20is%20performed%20using%20the%0Atabular%20RL%20method%20Double%20Q-Learning.%20Through%20extensive%20simulations%20we%20show%20that%0Athe%20presented%20method%20significantly%20increases%20the%20rate%20of%20successful%20landings%2C%0Awhile%20requiring%20less%20training%20time%20compared%20to%20other%20deep%20RL%20approaches.%0AFinally%2C%20we%20deploy%20and%20demonstrate%20our%20algorithm%20on%20real%20hardware.%20For%20all%0Aevaluation%20scenarios%20we%20provide%20statistics%20on%20the%20agent%27s%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.13192v2&entry.124074799=Read"},
{"title": "Gradient-Congruity Guided Federated Sparse Training", "author": "Chris Xing Tian and Yibing Liu and Haoliang Li and Ray C. C. Cheung and Shiqi Wang", "abstract": "  Edge computing allows artificial intelligence and machine learning models to\nbe deployed on edge devices, where they can learn from local data and\ncollaborate to form a global model. Federated learning (FL) is a distributed\nmachine learning technique that facilitates this process while preserving data\nprivacy. However, FL also faces challenges such as high computational and\ncommunication costs regarding resource-constrained devices, and poor\ngeneralization performance due to the heterogeneity of data across edge clients\nand the presence of out-of-distribution data. In this paper, we propose the\nGradient-Congruity Guided Federated Sparse Training (FedSGC), a novel method\nthat integrates dynamic sparse training and gradient congruity inspection into\nfederated learning framework to address these issues. Our method leverages the\nidea that the neurons, in which the associated gradients with conflicting\ndirections with respect to the global model contain irrelevant or less\ngeneralized information for other clients, and could be pruned during the\nsparse training process. Conversely, the neurons where the associated gradients\nwith consistent directions could be grown in a higher priority. In this way,\nFedSGC can greatly reduce the local computation and communication overheads\nwhile, at the same time, enhancing the generalization abilities of FL. We\nevaluate our method on challenging non-i.i.d settings and show that it achieves\ncompetitive accuracy with state-of-the-art FL methods across various scenarios\nwhile minimizing computation and communication costs.\n", "link": "http://arxiv.org/abs/2405.01189v1", "date": "2024-05-02", "relevancy": 2.0753, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.547}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4993}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-Congruity%20Guided%20Federated%20Sparse%20Training&body=Title%3A%20Gradient-Congruity%20Guided%20Federated%20Sparse%20Training%0AAuthor%3A%20Chris%20Xing%20Tian%20and%20Yibing%20Liu%20and%20Haoliang%20Li%20and%20Ray%20C.%20C.%20Cheung%20and%20Shiqi%20Wang%0AAbstract%3A%20%20%20Edge%20computing%20allows%20artificial%20intelligence%20and%20machine%20learning%20models%20to%0Abe%20deployed%20on%20edge%20devices%2C%20where%20they%20can%20learn%20from%20local%20data%20and%0Acollaborate%20to%20form%20a%20global%20model.%20Federated%20learning%20%28FL%29%20is%20a%20distributed%0Amachine%20learning%20technique%20that%20facilitates%20this%20process%20while%20preserving%20data%0Aprivacy.%20However%2C%20FL%20also%20faces%20challenges%20such%20as%20high%20computational%20and%0Acommunication%20costs%20regarding%20resource-constrained%20devices%2C%20and%20poor%0Ageneralization%20performance%20due%20to%20the%20heterogeneity%20of%20data%20across%20edge%20clients%0Aand%20the%20presence%20of%20out-of-distribution%20data.%20In%20this%20paper%2C%20we%20propose%20the%0AGradient-Congruity%20Guided%20Federated%20Sparse%20Training%20%28FedSGC%29%2C%20a%20novel%20method%0Athat%20integrates%20dynamic%20sparse%20training%20and%20gradient%20congruity%20inspection%20into%0Afederated%20learning%20framework%20to%20address%20these%20issues.%20Our%20method%20leverages%20the%0Aidea%20that%20the%20neurons%2C%20in%20which%20the%20associated%20gradients%20with%20conflicting%0Adirections%20with%20respect%20to%20the%20global%20model%20contain%20irrelevant%20or%20less%0Ageneralized%20information%20for%20other%20clients%2C%20and%20could%20be%20pruned%20during%20the%0Asparse%20training%20process.%20Conversely%2C%20the%20neurons%20where%20the%20associated%20gradients%0Awith%20consistent%20directions%20could%20be%20grown%20in%20a%20higher%20priority.%20In%20this%20way%2C%0AFedSGC%20can%20greatly%20reduce%20the%20local%20computation%20and%20communication%20overheads%0Awhile%2C%20at%20the%20same%20time%2C%20enhancing%20the%20generalization%20abilities%20of%20FL.%20We%0Aevaluate%20our%20method%20on%20challenging%20non-i.i.d%20settings%20and%20show%20that%20it%20achieves%0Acompetitive%20accuracy%20with%20state-of-the-art%20FL%20methods%20across%20various%20scenarios%0Awhile%20minimizing%20computation%20and%20communication%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-Congruity%2520Guided%2520Federated%2520Sparse%2520Training%26entry.906535625%3DChris%2520Xing%2520Tian%2520and%2520Yibing%2520Liu%2520and%2520Haoliang%2520Li%2520and%2520Ray%2520C.%2520C.%2520Cheung%2520and%2520Shiqi%2520Wang%26entry.1292438233%3D%2520%2520Edge%2520computing%2520allows%2520artificial%2520intelligence%2520and%2520machine%2520learning%2520models%2520to%250Abe%2520deployed%2520on%2520edge%2520devices%252C%2520where%2520they%2520can%2520learn%2520from%2520local%2520data%2520and%250Acollaborate%2520to%2520form%2520a%2520global%2520model.%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520distributed%250Amachine%2520learning%2520technique%2520that%2520facilitates%2520this%2520process%2520while%2520preserving%2520data%250Aprivacy.%2520However%252C%2520FL%2520also%2520faces%2520challenges%2520such%2520as%2520high%2520computational%2520and%250Acommunication%2520costs%2520regarding%2520resource-constrained%2520devices%252C%2520and%2520poor%250Ageneralization%2520performance%2520due%2520to%2520the%2520heterogeneity%2520of%2520data%2520across%2520edge%2520clients%250Aand%2520the%2520presence%2520of%2520out-of-distribution%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%250AGradient-Congruity%2520Guided%2520Federated%2520Sparse%2520Training%2520%2528FedSGC%2529%252C%2520a%2520novel%2520method%250Athat%2520integrates%2520dynamic%2520sparse%2520training%2520and%2520gradient%2520congruity%2520inspection%2520into%250Afederated%2520learning%2520framework%2520to%2520address%2520these%2520issues.%2520Our%2520method%2520leverages%2520the%250Aidea%2520that%2520the%2520neurons%252C%2520in%2520which%2520the%2520associated%2520gradients%2520with%2520conflicting%250Adirections%2520with%2520respect%2520to%2520the%2520global%2520model%2520contain%2520irrelevant%2520or%2520less%250Ageneralized%2520information%2520for%2520other%2520clients%252C%2520and%2520could%2520be%2520pruned%2520during%2520the%250Asparse%2520training%2520process.%2520Conversely%252C%2520the%2520neurons%2520where%2520the%2520associated%2520gradients%250Awith%2520consistent%2520directions%2520could%2520be%2520grown%2520in%2520a%2520higher%2520priority.%2520In%2520this%2520way%252C%250AFedSGC%2520can%2520greatly%2520reduce%2520the%2520local%2520computation%2520and%2520communication%2520overheads%250Awhile%252C%2520at%2520the%2520same%2520time%252C%2520enhancing%2520the%2520generalization%2520abilities%2520of%2520FL.%2520We%250Aevaluate%2520our%2520method%2520on%2520challenging%2520non-i.i.d%2520settings%2520and%2520show%2520that%2520it%2520achieves%250Acompetitive%2520accuracy%2520with%2520state-of-the-art%2520FL%2520methods%2520across%2520various%2520scenarios%250Awhile%2520minimizing%2520computation%2520and%2520communication%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Congruity%20Guided%20Federated%20Sparse%20Training&entry.906535625=Chris%20Xing%20Tian%20and%20Yibing%20Liu%20and%20Haoliang%20Li%20and%20Ray%20C.%20C.%20Cheung%20and%20Shiqi%20Wang&entry.1292438233=%20%20Edge%20computing%20allows%20artificial%20intelligence%20and%20machine%20learning%20models%20to%0Abe%20deployed%20on%20edge%20devices%2C%20where%20they%20can%20learn%20from%20local%20data%20and%0Acollaborate%20to%20form%20a%20global%20model.%20Federated%20learning%20%28FL%29%20is%20a%20distributed%0Amachine%20learning%20technique%20that%20facilitates%20this%20process%20while%20preserving%20data%0Aprivacy.%20However%2C%20FL%20also%20faces%20challenges%20such%20as%20high%20computational%20and%0Acommunication%20costs%20regarding%20resource-constrained%20devices%2C%20and%20poor%0Ageneralization%20performance%20due%20to%20the%20heterogeneity%20of%20data%20across%20edge%20clients%0Aand%20the%20presence%20of%20out-of-distribution%20data.%20In%20this%20paper%2C%20we%20propose%20the%0AGradient-Congruity%20Guided%20Federated%20Sparse%20Training%20%28FedSGC%29%2C%20a%20novel%20method%0Athat%20integrates%20dynamic%20sparse%20training%20and%20gradient%20congruity%20inspection%20into%0Afederated%20learning%20framework%20to%20address%20these%20issues.%20Our%20method%20leverages%20the%0Aidea%20that%20the%20neurons%2C%20in%20which%20the%20associated%20gradients%20with%20conflicting%0Adirections%20with%20respect%20to%20the%20global%20model%20contain%20irrelevant%20or%20less%0Ageneralized%20information%20for%20other%20clients%2C%20and%20could%20be%20pruned%20during%20the%0Asparse%20training%20process.%20Conversely%2C%20the%20neurons%20where%20the%20associated%20gradients%0Awith%20consistent%20directions%20could%20be%20grown%20in%20a%20higher%20priority.%20In%20this%20way%2C%0AFedSGC%20can%20greatly%20reduce%20the%20local%20computation%20and%20communication%20overheads%0Awhile%2C%20at%20the%20same%20time%2C%20enhancing%20the%20generalization%20abilities%20of%20FL.%20We%0Aevaluate%20our%20method%20on%20challenging%20non-i.i.d%20settings%20and%20show%20that%20it%20achieves%0Acompetitive%20accuracy%20with%20state-of-the-art%20FL%20methods%20across%20various%20scenarios%0Awhile%20minimizing%20computation%20and%20communication%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01189v1&entry.124074799=Read"},
{"title": "Fingerprint Matching with Localized Deep Representation", "author": "Yongjie Duan and Zhiyu Pan and Jianjiang Feng and Jie Zhou", "abstract": "  Compared to minutia-based fingerprint representations, fixed-length\nrepresentations are attractive due to simple and efficient matching. However,\nfixed-length fingerprint representations are limited in accuracy when matching\nfingerprints with different visible areas, which can occur due to different\nfinger poses or acquisition methods. To address this issue, we propose a\nlocalized deep representation of fingerprint, named LDRF. By focusing on the\ndiscriminative characteristics within local regions, LDRF provides a more\nrobust and accurate fixed-length representation for fingerprints with variable\nvisible areas. LDRF can be adapted to retain information within any valid area,\nmaking it highly flexible. The matching scores produced by LDRF also exhibit\nintuitive statistical characteristics, which led us to propose a matching score\nnormalization technique to mitigate the uncertainty in the cases of very small\noverlapping area. With this new technique, we can maintain a high level of\naccuracy and reliability in our fingerprint matching, even as the size of the\ndatabase grows rapidly. Our experimental results on 21 datasets containing over\n140K fingerprints of various finger poses and impression types show that LDRF\noutperforms other fixed-length representations and is robust to sensing\ntechnologies and impression types. Besides, the proposed matching score\nnormalization effectively reduces the false match rate (FMR) in large-scale\nidentification experiments comprising over 5.11 million fingerprints.\nSpecifically, this technique results in a reduction of two orders of magnitude\ncompared to matching without matching score normalization and five orders of\nmagnitude compared to prior works.\n", "link": "http://arxiv.org/abs/2311.18576v2", "date": "2024-05-02", "relevancy": 2.0661, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5766}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4751}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fingerprint%20Matching%20with%20Localized%20Deep%20Representation&body=Title%3A%20Fingerprint%20Matching%20with%20Localized%20Deep%20Representation%0AAuthor%3A%20Yongjie%20Duan%20and%20Zhiyu%20Pan%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20Compared%20to%20minutia-based%20fingerprint%20representations%2C%20fixed-length%0Arepresentations%20are%20attractive%20due%20to%20simple%20and%20efficient%20matching.%20However%2C%0Afixed-length%20fingerprint%20representations%20are%20limited%20in%20accuracy%20when%20matching%0Afingerprints%20with%20different%20visible%20areas%2C%20which%20can%20occur%20due%20to%20different%0Afinger%20poses%20or%20acquisition%20methods.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Alocalized%20deep%20representation%20of%20fingerprint%2C%20named%20LDRF.%20By%20focusing%20on%20the%0Adiscriminative%20characteristics%20within%20local%20regions%2C%20LDRF%20provides%20a%20more%0Arobust%20and%20accurate%20fixed-length%20representation%20for%20fingerprints%20with%20variable%0Avisible%20areas.%20LDRF%20can%20be%20adapted%20to%20retain%20information%20within%20any%20valid%20area%2C%0Amaking%20it%20highly%20flexible.%20The%20matching%20scores%20produced%20by%20LDRF%20also%20exhibit%0Aintuitive%20statistical%20characteristics%2C%20which%20led%20us%20to%20propose%20a%20matching%20score%0Anormalization%20technique%20to%20mitigate%20the%20uncertainty%20in%20the%20cases%20of%20very%20small%0Aoverlapping%20area.%20With%20this%20new%20technique%2C%20we%20can%20maintain%20a%20high%20level%20of%0Aaccuracy%20and%20reliability%20in%20our%20fingerprint%20matching%2C%20even%20as%20the%20size%20of%20the%0Adatabase%20grows%20rapidly.%20Our%20experimental%20results%20on%2021%20datasets%20containing%20over%0A140K%20fingerprints%20of%20various%20finger%20poses%20and%20impression%20types%20show%20that%20LDRF%0Aoutperforms%20other%20fixed-length%20representations%20and%20is%20robust%20to%20sensing%0Atechnologies%20and%20impression%20types.%20Besides%2C%20the%20proposed%20matching%20score%0Anormalization%20effectively%20reduces%20the%20false%20match%20rate%20%28FMR%29%20in%20large-scale%0Aidentification%20experiments%20comprising%20over%205.11%20million%20fingerprints.%0ASpecifically%2C%20this%20technique%20results%20in%20a%20reduction%20of%20two%20orders%20of%20magnitude%0Acompared%20to%20matching%20without%20matching%20score%20normalization%20and%20five%20orders%20of%0Amagnitude%20compared%20to%20prior%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18576v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFingerprint%2520Matching%2520with%2520Localized%2520Deep%2520Representation%26entry.906535625%3DYongjie%2520Duan%2520and%2520Zhiyu%2520Pan%2520and%2520Jianjiang%2520Feng%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520Compared%2520to%2520minutia-based%2520fingerprint%2520representations%252C%2520fixed-length%250Arepresentations%2520are%2520attractive%2520due%2520to%2520simple%2520and%2520efficient%2520matching.%2520However%252C%250Afixed-length%2520fingerprint%2520representations%2520are%2520limited%2520in%2520accuracy%2520when%2520matching%250Afingerprints%2520with%2520different%2520visible%2520areas%252C%2520which%2520can%2520occur%2520due%2520to%2520different%250Afinger%2520poses%2520or%2520acquisition%2520methods.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Alocalized%2520deep%2520representation%2520of%2520fingerprint%252C%2520named%2520LDRF.%2520By%2520focusing%2520on%2520the%250Adiscriminative%2520characteristics%2520within%2520local%2520regions%252C%2520LDRF%2520provides%2520a%2520more%250Arobust%2520and%2520accurate%2520fixed-length%2520representation%2520for%2520fingerprints%2520with%2520variable%250Avisible%2520areas.%2520LDRF%2520can%2520be%2520adapted%2520to%2520retain%2520information%2520within%2520any%2520valid%2520area%252C%250Amaking%2520it%2520highly%2520flexible.%2520The%2520matching%2520scores%2520produced%2520by%2520LDRF%2520also%2520exhibit%250Aintuitive%2520statistical%2520characteristics%252C%2520which%2520led%2520us%2520to%2520propose%2520a%2520matching%2520score%250Anormalization%2520technique%2520to%2520mitigate%2520the%2520uncertainty%2520in%2520the%2520cases%2520of%2520very%2520small%250Aoverlapping%2520area.%2520With%2520this%2520new%2520technique%252C%2520we%2520can%2520maintain%2520a%2520high%2520level%2520of%250Aaccuracy%2520and%2520reliability%2520in%2520our%2520fingerprint%2520matching%252C%2520even%2520as%2520the%2520size%2520of%2520the%250Adatabase%2520grows%2520rapidly.%2520Our%2520experimental%2520results%2520on%252021%2520datasets%2520containing%2520over%250A140K%2520fingerprints%2520of%2520various%2520finger%2520poses%2520and%2520impression%2520types%2520show%2520that%2520LDRF%250Aoutperforms%2520other%2520fixed-length%2520representations%2520and%2520is%2520robust%2520to%2520sensing%250Atechnologies%2520and%2520impression%2520types.%2520Besides%252C%2520the%2520proposed%2520matching%2520score%250Anormalization%2520effectively%2520reduces%2520the%2520false%2520match%2520rate%2520%2528FMR%2529%2520in%2520large-scale%250Aidentification%2520experiments%2520comprising%2520over%25205.11%2520million%2520fingerprints.%250ASpecifically%252C%2520this%2520technique%2520results%2520in%2520a%2520reduction%2520of%2520two%2520orders%2520of%2520magnitude%250Acompared%2520to%2520matching%2520without%2520matching%2520score%2520normalization%2520and%2520five%2520orders%2520of%250Amagnitude%2520compared%2520to%2520prior%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18576v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fingerprint%20Matching%20with%20Localized%20Deep%20Representation&entry.906535625=Yongjie%20Duan%20and%20Zhiyu%20Pan%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou&entry.1292438233=%20%20Compared%20to%20minutia-based%20fingerprint%20representations%2C%20fixed-length%0Arepresentations%20are%20attractive%20due%20to%20simple%20and%20efficient%20matching.%20However%2C%0Afixed-length%20fingerprint%20representations%20are%20limited%20in%20accuracy%20when%20matching%0Afingerprints%20with%20different%20visible%20areas%2C%20which%20can%20occur%20due%20to%20different%0Afinger%20poses%20or%20acquisition%20methods.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Alocalized%20deep%20representation%20of%20fingerprint%2C%20named%20LDRF.%20By%20focusing%20on%20the%0Adiscriminative%20characteristics%20within%20local%20regions%2C%20LDRF%20provides%20a%20more%0Arobust%20and%20accurate%20fixed-length%20representation%20for%20fingerprints%20with%20variable%0Avisible%20areas.%20LDRF%20can%20be%20adapted%20to%20retain%20information%20within%20any%20valid%20area%2C%0Amaking%20it%20highly%20flexible.%20The%20matching%20scores%20produced%20by%20LDRF%20also%20exhibit%0Aintuitive%20statistical%20characteristics%2C%20which%20led%20us%20to%20propose%20a%20matching%20score%0Anormalization%20technique%20to%20mitigate%20the%20uncertainty%20in%20the%20cases%20of%20very%20small%0Aoverlapping%20area.%20With%20this%20new%20technique%2C%20we%20can%20maintain%20a%20high%20level%20of%0Aaccuracy%20and%20reliability%20in%20our%20fingerprint%20matching%2C%20even%20as%20the%20size%20of%20the%0Adatabase%20grows%20rapidly.%20Our%20experimental%20results%20on%2021%20datasets%20containing%20over%0A140K%20fingerprints%20of%20various%20finger%20poses%20and%20impression%20types%20show%20that%20LDRF%0Aoutperforms%20other%20fixed-length%20representations%20and%20is%20robust%20to%20sensing%0Atechnologies%20and%20impression%20types.%20Besides%2C%20the%20proposed%20matching%20score%0Anormalization%20effectively%20reduces%20the%20false%20match%20rate%20%28FMR%29%20in%20large-scale%0Aidentification%20experiments%20comprising%20over%205.11%20million%20fingerprints.%0ASpecifically%2C%20this%20technique%20results%20in%20a%20reduction%20of%20two%20orders%20of%20magnitude%0Acompared%20to%20matching%20without%20matching%20score%20normalization%20and%20five%20orders%20of%0Amagnitude%20compared%20to%20prior%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18576v2&entry.124074799=Read"},
{"title": "DMON: A Simple yet Effective Approach for Argument Structure Learning", "author": "Wei Sun and Mingxiao Li and Jingyuan Sun and Jesse Davis and Marie-Francine Moens", "abstract": "  Argument structure learning~(ASL) entails predicting relations between\narguments. Because it can structure a document to facilitate its understanding,\nit has been widely applied in many fields~(medical, commercial, and scientific\ndomains). Despite its broad utilization, ASL remains a challenging task because\nit involves examining the complex relationships between the sentences in a\npotentially unstructured discourse. To resolve this problem, we have developed\na simple yet effective approach called Dual-tower Multi-scale cOnvolution\nneural Network~(DMON) for the ASL task. Specifically, we organize arguments\ninto a relationship matrix that together with the argument embeddings forms a\nrelationship tensor and design a mechanism to capture relations with contextual\narguments. Experimental results on three different-domain argument mining\ndatasets demonstrate that our framework outperforms state-of-the-art models.\nThe code is available at https://github.com/VRCMF/DMON.git .\n", "link": "http://arxiv.org/abs/2405.01216v1", "date": "2024-05-02", "relevancy": 2.063, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5428}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5037}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DMON%3A%20A%20Simple%20yet%20Effective%20Approach%20for%20Argument%20Structure%20Learning&body=Title%3A%20DMON%3A%20A%20Simple%20yet%20Effective%20Approach%20for%20Argument%20Structure%20Learning%0AAuthor%3A%20Wei%20Sun%20and%20Mingxiao%20Li%20and%20Jingyuan%20Sun%20and%20Jesse%20Davis%20and%20Marie-Francine%20Moens%0AAbstract%3A%20%20%20Argument%20structure%20learning~%28ASL%29%20entails%20predicting%20relations%20between%0Aarguments.%20Because%20it%20can%20structure%20a%20document%20to%20facilitate%20its%20understanding%2C%0Ait%20has%20been%20widely%20applied%20in%20many%20fields~%28medical%2C%20commercial%2C%20and%20scientific%0Adomains%29.%20Despite%20its%20broad%20utilization%2C%20ASL%20remains%20a%20challenging%20task%20because%0Ait%20involves%20examining%20the%20complex%20relationships%20between%20the%20sentences%20in%20a%0Apotentially%20unstructured%20discourse.%20To%20resolve%20this%20problem%2C%20we%20have%20developed%0Aa%20simple%20yet%20effective%20approach%20called%20Dual-tower%20Multi-scale%20cOnvolution%0Aneural%20Network~%28DMON%29%20for%20the%20ASL%20task.%20Specifically%2C%20we%20organize%20arguments%0Ainto%20a%20relationship%20matrix%20that%20together%20with%20the%20argument%20embeddings%20forms%20a%0Arelationship%20tensor%20and%20design%20a%20mechanism%20to%20capture%20relations%20with%20contextual%0Aarguments.%20Experimental%20results%20on%20three%20different-domain%20argument%20mining%0Adatasets%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20models.%0AThe%20code%20is%20available%20at%20https%3A//github.com/VRCMF/DMON.git%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDMON%253A%2520A%2520Simple%2520yet%2520Effective%2520Approach%2520for%2520Argument%2520Structure%2520Learning%26entry.906535625%3DWei%2520Sun%2520and%2520Mingxiao%2520Li%2520and%2520Jingyuan%2520Sun%2520and%2520Jesse%2520Davis%2520and%2520Marie-Francine%2520Moens%26entry.1292438233%3D%2520%2520Argument%2520structure%2520learning~%2528ASL%2529%2520entails%2520predicting%2520relations%2520between%250Aarguments.%2520Because%2520it%2520can%2520structure%2520a%2520document%2520to%2520facilitate%2520its%2520understanding%252C%250Ait%2520has%2520been%2520widely%2520applied%2520in%2520many%2520fields~%2528medical%252C%2520commercial%252C%2520and%2520scientific%250Adomains%2529.%2520Despite%2520its%2520broad%2520utilization%252C%2520ASL%2520remains%2520a%2520challenging%2520task%2520because%250Ait%2520involves%2520examining%2520the%2520complex%2520relationships%2520between%2520the%2520sentences%2520in%2520a%250Apotentially%2520unstructured%2520discourse.%2520To%2520resolve%2520this%2520problem%252C%2520we%2520have%2520developed%250Aa%2520simple%2520yet%2520effective%2520approach%2520called%2520Dual-tower%2520Multi-scale%2520cOnvolution%250Aneural%2520Network~%2528DMON%2529%2520for%2520the%2520ASL%2520task.%2520Specifically%252C%2520we%2520organize%2520arguments%250Ainto%2520a%2520relationship%2520matrix%2520that%2520together%2520with%2520the%2520argument%2520embeddings%2520forms%2520a%250Arelationship%2520tensor%2520and%2520design%2520a%2520mechanism%2520to%2520capture%2520relations%2520with%2520contextual%250Aarguments.%2520Experimental%2520results%2520on%2520three%2520different-domain%2520argument%2520mining%250Adatasets%2520demonstrate%2520that%2520our%2520framework%2520outperforms%2520state-of-the-art%2520models.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/VRCMF/DMON.git%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DMON%3A%20A%20Simple%20yet%20Effective%20Approach%20for%20Argument%20Structure%20Learning&entry.906535625=Wei%20Sun%20and%20Mingxiao%20Li%20and%20Jingyuan%20Sun%20and%20Jesse%20Davis%20and%20Marie-Francine%20Moens&entry.1292438233=%20%20Argument%20structure%20learning~%28ASL%29%20entails%20predicting%20relations%20between%0Aarguments.%20Because%20it%20can%20structure%20a%20document%20to%20facilitate%20its%20understanding%2C%0Ait%20has%20been%20widely%20applied%20in%20many%20fields~%28medical%2C%20commercial%2C%20and%20scientific%0Adomains%29.%20Despite%20its%20broad%20utilization%2C%20ASL%20remains%20a%20challenging%20task%20because%0Ait%20involves%20examining%20the%20complex%20relationships%20between%20the%20sentences%20in%20a%0Apotentially%20unstructured%20discourse.%20To%20resolve%20this%20problem%2C%20we%20have%20developed%0Aa%20simple%20yet%20effective%20approach%20called%20Dual-tower%20Multi-scale%20cOnvolution%0Aneural%20Network~%28DMON%29%20for%20the%20ASL%20task.%20Specifically%2C%20we%20organize%20arguments%0Ainto%20a%20relationship%20matrix%20that%20together%20with%20the%20argument%20embeddings%20forms%20a%0Arelationship%20tensor%20and%20design%20a%20mechanism%20to%20capture%20relations%20with%20contextual%0Aarguments.%20Experimental%20results%20on%20three%20different-domain%20argument%20mining%0Adatasets%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20models.%0AThe%20code%20is%20available%20at%20https%3A//github.com/VRCMF/DMON.git%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01216v1&entry.124074799=Read"},
{"title": "StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video\n  Generation", "author": "Yupeng Zhou and Daquan Zhou and Ming-Ming Cheng and Jiashi Feng and Qibin Hou", "abstract": "  For recent diffusion-based generative models, maintaining consistent content\nacross a series of generated images, especially those containing subjects and\ncomplex details, presents a significant challenge. In this paper, we propose a\nnew way of self-attention calculation, termed Consistent Self-Attention, that\nsignificantly boosts the consistency between the generated images and augments\nprevalent pretrained diffusion-based text-to-image models in a zero-shot\nmanner. To extend our method to long-range video generation, we further\nintroduce a novel semantic space temporal motion prediction module, named\nSemantic Motion Predictor. It is trained to estimate the motion conditions\nbetween two provided images in the semantic spaces. This module converts the\ngenerated sequence of images into videos with smooth transitions and consistent\nsubjects that are significantly more stable than the modules based on latent\nspaces only, especially in the context of long video generation. By merging\nthese two novel components, our framework, referred to as StoryDiffusion, can\ndescribe a text-based story with consistent images or videos encompassing a\nrich variety of contents. The proposed StoryDiffusion encompasses pioneering\nexplorations in visual story generation with the presentation of images and\nvideos, which we hope could inspire more research from the aspect of\narchitectural modifications. Our code is made publicly available at\nhttps://github.com/HVision-NKU/StoryDiffusion.\n", "link": "http://arxiv.org/abs/2405.01434v1", "date": "2024-05-02", "relevancy": 2.0551, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7072}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6851}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StoryDiffusion%3A%20Consistent%20Self-Attention%20for%20Long-Range%20Image%20and%20Video%0A%20%20Generation&body=Title%3A%20StoryDiffusion%3A%20Consistent%20Self-Attention%20for%20Long-Range%20Image%20and%20Video%0A%20%20Generation%0AAuthor%3A%20Yupeng%20Zhou%20and%20Daquan%20Zhou%20and%20Ming-Ming%20Cheng%20and%20Jiashi%20Feng%20and%20Qibin%20Hou%0AAbstract%3A%20%20%20For%20recent%20diffusion-based%20generative%20models%2C%20maintaining%20consistent%20content%0Aacross%20a%20series%20of%20generated%20images%2C%20especially%20those%20containing%20subjects%20and%0Acomplex%20details%2C%20presents%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20propose%20a%0Anew%20way%20of%20self-attention%20calculation%2C%20termed%20Consistent%20Self-Attention%2C%20that%0Asignificantly%20boosts%20the%20consistency%20between%20the%20generated%20images%20and%20augments%0Aprevalent%20pretrained%20diffusion-based%20text-to-image%20models%20in%20a%20zero-shot%0Amanner.%20To%20extend%20our%20method%20to%20long-range%20video%20generation%2C%20we%20further%0Aintroduce%20a%20novel%20semantic%20space%20temporal%20motion%20prediction%20module%2C%20named%0ASemantic%20Motion%20Predictor.%20It%20is%20trained%20to%20estimate%20the%20motion%20conditions%0Abetween%20two%20provided%20images%20in%20the%20semantic%20spaces.%20This%20module%20converts%20the%0Agenerated%20sequence%20of%20images%20into%20videos%20with%20smooth%20transitions%20and%20consistent%0Asubjects%20that%20are%20significantly%20more%20stable%20than%20the%20modules%20based%20on%20latent%0Aspaces%20only%2C%20especially%20in%20the%20context%20of%20long%20video%20generation.%20By%20merging%0Athese%20two%20novel%20components%2C%20our%20framework%2C%20referred%20to%20as%20StoryDiffusion%2C%20can%0Adescribe%20a%20text-based%20story%20with%20consistent%20images%20or%20videos%20encompassing%20a%0Arich%20variety%20of%20contents.%20The%20proposed%20StoryDiffusion%20encompasses%20pioneering%0Aexplorations%20in%20visual%20story%20generation%20with%20the%20presentation%20of%20images%20and%0Avideos%2C%20which%20we%20hope%20could%20inspire%20more%20research%20from%20the%20aspect%20of%0Aarchitectural%20modifications.%20Our%20code%20is%20made%20publicly%20available%20at%0Ahttps%3A//github.com/HVision-NKU/StoryDiffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStoryDiffusion%253A%2520Consistent%2520Self-Attention%2520for%2520Long-Range%2520Image%2520and%2520Video%250A%2520%2520Generation%26entry.906535625%3DYupeng%2520Zhou%2520and%2520Daquan%2520Zhou%2520and%2520Ming-Ming%2520Cheng%2520and%2520Jiashi%2520Feng%2520and%2520Qibin%2520Hou%26entry.1292438233%3D%2520%2520For%2520recent%2520diffusion-based%2520generative%2520models%252C%2520maintaining%2520consistent%2520content%250Aacross%2520a%2520series%2520of%2520generated%2520images%252C%2520especially%2520those%2520containing%2520subjects%2520and%250Acomplex%2520details%252C%2520presents%2520a%2520significant%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anew%2520way%2520of%2520self-attention%2520calculation%252C%2520termed%2520Consistent%2520Self-Attention%252C%2520that%250Asignificantly%2520boosts%2520the%2520consistency%2520between%2520the%2520generated%2520images%2520and%2520augments%250Aprevalent%2520pretrained%2520diffusion-based%2520text-to-image%2520models%2520in%2520a%2520zero-shot%250Amanner.%2520To%2520extend%2520our%2520method%2520to%2520long-range%2520video%2520generation%252C%2520we%2520further%250Aintroduce%2520a%2520novel%2520semantic%2520space%2520temporal%2520motion%2520prediction%2520module%252C%2520named%250ASemantic%2520Motion%2520Predictor.%2520It%2520is%2520trained%2520to%2520estimate%2520the%2520motion%2520conditions%250Abetween%2520two%2520provided%2520images%2520in%2520the%2520semantic%2520spaces.%2520This%2520module%2520converts%2520the%250Agenerated%2520sequence%2520of%2520images%2520into%2520videos%2520with%2520smooth%2520transitions%2520and%2520consistent%250Asubjects%2520that%2520are%2520significantly%2520more%2520stable%2520than%2520the%2520modules%2520based%2520on%2520latent%250Aspaces%2520only%252C%2520especially%2520in%2520the%2520context%2520of%2520long%2520video%2520generation.%2520By%2520merging%250Athese%2520two%2520novel%2520components%252C%2520our%2520framework%252C%2520referred%2520to%2520as%2520StoryDiffusion%252C%2520can%250Adescribe%2520a%2520text-based%2520story%2520with%2520consistent%2520images%2520or%2520videos%2520encompassing%2520a%250Arich%2520variety%2520of%2520contents.%2520The%2520proposed%2520StoryDiffusion%2520encompasses%2520pioneering%250Aexplorations%2520in%2520visual%2520story%2520generation%2520with%2520the%2520presentation%2520of%2520images%2520and%250Avideos%252C%2520which%2520we%2520hope%2520could%2520inspire%2520more%2520research%2520from%2520the%2520aspect%2520of%250Aarchitectural%2520modifications.%2520Our%2520code%2520is%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/HVision-NKU/StoryDiffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StoryDiffusion%3A%20Consistent%20Self-Attention%20for%20Long-Range%20Image%20and%20Video%0A%20%20Generation&entry.906535625=Yupeng%20Zhou%20and%20Daquan%20Zhou%20and%20Ming-Ming%20Cheng%20and%20Jiashi%20Feng%20and%20Qibin%20Hou&entry.1292438233=%20%20For%20recent%20diffusion-based%20generative%20models%2C%20maintaining%20consistent%20content%0Aacross%20a%20series%20of%20generated%20images%2C%20especially%20those%20containing%20subjects%20and%0Acomplex%20details%2C%20presents%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20propose%20a%0Anew%20way%20of%20self-attention%20calculation%2C%20termed%20Consistent%20Self-Attention%2C%20that%0Asignificantly%20boosts%20the%20consistency%20between%20the%20generated%20images%20and%20augments%0Aprevalent%20pretrained%20diffusion-based%20text-to-image%20models%20in%20a%20zero-shot%0Amanner.%20To%20extend%20our%20method%20to%20long-range%20video%20generation%2C%20we%20further%0Aintroduce%20a%20novel%20semantic%20space%20temporal%20motion%20prediction%20module%2C%20named%0ASemantic%20Motion%20Predictor.%20It%20is%20trained%20to%20estimate%20the%20motion%20conditions%0Abetween%20two%20provided%20images%20in%20the%20semantic%20spaces.%20This%20module%20converts%20the%0Agenerated%20sequence%20of%20images%20into%20videos%20with%20smooth%20transitions%20and%20consistent%0Asubjects%20that%20are%20significantly%20more%20stable%20than%20the%20modules%20based%20on%20latent%0Aspaces%20only%2C%20especially%20in%20the%20context%20of%20long%20video%20generation.%20By%20merging%0Athese%20two%20novel%20components%2C%20our%20framework%2C%20referred%20to%20as%20StoryDiffusion%2C%20can%0Adescribe%20a%20text-based%20story%20with%20consistent%20images%20or%20videos%20encompassing%20a%0Arich%20variety%20of%20contents.%20The%20proposed%20StoryDiffusion%20encompasses%20pioneering%0Aexplorations%20in%20visual%20story%20generation%20with%20the%20presentation%20of%20images%20and%0Avideos%2C%20which%20we%20hope%20could%20inspire%20more%20research%20from%20the%20aspect%20of%0Aarchitectural%20modifications.%20Our%20code%20is%20made%20publicly%20available%20at%0Ahttps%3A//github.com/HVision-NKU/StoryDiffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01434v1&entry.124074799=Read"},
{"title": "MFTraj: Map-Free, Behavior-Driven Trajectory Prediction for Autonomous\n  Driving", "author": "Haicheng Liao and Zhenning Li and Chengyue Wang and Huanming Shen and Bonan Wang and Dongping Liao and Guofa Li and Chengzhong Xu", "abstract": "  This paper introduces a trajectory prediction model tailored for autonomous\ndriving, focusing on capturing complex interactions in dynamic traffic\nscenarios without reliance on high-definition maps. The model, termed MFTraj,\nharnesses historical trajectory data combined with a novel dynamic geometric\ngraph-based behavior-aware module. At its core, an adaptive structure-aware\ninteractive graph convolutional network captures both positional and behavioral\nfeatures of road users, preserving spatial-temporal intricacies. Enhanced by a\nlinear attention mechanism, the model achieves computational efficiency and\nreduced parameter overhead. Evaluations on the Argoverse, NGSIM, HighD, and\nMoCAD datasets underscore MFTraj's robustness and adaptability, outperforming\nnumerous benchmarks even in data-challenged scenarios without the need for\nadditional information such as HD maps or vectorized maps. Importantly, it\nmaintains competitive performance even in scenarios with substantial missing\ndata, on par with most existing state-of-the-art models. The results and\nmethodology suggest a significant advancement in autonomous driving trajectory\nprediction, paving the way for safer and more efficient autonomous systems.\n", "link": "http://arxiv.org/abs/2405.01266v1", "date": "2024-05-02", "relevancy": 2.0493, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5776}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.508}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MFTraj%3A%20Map-Free%2C%20Behavior-Driven%20Trajectory%20Prediction%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20MFTraj%3A%20Map-Free%2C%20Behavior-Driven%20Trajectory%20Prediction%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Haicheng%20Liao%20and%20Zhenning%20Li%20and%20Chengyue%20Wang%20and%20Huanming%20Shen%20and%20Bonan%20Wang%20and%20Dongping%20Liao%20and%20Guofa%20Li%20and%20Chengzhong%20Xu%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20trajectory%20prediction%20model%20tailored%20for%20autonomous%0Adriving%2C%20focusing%20on%20capturing%20complex%20interactions%20in%20dynamic%20traffic%0Ascenarios%20without%20reliance%20on%20high-definition%20maps.%20The%20model%2C%20termed%20MFTraj%2C%0Aharnesses%20historical%20trajectory%20data%20combined%20with%20a%20novel%20dynamic%20geometric%0Agraph-based%20behavior-aware%20module.%20At%20its%20core%2C%20an%20adaptive%20structure-aware%0Ainteractive%20graph%20convolutional%20network%20captures%20both%20positional%20and%20behavioral%0Afeatures%20of%20road%20users%2C%20preserving%20spatial-temporal%20intricacies.%20Enhanced%20by%20a%0Alinear%20attention%20mechanism%2C%20the%20model%20achieves%20computational%20efficiency%20and%0Areduced%20parameter%20overhead.%20Evaluations%20on%20the%20Argoverse%2C%20NGSIM%2C%20HighD%2C%20and%0AMoCAD%20datasets%20underscore%20MFTraj%27s%20robustness%20and%20adaptability%2C%20outperforming%0Anumerous%20benchmarks%20even%20in%20data-challenged%20scenarios%20without%20the%20need%20for%0Aadditional%20information%20such%20as%20HD%20maps%20or%20vectorized%20maps.%20Importantly%2C%20it%0Amaintains%20competitive%20performance%20even%20in%20scenarios%20with%20substantial%20missing%0Adata%2C%20on%20par%20with%20most%20existing%20state-of-the-art%20models.%20The%20results%20and%0Amethodology%20suggest%20a%20significant%20advancement%20in%20autonomous%20driving%20trajectory%0Aprediction%2C%20paving%20the%20way%20for%20safer%20and%20more%20efficient%20autonomous%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMFTraj%253A%2520Map-Free%252C%2520Behavior-Driven%2520Trajectory%2520Prediction%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DHaicheng%2520Liao%2520and%2520Zhenning%2520Li%2520and%2520Chengyue%2520Wang%2520and%2520Huanming%2520Shen%2520and%2520Bonan%2520Wang%2520and%2520Dongping%2520Liao%2520and%2520Guofa%2520Li%2520and%2520Chengzhong%2520Xu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520trajectory%2520prediction%2520model%2520tailored%2520for%2520autonomous%250Adriving%252C%2520focusing%2520on%2520capturing%2520complex%2520interactions%2520in%2520dynamic%2520traffic%250Ascenarios%2520without%2520reliance%2520on%2520high-definition%2520maps.%2520The%2520model%252C%2520termed%2520MFTraj%252C%250Aharnesses%2520historical%2520trajectory%2520data%2520combined%2520with%2520a%2520novel%2520dynamic%2520geometric%250Agraph-based%2520behavior-aware%2520module.%2520At%2520its%2520core%252C%2520an%2520adaptive%2520structure-aware%250Ainteractive%2520graph%2520convolutional%2520network%2520captures%2520both%2520positional%2520and%2520behavioral%250Afeatures%2520of%2520road%2520users%252C%2520preserving%2520spatial-temporal%2520intricacies.%2520Enhanced%2520by%2520a%250Alinear%2520attention%2520mechanism%252C%2520the%2520model%2520achieves%2520computational%2520efficiency%2520and%250Areduced%2520parameter%2520overhead.%2520Evaluations%2520on%2520the%2520Argoverse%252C%2520NGSIM%252C%2520HighD%252C%2520and%250AMoCAD%2520datasets%2520underscore%2520MFTraj%2527s%2520robustness%2520and%2520adaptability%252C%2520outperforming%250Anumerous%2520benchmarks%2520even%2520in%2520data-challenged%2520scenarios%2520without%2520the%2520need%2520for%250Aadditional%2520information%2520such%2520as%2520HD%2520maps%2520or%2520vectorized%2520maps.%2520Importantly%252C%2520it%250Amaintains%2520competitive%2520performance%2520even%2520in%2520scenarios%2520with%2520substantial%2520missing%250Adata%252C%2520on%2520par%2520with%2520most%2520existing%2520state-of-the-art%2520models.%2520The%2520results%2520and%250Amethodology%2520suggest%2520a%2520significant%2520advancement%2520in%2520autonomous%2520driving%2520trajectory%250Aprediction%252C%2520paving%2520the%2520way%2520for%2520safer%2520and%2520more%2520efficient%2520autonomous%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MFTraj%3A%20Map-Free%2C%20Behavior-Driven%20Trajectory%20Prediction%20for%20Autonomous%0A%20%20Driving&entry.906535625=Haicheng%20Liao%20and%20Zhenning%20Li%20and%20Chengyue%20Wang%20and%20Huanming%20Shen%20and%20Bonan%20Wang%20and%20Dongping%20Liao%20and%20Guofa%20Li%20and%20Chengzhong%20Xu&entry.1292438233=%20%20This%20paper%20introduces%20a%20trajectory%20prediction%20model%20tailored%20for%20autonomous%0Adriving%2C%20focusing%20on%20capturing%20complex%20interactions%20in%20dynamic%20traffic%0Ascenarios%20without%20reliance%20on%20high-definition%20maps.%20The%20model%2C%20termed%20MFTraj%2C%0Aharnesses%20historical%20trajectory%20data%20combined%20with%20a%20novel%20dynamic%20geometric%0Agraph-based%20behavior-aware%20module.%20At%20its%20core%2C%20an%20adaptive%20structure-aware%0Ainteractive%20graph%20convolutional%20network%20captures%20both%20positional%20and%20behavioral%0Afeatures%20of%20road%20users%2C%20preserving%20spatial-temporal%20intricacies.%20Enhanced%20by%20a%0Alinear%20attention%20mechanism%2C%20the%20model%20achieves%20computational%20efficiency%20and%0Areduced%20parameter%20overhead.%20Evaluations%20on%20the%20Argoverse%2C%20NGSIM%2C%20HighD%2C%20and%0AMoCAD%20datasets%20underscore%20MFTraj%27s%20robustness%20and%20adaptability%2C%20outperforming%0Anumerous%20benchmarks%20even%20in%20data-challenged%20scenarios%20without%20the%20need%20for%0Aadditional%20information%20such%20as%20HD%20maps%20or%20vectorized%20maps.%20Importantly%2C%20it%0Amaintains%20competitive%20performance%20even%20in%20scenarios%20with%20substantial%20missing%0Adata%2C%20on%20par%20with%20most%20existing%20state-of-the-art%20models.%20The%20results%20and%0Amethodology%20suggest%20a%20significant%20advancement%20in%20autonomous%20driving%20trajectory%0Aprediction%2C%20paving%20the%20way%20for%20safer%20and%20more%20efficient%20autonomous%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01266v1&entry.124074799=Read"},
{"title": "Purify Unlearnable Examples via Rate-Constrained Variational\n  Autoencoders", "author": "Yi Yu and Yufei Wang and Song Xia and Wenhan Yang and Shijian Lu and Yap-Peng Tan and Alex C. Kot", "abstract": "  Unlearnable examples (UEs) seek to maximize testing error by making subtle\nmodifications to training examples that are correctly labeled. Defenses against\nthese poisoning attacks can be categorized based on whether specific\ninterventions are adopted during training. The first approach is training-time\ndefense, such as adversarial training, which can mitigate poisoning effects but\nis computationally intensive. The other approach is pre-training purification,\ne.g., image short squeezing, which consists of several simple compressions but\noften encounters challenges in dealing with various UEs. Our work provides a\nnovel disentanglement mechanism to build an efficient pre-training purification\nmethod. Firstly, we uncover rate-constrained variational autoencoders (VAEs),\ndemonstrating a clear tendency to suppress the perturbations in UEs. We\nsubsequently conduct a theoretical analysis for this phenomenon. Building upon\nthese insights, we introduce a disentangle variational autoencoder (D-VAE),\ncapable of disentangling the perturbations with learnable class-wise\nembeddings. Based on this network, a two-stage purification approach is\nnaturally developed. The first stage focuses on roughly eliminating\nperturbations, while the second stage produces refined, poison-free results,\nensuring effectiveness and robustness across various scenarios. Extensive\nexperiments demonstrate the remarkable performance of our method across\nCIFAR-10, CIFAR-100, and a 100-class ImageNet-subset. Code is available at\nhttps://github.com/yuyi-sd/D-VAE.\n", "link": "http://arxiv.org/abs/2405.01460v1", "date": "2024-05-02", "relevancy": 2.049, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5511}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5073}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Purify%20Unlearnable%20Examples%20via%20Rate-Constrained%20Variational%0A%20%20Autoencoders&body=Title%3A%20Purify%20Unlearnable%20Examples%20via%20Rate-Constrained%20Variational%0A%20%20Autoencoders%0AAuthor%3A%20Yi%20Yu%20and%20Yufei%20Wang%20and%20Song%20Xia%20and%20Wenhan%20Yang%20and%20Shijian%20Lu%20and%20Yap-Peng%20Tan%20and%20Alex%20C.%20Kot%0AAbstract%3A%20%20%20Unlearnable%20examples%20%28UEs%29%20seek%20to%20maximize%20testing%20error%20by%20making%20subtle%0Amodifications%20to%20training%20examples%20that%20are%20correctly%20labeled.%20Defenses%20against%0Athese%20poisoning%20attacks%20can%20be%20categorized%20based%20on%20whether%20specific%0Ainterventions%20are%20adopted%20during%20training.%20The%20first%20approach%20is%20training-time%0Adefense%2C%20such%20as%20adversarial%20training%2C%20which%20can%20mitigate%20poisoning%20effects%20but%0Ais%20computationally%20intensive.%20The%20other%20approach%20is%20pre-training%20purification%2C%0Ae.g.%2C%20image%20short%20squeezing%2C%20which%20consists%20of%20several%20simple%20compressions%20but%0Aoften%20encounters%20challenges%20in%20dealing%20with%20various%20UEs.%20Our%20work%20provides%20a%0Anovel%20disentanglement%20mechanism%20to%20build%20an%20efficient%20pre-training%20purification%0Amethod.%20Firstly%2C%20we%20uncover%20rate-constrained%20variational%20autoencoders%20%28VAEs%29%2C%0Ademonstrating%20a%20clear%20tendency%20to%20suppress%20the%20perturbations%20in%20UEs.%20We%0Asubsequently%20conduct%20a%20theoretical%20analysis%20for%20this%20phenomenon.%20Building%20upon%0Athese%20insights%2C%20we%20introduce%20a%20disentangle%20variational%20autoencoder%20%28D-VAE%29%2C%0Acapable%20of%20disentangling%20the%20perturbations%20with%20learnable%20class-wise%0Aembeddings.%20Based%20on%20this%20network%2C%20a%20two-stage%20purification%20approach%20is%0Anaturally%20developed.%20The%20first%20stage%20focuses%20on%20roughly%20eliminating%0Aperturbations%2C%20while%20the%20second%20stage%20produces%20refined%2C%20poison-free%20results%2C%0Aensuring%20effectiveness%20and%20robustness%20across%20various%20scenarios.%20Extensive%0Aexperiments%20demonstrate%20the%20remarkable%20performance%20of%20our%20method%20across%0ACIFAR-10%2C%20CIFAR-100%2C%20and%20a%20100-class%20ImageNet-subset.%20Code%20is%20available%20at%0Ahttps%3A//github.com/yuyi-sd/D-VAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPurify%2520Unlearnable%2520Examples%2520via%2520Rate-Constrained%2520Variational%250A%2520%2520Autoencoders%26entry.906535625%3DYi%2520Yu%2520and%2520Yufei%2520Wang%2520and%2520Song%2520Xia%2520and%2520Wenhan%2520Yang%2520and%2520Shijian%2520Lu%2520and%2520Yap-Peng%2520Tan%2520and%2520Alex%2520C.%2520Kot%26entry.1292438233%3D%2520%2520Unlearnable%2520examples%2520%2528UEs%2529%2520seek%2520to%2520maximize%2520testing%2520error%2520by%2520making%2520subtle%250Amodifications%2520to%2520training%2520examples%2520that%2520are%2520correctly%2520labeled.%2520Defenses%2520against%250Athese%2520poisoning%2520attacks%2520can%2520be%2520categorized%2520based%2520on%2520whether%2520specific%250Ainterventions%2520are%2520adopted%2520during%2520training.%2520The%2520first%2520approach%2520is%2520training-time%250Adefense%252C%2520such%2520as%2520adversarial%2520training%252C%2520which%2520can%2520mitigate%2520poisoning%2520effects%2520but%250Ais%2520computationally%2520intensive.%2520The%2520other%2520approach%2520is%2520pre-training%2520purification%252C%250Ae.g.%252C%2520image%2520short%2520squeezing%252C%2520which%2520consists%2520of%2520several%2520simple%2520compressions%2520but%250Aoften%2520encounters%2520challenges%2520in%2520dealing%2520with%2520various%2520UEs.%2520Our%2520work%2520provides%2520a%250Anovel%2520disentanglement%2520mechanism%2520to%2520build%2520an%2520efficient%2520pre-training%2520purification%250Amethod.%2520Firstly%252C%2520we%2520uncover%2520rate-constrained%2520variational%2520autoencoders%2520%2528VAEs%2529%252C%250Ademonstrating%2520a%2520clear%2520tendency%2520to%2520suppress%2520the%2520perturbations%2520in%2520UEs.%2520We%250Asubsequently%2520conduct%2520a%2520theoretical%2520analysis%2520for%2520this%2520phenomenon.%2520Building%2520upon%250Athese%2520insights%252C%2520we%2520introduce%2520a%2520disentangle%2520variational%2520autoencoder%2520%2528D-VAE%2529%252C%250Acapable%2520of%2520disentangling%2520the%2520perturbations%2520with%2520learnable%2520class-wise%250Aembeddings.%2520Based%2520on%2520this%2520network%252C%2520a%2520two-stage%2520purification%2520approach%2520is%250Anaturally%2520developed.%2520The%2520first%2520stage%2520focuses%2520on%2520roughly%2520eliminating%250Aperturbations%252C%2520while%2520the%2520second%2520stage%2520produces%2520refined%252C%2520poison-free%2520results%252C%250Aensuring%2520effectiveness%2520and%2520robustness%2520across%2520various%2520scenarios.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520remarkable%2520performance%2520of%2520our%2520method%2520across%250ACIFAR-10%252C%2520CIFAR-100%252C%2520and%2520a%2520100-class%2520ImageNet-subset.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/yuyi-sd/D-VAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Purify%20Unlearnable%20Examples%20via%20Rate-Constrained%20Variational%0A%20%20Autoencoders&entry.906535625=Yi%20Yu%20and%20Yufei%20Wang%20and%20Song%20Xia%20and%20Wenhan%20Yang%20and%20Shijian%20Lu%20and%20Yap-Peng%20Tan%20and%20Alex%20C.%20Kot&entry.1292438233=%20%20Unlearnable%20examples%20%28UEs%29%20seek%20to%20maximize%20testing%20error%20by%20making%20subtle%0Amodifications%20to%20training%20examples%20that%20are%20correctly%20labeled.%20Defenses%20against%0Athese%20poisoning%20attacks%20can%20be%20categorized%20based%20on%20whether%20specific%0Ainterventions%20are%20adopted%20during%20training.%20The%20first%20approach%20is%20training-time%0Adefense%2C%20such%20as%20adversarial%20training%2C%20which%20can%20mitigate%20poisoning%20effects%20but%0Ais%20computationally%20intensive.%20The%20other%20approach%20is%20pre-training%20purification%2C%0Ae.g.%2C%20image%20short%20squeezing%2C%20which%20consists%20of%20several%20simple%20compressions%20but%0Aoften%20encounters%20challenges%20in%20dealing%20with%20various%20UEs.%20Our%20work%20provides%20a%0Anovel%20disentanglement%20mechanism%20to%20build%20an%20efficient%20pre-training%20purification%0Amethod.%20Firstly%2C%20we%20uncover%20rate-constrained%20variational%20autoencoders%20%28VAEs%29%2C%0Ademonstrating%20a%20clear%20tendency%20to%20suppress%20the%20perturbations%20in%20UEs.%20We%0Asubsequently%20conduct%20a%20theoretical%20analysis%20for%20this%20phenomenon.%20Building%20upon%0Athese%20insights%2C%20we%20introduce%20a%20disentangle%20variational%20autoencoder%20%28D-VAE%29%2C%0Acapable%20of%20disentangling%20the%20perturbations%20with%20learnable%20class-wise%0Aembeddings.%20Based%20on%20this%20network%2C%20a%20two-stage%20purification%20approach%20is%0Anaturally%20developed.%20The%20first%20stage%20focuses%20on%20roughly%20eliminating%0Aperturbations%2C%20while%20the%20second%20stage%20produces%20refined%2C%20poison-free%20results%2C%0Aensuring%20effectiveness%20and%20robustness%20across%20various%20scenarios.%20Extensive%0Aexperiments%20demonstrate%20the%20remarkable%20performance%20of%20our%20method%20across%0ACIFAR-10%2C%20CIFAR-100%2C%20and%20a%20100-class%20ImageNet-subset.%20Code%20is%20available%20at%0Ahttps%3A//github.com/yuyi-sd/D-VAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01460v1&entry.124074799=Read"},
{"title": "Uncertainty for Active Learning on Graphs", "author": "Dominik Fuchsgruber and Tom Wollschl\u00e4ger and Bertrand Charpentier and Antonio Oroz and Stephan G\u00fcnnemann", "abstract": "  Uncertainty Sampling is an Active Learning strategy that aims to improve the\ndata efficiency of machine learning models by iteratively acquiring labels of\ndata points with the highest uncertainty. While it has proven effective for\nindependent data its applicability to graphs remains under-explored. We propose\nthe first extensive study of Uncertainty Sampling for node classification: (1)\nWe benchmark Uncertainty Sampling beyond predictive uncertainty and highlight a\nsignificant performance gap to other Active Learning strategies. (2) We develop\nground-truth Bayesian uncertainty estimates in terms of the data generating\nprocess and prove their effectiveness in guiding Uncertainty Sampling toward\noptimal queries. We confirm our results on synthetic data and design an\napproximate approach that consistently outperforms other uncertainty estimators\non real datasets. (3) Based on this analysis, we relate pitfalls in modeling\nuncertainty to existing methods. Our analysis enables and informs the\ndevelopment of principled uncertainty estimation on graphs.\n", "link": "http://arxiv.org/abs/2405.01462v1", "date": "2024-05-02", "relevancy": 2.046, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5345}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5211}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20for%20Active%20Learning%20on%20Graphs&body=Title%3A%20Uncertainty%20for%20Active%20Learning%20on%20Graphs%0AAuthor%3A%20Dominik%20Fuchsgruber%20and%20Tom%20Wollschl%C3%A4ger%20and%20Bertrand%20Charpentier%20and%20Antonio%20Oroz%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20Uncertainty%20Sampling%20is%20an%20Active%20Learning%20strategy%20that%20aims%20to%20improve%20the%0Adata%20efficiency%20of%20machine%20learning%20models%20by%20iteratively%20acquiring%20labels%20of%0Adata%20points%20with%20the%20highest%20uncertainty.%20While%20it%20has%20proven%20effective%20for%0Aindependent%20data%20its%20applicability%20to%20graphs%20remains%20under-explored.%20We%20propose%0Athe%20first%20extensive%20study%20of%20Uncertainty%20Sampling%20for%20node%20classification%3A%20%281%29%0AWe%20benchmark%20Uncertainty%20Sampling%20beyond%20predictive%20uncertainty%20and%20highlight%20a%0Asignificant%20performance%20gap%20to%20other%20Active%20Learning%20strategies.%20%282%29%20We%20develop%0Aground-truth%20Bayesian%20uncertainty%20estimates%20in%20terms%20of%20the%20data%20generating%0Aprocess%20and%20prove%20their%20effectiveness%20in%20guiding%20Uncertainty%20Sampling%20toward%0Aoptimal%20queries.%20We%20confirm%20our%20results%20on%20synthetic%20data%20and%20design%20an%0Aapproximate%20approach%20that%20consistently%20outperforms%20other%20uncertainty%20estimators%0Aon%20real%20datasets.%20%283%29%20Based%20on%20this%20analysis%2C%20we%20relate%20pitfalls%20in%20modeling%0Auncertainty%20to%20existing%20methods.%20Our%20analysis%20enables%20and%20informs%20the%0Adevelopment%20of%20principled%20uncertainty%20estimation%20on%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520for%2520Active%2520Learning%2520on%2520Graphs%26entry.906535625%3DDominik%2520Fuchsgruber%2520and%2520Tom%2520Wollschl%25C3%25A4ger%2520and%2520Bertrand%2520Charpentier%2520and%2520Antonio%2520Oroz%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520Uncertainty%2520Sampling%2520is%2520an%2520Active%2520Learning%2520strategy%2520that%2520aims%2520to%2520improve%2520the%250Adata%2520efficiency%2520of%2520machine%2520learning%2520models%2520by%2520iteratively%2520acquiring%2520labels%2520of%250Adata%2520points%2520with%2520the%2520highest%2520uncertainty.%2520While%2520it%2520has%2520proven%2520effective%2520for%250Aindependent%2520data%2520its%2520applicability%2520to%2520graphs%2520remains%2520under-explored.%2520We%2520propose%250Athe%2520first%2520extensive%2520study%2520of%2520Uncertainty%2520Sampling%2520for%2520node%2520classification%253A%2520%25281%2529%250AWe%2520benchmark%2520Uncertainty%2520Sampling%2520beyond%2520predictive%2520uncertainty%2520and%2520highlight%2520a%250Asignificant%2520performance%2520gap%2520to%2520other%2520Active%2520Learning%2520strategies.%2520%25282%2529%2520We%2520develop%250Aground-truth%2520Bayesian%2520uncertainty%2520estimates%2520in%2520terms%2520of%2520the%2520data%2520generating%250Aprocess%2520and%2520prove%2520their%2520effectiveness%2520in%2520guiding%2520Uncertainty%2520Sampling%2520toward%250Aoptimal%2520queries.%2520We%2520confirm%2520our%2520results%2520on%2520synthetic%2520data%2520and%2520design%2520an%250Aapproximate%2520approach%2520that%2520consistently%2520outperforms%2520other%2520uncertainty%2520estimators%250Aon%2520real%2520datasets.%2520%25283%2529%2520Based%2520on%2520this%2520analysis%252C%2520we%2520relate%2520pitfalls%2520in%2520modeling%250Auncertainty%2520to%2520existing%2520methods.%2520Our%2520analysis%2520enables%2520and%2520informs%2520the%250Adevelopment%2520of%2520principled%2520uncertainty%2520estimation%2520on%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20for%20Active%20Learning%20on%20Graphs&entry.906535625=Dominik%20Fuchsgruber%20and%20Tom%20Wollschl%C3%A4ger%20and%20Bertrand%20Charpentier%20and%20Antonio%20Oroz%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20Uncertainty%20Sampling%20is%20an%20Active%20Learning%20strategy%20that%20aims%20to%20improve%20the%0Adata%20efficiency%20of%20machine%20learning%20models%20by%20iteratively%20acquiring%20labels%20of%0Adata%20points%20with%20the%20highest%20uncertainty.%20While%20it%20has%20proven%20effective%20for%0Aindependent%20data%20its%20applicability%20to%20graphs%20remains%20under-explored.%20We%20propose%0Athe%20first%20extensive%20study%20of%20Uncertainty%20Sampling%20for%20node%20classification%3A%20%281%29%0AWe%20benchmark%20Uncertainty%20Sampling%20beyond%20predictive%20uncertainty%20and%20highlight%20a%0Asignificant%20performance%20gap%20to%20other%20Active%20Learning%20strategies.%20%282%29%20We%20develop%0Aground-truth%20Bayesian%20uncertainty%20estimates%20in%20terms%20of%20the%20data%20generating%0Aprocess%20and%20prove%20their%20effectiveness%20in%20guiding%20Uncertainty%20Sampling%20toward%0Aoptimal%20queries.%20We%20confirm%20our%20results%20on%20synthetic%20data%20and%20design%20an%0Aapproximate%20approach%20that%20consistently%20outperforms%20other%20uncertainty%20estimators%0Aon%20real%20datasets.%20%283%29%20Based%20on%20this%20analysis%2C%20we%20relate%20pitfalls%20in%20modeling%0Auncertainty%20to%20existing%20methods.%20Our%20analysis%20enables%20and%20informs%20the%0Adevelopment%20of%20principled%20uncertainty%20estimation%20on%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01462v1&entry.124074799=Read"},
{"title": "Prioritized Soft Q-Decomposition for Lexicographic Reinforcement\n  Learning", "author": "Finn Rietz and Erik Schaffernicht and Stefan Heinrich and Johannes Andreas Stork", "abstract": "  Reinforcement learning (RL) for complex tasks remains a challenge, primarily\ndue to the difficulties of engineering scalar reward functions and the inherent\ninefficiency of training models from scratch. Instead, it would be better to\nspecify complex tasks in terms of elementary subtasks and to reuse subtask\nsolutions whenever possible. In this work, we address continuous space\nlexicographic multi-objective RL problems, consisting of prioritized subtasks,\nwhich are notoriously difficult to solve. We show that these can be scalarized\nwith a subtask transformation and then solved incrementally using value\ndecomposition. Exploiting this insight, we propose prioritized soft\nQ-decomposition (PSQD), a novel algorithm for learning and adapting subtask\nsolutions under lexicographic priorities in continuous state-action spaces.\nPSQD offers the ability to reuse previously learned subtask solutions in a\nzero-shot composition, followed by an adaptation step. Its ability to use\nretained subtask training data for offline learning eliminates the need for new\nenvironment interaction during adaptation. We demonstrate the efficacy of our\napproach by presenting successful learning, reuse, and adaptation results for\nboth low- and high-dimensional simulated robot control tasks, as well as\noffline learning results. In contrast to baseline approaches, PSQD does not\ntrade off between conflicting subtasks or priority constraints and satisfies\nsubtask priorities during learning. PSQD provides an intuitive framework for\ntackling complex RL problems, offering insights into the inner workings of the\nsubtask composition.\n", "link": "http://arxiv.org/abs/2310.02360v2", "date": "2024-05-02", "relevancy": 2.0423, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5378}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5064}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prioritized%20Soft%20Q-Decomposition%20for%20Lexicographic%20Reinforcement%0A%20%20Learning&body=Title%3A%20Prioritized%20Soft%20Q-Decomposition%20for%20Lexicographic%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Finn%20Rietz%20and%20Erik%20Schaffernicht%20and%20Stefan%20Heinrich%20and%20Johannes%20Andreas%20Stork%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20for%20complex%20tasks%20remains%20a%20challenge%2C%20primarily%0Adue%20to%20the%20difficulties%20of%20engineering%20scalar%20reward%20functions%20and%20the%20inherent%0Ainefficiency%20of%20training%20models%20from%20scratch.%20Instead%2C%20it%20would%20be%20better%20to%0Aspecify%20complex%20tasks%20in%20terms%20of%20elementary%20subtasks%20and%20to%20reuse%20subtask%0Asolutions%20whenever%20possible.%20In%20this%20work%2C%20we%20address%20continuous%20space%0Alexicographic%20multi-objective%20RL%20problems%2C%20consisting%20of%20prioritized%20subtasks%2C%0Awhich%20are%20notoriously%20difficult%20to%20solve.%20We%20show%20that%20these%20can%20be%20scalarized%0Awith%20a%20subtask%20transformation%20and%20then%20solved%20incrementally%20using%20value%0Adecomposition.%20Exploiting%20this%20insight%2C%20we%20propose%20prioritized%20soft%0AQ-decomposition%20%28PSQD%29%2C%20a%20novel%20algorithm%20for%20learning%20and%20adapting%20subtask%0Asolutions%20under%20lexicographic%20priorities%20in%20continuous%20state-action%20spaces.%0APSQD%20offers%20the%20ability%20to%20reuse%20previously%20learned%20subtask%20solutions%20in%20a%0Azero-shot%20composition%2C%20followed%20by%20an%20adaptation%20step.%20Its%20ability%20to%20use%0Aretained%20subtask%20training%20data%20for%20offline%20learning%20eliminates%20the%20need%20for%20new%0Aenvironment%20interaction%20during%20adaptation.%20We%20demonstrate%20the%20efficacy%20of%20our%0Aapproach%20by%20presenting%20successful%20learning%2C%20reuse%2C%20and%20adaptation%20results%20for%0Aboth%20low-%20and%20high-dimensional%20simulated%20robot%20control%20tasks%2C%20as%20well%20as%0Aoffline%20learning%20results.%20In%20contrast%20to%20baseline%20approaches%2C%20PSQD%20does%20not%0Atrade%20off%20between%20conflicting%20subtasks%20or%20priority%20constraints%20and%20satisfies%0Asubtask%20priorities%20during%20learning.%20PSQD%20provides%20an%20intuitive%20framework%20for%0Atackling%20complex%20RL%20problems%2C%20offering%20insights%20into%20the%20inner%20workings%20of%20the%0Asubtask%20composition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02360v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrioritized%2520Soft%2520Q-Decomposition%2520for%2520Lexicographic%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DFinn%2520Rietz%2520and%2520Erik%2520Schaffernicht%2520and%2520Stefan%2520Heinrich%2520and%2520Johannes%2520Andreas%2520Stork%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520for%2520complex%2520tasks%2520remains%2520a%2520challenge%252C%2520primarily%250Adue%2520to%2520the%2520difficulties%2520of%2520engineering%2520scalar%2520reward%2520functions%2520and%2520the%2520inherent%250Ainefficiency%2520of%2520training%2520models%2520from%2520scratch.%2520Instead%252C%2520it%2520would%2520be%2520better%2520to%250Aspecify%2520complex%2520tasks%2520in%2520terms%2520of%2520elementary%2520subtasks%2520and%2520to%2520reuse%2520subtask%250Asolutions%2520whenever%2520possible.%2520In%2520this%2520work%252C%2520we%2520address%2520continuous%2520space%250Alexicographic%2520multi-objective%2520RL%2520problems%252C%2520consisting%2520of%2520prioritized%2520subtasks%252C%250Awhich%2520are%2520notoriously%2520difficult%2520to%2520solve.%2520We%2520show%2520that%2520these%2520can%2520be%2520scalarized%250Awith%2520a%2520subtask%2520transformation%2520and%2520then%2520solved%2520incrementally%2520using%2520value%250Adecomposition.%2520Exploiting%2520this%2520insight%252C%2520we%2520propose%2520prioritized%2520soft%250AQ-decomposition%2520%2528PSQD%2529%252C%2520a%2520novel%2520algorithm%2520for%2520learning%2520and%2520adapting%2520subtask%250Asolutions%2520under%2520lexicographic%2520priorities%2520in%2520continuous%2520state-action%2520spaces.%250APSQD%2520offers%2520the%2520ability%2520to%2520reuse%2520previously%2520learned%2520subtask%2520solutions%2520in%2520a%250Azero-shot%2520composition%252C%2520followed%2520by%2520an%2520adaptation%2520step.%2520Its%2520ability%2520to%2520use%250Aretained%2520subtask%2520training%2520data%2520for%2520offline%2520learning%2520eliminates%2520the%2520need%2520for%2520new%250Aenvironment%2520interaction%2520during%2520adaptation.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%250Aapproach%2520by%2520presenting%2520successful%2520learning%252C%2520reuse%252C%2520and%2520adaptation%2520results%2520for%250Aboth%2520low-%2520and%2520high-dimensional%2520simulated%2520robot%2520control%2520tasks%252C%2520as%2520well%2520as%250Aoffline%2520learning%2520results.%2520In%2520contrast%2520to%2520baseline%2520approaches%252C%2520PSQD%2520does%2520not%250Atrade%2520off%2520between%2520conflicting%2520subtasks%2520or%2520priority%2520constraints%2520and%2520satisfies%250Asubtask%2520priorities%2520during%2520learning.%2520PSQD%2520provides%2520an%2520intuitive%2520framework%2520for%250Atackling%2520complex%2520RL%2520problems%252C%2520offering%2520insights%2520into%2520the%2520inner%2520workings%2520of%2520the%250Asubtask%2520composition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02360v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prioritized%20Soft%20Q-Decomposition%20for%20Lexicographic%20Reinforcement%0A%20%20Learning&entry.906535625=Finn%20Rietz%20and%20Erik%20Schaffernicht%20and%20Stefan%20Heinrich%20and%20Johannes%20Andreas%20Stork&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20for%20complex%20tasks%20remains%20a%20challenge%2C%20primarily%0Adue%20to%20the%20difficulties%20of%20engineering%20scalar%20reward%20functions%20and%20the%20inherent%0Ainefficiency%20of%20training%20models%20from%20scratch.%20Instead%2C%20it%20would%20be%20better%20to%0Aspecify%20complex%20tasks%20in%20terms%20of%20elementary%20subtasks%20and%20to%20reuse%20subtask%0Asolutions%20whenever%20possible.%20In%20this%20work%2C%20we%20address%20continuous%20space%0Alexicographic%20multi-objective%20RL%20problems%2C%20consisting%20of%20prioritized%20subtasks%2C%0Awhich%20are%20notoriously%20difficult%20to%20solve.%20We%20show%20that%20these%20can%20be%20scalarized%0Awith%20a%20subtask%20transformation%20and%20then%20solved%20incrementally%20using%20value%0Adecomposition.%20Exploiting%20this%20insight%2C%20we%20propose%20prioritized%20soft%0AQ-decomposition%20%28PSQD%29%2C%20a%20novel%20algorithm%20for%20learning%20and%20adapting%20subtask%0Asolutions%20under%20lexicographic%20priorities%20in%20continuous%20state-action%20spaces.%0APSQD%20offers%20the%20ability%20to%20reuse%20previously%20learned%20subtask%20solutions%20in%20a%0Azero-shot%20composition%2C%20followed%20by%20an%20adaptation%20step.%20Its%20ability%20to%20use%0Aretained%20subtask%20training%20data%20for%20offline%20learning%20eliminates%20the%20need%20for%20new%0Aenvironment%20interaction%20during%20adaptation.%20We%20demonstrate%20the%20efficacy%20of%20our%0Aapproach%20by%20presenting%20successful%20learning%2C%20reuse%2C%20and%20adaptation%20results%20for%0Aboth%20low-%20and%20high-dimensional%20simulated%20robot%20control%20tasks%2C%20as%20well%20as%0Aoffline%20learning%20results.%20In%20contrast%20to%20baseline%20approaches%2C%20PSQD%20does%20not%0Atrade%20off%20between%20conflicting%20subtasks%20or%20priority%20constraints%20and%20satisfies%0Asubtask%20priorities%20during%20learning.%20PSQD%20provides%20an%20intuitive%20framework%20for%0Atackling%20complex%20RL%20problems%2C%20offering%20insights%20into%20the%20inner%20workings%20of%20the%0Asubtask%20composition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02360v2&entry.124074799=Read"},
{"title": "Quantifying Spatial Domain Explanations in BCI using Earth Mover's\n  Distance", "author": "Param Rajpura and Hubert Cecotti and Yogesh Kumar Meena", "abstract": "  Brain-computer interface (BCI) systems facilitate unique communication\nbetween humans and computers, benefiting severely disabled individuals. Despite\ndecades of research, BCIs are not fully integrated into clinical and commercial\nsettings. It's crucial to assess and explain BCI performance, offering clear\nexplanations for potential users to avoid frustration when it doesn't work as\nexpected. This work investigates the efficacy of different deep learning and\nRiemannian geometry-based classification models in the context of motor imagery\n(MI) based BCI using electroencephalography (EEG). We then propose an optimal\ntransport theory-based approach using earth mover's distance (EMD) to quantify\nthe comparison of the feature relevance map with the domain knowledge of\nneuroscience. For this, we utilized explainable AI (XAI) techniques for\ngenerating feature relevance in the spatial domain to identify important\nchannels for model outcomes. Three state-of-the-art models are implemented - 1)\nRiemannian geometry-based classifier, 2) EEGNet, and 3) EEG Conformer, and the\nobserved trend in the model's accuracy across different architectures on the\ndataset correlates with the proposed feature relevance metrics. The models with\ndiverse architectures perform significantly better when trained on channels\nrelevant to motor imagery than data-driven channel selection. This work focuses\nattention on the necessity for interpretability and incorporating metrics\nbeyond accuracy, underscores the value of combining domain knowledge and\nquantifying model interpretations with data-driven approaches in creating\nreliable and robust Brain-Computer Interfaces (BCIs).\n", "link": "http://arxiv.org/abs/2405.01277v1", "date": "2024-05-02", "relevancy": 2.0336, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5555}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5026}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Spatial%20Domain%20Explanations%20in%20BCI%20using%20Earth%20Mover%27s%0A%20%20Distance&body=Title%3A%20Quantifying%20Spatial%20Domain%20Explanations%20in%20BCI%20using%20Earth%20Mover%27s%0A%20%20Distance%0AAuthor%3A%20Param%20Rajpura%20and%20Hubert%20Cecotti%20and%20Yogesh%20Kumar%20Meena%0AAbstract%3A%20%20%20Brain-computer%20interface%20%28BCI%29%20systems%20facilitate%20unique%20communication%0Abetween%20humans%20and%20computers%2C%20benefiting%20severely%20disabled%20individuals.%20Despite%0Adecades%20of%20research%2C%20BCIs%20are%20not%20fully%20integrated%20into%20clinical%20and%20commercial%0Asettings.%20It%27s%20crucial%20to%20assess%20and%20explain%20BCI%20performance%2C%20offering%20clear%0Aexplanations%20for%20potential%20users%20to%20avoid%20frustration%20when%20it%20doesn%27t%20work%20as%0Aexpected.%20This%20work%20investigates%20the%20efficacy%20of%20different%20deep%20learning%20and%0ARiemannian%20geometry-based%20classification%20models%20in%20the%20context%20of%20motor%20imagery%0A%28MI%29%20based%20BCI%20using%20electroencephalography%20%28EEG%29.%20We%20then%20propose%20an%20optimal%0Atransport%20theory-based%20approach%20using%20earth%20mover%27s%20distance%20%28EMD%29%20to%20quantify%0Athe%20comparison%20of%20the%20feature%20relevance%20map%20with%20the%20domain%20knowledge%20of%0Aneuroscience.%20For%20this%2C%20we%20utilized%20explainable%20AI%20%28XAI%29%20techniques%20for%0Agenerating%20feature%20relevance%20in%20the%20spatial%20domain%20to%20identify%20important%0Achannels%20for%20model%20outcomes.%20Three%20state-of-the-art%20models%20are%20implemented%20-%201%29%0ARiemannian%20geometry-based%20classifier%2C%202%29%20EEGNet%2C%20and%203%29%20EEG%20Conformer%2C%20and%20the%0Aobserved%20trend%20in%20the%20model%27s%20accuracy%20across%20different%20architectures%20on%20the%0Adataset%20correlates%20with%20the%20proposed%20feature%20relevance%20metrics.%20The%20models%20with%0Adiverse%20architectures%20perform%20significantly%20better%20when%20trained%20on%20channels%0Arelevant%20to%20motor%20imagery%20than%20data-driven%20channel%20selection.%20This%20work%20focuses%0Aattention%20on%20the%20necessity%20for%20interpretability%20and%20incorporating%20metrics%0Abeyond%20accuracy%2C%20underscores%20the%20value%20of%20combining%20domain%20knowledge%20and%0Aquantifying%20model%20interpretations%20with%20data-driven%20approaches%20in%20creating%0Areliable%20and%20robust%20Brain-Computer%20Interfaces%20%28BCIs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Spatial%2520Domain%2520Explanations%2520in%2520BCI%2520using%2520Earth%2520Mover%2527s%250A%2520%2520Distance%26entry.906535625%3DParam%2520Rajpura%2520and%2520Hubert%2520Cecotti%2520and%2520Yogesh%2520Kumar%2520Meena%26entry.1292438233%3D%2520%2520Brain-computer%2520interface%2520%2528BCI%2529%2520systems%2520facilitate%2520unique%2520communication%250Abetween%2520humans%2520and%2520computers%252C%2520benefiting%2520severely%2520disabled%2520individuals.%2520Despite%250Adecades%2520of%2520research%252C%2520BCIs%2520are%2520not%2520fully%2520integrated%2520into%2520clinical%2520and%2520commercial%250Asettings.%2520It%2527s%2520crucial%2520to%2520assess%2520and%2520explain%2520BCI%2520performance%252C%2520offering%2520clear%250Aexplanations%2520for%2520potential%2520users%2520to%2520avoid%2520frustration%2520when%2520it%2520doesn%2527t%2520work%2520as%250Aexpected.%2520This%2520work%2520investigates%2520the%2520efficacy%2520of%2520different%2520deep%2520learning%2520and%250ARiemannian%2520geometry-based%2520classification%2520models%2520in%2520the%2520context%2520of%2520motor%2520imagery%250A%2528MI%2529%2520based%2520BCI%2520using%2520electroencephalography%2520%2528EEG%2529.%2520We%2520then%2520propose%2520an%2520optimal%250Atransport%2520theory-based%2520approach%2520using%2520earth%2520mover%2527s%2520distance%2520%2528EMD%2529%2520to%2520quantify%250Athe%2520comparison%2520of%2520the%2520feature%2520relevance%2520map%2520with%2520the%2520domain%2520knowledge%2520of%250Aneuroscience.%2520For%2520this%252C%2520we%2520utilized%2520explainable%2520AI%2520%2528XAI%2529%2520techniques%2520for%250Agenerating%2520feature%2520relevance%2520in%2520the%2520spatial%2520domain%2520to%2520identify%2520important%250Achannels%2520for%2520model%2520outcomes.%2520Three%2520state-of-the-art%2520models%2520are%2520implemented%2520-%25201%2529%250ARiemannian%2520geometry-based%2520classifier%252C%25202%2529%2520EEGNet%252C%2520and%25203%2529%2520EEG%2520Conformer%252C%2520and%2520the%250Aobserved%2520trend%2520in%2520the%2520model%2527s%2520accuracy%2520across%2520different%2520architectures%2520on%2520the%250Adataset%2520correlates%2520with%2520the%2520proposed%2520feature%2520relevance%2520metrics.%2520The%2520models%2520with%250Adiverse%2520architectures%2520perform%2520significantly%2520better%2520when%2520trained%2520on%2520channels%250Arelevant%2520to%2520motor%2520imagery%2520than%2520data-driven%2520channel%2520selection.%2520This%2520work%2520focuses%250Aattention%2520on%2520the%2520necessity%2520for%2520interpretability%2520and%2520incorporating%2520metrics%250Abeyond%2520accuracy%252C%2520underscores%2520the%2520value%2520of%2520combining%2520domain%2520knowledge%2520and%250Aquantifying%2520model%2520interpretations%2520with%2520data-driven%2520approaches%2520in%2520creating%250Areliable%2520and%2520robust%2520Brain-Computer%2520Interfaces%2520%2528BCIs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Spatial%20Domain%20Explanations%20in%20BCI%20using%20Earth%20Mover%27s%0A%20%20Distance&entry.906535625=Param%20Rajpura%20and%20Hubert%20Cecotti%20and%20Yogesh%20Kumar%20Meena&entry.1292438233=%20%20Brain-computer%20interface%20%28BCI%29%20systems%20facilitate%20unique%20communication%0Abetween%20humans%20and%20computers%2C%20benefiting%20severely%20disabled%20individuals.%20Despite%0Adecades%20of%20research%2C%20BCIs%20are%20not%20fully%20integrated%20into%20clinical%20and%20commercial%0Asettings.%20It%27s%20crucial%20to%20assess%20and%20explain%20BCI%20performance%2C%20offering%20clear%0Aexplanations%20for%20potential%20users%20to%20avoid%20frustration%20when%20it%20doesn%27t%20work%20as%0Aexpected.%20This%20work%20investigates%20the%20efficacy%20of%20different%20deep%20learning%20and%0ARiemannian%20geometry-based%20classification%20models%20in%20the%20context%20of%20motor%20imagery%0A%28MI%29%20based%20BCI%20using%20electroencephalography%20%28EEG%29.%20We%20then%20propose%20an%20optimal%0Atransport%20theory-based%20approach%20using%20earth%20mover%27s%20distance%20%28EMD%29%20to%20quantify%0Athe%20comparison%20of%20the%20feature%20relevance%20map%20with%20the%20domain%20knowledge%20of%0Aneuroscience.%20For%20this%2C%20we%20utilized%20explainable%20AI%20%28XAI%29%20techniques%20for%0Agenerating%20feature%20relevance%20in%20the%20spatial%20domain%20to%20identify%20important%0Achannels%20for%20model%20outcomes.%20Three%20state-of-the-art%20models%20are%20implemented%20-%201%29%0ARiemannian%20geometry-based%20classifier%2C%202%29%20EEGNet%2C%20and%203%29%20EEG%20Conformer%2C%20and%20the%0Aobserved%20trend%20in%20the%20model%27s%20accuracy%20across%20different%20architectures%20on%20the%0Adataset%20correlates%20with%20the%20proposed%20feature%20relevance%20metrics.%20The%20models%20with%0Adiverse%20architectures%20perform%20significantly%20better%20when%20trained%20on%20channels%0Arelevant%20to%20motor%20imagery%20than%20data-driven%20channel%20selection.%20This%20work%20focuses%0Aattention%20on%20the%20necessity%20for%20interpretability%20and%20incorporating%20metrics%0Abeyond%20accuracy%2C%20underscores%20the%20value%20of%20combining%20domain%20knowledge%20and%0Aquantifying%20model%20interpretations%20with%20data-driven%20approaches%20in%20creating%0Areliable%20and%20robust%20Brain-Computer%20Interfaces%20%28BCIs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01277v1&entry.124074799=Read"},
{"title": "Potential Energy based Mixture Model for Noisy Label Learning", "author": "Zijia Wang and Wenbin Yang and Zhisong Liu and Zhen Jia", "abstract": "  Training deep neural networks (DNNs) from noisy labels is an important and\nchallenging task. However, most existing approaches focus on the corrupted\nlabels and ignore the importance of inherent data structure. To bridge the gap\nbetween noisy labels and data, inspired by the concept of potential energy in\nphysics, we propose a novel Potential Energy based Mixture Model (PEMM) for\nnoise-labels learning. We innovate a distance-based classifier with the\npotential energy regularization on its class centers. Embedding our proposed\nclassifier with existing deep learning backbones, we can have robust networks\nwith better feature representations. They can preserve intrinsic structures\nfrom the data, resulting in a superior noisy tolerance. We conducted extensive\nexperiments to analyze the efficiency of our proposed model on several\nreal-world datasets. Quantitative results show that it can achieve\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2405.01186v1", "date": "2024-05-02", "relevancy": 2.0207, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5267}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5132}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Potential%20Energy%20based%20Mixture%20Model%20for%20Noisy%20Label%20Learning&body=Title%3A%20Potential%20Energy%20based%20Mixture%20Model%20for%20Noisy%20Label%20Learning%0AAuthor%3A%20Zijia%20Wang%20and%20Wenbin%20Yang%20and%20Zhisong%20Liu%20and%20Zhen%20Jia%0AAbstract%3A%20%20%20Training%20deep%20neural%20networks%20%28DNNs%29%20from%20noisy%20labels%20is%20an%20important%20and%0Achallenging%20task.%20However%2C%20most%20existing%20approaches%20focus%20on%20the%20corrupted%0Alabels%20and%20ignore%20the%20importance%20of%20inherent%20data%20structure.%20To%20bridge%20the%20gap%0Abetween%20noisy%20labels%20and%20data%2C%20inspired%20by%20the%20concept%20of%20potential%20energy%20in%0Aphysics%2C%20we%20propose%20a%20novel%20Potential%20Energy%20based%20Mixture%20Model%20%28PEMM%29%20for%0Anoise-labels%20learning.%20We%20innovate%20a%20distance-based%20classifier%20with%20the%0Apotential%20energy%20regularization%20on%20its%20class%20centers.%20Embedding%20our%20proposed%0Aclassifier%20with%20existing%20deep%20learning%20backbones%2C%20we%20can%20have%20robust%20networks%0Awith%20better%20feature%20representations.%20They%20can%20preserve%20intrinsic%20structures%0Afrom%20the%20data%2C%20resulting%20in%20a%20superior%20noisy%20tolerance.%20We%20conducted%20extensive%0Aexperiments%20to%20analyze%20the%20efficiency%20of%20our%20proposed%20model%20on%20several%0Areal-world%20datasets.%20Quantitative%20results%20show%20that%20it%20can%20achieve%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPotential%2520Energy%2520based%2520Mixture%2520Model%2520for%2520Noisy%2520Label%2520Learning%26entry.906535625%3DZijia%2520Wang%2520and%2520Wenbin%2520Yang%2520and%2520Zhisong%2520Liu%2520and%2520Zhen%2520Jia%26entry.1292438233%3D%2520%2520Training%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520from%2520noisy%2520labels%2520is%2520an%2520important%2520and%250Achallenging%2520task.%2520However%252C%2520most%2520existing%2520approaches%2520focus%2520on%2520the%2520corrupted%250Alabels%2520and%2520ignore%2520the%2520importance%2520of%2520inherent%2520data%2520structure.%2520To%2520bridge%2520the%2520gap%250Abetween%2520noisy%2520labels%2520and%2520data%252C%2520inspired%2520by%2520the%2520concept%2520of%2520potential%2520energy%2520in%250Aphysics%252C%2520we%2520propose%2520a%2520novel%2520Potential%2520Energy%2520based%2520Mixture%2520Model%2520%2528PEMM%2529%2520for%250Anoise-labels%2520learning.%2520We%2520innovate%2520a%2520distance-based%2520classifier%2520with%2520the%250Apotential%2520energy%2520regularization%2520on%2520its%2520class%2520centers.%2520Embedding%2520our%2520proposed%250Aclassifier%2520with%2520existing%2520deep%2520learning%2520backbones%252C%2520we%2520can%2520have%2520robust%2520networks%250Awith%2520better%2520feature%2520representations.%2520They%2520can%2520preserve%2520intrinsic%2520structures%250Afrom%2520the%2520data%252C%2520resulting%2520in%2520a%2520superior%2520noisy%2520tolerance.%2520We%2520conducted%2520extensive%250Aexperiments%2520to%2520analyze%2520the%2520efficiency%2520of%2520our%2520proposed%2520model%2520on%2520several%250Areal-world%2520datasets.%2520Quantitative%2520results%2520show%2520that%2520it%2520can%2520achieve%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Potential%20Energy%20based%20Mixture%20Model%20for%20Noisy%20Label%20Learning&entry.906535625=Zijia%20Wang%20and%20Wenbin%20Yang%20and%20Zhisong%20Liu%20and%20Zhen%20Jia&entry.1292438233=%20%20Training%20deep%20neural%20networks%20%28DNNs%29%20from%20noisy%20labels%20is%20an%20important%20and%0Achallenging%20task.%20However%2C%20most%20existing%20approaches%20focus%20on%20the%20corrupted%0Alabels%20and%20ignore%20the%20importance%20of%20inherent%20data%20structure.%20To%20bridge%20the%20gap%0Abetween%20noisy%20labels%20and%20data%2C%20inspired%20by%20the%20concept%20of%20potential%20energy%20in%0Aphysics%2C%20we%20propose%20a%20novel%20Potential%20Energy%20based%20Mixture%20Model%20%28PEMM%29%20for%0Anoise-labels%20learning.%20We%20innovate%20a%20distance-based%20classifier%20with%20the%0Apotential%20energy%20regularization%20on%20its%20class%20centers.%20Embedding%20our%20proposed%0Aclassifier%20with%20existing%20deep%20learning%20backbones%2C%20we%20can%20have%20robust%20networks%0Awith%20better%20feature%20representations.%20They%20can%20preserve%20intrinsic%20structures%0Afrom%20the%20data%2C%20resulting%20in%20a%20superior%20noisy%20tolerance.%20We%20conducted%20extensive%0Aexperiments%20to%20analyze%20the%20efficiency%20of%20our%20proposed%20model%20on%20several%0Areal-world%20datasets.%20Quantitative%20results%20show%20that%20it%20can%20achieve%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01186v1&entry.124074799=Read"},
{"title": "Digital Twin Generators for Disease Modeling", "author": "Nameyeh Alam and Jake Basilico and Daniele Bertolini and Satish Casie Chetty and Heather D'Angelo and Ryan Douglas and Charles K. Fisher and Franklin Fuller and Melissa Gomes and Rishabh Gupta and Alex Lang and Anton Loukianov and Rachel Mak-McCully and Cary Murray and Hanalei Pham and Susanna Qiao and Elena Ryapolova-Webb and Aaron Smith and Dimitri Theoharatos and Anil Tolwani and Eric W. Tramel and Anna Vidovszky and Judy Viduya and Jonathan R. Walsh", "abstract": "  A patient's digital twin is a computational model that describes the\nevolution of their health over time. Digital twins have the potential to\nrevolutionize medicine by enabling individual-level computer simulations of\nhuman health, which can be used to conduct more efficient clinical trials or to\nrecommend personalized treatment options. Due to the overwhelming complexity of\nhuman biology, machine learning approaches that leverage large datasets of\nhistorical patients' longitudinal health records to generate patients' digital\ntwins are more tractable than potential mechanistic models. In this manuscript,\nwe describe a neural network architecture that can learn conditional generative\nmodels of clinical trajectories, which we call Digital Twin Generators (DTGs),\nthat can create digital twins of individual patients. We show that the same\nneural network architecture can be trained to generate accurate digital twins\nfor patients across 13 different indications simply by changing the training\nset and tuning hyperparameters. By introducing a general purpose architecture,\nwe aim to unlock the ability to scale machine learning approaches to larger\ndatasets and across more indications so that a digital twin could be created\nfor any patient in the world.\n", "link": "http://arxiv.org/abs/2405.01488v1", "date": "2024-05-02", "relevancy": 2.0184, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5134}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5036}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digital%20Twin%20Generators%20for%20Disease%20Modeling&body=Title%3A%20Digital%20Twin%20Generators%20for%20Disease%20Modeling%0AAuthor%3A%20Nameyeh%20Alam%20and%20Jake%20Basilico%20and%20Daniele%20Bertolini%20and%20Satish%20Casie%20Chetty%20and%20Heather%20D%27Angelo%20and%20Ryan%20Douglas%20and%20Charles%20K.%20Fisher%20and%20Franklin%20Fuller%20and%20Melissa%20Gomes%20and%20Rishabh%20Gupta%20and%20Alex%20Lang%20and%20Anton%20Loukianov%20and%20Rachel%20Mak-McCully%20and%20Cary%20Murray%20and%20Hanalei%20Pham%20and%20Susanna%20Qiao%20and%20Elena%20Ryapolova-Webb%20and%20Aaron%20Smith%20and%20Dimitri%20Theoharatos%20and%20Anil%20Tolwani%20and%20Eric%20W.%20Tramel%20and%20Anna%20Vidovszky%20and%20Judy%20Viduya%20and%20Jonathan%20R.%20Walsh%0AAbstract%3A%20%20%20A%20patient%27s%20digital%20twin%20is%20a%20computational%20model%20that%20describes%20the%0Aevolution%20of%20their%20health%20over%20time.%20Digital%20twins%20have%20the%20potential%20to%0Arevolutionize%20medicine%20by%20enabling%20individual-level%20computer%20simulations%20of%0Ahuman%20health%2C%20which%20can%20be%20used%20to%20conduct%20more%20efficient%20clinical%20trials%20or%20to%0Arecommend%20personalized%20treatment%20options.%20Due%20to%20the%20overwhelming%20complexity%20of%0Ahuman%20biology%2C%20machine%20learning%20approaches%20that%20leverage%20large%20datasets%20of%0Ahistorical%20patients%27%20longitudinal%20health%20records%20to%20generate%20patients%27%20digital%0Atwins%20are%20more%20tractable%20than%20potential%20mechanistic%20models.%20In%20this%20manuscript%2C%0Awe%20describe%20a%20neural%20network%20architecture%20that%20can%20learn%20conditional%20generative%0Amodels%20of%20clinical%20trajectories%2C%20which%20we%20call%20Digital%20Twin%20Generators%20%28DTGs%29%2C%0Athat%20can%20create%20digital%20twins%20of%20individual%20patients.%20We%20show%20that%20the%20same%0Aneural%20network%20architecture%20can%20be%20trained%20to%20generate%20accurate%20digital%20twins%0Afor%20patients%20across%2013%20different%20indications%20simply%20by%20changing%20the%20training%0Aset%20and%20tuning%20hyperparameters.%20By%20introducing%20a%20general%20purpose%20architecture%2C%0Awe%20aim%20to%20unlock%20the%20ability%20to%20scale%20machine%20learning%20approaches%20to%20larger%0Adatasets%20and%20across%20more%20indications%20so%20that%20a%20digital%20twin%20could%20be%20created%0Afor%20any%20patient%20in%20the%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigital%2520Twin%2520Generators%2520for%2520Disease%2520Modeling%26entry.906535625%3DNameyeh%2520Alam%2520and%2520Jake%2520Basilico%2520and%2520Daniele%2520Bertolini%2520and%2520Satish%2520Casie%2520Chetty%2520and%2520Heather%2520D%2527Angelo%2520and%2520Ryan%2520Douglas%2520and%2520Charles%2520K.%2520Fisher%2520and%2520Franklin%2520Fuller%2520and%2520Melissa%2520Gomes%2520and%2520Rishabh%2520Gupta%2520and%2520Alex%2520Lang%2520and%2520Anton%2520Loukianov%2520and%2520Rachel%2520Mak-McCully%2520and%2520Cary%2520Murray%2520and%2520Hanalei%2520Pham%2520and%2520Susanna%2520Qiao%2520and%2520Elena%2520Ryapolova-Webb%2520and%2520Aaron%2520Smith%2520and%2520Dimitri%2520Theoharatos%2520and%2520Anil%2520Tolwani%2520and%2520Eric%2520W.%2520Tramel%2520and%2520Anna%2520Vidovszky%2520and%2520Judy%2520Viduya%2520and%2520Jonathan%2520R.%2520Walsh%26entry.1292438233%3D%2520%2520A%2520patient%2527s%2520digital%2520twin%2520is%2520a%2520computational%2520model%2520that%2520describes%2520the%250Aevolution%2520of%2520their%2520health%2520over%2520time.%2520Digital%2520twins%2520have%2520the%2520potential%2520to%250Arevolutionize%2520medicine%2520by%2520enabling%2520individual-level%2520computer%2520simulations%2520of%250Ahuman%2520health%252C%2520which%2520can%2520be%2520used%2520to%2520conduct%2520more%2520efficient%2520clinical%2520trials%2520or%2520to%250Arecommend%2520personalized%2520treatment%2520options.%2520Due%2520to%2520the%2520overwhelming%2520complexity%2520of%250Ahuman%2520biology%252C%2520machine%2520learning%2520approaches%2520that%2520leverage%2520large%2520datasets%2520of%250Ahistorical%2520patients%2527%2520longitudinal%2520health%2520records%2520to%2520generate%2520patients%2527%2520digital%250Atwins%2520are%2520more%2520tractable%2520than%2520potential%2520mechanistic%2520models.%2520In%2520this%2520manuscript%252C%250Awe%2520describe%2520a%2520neural%2520network%2520architecture%2520that%2520can%2520learn%2520conditional%2520generative%250Amodels%2520of%2520clinical%2520trajectories%252C%2520which%2520we%2520call%2520Digital%2520Twin%2520Generators%2520%2528DTGs%2529%252C%250Athat%2520can%2520create%2520digital%2520twins%2520of%2520individual%2520patients.%2520We%2520show%2520that%2520the%2520same%250Aneural%2520network%2520architecture%2520can%2520be%2520trained%2520to%2520generate%2520accurate%2520digital%2520twins%250Afor%2520patients%2520across%252013%2520different%2520indications%2520simply%2520by%2520changing%2520the%2520training%250Aset%2520and%2520tuning%2520hyperparameters.%2520By%2520introducing%2520a%2520general%2520purpose%2520architecture%252C%250Awe%2520aim%2520to%2520unlock%2520the%2520ability%2520to%2520scale%2520machine%2520learning%2520approaches%2520to%2520larger%250Adatasets%2520and%2520across%2520more%2520indications%2520so%2520that%2520a%2520digital%2520twin%2520could%2520be%2520created%250Afor%2520any%2520patient%2520in%2520the%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20Twin%20Generators%20for%20Disease%20Modeling&entry.906535625=Nameyeh%20Alam%20and%20Jake%20Basilico%20and%20Daniele%20Bertolini%20and%20Satish%20Casie%20Chetty%20and%20Heather%20D%27Angelo%20and%20Ryan%20Douglas%20and%20Charles%20K.%20Fisher%20and%20Franklin%20Fuller%20and%20Melissa%20Gomes%20and%20Rishabh%20Gupta%20and%20Alex%20Lang%20and%20Anton%20Loukianov%20and%20Rachel%20Mak-McCully%20and%20Cary%20Murray%20and%20Hanalei%20Pham%20and%20Susanna%20Qiao%20and%20Elena%20Ryapolova-Webb%20and%20Aaron%20Smith%20and%20Dimitri%20Theoharatos%20and%20Anil%20Tolwani%20and%20Eric%20W.%20Tramel%20and%20Anna%20Vidovszky%20and%20Judy%20Viduya%20and%20Jonathan%20R.%20Walsh&entry.1292438233=%20%20A%20patient%27s%20digital%20twin%20is%20a%20computational%20model%20that%20describes%20the%0Aevolution%20of%20their%20health%20over%20time.%20Digital%20twins%20have%20the%20potential%20to%0Arevolutionize%20medicine%20by%20enabling%20individual-level%20computer%20simulations%20of%0Ahuman%20health%2C%20which%20can%20be%20used%20to%20conduct%20more%20efficient%20clinical%20trials%20or%20to%0Arecommend%20personalized%20treatment%20options.%20Due%20to%20the%20overwhelming%20complexity%20of%0Ahuman%20biology%2C%20machine%20learning%20approaches%20that%20leverage%20large%20datasets%20of%0Ahistorical%20patients%27%20longitudinal%20health%20records%20to%20generate%20patients%27%20digital%0Atwins%20are%20more%20tractable%20than%20potential%20mechanistic%20models.%20In%20this%20manuscript%2C%0Awe%20describe%20a%20neural%20network%20architecture%20that%20can%20learn%20conditional%20generative%0Amodels%20of%20clinical%20trajectories%2C%20which%20we%20call%20Digital%20Twin%20Generators%20%28DTGs%29%2C%0Athat%20can%20create%20digital%20twins%20of%20individual%20patients.%20We%20show%20that%20the%20same%0Aneural%20network%20architecture%20can%20be%20trained%20to%20generate%20accurate%20digital%20twins%0Afor%20patients%20across%2013%20different%20indications%20simply%20by%20changing%20the%20training%0Aset%20and%20tuning%20hyperparameters.%20By%20introducing%20a%20general%20purpose%20architecture%2C%0Awe%20aim%20to%20unlock%20the%20ability%20to%20scale%20machine%20learning%20approaches%20to%20larger%0Adatasets%20and%20across%20more%20indications%20so%20that%20a%20digital%20twin%20could%20be%20created%0Afor%20any%20patient%20in%20the%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01488v1&entry.124074799=Read"},
{"title": "Conformal Decision Theory: Safe Autonomous Decisions from Imperfect\n  Predictions", "author": "Jordan Lekeufack and Anastasios N. Angelopoulos and Andrea Bajcsy and Michael I. Jordan and Jitendra Malik", "abstract": "  We introduce Conformal Decision Theory, a framework for producing safe\nautonomous decisions despite imperfect machine learning predictions. Examples\nof such decisions are ubiquitous, from robot planning algorithms that rely on\npedestrian predictions, to calibrating autonomous manufacturing to exhibit high\nthroughput and low error, to the choice of trusting a nominal policy versus\nswitching to a safe backup policy at run-time. The decisions produced by our\nalgorithms are safe in the sense that they come with provable statistical\nguarantees of having low risk without any assumptions on the world model\nwhatsoever; the observations need not be I.I.D. and can even be adversarial.\nThe theory extends results from conformal prediction to calibrate decisions\ndirectly, without requiring the construction of prediction sets. Experiments\ndemonstrate the utility of our approach in robot motion planning around humans,\nautomated stock trading, and robot manufacturing.\n", "link": "http://arxiv.org/abs/2310.05921v3", "date": "2024-05-02", "relevancy": 2.0148, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.553}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5017}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Decision%20Theory%3A%20Safe%20Autonomous%20Decisions%20from%20Imperfect%0A%20%20Predictions&body=Title%3A%20Conformal%20Decision%20Theory%3A%20Safe%20Autonomous%20Decisions%20from%20Imperfect%0A%20%20Predictions%0AAuthor%3A%20Jordan%20Lekeufack%20and%20Anastasios%20N.%20Angelopoulos%20and%20Andrea%20Bajcsy%20and%20Michael%20I.%20Jordan%20and%20Jitendra%20Malik%0AAbstract%3A%20%20%20We%20introduce%20Conformal%20Decision%20Theory%2C%20a%20framework%20for%20producing%20safe%0Aautonomous%20decisions%20despite%20imperfect%20machine%20learning%20predictions.%20Examples%0Aof%20such%20decisions%20are%20ubiquitous%2C%20from%20robot%20planning%20algorithms%20that%20rely%20on%0Apedestrian%20predictions%2C%20to%20calibrating%20autonomous%20manufacturing%20to%20exhibit%20high%0Athroughput%20and%20low%20error%2C%20to%20the%20choice%20of%20trusting%20a%20nominal%20policy%20versus%0Aswitching%20to%20a%20safe%20backup%20policy%20at%20run-time.%20The%20decisions%20produced%20by%20our%0Aalgorithms%20are%20safe%20in%20the%20sense%20that%20they%20come%20with%20provable%20statistical%0Aguarantees%20of%20having%20low%20risk%20without%20any%20assumptions%20on%20the%20world%20model%0Awhatsoever%3B%20the%20observations%20need%20not%20be%20I.I.D.%20and%20can%20even%20be%20adversarial.%0AThe%20theory%20extends%20results%20from%20conformal%20prediction%20to%20calibrate%20decisions%0Adirectly%2C%20without%20requiring%20the%20construction%20of%20prediction%20sets.%20Experiments%0Ademonstrate%20the%20utility%20of%20our%20approach%20in%20robot%20motion%20planning%20around%20humans%2C%0Aautomated%20stock%20trading%2C%20and%20robot%20manufacturing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05921v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Decision%2520Theory%253A%2520Safe%2520Autonomous%2520Decisions%2520from%2520Imperfect%250A%2520%2520Predictions%26entry.906535625%3DJordan%2520Lekeufack%2520and%2520Anastasios%2520N.%2520Angelopoulos%2520and%2520Andrea%2520Bajcsy%2520and%2520Michael%2520I.%2520Jordan%2520and%2520Jitendra%2520Malik%26entry.1292438233%3D%2520%2520We%2520introduce%2520Conformal%2520Decision%2520Theory%252C%2520a%2520framework%2520for%2520producing%2520safe%250Aautonomous%2520decisions%2520despite%2520imperfect%2520machine%2520learning%2520predictions.%2520Examples%250Aof%2520such%2520decisions%2520are%2520ubiquitous%252C%2520from%2520robot%2520planning%2520algorithms%2520that%2520rely%2520on%250Apedestrian%2520predictions%252C%2520to%2520calibrating%2520autonomous%2520manufacturing%2520to%2520exhibit%2520high%250Athroughput%2520and%2520low%2520error%252C%2520to%2520the%2520choice%2520of%2520trusting%2520a%2520nominal%2520policy%2520versus%250Aswitching%2520to%2520a%2520safe%2520backup%2520policy%2520at%2520run-time.%2520The%2520decisions%2520produced%2520by%2520our%250Aalgorithms%2520are%2520safe%2520in%2520the%2520sense%2520that%2520they%2520come%2520with%2520provable%2520statistical%250Aguarantees%2520of%2520having%2520low%2520risk%2520without%2520any%2520assumptions%2520on%2520the%2520world%2520model%250Awhatsoever%253B%2520the%2520observations%2520need%2520not%2520be%2520I.I.D.%2520and%2520can%2520even%2520be%2520adversarial.%250AThe%2520theory%2520extends%2520results%2520from%2520conformal%2520prediction%2520to%2520calibrate%2520decisions%250Adirectly%252C%2520without%2520requiring%2520the%2520construction%2520of%2520prediction%2520sets.%2520Experiments%250Ademonstrate%2520the%2520utility%2520of%2520our%2520approach%2520in%2520robot%2520motion%2520planning%2520around%2520humans%252C%250Aautomated%2520stock%2520trading%252C%2520and%2520robot%2520manufacturing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05921v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Decision%20Theory%3A%20Safe%20Autonomous%20Decisions%20from%20Imperfect%0A%20%20Predictions&entry.906535625=Jordan%20Lekeufack%20and%20Anastasios%20N.%20Angelopoulos%20and%20Andrea%20Bajcsy%20and%20Michael%20I.%20Jordan%20and%20Jitendra%20Malik&entry.1292438233=%20%20We%20introduce%20Conformal%20Decision%20Theory%2C%20a%20framework%20for%20producing%20safe%0Aautonomous%20decisions%20despite%20imperfect%20machine%20learning%20predictions.%20Examples%0Aof%20such%20decisions%20are%20ubiquitous%2C%20from%20robot%20planning%20algorithms%20that%20rely%20on%0Apedestrian%20predictions%2C%20to%20calibrating%20autonomous%20manufacturing%20to%20exhibit%20high%0Athroughput%20and%20low%20error%2C%20to%20the%20choice%20of%20trusting%20a%20nominal%20policy%20versus%0Aswitching%20to%20a%20safe%20backup%20policy%20at%20run-time.%20The%20decisions%20produced%20by%20our%0Aalgorithms%20are%20safe%20in%20the%20sense%20that%20they%20come%20with%20provable%20statistical%0Aguarantees%20of%20having%20low%20risk%20without%20any%20assumptions%20on%20the%20world%20model%0Awhatsoever%3B%20the%20observations%20need%20not%20be%20I.I.D.%20and%20can%20even%20be%20adversarial.%0AThe%20theory%20extends%20results%20from%20conformal%20prediction%20to%20calibrate%20decisions%0Adirectly%2C%20without%20requiring%20the%20construction%20of%20prediction%20sets.%20Experiments%0Ademonstrate%20the%20utility%20of%20our%20approach%20in%20robot%20motion%20planning%20around%20humans%2C%0Aautomated%20stock%20trading%2C%20and%20robot%20manufacturing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05921v3&entry.124074799=Read"},
{"title": "MEGA-DAgger: Imitation Learning with Multiple Imperfect Experts", "author": "Xiatao Sun and Shuo Yang and Mingyan Zhou and Kunpeng Liu and Rahul Mangharam", "abstract": "  Imitation learning has been widely applied to various autonomous systems\nthanks to recent development in interactive algorithms that address covariate\nshift and compounding errors induced by traditional approaches like behavior\ncloning. However, existing interactive imitation learning methods assume access\nto one perfect expert. Whereas in reality, it is more likely to have multiple\nimperfect experts instead. In this paper, we propose MEGA-DAgger, a new DAgger\nvariant that is suitable for interactive learning with multiple imperfect\nexperts. First, unsafe demonstrations are filtered while aggregating the\ntraining data, so the imperfect demonstrations have little influence when\ntraining the novice policy. Next, experts are evaluated and compared on\nscenarios-specific metrics to resolve the conflicted labels among experts.\nThrough experiments in autonomous racing scenarios, we demonstrate that policy\nlearned using MEGA-DAgger can outperform both experts and policies learned\nusing the state-of-the-art interactive imitation learning algorithms such as\nHuman-Gated DAgger. The supplementary video can be found at\n\\url{https://youtu.be/wPCht31MHrw}.\n", "link": "http://arxiv.org/abs/2303.00638v3", "date": "2024-05-02", "relevancy": 2.0038, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5185}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4977}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGA-DAgger%3A%20Imitation%20Learning%20with%20Multiple%20Imperfect%20Experts&body=Title%3A%20MEGA-DAgger%3A%20Imitation%20Learning%20with%20Multiple%20Imperfect%20Experts%0AAuthor%3A%20Xiatao%20Sun%20and%20Shuo%20Yang%20and%20Mingyan%20Zhou%20and%20Kunpeng%20Liu%20and%20Rahul%20Mangharam%0AAbstract%3A%20%20%20Imitation%20learning%20has%20been%20widely%20applied%20to%20various%20autonomous%20systems%0Athanks%20to%20recent%20development%20in%20interactive%20algorithms%20that%20address%20covariate%0Ashift%20and%20compounding%20errors%20induced%20by%20traditional%20approaches%20like%20behavior%0Acloning.%20However%2C%20existing%20interactive%20imitation%20learning%20methods%20assume%20access%0Ato%20one%20perfect%20expert.%20Whereas%20in%20reality%2C%20it%20is%20more%20likely%20to%20have%20multiple%0Aimperfect%20experts%20instead.%20In%20this%20paper%2C%20we%20propose%20MEGA-DAgger%2C%20a%20new%20DAgger%0Avariant%20that%20is%20suitable%20for%20interactive%20learning%20with%20multiple%20imperfect%0Aexperts.%20First%2C%20unsafe%20demonstrations%20are%20filtered%20while%20aggregating%20the%0Atraining%20data%2C%20so%20the%20imperfect%20demonstrations%20have%20little%20influence%20when%0Atraining%20the%20novice%20policy.%20Next%2C%20experts%20are%20evaluated%20and%20compared%20on%0Ascenarios-specific%20metrics%20to%20resolve%20the%20conflicted%20labels%20among%20experts.%0AThrough%20experiments%20in%20autonomous%20racing%20scenarios%2C%20we%20demonstrate%20that%20policy%0Alearned%20using%20MEGA-DAgger%20can%20outperform%20both%20experts%20and%20policies%20learned%0Ausing%20the%20state-of-the-art%20interactive%20imitation%20learning%20algorithms%20such%20as%0AHuman-Gated%20DAgger.%20The%20supplementary%20video%20can%20be%20found%20at%0A%5Curl%7Bhttps%3A//youtu.be/wPCht31MHrw%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.00638v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGA-DAgger%253A%2520Imitation%2520Learning%2520with%2520Multiple%2520Imperfect%2520Experts%26entry.906535625%3DXiatao%2520Sun%2520and%2520Shuo%2520Yang%2520and%2520Mingyan%2520Zhou%2520and%2520Kunpeng%2520Liu%2520and%2520Rahul%2520Mangharam%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520has%2520been%2520widely%2520applied%2520to%2520various%2520autonomous%2520systems%250Athanks%2520to%2520recent%2520development%2520in%2520interactive%2520algorithms%2520that%2520address%2520covariate%250Ashift%2520and%2520compounding%2520errors%2520induced%2520by%2520traditional%2520approaches%2520like%2520behavior%250Acloning.%2520However%252C%2520existing%2520interactive%2520imitation%2520learning%2520methods%2520assume%2520access%250Ato%2520one%2520perfect%2520expert.%2520Whereas%2520in%2520reality%252C%2520it%2520is%2520more%2520likely%2520to%2520have%2520multiple%250Aimperfect%2520experts%2520instead.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MEGA-DAgger%252C%2520a%2520new%2520DAgger%250Avariant%2520that%2520is%2520suitable%2520for%2520interactive%2520learning%2520with%2520multiple%2520imperfect%250Aexperts.%2520First%252C%2520unsafe%2520demonstrations%2520are%2520filtered%2520while%2520aggregating%2520the%250Atraining%2520data%252C%2520so%2520the%2520imperfect%2520demonstrations%2520have%2520little%2520influence%2520when%250Atraining%2520the%2520novice%2520policy.%2520Next%252C%2520experts%2520are%2520evaluated%2520and%2520compared%2520on%250Ascenarios-specific%2520metrics%2520to%2520resolve%2520the%2520conflicted%2520labels%2520among%2520experts.%250AThrough%2520experiments%2520in%2520autonomous%2520racing%2520scenarios%252C%2520we%2520demonstrate%2520that%2520policy%250Alearned%2520using%2520MEGA-DAgger%2520can%2520outperform%2520both%2520experts%2520and%2520policies%2520learned%250Ausing%2520the%2520state-of-the-art%2520interactive%2520imitation%2520learning%2520algorithms%2520such%2520as%250AHuman-Gated%2520DAgger.%2520The%2520supplementary%2520video%2520can%2520be%2520found%2520at%250A%255Curl%257Bhttps%253A//youtu.be/wPCht31MHrw%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.00638v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGA-DAgger%3A%20Imitation%20Learning%20with%20Multiple%20Imperfect%20Experts&entry.906535625=Xiatao%20Sun%20and%20Shuo%20Yang%20and%20Mingyan%20Zhou%20and%20Kunpeng%20Liu%20and%20Rahul%20Mangharam&entry.1292438233=%20%20Imitation%20learning%20has%20been%20widely%20applied%20to%20various%20autonomous%20systems%0Athanks%20to%20recent%20development%20in%20interactive%20algorithms%20that%20address%20covariate%0Ashift%20and%20compounding%20errors%20induced%20by%20traditional%20approaches%20like%20behavior%0Acloning.%20However%2C%20existing%20interactive%20imitation%20learning%20methods%20assume%20access%0Ato%20one%20perfect%20expert.%20Whereas%20in%20reality%2C%20it%20is%20more%20likely%20to%20have%20multiple%0Aimperfect%20experts%20instead.%20In%20this%20paper%2C%20we%20propose%20MEGA-DAgger%2C%20a%20new%20DAgger%0Avariant%20that%20is%20suitable%20for%20interactive%20learning%20with%20multiple%20imperfect%0Aexperts.%20First%2C%20unsafe%20demonstrations%20are%20filtered%20while%20aggregating%20the%0Atraining%20data%2C%20so%20the%20imperfect%20demonstrations%20have%20little%20influence%20when%0Atraining%20the%20novice%20policy.%20Next%2C%20experts%20are%20evaluated%20and%20compared%20on%0Ascenarios-specific%20metrics%20to%20resolve%20the%20conflicted%20labels%20among%20experts.%0AThrough%20experiments%20in%20autonomous%20racing%20scenarios%2C%20we%20demonstrate%20that%20policy%0Alearned%20using%20MEGA-DAgger%20can%20outperform%20both%20experts%20and%20policies%20learned%0Ausing%20the%20state-of-the-art%20interactive%20imitation%20learning%20algorithms%20such%20as%0AHuman-Gated%20DAgger.%20The%20supplementary%20video%20can%20be%20found%20at%0A%5Curl%7Bhttps%3A//youtu.be/wPCht31MHrw%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.00638v3&entry.124074799=Read"},
{"title": "An Advanced Framework for Ultra-Realistic Simulation and Digital\n  Twinning for Autonomous Vehicles", "author": "Yuankai He and Hanlin Chen and Weisong Shi", "abstract": "  Simulation is a fundamental tool in developing autonomous vehicles, enabling\nrigorous testing without the logistical and safety challenges associated with\nreal-world trials. As autonomous vehicle technologies evolve and public safety\ndemands increase, advanced, realistic simulation frameworks are critical.\nCurrent testing paradigms employ a mix of general-purpose and specialized\nsimulators, such as CARLA and IVRESS, to achieve high-fidelity results.\nHowever, these tools often struggle with compatibility due to differing\nplatform, hardware, and software requirements, severely hampering their\ncombined effectiveness. This paper introduces BlueICE, an advanced framework\nfor ultra-realistic simulation and digital twinning, to address these\nchallenges. BlueICE's innovative architecture allows for the decoupling of\ncomputing platforms, hardware, and software dependencies while offering\nresearchers customizable testing environments to meet diverse fidelity needs.\nKey features include containerization to ensure compatibility across different\nsystems, a unified communication bridge for seamless integration of various\nsimulation tools, and synchronized orchestration of input and output across\nsimulators. This framework facilitates the development of sophisticated digital\ntwins for autonomous vehicle testing and sets a new standard in simulation\naccuracy and flexibility. The paper further explores the application of BlueICE\nin two distinct case studies: the ICAT indoor testbed and the STAR campus\noutdoor testbed at the University of Delaware. These case studies demonstrate\nBlueICE's capability to create sophisticated digital twins for autonomous\nvehicle testing and underline its potential as a standardized testbed for\nfuture autonomous driving technologies.\n", "link": "http://arxiv.org/abs/2405.01328v1", "date": "2024-05-02", "relevancy": 2.0007, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4986}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Advanced%20Framework%20for%20Ultra-Realistic%20Simulation%20and%20Digital%0A%20%20Twinning%20for%20Autonomous%20Vehicles&body=Title%3A%20An%20Advanced%20Framework%20for%20Ultra-Realistic%20Simulation%20and%20Digital%0A%20%20Twinning%20for%20Autonomous%20Vehicles%0AAuthor%3A%20Yuankai%20He%20and%20Hanlin%20Chen%20and%20Weisong%20Shi%0AAbstract%3A%20%20%20Simulation%20is%20a%20fundamental%20tool%20in%20developing%20autonomous%20vehicles%2C%20enabling%0Arigorous%20testing%20without%20the%20logistical%20and%20safety%20challenges%20associated%20with%0Areal-world%20trials.%20As%20autonomous%20vehicle%20technologies%20evolve%20and%20public%20safety%0Ademands%20increase%2C%20advanced%2C%20realistic%20simulation%20frameworks%20are%20critical.%0ACurrent%20testing%20paradigms%20employ%20a%20mix%20of%20general-purpose%20and%20specialized%0Asimulators%2C%20such%20as%20CARLA%20and%20IVRESS%2C%20to%20achieve%20high-fidelity%20results.%0AHowever%2C%20these%20tools%20often%20struggle%20with%20compatibility%20due%20to%20differing%0Aplatform%2C%20hardware%2C%20and%20software%20requirements%2C%20severely%20hampering%20their%0Acombined%20effectiveness.%20This%20paper%20introduces%20BlueICE%2C%20an%20advanced%20framework%0Afor%20ultra-realistic%20simulation%20and%20digital%20twinning%2C%20to%20address%20these%0Achallenges.%20BlueICE%27s%20innovative%20architecture%20allows%20for%20the%20decoupling%20of%0Acomputing%20platforms%2C%20hardware%2C%20and%20software%20dependencies%20while%20offering%0Aresearchers%20customizable%20testing%20environments%20to%20meet%20diverse%20fidelity%20needs.%0AKey%20features%20include%20containerization%20to%20ensure%20compatibility%20across%20different%0Asystems%2C%20a%20unified%20communication%20bridge%20for%20seamless%20integration%20of%20various%0Asimulation%20tools%2C%20and%20synchronized%20orchestration%20of%20input%20and%20output%20across%0Asimulators.%20This%20framework%20facilitates%20the%20development%20of%20sophisticated%20digital%0Atwins%20for%20autonomous%20vehicle%20testing%20and%20sets%20a%20new%20standard%20in%20simulation%0Aaccuracy%20and%20flexibility.%20The%20paper%20further%20explores%20the%20application%20of%20BlueICE%0Ain%20two%20distinct%20case%20studies%3A%20the%20ICAT%20indoor%20testbed%20and%20the%20STAR%20campus%0Aoutdoor%20testbed%20at%20the%20University%20of%20Delaware.%20These%20case%20studies%20demonstrate%0ABlueICE%27s%20capability%20to%20create%20sophisticated%20digital%20twins%20for%20autonomous%0Avehicle%20testing%20and%20underline%20its%20potential%20as%20a%20standardized%20testbed%20for%0Afuture%20autonomous%20driving%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Advanced%2520Framework%2520for%2520Ultra-Realistic%2520Simulation%2520and%2520Digital%250A%2520%2520Twinning%2520for%2520Autonomous%2520Vehicles%26entry.906535625%3DYuankai%2520He%2520and%2520Hanlin%2520Chen%2520and%2520Weisong%2520Shi%26entry.1292438233%3D%2520%2520Simulation%2520is%2520a%2520fundamental%2520tool%2520in%2520developing%2520autonomous%2520vehicles%252C%2520enabling%250Arigorous%2520testing%2520without%2520the%2520logistical%2520and%2520safety%2520challenges%2520associated%2520with%250Areal-world%2520trials.%2520As%2520autonomous%2520vehicle%2520technologies%2520evolve%2520and%2520public%2520safety%250Ademands%2520increase%252C%2520advanced%252C%2520realistic%2520simulation%2520frameworks%2520are%2520critical.%250ACurrent%2520testing%2520paradigms%2520employ%2520a%2520mix%2520of%2520general-purpose%2520and%2520specialized%250Asimulators%252C%2520such%2520as%2520CARLA%2520and%2520IVRESS%252C%2520to%2520achieve%2520high-fidelity%2520results.%250AHowever%252C%2520these%2520tools%2520often%2520struggle%2520with%2520compatibility%2520due%2520to%2520differing%250Aplatform%252C%2520hardware%252C%2520and%2520software%2520requirements%252C%2520severely%2520hampering%2520their%250Acombined%2520effectiveness.%2520This%2520paper%2520introduces%2520BlueICE%252C%2520an%2520advanced%2520framework%250Afor%2520ultra-realistic%2520simulation%2520and%2520digital%2520twinning%252C%2520to%2520address%2520these%250Achallenges.%2520BlueICE%2527s%2520innovative%2520architecture%2520allows%2520for%2520the%2520decoupling%2520of%250Acomputing%2520platforms%252C%2520hardware%252C%2520and%2520software%2520dependencies%2520while%2520offering%250Aresearchers%2520customizable%2520testing%2520environments%2520to%2520meet%2520diverse%2520fidelity%2520needs.%250AKey%2520features%2520include%2520containerization%2520to%2520ensure%2520compatibility%2520across%2520different%250Asystems%252C%2520a%2520unified%2520communication%2520bridge%2520for%2520seamless%2520integration%2520of%2520various%250Asimulation%2520tools%252C%2520and%2520synchronized%2520orchestration%2520of%2520input%2520and%2520output%2520across%250Asimulators.%2520This%2520framework%2520facilitates%2520the%2520development%2520of%2520sophisticated%2520digital%250Atwins%2520for%2520autonomous%2520vehicle%2520testing%2520and%2520sets%2520a%2520new%2520standard%2520in%2520simulation%250Aaccuracy%2520and%2520flexibility.%2520The%2520paper%2520further%2520explores%2520the%2520application%2520of%2520BlueICE%250Ain%2520two%2520distinct%2520case%2520studies%253A%2520the%2520ICAT%2520indoor%2520testbed%2520and%2520the%2520STAR%2520campus%250Aoutdoor%2520testbed%2520at%2520the%2520University%2520of%2520Delaware.%2520These%2520case%2520studies%2520demonstrate%250ABlueICE%2527s%2520capability%2520to%2520create%2520sophisticated%2520digital%2520twins%2520for%2520autonomous%250Avehicle%2520testing%2520and%2520underline%2520its%2520potential%2520as%2520a%2520standardized%2520testbed%2520for%250Afuture%2520autonomous%2520driving%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Advanced%20Framework%20for%20Ultra-Realistic%20Simulation%20and%20Digital%0A%20%20Twinning%20for%20Autonomous%20Vehicles&entry.906535625=Yuankai%20He%20and%20Hanlin%20Chen%20and%20Weisong%20Shi&entry.1292438233=%20%20Simulation%20is%20a%20fundamental%20tool%20in%20developing%20autonomous%20vehicles%2C%20enabling%0Arigorous%20testing%20without%20the%20logistical%20and%20safety%20challenges%20associated%20with%0Areal-world%20trials.%20As%20autonomous%20vehicle%20technologies%20evolve%20and%20public%20safety%0Ademands%20increase%2C%20advanced%2C%20realistic%20simulation%20frameworks%20are%20critical.%0ACurrent%20testing%20paradigms%20employ%20a%20mix%20of%20general-purpose%20and%20specialized%0Asimulators%2C%20such%20as%20CARLA%20and%20IVRESS%2C%20to%20achieve%20high-fidelity%20results.%0AHowever%2C%20these%20tools%20often%20struggle%20with%20compatibility%20due%20to%20differing%0Aplatform%2C%20hardware%2C%20and%20software%20requirements%2C%20severely%20hampering%20their%0Acombined%20effectiveness.%20This%20paper%20introduces%20BlueICE%2C%20an%20advanced%20framework%0Afor%20ultra-realistic%20simulation%20and%20digital%20twinning%2C%20to%20address%20these%0Achallenges.%20BlueICE%27s%20innovative%20architecture%20allows%20for%20the%20decoupling%20of%0Acomputing%20platforms%2C%20hardware%2C%20and%20software%20dependencies%20while%20offering%0Aresearchers%20customizable%20testing%20environments%20to%20meet%20diverse%20fidelity%20needs.%0AKey%20features%20include%20containerization%20to%20ensure%20compatibility%20across%20different%0Asystems%2C%20a%20unified%20communication%20bridge%20for%20seamless%20integration%20of%20various%0Asimulation%20tools%2C%20and%20synchronized%20orchestration%20of%20input%20and%20output%20across%0Asimulators.%20This%20framework%20facilitates%20the%20development%20of%20sophisticated%20digital%0Atwins%20for%20autonomous%20vehicle%20testing%20and%20sets%20a%20new%20standard%20in%20simulation%0Aaccuracy%20and%20flexibility.%20The%20paper%20further%20explores%20the%20application%20of%20BlueICE%0Ain%20two%20distinct%20case%20studies%3A%20the%20ICAT%20indoor%20testbed%20and%20the%20STAR%20campus%0Aoutdoor%20testbed%20at%20the%20University%20of%20Delaware.%20These%20case%20studies%20demonstrate%0ABlueICE%27s%20capability%20to%20create%20sophisticated%20digital%20twins%20for%20autonomous%0Avehicle%20testing%20and%20underline%20its%20potential%20as%20a%20standardized%20testbed%20for%0Afuture%20autonomous%20driving%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01328v1&entry.124074799=Read"},
{"title": "Disentangled Representation Learning", "author": "Xin Wang and Hong Chen and Si'ao Tang and Zihao Wu and Wenwu Zhu", "abstract": "  Disentangled Representation Learning (DRL) aims to learn a model capable of\nidentifying and disentangling the underlying factors hidden in the observable\ndata in representation form. The process of separating underlying factors of\nvariation into variables with semantic meaning benefits in learning explainable\nrepresentations of data, which imitates the meaningful understanding process of\nhumans when observing an object or relation. As a general learning strategy,\nDRL has demonstrated its power in improving the model explainability,\ncontrolability, robustness, as well as generalization capacity in a wide range\nof scenarios such as computer vision, natural language processing, and data\nmining. In this article, we comprehensively investigate DRL from various\naspects including motivations, definitions, methodologies, evaluations,\napplications, and model designs. We first present two well-recognized\ndefinitions, i.e., Intuitive Definition and Group Theory Definition for\ndisentangled representation learning. We further categorize the methodologies\nfor DRL into four groups from the following perspectives, the model type,\nrepresentation structure, supervision signal, and independence assumption. We\nalso analyze principles to design different DRL models that may benefit\ndifferent tasks in practical applications. Finally, we point out challenges in\nDRL as well as potential research directions deserving future investigations.\nWe believe this work may provide insights for promoting the DRL research in the\ncommunity.\n", "link": "http://arxiv.org/abs/2211.11695v3", "date": "2024-05-02", "relevancy": 1.9904, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5161}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4888}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Representation%20Learning&body=Title%3A%20Disentangled%20Representation%20Learning%0AAuthor%3A%20Xin%20Wang%20and%20Hong%20Chen%20and%20Si%27ao%20Tang%20and%20Zihao%20Wu%20and%20Wenwu%20Zhu%0AAbstract%3A%20%20%20Disentangled%20Representation%20Learning%20%28DRL%29%20aims%20to%20learn%20a%20model%20capable%20of%0Aidentifying%20and%20disentangling%20the%20underlying%20factors%20hidden%20in%20the%20observable%0Adata%20in%20representation%20form.%20The%20process%20of%20separating%20underlying%20factors%20of%0Avariation%20into%20variables%20with%20semantic%20meaning%20benefits%20in%20learning%20explainable%0Arepresentations%20of%20data%2C%20which%20imitates%20the%20meaningful%20understanding%20process%20of%0Ahumans%20when%20observing%20an%20object%20or%20relation.%20As%20a%20general%20learning%20strategy%2C%0ADRL%20has%20demonstrated%20its%20power%20in%20improving%20the%20model%20explainability%2C%0Acontrolability%2C%20robustness%2C%20as%20well%20as%20generalization%20capacity%20in%20a%20wide%20range%0Aof%20scenarios%20such%20as%20computer%20vision%2C%20natural%20language%20processing%2C%20and%20data%0Amining.%20In%20this%20article%2C%20we%20comprehensively%20investigate%20DRL%20from%20various%0Aaspects%20including%20motivations%2C%20definitions%2C%20methodologies%2C%20evaluations%2C%0Aapplications%2C%20and%20model%20designs.%20We%20first%20present%20two%20well-recognized%0Adefinitions%2C%20i.e.%2C%20Intuitive%20Definition%20and%20Group%20Theory%20Definition%20for%0Adisentangled%20representation%20learning.%20We%20further%20categorize%20the%20methodologies%0Afor%20DRL%20into%20four%20groups%20from%20the%20following%20perspectives%2C%20the%20model%20type%2C%0Arepresentation%20structure%2C%20supervision%20signal%2C%20and%20independence%20assumption.%20We%0Aalso%20analyze%20principles%20to%20design%20different%20DRL%20models%20that%20may%20benefit%0Adifferent%20tasks%20in%20practical%20applications.%20Finally%2C%20we%20point%20out%20challenges%20in%0ADRL%20as%20well%20as%20potential%20research%20directions%20deserving%20future%20investigations.%0AWe%20believe%20this%20work%20may%20provide%20insights%20for%20promoting%20the%20DRL%20research%20in%20the%0Acommunity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.11695v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Representation%2520Learning%26entry.906535625%3DXin%2520Wang%2520and%2520Hong%2520Chen%2520and%2520Si%2527ao%2520Tang%2520and%2520Zihao%2520Wu%2520and%2520Wenwu%2520Zhu%26entry.1292438233%3D%2520%2520Disentangled%2520Representation%2520Learning%2520%2528DRL%2529%2520aims%2520to%2520learn%2520a%2520model%2520capable%2520of%250Aidentifying%2520and%2520disentangling%2520the%2520underlying%2520factors%2520hidden%2520in%2520the%2520observable%250Adata%2520in%2520representation%2520form.%2520The%2520process%2520of%2520separating%2520underlying%2520factors%2520of%250Avariation%2520into%2520variables%2520with%2520semantic%2520meaning%2520benefits%2520in%2520learning%2520explainable%250Arepresentations%2520of%2520data%252C%2520which%2520imitates%2520the%2520meaningful%2520understanding%2520process%2520of%250Ahumans%2520when%2520observing%2520an%2520object%2520or%2520relation.%2520As%2520a%2520general%2520learning%2520strategy%252C%250ADRL%2520has%2520demonstrated%2520its%2520power%2520in%2520improving%2520the%2520model%2520explainability%252C%250Acontrolability%252C%2520robustness%252C%2520as%2520well%2520as%2520generalization%2520capacity%2520in%2520a%2520wide%2520range%250Aof%2520scenarios%2520such%2520as%2520computer%2520vision%252C%2520natural%2520language%2520processing%252C%2520and%2520data%250Amining.%2520In%2520this%2520article%252C%2520we%2520comprehensively%2520investigate%2520DRL%2520from%2520various%250Aaspects%2520including%2520motivations%252C%2520definitions%252C%2520methodologies%252C%2520evaluations%252C%250Aapplications%252C%2520and%2520model%2520designs.%2520We%2520first%2520present%2520two%2520well-recognized%250Adefinitions%252C%2520i.e.%252C%2520Intuitive%2520Definition%2520and%2520Group%2520Theory%2520Definition%2520for%250Adisentangled%2520representation%2520learning.%2520We%2520further%2520categorize%2520the%2520methodologies%250Afor%2520DRL%2520into%2520four%2520groups%2520from%2520the%2520following%2520perspectives%252C%2520the%2520model%2520type%252C%250Arepresentation%2520structure%252C%2520supervision%2520signal%252C%2520and%2520independence%2520assumption.%2520We%250Aalso%2520analyze%2520principles%2520to%2520design%2520different%2520DRL%2520models%2520that%2520may%2520benefit%250Adifferent%2520tasks%2520in%2520practical%2520applications.%2520Finally%252C%2520we%2520point%2520out%2520challenges%2520in%250ADRL%2520as%2520well%2520as%2520potential%2520research%2520directions%2520deserving%2520future%2520investigations.%250AWe%2520believe%2520this%2520work%2520may%2520provide%2520insights%2520for%2520promoting%2520the%2520DRL%2520research%2520in%2520the%250Acommunity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.11695v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Representation%20Learning&entry.906535625=Xin%20Wang%20and%20Hong%20Chen%20and%20Si%27ao%20Tang%20and%20Zihao%20Wu%20and%20Wenwu%20Zhu&entry.1292438233=%20%20Disentangled%20Representation%20Learning%20%28DRL%29%20aims%20to%20learn%20a%20model%20capable%20of%0Aidentifying%20and%20disentangling%20the%20underlying%20factors%20hidden%20in%20the%20observable%0Adata%20in%20representation%20form.%20The%20process%20of%20separating%20underlying%20factors%20of%0Avariation%20into%20variables%20with%20semantic%20meaning%20benefits%20in%20learning%20explainable%0Arepresentations%20of%20data%2C%20which%20imitates%20the%20meaningful%20understanding%20process%20of%0Ahumans%20when%20observing%20an%20object%20or%20relation.%20As%20a%20general%20learning%20strategy%2C%0ADRL%20has%20demonstrated%20its%20power%20in%20improving%20the%20model%20explainability%2C%0Acontrolability%2C%20robustness%2C%20as%20well%20as%20generalization%20capacity%20in%20a%20wide%20range%0Aof%20scenarios%20such%20as%20computer%20vision%2C%20natural%20language%20processing%2C%20and%20data%0Amining.%20In%20this%20article%2C%20we%20comprehensively%20investigate%20DRL%20from%20various%0Aaspects%20including%20motivations%2C%20definitions%2C%20methodologies%2C%20evaluations%2C%0Aapplications%2C%20and%20model%20designs.%20We%20first%20present%20two%20well-recognized%0Adefinitions%2C%20i.e.%2C%20Intuitive%20Definition%20and%20Group%20Theory%20Definition%20for%0Adisentangled%20representation%20learning.%20We%20further%20categorize%20the%20methodologies%0Afor%20DRL%20into%20four%20groups%20from%20the%20following%20perspectives%2C%20the%20model%20type%2C%0Arepresentation%20structure%2C%20supervision%20signal%2C%20and%20independence%20assumption.%20We%0Aalso%20analyze%20principles%20to%20design%20different%20DRL%20models%20that%20may%20benefit%0Adifferent%20tasks%20in%20practical%20applications.%20Finally%2C%20we%20point%20out%20challenges%20in%0ADRL%20as%20well%20as%20potential%20research%20directions%20deserving%20future%20investigations.%0AWe%20believe%20this%20work%20may%20provide%20insights%20for%20promoting%20the%20DRL%20research%20in%20the%0Acommunity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.11695v3&entry.124074799=Read"},
{"title": "TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio\n  and Bone Conduction Speech Super Resolution and Enhancement on Mobile and\n  Wearable Platforms", "author": "Yueyuan Sui and Minghui Zhao and Junxi Xia and Xiaofan Jiang and Stephen Xia", "abstract": "  We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic\nand bone conduction speech enhancement, suitable for mobile and wearable\nplatforms. Bone conduction speech enhancement has been impractical to adopt in\nmobile and wearable platforms for several reasons: (i) data collection is\nlabor-intensive, resulting in scarcity; (ii) there exists a performance gap\nbetween state of-art models with memory footprints of hundreds of MBs and\nmethods better suited for resource-constrained systems. To adapt TRAMBA to\nvibration-based sensing modalities, we pre-train TRAMBA with audio speech\ndatasets that are widely available. Then, users fine-tune with a small amount\nof bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in\nPESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and\nan inference speed up of up to 465 times. We integrate TRAMBA into real systems\nand show that TRAMBA (i) improves battery life of wearables by up to 160% by\nrequiring less data sampling and transmission; (ii) generates higher quality\nvoice in noisy environments than over-the-air speech; (iii) requires a memory\nfootprint of less than 20.0 MB.\n", "link": "http://arxiv.org/abs/2405.01242v1", "date": "2024-05-02", "relevancy": 1.9869, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.504}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4964}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRAMBA%3A%20A%20Hybrid%20Transformer%20and%20Mamba%20Architecture%20for%20Practical%20Audio%0A%20%20and%20Bone%20Conduction%20Speech%20Super%20Resolution%20and%20Enhancement%20on%20Mobile%20and%0A%20%20Wearable%20Platforms&body=Title%3A%20TRAMBA%3A%20A%20Hybrid%20Transformer%20and%20Mamba%20Architecture%20for%20Practical%20Audio%0A%20%20and%20Bone%20Conduction%20Speech%20Super%20Resolution%20and%20Enhancement%20on%20Mobile%20and%0A%20%20Wearable%20Platforms%0AAuthor%3A%20Yueyuan%20Sui%20and%20Minghui%20Zhao%20and%20Junxi%20Xia%20and%20Xiaofan%20Jiang%20and%20Stephen%20Xia%0AAbstract%3A%20%20%20We%20propose%20TRAMBA%2C%20a%20hybrid%20transformer%20and%20Mamba%20architecture%20for%20acoustic%0Aand%20bone%20conduction%20speech%20enhancement%2C%20suitable%20for%20mobile%20and%20wearable%0Aplatforms.%20Bone%20conduction%20speech%20enhancement%20has%20been%20impractical%20to%20adopt%20in%0Amobile%20and%20wearable%20platforms%20for%20several%20reasons%3A%20%28i%29%20data%20collection%20is%0Alabor-intensive%2C%20resulting%20in%20scarcity%3B%20%28ii%29%20there%20exists%20a%20performance%20gap%0Abetween%20state%20of-art%20models%20with%20memory%20footprints%20of%20hundreds%20of%20MBs%20and%0Amethods%20better%20suited%20for%20resource-constrained%20systems.%20To%20adapt%20TRAMBA%20to%0Avibration-based%20sensing%20modalities%2C%20we%20pre-train%20TRAMBA%20with%20audio%20speech%0Adatasets%20that%20are%20widely%20available.%20Then%2C%20users%20fine-tune%20with%20a%20small%20amount%0Aof%20bone%20conduction%20data.%20TRAMBA%20outperforms%20state-of-art%20GANs%20by%20up%20to%207.3%25%20in%0APESQ%20and%201.8%25%20in%20STOI%2C%20with%20an%20order%20of%20magnitude%20smaller%20memory%20footprint%20and%0Aan%20inference%20speed%20up%20of%20up%20to%20465%20times.%20We%20integrate%20TRAMBA%20into%20real%20systems%0Aand%20show%20that%20TRAMBA%20%28i%29%20improves%20battery%20life%20of%20wearables%20by%20up%20to%20160%25%20by%0Arequiring%20less%20data%20sampling%20and%20transmission%3B%20%28ii%29%20generates%20higher%20quality%0Avoice%20in%20noisy%20environments%20than%20over-the-air%20speech%3B%20%28iii%29%20requires%20a%20memory%0Afootprint%20of%20less%20than%2020.0%20MB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRAMBA%253A%2520A%2520Hybrid%2520Transformer%2520and%2520Mamba%2520Architecture%2520for%2520Practical%2520Audio%250A%2520%2520and%2520Bone%2520Conduction%2520Speech%2520Super%2520Resolution%2520and%2520Enhancement%2520on%2520Mobile%2520and%250A%2520%2520Wearable%2520Platforms%26entry.906535625%3DYueyuan%2520Sui%2520and%2520Minghui%2520Zhao%2520and%2520Junxi%2520Xia%2520and%2520Xiaofan%2520Jiang%2520and%2520Stephen%2520Xia%26entry.1292438233%3D%2520%2520We%2520propose%2520TRAMBA%252C%2520a%2520hybrid%2520transformer%2520and%2520Mamba%2520architecture%2520for%2520acoustic%250Aand%2520bone%2520conduction%2520speech%2520enhancement%252C%2520suitable%2520for%2520mobile%2520and%2520wearable%250Aplatforms.%2520Bone%2520conduction%2520speech%2520enhancement%2520has%2520been%2520impractical%2520to%2520adopt%2520in%250Amobile%2520and%2520wearable%2520platforms%2520for%2520several%2520reasons%253A%2520%2528i%2529%2520data%2520collection%2520is%250Alabor-intensive%252C%2520resulting%2520in%2520scarcity%253B%2520%2528ii%2529%2520there%2520exists%2520a%2520performance%2520gap%250Abetween%2520state%2520of-art%2520models%2520with%2520memory%2520footprints%2520of%2520hundreds%2520of%2520MBs%2520and%250Amethods%2520better%2520suited%2520for%2520resource-constrained%2520systems.%2520To%2520adapt%2520TRAMBA%2520to%250Avibration-based%2520sensing%2520modalities%252C%2520we%2520pre-train%2520TRAMBA%2520with%2520audio%2520speech%250Adatasets%2520that%2520are%2520widely%2520available.%2520Then%252C%2520users%2520fine-tune%2520with%2520a%2520small%2520amount%250Aof%2520bone%2520conduction%2520data.%2520TRAMBA%2520outperforms%2520state-of-art%2520GANs%2520by%2520up%2520to%25207.3%2525%2520in%250APESQ%2520and%25201.8%2525%2520in%2520STOI%252C%2520with%2520an%2520order%2520of%2520magnitude%2520smaller%2520memory%2520footprint%2520and%250Aan%2520inference%2520speed%2520up%2520of%2520up%2520to%2520465%2520times.%2520We%2520integrate%2520TRAMBA%2520into%2520real%2520systems%250Aand%2520show%2520that%2520TRAMBA%2520%2528i%2529%2520improves%2520battery%2520life%2520of%2520wearables%2520by%2520up%2520to%2520160%2525%2520by%250Arequiring%2520less%2520data%2520sampling%2520and%2520transmission%253B%2520%2528ii%2529%2520generates%2520higher%2520quality%250Avoice%2520in%2520noisy%2520environments%2520than%2520over-the-air%2520speech%253B%2520%2528iii%2529%2520requires%2520a%2520memory%250Afootprint%2520of%2520less%2520than%252020.0%2520MB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRAMBA%3A%20A%20Hybrid%20Transformer%20and%20Mamba%20Architecture%20for%20Practical%20Audio%0A%20%20and%20Bone%20Conduction%20Speech%20Super%20Resolution%20and%20Enhancement%20on%20Mobile%20and%0A%20%20Wearable%20Platforms&entry.906535625=Yueyuan%20Sui%20and%20Minghui%20Zhao%20and%20Junxi%20Xia%20and%20Xiaofan%20Jiang%20and%20Stephen%20Xia&entry.1292438233=%20%20We%20propose%20TRAMBA%2C%20a%20hybrid%20transformer%20and%20Mamba%20architecture%20for%20acoustic%0Aand%20bone%20conduction%20speech%20enhancement%2C%20suitable%20for%20mobile%20and%20wearable%0Aplatforms.%20Bone%20conduction%20speech%20enhancement%20has%20been%20impractical%20to%20adopt%20in%0Amobile%20and%20wearable%20platforms%20for%20several%20reasons%3A%20%28i%29%20data%20collection%20is%0Alabor-intensive%2C%20resulting%20in%20scarcity%3B%20%28ii%29%20there%20exists%20a%20performance%20gap%0Abetween%20state%20of-art%20models%20with%20memory%20footprints%20of%20hundreds%20of%20MBs%20and%0Amethods%20better%20suited%20for%20resource-constrained%20systems.%20To%20adapt%20TRAMBA%20to%0Avibration-based%20sensing%20modalities%2C%20we%20pre-train%20TRAMBA%20with%20audio%20speech%0Adatasets%20that%20are%20widely%20available.%20Then%2C%20users%20fine-tune%20with%20a%20small%20amount%0Aof%20bone%20conduction%20data.%20TRAMBA%20outperforms%20state-of-art%20GANs%20by%20up%20to%207.3%25%20in%0APESQ%20and%201.8%25%20in%20STOI%2C%20with%20an%20order%20of%20magnitude%20smaller%20memory%20footprint%20and%0Aan%20inference%20speed%20up%20of%20up%20to%20465%20times.%20We%20integrate%20TRAMBA%20into%20real%20systems%0Aand%20show%20that%20TRAMBA%20%28i%29%20improves%20battery%20life%20of%20wearables%20by%20up%20to%20160%25%20by%0Arequiring%20less%20data%20sampling%20and%20transmission%3B%20%28ii%29%20generates%20higher%20quality%0Avoice%20in%20noisy%20environments%20than%20over-the-air%20speech%3B%20%28iii%29%20requires%20a%20memory%0Afootprint%20of%20less%20than%2020.0%20MB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01242v1&entry.124074799=Read"},
{"title": "Dynamic Online Ensembles of Basis Expansions", "author": "Daniel Waxman and Petar M. Djuri\u0107", "abstract": "  Practical Bayesian learning often requires (1) online inference, (2) dynamic\nmodels, and (3) ensembling over multiple different models. Recent advances have\nshown how to use random feature approximations to achieve scalable, online\nensembling of Gaussian processes with desirable theoretical properties and\nfruitful applications. One key to these methods' success is the inclusion of a\nrandom walk on the model parameters, which makes models dynamic. We show that\nthese methods can be generalized easily to any basis expansion model and that\nusing alternative basis expansions, such as Hilbert space Gaussian processes,\noften results in better performance. To simplify the process of choosing a\nspecific basis expansion, our method's generality also allows the ensembling of\nseveral entirely different models, for example, a Gaussian process and\npolynomial regression. Finally, we propose a novel method to ensemble static\nand dynamic models together.\n", "link": "http://arxiv.org/abs/2405.01365v1", "date": "2024-05-02", "relevancy": 1.9797, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5127}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4962}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Online%20Ensembles%20of%20Basis%20Expansions&body=Title%3A%20Dynamic%20Online%20Ensembles%20of%20Basis%20Expansions%0AAuthor%3A%20Daniel%20Waxman%20and%20Petar%20M.%20Djuri%C4%87%0AAbstract%3A%20%20%20Practical%20Bayesian%20learning%20often%20requires%20%281%29%20online%20inference%2C%20%282%29%20dynamic%0Amodels%2C%20and%20%283%29%20ensembling%20over%20multiple%20different%20models.%20Recent%20advances%20have%0Ashown%20how%20to%20use%20random%20feature%20approximations%20to%20achieve%20scalable%2C%20online%0Aensembling%20of%20Gaussian%20processes%20with%20desirable%20theoretical%20properties%20and%0Afruitful%20applications.%20One%20key%20to%20these%20methods%27%20success%20is%20the%20inclusion%20of%20a%0Arandom%20walk%20on%20the%20model%20parameters%2C%20which%20makes%20models%20dynamic.%20We%20show%20that%0Athese%20methods%20can%20be%20generalized%20easily%20to%20any%20basis%20expansion%20model%20and%20that%0Ausing%20alternative%20basis%20expansions%2C%20such%20as%20Hilbert%20space%20Gaussian%20processes%2C%0Aoften%20results%20in%20better%20performance.%20To%20simplify%20the%20process%20of%20choosing%20a%0Aspecific%20basis%20expansion%2C%20our%20method%27s%20generality%20also%20allows%20the%20ensembling%20of%0Aseveral%20entirely%20different%20models%2C%20for%20example%2C%20a%20Gaussian%20process%20and%0Apolynomial%20regression.%20Finally%2C%20we%20propose%20a%20novel%20method%20to%20ensemble%20static%0Aand%20dynamic%20models%20together.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Online%2520Ensembles%2520of%2520Basis%2520Expansions%26entry.906535625%3DDaniel%2520Waxman%2520and%2520Petar%2520M.%2520Djuri%25C4%2587%26entry.1292438233%3D%2520%2520Practical%2520Bayesian%2520learning%2520often%2520requires%2520%25281%2529%2520online%2520inference%252C%2520%25282%2529%2520dynamic%250Amodels%252C%2520and%2520%25283%2529%2520ensembling%2520over%2520multiple%2520different%2520models.%2520Recent%2520advances%2520have%250Ashown%2520how%2520to%2520use%2520random%2520feature%2520approximations%2520to%2520achieve%2520scalable%252C%2520online%250Aensembling%2520of%2520Gaussian%2520processes%2520with%2520desirable%2520theoretical%2520properties%2520and%250Afruitful%2520applications.%2520One%2520key%2520to%2520these%2520methods%2527%2520success%2520is%2520the%2520inclusion%2520of%2520a%250Arandom%2520walk%2520on%2520the%2520model%2520parameters%252C%2520which%2520makes%2520models%2520dynamic.%2520We%2520show%2520that%250Athese%2520methods%2520can%2520be%2520generalized%2520easily%2520to%2520any%2520basis%2520expansion%2520model%2520and%2520that%250Ausing%2520alternative%2520basis%2520expansions%252C%2520such%2520as%2520Hilbert%2520space%2520Gaussian%2520processes%252C%250Aoften%2520results%2520in%2520better%2520performance.%2520To%2520simplify%2520the%2520process%2520of%2520choosing%2520a%250Aspecific%2520basis%2520expansion%252C%2520our%2520method%2527s%2520generality%2520also%2520allows%2520the%2520ensembling%2520of%250Aseveral%2520entirely%2520different%2520models%252C%2520for%2520example%252C%2520a%2520Gaussian%2520process%2520and%250Apolynomial%2520regression.%2520Finally%252C%2520we%2520propose%2520a%2520novel%2520method%2520to%2520ensemble%2520static%250Aand%2520dynamic%2520models%2520together.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Online%20Ensembles%20of%20Basis%20Expansions&entry.906535625=Daniel%20Waxman%20and%20Petar%20M.%20Djuri%C4%87&entry.1292438233=%20%20Practical%20Bayesian%20learning%20often%20requires%20%281%29%20online%20inference%2C%20%282%29%20dynamic%0Amodels%2C%20and%20%283%29%20ensembling%20over%20multiple%20different%20models.%20Recent%20advances%20have%0Ashown%20how%20to%20use%20random%20feature%20approximations%20to%20achieve%20scalable%2C%20online%0Aensembling%20of%20Gaussian%20processes%20with%20desirable%20theoretical%20properties%20and%0Afruitful%20applications.%20One%20key%20to%20these%20methods%27%20success%20is%20the%20inclusion%20of%20a%0Arandom%20walk%20on%20the%20model%20parameters%2C%20which%20makes%20models%20dynamic.%20We%20show%20that%0Athese%20methods%20can%20be%20generalized%20easily%20to%20any%20basis%20expansion%20model%20and%20that%0Ausing%20alternative%20basis%20expansions%2C%20such%20as%20Hilbert%20space%20Gaussian%20processes%2C%0Aoften%20results%20in%20better%20performance.%20To%20simplify%20the%20process%20of%20choosing%20a%0Aspecific%20basis%20expansion%2C%20our%20method%27s%20generality%20also%20allows%20the%20ensembling%20of%0Aseveral%20entirely%20different%20models%2C%20for%20example%2C%20a%20Gaussian%20process%20and%0Apolynomial%20regression.%20Finally%2C%20we%20propose%20a%20novel%20method%20to%20ensemble%20static%0Aand%20dynamic%20models%20together.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01365v1&entry.124074799=Read"},
{"title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment", "author": "Gerald Shen and Zhilin Wang and Olivier Delalleau and Jiaqi Zeng and Yi Dong and Daniel Egert and Shengyang Sun and Jimmy Zhang and Sahil Jain and Ali Taghibakhshi and Markel Sanz Ausin and Ashwath Aithal and Oleksii Kuchaiev", "abstract": "  Aligning Large Language Models (LLMs) with human values and preferences is\nessential for making them helpful and safe. However, building efficient tools\nto perform alignment can be challenging, especially for the largest and most\ncompetent LLMs which often contain tens or hundreds of billions of parameters.\nWe create NeMo-Aligner, a toolkit for model alignment that can efficiently\nscale to using hundreds of GPUs for training. NeMo-Aligner comes with highly\noptimized and scalable implementations for major paradigms of model alignment\nsuch as: Reinforcement Learning from Human Feedback (RLHF), Direct Preference\nOptimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally,\nour toolkit supports running most of the alignment techniques in a Parameter\nEfficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for\nextensibility, allowing support for other alignment techniques with minimal\neffort. It is open-sourced with Apache 2.0 License and we invite community\ncontributions at https://github.com/NVIDIA/NeMo-Aligner\n", "link": "http://arxiv.org/abs/2405.01481v1", "date": "2024-05-02", "relevancy": 1.9746, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5103}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4845}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeMo-Aligner%3A%20Scalable%20Toolkit%20for%20Efficient%20Model%20Alignment&body=Title%3A%20NeMo-Aligner%3A%20Scalable%20Toolkit%20for%20Efficient%20Model%20Alignment%0AAuthor%3A%20Gerald%20Shen%20and%20Zhilin%20Wang%20and%20Olivier%20Delalleau%20and%20Jiaqi%20Zeng%20and%20Yi%20Dong%20and%20Daniel%20Egert%20and%20Shengyang%20Sun%20and%20Jimmy%20Zhang%20and%20Sahil%20Jain%20and%20Ali%20Taghibakhshi%20and%20Markel%20Sanz%20Ausin%20and%20Ashwath%20Aithal%20and%20Oleksii%20Kuchaiev%0AAbstract%3A%20%20%20Aligning%20Large%20Language%20Models%20%28LLMs%29%20with%20human%20values%20and%20preferences%20is%0Aessential%20for%20making%20them%20helpful%20and%20safe.%20However%2C%20building%20efficient%20tools%0Ato%20perform%20alignment%20can%20be%20challenging%2C%20especially%20for%20the%20largest%20and%20most%0Acompetent%20LLMs%20which%20often%20contain%20tens%20or%20hundreds%20of%20billions%20of%20parameters.%0AWe%20create%20NeMo-Aligner%2C%20a%20toolkit%20for%20model%20alignment%20that%20can%20efficiently%0Ascale%20to%20using%20hundreds%20of%20GPUs%20for%20training.%20NeMo-Aligner%20comes%20with%20highly%0Aoptimized%20and%20scalable%20implementations%20for%20major%20paradigms%20of%20model%20alignment%0Asuch%20as%3A%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20SteerLM%2C%20and%20Self-Play%20Fine-Tuning%20%28SPIN%29.%20Additionally%2C%0Aour%20toolkit%20supports%20running%20most%20of%20the%20alignment%20techniques%20in%20a%20Parameter%0AEfficient%20Fine-Tuning%20%28PEFT%29%20setting.%20NeMo-Aligner%20is%20designed%20for%0Aextensibility%2C%20allowing%20support%20for%20other%20alignment%20techniques%20with%20minimal%0Aeffort.%20It%20is%20open-sourced%20with%20Apache%202.0%20License%20and%20we%20invite%20community%0Acontributions%20at%20https%3A//github.com/NVIDIA/NeMo-Aligner%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeMo-Aligner%253A%2520Scalable%2520Toolkit%2520for%2520Efficient%2520Model%2520Alignment%26entry.906535625%3DGerald%2520Shen%2520and%2520Zhilin%2520Wang%2520and%2520Olivier%2520Delalleau%2520and%2520Jiaqi%2520Zeng%2520and%2520Yi%2520Dong%2520and%2520Daniel%2520Egert%2520and%2520Shengyang%2520Sun%2520and%2520Jimmy%2520Zhang%2520and%2520Sahil%2520Jain%2520and%2520Ali%2520Taghibakhshi%2520and%2520Markel%2520Sanz%2520Ausin%2520and%2520Ashwath%2520Aithal%2520and%2520Oleksii%2520Kuchaiev%26entry.1292438233%3D%2520%2520Aligning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520human%2520values%2520and%2520preferences%2520is%250Aessential%2520for%2520making%2520them%2520helpful%2520and%2520safe.%2520However%252C%2520building%2520efficient%2520tools%250Ato%2520perform%2520alignment%2520can%2520be%2520challenging%252C%2520especially%2520for%2520the%2520largest%2520and%2520most%250Acompetent%2520LLMs%2520which%2520often%2520contain%2520tens%2520or%2520hundreds%2520of%2520billions%2520of%2520parameters.%250AWe%2520create%2520NeMo-Aligner%252C%2520a%2520toolkit%2520for%2520model%2520alignment%2520that%2520can%2520efficiently%250Ascale%2520to%2520using%2520hundreds%2520of%2520GPUs%2520for%2520training.%2520NeMo-Aligner%2520comes%2520with%2520highly%250Aoptimized%2520and%2520scalable%2520implementations%2520for%2520major%2520paradigms%2520of%2520model%2520alignment%250Asuch%2520as%253A%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%252C%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529%252C%2520SteerLM%252C%2520and%2520Self-Play%2520Fine-Tuning%2520%2528SPIN%2529.%2520Additionally%252C%250Aour%2520toolkit%2520supports%2520running%2520most%2520of%2520the%2520alignment%2520techniques%2520in%2520a%2520Parameter%250AEfficient%2520Fine-Tuning%2520%2528PEFT%2529%2520setting.%2520NeMo-Aligner%2520is%2520designed%2520for%250Aextensibility%252C%2520allowing%2520support%2520for%2520other%2520alignment%2520techniques%2520with%2520minimal%250Aeffort.%2520It%2520is%2520open-sourced%2520with%2520Apache%25202.0%2520License%2520and%2520we%2520invite%2520community%250Acontributions%2520at%2520https%253A//github.com/NVIDIA/NeMo-Aligner%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeMo-Aligner%3A%20Scalable%20Toolkit%20for%20Efficient%20Model%20Alignment&entry.906535625=Gerald%20Shen%20and%20Zhilin%20Wang%20and%20Olivier%20Delalleau%20and%20Jiaqi%20Zeng%20and%20Yi%20Dong%20and%20Daniel%20Egert%20and%20Shengyang%20Sun%20and%20Jimmy%20Zhang%20and%20Sahil%20Jain%20and%20Ali%20Taghibakhshi%20and%20Markel%20Sanz%20Ausin%20and%20Ashwath%20Aithal%20and%20Oleksii%20Kuchaiev&entry.1292438233=%20%20Aligning%20Large%20Language%20Models%20%28LLMs%29%20with%20human%20values%20and%20preferences%20is%0Aessential%20for%20making%20them%20helpful%20and%20safe.%20However%2C%20building%20efficient%20tools%0Ato%20perform%20alignment%20can%20be%20challenging%2C%20especially%20for%20the%20largest%20and%20most%0Acompetent%20LLMs%20which%20often%20contain%20tens%20or%20hundreds%20of%20billions%20of%20parameters.%0AWe%20create%20NeMo-Aligner%2C%20a%20toolkit%20for%20model%20alignment%20that%20can%20efficiently%0Ascale%20to%20using%20hundreds%20of%20GPUs%20for%20training.%20NeMo-Aligner%20comes%20with%20highly%0Aoptimized%20and%20scalable%20implementations%20for%20major%20paradigms%20of%20model%20alignment%0Asuch%20as%3A%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20SteerLM%2C%20and%20Self-Play%20Fine-Tuning%20%28SPIN%29.%20Additionally%2C%0Aour%20toolkit%20supports%20running%20most%20of%20the%20alignment%20techniques%20in%20a%20Parameter%0AEfficient%20Fine-Tuning%20%28PEFT%29%20setting.%20NeMo-Aligner%20is%20designed%20for%0Aextensibility%2C%20allowing%20support%20for%20other%20alignment%20techniques%20with%20minimal%0Aeffort.%20It%20is%20open-sourced%20with%20Apache%202.0%20License%20and%20we%20invite%20community%0Acontributions%20at%20https%3A//github.com/NVIDIA/NeMo-Aligner%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01481v1&entry.124074799=Read"},
{"title": "Fourier Neural Operator with Learned Deformations for PDEs on General\n  Geometries", "author": "Zongyi Li and Daniel Zhengyu Huang and Burigede Liu and Anima Anandkumar", "abstract": "  Deep learning surrogate models have shown promise in solving partial\ndifferential equations (PDEs). Among them, the Fourier neural operator (FNO)\nachieves good accuracy, and is significantly faster compared to numerical\nsolvers, on a variety of PDEs, such as fluid flows. However, the FNO uses the\nFast Fourier transform (FFT), which is limited to rectangular domains with\nuniform grids. In this work, we propose a new framework, viz., geo-FNO, to\nsolve PDEs on arbitrary geometries. Geo-FNO learns to deform the input\n(physical) domain, which may be irregular, into a latent space with a uniform\ngrid. The FNO model with the FFT is applied in the latent space. The resulting\ngeo-FNO model has both the computation efficiency of FFT and the flexibility of\nhandling arbitrary geometries. Our geo-FNO is also flexible in terms of its\ninput formats, viz., point clouds, meshes, and design parameters are all valid\ninputs. We consider a variety of PDEs such as the Elasticity, Plasticity,\nEuler's, and Navier-Stokes equations, and both forward modeling and inverse\ndesign problems. Geo-FNO is $10^5$ times faster than the standard numerical\nsolvers and twice more accurate compared to direct interpolation on existing\nML-based PDE solvers such as the standard FNO.\n", "link": "http://arxiv.org/abs/2207.05209v2", "date": "2024-05-02", "relevancy": 1.9673, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5054}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4934}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fourier%20Neural%20Operator%20with%20Learned%20Deformations%20for%20PDEs%20on%20General%0A%20%20Geometries&body=Title%3A%20Fourier%20Neural%20Operator%20with%20Learned%20Deformations%20for%20PDEs%20on%20General%0A%20%20Geometries%0AAuthor%3A%20Zongyi%20Li%20and%20Daniel%20Zhengyu%20Huang%20and%20Burigede%20Liu%20and%20Anima%20Anandkumar%0AAbstract%3A%20%20%20Deep%20learning%20surrogate%20models%20have%20shown%20promise%20in%20solving%20partial%0Adifferential%20equations%20%28PDEs%29.%20Among%20them%2C%20the%20Fourier%20neural%20operator%20%28FNO%29%0Aachieves%20good%20accuracy%2C%20and%20is%20significantly%20faster%20compared%20to%20numerical%0Asolvers%2C%20on%20a%20variety%20of%20PDEs%2C%20such%20as%20fluid%20flows.%20However%2C%20the%20FNO%20uses%20the%0AFast%20Fourier%20transform%20%28FFT%29%2C%20which%20is%20limited%20to%20rectangular%20domains%20with%0Auniform%20grids.%20In%20this%20work%2C%20we%20propose%20a%20new%20framework%2C%20viz.%2C%20geo-FNO%2C%20to%0Asolve%20PDEs%20on%20arbitrary%20geometries.%20Geo-FNO%20learns%20to%20deform%20the%20input%0A%28physical%29%20domain%2C%20which%20may%20be%20irregular%2C%20into%20a%20latent%20space%20with%20a%20uniform%0Agrid.%20The%20FNO%20model%20with%20the%20FFT%20is%20applied%20in%20the%20latent%20space.%20The%20resulting%0Ageo-FNO%20model%20has%20both%20the%20computation%20efficiency%20of%20FFT%20and%20the%20flexibility%20of%0Ahandling%20arbitrary%20geometries.%20Our%20geo-FNO%20is%20also%20flexible%20in%20terms%20of%20its%0Ainput%20formats%2C%20viz.%2C%20point%20clouds%2C%20meshes%2C%20and%20design%20parameters%20are%20all%20valid%0Ainputs.%20We%20consider%20a%20variety%20of%20PDEs%20such%20as%20the%20Elasticity%2C%20Plasticity%2C%0AEuler%27s%2C%20and%20Navier-Stokes%20equations%2C%20and%20both%20forward%20modeling%20and%20inverse%0Adesign%20problems.%20Geo-FNO%20is%20%2410%5E5%24%20times%20faster%20than%20the%20standard%20numerical%0Asolvers%20and%20twice%20more%20accurate%20compared%20to%20direct%20interpolation%20on%20existing%0AML-based%20PDE%20solvers%20such%20as%20the%20standard%20FNO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.05209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFourier%2520Neural%2520Operator%2520with%2520Learned%2520Deformations%2520for%2520PDEs%2520on%2520General%250A%2520%2520Geometries%26entry.906535625%3DZongyi%2520Li%2520and%2520Daniel%2520Zhengyu%2520Huang%2520and%2520Burigede%2520Liu%2520and%2520Anima%2520Anandkumar%26entry.1292438233%3D%2520%2520Deep%2520learning%2520surrogate%2520models%2520have%2520shown%2520promise%2520in%2520solving%2520partial%250Adifferential%2520equations%2520%2528PDEs%2529.%2520Among%2520them%252C%2520the%2520Fourier%2520neural%2520operator%2520%2528FNO%2529%250Aachieves%2520good%2520accuracy%252C%2520and%2520is%2520significantly%2520faster%2520compared%2520to%2520numerical%250Asolvers%252C%2520on%2520a%2520variety%2520of%2520PDEs%252C%2520such%2520as%2520fluid%2520flows.%2520However%252C%2520the%2520FNO%2520uses%2520the%250AFast%2520Fourier%2520transform%2520%2528FFT%2529%252C%2520which%2520is%2520limited%2520to%2520rectangular%2520domains%2520with%250Auniform%2520grids.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520framework%252C%2520viz.%252C%2520geo-FNO%252C%2520to%250Asolve%2520PDEs%2520on%2520arbitrary%2520geometries.%2520Geo-FNO%2520learns%2520to%2520deform%2520the%2520input%250A%2528physical%2529%2520domain%252C%2520which%2520may%2520be%2520irregular%252C%2520into%2520a%2520latent%2520space%2520with%2520a%2520uniform%250Agrid.%2520The%2520FNO%2520model%2520with%2520the%2520FFT%2520is%2520applied%2520in%2520the%2520latent%2520space.%2520The%2520resulting%250Ageo-FNO%2520model%2520has%2520both%2520the%2520computation%2520efficiency%2520of%2520FFT%2520and%2520the%2520flexibility%2520of%250Ahandling%2520arbitrary%2520geometries.%2520Our%2520geo-FNO%2520is%2520also%2520flexible%2520in%2520terms%2520of%2520its%250Ainput%2520formats%252C%2520viz.%252C%2520point%2520clouds%252C%2520meshes%252C%2520and%2520design%2520parameters%2520are%2520all%2520valid%250Ainputs.%2520We%2520consider%2520a%2520variety%2520of%2520PDEs%2520such%2520as%2520the%2520Elasticity%252C%2520Plasticity%252C%250AEuler%2527s%252C%2520and%2520Navier-Stokes%2520equations%252C%2520and%2520both%2520forward%2520modeling%2520and%2520inverse%250Adesign%2520problems.%2520Geo-FNO%2520is%2520%252410%255E5%2524%2520times%2520faster%2520than%2520the%2520standard%2520numerical%250Asolvers%2520and%2520twice%2520more%2520accurate%2520compared%2520to%2520direct%2520interpolation%2520on%2520existing%250AML-based%2520PDE%2520solvers%2520such%2520as%2520the%2520standard%2520FNO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.05209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fourier%20Neural%20Operator%20with%20Learned%20Deformations%20for%20PDEs%20on%20General%0A%20%20Geometries&entry.906535625=Zongyi%20Li%20and%20Daniel%20Zhengyu%20Huang%20and%20Burigede%20Liu%20and%20Anima%20Anandkumar&entry.1292438233=%20%20Deep%20learning%20surrogate%20models%20have%20shown%20promise%20in%20solving%20partial%0Adifferential%20equations%20%28PDEs%29.%20Among%20them%2C%20the%20Fourier%20neural%20operator%20%28FNO%29%0Aachieves%20good%20accuracy%2C%20and%20is%20significantly%20faster%20compared%20to%20numerical%0Asolvers%2C%20on%20a%20variety%20of%20PDEs%2C%20such%20as%20fluid%20flows.%20However%2C%20the%20FNO%20uses%20the%0AFast%20Fourier%20transform%20%28FFT%29%2C%20which%20is%20limited%20to%20rectangular%20domains%20with%0Auniform%20grids.%20In%20this%20work%2C%20we%20propose%20a%20new%20framework%2C%20viz.%2C%20geo-FNO%2C%20to%0Asolve%20PDEs%20on%20arbitrary%20geometries.%20Geo-FNO%20learns%20to%20deform%20the%20input%0A%28physical%29%20domain%2C%20which%20may%20be%20irregular%2C%20into%20a%20latent%20space%20with%20a%20uniform%0Agrid.%20The%20FNO%20model%20with%20the%20FFT%20is%20applied%20in%20the%20latent%20space.%20The%20resulting%0Ageo-FNO%20model%20has%20both%20the%20computation%20efficiency%20of%20FFT%20and%20the%20flexibility%20of%0Ahandling%20arbitrary%20geometries.%20Our%20geo-FNO%20is%20also%20flexible%20in%20terms%20of%20its%0Ainput%20formats%2C%20viz.%2C%20point%20clouds%2C%20meshes%2C%20and%20design%20parameters%20are%20all%20valid%0Ainputs.%20We%20consider%20a%20variety%20of%20PDEs%20such%20as%20the%20Elasticity%2C%20Plasticity%2C%0AEuler%27s%2C%20and%20Navier-Stokes%20equations%2C%20and%20both%20forward%20modeling%20and%20inverse%0Adesign%20problems.%20Geo-FNO%20is%20%2410%5E5%24%20times%20faster%20than%20the%20standard%20numerical%0Asolvers%20and%20twice%20more%20accurate%20compared%20to%20direct%20interpolation%20on%20existing%0AML-based%20PDE%20solvers%20such%20as%20the%20standard%20FNO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.05209v2&entry.124074799=Read"},
{"title": "Accelerating Convergence in Bayesian Few-Shot Classification", "author": "Tianjun Ke and Haoqun Cao and Feng Zhou", "abstract": "  Bayesian few-shot classification has been a focal point in the field of\nfew-shot learning. This paper seamlessly integrates mirror descent-based\nvariational inference into Gaussian process-based few-shot classification,\naddressing the challenge of non-conjugate inference. By leveraging\nnon-Euclidean geometry, mirror descent achieves accelerated convergence by\nproviding the steepest descent direction along the corresponding manifold. It\nalso exhibits the parameterization invariance property concerning the\nvariational distribution. Experimental results demonstrate competitive\nclassification accuracy, improved uncertainty quantification, and faster\nconvergence compared to baseline models. Additionally, we investigate the\nimpact of hyperparameters and components. Code is publicly available at\nhttps://github.com/keanson/MD-BSFC.\n", "link": "http://arxiv.org/abs/2405.01507v1", "date": "2024-05-02", "relevancy": 1.9618, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5195}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5013}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Convergence%20in%20Bayesian%20Few-Shot%20Classification&body=Title%3A%20Accelerating%20Convergence%20in%20Bayesian%20Few-Shot%20Classification%0AAuthor%3A%20Tianjun%20Ke%20and%20Haoqun%20Cao%20and%20Feng%20Zhou%0AAbstract%3A%20%20%20Bayesian%20few-shot%20classification%20has%20been%20a%20focal%20point%20in%20the%20field%20of%0Afew-shot%20learning.%20This%20paper%20seamlessly%20integrates%20mirror%20descent-based%0Avariational%20inference%20into%20Gaussian%20process-based%20few-shot%20classification%2C%0Aaddressing%20the%20challenge%20of%20non-conjugate%20inference.%20By%20leveraging%0Anon-Euclidean%20geometry%2C%20mirror%20descent%20achieves%20accelerated%20convergence%20by%0Aproviding%20the%20steepest%20descent%20direction%20along%20the%20corresponding%20manifold.%20It%0Aalso%20exhibits%20the%20parameterization%20invariance%20property%20concerning%20the%0Avariational%20distribution.%20Experimental%20results%20demonstrate%20competitive%0Aclassification%20accuracy%2C%20improved%20uncertainty%20quantification%2C%20and%20faster%0Aconvergence%20compared%20to%20baseline%20models.%20Additionally%2C%20we%20investigate%20the%0Aimpact%20of%20hyperparameters%20and%20components.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/keanson/MD-BSFC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Convergence%2520in%2520Bayesian%2520Few-Shot%2520Classification%26entry.906535625%3DTianjun%2520Ke%2520and%2520Haoqun%2520Cao%2520and%2520Feng%2520Zhou%26entry.1292438233%3D%2520%2520Bayesian%2520few-shot%2520classification%2520has%2520been%2520a%2520focal%2520point%2520in%2520the%2520field%2520of%250Afew-shot%2520learning.%2520This%2520paper%2520seamlessly%2520integrates%2520mirror%2520descent-based%250Avariational%2520inference%2520into%2520Gaussian%2520process-based%2520few-shot%2520classification%252C%250Aaddressing%2520the%2520challenge%2520of%2520non-conjugate%2520inference.%2520By%2520leveraging%250Anon-Euclidean%2520geometry%252C%2520mirror%2520descent%2520achieves%2520accelerated%2520convergence%2520by%250Aproviding%2520the%2520steepest%2520descent%2520direction%2520along%2520the%2520corresponding%2520manifold.%2520It%250Aalso%2520exhibits%2520the%2520parameterization%2520invariance%2520property%2520concerning%2520the%250Avariational%2520distribution.%2520Experimental%2520results%2520demonstrate%2520competitive%250Aclassification%2520accuracy%252C%2520improved%2520uncertainty%2520quantification%252C%2520and%2520faster%250Aconvergence%2520compared%2520to%2520baseline%2520models.%2520Additionally%252C%2520we%2520investigate%2520the%250Aimpact%2520of%2520hyperparameters%2520and%2520components.%2520Code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/keanson/MD-BSFC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Convergence%20in%20Bayesian%20Few-Shot%20Classification&entry.906535625=Tianjun%20Ke%20and%20Haoqun%20Cao%20and%20Feng%20Zhou&entry.1292438233=%20%20Bayesian%20few-shot%20classification%20has%20been%20a%20focal%20point%20in%20the%20field%20of%0Afew-shot%20learning.%20This%20paper%20seamlessly%20integrates%20mirror%20descent-based%0Avariational%20inference%20into%20Gaussian%20process-based%20few-shot%20classification%2C%0Aaddressing%20the%20challenge%20of%20non-conjugate%20inference.%20By%20leveraging%0Anon-Euclidean%20geometry%2C%20mirror%20descent%20achieves%20accelerated%20convergence%20by%0Aproviding%20the%20steepest%20descent%20direction%20along%20the%20corresponding%20manifold.%20It%0Aalso%20exhibits%20the%20parameterization%20invariance%20property%20concerning%20the%0Avariational%20distribution.%20Experimental%20results%20demonstrate%20competitive%0Aclassification%20accuracy%2C%20improved%20uncertainty%20quantification%2C%20and%20faster%0Aconvergence%20compared%20to%20baseline%20models.%20Additionally%2C%20we%20investigate%20the%0Aimpact%20of%20hyperparameters%20and%20components.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/keanson/MD-BSFC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01507v1&entry.124074799=Read"},
{"title": "Probabilistic Feature Augmentation for AIS-Based Multi-Path Long-Term\n  Vessel Trajectory Forecasting", "author": "Gabriel Spadon and Jay Kumar and Derek Eden and Josh van Berkel and Tom Foster and Amilcar Soares and Ronan Fablet and Stan Matwin and Ronald Pelot", "abstract": "  Maritime transportation is paramount in achieving global economic growth,\nentailing concurrent ecological obligations in sustainability and safeguarding\nendangered marine species, most notably preserving large whale populations. In\nthis regard, the Automatic Identification System (AIS) data plays a significant\nrole by offering real-time streaming data on vessel movement, allowing enhanced\ntraffic monitoring. This study explores using AIS data to prevent\nvessel-to-whale collisions by forecasting long-term vessel trajectories from\nengineered AIS data sequences. For such a task, we have developed an\nencoder-decoder model architecture using Bidirectional Long Short-Term Memory\nNetworks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1\nto 3 hours of AIS data as input. We feed the model with probabilistic features\nengineered from historical AIS data that refer to each trajectory's potential\nroute and destination. The model then predicts the vessel's trajectory,\nconsidering these additional features by leveraging convolutional layers for\nspatial feature learning and a position-aware attention mechanism that\nincreases the importance of recent timesteps of a sequence during temporal\nfeature learning. The probabilistic features have an F1 Score of approximately\n85% and 75% for each feature type, respectively, demonstrating their\neffectiveness in augmenting information to the neural network. We test our\nmodel on the Gulf of St. Lawrence, a region known to be the habitat of North\nAtlantic Right Whales (NARW). Our model achieved a high R2 score of over 98%\nusing various techniques and features. It stands out among other approaches as\nit can make complex decisions during turnings and path selection. Our study\nhighlights the potential of data engineering and trajectory forecasting models\nfor marine life species preservation.\n", "link": "http://arxiv.org/abs/2310.18948v5", "date": "2024-05-02", "relevancy": 1.9601, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4906}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Feature%20Augmentation%20for%20AIS-Based%20Multi-Path%20Long-Term%0A%20%20Vessel%20Trajectory%20Forecasting&body=Title%3A%20Probabilistic%20Feature%20Augmentation%20for%20AIS-Based%20Multi-Path%20Long-Term%0A%20%20Vessel%20Trajectory%20Forecasting%0AAuthor%3A%20Gabriel%20Spadon%20and%20Jay%20Kumar%20and%20Derek%20Eden%20and%20Josh%20van%20Berkel%20and%20Tom%20Foster%20and%20Amilcar%20Soares%20and%20Ronan%20Fablet%20and%20Stan%20Matwin%20and%20Ronald%20Pelot%0AAbstract%3A%20%20%20Maritime%20transportation%20is%20paramount%20in%20achieving%20global%20economic%20growth%2C%0Aentailing%20concurrent%20ecological%20obligations%20in%20sustainability%20and%20safeguarding%0Aendangered%20marine%20species%2C%20most%20notably%20preserving%20large%20whale%20populations.%20In%0Athis%20regard%2C%20the%20Automatic%20Identification%20System%20%28AIS%29%20data%20plays%20a%20significant%0Arole%20by%20offering%20real-time%20streaming%20data%20on%20vessel%20movement%2C%20allowing%20enhanced%0Atraffic%20monitoring.%20This%20study%20explores%20using%20AIS%20data%20to%20prevent%0Avessel-to-whale%20collisions%20by%20forecasting%20long-term%20vessel%20trajectories%20from%0Aengineered%20AIS%20data%20sequences.%20For%20such%20a%20task%2C%20we%20have%20developed%20an%0Aencoder-decoder%20model%20architecture%20using%20Bidirectional%20Long%20Short-Term%20Memory%0ANetworks%20%28Bi-LSTM%29%20to%20predict%20the%20next%2012%20hours%20of%20vessel%20trajectories%20using%201%0Ato%203%20hours%20of%20AIS%20data%20as%20input.%20We%20feed%20the%20model%20with%20probabilistic%20features%0Aengineered%20from%20historical%20AIS%20data%20that%20refer%20to%20each%20trajectory%27s%20potential%0Aroute%20and%20destination.%20The%20model%20then%20predicts%20the%20vessel%27s%20trajectory%2C%0Aconsidering%20these%20additional%20features%20by%20leveraging%20convolutional%20layers%20for%0Aspatial%20feature%20learning%20and%20a%20position-aware%20attention%20mechanism%20that%0Aincreases%20the%20importance%20of%20recent%20timesteps%20of%20a%20sequence%20during%20temporal%0Afeature%20learning.%20The%20probabilistic%20features%20have%20an%20F1%20Score%20of%20approximately%0A85%25%20and%2075%25%20for%20each%20feature%20type%2C%20respectively%2C%20demonstrating%20their%0Aeffectiveness%20in%20augmenting%20information%20to%20the%20neural%20network.%20We%20test%20our%0Amodel%20on%20the%20Gulf%20of%20St.%20Lawrence%2C%20a%20region%20known%20to%20be%20the%20habitat%20of%20North%0AAtlantic%20Right%20Whales%20%28NARW%29.%20Our%20model%20achieved%20a%20high%20R2%20score%20of%20over%2098%25%0Ausing%20various%20techniques%20and%20features.%20It%20stands%20out%20among%20other%20approaches%20as%0Ait%20can%20make%20complex%20decisions%20during%20turnings%20and%20path%20selection.%20Our%20study%0Ahighlights%20the%20potential%20of%20data%20engineering%20and%20trajectory%20forecasting%20models%0Afor%20marine%20life%20species%20preservation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18948v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Feature%2520Augmentation%2520for%2520AIS-Based%2520Multi-Path%2520Long-Term%250A%2520%2520Vessel%2520Trajectory%2520Forecasting%26entry.906535625%3DGabriel%2520Spadon%2520and%2520Jay%2520Kumar%2520and%2520Derek%2520Eden%2520and%2520Josh%2520van%2520Berkel%2520and%2520Tom%2520Foster%2520and%2520Amilcar%2520Soares%2520and%2520Ronan%2520Fablet%2520and%2520Stan%2520Matwin%2520and%2520Ronald%2520Pelot%26entry.1292438233%3D%2520%2520Maritime%2520transportation%2520is%2520paramount%2520in%2520achieving%2520global%2520economic%2520growth%252C%250Aentailing%2520concurrent%2520ecological%2520obligations%2520in%2520sustainability%2520and%2520safeguarding%250Aendangered%2520marine%2520species%252C%2520most%2520notably%2520preserving%2520large%2520whale%2520populations.%2520In%250Athis%2520regard%252C%2520the%2520Automatic%2520Identification%2520System%2520%2528AIS%2529%2520data%2520plays%2520a%2520significant%250Arole%2520by%2520offering%2520real-time%2520streaming%2520data%2520on%2520vessel%2520movement%252C%2520allowing%2520enhanced%250Atraffic%2520monitoring.%2520This%2520study%2520explores%2520using%2520AIS%2520data%2520to%2520prevent%250Avessel-to-whale%2520collisions%2520by%2520forecasting%2520long-term%2520vessel%2520trajectories%2520from%250Aengineered%2520AIS%2520data%2520sequences.%2520For%2520such%2520a%2520task%252C%2520we%2520have%2520developed%2520an%250Aencoder-decoder%2520model%2520architecture%2520using%2520Bidirectional%2520Long%2520Short-Term%2520Memory%250ANetworks%2520%2528Bi-LSTM%2529%2520to%2520predict%2520the%2520next%252012%2520hours%2520of%2520vessel%2520trajectories%2520using%25201%250Ato%25203%2520hours%2520of%2520AIS%2520data%2520as%2520input.%2520We%2520feed%2520the%2520model%2520with%2520probabilistic%2520features%250Aengineered%2520from%2520historical%2520AIS%2520data%2520that%2520refer%2520to%2520each%2520trajectory%2527s%2520potential%250Aroute%2520and%2520destination.%2520The%2520model%2520then%2520predicts%2520the%2520vessel%2527s%2520trajectory%252C%250Aconsidering%2520these%2520additional%2520features%2520by%2520leveraging%2520convolutional%2520layers%2520for%250Aspatial%2520feature%2520learning%2520and%2520a%2520position-aware%2520attention%2520mechanism%2520that%250Aincreases%2520the%2520importance%2520of%2520recent%2520timesteps%2520of%2520a%2520sequence%2520during%2520temporal%250Afeature%2520learning.%2520The%2520probabilistic%2520features%2520have%2520an%2520F1%2520Score%2520of%2520approximately%250A85%2525%2520and%252075%2525%2520for%2520each%2520feature%2520type%252C%2520respectively%252C%2520demonstrating%2520their%250Aeffectiveness%2520in%2520augmenting%2520information%2520to%2520the%2520neural%2520network.%2520We%2520test%2520our%250Amodel%2520on%2520the%2520Gulf%2520of%2520St.%2520Lawrence%252C%2520a%2520region%2520known%2520to%2520be%2520the%2520habitat%2520of%2520North%250AAtlantic%2520Right%2520Whales%2520%2528NARW%2529.%2520Our%2520model%2520achieved%2520a%2520high%2520R2%2520score%2520of%2520over%252098%2525%250Ausing%2520various%2520techniques%2520and%2520features.%2520It%2520stands%2520out%2520among%2520other%2520approaches%2520as%250Ait%2520can%2520make%2520complex%2520decisions%2520during%2520turnings%2520and%2520path%2520selection.%2520Our%2520study%250Ahighlights%2520the%2520potential%2520of%2520data%2520engineering%2520and%2520trajectory%2520forecasting%2520models%250Afor%2520marine%2520life%2520species%2520preservation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.18948v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Feature%20Augmentation%20for%20AIS-Based%20Multi-Path%20Long-Term%0A%20%20Vessel%20Trajectory%20Forecasting&entry.906535625=Gabriel%20Spadon%20and%20Jay%20Kumar%20and%20Derek%20Eden%20and%20Josh%20van%20Berkel%20and%20Tom%20Foster%20and%20Amilcar%20Soares%20and%20Ronan%20Fablet%20and%20Stan%20Matwin%20and%20Ronald%20Pelot&entry.1292438233=%20%20Maritime%20transportation%20is%20paramount%20in%20achieving%20global%20economic%20growth%2C%0Aentailing%20concurrent%20ecological%20obligations%20in%20sustainability%20and%20safeguarding%0Aendangered%20marine%20species%2C%20most%20notably%20preserving%20large%20whale%20populations.%20In%0Athis%20regard%2C%20the%20Automatic%20Identification%20System%20%28AIS%29%20data%20plays%20a%20significant%0Arole%20by%20offering%20real-time%20streaming%20data%20on%20vessel%20movement%2C%20allowing%20enhanced%0Atraffic%20monitoring.%20This%20study%20explores%20using%20AIS%20data%20to%20prevent%0Avessel-to-whale%20collisions%20by%20forecasting%20long-term%20vessel%20trajectories%20from%0Aengineered%20AIS%20data%20sequences.%20For%20such%20a%20task%2C%20we%20have%20developed%20an%0Aencoder-decoder%20model%20architecture%20using%20Bidirectional%20Long%20Short-Term%20Memory%0ANetworks%20%28Bi-LSTM%29%20to%20predict%20the%20next%2012%20hours%20of%20vessel%20trajectories%20using%201%0Ato%203%20hours%20of%20AIS%20data%20as%20input.%20We%20feed%20the%20model%20with%20probabilistic%20features%0Aengineered%20from%20historical%20AIS%20data%20that%20refer%20to%20each%20trajectory%27s%20potential%0Aroute%20and%20destination.%20The%20model%20then%20predicts%20the%20vessel%27s%20trajectory%2C%0Aconsidering%20these%20additional%20features%20by%20leveraging%20convolutional%20layers%20for%0Aspatial%20feature%20learning%20and%20a%20position-aware%20attention%20mechanism%20that%0Aincreases%20the%20importance%20of%20recent%20timesteps%20of%20a%20sequence%20during%20temporal%0Afeature%20learning.%20The%20probabilistic%20features%20have%20an%20F1%20Score%20of%20approximately%0A85%25%20and%2075%25%20for%20each%20feature%20type%2C%20respectively%2C%20demonstrating%20their%0Aeffectiveness%20in%20augmenting%20information%20to%20the%20neural%20network.%20We%20test%20our%0Amodel%20on%20the%20Gulf%20of%20St.%20Lawrence%2C%20a%20region%20known%20to%20be%20the%20habitat%20of%20North%0AAtlantic%20Right%20Whales%20%28NARW%29.%20Our%20model%20achieved%20a%20high%20R2%20score%20of%20over%2098%25%0Ausing%20various%20techniques%20and%20features.%20It%20stands%20out%20among%20other%20approaches%20as%0Ait%20can%20make%20complex%20decisions%20during%20turnings%20and%20path%20selection.%20Our%20study%0Ahighlights%20the%20potential%20of%20data%20engineering%20and%20trajectory%20forecasting%20models%0Afor%20marine%20life%20species%20preservation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18948v5&entry.124074799=Read"},
{"title": "Neural Operator: Learning Maps Between Function Spaces", "author": "Nikola Kovachki and Zongyi Li and Burigede Liu and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar", "abstract": "  The classical development of neural networks has primarily focused on\nlearning mappings between finite dimensional Euclidean spaces or finite sets.\nWe propose a generalization of neural networks to learn operators, termed\nneural operators, that map between infinite dimensional function spaces. We\nformulate the neural operator as a composition of linear integral operators and\nnonlinear activation functions. We prove a universal approximation theorem for\nour proposed neural operator, showing that it can approximate any given\nnonlinear continuous operator. The proposed neural operators are also\ndiscretization-invariant, i.e., they share the same model parameters among\ndifferent discretization of the underlying function spaces. Furthermore, we\nintroduce four classes of efficient parameterization, viz., graph neural\noperators, multi-pole graph neural operators, low-rank neural operators, and\nFourier neural operators. An important application for neural operators is\nlearning surrogate maps for the solution operators of partial differential\nequations (PDEs). We consider standard PDEs such as the Burgers, Darcy\nsubsurface flow, and the Navier-Stokes equations, and show that the proposed\nneural operators have superior performance compared to existing machine\nlearning based methodologies, while being several orders of magnitude faster\nthan conventional PDE solvers.\n", "link": "http://arxiv.org/abs/2108.08481v6", "date": "2024-05-02", "relevancy": 1.9562, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5135}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4972}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Operator%3A%20Learning%20Maps%20Between%20Function%20Spaces&body=Title%3A%20Neural%20Operator%3A%20Learning%20Maps%20Between%20Function%20Spaces%0AAuthor%3A%20Nikola%20Kovachki%20and%20Zongyi%20Li%20and%20Burigede%20Liu%20and%20Kamyar%20Azizzadenesheli%20and%20Kaushik%20Bhattacharya%20and%20Andrew%20Stuart%20and%20Anima%20Anandkumar%0AAbstract%3A%20%20%20The%20classical%20development%20of%20neural%20networks%20has%20primarily%20focused%20on%0Alearning%20mappings%20between%20finite%20dimensional%20Euclidean%20spaces%20or%20finite%20sets.%0AWe%20propose%20a%20generalization%20of%20neural%20networks%20to%20learn%20operators%2C%20termed%0Aneural%20operators%2C%20that%20map%20between%20infinite%20dimensional%20function%20spaces.%20We%0Aformulate%20the%20neural%20operator%20as%20a%20composition%20of%20linear%20integral%20operators%20and%0Anonlinear%20activation%20functions.%20We%20prove%20a%20universal%20approximation%20theorem%20for%0Aour%20proposed%20neural%20operator%2C%20showing%20that%20it%20can%20approximate%20any%20given%0Anonlinear%20continuous%20operator.%20The%20proposed%20neural%20operators%20are%20also%0Adiscretization-invariant%2C%20i.e.%2C%20they%20share%20the%20same%20model%20parameters%20among%0Adifferent%20discretization%20of%20the%20underlying%20function%20spaces.%20Furthermore%2C%20we%0Aintroduce%20four%20classes%20of%20efficient%20parameterization%2C%20viz.%2C%20graph%20neural%0Aoperators%2C%20multi-pole%20graph%20neural%20operators%2C%20low-rank%20neural%20operators%2C%20and%0AFourier%20neural%20operators.%20An%20important%20application%20for%20neural%20operators%20is%0Alearning%20surrogate%20maps%20for%20the%20solution%20operators%20of%20partial%20differential%0Aequations%20%28PDEs%29.%20We%20consider%20standard%20PDEs%20such%20as%20the%20Burgers%2C%20Darcy%0Asubsurface%20flow%2C%20and%20the%20Navier-Stokes%20equations%2C%20and%20show%20that%20the%20proposed%0Aneural%20operators%20have%20superior%20performance%20compared%20to%20existing%20machine%0Alearning%20based%20methodologies%2C%20while%20being%20several%20orders%20of%20magnitude%20faster%0Athan%20conventional%20PDE%20solvers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2108.08481v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Operator%253A%2520Learning%2520Maps%2520Between%2520Function%2520Spaces%26entry.906535625%3DNikola%2520Kovachki%2520and%2520Zongyi%2520Li%2520and%2520Burigede%2520Liu%2520and%2520Kamyar%2520Azizzadenesheli%2520and%2520Kaushik%2520Bhattacharya%2520and%2520Andrew%2520Stuart%2520and%2520Anima%2520Anandkumar%26entry.1292438233%3D%2520%2520The%2520classical%2520development%2520of%2520neural%2520networks%2520has%2520primarily%2520focused%2520on%250Alearning%2520mappings%2520between%2520finite%2520dimensional%2520Euclidean%2520spaces%2520or%2520finite%2520sets.%250AWe%2520propose%2520a%2520generalization%2520of%2520neural%2520networks%2520to%2520learn%2520operators%252C%2520termed%250Aneural%2520operators%252C%2520that%2520map%2520between%2520infinite%2520dimensional%2520function%2520spaces.%2520We%250Aformulate%2520the%2520neural%2520operator%2520as%2520a%2520composition%2520of%2520linear%2520integral%2520operators%2520and%250Anonlinear%2520activation%2520functions.%2520We%2520prove%2520a%2520universal%2520approximation%2520theorem%2520for%250Aour%2520proposed%2520neural%2520operator%252C%2520showing%2520that%2520it%2520can%2520approximate%2520any%2520given%250Anonlinear%2520continuous%2520operator.%2520The%2520proposed%2520neural%2520operators%2520are%2520also%250Adiscretization-invariant%252C%2520i.e.%252C%2520they%2520share%2520the%2520same%2520model%2520parameters%2520among%250Adifferent%2520discretization%2520of%2520the%2520underlying%2520function%2520spaces.%2520Furthermore%252C%2520we%250Aintroduce%2520four%2520classes%2520of%2520efficient%2520parameterization%252C%2520viz.%252C%2520graph%2520neural%250Aoperators%252C%2520multi-pole%2520graph%2520neural%2520operators%252C%2520low-rank%2520neural%2520operators%252C%2520and%250AFourier%2520neural%2520operators.%2520An%2520important%2520application%2520for%2520neural%2520operators%2520is%250Alearning%2520surrogate%2520maps%2520for%2520the%2520solution%2520operators%2520of%2520partial%2520differential%250Aequations%2520%2528PDEs%2529.%2520We%2520consider%2520standard%2520PDEs%2520such%2520as%2520the%2520Burgers%252C%2520Darcy%250Asubsurface%2520flow%252C%2520and%2520the%2520Navier-Stokes%2520equations%252C%2520and%2520show%2520that%2520the%2520proposed%250Aneural%2520operators%2520have%2520superior%2520performance%2520compared%2520to%2520existing%2520machine%250Alearning%2520based%2520methodologies%252C%2520while%2520being%2520several%2520orders%2520of%2520magnitude%2520faster%250Athan%2520conventional%2520PDE%2520solvers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2108.08481v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Operator%3A%20Learning%20Maps%20Between%20Function%20Spaces&entry.906535625=Nikola%20Kovachki%20and%20Zongyi%20Li%20and%20Burigede%20Liu%20and%20Kamyar%20Azizzadenesheli%20and%20Kaushik%20Bhattacharya%20and%20Andrew%20Stuart%20and%20Anima%20Anandkumar&entry.1292438233=%20%20The%20classical%20development%20of%20neural%20networks%20has%20primarily%20focused%20on%0Alearning%20mappings%20between%20finite%20dimensional%20Euclidean%20spaces%20or%20finite%20sets.%0AWe%20propose%20a%20generalization%20of%20neural%20networks%20to%20learn%20operators%2C%20termed%0Aneural%20operators%2C%20that%20map%20between%20infinite%20dimensional%20function%20spaces.%20We%0Aformulate%20the%20neural%20operator%20as%20a%20composition%20of%20linear%20integral%20operators%20and%0Anonlinear%20activation%20functions.%20We%20prove%20a%20universal%20approximation%20theorem%20for%0Aour%20proposed%20neural%20operator%2C%20showing%20that%20it%20can%20approximate%20any%20given%0Anonlinear%20continuous%20operator.%20The%20proposed%20neural%20operators%20are%20also%0Adiscretization-invariant%2C%20i.e.%2C%20they%20share%20the%20same%20model%20parameters%20among%0Adifferent%20discretization%20of%20the%20underlying%20function%20spaces.%20Furthermore%2C%20we%0Aintroduce%20four%20classes%20of%20efficient%20parameterization%2C%20viz.%2C%20graph%20neural%0Aoperators%2C%20multi-pole%20graph%20neural%20operators%2C%20low-rank%20neural%20operators%2C%20and%0AFourier%20neural%20operators.%20An%20important%20application%20for%20neural%20operators%20is%0Alearning%20surrogate%20maps%20for%20the%20solution%20operators%20of%20partial%20differential%0Aequations%20%28PDEs%29.%20We%20consider%20standard%20PDEs%20such%20as%20the%20Burgers%2C%20Darcy%0Asubsurface%20flow%2C%20and%20the%20Navier-Stokes%20equations%2C%20and%20show%20that%20the%20proposed%0Aneural%20operators%20have%20superior%20performance%20compared%20to%20existing%20machine%0Alearning%20based%20methodologies%2C%20while%20being%20several%20orders%20of%20magnitude%20faster%0Athan%20conventional%20PDE%20solvers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2108.08481v6&entry.124074799=Read"},
{"title": "MBDP: A Model-based Approach to Achieve both Robustness and Sample\n  Efficiency via Double Dropout Planning", "author": "Wanpeng Zhang and Xi Xiao and Yao Yao and Mingzhe Chen and Dijun Luo", "abstract": "  Model-based reinforcement learning is a widely accepted solution for solving\nexcessive sample demands. However, the predictions of the dynamics models are\noften not accurate enough, and the resulting bias may incur catastrophic\ndecisions due to insufficient robustness. Therefore, it is highly desired to\ninvestigate how to improve the robustness of model-based RL algorithms while\nmaintaining high sampling efficiency. In this paper, we propose Model-Based\nDouble-dropout Planning (MBDP) to balance robustness and efficiency. MBDP\nconsists of two kinds of dropout mechanisms, where the rollout-dropout aims to\nimprove the robustness with a small cost of sample efficiency, while the\nmodel-dropout is designed to compensate for the lost efficiency at a slight\nexpense of robustness. By combining them in a complementary way, MBDP provides\na flexible control mechanism to meet different demands of robustness and\nefficiency by tuning two corresponding dropout ratios. The effectiveness of\nMBDP is demonstrated both theoretically and experimentally.\n", "link": "http://arxiv.org/abs/2108.01295v2", "date": "2024-05-02", "relevancy": 1.9545, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4926}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MBDP%3A%20A%20Model-based%20Approach%20to%20Achieve%20both%20Robustness%20and%20Sample%0A%20%20Efficiency%20via%20Double%20Dropout%20Planning&body=Title%3A%20MBDP%3A%20A%20Model-based%20Approach%20to%20Achieve%20both%20Robustness%20and%20Sample%0A%20%20Efficiency%20via%20Double%20Dropout%20Planning%0AAuthor%3A%20Wanpeng%20Zhang%20and%20Xi%20Xiao%20and%20Yao%20Yao%20and%20Mingzhe%20Chen%20and%20Dijun%20Luo%0AAbstract%3A%20%20%20Model-based%20reinforcement%20learning%20is%20a%20widely%20accepted%20solution%20for%20solving%0Aexcessive%20sample%20demands.%20However%2C%20the%20predictions%20of%20the%20dynamics%20models%20are%0Aoften%20not%20accurate%20enough%2C%20and%20the%20resulting%20bias%20may%20incur%20catastrophic%0Adecisions%20due%20to%20insufficient%20robustness.%20Therefore%2C%20it%20is%20highly%20desired%20to%0Ainvestigate%20how%20to%20improve%20the%20robustness%20of%20model-based%20RL%20algorithms%20while%0Amaintaining%20high%20sampling%20efficiency.%20In%20this%20paper%2C%20we%20propose%20Model-Based%0ADouble-dropout%20Planning%20%28MBDP%29%20to%20balance%20robustness%20and%20efficiency.%20MBDP%0Aconsists%20of%20two%20kinds%20of%20dropout%20mechanisms%2C%20where%20the%20rollout-dropout%20aims%20to%0Aimprove%20the%20robustness%20with%20a%20small%20cost%20of%20sample%20efficiency%2C%20while%20the%0Amodel-dropout%20is%20designed%20to%20compensate%20for%20the%20lost%20efficiency%20at%20a%20slight%0Aexpense%20of%20robustness.%20By%20combining%20them%20in%20a%20complementary%20way%2C%20MBDP%20provides%0Aa%20flexible%20control%20mechanism%20to%20meet%20different%20demands%20of%20robustness%20and%0Aefficiency%20by%20tuning%20two%20corresponding%20dropout%20ratios.%20The%20effectiveness%20of%0AMBDP%20is%20demonstrated%20both%20theoretically%20and%20experimentally.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2108.01295v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMBDP%253A%2520A%2520Model-based%2520Approach%2520to%2520Achieve%2520both%2520Robustness%2520and%2520Sample%250A%2520%2520Efficiency%2520via%2520Double%2520Dropout%2520Planning%26entry.906535625%3DWanpeng%2520Zhang%2520and%2520Xi%2520Xiao%2520and%2520Yao%2520Yao%2520and%2520Mingzhe%2520Chen%2520and%2520Dijun%2520Luo%26entry.1292438233%3D%2520%2520Model-based%2520reinforcement%2520learning%2520is%2520a%2520widely%2520accepted%2520solution%2520for%2520solving%250Aexcessive%2520sample%2520demands.%2520However%252C%2520the%2520predictions%2520of%2520the%2520dynamics%2520models%2520are%250Aoften%2520not%2520accurate%2520enough%252C%2520and%2520the%2520resulting%2520bias%2520may%2520incur%2520catastrophic%250Adecisions%2520due%2520to%2520insufficient%2520robustness.%2520Therefore%252C%2520it%2520is%2520highly%2520desired%2520to%250Ainvestigate%2520how%2520to%2520improve%2520the%2520robustness%2520of%2520model-based%2520RL%2520algorithms%2520while%250Amaintaining%2520high%2520sampling%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Model-Based%250ADouble-dropout%2520Planning%2520%2528MBDP%2529%2520to%2520balance%2520robustness%2520and%2520efficiency.%2520MBDP%250Aconsists%2520of%2520two%2520kinds%2520of%2520dropout%2520mechanisms%252C%2520where%2520the%2520rollout-dropout%2520aims%2520to%250Aimprove%2520the%2520robustness%2520with%2520a%2520small%2520cost%2520of%2520sample%2520efficiency%252C%2520while%2520the%250Amodel-dropout%2520is%2520designed%2520to%2520compensate%2520for%2520the%2520lost%2520efficiency%2520at%2520a%2520slight%250Aexpense%2520of%2520robustness.%2520By%2520combining%2520them%2520in%2520a%2520complementary%2520way%252C%2520MBDP%2520provides%250Aa%2520flexible%2520control%2520mechanism%2520to%2520meet%2520different%2520demands%2520of%2520robustness%2520and%250Aefficiency%2520by%2520tuning%2520two%2520corresponding%2520dropout%2520ratios.%2520The%2520effectiveness%2520of%250AMBDP%2520is%2520demonstrated%2520both%2520theoretically%2520and%2520experimentally.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2108.01295v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MBDP%3A%20A%20Model-based%20Approach%20to%20Achieve%20both%20Robustness%20and%20Sample%0A%20%20Efficiency%20via%20Double%20Dropout%20Planning&entry.906535625=Wanpeng%20Zhang%20and%20Xi%20Xiao%20and%20Yao%20Yao%20and%20Mingzhe%20Chen%20and%20Dijun%20Luo&entry.1292438233=%20%20Model-based%20reinforcement%20learning%20is%20a%20widely%20accepted%20solution%20for%20solving%0Aexcessive%20sample%20demands.%20However%2C%20the%20predictions%20of%20the%20dynamics%20models%20are%0Aoften%20not%20accurate%20enough%2C%20and%20the%20resulting%20bias%20may%20incur%20catastrophic%0Adecisions%20due%20to%20insufficient%20robustness.%20Therefore%2C%20it%20is%20highly%20desired%20to%0Ainvestigate%20how%20to%20improve%20the%20robustness%20of%20model-based%20RL%20algorithms%20while%0Amaintaining%20high%20sampling%20efficiency.%20In%20this%20paper%2C%20we%20propose%20Model-Based%0ADouble-dropout%20Planning%20%28MBDP%29%20to%20balance%20robustness%20and%20efficiency.%20MBDP%0Aconsists%20of%20two%20kinds%20of%20dropout%20mechanisms%2C%20where%20the%20rollout-dropout%20aims%20to%0Aimprove%20the%20robustness%20with%20a%20small%20cost%20of%20sample%20efficiency%2C%20while%20the%0Amodel-dropout%20is%20designed%20to%20compensate%20for%20the%20lost%20efficiency%20at%20a%20slight%0Aexpense%20of%20robustness.%20By%20combining%20them%20in%20a%20complementary%20way%2C%20MBDP%20provides%0Aa%20flexible%20control%20mechanism%20to%20meet%20different%20demands%20of%20robustness%20and%0Aefficiency%20by%20tuning%20two%20corresponding%20dropout%20ratios.%20The%20effectiveness%20of%0AMBDP%20is%20demonstrated%20both%20theoretically%20and%20experimentally.%0A&entry.1838667208=http%3A//arxiv.org/abs/2108.01295v2&entry.124074799=Read"},
{"title": "Koopman Data-Driven Predictive Control with Robust Stability and\n  Recursive Feasibility Guarantees", "author": "Thomas de Jong and Valentina Breschi and Maarten Schoukens and Mircea Lazar", "abstract": "  In this paper, we consider the design of data-driven predictive controllers\nfor nonlinear systems from input-output data via linear-in-control input\nKoopman lifted models. Instead of identifying and simulating a Koopman model to\npredict future outputs, we design a subspace predictive controller in the\nKoopman space. This allows us to learn the observables minimizing the\nmulti-step output prediction error of the Koopman subspace predictor,\npreventing the propagation of prediction errors. To avoid losing feasibility of\nour predictive control scheme due to prediction errors, we compute a terminal\ncost and terminal set in the Koopman space and we obtain recursive feasibility\nguarantees through an interpolated initial state. As a third contribution, we\nintroduce a novel regularization cost yielding input-to-state stability\nguarantees with respect to the prediction error for the resulting closed-loop\nsystem. The performance of the developed Koopman data-driven predictive control\nmethodology is illustrated on a nonlinear benchmark example from the\nliterature.\n", "link": "http://arxiv.org/abs/2405.01292v1", "date": "2024-05-02", "relevancy": 1.9512, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5467}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4988}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Koopman%20Data-Driven%20Predictive%20Control%20with%20Robust%20Stability%20and%0A%20%20Recursive%20Feasibility%20Guarantees&body=Title%3A%20Koopman%20Data-Driven%20Predictive%20Control%20with%20Robust%20Stability%20and%0A%20%20Recursive%20Feasibility%20Guarantees%0AAuthor%3A%20Thomas%20de%20Jong%20and%20Valentina%20Breschi%20and%20Maarten%20Schoukens%20and%20Mircea%20Lazar%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20the%20design%20of%20data-driven%20predictive%20controllers%0Afor%20nonlinear%20systems%20from%20input-output%20data%20via%20linear-in-control%20input%0AKoopman%20lifted%20models.%20Instead%20of%20identifying%20and%20simulating%20a%20Koopman%20model%20to%0Apredict%20future%20outputs%2C%20we%20design%20a%20subspace%20predictive%20controller%20in%20the%0AKoopman%20space.%20This%20allows%20us%20to%20learn%20the%20observables%20minimizing%20the%0Amulti-step%20output%20prediction%20error%20of%20the%20Koopman%20subspace%20predictor%2C%0Apreventing%20the%20propagation%20of%20prediction%20errors.%20To%20avoid%20losing%20feasibility%20of%0Aour%20predictive%20control%20scheme%20due%20to%20prediction%20errors%2C%20we%20compute%20a%20terminal%0Acost%20and%20terminal%20set%20in%20the%20Koopman%20space%20and%20we%20obtain%20recursive%20feasibility%0Aguarantees%20through%20an%20interpolated%20initial%20state.%20As%20a%20third%20contribution%2C%20we%0Aintroduce%20a%20novel%20regularization%20cost%20yielding%20input-to-state%20stability%0Aguarantees%20with%20respect%20to%20the%20prediction%20error%20for%20the%20resulting%20closed-loop%0Asystem.%20The%20performance%20of%20the%20developed%20Koopman%20data-driven%20predictive%20control%0Amethodology%20is%20illustrated%20on%20a%20nonlinear%20benchmark%20example%20from%20the%0Aliterature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKoopman%2520Data-Driven%2520Predictive%2520Control%2520with%2520Robust%2520Stability%2520and%250A%2520%2520Recursive%2520Feasibility%2520Guarantees%26entry.906535625%3DThomas%2520de%2520Jong%2520and%2520Valentina%2520Breschi%2520and%2520Maarten%2520Schoukens%2520and%2520Mircea%2520Lazar%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520the%2520design%2520of%2520data-driven%2520predictive%2520controllers%250Afor%2520nonlinear%2520systems%2520from%2520input-output%2520data%2520via%2520linear-in-control%2520input%250AKoopman%2520lifted%2520models.%2520Instead%2520of%2520identifying%2520and%2520simulating%2520a%2520Koopman%2520model%2520to%250Apredict%2520future%2520outputs%252C%2520we%2520design%2520a%2520subspace%2520predictive%2520controller%2520in%2520the%250AKoopman%2520space.%2520This%2520allows%2520us%2520to%2520learn%2520the%2520observables%2520minimizing%2520the%250Amulti-step%2520output%2520prediction%2520error%2520of%2520the%2520Koopman%2520subspace%2520predictor%252C%250Apreventing%2520the%2520propagation%2520of%2520prediction%2520errors.%2520To%2520avoid%2520losing%2520feasibility%2520of%250Aour%2520predictive%2520control%2520scheme%2520due%2520to%2520prediction%2520errors%252C%2520we%2520compute%2520a%2520terminal%250Acost%2520and%2520terminal%2520set%2520in%2520the%2520Koopman%2520space%2520and%2520we%2520obtain%2520recursive%2520feasibility%250Aguarantees%2520through%2520an%2520interpolated%2520initial%2520state.%2520As%2520a%2520third%2520contribution%252C%2520we%250Aintroduce%2520a%2520novel%2520regularization%2520cost%2520yielding%2520input-to-state%2520stability%250Aguarantees%2520with%2520respect%2520to%2520the%2520prediction%2520error%2520for%2520the%2520resulting%2520closed-loop%250Asystem.%2520The%2520performance%2520of%2520the%2520developed%2520Koopman%2520data-driven%2520predictive%2520control%250Amethodology%2520is%2520illustrated%2520on%2520a%2520nonlinear%2520benchmark%2520example%2520from%2520the%250Aliterature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Koopman%20Data-Driven%20Predictive%20Control%20with%20Robust%20Stability%20and%0A%20%20Recursive%20Feasibility%20Guarantees&entry.906535625=Thomas%20de%20Jong%20and%20Valentina%20Breschi%20and%20Maarten%20Schoukens%20and%20Mircea%20Lazar&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20the%20design%20of%20data-driven%20predictive%20controllers%0Afor%20nonlinear%20systems%20from%20input-output%20data%20via%20linear-in-control%20input%0AKoopman%20lifted%20models.%20Instead%20of%20identifying%20and%20simulating%20a%20Koopman%20model%20to%0Apredict%20future%20outputs%2C%20we%20design%20a%20subspace%20predictive%20controller%20in%20the%0AKoopman%20space.%20This%20allows%20us%20to%20learn%20the%20observables%20minimizing%20the%0Amulti-step%20output%20prediction%20error%20of%20the%20Koopman%20subspace%20predictor%2C%0Apreventing%20the%20propagation%20of%20prediction%20errors.%20To%20avoid%20losing%20feasibility%20of%0Aour%20predictive%20control%20scheme%20due%20to%20prediction%20errors%2C%20we%20compute%20a%20terminal%0Acost%20and%20terminal%20set%20in%20the%20Koopman%20space%20and%20we%20obtain%20recursive%20feasibility%0Aguarantees%20through%20an%20interpolated%20initial%20state.%20As%20a%20third%20contribution%2C%20we%0Aintroduce%20a%20novel%20regularization%20cost%20yielding%20input-to-state%20stability%0Aguarantees%20with%20respect%20to%20the%20prediction%20error%20for%20the%20resulting%20closed-loop%0Asystem.%20The%20performance%20of%20the%20developed%20Koopman%20data-driven%20predictive%20control%0Amethodology%20is%20illustrated%20on%20a%20nonlinear%20benchmark%20example%20from%20the%0Aliterature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01292v1&entry.124074799=Read"},
{"title": "Language Models As Semantic Indexers", "author": "Bowen Jin and Hansi Zeng and Guoyin Wang and Xiusi Chen and Tianxin Wei and Ruirui Li and Zhengyang Wang and Zheng Li and Yang Li and Hanqing Lu and Suhang Wang and Jiawei Han and Xianfeng Tang", "abstract": "  Semantic identifier (ID) is an important concept in information retrieval\nthat aims to preserve the semantics of objects such as documents and items\ninside their IDs. Previous studies typically adopt a two-stage pipeline to\nlearn semantic IDs by first procuring embeddings using off-the-shelf text\nencoders and then deriving IDs based on the embeddings. However, each step\nintroduces potential information loss, and there is usually an inherent\nmismatch between the distribution of embeddings within the latent space\nproduced by text encoders and the anticipated distribution required for\nsemantic indexing. It is non-trivial to design a method that can learn the\ndocument's semantic representations and its hierarchical structure\nsimultaneously, given that semantic IDs are discrete and sequentially\nstructured, and the semantic supervision is deficient. In this paper, we\nintroduce LMIndexer, a self-supervised framework to learn semantic IDs with a\ngenerative language model. We tackle the challenge of sequential discrete ID by\nintroducing a semantic indexer capable of generating neural sequential discrete\nrepresentations with progressive training and contrastive learning. In response\nto the semantic supervision deficiency, we propose to train the model with a\nself-supervised document reconstruction objective. We show the high quality of\nthe learned IDs and demonstrate their effectiveness on three tasks including\nrecommendation, product search, and document retrieval on five datasets from\nvarious domains. Code is available at\nhttps://github.com/PeterGriffinJin/LMIndexer.\n", "link": "http://arxiv.org/abs/2310.07815v2", "date": "2024-05-02", "relevancy": 1.9264, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4938}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4795}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20As%20Semantic%20Indexers&body=Title%3A%20Language%20Models%20As%20Semantic%20Indexers%0AAuthor%3A%20Bowen%20Jin%20and%20Hansi%20Zeng%20and%20Guoyin%20Wang%20and%20Xiusi%20Chen%20and%20Tianxin%20Wei%20and%20Ruirui%20Li%20and%20Zhengyang%20Wang%20and%20Zheng%20Li%20and%20Yang%20Li%20and%20Hanqing%20Lu%20and%20Suhang%20Wang%20and%20Jiawei%20Han%20and%20Xianfeng%20Tang%0AAbstract%3A%20%20%20Semantic%20identifier%20%28ID%29%20is%20an%20important%20concept%20in%20information%20retrieval%0Athat%20aims%20to%20preserve%20the%20semantics%20of%20objects%20such%20as%20documents%20and%20items%0Ainside%20their%20IDs.%20Previous%20studies%20typically%20adopt%20a%20two-stage%20pipeline%20to%0Alearn%20semantic%20IDs%20by%20first%20procuring%20embeddings%20using%20off-the-shelf%20text%0Aencoders%20and%20then%20deriving%20IDs%20based%20on%20the%20embeddings.%20However%2C%20each%20step%0Aintroduces%20potential%20information%20loss%2C%20and%20there%20is%20usually%20an%20inherent%0Amismatch%20between%20the%20distribution%20of%20embeddings%20within%20the%20latent%20space%0Aproduced%20by%20text%20encoders%20and%20the%20anticipated%20distribution%20required%20for%0Asemantic%20indexing.%20It%20is%20non-trivial%20to%20design%20a%20method%20that%20can%20learn%20the%0Adocument%27s%20semantic%20representations%20and%20its%20hierarchical%20structure%0Asimultaneously%2C%20given%20that%20semantic%20IDs%20are%20discrete%20and%20sequentially%0Astructured%2C%20and%20the%20semantic%20supervision%20is%20deficient.%20In%20this%20paper%2C%20we%0Aintroduce%20LMIndexer%2C%20a%20self-supervised%20framework%20to%20learn%20semantic%20IDs%20with%20a%0Agenerative%20language%20model.%20We%20tackle%20the%20challenge%20of%20sequential%20discrete%20ID%20by%0Aintroducing%20a%20semantic%20indexer%20capable%20of%20generating%20neural%20sequential%20discrete%0Arepresentations%20with%20progressive%20training%20and%20contrastive%20learning.%20In%20response%0Ato%20the%20semantic%20supervision%20deficiency%2C%20we%20propose%20to%20train%20the%20model%20with%20a%0Aself-supervised%20document%20reconstruction%20objective.%20We%20show%20the%20high%20quality%20of%0Athe%20learned%20IDs%20and%20demonstrate%20their%20effectiveness%20on%20three%20tasks%20including%0Arecommendation%2C%20product%20search%2C%20and%20document%20retrieval%20on%20five%20datasets%20from%0Avarious%20domains.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PeterGriffinJin/LMIndexer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520As%2520Semantic%2520Indexers%26entry.906535625%3DBowen%2520Jin%2520and%2520Hansi%2520Zeng%2520and%2520Guoyin%2520Wang%2520and%2520Xiusi%2520Chen%2520and%2520Tianxin%2520Wei%2520and%2520Ruirui%2520Li%2520and%2520Zhengyang%2520Wang%2520and%2520Zheng%2520Li%2520and%2520Yang%2520Li%2520and%2520Hanqing%2520Lu%2520and%2520Suhang%2520Wang%2520and%2520Jiawei%2520Han%2520and%2520Xianfeng%2520Tang%26entry.1292438233%3D%2520%2520Semantic%2520identifier%2520%2528ID%2529%2520is%2520an%2520important%2520concept%2520in%2520information%2520retrieval%250Athat%2520aims%2520to%2520preserve%2520the%2520semantics%2520of%2520objects%2520such%2520as%2520documents%2520and%2520items%250Ainside%2520their%2520IDs.%2520Previous%2520studies%2520typically%2520adopt%2520a%2520two-stage%2520pipeline%2520to%250Alearn%2520semantic%2520IDs%2520by%2520first%2520procuring%2520embeddings%2520using%2520off-the-shelf%2520text%250Aencoders%2520and%2520then%2520deriving%2520IDs%2520based%2520on%2520the%2520embeddings.%2520However%252C%2520each%2520step%250Aintroduces%2520potential%2520information%2520loss%252C%2520and%2520there%2520is%2520usually%2520an%2520inherent%250Amismatch%2520between%2520the%2520distribution%2520of%2520embeddings%2520within%2520the%2520latent%2520space%250Aproduced%2520by%2520text%2520encoders%2520and%2520the%2520anticipated%2520distribution%2520required%2520for%250Asemantic%2520indexing.%2520It%2520is%2520non-trivial%2520to%2520design%2520a%2520method%2520that%2520can%2520learn%2520the%250Adocument%2527s%2520semantic%2520representations%2520and%2520its%2520hierarchical%2520structure%250Asimultaneously%252C%2520given%2520that%2520semantic%2520IDs%2520are%2520discrete%2520and%2520sequentially%250Astructured%252C%2520and%2520the%2520semantic%2520supervision%2520is%2520deficient.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520LMIndexer%252C%2520a%2520self-supervised%2520framework%2520to%2520learn%2520semantic%2520IDs%2520with%2520a%250Agenerative%2520language%2520model.%2520We%2520tackle%2520the%2520challenge%2520of%2520sequential%2520discrete%2520ID%2520by%250Aintroducing%2520a%2520semantic%2520indexer%2520capable%2520of%2520generating%2520neural%2520sequential%2520discrete%250Arepresentations%2520with%2520progressive%2520training%2520and%2520contrastive%2520learning.%2520In%2520response%250Ato%2520the%2520semantic%2520supervision%2520deficiency%252C%2520we%2520propose%2520to%2520train%2520the%2520model%2520with%2520a%250Aself-supervised%2520document%2520reconstruction%2520objective.%2520We%2520show%2520the%2520high%2520quality%2520of%250Athe%2520learned%2520IDs%2520and%2520demonstrate%2520their%2520effectiveness%2520on%2520three%2520tasks%2520including%250Arecommendation%252C%2520product%2520search%252C%2520and%2520document%2520retrieval%2520on%2520five%2520datasets%2520from%250Avarious%2520domains.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/PeterGriffinJin/LMIndexer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.07815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20As%20Semantic%20Indexers&entry.906535625=Bowen%20Jin%20and%20Hansi%20Zeng%20and%20Guoyin%20Wang%20and%20Xiusi%20Chen%20and%20Tianxin%20Wei%20and%20Ruirui%20Li%20and%20Zhengyang%20Wang%20and%20Zheng%20Li%20and%20Yang%20Li%20and%20Hanqing%20Lu%20and%20Suhang%20Wang%20and%20Jiawei%20Han%20and%20Xianfeng%20Tang&entry.1292438233=%20%20Semantic%20identifier%20%28ID%29%20is%20an%20important%20concept%20in%20information%20retrieval%0Athat%20aims%20to%20preserve%20the%20semantics%20of%20objects%20such%20as%20documents%20and%20items%0Ainside%20their%20IDs.%20Previous%20studies%20typically%20adopt%20a%20two-stage%20pipeline%20to%0Alearn%20semantic%20IDs%20by%20first%20procuring%20embeddings%20using%20off-the-shelf%20text%0Aencoders%20and%20then%20deriving%20IDs%20based%20on%20the%20embeddings.%20However%2C%20each%20step%0Aintroduces%20potential%20information%20loss%2C%20and%20there%20is%20usually%20an%20inherent%0Amismatch%20between%20the%20distribution%20of%20embeddings%20within%20the%20latent%20space%0Aproduced%20by%20text%20encoders%20and%20the%20anticipated%20distribution%20required%20for%0Asemantic%20indexing.%20It%20is%20non-trivial%20to%20design%20a%20method%20that%20can%20learn%20the%0Adocument%27s%20semantic%20representations%20and%20its%20hierarchical%20structure%0Asimultaneously%2C%20given%20that%20semantic%20IDs%20are%20discrete%20and%20sequentially%0Astructured%2C%20and%20the%20semantic%20supervision%20is%20deficient.%20In%20this%20paper%2C%20we%0Aintroduce%20LMIndexer%2C%20a%20self-supervised%20framework%20to%20learn%20semantic%20IDs%20with%20a%0Agenerative%20language%20model.%20We%20tackle%20the%20challenge%20of%20sequential%20discrete%20ID%20by%0Aintroducing%20a%20semantic%20indexer%20capable%20of%20generating%20neural%20sequential%20discrete%0Arepresentations%20with%20progressive%20training%20and%20contrastive%20learning.%20In%20response%0Ato%20the%20semantic%20supervision%20deficiency%2C%20we%20propose%20to%20train%20the%20model%20with%20a%0Aself-supervised%20document%20reconstruction%20objective.%20We%20show%20the%20high%20quality%20of%0Athe%20learned%20IDs%20and%20demonstrate%20their%20effectiveness%20on%20three%20tasks%20including%0Arecommendation%2C%20product%20search%2C%20and%20document%20retrieval%20on%20five%20datasets%20from%0Avarious%20domains.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PeterGriffinJin/LMIndexer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07815v2&entry.124074799=Read"},
{"title": "Conformal online model aggregation", "author": "Matteo Gasparin and Aaditya Ramdas", "abstract": "  Conformal prediction equips machine learning models with a reasonable notion\nof uncertainty quantification without making strong distributional assumptions.\nIt wraps around any black-box prediction model and converts point predictions\ninto set predictions that have a predefined marginal coverage guarantee.\nHowever, conformal prediction only works if we fix the underlying machine\nlearning model in advance. A relatively unaddressed issue in conformal\nprediction is that of model selection and/or aggregation: for a given problem,\nwhich of the plethora of prediction methods (random forests, neural nets,\nregularized linear models, etc.) should we conformalize? This paper proposes a\nnew approach towards conformal model aggregation in online settings that is\nbased on combining the prediction sets from several algorithms by voting, where\nweights on the models are adapted over time based on past performance.\n", "link": "http://arxiv.org/abs/2403.15527v2", "date": "2024-05-02", "relevancy": 1.9172, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4914}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4875}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20online%20model%20aggregation&body=Title%3A%20Conformal%20online%20model%20aggregation%0AAuthor%3A%20Matteo%20Gasparin%20and%20Aaditya%20Ramdas%0AAbstract%3A%20%20%20Conformal%20prediction%20equips%20machine%20learning%20models%20with%20a%20reasonable%20notion%0Aof%20uncertainty%20quantification%20without%20making%20strong%20distributional%20assumptions.%0AIt%20wraps%20around%20any%20black-box%20prediction%20model%20and%20converts%20point%20predictions%0Ainto%20set%20predictions%20that%20have%20a%20predefined%20marginal%20coverage%20guarantee.%0AHowever%2C%20conformal%20prediction%20only%20works%20if%20we%20fix%20the%20underlying%20machine%0Alearning%20model%20in%20advance.%20A%20relatively%20unaddressed%20issue%20in%20conformal%0Aprediction%20is%20that%20of%20model%20selection%20and/or%20aggregation%3A%20for%20a%20given%20problem%2C%0Awhich%20of%20the%20plethora%20of%20prediction%20methods%20%28random%20forests%2C%20neural%20nets%2C%0Aregularized%20linear%20models%2C%20etc.%29%20should%20we%20conformalize%3F%20This%20paper%20proposes%20a%0Anew%20approach%20towards%20conformal%20model%20aggregation%20in%20online%20settings%20that%20is%0Abased%20on%20combining%20the%20prediction%20sets%20from%20several%20algorithms%20by%20voting%2C%20where%0Aweights%20on%20the%20models%20are%20adapted%20over%20time%20based%20on%20past%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15527v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520online%2520model%2520aggregation%26entry.906535625%3DMatteo%2520Gasparin%2520and%2520Aaditya%2520Ramdas%26entry.1292438233%3D%2520%2520Conformal%2520prediction%2520equips%2520machine%2520learning%2520models%2520with%2520a%2520reasonable%2520notion%250Aof%2520uncertainty%2520quantification%2520without%2520making%2520strong%2520distributional%2520assumptions.%250AIt%2520wraps%2520around%2520any%2520black-box%2520prediction%2520model%2520and%2520converts%2520point%2520predictions%250Ainto%2520set%2520predictions%2520that%2520have%2520a%2520predefined%2520marginal%2520coverage%2520guarantee.%250AHowever%252C%2520conformal%2520prediction%2520only%2520works%2520if%2520we%2520fix%2520the%2520underlying%2520machine%250Alearning%2520model%2520in%2520advance.%2520A%2520relatively%2520unaddressed%2520issue%2520in%2520conformal%250Aprediction%2520is%2520that%2520of%2520model%2520selection%2520and/or%2520aggregation%253A%2520for%2520a%2520given%2520problem%252C%250Awhich%2520of%2520the%2520plethora%2520of%2520prediction%2520methods%2520%2528random%2520forests%252C%2520neural%2520nets%252C%250Aregularized%2520linear%2520models%252C%2520etc.%2529%2520should%2520we%2520conformalize%253F%2520This%2520paper%2520proposes%2520a%250Anew%2520approach%2520towards%2520conformal%2520model%2520aggregation%2520in%2520online%2520settings%2520that%2520is%250Abased%2520on%2520combining%2520the%2520prediction%2520sets%2520from%2520several%2520algorithms%2520by%2520voting%252C%2520where%250Aweights%2520on%2520the%2520models%2520are%2520adapted%2520over%2520time%2520based%2520on%2520past%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15527v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20online%20model%20aggregation&entry.906535625=Matteo%20Gasparin%20and%20Aaditya%20Ramdas&entry.1292438233=%20%20Conformal%20prediction%20equips%20machine%20learning%20models%20with%20a%20reasonable%20notion%0Aof%20uncertainty%20quantification%20without%20making%20strong%20distributional%20assumptions.%0AIt%20wraps%20around%20any%20black-box%20prediction%20model%20and%20converts%20point%20predictions%0Ainto%20set%20predictions%20that%20have%20a%20predefined%20marginal%20coverage%20guarantee.%0AHowever%2C%20conformal%20prediction%20only%20works%20if%20we%20fix%20the%20underlying%20machine%0Alearning%20model%20in%20advance.%20A%20relatively%20unaddressed%20issue%20in%20conformal%0Aprediction%20is%20that%20of%20model%20selection%20and/or%20aggregation%3A%20for%20a%20given%20problem%2C%0Awhich%20of%20the%20plethora%20of%20prediction%20methods%20%28random%20forests%2C%20neural%20nets%2C%0Aregularized%20linear%20models%2C%20etc.%29%20should%20we%20conformalize%3F%20This%20paper%20proposes%20a%0Anew%20approach%20towards%20conformal%20model%20aggregation%20in%20online%20settings%20that%20is%0Abased%20on%20combining%20the%20prediction%20sets%20from%20several%20algorithms%20by%20voting%2C%20where%0Aweights%20on%20the%20models%20are%20adapted%20over%20time%20based%20on%20past%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15527v2&entry.124074799=Read"},
{"title": "DynamicLight: Two-Stage Dynamic Traffic Signal Timing", "author": "Liang Zhang and Yutong Zhang and Shubin Xie and Jianming Deng and Chen Li", "abstract": "  Reinforcement learning (RL) is gaining popularity as an effective approach\nfor traffic signal control (TSC) and is increasingly applied in this domain.\nHowever, most existing RL methodologies are confined to a single-stage TSC\nframework, primarily focusing on selecting an appropriate traffic signal phase\nat fixed action intervals, leading to inflexible and less adaptable phase\ndurations. To address such limitations, we introduce a novel two-stage TSC\nframework named DynamicLight. This framework initiates with a phase control\nstrategy responsible for determining the optimal traffic phase, followed by a\nduration control strategy tasked with determining the corresponding phase\nduration. Experimental results show that DynamicLight outperforms\nstate-of-the-art TSC models and exhibits exceptional model generalization\ncapabilities. Additionally, the robustness and potential for real-world\nimplementation of DynamicLight are further demonstrated and validated through\nvarious DynamicLight variants. The code is released at\nhttps://github.com/LiangZhang1996/DynamicLight.\n", "link": "http://arxiv.org/abs/2211.01025v2", "date": "2024-05-02", "relevancy": 1.9007, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.488}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4674}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicLight%3A%20Two-Stage%20Dynamic%20Traffic%20Signal%20Timing&body=Title%3A%20DynamicLight%3A%20Two-Stage%20Dynamic%20Traffic%20Signal%20Timing%0AAuthor%3A%20Liang%20Zhang%20and%20Yutong%20Zhang%20and%20Shubin%20Xie%20and%20Jianming%20Deng%20and%20Chen%20Li%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20is%20gaining%20popularity%20as%20an%20effective%20approach%0Afor%20traffic%20signal%20control%20%28TSC%29%20and%20is%20increasingly%20applied%20in%20this%20domain.%0AHowever%2C%20most%20existing%20RL%20methodologies%20are%20confined%20to%20a%20single-stage%20TSC%0Aframework%2C%20primarily%20focusing%20on%20selecting%20an%20appropriate%20traffic%20signal%20phase%0Aat%20fixed%20action%20intervals%2C%20leading%20to%20inflexible%20and%20less%20adaptable%20phase%0Adurations.%20To%20address%20such%20limitations%2C%20we%20introduce%20a%20novel%20two-stage%20TSC%0Aframework%20named%20DynamicLight.%20This%20framework%20initiates%20with%20a%20phase%20control%0Astrategy%20responsible%20for%20determining%20the%20optimal%20traffic%20phase%2C%20followed%20by%20a%0Aduration%20control%20strategy%20tasked%20with%20determining%20the%20corresponding%20phase%0Aduration.%20Experimental%20results%20show%20that%20DynamicLight%20outperforms%0Astate-of-the-art%20TSC%20models%20and%20exhibits%20exceptional%20model%20generalization%0Acapabilities.%20Additionally%2C%20the%20robustness%20and%20potential%20for%20real-world%0Aimplementation%20of%20DynamicLight%20are%20further%20demonstrated%20and%20validated%20through%0Avarious%20DynamicLight%20variants.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/LiangZhang1996/DynamicLight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.01025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicLight%253A%2520Two-Stage%2520Dynamic%2520Traffic%2520Signal%2520Timing%26entry.906535625%3DLiang%2520Zhang%2520and%2520Yutong%2520Zhang%2520and%2520Shubin%2520Xie%2520and%2520Jianming%2520Deng%2520and%2520Chen%2520Li%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520is%2520gaining%2520popularity%2520as%2520an%2520effective%2520approach%250Afor%2520traffic%2520signal%2520control%2520%2528TSC%2529%2520and%2520is%2520increasingly%2520applied%2520in%2520this%2520domain.%250AHowever%252C%2520most%2520existing%2520RL%2520methodologies%2520are%2520confined%2520to%2520a%2520single-stage%2520TSC%250Aframework%252C%2520primarily%2520focusing%2520on%2520selecting%2520an%2520appropriate%2520traffic%2520signal%2520phase%250Aat%2520fixed%2520action%2520intervals%252C%2520leading%2520to%2520inflexible%2520and%2520less%2520adaptable%2520phase%250Adurations.%2520To%2520address%2520such%2520limitations%252C%2520we%2520introduce%2520a%2520novel%2520two-stage%2520TSC%250Aframework%2520named%2520DynamicLight.%2520This%2520framework%2520initiates%2520with%2520a%2520phase%2520control%250Astrategy%2520responsible%2520for%2520determining%2520the%2520optimal%2520traffic%2520phase%252C%2520followed%2520by%2520a%250Aduration%2520control%2520strategy%2520tasked%2520with%2520determining%2520the%2520corresponding%2520phase%250Aduration.%2520Experimental%2520results%2520show%2520that%2520DynamicLight%2520outperforms%250Astate-of-the-art%2520TSC%2520models%2520and%2520exhibits%2520exceptional%2520model%2520generalization%250Acapabilities.%2520Additionally%252C%2520the%2520robustness%2520and%2520potential%2520for%2520real-world%250Aimplementation%2520of%2520DynamicLight%2520are%2520further%2520demonstrated%2520and%2520validated%2520through%250Avarious%2520DynamicLight%2520variants.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/LiangZhang1996/DynamicLight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.01025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicLight%3A%20Two-Stage%20Dynamic%20Traffic%20Signal%20Timing&entry.906535625=Liang%20Zhang%20and%20Yutong%20Zhang%20and%20Shubin%20Xie%20and%20Jianming%20Deng%20and%20Chen%20Li&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20is%20gaining%20popularity%20as%20an%20effective%20approach%0Afor%20traffic%20signal%20control%20%28TSC%29%20and%20is%20increasingly%20applied%20in%20this%20domain.%0AHowever%2C%20most%20existing%20RL%20methodologies%20are%20confined%20to%20a%20single-stage%20TSC%0Aframework%2C%20primarily%20focusing%20on%20selecting%20an%20appropriate%20traffic%20signal%20phase%0Aat%20fixed%20action%20intervals%2C%20leading%20to%20inflexible%20and%20less%20adaptable%20phase%0Adurations.%20To%20address%20such%20limitations%2C%20we%20introduce%20a%20novel%20two-stage%20TSC%0Aframework%20named%20DynamicLight.%20This%20framework%20initiates%20with%20a%20phase%20control%0Astrategy%20responsible%20for%20determining%20the%20optimal%20traffic%20phase%2C%20followed%20by%20a%0Aduration%20control%20strategy%20tasked%20with%20determining%20the%20corresponding%20phase%0Aduration.%20Experimental%20results%20show%20that%20DynamicLight%20outperforms%0Astate-of-the-art%20TSC%20models%20and%20exhibits%20exceptional%20model%20generalization%0Acapabilities.%20Additionally%2C%20the%20robustness%20and%20potential%20for%20real-world%0Aimplementation%20of%20DynamicLight%20are%20further%20demonstrated%20and%20validated%20through%0Avarious%20DynamicLight%20variants.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/LiangZhang1996/DynamicLight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.01025v2&entry.124074799=Read"},
{"title": "Graph is all you need? Lightweight data-agnostic neural architecture\n  search without training", "author": "Zhenhan Huang and Tejaswini Pedapati and Pin-Yu Chen and Chunhen Jiang and Jianxi Gao", "abstract": "  Neural architecture search (NAS) enables the automatic design of neural\nnetwork models. However, training the candidates generated by the search\nalgorithm for performance evaluation incurs considerable computational\noverhead. Our method, dubbed nasgraph, remarkably reduces the computational\ncosts by converting neural architectures to graphs and using the average\ndegree, a graph measure, as the proxy in lieu of the evaluation metric. Our\ntraining-free NAS method is data-agnostic and light-weight. It can find the\nbest architecture among 200 randomly sampled architectures from NAS-Bench201 in\n217 CPU seconds. Besides, our method is able to achieve competitive performance\non various datasets including NASBench-101, NASBench-201, and NDS search\nspaces. We also demonstrate that nasgraph generalizes to more challenging tasks\non Micro TransNAS-Bench-101.\n", "link": "http://arxiv.org/abs/2405.01306v1", "date": "2024-05-02", "relevancy": 1.8995, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4909}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4638}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20is%20all%20you%20need%3F%20Lightweight%20data-agnostic%20neural%20architecture%0A%20%20search%20without%20training&body=Title%3A%20Graph%20is%20all%20you%20need%3F%20Lightweight%20data-agnostic%20neural%20architecture%0A%20%20search%20without%20training%0AAuthor%3A%20Zhenhan%20Huang%20and%20Tejaswini%20Pedapati%20and%20Pin-Yu%20Chen%20and%20Chunhen%20Jiang%20and%20Jianxi%20Gao%0AAbstract%3A%20%20%20Neural%20architecture%20search%20%28NAS%29%20enables%20the%20automatic%20design%20of%20neural%0Anetwork%20models.%20However%2C%20training%20the%20candidates%20generated%20by%20the%20search%0Aalgorithm%20for%20performance%20evaluation%20incurs%20considerable%20computational%0Aoverhead.%20Our%20method%2C%20dubbed%20nasgraph%2C%20remarkably%20reduces%20the%20computational%0Acosts%20by%20converting%20neural%20architectures%20to%20graphs%20and%20using%20the%20average%0Adegree%2C%20a%20graph%20measure%2C%20as%20the%20proxy%20in%20lieu%20of%20the%20evaluation%20metric.%20Our%0Atraining-free%20NAS%20method%20is%20data-agnostic%20and%20light-weight.%20It%20can%20find%20the%0Abest%20architecture%20among%20200%20randomly%20sampled%20architectures%20from%20NAS-Bench201%20in%0A217%20CPU%20seconds.%20Besides%2C%20our%20method%20is%20able%20to%20achieve%20competitive%20performance%0Aon%20various%20datasets%20including%20NASBench-101%2C%20NASBench-201%2C%20and%20NDS%20search%0Aspaces.%20We%20also%20demonstrate%20that%20nasgraph%20generalizes%20to%20more%20challenging%20tasks%0Aon%20Micro%20TransNAS-Bench-101.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520is%2520all%2520you%2520need%253F%2520Lightweight%2520data-agnostic%2520neural%2520architecture%250A%2520%2520search%2520without%2520training%26entry.906535625%3DZhenhan%2520Huang%2520and%2520Tejaswini%2520Pedapati%2520and%2520Pin-Yu%2520Chen%2520and%2520Chunhen%2520Jiang%2520and%2520Jianxi%2520Gao%26entry.1292438233%3D%2520%2520Neural%2520architecture%2520search%2520%2528NAS%2529%2520enables%2520the%2520automatic%2520design%2520of%2520neural%250Anetwork%2520models.%2520However%252C%2520training%2520the%2520candidates%2520generated%2520by%2520the%2520search%250Aalgorithm%2520for%2520performance%2520evaluation%2520incurs%2520considerable%2520computational%250Aoverhead.%2520Our%2520method%252C%2520dubbed%2520nasgraph%252C%2520remarkably%2520reduces%2520the%2520computational%250Acosts%2520by%2520converting%2520neural%2520architectures%2520to%2520graphs%2520and%2520using%2520the%2520average%250Adegree%252C%2520a%2520graph%2520measure%252C%2520as%2520the%2520proxy%2520in%2520lieu%2520of%2520the%2520evaluation%2520metric.%2520Our%250Atraining-free%2520NAS%2520method%2520is%2520data-agnostic%2520and%2520light-weight.%2520It%2520can%2520find%2520the%250Abest%2520architecture%2520among%2520200%2520randomly%2520sampled%2520architectures%2520from%2520NAS-Bench201%2520in%250A217%2520CPU%2520seconds.%2520Besides%252C%2520our%2520method%2520is%2520able%2520to%2520achieve%2520competitive%2520performance%250Aon%2520various%2520datasets%2520including%2520NASBench-101%252C%2520NASBench-201%252C%2520and%2520NDS%2520search%250Aspaces.%2520We%2520also%2520demonstrate%2520that%2520nasgraph%2520generalizes%2520to%2520more%2520challenging%2520tasks%250Aon%2520Micro%2520TransNAS-Bench-101.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20is%20all%20you%20need%3F%20Lightweight%20data-agnostic%20neural%20architecture%0A%20%20search%20without%20training&entry.906535625=Zhenhan%20Huang%20and%20Tejaswini%20Pedapati%20and%20Pin-Yu%20Chen%20and%20Chunhen%20Jiang%20and%20Jianxi%20Gao&entry.1292438233=%20%20Neural%20architecture%20search%20%28NAS%29%20enables%20the%20automatic%20design%20of%20neural%0Anetwork%20models.%20However%2C%20training%20the%20candidates%20generated%20by%20the%20search%0Aalgorithm%20for%20performance%20evaluation%20incurs%20considerable%20computational%0Aoverhead.%20Our%20method%2C%20dubbed%20nasgraph%2C%20remarkably%20reduces%20the%20computational%0Acosts%20by%20converting%20neural%20architectures%20to%20graphs%20and%20using%20the%20average%0Adegree%2C%20a%20graph%20measure%2C%20as%20the%20proxy%20in%20lieu%20of%20the%20evaluation%20metric.%20Our%0Atraining-free%20NAS%20method%20is%20data-agnostic%20and%20light-weight.%20It%20can%20find%20the%0Abest%20architecture%20among%20200%20randomly%20sampled%20architectures%20from%20NAS-Bench201%20in%0A217%20CPU%20seconds.%20Besides%2C%20our%20method%20is%20able%20to%20achieve%20competitive%20performance%0Aon%20various%20datasets%20including%20NASBench-101%2C%20NASBench-201%2C%20and%20NDS%20search%0Aspaces.%20We%20also%20demonstrate%20that%20nasgraph%20generalizes%20to%20more%20challenging%20tasks%0Aon%20Micro%20TransNAS-Bench-101.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01306v1&entry.124074799=Read"},
{"title": "DataLight: Offline Data-Driven Traffic Signal Control", "author": "Liang Zhang and Yutong Zhang and Jianming Deng and Chen Li", "abstract": "  Reinforcement learning (RL) has emerged as a promising solution for\naddressing traffic signal control (TSC) challenges. While most RL-based TSC\nsystems typically employ an online approach, facilitating frequent active\ninteraction with the environment, learning such strategies in the real world is\nimpractical due to safety and risk concerns. To tackle these challenges, this\nstudy introduces an innovative offline data-driven approach, called DataLight.\nDataLight employs effective state representations and reward function by\ncapturing vehicular speed information within the environment. It then segments\nroads to capture spatial information and further enhances the spatially\nsegmented state representations with sequential modeling. The experimental\nresults demonstrate the effectiveness of DataLight, showcasing superior\nperformance compared to both state-of-the-art online and offline TSC methods.\nAdditionally, DataLight exhibits robust learning capabilities concerning\nreal-world deployment issues. The code is available at\nhttps://github.com/LiangZhang1996/DataLight.\n", "link": "http://arxiv.org/abs/2303.10828v2", "date": "2024-05-02", "relevancy": 1.8969, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4812}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4703}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DataLight%3A%20Offline%20Data-Driven%20Traffic%20Signal%20Control&body=Title%3A%20DataLight%3A%20Offline%20Data-Driven%20Traffic%20Signal%20Control%0AAuthor%3A%20Liang%20Zhang%20and%20Yutong%20Zhang%20and%20Jianming%20Deng%20and%20Chen%20Li%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20promising%20solution%20for%0Aaddressing%20traffic%20signal%20control%20%28TSC%29%20challenges.%20While%20most%20RL-based%20TSC%0Asystems%20typically%20employ%20an%20online%20approach%2C%20facilitating%20frequent%20active%0Ainteraction%20with%20the%20environment%2C%20learning%20such%20strategies%20in%20the%20real%20world%20is%0Aimpractical%20due%20to%20safety%20and%20risk%20concerns.%20To%20tackle%20these%20challenges%2C%20this%0Astudy%20introduces%20an%20innovative%20offline%20data-driven%20approach%2C%20called%20DataLight.%0ADataLight%20employs%20effective%20state%20representations%20and%20reward%20function%20by%0Acapturing%20vehicular%20speed%20information%20within%20the%20environment.%20It%20then%20segments%0Aroads%20to%20capture%20spatial%20information%20and%20further%20enhances%20the%20spatially%0Asegmented%20state%20representations%20with%20sequential%20modeling.%20The%20experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20DataLight%2C%20showcasing%20superior%0Aperformance%20compared%20to%20both%20state-of-the-art%20online%20and%20offline%20TSC%20methods.%0AAdditionally%2C%20DataLight%20exhibits%20robust%20learning%20capabilities%20concerning%0Areal-world%20deployment%20issues.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiangZhang1996/DataLight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.10828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataLight%253A%2520Offline%2520Data-Driven%2520Traffic%2520Signal%2520Control%26entry.906535625%3DLiang%2520Zhang%2520and%2520Yutong%2520Zhang%2520and%2520Jianming%2520Deng%2520and%2520Chen%2520Li%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520for%250Aaddressing%2520traffic%2520signal%2520control%2520%2528TSC%2529%2520challenges.%2520While%2520most%2520RL-based%2520TSC%250Asystems%2520typically%2520employ%2520an%2520online%2520approach%252C%2520facilitating%2520frequent%2520active%250Ainteraction%2520with%2520the%2520environment%252C%2520learning%2520such%2520strategies%2520in%2520the%2520real%2520world%2520is%250Aimpractical%2520due%2520to%2520safety%2520and%2520risk%2520concerns.%2520To%2520tackle%2520these%2520challenges%252C%2520this%250Astudy%2520introduces%2520an%2520innovative%2520offline%2520data-driven%2520approach%252C%2520called%2520DataLight.%250ADataLight%2520employs%2520effective%2520state%2520representations%2520and%2520reward%2520function%2520by%250Acapturing%2520vehicular%2520speed%2520information%2520within%2520the%2520environment.%2520It%2520then%2520segments%250Aroads%2520to%2520capture%2520spatial%2520information%2520and%2520further%2520enhances%2520the%2520spatially%250Asegmented%2520state%2520representations%2520with%2520sequential%2520modeling.%2520The%2520experimental%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520DataLight%252C%2520showcasing%2520superior%250Aperformance%2520compared%2520to%2520both%2520state-of-the-art%2520online%2520and%2520offline%2520TSC%2520methods.%250AAdditionally%252C%2520DataLight%2520exhibits%2520robust%2520learning%2520capabilities%2520concerning%250Areal-world%2520deployment%2520issues.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LiangZhang1996/DataLight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.10828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DataLight%3A%20Offline%20Data-Driven%20Traffic%20Signal%20Control&entry.906535625=Liang%20Zhang%20and%20Yutong%20Zhang%20and%20Jianming%20Deng%20and%20Chen%20Li&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20promising%20solution%20for%0Aaddressing%20traffic%20signal%20control%20%28TSC%29%20challenges.%20While%20most%20RL-based%20TSC%0Asystems%20typically%20employ%20an%20online%20approach%2C%20facilitating%20frequent%20active%0Ainteraction%20with%20the%20environment%2C%20learning%20such%20strategies%20in%20the%20real%20world%20is%0Aimpractical%20due%20to%20safety%20and%20risk%20concerns.%20To%20tackle%20these%20challenges%2C%20this%0Astudy%20introduces%20an%20innovative%20offline%20data-driven%20approach%2C%20called%20DataLight.%0ADataLight%20employs%20effective%20state%20representations%20and%20reward%20function%20by%0Acapturing%20vehicular%20speed%20information%20within%20the%20environment.%20It%20then%20segments%0Aroads%20to%20capture%20spatial%20information%20and%20further%20enhances%20the%20spatially%0Asegmented%20state%20representations%20with%20sequential%20modeling.%20The%20experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20DataLight%2C%20showcasing%20superior%0Aperformance%20compared%20to%20both%20state-of-the-art%20online%20and%20offline%20TSC%20methods.%0AAdditionally%2C%20DataLight%20exhibits%20robust%20learning%20capabilities%20concerning%0Areal-world%20deployment%20issues.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiangZhang1996/DataLight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.10828v2&entry.124074799=Read"},
{"title": "Towards Cross-Scale Attention and Surface Supervision for Fractured Bone\n  Segmentation in CT", "author": "Yu Zhou and Xiahao Zou and Yi Wang", "abstract": "  Bone segmentation is an essential step for the preoperative planning of\nfracture trauma surgery. The automated segmentation of fractured bone from\ncomputed tomography (CT) scans remains challenging, due to the large\ndifferences of fractures in position and morphology, and also the inherent\nanatomical characteristics of different bone structures. To alleviate these\nissues, we propose a cross-scale attention mechanism as well as a surface\nsupervision strategy for fractured bone segmentation in CT. Specifically, a\ncross-scale attention mechanism is introduced to effectively aggregate the\nfeatures among different scales to provide more powerful fracture\nrepresentation. Moreover, a surface supervision strategy is employed, which\nexplicitly constrains the network to pay more attention to the bone boundary.\nThe efficacy of the proposed method is evaluated on a public dataset containing\nCT scans with hip fractures. The evaluation metrics are Dice similarity\ncoefficient (DSC), average symmetric surface distance (ASSD), and Hausdorff\ndistance (95HD). The proposed method achieves an average DSC of 93.36%, ASSD of\n0.85mm, 95HD of 7.51mm. Our method offers an effective fracture segmentation\napproach for the pelvic CT examinations, and has the potential to be used for\nimproving the segmentation performance of other types of fractures.\n", "link": "http://arxiv.org/abs/2405.01204v1", "date": "2024-05-02", "relevancy": 1.89, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.482}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4662}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Cross-Scale%20Attention%20and%20Surface%20Supervision%20for%20Fractured%20Bone%0A%20%20Segmentation%20in%20CT&body=Title%3A%20Towards%20Cross-Scale%20Attention%20and%20Surface%20Supervision%20for%20Fractured%20Bone%0A%20%20Segmentation%20in%20CT%0AAuthor%3A%20Yu%20Zhou%20and%20Xiahao%20Zou%20and%20Yi%20Wang%0AAbstract%3A%20%20%20Bone%20segmentation%20is%20an%20essential%20step%20for%20the%20preoperative%20planning%20of%0Afracture%20trauma%20surgery.%20The%20automated%20segmentation%20of%20fractured%20bone%20from%0Acomputed%20tomography%20%28CT%29%20scans%20remains%20challenging%2C%20due%20to%20the%20large%0Adifferences%20of%20fractures%20in%20position%20and%20morphology%2C%20and%20also%20the%20inherent%0Aanatomical%20characteristics%20of%20different%20bone%20structures.%20To%20alleviate%20these%0Aissues%2C%20we%20propose%20a%20cross-scale%20attention%20mechanism%20as%20well%20as%20a%20surface%0Asupervision%20strategy%20for%20fractured%20bone%20segmentation%20in%20CT.%20Specifically%2C%20a%0Across-scale%20attention%20mechanism%20is%20introduced%20to%20effectively%20aggregate%20the%0Afeatures%20among%20different%20scales%20to%20provide%20more%20powerful%20fracture%0Arepresentation.%20Moreover%2C%20a%20surface%20supervision%20strategy%20is%20employed%2C%20which%0Aexplicitly%20constrains%20the%20network%20to%20pay%20more%20attention%20to%20the%20bone%20boundary.%0AThe%20efficacy%20of%20the%20proposed%20method%20is%20evaluated%20on%20a%20public%20dataset%20containing%0ACT%20scans%20with%20hip%20fractures.%20The%20evaluation%20metrics%20are%20Dice%20similarity%0Acoefficient%20%28DSC%29%2C%20average%20symmetric%20surface%20distance%20%28ASSD%29%2C%20and%20Hausdorff%0Adistance%20%2895HD%29.%20The%20proposed%20method%20achieves%20an%20average%20DSC%20of%2093.36%25%2C%20ASSD%20of%0A0.85mm%2C%2095HD%20of%207.51mm.%20Our%20method%20offers%20an%20effective%20fracture%20segmentation%0Aapproach%20for%20the%20pelvic%20CT%20examinations%2C%20and%20has%20the%20potential%20to%20be%20used%20for%0Aimproving%20the%20segmentation%20performance%20of%20other%20types%20of%20fractures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Cross-Scale%2520Attention%2520and%2520Surface%2520Supervision%2520for%2520Fractured%2520Bone%250A%2520%2520Segmentation%2520in%2520CT%26entry.906535625%3DYu%2520Zhou%2520and%2520Xiahao%2520Zou%2520and%2520Yi%2520Wang%26entry.1292438233%3D%2520%2520Bone%2520segmentation%2520is%2520an%2520essential%2520step%2520for%2520the%2520preoperative%2520planning%2520of%250Afracture%2520trauma%2520surgery.%2520The%2520automated%2520segmentation%2520of%2520fractured%2520bone%2520from%250Acomputed%2520tomography%2520%2528CT%2529%2520scans%2520remains%2520challenging%252C%2520due%2520to%2520the%2520large%250Adifferences%2520of%2520fractures%2520in%2520position%2520and%2520morphology%252C%2520and%2520also%2520the%2520inherent%250Aanatomical%2520characteristics%2520of%2520different%2520bone%2520structures.%2520To%2520alleviate%2520these%250Aissues%252C%2520we%2520propose%2520a%2520cross-scale%2520attention%2520mechanism%2520as%2520well%2520as%2520a%2520surface%250Asupervision%2520strategy%2520for%2520fractured%2520bone%2520segmentation%2520in%2520CT.%2520Specifically%252C%2520a%250Across-scale%2520attention%2520mechanism%2520is%2520introduced%2520to%2520effectively%2520aggregate%2520the%250Afeatures%2520among%2520different%2520scales%2520to%2520provide%2520more%2520powerful%2520fracture%250Arepresentation.%2520Moreover%252C%2520a%2520surface%2520supervision%2520strategy%2520is%2520employed%252C%2520which%250Aexplicitly%2520constrains%2520the%2520network%2520to%2520pay%2520more%2520attention%2520to%2520the%2520bone%2520boundary.%250AThe%2520efficacy%2520of%2520the%2520proposed%2520method%2520is%2520evaluated%2520on%2520a%2520public%2520dataset%2520containing%250ACT%2520scans%2520with%2520hip%2520fractures.%2520The%2520evaluation%2520metrics%2520are%2520Dice%2520similarity%250Acoefficient%2520%2528DSC%2529%252C%2520average%2520symmetric%2520surface%2520distance%2520%2528ASSD%2529%252C%2520and%2520Hausdorff%250Adistance%2520%252895HD%2529.%2520The%2520proposed%2520method%2520achieves%2520an%2520average%2520DSC%2520of%252093.36%2525%252C%2520ASSD%2520of%250A0.85mm%252C%252095HD%2520of%25207.51mm.%2520Our%2520method%2520offers%2520an%2520effective%2520fracture%2520segmentation%250Aapproach%2520for%2520the%2520pelvic%2520CT%2520examinations%252C%2520and%2520has%2520the%2520potential%2520to%2520be%2520used%2520for%250Aimproving%2520the%2520segmentation%2520performance%2520of%2520other%2520types%2520of%2520fractures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Cross-Scale%20Attention%20and%20Surface%20Supervision%20for%20Fractured%20Bone%0A%20%20Segmentation%20in%20CT&entry.906535625=Yu%20Zhou%20and%20Xiahao%20Zou%20and%20Yi%20Wang&entry.1292438233=%20%20Bone%20segmentation%20is%20an%20essential%20step%20for%20the%20preoperative%20planning%20of%0Afracture%20trauma%20surgery.%20The%20automated%20segmentation%20of%20fractured%20bone%20from%0Acomputed%20tomography%20%28CT%29%20scans%20remains%20challenging%2C%20due%20to%20the%20large%0Adifferences%20of%20fractures%20in%20position%20and%20morphology%2C%20and%20also%20the%20inherent%0Aanatomical%20characteristics%20of%20different%20bone%20structures.%20To%20alleviate%20these%0Aissues%2C%20we%20propose%20a%20cross-scale%20attention%20mechanism%20as%20well%20as%20a%20surface%0Asupervision%20strategy%20for%20fractured%20bone%20segmentation%20in%20CT.%20Specifically%2C%20a%0Across-scale%20attention%20mechanism%20is%20introduced%20to%20effectively%20aggregate%20the%0Afeatures%20among%20different%20scales%20to%20provide%20more%20powerful%20fracture%0Arepresentation.%20Moreover%2C%20a%20surface%20supervision%20strategy%20is%20employed%2C%20which%0Aexplicitly%20constrains%20the%20network%20to%20pay%20more%20attention%20to%20the%20bone%20boundary.%0AThe%20efficacy%20of%20the%20proposed%20method%20is%20evaluated%20on%20a%20public%20dataset%20containing%0ACT%20scans%20with%20hip%20fractures.%20The%20evaluation%20metrics%20are%20Dice%20similarity%0Acoefficient%20%28DSC%29%2C%20average%20symmetric%20surface%20distance%20%28ASSD%29%2C%20and%20Hausdorff%0Adistance%20%2895HD%29.%20The%20proposed%20method%20achieves%20an%20average%20DSC%20of%2093.36%25%2C%20ASSD%20of%0A0.85mm%2C%2095HD%20of%207.51mm.%20Our%20method%20offers%20an%20effective%20fracture%20segmentation%0Aapproach%20for%20the%20pelvic%20CT%20examinations%2C%20and%20has%20the%20potential%20to%20be%20used%20for%0Aimproving%20the%20segmentation%20performance%20of%20other%20types%20of%20fractures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01204v1&entry.124074799=Read"},
{"title": "Fewer Truncations Improve Language Modeling", "author": "Hantian Ding and Zijian Wang and Giovanni Paolini and Varun Kumar and Anoop Deoras and Dan Roth and Stefano Soatto", "abstract": "  In large language model training, input documents are typically concatenated\ntogether and then split into sequences of equal length to avoid padding tokens.\nDespite its efficiency, the concatenation approach compromises data integrity\n-- it inevitably breaks many documents into incomplete pieces, leading to\nexcessive truncations that hinder the model from learning to compose logically\ncoherent and factually consistent content that is grounded on the complete\ncontext. To address the issue, we propose Best-fit Packing, a scalable and\nefficient method that packs documents into training sequences through\nlength-aware combinatorial optimization. Our method completely eliminates\nunnecessary truncations while retaining the same training efficiency as\nconcatenation. Empirical results from both text and code pre-training show that\nour method achieves superior performance (e.g., relatively +4.7% on reading\ncomprehension; +16.8% in context following; and +9.2% on program synthesis),\nand reduces closed-domain hallucination effectively by up to 58.3%.\n", "link": "http://arxiv.org/abs/2404.10830v2", "date": "2024-05-02", "relevancy": 1.8857, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4745}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4726}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fewer%20Truncations%20Improve%20Language%20Modeling&body=Title%3A%20Fewer%20Truncations%20Improve%20Language%20Modeling%0AAuthor%3A%20Hantian%20Ding%20and%20Zijian%20Wang%20and%20Giovanni%20Paolini%20and%20Varun%20Kumar%20and%20Anoop%20Deoras%20and%20Dan%20Roth%20and%20Stefano%20Soatto%0AAbstract%3A%20%20%20In%20large%20language%20model%20training%2C%20input%20documents%20are%20typically%20concatenated%0Atogether%20and%20then%20split%20into%20sequences%20of%20equal%20length%20to%20avoid%20padding%20tokens.%0ADespite%20its%20efficiency%2C%20the%20concatenation%20approach%20compromises%20data%20integrity%0A--%20it%20inevitably%20breaks%20many%20documents%20into%20incomplete%20pieces%2C%20leading%20to%0Aexcessive%20truncations%20that%20hinder%20the%20model%20from%20learning%20to%20compose%20logically%0Acoherent%20and%20factually%20consistent%20content%20that%20is%20grounded%20on%20the%20complete%0Acontext.%20To%20address%20the%20issue%2C%20we%20propose%20Best-fit%20Packing%2C%20a%20scalable%20and%0Aefficient%20method%20that%20packs%20documents%20into%20training%20sequences%20through%0Alength-aware%20combinatorial%20optimization.%20Our%20method%20completely%20eliminates%0Aunnecessary%20truncations%20while%20retaining%20the%20same%20training%20efficiency%20as%0Aconcatenation.%20Empirical%20results%20from%20both%20text%20and%20code%20pre-training%20show%20that%0Aour%20method%20achieves%20superior%20performance%20%28e.g.%2C%20relatively%20%2B4.7%25%20on%20reading%0Acomprehension%3B%20%2B16.8%25%20in%20context%20following%3B%20and%20%2B9.2%25%20on%20program%20synthesis%29%2C%0Aand%20reduces%20closed-domain%20hallucination%20effectively%20by%20up%20to%2058.3%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10830v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFewer%2520Truncations%2520Improve%2520Language%2520Modeling%26entry.906535625%3DHantian%2520Ding%2520and%2520Zijian%2520Wang%2520and%2520Giovanni%2520Paolini%2520and%2520Varun%2520Kumar%2520and%2520Anoop%2520Deoras%2520and%2520Dan%2520Roth%2520and%2520Stefano%2520Soatto%26entry.1292438233%3D%2520%2520In%2520large%2520language%2520model%2520training%252C%2520input%2520documents%2520are%2520typically%2520concatenated%250Atogether%2520and%2520then%2520split%2520into%2520sequences%2520of%2520equal%2520length%2520to%2520avoid%2520padding%2520tokens.%250ADespite%2520its%2520efficiency%252C%2520the%2520concatenation%2520approach%2520compromises%2520data%2520integrity%250A--%2520it%2520inevitably%2520breaks%2520many%2520documents%2520into%2520incomplete%2520pieces%252C%2520leading%2520to%250Aexcessive%2520truncations%2520that%2520hinder%2520the%2520model%2520from%2520learning%2520to%2520compose%2520logically%250Acoherent%2520and%2520factually%2520consistent%2520content%2520that%2520is%2520grounded%2520on%2520the%2520complete%250Acontext.%2520To%2520address%2520the%2520issue%252C%2520we%2520propose%2520Best-fit%2520Packing%252C%2520a%2520scalable%2520and%250Aefficient%2520method%2520that%2520packs%2520documents%2520into%2520training%2520sequences%2520through%250Alength-aware%2520combinatorial%2520optimization.%2520Our%2520method%2520completely%2520eliminates%250Aunnecessary%2520truncations%2520while%2520retaining%2520the%2520same%2520training%2520efficiency%2520as%250Aconcatenation.%2520Empirical%2520results%2520from%2520both%2520text%2520and%2520code%2520pre-training%2520show%2520that%250Aour%2520method%2520achieves%2520superior%2520performance%2520%2528e.g.%252C%2520relatively%2520%252B4.7%2525%2520on%2520reading%250Acomprehension%253B%2520%252B16.8%2525%2520in%2520context%2520following%253B%2520and%2520%252B9.2%2525%2520on%2520program%2520synthesis%2529%252C%250Aand%2520reduces%2520closed-domain%2520hallucination%2520effectively%2520by%2520up%2520to%252058.3%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10830v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fewer%20Truncations%20Improve%20Language%20Modeling&entry.906535625=Hantian%20Ding%20and%20Zijian%20Wang%20and%20Giovanni%20Paolini%20and%20Varun%20Kumar%20and%20Anoop%20Deoras%20and%20Dan%20Roth%20and%20Stefano%20Soatto&entry.1292438233=%20%20In%20large%20language%20model%20training%2C%20input%20documents%20are%20typically%20concatenated%0Atogether%20and%20then%20split%20into%20sequences%20of%20equal%20length%20to%20avoid%20padding%20tokens.%0ADespite%20its%20efficiency%2C%20the%20concatenation%20approach%20compromises%20data%20integrity%0A--%20it%20inevitably%20breaks%20many%20documents%20into%20incomplete%20pieces%2C%20leading%20to%0Aexcessive%20truncations%20that%20hinder%20the%20model%20from%20learning%20to%20compose%20logically%0Acoherent%20and%20factually%20consistent%20content%20that%20is%20grounded%20on%20the%20complete%0Acontext.%20To%20address%20the%20issue%2C%20we%20propose%20Best-fit%20Packing%2C%20a%20scalable%20and%0Aefficient%20method%20that%20packs%20documents%20into%20training%20sequences%20through%0Alength-aware%20combinatorial%20optimization.%20Our%20method%20completely%20eliminates%0Aunnecessary%20truncations%20while%20retaining%20the%20same%20training%20efficiency%20as%0Aconcatenation.%20Empirical%20results%20from%20both%20text%20and%20code%20pre-training%20show%20that%0Aour%20method%20achieves%20superior%20performance%20%28e.g.%2C%20relatively%20%2B4.7%25%20on%20reading%0Acomprehension%3B%20%2B16.8%25%20in%20context%20following%3B%20and%20%2B9.2%25%20on%20program%20synthesis%29%2C%0Aand%20reduces%20closed-domain%20hallucination%20effectively%20by%20up%20to%2058.3%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10830v2&entry.124074799=Read"},
{"title": "TS-RSR: A provably efficient approach for batch bayesian optimization", "author": "Zhaolin Ren and Na Li", "abstract": "  This paper presents a new approach for batch Bayesian Optimization (BO)\ncalled Thompson Sampling-Regret to Sigma Ratio directed sampling (TS-RSR),\nwhere we sample a new batch of actions by minimizing a Thompson Sampling\napproximation of a regret to uncertainty ratio. Our sampling objective is able\nto coordinate the actions chosen in each batch in a way that minimizes\nredundancy between points whilst focusing on points with high predictive means\nor high uncertainty. Theoretically, we provide rigorous convergence guarantees\non our algorithm's regret, and numerically, we demonstrate that our method\nattains state-of-the-art performance on a range of challenging synthetic and\nrealistic test functions, where it outperforms several competitive benchmark\nbatch BO algorithms.\n", "link": "http://arxiv.org/abs/2403.04764v3", "date": "2024-05-02", "relevancy": 1.8777, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4778}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TS-RSR%3A%20A%20provably%20efficient%20approach%20for%20batch%20bayesian%20optimization&body=Title%3A%20TS-RSR%3A%20A%20provably%20efficient%20approach%20for%20batch%20bayesian%20optimization%0AAuthor%3A%20Zhaolin%20Ren%20and%20Na%20Li%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20approach%20for%20batch%20Bayesian%20Optimization%20%28BO%29%0Acalled%20Thompson%20Sampling-Regret%20to%20Sigma%20Ratio%20directed%20sampling%20%28TS-RSR%29%2C%0Awhere%20we%20sample%20a%20new%20batch%20of%20actions%20by%20minimizing%20a%20Thompson%20Sampling%0Aapproximation%20of%20a%20regret%20to%20uncertainty%20ratio.%20Our%20sampling%20objective%20is%20able%0Ato%20coordinate%20the%20actions%20chosen%20in%20each%20batch%20in%20a%20way%20that%20minimizes%0Aredundancy%20between%20points%20whilst%20focusing%20on%20points%20with%20high%20predictive%20means%0Aor%20high%20uncertainty.%20Theoretically%2C%20we%20provide%20rigorous%20convergence%20guarantees%0Aon%20our%20algorithm%27s%20regret%2C%20and%20numerically%2C%20we%20demonstrate%20that%20our%20method%0Aattains%20state-of-the-art%20performance%20on%20a%20range%20of%20challenging%20synthetic%20and%0Arealistic%20test%20functions%2C%20where%20it%20outperforms%20several%20competitive%20benchmark%0Abatch%20BO%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04764v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTS-RSR%253A%2520A%2520provably%2520efficient%2520approach%2520for%2520batch%2520bayesian%2520optimization%26entry.906535625%3DZhaolin%2520Ren%2520and%2520Na%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520new%2520approach%2520for%2520batch%2520Bayesian%2520Optimization%2520%2528BO%2529%250Acalled%2520Thompson%2520Sampling-Regret%2520to%2520Sigma%2520Ratio%2520directed%2520sampling%2520%2528TS-RSR%2529%252C%250Awhere%2520we%2520sample%2520a%2520new%2520batch%2520of%2520actions%2520by%2520minimizing%2520a%2520Thompson%2520Sampling%250Aapproximation%2520of%2520a%2520regret%2520to%2520uncertainty%2520ratio.%2520Our%2520sampling%2520objective%2520is%2520able%250Ato%2520coordinate%2520the%2520actions%2520chosen%2520in%2520each%2520batch%2520in%2520a%2520way%2520that%2520minimizes%250Aredundancy%2520between%2520points%2520whilst%2520focusing%2520on%2520points%2520with%2520high%2520predictive%2520means%250Aor%2520high%2520uncertainty.%2520Theoretically%252C%2520we%2520provide%2520rigorous%2520convergence%2520guarantees%250Aon%2520our%2520algorithm%2527s%2520regret%252C%2520and%2520numerically%252C%2520we%2520demonstrate%2520that%2520our%2520method%250Aattains%2520state-of-the-art%2520performance%2520on%2520a%2520range%2520of%2520challenging%2520synthetic%2520and%250Arealistic%2520test%2520functions%252C%2520where%2520it%2520outperforms%2520several%2520competitive%2520benchmark%250Abatch%2520BO%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04764v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TS-RSR%3A%20A%20provably%20efficient%20approach%20for%20batch%20bayesian%20optimization&entry.906535625=Zhaolin%20Ren%20and%20Na%20Li&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20approach%20for%20batch%20Bayesian%20Optimization%20%28BO%29%0Acalled%20Thompson%20Sampling-Regret%20to%20Sigma%20Ratio%20directed%20sampling%20%28TS-RSR%29%2C%0Awhere%20we%20sample%20a%20new%20batch%20of%20actions%20by%20minimizing%20a%20Thompson%20Sampling%0Aapproximation%20of%20a%20regret%20to%20uncertainty%20ratio.%20Our%20sampling%20objective%20is%20able%0Ato%20coordinate%20the%20actions%20chosen%20in%20each%20batch%20in%20a%20way%20that%20minimizes%0Aredundancy%20between%20points%20whilst%20focusing%20on%20points%20with%20high%20predictive%20means%0Aor%20high%20uncertainty.%20Theoretically%2C%20we%20provide%20rigorous%20convergence%20guarantees%0Aon%20our%20algorithm%27s%20regret%2C%20and%20numerically%2C%20we%20demonstrate%20that%20our%20method%0Aattains%20state-of-the-art%20performance%20on%20a%20range%20of%20challenging%20synthetic%20and%0Arealistic%20test%20functions%2C%20where%20it%20outperforms%20several%20competitive%20benchmark%0Abatch%20BO%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04764v3&entry.124074799=Read"},
{"title": "Operational Support Estimator Networks", "author": "Mete Ahishali and Mehmet Yamac and Serkan Kiranyaz and Moncef Gabbouj", "abstract": "  In this work, we propose a novel approach called Operational Support\nEstimator Networks (OSENs) for the support estimation task. Support Estimation\n(SE) is defined as finding the locations of non-zero elements in sparse\nsignals. By its very nature, the mapping between the measurement and sparse\nsignal is a non-linear operation. Traditional support estimators rely on\ncomputationally expensive iterative signal recovery techniques to achieve such\nnon-linearity. Contrary to the convolutional layers, the proposed OSEN approach\nconsists of operational layers that can learn such complex non-linearities\nwithout the need for deep networks. In this way, the performance of\nnon-iterative support estimation is greatly improved. Moreover, the operational\nlayers comprise so-called generative super neurons with non-local kernels. The\nkernel location for each neuron/feature map is optimized jointly for the SE\ntask during training. We evaluate the OSENs in three different applications: i.\nsupport estimation from Compressive Sensing (CS) measurements, ii.\nrepresentation-based classification, and iii. learning-aided CS reconstruction\nwhere the output of OSENs is used as prior knowledge to the CS algorithm for\nenhanced reconstruction. Experimental results show that the proposed approach\nachieves computational efficiency and outperforms competing methods, especially\nat low measurement rates by significant margins. The software implementation is\nshared at https://github.com/meteahishali/OSEN.\n", "link": "http://arxiv.org/abs/2307.06065v3", "date": "2024-05-02", "relevancy": 1.8727, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4742}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4666}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Operational%20Support%20Estimator%20Networks&body=Title%3A%20Operational%20Support%20Estimator%20Networks%0AAuthor%3A%20Mete%20Ahishali%20and%20Mehmet%20Yamac%20and%20Serkan%20Kiranyaz%20and%20Moncef%20Gabbouj%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20called%20Operational%20Support%0AEstimator%20Networks%20%28OSENs%29%20for%20the%20support%20estimation%20task.%20Support%20Estimation%0A%28SE%29%20is%20defined%20as%20finding%20the%20locations%20of%20non-zero%20elements%20in%20sparse%0Asignals.%20By%20its%20very%20nature%2C%20the%20mapping%20between%20the%20measurement%20and%20sparse%0Asignal%20is%20a%20non-linear%20operation.%20Traditional%20support%20estimators%20rely%20on%0Acomputationally%20expensive%20iterative%20signal%20recovery%20techniques%20to%20achieve%20such%0Anon-linearity.%20Contrary%20to%20the%20convolutional%20layers%2C%20the%20proposed%20OSEN%20approach%0Aconsists%20of%20operational%20layers%20that%20can%20learn%20such%20complex%20non-linearities%0Awithout%20the%20need%20for%20deep%20networks.%20In%20this%20way%2C%20the%20performance%20of%0Anon-iterative%20support%20estimation%20is%20greatly%20improved.%20Moreover%2C%20the%20operational%0Alayers%20comprise%20so-called%20generative%20super%20neurons%20with%20non-local%20kernels.%20The%0Akernel%20location%20for%20each%20neuron/feature%20map%20is%20optimized%20jointly%20for%20the%20SE%0Atask%20during%20training.%20We%20evaluate%20the%20OSENs%20in%20three%20different%20applications%3A%20i.%0Asupport%20estimation%20from%20Compressive%20Sensing%20%28CS%29%20measurements%2C%20ii.%0Arepresentation-based%20classification%2C%20and%20iii.%20learning-aided%20CS%20reconstruction%0Awhere%20the%20output%20of%20OSENs%20is%20used%20as%20prior%20knowledge%20to%20the%20CS%20algorithm%20for%0Aenhanced%20reconstruction.%20Experimental%20results%20show%20that%20the%20proposed%20approach%0Aachieves%20computational%20efficiency%20and%20outperforms%20competing%20methods%2C%20especially%0Aat%20low%20measurement%20rates%20by%20significant%20margins.%20The%20software%20implementation%20is%0Ashared%20at%20https%3A//github.com/meteahishali/OSEN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.06065v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOperational%2520Support%2520Estimator%2520Networks%26entry.906535625%3DMete%2520Ahishali%2520and%2520Mehmet%2520Yamac%2520and%2520Serkan%2520Kiranyaz%2520and%2520Moncef%2520Gabbouj%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%2520called%2520Operational%2520Support%250AEstimator%2520Networks%2520%2528OSENs%2529%2520for%2520the%2520support%2520estimation%2520task.%2520Support%2520Estimation%250A%2528SE%2529%2520is%2520defined%2520as%2520finding%2520the%2520locations%2520of%2520non-zero%2520elements%2520in%2520sparse%250Asignals.%2520By%2520its%2520very%2520nature%252C%2520the%2520mapping%2520between%2520the%2520measurement%2520and%2520sparse%250Asignal%2520is%2520a%2520non-linear%2520operation.%2520Traditional%2520support%2520estimators%2520rely%2520on%250Acomputationally%2520expensive%2520iterative%2520signal%2520recovery%2520techniques%2520to%2520achieve%2520such%250Anon-linearity.%2520Contrary%2520to%2520the%2520convolutional%2520layers%252C%2520the%2520proposed%2520OSEN%2520approach%250Aconsists%2520of%2520operational%2520layers%2520that%2520can%2520learn%2520such%2520complex%2520non-linearities%250Awithout%2520the%2520need%2520for%2520deep%2520networks.%2520In%2520this%2520way%252C%2520the%2520performance%2520of%250Anon-iterative%2520support%2520estimation%2520is%2520greatly%2520improved.%2520Moreover%252C%2520the%2520operational%250Alayers%2520comprise%2520so-called%2520generative%2520super%2520neurons%2520with%2520non-local%2520kernels.%2520The%250Akernel%2520location%2520for%2520each%2520neuron/feature%2520map%2520is%2520optimized%2520jointly%2520for%2520the%2520SE%250Atask%2520during%2520training.%2520We%2520evaluate%2520the%2520OSENs%2520in%2520three%2520different%2520applications%253A%2520i.%250Asupport%2520estimation%2520from%2520Compressive%2520Sensing%2520%2528CS%2529%2520measurements%252C%2520ii.%250Arepresentation-based%2520classification%252C%2520and%2520iii.%2520learning-aided%2520CS%2520reconstruction%250Awhere%2520the%2520output%2520of%2520OSENs%2520is%2520used%2520as%2520prior%2520knowledge%2520to%2520the%2520CS%2520algorithm%2520for%250Aenhanced%2520reconstruction.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520approach%250Aachieves%2520computational%2520efficiency%2520and%2520outperforms%2520competing%2520methods%252C%2520especially%250Aat%2520low%2520measurement%2520rates%2520by%2520significant%2520margins.%2520The%2520software%2520implementation%2520is%250Ashared%2520at%2520https%253A//github.com/meteahishali/OSEN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.06065v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Operational%20Support%20Estimator%20Networks&entry.906535625=Mete%20Ahishali%20and%20Mehmet%20Yamac%20and%20Serkan%20Kiranyaz%20and%20Moncef%20Gabbouj&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20called%20Operational%20Support%0AEstimator%20Networks%20%28OSENs%29%20for%20the%20support%20estimation%20task.%20Support%20Estimation%0A%28SE%29%20is%20defined%20as%20finding%20the%20locations%20of%20non-zero%20elements%20in%20sparse%0Asignals.%20By%20its%20very%20nature%2C%20the%20mapping%20between%20the%20measurement%20and%20sparse%0Asignal%20is%20a%20non-linear%20operation.%20Traditional%20support%20estimators%20rely%20on%0Acomputationally%20expensive%20iterative%20signal%20recovery%20techniques%20to%20achieve%20such%0Anon-linearity.%20Contrary%20to%20the%20convolutional%20layers%2C%20the%20proposed%20OSEN%20approach%0Aconsists%20of%20operational%20layers%20that%20can%20learn%20such%20complex%20non-linearities%0Awithout%20the%20need%20for%20deep%20networks.%20In%20this%20way%2C%20the%20performance%20of%0Anon-iterative%20support%20estimation%20is%20greatly%20improved.%20Moreover%2C%20the%20operational%0Alayers%20comprise%20so-called%20generative%20super%20neurons%20with%20non-local%20kernels.%20The%0Akernel%20location%20for%20each%20neuron/feature%20map%20is%20optimized%20jointly%20for%20the%20SE%0Atask%20during%20training.%20We%20evaluate%20the%20OSENs%20in%20three%20different%20applications%3A%20i.%0Asupport%20estimation%20from%20Compressive%20Sensing%20%28CS%29%20measurements%2C%20ii.%0Arepresentation-based%20classification%2C%20and%20iii.%20learning-aided%20CS%20reconstruction%0Awhere%20the%20output%20of%20OSENs%20is%20used%20as%20prior%20knowledge%20to%20the%20CS%20algorithm%20for%0Aenhanced%20reconstruction.%20Experimental%20results%20show%20that%20the%20proposed%20approach%0Aachieves%20computational%20efficiency%20and%20outperforms%20competing%20methods%2C%20especially%0Aat%20low%20measurement%20rates%20by%20significant%20margins.%20The%20software%20implementation%20is%0Ashared%20at%20https%3A//github.com/meteahishali/OSEN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.06065v3&entry.124074799=Read"},
{"title": "Natural Language to Verilog: Design of a Recurrent Spiking Neural\n  Network using Large Language Models and ChatGPT", "author": "Paola Vitolo and George Psaltakis and Michael Tomlinson and Gian Domenico Licciardo and Andreas G. Andreou", "abstract": "  This paper investigates the use of Large Language Models (LLMs) for\nautomating the generation of hardware description code, aiming to explore their\npotential in supporting and enhancing the development of efficient neuromorphic\ncomputing architectures. Building on our prior work, we employ OpenAI's\nChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a\nprogrammable recurrent spiking neural network, while also generating test\nbenches to assess the system's correctness. The resultant design was validated\nin three case studies, the exclusive OR,the IRIS flower classification and the\nMNIST hand-written digit classification, achieving accuracies of up to 96.6%.\nTo verify its synthesizability and implementability, the design was prototyped\non a field-programmable gate array and implemented on SkyWater 130 nm\ntechnology by using an open-source electronic design automation flow.\nAdditionally, we have submitted it to Tiny Tapeout 6 chip fabrication program\nto further evaluate the system on-chip performance in the future.\n", "link": "http://arxiv.org/abs/2405.01419v1", "date": "2024-05-02", "relevancy": 1.8659, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4745}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4666}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20Language%20to%20Verilog%3A%20Design%20of%20a%20Recurrent%20Spiking%20Neural%0A%20%20Network%20using%20Large%20Language%20Models%20and%20ChatGPT&body=Title%3A%20Natural%20Language%20to%20Verilog%3A%20Design%20of%20a%20Recurrent%20Spiking%20Neural%0A%20%20Network%20using%20Large%20Language%20Models%20and%20ChatGPT%0AAuthor%3A%20Paola%20Vitolo%20and%20George%20Psaltakis%20and%20Michael%20Tomlinson%20and%20Gian%20Domenico%20Licciardo%20and%20Andreas%20G.%20Andreou%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%0Aautomating%20the%20generation%20of%20hardware%20description%20code%2C%20aiming%20to%20explore%20their%0Apotential%20in%20supporting%20and%20enhancing%20the%20development%20of%20efficient%20neuromorphic%0Acomputing%20architectures.%20Building%20on%20our%20prior%20work%2C%20we%20employ%20OpenAI%27s%0AChatGPT4%20and%20natural%20language%20prompts%20to%20synthesize%20a%20RTL%20Verilog%20module%20of%20a%0Aprogrammable%20recurrent%20spiking%20neural%20network%2C%20while%20also%20generating%20test%0Abenches%20to%20assess%20the%20system%27s%20correctness.%20The%20resultant%20design%20was%20validated%0Ain%20three%20case%20studies%2C%20the%20exclusive%20OR%2Cthe%20IRIS%20flower%20classification%20and%20the%0AMNIST%20hand-written%20digit%20classification%2C%20achieving%20accuracies%20of%20up%20to%2096.6%25.%0ATo%20verify%20its%20synthesizability%20and%20implementability%2C%20the%20design%20was%20prototyped%0Aon%20a%20field-programmable%20gate%20array%20and%20implemented%20on%20SkyWater%20130%20nm%0Atechnology%20by%20using%20an%20open-source%20electronic%20design%20automation%20flow.%0AAdditionally%2C%20we%20have%20submitted%20it%20to%20Tiny%20Tapeout%206%20chip%20fabrication%20program%0Ato%20further%20evaluate%20the%20system%20on-chip%20performance%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520Language%2520to%2520Verilog%253A%2520Design%2520of%2520a%2520Recurrent%2520Spiking%2520Neural%250A%2520%2520Network%2520using%2520Large%2520Language%2520Models%2520and%2520ChatGPT%26entry.906535625%3DPaola%2520Vitolo%2520and%2520George%2520Psaltakis%2520and%2520Michael%2520Tomlinson%2520and%2520Gian%2520Domenico%2520Licciardo%2520and%2520Andreas%2520G.%2520Andreou%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520use%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%250Aautomating%2520the%2520generation%2520of%2520hardware%2520description%2520code%252C%2520aiming%2520to%2520explore%2520their%250Apotential%2520in%2520supporting%2520and%2520enhancing%2520the%2520development%2520of%2520efficient%2520neuromorphic%250Acomputing%2520architectures.%2520Building%2520on%2520our%2520prior%2520work%252C%2520we%2520employ%2520OpenAI%2527s%250AChatGPT4%2520and%2520natural%2520language%2520prompts%2520to%2520synthesize%2520a%2520RTL%2520Verilog%2520module%2520of%2520a%250Aprogrammable%2520recurrent%2520spiking%2520neural%2520network%252C%2520while%2520also%2520generating%2520test%250Abenches%2520to%2520assess%2520the%2520system%2527s%2520correctness.%2520The%2520resultant%2520design%2520was%2520validated%250Ain%2520three%2520case%2520studies%252C%2520the%2520exclusive%2520OR%252Cthe%2520IRIS%2520flower%2520classification%2520and%2520the%250AMNIST%2520hand-written%2520digit%2520classification%252C%2520achieving%2520accuracies%2520of%2520up%2520to%252096.6%2525.%250ATo%2520verify%2520its%2520synthesizability%2520and%2520implementability%252C%2520the%2520design%2520was%2520prototyped%250Aon%2520a%2520field-programmable%2520gate%2520array%2520and%2520implemented%2520on%2520SkyWater%2520130%2520nm%250Atechnology%2520by%2520using%2520an%2520open-source%2520electronic%2520design%2520automation%2520flow.%250AAdditionally%252C%2520we%2520have%2520submitted%2520it%2520to%2520Tiny%2520Tapeout%25206%2520chip%2520fabrication%2520program%250Ato%2520further%2520evaluate%2520the%2520system%2520on-chip%2520performance%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20Language%20to%20Verilog%3A%20Design%20of%20a%20Recurrent%20Spiking%20Neural%0A%20%20Network%20using%20Large%20Language%20Models%20and%20ChatGPT&entry.906535625=Paola%20Vitolo%20and%20George%20Psaltakis%20and%20Michael%20Tomlinson%20and%20Gian%20Domenico%20Licciardo%20and%20Andreas%20G.%20Andreou&entry.1292438233=%20%20This%20paper%20investigates%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%0Aautomating%20the%20generation%20of%20hardware%20description%20code%2C%20aiming%20to%20explore%20their%0Apotential%20in%20supporting%20and%20enhancing%20the%20development%20of%20efficient%20neuromorphic%0Acomputing%20architectures.%20Building%20on%20our%20prior%20work%2C%20we%20employ%20OpenAI%27s%0AChatGPT4%20and%20natural%20language%20prompts%20to%20synthesize%20a%20RTL%20Verilog%20module%20of%20a%0Aprogrammable%20recurrent%20spiking%20neural%20network%2C%20while%20also%20generating%20test%0Abenches%20to%20assess%20the%20system%27s%20correctness.%20The%20resultant%20design%20was%20validated%0Ain%20three%20case%20studies%2C%20the%20exclusive%20OR%2Cthe%20IRIS%20flower%20classification%20and%20the%0AMNIST%20hand-written%20digit%20classification%2C%20achieving%20accuracies%20of%20up%20to%2096.6%25.%0ATo%20verify%20its%20synthesizability%20and%20implementability%2C%20the%20design%20was%20prototyped%0Aon%20a%20field-programmable%20gate%20array%20and%20implemented%20on%20SkyWater%20130%20nm%0Atechnology%20by%20using%20an%20open-source%20electronic%20design%20automation%20flow.%0AAdditionally%2C%20we%20have%20submitted%20it%20to%20Tiny%20Tapeout%206%20chip%20fabrication%20program%0Ato%20further%20evaluate%20the%20system%20on-chip%20performance%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01419v1&entry.124074799=Read"},
{"title": "Beyond Individual Input for Deep Anomaly Detection on Tabular Data", "author": "Hugo Thimonier and Fabrice Popineau and Arpad Rimmel and Bich-Li\u00ean Doan", "abstract": "  Anomaly detection is vital in many domains, such as finance, healthcare, and\ncybersecurity. In this paper, we propose a novel deep anomaly detection method\nfor tabular data that leverages Non-Parametric Transformers (NPTs), a model\ninitially proposed for supervised tasks, to capture both feature-feature and\nsample-sample dependencies. In a reconstruction-based framework, we train an\nNPT to reconstruct masked features of normal samples. In a non-parametric\nfashion, we leverage the whole training set during inference and use the\nmodel's ability to reconstruct the masked features to generate an anomaly\nscore. To the best of our knowledge, this is the first work to successfully\ncombine feature-feature and sample-sample dependencies for anomaly detection on\ntabular datasets. Through extensive experiments on 31 benchmark tabular\ndatasets, we demonstrate that our method achieves state-of-the-art performance,\noutperforming existing methods by 2.4% and 1.2% in terms of F1-score and AUROC,\nrespectively. Our ablation study further proves that modeling both types of\ndependencies is crucial for anomaly detection on tabular data.\n", "link": "http://arxiv.org/abs/2305.15121v6", "date": "2024-05-02", "relevancy": 1.8649, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4712}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.464}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Individual%20Input%20for%20Deep%20Anomaly%20Detection%20on%20Tabular%20Data&body=Title%3A%20Beyond%20Individual%20Input%20for%20Deep%20Anomaly%20Detection%20on%20Tabular%20Data%0AAuthor%3A%20Hugo%20Thimonier%20and%20Fabrice%20Popineau%20and%20Arpad%20Rimmel%20and%20Bich-Li%C3%AAn%20Doan%0AAbstract%3A%20%20%20Anomaly%20detection%20is%20vital%20in%20many%20domains%2C%20such%20as%20finance%2C%20healthcare%2C%20and%0Acybersecurity.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20deep%20anomaly%20detection%20method%0Afor%20tabular%20data%20that%20leverages%20Non-Parametric%20Transformers%20%28NPTs%29%2C%20a%20model%0Ainitially%20proposed%20for%20supervised%20tasks%2C%20to%20capture%20both%20feature-feature%20and%0Asample-sample%20dependencies.%20In%20a%20reconstruction-based%20framework%2C%20we%20train%20an%0ANPT%20to%20reconstruct%20masked%20features%20of%20normal%20samples.%20In%20a%20non-parametric%0Afashion%2C%20we%20leverage%20the%20whole%20training%20set%20during%20inference%20and%20use%20the%0Amodel%27s%20ability%20to%20reconstruct%20the%20masked%20features%20to%20generate%20an%20anomaly%0Ascore.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20successfully%0Acombine%20feature-feature%20and%20sample-sample%20dependencies%20for%20anomaly%20detection%20on%0Atabular%20datasets.%20Through%20extensive%20experiments%20on%2031%20benchmark%20tabular%0Adatasets%2C%20we%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%2C%0Aoutperforming%20existing%20methods%20by%202.4%25%20and%201.2%25%20in%20terms%20of%20F1-score%20and%20AUROC%2C%0Arespectively.%20Our%20ablation%20study%20further%20proves%20that%20modeling%20both%20types%20of%0Adependencies%20is%20crucial%20for%20anomaly%20detection%20on%20tabular%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15121v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Individual%2520Input%2520for%2520Deep%2520Anomaly%2520Detection%2520on%2520Tabular%2520Data%26entry.906535625%3DHugo%2520Thimonier%2520and%2520Fabrice%2520Popineau%2520and%2520Arpad%2520Rimmel%2520and%2520Bich-Li%25C3%25AAn%2520Doan%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520is%2520vital%2520in%2520many%2520domains%252C%2520such%2520as%2520finance%252C%2520healthcare%252C%2520and%250Acybersecurity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520deep%2520anomaly%2520detection%2520method%250Afor%2520tabular%2520data%2520that%2520leverages%2520Non-Parametric%2520Transformers%2520%2528NPTs%2529%252C%2520a%2520model%250Ainitially%2520proposed%2520for%2520supervised%2520tasks%252C%2520to%2520capture%2520both%2520feature-feature%2520and%250Asample-sample%2520dependencies.%2520In%2520a%2520reconstruction-based%2520framework%252C%2520we%2520train%2520an%250ANPT%2520to%2520reconstruct%2520masked%2520features%2520of%2520normal%2520samples.%2520In%2520a%2520non-parametric%250Afashion%252C%2520we%2520leverage%2520the%2520whole%2520training%2520set%2520during%2520inference%2520and%2520use%2520the%250Amodel%2527s%2520ability%2520to%2520reconstruct%2520the%2520masked%2520features%2520to%2520generate%2520an%2520anomaly%250Ascore.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520successfully%250Acombine%2520feature-feature%2520and%2520sample-sample%2520dependencies%2520for%2520anomaly%2520detection%2520on%250Atabular%2520datasets.%2520Through%2520extensive%2520experiments%2520on%252031%2520benchmark%2520tabular%250Adatasets%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%252C%250Aoutperforming%2520existing%2520methods%2520by%25202.4%2525%2520and%25201.2%2525%2520in%2520terms%2520of%2520F1-score%2520and%2520AUROC%252C%250Arespectively.%2520Our%2520ablation%2520study%2520further%2520proves%2520that%2520modeling%2520both%2520types%2520of%250Adependencies%2520is%2520crucial%2520for%2520anomaly%2520detection%2520on%2520tabular%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15121v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Individual%20Input%20for%20Deep%20Anomaly%20Detection%20on%20Tabular%20Data&entry.906535625=Hugo%20Thimonier%20and%20Fabrice%20Popineau%20and%20Arpad%20Rimmel%20and%20Bich-Li%C3%AAn%20Doan&entry.1292438233=%20%20Anomaly%20detection%20is%20vital%20in%20many%20domains%2C%20such%20as%20finance%2C%20healthcare%2C%20and%0Acybersecurity.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20deep%20anomaly%20detection%20method%0Afor%20tabular%20data%20that%20leverages%20Non-Parametric%20Transformers%20%28NPTs%29%2C%20a%20model%0Ainitially%20proposed%20for%20supervised%20tasks%2C%20to%20capture%20both%20feature-feature%20and%0Asample-sample%20dependencies.%20In%20a%20reconstruction-based%20framework%2C%20we%20train%20an%0ANPT%20to%20reconstruct%20masked%20features%20of%20normal%20samples.%20In%20a%20non-parametric%0Afashion%2C%20we%20leverage%20the%20whole%20training%20set%20during%20inference%20and%20use%20the%0Amodel%27s%20ability%20to%20reconstruct%20the%20masked%20features%20to%20generate%20an%20anomaly%0Ascore.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20successfully%0Acombine%20feature-feature%20and%20sample-sample%20dependencies%20for%20anomaly%20detection%20on%0Atabular%20datasets.%20Through%20extensive%20experiments%20on%2031%20benchmark%20tabular%0Adatasets%2C%20we%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%2C%0Aoutperforming%20existing%20methods%20by%202.4%25%20and%201.2%25%20in%20terms%20of%20F1-score%20and%20AUROC%2C%0Arespectively.%20Our%20ablation%20study%20further%20proves%20that%20modeling%20both%20types%20of%0Adependencies%20is%20crucial%20for%20anomaly%20detection%20on%20tabular%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15121v6&entry.124074799=Read"},
{"title": "In-and-Out: Algorithmic Diffusion for Sampling Convex Bodies", "author": "Yunbum Kook and Santosh S. Vempala and Matthew S. Zhang", "abstract": "  We present a new random walk for uniformly sampling high-dimensional convex\nbodies. It achieves state-of-the-art runtime complexity with stronger\nguarantees on the output than previously known, namely in R\\'enyi divergence\n(which implies TV, $\\mathcal{W}_2$, KL, $\\chi^2$). The proof departs from known\napproaches for polytime algorithms for the problem -- we utilize a stochastic\ndiffusion perspective to show contraction to the target distribution with the\nrate of convergence determined by functional isoperimetric constants of the\nstationary density.\n", "link": "http://arxiv.org/abs/2405.01425v1", "date": "2024-05-02", "relevancy": 1.8552, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4813}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4684}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-and-Out%3A%20Algorithmic%20Diffusion%20for%20Sampling%20Convex%20Bodies&body=Title%3A%20In-and-Out%3A%20Algorithmic%20Diffusion%20for%20Sampling%20Convex%20Bodies%0AAuthor%3A%20Yunbum%20Kook%20and%20Santosh%20S.%20Vempala%20and%20Matthew%20S.%20Zhang%0AAbstract%3A%20%20%20We%20present%20a%20new%20random%20walk%20for%20uniformly%20sampling%20high-dimensional%20convex%0Abodies.%20It%20achieves%20state-of-the-art%20runtime%20complexity%20with%20stronger%0Aguarantees%20on%20the%20output%20than%20previously%20known%2C%20namely%20in%20R%5C%27enyi%20divergence%0A%28which%20implies%20TV%2C%20%24%5Cmathcal%7BW%7D_2%24%2C%20KL%2C%20%24%5Cchi%5E2%24%29.%20The%20proof%20departs%20from%20known%0Aapproaches%20for%20polytime%20algorithms%20for%20the%20problem%20--%20we%20utilize%20a%20stochastic%0Adiffusion%20perspective%20to%20show%20contraction%20to%20the%20target%20distribution%20with%20the%0Arate%20of%20convergence%20determined%20by%20functional%20isoperimetric%20constants%20of%20the%0Astationary%20density.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-and-Out%253A%2520Algorithmic%2520Diffusion%2520for%2520Sampling%2520Convex%2520Bodies%26entry.906535625%3DYunbum%2520Kook%2520and%2520Santosh%2520S.%2520Vempala%2520and%2520Matthew%2520S.%2520Zhang%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520random%2520walk%2520for%2520uniformly%2520sampling%2520high-dimensional%2520convex%250Abodies.%2520It%2520achieves%2520state-of-the-art%2520runtime%2520complexity%2520with%2520stronger%250Aguarantees%2520on%2520the%2520output%2520than%2520previously%2520known%252C%2520namely%2520in%2520R%255C%2527enyi%2520divergence%250A%2528which%2520implies%2520TV%252C%2520%2524%255Cmathcal%257BW%257D_2%2524%252C%2520KL%252C%2520%2524%255Cchi%255E2%2524%2529.%2520The%2520proof%2520departs%2520from%2520known%250Aapproaches%2520for%2520polytime%2520algorithms%2520for%2520the%2520problem%2520--%2520we%2520utilize%2520a%2520stochastic%250Adiffusion%2520perspective%2520to%2520show%2520contraction%2520to%2520the%2520target%2520distribution%2520with%2520the%250Arate%2520of%2520convergence%2520determined%2520by%2520functional%2520isoperimetric%2520constants%2520of%2520the%250Astationary%2520density.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-and-Out%3A%20Algorithmic%20Diffusion%20for%20Sampling%20Convex%20Bodies&entry.906535625=Yunbum%20Kook%20and%20Santosh%20S.%20Vempala%20and%20Matthew%20S.%20Zhang&entry.1292438233=%20%20We%20present%20a%20new%20random%20walk%20for%20uniformly%20sampling%20high-dimensional%20convex%0Abodies.%20It%20achieves%20state-of-the-art%20runtime%20complexity%20with%20stronger%0Aguarantees%20on%20the%20output%20than%20previously%20known%2C%20namely%20in%20R%5C%27enyi%20divergence%0A%28which%20implies%20TV%2C%20%24%5Cmathcal%7BW%7D_2%24%2C%20KL%2C%20%24%5Cchi%5E2%24%29.%20The%20proof%20departs%20from%20known%0Aapproaches%20for%20polytime%20algorithms%20for%20the%20problem%20--%20we%20utilize%20a%20stochastic%0Adiffusion%20perspective%20to%20show%20contraction%20to%20the%20target%20distribution%20with%20the%0Arate%20of%20convergence%20determined%20by%20functional%20isoperimetric%20constants%20of%20the%0Astationary%20density.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01425v1&entry.124074799=Read"},
{"title": "Invariant Risk Minimization Is A Total Variation Model", "author": "Zhao-Rong Lai and Wei-Wen Wang", "abstract": "  Invariant risk minimization (IRM) is an arising approach to generalize\ninvariant features to different environments in machine learning. While most\nrelated works focus on new IRM settings or new application scenarios, the\nmathematical essence of IRM remains to be properly explained. We verify that\nIRM is essentially a total variation based on $L^2$ norm (TV-$\\ell_2$) of the\nlearning risk with respect to the classifier variable. Moreover, we propose a\nnovel IRM framework based on the TV-$\\ell_1$ model. It not only expands the\nclasses of functions that can be used as the learning risk, but also has robust\nperformance in denoising and invariant feature preservation based on the coarea\nformula. We also illustrate some requirements for IRM-TV-$\\ell_1$ to achieve\nout-of-distribution generalization. Experimental results show that the proposed\nframework achieves competitive performance in several benchmark machine\nlearning scenarios.\n", "link": "http://arxiv.org/abs/2405.01389v1", "date": "2024-05-02", "relevancy": 1.8494, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4653}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4622}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invariant%20Risk%20Minimization%20Is%20A%20Total%20Variation%20Model&body=Title%3A%20Invariant%20Risk%20Minimization%20Is%20A%20Total%20Variation%20Model%0AAuthor%3A%20Zhao-Rong%20Lai%20and%20Wei-Wen%20Wang%0AAbstract%3A%20%20%20Invariant%20risk%20minimization%20%28IRM%29%20is%20an%20arising%20approach%20to%20generalize%0Ainvariant%20features%20to%20different%20environments%20in%20machine%20learning.%20While%20most%0Arelated%20works%20focus%20on%20new%20IRM%20settings%20or%20new%20application%20scenarios%2C%20the%0Amathematical%20essence%20of%20IRM%20remains%20to%20be%20properly%20explained.%20We%20verify%20that%0AIRM%20is%20essentially%20a%20total%20variation%20based%20on%20%24L%5E2%24%20norm%20%28TV-%24%5Cell_2%24%29%20of%20the%0Alearning%20risk%20with%20respect%20to%20the%20classifier%20variable.%20Moreover%2C%20we%20propose%20a%0Anovel%20IRM%20framework%20based%20on%20the%20TV-%24%5Cell_1%24%20model.%20It%20not%20only%20expands%20the%0Aclasses%20of%20functions%20that%20can%20be%20used%20as%20the%20learning%20risk%2C%20but%20also%20has%20robust%0Aperformance%20in%20denoising%20and%20invariant%20feature%20preservation%20based%20on%20the%20coarea%0Aformula.%20We%20also%20illustrate%20some%20requirements%20for%20IRM-TV-%24%5Cell_1%24%20to%20achieve%0Aout-of-distribution%20generalization.%20Experimental%20results%20show%20that%20the%20proposed%0Aframework%20achieves%20competitive%20performance%20in%20several%20benchmark%20machine%0Alearning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvariant%2520Risk%2520Minimization%2520Is%2520A%2520Total%2520Variation%2520Model%26entry.906535625%3DZhao-Rong%2520Lai%2520and%2520Wei-Wen%2520Wang%26entry.1292438233%3D%2520%2520Invariant%2520risk%2520minimization%2520%2528IRM%2529%2520is%2520an%2520arising%2520approach%2520to%2520generalize%250Ainvariant%2520features%2520to%2520different%2520environments%2520in%2520machine%2520learning.%2520While%2520most%250Arelated%2520works%2520focus%2520on%2520new%2520IRM%2520settings%2520or%2520new%2520application%2520scenarios%252C%2520the%250Amathematical%2520essence%2520of%2520IRM%2520remains%2520to%2520be%2520properly%2520explained.%2520We%2520verify%2520that%250AIRM%2520is%2520essentially%2520a%2520total%2520variation%2520based%2520on%2520%2524L%255E2%2524%2520norm%2520%2528TV-%2524%255Cell_2%2524%2529%2520of%2520the%250Alearning%2520risk%2520with%2520respect%2520to%2520the%2520classifier%2520variable.%2520Moreover%252C%2520we%2520propose%2520a%250Anovel%2520IRM%2520framework%2520based%2520on%2520the%2520TV-%2524%255Cell_1%2524%2520model.%2520It%2520not%2520only%2520expands%2520the%250Aclasses%2520of%2520functions%2520that%2520can%2520be%2520used%2520as%2520the%2520learning%2520risk%252C%2520but%2520also%2520has%2520robust%250Aperformance%2520in%2520denoising%2520and%2520invariant%2520feature%2520preservation%2520based%2520on%2520the%2520coarea%250Aformula.%2520We%2520also%2520illustrate%2520some%2520requirements%2520for%2520IRM-TV-%2524%255Cell_1%2524%2520to%2520achieve%250Aout-of-distribution%2520generalization.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%250Aframework%2520achieves%2520competitive%2520performance%2520in%2520several%2520benchmark%2520machine%250Alearning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invariant%20Risk%20Minimization%20Is%20A%20Total%20Variation%20Model&entry.906535625=Zhao-Rong%20Lai%20and%20Wei-Wen%20Wang&entry.1292438233=%20%20Invariant%20risk%20minimization%20%28IRM%29%20is%20an%20arising%20approach%20to%20generalize%0Ainvariant%20features%20to%20different%20environments%20in%20machine%20learning.%20While%20most%0Arelated%20works%20focus%20on%20new%20IRM%20settings%20or%20new%20application%20scenarios%2C%20the%0Amathematical%20essence%20of%20IRM%20remains%20to%20be%20properly%20explained.%20We%20verify%20that%0AIRM%20is%20essentially%20a%20total%20variation%20based%20on%20%24L%5E2%24%20norm%20%28TV-%24%5Cell_2%24%29%20of%20the%0Alearning%20risk%20with%20respect%20to%20the%20classifier%20variable.%20Moreover%2C%20we%20propose%20a%0Anovel%20IRM%20framework%20based%20on%20the%20TV-%24%5Cell_1%24%20model.%20It%20not%20only%20expands%20the%0Aclasses%20of%20functions%20that%20can%20be%20used%20as%20the%20learning%20risk%2C%20but%20also%20has%20robust%0Aperformance%20in%20denoising%20and%20invariant%20feature%20preservation%20based%20on%20the%20coarea%0Aformula.%20We%20also%20illustrate%20some%20requirements%20for%20IRM-TV-%24%5Cell_1%24%20to%20achieve%0Aout-of-distribution%20generalization.%20Experimental%20results%20show%20that%20the%20proposed%0Aframework%20achieves%20competitive%20performance%20in%20several%20benchmark%20machine%0Alearning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01389v1&entry.124074799=Read"},
{"title": "Continual Diffusion: Continual Customization of Text-to-Image Diffusion\n  with C-LoRA", "author": "James Seale Smith and Yen-Chang Hsu and Lingyu Zhang and Ting Hua and Zsolt Kira and Yilin Shen and Hongxia Jin", "abstract": "  Recent works demonstrate a remarkable ability to customize text-to-image\ndiffusion models while only providing a few example images. What happens if you\ntry to customize such models using multiple, fine-grained concepts in a\nsequential (i.e., continual) manner? In our work, we show that recent\nstate-of-the-art customization of text-to-image models suffer from catastrophic\nforgetting when new concepts arrive sequentially. Specifically, when adding a\nnew concept, the ability to generate high quality images of past, similar\nconcepts degrade. To circumvent this forgetting, we propose a new method,\nC-LoRA, composed of a continually self-regularized low-rank adaptation in cross\nattention layers of the popular Stable Diffusion model. Furthermore, we use\ncustomization prompts which do not include the word of the customized object\n(i.e., \"person\" for a human face dataset) and are initialized as completely\nrandom embeddings. Importantly, our method induces only marginal additional\nparameter costs and requires no storage of user data for replay. We show that\nC-LoRA not only outperforms several baselines for our proposed setting of\ntext-to-image continual customization, which we refer to as Continual\nDiffusion, but that we achieve a new state-of-the-art in the well-established\nrehearsal-free continual learning setting for image classification. The high\nachieving performance of C-LoRA in two separate domains positions it as a\ncompelling solution for a wide range of applications, and we believe it has\nsignificant potential for practical impact. Project page:\nhttps://jamessealesmith.github.io/continual-diffusion/\n", "link": "http://arxiv.org/abs/2304.06027v2", "date": "2024-05-02", "relevancy": 1.8472, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6926}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6025}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Diffusion%3A%20Continual%20Customization%20of%20Text-to-Image%20Diffusion%0A%20%20with%20C-LoRA&body=Title%3A%20Continual%20Diffusion%3A%20Continual%20Customization%20of%20Text-to-Image%20Diffusion%0A%20%20with%20C-LoRA%0AAuthor%3A%20James%20Seale%20Smith%20and%20Yen-Chang%20Hsu%20and%20Lingyu%20Zhang%20and%20Ting%20Hua%20and%20Zsolt%20Kira%20and%20Yilin%20Shen%20and%20Hongxia%20Jin%0AAbstract%3A%20%20%20Recent%20works%20demonstrate%20a%20remarkable%20ability%20to%20customize%20text-to-image%0Adiffusion%20models%20while%20only%20providing%20a%20few%20example%20images.%20What%20happens%20if%20you%0Atry%20to%20customize%20such%20models%20using%20multiple%2C%20fine-grained%20concepts%20in%20a%0Asequential%20%28i.e.%2C%20continual%29%20manner%3F%20In%20our%20work%2C%20we%20show%20that%20recent%0Astate-of-the-art%20customization%20of%20text-to-image%20models%20suffer%20from%20catastrophic%0Aforgetting%20when%20new%20concepts%20arrive%20sequentially.%20Specifically%2C%20when%20adding%20a%0Anew%20concept%2C%20the%20ability%20to%20generate%20high%20quality%20images%20of%20past%2C%20similar%0Aconcepts%20degrade.%20To%20circumvent%20this%20forgetting%2C%20we%20propose%20a%20new%20method%2C%0AC-LoRA%2C%20composed%20of%20a%20continually%20self-regularized%20low-rank%20adaptation%20in%20cross%0Aattention%20layers%20of%20the%20popular%20Stable%20Diffusion%20model.%20Furthermore%2C%20we%20use%0Acustomization%20prompts%20which%20do%20not%20include%20the%20word%20of%20the%20customized%20object%0A%28i.e.%2C%20%22person%22%20for%20a%20human%20face%20dataset%29%20and%20are%20initialized%20as%20completely%0Arandom%20embeddings.%20Importantly%2C%20our%20method%20induces%20only%20marginal%20additional%0Aparameter%20costs%20and%20requires%20no%20storage%20of%20user%20data%20for%20replay.%20We%20show%20that%0AC-LoRA%20not%20only%20outperforms%20several%20baselines%20for%20our%20proposed%20setting%20of%0Atext-to-image%20continual%20customization%2C%20which%20we%20refer%20to%20as%20Continual%0ADiffusion%2C%20but%20that%20we%20achieve%20a%20new%20state-of-the-art%20in%20the%20well-established%0Arehearsal-free%20continual%20learning%20setting%20for%20image%20classification.%20The%20high%0Aachieving%20performance%20of%20C-LoRA%20in%20two%20separate%20domains%20positions%20it%20as%20a%0Acompelling%20solution%20for%20a%20wide%20range%20of%20applications%2C%20and%20we%20believe%20it%20has%0Asignificant%20potential%20for%20practical%20impact.%20Project%20page%3A%0Ahttps%3A//jamessealesmith.github.io/continual-diffusion/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.06027v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Diffusion%253A%2520Continual%2520Customization%2520of%2520Text-to-Image%2520Diffusion%250A%2520%2520with%2520C-LoRA%26entry.906535625%3DJames%2520Seale%2520Smith%2520and%2520Yen-Chang%2520Hsu%2520and%2520Lingyu%2520Zhang%2520and%2520Ting%2520Hua%2520and%2520Zsolt%2520Kira%2520and%2520Yilin%2520Shen%2520and%2520Hongxia%2520Jin%26entry.1292438233%3D%2520%2520Recent%2520works%2520demonstrate%2520a%2520remarkable%2520ability%2520to%2520customize%2520text-to-image%250Adiffusion%2520models%2520while%2520only%2520providing%2520a%2520few%2520example%2520images.%2520What%2520happens%2520if%2520you%250Atry%2520to%2520customize%2520such%2520models%2520using%2520multiple%252C%2520fine-grained%2520concepts%2520in%2520a%250Asequential%2520%2528i.e.%252C%2520continual%2529%2520manner%253F%2520In%2520our%2520work%252C%2520we%2520show%2520that%2520recent%250Astate-of-the-art%2520customization%2520of%2520text-to-image%2520models%2520suffer%2520from%2520catastrophic%250Aforgetting%2520when%2520new%2520concepts%2520arrive%2520sequentially.%2520Specifically%252C%2520when%2520adding%2520a%250Anew%2520concept%252C%2520the%2520ability%2520to%2520generate%2520high%2520quality%2520images%2520of%2520past%252C%2520similar%250Aconcepts%2520degrade.%2520To%2520circumvent%2520this%2520forgetting%252C%2520we%2520propose%2520a%2520new%2520method%252C%250AC-LoRA%252C%2520composed%2520of%2520a%2520continually%2520self-regularized%2520low-rank%2520adaptation%2520in%2520cross%250Aattention%2520layers%2520of%2520the%2520popular%2520Stable%2520Diffusion%2520model.%2520Furthermore%252C%2520we%2520use%250Acustomization%2520prompts%2520which%2520do%2520not%2520include%2520the%2520word%2520of%2520the%2520customized%2520object%250A%2528i.e.%252C%2520%2522person%2522%2520for%2520a%2520human%2520face%2520dataset%2529%2520and%2520are%2520initialized%2520as%2520completely%250Arandom%2520embeddings.%2520Importantly%252C%2520our%2520method%2520induces%2520only%2520marginal%2520additional%250Aparameter%2520costs%2520and%2520requires%2520no%2520storage%2520of%2520user%2520data%2520for%2520replay.%2520We%2520show%2520that%250AC-LoRA%2520not%2520only%2520outperforms%2520several%2520baselines%2520for%2520our%2520proposed%2520setting%2520of%250Atext-to-image%2520continual%2520customization%252C%2520which%2520we%2520refer%2520to%2520as%2520Continual%250ADiffusion%252C%2520but%2520that%2520we%2520achieve%2520a%2520new%2520state-of-the-art%2520in%2520the%2520well-established%250Arehearsal-free%2520continual%2520learning%2520setting%2520for%2520image%2520classification.%2520The%2520high%250Aachieving%2520performance%2520of%2520C-LoRA%2520in%2520two%2520separate%2520domains%2520positions%2520it%2520as%2520a%250Acompelling%2520solution%2520for%2520a%2520wide%2520range%2520of%2520applications%252C%2520and%2520we%2520believe%2520it%2520has%250Asignificant%2520potential%2520for%2520practical%2520impact.%2520Project%2520page%253A%250Ahttps%253A//jamessealesmith.github.io/continual-diffusion/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.06027v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Diffusion%3A%20Continual%20Customization%20of%20Text-to-Image%20Diffusion%0A%20%20with%20C-LoRA&entry.906535625=James%20Seale%20Smith%20and%20Yen-Chang%20Hsu%20and%20Lingyu%20Zhang%20and%20Ting%20Hua%20and%20Zsolt%20Kira%20and%20Yilin%20Shen%20and%20Hongxia%20Jin&entry.1292438233=%20%20Recent%20works%20demonstrate%20a%20remarkable%20ability%20to%20customize%20text-to-image%0Adiffusion%20models%20while%20only%20providing%20a%20few%20example%20images.%20What%20happens%20if%20you%0Atry%20to%20customize%20such%20models%20using%20multiple%2C%20fine-grained%20concepts%20in%20a%0Asequential%20%28i.e.%2C%20continual%29%20manner%3F%20In%20our%20work%2C%20we%20show%20that%20recent%0Astate-of-the-art%20customization%20of%20text-to-image%20models%20suffer%20from%20catastrophic%0Aforgetting%20when%20new%20concepts%20arrive%20sequentially.%20Specifically%2C%20when%20adding%20a%0Anew%20concept%2C%20the%20ability%20to%20generate%20high%20quality%20images%20of%20past%2C%20similar%0Aconcepts%20degrade.%20To%20circumvent%20this%20forgetting%2C%20we%20propose%20a%20new%20method%2C%0AC-LoRA%2C%20composed%20of%20a%20continually%20self-regularized%20low-rank%20adaptation%20in%20cross%0Aattention%20layers%20of%20the%20popular%20Stable%20Diffusion%20model.%20Furthermore%2C%20we%20use%0Acustomization%20prompts%20which%20do%20not%20include%20the%20word%20of%20the%20customized%20object%0A%28i.e.%2C%20%22person%22%20for%20a%20human%20face%20dataset%29%20and%20are%20initialized%20as%20completely%0Arandom%20embeddings.%20Importantly%2C%20our%20method%20induces%20only%20marginal%20additional%0Aparameter%20costs%20and%20requires%20no%20storage%20of%20user%20data%20for%20replay.%20We%20show%20that%0AC-LoRA%20not%20only%20outperforms%20several%20baselines%20for%20our%20proposed%20setting%20of%0Atext-to-image%20continual%20customization%2C%20which%20we%20refer%20to%20as%20Continual%0ADiffusion%2C%20but%20that%20we%20achieve%20a%20new%20state-of-the-art%20in%20the%20well-established%0Arehearsal-free%20continual%20learning%20setting%20for%20image%20classification.%20The%20high%0Aachieving%20performance%20of%20C-LoRA%20in%20two%20separate%20domains%20positions%20it%20as%20a%0Acompelling%20solution%20for%20a%20wide%20range%20of%20applications%2C%20and%20we%20believe%20it%20has%0Asignificant%20potential%20for%20practical%20impact.%20Project%20page%3A%0Ahttps%3A//jamessealesmith.github.io/continual-diffusion/%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.06027v2&entry.124074799=Read"},
{"title": "Accelerating Diffusion Models for Inverse Problems through Shortcut\n  Sampling", "author": "Gongye Liu and Haoze Sun and Jiayi Li and Fei Yin and Yujiu Yang", "abstract": "  Diffusion models have recently demonstrated an impressive ability to address\ninverse problems in an unsupervised manner. While existing methods primarily\nfocus on modifying the posterior sampling process, the potential of the forward\nprocess remains largely unexplored. In this work, we propose Shortcut Sampling\nfor Diffusion(SSD), a novel approach for solving inverse problems in a\nzero-shot manner. Instead of initiating from random noise, the core concept of\nSSD is to find a specific transitional state that bridges the measurement image\ny and the restored image x. By utilizing the shortcut path of \"input -\ntransitional state - output\", SSD can achieve precise restoration with fewer\nsteps. To derive the transitional state during the forward process, we\nintroduce Distortion Adaptive Inversion. Moreover, we apply back projection as\nadditional consistency constraints during the generation process.\nExperimentally, we demonstrate SSD's effectiveness on multiple representative\nIR tasks. Our method achieves competitive results with only 30 NFEs compared to\nstate-of-the-art zero-shot methods(100 NFEs) and outperforms them with 100 NFEs\nin certain tasks. Code is available at https://github.com/GongyeLiu/SSD\n", "link": "http://arxiv.org/abs/2305.16965v2", "date": "2024-05-02", "relevancy": 1.8428, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6766}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6011}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Diffusion%20Models%20for%20Inverse%20Problems%20through%20Shortcut%0A%20%20Sampling&body=Title%3A%20Accelerating%20Diffusion%20Models%20for%20Inverse%20Problems%20through%20Shortcut%0A%20%20Sampling%0AAuthor%3A%20Gongye%20Liu%20and%20Haoze%20Sun%20and%20Jiayi%20Li%20and%20Fei%20Yin%20and%20Yujiu%20Yang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20recently%20demonstrated%20an%20impressive%20ability%20to%20address%0Ainverse%20problems%20in%20an%20unsupervised%20manner.%20While%20existing%20methods%20primarily%0Afocus%20on%20modifying%20the%20posterior%20sampling%20process%2C%20the%20potential%20of%20the%20forward%0Aprocess%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20propose%20Shortcut%20Sampling%0Afor%20Diffusion%28SSD%29%2C%20a%20novel%20approach%20for%20solving%20inverse%20problems%20in%20a%0Azero-shot%20manner.%20Instead%20of%20initiating%20from%20random%20noise%2C%20the%20core%20concept%20of%0ASSD%20is%20to%20find%20a%20specific%20transitional%20state%20that%20bridges%20the%20measurement%20image%0Ay%20and%20the%20restored%20image%20x.%20By%20utilizing%20the%20shortcut%20path%20of%20%22input%20-%0Atransitional%20state%20-%20output%22%2C%20SSD%20can%20achieve%20precise%20restoration%20with%20fewer%0Asteps.%20To%20derive%20the%20transitional%20state%20during%20the%20forward%20process%2C%20we%0Aintroduce%20Distortion%20Adaptive%20Inversion.%20Moreover%2C%20we%20apply%20back%20projection%20as%0Aadditional%20consistency%20constraints%20during%20the%20generation%20process.%0AExperimentally%2C%20we%20demonstrate%20SSD%27s%20effectiveness%20on%20multiple%20representative%0AIR%20tasks.%20Our%20method%20achieves%20competitive%20results%20with%20only%2030%20NFEs%20compared%20to%0Astate-of-the-art%20zero-shot%20methods%28100%20NFEs%29%20and%20outperforms%20them%20with%20100%20NFEs%0Ain%20certain%20tasks.%20Code%20is%20available%20at%20https%3A//github.com/GongyeLiu/SSD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.16965v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Diffusion%2520Models%2520for%2520Inverse%2520Problems%2520through%2520Shortcut%250A%2520%2520Sampling%26entry.906535625%3DGongye%2520Liu%2520and%2520Haoze%2520Sun%2520and%2520Jiayi%2520Li%2520and%2520Fei%2520Yin%2520and%2520Yujiu%2520Yang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520recently%2520demonstrated%2520an%2520impressive%2520ability%2520to%2520address%250Ainverse%2520problems%2520in%2520an%2520unsupervised%2520manner.%2520While%2520existing%2520methods%2520primarily%250Afocus%2520on%2520modifying%2520the%2520posterior%2520sampling%2520process%252C%2520the%2520potential%2520of%2520the%2520forward%250Aprocess%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520propose%2520Shortcut%2520Sampling%250Afor%2520Diffusion%2528SSD%2529%252C%2520a%2520novel%2520approach%2520for%2520solving%2520inverse%2520problems%2520in%2520a%250Azero-shot%2520manner.%2520Instead%2520of%2520initiating%2520from%2520random%2520noise%252C%2520the%2520core%2520concept%2520of%250ASSD%2520is%2520to%2520find%2520a%2520specific%2520transitional%2520state%2520that%2520bridges%2520the%2520measurement%2520image%250Ay%2520and%2520the%2520restored%2520image%2520x.%2520By%2520utilizing%2520the%2520shortcut%2520path%2520of%2520%2522input%2520-%250Atransitional%2520state%2520-%2520output%2522%252C%2520SSD%2520can%2520achieve%2520precise%2520restoration%2520with%2520fewer%250Asteps.%2520To%2520derive%2520the%2520transitional%2520state%2520during%2520the%2520forward%2520process%252C%2520we%250Aintroduce%2520Distortion%2520Adaptive%2520Inversion.%2520Moreover%252C%2520we%2520apply%2520back%2520projection%2520as%250Aadditional%2520consistency%2520constraints%2520during%2520the%2520generation%2520process.%250AExperimentally%252C%2520we%2520demonstrate%2520SSD%2527s%2520effectiveness%2520on%2520multiple%2520representative%250AIR%2520tasks.%2520Our%2520method%2520achieves%2520competitive%2520results%2520with%2520only%252030%2520NFEs%2520compared%2520to%250Astate-of-the-art%2520zero-shot%2520methods%2528100%2520NFEs%2529%2520and%2520outperforms%2520them%2520with%2520100%2520NFEs%250Ain%2520certain%2520tasks.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/GongyeLiu/SSD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.16965v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Diffusion%20Models%20for%20Inverse%20Problems%20through%20Shortcut%0A%20%20Sampling&entry.906535625=Gongye%20Liu%20and%20Haoze%20Sun%20and%20Jiayi%20Li%20and%20Fei%20Yin%20and%20Yujiu%20Yang&entry.1292438233=%20%20Diffusion%20models%20have%20recently%20demonstrated%20an%20impressive%20ability%20to%20address%0Ainverse%20problems%20in%20an%20unsupervised%20manner.%20While%20existing%20methods%20primarily%0Afocus%20on%20modifying%20the%20posterior%20sampling%20process%2C%20the%20potential%20of%20the%20forward%0Aprocess%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20propose%20Shortcut%20Sampling%0Afor%20Diffusion%28SSD%29%2C%20a%20novel%20approach%20for%20solving%20inverse%20problems%20in%20a%0Azero-shot%20manner.%20Instead%20of%20initiating%20from%20random%20noise%2C%20the%20core%20concept%20of%0ASSD%20is%20to%20find%20a%20specific%20transitional%20state%20that%20bridges%20the%20measurement%20image%0Ay%20and%20the%20restored%20image%20x.%20By%20utilizing%20the%20shortcut%20path%20of%20%22input%20-%0Atransitional%20state%20-%20output%22%2C%20SSD%20can%20achieve%20precise%20restoration%20with%20fewer%0Asteps.%20To%20derive%20the%20transitional%20state%20during%20the%20forward%20process%2C%20we%0Aintroduce%20Distortion%20Adaptive%20Inversion.%20Moreover%2C%20we%20apply%20back%20projection%20as%0Aadditional%20consistency%20constraints%20during%20the%20generation%20process.%0AExperimentally%2C%20we%20demonstrate%20SSD%27s%20effectiveness%20on%20multiple%20representative%0AIR%20tasks.%20Our%20method%20achieves%20competitive%20results%20with%20only%2030%20NFEs%20compared%20to%0Astate-of-the-art%20zero-shot%20methods%28100%20NFEs%29%20and%20outperforms%20them%20with%20100%20NFEs%0Ain%20certain%20tasks.%20Code%20is%20available%20at%20https%3A//github.com/GongyeLiu/SSD%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.16965v2&entry.124074799=Read"},
{"title": "SynFlowNet: Towards Molecule Design with Guaranteed Synthesis Pathways", "author": "Miruna Cretu and Charles Harris and Julien Roy and Emmanuel Bengio and Pietro Li\u00f2", "abstract": "  Recent breakthroughs in generative modelling have led to a number of works\nproposing molecular generation models for drug discovery. While these models\nperform well at capturing drug-like motifs, they are known to often produce\nsynthetically inaccessible molecules. This is because they are trained to\ncompose atoms or fragments in a way that approximates the training\ndistribution, but they are not explicitly aware of the synthesis constraints\nthat come with making molecules in the lab. To address this issue, we introduce\nSynFlowNet, a GFlowNet model whose action space uses chemically validated\nreactions and reactants to sequentially build new molecules. We evaluate our\napproach using synthetic accessibility scores and an independent retrosynthesis\ntool. SynFlowNet consistently samples synthetically feasible molecules, while\nstill being able to find diverse and high-utility candidates. Furthermore, we\ncompare molecules designed with SynFlowNet to experimentally validated actives,\nand find that they show comparable properties of interest, such as molecular\nweight, SA score and predicted protein binding affinity.\n", "link": "http://arxiv.org/abs/2405.01155v1", "date": "2024-05-02", "relevancy": 1.8368, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.519}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4665}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynFlowNet%3A%20Towards%20Molecule%20Design%20with%20Guaranteed%20Synthesis%20Pathways&body=Title%3A%20SynFlowNet%3A%20Towards%20Molecule%20Design%20with%20Guaranteed%20Synthesis%20Pathways%0AAuthor%3A%20Miruna%20Cretu%20and%20Charles%20Harris%20and%20Julien%20Roy%20and%20Emmanuel%20Bengio%20and%20Pietro%20Li%C3%B2%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20generative%20modelling%20have%20led%20to%20a%20number%20of%20works%0Aproposing%20molecular%20generation%20models%20for%20drug%20discovery.%20While%20these%20models%0Aperform%20well%20at%20capturing%20drug-like%20motifs%2C%20they%20are%20known%20to%20often%20produce%0Asynthetically%20inaccessible%20molecules.%20This%20is%20because%20they%20are%20trained%20to%0Acompose%20atoms%20or%20fragments%20in%20a%20way%20that%20approximates%20the%20training%0Adistribution%2C%20but%20they%20are%20not%20explicitly%20aware%20of%20the%20synthesis%20constraints%0Athat%20come%20with%20making%20molecules%20in%20the%20lab.%20To%20address%20this%20issue%2C%20we%20introduce%0ASynFlowNet%2C%20a%20GFlowNet%20model%20whose%20action%20space%20uses%20chemically%20validated%0Areactions%20and%20reactants%20to%20sequentially%20build%20new%20molecules.%20We%20evaluate%20our%0Aapproach%20using%20synthetic%20accessibility%20scores%20and%20an%20independent%20retrosynthesis%0Atool.%20SynFlowNet%20consistently%20samples%20synthetically%20feasible%20molecules%2C%20while%0Astill%20being%20able%20to%20find%20diverse%20and%20high-utility%20candidates.%20Furthermore%2C%20we%0Acompare%20molecules%20designed%20with%20SynFlowNet%20to%20experimentally%20validated%20actives%2C%0Aand%20find%20that%20they%20show%20comparable%20properties%20of%20interest%2C%20such%20as%20molecular%0Aweight%2C%20SA%20score%20and%20predicted%20protein%20binding%20affinity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynFlowNet%253A%2520Towards%2520Molecule%2520Design%2520with%2520Guaranteed%2520Synthesis%2520Pathways%26entry.906535625%3DMiruna%2520Cretu%2520and%2520Charles%2520Harris%2520and%2520Julien%2520Roy%2520and%2520Emmanuel%2520Bengio%2520and%2520Pietro%2520Li%25C3%25B2%26entry.1292438233%3D%2520%2520Recent%2520breakthroughs%2520in%2520generative%2520modelling%2520have%2520led%2520to%2520a%2520number%2520of%2520works%250Aproposing%2520molecular%2520generation%2520models%2520for%2520drug%2520discovery.%2520While%2520these%2520models%250Aperform%2520well%2520at%2520capturing%2520drug-like%2520motifs%252C%2520they%2520are%2520known%2520to%2520often%2520produce%250Asynthetically%2520inaccessible%2520molecules.%2520This%2520is%2520because%2520they%2520are%2520trained%2520to%250Acompose%2520atoms%2520or%2520fragments%2520in%2520a%2520way%2520that%2520approximates%2520the%2520training%250Adistribution%252C%2520but%2520they%2520are%2520not%2520explicitly%2520aware%2520of%2520the%2520synthesis%2520constraints%250Athat%2520come%2520with%2520making%2520molecules%2520in%2520the%2520lab.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%250ASynFlowNet%252C%2520a%2520GFlowNet%2520model%2520whose%2520action%2520space%2520uses%2520chemically%2520validated%250Areactions%2520and%2520reactants%2520to%2520sequentially%2520build%2520new%2520molecules.%2520We%2520evaluate%2520our%250Aapproach%2520using%2520synthetic%2520accessibility%2520scores%2520and%2520an%2520independent%2520retrosynthesis%250Atool.%2520SynFlowNet%2520consistently%2520samples%2520synthetically%2520feasible%2520molecules%252C%2520while%250Astill%2520being%2520able%2520to%2520find%2520diverse%2520and%2520high-utility%2520candidates.%2520Furthermore%252C%2520we%250Acompare%2520molecules%2520designed%2520with%2520SynFlowNet%2520to%2520experimentally%2520validated%2520actives%252C%250Aand%2520find%2520that%2520they%2520show%2520comparable%2520properties%2520of%2520interest%252C%2520such%2520as%2520molecular%250Aweight%252C%2520SA%2520score%2520and%2520predicted%2520protein%2520binding%2520affinity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynFlowNet%3A%20Towards%20Molecule%20Design%20with%20Guaranteed%20Synthesis%20Pathways&entry.906535625=Miruna%20Cretu%20and%20Charles%20Harris%20and%20Julien%20Roy%20and%20Emmanuel%20Bengio%20and%20Pietro%20Li%C3%B2&entry.1292438233=%20%20Recent%20breakthroughs%20in%20generative%20modelling%20have%20led%20to%20a%20number%20of%20works%0Aproposing%20molecular%20generation%20models%20for%20drug%20discovery.%20While%20these%20models%0Aperform%20well%20at%20capturing%20drug-like%20motifs%2C%20they%20are%20known%20to%20often%20produce%0Asynthetically%20inaccessible%20molecules.%20This%20is%20because%20they%20are%20trained%20to%0Acompose%20atoms%20or%20fragments%20in%20a%20way%20that%20approximates%20the%20training%0Adistribution%2C%20but%20they%20are%20not%20explicitly%20aware%20of%20the%20synthesis%20constraints%0Athat%20come%20with%20making%20molecules%20in%20the%20lab.%20To%20address%20this%20issue%2C%20we%20introduce%0ASynFlowNet%2C%20a%20GFlowNet%20model%20whose%20action%20space%20uses%20chemically%20validated%0Areactions%20and%20reactants%20to%20sequentially%20build%20new%20molecules.%20We%20evaluate%20our%0Aapproach%20using%20synthetic%20accessibility%20scores%20and%20an%20independent%20retrosynthesis%0Atool.%20SynFlowNet%20consistently%20samples%20synthetically%20feasible%20molecules%2C%20while%0Astill%20being%20able%20to%20find%20diverse%20and%20high-utility%20candidates.%20Furthermore%2C%20we%0Acompare%20molecules%20designed%20with%20SynFlowNet%20to%20experimentally%20validated%20actives%2C%0Aand%20find%20that%20they%20show%20comparable%20properties%20of%20interest%2C%20such%20as%20molecular%0Aweight%2C%20SA%20score%20and%20predicted%20protein%20binding%20affinity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01155v1&entry.124074799=Read"},
{"title": "FeNNol: an Efficient and Flexible Library for Building\n  Force-field-enhanced Neural Network Potentials", "author": "Thomas Pl\u00e9 and Olivier Adjoua and Louis Lagard\u00e8re and Jean-Philip Piquemal", "abstract": "  Neural network interatomic potentials (NNPs) have recently proven to be\npowerful tools to accurately model complex molecular systems while bypassing\nthe high numerical cost of ab-initio molecular dynamics simulations. In recent\nyears, numerous advances in model architectures as well as the development of\nhybrid models combining machine-learning (ML) with more traditional,\nphysically-motivated, force-field interactions have considerably increased the\ndesign space of ML potentials. In this paper, we present FeNNol, a new library\nfor building, training and running force-field-enhanced neural network\npotentials. It provides a flexible and modular system for building hybrid\nmodels, allowing to easily combine state-of-the-art embeddings with\nML-parameterized physical interaction terms without the need for explicit\nprogramming. Furthermore, FeNNol leverages the automatic differentiation and\njust-in-time compilation features of the Jax Python library to enable fast\nevaluation of NNPs, shrinking the performance gap between ML potentials and\nstandard force-fields. This is demonstrated with the popular ANI-2x model\nreaching simulation speeds nearly on par with the AMOEBA polarizable\nforce-field on commodity GPUs (GPU=Graphics processing unit). We hope that\nFeNNol will facilitate the development and application of new hybrid NNP\narchitectures for a wide range of molecular simulation problems.\n", "link": "http://arxiv.org/abs/2405.01491v1", "date": "2024-05-02", "relevancy": 1.8353, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4836}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4616}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FeNNol%3A%20an%20Efficient%20and%20Flexible%20Library%20for%20Building%0A%20%20Force-field-enhanced%20Neural%20Network%20Potentials&body=Title%3A%20FeNNol%3A%20an%20Efficient%20and%20Flexible%20Library%20for%20Building%0A%20%20Force-field-enhanced%20Neural%20Network%20Potentials%0AAuthor%3A%20Thomas%20Pl%C3%A9%20and%20Olivier%20Adjoua%20and%20Louis%20Lagard%C3%A8re%20and%20Jean-Philip%20Piquemal%0AAbstract%3A%20%20%20Neural%20network%20interatomic%20potentials%20%28NNPs%29%20have%20recently%20proven%20to%20be%0Apowerful%20tools%20to%20accurately%20model%20complex%20molecular%20systems%20while%20bypassing%0Athe%20high%20numerical%20cost%20of%20ab-initio%20molecular%20dynamics%20simulations.%20In%20recent%0Ayears%2C%20numerous%20advances%20in%20model%20architectures%20as%20well%20as%20the%20development%20of%0Ahybrid%20models%20combining%20machine-learning%20%28ML%29%20with%20more%20traditional%2C%0Aphysically-motivated%2C%20force-field%20interactions%20have%20considerably%20increased%20the%0Adesign%20space%20of%20ML%20potentials.%20In%20this%20paper%2C%20we%20present%20FeNNol%2C%20a%20new%20library%0Afor%20building%2C%20training%20and%20running%20force-field-enhanced%20neural%20network%0Apotentials.%20It%20provides%20a%20flexible%20and%20modular%20system%20for%20building%20hybrid%0Amodels%2C%20allowing%20to%20easily%20combine%20state-of-the-art%20embeddings%20with%0AML-parameterized%20physical%20interaction%20terms%20without%20the%20need%20for%20explicit%0Aprogramming.%20Furthermore%2C%20FeNNol%20leverages%20the%20automatic%20differentiation%20and%0Ajust-in-time%20compilation%20features%20of%20the%20Jax%20Python%20library%20to%20enable%20fast%0Aevaluation%20of%20NNPs%2C%20shrinking%20the%20performance%20gap%20between%20ML%20potentials%20and%0Astandard%20force-fields.%20This%20is%20demonstrated%20with%20the%20popular%20ANI-2x%20model%0Areaching%20simulation%20speeds%20nearly%20on%20par%20with%20the%20AMOEBA%20polarizable%0Aforce-field%20on%20commodity%20GPUs%20%28GPU%3DGraphics%20processing%20unit%29.%20We%20hope%20that%0AFeNNol%20will%20facilitate%20the%20development%20and%20application%20of%20new%20hybrid%20NNP%0Aarchitectures%20for%20a%20wide%20range%20of%20molecular%20simulation%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeNNol%253A%2520an%2520Efficient%2520and%2520Flexible%2520Library%2520for%2520Building%250A%2520%2520Force-field-enhanced%2520Neural%2520Network%2520Potentials%26entry.906535625%3DThomas%2520Pl%25C3%25A9%2520and%2520Olivier%2520Adjoua%2520and%2520Louis%2520Lagard%25C3%25A8re%2520and%2520Jean-Philip%2520Piquemal%26entry.1292438233%3D%2520%2520Neural%2520network%2520interatomic%2520potentials%2520%2528NNPs%2529%2520have%2520recently%2520proven%2520to%2520be%250Apowerful%2520tools%2520to%2520accurately%2520model%2520complex%2520molecular%2520systems%2520while%2520bypassing%250Athe%2520high%2520numerical%2520cost%2520of%2520ab-initio%2520molecular%2520dynamics%2520simulations.%2520In%2520recent%250Ayears%252C%2520numerous%2520advances%2520in%2520model%2520architectures%2520as%2520well%2520as%2520the%2520development%2520of%250Ahybrid%2520models%2520combining%2520machine-learning%2520%2528ML%2529%2520with%2520more%2520traditional%252C%250Aphysically-motivated%252C%2520force-field%2520interactions%2520have%2520considerably%2520increased%2520the%250Adesign%2520space%2520of%2520ML%2520potentials.%2520In%2520this%2520paper%252C%2520we%2520present%2520FeNNol%252C%2520a%2520new%2520library%250Afor%2520building%252C%2520training%2520and%2520running%2520force-field-enhanced%2520neural%2520network%250Apotentials.%2520It%2520provides%2520a%2520flexible%2520and%2520modular%2520system%2520for%2520building%2520hybrid%250Amodels%252C%2520allowing%2520to%2520easily%2520combine%2520state-of-the-art%2520embeddings%2520with%250AML-parameterized%2520physical%2520interaction%2520terms%2520without%2520the%2520need%2520for%2520explicit%250Aprogramming.%2520Furthermore%252C%2520FeNNol%2520leverages%2520the%2520automatic%2520differentiation%2520and%250Ajust-in-time%2520compilation%2520features%2520of%2520the%2520Jax%2520Python%2520library%2520to%2520enable%2520fast%250Aevaluation%2520of%2520NNPs%252C%2520shrinking%2520the%2520performance%2520gap%2520between%2520ML%2520potentials%2520and%250Astandard%2520force-fields.%2520This%2520is%2520demonstrated%2520with%2520the%2520popular%2520ANI-2x%2520model%250Areaching%2520simulation%2520speeds%2520nearly%2520on%2520par%2520with%2520the%2520AMOEBA%2520polarizable%250Aforce-field%2520on%2520commodity%2520GPUs%2520%2528GPU%253DGraphics%2520processing%2520unit%2529.%2520We%2520hope%2520that%250AFeNNol%2520will%2520facilitate%2520the%2520development%2520and%2520application%2520of%2520new%2520hybrid%2520NNP%250Aarchitectures%2520for%2520a%2520wide%2520range%2520of%2520molecular%2520simulation%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FeNNol%3A%20an%20Efficient%20and%20Flexible%20Library%20for%20Building%0A%20%20Force-field-enhanced%20Neural%20Network%20Potentials&entry.906535625=Thomas%20Pl%C3%A9%20and%20Olivier%20Adjoua%20and%20Louis%20Lagard%C3%A8re%20and%20Jean-Philip%20Piquemal&entry.1292438233=%20%20Neural%20network%20interatomic%20potentials%20%28NNPs%29%20have%20recently%20proven%20to%20be%0Apowerful%20tools%20to%20accurately%20model%20complex%20molecular%20systems%20while%20bypassing%0Athe%20high%20numerical%20cost%20of%20ab-initio%20molecular%20dynamics%20simulations.%20In%20recent%0Ayears%2C%20numerous%20advances%20in%20model%20architectures%20as%20well%20as%20the%20development%20of%0Ahybrid%20models%20combining%20machine-learning%20%28ML%29%20with%20more%20traditional%2C%0Aphysically-motivated%2C%20force-field%20interactions%20have%20considerably%20increased%20the%0Adesign%20space%20of%20ML%20potentials.%20In%20this%20paper%2C%20we%20present%20FeNNol%2C%20a%20new%20library%0Afor%20building%2C%20training%20and%20running%20force-field-enhanced%20neural%20network%0Apotentials.%20It%20provides%20a%20flexible%20and%20modular%20system%20for%20building%20hybrid%0Amodels%2C%20allowing%20to%20easily%20combine%20state-of-the-art%20embeddings%20with%0AML-parameterized%20physical%20interaction%20terms%20without%20the%20need%20for%20explicit%0Aprogramming.%20Furthermore%2C%20FeNNol%20leverages%20the%20automatic%20differentiation%20and%0Ajust-in-time%20compilation%20features%20of%20the%20Jax%20Python%20library%20to%20enable%20fast%0Aevaluation%20of%20NNPs%2C%20shrinking%20the%20performance%20gap%20between%20ML%20potentials%20and%0Astandard%20force-fields.%20This%20is%20demonstrated%20with%20the%20popular%20ANI-2x%20model%0Areaching%20simulation%20speeds%20nearly%20on%20par%20with%20the%20AMOEBA%20polarizable%0Aforce-field%20on%20commodity%20GPUs%20%28GPU%3DGraphics%20processing%20unit%29.%20We%20hope%20that%0AFeNNol%20will%20facilitate%20the%20development%20and%20application%20of%20new%20hybrid%20NNP%0Aarchitectures%20for%20a%20wide%20range%20of%20molecular%20simulation%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01491v1&entry.124074799=Read"},
{"title": "Chronos: Learning the Language of Time Series", "author": "Abdul Fatir Ansari and Lorenzo Stella and Caner Turkmen and Xiyuan Zhang and Pedro Mercado and Huibin Shen and Oleksandr Shchur and Syama Sundar Rangapuram and Sebastian Pineda Arango and Shubham Kapoor and Jasper Zschiegner and Danielle C. Maddix and Hao Wang and Michael W. Mahoney and Kari Torkkola and Andrew Gordon Wilson and Michael Bohlke-Schneider and Yuyang Wang", "abstract": "  We introduce Chronos, a simple yet effective framework for pretrained\nprobabilistic time series models. Chronos tokenizes time series values using\nscaling and quantization into a fixed vocabulary and trains existing\ntransformer-based language model architectures on these tokenized time series\nvia the cross-entropy loss. We pretrained Chronos models based on the T5 family\n(ranging from 20M to 710M parameters) on a large collection of publicly\navailable datasets, complemented by a synthetic dataset that we generated via\nGaussian processes to improve generalization. In a comprehensive benchmark\nconsisting of 42 datasets, and comprising both classical local models and deep\nlearning methods, we show that Chronos models: (a) significantly outperform\nother methods on datasets that were part of the training corpus; and (b) have\ncomparable and occasionally superior zero-shot performance on new datasets,\nrelative to methods that were trained specifically on them. Our results\ndemonstrate that Chronos models can leverage time series data from diverse\ndomains to improve zero-shot accuracy on unseen forecasting tasks, positioning\npretrained models as a viable tool to greatly simplify forecasting pipelines.\n", "link": "http://arxiv.org/abs/2403.07815v2", "date": "2024-05-02", "relevancy": 1.8212, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4722}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4591}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chronos%3A%20Learning%20the%20Language%20of%20Time%20Series&body=Title%3A%20Chronos%3A%20Learning%20the%20Language%20of%20Time%20Series%0AAuthor%3A%20Abdul%20Fatir%20Ansari%20and%20Lorenzo%20Stella%20and%20Caner%20Turkmen%20and%20Xiyuan%20Zhang%20and%20Pedro%20Mercado%20and%20Huibin%20Shen%20and%20Oleksandr%20Shchur%20and%20Syama%20Sundar%20Rangapuram%20and%20Sebastian%20Pineda%20Arango%20and%20Shubham%20Kapoor%20and%20Jasper%20Zschiegner%20and%20Danielle%20C.%20Maddix%20and%20Hao%20Wang%20and%20Michael%20W.%20Mahoney%20and%20Kari%20Torkkola%20and%20Andrew%20Gordon%20Wilson%20and%20Michael%20Bohlke-Schneider%20and%20Yuyang%20Wang%0AAbstract%3A%20%20%20We%20introduce%20Chronos%2C%20a%20simple%20yet%20effective%20framework%20for%20pretrained%0Aprobabilistic%20time%20series%20models.%20Chronos%20tokenizes%20time%20series%20values%20using%0Ascaling%20and%20quantization%20into%20a%20fixed%20vocabulary%20and%20trains%20existing%0Atransformer-based%20language%20model%20architectures%20on%20these%20tokenized%20time%20series%0Avia%20the%20cross-entropy%20loss.%20We%20pretrained%20Chronos%20models%20based%20on%20the%20T5%20family%0A%28ranging%20from%2020M%20to%20710M%20parameters%29%20on%20a%20large%20collection%20of%20publicly%0Aavailable%20datasets%2C%20complemented%20by%20a%20synthetic%20dataset%20that%20we%20generated%20via%0AGaussian%20processes%20to%20improve%20generalization.%20In%20a%20comprehensive%20benchmark%0Aconsisting%20of%2042%20datasets%2C%20and%20comprising%20both%20classical%20local%20models%20and%20deep%0Alearning%20methods%2C%20we%20show%20that%20Chronos%20models%3A%20%28a%29%20significantly%20outperform%0Aother%20methods%20on%20datasets%20that%20were%20part%20of%20the%20training%20corpus%3B%20and%20%28b%29%20have%0Acomparable%20and%20occasionally%20superior%20zero-shot%20performance%20on%20new%20datasets%2C%0Arelative%20to%20methods%20that%20were%20trained%20specifically%20on%20them.%20Our%20results%0Ademonstrate%20that%20Chronos%20models%20can%20leverage%20time%20series%20data%20from%20diverse%0Adomains%20to%20improve%20zero-shot%20accuracy%20on%20unseen%20forecasting%20tasks%2C%20positioning%0Apretrained%20models%20as%20a%20viable%20tool%20to%20greatly%20simplify%20forecasting%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChronos%253A%2520Learning%2520the%2520Language%2520of%2520Time%2520Series%26entry.906535625%3DAbdul%2520Fatir%2520Ansari%2520and%2520Lorenzo%2520Stella%2520and%2520Caner%2520Turkmen%2520and%2520Xiyuan%2520Zhang%2520and%2520Pedro%2520Mercado%2520and%2520Huibin%2520Shen%2520and%2520Oleksandr%2520Shchur%2520and%2520Syama%2520Sundar%2520Rangapuram%2520and%2520Sebastian%2520Pineda%2520Arango%2520and%2520Shubham%2520Kapoor%2520and%2520Jasper%2520Zschiegner%2520and%2520Danielle%2520C.%2520Maddix%2520and%2520Hao%2520Wang%2520and%2520Michael%2520W.%2520Mahoney%2520and%2520Kari%2520Torkkola%2520and%2520Andrew%2520Gordon%2520Wilson%2520and%2520Michael%2520Bohlke-Schneider%2520and%2520Yuyang%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Chronos%252C%2520a%2520simple%2520yet%2520effective%2520framework%2520for%2520pretrained%250Aprobabilistic%2520time%2520series%2520models.%2520Chronos%2520tokenizes%2520time%2520series%2520values%2520using%250Ascaling%2520and%2520quantization%2520into%2520a%2520fixed%2520vocabulary%2520and%2520trains%2520existing%250Atransformer-based%2520language%2520model%2520architectures%2520on%2520these%2520tokenized%2520time%2520series%250Avia%2520the%2520cross-entropy%2520loss.%2520We%2520pretrained%2520Chronos%2520models%2520based%2520on%2520the%2520T5%2520family%250A%2528ranging%2520from%252020M%2520to%2520710M%2520parameters%2529%2520on%2520a%2520large%2520collection%2520of%2520publicly%250Aavailable%2520datasets%252C%2520complemented%2520by%2520a%2520synthetic%2520dataset%2520that%2520we%2520generated%2520via%250AGaussian%2520processes%2520to%2520improve%2520generalization.%2520In%2520a%2520comprehensive%2520benchmark%250Aconsisting%2520of%252042%2520datasets%252C%2520and%2520comprising%2520both%2520classical%2520local%2520models%2520and%2520deep%250Alearning%2520methods%252C%2520we%2520show%2520that%2520Chronos%2520models%253A%2520%2528a%2529%2520significantly%2520outperform%250Aother%2520methods%2520on%2520datasets%2520that%2520were%2520part%2520of%2520the%2520training%2520corpus%253B%2520and%2520%2528b%2529%2520have%250Acomparable%2520and%2520occasionally%2520superior%2520zero-shot%2520performance%2520on%2520new%2520datasets%252C%250Arelative%2520to%2520methods%2520that%2520were%2520trained%2520specifically%2520on%2520them.%2520Our%2520results%250Ademonstrate%2520that%2520Chronos%2520models%2520can%2520leverage%2520time%2520series%2520data%2520from%2520diverse%250Adomains%2520to%2520improve%2520zero-shot%2520accuracy%2520on%2520unseen%2520forecasting%2520tasks%252C%2520positioning%250Apretrained%2520models%2520as%2520a%2520viable%2520tool%2520to%2520greatly%2520simplify%2520forecasting%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chronos%3A%20Learning%20the%20Language%20of%20Time%20Series&entry.906535625=Abdul%20Fatir%20Ansari%20and%20Lorenzo%20Stella%20and%20Caner%20Turkmen%20and%20Xiyuan%20Zhang%20and%20Pedro%20Mercado%20and%20Huibin%20Shen%20and%20Oleksandr%20Shchur%20and%20Syama%20Sundar%20Rangapuram%20and%20Sebastian%20Pineda%20Arango%20and%20Shubham%20Kapoor%20and%20Jasper%20Zschiegner%20and%20Danielle%20C.%20Maddix%20and%20Hao%20Wang%20and%20Michael%20W.%20Mahoney%20and%20Kari%20Torkkola%20and%20Andrew%20Gordon%20Wilson%20and%20Michael%20Bohlke-Schneider%20and%20Yuyang%20Wang&entry.1292438233=%20%20We%20introduce%20Chronos%2C%20a%20simple%20yet%20effective%20framework%20for%20pretrained%0Aprobabilistic%20time%20series%20models.%20Chronos%20tokenizes%20time%20series%20values%20using%0Ascaling%20and%20quantization%20into%20a%20fixed%20vocabulary%20and%20trains%20existing%0Atransformer-based%20language%20model%20architectures%20on%20these%20tokenized%20time%20series%0Avia%20the%20cross-entropy%20loss.%20We%20pretrained%20Chronos%20models%20based%20on%20the%20T5%20family%0A%28ranging%20from%2020M%20to%20710M%20parameters%29%20on%20a%20large%20collection%20of%20publicly%0Aavailable%20datasets%2C%20complemented%20by%20a%20synthetic%20dataset%20that%20we%20generated%20via%0AGaussian%20processes%20to%20improve%20generalization.%20In%20a%20comprehensive%20benchmark%0Aconsisting%20of%2042%20datasets%2C%20and%20comprising%20both%20classical%20local%20models%20and%20deep%0Alearning%20methods%2C%20we%20show%20that%20Chronos%20models%3A%20%28a%29%20significantly%20outperform%0Aother%20methods%20on%20datasets%20that%20were%20part%20of%20the%20training%20corpus%3B%20and%20%28b%29%20have%0Acomparable%20and%20occasionally%20superior%20zero-shot%20performance%20on%20new%20datasets%2C%0Arelative%20to%20methods%20that%20were%20trained%20specifically%20on%20them.%20Our%20results%0Ademonstrate%20that%20Chronos%20models%20can%20leverage%20time%20series%20data%20from%20diverse%0Adomains%20to%20improve%20zero-shot%20accuracy%20on%20unseen%20forecasting%20tasks%2C%20positioning%0Apretrained%20models%20as%20a%20viable%20tool%20to%20greatly%20simplify%20forecasting%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07815v2&entry.124074799=Read"},
{"title": "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked", "author": "Mansi Phute and Alec Helbling and Matthew Hull and ShengYun Peng and Sebastian Szyller and Cory Cornelius and Duen Horng Chau", "abstract": "  Large language models (LLMs) are popular for high-quality text generation but\ncan produce harmful content, even when aligned with human values through\nreinforcement learning. Adversarial prompts can bypass their safety measures.\nWe propose LLM Self Defense, a simple approach to defend against these attacks\nby having an LLM screen the induced responses. Our method does not require any\nfine-tuning, input preprocessing, or iterative output generation. Instead, we\nincorporate the generated content into a pre-defined prompt and employ another\ninstance of an LLM to analyze the text and predict whether it is harmful. We\ntest LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent\nLLMs against various types of attacks, such as forcefully inducing affirmative\nresponses to prompts and prompt engineering attacks. Notably, LLM Self Defense\nsucceeds in reducing the attack success rate to virtually 0 using both GPT 3.5\nand Llama 2. The code is publicly available at\nhttps://github.com/poloclub/llm-self-defense\n", "link": "http://arxiv.org/abs/2308.07308v4", "date": "2024-05-02", "relevancy": 1.803, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4711}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4508}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Self%20Defense%3A%20By%20Self%20Examination%2C%20LLMs%20Know%20They%20Are%20Being%20Tricked&body=Title%3A%20LLM%20Self%20Defense%3A%20By%20Self%20Examination%2C%20LLMs%20Know%20They%20Are%20Being%20Tricked%0AAuthor%3A%20Mansi%20Phute%20and%20Alec%20Helbling%20and%20Matthew%20Hull%20and%20ShengYun%20Peng%20and%20Sebastian%20Szyller%20and%20Cory%20Cornelius%20and%20Duen%20Horng%20Chau%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20popular%20for%20high-quality%20text%20generation%20but%0Acan%20produce%20harmful%20content%2C%20even%20when%20aligned%20with%20human%20values%20through%0Areinforcement%20learning.%20Adversarial%20prompts%20can%20bypass%20their%20safety%20measures.%0AWe%20propose%20LLM%20Self%20Defense%2C%20a%20simple%20approach%20to%20defend%20against%20these%20attacks%0Aby%20having%20an%20LLM%20screen%20the%20induced%20responses.%20Our%20method%20does%20not%20require%20any%0Afine-tuning%2C%20input%20preprocessing%2C%20or%20iterative%20output%20generation.%20Instead%2C%20we%0Aincorporate%20the%20generated%20content%20into%20a%20pre-defined%20prompt%20and%20employ%20another%0Ainstance%20of%20an%20LLM%20to%20analyze%20the%20text%20and%20predict%20whether%20it%20is%20harmful.%20We%0Atest%20LLM%20Self%20Defense%20on%20GPT%203.5%20and%20Llama%202%2C%20two%20of%20the%20current%20most%20prominent%0ALLMs%20against%20various%20types%20of%20attacks%2C%20such%20as%20forcefully%20inducing%20affirmative%0Aresponses%20to%20prompts%20and%20prompt%20engineering%20attacks.%20Notably%2C%20LLM%20Self%20Defense%0Asucceeds%20in%20reducing%20the%20attack%20success%20rate%20to%20virtually%200%20using%20both%20GPT%203.5%0Aand%20Llama%202.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/poloclub/llm-self-defense%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.07308v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Self%2520Defense%253A%2520By%2520Self%2520Examination%252C%2520LLMs%2520Know%2520They%2520Are%2520Being%2520Tricked%26entry.906535625%3DMansi%2520Phute%2520and%2520Alec%2520Helbling%2520and%2520Matthew%2520Hull%2520and%2520ShengYun%2520Peng%2520and%2520Sebastian%2520Szyller%2520and%2520Cory%2520Cornelius%2520and%2520Duen%2520Horng%2520Chau%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520popular%2520for%2520high-quality%2520text%2520generation%2520but%250Acan%2520produce%2520harmful%2520content%252C%2520even%2520when%2520aligned%2520with%2520human%2520values%2520through%250Areinforcement%2520learning.%2520Adversarial%2520prompts%2520can%2520bypass%2520their%2520safety%2520measures.%250AWe%2520propose%2520LLM%2520Self%2520Defense%252C%2520a%2520simple%2520approach%2520to%2520defend%2520against%2520these%2520attacks%250Aby%2520having%2520an%2520LLM%2520screen%2520the%2520induced%2520responses.%2520Our%2520method%2520does%2520not%2520require%2520any%250Afine-tuning%252C%2520input%2520preprocessing%252C%2520or%2520iterative%2520output%2520generation.%2520Instead%252C%2520we%250Aincorporate%2520the%2520generated%2520content%2520into%2520a%2520pre-defined%2520prompt%2520and%2520employ%2520another%250Ainstance%2520of%2520an%2520LLM%2520to%2520analyze%2520the%2520text%2520and%2520predict%2520whether%2520it%2520is%2520harmful.%2520We%250Atest%2520LLM%2520Self%2520Defense%2520on%2520GPT%25203.5%2520and%2520Llama%25202%252C%2520two%2520of%2520the%2520current%2520most%2520prominent%250ALLMs%2520against%2520various%2520types%2520of%2520attacks%252C%2520such%2520as%2520forcefully%2520inducing%2520affirmative%250Aresponses%2520to%2520prompts%2520and%2520prompt%2520engineering%2520attacks.%2520Notably%252C%2520LLM%2520Self%2520Defense%250Asucceeds%2520in%2520reducing%2520the%2520attack%2520success%2520rate%2520to%2520virtually%25200%2520using%2520both%2520GPT%25203.5%250Aand%2520Llama%25202.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/poloclub/llm-self-defense%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.07308v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Self%20Defense%3A%20By%20Self%20Examination%2C%20LLMs%20Know%20They%20Are%20Being%20Tricked&entry.906535625=Mansi%20Phute%20and%20Alec%20Helbling%20and%20Matthew%20Hull%20and%20ShengYun%20Peng%20and%20Sebastian%20Szyller%20and%20Cory%20Cornelius%20and%20Duen%20Horng%20Chau&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20popular%20for%20high-quality%20text%20generation%20but%0Acan%20produce%20harmful%20content%2C%20even%20when%20aligned%20with%20human%20values%20through%0Areinforcement%20learning.%20Adversarial%20prompts%20can%20bypass%20their%20safety%20measures.%0AWe%20propose%20LLM%20Self%20Defense%2C%20a%20simple%20approach%20to%20defend%20against%20these%20attacks%0Aby%20having%20an%20LLM%20screen%20the%20induced%20responses.%20Our%20method%20does%20not%20require%20any%0Afine-tuning%2C%20input%20preprocessing%2C%20or%20iterative%20output%20generation.%20Instead%2C%20we%0Aincorporate%20the%20generated%20content%20into%20a%20pre-defined%20prompt%20and%20employ%20another%0Ainstance%20of%20an%20LLM%20to%20analyze%20the%20text%20and%20predict%20whether%20it%20is%20harmful.%20We%0Atest%20LLM%20Self%20Defense%20on%20GPT%203.5%20and%20Llama%202%2C%20two%20of%20the%20current%20most%20prominent%0ALLMs%20against%20various%20types%20of%20attacks%2C%20such%20as%20forcefully%20inducing%20affirmative%0Aresponses%20to%20prompts%20and%20prompt%20engineering%20attacks.%20Notably%2C%20LLM%20Self%20Defense%0Asucceeds%20in%20reducing%20the%20attack%20success%20rate%20to%20virtually%200%20using%20both%20GPT%203.5%0Aand%20Llama%202.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/poloclub/llm-self-defense%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.07308v4&entry.124074799=Read"},
{"title": "UQA: Corpus for Urdu Question Answering", "author": "Samee Arif and Sualeha Farid and Awais Athar and Agha Ali Raza", "abstract": "  This paper introduces UQA, a novel dataset for question answering and text\ncomprehension in Urdu, a low-resource language with over 70 million native\nspeakers. UQA is generated by translating the Stanford Question Answering\nDataset (SQuAD2.0), a large-scale English QA dataset, using a technique called\nEATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans in\nthe translated context paragraphs. The paper describes the process of selecting\nand evaluating the best translation model among two candidates: Google\nTranslator and Seamless M4T. The paper also benchmarks several state-of-the-art\nmultilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, and\nreports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and\n74.56 EM. UQA is a valuable resource for developing and testing multilingual\nNLP systems for Urdu and for enhancing the cross-lingual transferability of\nexisting models. Further, the paper demonstrates the effectiveness of EATS for\ncreating high-quality datasets for other languages and domains. The UQA dataset\nand the code are publicly available at www.github.com/sameearif/UQA.\n", "link": "http://arxiv.org/abs/2405.01458v1", "date": "2024-05-02", "relevancy": 1.8005, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5042}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4486}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UQA%3A%20Corpus%20for%20Urdu%20Question%20Answering&body=Title%3A%20UQA%3A%20Corpus%20for%20Urdu%20Question%20Answering%0AAuthor%3A%20Samee%20Arif%20and%20Sualeha%20Farid%20and%20Awais%20Athar%20and%20Agha%20Ali%20Raza%0AAbstract%3A%20%20%20This%20paper%20introduces%20UQA%2C%20a%20novel%20dataset%20for%20question%20answering%20and%20text%0Acomprehension%20in%20Urdu%2C%20a%20low-resource%20language%20with%20over%2070%20million%20native%0Aspeakers.%20UQA%20is%20generated%20by%20translating%20the%20Stanford%20Question%20Answering%0ADataset%20%28SQuAD2.0%29%2C%20a%20large-scale%20English%20QA%20dataset%2C%20using%20a%20technique%20called%0AEATS%20%28Enclose%20to%20Anchor%2C%20Translate%2C%20Seek%29%2C%20which%20preserves%20the%20answer%20spans%20in%0Athe%20translated%20context%20paragraphs.%20The%20paper%20describes%20the%20process%20of%20selecting%0Aand%20evaluating%20the%20best%20translation%20model%20among%20two%20candidates%3A%20Google%0ATranslator%20and%20Seamless%20M4T.%20The%20paper%20also%20benchmarks%20several%20state-of-the-art%0Amultilingual%20QA%20models%20on%20UQA%2C%20including%20mBERT%2C%20XLM-RoBERTa%2C%20and%20mT5%2C%20and%0Areports%20promising%20results.%20For%20XLM-RoBERTa-XL%2C%20we%20have%20an%20F1%20score%20of%2085.99%20and%0A74.56%20EM.%20UQA%20is%20a%20valuable%20resource%20for%20developing%20and%20testing%20multilingual%0ANLP%20systems%20for%20Urdu%20and%20for%20enhancing%20the%20cross-lingual%20transferability%20of%0Aexisting%20models.%20Further%2C%20the%20paper%20demonstrates%20the%20effectiveness%20of%20EATS%20for%0Acreating%20high-quality%20datasets%20for%20other%20languages%20and%20domains.%20The%20UQA%20dataset%0Aand%20the%20code%20are%20publicly%20available%20at%20www.github.com/sameearif/UQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUQA%253A%2520Corpus%2520for%2520Urdu%2520Question%2520Answering%26entry.906535625%3DSamee%2520Arif%2520and%2520Sualeha%2520Farid%2520and%2520Awais%2520Athar%2520and%2520Agha%2520Ali%2520Raza%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520UQA%252C%2520a%2520novel%2520dataset%2520for%2520question%2520answering%2520and%2520text%250Acomprehension%2520in%2520Urdu%252C%2520a%2520low-resource%2520language%2520with%2520over%252070%2520million%2520native%250Aspeakers.%2520UQA%2520is%2520generated%2520by%2520translating%2520the%2520Stanford%2520Question%2520Answering%250ADataset%2520%2528SQuAD2.0%2529%252C%2520a%2520large-scale%2520English%2520QA%2520dataset%252C%2520using%2520a%2520technique%2520called%250AEATS%2520%2528Enclose%2520to%2520Anchor%252C%2520Translate%252C%2520Seek%2529%252C%2520which%2520preserves%2520the%2520answer%2520spans%2520in%250Athe%2520translated%2520context%2520paragraphs.%2520The%2520paper%2520describes%2520the%2520process%2520of%2520selecting%250Aand%2520evaluating%2520the%2520best%2520translation%2520model%2520among%2520two%2520candidates%253A%2520Google%250ATranslator%2520and%2520Seamless%2520M4T.%2520The%2520paper%2520also%2520benchmarks%2520several%2520state-of-the-art%250Amultilingual%2520QA%2520models%2520on%2520UQA%252C%2520including%2520mBERT%252C%2520XLM-RoBERTa%252C%2520and%2520mT5%252C%2520and%250Areports%2520promising%2520results.%2520For%2520XLM-RoBERTa-XL%252C%2520we%2520have%2520an%2520F1%2520score%2520of%252085.99%2520and%250A74.56%2520EM.%2520UQA%2520is%2520a%2520valuable%2520resource%2520for%2520developing%2520and%2520testing%2520multilingual%250ANLP%2520systems%2520for%2520Urdu%2520and%2520for%2520enhancing%2520the%2520cross-lingual%2520transferability%2520of%250Aexisting%2520models.%2520Further%252C%2520the%2520paper%2520demonstrates%2520the%2520effectiveness%2520of%2520EATS%2520for%250Acreating%2520high-quality%2520datasets%2520for%2520other%2520languages%2520and%2520domains.%2520The%2520UQA%2520dataset%250Aand%2520the%2520code%2520are%2520publicly%2520available%2520at%2520www.github.com/sameearif/UQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UQA%3A%20Corpus%20for%20Urdu%20Question%20Answering&entry.906535625=Samee%20Arif%20and%20Sualeha%20Farid%20and%20Awais%20Athar%20and%20Agha%20Ali%20Raza&entry.1292438233=%20%20This%20paper%20introduces%20UQA%2C%20a%20novel%20dataset%20for%20question%20answering%20and%20text%0Acomprehension%20in%20Urdu%2C%20a%20low-resource%20language%20with%20over%2070%20million%20native%0Aspeakers.%20UQA%20is%20generated%20by%20translating%20the%20Stanford%20Question%20Answering%0ADataset%20%28SQuAD2.0%29%2C%20a%20large-scale%20English%20QA%20dataset%2C%20using%20a%20technique%20called%0AEATS%20%28Enclose%20to%20Anchor%2C%20Translate%2C%20Seek%29%2C%20which%20preserves%20the%20answer%20spans%20in%0Athe%20translated%20context%20paragraphs.%20The%20paper%20describes%20the%20process%20of%20selecting%0Aand%20evaluating%20the%20best%20translation%20model%20among%20two%20candidates%3A%20Google%0ATranslator%20and%20Seamless%20M4T.%20The%20paper%20also%20benchmarks%20several%20state-of-the-art%0Amultilingual%20QA%20models%20on%20UQA%2C%20including%20mBERT%2C%20XLM-RoBERTa%2C%20and%20mT5%2C%20and%0Areports%20promising%20results.%20For%20XLM-RoBERTa-XL%2C%20we%20have%20an%20F1%20score%20of%2085.99%20and%0A74.56%20EM.%20UQA%20is%20a%20valuable%20resource%20for%20developing%20and%20testing%20multilingual%0ANLP%20systems%20for%20Urdu%20and%20for%20enhancing%20the%20cross-lingual%20transferability%20of%0Aexisting%20models.%20Further%2C%20the%20paper%20demonstrates%20the%20effectiveness%20of%20EATS%20for%0Acreating%20high-quality%20datasets%20for%20other%20languages%20and%20domains.%20The%20UQA%20dataset%0Aand%20the%20code%20are%20publicly%20available%20at%20www.github.com/sameearif/UQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01458v1&entry.124074799=Read"},
{"title": "FLAME: Factuality-Aware Alignment for Large Language Models", "author": "Sheng-Chieh Lin and Luyu Gao and Barlas Oguz and Wenhan Xiong and Jimmy Lin and Wen-tau Yih and Xilun Chen", "abstract": "  Alignment is a standard procedure to fine-tune pre-trained large language\nmodels (LLMs) to follow natural language instructions and serve as helpful AI\nassistants. We have observed, however, that the conventional alignment process\nfails to enhance the factual accuracy of LLMs, and often leads to the\ngeneration of more false facts (i.e. hallucination). In this paper, we study\nhow to make the LLM alignment process more factual, by first identifying\nfactors that lead to hallucination in both alignment steps:\\ supervised\nfine-tuning (SFT) and reinforcement learning (RL). In particular, we find that\ntraining the LLM on new knowledge or unfamiliar texts can encourage\nhallucination. This makes SFT less factual as it trains on human labeled data\nthat may be novel to the LLM. Furthermore, reward functions used in standard RL\ncan also encourage hallucination, because it guides the LLM to provide more\nhelpful responses on a diverse set of instructions, often preferring longer and\nmore detailed responses. Based on these observations, we propose\nfactuality-aware alignment, comprised of factuality-aware SFT and\nfactuality-aware RL through direct preference optimization. Experiments show\nthat our proposed factuality-aware alignment guides LLMs to output more factual\nresponses while maintaining instruction-following capability.\n", "link": "http://arxiv.org/abs/2405.01525v1", "date": "2024-05-02", "relevancy": 1.7985, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.47}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4487}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLAME%3A%20Factuality-Aware%20Alignment%20for%20Large%20Language%20Models&body=Title%3A%20FLAME%3A%20Factuality-Aware%20Alignment%20for%20Large%20Language%20Models%0AAuthor%3A%20Sheng-Chieh%20Lin%20and%20Luyu%20Gao%20and%20Barlas%20Oguz%20and%20Wenhan%20Xiong%20and%20Jimmy%20Lin%20and%20Wen-tau%20Yih%20and%20Xilun%20Chen%0AAbstract%3A%20%20%20Alignment%20is%20a%20standard%20procedure%20to%20fine-tune%20pre-trained%20large%20language%0Amodels%20%28LLMs%29%20to%20follow%20natural%20language%20instructions%20and%20serve%20as%20helpful%20AI%0Aassistants.%20We%20have%20observed%2C%20however%2C%20that%20the%20conventional%20alignment%20process%0Afails%20to%20enhance%20the%20factual%20accuracy%20of%20LLMs%2C%20and%20often%20leads%20to%20the%0Ageneration%20of%20more%20false%20facts%20%28i.e.%20hallucination%29.%20In%20this%20paper%2C%20we%20study%0Ahow%20to%20make%20the%20LLM%20alignment%20process%20more%20factual%2C%20by%20first%20identifying%0Afactors%20that%20lead%20to%20hallucination%20in%20both%20alignment%20steps%3A%5C%20supervised%0Afine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20In%20particular%2C%20we%20find%20that%0Atraining%20the%20LLM%20on%20new%20knowledge%20or%20unfamiliar%20texts%20can%20encourage%0Ahallucination.%20This%20makes%20SFT%20less%20factual%20as%20it%20trains%20on%20human%20labeled%20data%0Athat%20may%20be%20novel%20to%20the%20LLM.%20Furthermore%2C%20reward%20functions%20used%20in%20standard%20RL%0Acan%20also%20encourage%20hallucination%2C%20because%20it%20guides%20the%20LLM%20to%20provide%20more%0Ahelpful%20responses%20on%20a%20diverse%20set%20of%20instructions%2C%20often%20preferring%20longer%20and%0Amore%20detailed%20responses.%20Based%20on%20these%20observations%2C%20we%20propose%0Afactuality-aware%20alignment%2C%20comprised%20of%20factuality-aware%20SFT%20and%0Afactuality-aware%20RL%20through%20direct%20preference%20optimization.%20Experiments%20show%0Athat%20our%20proposed%20factuality-aware%20alignment%20guides%20LLMs%20to%20output%20more%20factual%0Aresponses%20while%20maintaining%20instruction-following%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLAME%253A%2520Factuality-Aware%2520Alignment%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DSheng-Chieh%2520Lin%2520and%2520Luyu%2520Gao%2520and%2520Barlas%2520Oguz%2520and%2520Wenhan%2520Xiong%2520and%2520Jimmy%2520Lin%2520and%2520Wen-tau%2520Yih%2520and%2520Xilun%2520Chen%26entry.1292438233%3D%2520%2520Alignment%2520is%2520a%2520standard%2520procedure%2520to%2520fine-tune%2520pre-trained%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520follow%2520natural%2520language%2520instructions%2520and%2520serve%2520as%2520helpful%2520AI%250Aassistants.%2520We%2520have%2520observed%252C%2520however%252C%2520that%2520the%2520conventional%2520alignment%2520process%250Afails%2520to%2520enhance%2520the%2520factual%2520accuracy%2520of%2520LLMs%252C%2520and%2520often%2520leads%2520to%2520the%250Ageneration%2520of%2520more%2520false%2520facts%2520%2528i.e.%2520hallucination%2529.%2520In%2520this%2520paper%252C%2520we%2520study%250Ahow%2520to%2520make%2520the%2520LLM%2520alignment%2520process%2520more%2520factual%252C%2520by%2520first%2520identifying%250Afactors%2520that%2520lead%2520to%2520hallucination%2520in%2520both%2520alignment%2520steps%253A%255C%2520supervised%250Afine-tuning%2520%2528SFT%2529%2520and%2520reinforcement%2520learning%2520%2528RL%2529.%2520In%2520particular%252C%2520we%2520find%2520that%250Atraining%2520the%2520LLM%2520on%2520new%2520knowledge%2520or%2520unfamiliar%2520texts%2520can%2520encourage%250Ahallucination.%2520This%2520makes%2520SFT%2520less%2520factual%2520as%2520it%2520trains%2520on%2520human%2520labeled%2520data%250Athat%2520may%2520be%2520novel%2520to%2520the%2520LLM.%2520Furthermore%252C%2520reward%2520functions%2520used%2520in%2520standard%2520RL%250Acan%2520also%2520encourage%2520hallucination%252C%2520because%2520it%2520guides%2520the%2520LLM%2520to%2520provide%2520more%250Ahelpful%2520responses%2520on%2520a%2520diverse%2520set%2520of%2520instructions%252C%2520often%2520preferring%2520longer%2520and%250Amore%2520detailed%2520responses.%2520Based%2520on%2520these%2520observations%252C%2520we%2520propose%250Afactuality-aware%2520alignment%252C%2520comprised%2520of%2520factuality-aware%2520SFT%2520and%250Afactuality-aware%2520RL%2520through%2520direct%2520preference%2520optimization.%2520Experiments%2520show%250Athat%2520our%2520proposed%2520factuality-aware%2520alignment%2520guides%2520LLMs%2520to%2520output%2520more%2520factual%250Aresponses%2520while%2520maintaining%2520instruction-following%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLAME%3A%20Factuality-Aware%20Alignment%20for%20Large%20Language%20Models&entry.906535625=Sheng-Chieh%20Lin%20and%20Luyu%20Gao%20and%20Barlas%20Oguz%20and%20Wenhan%20Xiong%20and%20Jimmy%20Lin%20and%20Wen-tau%20Yih%20and%20Xilun%20Chen&entry.1292438233=%20%20Alignment%20is%20a%20standard%20procedure%20to%20fine-tune%20pre-trained%20large%20language%0Amodels%20%28LLMs%29%20to%20follow%20natural%20language%20instructions%20and%20serve%20as%20helpful%20AI%0Aassistants.%20We%20have%20observed%2C%20however%2C%20that%20the%20conventional%20alignment%20process%0Afails%20to%20enhance%20the%20factual%20accuracy%20of%20LLMs%2C%20and%20often%20leads%20to%20the%0Ageneration%20of%20more%20false%20facts%20%28i.e.%20hallucination%29.%20In%20this%20paper%2C%20we%20study%0Ahow%20to%20make%20the%20LLM%20alignment%20process%20more%20factual%2C%20by%20first%20identifying%0Afactors%20that%20lead%20to%20hallucination%20in%20both%20alignment%20steps%3A%5C%20supervised%0Afine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20In%20particular%2C%20we%20find%20that%0Atraining%20the%20LLM%20on%20new%20knowledge%20or%20unfamiliar%20texts%20can%20encourage%0Ahallucination.%20This%20makes%20SFT%20less%20factual%20as%20it%20trains%20on%20human%20labeled%20data%0Athat%20may%20be%20novel%20to%20the%20LLM.%20Furthermore%2C%20reward%20functions%20used%20in%20standard%20RL%0Acan%20also%20encourage%20hallucination%2C%20because%20it%20guides%20the%20LLM%20to%20provide%20more%0Ahelpful%20responses%20on%20a%20diverse%20set%20of%20instructions%2C%20often%20preferring%20longer%20and%0Amore%20detailed%20responses.%20Based%20on%20these%20observations%2C%20we%20propose%0Afactuality-aware%20alignment%2C%20comprised%20of%20factuality-aware%20SFT%20and%0Afactuality-aware%20RL%20through%20direct%20preference%20optimization.%20Experiments%20show%0Athat%20our%20proposed%20factuality-aware%20alignment%20guides%20LLMs%20to%20output%20more%20factual%0Aresponses%20while%20maintaining%20instruction-following%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01525v1&entry.124074799=Read"},
{"title": "LOG-LIO2: A LiDAR-Inertial Odometry with Efficient Uncertainty Analysis", "author": "Kai Huang and Junqiao Zhao and Jiaye Lin and Zhongyang Zhu and Shuangfu Song and Chen Ye and Tiantian Feng", "abstract": "  Uncertainty in LiDAR measurements, stemming from factors such as range\nsensing, is crucial for LIO (LiDAR-Inertial Odometry) systems as it affects the\naccurate weighting in the loss function. While recent LIO systems address\nuncertainty related to range sensing, the impact of incident angle on\nuncertainty is often overlooked by the community. Moreover, the existing\nuncertainty propagation methods suffer from computational inefficiency. This\npaper proposes a comprehensive point uncertainty model that accounts for both\nthe uncertainties from LiDAR measurements and surface characteristics, along\nwith an efficient local uncertainty analytical method for LiDAR-based state\nestimation problem. We employ a projection operator that separates the\nuncertainty into the ray direction and its orthogonal plane. Then, we derive\nincremental Jacobian matrices of eigenvalues and eigenvectors w.r.t. points,\nwhich enables a fast approximation of uncertainty propagation. This approach\neliminates the requirement for redundant traversal of points, significantly\nreducing the time complexity of uncertainty propagation from $\\mathcal{O} (n)$\nto $\\mathcal{O} (1)$ when a new point is added. Simulations and experiments on\npublic datasets are conducted to validate the accuracy and efficiency of our\nformulations. The proposed methods have been integrated into a LIO system,\nwhich is available at https://github.com/tiev-tongji/LOG-LIO2.\n", "link": "http://arxiv.org/abs/2405.01316v1", "date": "2024-05-02", "relevancy": 1.7921, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6237}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOG-LIO2%3A%20A%20LiDAR-Inertial%20Odometry%20with%20Efficient%20Uncertainty%20Analysis&body=Title%3A%20LOG-LIO2%3A%20A%20LiDAR-Inertial%20Odometry%20with%20Efficient%20Uncertainty%20Analysis%0AAuthor%3A%20Kai%20Huang%20and%20Junqiao%20Zhao%20and%20Jiaye%20Lin%20and%20Zhongyang%20Zhu%20and%20Shuangfu%20Song%20and%20Chen%20Ye%20and%20Tiantian%20Feng%0AAbstract%3A%20%20%20Uncertainty%20in%20LiDAR%20measurements%2C%20stemming%20from%20factors%20such%20as%20range%0Asensing%2C%20is%20crucial%20for%20LIO%20%28LiDAR-Inertial%20Odometry%29%20systems%20as%20it%20affects%20the%0Aaccurate%20weighting%20in%20the%20loss%20function.%20While%20recent%20LIO%20systems%20address%0Auncertainty%20related%20to%20range%20sensing%2C%20the%20impact%20of%20incident%20angle%20on%0Auncertainty%20is%20often%20overlooked%20by%20the%20community.%20Moreover%2C%20the%20existing%0Auncertainty%20propagation%20methods%20suffer%20from%20computational%20inefficiency.%20This%0Apaper%20proposes%20a%20comprehensive%20point%20uncertainty%20model%20that%20accounts%20for%20both%0Athe%20uncertainties%20from%20LiDAR%20measurements%20and%20surface%20characteristics%2C%20along%0Awith%20an%20efficient%20local%20uncertainty%20analytical%20method%20for%20LiDAR-based%20state%0Aestimation%20problem.%20We%20employ%20a%20projection%20operator%20that%20separates%20the%0Auncertainty%20into%20the%20ray%20direction%20and%20its%20orthogonal%20plane.%20Then%2C%20we%20derive%0Aincremental%20Jacobian%20matrices%20of%20eigenvalues%20and%20eigenvectors%20w.r.t.%20points%2C%0Awhich%20enables%20a%20fast%20approximation%20of%20uncertainty%20propagation.%20This%20approach%0Aeliminates%20the%20requirement%20for%20redundant%20traversal%20of%20points%2C%20significantly%0Areducing%20the%20time%20complexity%20of%20uncertainty%20propagation%20from%20%24%5Cmathcal%7BO%7D%20%28n%29%24%0Ato%20%24%5Cmathcal%7BO%7D%20%281%29%24%20when%20a%20new%20point%20is%20added.%20Simulations%20and%20experiments%20on%0Apublic%20datasets%20are%20conducted%20to%20validate%20the%20accuracy%20and%20efficiency%20of%20our%0Aformulations.%20The%20proposed%20methods%20have%20been%20integrated%20into%20a%20LIO%20system%2C%0Awhich%20is%20available%20at%20https%3A//github.com/tiev-tongji/LOG-LIO2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOG-LIO2%253A%2520A%2520LiDAR-Inertial%2520Odometry%2520with%2520Efficient%2520Uncertainty%2520Analysis%26entry.906535625%3DKai%2520Huang%2520and%2520Junqiao%2520Zhao%2520and%2520Jiaye%2520Lin%2520and%2520Zhongyang%2520Zhu%2520and%2520Shuangfu%2520Song%2520and%2520Chen%2520Ye%2520and%2520Tiantian%2520Feng%26entry.1292438233%3D%2520%2520Uncertainty%2520in%2520LiDAR%2520measurements%252C%2520stemming%2520from%2520factors%2520such%2520as%2520range%250Asensing%252C%2520is%2520crucial%2520for%2520LIO%2520%2528LiDAR-Inertial%2520Odometry%2529%2520systems%2520as%2520it%2520affects%2520the%250Aaccurate%2520weighting%2520in%2520the%2520loss%2520function.%2520While%2520recent%2520LIO%2520systems%2520address%250Auncertainty%2520related%2520to%2520range%2520sensing%252C%2520the%2520impact%2520of%2520incident%2520angle%2520on%250Auncertainty%2520is%2520often%2520overlooked%2520by%2520the%2520community.%2520Moreover%252C%2520the%2520existing%250Auncertainty%2520propagation%2520methods%2520suffer%2520from%2520computational%2520inefficiency.%2520This%250Apaper%2520proposes%2520a%2520comprehensive%2520point%2520uncertainty%2520model%2520that%2520accounts%2520for%2520both%250Athe%2520uncertainties%2520from%2520LiDAR%2520measurements%2520and%2520surface%2520characteristics%252C%2520along%250Awith%2520an%2520efficient%2520local%2520uncertainty%2520analytical%2520method%2520for%2520LiDAR-based%2520state%250Aestimation%2520problem.%2520We%2520employ%2520a%2520projection%2520operator%2520that%2520separates%2520the%250Auncertainty%2520into%2520the%2520ray%2520direction%2520and%2520its%2520orthogonal%2520plane.%2520Then%252C%2520we%2520derive%250Aincremental%2520Jacobian%2520matrices%2520of%2520eigenvalues%2520and%2520eigenvectors%2520w.r.t.%2520points%252C%250Awhich%2520enables%2520a%2520fast%2520approximation%2520of%2520uncertainty%2520propagation.%2520This%2520approach%250Aeliminates%2520the%2520requirement%2520for%2520redundant%2520traversal%2520of%2520points%252C%2520significantly%250Areducing%2520the%2520time%2520complexity%2520of%2520uncertainty%2520propagation%2520from%2520%2524%255Cmathcal%257BO%257D%2520%2528n%2529%2524%250Ato%2520%2524%255Cmathcal%257BO%257D%2520%25281%2529%2524%2520when%2520a%2520new%2520point%2520is%2520added.%2520Simulations%2520and%2520experiments%2520on%250Apublic%2520datasets%2520are%2520conducted%2520to%2520validate%2520the%2520accuracy%2520and%2520efficiency%2520of%2520our%250Aformulations.%2520The%2520proposed%2520methods%2520have%2520been%2520integrated%2520into%2520a%2520LIO%2520system%252C%250Awhich%2520is%2520available%2520at%2520https%253A//github.com/tiev-tongji/LOG-LIO2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOG-LIO2%3A%20A%20LiDAR-Inertial%20Odometry%20with%20Efficient%20Uncertainty%20Analysis&entry.906535625=Kai%20Huang%20and%20Junqiao%20Zhao%20and%20Jiaye%20Lin%20and%20Zhongyang%20Zhu%20and%20Shuangfu%20Song%20and%20Chen%20Ye%20and%20Tiantian%20Feng&entry.1292438233=%20%20Uncertainty%20in%20LiDAR%20measurements%2C%20stemming%20from%20factors%20such%20as%20range%0Asensing%2C%20is%20crucial%20for%20LIO%20%28LiDAR-Inertial%20Odometry%29%20systems%20as%20it%20affects%20the%0Aaccurate%20weighting%20in%20the%20loss%20function.%20While%20recent%20LIO%20systems%20address%0Auncertainty%20related%20to%20range%20sensing%2C%20the%20impact%20of%20incident%20angle%20on%0Auncertainty%20is%20often%20overlooked%20by%20the%20community.%20Moreover%2C%20the%20existing%0Auncertainty%20propagation%20methods%20suffer%20from%20computational%20inefficiency.%20This%0Apaper%20proposes%20a%20comprehensive%20point%20uncertainty%20model%20that%20accounts%20for%20both%0Athe%20uncertainties%20from%20LiDAR%20measurements%20and%20surface%20characteristics%2C%20along%0Awith%20an%20efficient%20local%20uncertainty%20analytical%20method%20for%20LiDAR-based%20state%0Aestimation%20problem.%20We%20employ%20a%20projection%20operator%20that%20separates%20the%0Auncertainty%20into%20the%20ray%20direction%20and%20its%20orthogonal%20plane.%20Then%2C%20we%20derive%0Aincremental%20Jacobian%20matrices%20of%20eigenvalues%20and%20eigenvectors%20w.r.t.%20points%2C%0Awhich%20enables%20a%20fast%20approximation%20of%20uncertainty%20propagation.%20This%20approach%0Aeliminates%20the%20requirement%20for%20redundant%20traversal%20of%20points%2C%20significantly%0Areducing%20the%20time%20complexity%20of%20uncertainty%20propagation%20from%20%24%5Cmathcal%7BO%7D%20%28n%29%24%0Ato%20%24%5Cmathcal%7BO%7D%20%281%29%24%20when%20a%20new%20point%20is%20added.%20Simulations%20and%20experiments%20on%0Apublic%20datasets%20are%20conducted%20to%20validate%20the%20accuracy%20and%20efficiency%20of%20our%0Aformulations.%20The%20proposed%20methods%20have%20been%20integrated%20into%20a%20LIO%20system%2C%0Awhich%20is%20available%20at%20https%3A//github.com/tiev-tongji/LOG-LIO2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01316v1&entry.124074799=Read"},
{"title": "Tabular and Deep Reinforcement Learning for Gittins Index", "author": "Harshit Dhankar and Kshitij Mishra and Tejas Bodas", "abstract": "  In the realm of multi-arm bandit problems, the Gittins index policy is known\nto be optimal in maximizing the expected total discounted reward obtained from\npulling the Markovian arms. In most realistic scenarios however, the Markovian\nstate transition probabilities are unknown and therefore the Gittins indices\ncannot be computed. One can then resort to reinforcement learning (RL)\nalgorithms that explore the state space to learn these indices while exploiting\nto maximize the reward collected. In this work, we propose tabular (QGI) and\nDeep RL (DGN) algorithms for learning the Gittins index that are based on the\nretirement formulation for the multi-arm bandit problem. When compared with\nexisting RL algorithms that learn the Gittins index, our algorithms have a\nlower run time, require less storage space (small Q-table size in QGI and\nsmaller replay buffer in DGN), and illustrate better empirical convergence to\nthe Gittins index. This makes our algorithm well suited for problems with large\nstate spaces and is a viable alternative to existing methods. As a key\napplication, we demonstrate the use of our algorithms in minimizing the mean\nflowtime in a job scheduling problem when jobs are available in batches and\nhave an unknown service time distribution. \\\n", "link": "http://arxiv.org/abs/2405.01157v1", "date": "2024-05-02", "relevancy": 1.7901, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4608}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4419}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tabular%20and%20Deep%20Reinforcement%20Learning%20for%20Gittins%20Index&body=Title%3A%20Tabular%20and%20Deep%20Reinforcement%20Learning%20for%20Gittins%20Index%0AAuthor%3A%20Harshit%20Dhankar%20and%20Kshitij%20Mishra%20and%20Tejas%20Bodas%0AAbstract%3A%20%20%20In%20the%20realm%20of%20multi-arm%20bandit%20problems%2C%20the%20Gittins%20index%20policy%20is%20known%0Ato%20be%20optimal%20in%20maximizing%20the%20expected%20total%20discounted%20reward%20obtained%20from%0Apulling%20the%20Markovian%20arms.%20In%20most%20realistic%20scenarios%20however%2C%20the%20Markovian%0Astate%20transition%20probabilities%20are%20unknown%20and%20therefore%20the%20Gittins%20indices%0Acannot%20be%20computed.%20One%20can%20then%20resort%20to%20reinforcement%20learning%20%28RL%29%0Aalgorithms%20that%20explore%20the%20state%20space%20to%20learn%20these%20indices%20while%20exploiting%0Ato%20maximize%20the%20reward%20collected.%20In%20this%20work%2C%20we%20propose%20tabular%20%28QGI%29%20and%0ADeep%20RL%20%28DGN%29%20algorithms%20for%20learning%20the%20Gittins%20index%20that%20are%20based%20on%20the%0Aretirement%20formulation%20for%20the%20multi-arm%20bandit%20problem.%20When%20compared%20with%0Aexisting%20RL%20algorithms%20that%20learn%20the%20Gittins%20index%2C%20our%20algorithms%20have%20a%0Alower%20run%20time%2C%20require%20less%20storage%20space%20%28small%20Q-table%20size%20in%20QGI%20and%0Asmaller%20replay%20buffer%20in%20DGN%29%2C%20and%20illustrate%20better%20empirical%20convergence%20to%0Athe%20Gittins%20index.%20This%20makes%20our%20algorithm%20well%20suited%20for%20problems%20with%20large%0Astate%20spaces%20and%20is%20a%20viable%20alternative%20to%20existing%20methods.%20As%20a%20key%0Aapplication%2C%20we%20demonstrate%20the%20use%20of%20our%20algorithms%20in%20minimizing%20the%20mean%0Aflowtime%20in%20a%20job%20scheduling%20problem%20when%20jobs%20are%20available%20in%20batches%20and%0Ahave%20an%20unknown%20service%20time%20distribution.%20%5C%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabular%2520and%2520Deep%2520Reinforcement%2520Learning%2520for%2520Gittins%2520Index%26entry.906535625%3DHarshit%2520Dhankar%2520and%2520Kshitij%2520Mishra%2520and%2520Tejas%2520Bodas%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520multi-arm%2520bandit%2520problems%252C%2520the%2520Gittins%2520index%2520policy%2520is%2520known%250Ato%2520be%2520optimal%2520in%2520maximizing%2520the%2520expected%2520total%2520discounted%2520reward%2520obtained%2520from%250Apulling%2520the%2520Markovian%2520arms.%2520In%2520most%2520realistic%2520scenarios%2520however%252C%2520the%2520Markovian%250Astate%2520transition%2520probabilities%2520are%2520unknown%2520and%2520therefore%2520the%2520Gittins%2520indices%250Acannot%2520be%2520computed.%2520One%2520can%2520then%2520resort%2520to%2520reinforcement%2520learning%2520%2528RL%2529%250Aalgorithms%2520that%2520explore%2520the%2520state%2520space%2520to%2520learn%2520these%2520indices%2520while%2520exploiting%250Ato%2520maximize%2520the%2520reward%2520collected.%2520In%2520this%2520work%252C%2520we%2520propose%2520tabular%2520%2528QGI%2529%2520and%250ADeep%2520RL%2520%2528DGN%2529%2520algorithms%2520for%2520learning%2520the%2520Gittins%2520index%2520that%2520are%2520based%2520on%2520the%250Aretirement%2520formulation%2520for%2520the%2520multi-arm%2520bandit%2520problem.%2520When%2520compared%2520with%250Aexisting%2520RL%2520algorithms%2520that%2520learn%2520the%2520Gittins%2520index%252C%2520our%2520algorithms%2520have%2520a%250Alower%2520run%2520time%252C%2520require%2520less%2520storage%2520space%2520%2528small%2520Q-table%2520size%2520in%2520QGI%2520and%250Asmaller%2520replay%2520buffer%2520in%2520DGN%2529%252C%2520and%2520illustrate%2520better%2520empirical%2520convergence%2520to%250Athe%2520Gittins%2520index.%2520This%2520makes%2520our%2520algorithm%2520well%2520suited%2520for%2520problems%2520with%2520large%250Astate%2520spaces%2520and%2520is%2520a%2520viable%2520alternative%2520to%2520existing%2520methods.%2520As%2520a%2520key%250Aapplication%252C%2520we%2520demonstrate%2520the%2520use%2520of%2520our%2520algorithms%2520in%2520minimizing%2520the%2520mean%250Aflowtime%2520in%2520a%2520job%2520scheduling%2520problem%2520when%2520jobs%2520are%2520available%2520in%2520batches%2520and%250Ahave%2520an%2520unknown%2520service%2520time%2520distribution.%2520%255C%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tabular%20and%20Deep%20Reinforcement%20Learning%20for%20Gittins%20Index&entry.906535625=Harshit%20Dhankar%20and%20Kshitij%20Mishra%20and%20Tejas%20Bodas&entry.1292438233=%20%20In%20the%20realm%20of%20multi-arm%20bandit%20problems%2C%20the%20Gittins%20index%20policy%20is%20known%0Ato%20be%20optimal%20in%20maximizing%20the%20expected%20total%20discounted%20reward%20obtained%20from%0Apulling%20the%20Markovian%20arms.%20In%20most%20realistic%20scenarios%20however%2C%20the%20Markovian%0Astate%20transition%20probabilities%20are%20unknown%20and%20therefore%20the%20Gittins%20indices%0Acannot%20be%20computed.%20One%20can%20then%20resort%20to%20reinforcement%20learning%20%28RL%29%0Aalgorithms%20that%20explore%20the%20state%20space%20to%20learn%20these%20indices%20while%20exploiting%0Ato%20maximize%20the%20reward%20collected.%20In%20this%20work%2C%20we%20propose%20tabular%20%28QGI%29%20and%0ADeep%20RL%20%28DGN%29%20algorithms%20for%20learning%20the%20Gittins%20index%20that%20are%20based%20on%20the%0Aretirement%20formulation%20for%20the%20multi-arm%20bandit%20problem.%20When%20compared%20with%0Aexisting%20RL%20algorithms%20that%20learn%20the%20Gittins%20index%2C%20our%20algorithms%20have%20a%0Alower%20run%20time%2C%20require%20less%20storage%20space%20%28small%20Q-table%20size%20in%20QGI%20and%0Asmaller%20replay%20buffer%20in%20DGN%29%2C%20and%20illustrate%20better%20empirical%20convergence%20to%0Athe%20Gittins%20index.%20This%20makes%20our%20algorithm%20well%20suited%20for%20problems%20with%20large%0Astate%20spaces%20and%20is%20a%20viable%20alternative%20to%20existing%20methods.%20As%20a%20key%0Aapplication%2C%20we%20demonstrate%20the%20use%20of%20our%20algorithms%20in%20minimizing%20the%20mean%0Aflowtime%20in%20a%20job%20scheduling%20problem%20when%20jobs%20are%20available%20in%20batches%20and%0Ahave%20an%20unknown%20service%20time%20distribution.%20%5C%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01157v1&entry.124074799=Read"},
{"title": "KAN: Kolmogorov-Arnold Networks", "author": "Ziming Liu and Yixuan Wang and Sachin Vaidya and Fabian Ruehle and James Halverson and Marin Solja\u010di\u0107 and Thomas Y. Hou and Max Tegmark", "abstract": "  Inspired by the Kolmogorov-Arnold representation theorem, we propose\nKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer\nPerceptrons (MLPs). While MLPs have fixed activation functions on nodes\n(\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\nKANs have no linear weights at all -- every weight parameter is replaced by a\nunivariate function parametrized as a spline. We show that this seemingly\nsimple change makes KANs outperform MLPs in terms of accuracy and\ninterpretability. For accuracy, much smaller KANs can achieve comparable or\nbetter accuracy than much larger MLPs in data fitting and PDE solving.\nTheoretically and empirically, KANs possess faster neural scaling laws than\nMLPs. For interpretability, KANs can be intuitively visualized and can easily\ninteract with human users. Through two examples in mathematics and physics,\nKANs are shown to be useful collaborators helping scientists (re)discover\nmathematical and physical laws. In summary, KANs are promising alternatives for\nMLPs, opening opportunities for further improving today's deep learning models\nwhich rely heavily on MLPs.\n", "link": "http://arxiv.org/abs/2404.19756v2", "date": "2024-05-02", "relevancy": 1.7839, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4828}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4628}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KAN%3A%20Kolmogorov-Arnold%20Networks&body=Title%3A%20KAN%3A%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Ziming%20Liu%20and%20Yixuan%20Wang%20and%20Sachin%20Vaidya%20and%20Fabian%20Ruehle%20and%20James%20Halverson%20and%20Marin%20Solja%C4%8Di%C4%87%20and%20Thomas%20Y.%20Hou%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20Inspired%20by%20the%20Kolmogorov-Arnold%20representation%20theorem%2C%20we%20propose%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20as%20promising%20alternatives%20to%20Multi-Layer%0APerceptrons%20%28MLPs%29.%20While%20MLPs%20have%20fixed%20activation%20functions%20on%20nodes%0A%28%22neurons%22%29%2C%20KANs%20have%20learnable%20activation%20functions%20on%20edges%20%28%22weights%22%29.%0AKANs%20have%20no%20linear%20weights%20at%20all%20--%20every%20weight%20parameter%20is%20replaced%20by%20a%0Aunivariate%20function%20parametrized%20as%20a%20spline.%20We%20show%20that%20this%20seemingly%0Asimple%20change%20makes%20KANs%20outperform%20MLPs%20in%20terms%20of%20accuracy%20and%0Ainterpretability.%20For%20accuracy%2C%20much%20smaller%20KANs%20can%20achieve%20comparable%20or%0Abetter%20accuracy%20than%20much%20larger%20MLPs%20in%20data%20fitting%20and%20PDE%20solving.%0ATheoretically%20and%20empirically%2C%20KANs%20possess%20faster%20neural%20scaling%20laws%20than%0AMLPs.%20For%20interpretability%2C%20KANs%20can%20be%20intuitively%20visualized%20and%20can%20easily%0Ainteract%20with%20human%20users.%20Through%20two%20examples%20in%20mathematics%20and%20physics%2C%0AKANs%20are%20shown%20to%20be%20useful%20collaborators%20helping%20scientists%20%28re%29discover%0Amathematical%20and%20physical%20laws.%20In%20summary%2C%20KANs%20are%20promising%20alternatives%20for%0AMLPs%2C%20opening%20opportunities%20for%20further%20improving%20today%27s%20deep%20learning%20models%0Awhich%20rely%20heavily%20on%20MLPs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19756v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKAN%253A%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DZiming%2520Liu%2520and%2520Yixuan%2520Wang%2520and%2520Sachin%2520Vaidya%2520and%2520Fabian%2520Ruehle%2520and%2520James%2520Halverson%2520and%2520Marin%2520Solja%25C4%258Di%25C4%2587%2520and%2520Thomas%2520Y.%2520Hou%2520and%2520Max%2520Tegmark%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520Kolmogorov-Arnold%2520representation%2520theorem%252C%2520we%2520propose%250AKolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520as%2520promising%2520alternatives%2520to%2520Multi-Layer%250APerceptrons%2520%2528MLPs%2529.%2520While%2520MLPs%2520have%2520fixed%2520activation%2520functions%2520on%2520nodes%250A%2528%2522neurons%2522%2529%252C%2520KANs%2520have%2520learnable%2520activation%2520functions%2520on%2520edges%2520%2528%2522weights%2522%2529.%250AKANs%2520have%2520no%2520linear%2520weights%2520at%2520all%2520--%2520every%2520weight%2520parameter%2520is%2520replaced%2520by%2520a%250Aunivariate%2520function%2520parametrized%2520as%2520a%2520spline.%2520We%2520show%2520that%2520this%2520seemingly%250Asimple%2520change%2520makes%2520KANs%2520outperform%2520MLPs%2520in%2520terms%2520of%2520accuracy%2520and%250Ainterpretability.%2520For%2520accuracy%252C%2520much%2520smaller%2520KANs%2520can%2520achieve%2520comparable%2520or%250Abetter%2520accuracy%2520than%2520much%2520larger%2520MLPs%2520in%2520data%2520fitting%2520and%2520PDE%2520solving.%250ATheoretically%2520and%2520empirically%252C%2520KANs%2520possess%2520faster%2520neural%2520scaling%2520laws%2520than%250AMLPs.%2520For%2520interpretability%252C%2520KANs%2520can%2520be%2520intuitively%2520visualized%2520and%2520can%2520easily%250Ainteract%2520with%2520human%2520users.%2520Through%2520two%2520examples%2520in%2520mathematics%2520and%2520physics%252C%250AKANs%2520are%2520shown%2520to%2520be%2520useful%2520collaborators%2520helping%2520scientists%2520%2528re%2529discover%250Amathematical%2520and%2520physical%2520laws.%2520In%2520summary%252C%2520KANs%2520are%2520promising%2520alternatives%2520for%250AMLPs%252C%2520opening%2520opportunities%2520for%2520further%2520improving%2520today%2527s%2520deep%2520learning%2520models%250Awhich%2520rely%2520heavily%2520on%2520MLPs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19756v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KAN%3A%20Kolmogorov-Arnold%20Networks&entry.906535625=Ziming%20Liu%20and%20Yixuan%20Wang%20and%20Sachin%20Vaidya%20and%20Fabian%20Ruehle%20and%20James%20Halverson%20and%20Marin%20Solja%C4%8Di%C4%87%20and%20Thomas%20Y.%20Hou%20and%20Max%20Tegmark&entry.1292438233=%20%20Inspired%20by%20the%20Kolmogorov-Arnold%20representation%20theorem%2C%20we%20propose%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20as%20promising%20alternatives%20to%20Multi-Layer%0APerceptrons%20%28MLPs%29.%20While%20MLPs%20have%20fixed%20activation%20functions%20on%20nodes%0A%28%22neurons%22%29%2C%20KANs%20have%20learnable%20activation%20functions%20on%20edges%20%28%22weights%22%29.%0AKANs%20have%20no%20linear%20weights%20at%20all%20--%20every%20weight%20parameter%20is%20replaced%20by%20a%0Aunivariate%20function%20parametrized%20as%20a%20spline.%20We%20show%20that%20this%20seemingly%0Asimple%20change%20makes%20KANs%20outperform%20MLPs%20in%20terms%20of%20accuracy%20and%0Ainterpretability.%20For%20accuracy%2C%20much%20smaller%20KANs%20can%20achieve%20comparable%20or%0Abetter%20accuracy%20than%20much%20larger%20MLPs%20in%20data%20fitting%20and%20PDE%20solving.%0ATheoretically%20and%20empirically%2C%20KANs%20possess%20faster%20neural%20scaling%20laws%20than%0AMLPs.%20For%20interpretability%2C%20KANs%20can%20be%20intuitively%20visualized%20and%20can%20easily%0Ainteract%20with%20human%20users.%20Through%20two%20examples%20in%20mathematics%20and%20physics%2C%0AKANs%20are%20shown%20to%20be%20useful%20collaborators%20helping%20scientists%20%28re%29discover%0Amathematical%20and%20physical%20laws.%20In%20summary%2C%20KANs%20are%20promising%20alternatives%20for%0AMLPs%2C%20opening%20opportunities%20for%20further%20improving%20today%27s%20deep%20learning%20models%0Awhich%20rely%20heavily%20on%20MLPs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19756v2&entry.124074799=Read"},
{"title": "Mathematics of Differential Machine Learning in Derivative Pricing and\n  Hedging", "author": "Pedro Duarte Gomes", "abstract": "  This article introduces the groundbreaking concept of the financial\ndifferential machine learning algorithm through a rigorous mathematical\nframework. Diverging from existing literature on financial machine learning,\nthe work highlights the profound implications of theoretical assumptions within\nfinancial models on the construction of machine learning algorithms.\n  This endeavour is particularly timely as the finance landscape witnesses a\nsurge in interest towards data-driven models for the valuation and hedging of\nderivative products. Notably, the predictive capabilities of neural networks\nhave garnered substantial attention in both academic research and practical\nfinancial applications.\n  The approach offers a unified theoretical foundation that facilitates\ncomprehensive comparisons, both at a theoretical level and in experimental\noutcomes. Importantly, this theoretical grounding lends substantial weight to\nthe experimental results, affirming the differential machine learning method's\noptimality within the prevailing context.\n  By anchoring the insights in rigorous mathematics, the article bridges the\ngap between abstract financial concepts and practical algorithmic\nimplementations.\n", "link": "http://arxiv.org/abs/2405.01233v1", "date": "2024-05-02", "relevancy": 1.3274, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4496}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4412}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mathematics%20of%20Differential%20Machine%20Learning%20in%20Derivative%20Pricing%20and%0A%20%20Hedging&body=Title%3A%20Mathematics%20of%20Differential%20Machine%20Learning%20in%20Derivative%20Pricing%20and%0A%20%20Hedging%0AAuthor%3A%20Pedro%20Duarte%20Gomes%0AAbstract%3A%20%20%20This%20article%20introduces%20the%20groundbreaking%20concept%20of%20the%20financial%0Adifferential%20machine%20learning%20algorithm%20through%20a%20rigorous%20mathematical%0Aframework.%20Diverging%20from%20existing%20literature%20on%20financial%20machine%20learning%2C%0Athe%20work%20highlights%20the%20profound%20implications%20of%20theoretical%20assumptions%20within%0Afinancial%20models%20on%20the%20construction%20of%20machine%20learning%20algorithms.%0A%20%20This%20endeavour%20is%20particularly%20timely%20as%20the%20finance%20landscape%20witnesses%20a%0Asurge%20in%20interest%20towards%20data-driven%20models%20for%20the%20valuation%20and%20hedging%20of%0Aderivative%20products.%20Notably%2C%20the%20predictive%20capabilities%20of%20neural%20networks%0Ahave%20garnered%20substantial%20attention%20in%20both%20academic%20research%20and%20practical%0Afinancial%20applications.%0A%20%20The%20approach%20offers%20a%20unified%20theoretical%20foundation%20that%20facilitates%0Acomprehensive%20comparisons%2C%20both%20at%20a%20theoretical%20level%20and%20in%20experimental%0Aoutcomes.%20Importantly%2C%20this%20theoretical%20grounding%20lends%20substantial%20weight%20to%0Athe%20experimental%20results%2C%20affirming%20the%20differential%20machine%20learning%20method%27s%0Aoptimality%20within%20the%20prevailing%20context.%0A%20%20By%20anchoring%20the%20insights%20in%20rigorous%20mathematics%2C%20the%20article%20bridges%20the%0Agap%20between%20abstract%20financial%20concepts%20and%20practical%20algorithmic%0Aimplementations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathematics%2520of%2520Differential%2520Machine%2520Learning%2520in%2520Derivative%2520Pricing%2520and%250A%2520%2520Hedging%26entry.906535625%3DPedro%2520Duarte%2520Gomes%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520the%2520groundbreaking%2520concept%2520of%2520the%2520financial%250Adifferential%2520machine%2520learning%2520algorithm%2520through%2520a%2520rigorous%2520mathematical%250Aframework.%2520Diverging%2520from%2520existing%2520literature%2520on%2520financial%2520machine%2520learning%252C%250Athe%2520work%2520highlights%2520the%2520profound%2520implications%2520of%2520theoretical%2520assumptions%2520within%250Afinancial%2520models%2520on%2520the%2520construction%2520of%2520machine%2520learning%2520algorithms.%250A%2520%2520This%2520endeavour%2520is%2520particularly%2520timely%2520as%2520the%2520finance%2520landscape%2520witnesses%2520a%250Asurge%2520in%2520interest%2520towards%2520data-driven%2520models%2520for%2520the%2520valuation%2520and%2520hedging%2520of%250Aderivative%2520products.%2520Notably%252C%2520the%2520predictive%2520capabilities%2520of%2520neural%2520networks%250Ahave%2520garnered%2520substantial%2520attention%2520in%2520both%2520academic%2520research%2520and%2520practical%250Afinancial%2520applications.%250A%2520%2520The%2520approach%2520offers%2520a%2520unified%2520theoretical%2520foundation%2520that%2520facilitates%250Acomprehensive%2520comparisons%252C%2520both%2520at%2520a%2520theoretical%2520level%2520and%2520in%2520experimental%250Aoutcomes.%2520Importantly%252C%2520this%2520theoretical%2520grounding%2520lends%2520substantial%2520weight%2520to%250Athe%2520experimental%2520results%252C%2520affirming%2520the%2520differential%2520machine%2520learning%2520method%2527s%250Aoptimality%2520within%2520the%2520prevailing%2520context.%250A%2520%2520By%2520anchoring%2520the%2520insights%2520in%2520rigorous%2520mathematics%252C%2520the%2520article%2520bridges%2520the%250Agap%2520between%2520abstract%2520financial%2520concepts%2520and%2520practical%2520algorithmic%250Aimplementations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mathematics%20of%20Differential%20Machine%20Learning%20in%20Derivative%20Pricing%20and%0A%20%20Hedging&entry.906535625=Pedro%20Duarte%20Gomes&entry.1292438233=%20%20This%20article%20introduces%20the%20groundbreaking%20concept%20of%20the%20financial%0Adifferential%20machine%20learning%20algorithm%20through%20a%20rigorous%20mathematical%0Aframework.%20Diverging%20from%20existing%20literature%20on%20financial%20machine%20learning%2C%0Athe%20work%20highlights%20the%20profound%20implications%20of%20theoretical%20assumptions%20within%0Afinancial%20models%20on%20the%20construction%20of%20machine%20learning%20algorithms.%0A%20%20This%20endeavour%20is%20particularly%20timely%20as%20the%20finance%20landscape%20witnesses%20a%0Asurge%20in%20interest%20towards%20data-driven%20models%20for%20the%20valuation%20and%20hedging%20of%0Aderivative%20products.%20Notably%2C%20the%20predictive%20capabilities%20of%20neural%20networks%0Ahave%20garnered%20substantial%20attention%20in%20both%20academic%20research%20and%20practical%0Afinancial%20applications.%0A%20%20The%20approach%20offers%20a%20unified%20theoretical%20foundation%20that%20facilitates%0Acomprehensive%20comparisons%2C%20both%20at%20a%20theoretical%20level%20and%20in%20experimental%0Aoutcomes.%20Importantly%2C%20this%20theoretical%20grounding%20lends%20substantial%20weight%20to%0Athe%20experimental%20results%2C%20affirming%20the%20differential%20machine%20learning%20method%27s%0Aoptimality%20within%20the%20prevailing%20context.%0A%20%20By%20anchoring%20the%20insights%20in%20rigorous%20mathematics%2C%20the%20article%20bridges%20the%0Agap%20between%20abstract%20financial%20concepts%20and%20practical%20algorithmic%0Aimplementations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01233v1&entry.124074799=Read"},
{"title": "Towards Interpretable Reinforcement Learning with Constrained\n  Normalizing Flow Policies", "author": "Finn Rietz and Erik Schaffernicht and Stefan Heinrich and Johannes A. Stork", "abstract": "  Reinforcement learning policies are typically represented by black-box neural\nnetworks, which are non-interpretable and not well-suited for safety-critical\ndomains. To address both of these issues, we propose constrained normalizing\nflow policies as interpretable and safe-by-construction policy models. We\nachieve safety for reinforcement learning problems with instantaneous safety\nconstraints, for which we can exploit domain knowledge by analytically\nconstructing a normalizing flow that ensures constraint satisfaction. The\nnormalizing flow corresponds to an interpretable sequence of transformations on\naction samples, each ensuring alignment with respect to a particular\nconstraint. Our experiments reveal benefits beyond interpretability in an\neasier learning objective and maintained constraint satisfaction throughout the\nentire learning process. Our approach leverages constraints over reward\nengineering while offering enhanced interpretability, safety, and direct means\nof providing domain knowledge to the agent without relying on complex reward\nfunctions.\n", "link": "http://arxiv.org/abs/2405.01198v1", "date": "2024-05-02", "relevancy": 1.4889, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5383}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4887}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Interpretable%20Reinforcement%20Learning%20with%20Constrained%0A%20%20Normalizing%20Flow%20Policies&body=Title%3A%20Towards%20Interpretable%20Reinforcement%20Learning%20with%20Constrained%0A%20%20Normalizing%20Flow%20Policies%0AAuthor%3A%20Finn%20Rietz%20and%20Erik%20Schaffernicht%20and%20Stefan%20Heinrich%20and%20Johannes%20A.%20Stork%0AAbstract%3A%20%20%20Reinforcement%20learning%20policies%20are%20typically%20represented%20by%20black-box%20neural%0Anetworks%2C%20which%20are%20non-interpretable%20and%20not%20well-suited%20for%20safety-critical%0Adomains.%20To%20address%20both%20of%20these%20issues%2C%20we%20propose%20constrained%20normalizing%0Aflow%20policies%20as%20interpretable%20and%20safe-by-construction%20policy%20models.%20We%0Aachieve%20safety%20for%20reinforcement%20learning%20problems%20with%20instantaneous%20safety%0Aconstraints%2C%20for%20which%20we%20can%20exploit%20domain%20knowledge%20by%20analytically%0Aconstructing%20a%20normalizing%20flow%20that%20ensures%20constraint%20satisfaction.%20The%0Anormalizing%20flow%20corresponds%20to%20an%20interpretable%20sequence%20of%20transformations%20on%0Aaction%20samples%2C%20each%20ensuring%20alignment%20with%20respect%20to%20a%20particular%0Aconstraint.%20Our%20experiments%20reveal%20benefits%20beyond%20interpretability%20in%20an%0Aeasier%20learning%20objective%20and%20maintained%20constraint%20satisfaction%20throughout%20the%0Aentire%20learning%20process.%20Our%20approach%20leverages%20constraints%20over%20reward%0Aengineering%20while%20offering%20enhanced%20interpretability%2C%20safety%2C%20and%20direct%20means%0Aof%20providing%20domain%20knowledge%20to%20the%20agent%20without%20relying%20on%20complex%20reward%0Afunctions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Interpretable%2520Reinforcement%2520Learning%2520with%2520Constrained%250A%2520%2520Normalizing%2520Flow%2520Policies%26entry.906535625%3DFinn%2520Rietz%2520and%2520Erik%2520Schaffernicht%2520and%2520Stefan%2520Heinrich%2520and%2520Johannes%2520A.%2520Stork%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520policies%2520are%2520typically%2520represented%2520by%2520black-box%2520neural%250Anetworks%252C%2520which%2520are%2520non-interpretable%2520and%2520not%2520well-suited%2520for%2520safety-critical%250Adomains.%2520To%2520address%2520both%2520of%2520these%2520issues%252C%2520we%2520propose%2520constrained%2520normalizing%250Aflow%2520policies%2520as%2520interpretable%2520and%2520safe-by-construction%2520policy%2520models.%2520We%250Aachieve%2520safety%2520for%2520reinforcement%2520learning%2520problems%2520with%2520instantaneous%2520safety%250Aconstraints%252C%2520for%2520which%2520we%2520can%2520exploit%2520domain%2520knowledge%2520by%2520analytically%250Aconstructing%2520a%2520normalizing%2520flow%2520that%2520ensures%2520constraint%2520satisfaction.%2520The%250Anormalizing%2520flow%2520corresponds%2520to%2520an%2520interpretable%2520sequence%2520of%2520transformations%2520on%250Aaction%2520samples%252C%2520each%2520ensuring%2520alignment%2520with%2520respect%2520to%2520a%2520particular%250Aconstraint.%2520Our%2520experiments%2520reveal%2520benefits%2520beyond%2520interpretability%2520in%2520an%250Aeasier%2520learning%2520objective%2520and%2520maintained%2520constraint%2520satisfaction%2520throughout%2520the%250Aentire%2520learning%2520process.%2520Our%2520approach%2520leverages%2520constraints%2520over%2520reward%250Aengineering%2520while%2520offering%2520enhanced%2520interpretability%252C%2520safety%252C%2520and%2520direct%2520means%250Aof%2520providing%2520domain%2520knowledge%2520to%2520the%2520agent%2520without%2520relying%2520on%2520complex%2520reward%250Afunctions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Interpretable%20Reinforcement%20Learning%20with%20Constrained%0A%20%20Normalizing%20Flow%20Policies&entry.906535625=Finn%20Rietz%20and%20Erik%20Schaffernicht%20and%20Stefan%20Heinrich%20and%20Johannes%20A.%20Stork&entry.1292438233=%20%20Reinforcement%20learning%20policies%20are%20typically%20represented%20by%20black-box%20neural%0Anetworks%2C%20which%20are%20non-interpretable%20and%20not%20well-suited%20for%20safety-critical%0Adomains.%20To%20address%20both%20of%20these%20issues%2C%20we%20propose%20constrained%20normalizing%0Aflow%20policies%20as%20interpretable%20and%20safe-by-construction%20policy%20models.%20We%0Aachieve%20safety%20for%20reinforcement%20learning%20problems%20with%20instantaneous%20safety%0Aconstraints%2C%20for%20which%20we%20can%20exploit%20domain%20knowledge%20by%20analytically%0Aconstructing%20a%20normalizing%20flow%20that%20ensures%20constraint%20satisfaction.%20The%0Anormalizing%20flow%20corresponds%20to%20an%20interpretable%20sequence%20of%20transformations%20on%0Aaction%20samples%2C%20each%20ensuring%20alignment%20with%20respect%20to%20a%20particular%0Aconstraint.%20Our%20experiments%20reveal%20benefits%20beyond%20interpretability%20in%20an%0Aeasier%20learning%20objective%20and%20maintained%20constraint%20satisfaction%20throughout%20the%0Aentire%20learning%20process.%20Our%20approach%20leverages%20constraints%20over%20reward%0Aengineering%20while%20offering%20enhanced%20interpretability%2C%20safety%2C%20and%20direct%20means%0Aof%20providing%20domain%20knowledge%20to%20the%20agent%20without%20relying%20on%20complex%20reward%0Afunctions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01198v1&entry.124074799=Read"},
{"title": "Improving Intervention Efficacy via Concept Realignment in Concept\n  Bottleneck Models", "author": "Nishad Singhi and Jae Myung Kim and Karsten Roth and Zeynep Akata", "abstract": "  Concept Bottleneck Models (CBMs) ground image classification on\nhuman-understandable concepts to allow for interpretable model decisions.\nCrucially, the CBM design inherently allows for human interventions, in which\nexpert users are given the ability to modify potentially misaligned concept\nchoices to influence the decision behavior of the model in an interpretable\nfashion. However, existing approaches often require numerous human\ninterventions per image to achieve strong performances, posing practical\nchallenges in scenarios where obtaining human feedback is expensive. In this\npaper, we find that this is noticeably driven by an independent treatment of\nconcepts during intervention, wherein a change of one concept does not\ninfluence the use of other ones in the model's final decision. To address this\nissue, we introduce a trainable concept intervention realignment module, which\nleverages concept relations to realign concept assignments post-intervention.\nAcross standard, real-world benchmarks, we find that concept realignment can\nsignificantly improve intervention efficacy; significantly reducing the number\nof interventions needed to reach a target classification performance or concept\nprediction accuracy. In addition, it easily integrates into existing\nconcept-based architectures without requiring changes to the models themselves.\nThis reduced cost of human-model collaboration is crucial to enhancing the\nfeasibility of CBMs in resource-constrained environments.\n", "link": "http://arxiv.org/abs/2405.01531v1", "date": "2024-05-02", "relevancy": 1.4317, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4781}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4764}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Intervention%20Efficacy%20via%20Concept%20Realignment%20in%20Concept%0A%20%20Bottleneck%20Models&body=Title%3A%20Improving%20Intervention%20Efficacy%20via%20Concept%20Realignment%20in%20Concept%0A%20%20Bottleneck%20Models%0AAuthor%3A%20Nishad%20Singhi%20and%20Jae%20Myung%20Kim%20and%20Karsten%20Roth%20and%20Zeynep%20Akata%0AAbstract%3A%20%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20ground%20image%20classification%20on%0Ahuman-understandable%20concepts%20to%20allow%20for%20interpretable%20model%20decisions.%0ACrucially%2C%20the%20CBM%20design%20inherently%20allows%20for%20human%20interventions%2C%20in%20which%0Aexpert%20users%20are%20given%20the%20ability%20to%20modify%20potentially%20misaligned%20concept%0Achoices%20to%20influence%20the%20decision%20behavior%20of%20the%20model%20in%20an%20interpretable%0Afashion.%20However%2C%20existing%20approaches%20often%20require%20numerous%20human%0Ainterventions%20per%20image%20to%20achieve%20strong%20performances%2C%20posing%20practical%0Achallenges%20in%20scenarios%20where%20obtaining%20human%20feedback%20is%20expensive.%20In%20this%0Apaper%2C%20we%20find%20that%20this%20is%20noticeably%20driven%20by%20an%20independent%20treatment%20of%0Aconcepts%20during%20intervention%2C%20wherein%20a%20change%20of%20one%20concept%20does%20not%0Ainfluence%20the%20use%20of%20other%20ones%20in%20the%20model%27s%20final%20decision.%20To%20address%20this%0Aissue%2C%20we%20introduce%20a%20trainable%20concept%20intervention%20realignment%20module%2C%20which%0Aleverages%20concept%20relations%20to%20realign%20concept%20assignments%20post-intervention.%0AAcross%20standard%2C%20real-world%20benchmarks%2C%20we%20find%20that%20concept%20realignment%20can%0Asignificantly%20improve%20intervention%20efficacy%3B%20significantly%20reducing%20the%20number%0Aof%20interventions%20needed%20to%20reach%20a%20target%20classification%20performance%20or%20concept%0Aprediction%20accuracy.%20In%20addition%2C%20it%20easily%20integrates%20into%20existing%0Aconcept-based%20architectures%20without%20requiring%20changes%20to%20the%20models%20themselves.%0AThis%20reduced%20cost%20of%20human-model%20collaboration%20is%20crucial%20to%20enhancing%20the%0Afeasibility%20of%20CBMs%20in%20resource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Intervention%2520Efficacy%2520via%2520Concept%2520Realignment%2520in%2520Concept%250A%2520%2520Bottleneck%2520Models%26entry.906535625%3DNishad%2520Singhi%2520and%2520Jae%2520Myung%2520Kim%2520and%2520Karsten%2520Roth%2520and%2520Zeynep%2520Akata%26entry.1292438233%3D%2520%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520ground%2520image%2520classification%2520on%250Ahuman-understandable%2520concepts%2520to%2520allow%2520for%2520interpretable%2520model%2520decisions.%250ACrucially%252C%2520the%2520CBM%2520design%2520inherently%2520allows%2520for%2520human%2520interventions%252C%2520in%2520which%250Aexpert%2520users%2520are%2520given%2520the%2520ability%2520to%2520modify%2520potentially%2520misaligned%2520concept%250Achoices%2520to%2520influence%2520the%2520decision%2520behavior%2520of%2520the%2520model%2520in%2520an%2520interpretable%250Afashion.%2520However%252C%2520existing%2520approaches%2520often%2520require%2520numerous%2520human%250Ainterventions%2520per%2520image%2520to%2520achieve%2520strong%2520performances%252C%2520posing%2520practical%250Achallenges%2520in%2520scenarios%2520where%2520obtaining%2520human%2520feedback%2520is%2520expensive.%2520In%2520this%250Apaper%252C%2520we%2520find%2520that%2520this%2520is%2520noticeably%2520driven%2520by%2520an%2520independent%2520treatment%2520of%250Aconcepts%2520during%2520intervention%252C%2520wherein%2520a%2520change%2520of%2520one%2520concept%2520does%2520not%250Ainfluence%2520the%2520use%2520of%2520other%2520ones%2520in%2520the%2520model%2527s%2520final%2520decision.%2520To%2520address%2520this%250Aissue%252C%2520we%2520introduce%2520a%2520trainable%2520concept%2520intervention%2520realignment%2520module%252C%2520which%250Aleverages%2520concept%2520relations%2520to%2520realign%2520concept%2520assignments%2520post-intervention.%250AAcross%2520standard%252C%2520real-world%2520benchmarks%252C%2520we%2520find%2520that%2520concept%2520realignment%2520can%250Asignificantly%2520improve%2520intervention%2520efficacy%253B%2520significantly%2520reducing%2520the%2520number%250Aof%2520interventions%2520needed%2520to%2520reach%2520a%2520target%2520classification%2520performance%2520or%2520concept%250Aprediction%2520accuracy.%2520In%2520addition%252C%2520it%2520easily%2520integrates%2520into%2520existing%250Aconcept-based%2520architectures%2520without%2520requiring%2520changes%2520to%2520the%2520models%2520themselves.%250AThis%2520reduced%2520cost%2520of%2520human-model%2520collaboration%2520is%2520crucial%2520to%2520enhancing%2520the%250Afeasibility%2520of%2520CBMs%2520in%2520resource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Intervention%20Efficacy%20via%20Concept%20Realignment%20in%20Concept%0A%20%20Bottleneck%20Models&entry.906535625=Nishad%20Singhi%20and%20Jae%20Myung%20Kim%20and%20Karsten%20Roth%20and%20Zeynep%20Akata&entry.1292438233=%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20ground%20image%20classification%20on%0Ahuman-understandable%20concepts%20to%20allow%20for%20interpretable%20model%20decisions.%0ACrucially%2C%20the%20CBM%20design%20inherently%20allows%20for%20human%20interventions%2C%20in%20which%0Aexpert%20users%20are%20given%20the%20ability%20to%20modify%20potentially%20misaligned%20concept%0Achoices%20to%20influence%20the%20decision%20behavior%20of%20the%20model%20in%20an%20interpretable%0Afashion.%20However%2C%20existing%20approaches%20often%20require%20numerous%20human%0Ainterventions%20per%20image%20to%20achieve%20strong%20performances%2C%20posing%20practical%0Achallenges%20in%20scenarios%20where%20obtaining%20human%20feedback%20is%20expensive.%20In%20this%0Apaper%2C%20we%20find%20that%20this%20is%20noticeably%20driven%20by%20an%20independent%20treatment%20of%0Aconcepts%20during%20intervention%2C%20wherein%20a%20change%20of%20one%20concept%20does%20not%0Ainfluence%20the%20use%20of%20other%20ones%20in%20the%20model%27s%20final%20decision.%20To%20address%20this%0Aissue%2C%20we%20introduce%20a%20trainable%20concept%20intervention%20realignment%20module%2C%20which%0Aleverages%20concept%20relations%20to%20realign%20concept%20assignments%20post-intervention.%0AAcross%20standard%2C%20real-world%20benchmarks%2C%20we%20find%20that%20concept%20realignment%20can%0Asignificantly%20improve%20intervention%20efficacy%3B%20significantly%20reducing%20the%20number%0Aof%20interventions%20needed%20to%20reach%20a%20target%20classification%20performance%20or%20concept%0Aprediction%20accuracy.%20In%20addition%2C%20it%20easily%20integrates%20into%20existing%0Aconcept-based%20architectures%20without%20requiring%20changes%20to%20the%20models%20themselves.%0AThis%20reduced%20cost%20of%20human-model%20collaboration%20is%20crucial%20to%20enhancing%20the%0Afeasibility%20of%20CBMs%20in%20resource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01531v1&entry.124074799=Read"},
{"title": "Transformer-Aided Semantic Communications", "author": "Matin Mortaheb and Erciyes Karakaya and Mohammad A. Amir Khojastepour and Sennur Ulukus", "abstract": "  The transformer structure employed in large language models (LLMs), as a\nspecialized category of deep neural networks (DNNs) featuring attention\nmechanisms, stands out for their ability to identify and highlight the most\nrelevant aspects of input data. Such a capability is particularly beneficial in\naddressing a variety of communication challenges, notably in the realm of\nsemantic communication where proper encoding of the relevant data is critical\nespecially in systems with limited bandwidth. In this work, we employ vision\ntransformers specifically for the purpose of compression and compact\nrepresentation of the input image, with the goal of preserving semantic\ninformation throughout the transmission process. Through the use of the\nattention mechanism inherent in transformers, we create an attention mask. This\nmask effectively prioritizes critical segments of images for transmission,\nensuring that the reconstruction phase focuses on key objects highlighted by\nthe mask. Our methodology significantly improves the quality of semantic\ncommunication and optimizes bandwidth usage by encoding different parts of the\ndata in accordance with their semantic information content, thus enhancing\noverall efficiency. We evaluate the effectiveness of our proposed framework\nusing the TinyImageNet dataset, focusing on both reconstruction quality and\naccuracy. Our evaluation results demonstrate that our framework successfully\npreserves semantic information, even when only a fraction of the encoded data\nis transmitted, according to the intended compression rates.\n", "link": "http://arxiv.org/abs/2405.01521v1", "date": "2024-05-02", "relevancy": 1.7342, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6236}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5682}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer-Aided%20Semantic%20Communications&body=Title%3A%20Transformer-Aided%20Semantic%20Communications%0AAuthor%3A%20Matin%20Mortaheb%20and%20Erciyes%20Karakaya%20and%20Mohammad%20A.%20Amir%20Khojastepour%20and%20Sennur%20Ulukus%0AAbstract%3A%20%20%20The%20transformer%20structure%20employed%20in%20large%20language%20models%20%28LLMs%29%2C%20as%20a%0Aspecialized%20category%20of%20deep%20neural%20networks%20%28DNNs%29%20featuring%20attention%0Amechanisms%2C%20stands%20out%20for%20their%20ability%20to%20identify%20and%20highlight%20the%20most%0Arelevant%20aspects%20of%20input%20data.%20Such%20a%20capability%20is%20particularly%20beneficial%20in%0Aaddressing%20a%20variety%20of%20communication%20challenges%2C%20notably%20in%20the%20realm%20of%0Asemantic%20communication%20where%20proper%20encoding%20of%20the%20relevant%20data%20is%20critical%0Aespecially%20in%20systems%20with%20limited%20bandwidth.%20In%20this%20work%2C%20we%20employ%20vision%0Atransformers%20specifically%20for%20the%20purpose%20of%20compression%20and%20compact%0Arepresentation%20of%20the%20input%20image%2C%20with%20the%20goal%20of%20preserving%20semantic%0Ainformation%20throughout%20the%20transmission%20process.%20Through%20the%20use%20of%20the%0Aattention%20mechanism%20inherent%20in%20transformers%2C%20we%20create%20an%20attention%20mask.%20This%0Amask%20effectively%20prioritizes%20critical%20segments%20of%20images%20for%20transmission%2C%0Aensuring%20that%20the%20reconstruction%20phase%20focuses%20on%20key%20objects%20highlighted%20by%0Athe%20mask.%20Our%20methodology%20significantly%20improves%20the%20quality%20of%20semantic%0Acommunication%20and%20optimizes%20bandwidth%20usage%20by%20encoding%20different%20parts%20of%20the%0Adata%20in%20accordance%20with%20their%20semantic%20information%20content%2C%20thus%20enhancing%0Aoverall%20efficiency.%20We%20evaluate%20the%20effectiveness%20of%20our%20proposed%20framework%0Ausing%20the%20TinyImageNet%20dataset%2C%20focusing%20on%20both%20reconstruction%20quality%20and%0Aaccuracy.%20Our%20evaluation%20results%20demonstrate%20that%20our%20framework%20successfully%0Apreserves%20semantic%20information%2C%20even%20when%20only%20a%20fraction%20of%20the%20encoded%20data%0Ais%20transmitted%2C%20according%20to%20the%20intended%20compression%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer-Aided%2520Semantic%2520Communications%26entry.906535625%3DMatin%2520Mortaheb%2520and%2520Erciyes%2520Karakaya%2520and%2520Mohammad%2520A.%2520Amir%2520Khojastepour%2520and%2520Sennur%2520Ulukus%26entry.1292438233%3D%2520%2520The%2520transformer%2520structure%2520employed%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520as%2520a%250Aspecialized%2520category%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520featuring%2520attention%250Amechanisms%252C%2520stands%2520out%2520for%2520their%2520ability%2520to%2520identify%2520and%2520highlight%2520the%2520most%250Arelevant%2520aspects%2520of%2520input%2520data.%2520Such%2520a%2520capability%2520is%2520particularly%2520beneficial%2520in%250Aaddressing%2520a%2520variety%2520of%2520communication%2520challenges%252C%2520notably%2520in%2520the%2520realm%2520of%250Asemantic%2520communication%2520where%2520proper%2520encoding%2520of%2520the%2520relevant%2520data%2520is%2520critical%250Aespecially%2520in%2520systems%2520with%2520limited%2520bandwidth.%2520In%2520this%2520work%252C%2520we%2520employ%2520vision%250Atransformers%2520specifically%2520for%2520the%2520purpose%2520of%2520compression%2520and%2520compact%250Arepresentation%2520of%2520the%2520input%2520image%252C%2520with%2520the%2520goal%2520of%2520preserving%2520semantic%250Ainformation%2520throughout%2520the%2520transmission%2520process.%2520Through%2520the%2520use%2520of%2520the%250Aattention%2520mechanism%2520inherent%2520in%2520transformers%252C%2520we%2520create%2520an%2520attention%2520mask.%2520This%250Amask%2520effectively%2520prioritizes%2520critical%2520segments%2520of%2520images%2520for%2520transmission%252C%250Aensuring%2520that%2520the%2520reconstruction%2520phase%2520focuses%2520on%2520key%2520objects%2520highlighted%2520by%250Athe%2520mask.%2520Our%2520methodology%2520significantly%2520improves%2520the%2520quality%2520of%2520semantic%250Acommunication%2520and%2520optimizes%2520bandwidth%2520usage%2520by%2520encoding%2520different%2520parts%2520of%2520the%250Adata%2520in%2520accordance%2520with%2520their%2520semantic%2520information%2520content%252C%2520thus%2520enhancing%250Aoverall%2520efficiency.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520framework%250Ausing%2520the%2520TinyImageNet%2520dataset%252C%2520focusing%2520on%2520both%2520reconstruction%2520quality%2520and%250Aaccuracy.%2520Our%2520evaluation%2520results%2520demonstrate%2520that%2520our%2520framework%2520successfully%250Apreserves%2520semantic%2520information%252C%2520even%2520when%2520only%2520a%2520fraction%2520of%2520the%2520encoded%2520data%250Ais%2520transmitted%252C%2520according%2520to%2520the%2520intended%2520compression%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer-Aided%20Semantic%20Communications&entry.906535625=Matin%20Mortaheb%20and%20Erciyes%20Karakaya%20and%20Mohammad%20A.%20Amir%20Khojastepour%20and%20Sennur%20Ulukus&entry.1292438233=%20%20The%20transformer%20structure%20employed%20in%20large%20language%20models%20%28LLMs%29%2C%20as%20a%0Aspecialized%20category%20of%20deep%20neural%20networks%20%28DNNs%29%20featuring%20attention%0Amechanisms%2C%20stands%20out%20for%20their%20ability%20to%20identify%20and%20highlight%20the%20most%0Arelevant%20aspects%20of%20input%20data.%20Such%20a%20capability%20is%20particularly%20beneficial%20in%0Aaddressing%20a%20variety%20of%20communication%20challenges%2C%20notably%20in%20the%20realm%20of%0Asemantic%20communication%20where%20proper%20encoding%20of%20the%20relevant%20data%20is%20critical%0Aespecially%20in%20systems%20with%20limited%20bandwidth.%20In%20this%20work%2C%20we%20employ%20vision%0Atransformers%20specifically%20for%20the%20purpose%20of%20compression%20and%20compact%0Arepresentation%20of%20the%20input%20image%2C%20with%20the%20goal%20of%20preserving%20semantic%0Ainformation%20throughout%20the%20transmission%20process.%20Through%20the%20use%20of%20the%0Aattention%20mechanism%20inherent%20in%20transformers%2C%20we%20create%20an%20attention%20mask.%20This%0Amask%20effectively%20prioritizes%20critical%20segments%20of%20images%20for%20transmission%2C%0Aensuring%20that%20the%20reconstruction%20phase%20focuses%20on%20key%20objects%20highlighted%20by%0Athe%20mask.%20Our%20methodology%20significantly%20improves%20the%20quality%20of%20semantic%0Acommunication%20and%20optimizes%20bandwidth%20usage%20by%20encoding%20different%20parts%20of%20the%0Adata%20in%20accordance%20with%20their%20semantic%20information%20content%2C%20thus%20enhancing%0Aoverall%20efficiency.%20We%20evaluate%20the%20effectiveness%20of%20our%20proposed%20framework%0Ausing%20the%20TinyImageNet%20dataset%2C%20focusing%20on%20both%20reconstruction%20quality%20and%0Aaccuracy.%20Our%20evaluation%20results%20demonstrate%20that%20our%20framework%20successfully%0Apreserves%20semantic%20information%2C%20even%20when%20only%20a%20fraction%20of%20the%20encoded%20data%0Ais%20transmitted%2C%20according%20to%20the%20intended%20compression%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01521v1&entry.124074799=Read"},
{"title": "Dynamic Local Average Treatment Effects", "author": "Ravi B. Sojitra and Vasilis Syrgkanis", "abstract": "  We consider Dynamic Treatment Regimes (DTRs) with one sided non-compliance\nthat arise in applications such as digital recommendations and adaptive medical\ntrials. These are settings where decision makers encourage individuals to take\ntreatments over time, but adapt encouragements based on previous\nencouragements, treatments, states, and outcomes. Importantly, individuals may\nchoose to (not) comply with a treatment recommendation, whenever it is made\navailable to them, based on unobserved confounding factors. We provide\nnon-parametric identification, estimation, and inference for Dynamic Local\nAverage Treatment Effects, which are expected values of multi-period treatment\ncontrasts among appropriately defined complier subpopulations. Under standard\nassumptions in the Instrumental Variable and DTR literature, we show that one\ncan identify local average effects of contrasts that correspond to offering\ntreatment at any single time step. Under an additional cross-period\neffect-compliance independence assumption, which is satisfied in Staggered\nAdoption settings and a generalization of them, which we define as Staggered\nCompliance settings, we identify local average treatment effects of treating in\nmultiple time periods.\n", "link": "http://arxiv.org/abs/2405.01463v1", "date": "2024-05-02", "relevancy": 1.6667, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4391}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4115}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Local%20Average%20Treatment%20Effects&body=Title%3A%20Dynamic%20Local%20Average%20Treatment%20Effects%0AAuthor%3A%20Ravi%20B.%20Sojitra%20and%20Vasilis%20Syrgkanis%0AAbstract%3A%20%20%20We%20consider%20Dynamic%20Treatment%20Regimes%20%28DTRs%29%20with%20one%20sided%20non-compliance%0Athat%20arise%20in%20applications%20such%20as%20digital%20recommendations%20and%20adaptive%20medical%0Atrials.%20These%20are%20settings%20where%20decision%20makers%20encourage%20individuals%20to%20take%0Atreatments%20over%20time%2C%20but%20adapt%20encouragements%20based%20on%20previous%0Aencouragements%2C%20treatments%2C%20states%2C%20and%20outcomes.%20Importantly%2C%20individuals%20may%0Achoose%20to%20%28not%29%20comply%20with%20a%20treatment%20recommendation%2C%20whenever%20it%20is%20made%0Aavailable%20to%20them%2C%20based%20on%20unobserved%20confounding%20factors.%20We%20provide%0Anon-parametric%20identification%2C%20estimation%2C%20and%20inference%20for%20Dynamic%20Local%0AAverage%20Treatment%20Effects%2C%20which%20are%20expected%20values%20of%20multi-period%20treatment%0Acontrasts%20among%20appropriately%20defined%20complier%20subpopulations.%20Under%20standard%0Aassumptions%20in%20the%20Instrumental%20Variable%20and%20DTR%20literature%2C%20we%20show%20that%20one%0Acan%20identify%20local%20average%20effects%20of%20contrasts%20that%20correspond%20to%20offering%0Atreatment%20at%20any%20single%20time%20step.%20Under%20an%20additional%20cross-period%0Aeffect-compliance%20independence%20assumption%2C%20which%20is%20satisfied%20in%20Staggered%0AAdoption%20settings%20and%20a%20generalization%20of%20them%2C%20which%20we%20define%20as%20Staggered%0ACompliance%20settings%2C%20we%20identify%20local%20average%20treatment%20effects%20of%20treating%20in%0Amultiple%20time%20periods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Local%2520Average%2520Treatment%2520Effects%26entry.906535625%3DRavi%2520B.%2520Sojitra%2520and%2520Vasilis%2520Syrgkanis%26entry.1292438233%3D%2520%2520We%2520consider%2520Dynamic%2520Treatment%2520Regimes%2520%2528DTRs%2529%2520with%2520one%2520sided%2520non-compliance%250Athat%2520arise%2520in%2520applications%2520such%2520as%2520digital%2520recommendations%2520and%2520adaptive%2520medical%250Atrials.%2520These%2520are%2520settings%2520where%2520decision%2520makers%2520encourage%2520individuals%2520to%2520take%250Atreatments%2520over%2520time%252C%2520but%2520adapt%2520encouragements%2520based%2520on%2520previous%250Aencouragements%252C%2520treatments%252C%2520states%252C%2520and%2520outcomes.%2520Importantly%252C%2520individuals%2520may%250Achoose%2520to%2520%2528not%2529%2520comply%2520with%2520a%2520treatment%2520recommendation%252C%2520whenever%2520it%2520is%2520made%250Aavailable%2520to%2520them%252C%2520based%2520on%2520unobserved%2520confounding%2520factors.%2520We%2520provide%250Anon-parametric%2520identification%252C%2520estimation%252C%2520and%2520inference%2520for%2520Dynamic%2520Local%250AAverage%2520Treatment%2520Effects%252C%2520which%2520are%2520expected%2520values%2520of%2520multi-period%2520treatment%250Acontrasts%2520among%2520appropriately%2520defined%2520complier%2520subpopulations.%2520Under%2520standard%250Aassumptions%2520in%2520the%2520Instrumental%2520Variable%2520and%2520DTR%2520literature%252C%2520we%2520show%2520that%2520one%250Acan%2520identify%2520local%2520average%2520effects%2520of%2520contrasts%2520that%2520correspond%2520to%2520offering%250Atreatment%2520at%2520any%2520single%2520time%2520step.%2520Under%2520an%2520additional%2520cross-period%250Aeffect-compliance%2520independence%2520assumption%252C%2520which%2520is%2520satisfied%2520in%2520Staggered%250AAdoption%2520settings%2520and%2520a%2520generalization%2520of%2520them%252C%2520which%2520we%2520define%2520as%2520Staggered%250ACompliance%2520settings%252C%2520we%2520identify%2520local%2520average%2520treatment%2520effects%2520of%2520treating%2520in%250Amultiple%2520time%2520periods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Local%20Average%20Treatment%20Effects&entry.906535625=Ravi%20B.%20Sojitra%20and%20Vasilis%20Syrgkanis&entry.1292438233=%20%20We%20consider%20Dynamic%20Treatment%20Regimes%20%28DTRs%29%20with%20one%20sided%20non-compliance%0Athat%20arise%20in%20applications%20such%20as%20digital%20recommendations%20and%20adaptive%20medical%0Atrials.%20These%20are%20settings%20where%20decision%20makers%20encourage%20individuals%20to%20take%0Atreatments%20over%20time%2C%20but%20adapt%20encouragements%20based%20on%20previous%0Aencouragements%2C%20treatments%2C%20states%2C%20and%20outcomes.%20Importantly%2C%20individuals%20may%0Achoose%20to%20%28not%29%20comply%20with%20a%20treatment%20recommendation%2C%20whenever%20it%20is%20made%0Aavailable%20to%20them%2C%20based%20on%20unobserved%20confounding%20factors.%20We%20provide%0Anon-parametric%20identification%2C%20estimation%2C%20and%20inference%20for%20Dynamic%20Local%0AAverage%20Treatment%20Effects%2C%20which%20are%20expected%20values%20of%20multi-period%20treatment%0Acontrasts%20among%20appropriately%20defined%20complier%20subpopulations.%20Under%20standard%0Aassumptions%20in%20the%20Instrumental%20Variable%20and%20DTR%20literature%2C%20we%20show%20that%20one%0Acan%20identify%20local%20average%20effects%20of%20contrasts%20that%20correspond%20to%20offering%0Atreatment%20at%20any%20single%20time%20step.%20Under%20an%20additional%20cross-period%0Aeffect-compliance%20independence%20assumption%2C%20which%20is%20satisfied%20in%20Staggered%0AAdoption%20settings%20and%20a%20generalization%20of%20them%2C%20which%20we%20define%20as%20Staggered%0ACompliance%20settings%2C%20we%20identify%20local%20average%20treatment%20effects%20of%20treating%20in%0Amultiple%20time%20periods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01463v1&entry.124074799=Read"},
{"title": "MULTIGAIN 2.0: MDP controller synthesis for multiple mean-payoff, LTL\n  and steady-state constraints", "author": "Severin Bals and Alexandros Evangelidis and Jan K\u0159et\u00ednsk\u00fd and Jakob Waibel", "abstract": "  We present MULTIGAIN 2.0, a major extension to the controller synthesis tool\nMULTIGAIN, built on top of the probabilistic model checker PRISM. This new\nversion extends MULTIGAIN's multi-objective capabilities, by allowing for the\nformal verification and synthesis of controllers for probabilistic systems with\nmulti-dimensional long-run average reward structures, steady-state constraints,\nand linear temporal logic properties. Additionally, MULTIGAIN 2.0 can modify\nthe underlying linear program to prevent unbounded-memory and other unintuitive\nsolutions and visualizes Pareto curves, in the two- and three-dimensional\ncases, to facilitate trade-off analysis in multi-objective scenarios.\n", "link": "http://arxiv.org/abs/2305.16752v2", "date": "2024-05-02", "relevancy": 1.3551, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4619}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4512}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MULTIGAIN%202.0%3A%20MDP%20controller%20synthesis%20for%20multiple%20mean-payoff%2C%20LTL%0A%20%20and%20steady-state%20constraints&body=Title%3A%20MULTIGAIN%202.0%3A%20MDP%20controller%20synthesis%20for%20multiple%20mean-payoff%2C%20LTL%0A%20%20and%20steady-state%20constraints%0AAuthor%3A%20Severin%20Bals%20and%20Alexandros%20Evangelidis%20and%20Jan%20K%C5%99et%C3%ADnsk%C3%BD%20and%20Jakob%20Waibel%0AAbstract%3A%20%20%20We%20present%20MULTIGAIN%202.0%2C%20a%20major%20extension%20to%20the%20controller%20synthesis%20tool%0AMULTIGAIN%2C%20built%20on%20top%20of%20the%20probabilistic%20model%20checker%20PRISM.%20This%20new%0Aversion%20extends%20MULTIGAIN%27s%20multi-objective%20capabilities%2C%20by%20allowing%20for%20the%0Aformal%20verification%20and%20synthesis%20of%20controllers%20for%20probabilistic%20systems%20with%0Amulti-dimensional%20long-run%20average%20reward%20structures%2C%20steady-state%20constraints%2C%0Aand%20linear%20temporal%20logic%20properties.%20Additionally%2C%20MULTIGAIN%202.0%20can%20modify%0Athe%20underlying%20linear%20program%20to%20prevent%20unbounded-memory%20and%20other%20unintuitive%0Asolutions%20and%20visualizes%20Pareto%20curves%2C%20in%20the%20two-%20and%20three-dimensional%0Acases%2C%20to%20facilitate%20trade-off%20analysis%20in%20multi-objective%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.16752v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMULTIGAIN%25202.0%253A%2520MDP%2520controller%2520synthesis%2520for%2520multiple%2520mean-payoff%252C%2520LTL%250A%2520%2520and%2520steady-state%2520constraints%26entry.906535625%3DSeverin%2520Bals%2520and%2520Alexandros%2520Evangelidis%2520and%2520Jan%2520K%25C5%2599et%25C3%25ADnsk%25C3%25BD%2520and%2520Jakob%2520Waibel%26entry.1292438233%3D%2520%2520We%2520present%2520MULTIGAIN%25202.0%252C%2520a%2520major%2520extension%2520to%2520the%2520controller%2520synthesis%2520tool%250AMULTIGAIN%252C%2520built%2520on%2520top%2520of%2520the%2520probabilistic%2520model%2520checker%2520PRISM.%2520This%2520new%250Aversion%2520extends%2520MULTIGAIN%2527s%2520multi-objective%2520capabilities%252C%2520by%2520allowing%2520for%2520the%250Aformal%2520verification%2520and%2520synthesis%2520of%2520controllers%2520for%2520probabilistic%2520systems%2520with%250Amulti-dimensional%2520long-run%2520average%2520reward%2520structures%252C%2520steady-state%2520constraints%252C%250Aand%2520linear%2520temporal%2520logic%2520properties.%2520Additionally%252C%2520MULTIGAIN%25202.0%2520can%2520modify%250Athe%2520underlying%2520linear%2520program%2520to%2520prevent%2520unbounded-memory%2520and%2520other%2520unintuitive%250Asolutions%2520and%2520visualizes%2520Pareto%2520curves%252C%2520in%2520the%2520two-%2520and%2520three-dimensional%250Acases%252C%2520to%2520facilitate%2520trade-off%2520analysis%2520in%2520multi-objective%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.16752v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MULTIGAIN%202.0%3A%20MDP%20controller%20synthesis%20for%20multiple%20mean-payoff%2C%20LTL%0A%20%20and%20steady-state%20constraints&entry.906535625=Severin%20Bals%20and%20Alexandros%20Evangelidis%20and%20Jan%20K%C5%99et%C3%ADnsk%C3%BD%20and%20Jakob%20Waibel&entry.1292438233=%20%20We%20present%20MULTIGAIN%202.0%2C%20a%20major%20extension%20to%20the%20controller%20synthesis%20tool%0AMULTIGAIN%2C%20built%20on%20top%20of%20the%20probabilistic%20model%20checker%20PRISM.%20This%20new%0Aversion%20extends%20MULTIGAIN%27s%20multi-objective%20capabilities%2C%20by%20allowing%20for%20the%0Aformal%20verification%20and%20synthesis%20of%20controllers%20for%20probabilistic%20systems%20with%0Amulti-dimensional%20long-run%20average%20reward%20structures%2C%20steady-state%20constraints%2C%0Aand%20linear%20temporal%20logic%20properties.%20Additionally%2C%20MULTIGAIN%202.0%20can%20modify%0Athe%20underlying%20linear%20program%20to%20prevent%20unbounded-memory%20and%20other%20unintuitive%0Asolutions%20and%20visualizes%20Pareto%20curves%2C%20in%20the%20two-%20and%20three-dimensional%0Acases%2C%20to%20facilitate%20trade-off%20analysis%20in%20multi-objective%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.16752v2&entry.124074799=Read"},
{"title": "ContactNet: Online Multi-Contact Planning for Acyclic Legged Robot\n  Locomotion", "author": "Angelo Bratta and Avadesh Meduri and Michele Focchi and Ludovic Righetti and Claudio Semini", "abstract": "  In legged logomotion, online trajectory optimization techniques generally\ndepend on heuristic-based contact planners in order to have low computation\ntimes and achieve high replanning frequencies. In this work, we propose\nContactNet, a fast acyclic contact planner based on a multi-output regression\nneural network. ContactNet ranks discretized stepping regions, allowing to\nquickly choose the best feasible solution, even in complex environments. The\nlow computation time, in the order of 1 ms, makes possible the execution of the\ncontact planner concurrently with a trajectory optimizer in a Model Predictive\nControl (MPC) fashion. We demonstrate the effectiveness of the approach in\nsimulation in different complex scenarios with the quadruped robot Solo12.\n", "link": "http://arxiv.org/abs/2209.15566v3", "date": "2024-05-02", "relevancy": 1.5775, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5567}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5199}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContactNet%3A%20Online%20Multi-Contact%20Planning%20for%20Acyclic%20Legged%20Robot%0A%20%20Locomotion&body=Title%3A%20ContactNet%3A%20Online%20Multi-Contact%20Planning%20for%20Acyclic%20Legged%20Robot%0A%20%20Locomotion%0AAuthor%3A%20Angelo%20Bratta%20and%20Avadesh%20Meduri%20and%20Michele%20Focchi%20and%20Ludovic%20Righetti%20and%20Claudio%20Semini%0AAbstract%3A%20%20%20In%20legged%20logomotion%2C%20online%20trajectory%20optimization%20techniques%20generally%0Adepend%20on%20heuristic-based%20contact%20planners%20in%20order%20to%20have%20low%20computation%0Atimes%20and%20achieve%20high%20replanning%20frequencies.%20In%20this%20work%2C%20we%20propose%0AContactNet%2C%20a%20fast%20acyclic%20contact%20planner%20based%20on%20a%20multi-output%20regression%0Aneural%20network.%20ContactNet%20ranks%20discretized%20stepping%20regions%2C%20allowing%20to%0Aquickly%20choose%20the%20best%20feasible%20solution%2C%20even%20in%20complex%20environments.%20The%0Alow%20computation%20time%2C%20in%20the%20order%20of%201%20ms%2C%20makes%20possible%20the%20execution%20of%20the%0Acontact%20planner%20concurrently%20with%20a%20trajectory%20optimizer%20in%20a%20Model%20Predictive%0AControl%20%28MPC%29%20fashion.%20We%20demonstrate%20the%20effectiveness%20of%20the%20approach%20in%0Asimulation%20in%20different%20complex%20scenarios%20with%20the%20quadruped%20robot%20Solo12.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.15566v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContactNet%253A%2520Online%2520Multi-Contact%2520Planning%2520for%2520Acyclic%2520Legged%2520Robot%250A%2520%2520Locomotion%26entry.906535625%3DAngelo%2520Bratta%2520and%2520Avadesh%2520Meduri%2520and%2520Michele%2520Focchi%2520and%2520Ludovic%2520Righetti%2520and%2520Claudio%2520Semini%26entry.1292438233%3D%2520%2520In%2520legged%2520logomotion%252C%2520online%2520trajectory%2520optimization%2520techniques%2520generally%250Adepend%2520on%2520heuristic-based%2520contact%2520planners%2520in%2520order%2520to%2520have%2520low%2520computation%250Atimes%2520and%2520achieve%2520high%2520replanning%2520frequencies.%2520In%2520this%2520work%252C%2520we%2520propose%250AContactNet%252C%2520a%2520fast%2520acyclic%2520contact%2520planner%2520based%2520on%2520a%2520multi-output%2520regression%250Aneural%2520network.%2520ContactNet%2520ranks%2520discretized%2520stepping%2520regions%252C%2520allowing%2520to%250Aquickly%2520choose%2520the%2520best%2520feasible%2520solution%252C%2520even%2520in%2520complex%2520environments.%2520The%250Alow%2520computation%2520time%252C%2520in%2520the%2520order%2520of%25201%2520ms%252C%2520makes%2520possible%2520the%2520execution%2520of%2520the%250Acontact%2520planner%2520concurrently%2520with%2520a%2520trajectory%2520optimizer%2520in%2520a%2520Model%2520Predictive%250AControl%2520%2528MPC%2529%2520fashion.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520approach%2520in%250Asimulation%2520in%2520different%2520complex%2520scenarios%2520with%2520the%2520quadruped%2520robot%2520Solo12.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.15566v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContactNet%3A%20Online%20Multi-Contact%20Planning%20for%20Acyclic%20Legged%20Robot%0A%20%20Locomotion&entry.906535625=Angelo%20Bratta%20and%20Avadesh%20Meduri%20and%20Michele%20Focchi%20and%20Ludovic%20Righetti%20and%20Claudio%20Semini&entry.1292438233=%20%20In%20legged%20logomotion%2C%20online%20trajectory%20optimization%20techniques%20generally%0Adepend%20on%20heuristic-based%20contact%20planners%20in%20order%20to%20have%20low%20computation%0Atimes%20and%20achieve%20high%20replanning%20frequencies.%20In%20this%20work%2C%20we%20propose%0AContactNet%2C%20a%20fast%20acyclic%20contact%20planner%20based%20on%20a%20multi-output%20regression%0Aneural%20network.%20ContactNet%20ranks%20discretized%20stepping%20regions%2C%20allowing%20to%0Aquickly%20choose%20the%20best%20feasible%20solution%2C%20even%20in%20complex%20environments.%20The%0Alow%20computation%20time%2C%20in%20the%20order%20of%201%20ms%2C%20makes%20possible%20the%20execution%20of%20the%0Acontact%20planner%20concurrently%20with%20a%20trajectory%20optimizer%20in%20a%20Model%20Predictive%0AControl%20%28MPC%29%20fashion.%20We%20demonstrate%20the%20effectiveness%20of%20the%20approach%20in%0Asimulation%20in%20different%20complex%20scenarios%20with%20the%20quadruped%20robot%20Solo12.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.15566v3&entry.124074799=Read"},
{"title": "Multi-modal Learnable Queries for Image Aesthetics Assessment", "author": "Zhiwei Xiong and Yunfan Zhang and Zhiqi Shen and Peiran Ren and Han Yu", "abstract": "  Image aesthetics assessment (IAA) is attracting wide interest with the\nprevalence of social media. The problem is challenging due to its subjective\nand ambiguous nature. Instead of directly extracting aesthetic features solely\nfrom the image, user comments associated with an image could potentially\nprovide complementary knowledge that is useful for IAA. With existing\nlarge-scale pre-trained models demonstrating strong capabilities in extracting\nhigh-quality transferable visual and textual features, learnable queries are\nshown to be effective in extracting useful features from the pre-trained visual\nfeatures. Therefore, in this paper, we propose MMLQ, which utilizes multi-modal\nlearnable queries to extract aesthetics-related features from multi-modal\npre-trained features. Extensive experimental results demonstrate that MMLQ\nachieves new state-of-the-art performance on multi-modal IAA, beating previous\nmethods by 7.7% and 8.3% in terms of SRCC and PLCC, respectively.\n", "link": "http://arxiv.org/abs/2405.01326v1", "date": "2024-05-02", "relevancy": 1.5998, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5432}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5377}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Learnable%20Queries%20for%20Image%20Aesthetics%20Assessment&body=Title%3A%20Multi-modal%20Learnable%20Queries%20for%20Image%20Aesthetics%20Assessment%0AAuthor%3A%20Zhiwei%20Xiong%20and%20Yunfan%20Zhang%20and%20Zhiqi%20Shen%20and%20Peiran%20Ren%20and%20Han%20Yu%0AAbstract%3A%20%20%20Image%20aesthetics%20assessment%20%28IAA%29%20is%20attracting%20wide%20interest%20with%20the%0Aprevalence%20of%20social%20media.%20The%20problem%20is%20challenging%20due%20to%20its%20subjective%0Aand%20ambiguous%20nature.%20Instead%20of%20directly%20extracting%20aesthetic%20features%20solely%0Afrom%20the%20image%2C%20user%20comments%20associated%20with%20an%20image%20could%20potentially%0Aprovide%20complementary%20knowledge%20that%20is%20useful%20for%20IAA.%20With%20existing%0Alarge-scale%20pre-trained%20models%20demonstrating%20strong%20capabilities%20in%20extracting%0Ahigh-quality%20transferable%20visual%20and%20textual%20features%2C%20learnable%20queries%20are%0Ashown%20to%20be%20effective%20in%20extracting%20useful%20features%20from%20the%20pre-trained%20visual%0Afeatures.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20MMLQ%2C%20which%20utilizes%20multi-modal%0Alearnable%20queries%20to%20extract%20aesthetics-related%20features%20from%20multi-modal%0Apre-trained%20features.%20Extensive%20experimental%20results%20demonstrate%20that%20MMLQ%0Aachieves%20new%20state-of-the-art%20performance%20on%20multi-modal%20IAA%2C%20beating%20previous%0Amethods%20by%207.7%25%20and%208.3%25%20in%20terms%20of%20SRCC%20and%20PLCC%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Learnable%2520Queries%2520for%2520Image%2520Aesthetics%2520Assessment%26entry.906535625%3DZhiwei%2520Xiong%2520and%2520Yunfan%2520Zhang%2520and%2520Zhiqi%2520Shen%2520and%2520Peiran%2520Ren%2520and%2520Han%2520Yu%26entry.1292438233%3D%2520%2520Image%2520aesthetics%2520assessment%2520%2528IAA%2529%2520is%2520attracting%2520wide%2520interest%2520with%2520the%250Aprevalence%2520of%2520social%2520media.%2520The%2520problem%2520is%2520challenging%2520due%2520to%2520its%2520subjective%250Aand%2520ambiguous%2520nature.%2520Instead%2520of%2520directly%2520extracting%2520aesthetic%2520features%2520solely%250Afrom%2520the%2520image%252C%2520user%2520comments%2520associated%2520with%2520an%2520image%2520could%2520potentially%250Aprovide%2520complementary%2520knowledge%2520that%2520is%2520useful%2520for%2520IAA.%2520With%2520existing%250Alarge-scale%2520pre-trained%2520models%2520demonstrating%2520strong%2520capabilities%2520in%2520extracting%250Ahigh-quality%2520transferable%2520visual%2520and%2520textual%2520features%252C%2520learnable%2520queries%2520are%250Ashown%2520to%2520be%2520effective%2520in%2520extracting%2520useful%2520features%2520from%2520the%2520pre-trained%2520visual%250Afeatures.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520MMLQ%252C%2520which%2520utilizes%2520multi-modal%250Alearnable%2520queries%2520to%2520extract%2520aesthetics-related%2520features%2520from%2520multi-modal%250Apre-trained%2520features.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520MMLQ%250Aachieves%2520new%2520state-of-the-art%2520performance%2520on%2520multi-modal%2520IAA%252C%2520beating%2520previous%250Amethods%2520by%25207.7%2525%2520and%25208.3%2525%2520in%2520terms%2520of%2520SRCC%2520and%2520PLCC%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Learnable%20Queries%20for%20Image%20Aesthetics%20Assessment&entry.906535625=Zhiwei%20Xiong%20and%20Yunfan%20Zhang%20and%20Zhiqi%20Shen%20and%20Peiran%20Ren%20and%20Han%20Yu&entry.1292438233=%20%20Image%20aesthetics%20assessment%20%28IAA%29%20is%20attracting%20wide%20interest%20with%20the%0Aprevalence%20of%20social%20media.%20The%20problem%20is%20challenging%20due%20to%20its%20subjective%0Aand%20ambiguous%20nature.%20Instead%20of%20directly%20extracting%20aesthetic%20features%20solely%0Afrom%20the%20image%2C%20user%20comments%20associated%20with%20an%20image%20could%20potentially%0Aprovide%20complementary%20knowledge%20that%20is%20useful%20for%20IAA.%20With%20existing%0Alarge-scale%20pre-trained%20models%20demonstrating%20strong%20capabilities%20in%20extracting%0Ahigh-quality%20transferable%20visual%20and%20textual%20features%2C%20learnable%20queries%20are%0Ashown%20to%20be%20effective%20in%20extracting%20useful%20features%20from%20the%20pre-trained%20visual%0Afeatures.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20MMLQ%2C%20which%20utilizes%20multi-modal%0Alearnable%20queries%20to%20extract%20aesthetics-related%20features%20from%20multi-modal%0Apre-trained%20features.%20Extensive%20experimental%20results%20demonstrate%20that%20MMLQ%0Aachieves%20new%20state-of-the-art%20performance%20on%20multi-modal%20IAA%2C%20beating%20previous%0Amethods%20by%207.7%25%20and%208.3%25%20in%20terms%20of%20SRCC%20and%20PLCC%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01326v1&entry.124074799=Read"},
{"title": "Learning Force Control for Legged Manipulation", "author": "Tifanny Portela and Gabriel B. Margolis and Yandong Ji and Pulkit Agrawal", "abstract": "  Controlling contact forces during interactions is critical for locomotion and\nmanipulation tasks. While sim-to-real reinforcement learning (RL) has succeeded\nin many contact-rich problems, current RL methods achieve forceful interactions\nimplicitly without explicitly regulating forces. We propose a method for\ntraining RL policies for direct force control without requiring access to force\nsensing. We showcase our method on a whole-body control platform of a quadruped\nrobot with an arm. Such force control enables us to perform gravity\ncompensation and impedance control, unlocking compliant whole-body\nmanipulation. The learned whole-body controller with variable compliance makes\nit intuitive for humans to teleoperate the robot by only commanding the\nmanipulator, and the robot's body adjusts automatically to achieve the desired\nposition and force. Consequently, a human teleoperator can easily demonstrate a\nwide variety of loco-manipulation tasks. To the best of our knowledge, we\nprovide the first deployment of learned whole-body force control in legged\nmanipulators, paving the way for more versatile and adaptable legged robots.\n", "link": "http://arxiv.org/abs/2405.01402v1", "date": "2024-05-02", "relevancy": 1.683, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5879}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5586}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Force%20Control%20for%20Legged%20Manipulation&body=Title%3A%20Learning%20Force%20Control%20for%20Legged%20Manipulation%0AAuthor%3A%20Tifanny%20Portela%20and%20Gabriel%20B.%20Margolis%20and%20Yandong%20Ji%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20Controlling%20contact%20forces%20during%20interactions%20is%20critical%20for%20locomotion%20and%0Amanipulation%20tasks.%20While%20sim-to-real%20reinforcement%20learning%20%28RL%29%20has%20succeeded%0Ain%20many%20contact-rich%20problems%2C%20current%20RL%20methods%20achieve%20forceful%20interactions%0Aimplicitly%20without%20explicitly%20regulating%20forces.%20We%20propose%20a%20method%20for%0Atraining%20RL%20policies%20for%20direct%20force%20control%20without%20requiring%20access%20to%20force%0Asensing.%20We%20showcase%20our%20method%20on%20a%20whole-body%20control%20platform%20of%20a%20quadruped%0Arobot%20with%20an%20arm.%20Such%20force%20control%20enables%20us%20to%20perform%20gravity%0Acompensation%20and%20impedance%20control%2C%20unlocking%20compliant%20whole-body%0Amanipulation.%20The%20learned%20whole-body%20controller%20with%20variable%20compliance%20makes%0Ait%20intuitive%20for%20humans%20to%20teleoperate%20the%20robot%20by%20only%20commanding%20the%0Amanipulator%2C%20and%20the%20robot%27s%20body%20adjusts%20automatically%20to%20achieve%20the%20desired%0Aposition%20and%20force.%20Consequently%2C%20a%20human%20teleoperator%20can%20easily%20demonstrate%20a%0Awide%20variety%20of%20loco-manipulation%20tasks.%20To%20the%20best%20of%20our%20knowledge%2C%20we%0Aprovide%20the%20first%20deployment%20of%20learned%20whole-body%20force%20control%20in%20legged%0Amanipulators%2C%20paving%20the%20way%20for%20more%20versatile%20and%20adaptable%20legged%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Force%2520Control%2520for%2520Legged%2520Manipulation%26entry.906535625%3DTifanny%2520Portela%2520and%2520Gabriel%2520B.%2520Margolis%2520and%2520Yandong%2520Ji%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520Controlling%2520contact%2520forces%2520during%2520interactions%2520is%2520critical%2520for%2520locomotion%2520and%250Amanipulation%2520tasks.%2520While%2520sim-to-real%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520succeeded%250Ain%2520many%2520contact-rich%2520problems%252C%2520current%2520RL%2520methods%2520achieve%2520forceful%2520interactions%250Aimplicitly%2520without%2520explicitly%2520regulating%2520forces.%2520We%2520propose%2520a%2520method%2520for%250Atraining%2520RL%2520policies%2520for%2520direct%2520force%2520control%2520without%2520requiring%2520access%2520to%2520force%250Asensing.%2520We%2520showcase%2520our%2520method%2520on%2520a%2520whole-body%2520control%2520platform%2520of%2520a%2520quadruped%250Arobot%2520with%2520an%2520arm.%2520Such%2520force%2520control%2520enables%2520us%2520to%2520perform%2520gravity%250Acompensation%2520and%2520impedance%2520control%252C%2520unlocking%2520compliant%2520whole-body%250Amanipulation.%2520The%2520learned%2520whole-body%2520controller%2520with%2520variable%2520compliance%2520makes%250Ait%2520intuitive%2520for%2520humans%2520to%2520teleoperate%2520the%2520robot%2520by%2520only%2520commanding%2520the%250Amanipulator%252C%2520and%2520the%2520robot%2527s%2520body%2520adjusts%2520automatically%2520to%2520achieve%2520the%2520desired%250Aposition%2520and%2520force.%2520Consequently%252C%2520a%2520human%2520teleoperator%2520can%2520easily%2520demonstrate%2520a%250Awide%2520variety%2520of%2520loco-manipulation%2520tasks.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%250Aprovide%2520the%2520first%2520deployment%2520of%2520learned%2520whole-body%2520force%2520control%2520in%2520legged%250Amanipulators%252C%2520paving%2520the%2520way%2520for%2520more%2520versatile%2520and%2520adaptable%2520legged%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Force%20Control%20for%20Legged%20Manipulation&entry.906535625=Tifanny%20Portela%20and%20Gabriel%20B.%20Margolis%20and%20Yandong%20Ji%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20Controlling%20contact%20forces%20during%20interactions%20is%20critical%20for%20locomotion%20and%0Amanipulation%20tasks.%20While%20sim-to-real%20reinforcement%20learning%20%28RL%29%20has%20succeeded%0Ain%20many%20contact-rich%20problems%2C%20current%20RL%20methods%20achieve%20forceful%20interactions%0Aimplicitly%20without%20explicitly%20regulating%20forces.%20We%20propose%20a%20method%20for%0Atraining%20RL%20policies%20for%20direct%20force%20control%20without%20requiring%20access%20to%20force%0Asensing.%20We%20showcase%20our%20method%20on%20a%20whole-body%20control%20platform%20of%20a%20quadruped%0Arobot%20with%20an%20arm.%20Such%20force%20control%20enables%20us%20to%20perform%20gravity%0Acompensation%20and%20impedance%20control%2C%20unlocking%20compliant%20whole-body%0Amanipulation.%20The%20learned%20whole-body%20controller%20with%20variable%20compliance%20makes%0Ait%20intuitive%20for%20humans%20to%20teleoperate%20the%20robot%20by%20only%20commanding%20the%0Amanipulator%2C%20and%20the%20robot%27s%20body%20adjusts%20automatically%20to%20achieve%20the%20desired%0Aposition%20and%20force.%20Consequently%2C%20a%20human%20teleoperator%20can%20easily%20demonstrate%20a%0Awide%20variety%20of%20loco-manipulation%20tasks.%20To%20the%20best%20of%20our%20knowledge%2C%20we%0Aprovide%20the%20first%20deployment%20of%20learned%20whole-body%20force%20control%20in%20legged%0Amanipulators%2C%20paving%20the%20way%20for%20more%20versatile%20and%20adaptable%20legged%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01402v1&entry.124074799=Read"},
{"title": "Customizing Text-to-Image Models with a Single Image Pair", "author": "Maxwell Jones and Sheng-Yu Wang and Nupur Kumari and David Bau and Jun-Yan Zhu", "abstract": "  Art reinterpretation is the practice of creating a variation of a reference\nwork, making a paired artwork that exhibits a distinct artistic style. We ask\nif such an image pair can be used to customize a generative model to capture\nthe demonstrated stylistic difference. We propose Pair Customization, a new\ncustomization method that learns stylistic difference from a single image pair\nand then applies the acquired style to the generation process. Unlike existing\nmethods that learn to mimic a single concept from a collection of images, our\nmethod captures the stylistic difference between paired images. This allows us\nto apply a stylistic change without overfitting to the specific image content\nin the examples. To address this new task, we employ a joint optimization\nmethod that explicitly separates the style and content into distinct LoRA\nweight spaces. We optimize these style and content weights to reproduce the\nstyle and content images while encouraging their orthogonality. During\ninference, we modify the diffusion process via a new style guidance based on\nour learned weights. Both qualitative and quantitative experiments show that\nour method can effectively learn style while avoiding overfitting to image\ncontent, highlighting the potential of modeling such stylistic differences from\na single image pair.\n", "link": "http://arxiv.org/abs/2405.01536v1", "date": "2024-05-02", "relevancy": 1.194, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6458}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6104}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Customizing%20Text-to-Image%20Models%20with%20a%20Single%20Image%20Pair&body=Title%3A%20Customizing%20Text-to-Image%20Models%20with%20a%20Single%20Image%20Pair%0AAuthor%3A%20Maxwell%20Jones%20and%20Sheng-Yu%20Wang%20and%20Nupur%20Kumari%20and%20David%20Bau%20and%20Jun-Yan%20Zhu%0AAbstract%3A%20%20%20Art%20reinterpretation%20is%20the%20practice%20of%20creating%20a%20variation%20of%20a%20reference%0Awork%2C%20making%20a%20paired%20artwork%20that%20exhibits%20a%20distinct%20artistic%20style.%20We%20ask%0Aif%20such%20an%20image%20pair%20can%20be%20used%20to%20customize%20a%20generative%20model%20to%20capture%0Athe%20demonstrated%20stylistic%20difference.%20We%20propose%20Pair%20Customization%2C%20a%20new%0Acustomization%20method%20that%20learns%20stylistic%20difference%20from%20a%20single%20image%20pair%0Aand%20then%20applies%20the%20acquired%20style%20to%20the%20generation%20process.%20Unlike%20existing%0Amethods%20that%20learn%20to%20mimic%20a%20single%20concept%20from%20a%20collection%20of%20images%2C%20our%0Amethod%20captures%20the%20stylistic%20difference%20between%20paired%20images.%20This%20allows%20us%0Ato%20apply%20a%20stylistic%20change%20without%20overfitting%20to%20the%20specific%20image%20content%0Ain%20the%20examples.%20To%20address%20this%20new%20task%2C%20we%20employ%20a%20joint%20optimization%0Amethod%20that%20explicitly%20separates%20the%20style%20and%20content%20into%20distinct%20LoRA%0Aweight%20spaces.%20We%20optimize%20these%20style%20and%20content%20weights%20to%20reproduce%20the%0Astyle%20and%20content%20images%20while%20encouraging%20their%20orthogonality.%20During%0Ainference%2C%20we%20modify%20the%20diffusion%20process%20via%20a%20new%20style%20guidance%20based%20on%0Aour%20learned%20weights.%20Both%20qualitative%20and%20quantitative%20experiments%20show%20that%0Aour%20method%20can%20effectively%20learn%20style%20while%20avoiding%20overfitting%20to%20image%0Acontent%2C%20highlighting%20the%20potential%20of%20modeling%20such%20stylistic%20differences%20from%0Aa%20single%20image%20pair.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCustomizing%2520Text-to-Image%2520Models%2520with%2520a%2520Single%2520Image%2520Pair%26entry.906535625%3DMaxwell%2520Jones%2520and%2520Sheng-Yu%2520Wang%2520and%2520Nupur%2520Kumari%2520and%2520David%2520Bau%2520and%2520Jun-Yan%2520Zhu%26entry.1292438233%3D%2520%2520Art%2520reinterpretation%2520is%2520the%2520practice%2520of%2520creating%2520a%2520variation%2520of%2520a%2520reference%250Awork%252C%2520making%2520a%2520paired%2520artwork%2520that%2520exhibits%2520a%2520distinct%2520artistic%2520style.%2520We%2520ask%250Aif%2520such%2520an%2520image%2520pair%2520can%2520be%2520used%2520to%2520customize%2520a%2520generative%2520model%2520to%2520capture%250Athe%2520demonstrated%2520stylistic%2520difference.%2520We%2520propose%2520Pair%2520Customization%252C%2520a%2520new%250Acustomization%2520method%2520that%2520learns%2520stylistic%2520difference%2520from%2520a%2520single%2520image%2520pair%250Aand%2520then%2520applies%2520the%2520acquired%2520style%2520to%2520the%2520generation%2520process.%2520Unlike%2520existing%250Amethods%2520that%2520learn%2520to%2520mimic%2520a%2520single%2520concept%2520from%2520a%2520collection%2520of%2520images%252C%2520our%250Amethod%2520captures%2520the%2520stylistic%2520difference%2520between%2520paired%2520images.%2520This%2520allows%2520us%250Ato%2520apply%2520a%2520stylistic%2520change%2520without%2520overfitting%2520to%2520the%2520specific%2520image%2520content%250Ain%2520the%2520examples.%2520To%2520address%2520this%2520new%2520task%252C%2520we%2520employ%2520a%2520joint%2520optimization%250Amethod%2520that%2520explicitly%2520separates%2520the%2520style%2520and%2520content%2520into%2520distinct%2520LoRA%250Aweight%2520spaces.%2520We%2520optimize%2520these%2520style%2520and%2520content%2520weights%2520to%2520reproduce%2520the%250Astyle%2520and%2520content%2520images%2520while%2520encouraging%2520their%2520orthogonality.%2520During%250Ainference%252C%2520we%2520modify%2520the%2520diffusion%2520process%2520via%2520a%2520new%2520style%2520guidance%2520based%2520on%250Aour%2520learned%2520weights.%2520Both%2520qualitative%2520and%2520quantitative%2520experiments%2520show%2520that%250Aour%2520method%2520can%2520effectively%2520learn%2520style%2520while%2520avoiding%2520overfitting%2520to%2520image%250Acontent%252C%2520highlighting%2520the%2520potential%2520of%2520modeling%2520such%2520stylistic%2520differences%2520from%250Aa%2520single%2520image%2520pair.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Customizing%20Text-to-Image%20Models%20with%20a%20Single%20Image%20Pair&entry.906535625=Maxwell%20Jones%20and%20Sheng-Yu%20Wang%20and%20Nupur%20Kumari%20and%20David%20Bau%20and%20Jun-Yan%20Zhu&entry.1292438233=%20%20Art%20reinterpretation%20is%20the%20practice%20of%20creating%20a%20variation%20of%20a%20reference%0Awork%2C%20making%20a%20paired%20artwork%20that%20exhibits%20a%20distinct%20artistic%20style.%20We%20ask%0Aif%20such%20an%20image%20pair%20can%20be%20used%20to%20customize%20a%20generative%20model%20to%20capture%0Athe%20demonstrated%20stylistic%20difference.%20We%20propose%20Pair%20Customization%2C%20a%20new%0Acustomization%20method%20that%20learns%20stylistic%20difference%20from%20a%20single%20image%20pair%0Aand%20then%20applies%20the%20acquired%20style%20to%20the%20generation%20process.%20Unlike%20existing%0Amethods%20that%20learn%20to%20mimic%20a%20single%20concept%20from%20a%20collection%20of%20images%2C%20our%0Amethod%20captures%20the%20stylistic%20difference%20between%20paired%20images.%20This%20allows%20us%0Ato%20apply%20a%20stylistic%20change%20without%20overfitting%20to%20the%20specific%20image%20content%0Ain%20the%20examples.%20To%20address%20this%20new%20task%2C%20we%20employ%20a%20joint%20optimization%0Amethod%20that%20explicitly%20separates%20the%20style%20and%20content%20into%20distinct%20LoRA%0Aweight%20spaces.%20We%20optimize%20these%20style%20and%20content%20weights%20to%20reproduce%20the%0Astyle%20and%20content%20images%20while%20encouraging%20their%20orthogonality.%20During%0Ainference%2C%20we%20modify%20the%20diffusion%20process%20via%20a%20new%20style%20guidance%20based%20on%0Aour%20learned%20weights.%20Both%20qualitative%20and%20quantitative%20experiments%20show%20that%0Aour%20method%20can%20effectively%20learn%20style%20while%20avoiding%20overfitting%20to%20image%0Acontent%2C%20highlighting%20the%20potential%20of%20modeling%20such%20stylistic%20differences%20from%0Aa%20single%20image%20pair.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01536v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


