<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241111.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid\n  Transparency", "author": "Florian Hahlbohm and Fabian Friederichs and Tim Weyrich and Linus Franke and Moritz Kappel and Susana Castillo and Marc Stamminger and Martin Eisemann and Marcus Magnor", "abstract": "  3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, both\nfor inverse rendering as well as real-time exploration of scenes. In these\napplications, coherence across camera frames and multiple views is crucial, be\nit for robust convergence of a scene reconstruction or for artifact-free\nfly-throughs. Recent work started mitigating artifacts that break multi-view\ncoherence, including popping artifacts due to inconsistent transparency sorting\nand perspective-correct outlines of (2D) splats. At the same time, real-time\nrequirements forced such implementations to accept compromises in how\ntransparency of large assemblies of 3D Gaussians is resolved, in turn breaking\ncoherence in other ways. In our work, we aim at achieving maximum coherence, by\nrendering fully perspective-correct 3D Gaussians while using a high-quality\napproximation of accurate blending, hybrid transparency, on a per-pixel level,\nin order to retain real-time frame rates. Our fast and perspectively accurate\napproach for evaluation of 3D Gaussians does not require matrix inversions,\nthereby ensuring numerical stability and eliminating the need for special\nhandling of degenerate splats, and the hybrid transparency formulation for\nblending maintains similar quality as fully resolved per-pixel transparencies\nat a fraction of the rendering costs. We further show that each of these two\ncomponents can be independently integrated into Gaussian splatting systems. In\ncombination, they achieve up to 2$\\times$ higher frame rates, 2$\\times$ faster\noptimization, and equal or better image quality with fewer rendering artifacts\ncompared to traditional 3DGS on common benchmarks.\n", "link": "http://arxiv.org/abs/2410.08129v2", "date": "2024-11-11", "relevancy": 3.3481, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7163}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.651}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Perspective-Correct%203D%20Gaussian%20Splatting%20Using%20Hybrid%0A%20%20Transparency&body=Title%3A%20Efficient%20Perspective-Correct%203D%20Gaussian%20Splatting%20Using%20Hybrid%0A%20%20Transparency%0AAuthor%3A%20Florian%20Hahlbohm%20and%20Fabian%20Friederichs%20and%20Tim%20Weyrich%20and%20Linus%20Franke%20and%20Moritz%20Kappel%20and%20Susana%20Castillo%20and%20Marc%20Stamminger%20and%20Martin%20Eisemann%20and%20Marcus%20Magnor%0AAbstract%3A%20%20%203D%20Gaussian%20Splats%20%283DGS%29%20have%20proven%20a%20versatile%20rendering%20primitive%2C%20both%0Afor%20inverse%20rendering%20as%20well%20as%20real-time%20exploration%20of%20scenes.%20In%20these%0Aapplications%2C%20coherence%20across%20camera%20frames%20and%20multiple%20views%20is%20crucial%2C%20be%0Ait%20for%20robust%20convergence%20of%20a%20scene%20reconstruction%20or%20for%20artifact-free%0Afly-throughs.%20Recent%20work%20started%20mitigating%20artifacts%20that%20break%20multi-view%0Acoherence%2C%20including%20popping%20artifacts%20due%20to%20inconsistent%20transparency%20sorting%0Aand%20perspective-correct%20outlines%20of%20%282D%29%20splats.%20At%20the%20same%20time%2C%20real-time%0Arequirements%20forced%20such%20implementations%20to%20accept%20compromises%20in%20how%0Atransparency%20of%20large%20assemblies%20of%203D%20Gaussians%20is%20resolved%2C%20in%20turn%20breaking%0Acoherence%20in%20other%20ways.%20In%20our%20work%2C%20we%20aim%20at%20achieving%20maximum%20coherence%2C%20by%0Arendering%20fully%20perspective-correct%203D%20Gaussians%20while%20using%20a%20high-quality%0Aapproximation%20of%20accurate%20blending%2C%20hybrid%20transparency%2C%20on%20a%20per-pixel%20level%2C%0Ain%20order%20to%20retain%20real-time%20frame%20rates.%20Our%20fast%20and%20perspectively%20accurate%0Aapproach%20for%20evaluation%20of%203D%20Gaussians%20does%20not%20require%20matrix%20inversions%2C%0Athereby%20ensuring%20numerical%20stability%20and%20eliminating%20the%20need%20for%20special%0Ahandling%20of%20degenerate%20splats%2C%20and%20the%20hybrid%20transparency%20formulation%20for%0Ablending%20maintains%20similar%20quality%20as%20fully%20resolved%20per-pixel%20transparencies%0Aat%20a%20fraction%20of%20the%20rendering%20costs.%20We%20further%20show%20that%20each%20of%20these%20two%0Acomponents%20can%20be%20independently%20integrated%20into%20Gaussian%20splatting%20systems.%20In%0Acombination%2C%20they%20achieve%20up%20to%202%24%5Ctimes%24%20higher%20frame%20rates%2C%202%24%5Ctimes%24%20faster%0Aoptimization%2C%20and%20equal%20or%20better%20image%20quality%20with%20fewer%20rendering%20artifacts%0Acompared%20to%20traditional%203DGS%20on%20common%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08129v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Perspective-Correct%25203D%2520Gaussian%2520Splatting%2520Using%2520Hybrid%250A%2520%2520Transparency%26entry.906535625%3DFlorian%2520Hahlbohm%2520and%2520Fabian%2520Friederichs%2520and%2520Tim%2520Weyrich%2520and%2520Linus%2520Franke%2520and%2520Moritz%2520Kappel%2520and%2520Susana%2520Castillo%2520and%2520Marc%2520Stamminger%2520and%2520Martin%2520Eisemann%2520and%2520Marcus%2520Magnor%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splats%2520%25283DGS%2529%2520have%2520proven%2520a%2520versatile%2520rendering%2520primitive%252C%2520both%250Afor%2520inverse%2520rendering%2520as%2520well%2520as%2520real-time%2520exploration%2520of%2520scenes.%2520In%2520these%250Aapplications%252C%2520coherence%2520across%2520camera%2520frames%2520and%2520multiple%2520views%2520is%2520crucial%252C%2520be%250Ait%2520for%2520robust%2520convergence%2520of%2520a%2520scene%2520reconstruction%2520or%2520for%2520artifact-free%250Afly-throughs.%2520Recent%2520work%2520started%2520mitigating%2520artifacts%2520that%2520break%2520multi-view%250Acoherence%252C%2520including%2520popping%2520artifacts%2520due%2520to%2520inconsistent%2520transparency%2520sorting%250Aand%2520perspective-correct%2520outlines%2520of%2520%25282D%2529%2520splats.%2520At%2520the%2520same%2520time%252C%2520real-time%250Arequirements%2520forced%2520such%2520implementations%2520to%2520accept%2520compromises%2520in%2520how%250Atransparency%2520of%2520large%2520assemblies%2520of%25203D%2520Gaussians%2520is%2520resolved%252C%2520in%2520turn%2520breaking%250Acoherence%2520in%2520other%2520ways.%2520In%2520our%2520work%252C%2520we%2520aim%2520at%2520achieving%2520maximum%2520coherence%252C%2520by%250Arendering%2520fully%2520perspective-correct%25203D%2520Gaussians%2520while%2520using%2520a%2520high-quality%250Aapproximation%2520of%2520accurate%2520blending%252C%2520hybrid%2520transparency%252C%2520on%2520a%2520per-pixel%2520level%252C%250Ain%2520order%2520to%2520retain%2520real-time%2520frame%2520rates.%2520Our%2520fast%2520and%2520perspectively%2520accurate%250Aapproach%2520for%2520evaluation%2520of%25203D%2520Gaussians%2520does%2520not%2520require%2520matrix%2520inversions%252C%250Athereby%2520ensuring%2520numerical%2520stability%2520and%2520eliminating%2520the%2520need%2520for%2520special%250Ahandling%2520of%2520degenerate%2520splats%252C%2520and%2520the%2520hybrid%2520transparency%2520formulation%2520for%250Ablending%2520maintains%2520similar%2520quality%2520as%2520fully%2520resolved%2520per-pixel%2520transparencies%250Aat%2520a%2520fraction%2520of%2520the%2520rendering%2520costs.%2520We%2520further%2520show%2520that%2520each%2520of%2520these%2520two%250Acomponents%2520can%2520be%2520independently%2520integrated%2520into%2520Gaussian%2520splatting%2520systems.%2520In%250Acombination%252C%2520they%2520achieve%2520up%2520to%25202%2524%255Ctimes%2524%2520higher%2520frame%2520rates%252C%25202%2524%255Ctimes%2524%2520faster%250Aoptimization%252C%2520and%2520equal%2520or%2520better%2520image%2520quality%2520with%2520fewer%2520rendering%2520artifacts%250Acompared%2520to%2520traditional%25203DGS%2520on%2520common%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08129v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Perspective-Correct%203D%20Gaussian%20Splatting%20Using%20Hybrid%0A%20%20Transparency&entry.906535625=Florian%20Hahlbohm%20and%20Fabian%20Friederichs%20and%20Tim%20Weyrich%20and%20Linus%20Franke%20and%20Moritz%20Kappel%20and%20Susana%20Castillo%20and%20Marc%20Stamminger%20and%20Martin%20Eisemann%20and%20Marcus%20Magnor&entry.1292438233=%20%203D%20Gaussian%20Splats%20%283DGS%29%20have%20proven%20a%20versatile%20rendering%20primitive%2C%20both%0Afor%20inverse%20rendering%20as%20well%20as%20real-time%20exploration%20of%20scenes.%20In%20these%0Aapplications%2C%20coherence%20across%20camera%20frames%20and%20multiple%20views%20is%20crucial%2C%20be%0Ait%20for%20robust%20convergence%20of%20a%20scene%20reconstruction%20or%20for%20artifact-free%0Afly-throughs.%20Recent%20work%20started%20mitigating%20artifacts%20that%20break%20multi-view%0Acoherence%2C%20including%20popping%20artifacts%20due%20to%20inconsistent%20transparency%20sorting%0Aand%20perspective-correct%20outlines%20of%20%282D%29%20splats.%20At%20the%20same%20time%2C%20real-time%0Arequirements%20forced%20such%20implementations%20to%20accept%20compromises%20in%20how%0Atransparency%20of%20large%20assemblies%20of%203D%20Gaussians%20is%20resolved%2C%20in%20turn%20breaking%0Acoherence%20in%20other%20ways.%20In%20our%20work%2C%20we%20aim%20at%20achieving%20maximum%20coherence%2C%20by%0Arendering%20fully%20perspective-correct%203D%20Gaussians%20while%20using%20a%20high-quality%0Aapproximation%20of%20accurate%20blending%2C%20hybrid%20transparency%2C%20on%20a%20per-pixel%20level%2C%0Ain%20order%20to%20retain%20real-time%20frame%20rates.%20Our%20fast%20and%20perspectively%20accurate%0Aapproach%20for%20evaluation%20of%203D%20Gaussians%20does%20not%20require%20matrix%20inversions%2C%0Athereby%20ensuring%20numerical%20stability%20and%20eliminating%20the%20need%20for%20special%0Ahandling%20of%20degenerate%20splats%2C%20and%20the%20hybrid%20transparency%20formulation%20for%0Ablending%20maintains%20similar%20quality%20as%20fully%20resolved%20per-pixel%20transparencies%0Aat%20a%20fraction%20of%20the%20rendering%20costs.%20We%20further%20show%20that%20each%20of%20these%20two%0Acomponents%20can%20be%20independently%20integrated%20into%20Gaussian%20splatting%20systems.%20In%0Acombination%2C%20they%20achieve%20up%20to%202%24%5Ctimes%24%20higher%20frame%20rates%2C%202%24%5Ctimes%24%20faster%0Aoptimization%2C%20and%20equal%20or%20better%20image%20quality%20with%20fewer%20rendering%20artifacts%0Acompared%20to%20traditional%203DGS%20on%20common%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08129v2&entry.124074799=Read"},
{"title": "A Hierarchical Compression Technique for 3D Gaussian Splatting\n  Compression", "author": "He Huang and Wenjie Huang and Qi Yang and Yiling Xu and Zhu li", "abstract": "  3D Gaussian Splatting (GS) demonstrates excellent rendering quality and\ngeneration speed in novel view synthesis. However, substantial data size poses\nchallenges for storage and transmission, making 3D GS compression an essential\ntechnology. Current 3D GS compression research primarily focuses on developing\nmore compact scene representations, such as converting explicit 3D GS data into\nimplicit forms. In contrast, compression of the GS data itself has hardly been\nexplored. To address this gap, we propose a Hierarchical GS Compression (HGSC)\ntechnique. Initially, we prune unimportant Gaussians based on importance scores\nderived from both global and local significance, effectively reducing\nredundancy while maintaining visual quality. An Octree structure is used to\ncompress 3D positions. Based on the 3D GS Octree, we implement a hierarchical\nattribute compression strategy by employing a KD-tree to partition the 3D GS\ninto multiple blocks. We apply farthest point sampling to select anchor\nprimitives within each block and others as non-anchor primitives with varying\nLevels of Details (LoDs). Anchor primitives serve as reference points for\npredicting non-anchor primitives across different LoDs to reduce spatial\nredundancy. For anchor primitives, we use the region adaptive hierarchical\ntransform to achieve near-lossless compression of various attributes. For\nnon-anchor primitives, each is predicted based on the k-nearest anchor\nprimitives. To further minimize prediction errors, the reconstructed LoD and\nanchor primitives are combined to form new anchor primitives to predict the\nnext LoD. Our method notably achieves superior compression quality and a\nsignificant data size reduction of over 4.5 times compared to the\nstate-of-the-art compression method on small scenes datasets.\n", "link": "http://arxiv.org/abs/2411.06976v1", "date": "2024-11-11", "relevancy": 3.2469, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6667}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6614}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hierarchical%20Compression%20Technique%20for%203D%20Gaussian%20Splatting%0A%20%20Compression&body=Title%3A%20A%20Hierarchical%20Compression%20Technique%20for%203D%20Gaussian%20Splatting%0A%20%20Compression%0AAuthor%3A%20He%20Huang%20and%20Wenjie%20Huang%20and%20Qi%20Yang%20and%20Yiling%20Xu%20and%20Zhu%20li%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%28GS%29%20demonstrates%20excellent%20rendering%20quality%20and%0Ageneration%20speed%20in%20novel%20view%20synthesis.%20However%2C%20substantial%20data%20size%20poses%0Achallenges%20for%20storage%20and%20transmission%2C%20making%203D%20GS%20compression%20an%20essential%0Atechnology.%20Current%203D%20GS%20compression%20research%20primarily%20focuses%20on%20developing%0Amore%20compact%20scene%20representations%2C%20such%20as%20converting%20explicit%203D%20GS%20data%20into%0Aimplicit%20forms.%20In%20contrast%2C%20compression%20of%20the%20GS%20data%20itself%20has%20hardly%20been%0Aexplored.%20To%20address%20this%20gap%2C%20we%20propose%20a%20Hierarchical%20GS%20Compression%20%28HGSC%29%0Atechnique.%20Initially%2C%20we%20prune%20unimportant%20Gaussians%20based%20on%20importance%20scores%0Aderived%20from%20both%20global%20and%20local%20significance%2C%20effectively%20reducing%0Aredundancy%20while%20maintaining%20visual%20quality.%20An%20Octree%20structure%20is%20used%20to%0Acompress%203D%20positions.%20Based%20on%20the%203D%20GS%20Octree%2C%20we%20implement%20a%20hierarchical%0Aattribute%20compression%20strategy%20by%20employing%20a%20KD-tree%20to%20partition%20the%203D%20GS%0Ainto%20multiple%20blocks.%20We%20apply%20farthest%20point%20sampling%20to%20select%20anchor%0Aprimitives%20within%20each%20block%20and%20others%20as%20non-anchor%20primitives%20with%20varying%0ALevels%20of%20Details%20%28LoDs%29.%20Anchor%20primitives%20serve%20as%20reference%20points%20for%0Apredicting%20non-anchor%20primitives%20across%20different%20LoDs%20to%20reduce%20spatial%0Aredundancy.%20For%20anchor%20primitives%2C%20we%20use%20the%20region%20adaptive%20hierarchical%0Atransform%20to%20achieve%20near-lossless%20compression%20of%20various%20attributes.%20For%0Anon-anchor%20primitives%2C%20each%20is%20predicted%20based%20on%20the%20k-nearest%20anchor%0Aprimitives.%20To%20further%20minimize%20prediction%20errors%2C%20the%20reconstructed%20LoD%20and%0Aanchor%20primitives%20are%20combined%20to%20form%20new%20anchor%20primitives%20to%20predict%20the%0Anext%20LoD.%20Our%20method%20notably%20achieves%20superior%20compression%20quality%20and%20a%0Asignificant%20data%20size%20reduction%20of%20over%204.5%20times%20compared%20to%20the%0Astate-of-the-art%20compression%20method%20on%20small%20scenes%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hierarchical%2520Compression%2520Technique%2520for%25203D%2520Gaussian%2520Splatting%250A%2520%2520Compression%26entry.906535625%3DHe%2520Huang%2520and%2520Wenjie%2520Huang%2520and%2520Qi%2520Yang%2520and%2520Yiling%2520Xu%2520and%2520Zhu%2520li%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%2528GS%2529%2520demonstrates%2520excellent%2520rendering%2520quality%2520and%250Ageneration%2520speed%2520in%2520novel%2520view%2520synthesis.%2520However%252C%2520substantial%2520data%2520size%2520poses%250Achallenges%2520for%2520storage%2520and%2520transmission%252C%2520making%25203D%2520GS%2520compression%2520an%2520essential%250Atechnology.%2520Current%25203D%2520GS%2520compression%2520research%2520primarily%2520focuses%2520on%2520developing%250Amore%2520compact%2520scene%2520representations%252C%2520such%2520as%2520converting%2520explicit%25203D%2520GS%2520data%2520into%250Aimplicit%2520forms.%2520In%2520contrast%252C%2520compression%2520of%2520the%2520GS%2520data%2520itself%2520has%2520hardly%2520been%250Aexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520Hierarchical%2520GS%2520Compression%2520%2528HGSC%2529%250Atechnique.%2520Initially%252C%2520we%2520prune%2520unimportant%2520Gaussians%2520based%2520on%2520importance%2520scores%250Aderived%2520from%2520both%2520global%2520and%2520local%2520significance%252C%2520effectively%2520reducing%250Aredundancy%2520while%2520maintaining%2520visual%2520quality.%2520An%2520Octree%2520structure%2520is%2520used%2520to%250Acompress%25203D%2520positions.%2520Based%2520on%2520the%25203D%2520GS%2520Octree%252C%2520we%2520implement%2520a%2520hierarchical%250Aattribute%2520compression%2520strategy%2520by%2520employing%2520a%2520KD-tree%2520to%2520partition%2520the%25203D%2520GS%250Ainto%2520multiple%2520blocks.%2520We%2520apply%2520farthest%2520point%2520sampling%2520to%2520select%2520anchor%250Aprimitives%2520within%2520each%2520block%2520and%2520others%2520as%2520non-anchor%2520primitives%2520with%2520varying%250ALevels%2520of%2520Details%2520%2528LoDs%2529.%2520Anchor%2520primitives%2520serve%2520as%2520reference%2520points%2520for%250Apredicting%2520non-anchor%2520primitives%2520across%2520different%2520LoDs%2520to%2520reduce%2520spatial%250Aredundancy.%2520For%2520anchor%2520primitives%252C%2520we%2520use%2520the%2520region%2520adaptive%2520hierarchical%250Atransform%2520to%2520achieve%2520near-lossless%2520compression%2520of%2520various%2520attributes.%2520For%250Anon-anchor%2520primitives%252C%2520each%2520is%2520predicted%2520based%2520on%2520the%2520k-nearest%2520anchor%250Aprimitives.%2520To%2520further%2520minimize%2520prediction%2520errors%252C%2520the%2520reconstructed%2520LoD%2520and%250Aanchor%2520primitives%2520are%2520combined%2520to%2520form%2520new%2520anchor%2520primitives%2520to%2520predict%2520the%250Anext%2520LoD.%2520Our%2520method%2520notably%2520achieves%2520superior%2520compression%2520quality%2520and%2520a%250Asignificant%2520data%2520size%2520reduction%2520of%2520over%25204.5%2520times%2520compared%2520to%2520the%250Astate-of-the-art%2520compression%2520method%2520on%2520small%2520scenes%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hierarchical%20Compression%20Technique%20for%203D%20Gaussian%20Splatting%0A%20%20Compression&entry.906535625=He%20Huang%20and%20Wenjie%20Huang%20and%20Qi%20Yang%20and%20Yiling%20Xu%20and%20Zhu%20li&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%28GS%29%20demonstrates%20excellent%20rendering%20quality%20and%0Ageneration%20speed%20in%20novel%20view%20synthesis.%20However%2C%20substantial%20data%20size%20poses%0Achallenges%20for%20storage%20and%20transmission%2C%20making%203D%20GS%20compression%20an%20essential%0Atechnology.%20Current%203D%20GS%20compression%20research%20primarily%20focuses%20on%20developing%0Amore%20compact%20scene%20representations%2C%20such%20as%20converting%20explicit%203D%20GS%20data%20into%0Aimplicit%20forms.%20In%20contrast%2C%20compression%20of%20the%20GS%20data%20itself%20has%20hardly%20been%0Aexplored.%20To%20address%20this%20gap%2C%20we%20propose%20a%20Hierarchical%20GS%20Compression%20%28HGSC%29%0Atechnique.%20Initially%2C%20we%20prune%20unimportant%20Gaussians%20based%20on%20importance%20scores%0Aderived%20from%20both%20global%20and%20local%20significance%2C%20effectively%20reducing%0Aredundancy%20while%20maintaining%20visual%20quality.%20An%20Octree%20structure%20is%20used%20to%0Acompress%203D%20positions.%20Based%20on%20the%203D%20GS%20Octree%2C%20we%20implement%20a%20hierarchical%0Aattribute%20compression%20strategy%20by%20employing%20a%20KD-tree%20to%20partition%20the%203D%20GS%0Ainto%20multiple%20blocks.%20We%20apply%20farthest%20point%20sampling%20to%20select%20anchor%0Aprimitives%20within%20each%20block%20and%20others%20as%20non-anchor%20primitives%20with%20varying%0ALevels%20of%20Details%20%28LoDs%29.%20Anchor%20primitives%20serve%20as%20reference%20points%20for%0Apredicting%20non-anchor%20primitives%20across%20different%20LoDs%20to%20reduce%20spatial%0Aredundancy.%20For%20anchor%20primitives%2C%20we%20use%20the%20region%20adaptive%20hierarchical%0Atransform%20to%20achieve%20near-lossless%20compression%20of%20various%20attributes.%20For%0Anon-anchor%20primitives%2C%20each%20is%20predicted%20based%20on%20the%20k-nearest%20anchor%0Aprimitives.%20To%20further%20minimize%20prediction%20errors%2C%20the%20reconstructed%20LoD%20and%0Aanchor%20primitives%20are%20combined%20to%20form%20new%20anchor%20primitives%20to%20predict%20the%0Anext%20LoD.%20Our%20method%20notably%20achieves%20superior%20compression%20quality%20and%20a%0Asignificant%20data%20size%20reduction%20of%20over%204.5%20times%20compared%20to%20the%0Astate-of-the-art%20compression%20method%20on%20small%20scenes%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06976v1&entry.124074799=Read"},
{"title": "Unified Lexical Representation for Interpretable Visual-Language\n  Alignment", "author": "Yifan Li and Yikai Wang and Yanwei Fu and Dongyu Ru and Zheng Zhang and Tong He", "abstract": "  Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's\ngroundbreaking work. Although CLIP performs well, the typical direct latent\nfeature alignment lacks clarity in its representation and similarity scores. On\nthe other hand, lexical representation, a vector whose element represents the\nsimilarity between the sample and a word from the vocabulary, is a natural\nsparse representation and interpretable, providing exact matches for individual\nwords. However, lexical representations are difficult to learn due to no\nground-truth supervision and false-discovery issues, and thus requires complex\ndesign to train effectively. In this paper, we introduce LexVLA, a more\ninterpretable VLA framework by learning a unified lexical representation for\nboth modalities without complex design. We use DINOv2 as our visual model for\nits local-inclined features and Llama 2, a generative language model, to\nleverage its in-context lexical prediction ability. To avoid the false\ndiscovery, we propose an overuse penalty to refrain the lexical representation\nfrom falsely frequently activating meaningless words. We demonstrate that these\ntwo pre-trained uni-modal models can be well-aligned by fine-tuning on the\nmodest multi-modal dataset and avoid intricate training configurations. On\ncross-modal retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal\ndataset, outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M)\nand those trained from scratch on even bigger datasets (e.g., 1.1B data,\nincluding CC-12M). We conduct extensive experiments to analyze LexVLA. Codes\nare available at https://github.com/Clementine24/LexVLA.\n", "link": "http://arxiv.org/abs/2407.17827v2", "date": "2024-11-11", "relevancy": 3.1765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6373}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Lexical%20Representation%20for%20Interpretable%20Visual-Language%0A%20%20Alignment&body=Title%3A%20Unified%20Lexical%20Representation%20for%20Interpretable%20Visual-Language%0A%20%20Alignment%0AAuthor%3A%20Yifan%20Li%20and%20Yikai%20Wang%20and%20Yanwei%20Fu%20and%20Dongyu%20Ru%20and%20Zheng%20Zhang%20and%20Tong%20He%0AAbstract%3A%20%20%20Visual-Language%20Alignment%20%28VLA%29%20has%20gained%20a%20lot%20of%20attention%20since%20CLIP%27s%0Agroundbreaking%20work.%20Although%20CLIP%20performs%20well%2C%20the%20typical%20direct%20latent%0Afeature%20alignment%20lacks%20clarity%20in%20its%20representation%20and%20similarity%20scores.%20On%0Athe%20other%20hand%2C%20lexical%20representation%2C%20a%20vector%20whose%20element%20represents%20the%0Asimilarity%20between%20the%20sample%20and%20a%20word%20from%20the%20vocabulary%2C%20is%20a%20natural%0Asparse%20representation%20and%20interpretable%2C%20providing%20exact%20matches%20for%20individual%0Awords.%20However%2C%20lexical%20representations%20are%20difficult%20to%20learn%20due%20to%20no%0Aground-truth%20supervision%20and%20false-discovery%20issues%2C%20and%20thus%20requires%20complex%0Adesign%20to%20train%20effectively.%20In%20this%20paper%2C%20we%20introduce%20LexVLA%2C%20a%20more%0Ainterpretable%20VLA%20framework%20by%20learning%20a%20unified%20lexical%20representation%20for%0Aboth%20modalities%20without%20complex%20design.%20We%20use%20DINOv2%20as%20our%20visual%20model%20for%0Aits%20local-inclined%20features%20and%20Llama%202%2C%20a%20generative%20language%20model%2C%20to%0Aleverage%20its%20in-context%20lexical%20prediction%20ability.%20To%20avoid%20the%20false%0Adiscovery%2C%20we%20propose%20an%20overuse%20penalty%20to%20refrain%20the%20lexical%20representation%0Afrom%20falsely%20frequently%20activating%20meaningless%20words.%20We%20demonstrate%20that%20these%0Atwo%20pre-trained%20uni-modal%20models%20can%20be%20well-aligned%20by%20fine-tuning%20on%20the%0Amodest%20multi-modal%20dataset%20and%20avoid%20intricate%20training%20configurations.%20On%0Across-modal%20retrieval%20benchmarks%2C%20LexVLA%2C%20trained%20on%20the%20CC-12M%20multi-modal%0Adataset%2C%20outperforms%20baselines%20fine-tuned%20on%20larger%20datasets%20%28e.g.%2C%20YFCC15M%29%0Aand%20those%20trained%20from%20scratch%20on%20even%20bigger%20datasets%20%28e.g.%2C%201.1B%20data%2C%0Aincluding%20CC-12M%29.%20We%20conduct%20extensive%20experiments%20to%20analyze%20LexVLA.%20Codes%0Aare%20available%20at%20https%3A//github.com/Clementine24/LexVLA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17827v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Lexical%2520Representation%2520for%2520Interpretable%2520Visual-Language%250A%2520%2520Alignment%26entry.906535625%3DYifan%2520Li%2520and%2520Yikai%2520Wang%2520and%2520Yanwei%2520Fu%2520and%2520Dongyu%2520Ru%2520and%2520Zheng%2520Zhang%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520Visual-Language%2520Alignment%2520%2528VLA%2529%2520has%2520gained%2520a%2520lot%2520of%2520attention%2520since%2520CLIP%2527s%250Agroundbreaking%2520work.%2520Although%2520CLIP%2520performs%2520well%252C%2520the%2520typical%2520direct%2520latent%250Afeature%2520alignment%2520lacks%2520clarity%2520in%2520its%2520representation%2520and%2520similarity%2520scores.%2520On%250Athe%2520other%2520hand%252C%2520lexical%2520representation%252C%2520a%2520vector%2520whose%2520element%2520represents%2520the%250Asimilarity%2520between%2520the%2520sample%2520and%2520a%2520word%2520from%2520the%2520vocabulary%252C%2520is%2520a%2520natural%250Asparse%2520representation%2520and%2520interpretable%252C%2520providing%2520exact%2520matches%2520for%2520individual%250Awords.%2520However%252C%2520lexical%2520representations%2520are%2520difficult%2520to%2520learn%2520due%2520to%2520no%250Aground-truth%2520supervision%2520and%2520false-discovery%2520issues%252C%2520and%2520thus%2520requires%2520complex%250Adesign%2520to%2520train%2520effectively.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520LexVLA%252C%2520a%2520more%250Ainterpretable%2520VLA%2520framework%2520by%2520learning%2520a%2520unified%2520lexical%2520representation%2520for%250Aboth%2520modalities%2520without%2520complex%2520design.%2520We%2520use%2520DINOv2%2520as%2520our%2520visual%2520model%2520for%250Aits%2520local-inclined%2520features%2520and%2520Llama%25202%252C%2520a%2520generative%2520language%2520model%252C%2520to%250Aleverage%2520its%2520in-context%2520lexical%2520prediction%2520ability.%2520To%2520avoid%2520the%2520false%250Adiscovery%252C%2520we%2520propose%2520an%2520overuse%2520penalty%2520to%2520refrain%2520the%2520lexical%2520representation%250Afrom%2520falsely%2520frequently%2520activating%2520meaningless%2520words.%2520We%2520demonstrate%2520that%2520these%250Atwo%2520pre-trained%2520uni-modal%2520models%2520can%2520be%2520well-aligned%2520by%2520fine-tuning%2520on%2520the%250Amodest%2520multi-modal%2520dataset%2520and%2520avoid%2520intricate%2520training%2520configurations.%2520On%250Across-modal%2520retrieval%2520benchmarks%252C%2520LexVLA%252C%2520trained%2520on%2520the%2520CC-12M%2520multi-modal%250Adataset%252C%2520outperforms%2520baselines%2520fine-tuned%2520on%2520larger%2520datasets%2520%2528e.g.%252C%2520YFCC15M%2529%250Aand%2520those%2520trained%2520from%2520scratch%2520on%2520even%2520bigger%2520datasets%2520%2528e.g.%252C%25201.1B%2520data%252C%250Aincluding%2520CC-12M%2529.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520analyze%2520LexVLA.%2520Codes%250Aare%2520available%2520at%2520https%253A//github.com/Clementine24/LexVLA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17827v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Lexical%20Representation%20for%20Interpretable%20Visual-Language%0A%20%20Alignment&entry.906535625=Yifan%20Li%20and%20Yikai%20Wang%20and%20Yanwei%20Fu%20and%20Dongyu%20Ru%20and%20Zheng%20Zhang%20and%20Tong%20He&entry.1292438233=%20%20Visual-Language%20Alignment%20%28VLA%29%20has%20gained%20a%20lot%20of%20attention%20since%20CLIP%27s%0Agroundbreaking%20work.%20Although%20CLIP%20performs%20well%2C%20the%20typical%20direct%20latent%0Afeature%20alignment%20lacks%20clarity%20in%20its%20representation%20and%20similarity%20scores.%20On%0Athe%20other%20hand%2C%20lexical%20representation%2C%20a%20vector%20whose%20element%20represents%20the%0Asimilarity%20between%20the%20sample%20and%20a%20word%20from%20the%20vocabulary%2C%20is%20a%20natural%0Asparse%20representation%20and%20interpretable%2C%20providing%20exact%20matches%20for%20individual%0Awords.%20However%2C%20lexical%20representations%20are%20difficult%20to%20learn%20due%20to%20no%0Aground-truth%20supervision%20and%20false-discovery%20issues%2C%20and%20thus%20requires%20complex%0Adesign%20to%20train%20effectively.%20In%20this%20paper%2C%20we%20introduce%20LexVLA%2C%20a%20more%0Ainterpretable%20VLA%20framework%20by%20learning%20a%20unified%20lexical%20representation%20for%0Aboth%20modalities%20without%20complex%20design.%20We%20use%20DINOv2%20as%20our%20visual%20model%20for%0Aits%20local-inclined%20features%20and%20Llama%202%2C%20a%20generative%20language%20model%2C%20to%0Aleverage%20its%20in-context%20lexical%20prediction%20ability.%20To%20avoid%20the%20false%0Adiscovery%2C%20we%20propose%20an%20overuse%20penalty%20to%20refrain%20the%20lexical%20representation%0Afrom%20falsely%20frequently%20activating%20meaningless%20words.%20We%20demonstrate%20that%20these%0Atwo%20pre-trained%20uni-modal%20models%20can%20be%20well-aligned%20by%20fine-tuning%20on%20the%0Amodest%20multi-modal%20dataset%20and%20avoid%20intricate%20training%20configurations.%20On%0Across-modal%20retrieval%20benchmarks%2C%20LexVLA%2C%20trained%20on%20the%20CC-12M%20multi-modal%0Adataset%2C%20outperforms%20baselines%20fine-tuned%20on%20larger%20datasets%20%28e.g.%2C%20YFCC15M%29%0Aand%20those%20trained%20from%20scratch%20on%20even%20bigger%20datasets%20%28e.g.%2C%201.1B%20data%2C%0Aincluding%20CC-12M%29.%20We%20conduct%20extensive%20experiments%20to%20analyze%20LexVLA.%20Codes%0Aare%20available%20at%20https%3A//github.com/Clementine24/LexVLA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17827v2&entry.124074799=Read"},
{"title": "Edify 3D: Scalable High-Quality 3D Asset Generation", "author": " NVIDIA and  : and Maciej Bala and Yin Cui and Yifan Ding and Yunhao Ge and Zekun Hao and Jon Hasselgren and Jacob Huffman and Jingyi Jin and J. P. Lewis and Zhaoshuo Li and Chen-Hsuan Lin and Yen-Chen Lin and Tsung-Yi Lin and Ming-Yu Liu and Alice Luo and Qianli Ma and Jacob Munkberg and Stella Shi and Fangyin Wei and Donglai Xiang and Jiashu Xu and Xiaohui Zeng and Qinsheng Zhang", "abstract": "  We introduce Edify 3D, an advanced solution designed for high-quality 3D\nasset generation. Our method first synthesizes RGB and surface normal images of\nthe described object at multiple viewpoints using a diffusion model. The\nmulti-view observations are then used to reconstruct the shape, texture, and\nPBR materials of the object. Our method can generate high-quality 3D assets\nwith detailed geometry, clean shape topologies, high-resolution textures, and\nmaterials within 2 minutes of runtime.\n", "link": "http://arxiv.org/abs/2411.07135v1", "date": "2024-11-11", "relevancy": 3.1738, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6706}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6706}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edify%203D%3A%20Scalable%20High-Quality%203D%20Asset%20Generation&body=Title%3A%20Edify%203D%3A%20Scalable%20High-Quality%203D%20Asset%20Generation%0AAuthor%3A%20%20NVIDIA%20and%20%20%3A%20and%20Maciej%20Bala%20and%20Yin%20Cui%20and%20Yifan%20Ding%20and%20Yunhao%20Ge%20and%20Zekun%20Hao%20and%20Jon%20Hasselgren%20and%20Jacob%20Huffman%20and%20Jingyi%20Jin%20and%20J.%20P.%20Lewis%20and%20Zhaoshuo%20Li%20and%20Chen-Hsuan%20Lin%20and%20Yen-Chen%20Lin%20and%20Tsung-Yi%20Lin%20and%20Ming-Yu%20Liu%20and%20Alice%20Luo%20and%20Qianli%20Ma%20and%20Jacob%20Munkberg%20and%20Stella%20Shi%20and%20Fangyin%20Wei%20and%20Donglai%20Xiang%20and%20Jiashu%20Xu%20and%20Xiaohui%20Zeng%20and%20Qinsheng%20Zhang%0AAbstract%3A%20%20%20We%20introduce%20Edify%203D%2C%20an%20advanced%20solution%20designed%20for%20high-quality%203D%0Aasset%20generation.%20Our%20method%20first%20synthesizes%20RGB%20and%20surface%20normal%20images%20of%0Athe%20described%20object%20at%20multiple%20viewpoints%20using%20a%20diffusion%20model.%20The%0Amulti-view%20observations%20are%20then%20used%20to%20reconstruct%20the%20shape%2C%20texture%2C%20and%0APBR%20materials%20of%20the%20object.%20Our%20method%20can%20generate%20high-quality%203D%20assets%0Awith%20detailed%20geometry%2C%20clean%20shape%20topologies%2C%20high-resolution%20textures%2C%20and%0Amaterials%20within%202%20minutes%20of%20runtime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdify%25203D%253A%2520Scalable%2520High-Quality%25203D%2520Asset%2520Generation%26entry.906535625%3D%2520NVIDIA%2520and%2520%2520%253A%2520and%2520Maciej%2520Bala%2520and%2520Yin%2520Cui%2520and%2520Yifan%2520Ding%2520and%2520Yunhao%2520Ge%2520and%2520Zekun%2520Hao%2520and%2520Jon%2520Hasselgren%2520and%2520Jacob%2520Huffman%2520and%2520Jingyi%2520Jin%2520and%2520J.%2520P.%2520Lewis%2520and%2520Zhaoshuo%2520Li%2520and%2520Chen-Hsuan%2520Lin%2520and%2520Yen-Chen%2520Lin%2520and%2520Tsung-Yi%2520Lin%2520and%2520Ming-Yu%2520Liu%2520and%2520Alice%2520Luo%2520and%2520Qianli%2520Ma%2520and%2520Jacob%2520Munkberg%2520and%2520Stella%2520Shi%2520and%2520Fangyin%2520Wei%2520and%2520Donglai%2520Xiang%2520and%2520Jiashu%2520Xu%2520and%2520Xiaohui%2520Zeng%2520and%2520Qinsheng%2520Zhang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Edify%25203D%252C%2520an%2520advanced%2520solution%2520designed%2520for%2520high-quality%25203D%250Aasset%2520generation.%2520Our%2520method%2520first%2520synthesizes%2520RGB%2520and%2520surface%2520normal%2520images%2520of%250Athe%2520described%2520object%2520at%2520multiple%2520viewpoints%2520using%2520a%2520diffusion%2520model.%2520The%250Amulti-view%2520observations%2520are%2520then%2520used%2520to%2520reconstruct%2520the%2520shape%252C%2520texture%252C%2520and%250APBR%2520materials%2520of%2520the%2520object.%2520Our%2520method%2520can%2520generate%2520high-quality%25203D%2520assets%250Awith%2520detailed%2520geometry%252C%2520clean%2520shape%2520topologies%252C%2520high-resolution%2520textures%252C%2520and%250Amaterials%2520within%25202%2520minutes%2520of%2520runtime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edify%203D%3A%20Scalable%20High-Quality%203D%20Asset%20Generation&entry.906535625=%20NVIDIA%20and%20%20%3A%20and%20Maciej%20Bala%20and%20Yin%20Cui%20and%20Yifan%20Ding%20and%20Yunhao%20Ge%20and%20Zekun%20Hao%20and%20Jon%20Hasselgren%20and%20Jacob%20Huffman%20and%20Jingyi%20Jin%20and%20J.%20P.%20Lewis%20and%20Zhaoshuo%20Li%20and%20Chen-Hsuan%20Lin%20and%20Yen-Chen%20Lin%20and%20Tsung-Yi%20Lin%20and%20Ming-Yu%20Liu%20and%20Alice%20Luo%20and%20Qianli%20Ma%20and%20Jacob%20Munkberg%20and%20Stella%20Shi%20and%20Fangyin%20Wei%20and%20Donglai%20Xiang%20and%20Jiashu%20Xu%20and%20Xiaohui%20Zeng%20and%20Qinsheng%20Zhang&entry.1292438233=%20%20We%20introduce%20Edify%203D%2C%20an%20advanced%20solution%20designed%20for%20high-quality%203D%0Aasset%20generation.%20Our%20method%20first%20synthesizes%20RGB%20and%20surface%20normal%20images%20of%0Athe%20described%20object%20at%20multiple%20viewpoints%20using%20a%20diffusion%20model.%20The%0Amulti-view%20observations%20are%20then%20used%20to%20reconstruct%20the%20shape%2C%20texture%2C%20and%0APBR%20materials%20of%20the%20object.%20Our%20method%20can%20generate%20high-quality%203D%20assets%0Awith%20detailed%20geometry%2C%20clean%20shape%20topologies%2C%20high-resolution%20textures%2C%20and%0Amaterials%20within%202%20minutes%20of%20runtime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07135v1&entry.124074799=Read"},
{"title": "Understanding Long Videos with Multimodal Language Models", "author": "Kanchana Ranasinghe and Xiang Li and Kumara Kahatapitiya and Michael S. Ryoo", "abstract": "  Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly.\n", "link": "http://arxiv.org/abs/2403.16998v2", "date": "2024-11-11", "relevancy": 3.0678, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Long%20Videos%20with%20Multimodal%20Language%20Models&body=Title%3A%20Understanding%20Long%20Videos%20with%20Multimodal%20Language%20Models%0AAuthor%3A%20Kanchana%20Ranasinghe%20and%20Xiang%20Li%20and%20Kumara%20Kahatapitiya%20and%20Michael%20S.%20Ryoo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20allowed%20recent%20LLM-based%20approaches%20to%0Aachieve%20excellent%20performance%20on%20long-video%20understanding%20benchmarks.%20We%0Ainvestigate%20how%20extensive%20world%20knowledge%20and%20strong%20reasoning%20skills%20of%0Aunderlying%20LLMs%20influence%20this%20strong%20performance.%20Surprisingly%2C%20we%20discover%0Athat%20LLM-based%20approaches%20can%20yield%20surprisingly%20good%20accuracy%20on%20long-video%0Atasks%20with%20limited%20video%20information%2C%20sometimes%20even%20with%20no%20video%20specific%0Ainformation.%20Building%20on%20this%2C%20we%20exploring%20injecting%20video-specific%0Ainformation%20into%20an%20LLM-based%20framework.%20We%20utilize%20off-the-shelf%20vision%20tools%0Ato%20extract%20three%20object-centric%20information%20modalities%20from%20videos%20and%20then%0Aleverage%20natural%20language%20as%20a%20medium%20for%20fusing%20this%20information.%20Our%0Aresulting%20Multimodal%20Video%20Understanding%20%28MVU%29%20framework%20demonstrates%0Astate-of-the-art%20performance%20across%20multiple%20video%20understanding%20benchmarks.%0AStrong%20performance%20also%20on%20robotics%20domain%20tasks%20establish%20its%20strong%0Agenerality.%20Our%20code%20will%20be%20released%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16998v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Long%2520Videos%2520with%2520Multimodal%2520Language%2520Models%26entry.906535625%3DKanchana%2520Ranasinghe%2520and%2520Xiang%2520Li%2520and%2520Kumara%2520Kahatapitiya%2520and%2520Michael%2520S.%2520Ryoo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520allowed%2520recent%2520LLM-based%2520approaches%2520to%250Aachieve%2520excellent%2520performance%2520on%2520long-video%2520understanding%2520benchmarks.%2520We%250Ainvestigate%2520how%2520extensive%2520world%2520knowledge%2520and%2520strong%2520reasoning%2520skills%2520of%250Aunderlying%2520LLMs%2520influence%2520this%2520strong%2520performance.%2520Surprisingly%252C%2520we%2520discover%250Athat%2520LLM-based%2520approaches%2520can%2520yield%2520surprisingly%2520good%2520accuracy%2520on%2520long-video%250Atasks%2520with%2520limited%2520video%2520information%252C%2520sometimes%2520even%2520with%2520no%2520video%2520specific%250Ainformation.%2520Building%2520on%2520this%252C%2520we%2520exploring%2520injecting%2520video-specific%250Ainformation%2520into%2520an%2520LLM-based%2520framework.%2520We%2520utilize%2520off-the-shelf%2520vision%2520tools%250Ato%2520extract%2520three%2520object-centric%2520information%2520modalities%2520from%2520videos%2520and%2520then%250Aleverage%2520natural%2520language%2520as%2520a%2520medium%2520for%2520fusing%2520this%2520information.%2520Our%250Aresulting%2520Multimodal%2520Video%2520Understanding%2520%2528MVU%2529%2520framework%2520demonstrates%250Astate-of-the-art%2520performance%2520across%2520multiple%2520video%2520understanding%2520benchmarks.%250AStrong%2520performance%2520also%2520on%2520robotics%2520domain%2520tasks%2520establish%2520its%2520strong%250Agenerality.%2520Our%2520code%2520will%2520be%2520released%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16998v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Long%20Videos%20with%20Multimodal%20Language%20Models&entry.906535625=Kanchana%20Ranasinghe%20and%20Xiang%20Li%20and%20Kumara%20Kahatapitiya%20and%20Michael%20S.%20Ryoo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20allowed%20recent%20LLM-based%20approaches%20to%0Aachieve%20excellent%20performance%20on%20long-video%20understanding%20benchmarks.%20We%0Ainvestigate%20how%20extensive%20world%20knowledge%20and%20strong%20reasoning%20skills%20of%0Aunderlying%20LLMs%20influence%20this%20strong%20performance.%20Surprisingly%2C%20we%20discover%0Athat%20LLM-based%20approaches%20can%20yield%20surprisingly%20good%20accuracy%20on%20long-video%0Atasks%20with%20limited%20video%20information%2C%20sometimes%20even%20with%20no%20video%20specific%0Ainformation.%20Building%20on%20this%2C%20we%20exploring%20injecting%20video-specific%0Ainformation%20into%20an%20LLM-based%20framework.%20We%20utilize%20off-the-shelf%20vision%20tools%0Ato%20extract%20three%20object-centric%20information%20modalities%20from%20videos%20and%20then%0Aleverage%20natural%20language%20as%20a%20medium%20for%20fusing%20this%20information.%20Our%0Aresulting%20Multimodal%20Video%20Understanding%20%28MVU%29%20framework%20demonstrates%0Astate-of-the-art%20performance%20across%20multiple%20video%20understanding%20benchmarks.%0AStrong%20performance%20also%20on%20robotics%20domain%20tasks%20establish%20its%20strong%0Agenerality.%20Our%20code%20will%20be%20released%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16998v2&entry.124074799=Read"},
{"title": "SAMPart3D: Segment Any Part in 3D Objects", "author": "Yunhan Yang and Yukun Huang and Yuan-Chen Guo and Liangjun Lu and Xiaoyang Wu and Edmund Y. Lam and Yan-Pei Cao and Xihui Liu", "abstract": "  3D part segmentation is a crucial and challenging task in 3D perception,\nplaying a vital role in applications such as robotics, 3D generation, and 3D\nediting. Recent methods harness the powerful Vision Language Models (VLMs) for\n2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation.\nHowever, these methods are limited by their reliance on text prompts, which\nrestricts the scalability to large-scale unlabeled datasets and the flexibility\nin handling part ambiguities. In this work, we introduce SAMPart3D, a scalable\nzero-shot 3D part segmentation framework that segments any 3D object into\nsemantic parts at multiple granularities, without requiring predefined part\nlabel sets as text prompts. For scalability, we use text-agnostic vision\nfoundation models to distill a 3D feature extraction backbone, allowing scaling\nto large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we\ndistill scale-conditioned part-aware 3D features for 3D part segmentation at\nmultiple granularities. Once the segmented parts are obtained from the\nscale-conditioned part-aware 3D features, we use VLMs to assign semantic labels\nto each part based on the multi-view renderings. Compared to previous methods,\nour SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse\nand handle complex, non-ordinary objects. Additionally, we contribute a new 3D\npart segmentation benchmark to address the lack of diversity and complexity of\nobjects and parts in existing benchmarks. Experiments show that our SAMPart3D\nsignificantly outperforms existing zero-shot 3D part segmentation methods, and\ncan facilitate various applications such as part-level editing and interactive\nsegmentation.\n", "link": "http://arxiv.org/abs/2411.07184v1", "date": "2024-11-11", "relevancy": 3.0109, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMPart3D%3A%20Segment%20Any%20Part%20in%203D%20Objects&body=Title%3A%20SAMPart3D%3A%20Segment%20Any%20Part%20in%203D%20Objects%0AAuthor%3A%20Yunhan%20Yang%20and%20Yukun%20Huang%20and%20Yuan-Chen%20Guo%20and%20Liangjun%20Lu%20and%20Xiaoyang%20Wu%20and%20Edmund%20Y.%20Lam%20and%20Yan-Pei%20Cao%20and%20Xihui%20Liu%0AAbstract%3A%20%20%203D%20part%20segmentation%20is%20a%20crucial%20and%20challenging%20task%20in%203D%20perception%2C%0Aplaying%20a%20vital%20role%20in%20applications%20such%20as%20robotics%2C%203D%20generation%2C%20and%203D%0Aediting.%20Recent%20methods%20harness%20the%20powerful%20Vision%20Language%20Models%20%28VLMs%29%20for%0A2D-to-3D%20knowledge%20distillation%2C%20achieving%20zero-shot%203D%20part%20segmentation.%0AHowever%2C%20these%20methods%20are%20limited%20by%20their%20reliance%20on%20text%20prompts%2C%20which%0Arestricts%20the%20scalability%20to%20large-scale%20unlabeled%20datasets%20and%20the%20flexibility%0Ain%20handling%20part%20ambiguities.%20In%20this%20work%2C%20we%20introduce%20SAMPart3D%2C%20a%20scalable%0Azero-shot%203D%20part%20segmentation%20framework%20that%20segments%20any%203D%20object%20into%0Asemantic%20parts%20at%20multiple%20granularities%2C%20without%20requiring%20predefined%20part%0Alabel%20sets%20as%20text%20prompts.%20For%20scalability%2C%20we%20use%20text-agnostic%20vision%0Afoundation%20models%20to%20distill%20a%203D%20feature%20extraction%20backbone%2C%20allowing%20scaling%0Ato%20large%20unlabeled%203D%20datasets%20to%20learn%20rich%203D%20priors.%20For%20flexibility%2C%20we%0Adistill%20scale-conditioned%20part-aware%203D%20features%20for%203D%20part%20segmentation%20at%0Amultiple%20granularities.%20Once%20the%20segmented%20parts%20are%20obtained%20from%20the%0Ascale-conditioned%20part-aware%203D%20features%2C%20we%20use%20VLMs%20to%20assign%20semantic%20labels%0Ato%20each%20part%20based%20on%20the%20multi-view%20renderings.%20Compared%20to%20previous%20methods%2C%0Aour%20SAMPart3D%20can%20scale%20to%20the%20recent%20large-scale%203D%20object%20dataset%20Objaverse%0Aand%20handle%20complex%2C%20non-ordinary%20objects.%20Additionally%2C%20we%20contribute%20a%20new%203D%0Apart%20segmentation%20benchmark%20to%20address%20the%20lack%20of%20diversity%20and%20complexity%20of%0Aobjects%20and%20parts%20in%20existing%20benchmarks.%20Experiments%20show%20that%20our%20SAMPart3D%0Asignificantly%20outperforms%20existing%20zero-shot%203D%20part%20segmentation%20methods%2C%20and%0Acan%20facilitate%20various%20applications%20such%20as%20part-level%20editing%20and%20interactive%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMPart3D%253A%2520Segment%2520Any%2520Part%2520in%25203D%2520Objects%26entry.906535625%3DYunhan%2520Yang%2520and%2520Yukun%2520Huang%2520and%2520Yuan-Chen%2520Guo%2520and%2520Liangjun%2520Lu%2520and%2520Xiaoyang%2520Wu%2520and%2520Edmund%2520Y.%2520Lam%2520and%2520Yan-Pei%2520Cao%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%25203D%2520part%2520segmentation%2520is%2520a%2520crucial%2520and%2520challenging%2520task%2520in%25203D%2520perception%252C%250Aplaying%2520a%2520vital%2520role%2520in%2520applications%2520such%2520as%2520robotics%252C%25203D%2520generation%252C%2520and%25203D%250Aediting.%2520Recent%2520methods%2520harness%2520the%2520powerful%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520for%250A2D-to-3D%2520knowledge%2520distillation%252C%2520achieving%2520zero-shot%25203D%2520part%2520segmentation.%250AHowever%252C%2520these%2520methods%2520are%2520limited%2520by%2520their%2520reliance%2520on%2520text%2520prompts%252C%2520which%250Arestricts%2520the%2520scalability%2520to%2520large-scale%2520unlabeled%2520datasets%2520and%2520the%2520flexibility%250Ain%2520handling%2520part%2520ambiguities.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SAMPart3D%252C%2520a%2520scalable%250Azero-shot%25203D%2520part%2520segmentation%2520framework%2520that%2520segments%2520any%25203D%2520object%2520into%250Asemantic%2520parts%2520at%2520multiple%2520granularities%252C%2520without%2520requiring%2520predefined%2520part%250Alabel%2520sets%2520as%2520text%2520prompts.%2520For%2520scalability%252C%2520we%2520use%2520text-agnostic%2520vision%250Afoundation%2520models%2520to%2520distill%2520a%25203D%2520feature%2520extraction%2520backbone%252C%2520allowing%2520scaling%250Ato%2520large%2520unlabeled%25203D%2520datasets%2520to%2520learn%2520rich%25203D%2520priors.%2520For%2520flexibility%252C%2520we%250Adistill%2520scale-conditioned%2520part-aware%25203D%2520features%2520for%25203D%2520part%2520segmentation%2520at%250Amultiple%2520granularities.%2520Once%2520the%2520segmented%2520parts%2520are%2520obtained%2520from%2520the%250Ascale-conditioned%2520part-aware%25203D%2520features%252C%2520we%2520use%2520VLMs%2520to%2520assign%2520semantic%2520labels%250Ato%2520each%2520part%2520based%2520on%2520the%2520multi-view%2520renderings.%2520Compared%2520to%2520previous%2520methods%252C%250Aour%2520SAMPart3D%2520can%2520scale%2520to%2520the%2520recent%2520large-scale%25203D%2520object%2520dataset%2520Objaverse%250Aand%2520handle%2520complex%252C%2520non-ordinary%2520objects.%2520Additionally%252C%2520we%2520contribute%2520a%2520new%25203D%250Apart%2520segmentation%2520benchmark%2520to%2520address%2520the%2520lack%2520of%2520diversity%2520and%2520complexity%2520of%250Aobjects%2520and%2520parts%2520in%2520existing%2520benchmarks.%2520Experiments%2520show%2520that%2520our%2520SAMPart3D%250Asignificantly%2520outperforms%2520existing%2520zero-shot%25203D%2520part%2520segmentation%2520methods%252C%2520and%250Acan%2520facilitate%2520various%2520applications%2520such%2520as%2520part-level%2520editing%2520and%2520interactive%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMPart3D%3A%20Segment%20Any%20Part%20in%203D%20Objects&entry.906535625=Yunhan%20Yang%20and%20Yukun%20Huang%20and%20Yuan-Chen%20Guo%20and%20Liangjun%20Lu%20and%20Xiaoyang%20Wu%20and%20Edmund%20Y.%20Lam%20and%20Yan-Pei%20Cao%20and%20Xihui%20Liu&entry.1292438233=%20%203D%20part%20segmentation%20is%20a%20crucial%20and%20challenging%20task%20in%203D%20perception%2C%0Aplaying%20a%20vital%20role%20in%20applications%20such%20as%20robotics%2C%203D%20generation%2C%20and%203D%0Aediting.%20Recent%20methods%20harness%20the%20powerful%20Vision%20Language%20Models%20%28VLMs%29%20for%0A2D-to-3D%20knowledge%20distillation%2C%20achieving%20zero-shot%203D%20part%20segmentation.%0AHowever%2C%20these%20methods%20are%20limited%20by%20their%20reliance%20on%20text%20prompts%2C%20which%0Arestricts%20the%20scalability%20to%20large-scale%20unlabeled%20datasets%20and%20the%20flexibility%0Ain%20handling%20part%20ambiguities.%20In%20this%20work%2C%20we%20introduce%20SAMPart3D%2C%20a%20scalable%0Azero-shot%203D%20part%20segmentation%20framework%20that%20segments%20any%203D%20object%20into%0Asemantic%20parts%20at%20multiple%20granularities%2C%20without%20requiring%20predefined%20part%0Alabel%20sets%20as%20text%20prompts.%20For%20scalability%2C%20we%20use%20text-agnostic%20vision%0Afoundation%20models%20to%20distill%20a%203D%20feature%20extraction%20backbone%2C%20allowing%20scaling%0Ato%20large%20unlabeled%203D%20datasets%20to%20learn%20rich%203D%20priors.%20For%20flexibility%2C%20we%0Adistill%20scale-conditioned%20part-aware%203D%20features%20for%203D%20part%20segmentation%20at%0Amultiple%20granularities.%20Once%20the%20segmented%20parts%20are%20obtained%20from%20the%0Ascale-conditioned%20part-aware%203D%20features%2C%20we%20use%20VLMs%20to%20assign%20semantic%20labels%0Ato%20each%20part%20based%20on%20the%20multi-view%20renderings.%20Compared%20to%20previous%20methods%2C%0Aour%20SAMPart3D%20can%20scale%20to%20the%20recent%20large-scale%203D%20object%20dataset%20Objaverse%0Aand%20handle%20complex%2C%20non-ordinary%20objects.%20Additionally%2C%20we%20contribute%20a%20new%203D%0Apart%20segmentation%20benchmark%20to%20address%20the%20lack%20of%20diversity%20and%20complexity%20of%0Aobjects%20and%20parts%20in%20existing%20benchmarks.%20Experiments%20show%20that%20our%20SAMPart3D%0Asignificantly%20outperforms%20existing%20zero-shot%203D%20part%20segmentation%20methods%2C%20and%0Acan%20facilitate%20various%20applications%20such%20as%20part-level%20editing%20and%20interactive%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07184v1&entry.124074799=Read"},
{"title": "MSTA3D: Multi-scale Twin-attention for 3D Instance Segmentation", "author": "Duc Dang Trung Tran and Byeongkeun Kang and Yeejin Lee", "abstract": "  Recently, transformer-based techniques incorporating superpoints have become\nprevalent in 3D instance segmentation. However, they often encounter an\nover-segmentation problem, especially noticeable with large objects.\nAdditionally, unreliable mask predictions stemming from superpoint mask\nprediction further compound this issue. To address these challenges, we propose\na novel framework called MSTA3D. It leverages multi-scale feature\nrepresentation and introduces a twin-attention mechanism to effectively capture\nthem. Furthermore, MSTA3D integrates a box query with a box regularizer,\noffering a complementary spatial constraint alongside semantic queries.\nExperimental evaluations on ScanNetV2, ScanNet200 and S3DIS datasets\ndemonstrate that our approach surpasses state-of-the-art 3D instance\nsegmentation methods.\n", "link": "http://arxiv.org/abs/2411.01781v3", "date": "2024-11-11", "relevancy": 2.9864, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6213}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5912}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSTA3D%3A%20Multi-scale%20Twin-attention%20for%203D%20Instance%20Segmentation&body=Title%3A%20MSTA3D%3A%20Multi-scale%20Twin-attention%20for%203D%20Instance%20Segmentation%0AAuthor%3A%20Duc%20Dang%20Trung%20Tran%20and%20Byeongkeun%20Kang%20and%20Yeejin%20Lee%0AAbstract%3A%20%20%20Recently%2C%20transformer-based%20techniques%20incorporating%20superpoints%20have%20become%0Aprevalent%20in%203D%20instance%20segmentation.%20However%2C%20they%20often%20encounter%20an%0Aover-segmentation%20problem%2C%20especially%20noticeable%20with%20large%20objects.%0AAdditionally%2C%20unreliable%20mask%20predictions%20stemming%20from%20superpoint%20mask%0Aprediction%20further%20compound%20this%20issue.%20To%20address%20these%20challenges%2C%20we%20propose%0Aa%20novel%20framework%20called%20MSTA3D.%20It%20leverages%20multi-scale%20feature%0Arepresentation%20and%20introduces%20a%20twin-attention%20mechanism%20to%20effectively%20capture%0Athem.%20Furthermore%2C%20MSTA3D%20integrates%20a%20box%20query%20with%20a%20box%20regularizer%2C%0Aoffering%20a%20complementary%20spatial%20constraint%20alongside%20semantic%20queries.%0AExperimental%20evaluations%20on%20ScanNetV2%2C%20ScanNet200%20and%20S3DIS%20datasets%0Ademonstrate%20that%20our%20approach%20surpasses%20state-of-the-art%203D%20instance%0Asegmentation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01781v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSTA3D%253A%2520Multi-scale%2520Twin-attention%2520for%25203D%2520Instance%2520Segmentation%26entry.906535625%3DDuc%2520Dang%2520Trung%2520Tran%2520and%2520Byeongkeun%2520Kang%2520and%2520Yeejin%2520Lee%26entry.1292438233%3D%2520%2520Recently%252C%2520transformer-based%2520techniques%2520incorporating%2520superpoints%2520have%2520become%250Aprevalent%2520in%25203D%2520instance%2520segmentation.%2520However%252C%2520they%2520often%2520encounter%2520an%250Aover-segmentation%2520problem%252C%2520especially%2520noticeable%2520with%2520large%2520objects.%250AAdditionally%252C%2520unreliable%2520mask%2520predictions%2520stemming%2520from%2520superpoint%2520mask%250Aprediction%2520further%2520compound%2520this%2520issue.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250Aa%2520novel%2520framework%2520called%2520MSTA3D.%2520It%2520leverages%2520multi-scale%2520feature%250Arepresentation%2520and%2520introduces%2520a%2520twin-attention%2520mechanism%2520to%2520effectively%2520capture%250Athem.%2520Furthermore%252C%2520MSTA3D%2520integrates%2520a%2520box%2520query%2520with%2520a%2520box%2520regularizer%252C%250Aoffering%2520a%2520complementary%2520spatial%2520constraint%2520alongside%2520semantic%2520queries.%250AExperimental%2520evaluations%2520on%2520ScanNetV2%252C%2520ScanNet200%2520and%2520S3DIS%2520datasets%250Ademonstrate%2520that%2520our%2520approach%2520surpasses%2520state-of-the-art%25203D%2520instance%250Asegmentation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01781v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSTA3D%3A%20Multi-scale%20Twin-attention%20for%203D%20Instance%20Segmentation&entry.906535625=Duc%20Dang%20Trung%20Tran%20and%20Byeongkeun%20Kang%20and%20Yeejin%20Lee&entry.1292438233=%20%20Recently%2C%20transformer-based%20techniques%20incorporating%20superpoints%20have%20become%0Aprevalent%20in%203D%20instance%20segmentation.%20However%2C%20they%20often%20encounter%20an%0Aover-segmentation%20problem%2C%20especially%20noticeable%20with%20large%20objects.%0AAdditionally%2C%20unreliable%20mask%20predictions%20stemming%20from%20superpoint%20mask%0Aprediction%20further%20compound%20this%20issue.%20To%20address%20these%20challenges%2C%20we%20propose%0Aa%20novel%20framework%20called%20MSTA3D.%20It%20leverages%20multi-scale%20feature%0Arepresentation%20and%20introduces%20a%20twin-attention%20mechanism%20to%20effectively%20capture%0Athem.%20Furthermore%2C%20MSTA3D%20integrates%20a%20box%20query%20with%20a%20box%20regularizer%2C%0Aoffering%20a%20complementary%20spatial%20constraint%20alongside%20semantic%20queries.%0AExperimental%20evaluations%20on%20ScanNetV2%2C%20ScanNet200%20and%20S3DIS%20datasets%0Ademonstrate%20that%20our%20approach%20surpasses%20state-of-the-art%203D%20instance%0Asegmentation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01781v3&entry.124074799=Read"},
{"title": "Reminding Multimodal Large Language Models of Object-aware Knowledge\n  with Retrieved Tags", "author": "Daiqing Qi and Handong Zhao and Zijun Wei and Sheng Li", "abstract": "  Despite recent advances in the general visual instruction-following ability\nof Multimodal Large Language Models (MLLMs), they still struggle with critical\nproblems when required to provide a precise and detailed response to a visual\ninstruction: (1) failure to identify novel objects or entities, (2) mention of\nnon-existent objects, and (3) neglect of object's attributed details. Intuitive\nsolutions include improving the size and quality of data or using larger\nfoundation models. They show effectiveness in mitigating these issues, but at\nan expensive cost of collecting a vast amount of new data and introducing a\nsignificantly larger model. Standing at the intersection of these approaches,\nwe examine the three object-oriented problems from the perspective of the\nimage-to-text mapping process by the multimodal connector. In this paper, we\nfirst identify the limitations of multimodal connectors stemming from\ninsufficient training data. Driven by this, we propose to enhance the mapping\nwith retrieval-augmented tag tokens, which contain rich object-aware\ninformation such as object names and attributes. With our Tag-grounded visual\ninstruction tuning with retrieval Augmentation (TUNA), we outperform baselines\nthat share the same language model and training data on 12 benchmarks.\nFurthermore, we show the zero-shot capability of TUNA when provided with\nspecific datastores.\n", "link": "http://arxiv.org/abs/2406.10839v2", "date": "2024-11-11", "relevancy": 2.9569, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reminding%20Multimodal%20Large%20Language%20Models%20of%20Object-aware%20Knowledge%0A%20%20with%20Retrieved%20Tags&body=Title%3A%20Reminding%20Multimodal%20Large%20Language%20Models%20of%20Object-aware%20Knowledge%0A%20%20with%20Retrieved%20Tags%0AAuthor%3A%20Daiqing%20Qi%20and%20Handong%20Zhao%20and%20Zijun%20Wei%20and%20Sheng%20Li%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20the%20general%20visual%20instruction-following%20ability%0Aof%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20they%20still%20struggle%20with%20critical%0Aproblems%20when%20required%20to%20provide%20a%20precise%20and%20detailed%20response%20to%20a%20visual%0Ainstruction%3A%20%281%29%20failure%20to%20identify%20novel%20objects%20or%20entities%2C%20%282%29%20mention%20of%0Anon-existent%20objects%2C%20and%20%283%29%20neglect%20of%20object%27s%20attributed%20details.%20Intuitive%0Asolutions%20include%20improving%20the%20size%20and%20quality%20of%20data%20or%20using%20larger%0Afoundation%20models.%20They%20show%20effectiveness%20in%20mitigating%20these%20issues%2C%20but%20at%0Aan%20expensive%20cost%20of%20collecting%20a%20vast%20amount%20of%20new%20data%20and%20introducing%20a%0Asignificantly%20larger%20model.%20Standing%20at%20the%20intersection%20of%20these%20approaches%2C%0Awe%20examine%20the%20three%20object-oriented%20problems%20from%20the%20perspective%20of%20the%0Aimage-to-text%20mapping%20process%20by%20the%20multimodal%20connector.%20In%20this%20paper%2C%20we%0Afirst%20identify%20the%20limitations%20of%20multimodal%20connectors%20stemming%20from%0Ainsufficient%20training%20data.%20Driven%20by%20this%2C%20we%20propose%20to%20enhance%20the%20mapping%0Awith%20retrieval-augmented%20tag%20tokens%2C%20which%20contain%20rich%20object-aware%0Ainformation%20such%20as%20object%20names%20and%20attributes.%20With%20our%20Tag-grounded%20visual%0Ainstruction%20tuning%20with%20retrieval%20Augmentation%20%28TUNA%29%2C%20we%20outperform%20baselines%0Athat%20share%20the%20same%20language%20model%20and%20training%20data%20on%2012%20benchmarks.%0AFurthermore%2C%20we%20show%20the%20zero-shot%20capability%20of%20TUNA%20when%20provided%20with%0Aspecific%20datastores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReminding%2520Multimodal%2520Large%2520Language%2520Models%2520of%2520Object-aware%2520Knowledge%250A%2520%2520with%2520Retrieved%2520Tags%26entry.906535625%3DDaiqing%2520Qi%2520and%2520Handong%2520Zhao%2520and%2520Zijun%2520Wei%2520and%2520Sheng%2520Li%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520the%2520general%2520visual%2520instruction-following%2520ability%250Aof%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520they%2520still%2520struggle%2520with%2520critical%250Aproblems%2520when%2520required%2520to%2520provide%2520a%2520precise%2520and%2520detailed%2520response%2520to%2520a%2520visual%250Ainstruction%253A%2520%25281%2529%2520failure%2520to%2520identify%2520novel%2520objects%2520or%2520entities%252C%2520%25282%2529%2520mention%2520of%250Anon-existent%2520objects%252C%2520and%2520%25283%2529%2520neglect%2520of%2520object%2527s%2520attributed%2520details.%2520Intuitive%250Asolutions%2520include%2520improving%2520the%2520size%2520and%2520quality%2520of%2520data%2520or%2520using%2520larger%250Afoundation%2520models.%2520They%2520show%2520effectiveness%2520in%2520mitigating%2520these%2520issues%252C%2520but%2520at%250Aan%2520expensive%2520cost%2520of%2520collecting%2520a%2520vast%2520amount%2520of%2520new%2520data%2520and%2520introducing%2520a%250Asignificantly%2520larger%2520model.%2520Standing%2520at%2520the%2520intersection%2520of%2520these%2520approaches%252C%250Awe%2520examine%2520the%2520three%2520object-oriented%2520problems%2520from%2520the%2520perspective%2520of%2520the%250Aimage-to-text%2520mapping%2520process%2520by%2520the%2520multimodal%2520connector.%2520In%2520this%2520paper%252C%2520we%250Afirst%2520identify%2520the%2520limitations%2520of%2520multimodal%2520connectors%2520stemming%2520from%250Ainsufficient%2520training%2520data.%2520Driven%2520by%2520this%252C%2520we%2520propose%2520to%2520enhance%2520the%2520mapping%250Awith%2520retrieval-augmented%2520tag%2520tokens%252C%2520which%2520contain%2520rich%2520object-aware%250Ainformation%2520such%2520as%2520object%2520names%2520and%2520attributes.%2520With%2520our%2520Tag-grounded%2520visual%250Ainstruction%2520tuning%2520with%2520retrieval%2520Augmentation%2520%2528TUNA%2529%252C%2520we%2520outperform%2520baselines%250Athat%2520share%2520the%2520same%2520language%2520model%2520and%2520training%2520data%2520on%252012%2520benchmarks.%250AFurthermore%252C%2520we%2520show%2520the%2520zero-shot%2520capability%2520of%2520TUNA%2520when%2520provided%2520with%250Aspecific%2520datastores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reminding%20Multimodal%20Large%20Language%20Models%20of%20Object-aware%20Knowledge%0A%20%20with%20Retrieved%20Tags&entry.906535625=Daiqing%20Qi%20and%20Handong%20Zhao%20and%20Zijun%20Wei%20and%20Sheng%20Li&entry.1292438233=%20%20Despite%20recent%20advances%20in%20the%20general%20visual%20instruction-following%20ability%0Aof%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20they%20still%20struggle%20with%20critical%0Aproblems%20when%20required%20to%20provide%20a%20precise%20and%20detailed%20response%20to%20a%20visual%0Ainstruction%3A%20%281%29%20failure%20to%20identify%20novel%20objects%20or%20entities%2C%20%282%29%20mention%20of%0Anon-existent%20objects%2C%20and%20%283%29%20neglect%20of%20object%27s%20attributed%20details.%20Intuitive%0Asolutions%20include%20improving%20the%20size%20and%20quality%20of%20data%20or%20using%20larger%0Afoundation%20models.%20They%20show%20effectiveness%20in%20mitigating%20these%20issues%2C%20but%20at%0Aan%20expensive%20cost%20of%20collecting%20a%20vast%20amount%20of%20new%20data%20and%20introducing%20a%0Asignificantly%20larger%20model.%20Standing%20at%20the%20intersection%20of%20these%20approaches%2C%0Awe%20examine%20the%20three%20object-oriented%20problems%20from%20the%20perspective%20of%20the%0Aimage-to-text%20mapping%20process%20by%20the%20multimodal%20connector.%20In%20this%20paper%2C%20we%0Afirst%20identify%20the%20limitations%20of%20multimodal%20connectors%20stemming%20from%0Ainsufficient%20training%20data.%20Driven%20by%20this%2C%20we%20propose%20to%20enhance%20the%20mapping%0Awith%20retrieval-augmented%20tag%20tokens%2C%20which%20contain%20rich%20object-aware%0Ainformation%20such%20as%20object%20names%20and%20attributes.%20With%20our%20Tag-grounded%20visual%0Ainstruction%20tuning%20with%20retrieval%20Augmentation%20%28TUNA%29%2C%20we%20outperform%20baselines%0Athat%20share%20the%20same%20language%20model%20and%20training%20data%20on%2012%20benchmarks.%0AFurthermore%2C%20we%20show%20the%20zero-shot%20capability%20of%20TUNA%20when%20provided%20with%0Aspecific%20datastores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10839v2&entry.124074799=Read"},
{"title": "StoryTeller: Improving Long Video Description through Global\n  Audio-Visual Character Identification", "author": "Yichen He and Yuan Lin and Jianchao Wu and Hanchong Zhang and Yuchen Zhang and Ruicheng Le", "abstract": "  Existing large vision-language models (LVLMs) are largely limited to\nprocessing short, seconds-long videos and struggle with generating coherent\ndescriptions for extended video spanning minutes or more. Long video\ndescription introduces new challenges, such as plot-level consistency across\ndescriptions. To address these, we figure out audio-visual character\nidentification, matching character names to each dialogue, as a key factor. We\npropose StoryTeller, a system for generating dense descriptions of long videos,\nincorporating both low-level visual concepts and high-level plot information.\nStoryTeller uses a multimodal large language model that integrates visual,\naudio, and text modalities to perform audio-visual character identification on\nminute-long video clips. The results are then fed into a LVLM to enhance\nconsistency of video description. We validate our approach on movie description\ntasks and introduce MovieStory101, a dataset with dense descriptions for\nthree-minute movie clips. To evaluate long video descriptions, we create\nMovieQA, a large set of multiple-choice questions for the MovieStory101 test\nset. We assess descriptions by inputting them into GPT-4 to answer these\nquestions, using accuracy as an automatic evaluation metric. Experiments show\nthat StoryTeller outperforms all open and closed-source baselines on MovieQA,\nachieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and\ndemonstrating a +15.56% advantage in human side-by-side evaluations.\nAdditionally, incorporating audio-visual character identification from\nStoryTeller improves the performance of all video description models, with\nGemini-1.5-pro and GPT-4o showing relative improvement of 5.5% and 13.0%,\nrespectively, in accuracy on MovieQA.\n", "link": "http://arxiv.org/abs/2411.07076v1", "date": "2024-11-11", "relevancy": 2.9532, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5966}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StoryTeller%3A%20Improving%20Long%20Video%20Description%20through%20Global%0A%20%20Audio-Visual%20Character%20Identification&body=Title%3A%20StoryTeller%3A%20Improving%20Long%20Video%20Description%20through%20Global%0A%20%20Audio-Visual%20Character%20Identification%0AAuthor%3A%20Yichen%20He%20and%20Yuan%20Lin%20and%20Jianchao%20Wu%20and%20Hanchong%20Zhang%20and%20Yuchen%20Zhang%20and%20Ruicheng%20Le%0AAbstract%3A%20%20%20Existing%20large%20vision-language%20models%20%28LVLMs%29%20are%20largely%20limited%20to%0Aprocessing%20short%2C%20seconds-long%20videos%20and%20struggle%20with%20generating%20coherent%0Adescriptions%20for%20extended%20video%20spanning%20minutes%20or%20more.%20Long%20video%0Adescription%20introduces%20new%20challenges%2C%20such%20as%20plot-level%20consistency%20across%0Adescriptions.%20To%20address%20these%2C%20we%20figure%20out%20audio-visual%20character%0Aidentification%2C%20matching%20character%20names%20to%20each%20dialogue%2C%20as%20a%20key%20factor.%20We%0Apropose%20StoryTeller%2C%20a%20system%20for%20generating%20dense%20descriptions%20of%20long%20videos%2C%0Aincorporating%20both%20low-level%20visual%20concepts%20and%20high-level%20plot%20information.%0AStoryTeller%20uses%20a%20multimodal%20large%20language%20model%20that%20integrates%20visual%2C%0Aaudio%2C%20and%20text%20modalities%20to%20perform%20audio-visual%20character%20identification%20on%0Aminute-long%20video%20clips.%20The%20results%20are%20then%20fed%20into%20a%20LVLM%20to%20enhance%0Aconsistency%20of%20video%20description.%20We%20validate%20our%20approach%20on%20movie%20description%0Atasks%20and%20introduce%20MovieStory101%2C%20a%20dataset%20with%20dense%20descriptions%20for%0Athree-minute%20movie%20clips.%20To%20evaluate%20long%20video%20descriptions%2C%20we%20create%0AMovieQA%2C%20a%20large%20set%20of%20multiple-choice%20questions%20for%20the%20MovieStory101%20test%0Aset.%20We%20assess%20descriptions%20by%20inputting%20them%20into%20GPT-4%20to%20answer%20these%0Aquestions%2C%20using%20accuracy%20as%20an%20automatic%20evaluation%20metric.%20Experiments%20show%0Athat%20StoryTeller%20outperforms%20all%20open%20and%20closed-source%20baselines%20on%20MovieQA%2C%0Aachieving%209.5%25%20higher%20accuracy%20than%20the%20strongest%20baseline%2C%20Gemini-1.5-pro%2C%20and%0Ademonstrating%20a%20%2B15.56%25%20advantage%20in%20human%20side-by-side%20evaluations.%0AAdditionally%2C%20incorporating%20audio-visual%20character%20identification%20from%0AStoryTeller%20improves%20the%20performance%20of%20all%20video%20description%20models%2C%20with%0AGemini-1.5-pro%20and%20GPT-4o%20showing%20relative%20improvement%20of%205.5%25%20and%2013.0%25%2C%0Arespectively%2C%20in%20accuracy%20on%20MovieQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStoryTeller%253A%2520Improving%2520Long%2520Video%2520Description%2520through%2520Global%250A%2520%2520Audio-Visual%2520Character%2520Identification%26entry.906535625%3DYichen%2520He%2520and%2520Yuan%2520Lin%2520and%2520Jianchao%2520Wu%2520and%2520Hanchong%2520Zhang%2520and%2520Yuchen%2520Zhang%2520and%2520Ruicheng%2520Le%26entry.1292438233%3D%2520%2520Existing%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520are%2520largely%2520limited%2520to%250Aprocessing%2520short%252C%2520seconds-long%2520videos%2520and%2520struggle%2520with%2520generating%2520coherent%250Adescriptions%2520for%2520extended%2520video%2520spanning%2520minutes%2520or%2520more.%2520Long%2520video%250Adescription%2520introduces%2520new%2520challenges%252C%2520such%2520as%2520plot-level%2520consistency%2520across%250Adescriptions.%2520To%2520address%2520these%252C%2520we%2520figure%2520out%2520audio-visual%2520character%250Aidentification%252C%2520matching%2520character%2520names%2520to%2520each%2520dialogue%252C%2520as%2520a%2520key%2520factor.%2520We%250Apropose%2520StoryTeller%252C%2520a%2520system%2520for%2520generating%2520dense%2520descriptions%2520of%2520long%2520videos%252C%250Aincorporating%2520both%2520low-level%2520visual%2520concepts%2520and%2520high-level%2520plot%2520information.%250AStoryTeller%2520uses%2520a%2520multimodal%2520large%2520language%2520model%2520that%2520integrates%2520visual%252C%250Aaudio%252C%2520and%2520text%2520modalities%2520to%2520perform%2520audio-visual%2520character%2520identification%2520on%250Aminute-long%2520video%2520clips.%2520The%2520results%2520are%2520then%2520fed%2520into%2520a%2520LVLM%2520to%2520enhance%250Aconsistency%2520of%2520video%2520description.%2520We%2520validate%2520our%2520approach%2520on%2520movie%2520description%250Atasks%2520and%2520introduce%2520MovieStory101%252C%2520a%2520dataset%2520with%2520dense%2520descriptions%2520for%250Athree-minute%2520movie%2520clips.%2520To%2520evaluate%2520long%2520video%2520descriptions%252C%2520we%2520create%250AMovieQA%252C%2520a%2520large%2520set%2520of%2520multiple-choice%2520questions%2520for%2520the%2520MovieStory101%2520test%250Aset.%2520We%2520assess%2520descriptions%2520by%2520inputting%2520them%2520into%2520GPT-4%2520to%2520answer%2520these%250Aquestions%252C%2520using%2520accuracy%2520as%2520an%2520automatic%2520evaluation%2520metric.%2520Experiments%2520show%250Athat%2520StoryTeller%2520outperforms%2520all%2520open%2520and%2520closed-source%2520baselines%2520on%2520MovieQA%252C%250Aachieving%25209.5%2525%2520higher%2520accuracy%2520than%2520the%2520strongest%2520baseline%252C%2520Gemini-1.5-pro%252C%2520and%250Ademonstrating%2520a%2520%252B15.56%2525%2520advantage%2520in%2520human%2520side-by-side%2520evaluations.%250AAdditionally%252C%2520incorporating%2520audio-visual%2520character%2520identification%2520from%250AStoryTeller%2520improves%2520the%2520performance%2520of%2520all%2520video%2520description%2520models%252C%2520with%250AGemini-1.5-pro%2520and%2520GPT-4o%2520showing%2520relative%2520improvement%2520of%25205.5%2525%2520and%252013.0%2525%252C%250Arespectively%252C%2520in%2520accuracy%2520on%2520MovieQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StoryTeller%3A%20Improving%20Long%20Video%20Description%20through%20Global%0A%20%20Audio-Visual%20Character%20Identification&entry.906535625=Yichen%20He%20and%20Yuan%20Lin%20and%20Jianchao%20Wu%20and%20Hanchong%20Zhang%20and%20Yuchen%20Zhang%20and%20Ruicheng%20Le&entry.1292438233=%20%20Existing%20large%20vision-language%20models%20%28LVLMs%29%20are%20largely%20limited%20to%0Aprocessing%20short%2C%20seconds-long%20videos%20and%20struggle%20with%20generating%20coherent%0Adescriptions%20for%20extended%20video%20spanning%20minutes%20or%20more.%20Long%20video%0Adescription%20introduces%20new%20challenges%2C%20such%20as%20plot-level%20consistency%20across%0Adescriptions.%20To%20address%20these%2C%20we%20figure%20out%20audio-visual%20character%0Aidentification%2C%20matching%20character%20names%20to%20each%20dialogue%2C%20as%20a%20key%20factor.%20We%0Apropose%20StoryTeller%2C%20a%20system%20for%20generating%20dense%20descriptions%20of%20long%20videos%2C%0Aincorporating%20both%20low-level%20visual%20concepts%20and%20high-level%20plot%20information.%0AStoryTeller%20uses%20a%20multimodal%20large%20language%20model%20that%20integrates%20visual%2C%0Aaudio%2C%20and%20text%20modalities%20to%20perform%20audio-visual%20character%20identification%20on%0Aminute-long%20video%20clips.%20The%20results%20are%20then%20fed%20into%20a%20LVLM%20to%20enhance%0Aconsistency%20of%20video%20description.%20We%20validate%20our%20approach%20on%20movie%20description%0Atasks%20and%20introduce%20MovieStory101%2C%20a%20dataset%20with%20dense%20descriptions%20for%0Athree-minute%20movie%20clips.%20To%20evaluate%20long%20video%20descriptions%2C%20we%20create%0AMovieQA%2C%20a%20large%20set%20of%20multiple-choice%20questions%20for%20the%20MovieStory101%20test%0Aset.%20We%20assess%20descriptions%20by%20inputting%20them%20into%20GPT-4%20to%20answer%20these%0Aquestions%2C%20using%20accuracy%20as%20an%20automatic%20evaluation%20metric.%20Experiments%20show%0Athat%20StoryTeller%20outperforms%20all%20open%20and%20closed-source%20baselines%20on%20MovieQA%2C%0Aachieving%209.5%25%20higher%20accuracy%20than%20the%20strongest%20baseline%2C%20Gemini-1.5-pro%2C%20and%0Ademonstrating%20a%20%2B15.56%25%20advantage%20in%20human%20side-by-side%20evaluations.%0AAdditionally%2C%20incorporating%20audio-visual%20character%20identification%20from%0AStoryTeller%20improves%20the%20performance%20of%20all%20video%20description%20models%2C%20with%0AGemini-1.5-pro%20and%20GPT-4o%20showing%20relative%20improvement%20of%205.5%25%20and%2013.0%25%2C%0Arespectively%2C%20in%20accuracy%20on%20MovieQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07076v1&entry.124074799=Read"},
{"title": "Multimodal Structure-Aware Quantum Data Processing", "author": "Hala Hawashin and Mehrnoosh Sadrzadeh", "abstract": "  While large language models (LLMs) have advanced the field of natural\nlanguage processing (NLP), their \"black box\" nature obscures their\ndecision-making processes. To address this, researchers developed structured\napproaches using higher order tensors. These are able to model linguistic\nrelations, but stall when training on classical computers due to their\nexcessive size. Tensors are natural inhabitants of quantum systems and training\non quantum computers provides a solution by translating text to variational\nquantum circuits. In this paper, we develop MultiQ-NLP: a framework for\nstructure-aware data processing with multimodal text+image data. Here,\n\"structure\" refers to syntactic and grammatical relationships in language, as\nwell as the hierarchical organization of visual elements in images. We enrich\nthe translation with new types and type homomorphisms and develop novel\narchitectures to represent structure. When tested on a main stream image\nclassification task (SVO Probes), our best model showed a par performance with\nthe state of the art classical models; moreover the best model was fully\nstructured.\n", "link": "http://arxiv.org/abs/2411.04242v3", "date": "2024-11-11", "relevancy": 2.8867, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Structure-Aware%20Quantum%20Data%20Processing&body=Title%3A%20Multimodal%20Structure-Aware%20Quantum%20Data%20Processing%0AAuthor%3A%20Hala%20Hawashin%20and%20Mehrnoosh%20Sadrzadeh%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20have%20advanced%20the%20field%20of%20natural%0Alanguage%20processing%20%28NLP%29%2C%20their%20%22black%20box%22%20nature%20obscures%20their%0Adecision-making%20processes.%20To%20address%20this%2C%20researchers%20developed%20structured%0Aapproaches%20using%20higher%20order%20tensors.%20These%20are%20able%20to%20model%20linguistic%0Arelations%2C%20but%20stall%20when%20training%20on%20classical%20computers%20due%20to%20their%0Aexcessive%20size.%20Tensors%20are%20natural%20inhabitants%20of%20quantum%20systems%20and%20training%0Aon%20quantum%20computers%20provides%20a%20solution%20by%20translating%20text%20to%20variational%0Aquantum%20circuits.%20In%20this%20paper%2C%20we%20develop%20MultiQ-NLP%3A%20a%20framework%20for%0Astructure-aware%20data%20processing%20with%20multimodal%20text%2Bimage%20data.%20Here%2C%0A%22structure%22%20refers%20to%20syntactic%20and%20grammatical%20relationships%20in%20language%2C%20as%0Awell%20as%20the%20hierarchical%20organization%20of%20visual%20elements%20in%20images.%20We%20enrich%0Athe%20translation%20with%20new%20types%20and%20type%20homomorphisms%20and%20develop%20novel%0Aarchitectures%20to%20represent%20structure.%20When%20tested%20on%20a%20main%20stream%20image%0Aclassification%20task%20%28SVO%20Probes%29%2C%20our%20best%20model%20showed%20a%20par%20performance%20with%0Athe%20state%20of%20the%20art%20classical%20models%3B%20moreover%20the%20best%20model%20was%20fully%0Astructured.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04242v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Structure-Aware%2520Quantum%2520Data%2520Processing%26entry.906535625%3DHala%2520Hawashin%2520and%2520Mehrnoosh%2520Sadrzadeh%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520advanced%2520the%2520field%2520of%2520natural%250Alanguage%2520processing%2520%2528NLP%2529%252C%2520their%2520%2522black%2520box%2522%2520nature%2520obscures%2520their%250Adecision-making%2520processes.%2520To%2520address%2520this%252C%2520researchers%2520developed%2520structured%250Aapproaches%2520using%2520higher%2520order%2520tensors.%2520These%2520are%2520able%2520to%2520model%2520linguistic%250Arelations%252C%2520but%2520stall%2520when%2520training%2520on%2520classical%2520computers%2520due%2520to%2520their%250Aexcessive%2520size.%2520Tensors%2520are%2520natural%2520inhabitants%2520of%2520quantum%2520systems%2520and%2520training%250Aon%2520quantum%2520computers%2520provides%2520a%2520solution%2520by%2520translating%2520text%2520to%2520variational%250Aquantum%2520circuits.%2520In%2520this%2520paper%252C%2520we%2520develop%2520MultiQ-NLP%253A%2520a%2520framework%2520for%250Astructure-aware%2520data%2520processing%2520with%2520multimodal%2520text%252Bimage%2520data.%2520Here%252C%250A%2522structure%2522%2520refers%2520to%2520syntactic%2520and%2520grammatical%2520relationships%2520in%2520language%252C%2520as%250Awell%2520as%2520the%2520hierarchical%2520organization%2520of%2520visual%2520elements%2520in%2520images.%2520We%2520enrich%250Athe%2520translation%2520with%2520new%2520types%2520and%2520type%2520homomorphisms%2520and%2520develop%2520novel%250Aarchitectures%2520to%2520represent%2520structure.%2520When%2520tested%2520on%2520a%2520main%2520stream%2520image%250Aclassification%2520task%2520%2528SVO%2520Probes%2529%252C%2520our%2520best%2520model%2520showed%2520a%2520par%2520performance%2520with%250Athe%2520state%2520of%2520the%2520art%2520classical%2520models%253B%2520moreover%2520the%2520best%2520model%2520was%2520fully%250Astructured.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04242v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Structure-Aware%20Quantum%20Data%20Processing&entry.906535625=Hala%20Hawashin%20and%20Mehrnoosh%20Sadrzadeh&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20have%20advanced%20the%20field%20of%20natural%0Alanguage%20processing%20%28NLP%29%2C%20their%20%22black%20box%22%20nature%20obscures%20their%0Adecision-making%20processes.%20To%20address%20this%2C%20researchers%20developed%20structured%0Aapproaches%20using%20higher%20order%20tensors.%20These%20are%20able%20to%20model%20linguistic%0Arelations%2C%20but%20stall%20when%20training%20on%20classical%20computers%20due%20to%20their%0Aexcessive%20size.%20Tensors%20are%20natural%20inhabitants%20of%20quantum%20systems%20and%20training%0Aon%20quantum%20computers%20provides%20a%20solution%20by%20translating%20text%20to%20variational%0Aquantum%20circuits.%20In%20this%20paper%2C%20we%20develop%20MultiQ-NLP%3A%20a%20framework%20for%0Astructure-aware%20data%20processing%20with%20multimodal%20text%2Bimage%20data.%20Here%2C%0A%22structure%22%20refers%20to%20syntactic%20and%20grammatical%20relationships%20in%20language%2C%20as%0Awell%20as%20the%20hierarchical%20organization%20of%20visual%20elements%20in%20images.%20We%20enrich%0Athe%20translation%20with%20new%20types%20and%20type%20homomorphisms%20and%20develop%20novel%0Aarchitectures%20to%20represent%20structure.%20When%20tested%20on%20a%20main%20stream%20image%0Aclassification%20task%20%28SVO%20Probes%29%2C%20our%20best%20model%20showed%20a%20par%20performance%20with%0Athe%20state%20of%20the%20art%20classical%20models%3B%20moreover%20the%20best%20model%20was%20fully%0Astructured.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04242v3&entry.124074799=Read"},
{"title": "VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote\n  Sensing Image Understanding", "author": "Xiang Li and Jian Ding and Mohamed Elhoseiny", "abstract": "  We introduce a new benchmark designed to advance the development of\ngeneral-purpose, large-scale vision-language models for remote sensing images.\nAlthough several vision-language datasets in remote sensing have been proposed\nto pursue this goal, existing datasets are typically tailored to single tasks,\nlack detailed object information, or suffer from inadequate quality control.\nExploring these improvement opportunities, we present a Versatile\nvision-language Benchmark for Remote Sensing image understanding, termed\nVRSBench. This benchmark comprises 29,614 images, with 29,614 human-verified\ndetailed captions, 52,472 object references, and 123,221 question-answer pairs.\nIt facilitates the training and evaluation of vision-language models across a\nbroad spectrum of remote sensing image understanding tasks. We further\nevaluated state-of-the-art models on this benchmark for three vision-language\ntasks: image captioning, visual grounding, and visual question answering. Our\nwork aims to significantly contribute to the development of advanced\nvision-language models in the field of remote sensing. The data and code can be\naccessed at https://github.com/lx709/VRSBench.\n", "link": "http://arxiv.org/abs/2406.12384v2", "date": "2024-11-11", "relevancy": 2.8737, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VRSBench%3A%20A%20Versatile%20Vision-Language%20Benchmark%20Dataset%20for%20Remote%0A%20%20Sensing%20Image%20Understanding&body=Title%3A%20VRSBench%3A%20A%20Versatile%20Vision-Language%20Benchmark%20Dataset%20for%20Remote%0A%20%20Sensing%20Image%20Understanding%0AAuthor%3A%20Xiang%20Li%20and%20Jian%20Ding%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20benchmark%20designed%20to%20advance%20the%20development%20of%0Ageneral-purpose%2C%20large-scale%20vision-language%20models%20for%20remote%20sensing%20images.%0AAlthough%20several%20vision-language%20datasets%20in%20remote%20sensing%20have%20been%20proposed%0Ato%20pursue%20this%20goal%2C%20existing%20datasets%20are%20typically%20tailored%20to%20single%20tasks%2C%0Alack%20detailed%20object%20information%2C%20or%20suffer%20from%20inadequate%20quality%20control.%0AExploring%20these%20improvement%20opportunities%2C%20we%20present%20a%20Versatile%0Avision-language%20Benchmark%20for%20Remote%20Sensing%20image%20understanding%2C%20termed%0AVRSBench.%20This%20benchmark%20comprises%2029%2C614%20images%2C%20with%2029%2C614%20human-verified%0Adetailed%20captions%2C%2052%2C472%20object%20references%2C%20and%20123%2C221%20question-answer%20pairs.%0AIt%20facilitates%20the%20training%20and%20evaluation%20of%20vision-language%20models%20across%20a%0Abroad%20spectrum%20of%20remote%20sensing%20image%20understanding%20tasks.%20We%20further%0Aevaluated%20state-of-the-art%20models%20on%20this%20benchmark%20for%20three%20vision-language%0Atasks%3A%20image%20captioning%2C%20visual%20grounding%2C%20and%20visual%20question%20answering.%20Our%0Awork%20aims%20to%20significantly%20contribute%20to%20the%20development%20of%20advanced%0Avision-language%20models%20in%20the%20field%20of%20remote%20sensing.%20The%20data%20and%20code%20can%20be%0Aaccessed%20at%20https%3A//github.com/lx709/VRSBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12384v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVRSBench%253A%2520A%2520Versatile%2520Vision-Language%2520Benchmark%2520Dataset%2520for%2520Remote%250A%2520%2520Sensing%2520Image%2520Understanding%26entry.906535625%3DXiang%2520Li%2520and%2520Jian%2520Ding%2520and%2520Mohamed%2520Elhoseiny%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520benchmark%2520designed%2520to%2520advance%2520the%2520development%2520of%250Ageneral-purpose%252C%2520large-scale%2520vision-language%2520models%2520for%2520remote%2520sensing%2520images.%250AAlthough%2520several%2520vision-language%2520datasets%2520in%2520remote%2520sensing%2520have%2520been%2520proposed%250Ato%2520pursue%2520this%2520goal%252C%2520existing%2520datasets%2520are%2520typically%2520tailored%2520to%2520single%2520tasks%252C%250Alack%2520detailed%2520object%2520information%252C%2520or%2520suffer%2520from%2520inadequate%2520quality%2520control.%250AExploring%2520these%2520improvement%2520opportunities%252C%2520we%2520present%2520a%2520Versatile%250Avision-language%2520Benchmark%2520for%2520Remote%2520Sensing%2520image%2520understanding%252C%2520termed%250AVRSBench.%2520This%2520benchmark%2520comprises%252029%252C614%2520images%252C%2520with%252029%252C614%2520human-verified%250Adetailed%2520captions%252C%252052%252C472%2520object%2520references%252C%2520and%2520123%252C221%2520question-answer%2520pairs.%250AIt%2520facilitates%2520the%2520training%2520and%2520evaluation%2520of%2520vision-language%2520models%2520across%2520a%250Abroad%2520spectrum%2520of%2520remote%2520sensing%2520image%2520understanding%2520tasks.%2520We%2520further%250Aevaluated%2520state-of-the-art%2520models%2520on%2520this%2520benchmark%2520for%2520three%2520vision-language%250Atasks%253A%2520image%2520captioning%252C%2520visual%2520grounding%252C%2520and%2520visual%2520question%2520answering.%2520Our%250Awork%2520aims%2520to%2520significantly%2520contribute%2520to%2520the%2520development%2520of%2520advanced%250Avision-language%2520models%2520in%2520the%2520field%2520of%2520remote%2520sensing.%2520The%2520data%2520and%2520code%2520can%2520be%250Aaccessed%2520at%2520https%253A//github.com/lx709/VRSBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12384v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VRSBench%3A%20A%20Versatile%20Vision-Language%20Benchmark%20Dataset%20for%20Remote%0A%20%20Sensing%20Image%20Understanding&entry.906535625=Xiang%20Li%20and%20Jian%20Ding%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20We%20introduce%20a%20new%20benchmark%20designed%20to%20advance%20the%20development%20of%0Ageneral-purpose%2C%20large-scale%20vision-language%20models%20for%20remote%20sensing%20images.%0AAlthough%20several%20vision-language%20datasets%20in%20remote%20sensing%20have%20been%20proposed%0Ato%20pursue%20this%20goal%2C%20existing%20datasets%20are%20typically%20tailored%20to%20single%20tasks%2C%0Alack%20detailed%20object%20information%2C%20or%20suffer%20from%20inadequate%20quality%20control.%0AExploring%20these%20improvement%20opportunities%2C%20we%20present%20a%20Versatile%0Avision-language%20Benchmark%20for%20Remote%20Sensing%20image%20understanding%2C%20termed%0AVRSBench.%20This%20benchmark%20comprises%2029%2C614%20images%2C%20with%2029%2C614%20human-verified%0Adetailed%20captions%2C%2052%2C472%20object%20references%2C%20and%20123%2C221%20question-answer%20pairs.%0AIt%20facilitates%20the%20training%20and%20evaluation%20of%20vision-language%20models%20across%20a%0Abroad%20spectrum%20of%20remote%20sensing%20image%20understanding%20tasks.%20We%20further%0Aevaluated%20state-of-the-art%20models%20on%20this%20benchmark%20for%20three%20vision-language%0Atasks%3A%20image%20captioning%2C%20visual%20grounding%2C%20and%20visual%20question%20answering.%20Our%0Awork%20aims%20to%20significantly%20contribute%20to%20the%20development%20of%20advanced%0Avision-language%20models%20in%20the%20field%20of%20remote%20sensing.%20The%20data%20and%20code%20can%20be%0Aaccessed%20at%20https%3A//github.com/lx709/VRSBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12384v2&entry.124074799=Read"},
{"title": "Learning from Limited and Imperfect Data", "author": "Harsh Rangwani", "abstract": "  The datasets used for Deep Neural Network training (e.g., ImageNet, MSCOCO,\netc.) are often manually balanced across categories (classes) to facilitate\nlearning of all the categories. This curation process is often expensive and\nrequires throwing away precious annotated data to balance the frequency across\nclasses. This is because the distribution of data in the world (e.g., internet,\netc.) significantly differs from the well-curated datasets and is often\nover-populated with samples from common categories. The algorithms designed for\nwell-curated datasets perform suboptimally when used to learn from imperfect\ndatasets with long-tailed imbalances and distribution shifts. For deep models\nto be widely used, getting away with the costly curation process by developing\nrobust algorithms that can learn from real-world data distribution is\nnecessary. Toward this goal, we develop practical algorithms for Deep Neural\nNetworks that can learn from limited and imperfect data present in the real\nworld. These works are divided into four segments, each covering a scenario of\nlearning from limited or imperfect data. The first part of the works focuses on\nLearning Generative Models for Long-Tail Data, where we mitigate the\nmode-collapse for tail (minority) classes and enable diverse aesthetic image\ngenerations as head (majority) classes. In the second part, we enable effective\ngeneralization on tail classes through Inductive Regularization schemes, which\nallow tail classes to generalize as the head classes without enforcing explicit\ngeneration of images. In the third part, we develop algorithms for Optimizing\nRelevant Metrics compared to the average accuracy for learning from long-tailed\ndata with limited annotation (semi-supervised), followed by the fourth part,\nwhich focuses on the effective domain adaptation of the model to various\ndomains with zero to very few labeled samples.\n", "link": "http://arxiv.org/abs/2411.07229v1", "date": "2024-11-11", "relevancy": 2.7996, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5714}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5672}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Limited%20and%20Imperfect%20Data&body=Title%3A%20Learning%20from%20Limited%20and%20Imperfect%20Data%0AAuthor%3A%20Harsh%20Rangwani%0AAbstract%3A%20%20%20The%20datasets%20used%20for%20Deep%20Neural%20Network%20training%20%28e.g.%2C%20ImageNet%2C%20MSCOCO%2C%0Aetc.%29%20are%20often%20manually%20balanced%20across%20categories%20%28classes%29%20to%20facilitate%0Alearning%20of%20all%20the%20categories.%20This%20curation%20process%20is%20often%20expensive%20and%0Arequires%20throwing%20away%20precious%20annotated%20data%20to%20balance%20the%20frequency%20across%0Aclasses.%20This%20is%20because%20the%20distribution%20of%20data%20in%20the%20world%20%28e.g.%2C%20internet%2C%0Aetc.%29%20significantly%20differs%20from%20the%20well-curated%20datasets%20and%20is%20often%0Aover-populated%20with%20samples%20from%20common%20categories.%20The%20algorithms%20designed%20for%0Awell-curated%20datasets%20perform%20suboptimally%20when%20used%20to%20learn%20from%20imperfect%0Adatasets%20with%20long-tailed%20imbalances%20and%20distribution%20shifts.%20For%20deep%20models%0Ato%20be%20widely%20used%2C%20getting%20away%20with%20the%20costly%20curation%20process%20by%20developing%0Arobust%20algorithms%20that%20can%20learn%20from%20real-world%20data%20distribution%20is%0Anecessary.%20Toward%20this%20goal%2C%20we%20develop%20practical%20algorithms%20for%20Deep%20Neural%0ANetworks%20that%20can%20learn%20from%20limited%20and%20imperfect%20data%20present%20in%20the%20real%0Aworld.%20These%20works%20are%20divided%20into%20four%20segments%2C%20each%20covering%20a%20scenario%20of%0Alearning%20from%20limited%20or%20imperfect%20data.%20The%20first%20part%20of%20the%20works%20focuses%20on%0ALearning%20Generative%20Models%20for%20Long-Tail%20Data%2C%20where%20we%20mitigate%20the%0Amode-collapse%20for%20tail%20%28minority%29%20classes%20and%20enable%20diverse%20aesthetic%20image%0Agenerations%20as%20head%20%28majority%29%20classes.%20In%20the%20second%20part%2C%20we%20enable%20effective%0Ageneralization%20on%20tail%20classes%20through%20Inductive%20Regularization%20schemes%2C%20which%0Aallow%20tail%20classes%20to%20generalize%20as%20the%20head%20classes%20without%20enforcing%20explicit%0Ageneration%20of%20images.%20In%20the%20third%20part%2C%20we%20develop%20algorithms%20for%20Optimizing%0ARelevant%20Metrics%20compared%20to%20the%20average%20accuracy%20for%20learning%20from%20long-tailed%0Adata%20with%20limited%20annotation%20%28semi-supervised%29%2C%20followed%20by%20the%20fourth%20part%2C%0Awhich%20focuses%20on%20the%20effective%20domain%20adaptation%20of%20the%20model%20to%20various%0Adomains%20with%20zero%20to%20very%20few%20labeled%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Limited%2520and%2520Imperfect%2520Data%26entry.906535625%3DHarsh%2520Rangwani%26entry.1292438233%3D%2520%2520The%2520datasets%2520used%2520for%2520Deep%2520Neural%2520Network%2520training%2520%2528e.g.%252C%2520ImageNet%252C%2520MSCOCO%252C%250Aetc.%2529%2520are%2520often%2520manually%2520balanced%2520across%2520categories%2520%2528classes%2529%2520to%2520facilitate%250Alearning%2520of%2520all%2520the%2520categories.%2520This%2520curation%2520process%2520is%2520often%2520expensive%2520and%250Arequires%2520throwing%2520away%2520precious%2520annotated%2520data%2520to%2520balance%2520the%2520frequency%2520across%250Aclasses.%2520This%2520is%2520because%2520the%2520distribution%2520of%2520data%2520in%2520the%2520world%2520%2528e.g.%252C%2520internet%252C%250Aetc.%2529%2520significantly%2520differs%2520from%2520the%2520well-curated%2520datasets%2520and%2520is%2520often%250Aover-populated%2520with%2520samples%2520from%2520common%2520categories.%2520The%2520algorithms%2520designed%2520for%250Awell-curated%2520datasets%2520perform%2520suboptimally%2520when%2520used%2520to%2520learn%2520from%2520imperfect%250Adatasets%2520with%2520long-tailed%2520imbalances%2520and%2520distribution%2520shifts.%2520For%2520deep%2520models%250Ato%2520be%2520widely%2520used%252C%2520getting%2520away%2520with%2520the%2520costly%2520curation%2520process%2520by%2520developing%250Arobust%2520algorithms%2520that%2520can%2520learn%2520from%2520real-world%2520data%2520distribution%2520is%250Anecessary.%2520Toward%2520this%2520goal%252C%2520we%2520develop%2520practical%2520algorithms%2520for%2520Deep%2520Neural%250ANetworks%2520that%2520can%2520learn%2520from%2520limited%2520and%2520imperfect%2520data%2520present%2520in%2520the%2520real%250Aworld.%2520These%2520works%2520are%2520divided%2520into%2520four%2520segments%252C%2520each%2520covering%2520a%2520scenario%2520of%250Alearning%2520from%2520limited%2520or%2520imperfect%2520data.%2520The%2520first%2520part%2520of%2520the%2520works%2520focuses%2520on%250ALearning%2520Generative%2520Models%2520for%2520Long-Tail%2520Data%252C%2520where%2520we%2520mitigate%2520the%250Amode-collapse%2520for%2520tail%2520%2528minority%2529%2520classes%2520and%2520enable%2520diverse%2520aesthetic%2520image%250Agenerations%2520as%2520head%2520%2528majority%2529%2520classes.%2520In%2520the%2520second%2520part%252C%2520we%2520enable%2520effective%250Ageneralization%2520on%2520tail%2520classes%2520through%2520Inductive%2520Regularization%2520schemes%252C%2520which%250Aallow%2520tail%2520classes%2520to%2520generalize%2520as%2520the%2520head%2520classes%2520without%2520enforcing%2520explicit%250Ageneration%2520of%2520images.%2520In%2520the%2520third%2520part%252C%2520we%2520develop%2520algorithms%2520for%2520Optimizing%250ARelevant%2520Metrics%2520compared%2520to%2520the%2520average%2520accuracy%2520for%2520learning%2520from%2520long-tailed%250Adata%2520with%2520limited%2520annotation%2520%2528semi-supervised%2529%252C%2520followed%2520by%2520the%2520fourth%2520part%252C%250Awhich%2520focuses%2520on%2520the%2520effective%2520domain%2520adaptation%2520of%2520the%2520model%2520to%2520various%250Adomains%2520with%2520zero%2520to%2520very%2520few%2520labeled%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Limited%20and%20Imperfect%20Data&entry.906535625=Harsh%20Rangwani&entry.1292438233=%20%20The%20datasets%20used%20for%20Deep%20Neural%20Network%20training%20%28e.g.%2C%20ImageNet%2C%20MSCOCO%2C%0Aetc.%29%20are%20often%20manually%20balanced%20across%20categories%20%28classes%29%20to%20facilitate%0Alearning%20of%20all%20the%20categories.%20This%20curation%20process%20is%20often%20expensive%20and%0Arequires%20throwing%20away%20precious%20annotated%20data%20to%20balance%20the%20frequency%20across%0Aclasses.%20This%20is%20because%20the%20distribution%20of%20data%20in%20the%20world%20%28e.g.%2C%20internet%2C%0Aetc.%29%20significantly%20differs%20from%20the%20well-curated%20datasets%20and%20is%20often%0Aover-populated%20with%20samples%20from%20common%20categories.%20The%20algorithms%20designed%20for%0Awell-curated%20datasets%20perform%20suboptimally%20when%20used%20to%20learn%20from%20imperfect%0Adatasets%20with%20long-tailed%20imbalances%20and%20distribution%20shifts.%20For%20deep%20models%0Ato%20be%20widely%20used%2C%20getting%20away%20with%20the%20costly%20curation%20process%20by%20developing%0Arobust%20algorithms%20that%20can%20learn%20from%20real-world%20data%20distribution%20is%0Anecessary.%20Toward%20this%20goal%2C%20we%20develop%20practical%20algorithms%20for%20Deep%20Neural%0ANetworks%20that%20can%20learn%20from%20limited%20and%20imperfect%20data%20present%20in%20the%20real%0Aworld.%20These%20works%20are%20divided%20into%20four%20segments%2C%20each%20covering%20a%20scenario%20of%0Alearning%20from%20limited%20or%20imperfect%20data.%20The%20first%20part%20of%20the%20works%20focuses%20on%0ALearning%20Generative%20Models%20for%20Long-Tail%20Data%2C%20where%20we%20mitigate%20the%0Amode-collapse%20for%20tail%20%28minority%29%20classes%20and%20enable%20diverse%20aesthetic%20image%0Agenerations%20as%20head%20%28majority%29%20classes.%20In%20the%20second%20part%2C%20we%20enable%20effective%0Ageneralization%20on%20tail%20classes%20through%20Inductive%20Regularization%20schemes%2C%20which%0Aallow%20tail%20classes%20to%20generalize%20as%20the%20head%20classes%20without%20enforcing%20explicit%0Ageneration%20of%20images.%20In%20the%20third%20part%2C%20we%20develop%20algorithms%20for%20Optimizing%0ARelevant%20Metrics%20compared%20to%20the%20average%20accuracy%20for%20learning%20from%20long-tailed%0Adata%20with%20limited%20annotation%20%28semi-supervised%29%2C%20followed%20by%20the%20fourth%20part%2C%0Awhich%20focuses%20on%20the%20effective%20domain%20adaptation%20of%20the%20model%20to%20various%0Adomains%20with%20zero%20to%20very%20few%20labeled%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07229v1&entry.124074799=Read"},
{"title": "SIESEF-FusionNet: Spatial Inter-correlation Enhancement and\n  Spatially-Embedded Feature Fusion Network for LiDAR Point Cloud Semantic\n  Segmentation", "author": "Jiale Chen and Fei Xia and Jianliang Mao and Haoping Wang and Chuanlin Zhang", "abstract": "  The ambiguity at the boundaries of different semantic classes in point cloud\nsemantic segmentation often leads to incorrect decisions in intelligent\nperception systems, such as autonomous driving. Hence, accurate delineation of\nthe boundaries is crucial for improving safety in autonomous driving. A novel\nspatial inter-correlation enhancement and spatially-embedded feature fusion\nnetwork (SIESEF-FusionNet) is proposed in this paper, enhancing spatial\ninter-correlation by combining inverse distance weighting and angular\ncompensation to extract more beneficial spatial information without causing\nredundancy. Meanwhile, a new spatial adaptive pooling module is also designed,\nembedding enhanced spatial information into semantic features for strengthening\nthe context-awareness of semantic features. Experimental results demonstrate\nthat 83.7% mIoU and 97.8% OA are achieved by SIESEF-FusionNet on the Toronto3D\ndataset, with performance superior to other baseline methods. A value of 61.1%\nmIoU is reached on the semanticKITTI dataset, where a marked improvement in\nsegmentation performance is observed. In addition, the effectiveness and\nplug-and-play capability of the proposed modules are further verified through\nablation studies.\n", "link": "http://arxiv.org/abs/2411.06991v1", "date": "2024-11-11", "relevancy": 2.79, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5807}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5549}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIESEF-FusionNet%3A%20Spatial%20Inter-correlation%20Enhancement%20and%0A%20%20Spatially-Embedded%20Feature%20Fusion%20Network%20for%20LiDAR%20Point%20Cloud%20Semantic%0A%20%20Segmentation&body=Title%3A%20SIESEF-FusionNet%3A%20Spatial%20Inter-correlation%20Enhancement%20and%0A%20%20Spatially-Embedded%20Feature%20Fusion%20Network%20for%20LiDAR%20Point%20Cloud%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Jiale%20Chen%20and%20Fei%20Xia%20and%20Jianliang%20Mao%20and%20Haoping%20Wang%20and%20Chuanlin%20Zhang%0AAbstract%3A%20%20%20The%20ambiguity%20at%20the%20boundaries%20of%20different%20semantic%20classes%20in%20point%20cloud%0Asemantic%20segmentation%20often%20leads%20to%20incorrect%20decisions%20in%20intelligent%0Aperception%20systems%2C%20such%20as%20autonomous%20driving.%20Hence%2C%20accurate%20delineation%20of%0Athe%20boundaries%20is%20crucial%20for%20improving%20safety%20in%20autonomous%20driving.%20A%20novel%0Aspatial%20inter-correlation%20enhancement%20and%20spatially-embedded%20feature%20fusion%0Anetwork%20%28SIESEF-FusionNet%29%20is%20proposed%20in%20this%20paper%2C%20enhancing%20spatial%0Ainter-correlation%20by%20combining%20inverse%20distance%20weighting%20and%20angular%0Acompensation%20to%20extract%20more%20beneficial%20spatial%20information%20without%20causing%0Aredundancy.%20Meanwhile%2C%20a%20new%20spatial%20adaptive%20pooling%20module%20is%20also%20designed%2C%0Aembedding%20enhanced%20spatial%20information%20into%20semantic%20features%20for%20strengthening%0Athe%20context-awareness%20of%20semantic%20features.%20Experimental%20results%20demonstrate%0Athat%2083.7%25%20mIoU%20and%2097.8%25%20OA%20are%20achieved%20by%20SIESEF-FusionNet%20on%20the%20Toronto3D%0Adataset%2C%20with%20performance%20superior%20to%20other%20baseline%20methods.%20A%20value%20of%2061.1%25%0AmIoU%20is%20reached%20on%20the%20semanticKITTI%20dataset%2C%20where%20a%20marked%20improvement%20in%0Asegmentation%20performance%20is%20observed.%20In%20addition%2C%20the%20effectiveness%20and%0Aplug-and-play%20capability%20of%20the%20proposed%20modules%20are%20further%20verified%20through%0Aablation%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIESEF-FusionNet%253A%2520Spatial%2520Inter-correlation%2520Enhancement%2520and%250A%2520%2520Spatially-Embedded%2520Feature%2520Fusion%2520Network%2520for%2520LiDAR%2520Point%2520Cloud%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DJiale%2520Chen%2520and%2520Fei%2520Xia%2520and%2520Jianliang%2520Mao%2520and%2520Haoping%2520Wang%2520and%2520Chuanlin%2520Zhang%26entry.1292438233%3D%2520%2520The%2520ambiguity%2520at%2520the%2520boundaries%2520of%2520different%2520semantic%2520classes%2520in%2520point%2520cloud%250Asemantic%2520segmentation%2520often%2520leads%2520to%2520incorrect%2520decisions%2520in%2520intelligent%250Aperception%2520systems%252C%2520such%2520as%2520autonomous%2520driving.%2520Hence%252C%2520accurate%2520delineation%2520of%250Athe%2520boundaries%2520is%2520crucial%2520for%2520improving%2520safety%2520in%2520autonomous%2520driving.%2520A%2520novel%250Aspatial%2520inter-correlation%2520enhancement%2520and%2520spatially-embedded%2520feature%2520fusion%250Anetwork%2520%2528SIESEF-FusionNet%2529%2520is%2520proposed%2520in%2520this%2520paper%252C%2520enhancing%2520spatial%250Ainter-correlation%2520by%2520combining%2520inverse%2520distance%2520weighting%2520and%2520angular%250Acompensation%2520to%2520extract%2520more%2520beneficial%2520spatial%2520information%2520without%2520causing%250Aredundancy.%2520Meanwhile%252C%2520a%2520new%2520spatial%2520adaptive%2520pooling%2520module%2520is%2520also%2520designed%252C%250Aembedding%2520enhanced%2520spatial%2520information%2520into%2520semantic%2520features%2520for%2520strengthening%250Athe%2520context-awareness%2520of%2520semantic%2520features.%2520Experimental%2520results%2520demonstrate%250Athat%252083.7%2525%2520mIoU%2520and%252097.8%2525%2520OA%2520are%2520achieved%2520by%2520SIESEF-FusionNet%2520on%2520the%2520Toronto3D%250Adataset%252C%2520with%2520performance%2520superior%2520to%2520other%2520baseline%2520methods.%2520A%2520value%2520of%252061.1%2525%250AmIoU%2520is%2520reached%2520on%2520the%2520semanticKITTI%2520dataset%252C%2520where%2520a%2520marked%2520improvement%2520in%250Asegmentation%2520performance%2520is%2520observed.%2520In%2520addition%252C%2520the%2520effectiveness%2520and%250Aplug-and-play%2520capability%2520of%2520the%2520proposed%2520modules%2520are%2520further%2520verified%2520through%250Aablation%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIESEF-FusionNet%3A%20Spatial%20Inter-correlation%20Enhancement%20and%0A%20%20Spatially-Embedded%20Feature%20Fusion%20Network%20for%20LiDAR%20Point%20Cloud%20Semantic%0A%20%20Segmentation&entry.906535625=Jiale%20Chen%20and%20Fei%20Xia%20and%20Jianliang%20Mao%20and%20Haoping%20Wang%20and%20Chuanlin%20Zhang&entry.1292438233=%20%20The%20ambiguity%20at%20the%20boundaries%20of%20different%20semantic%20classes%20in%20point%20cloud%0Asemantic%20segmentation%20often%20leads%20to%20incorrect%20decisions%20in%20intelligent%0Aperception%20systems%2C%20such%20as%20autonomous%20driving.%20Hence%2C%20accurate%20delineation%20of%0Athe%20boundaries%20is%20crucial%20for%20improving%20safety%20in%20autonomous%20driving.%20A%20novel%0Aspatial%20inter-correlation%20enhancement%20and%20spatially-embedded%20feature%20fusion%0Anetwork%20%28SIESEF-FusionNet%29%20is%20proposed%20in%20this%20paper%2C%20enhancing%20spatial%0Ainter-correlation%20by%20combining%20inverse%20distance%20weighting%20and%20angular%0Acompensation%20to%20extract%20more%20beneficial%20spatial%20information%20without%20causing%0Aredundancy.%20Meanwhile%2C%20a%20new%20spatial%20adaptive%20pooling%20module%20is%20also%20designed%2C%0Aembedding%20enhanced%20spatial%20information%20into%20semantic%20features%20for%20strengthening%0Athe%20context-awareness%20of%20semantic%20features.%20Experimental%20results%20demonstrate%0Athat%2083.7%25%20mIoU%20and%2097.8%25%20OA%20are%20achieved%20by%20SIESEF-FusionNet%20on%20the%20Toronto3D%0Adataset%2C%20with%20performance%20superior%20to%20other%20baseline%20methods.%20A%20value%20of%2061.1%25%0AmIoU%20is%20reached%20on%20the%20semanticKITTI%20dataset%2C%20where%20a%20marked%20improvement%20in%0Asegmentation%20performance%20is%20observed.%20In%20addition%2C%20the%20effectiveness%20and%0Aplug-and-play%20capability%20of%20the%20proposed%20modules%20are%20further%20verified%20through%0Aablation%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06991v1&entry.124074799=Read"},
{"title": "Token Merging for Training-Free Semantic Binding in Text-to-Image\n  Synthesis", "author": "Taihang Hu and Linxuan Li and Joost van de Weijer and Hongcheng Gao and Fahad Shahbaz Khan and Jian Yang and Ming-Ming Cheng and Kai Wang and Yaxing Wang", "abstract": "  Although text-to-image (T2I) models exhibit remarkable generation\ncapabilities, they frequently fail to accurately bind semantically related\nobjects or attributes in the input prompts; a challenge termed semantic\nbinding. Previous approaches either involve intensive fine-tuning of the entire\nT2I model or require users or large language models to specify generation\nlayouts, adding complexity. In this paper, we define semantic binding as the\ntask of associating a given object with its attribute, termed attribute\nbinding, or linking it to other related sub-objects, referred to as object\nbinding. We introduce a novel method called Token Merging (ToMe), which\nenhances semantic binding by aggregating relevant tokens into a single\ncomposite token. This ensures that the object, its attributes and sub-objects\nall share the same cross-attention map. Additionally, to address potential\nconfusion among main objects with complex textual prompts, we propose end token\nsubstitution as a complementary strategy. To further refine our approach in the\ninitial stages of T2I generation, where layouts are determined, we incorporate\ntwo auxiliary losses, an entropy loss and a semantic binding loss, to\niteratively update the composite token to improve the generation integrity. We\nconducted extensive experiments to validate the effectiveness of ToMe,\ncomparing it against various existing methods on the T2I-CompBench and our\nproposed GPT-4o object binding benchmark. Our method is particularly effective\nin complex scenarios that involve multiple objects and attributes, which\nprevious methods often fail to address. The code will be publicly available at\n\\url{https://github.com/hutaihang/ToMe}.\n", "link": "http://arxiv.org/abs/2411.07132v1", "date": "2024-11-11", "relevancy": 2.7693, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.564}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.559}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token%20Merging%20for%20Training-Free%20Semantic%20Binding%20in%20Text-to-Image%0A%20%20Synthesis&body=Title%3A%20Token%20Merging%20for%20Training-Free%20Semantic%20Binding%20in%20Text-to-Image%0A%20%20Synthesis%0AAuthor%3A%20Taihang%20Hu%20and%20Linxuan%20Li%20and%20Joost%20van%20de%20Weijer%20and%20Hongcheng%20Gao%20and%20Fahad%20Shahbaz%20Khan%20and%20Jian%20Yang%20and%20Ming-Ming%20Cheng%20and%20Kai%20Wang%20and%20Yaxing%20Wang%0AAbstract%3A%20%20%20Although%20text-to-image%20%28T2I%29%20models%20exhibit%20remarkable%20generation%0Acapabilities%2C%20they%20frequently%20fail%20to%20accurately%20bind%20semantically%20related%0Aobjects%20or%20attributes%20in%20the%20input%20prompts%3B%20a%20challenge%20termed%20semantic%0Abinding.%20Previous%20approaches%20either%20involve%20intensive%20fine-tuning%20of%20the%20entire%0AT2I%20model%20or%20require%20users%20or%20large%20language%20models%20to%20specify%20generation%0Alayouts%2C%20adding%20complexity.%20In%20this%20paper%2C%20we%20define%20semantic%20binding%20as%20the%0Atask%20of%20associating%20a%20given%20object%20with%20its%20attribute%2C%20termed%20attribute%0Abinding%2C%20or%20linking%20it%20to%20other%20related%20sub-objects%2C%20referred%20to%20as%20object%0Abinding.%20We%20introduce%20a%20novel%20method%20called%20Token%20Merging%20%28ToMe%29%2C%20which%0Aenhances%20semantic%20binding%20by%20aggregating%20relevant%20tokens%20into%20a%20single%0Acomposite%20token.%20This%20ensures%20that%20the%20object%2C%20its%20attributes%20and%20sub-objects%0Aall%20share%20the%20same%20cross-attention%20map.%20Additionally%2C%20to%20address%20potential%0Aconfusion%20among%20main%20objects%20with%20complex%20textual%20prompts%2C%20we%20propose%20end%20token%0Asubstitution%20as%20a%20complementary%20strategy.%20To%20further%20refine%20our%20approach%20in%20the%0Ainitial%20stages%20of%20T2I%20generation%2C%20where%20layouts%20are%20determined%2C%20we%20incorporate%0Atwo%20auxiliary%20losses%2C%20an%20entropy%20loss%20and%20a%20semantic%20binding%20loss%2C%20to%0Aiteratively%20update%20the%20composite%20token%20to%20improve%20the%20generation%20integrity.%20We%0Aconducted%20extensive%20experiments%20to%20validate%20the%20effectiveness%20of%20ToMe%2C%0Acomparing%20it%20against%20various%20existing%20methods%20on%20the%20T2I-CompBench%20and%20our%0Aproposed%20GPT-4o%20object%20binding%20benchmark.%20Our%20method%20is%20particularly%20effective%0Ain%20complex%20scenarios%20that%20involve%20multiple%20objects%20and%20attributes%2C%20which%0Aprevious%20methods%20often%20fail%20to%20address.%20The%20code%20will%20be%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/hutaihang/ToMe%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken%2520Merging%2520for%2520Training-Free%2520Semantic%2520Binding%2520in%2520Text-to-Image%250A%2520%2520Synthesis%26entry.906535625%3DTaihang%2520Hu%2520and%2520Linxuan%2520Li%2520and%2520Joost%2520van%2520de%2520Weijer%2520and%2520Hongcheng%2520Gao%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Jian%2520Yang%2520and%2520Ming-Ming%2520Cheng%2520and%2520Kai%2520Wang%2520and%2520Yaxing%2520Wang%26entry.1292438233%3D%2520%2520Although%2520text-to-image%2520%2528T2I%2529%2520models%2520exhibit%2520remarkable%2520generation%250Acapabilities%252C%2520they%2520frequently%2520fail%2520to%2520accurately%2520bind%2520semantically%2520related%250Aobjects%2520or%2520attributes%2520in%2520the%2520input%2520prompts%253B%2520a%2520challenge%2520termed%2520semantic%250Abinding.%2520Previous%2520approaches%2520either%2520involve%2520intensive%2520fine-tuning%2520of%2520the%2520entire%250AT2I%2520model%2520or%2520require%2520users%2520or%2520large%2520language%2520models%2520to%2520specify%2520generation%250Alayouts%252C%2520adding%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520define%2520semantic%2520binding%2520as%2520the%250Atask%2520of%2520associating%2520a%2520given%2520object%2520with%2520its%2520attribute%252C%2520termed%2520attribute%250Abinding%252C%2520or%2520linking%2520it%2520to%2520other%2520related%2520sub-objects%252C%2520referred%2520to%2520as%2520object%250Abinding.%2520We%2520introduce%2520a%2520novel%2520method%2520called%2520Token%2520Merging%2520%2528ToMe%2529%252C%2520which%250Aenhances%2520semantic%2520binding%2520by%2520aggregating%2520relevant%2520tokens%2520into%2520a%2520single%250Acomposite%2520token.%2520This%2520ensures%2520that%2520the%2520object%252C%2520its%2520attributes%2520and%2520sub-objects%250Aall%2520share%2520the%2520same%2520cross-attention%2520map.%2520Additionally%252C%2520to%2520address%2520potential%250Aconfusion%2520among%2520main%2520objects%2520with%2520complex%2520textual%2520prompts%252C%2520we%2520propose%2520end%2520token%250Asubstitution%2520as%2520a%2520complementary%2520strategy.%2520To%2520further%2520refine%2520our%2520approach%2520in%2520the%250Ainitial%2520stages%2520of%2520T2I%2520generation%252C%2520where%2520layouts%2520are%2520determined%252C%2520we%2520incorporate%250Atwo%2520auxiliary%2520losses%252C%2520an%2520entropy%2520loss%2520and%2520a%2520semantic%2520binding%2520loss%252C%2520to%250Aiteratively%2520update%2520the%2520composite%2520token%2520to%2520improve%2520the%2520generation%2520integrity.%2520We%250Aconducted%2520extensive%2520experiments%2520to%2520validate%2520the%2520effectiveness%2520of%2520ToMe%252C%250Acomparing%2520it%2520against%2520various%2520existing%2520methods%2520on%2520the%2520T2I-CompBench%2520and%2520our%250Aproposed%2520GPT-4o%2520object%2520binding%2520benchmark.%2520Our%2520method%2520is%2520particularly%2520effective%250Ain%2520complex%2520scenarios%2520that%2520involve%2520multiple%2520objects%2520and%2520attributes%252C%2520which%250Aprevious%2520methods%2520often%2520fail%2520to%2520address.%2520The%2520code%2520will%2520be%2520publicly%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/hutaihang/ToMe%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Merging%20for%20Training-Free%20Semantic%20Binding%20in%20Text-to-Image%0A%20%20Synthesis&entry.906535625=Taihang%20Hu%20and%20Linxuan%20Li%20and%20Joost%20van%20de%20Weijer%20and%20Hongcheng%20Gao%20and%20Fahad%20Shahbaz%20Khan%20and%20Jian%20Yang%20and%20Ming-Ming%20Cheng%20and%20Kai%20Wang%20and%20Yaxing%20Wang&entry.1292438233=%20%20Although%20text-to-image%20%28T2I%29%20models%20exhibit%20remarkable%20generation%0Acapabilities%2C%20they%20frequently%20fail%20to%20accurately%20bind%20semantically%20related%0Aobjects%20or%20attributes%20in%20the%20input%20prompts%3B%20a%20challenge%20termed%20semantic%0Abinding.%20Previous%20approaches%20either%20involve%20intensive%20fine-tuning%20of%20the%20entire%0AT2I%20model%20or%20require%20users%20or%20large%20language%20models%20to%20specify%20generation%0Alayouts%2C%20adding%20complexity.%20In%20this%20paper%2C%20we%20define%20semantic%20binding%20as%20the%0Atask%20of%20associating%20a%20given%20object%20with%20its%20attribute%2C%20termed%20attribute%0Abinding%2C%20or%20linking%20it%20to%20other%20related%20sub-objects%2C%20referred%20to%20as%20object%0Abinding.%20We%20introduce%20a%20novel%20method%20called%20Token%20Merging%20%28ToMe%29%2C%20which%0Aenhances%20semantic%20binding%20by%20aggregating%20relevant%20tokens%20into%20a%20single%0Acomposite%20token.%20This%20ensures%20that%20the%20object%2C%20its%20attributes%20and%20sub-objects%0Aall%20share%20the%20same%20cross-attention%20map.%20Additionally%2C%20to%20address%20potential%0Aconfusion%20among%20main%20objects%20with%20complex%20textual%20prompts%2C%20we%20propose%20end%20token%0Asubstitution%20as%20a%20complementary%20strategy.%20To%20further%20refine%20our%20approach%20in%20the%0Ainitial%20stages%20of%20T2I%20generation%2C%20where%20layouts%20are%20determined%2C%20we%20incorporate%0Atwo%20auxiliary%20losses%2C%20an%20entropy%20loss%20and%20a%20semantic%20binding%20loss%2C%20to%0Aiteratively%20update%20the%20composite%20token%20to%20improve%20the%20generation%20integrity.%20We%0Aconducted%20extensive%20experiments%20to%20validate%20the%20effectiveness%20of%20ToMe%2C%0Acomparing%20it%20against%20various%20existing%20methods%20on%20the%20T2I-CompBench%20and%20our%0Aproposed%20GPT-4o%20object%20binding%20benchmark.%20Our%20method%20is%20particularly%20effective%0Ain%20complex%20scenarios%20that%20involve%20multiple%20objects%20and%20attributes%2C%20which%0Aprevious%20methods%20often%20fail%20to%20address.%20The%20code%20will%20be%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/hutaihang/ToMe%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07132v1&entry.124074799=Read"},
{"title": "Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI", "author": "Bruno Viti and Franz Thaler and Kathrin Lisa Kapper and Martin Urschler and Martin Holler and Elias Karabelas", "abstract": "  Segmentation of cardiac magnetic resonance images (MRI) is crucial for the\nanalysis and assessment of cardiac function, helping to diagnose and treat\nvarious cardiovascular diseases. Most recent techniques rely on deep learning\nand usually require an extensive amount of labeled data. To overcome this\nproblem, few-shot learning has the capability of reducing data dependency on\nlabeled data. In this work, we introduce a new method that merges few-shot\nlearning with a U-Net architecture and Gaussian Process Emulators (GPEs),\nenhancing data integration from a support set for improved performance. GPEs\nare trained to learn the relation between the support images and the\ncorresponding masks in latent space, facilitating the segmentation of unseen\nquery images given only a small labeled support set at inference. We test our\nmodel with the M&Ms-2 public dataset to assess its ability to segment the heart\nin cardiac magnetic resonance imaging from different orientations, and compare\nit with state-of-the-art unsupervised and few-shot methods. Our architecture\nshows higher DICE coefficients compared to these methods, especially in the\nmore challenging setups where the size of the support set is considerably\nsmall.\n", "link": "http://arxiv.org/abs/2411.06911v1", "date": "2024-11-11", "relevancy": 2.7671, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5739}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5495}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Process%20Emulators%20for%20Few-Shot%20Segmentation%20in%20Cardiac%20MRI&body=Title%3A%20Gaussian%20Process%20Emulators%20for%20Few-Shot%20Segmentation%20in%20Cardiac%20MRI%0AAuthor%3A%20Bruno%20Viti%20and%20Franz%20Thaler%20and%20Kathrin%20Lisa%20Kapper%20and%20Martin%20Urschler%20and%20Martin%20Holler%20and%20Elias%20Karabelas%0AAbstract%3A%20%20%20Segmentation%20of%20cardiac%20magnetic%20resonance%20images%20%28MRI%29%20is%20crucial%20for%20the%0Aanalysis%20and%20assessment%20of%20cardiac%20function%2C%20helping%20to%20diagnose%20and%20treat%0Avarious%20cardiovascular%20diseases.%20Most%20recent%20techniques%20rely%20on%20deep%20learning%0Aand%20usually%20require%20an%20extensive%20amount%20of%20labeled%20data.%20To%20overcome%20this%0Aproblem%2C%20few-shot%20learning%20has%20the%20capability%20of%20reducing%20data%20dependency%20on%0Alabeled%20data.%20In%20this%20work%2C%20we%20introduce%20a%20new%20method%20that%20merges%20few-shot%0Alearning%20with%20a%20U-Net%20architecture%20and%20Gaussian%20Process%20Emulators%20%28GPEs%29%2C%0Aenhancing%20data%20integration%20from%20a%20support%20set%20for%20improved%20performance.%20GPEs%0Aare%20trained%20to%20learn%20the%20relation%20between%20the%20support%20images%20and%20the%0Acorresponding%20masks%20in%20latent%20space%2C%20facilitating%20the%20segmentation%20of%20unseen%0Aquery%20images%20given%20only%20a%20small%20labeled%20support%20set%20at%20inference.%20We%20test%20our%0Amodel%20with%20the%20M%26Ms-2%20public%20dataset%20to%20assess%20its%20ability%20to%20segment%20the%20heart%0Ain%20cardiac%20magnetic%20resonance%20imaging%20from%20different%20orientations%2C%20and%20compare%0Ait%20with%20state-of-the-art%20unsupervised%20and%20few-shot%20methods.%20Our%20architecture%0Ashows%20higher%20DICE%20coefficients%20compared%20to%20these%20methods%2C%20especially%20in%20the%0Amore%20challenging%20setups%20where%20the%20size%20of%20the%20support%20set%20is%20considerably%0Asmall.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Process%2520Emulators%2520for%2520Few-Shot%2520Segmentation%2520in%2520Cardiac%2520MRI%26entry.906535625%3DBruno%2520Viti%2520and%2520Franz%2520Thaler%2520and%2520Kathrin%2520Lisa%2520Kapper%2520and%2520Martin%2520Urschler%2520and%2520Martin%2520Holler%2520and%2520Elias%2520Karabelas%26entry.1292438233%3D%2520%2520Segmentation%2520of%2520cardiac%2520magnetic%2520resonance%2520images%2520%2528MRI%2529%2520is%2520crucial%2520for%2520the%250Aanalysis%2520and%2520assessment%2520of%2520cardiac%2520function%252C%2520helping%2520to%2520diagnose%2520and%2520treat%250Avarious%2520cardiovascular%2520diseases.%2520Most%2520recent%2520techniques%2520rely%2520on%2520deep%2520learning%250Aand%2520usually%2520require%2520an%2520extensive%2520amount%2520of%2520labeled%2520data.%2520To%2520overcome%2520this%250Aproblem%252C%2520few-shot%2520learning%2520has%2520the%2520capability%2520of%2520reducing%2520data%2520dependency%2520on%250Alabeled%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520method%2520that%2520merges%2520few-shot%250Alearning%2520with%2520a%2520U-Net%2520architecture%2520and%2520Gaussian%2520Process%2520Emulators%2520%2528GPEs%2529%252C%250Aenhancing%2520data%2520integration%2520from%2520a%2520support%2520set%2520for%2520improved%2520performance.%2520GPEs%250Aare%2520trained%2520to%2520learn%2520the%2520relation%2520between%2520the%2520support%2520images%2520and%2520the%250Acorresponding%2520masks%2520in%2520latent%2520space%252C%2520facilitating%2520the%2520segmentation%2520of%2520unseen%250Aquery%2520images%2520given%2520only%2520a%2520small%2520labeled%2520support%2520set%2520at%2520inference.%2520We%2520test%2520our%250Amodel%2520with%2520the%2520M%2526Ms-2%2520public%2520dataset%2520to%2520assess%2520its%2520ability%2520to%2520segment%2520the%2520heart%250Ain%2520cardiac%2520magnetic%2520resonance%2520imaging%2520from%2520different%2520orientations%252C%2520and%2520compare%250Ait%2520with%2520state-of-the-art%2520unsupervised%2520and%2520few-shot%2520methods.%2520Our%2520architecture%250Ashows%2520higher%2520DICE%2520coefficients%2520compared%2520to%2520these%2520methods%252C%2520especially%2520in%2520the%250Amore%2520challenging%2520setups%2520where%2520the%2520size%2520of%2520the%2520support%2520set%2520is%2520considerably%250Asmall.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Process%20Emulators%20for%20Few-Shot%20Segmentation%20in%20Cardiac%20MRI&entry.906535625=Bruno%20Viti%20and%20Franz%20Thaler%20and%20Kathrin%20Lisa%20Kapper%20and%20Martin%20Urschler%20and%20Martin%20Holler%20and%20Elias%20Karabelas&entry.1292438233=%20%20Segmentation%20of%20cardiac%20magnetic%20resonance%20images%20%28MRI%29%20is%20crucial%20for%20the%0Aanalysis%20and%20assessment%20of%20cardiac%20function%2C%20helping%20to%20diagnose%20and%20treat%0Avarious%20cardiovascular%20diseases.%20Most%20recent%20techniques%20rely%20on%20deep%20learning%0Aand%20usually%20require%20an%20extensive%20amount%20of%20labeled%20data.%20To%20overcome%20this%0Aproblem%2C%20few-shot%20learning%20has%20the%20capability%20of%20reducing%20data%20dependency%20on%0Alabeled%20data.%20In%20this%20work%2C%20we%20introduce%20a%20new%20method%20that%20merges%20few-shot%0Alearning%20with%20a%20U-Net%20architecture%20and%20Gaussian%20Process%20Emulators%20%28GPEs%29%2C%0Aenhancing%20data%20integration%20from%20a%20support%20set%20for%20improved%20performance.%20GPEs%0Aare%20trained%20to%20learn%20the%20relation%20between%20the%20support%20images%20and%20the%0Acorresponding%20masks%20in%20latent%20space%2C%20facilitating%20the%20segmentation%20of%20unseen%0Aquery%20images%20given%20only%20a%20small%20labeled%20support%20set%20at%20inference.%20We%20test%20our%0Amodel%20with%20the%20M%26Ms-2%20public%20dataset%20to%20assess%20its%20ability%20to%20segment%20the%20heart%0Ain%20cardiac%20magnetic%20resonance%20imaging%20from%20different%20orientations%2C%20and%20compare%0Ait%20with%20state-of-the-art%20unsupervised%20and%20few-shot%20methods.%20Our%20architecture%0Ashows%20higher%20DICE%20coefficients%20compared%20to%20these%20methods%2C%20especially%20in%20the%0Amore%20challenging%20setups%20where%20the%20size%20of%20the%20support%20set%20is%20considerably%0Asmall.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06911v1&entry.124074799=Read"},
{"title": "Fine Structure-Aware Sampling: A New Sampling Training Scheme for\n  Pixel-Aligned Implicit Models in Single-View Human Reconstruction", "author": "Kennard Yanting Chan and Fayao Liu and Guosheng Lin and Chuan Sheng Foo and Weisi Lin", "abstract": "  Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for\nsingle-view clothed human reconstruction. These models need to be trained using\na sampling training scheme. Existing sampling training schemes either fail to\ncapture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in\nreconstructed meshes. To address these problems, we introduce Fine\nStructured-Aware Sampling (FSS), a new sampling training scheme to train\npixel-aligned implicit models for single-view human reconstruction. FSS\nresolves the aforementioned problems by proactively adapting to the thickness\nand complexity of surfaces. In addition, unlike existing sampling training\nschemes, FSS shows how normals of sample points can be capitalized in the\ntraining process to improve results. Lastly, to further improve the training\nprocess, FSS proposes a mesh thickness loss signal for pixel-aligned implicit\nmodels. It becomes computationally feasible to introduce this loss once a\nslight reworking of the pixel-aligned implicit function framework is carried\nout. Our results show that our methods significantly outperform SOTA methods\nqualitatively and quantitatively. Our code is publicly available at\nhttps://github.com/kcyt/FSS.\n", "link": "http://arxiv.org/abs/2402.19197v2", "date": "2024-11-11", "relevancy": 2.7616, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5863}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5379}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine%20Structure-Aware%20Sampling%3A%20A%20New%20Sampling%20Training%20Scheme%20for%0A%20%20Pixel-Aligned%20Implicit%20Models%20in%20Single-View%20Human%20Reconstruction&body=Title%3A%20Fine%20Structure-Aware%20Sampling%3A%20A%20New%20Sampling%20Training%20Scheme%20for%0A%20%20Pixel-Aligned%20Implicit%20Models%20in%20Single-View%20Human%20Reconstruction%0AAuthor%3A%20Kennard%20Yanting%20Chan%20and%20Fayao%20Liu%20and%20Guosheng%20Lin%20and%20Chuan%20Sheng%20Foo%20and%20Weisi%20Lin%0AAbstract%3A%20%20%20Pixel-aligned%20implicit%20models%2C%20such%20as%20PIFu%2C%20PIFuHD%2C%20and%20ICON%2C%20are%20used%20for%0Asingle-view%20clothed%20human%20reconstruction.%20These%20models%20need%20to%20be%20trained%20using%0Aa%20sampling%20training%20scheme.%20Existing%20sampling%20training%20schemes%20either%20fail%20to%0Acapture%20thin%20surfaces%20%28e.g.%20ears%2C%20fingers%29%20or%20cause%20noisy%20artefacts%20in%0Areconstructed%20meshes.%20To%20address%20these%20problems%2C%20we%20introduce%20Fine%0AStructured-Aware%20Sampling%20%28FSS%29%2C%20a%20new%20sampling%20training%20scheme%20to%20train%0Apixel-aligned%20implicit%20models%20for%20single-view%20human%20reconstruction.%20FSS%0Aresolves%20the%20aforementioned%20problems%20by%20proactively%20adapting%20to%20the%20thickness%0Aand%20complexity%20of%20surfaces.%20In%20addition%2C%20unlike%20existing%20sampling%20training%0Aschemes%2C%20FSS%20shows%20how%20normals%20of%20sample%20points%20can%20be%20capitalized%20in%20the%0Atraining%20process%20to%20improve%20results.%20Lastly%2C%20to%20further%20improve%20the%20training%0Aprocess%2C%20FSS%20proposes%20a%20mesh%20thickness%20loss%20signal%20for%20pixel-aligned%20implicit%0Amodels.%20It%20becomes%20computationally%20feasible%20to%20introduce%20this%20loss%20once%20a%0Aslight%20reworking%20of%20the%20pixel-aligned%20implicit%20function%20framework%20is%20carried%0Aout.%20Our%20results%20show%20that%20our%20methods%20significantly%20outperform%20SOTA%20methods%0Aqualitatively%20and%20quantitatively.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/kcyt/FSS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19197v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine%2520Structure-Aware%2520Sampling%253A%2520A%2520New%2520Sampling%2520Training%2520Scheme%2520for%250A%2520%2520Pixel-Aligned%2520Implicit%2520Models%2520in%2520Single-View%2520Human%2520Reconstruction%26entry.906535625%3DKennard%2520Yanting%2520Chan%2520and%2520Fayao%2520Liu%2520and%2520Guosheng%2520Lin%2520and%2520Chuan%2520Sheng%2520Foo%2520and%2520Weisi%2520Lin%26entry.1292438233%3D%2520%2520Pixel-aligned%2520implicit%2520models%252C%2520such%2520as%2520PIFu%252C%2520PIFuHD%252C%2520and%2520ICON%252C%2520are%2520used%2520for%250Asingle-view%2520clothed%2520human%2520reconstruction.%2520These%2520models%2520need%2520to%2520be%2520trained%2520using%250Aa%2520sampling%2520training%2520scheme.%2520Existing%2520sampling%2520training%2520schemes%2520either%2520fail%2520to%250Acapture%2520thin%2520surfaces%2520%2528e.g.%2520ears%252C%2520fingers%2529%2520or%2520cause%2520noisy%2520artefacts%2520in%250Areconstructed%2520meshes.%2520To%2520address%2520these%2520problems%252C%2520we%2520introduce%2520Fine%250AStructured-Aware%2520Sampling%2520%2528FSS%2529%252C%2520a%2520new%2520sampling%2520training%2520scheme%2520to%2520train%250Apixel-aligned%2520implicit%2520models%2520for%2520single-view%2520human%2520reconstruction.%2520FSS%250Aresolves%2520the%2520aforementioned%2520problems%2520by%2520proactively%2520adapting%2520to%2520the%2520thickness%250Aand%2520complexity%2520of%2520surfaces.%2520In%2520addition%252C%2520unlike%2520existing%2520sampling%2520training%250Aschemes%252C%2520FSS%2520shows%2520how%2520normals%2520of%2520sample%2520points%2520can%2520be%2520capitalized%2520in%2520the%250Atraining%2520process%2520to%2520improve%2520results.%2520Lastly%252C%2520to%2520further%2520improve%2520the%2520training%250Aprocess%252C%2520FSS%2520proposes%2520a%2520mesh%2520thickness%2520loss%2520signal%2520for%2520pixel-aligned%2520implicit%250Amodels.%2520It%2520becomes%2520computationally%2520feasible%2520to%2520introduce%2520this%2520loss%2520once%2520a%250Aslight%2520reworking%2520of%2520the%2520pixel-aligned%2520implicit%2520function%2520framework%2520is%2520carried%250Aout.%2520Our%2520results%2520show%2520that%2520our%2520methods%2520significantly%2520outperform%2520SOTA%2520methods%250Aqualitatively%2520and%2520quantitatively.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/kcyt/FSS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19197v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine%20Structure-Aware%20Sampling%3A%20A%20New%20Sampling%20Training%20Scheme%20for%0A%20%20Pixel-Aligned%20Implicit%20Models%20in%20Single-View%20Human%20Reconstruction&entry.906535625=Kennard%20Yanting%20Chan%20and%20Fayao%20Liu%20and%20Guosheng%20Lin%20and%20Chuan%20Sheng%20Foo%20and%20Weisi%20Lin&entry.1292438233=%20%20Pixel-aligned%20implicit%20models%2C%20such%20as%20PIFu%2C%20PIFuHD%2C%20and%20ICON%2C%20are%20used%20for%0Asingle-view%20clothed%20human%20reconstruction.%20These%20models%20need%20to%20be%20trained%20using%0Aa%20sampling%20training%20scheme.%20Existing%20sampling%20training%20schemes%20either%20fail%20to%0Acapture%20thin%20surfaces%20%28e.g.%20ears%2C%20fingers%29%20or%20cause%20noisy%20artefacts%20in%0Areconstructed%20meshes.%20To%20address%20these%20problems%2C%20we%20introduce%20Fine%0AStructured-Aware%20Sampling%20%28FSS%29%2C%20a%20new%20sampling%20training%20scheme%20to%20train%0Apixel-aligned%20implicit%20models%20for%20single-view%20human%20reconstruction.%20FSS%0Aresolves%20the%20aforementioned%20problems%20by%20proactively%20adapting%20to%20the%20thickness%0Aand%20complexity%20of%20surfaces.%20In%20addition%2C%20unlike%20existing%20sampling%20training%0Aschemes%2C%20FSS%20shows%20how%20normals%20of%20sample%20points%20can%20be%20capitalized%20in%20the%0Atraining%20process%20to%20improve%20results.%20Lastly%2C%20to%20further%20improve%20the%20training%0Aprocess%2C%20FSS%20proposes%20a%20mesh%20thickness%20loss%20signal%20for%20pixel-aligned%20implicit%0Amodels.%20It%20becomes%20computationally%20feasible%20to%20introduce%20this%20loss%20once%20a%0Aslight%20reworking%20of%20the%20pixel-aligned%20implicit%20function%20framework%20is%20carried%0Aout.%20Our%20results%20show%20that%20our%20methods%20significantly%20outperform%20SOTA%20methods%0Aqualitatively%20and%20quantitatively.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/kcyt/FSS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19197v2&entry.124074799=Read"},
{"title": "CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal\n  Large Language Models", "author": "Junho Kim and Hyungjin Chung and Byung-Hoon Kim", "abstract": "  Category-agnostic pose estimation (CAPE) has traditionally relied on support\nimages with annotated keypoints, a process that is often cumbersome and may\nfail to fully capture the necessary correspondences across diverse object\ncategories. Recent efforts have begun exploring the use of text-based queries,\nwhere the need for support keypoints is eliminated. However, the optimal use of\ntextual descriptions for keypoints remains an underexplored area. In this work,\nwe introduce CapeLLM, a novel approach that leverages a text-based multimodal\nlarge language model (MLLM) for CAPE. Our method only employs query image and\ndetailed text descriptions as an input to estimate category-agnostic keypoints.\nWe conduct extensive experiments to systematically explore the design space of\nLLM-based CAPE, investigating factors such as choosing the optimal description\nfor keypoints, neural network architectures, and training strategies. Thanks to\nthe advanced reasoning capabilities of the pre-trained MLLM, CapeLLM\ndemonstrates superior generalization and robust performance. Our approach sets\na new state-of-the-art on the MP-100 benchmark in the challenging 1-shot\nsetting, marking a significant advancement in the field of category-agnostic\npose estimation.\n", "link": "http://arxiv.org/abs/2411.06869v1", "date": "2024-11-11", "relevancy": 2.7566, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CapeLLM%3A%20Support-Free%20Category-Agnostic%20Pose%20Estimation%20with%20Multimodal%0A%20%20Large%20Language%20Models&body=Title%3A%20CapeLLM%3A%20Support-Free%20Category-Agnostic%20Pose%20Estimation%20with%20Multimodal%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Junho%20Kim%20and%20Hyungjin%20Chung%20and%20Byung-Hoon%20Kim%0AAbstract%3A%20%20%20Category-agnostic%20pose%20estimation%20%28CAPE%29%20has%20traditionally%20relied%20on%20support%0Aimages%20with%20annotated%20keypoints%2C%20a%20process%20that%20is%20often%20cumbersome%20and%20may%0Afail%20to%20fully%20capture%20the%20necessary%20correspondences%20across%20diverse%20object%0Acategories.%20Recent%20efforts%20have%20begun%20exploring%20the%20use%20of%20text-based%20queries%2C%0Awhere%20the%20need%20for%20support%20keypoints%20is%20eliminated.%20However%2C%20the%20optimal%20use%20of%0Atextual%20descriptions%20for%20keypoints%20remains%20an%20underexplored%20area.%20In%20this%20work%2C%0Awe%20introduce%20CapeLLM%2C%20a%20novel%20approach%20that%20leverages%20a%20text-based%20multimodal%0Alarge%20language%20model%20%28MLLM%29%20for%20CAPE.%20Our%20method%20only%20employs%20query%20image%20and%0Adetailed%20text%20descriptions%20as%20an%20input%20to%20estimate%20category-agnostic%20keypoints.%0AWe%20conduct%20extensive%20experiments%20to%20systematically%20explore%20the%20design%20space%20of%0ALLM-based%20CAPE%2C%20investigating%20factors%20such%20as%20choosing%20the%20optimal%20description%0Afor%20keypoints%2C%20neural%20network%20architectures%2C%20and%20training%20strategies.%20Thanks%20to%0Athe%20advanced%20reasoning%20capabilities%20of%20the%20pre-trained%20MLLM%2C%20CapeLLM%0Ademonstrates%20superior%20generalization%20and%20robust%20performance.%20Our%20approach%20sets%0Aa%20new%20state-of-the-art%20on%20the%20MP-100%20benchmark%20in%20the%20challenging%201-shot%0Asetting%2C%20marking%20a%20significant%20advancement%20in%20the%20field%20of%20category-agnostic%0Apose%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapeLLM%253A%2520Support-Free%2520Category-Agnostic%2520Pose%2520Estimation%2520with%2520Multimodal%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DJunho%2520Kim%2520and%2520Hyungjin%2520Chung%2520and%2520Byung-Hoon%2520Kim%26entry.1292438233%3D%2520%2520Category-agnostic%2520pose%2520estimation%2520%2528CAPE%2529%2520has%2520traditionally%2520relied%2520on%2520support%250Aimages%2520with%2520annotated%2520keypoints%252C%2520a%2520process%2520that%2520is%2520often%2520cumbersome%2520and%2520may%250Afail%2520to%2520fully%2520capture%2520the%2520necessary%2520correspondences%2520across%2520diverse%2520object%250Acategories.%2520Recent%2520efforts%2520have%2520begun%2520exploring%2520the%2520use%2520of%2520text-based%2520queries%252C%250Awhere%2520the%2520need%2520for%2520support%2520keypoints%2520is%2520eliminated.%2520However%252C%2520the%2520optimal%2520use%2520of%250Atextual%2520descriptions%2520for%2520keypoints%2520remains%2520an%2520underexplored%2520area.%2520In%2520this%2520work%252C%250Awe%2520introduce%2520CapeLLM%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520a%2520text-based%2520multimodal%250Alarge%2520language%2520model%2520%2528MLLM%2529%2520for%2520CAPE.%2520Our%2520method%2520only%2520employs%2520query%2520image%2520and%250Adetailed%2520text%2520descriptions%2520as%2520an%2520input%2520to%2520estimate%2520category-agnostic%2520keypoints.%250AWe%2520conduct%2520extensive%2520experiments%2520to%2520systematically%2520explore%2520the%2520design%2520space%2520of%250ALLM-based%2520CAPE%252C%2520investigating%2520factors%2520such%2520as%2520choosing%2520the%2520optimal%2520description%250Afor%2520keypoints%252C%2520neural%2520network%2520architectures%252C%2520and%2520training%2520strategies.%2520Thanks%2520to%250Athe%2520advanced%2520reasoning%2520capabilities%2520of%2520the%2520pre-trained%2520MLLM%252C%2520CapeLLM%250Ademonstrates%2520superior%2520generalization%2520and%2520robust%2520performance.%2520Our%2520approach%2520sets%250Aa%2520new%2520state-of-the-art%2520on%2520the%2520MP-100%2520benchmark%2520in%2520the%2520challenging%25201-shot%250Asetting%252C%2520marking%2520a%2520significant%2520advancement%2520in%2520the%2520field%2520of%2520category-agnostic%250Apose%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CapeLLM%3A%20Support-Free%20Category-Agnostic%20Pose%20Estimation%20with%20Multimodal%0A%20%20Large%20Language%20Models&entry.906535625=Junho%20Kim%20and%20Hyungjin%20Chung%20and%20Byung-Hoon%20Kim&entry.1292438233=%20%20Category-agnostic%20pose%20estimation%20%28CAPE%29%20has%20traditionally%20relied%20on%20support%0Aimages%20with%20annotated%20keypoints%2C%20a%20process%20that%20is%20often%20cumbersome%20and%20may%0Afail%20to%20fully%20capture%20the%20necessary%20correspondences%20across%20diverse%20object%0Acategories.%20Recent%20efforts%20have%20begun%20exploring%20the%20use%20of%20text-based%20queries%2C%0Awhere%20the%20need%20for%20support%20keypoints%20is%20eliminated.%20However%2C%20the%20optimal%20use%20of%0Atextual%20descriptions%20for%20keypoints%20remains%20an%20underexplored%20area.%20In%20this%20work%2C%0Awe%20introduce%20CapeLLM%2C%20a%20novel%20approach%20that%20leverages%20a%20text-based%20multimodal%0Alarge%20language%20model%20%28MLLM%29%20for%20CAPE.%20Our%20method%20only%20employs%20query%20image%20and%0Adetailed%20text%20descriptions%20as%20an%20input%20to%20estimate%20category-agnostic%20keypoints.%0AWe%20conduct%20extensive%20experiments%20to%20systematically%20explore%20the%20design%20space%20of%0ALLM-based%20CAPE%2C%20investigating%20factors%20such%20as%20choosing%20the%20optimal%20description%0Afor%20keypoints%2C%20neural%20network%20architectures%2C%20and%20training%20strategies.%20Thanks%20to%0Athe%20advanced%20reasoning%20capabilities%20of%20the%20pre-trained%20MLLM%2C%20CapeLLM%0Ademonstrates%20superior%20generalization%20and%20robust%20performance.%20Our%20approach%20sets%0Aa%20new%20state-of-the-art%20on%20the%20MP-100%20benchmark%20in%20the%20challenging%201-shot%0Asetting%2C%20marking%20a%20significant%20advancement%20in%20the%20field%20of%20category-agnostic%0Apose%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06869v1&entry.124074799=Read"},
{"title": "MapSAM: Adapting Segment Anything Model for Automated Feature Detection\n  in Historical Maps", "author": "Xue Xia and Daiwei Zhang and Wenxuan Song and Wei Huang and Lorenz Hurni", "abstract": "  Automated feature detection in historical maps can significantly accelerate\nthe reconstruction of the geospatial past. However, this process is often\nconstrained by the time-consuming task of manually digitizing sufficient\nhigh-quality training data. The emergence of visual foundation models, such as\nthe Segment Anything Model (SAM), offers a promising solution due to their\nremarkable generalization capabilities and rapid adaptation to new data\ndistributions. Despite this, directly applying SAM in a zero-shot manner to\nhistorical map segmentation poses significant challenges, including poor\nrecognition of certain geospatial features and a reliance on input prompts,\nwhich limits its ability to be fully automated. To address these challenges, we\nintroduce MapSAM, a parameter-efficient fine-tuning strategy that adapts SAM\ninto a prompt-free and versatile solution for various downstream historical map\nsegmentation tasks. Specifically, we employ Weight-Decomposed Low-Rank\nAdaptation (DoRA) to integrate domain-specific knowledge into the image\nencoder. Additionally, we develop an automatic prompt generation process,\neliminating the need for manual input. We further enhance the positional prompt\nin SAM, transforming it into a higher-level positional-semantic prompt, and\nmodify the cross-attention mechanism in the mask decoder with masked attention\nfor more effective feature aggregation. The proposed MapSAM framework\ndemonstrates promising performance across two distinct historical map\nsegmentation tasks: one focused on linear features and the other on areal\nfeatures. Experimental results show that it adapts well to various features,\neven when fine-tuned with extremely limited data (e.g. 10 shots).\n", "link": "http://arxiv.org/abs/2411.06971v1", "date": "2024-11-11", "relevancy": 2.74, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5668}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapSAM%3A%20Adapting%20Segment%20Anything%20Model%20for%20Automated%20Feature%20Detection%0A%20%20in%20Historical%20Maps&body=Title%3A%20MapSAM%3A%20Adapting%20Segment%20Anything%20Model%20for%20Automated%20Feature%20Detection%0A%20%20in%20Historical%20Maps%0AAuthor%3A%20Xue%20Xia%20and%20Daiwei%20Zhang%20and%20Wenxuan%20Song%20and%20Wei%20Huang%20and%20Lorenz%20Hurni%0AAbstract%3A%20%20%20Automated%20feature%20detection%20in%20historical%20maps%20can%20significantly%20accelerate%0Athe%20reconstruction%20of%20the%20geospatial%20past.%20However%2C%20this%20process%20is%20often%0Aconstrained%20by%20the%20time-consuming%20task%20of%20manually%20digitizing%20sufficient%0Ahigh-quality%20training%20data.%20The%20emergence%20of%20visual%20foundation%20models%2C%20such%20as%0Athe%20Segment%20Anything%20Model%20%28SAM%29%2C%20offers%20a%20promising%20solution%20due%20to%20their%0Aremarkable%20generalization%20capabilities%20and%20rapid%20adaptation%20to%20new%20data%0Adistributions.%20Despite%20this%2C%20directly%20applying%20SAM%20in%20a%20zero-shot%20manner%20to%0Ahistorical%20map%20segmentation%20poses%20significant%20challenges%2C%20including%20poor%0Arecognition%20of%20certain%20geospatial%20features%20and%20a%20reliance%20on%20input%20prompts%2C%0Awhich%20limits%20its%20ability%20to%20be%20fully%20automated.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20MapSAM%2C%20a%20parameter-efficient%20fine-tuning%20strategy%20that%20adapts%20SAM%0Ainto%20a%20prompt-free%20and%20versatile%20solution%20for%20various%20downstream%20historical%20map%0Asegmentation%20tasks.%20Specifically%2C%20we%20employ%20Weight-Decomposed%20Low-Rank%0AAdaptation%20%28DoRA%29%20to%20integrate%20domain-specific%20knowledge%20into%20the%20image%0Aencoder.%20Additionally%2C%20we%20develop%20an%20automatic%20prompt%20generation%20process%2C%0Aeliminating%20the%20need%20for%20manual%20input.%20We%20further%20enhance%20the%20positional%20prompt%0Ain%20SAM%2C%20transforming%20it%20into%20a%20higher-level%20positional-semantic%20prompt%2C%20and%0Amodify%20the%20cross-attention%20mechanism%20in%20the%20mask%20decoder%20with%20masked%20attention%0Afor%20more%20effective%20feature%20aggregation.%20The%20proposed%20MapSAM%20framework%0Ademonstrates%20promising%20performance%20across%20two%20distinct%20historical%20map%0Asegmentation%20tasks%3A%20one%20focused%20on%20linear%20features%20and%20the%20other%20on%20areal%0Afeatures.%20Experimental%20results%20show%20that%20it%20adapts%20well%20to%20various%20features%2C%0Aeven%20when%20fine-tuned%20with%20extremely%20limited%20data%20%28e.g.%2010%20shots%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapSAM%253A%2520Adapting%2520Segment%2520Anything%2520Model%2520for%2520Automated%2520Feature%2520Detection%250A%2520%2520in%2520Historical%2520Maps%26entry.906535625%3DXue%2520Xia%2520and%2520Daiwei%2520Zhang%2520and%2520Wenxuan%2520Song%2520and%2520Wei%2520Huang%2520and%2520Lorenz%2520Hurni%26entry.1292438233%3D%2520%2520Automated%2520feature%2520detection%2520in%2520historical%2520maps%2520can%2520significantly%2520accelerate%250Athe%2520reconstruction%2520of%2520the%2520geospatial%2520past.%2520However%252C%2520this%2520process%2520is%2520often%250Aconstrained%2520by%2520the%2520time-consuming%2520task%2520of%2520manually%2520digitizing%2520sufficient%250Ahigh-quality%2520training%2520data.%2520The%2520emergence%2520of%2520visual%2520foundation%2520models%252C%2520such%2520as%250Athe%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520offers%2520a%2520promising%2520solution%2520due%2520to%2520their%250Aremarkable%2520generalization%2520capabilities%2520and%2520rapid%2520adaptation%2520to%2520new%2520data%250Adistributions.%2520Despite%2520this%252C%2520directly%2520applying%2520SAM%2520in%2520a%2520zero-shot%2520manner%2520to%250Ahistorical%2520map%2520segmentation%2520poses%2520significant%2520challenges%252C%2520including%2520poor%250Arecognition%2520of%2520certain%2520geospatial%2520features%2520and%2520a%2520reliance%2520on%2520input%2520prompts%252C%250Awhich%2520limits%2520its%2520ability%2520to%2520be%2520fully%2520automated.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520MapSAM%252C%2520a%2520parameter-efficient%2520fine-tuning%2520strategy%2520that%2520adapts%2520SAM%250Ainto%2520a%2520prompt-free%2520and%2520versatile%2520solution%2520for%2520various%2520downstream%2520historical%2520map%250Asegmentation%2520tasks.%2520Specifically%252C%2520we%2520employ%2520Weight-Decomposed%2520Low-Rank%250AAdaptation%2520%2528DoRA%2529%2520to%2520integrate%2520domain-specific%2520knowledge%2520into%2520the%2520image%250Aencoder.%2520Additionally%252C%2520we%2520develop%2520an%2520automatic%2520prompt%2520generation%2520process%252C%250Aeliminating%2520the%2520need%2520for%2520manual%2520input.%2520We%2520further%2520enhance%2520the%2520positional%2520prompt%250Ain%2520SAM%252C%2520transforming%2520it%2520into%2520a%2520higher-level%2520positional-semantic%2520prompt%252C%2520and%250Amodify%2520the%2520cross-attention%2520mechanism%2520in%2520the%2520mask%2520decoder%2520with%2520masked%2520attention%250Afor%2520more%2520effective%2520feature%2520aggregation.%2520The%2520proposed%2520MapSAM%2520framework%250Ademonstrates%2520promising%2520performance%2520across%2520two%2520distinct%2520historical%2520map%250Asegmentation%2520tasks%253A%2520one%2520focused%2520on%2520linear%2520features%2520and%2520the%2520other%2520on%2520areal%250Afeatures.%2520Experimental%2520results%2520show%2520that%2520it%2520adapts%2520well%2520to%2520various%2520features%252C%250Aeven%2520when%2520fine-tuned%2520with%2520extremely%2520limited%2520data%2520%2528e.g.%252010%2520shots%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapSAM%3A%20Adapting%20Segment%20Anything%20Model%20for%20Automated%20Feature%20Detection%0A%20%20in%20Historical%20Maps&entry.906535625=Xue%20Xia%20and%20Daiwei%20Zhang%20and%20Wenxuan%20Song%20and%20Wei%20Huang%20and%20Lorenz%20Hurni&entry.1292438233=%20%20Automated%20feature%20detection%20in%20historical%20maps%20can%20significantly%20accelerate%0Athe%20reconstruction%20of%20the%20geospatial%20past.%20However%2C%20this%20process%20is%20often%0Aconstrained%20by%20the%20time-consuming%20task%20of%20manually%20digitizing%20sufficient%0Ahigh-quality%20training%20data.%20The%20emergence%20of%20visual%20foundation%20models%2C%20such%20as%0Athe%20Segment%20Anything%20Model%20%28SAM%29%2C%20offers%20a%20promising%20solution%20due%20to%20their%0Aremarkable%20generalization%20capabilities%20and%20rapid%20adaptation%20to%20new%20data%0Adistributions.%20Despite%20this%2C%20directly%20applying%20SAM%20in%20a%20zero-shot%20manner%20to%0Ahistorical%20map%20segmentation%20poses%20significant%20challenges%2C%20including%20poor%0Arecognition%20of%20certain%20geospatial%20features%20and%20a%20reliance%20on%20input%20prompts%2C%0Awhich%20limits%20its%20ability%20to%20be%20fully%20automated.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20MapSAM%2C%20a%20parameter-efficient%20fine-tuning%20strategy%20that%20adapts%20SAM%0Ainto%20a%20prompt-free%20and%20versatile%20solution%20for%20various%20downstream%20historical%20map%0Asegmentation%20tasks.%20Specifically%2C%20we%20employ%20Weight-Decomposed%20Low-Rank%0AAdaptation%20%28DoRA%29%20to%20integrate%20domain-specific%20knowledge%20into%20the%20image%0Aencoder.%20Additionally%2C%20we%20develop%20an%20automatic%20prompt%20generation%20process%2C%0Aeliminating%20the%20need%20for%20manual%20input.%20We%20further%20enhance%20the%20positional%20prompt%0Ain%20SAM%2C%20transforming%20it%20into%20a%20higher-level%20positional-semantic%20prompt%2C%20and%0Amodify%20the%20cross-attention%20mechanism%20in%20the%20mask%20decoder%20with%20masked%20attention%0Afor%20more%20effective%20feature%20aggregation.%20The%20proposed%20MapSAM%20framework%0Ademonstrates%20promising%20performance%20across%20two%20distinct%20historical%20map%0Asegmentation%20tasks%3A%20one%20focused%20on%20linear%20features%20and%20the%20other%20on%20areal%0Afeatures.%20Experimental%20results%20show%20that%20it%20adapts%20well%20to%20various%20features%2C%0Aeven%20when%20fine-tuned%20with%20extremely%20limited%20data%20%28e.g.%2010%20shots%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06971v1&entry.124074799=Read"},
{"title": "Veri-Car: Towards Open-world Vehicle Information Retrieval", "author": "Andr\u00e9s Mu\u00f1oz and Nancy Thomas and Annita Vapsi and Daciel Borrajo", "abstract": "  Many industrial and service sectors require tools to extract vehicle\ncharacteristics from images. This is a complex task not only by the variety of\nnoise, and large number of classes, but also by the constant introduction of\nnew vehicle models to the market. In this paper, we present Veri-Car, an\ninformation retrieval integrated approach designed to help on this task. It\nleverages supervised learning techniques to accurately identify the make, type,\nmodel, year, color, and license plate of cars. The approach also addresses the\nchallenge of handling open-world problems, where new car models and variations\nfrequently emerge, by employing a sophisticated combination of pre-trained\nmodels, and a hierarchical multi-similarity loss. Veri-Car demonstrates robust\nperformance, achieving high precision and accuracy in classifying both seen and\nunseen data. Additionally, it integrates an ensemble license plate detection,\nand an OCR model to extract license plate numbers with impressive accuracy.\n", "link": "http://arxiv.org/abs/2411.06864v1", "date": "2024-11-11", "relevancy": 2.7112, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5443}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5412}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Veri-Car%3A%20Towards%20Open-world%20Vehicle%20Information%20Retrieval&body=Title%3A%20Veri-Car%3A%20Towards%20Open-world%20Vehicle%20Information%20Retrieval%0AAuthor%3A%20Andr%C3%A9s%20Mu%C3%B1oz%20and%20Nancy%20Thomas%20and%20Annita%20Vapsi%20and%20Daciel%20Borrajo%0AAbstract%3A%20%20%20Many%20industrial%20and%20service%20sectors%20require%20tools%20to%20extract%20vehicle%0Acharacteristics%20from%20images.%20This%20is%20a%20complex%20task%20not%20only%20by%20the%20variety%20of%0Anoise%2C%20and%20large%20number%20of%20classes%2C%20but%20also%20by%20the%20constant%20introduction%20of%0Anew%20vehicle%20models%20to%20the%20market.%20In%20this%20paper%2C%20we%20present%20Veri-Car%2C%20an%0Ainformation%20retrieval%20integrated%20approach%20designed%20to%20help%20on%20this%20task.%20It%0Aleverages%20supervised%20learning%20techniques%20to%20accurately%20identify%20the%20make%2C%20type%2C%0Amodel%2C%20year%2C%20color%2C%20and%20license%20plate%20of%20cars.%20The%20approach%20also%20addresses%20the%0Achallenge%20of%20handling%20open-world%20problems%2C%20where%20new%20car%20models%20and%20variations%0Afrequently%20emerge%2C%20by%20employing%20a%20sophisticated%20combination%20of%20pre-trained%0Amodels%2C%20and%20a%20hierarchical%20multi-similarity%20loss.%20Veri-Car%20demonstrates%20robust%0Aperformance%2C%20achieving%20high%20precision%20and%20accuracy%20in%20classifying%20both%20seen%20and%0Aunseen%20data.%20Additionally%2C%20it%20integrates%20an%20ensemble%20license%20plate%20detection%2C%0Aand%20an%20OCR%20model%20to%20extract%20license%20plate%20numbers%20with%20impressive%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeri-Car%253A%2520Towards%2520Open-world%2520Vehicle%2520Information%2520Retrieval%26entry.906535625%3DAndr%25C3%25A9s%2520Mu%25C3%25B1oz%2520and%2520Nancy%2520Thomas%2520and%2520Annita%2520Vapsi%2520and%2520Daciel%2520Borrajo%26entry.1292438233%3D%2520%2520Many%2520industrial%2520and%2520service%2520sectors%2520require%2520tools%2520to%2520extract%2520vehicle%250Acharacteristics%2520from%2520images.%2520This%2520is%2520a%2520complex%2520task%2520not%2520only%2520by%2520the%2520variety%2520of%250Anoise%252C%2520and%2520large%2520number%2520of%2520classes%252C%2520but%2520also%2520by%2520the%2520constant%2520introduction%2520of%250Anew%2520vehicle%2520models%2520to%2520the%2520market.%2520In%2520this%2520paper%252C%2520we%2520present%2520Veri-Car%252C%2520an%250Ainformation%2520retrieval%2520integrated%2520approach%2520designed%2520to%2520help%2520on%2520this%2520task.%2520It%250Aleverages%2520supervised%2520learning%2520techniques%2520to%2520accurately%2520identify%2520the%2520make%252C%2520type%252C%250Amodel%252C%2520year%252C%2520color%252C%2520and%2520license%2520plate%2520of%2520cars.%2520The%2520approach%2520also%2520addresses%2520the%250Achallenge%2520of%2520handling%2520open-world%2520problems%252C%2520where%2520new%2520car%2520models%2520and%2520variations%250Afrequently%2520emerge%252C%2520by%2520employing%2520a%2520sophisticated%2520combination%2520of%2520pre-trained%250Amodels%252C%2520and%2520a%2520hierarchical%2520multi-similarity%2520loss.%2520Veri-Car%2520demonstrates%2520robust%250Aperformance%252C%2520achieving%2520high%2520precision%2520and%2520accuracy%2520in%2520classifying%2520both%2520seen%2520and%250Aunseen%2520data.%2520Additionally%252C%2520it%2520integrates%2520an%2520ensemble%2520license%2520plate%2520detection%252C%250Aand%2520an%2520OCR%2520model%2520to%2520extract%2520license%2520plate%2520numbers%2520with%2520impressive%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Veri-Car%3A%20Towards%20Open-world%20Vehicle%20Information%20Retrieval&entry.906535625=Andr%C3%A9s%20Mu%C3%B1oz%20and%20Nancy%20Thomas%20and%20Annita%20Vapsi%20and%20Daciel%20Borrajo&entry.1292438233=%20%20Many%20industrial%20and%20service%20sectors%20require%20tools%20to%20extract%20vehicle%0Acharacteristics%20from%20images.%20This%20is%20a%20complex%20task%20not%20only%20by%20the%20variety%20of%0Anoise%2C%20and%20large%20number%20of%20classes%2C%20but%20also%20by%20the%20constant%20introduction%20of%0Anew%20vehicle%20models%20to%20the%20market.%20In%20this%20paper%2C%20we%20present%20Veri-Car%2C%20an%0Ainformation%20retrieval%20integrated%20approach%20designed%20to%20help%20on%20this%20task.%20It%0Aleverages%20supervised%20learning%20techniques%20to%20accurately%20identify%20the%20make%2C%20type%2C%0Amodel%2C%20year%2C%20color%2C%20and%20license%20plate%20of%20cars.%20The%20approach%20also%20addresses%20the%0Achallenge%20of%20handling%20open-world%20problems%2C%20where%20new%20car%20models%20and%20variations%0Afrequently%20emerge%2C%20by%20employing%20a%20sophisticated%20combination%20of%20pre-trained%0Amodels%2C%20and%20a%20hierarchical%20multi-similarity%20loss.%20Veri-Car%20demonstrates%20robust%0Aperformance%2C%20achieving%20high%20precision%20and%20accuracy%20in%20classifying%20both%20seen%20and%0Aunseen%20data.%20Additionally%2C%20it%20integrates%20an%20ensemble%20license%20plate%20detection%2C%0Aand%20an%20OCR%20model%20to%20extract%20license%20plate%20numbers%20with%20impressive%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06864v1&entry.124074799=Read"},
{"title": "Deep Augmentation: Self-Supervised Learning with Transformations in\n  Activation Space", "author": "Rickard Br\u00fcel-Gabrielsson and Tongzhou Wang and Manel Baradad and Justin Solomon", "abstract": "  We introduce Deep Augmentation, an approach to implicit data augmentation\nusing dropout or PCA to transform a targeted layer within a neural network to\nimprove performance and generalization. We demonstrate Deep Augmentation\nthrough extensive experiments on contrastive learning tasks in NLP, computer\nvision, and graph learning. We observe substantial performance gains with\nTransformers, ResNets, and Graph Neural Networks as the underlying models in\ncontrastive learning, but observe inverse effects on the corresponding\nsupervised problems. Our analysis suggests that Deep Augmentation alleviates\nco-adaptation between layers, a problem exhibited by self-supervised learning\nwhere ground truth labels are not available. We use this observation to\nformulate a method for selecting which layer to target; in particular, our\nexperimentation reveals that targeting deeper layers with Deep Augmentation\noutperforms augmenting the input data. The simple network- and\nmodality-agnostic nature of this approach enables its integration into various\nmachine learning pipelines.\n", "link": "http://arxiv.org/abs/2303.14537v3", "date": "2024-11-11", "relevancy": 2.7082, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5489}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.544}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Augmentation%3A%20Self-Supervised%20Learning%20with%20Transformations%20in%0A%20%20Activation%20Space&body=Title%3A%20Deep%20Augmentation%3A%20Self-Supervised%20Learning%20with%20Transformations%20in%0A%20%20Activation%20Space%0AAuthor%3A%20Rickard%20Br%C3%BCel-Gabrielsson%20and%20Tongzhou%20Wang%20and%20Manel%20Baradad%20and%20Justin%20Solomon%0AAbstract%3A%20%20%20We%20introduce%20Deep%20Augmentation%2C%20an%20approach%20to%20implicit%20data%20augmentation%0Ausing%20dropout%20or%20PCA%20to%20transform%20a%20targeted%20layer%20within%20a%20neural%20network%20to%0Aimprove%20performance%20and%20generalization.%20We%20demonstrate%20Deep%20Augmentation%0Athrough%20extensive%20experiments%20on%20contrastive%20learning%20tasks%20in%20NLP%2C%20computer%0Avision%2C%20and%20graph%20learning.%20We%20observe%20substantial%20performance%20gains%20with%0ATransformers%2C%20ResNets%2C%20and%20Graph%20Neural%20Networks%20as%20the%20underlying%20models%20in%0Acontrastive%20learning%2C%20but%20observe%20inverse%20effects%20on%20the%20corresponding%0Asupervised%20problems.%20Our%20analysis%20suggests%20that%20Deep%20Augmentation%20alleviates%0Aco-adaptation%20between%20layers%2C%20a%20problem%20exhibited%20by%20self-supervised%20learning%0Awhere%20ground%20truth%20labels%20are%20not%20available.%20We%20use%20this%20observation%20to%0Aformulate%20a%20method%20for%20selecting%20which%20layer%20to%20target%3B%20in%20particular%2C%20our%0Aexperimentation%20reveals%20that%20targeting%20deeper%20layers%20with%20Deep%20Augmentation%0Aoutperforms%20augmenting%20the%20input%20data.%20The%20simple%20network-%20and%0Amodality-agnostic%20nature%20of%20this%20approach%20enables%20its%20integration%20into%20various%0Amachine%20learning%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.14537v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Augmentation%253A%2520Self-Supervised%2520Learning%2520with%2520Transformations%2520in%250A%2520%2520Activation%2520Space%26entry.906535625%3DRickard%2520Br%25C3%25BCel-Gabrielsson%2520and%2520Tongzhou%2520Wang%2520and%2520Manel%2520Baradad%2520and%2520Justin%2520Solomon%26entry.1292438233%3D%2520%2520We%2520introduce%2520Deep%2520Augmentation%252C%2520an%2520approach%2520to%2520implicit%2520data%2520augmentation%250Ausing%2520dropout%2520or%2520PCA%2520to%2520transform%2520a%2520targeted%2520layer%2520within%2520a%2520neural%2520network%2520to%250Aimprove%2520performance%2520and%2520generalization.%2520We%2520demonstrate%2520Deep%2520Augmentation%250Athrough%2520extensive%2520experiments%2520on%2520contrastive%2520learning%2520tasks%2520in%2520NLP%252C%2520computer%250Avision%252C%2520and%2520graph%2520learning.%2520We%2520observe%2520substantial%2520performance%2520gains%2520with%250ATransformers%252C%2520ResNets%252C%2520and%2520Graph%2520Neural%2520Networks%2520as%2520the%2520underlying%2520models%2520in%250Acontrastive%2520learning%252C%2520but%2520observe%2520inverse%2520effects%2520on%2520the%2520corresponding%250Asupervised%2520problems.%2520Our%2520analysis%2520suggests%2520that%2520Deep%2520Augmentation%2520alleviates%250Aco-adaptation%2520between%2520layers%252C%2520a%2520problem%2520exhibited%2520by%2520self-supervised%2520learning%250Awhere%2520ground%2520truth%2520labels%2520are%2520not%2520available.%2520We%2520use%2520this%2520observation%2520to%250Aformulate%2520a%2520method%2520for%2520selecting%2520which%2520layer%2520to%2520target%253B%2520in%2520particular%252C%2520our%250Aexperimentation%2520reveals%2520that%2520targeting%2520deeper%2520layers%2520with%2520Deep%2520Augmentation%250Aoutperforms%2520augmenting%2520the%2520input%2520data.%2520The%2520simple%2520network-%2520and%250Amodality-agnostic%2520nature%2520of%2520this%2520approach%2520enables%2520its%2520integration%2520into%2520various%250Amachine%2520learning%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.14537v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Augmentation%3A%20Self-Supervised%20Learning%20with%20Transformations%20in%0A%20%20Activation%20Space&entry.906535625=Rickard%20Br%C3%BCel-Gabrielsson%20and%20Tongzhou%20Wang%20and%20Manel%20Baradad%20and%20Justin%20Solomon&entry.1292438233=%20%20We%20introduce%20Deep%20Augmentation%2C%20an%20approach%20to%20implicit%20data%20augmentation%0Ausing%20dropout%20or%20PCA%20to%20transform%20a%20targeted%20layer%20within%20a%20neural%20network%20to%0Aimprove%20performance%20and%20generalization.%20We%20demonstrate%20Deep%20Augmentation%0Athrough%20extensive%20experiments%20on%20contrastive%20learning%20tasks%20in%20NLP%2C%20computer%0Avision%2C%20and%20graph%20learning.%20We%20observe%20substantial%20performance%20gains%20with%0ATransformers%2C%20ResNets%2C%20and%20Graph%20Neural%20Networks%20as%20the%20underlying%20models%20in%0Acontrastive%20learning%2C%20but%20observe%20inverse%20effects%20on%20the%20corresponding%0Asupervised%20problems.%20Our%20analysis%20suggests%20that%20Deep%20Augmentation%20alleviates%0Aco-adaptation%20between%20layers%2C%20a%20problem%20exhibited%20by%20self-supervised%20learning%0Awhere%20ground%20truth%20labels%20are%20not%20available.%20We%20use%20this%20observation%20to%0Aformulate%20a%20method%20for%20selecting%20which%20layer%20to%20target%3B%20in%20particular%2C%20our%0Aexperimentation%20reveals%20that%20targeting%20deeper%20layers%20with%20Deep%20Augmentation%0Aoutperforms%20augmenting%20the%20input%20data.%20The%20simple%20network-%20and%0Amodality-agnostic%20nature%20of%20this%20approach%20enables%20its%20integration%20into%20various%0Amachine%20learning%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.14537v3&entry.124074799=Read"},
{"title": "ZAHA: Introducing the Level of Facade Generalization and the Large-Scale\n  Point Cloud Facade Semantic Segmentation Benchmark Dataset", "author": "Olaf Wysocki and Yue Tan and Thomas Froech and Yan Xia and Magdalena Wysocki and Ludwig Hoegner and Daniel Cremers and Christoph Holst", "abstract": "  Facade semantic segmentation is a long-standing challenge in photogrammetry\nand computer vision. Although the last decades have witnessed the influx of\nfacade segmentation methods, there is a lack of comprehensive facade classes\nand data covering the architectural variability. In ZAHA, we introduce Level of\nFacade Generalization (LoFG), novel hierarchical facade classes designed based\non international urban modeling standards, ensuring compatibility with\nreal-world challenging classes and uniform methods' comparison. Realizing the\nLoFG, we present to date the largest semantic 3D facade segmentation dataset,\nproviding 601 million annotated points at five and 15 classes of LoFG2 and\nLoFG3, respectively. Moreover, we analyze the performance of baseline semantic\nsegmentation methods on our introduced LoFG classes and data, complementing it\nwith a discussion on the unresolved challenges for facade segmentation. We\nfirmly believe that ZAHA shall facilitate further development of 3D facade\nsemantic segmentation methods, enabling robust segmentation indispensable in\ncreating urban digital twins.\n", "link": "http://arxiv.org/abs/2411.04865v3", "date": "2024-11-11", "relevancy": 2.6939, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZAHA%3A%20Introducing%20the%20Level%20of%20Facade%20Generalization%20and%20the%20Large-Scale%0A%20%20Point%20Cloud%20Facade%20Semantic%20Segmentation%20Benchmark%20Dataset&body=Title%3A%20ZAHA%3A%20Introducing%20the%20Level%20of%20Facade%20Generalization%20and%20the%20Large-Scale%0A%20%20Point%20Cloud%20Facade%20Semantic%20Segmentation%20Benchmark%20Dataset%0AAuthor%3A%20Olaf%20Wysocki%20and%20Yue%20Tan%20and%20Thomas%20Froech%20and%20Yan%20Xia%20and%20Magdalena%20Wysocki%20and%20Ludwig%20Hoegner%20and%20Daniel%20Cremers%20and%20Christoph%20Holst%0AAbstract%3A%20%20%20Facade%20semantic%20segmentation%20is%20a%20long-standing%20challenge%20in%20photogrammetry%0Aand%20computer%20vision.%20Although%20the%20last%20decades%20have%20witnessed%20the%20influx%20of%0Afacade%20segmentation%20methods%2C%20there%20is%20a%20lack%20of%20comprehensive%20facade%20classes%0Aand%20data%20covering%20the%20architectural%20variability.%20In%20ZAHA%2C%20we%20introduce%20Level%20of%0AFacade%20Generalization%20%28LoFG%29%2C%20novel%20hierarchical%20facade%20classes%20designed%20based%0Aon%20international%20urban%20modeling%20standards%2C%20ensuring%20compatibility%20with%0Areal-world%20challenging%20classes%20and%20uniform%20methods%27%20comparison.%20Realizing%20the%0ALoFG%2C%20we%20present%20to%20date%20the%20largest%20semantic%203D%20facade%20segmentation%20dataset%2C%0Aproviding%20601%20million%20annotated%20points%20at%20five%20and%2015%20classes%20of%20LoFG2%20and%0ALoFG3%2C%20respectively.%20Moreover%2C%20we%20analyze%20the%20performance%20of%20baseline%20semantic%0Asegmentation%20methods%20on%20our%20introduced%20LoFG%20classes%20and%20data%2C%20complementing%20it%0Awith%20a%20discussion%20on%20the%20unresolved%20challenges%20for%20facade%20segmentation.%20We%0Afirmly%20believe%20that%20ZAHA%20shall%20facilitate%20further%20development%20of%203D%20facade%0Asemantic%20segmentation%20methods%2C%20enabling%20robust%20segmentation%20indispensable%20in%0Acreating%20urban%20digital%20twins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04865v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZAHA%253A%2520Introducing%2520the%2520Level%2520of%2520Facade%2520Generalization%2520and%2520the%2520Large-Scale%250A%2520%2520Point%2520Cloud%2520Facade%2520Semantic%2520Segmentation%2520Benchmark%2520Dataset%26entry.906535625%3DOlaf%2520Wysocki%2520and%2520Yue%2520Tan%2520and%2520Thomas%2520Froech%2520and%2520Yan%2520Xia%2520and%2520Magdalena%2520Wysocki%2520and%2520Ludwig%2520Hoegner%2520and%2520Daniel%2520Cremers%2520and%2520Christoph%2520Holst%26entry.1292438233%3D%2520%2520Facade%2520semantic%2520segmentation%2520is%2520a%2520long-standing%2520challenge%2520in%2520photogrammetry%250Aand%2520computer%2520vision.%2520Although%2520the%2520last%2520decades%2520have%2520witnessed%2520the%2520influx%2520of%250Afacade%2520segmentation%2520methods%252C%2520there%2520is%2520a%2520lack%2520of%2520comprehensive%2520facade%2520classes%250Aand%2520data%2520covering%2520the%2520architectural%2520variability.%2520In%2520ZAHA%252C%2520we%2520introduce%2520Level%2520of%250AFacade%2520Generalization%2520%2528LoFG%2529%252C%2520novel%2520hierarchical%2520facade%2520classes%2520designed%2520based%250Aon%2520international%2520urban%2520modeling%2520standards%252C%2520ensuring%2520compatibility%2520with%250Areal-world%2520challenging%2520classes%2520and%2520uniform%2520methods%2527%2520comparison.%2520Realizing%2520the%250ALoFG%252C%2520we%2520present%2520to%2520date%2520the%2520largest%2520semantic%25203D%2520facade%2520segmentation%2520dataset%252C%250Aproviding%2520601%2520million%2520annotated%2520points%2520at%2520five%2520and%252015%2520classes%2520of%2520LoFG2%2520and%250ALoFG3%252C%2520respectively.%2520Moreover%252C%2520we%2520analyze%2520the%2520performance%2520of%2520baseline%2520semantic%250Asegmentation%2520methods%2520on%2520our%2520introduced%2520LoFG%2520classes%2520and%2520data%252C%2520complementing%2520it%250Awith%2520a%2520discussion%2520on%2520the%2520unresolved%2520challenges%2520for%2520facade%2520segmentation.%2520We%250Afirmly%2520believe%2520that%2520ZAHA%2520shall%2520facilitate%2520further%2520development%2520of%25203D%2520facade%250Asemantic%2520segmentation%2520methods%252C%2520enabling%2520robust%2520segmentation%2520indispensable%2520in%250Acreating%2520urban%2520digital%2520twins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04865v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZAHA%3A%20Introducing%20the%20Level%20of%20Facade%20Generalization%20and%20the%20Large-Scale%0A%20%20Point%20Cloud%20Facade%20Semantic%20Segmentation%20Benchmark%20Dataset&entry.906535625=Olaf%20Wysocki%20and%20Yue%20Tan%20and%20Thomas%20Froech%20and%20Yan%20Xia%20and%20Magdalena%20Wysocki%20and%20Ludwig%20Hoegner%20and%20Daniel%20Cremers%20and%20Christoph%20Holst&entry.1292438233=%20%20Facade%20semantic%20segmentation%20is%20a%20long-standing%20challenge%20in%20photogrammetry%0Aand%20computer%20vision.%20Although%20the%20last%20decades%20have%20witnessed%20the%20influx%20of%0Afacade%20segmentation%20methods%2C%20there%20is%20a%20lack%20of%20comprehensive%20facade%20classes%0Aand%20data%20covering%20the%20architectural%20variability.%20In%20ZAHA%2C%20we%20introduce%20Level%20of%0AFacade%20Generalization%20%28LoFG%29%2C%20novel%20hierarchical%20facade%20classes%20designed%20based%0Aon%20international%20urban%20modeling%20standards%2C%20ensuring%20compatibility%20with%0Areal-world%20challenging%20classes%20and%20uniform%20methods%27%20comparison.%20Realizing%20the%0ALoFG%2C%20we%20present%20to%20date%20the%20largest%20semantic%203D%20facade%20segmentation%20dataset%2C%0Aproviding%20601%20million%20annotated%20points%20at%20five%20and%2015%20classes%20of%20LoFG2%20and%0ALoFG3%2C%20respectively.%20Moreover%2C%20we%20analyze%20the%20performance%20of%20baseline%20semantic%0Asegmentation%20methods%20on%20our%20introduced%20LoFG%20classes%20and%20data%2C%20complementing%20it%0Awith%20a%20discussion%20on%20the%20unresolved%20challenges%20for%20facade%20segmentation.%20We%0Afirmly%20believe%20that%20ZAHA%20shall%20facilitate%20further%20development%20of%203D%20facade%0Asemantic%20segmentation%20methods%2C%20enabling%20robust%20segmentation%20indispensable%20in%0Acreating%20urban%20digital%20twins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04865v3&entry.124074799=Read"},
{"title": "Extreme Rotation Estimation in the Wild", "author": "Hana Bezalel and Dotan Ankri and Ruojin Cai and Hadar Averbuch-Elor", "abstract": "  We present a technique and benchmark dataset for estimating the relative 3D\norientation between a pair of Internet images captured in an extreme setting,\nwhere the images have limited or non-overlapping field of views. Prior work\ntargeting extreme rotation estimation assume constrained 3D environments and\nemulate perspective images by cropping regions from panoramic views. However,\nreal images captured in the wild are highly diverse, exhibiting variation in\nboth appearance and camera intrinsics. In this work, we propose a\nTransformer-based method for estimating relative rotations in extreme\nreal-world settings, and contribute the ExtremeLandmarkPairs dataset, assembled\nfrom scene-level Internet photo collections. Our evaluation demonstrates that\nour approach succeeds in estimating the relative rotations in a wide variety of\nextremeview Internet image pairs, outperforming various baselines, including\ndedicated rotation estimation techniques and contemporary 3D reconstruction\nmethods.\n", "link": "http://arxiv.org/abs/2411.07096v1", "date": "2024-11-11", "relevancy": 2.6812, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5461}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5345}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extreme%20Rotation%20Estimation%20in%20the%20Wild&body=Title%3A%20Extreme%20Rotation%20Estimation%20in%20the%20Wild%0AAuthor%3A%20Hana%20Bezalel%20and%20Dotan%20Ankri%20and%20Ruojin%20Cai%20and%20Hadar%20Averbuch-Elor%0AAbstract%3A%20%20%20We%20present%20a%20technique%20and%20benchmark%20dataset%20for%20estimating%20the%20relative%203D%0Aorientation%20between%20a%20pair%20of%20Internet%20images%20captured%20in%20an%20extreme%20setting%2C%0Awhere%20the%20images%20have%20limited%20or%20non-overlapping%20field%20of%20views.%20Prior%20work%0Atargeting%20extreme%20rotation%20estimation%20assume%20constrained%203D%20environments%20and%0Aemulate%20perspective%20images%20by%20cropping%20regions%20from%20panoramic%20views.%20However%2C%0Areal%20images%20captured%20in%20the%20wild%20are%20highly%20diverse%2C%20exhibiting%20variation%20in%0Aboth%20appearance%20and%20camera%20intrinsics.%20In%20this%20work%2C%20we%20propose%20a%0ATransformer-based%20method%20for%20estimating%20relative%20rotations%20in%20extreme%0Areal-world%20settings%2C%20and%20contribute%20the%20ExtremeLandmarkPairs%20dataset%2C%20assembled%0Afrom%20scene-level%20Internet%20photo%20collections.%20Our%20evaluation%20demonstrates%20that%0Aour%20approach%20succeeds%20in%20estimating%20the%20relative%20rotations%20in%20a%20wide%20variety%20of%0Aextremeview%20Internet%20image%20pairs%2C%20outperforming%20various%20baselines%2C%20including%0Adedicated%20rotation%20estimation%20techniques%20and%20contemporary%203D%20reconstruction%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtreme%2520Rotation%2520Estimation%2520in%2520the%2520Wild%26entry.906535625%3DHana%2520Bezalel%2520and%2520Dotan%2520Ankri%2520and%2520Ruojin%2520Cai%2520and%2520Hadar%2520Averbuch-Elor%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520technique%2520and%2520benchmark%2520dataset%2520for%2520estimating%2520the%2520relative%25203D%250Aorientation%2520between%2520a%2520pair%2520of%2520Internet%2520images%2520captured%2520in%2520an%2520extreme%2520setting%252C%250Awhere%2520the%2520images%2520have%2520limited%2520or%2520non-overlapping%2520field%2520of%2520views.%2520Prior%2520work%250Atargeting%2520extreme%2520rotation%2520estimation%2520assume%2520constrained%25203D%2520environments%2520and%250Aemulate%2520perspective%2520images%2520by%2520cropping%2520regions%2520from%2520panoramic%2520views.%2520However%252C%250Areal%2520images%2520captured%2520in%2520the%2520wild%2520are%2520highly%2520diverse%252C%2520exhibiting%2520variation%2520in%250Aboth%2520appearance%2520and%2520camera%2520intrinsics.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250ATransformer-based%2520method%2520for%2520estimating%2520relative%2520rotations%2520in%2520extreme%250Areal-world%2520settings%252C%2520and%2520contribute%2520the%2520ExtremeLandmarkPairs%2520dataset%252C%2520assembled%250Afrom%2520scene-level%2520Internet%2520photo%2520collections.%2520Our%2520evaluation%2520demonstrates%2520that%250Aour%2520approach%2520succeeds%2520in%2520estimating%2520the%2520relative%2520rotations%2520in%2520a%2520wide%2520variety%2520of%250Aextremeview%2520Internet%2520image%2520pairs%252C%2520outperforming%2520various%2520baselines%252C%2520including%250Adedicated%2520rotation%2520estimation%2520techniques%2520and%2520contemporary%25203D%2520reconstruction%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extreme%20Rotation%20Estimation%20in%20the%20Wild&entry.906535625=Hana%20Bezalel%20and%20Dotan%20Ankri%20and%20Ruojin%20Cai%20and%20Hadar%20Averbuch-Elor&entry.1292438233=%20%20We%20present%20a%20technique%20and%20benchmark%20dataset%20for%20estimating%20the%20relative%203D%0Aorientation%20between%20a%20pair%20of%20Internet%20images%20captured%20in%20an%20extreme%20setting%2C%0Awhere%20the%20images%20have%20limited%20or%20non-overlapping%20field%20of%20views.%20Prior%20work%0Atargeting%20extreme%20rotation%20estimation%20assume%20constrained%203D%20environments%20and%0Aemulate%20perspective%20images%20by%20cropping%20regions%20from%20panoramic%20views.%20However%2C%0Areal%20images%20captured%20in%20the%20wild%20are%20highly%20diverse%2C%20exhibiting%20variation%20in%0Aboth%20appearance%20and%20camera%20intrinsics.%20In%20this%20work%2C%20we%20propose%20a%0ATransformer-based%20method%20for%20estimating%20relative%20rotations%20in%20extreme%0Areal-world%20settings%2C%20and%20contribute%20the%20ExtremeLandmarkPairs%20dataset%2C%20assembled%0Afrom%20scene-level%20Internet%20photo%20collections.%20Our%20evaluation%20demonstrates%20that%0Aour%20approach%20succeeds%20in%20estimating%20the%20relative%20rotations%20in%20a%20wide%20variety%20of%0Aextremeview%20Internet%20image%20pairs%2C%20outperforming%20various%20baselines%2C%20including%0Adedicated%20rotation%20estimation%20techniques%20and%20contemporary%203D%20reconstruction%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07096v1&entry.124074799=Read"},
{"title": "IntegratedPIFu: Integrated Pixel Aligned Implicit Function for\n  Single-view Human Reconstruction", "author": "Kennard Yanting Chan and Guosheng Lin and Haiyu Zhao and Weisi Lin", "abstract": "  We propose IntegratedPIFu, a new pixel aligned implicit model that builds on\nthe foundation set by PIFuHD. IntegratedPIFu shows how depth and human parsing\ninformation can be predicted and capitalised upon in a pixel-aligned implicit\nmodel. In addition, IntegratedPIFu introduces depth oriented sampling, a novel\ntraining scheme that improve any pixel aligned implicit model ability to\nreconstruct important human features without noisy artefacts. Lastly,\nIntegratedPIFu presents a new architecture that, despite using less model\nparameters than PIFuHD, is able to improves the structural correctness of\nreconstructed meshes. Our results show that IntegratedPIFu significantly\noutperforms existing state of the arts methods on single view human\nreconstruction. Our code has been made available online.\n", "link": "http://arxiv.org/abs/2211.07955v3", "date": "2024-11-11", "relevancy": 2.6589, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5626}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5174}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IntegratedPIFu%3A%20Integrated%20Pixel%20Aligned%20Implicit%20Function%20for%0A%20%20Single-view%20Human%20Reconstruction&body=Title%3A%20IntegratedPIFu%3A%20Integrated%20Pixel%20Aligned%20Implicit%20Function%20for%0A%20%20Single-view%20Human%20Reconstruction%0AAuthor%3A%20Kennard%20Yanting%20Chan%20and%20Guosheng%20Lin%20and%20Haiyu%20Zhao%20and%20Weisi%20Lin%0AAbstract%3A%20%20%20We%20propose%20IntegratedPIFu%2C%20a%20new%20pixel%20aligned%20implicit%20model%20that%20builds%20on%0Athe%20foundation%20set%20by%20PIFuHD.%20IntegratedPIFu%20shows%20how%20depth%20and%20human%20parsing%0Ainformation%20can%20be%20predicted%20and%20capitalised%20upon%20in%20a%20pixel-aligned%20implicit%0Amodel.%20In%20addition%2C%20IntegratedPIFu%20introduces%20depth%20oriented%20sampling%2C%20a%20novel%0Atraining%20scheme%20that%20improve%20any%20pixel%20aligned%20implicit%20model%20ability%20to%0Areconstruct%20important%20human%20features%20without%20noisy%20artefacts.%20Lastly%2C%0AIntegratedPIFu%20presents%20a%20new%20architecture%20that%2C%20despite%20using%20less%20model%0Aparameters%20than%20PIFuHD%2C%20is%20able%20to%20improves%20the%20structural%20correctness%20of%0Areconstructed%20meshes.%20Our%20results%20show%20that%20IntegratedPIFu%20significantly%0Aoutperforms%20existing%20state%20of%20the%20arts%20methods%20on%20single%20view%20human%0Areconstruction.%20Our%20code%20has%20been%20made%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.07955v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegratedPIFu%253A%2520Integrated%2520Pixel%2520Aligned%2520Implicit%2520Function%2520for%250A%2520%2520Single-view%2520Human%2520Reconstruction%26entry.906535625%3DKennard%2520Yanting%2520Chan%2520and%2520Guosheng%2520Lin%2520and%2520Haiyu%2520Zhao%2520and%2520Weisi%2520Lin%26entry.1292438233%3D%2520%2520We%2520propose%2520IntegratedPIFu%252C%2520a%2520new%2520pixel%2520aligned%2520implicit%2520model%2520that%2520builds%2520on%250Athe%2520foundation%2520set%2520by%2520PIFuHD.%2520IntegratedPIFu%2520shows%2520how%2520depth%2520and%2520human%2520parsing%250Ainformation%2520can%2520be%2520predicted%2520and%2520capitalised%2520upon%2520in%2520a%2520pixel-aligned%2520implicit%250Amodel.%2520In%2520addition%252C%2520IntegratedPIFu%2520introduces%2520depth%2520oriented%2520sampling%252C%2520a%2520novel%250Atraining%2520scheme%2520that%2520improve%2520any%2520pixel%2520aligned%2520implicit%2520model%2520ability%2520to%250Areconstruct%2520important%2520human%2520features%2520without%2520noisy%2520artefacts.%2520Lastly%252C%250AIntegratedPIFu%2520presents%2520a%2520new%2520architecture%2520that%252C%2520despite%2520using%2520less%2520model%250Aparameters%2520than%2520PIFuHD%252C%2520is%2520able%2520to%2520improves%2520the%2520structural%2520correctness%2520of%250Areconstructed%2520meshes.%2520Our%2520results%2520show%2520that%2520IntegratedPIFu%2520significantly%250Aoutperforms%2520existing%2520state%2520of%2520the%2520arts%2520methods%2520on%2520single%2520view%2520human%250Areconstruction.%2520Our%2520code%2520has%2520been%2520made%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.07955v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IntegratedPIFu%3A%20Integrated%20Pixel%20Aligned%20Implicit%20Function%20for%0A%20%20Single-view%20Human%20Reconstruction&entry.906535625=Kennard%20Yanting%20Chan%20and%20Guosheng%20Lin%20and%20Haiyu%20Zhao%20and%20Weisi%20Lin&entry.1292438233=%20%20We%20propose%20IntegratedPIFu%2C%20a%20new%20pixel%20aligned%20implicit%20model%20that%20builds%20on%0Athe%20foundation%20set%20by%20PIFuHD.%20IntegratedPIFu%20shows%20how%20depth%20and%20human%20parsing%0Ainformation%20can%20be%20predicted%20and%20capitalised%20upon%20in%20a%20pixel-aligned%20implicit%0Amodel.%20In%20addition%2C%20IntegratedPIFu%20introduces%20depth%20oriented%20sampling%2C%20a%20novel%0Atraining%20scheme%20that%20improve%20any%20pixel%20aligned%20implicit%20model%20ability%20to%0Areconstruct%20important%20human%20features%20without%20noisy%20artefacts.%20Lastly%2C%0AIntegratedPIFu%20presents%20a%20new%20architecture%20that%2C%20despite%20using%20less%20model%0Aparameters%20than%20PIFuHD%2C%20is%20able%20to%20improves%20the%20structural%20correctness%20of%0Areconstructed%20meshes.%20Our%20results%20show%20that%20IntegratedPIFu%20significantly%0Aoutperforms%20existing%20state%20of%20the%20arts%20methods%20on%20single%20view%20human%0Areconstruction.%20Our%20code%20has%20been%20made%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.07955v3&entry.124074799=Read"},
{"title": "Data-driven discovery of mechanical models directly from MRI spectral\n  data", "author": "D. G. J. Heesterbeek and M. H. C. van Riel and T. van Leeuwen and C. A. T. van den Berg and A. Sbrizzi", "abstract": "  Finding interpretable biomechanical models can provide insight into the\nfunctionality of organs with regard to physiology and disease. However,\nidentifying broadly applicable dynamical models for in vivo tissue remains\nchallenging. In this proof of concept study we propose a reconstruction\nframework for data-driven discovery of dynamical models from experimentally\nobtained undersampled MRI spectral data. The method makes use of the previously\ndeveloped spectro-dynamic framework which allows for reconstruction of\ndisplacement fields at high spatial and temporal resolution required for model\nidentification. The proposed framework combines this method with data-driven\ndiscovery of interpretable models using Sparse Identification of Non-linear\nDynamics (SINDy). The design of the reconstruction algorithm is such that a\nsymbiotic relation between the reconstruction of the displacement fields and\nthe model identification is created. Our method does not rely on periodicity of\nthe motion. It is successfully validated using spectral data of a dynamic\nphantom gathered on a clinical MRI scanner. The dynamic phantom is programmed\nto perform motion adhering to 5 different (non-linear) ordinary differential\nequations. The proposed framework performed better than a 2-step approach where\nthe displacement fields were first reconstructed from the undersampled data\nwithout any information on the model, followed by data-driven discovery of the\nmodel using the reconstructed displacement fields. This study serves as a first\nstep in the direction of data-driven discovery of in vivo models.\n", "link": "http://arxiv.org/abs/2411.06958v1", "date": "2024-11-11", "relevancy": 2.6475, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5348}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5348}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-driven%20discovery%20of%20mechanical%20models%20directly%20from%20MRI%20spectral%0A%20%20data&body=Title%3A%20Data-driven%20discovery%20of%20mechanical%20models%20directly%20from%20MRI%20spectral%0A%20%20data%0AAuthor%3A%20D.%20G.%20J.%20Heesterbeek%20and%20M.%20H.%20C.%20van%20Riel%20and%20T.%20van%20Leeuwen%20and%20C.%20A.%20T.%20van%20den%20Berg%20and%20A.%20Sbrizzi%0AAbstract%3A%20%20%20Finding%20interpretable%20biomechanical%20models%20can%20provide%20insight%20into%20the%0Afunctionality%20of%20organs%20with%20regard%20to%20physiology%20and%20disease.%20However%2C%0Aidentifying%20broadly%20applicable%20dynamical%20models%20for%20in%20vivo%20tissue%20remains%0Achallenging.%20In%20this%20proof%20of%20concept%20study%20we%20propose%20a%20reconstruction%0Aframework%20for%20data-driven%20discovery%20of%20dynamical%20models%20from%20experimentally%0Aobtained%20undersampled%20MRI%20spectral%20data.%20The%20method%20makes%20use%20of%20the%20previously%0Adeveloped%20spectro-dynamic%20framework%20which%20allows%20for%20reconstruction%20of%0Adisplacement%20fields%20at%20high%20spatial%20and%20temporal%20resolution%20required%20for%20model%0Aidentification.%20The%20proposed%20framework%20combines%20this%20method%20with%20data-driven%0Adiscovery%20of%20interpretable%20models%20using%20Sparse%20Identification%20of%20Non-linear%0ADynamics%20%28SINDy%29.%20The%20design%20of%20the%20reconstruction%20algorithm%20is%20such%20that%20a%0Asymbiotic%20relation%20between%20the%20reconstruction%20of%20the%20displacement%20fields%20and%0Athe%20model%20identification%20is%20created.%20Our%20method%20does%20not%20rely%20on%20periodicity%20of%0Athe%20motion.%20It%20is%20successfully%20validated%20using%20spectral%20data%20of%20a%20dynamic%0Aphantom%20gathered%20on%20a%20clinical%20MRI%20scanner.%20The%20dynamic%20phantom%20is%20programmed%0Ato%20perform%20motion%20adhering%20to%205%20different%20%28non-linear%29%20ordinary%20differential%0Aequations.%20The%20proposed%20framework%20performed%20better%20than%20a%202-step%20approach%20where%0Athe%20displacement%20fields%20were%20first%20reconstructed%20from%20the%20undersampled%20data%0Awithout%20any%20information%20on%20the%20model%2C%20followed%20by%20data-driven%20discovery%20of%20the%0Amodel%20using%20the%20reconstructed%20displacement%20fields.%20This%20study%20serves%20as%20a%20first%0Astep%20in%20the%20direction%20of%20data-driven%20discovery%20of%20in%20vivo%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-driven%2520discovery%2520of%2520mechanical%2520models%2520directly%2520from%2520MRI%2520spectral%250A%2520%2520data%26entry.906535625%3DD.%2520G.%2520J.%2520Heesterbeek%2520and%2520M.%2520H.%2520C.%2520van%2520Riel%2520and%2520T.%2520van%2520Leeuwen%2520and%2520C.%2520A.%2520T.%2520van%2520den%2520Berg%2520and%2520A.%2520Sbrizzi%26entry.1292438233%3D%2520%2520Finding%2520interpretable%2520biomechanical%2520models%2520can%2520provide%2520insight%2520into%2520the%250Afunctionality%2520of%2520organs%2520with%2520regard%2520to%2520physiology%2520and%2520disease.%2520However%252C%250Aidentifying%2520broadly%2520applicable%2520dynamical%2520models%2520for%2520in%2520vivo%2520tissue%2520remains%250Achallenging.%2520In%2520this%2520proof%2520of%2520concept%2520study%2520we%2520propose%2520a%2520reconstruction%250Aframework%2520for%2520data-driven%2520discovery%2520of%2520dynamical%2520models%2520from%2520experimentally%250Aobtained%2520undersampled%2520MRI%2520spectral%2520data.%2520The%2520method%2520makes%2520use%2520of%2520the%2520previously%250Adeveloped%2520spectro-dynamic%2520framework%2520which%2520allows%2520for%2520reconstruction%2520of%250Adisplacement%2520fields%2520at%2520high%2520spatial%2520and%2520temporal%2520resolution%2520required%2520for%2520model%250Aidentification.%2520The%2520proposed%2520framework%2520combines%2520this%2520method%2520with%2520data-driven%250Adiscovery%2520of%2520interpretable%2520models%2520using%2520Sparse%2520Identification%2520of%2520Non-linear%250ADynamics%2520%2528SINDy%2529.%2520The%2520design%2520of%2520the%2520reconstruction%2520algorithm%2520is%2520such%2520that%2520a%250Asymbiotic%2520relation%2520between%2520the%2520reconstruction%2520of%2520the%2520displacement%2520fields%2520and%250Athe%2520model%2520identification%2520is%2520created.%2520Our%2520method%2520does%2520not%2520rely%2520on%2520periodicity%2520of%250Athe%2520motion.%2520It%2520is%2520successfully%2520validated%2520using%2520spectral%2520data%2520of%2520a%2520dynamic%250Aphantom%2520gathered%2520on%2520a%2520clinical%2520MRI%2520scanner.%2520The%2520dynamic%2520phantom%2520is%2520programmed%250Ato%2520perform%2520motion%2520adhering%2520to%25205%2520different%2520%2528non-linear%2529%2520ordinary%2520differential%250Aequations.%2520The%2520proposed%2520framework%2520performed%2520better%2520than%2520a%25202-step%2520approach%2520where%250Athe%2520displacement%2520fields%2520were%2520first%2520reconstructed%2520from%2520the%2520undersampled%2520data%250Awithout%2520any%2520information%2520on%2520the%2520model%252C%2520followed%2520by%2520data-driven%2520discovery%2520of%2520the%250Amodel%2520using%2520the%2520reconstructed%2520displacement%2520fields.%2520This%2520study%2520serves%2520as%2520a%2520first%250Astep%2520in%2520the%2520direction%2520of%2520data-driven%2520discovery%2520of%2520in%2520vivo%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-driven%20discovery%20of%20mechanical%20models%20directly%20from%20MRI%20spectral%0A%20%20data&entry.906535625=D.%20G.%20J.%20Heesterbeek%20and%20M.%20H.%20C.%20van%20Riel%20and%20T.%20van%20Leeuwen%20and%20C.%20A.%20T.%20van%20den%20Berg%20and%20A.%20Sbrizzi&entry.1292438233=%20%20Finding%20interpretable%20biomechanical%20models%20can%20provide%20insight%20into%20the%0Afunctionality%20of%20organs%20with%20regard%20to%20physiology%20and%20disease.%20However%2C%0Aidentifying%20broadly%20applicable%20dynamical%20models%20for%20in%20vivo%20tissue%20remains%0Achallenging.%20In%20this%20proof%20of%20concept%20study%20we%20propose%20a%20reconstruction%0Aframework%20for%20data-driven%20discovery%20of%20dynamical%20models%20from%20experimentally%0Aobtained%20undersampled%20MRI%20spectral%20data.%20The%20method%20makes%20use%20of%20the%20previously%0Adeveloped%20spectro-dynamic%20framework%20which%20allows%20for%20reconstruction%20of%0Adisplacement%20fields%20at%20high%20spatial%20and%20temporal%20resolution%20required%20for%20model%0Aidentification.%20The%20proposed%20framework%20combines%20this%20method%20with%20data-driven%0Adiscovery%20of%20interpretable%20models%20using%20Sparse%20Identification%20of%20Non-linear%0ADynamics%20%28SINDy%29.%20The%20design%20of%20the%20reconstruction%20algorithm%20is%20such%20that%20a%0Asymbiotic%20relation%20between%20the%20reconstruction%20of%20the%20displacement%20fields%20and%0Athe%20model%20identification%20is%20created.%20Our%20method%20does%20not%20rely%20on%20periodicity%20of%0Athe%20motion.%20It%20is%20successfully%20validated%20using%20spectral%20data%20of%20a%20dynamic%0Aphantom%20gathered%20on%20a%20clinical%20MRI%20scanner.%20The%20dynamic%20phantom%20is%20programmed%0Ato%20perform%20motion%20adhering%20to%205%20different%20%28non-linear%29%20ordinary%20differential%0Aequations.%20The%20proposed%20framework%20performed%20better%20than%20a%202-step%20approach%20where%0Athe%20displacement%20fields%20were%20first%20reconstructed%20from%20the%20undersampled%20data%0Awithout%20any%20information%20on%20the%20model%2C%20followed%20by%20data-driven%20discovery%20of%20the%0Amodel%20using%20the%20reconstructed%20displacement%20fields.%20This%20study%20serves%20as%20a%20first%0Astep%20in%20the%20direction%20of%20data-driven%20discovery%20of%20in%20vivo%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06958v1&entry.124074799=Read"},
{"title": "MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual\n  Language Modeling", "author": "Tomasz Limisiewicz and Terra Blevins and Hila Gonen and Orevaoghene Ahia and Luke Zettlemoyer", "abstract": "  A major consideration in multilingual language modeling is how to best\nrepresent languages with diverse vocabularies and scripts. Although\ncontemporary text encoding methods cover most of the world's writing systems,\nthey exhibit bias towards the high-resource languages of the Global West. As a\nresult, texts of underrepresented languages tend to be segmented into long\nsequences of linguistically meaningless units. To address the disparities, we\nintroduce a new paradigm that encodes the same information with segments of\nconsistent size across diverse languages. Our encoding convention (MYTE) is\nbased on morphemes, as their inventories are more balanced across languages\nthan characters, which are used in previous methods. We show that MYTE produces\nshorter encodings for all 99 analyzed languages, with the most notable\nimprovements for non-European languages and non-Latin scripts. This, in turn,\nimproves multilingual LM performance and diminishes the perplexity gap\nthroughout diverse languages.\n", "link": "http://arxiv.org/abs/2403.10691v2", "date": "2024-11-11", "relevancy": 2.6393, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MYTE%3A%20Morphology-Driven%20Byte%20Encoding%20for%20Better%20and%20Fairer%20Multilingual%0A%20%20Language%20Modeling&body=Title%3A%20MYTE%3A%20Morphology-Driven%20Byte%20Encoding%20for%20Better%20and%20Fairer%20Multilingual%0A%20%20Language%20Modeling%0AAuthor%3A%20Tomasz%20Limisiewicz%20and%20Terra%20Blevins%20and%20Hila%20Gonen%20and%20Orevaoghene%20Ahia%20and%20Luke%20Zettlemoyer%0AAbstract%3A%20%20%20A%20major%20consideration%20in%20multilingual%20language%20modeling%20is%20how%20to%20best%0Arepresent%20languages%20with%20diverse%20vocabularies%20and%20scripts.%20Although%0Acontemporary%20text%20encoding%20methods%20cover%20most%20of%20the%20world%27s%20writing%20systems%2C%0Athey%20exhibit%20bias%20towards%20the%20high-resource%20languages%20of%20the%20Global%20West.%20As%20a%0Aresult%2C%20texts%20of%20underrepresented%20languages%20tend%20to%20be%20segmented%20into%20long%0Asequences%20of%20linguistically%20meaningless%20units.%20To%20address%20the%20disparities%2C%20we%0Aintroduce%20a%20new%20paradigm%20that%20encodes%20the%20same%20information%20with%20segments%20of%0Aconsistent%20size%20across%20diverse%20languages.%20Our%20encoding%20convention%20%28MYTE%29%20is%0Abased%20on%20morphemes%2C%20as%20their%20inventories%20are%20more%20balanced%20across%20languages%0Athan%20characters%2C%20which%20are%20used%20in%20previous%20methods.%20We%20show%20that%20MYTE%20produces%0Ashorter%20encodings%20for%20all%2099%20analyzed%20languages%2C%20with%20the%20most%20notable%0Aimprovements%20for%20non-European%20languages%20and%20non-Latin%20scripts.%20This%2C%20in%20turn%2C%0Aimproves%20multilingual%20LM%20performance%20and%20diminishes%20the%20perplexity%20gap%0Athroughout%20diverse%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10691v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMYTE%253A%2520Morphology-Driven%2520Byte%2520Encoding%2520for%2520Better%2520and%2520Fairer%2520Multilingual%250A%2520%2520Language%2520Modeling%26entry.906535625%3DTomasz%2520Limisiewicz%2520and%2520Terra%2520Blevins%2520and%2520Hila%2520Gonen%2520and%2520Orevaoghene%2520Ahia%2520and%2520Luke%2520Zettlemoyer%26entry.1292438233%3D%2520%2520A%2520major%2520consideration%2520in%2520multilingual%2520language%2520modeling%2520is%2520how%2520to%2520best%250Arepresent%2520languages%2520with%2520diverse%2520vocabularies%2520and%2520scripts.%2520Although%250Acontemporary%2520text%2520encoding%2520methods%2520cover%2520most%2520of%2520the%2520world%2527s%2520writing%2520systems%252C%250Athey%2520exhibit%2520bias%2520towards%2520the%2520high-resource%2520languages%2520of%2520the%2520Global%2520West.%2520As%2520a%250Aresult%252C%2520texts%2520of%2520underrepresented%2520languages%2520tend%2520to%2520be%2520segmented%2520into%2520long%250Asequences%2520of%2520linguistically%2520meaningless%2520units.%2520To%2520address%2520the%2520disparities%252C%2520we%250Aintroduce%2520a%2520new%2520paradigm%2520that%2520encodes%2520the%2520same%2520information%2520with%2520segments%2520of%250Aconsistent%2520size%2520across%2520diverse%2520languages.%2520Our%2520encoding%2520convention%2520%2528MYTE%2529%2520is%250Abased%2520on%2520morphemes%252C%2520as%2520their%2520inventories%2520are%2520more%2520balanced%2520across%2520languages%250Athan%2520characters%252C%2520which%2520are%2520used%2520in%2520previous%2520methods.%2520We%2520show%2520that%2520MYTE%2520produces%250Ashorter%2520encodings%2520for%2520all%252099%2520analyzed%2520languages%252C%2520with%2520the%2520most%2520notable%250Aimprovements%2520for%2520non-European%2520languages%2520and%2520non-Latin%2520scripts.%2520This%252C%2520in%2520turn%252C%250Aimproves%2520multilingual%2520LM%2520performance%2520and%2520diminishes%2520the%2520perplexity%2520gap%250Athroughout%2520diverse%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10691v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MYTE%3A%20Morphology-Driven%20Byte%20Encoding%20for%20Better%20and%20Fairer%20Multilingual%0A%20%20Language%20Modeling&entry.906535625=Tomasz%20Limisiewicz%20and%20Terra%20Blevins%20and%20Hila%20Gonen%20and%20Orevaoghene%20Ahia%20and%20Luke%20Zettlemoyer&entry.1292438233=%20%20A%20major%20consideration%20in%20multilingual%20language%20modeling%20is%20how%20to%20best%0Arepresent%20languages%20with%20diverse%20vocabularies%20and%20scripts.%20Although%0Acontemporary%20text%20encoding%20methods%20cover%20most%20of%20the%20world%27s%20writing%20systems%2C%0Athey%20exhibit%20bias%20towards%20the%20high-resource%20languages%20of%20the%20Global%20West.%20As%20a%0Aresult%2C%20texts%20of%20underrepresented%20languages%20tend%20to%20be%20segmented%20into%20long%0Asequences%20of%20linguistically%20meaningless%20units.%20To%20address%20the%20disparities%2C%20we%0Aintroduce%20a%20new%20paradigm%20that%20encodes%20the%20same%20information%20with%20segments%20of%0Aconsistent%20size%20across%20diverse%20languages.%20Our%20encoding%20convention%20%28MYTE%29%20is%0Abased%20on%20morphemes%2C%20as%20their%20inventories%20are%20more%20balanced%20across%20languages%0Athan%20characters%2C%20which%20are%20used%20in%20previous%20methods.%20We%20show%20that%20MYTE%20produces%0Ashorter%20encodings%20for%20all%2099%20analyzed%20languages%2C%20with%20the%20most%20notable%0Aimprovements%20for%20non-European%20languages%20and%20non-Latin%20scripts.%20This%2C%20in%20turn%2C%0Aimproves%20multilingual%20LM%20performance%20and%20diminishes%20the%20perplexity%20gap%0Athroughout%20diverse%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10691v2&entry.124074799=Read"},
{"title": "MAN TruckScenes: A multimodal dataset for autonomous trucking in diverse\n  conditions", "author": "Felix Fent and Fabian Kuttenreich and Florian Ruch and Farija Rizwin and Stefan Juergens and Lorenz Lechermann and Christian Nissler and Andrea Perl and Ulrich Voll and Min Yan and Markus Lienkamp", "abstract": "  Autonomous trucking is a promising technology that can greatly impact modern\nlogistics and the environment. Ensuring its safety on public roads is one of\nthe main duties that requires an accurate perception of the environment. To\nachieve this, machine learning methods rely on large datasets, but to this day,\nno such datasets are available for autonomous trucks. In this work, we present\nMAN TruckScenes, the first multimodal dataset for autonomous trucking. MAN\nTruckScenes allows the research community to come into contact with\ntruck-specific challenges, such as trailer occlusions, novel sensor\nperspectives, and terminal environments for the first time. It comprises more\nthan 740 scenes of 20s each within a multitude of different environmental\nconditions. The sensor set includes 4 cameras, 6 lidar, 6 radar sensors, 2\nIMUs, and a high-precision GNSS. The dataset's 3D bounding boxes were manually\nannotated and carefully reviewed to achieve a high quality standard. Bounding\nboxes are available for 27 object classes, 15 attributes, and a range of more\nthan 230m. The scenes are tagged according to 34 distinct scene tags, and all\nobjects are tracked throughout the scene to promote a wide range of\napplications. Additionally, MAN TruckScenes is the first dataset to provide 4D\nradar data with 360{\\deg} coverage and is thereby the largest radar dataset\nwith annotated 3D bounding boxes. Finally, we provide extensive dataset\nanalysis and baseline results. The dataset, development kit, and more are\navailable online.\n", "link": "http://arxiv.org/abs/2407.07462v2", "date": "2024-11-11", "relevancy": 2.6308, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAN%20TruckScenes%3A%20A%20multimodal%20dataset%20for%20autonomous%20trucking%20in%20diverse%0A%20%20conditions&body=Title%3A%20MAN%20TruckScenes%3A%20A%20multimodal%20dataset%20for%20autonomous%20trucking%20in%20diverse%0A%20%20conditions%0AAuthor%3A%20Felix%20Fent%20and%20Fabian%20Kuttenreich%20and%20Florian%20Ruch%20and%20Farija%20Rizwin%20and%20Stefan%20Juergens%20and%20Lorenz%20Lechermann%20and%20Christian%20Nissler%20and%20Andrea%20Perl%20and%20Ulrich%20Voll%20and%20Min%20Yan%20and%20Markus%20Lienkamp%0AAbstract%3A%20%20%20Autonomous%20trucking%20is%20a%20promising%20technology%20that%20can%20greatly%20impact%20modern%0Alogistics%20and%20the%20environment.%20Ensuring%20its%20safety%20on%20public%20roads%20is%20one%20of%0Athe%20main%20duties%20that%20requires%20an%20accurate%20perception%20of%20the%20environment.%20To%0Aachieve%20this%2C%20machine%20learning%20methods%20rely%20on%20large%20datasets%2C%20but%20to%20this%20day%2C%0Ano%20such%20datasets%20are%20available%20for%20autonomous%20trucks.%20In%20this%20work%2C%20we%20present%0AMAN%20TruckScenes%2C%20the%20first%20multimodal%20dataset%20for%20autonomous%20trucking.%20MAN%0ATruckScenes%20allows%20the%20research%20community%20to%20come%20into%20contact%20with%0Atruck-specific%20challenges%2C%20such%20as%20trailer%20occlusions%2C%20novel%20sensor%0Aperspectives%2C%20and%20terminal%20environments%20for%20the%20first%20time.%20It%20comprises%20more%0Athan%20740%20scenes%20of%2020s%20each%20within%20a%20multitude%20of%20different%20environmental%0Aconditions.%20The%20sensor%20set%20includes%204%20cameras%2C%206%20lidar%2C%206%20radar%20sensors%2C%202%0AIMUs%2C%20and%20a%20high-precision%20GNSS.%20The%20dataset%27s%203D%20bounding%20boxes%20were%20manually%0Aannotated%20and%20carefully%20reviewed%20to%20achieve%20a%20high%20quality%20standard.%20Bounding%0Aboxes%20are%20available%20for%2027%20object%20classes%2C%2015%20attributes%2C%20and%20a%20range%20of%20more%0Athan%20230m.%20The%20scenes%20are%20tagged%20according%20to%2034%20distinct%20scene%20tags%2C%20and%20all%0Aobjects%20are%20tracked%20throughout%20the%20scene%20to%20promote%20a%20wide%20range%20of%0Aapplications.%20Additionally%2C%20MAN%20TruckScenes%20is%20the%20first%20dataset%20to%20provide%204D%0Aradar%20data%20with%20360%7B%5Cdeg%7D%20coverage%20and%20is%20thereby%20the%20largest%20radar%20dataset%0Awith%20annotated%203D%20bounding%20boxes.%20Finally%2C%20we%20provide%20extensive%20dataset%0Aanalysis%20and%20baseline%20results.%20The%20dataset%2C%20development%20kit%2C%20and%20more%20are%0Aavailable%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07462v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAN%2520TruckScenes%253A%2520A%2520multimodal%2520dataset%2520for%2520autonomous%2520trucking%2520in%2520diverse%250A%2520%2520conditions%26entry.906535625%3DFelix%2520Fent%2520and%2520Fabian%2520Kuttenreich%2520and%2520Florian%2520Ruch%2520and%2520Farija%2520Rizwin%2520and%2520Stefan%2520Juergens%2520and%2520Lorenz%2520Lechermann%2520and%2520Christian%2520Nissler%2520and%2520Andrea%2520Perl%2520and%2520Ulrich%2520Voll%2520and%2520Min%2520Yan%2520and%2520Markus%2520Lienkamp%26entry.1292438233%3D%2520%2520Autonomous%2520trucking%2520is%2520a%2520promising%2520technology%2520that%2520can%2520greatly%2520impact%2520modern%250Alogistics%2520and%2520the%2520environment.%2520Ensuring%2520its%2520safety%2520on%2520public%2520roads%2520is%2520one%2520of%250Athe%2520main%2520duties%2520that%2520requires%2520an%2520accurate%2520perception%2520of%2520the%2520environment.%2520To%250Aachieve%2520this%252C%2520machine%2520learning%2520methods%2520rely%2520on%2520large%2520datasets%252C%2520but%2520to%2520this%2520day%252C%250Ano%2520such%2520datasets%2520are%2520available%2520for%2520autonomous%2520trucks.%2520In%2520this%2520work%252C%2520we%2520present%250AMAN%2520TruckScenes%252C%2520the%2520first%2520multimodal%2520dataset%2520for%2520autonomous%2520trucking.%2520MAN%250ATruckScenes%2520allows%2520the%2520research%2520community%2520to%2520come%2520into%2520contact%2520with%250Atruck-specific%2520challenges%252C%2520such%2520as%2520trailer%2520occlusions%252C%2520novel%2520sensor%250Aperspectives%252C%2520and%2520terminal%2520environments%2520for%2520the%2520first%2520time.%2520It%2520comprises%2520more%250Athan%2520740%2520scenes%2520of%252020s%2520each%2520within%2520a%2520multitude%2520of%2520different%2520environmental%250Aconditions.%2520The%2520sensor%2520set%2520includes%25204%2520cameras%252C%25206%2520lidar%252C%25206%2520radar%2520sensors%252C%25202%250AIMUs%252C%2520and%2520a%2520high-precision%2520GNSS.%2520The%2520dataset%2527s%25203D%2520bounding%2520boxes%2520were%2520manually%250Aannotated%2520and%2520carefully%2520reviewed%2520to%2520achieve%2520a%2520high%2520quality%2520standard.%2520Bounding%250Aboxes%2520are%2520available%2520for%252027%2520object%2520classes%252C%252015%2520attributes%252C%2520and%2520a%2520range%2520of%2520more%250Athan%2520230m.%2520The%2520scenes%2520are%2520tagged%2520according%2520to%252034%2520distinct%2520scene%2520tags%252C%2520and%2520all%250Aobjects%2520are%2520tracked%2520throughout%2520the%2520scene%2520to%2520promote%2520a%2520wide%2520range%2520of%250Aapplications.%2520Additionally%252C%2520MAN%2520TruckScenes%2520is%2520the%2520first%2520dataset%2520to%2520provide%25204D%250Aradar%2520data%2520with%2520360%257B%255Cdeg%257D%2520coverage%2520and%2520is%2520thereby%2520the%2520largest%2520radar%2520dataset%250Awith%2520annotated%25203D%2520bounding%2520boxes.%2520Finally%252C%2520we%2520provide%2520extensive%2520dataset%250Aanalysis%2520and%2520baseline%2520results.%2520The%2520dataset%252C%2520development%2520kit%252C%2520and%2520more%2520are%250Aavailable%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07462v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAN%20TruckScenes%3A%20A%20multimodal%20dataset%20for%20autonomous%20trucking%20in%20diverse%0A%20%20conditions&entry.906535625=Felix%20Fent%20and%20Fabian%20Kuttenreich%20and%20Florian%20Ruch%20and%20Farija%20Rizwin%20and%20Stefan%20Juergens%20and%20Lorenz%20Lechermann%20and%20Christian%20Nissler%20and%20Andrea%20Perl%20and%20Ulrich%20Voll%20and%20Min%20Yan%20and%20Markus%20Lienkamp&entry.1292438233=%20%20Autonomous%20trucking%20is%20a%20promising%20technology%20that%20can%20greatly%20impact%20modern%0Alogistics%20and%20the%20environment.%20Ensuring%20its%20safety%20on%20public%20roads%20is%20one%20of%0Athe%20main%20duties%20that%20requires%20an%20accurate%20perception%20of%20the%20environment.%20To%0Aachieve%20this%2C%20machine%20learning%20methods%20rely%20on%20large%20datasets%2C%20but%20to%20this%20day%2C%0Ano%20such%20datasets%20are%20available%20for%20autonomous%20trucks.%20In%20this%20work%2C%20we%20present%0AMAN%20TruckScenes%2C%20the%20first%20multimodal%20dataset%20for%20autonomous%20trucking.%20MAN%0ATruckScenes%20allows%20the%20research%20community%20to%20come%20into%20contact%20with%0Atruck-specific%20challenges%2C%20such%20as%20trailer%20occlusions%2C%20novel%20sensor%0Aperspectives%2C%20and%20terminal%20environments%20for%20the%20first%20time.%20It%20comprises%20more%0Athan%20740%20scenes%20of%2020s%20each%20within%20a%20multitude%20of%20different%20environmental%0Aconditions.%20The%20sensor%20set%20includes%204%20cameras%2C%206%20lidar%2C%206%20radar%20sensors%2C%202%0AIMUs%2C%20and%20a%20high-precision%20GNSS.%20The%20dataset%27s%203D%20bounding%20boxes%20were%20manually%0Aannotated%20and%20carefully%20reviewed%20to%20achieve%20a%20high%20quality%20standard.%20Bounding%0Aboxes%20are%20available%20for%2027%20object%20classes%2C%2015%20attributes%2C%20and%20a%20range%20of%20more%0Athan%20230m.%20The%20scenes%20are%20tagged%20according%20to%2034%20distinct%20scene%20tags%2C%20and%20all%0Aobjects%20are%20tracked%20throughout%20the%20scene%20to%20promote%20a%20wide%20range%20of%0Aapplications.%20Additionally%2C%20MAN%20TruckScenes%20is%20the%20first%20dataset%20to%20provide%204D%0Aradar%20data%20with%20360%7B%5Cdeg%7D%20coverage%20and%20is%20thereby%20the%20largest%20radar%20dataset%0Awith%20annotated%203D%20bounding%20boxes.%20Finally%2C%20we%20provide%20extensive%20dataset%0Aanalysis%20and%20baseline%20results.%20The%20dataset%2C%20development%20kit%2C%20and%20more%20are%0Aavailable%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07462v2&entry.124074799=Read"},
{"title": "Learning Dynamics from Multicellular Graphs with Deep Neural Networks", "author": "Haiqian Yang and Florian Meyer and Shaoxun Huang and Liu Yang and Cristiana Lungu and Monilola A. Olayioye and Markus J. Buehler and Ming Guo", "abstract": "  Multicellular self-assembly into functional structures is a dynamic process\nthat is critical in the development and diseases, including embryo development,\norgan formation, tumor invasion, and others. Being able to infer collective\ncell migratory dynamics from their static configuration is valuable for both\nunderstanding and predicting these complex processes. However, the\nidentification of structural features that can indicate multicellular motion\nhas been difficult, and existing metrics largely rely on physical instincts.\nHere we show that using a graph neural network (GNN), the motion of\nmulticellular collectives can be inferred from a static snapshot of cell\npositions, in both experimental and synthetic datasets.\n", "link": "http://arxiv.org/abs/2401.12196v3", "date": "2024-11-11", "relevancy": 2.6023, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5436}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5126}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Dynamics%20from%20Multicellular%20Graphs%20with%20Deep%20Neural%20Networks&body=Title%3A%20Learning%20Dynamics%20from%20Multicellular%20Graphs%20with%20Deep%20Neural%20Networks%0AAuthor%3A%20Haiqian%20Yang%20and%20Florian%20Meyer%20and%20Shaoxun%20Huang%20and%20Liu%20Yang%20and%20Cristiana%20Lungu%20and%20Monilola%20A.%20Olayioye%20and%20Markus%20J.%20Buehler%20and%20Ming%20Guo%0AAbstract%3A%20%20%20Multicellular%20self-assembly%20into%20functional%20structures%20is%20a%20dynamic%20process%0Athat%20is%20critical%20in%20the%20development%20and%20diseases%2C%20including%20embryo%20development%2C%0Aorgan%20formation%2C%20tumor%20invasion%2C%20and%20others.%20Being%20able%20to%20infer%20collective%0Acell%20migratory%20dynamics%20from%20their%20static%20configuration%20is%20valuable%20for%20both%0Aunderstanding%20and%20predicting%20these%20complex%20processes.%20However%2C%20the%0Aidentification%20of%20structural%20features%20that%20can%20indicate%20multicellular%20motion%0Ahas%20been%20difficult%2C%20and%20existing%20metrics%20largely%20rely%20on%20physical%20instincts.%0AHere%20we%20show%20that%20using%20a%20graph%20neural%20network%20%28GNN%29%2C%20the%20motion%20of%0Amulticellular%20collectives%20can%20be%20inferred%20from%20a%20static%20snapshot%20of%20cell%0Apositions%2C%20in%20both%20experimental%20and%20synthetic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12196v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Dynamics%2520from%2520Multicellular%2520Graphs%2520with%2520Deep%2520Neural%2520Networks%26entry.906535625%3DHaiqian%2520Yang%2520and%2520Florian%2520Meyer%2520and%2520Shaoxun%2520Huang%2520and%2520Liu%2520Yang%2520and%2520Cristiana%2520Lungu%2520and%2520Monilola%2520A.%2520Olayioye%2520and%2520Markus%2520J.%2520Buehler%2520and%2520Ming%2520Guo%26entry.1292438233%3D%2520%2520Multicellular%2520self-assembly%2520into%2520functional%2520structures%2520is%2520a%2520dynamic%2520process%250Athat%2520is%2520critical%2520in%2520the%2520development%2520and%2520diseases%252C%2520including%2520embryo%2520development%252C%250Aorgan%2520formation%252C%2520tumor%2520invasion%252C%2520and%2520others.%2520Being%2520able%2520to%2520infer%2520collective%250Acell%2520migratory%2520dynamics%2520from%2520their%2520static%2520configuration%2520is%2520valuable%2520for%2520both%250Aunderstanding%2520and%2520predicting%2520these%2520complex%2520processes.%2520However%252C%2520the%250Aidentification%2520of%2520structural%2520features%2520that%2520can%2520indicate%2520multicellular%2520motion%250Ahas%2520been%2520difficult%252C%2520and%2520existing%2520metrics%2520largely%2520rely%2520on%2520physical%2520instincts.%250AHere%2520we%2520show%2520that%2520using%2520a%2520graph%2520neural%2520network%2520%2528GNN%2529%252C%2520the%2520motion%2520of%250Amulticellular%2520collectives%2520can%2520be%2520inferred%2520from%2520a%2520static%2520snapshot%2520of%2520cell%250Apositions%252C%2520in%2520both%2520experimental%2520and%2520synthetic%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12196v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Dynamics%20from%20Multicellular%20Graphs%20with%20Deep%20Neural%20Networks&entry.906535625=Haiqian%20Yang%20and%20Florian%20Meyer%20and%20Shaoxun%20Huang%20and%20Liu%20Yang%20and%20Cristiana%20Lungu%20and%20Monilola%20A.%20Olayioye%20and%20Markus%20J.%20Buehler%20and%20Ming%20Guo&entry.1292438233=%20%20Multicellular%20self-assembly%20into%20functional%20structures%20is%20a%20dynamic%20process%0Athat%20is%20critical%20in%20the%20development%20and%20diseases%2C%20including%20embryo%20development%2C%0Aorgan%20formation%2C%20tumor%20invasion%2C%20and%20others.%20Being%20able%20to%20infer%20collective%0Acell%20migratory%20dynamics%20from%20their%20static%20configuration%20is%20valuable%20for%20both%0Aunderstanding%20and%20predicting%20these%20complex%20processes.%20However%2C%20the%0Aidentification%20of%20structural%20features%20that%20can%20indicate%20multicellular%20motion%0Ahas%20been%20difficult%2C%20and%20existing%20metrics%20largely%20rely%20on%20physical%20instincts.%0AHere%20we%20show%20that%20using%20a%20graph%20neural%20network%20%28GNN%29%2C%20the%20motion%20of%0Amulticellular%20collectives%20can%20be%20inferred%20from%20a%20static%20snapshot%20of%20cell%0Apositions%2C%20in%20both%20experimental%20and%20synthetic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12196v3&entry.124074799=Read"},
{"title": "Revisiting, Benchmarking and Understanding Unsupervised Graph Domain\n  Adaptation", "author": "Meihan Liu and Zhen Zhang and Jiachen Tang and Jiajun Bu and Bingsheng He and Sheng Zhou", "abstract": "  Unsupervised Graph Domain Adaptation (UGDA) involves the transfer of\nknowledge from a label-rich source graph to an unlabeled target graph under\ndomain discrepancies. Despite the proliferation of methods designed for this\nemerging task, the lack of standard experimental settings and fair performance\ncomparisons makes it challenging to understand which and when models perform\nwell across different scenarios. To fill this gap, we present the first\ncomprehensive benchmark for unsupervised graph domain adaptation named\nGDABench, which encompasses 16 algorithms across 5 datasets with 74 adaptation\ntasks. Through extensive experiments, we observe that the performance of\ncurrent UGDA models varies significantly across different datasets and\nadaptation scenarios. Specifically, we recognize that when the source and\ntarget graphs face significant distribution shifts, it is imperative to\nformulate strategies to effectively address and mitigate graph structural\nshifts. We also find that with appropriate neighbourhood aggregation\nmechanisms, simple GNN variants can even surpass state-of-the-art UGDA\nbaselines. To facilitate reproducibility, we have developed an easy-to-use\nlibrary PyGDA for training and evaluating existing UGDA methods, providing a\nstandardized platform in this community. Our source codes and datasets can be\nfound at: https://github.com/pygda-team/pygda.\n", "link": "http://arxiv.org/abs/2407.11052v2", "date": "2024-11-11", "relevancy": 2.6022, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5347}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.525}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%2C%20Benchmarking%20and%20Understanding%20Unsupervised%20Graph%20Domain%0A%20%20Adaptation&body=Title%3A%20Revisiting%2C%20Benchmarking%20and%20Understanding%20Unsupervised%20Graph%20Domain%0A%20%20Adaptation%0AAuthor%3A%20Meihan%20Liu%20and%20Zhen%20Zhang%20and%20Jiachen%20Tang%20and%20Jiajun%20Bu%20and%20Bingsheng%20He%20and%20Sheng%20Zhou%0AAbstract%3A%20%20%20Unsupervised%20Graph%20Domain%20Adaptation%20%28UGDA%29%20involves%20the%20transfer%20of%0Aknowledge%20from%20a%20label-rich%20source%20graph%20to%20an%20unlabeled%20target%20graph%20under%0Adomain%20discrepancies.%20Despite%20the%20proliferation%20of%20methods%20designed%20for%20this%0Aemerging%20task%2C%20the%20lack%20of%20standard%20experimental%20settings%20and%20fair%20performance%0Acomparisons%20makes%20it%20challenging%20to%20understand%20which%20and%20when%20models%20perform%0Awell%20across%20different%20scenarios.%20To%20fill%20this%20gap%2C%20we%20present%20the%20first%0Acomprehensive%20benchmark%20for%20unsupervised%20graph%20domain%20adaptation%20named%0AGDABench%2C%20which%20encompasses%2016%20algorithms%20across%205%20datasets%20with%2074%20adaptation%0Atasks.%20Through%20extensive%20experiments%2C%20we%20observe%20that%20the%20performance%20of%0Acurrent%20UGDA%20models%20varies%20significantly%20across%20different%20datasets%20and%0Aadaptation%20scenarios.%20Specifically%2C%20we%20recognize%20that%20when%20the%20source%20and%0Atarget%20graphs%20face%20significant%20distribution%20shifts%2C%20it%20is%20imperative%20to%0Aformulate%20strategies%20to%20effectively%20address%20and%20mitigate%20graph%20structural%0Ashifts.%20We%20also%20find%20that%20with%20appropriate%20neighbourhood%20aggregation%0Amechanisms%2C%20simple%20GNN%20variants%20can%20even%20surpass%20state-of-the-art%20UGDA%0Abaselines.%20To%20facilitate%20reproducibility%2C%20we%20have%20developed%20an%20easy-to-use%0Alibrary%20PyGDA%20for%20training%20and%20evaluating%20existing%20UGDA%20methods%2C%20providing%20a%0Astandardized%20platform%20in%20this%20community.%20Our%20source%20codes%20and%20datasets%20can%20be%0Afound%20at%3A%20https%3A//github.com/pygda-team/pygda.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%252C%2520Benchmarking%2520and%2520Understanding%2520Unsupervised%2520Graph%2520Domain%250A%2520%2520Adaptation%26entry.906535625%3DMeihan%2520Liu%2520and%2520Zhen%2520Zhang%2520and%2520Jiachen%2520Tang%2520and%2520Jiajun%2520Bu%2520and%2520Bingsheng%2520He%2520and%2520Sheng%2520Zhou%26entry.1292438233%3D%2520%2520Unsupervised%2520Graph%2520Domain%2520Adaptation%2520%2528UGDA%2529%2520involves%2520the%2520transfer%2520of%250Aknowledge%2520from%2520a%2520label-rich%2520source%2520graph%2520to%2520an%2520unlabeled%2520target%2520graph%2520under%250Adomain%2520discrepancies.%2520Despite%2520the%2520proliferation%2520of%2520methods%2520designed%2520for%2520this%250Aemerging%2520task%252C%2520the%2520lack%2520of%2520standard%2520experimental%2520settings%2520and%2520fair%2520performance%250Acomparisons%2520makes%2520it%2520challenging%2520to%2520understand%2520which%2520and%2520when%2520models%2520perform%250Awell%2520across%2520different%2520scenarios.%2520To%2520fill%2520this%2520gap%252C%2520we%2520present%2520the%2520first%250Acomprehensive%2520benchmark%2520for%2520unsupervised%2520graph%2520domain%2520adaptation%2520named%250AGDABench%252C%2520which%2520encompasses%252016%2520algorithms%2520across%25205%2520datasets%2520with%252074%2520adaptation%250Atasks.%2520Through%2520extensive%2520experiments%252C%2520we%2520observe%2520that%2520the%2520performance%2520of%250Acurrent%2520UGDA%2520models%2520varies%2520significantly%2520across%2520different%2520datasets%2520and%250Aadaptation%2520scenarios.%2520Specifically%252C%2520we%2520recognize%2520that%2520when%2520the%2520source%2520and%250Atarget%2520graphs%2520face%2520significant%2520distribution%2520shifts%252C%2520it%2520is%2520imperative%2520to%250Aformulate%2520strategies%2520to%2520effectively%2520address%2520and%2520mitigate%2520graph%2520structural%250Ashifts.%2520We%2520also%2520find%2520that%2520with%2520appropriate%2520neighbourhood%2520aggregation%250Amechanisms%252C%2520simple%2520GNN%2520variants%2520can%2520even%2520surpass%2520state-of-the-art%2520UGDA%250Abaselines.%2520To%2520facilitate%2520reproducibility%252C%2520we%2520have%2520developed%2520an%2520easy-to-use%250Alibrary%2520PyGDA%2520for%2520training%2520and%2520evaluating%2520existing%2520UGDA%2520methods%252C%2520providing%2520a%250Astandardized%2520platform%2520in%2520this%2520community.%2520Our%2520source%2520codes%2520and%2520datasets%2520can%2520be%250Afound%2520at%253A%2520https%253A//github.com/pygda-team/pygda.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%2C%20Benchmarking%20and%20Understanding%20Unsupervised%20Graph%20Domain%0A%20%20Adaptation&entry.906535625=Meihan%20Liu%20and%20Zhen%20Zhang%20and%20Jiachen%20Tang%20and%20Jiajun%20Bu%20and%20Bingsheng%20He%20and%20Sheng%20Zhou&entry.1292438233=%20%20Unsupervised%20Graph%20Domain%20Adaptation%20%28UGDA%29%20involves%20the%20transfer%20of%0Aknowledge%20from%20a%20label-rich%20source%20graph%20to%20an%20unlabeled%20target%20graph%20under%0Adomain%20discrepancies.%20Despite%20the%20proliferation%20of%20methods%20designed%20for%20this%0Aemerging%20task%2C%20the%20lack%20of%20standard%20experimental%20settings%20and%20fair%20performance%0Acomparisons%20makes%20it%20challenging%20to%20understand%20which%20and%20when%20models%20perform%0Awell%20across%20different%20scenarios.%20To%20fill%20this%20gap%2C%20we%20present%20the%20first%0Acomprehensive%20benchmark%20for%20unsupervised%20graph%20domain%20adaptation%20named%0AGDABench%2C%20which%20encompasses%2016%20algorithms%20across%205%20datasets%20with%2074%20adaptation%0Atasks.%20Through%20extensive%20experiments%2C%20we%20observe%20that%20the%20performance%20of%0Acurrent%20UGDA%20models%20varies%20significantly%20across%20different%20datasets%20and%0Aadaptation%20scenarios.%20Specifically%2C%20we%20recognize%20that%20when%20the%20source%20and%0Atarget%20graphs%20face%20significant%20distribution%20shifts%2C%20it%20is%20imperative%20to%0Aformulate%20strategies%20to%20effectively%20address%20and%20mitigate%20graph%20structural%0Ashifts.%20We%20also%20find%20that%20with%20appropriate%20neighbourhood%20aggregation%0Amechanisms%2C%20simple%20GNN%20variants%20can%20even%20surpass%20state-of-the-art%20UGDA%0Abaselines.%20To%20facilitate%20reproducibility%2C%20we%20have%20developed%20an%20easy-to-use%0Alibrary%20PyGDA%20for%20training%20and%20evaluating%20existing%20UGDA%20methods%2C%20providing%20a%0Astandardized%20platform%20in%20this%20community.%20Our%20source%20codes%20and%20datasets%20can%20be%0Afound%20at%3A%20https%3A//github.com/pygda-team/pygda.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11052v2&entry.124074799=Read"},
{"title": "Decoding Visual Experience and Mapping Semantics through Whole-Brain\n  Analysis Using fMRI Foundation Models", "author": "Yanchen Wang and Adam Turnbull and Tiange Xiang and Yunlong Xu and Sa Zhou and Adnan Masoud and Shekoofeh Azizi and Feng Vankee Lin and Ehsan Adeli", "abstract": "  Neural decoding, the process of understanding how brain activity corresponds\nto different stimuli, has been a primary objective in cognitive sciences. Over\nthe past three decades, advancements in functional Magnetic Resonance Imaging\nand machine learning have greatly improved our ability to map visual stimuli to\nbrain activity, especially in the visual cortex. Concurrently, research has\nexpanded into decoding more complex processes like language and memory across\nthe whole brain, utilizing techniques to handle greater variability and improve\nsignal accuracy. We argue that \"seeing\" involves more than just mapping visual\nstimuli onto the visual cortex; it engages the entire brain, as various\nemotions and cognitive states can emerge from observing different scenes. In\nthis paper, we develop algorithms to enhance our understanding of visual\nprocesses by incorporating whole-brain activation maps while individuals are\nexposed to visual stimuli. We utilize large-scale fMRI encoders and Image\ngenerative models pre-trained on large public datasets, which are then\nfine-tuned through Image-fMRI contrastive learning. Our models hence can decode\nvisual experience across the entire cerebral cortex, surpassing the traditional\nconfines of the visual cortex. We first compare our method with\nstate-of-the-art approaches to decoding visual processing and show improved\npredictive semantic accuracy by 43%. A network ablation analysis suggests that\nbeyond the visual cortex, the default mode network contributes most to decoding\nstimuli, in line with the proposed role of this network in sense-making and\nsemantic processing. Additionally, we implemented zero-shot imagination\ndecoding on an extra validation dataset, achieving a p-value of 0.0206 for\nmapping the reconstructed images and ground-truth text stimuli, which\nsubstantiates the model's capability to capture semantic meanings across\nvarious scenarios.\n", "link": "http://arxiv.org/abs/2411.07121v1", "date": "2024-11-11", "relevancy": 2.592, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.67}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.67}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Visual%20Experience%20and%20Mapping%20Semantics%20through%20Whole-Brain%0A%20%20Analysis%20Using%20fMRI%20Foundation%20Models&body=Title%3A%20Decoding%20Visual%20Experience%20and%20Mapping%20Semantics%20through%20Whole-Brain%0A%20%20Analysis%20Using%20fMRI%20Foundation%20Models%0AAuthor%3A%20Yanchen%20Wang%20and%20Adam%20Turnbull%20and%20Tiange%20Xiang%20and%20Yunlong%20Xu%20and%20Sa%20Zhou%20and%20Adnan%20Masoud%20and%20Shekoofeh%20Azizi%20and%20Feng%20Vankee%20Lin%20and%20Ehsan%20Adeli%0AAbstract%3A%20%20%20Neural%20decoding%2C%20the%20process%20of%20understanding%20how%20brain%20activity%20corresponds%0Ato%20different%20stimuli%2C%20has%20been%20a%20primary%20objective%20in%20cognitive%20sciences.%20Over%0Athe%20past%20three%20decades%2C%20advancements%20in%20functional%20Magnetic%20Resonance%20Imaging%0Aand%20machine%20learning%20have%20greatly%20improved%20our%20ability%20to%20map%20visual%20stimuli%20to%0Abrain%20activity%2C%20especially%20in%20the%20visual%20cortex.%20Concurrently%2C%20research%20has%0Aexpanded%20into%20decoding%20more%20complex%20processes%20like%20language%20and%20memory%20across%0Athe%20whole%20brain%2C%20utilizing%20techniques%20to%20handle%20greater%20variability%20and%20improve%0Asignal%20accuracy.%20We%20argue%20that%20%22seeing%22%20involves%20more%20than%20just%20mapping%20visual%0Astimuli%20onto%20the%20visual%20cortex%3B%20it%20engages%20the%20entire%20brain%2C%20as%20various%0Aemotions%20and%20cognitive%20states%20can%20emerge%20from%20observing%20different%20scenes.%20In%0Athis%20paper%2C%20we%20develop%20algorithms%20to%20enhance%20our%20understanding%20of%20visual%0Aprocesses%20by%20incorporating%20whole-brain%20activation%20maps%20while%20individuals%20are%0Aexposed%20to%20visual%20stimuli.%20We%20utilize%20large-scale%20fMRI%20encoders%20and%20Image%0Agenerative%20models%20pre-trained%20on%20large%20public%20datasets%2C%20which%20are%20then%0Afine-tuned%20through%20Image-fMRI%20contrastive%20learning.%20Our%20models%20hence%20can%20decode%0Avisual%20experience%20across%20the%20entire%20cerebral%20cortex%2C%20surpassing%20the%20traditional%0Aconfines%20of%20the%20visual%20cortex.%20We%20first%20compare%20our%20method%20with%0Astate-of-the-art%20approaches%20to%20decoding%20visual%20processing%20and%20show%20improved%0Apredictive%20semantic%20accuracy%20by%2043%25.%20A%20network%20ablation%20analysis%20suggests%20that%0Abeyond%20the%20visual%20cortex%2C%20the%20default%20mode%20network%20contributes%20most%20to%20decoding%0Astimuli%2C%20in%20line%20with%20the%20proposed%20role%20of%20this%20network%20in%20sense-making%20and%0Asemantic%20processing.%20Additionally%2C%20we%20implemented%20zero-shot%20imagination%0Adecoding%20on%20an%20extra%20validation%20dataset%2C%20achieving%20a%20p-value%20of%200.0206%20for%0Amapping%20the%20reconstructed%20images%20and%20ground-truth%20text%20stimuli%2C%20which%0Asubstantiates%20the%20model%27s%20capability%20to%20capture%20semantic%20meanings%20across%0Avarious%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Visual%2520Experience%2520and%2520Mapping%2520Semantics%2520through%2520Whole-Brain%250A%2520%2520Analysis%2520Using%2520fMRI%2520Foundation%2520Models%26entry.906535625%3DYanchen%2520Wang%2520and%2520Adam%2520Turnbull%2520and%2520Tiange%2520Xiang%2520and%2520Yunlong%2520Xu%2520and%2520Sa%2520Zhou%2520and%2520Adnan%2520Masoud%2520and%2520Shekoofeh%2520Azizi%2520and%2520Feng%2520Vankee%2520Lin%2520and%2520Ehsan%2520Adeli%26entry.1292438233%3D%2520%2520Neural%2520decoding%252C%2520the%2520process%2520of%2520understanding%2520how%2520brain%2520activity%2520corresponds%250Ato%2520different%2520stimuli%252C%2520has%2520been%2520a%2520primary%2520objective%2520in%2520cognitive%2520sciences.%2520Over%250Athe%2520past%2520three%2520decades%252C%2520advancements%2520in%2520functional%2520Magnetic%2520Resonance%2520Imaging%250Aand%2520machine%2520learning%2520have%2520greatly%2520improved%2520our%2520ability%2520to%2520map%2520visual%2520stimuli%2520to%250Abrain%2520activity%252C%2520especially%2520in%2520the%2520visual%2520cortex.%2520Concurrently%252C%2520research%2520has%250Aexpanded%2520into%2520decoding%2520more%2520complex%2520processes%2520like%2520language%2520and%2520memory%2520across%250Athe%2520whole%2520brain%252C%2520utilizing%2520techniques%2520to%2520handle%2520greater%2520variability%2520and%2520improve%250Asignal%2520accuracy.%2520We%2520argue%2520that%2520%2522seeing%2522%2520involves%2520more%2520than%2520just%2520mapping%2520visual%250Astimuli%2520onto%2520the%2520visual%2520cortex%253B%2520it%2520engages%2520the%2520entire%2520brain%252C%2520as%2520various%250Aemotions%2520and%2520cognitive%2520states%2520can%2520emerge%2520from%2520observing%2520different%2520scenes.%2520In%250Athis%2520paper%252C%2520we%2520develop%2520algorithms%2520to%2520enhance%2520our%2520understanding%2520of%2520visual%250Aprocesses%2520by%2520incorporating%2520whole-brain%2520activation%2520maps%2520while%2520individuals%2520are%250Aexposed%2520to%2520visual%2520stimuli.%2520We%2520utilize%2520large-scale%2520fMRI%2520encoders%2520and%2520Image%250Agenerative%2520models%2520pre-trained%2520on%2520large%2520public%2520datasets%252C%2520which%2520are%2520then%250Afine-tuned%2520through%2520Image-fMRI%2520contrastive%2520learning.%2520Our%2520models%2520hence%2520can%2520decode%250Avisual%2520experience%2520across%2520the%2520entire%2520cerebral%2520cortex%252C%2520surpassing%2520the%2520traditional%250Aconfines%2520of%2520the%2520visual%2520cortex.%2520We%2520first%2520compare%2520our%2520method%2520with%250Astate-of-the-art%2520approaches%2520to%2520decoding%2520visual%2520processing%2520and%2520show%2520improved%250Apredictive%2520semantic%2520accuracy%2520by%252043%2525.%2520A%2520network%2520ablation%2520analysis%2520suggests%2520that%250Abeyond%2520the%2520visual%2520cortex%252C%2520the%2520default%2520mode%2520network%2520contributes%2520most%2520to%2520decoding%250Astimuli%252C%2520in%2520line%2520with%2520the%2520proposed%2520role%2520of%2520this%2520network%2520in%2520sense-making%2520and%250Asemantic%2520processing.%2520Additionally%252C%2520we%2520implemented%2520zero-shot%2520imagination%250Adecoding%2520on%2520an%2520extra%2520validation%2520dataset%252C%2520achieving%2520a%2520p-value%2520of%25200.0206%2520for%250Amapping%2520the%2520reconstructed%2520images%2520and%2520ground-truth%2520text%2520stimuli%252C%2520which%250Asubstantiates%2520the%2520model%2527s%2520capability%2520to%2520capture%2520semantic%2520meanings%2520across%250Avarious%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Visual%20Experience%20and%20Mapping%20Semantics%20through%20Whole-Brain%0A%20%20Analysis%20Using%20fMRI%20Foundation%20Models&entry.906535625=Yanchen%20Wang%20and%20Adam%20Turnbull%20and%20Tiange%20Xiang%20and%20Yunlong%20Xu%20and%20Sa%20Zhou%20and%20Adnan%20Masoud%20and%20Shekoofeh%20Azizi%20and%20Feng%20Vankee%20Lin%20and%20Ehsan%20Adeli&entry.1292438233=%20%20Neural%20decoding%2C%20the%20process%20of%20understanding%20how%20brain%20activity%20corresponds%0Ato%20different%20stimuli%2C%20has%20been%20a%20primary%20objective%20in%20cognitive%20sciences.%20Over%0Athe%20past%20three%20decades%2C%20advancements%20in%20functional%20Magnetic%20Resonance%20Imaging%0Aand%20machine%20learning%20have%20greatly%20improved%20our%20ability%20to%20map%20visual%20stimuli%20to%0Abrain%20activity%2C%20especially%20in%20the%20visual%20cortex.%20Concurrently%2C%20research%20has%0Aexpanded%20into%20decoding%20more%20complex%20processes%20like%20language%20and%20memory%20across%0Athe%20whole%20brain%2C%20utilizing%20techniques%20to%20handle%20greater%20variability%20and%20improve%0Asignal%20accuracy.%20We%20argue%20that%20%22seeing%22%20involves%20more%20than%20just%20mapping%20visual%0Astimuli%20onto%20the%20visual%20cortex%3B%20it%20engages%20the%20entire%20brain%2C%20as%20various%0Aemotions%20and%20cognitive%20states%20can%20emerge%20from%20observing%20different%20scenes.%20In%0Athis%20paper%2C%20we%20develop%20algorithms%20to%20enhance%20our%20understanding%20of%20visual%0Aprocesses%20by%20incorporating%20whole-brain%20activation%20maps%20while%20individuals%20are%0Aexposed%20to%20visual%20stimuli.%20We%20utilize%20large-scale%20fMRI%20encoders%20and%20Image%0Agenerative%20models%20pre-trained%20on%20large%20public%20datasets%2C%20which%20are%20then%0Afine-tuned%20through%20Image-fMRI%20contrastive%20learning.%20Our%20models%20hence%20can%20decode%0Avisual%20experience%20across%20the%20entire%20cerebral%20cortex%2C%20surpassing%20the%20traditional%0Aconfines%20of%20the%20visual%20cortex.%20We%20first%20compare%20our%20method%20with%0Astate-of-the-art%20approaches%20to%20decoding%20visual%20processing%20and%20show%20improved%0Apredictive%20semantic%20accuracy%20by%2043%25.%20A%20network%20ablation%20analysis%20suggests%20that%0Abeyond%20the%20visual%20cortex%2C%20the%20default%20mode%20network%20contributes%20most%20to%20decoding%0Astimuli%2C%20in%20line%20with%20the%20proposed%20role%20of%20this%20network%20in%20sense-making%20and%0Asemantic%20processing.%20Additionally%2C%20we%20implemented%20zero-shot%20imagination%0Adecoding%20on%20an%20extra%20validation%20dataset%2C%20achieving%20a%20p-value%20of%200.0206%20for%0Amapping%20the%20reconstructed%20images%20and%20ground-truth%20text%20stimuli%2C%20which%0Asubstantiates%20the%20model%27s%20capability%20to%20capture%20semantic%20meanings%20across%0Avarious%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07121v1&entry.124074799=Read"},
{"title": "NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics", "author": "David Robinson and Marius Miron and Masato Hagiwara and Olivier Pietquin", "abstract": "  Large language models (LLMs) prompted with text and audio represent the state\nof the art in various auditory tasks, including speech, music, and general\naudio, showing emergent abilities on unseen tasks. However, these capabilities\nhave yet to be fully demonstrated in bioacoustics tasks, such as detecting\nanimal vocalizations in large recordings, classifying rare and endangered\nspecies, and labeling context and behavior - tasks that are crucial for\nconservation, biodiversity monitoring, and the study of animal behavior. In\nthis work, we present NatureLM-audio, the first audio-language foundation model\nspecifically designed for bioacoustics. Our carefully curated training dataset\ncomprises text-audio pairs spanning a diverse range of bioacoustics, speech,\nand music data, designed to address the challenges posed by limited annotated\ndatasets in the field. We demonstrate successful transfer of learned\nrepresentations from music and speech to bioacoustics, and our model shows\npromising generalization to unseen taxa and tasks. Importantly, we test\nNatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of\nthe art (SotA) on several bioacoustics tasks, including zero-shot\nclassification of unseen species. To advance bioacoustics research, we also\nopen-source the code for generating training and benchmark data, as well as for\ntraining the model.\n", "link": "http://arxiv.org/abs/2411.07186v1", "date": "2024-11-11", "relevancy": 2.5848, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5318}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5318}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NatureLM-audio%3A%20an%20Audio-Language%20Foundation%20Model%20for%20Bioacoustics&body=Title%3A%20NatureLM-audio%3A%20an%20Audio-Language%20Foundation%20Model%20for%20Bioacoustics%0AAuthor%3A%20David%20Robinson%20and%20Marius%20Miron%20and%20Masato%20Hagiwara%20and%20Olivier%20Pietquin%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20prompted%20with%20text%20and%20audio%20represent%20the%20state%0Aof%20the%20art%20in%20various%20auditory%20tasks%2C%20including%20speech%2C%20music%2C%20and%20general%0Aaudio%2C%20showing%20emergent%20abilities%20on%20unseen%20tasks.%20However%2C%20these%20capabilities%0Ahave%20yet%20to%20be%20fully%20demonstrated%20in%20bioacoustics%20tasks%2C%20such%20as%20detecting%0Aanimal%20vocalizations%20in%20large%20recordings%2C%20classifying%20rare%20and%20endangered%0Aspecies%2C%20and%20labeling%20context%20and%20behavior%20-%20tasks%20that%20are%20crucial%20for%0Aconservation%2C%20biodiversity%20monitoring%2C%20and%20the%20study%20of%20animal%20behavior.%20In%0Athis%20work%2C%20we%20present%20NatureLM-audio%2C%20the%20first%20audio-language%20foundation%20model%0Aspecifically%20designed%20for%20bioacoustics.%20Our%20carefully%20curated%20training%20dataset%0Acomprises%20text-audio%20pairs%20spanning%20a%20diverse%20range%20of%20bioacoustics%2C%20speech%2C%0Aand%20music%20data%2C%20designed%20to%20address%20the%20challenges%20posed%20by%20limited%20annotated%0Adatasets%20in%20the%20field.%20We%20demonstrate%20successful%20transfer%20of%20learned%0Arepresentations%20from%20music%20and%20speech%20to%20bioacoustics%2C%20and%20our%20model%20shows%0Apromising%20generalization%20to%20unseen%20taxa%20and%20tasks.%20Importantly%2C%20we%20test%0ANatureLM-audio%20on%20a%20novel%20benchmark%20%28BEANS-Zero%29%20and%20it%20sets%20the%20new%20state%20of%0Athe%20art%20%28SotA%29%20on%20several%20bioacoustics%20tasks%2C%20including%20zero-shot%0Aclassification%20of%20unseen%20species.%20To%20advance%20bioacoustics%20research%2C%20we%20also%0Aopen-source%20the%20code%20for%20generating%20training%20and%20benchmark%20data%2C%20as%20well%20as%20for%0Atraining%20the%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatureLM-audio%253A%2520an%2520Audio-Language%2520Foundation%2520Model%2520for%2520Bioacoustics%26entry.906535625%3DDavid%2520Robinson%2520and%2520Marius%2520Miron%2520and%2520Masato%2520Hagiwara%2520and%2520Olivier%2520Pietquin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520prompted%2520with%2520text%2520and%2520audio%2520represent%2520the%2520state%250Aof%2520the%2520art%2520in%2520various%2520auditory%2520tasks%252C%2520including%2520speech%252C%2520music%252C%2520and%2520general%250Aaudio%252C%2520showing%2520emergent%2520abilities%2520on%2520unseen%2520tasks.%2520However%252C%2520these%2520capabilities%250Ahave%2520yet%2520to%2520be%2520fully%2520demonstrated%2520in%2520bioacoustics%2520tasks%252C%2520such%2520as%2520detecting%250Aanimal%2520vocalizations%2520in%2520large%2520recordings%252C%2520classifying%2520rare%2520and%2520endangered%250Aspecies%252C%2520and%2520labeling%2520context%2520and%2520behavior%2520-%2520tasks%2520that%2520are%2520crucial%2520for%250Aconservation%252C%2520biodiversity%2520monitoring%252C%2520and%2520the%2520study%2520of%2520animal%2520behavior.%2520In%250Athis%2520work%252C%2520we%2520present%2520NatureLM-audio%252C%2520the%2520first%2520audio-language%2520foundation%2520model%250Aspecifically%2520designed%2520for%2520bioacoustics.%2520Our%2520carefully%2520curated%2520training%2520dataset%250Acomprises%2520text-audio%2520pairs%2520spanning%2520a%2520diverse%2520range%2520of%2520bioacoustics%252C%2520speech%252C%250Aand%2520music%2520data%252C%2520designed%2520to%2520address%2520the%2520challenges%2520posed%2520by%2520limited%2520annotated%250Adatasets%2520in%2520the%2520field.%2520We%2520demonstrate%2520successful%2520transfer%2520of%2520learned%250Arepresentations%2520from%2520music%2520and%2520speech%2520to%2520bioacoustics%252C%2520and%2520our%2520model%2520shows%250Apromising%2520generalization%2520to%2520unseen%2520taxa%2520and%2520tasks.%2520Importantly%252C%2520we%2520test%250ANatureLM-audio%2520on%2520a%2520novel%2520benchmark%2520%2528BEANS-Zero%2529%2520and%2520it%2520sets%2520the%2520new%2520state%2520of%250Athe%2520art%2520%2528SotA%2529%2520on%2520several%2520bioacoustics%2520tasks%252C%2520including%2520zero-shot%250Aclassification%2520of%2520unseen%2520species.%2520To%2520advance%2520bioacoustics%2520research%252C%2520we%2520also%250Aopen-source%2520the%2520code%2520for%2520generating%2520training%2520and%2520benchmark%2520data%252C%2520as%2520well%2520as%2520for%250Atraining%2520the%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NatureLM-audio%3A%20an%20Audio-Language%20Foundation%20Model%20for%20Bioacoustics&entry.906535625=David%20Robinson%20and%20Marius%20Miron%20and%20Masato%20Hagiwara%20and%20Olivier%20Pietquin&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20prompted%20with%20text%20and%20audio%20represent%20the%20state%0Aof%20the%20art%20in%20various%20auditory%20tasks%2C%20including%20speech%2C%20music%2C%20and%20general%0Aaudio%2C%20showing%20emergent%20abilities%20on%20unseen%20tasks.%20However%2C%20these%20capabilities%0Ahave%20yet%20to%20be%20fully%20demonstrated%20in%20bioacoustics%20tasks%2C%20such%20as%20detecting%0Aanimal%20vocalizations%20in%20large%20recordings%2C%20classifying%20rare%20and%20endangered%0Aspecies%2C%20and%20labeling%20context%20and%20behavior%20-%20tasks%20that%20are%20crucial%20for%0Aconservation%2C%20biodiversity%20monitoring%2C%20and%20the%20study%20of%20animal%20behavior.%20In%0Athis%20work%2C%20we%20present%20NatureLM-audio%2C%20the%20first%20audio-language%20foundation%20model%0Aspecifically%20designed%20for%20bioacoustics.%20Our%20carefully%20curated%20training%20dataset%0Acomprises%20text-audio%20pairs%20spanning%20a%20diverse%20range%20of%20bioacoustics%2C%20speech%2C%0Aand%20music%20data%2C%20designed%20to%20address%20the%20challenges%20posed%20by%20limited%20annotated%0Adatasets%20in%20the%20field.%20We%20demonstrate%20successful%20transfer%20of%20learned%0Arepresentations%20from%20music%20and%20speech%20to%20bioacoustics%2C%20and%20our%20model%20shows%0Apromising%20generalization%20to%20unseen%20taxa%20and%20tasks.%20Importantly%2C%20we%20test%0ANatureLM-audio%20on%20a%20novel%20benchmark%20%28BEANS-Zero%29%20and%20it%20sets%20the%20new%20state%20of%0Athe%20art%20%28SotA%29%20on%20several%20bioacoustics%20tasks%2C%20including%20zero-shot%0Aclassification%20of%20unseen%20species.%20To%20advance%20bioacoustics%20research%2C%20we%20also%0Aopen-source%20the%20code%20for%20generating%20training%20and%20benchmark%20data%2C%20as%20well%20as%20for%0Atraining%20the%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07186v1&entry.124074799=Read"},
{"title": "Classification of residential and non-residential buildings based on\n  satellite data using deep learning", "author": "Jai G Singla", "abstract": "  Accurate classification of buildings into residential and non-residential\ncategories is crucial for urban planning, infrastructure development,\npopulation estimation and resource allocation. It is a complex job to carry out\nautomatic classification of residential and nonresidential buildings manually\nusing satellite data. In this paper, we are proposing a novel deep learning\napproach that combines high-resolution satellite data (50 cm resolution Image +\n1m grid interval DEM) and vector data to achieve high-performance building\nclassification. Our architecture leverages LeakyReLU and ReLU activations to\ncapture nonlinearities in the data and employs feature-engineering techniques\nto eliminate highly correlated features, resulting in improved computational\nefficiency. Experimental results on a large-scale dataset demonstrate the\neffectiveness of our model, achieving an impressive overall F1 -score of\n0.9936. The proposed approach offers a scalable and accurate solution for\nbuilding classification, enabling informed decision-making in urban planning\nand resource allocation. This research contributes to the field of urban\nanalysis by providing a valuable tool for understanding the built environment\nand optimizing resource utilization.\n", "link": "http://arxiv.org/abs/2411.06879v1", "date": "2024-11-11", "relevancy": 2.5579, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5369}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5161}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20residential%20and%20non-residential%20buildings%20based%20on%0A%20%20satellite%20data%20using%20deep%20learning&body=Title%3A%20Classification%20of%20residential%20and%20non-residential%20buildings%20based%20on%0A%20%20satellite%20data%20using%20deep%20learning%0AAuthor%3A%20Jai%20G%20Singla%0AAbstract%3A%20%20%20Accurate%20classification%20of%20buildings%20into%20residential%20and%20non-residential%0Acategories%20is%20crucial%20for%20urban%20planning%2C%20infrastructure%20development%2C%0Apopulation%20estimation%20and%20resource%20allocation.%20It%20is%20a%20complex%20job%20to%20carry%20out%0Aautomatic%20classification%20of%20residential%20and%20nonresidential%20buildings%20manually%0Ausing%20satellite%20data.%20In%20this%20paper%2C%20we%20are%20proposing%20a%20novel%20deep%20learning%0Aapproach%20that%20combines%20high-resolution%20satellite%20data%20%2850%20cm%20resolution%20Image%20%2B%0A1m%20grid%20interval%20DEM%29%20and%20vector%20data%20to%20achieve%20high-performance%20building%0Aclassification.%20Our%20architecture%20leverages%20LeakyReLU%20and%20ReLU%20activations%20to%0Acapture%20nonlinearities%20in%20the%20data%20and%20employs%20feature-engineering%20techniques%0Ato%20eliminate%20highly%20correlated%20features%2C%20resulting%20in%20improved%20computational%0Aefficiency.%20Experimental%20results%20on%20a%20large-scale%20dataset%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%2C%20achieving%20an%20impressive%20overall%20F1%20-score%20of%0A0.9936.%20The%20proposed%20approach%20offers%20a%20scalable%20and%20accurate%20solution%20for%0Abuilding%20classification%2C%20enabling%20informed%20decision-making%20in%20urban%20planning%0Aand%20resource%20allocation.%20This%20research%20contributes%20to%20the%20field%20of%20urban%0Aanalysis%20by%20providing%20a%20valuable%20tool%20for%20understanding%20the%20built%20environment%0Aand%20optimizing%20resource%20utilization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520residential%2520and%2520non-residential%2520buildings%2520based%2520on%250A%2520%2520satellite%2520data%2520using%2520deep%2520learning%26entry.906535625%3DJai%2520G%2520Singla%26entry.1292438233%3D%2520%2520Accurate%2520classification%2520of%2520buildings%2520into%2520residential%2520and%2520non-residential%250Acategories%2520is%2520crucial%2520for%2520urban%2520planning%252C%2520infrastructure%2520development%252C%250Apopulation%2520estimation%2520and%2520resource%2520allocation.%2520It%2520is%2520a%2520complex%2520job%2520to%2520carry%2520out%250Aautomatic%2520classification%2520of%2520residential%2520and%2520nonresidential%2520buildings%2520manually%250Ausing%2520satellite%2520data.%2520In%2520this%2520paper%252C%2520we%2520are%2520proposing%2520a%2520novel%2520deep%2520learning%250Aapproach%2520that%2520combines%2520high-resolution%2520satellite%2520data%2520%252850%2520cm%2520resolution%2520Image%2520%252B%250A1m%2520grid%2520interval%2520DEM%2529%2520and%2520vector%2520data%2520to%2520achieve%2520high-performance%2520building%250Aclassification.%2520Our%2520architecture%2520leverages%2520LeakyReLU%2520and%2520ReLU%2520activations%2520to%250Acapture%2520nonlinearities%2520in%2520the%2520data%2520and%2520employs%2520feature-engineering%2520techniques%250Ato%2520eliminate%2520highly%2520correlated%2520features%252C%2520resulting%2520in%2520improved%2520computational%250Aefficiency.%2520Experimental%2520results%2520on%2520a%2520large-scale%2520dataset%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520model%252C%2520achieving%2520an%2520impressive%2520overall%2520F1%2520-score%2520of%250A0.9936.%2520The%2520proposed%2520approach%2520offers%2520a%2520scalable%2520and%2520accurate%2520solution%2520for%250Abuilding%2520classification%252C%2520enabling%2520informed%2520decision-making%2520in%2520urban%2520planning%250Aand%2520resource%2520allocation.%2520This%2520research%2520contributes%2520to%2520the%2520field%2520of%2520urban%250Aanalysis%2520by%2520providing%2520a%2520valuable%2520tool%2520for%2520understanding%2520the%2520built%2520environment%250Aand%2520optimizing%2520resource%2520utilization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20residential%20and%20non-residential%20buildings%20based%20on%0A%20%20satellite%20data%20using%20deep%20learning&entry.906535625=Jai%20G%20Singla&entry.1292438233=%20%20Accurate%20classification%20of%20buildings%20into%20residential%20and%20non-residential%0Acategories%20is%20crucial%20for%20urban%20planning%2C%20infrastructure%20development%2C%0Apopulation%20estimation%20and%20resource%20allocation.%20It%20is%20a%20complex%20job%20to%20carry%20out%0Aautomatic%20classification%20of%20residential%20and%20nonresidential%20buildings%20manually%0Ausing%20satellite%20data.%20In%20this%20paper%2C%20we%20are%20proposing%20a%20novel%20deep%20learning%0Aapproach%20that%20combines%20high-resolution%20satellite%20data%20%2850%20cm%20resolution%20Image%20%2B%0A1m%20grid%20interval%20DEM%29%20and%20vector%20data%20to%20achieve%20high-performance%20building%0Aclassification.%20Our%20architecture%20leverages%20LeakyReLU%20and%20ReLU%20activations%20to%0Acapture%20nonlinearities%20in%20the%20data%20and%20employs%20feature-engineering%20techniques%0Ato%20eliminate%20highly%20correlated%20features%2C%20resulting%20in%20improved%20computational%0Aefficiency.%20Experimental%20results%20on%20a%20large-scale%20dataset%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%2C%20achieving%20an%20impressive%20overall%20F1%20-score%20of%0A0.9936.%20The%20proposed%20approach%20offers%20a%20scalable%20and%20accurate%20solution%20for%0Abuilding%20classification%2C%20enabling%20informed%20decision-making%20in%20urban%20planning%0Aand%20resource%20allocation.%20This%20research%20contributes%20to%20the%20field%20of%20urban%0Aanalysis%20by%20providing%20a%20valuable%20tool%20for%20understanding%20the%20built%20environment%0Aand%20optimizing%20resource%20utilization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06879v1&entry.124074799=Read"},
{"title": "Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt\n  Tensor Products", "author": "Shengjie Luo and Tianlang Chen and Aditi S. Krishnapriyan", "abstract": "  Developing equivariant neural networks for the E(3) group plays an important\nrole in modeling 3D data across real-world applications. Enforcing this\nequivariance primarily involves the tensor products of irreducible\nrepresentations (irreps). However, the computational complexity of such\noperations increases significantly as higher-order tensors are used. In this\nwork, we propose a systematic approach to substantially accelerate the\ncomputation of the tensor products of irreps. We mathematically connect the\ncommonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are\nintegrals of products of three spherical harmonics. Through Gaunt coefficients,\nthe tensor product of irreps becomes equivalent to the multiplication between\nspherical functions represented by spherical harmonics. This perspective\nfurther allows us to change the basis for the equivariant operations from\nspherical harmonics to a 2D Fourier basis. Consequently, the multiplication\nbetween spherical functions represented by a 2D Fourier basis can be\nefficiently computed via the convolution theorem and Fast Fourier Transforms.\nThis transformation reduces the complexity of full tensor products of irreps\nfrom $\\mathcal{O}(L^6)$ to $\\mathcal{O}(L^3)$, where $L$ is the max degree of\nirreps. Leveraging this approach, we introduce the Gaunt Tensor Product, which\nserves as a new method to construct efficient equivariant operations across\ndifferent model architectures. Our experiments on the Open Catalyst Project and\n3BPA datasets demonstrate both the increased efficiency and improved\nperformance of our approach.\n", "link": "http://arxiv.org/abs/2401.10216v2", "date": "2024-11-11", "relevancy": 2.5478, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5222}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5034}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20Efficient%20Equivariant%20Operations%20in%20the%20Fourier%20Basis%20via%20Gaunt%0A%20%20Tensor%20Products&body=Title%3A%20Enabling%20Efficient%20Equivariant%20Operations%20in%20the%20Fourier%20Basis%20via%20Gaunt%0A%20%20Tensor%20Products%0AAuthor%3A%20Shengjie%20Luo%20and%20Tianlang%20Chen%20and%20Aditi%20S.%20Krishnapriyan%0AAbstract%3A%20%20%20Developing%20equivariant%20neural%20networks%20for%20the%20E%283%29%20group%20plays%20an%20important%0Arole%20in%20modeling%203D%20data%20across%20real-world%20applications.%20Enforcing%20this%0Aequivariance%20primarily%20involves%20the%20tensor%20products%20of%20irreducible%0Arepresentations%20%28irreps%29.%20However%2C%20the%20computational%20complexity%20of%20such%0Aoperations%20increases%20significantly%20as%20higher-order%20tensors%20are%20used.%20In%20this%0Awork%2C%20we%20propose%20a%20systematic%20approach%20to%20substantially%20accelerate%20the%0Acomputation%20of%20the%20tensor%20products%20of%20irreps.%20We%20mathematically%20connect%20the%0Acommonly%20used%20Clebsch-Gordan%20coefficients%20to%20the%20Gaunt%20coefficients%2C%20which%20are%0Aintegrals%20of%20products%20of%20three%20spherical%20harmonics.%20Through%20Gaunt%20coefficients%2C%0Athe%20tensor%20product%20of%20irreps%20becomes%20equivalent%20to%20the%20multiplication%20between%0Aspherical%20functions%20represented%20by%20spherical%20harmonics.%20This%20perspective%0Afurther%20allows%20us%20to%20change%20the%20basis%20for%20the%20equivariant%20operations%20from%0Aspherical%20harmonics%20to%20a%202D%20Fourier%20basis.%20Consequently%2C%20the%20multiplication%0Abetween%20spherical%20functions%20represented%20by%20a%202D%20Fourier%20basis%20can%20be%0Aefficiently%20computed%20via%20the%20convolution%20theorem%20and%20Fast%20Fourier%20Transforms.%0AThis%20transformation%20reduces%20the%20complexity%20of%20full%20tensor%20products%20of%20irreps%0Afrom%20%24%5Cmathcal%7BO%7D%28L%5E6%29%24%20to%20%24%5Cmathcal%7BO%7D%28L%5E3%29%24%2C%20where%20%24L%24%20is%20the%20max%20degree%20of%0Airreps.%20Leveraging%20this%20approach%2C%20we%20introduce%20the%20Gaunt%20Tensor%20Product%2C%20which%0Aserves%20as%20a%20new%20method%20to%20construct%20efficient%20equivariant%20operations%20across%0Adifferent%20model%20architectures.%20Our%20experiments%20on%20the%20Open%20Catalyst%20Project%20and%0A3BPA%20datasets%20demonstrate%20both%20the%20increased%20efficiency%20and%20improved%0Aperformance%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520Efficient%2520Equivariant%2520Operations%2520in%2520the%2520Fourier%2520Basis%2520via%2520Gaunt%250A%2520%2520Tensor%2520Products%26entry.906535625%3DShengjie%2520Luo%2520and%2520Tianlang%2520Chen%2520and%2520Aditi%2520S.%2520Krishnapriyan%26entry.1292438233%3D%2520%2520Developing%2520equivariant%2520neural%2520networks%2520for%2520the%2520E%25283%2529%2520group%2520plays%2520an%2520important%250Arole%2520in%2520modeling%25203D%2520data%2520across%2520real-world%2520applications.%2520Enforcing%2520this%250Aequivariance%2520primarily%2520involves%2520the%2520tensor%2520products%2520of%2520irreducible%250Arepresentations%2520%2528irreps%2529.%2520However%252C%2520the%2520computational%2520complexity%2520of%2520such%250Aoperations%2520increases%2520significantly%2520as%2520higher-order%2520tensors%2520are%2520used.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520systematic%2520approach%2520to%2520substantially%2520accelerate%2520the%250Acomputation%2520of%2520the%2520tensor%2520products%2520of%2520irreps.%2520We%2520mathematically%2520connect%2520the%250Acommonly%2520used%2520Clebsch-Gordan%2520coefficients%2520to%2520the%2520Gaunt%2520coefficients%252C%2520which%2520are%250Aintegrals%2520of%2520products%2520of%2520three%2520spherical%2520harmonics.%2520Through%2520Gaunt%2520coefficients%252C%250Athe%2520tensor%2520product%2520of%2520irreps%2520becomes%2520equivalent%2520to%2520the%2520multiplication%2520between%250Aspherical%2520functions%2520represented%2520by%2520spherical%2520harmonics.%2520This%2520perspective%250Afurther%2520allows%2520us%2520to%2520change%2520the%2520basis%2520for%2520the%2520equivariant%2520operations%2520from%250Aspherical%2520harmonics%2520to%2520a%25202D%2520Fourier%2520basis.%2520Consequently%252C%2520the%2520multiplication%250Abetween%2520spherical%2520functions%2520represented%2520by%2520a%25202D%2520Fourier%2520basis%2520can%2520be%250Aefficiently%2520computed%2520via%2520the%2520convolution%2520theorem%2520and%2520Fast%2520Fourier%2520Transforms.%250AThis%2520transformation%2520reduces%2520the%2520complexity%2520of%2520full%2520tensor%2520products%2520of%2520irreps%250Afrom%2520%2524%255Cmathcal%257BO%257D%2528L%255E6%2529%2524%2520to%2520%2524%255Cmathcal%257BO%257D%2528L%255E3%2529%2524%252C%2520where%2520%2524L%2524%2520is%2520the%2520max%2520degree%2520of%250Airreps.%2520Leveraging%2520this%2520approach%252C%2520we%2520introduce%2520the%2520Gaunt%2520Tensor%2520Product%252C%2520which%250Aserves%2520as%2520a%2520new%2520method%2520to%2520construct%2520efficient%2520equivariant%2520operations%2520across%250Adifferent%2520model%2520architectures.%2520Our%2520experiments%2520on%2520the%2520Open%2520Catalyst%2520Project%2520and%250A3BPA%2520datasets%2520demonstrate%2520both%2520the%2520increased%2520efficiency%2520and%2520improved%250Aperformance%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Efficient%20Equivariant%20Operations%20in%20the%20Fourier%20Basis%20via%20Gaunt%0A%20%20Tensor%20Products&entry.906535625=Shengjie%20Luo%20and%20Tianlang%20Chen%20and%20Aditi%20S.%20Krishnapriyan&entry.1292438233=%20%20Developing%20equivariant%20neural%20networks%20for%20the%20E%283%29%20group%20plays%20an%20important%0Arole%20in%20modeling%203D%20data%20across%20real-world%20applications.%20Enforcing%20this%0Aequivariance%20primarily%20involves%20the%20tensor%20products%20of%20irreducible%0Arepresentations%20%28irreps%29.%20However%2C%20the%20computational%20complexity%20of%20such%0Aoperations%20increases%20significantly%20as%20higher-order%20tensors%20are%20used.%20In%20this%0Awork%2C%20we%20propose%20a%20systematic%20approach%20to%20substantially%20accelerate%20the%0Acomputation%20of%20the%20tensor%20products%20of%20irreps.%20We%20mathematically%20connect%20the%0Acommonly%20used%20Clebsch-Gordan%20coefficients%20to%20the%20Gaunt%20coefficients%2C%20which%20are%0Aintegrals%20of%20products%20of%20three%20spherical%20harmonics.%20Through%20Gaunt%20coefficients%2C%0Athe%20tensor%20product%20of%20irreps%20becomes%20equivalent%20to%20the%20multiplication%20between%0Aspherical%20functions%20represented%20by%20spherical%20harmonics.%20This%20perspective%0Afurther%20allows%20us%20to%20change%20the%20basis%20for%20the%20equivariant%20operations%20from%0Aspherical%20harmonics%20to%20a%202D%20Fourier%20basis.%20Consequently%2C%20the%20multiplication%0Abetween%20spherical%20functions%20represented%20by%20a%202D%20Fourier%20basis%20can%20be%0Aefficiently%20computed%20via%20the%20convolution%20theorem%20and%20Fast%20Fourier%20Transforms.%0AThis%20transformation%20reduces%20the%20complexity%20of%20full%20tensor%20products%20of%20irreps%0Afrom%20%24%5Cmathcal%7BO%7D%28L%5E6%29%24%20to%20%24%5Cmathcal%7BO%7D%28L%5E3%29%24%2C%20where%20%24L%24%20is%20the%20max%20degree%20of%0Airreps.%20Leveraging%20this%20approach%2C%20we%20introduce%20the%20Gaunt%20Tensor%20Product%2C%20which%0Aserves%20as%20a%20new%20method%20to%20construct%20efficient%20equivariant%20operations%20across%0Adifferent%20model%20architectures.%20Our%20experiments%20on%20the%20Open%20Catalyst%20Project%20and%0A3BPA%20datasets%20demonstrate%20both%20the%20increased%20efficiency%20and%20improved%0Aperformance%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10216v2&entry.124074799=Read"},
{"title": "LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language\n  Models", "author": "Runming Yang and Taiqiang Wu and Jiahao Wang and Pengfei Hu and Ngai Wong and Yujiu Yang", "abstract": "  In this paper, we propose a novel LLM-Neo framework that efficiently\ntransfers knowledge from a large language model (LLM) teacher to a compact\nstudent. Initially, we revisit the knowledge distillation (KD) and low-rank\nadaption (LoRA), and argue that they share the same paradigm. Inspired by this\nobservation, we explore the strategy that combines LoRA and KD to enhance the\nefficiency of knowledge transfer. We first summarize some guidelines for this\ndesign and further develop the LLM-Neo. Experimental results on compressing\nLlama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further\nanalysis demonstrates the robustness of the proposed LLM-Neo on variants of\nLoRA. The trained models have been available at\n\\href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{this\nrepository}.\n", "link": "http://arxiv.org/abs/2411.06839v1", "date": "2024-11-11", "relevancy": 2.5474, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Neo%3A%20Parameter%20Efficient%20Knowledge%20Distillation%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20LLM-Neo%3A%20Parameter%20Efficient%20Knowledge%20Distillation%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Runming%20Yang%20and%20Taiqiang%20Wu%20and%20Jiahao%20Wang%20and%20Pengfei%20Hu%20and%20Ngai%20Wong%20and%20Yujiu%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20LLM-Neo%20framework%20that%20efficiently%0Atransfers%20knowledge%20from%20a%20large%20language%20model%20%28LLM%29%20teacher%20to%20a%20compact%0Astudent.%20Initially%2C%20we%20revisit%20the%20knowledge%20distillation%20%28KD%29%20and%20low-rank%0Aadaption%20%28LoRA%29%2C%20and%20argue%20that%20they%20share%20the%20same%20paradigm.%20Inspired%20by%20this%0Aobservation%2C%20we%20explore%20the%20strategy%20that%20combines%20LoRA%20and%20KD%20to%20enhance%20the%0Aefficiency%20of%20knowledge%20transfer.%20We%20first%20summarize%20some%20guidelines%20for%20this%0Adesign%20and%20further%20develop%20the%20LLM-Neo.%20Experimental%20results%20on%20compressing%0ALlama%202%20and%20Llama%203%20show%20that%20LLM-Neo%20outperforms%20various%20baselines.%20Further%0Aanalysis%20demonstrates%20the%20robustness%20of%20the%20proposed%20LLM-Neo%20on%20variants%20of%0ALoRA.%20The%20trained%20models%20have%20been%20available%20at%0A%5Chref%7Bhttps%3A//huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba%7D%7Bthis%0Arepository%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Neo%253A%2520Parameter%2520Efficient%2520Knowledge%2520Distillation%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DRunming%2520Yang%2520and%2520Taiqiang%2520Wu%2520and%2520Jiahao%2520Wang%2520and%2520Pengfei%2520Hu%2520and%2520Ngai%2520Wong%2520and%2520Yujiu%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520LLM-Neo%2520framework%2520that%2520efficiently%250Atransfers%2520knowledge%2520from%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520teacher%2520to%2520a%2520compact%250Astudent.%2520Initially%252C%2520we%2520revisit%2520the%2520knowledge%2520distillation%2520%2528KD%2529%2520and%2520low-rank%250Aadaption%2520%2528LoRA%2529%252C%2520and%2520argue%2520that%2520they%2520share%2520the%2520same%2520paradigm.%2520Inspired%2520by%2520this%250Aobservation%252C%2520we%2520explore%2520the%2520strategy%2520that%2520combines%2520LoRA%2520and%2520KD%2520to%2520enhance%2520the%250Aefficiency%2520of%2520knowledge%2520transfer.%2520We%2520first%2520summarize%2520some%2520guidelines%2520for%2520this%250Adesign%2520and%2520further%2520develop%2520the%2520LLM-Neo.%2520Experimental%2520results%2520on%2520compressing%250ALlama%25202%2520and%2520Llama%25203%2520show%2520that%2520LLM-Neo%2520outperforms%2520various%2520baselines.%2520Further%250Aanalysis%2520demonstrates%2520the%2520robustness%2520of%2520the%2520proposed%2520LLM-Neo%2520on%2520variants%2520of%250ALoRA.%2520The%2520trained%2520models%2520have%2520been%2520available%2520at%250A%255Chref%257Bhttps%253A//huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba%257D%257Bthis%250Arepository%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Neo%3A%20Parameter%20Efficient%20Knowledge%20Distillation%20for%20Large%20Language%0A%20%20Models&entry.906535625=Runming%20Yang%20and%20Taiqiang%20Wu%20and%20Jiahao%20Wang%20and%20Pengfei%20Hu%20and%20Ngai%20Wong%20and%20Yujiu%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20LLM-Neo%20framework%20that%20efficiently%0Atransfers%20knowledge%20from%20a%20large%20language%20model%20%28LLM%29%20teacher%20to%20a%20compact%0Astudent.%20Initially%2C%20we%20revisit%20the%20knowledge%20distillation%20%28KD%29%20and%20low-rank%0Aadaption%20%28LoRA%29%2C%20and%20argue%20that%20they%20share%20the%20same%20paradigm.%20Inspired%20by%20this%0Aobservation%2C%20we%20explore%20the%20strategy%20that%20combines%20LoRA%20and%20KD%20to%20enhance%20the%0Aefficiency%20of%20knowledge%20transfer.%20We%20first%20summarize%20some%20guidelines%20for%20this%0Adesign%20and%20further%20develop%20the%20LLM-Neo.%20Experimental%20results%20on%20compressing%0ALlama%202%20and%20Llama%203%20show%20that%20LLM-Neo%20outperforms%20various%20baselines.%20Further%0Aanalysis%20demonstrates%20the%20robustness%20of%20the%20proposed%20LLM-Neo%20on%20variants%20of%0ALoRA.%20The%20trained%20models%20have%20been%20available%20at%0A%5Chref%7Bhttps%3A//huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba%7D%7Bthis%0Arepository%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06839v1&entry.124074799=Read"},
{"title": "Variational Graph Contrastive Learning", "author": "Shifeng Xie and Jhony H. Giraldo", "abstract": "  Graph representation learning (GRL) is a fundamental task in machine\nlearning, aiming to encode high-dimensional graph-structured data into\nlow-dimensional vectors. Self-supervised learning (SSL) methods are widely used\nin GRL because they can avoid expensive human annotation. In this work, we\npropose a novel Subgraph Gaussian Embedding Contrast (SGEC) method. Our\napproach introduces a subgraph Gaussian embedding module, which adaptively maps\nsubgraphs to a structured Gaussian space, ensuring the preservation of graph\ncharacteristics while controlling the distribution of generated subgraphs. We\nemploy optimal transport distances, including Wasserstein and\nGromov-Wasserstein distances, to effectively measure the similarity between\nsubgraphs, enhancing the robustness of the contrastive learning process.\nExtensive experiments across multiple benchmarks demonstrate that SGEC\noutperforms or presents competitive performance against state-of-the-art\napproaches. Our findings provide insights into the design of SSL methods for\nGRL, emphasizing the importance of the distribution of the generated\ncontrastive pairs.\n", "link": "http://arxiv.org/abs/2411.07150v1", "date": "2024-11-11", "relevancy": 2.5356, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5464}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4915}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Graph%20Contrastive%20Learning&body=Title%3A%20Variational%20Graph%20Contrastive%20Learning%0AAuthor%3A%20Shifeng%20Xie%20and%20Jhony%20H.%20Giraldo%0AAbstract%3A%20%20%20Graph%20representation%20learning%20%28GRL%29%20is%20a%20fundamental%20task%20in%20machine%0Alearning%2C%20aiming%20to%20encode%20high-dimensional%20graph-structured%20data%20into%0Alow-dimensional%20vectors.%20Self-supervised%20learning%20%28SSL%29%20methods%20are%20widely%20used%0Ain%20GRL%20because%20they%20can%20avoid%20expensive%20human%20annotation.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20Subgraph%20Gaussian%20Embedding%20Contrast%20%28SGEC%29%20method.%20Our%0Aapproach%20introduces%20a%20subgraph%20Gaussian%20embedding%20module%2C%20which%20adaptively%20maps%0Asubgraphs%20to%20a%20structured%20Gaussian%20space%2C%20ensuring%20the%20preservation%20of%20graph%0Acharacteristics%20while%20controlling%20the%20distribution%20of%20generated%20subgraphs.%20We%0Aemploy%20optimal%20transport%20distances%2C%20including%20Wasserstein%20and%0AGromov-Wasserstein%20distances%2C%20to%20effectively%20measure%20the%20similarity%20between%0Asubgraphs%2C%20enhancing%20the%20robustness%20of%20the%20contrastive%20learning%20process.%0AExtensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20SGEC%0Aoutperforms%20or%20presents%20competitive%20performance%20against%20state-of-the-art%0Aapproaches.%20Our%20findings%20provide%20insights%20into%20the%20design%20of%20SSL%20methods%20for%0AGRL%2C%20emphasizing%20the%20importance%20of%20the%20distribution%20of%20the%20generated%0Acontrastive%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Graph%2520Contrastive%2520Learning%26entry.906535625%3DShifeng%2520Xie%2520and%2520Jhony%2520H.%2520Giraldo%26entry.1292438233%3D%2520%2520Graph%2520representation%2520learning%2520%2528GRL%2529%2520is%2520a%2520fundamental%2520task%2520in%2520machine%250Alearning%252C%2520aiming%2520to%2520encode%2520high-dimensional%2520graph-structured%2520data%2520into%250Alow-dimensional%2520vectors.%2520Self-supervised%2520learning%2520%2528SSL%2529%2520methods%2520are%2520widely%2520used%250Ain%2520GRL%2520because%2520they%2520can%2520avoid%2520expensive%2520human%2520annotation.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520Subgraph%2520Gaussian%2520Embedding%2520Contrast%2520%2528SGEC%2529%2520method.%2520Our%250Aapproach%2520introduces%2520a%2520subgraph%2520Gaussian%2520embedding%2520module%252C%2520which%2520adaptively%2520maps%250Asubgraphs%2520to%2520a%2520structured%2520Gaussian%2520space%252C%2520ensuring%2520the%2520preservation%2520of%2520graph%250Acharacteristics%2520while%2520controlling%2520the%2520distribution%2520of%2520generated%2520subgraphs.%2520We%250Aemploy%2520optimal%2520transport%2520distances%252C%2520including%2520Wasserstein%2520and%250AGromov-Wasserstein%2520distances%252C%2520to%2520effectively%2520measure%2520the%2520similarity%2520between%250Asubgraphs%252C%2520enhancing%2520the%2520robustness%2520of%2520the%2520contrastive%2520learning%2520process.%250AExtensive%2520experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520SGEC%250Aoutperforms%2520or%2520presents%2520competitive%2520performance%2520against%2520state-of-the-art%250Aapproaches.%2520Our%2520findings%2520provide%2520insights%2520into%2520the%2520design%2520of%2520SSL%2520methods%2520for%250AGRL%252C%2520emphasizing%2520the%2520importance%2520of%2520the%2520distribution%2520of%2520the%2520generated%250Acontrastive%2520pairs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Graph%20Contrastive%20Learning&entry.906535625=Shifeng%20Xie%20and%20Jhony%20H.%20Giraldo&entry.1292438233=%20%20Graph%20representation%20learning%20%28GRL%29%20is%20a%20fundamental%20task%20in%20machine%0Alearning%2C%20aiming%20to%20encode%20high-dimensional%20graph-structured%20data%20into%0Alow-dimensional%20vectors.%20Self-supervised%20learning%20%28SSL%29%20methods%20are%20widely%20used%0Ain%20GRL%20because%20they%20can%20avoid%20expensive%20human%20annotation.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20Subgraph%20Gaussian%20Embedding%20Contrast%20%28SGEC%29%20method.%20Our%0Aapproach%20introduces%20a%20subgraph%20Gaussian%20embedding%20module%2C%20which%20adaptively%20maps%0Asubgraphs%20to%20a%20structured%20Gaussian%20space%2C%20ensuring%20the%20preservation%20of%20graph%0Acharacteristics%20while%20controlling%20the%20distribution%20of%20generated%20subgraphs.%20We%0Aemploy%20optimal%20transport%20distances%2C%20including%20Wasserstein%20and%0AGromov-Wasserstein%20distances%2C%20to%20effectively%20measure%20the%20similarity%20between%0Asubgraphs%2C%20enhancing%20the%20robustness%20of%20the%20contrastive%20learning%20process.%0AExtensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20SGEC%0Aoutperforms%20or%20presents%20competitive%20performance%20against%20state-of-the-art%0Aapproaches.%20Our%20findings%20provide%20insights%20into%20the%20design%20of%20SSL%20methods%20for%0AGRL%2C%20emphasizing%20the%20importance%20of%20the%20distribution%20of%20the%20generated%0Acontrastive%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07150v1&entry.124074799=Read"},
{"title": "INQUIRE: A Natural World Text-to-Image Retrieval Benchmark", "author": "Edward Vendrow and Omiros Pantazis and Alexander Shepard and Gabriel Brostow and Kate E. Jones and Oisin Mac Aodha and Sara Beery and Grant Van Horn", "abstract": "  We introduce INQUIRE, a text-to-image retrieval benchmark designed to\nchallenge multimodal vision-language models on expert-level queries. INQUIRE\nincludes iNaturalist 2024 (iNat24), a new dataset of five million natural world\nimages, along with 250 expert-level retrieval queries. These queries are paired\nwith all relevant images comprehensively labeled within iNat24, comprising\n33,000 total matches. Queries span categories such as species identification,\ncontext, behavior, and appearance, emphasizing tasks that require nuanced image\nunderstanding and domain expertise. Our benchmark evaluates two core retrieval\ntasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)\nINQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed\nevaluation of a range of recent multimodal models demonstrates that INQUIRE\nposes a significant challenge, with the best models failing to achieve an\nmAP@50 above 50%. In addition, we show that reranking with more powerful\nmultimodal models can enhance retrieval performance, yet there remains a\nsignificant margin for improvement. By focusing on scientifically-motivated\necological challenges, INQUIRE aims to bridge the gap between AI capabilities\nand the needs of real-world scientific inquiry, encouraging the development of\nretrieval systems that can assist with accelerating ecological and biodiversity\nresearch. Our dataset and code are available at\nhttps://inquire-benchmark.github.io\n", "link": "http://arxiv.org/abs/2411.02537v3", "date": "2024-11-11", "relevancy": 2.5271, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5255}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5255}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20INQUIRE%3A%20A%20Natural%20World%20Text-to-Image%20Retrieval%20Benchmark&body=Title%3A%20INQUIRE%3A%20A%20Natural%20World%20Text-to-Image%20Retrieval%20Benchmark%0AAuthor%3A%20Edward%20Vendrow%20and%20Omiros%20Pantazis%20and%20Alexander%20Shepard%20and%20Gabriel%20Brostow%20and%20Kate%20E.%20Jones%20and%20Oisin%20Mac%20Aodha%20and%20Sara%20Beery%20and%20Grant%20Van%20Horn%0AAbstract%3A%20%20%20We%20introduce%20INQUIRE%2C%20a%20text-to-image%20retrieval%20benchmark%20designed%20to%0Achallenge%20multimodal%20vision-language%20models%20on%20expert-level%20queries.%20INQUIRE%0Aincludes%20iNaturalist%202024%20%28iNat24%29%2C%20a%20new%20dataset%20of%20five%20million%20natural%20world%0Aimages%2C%20along%20with%20250%20expert-level%20retrieval%20queries.%20These%20queries%20are%20paired%0Awith%20all%20relevant%20images%20comprehensively%20labeled%20within%20iNat24%2C%20comprising%0A33%2C000%20total%20matches.%20Queries%20span%20categories%20such%20as%20species%20identification%2C%0Acontext%2C%20behavior%2C%20and%20appearance%2C%20emphasizing%20tasks%20that%20require%20nuanced%20image%0Aunderstanding%20and%20domain%20expertise.%20Our%20benchmark%20evaluates%20two%20core%20retrieval%0Atasks%3A%20%281%29%20INQUIRE-Fullrank%2C%20a%20full%20dataset%20ranking%20task%2C%20and%20%282%29%0AINQUIRE-Rerank%2C%20a%20reranking%20task%20for%20refining%20top-100%20retrievals.%20Detailed%0Aevaluation%20of%20a%20range%20of%20recent%20multimodal%20models%20demonstrates%20that%20INQUIRE%0Aposes%20a%20significant%20challenge%2C%20with%20the%20best%20models%20failing%20to%20achieve%20an%0AmAP%4050%20above%2050%25.%20In%20addition%2C%20we%20show%20that%20reranking%20with%20more%20powerful%0Amultimodal%20models%20can%20enhance%20retrieval%20performance%2C%20yet%20there%20remains%20a%0Asignificant%20margin%20for%20improvement.%20By%20focusing%20on%20scientifically-motivated%0Aecological%20challenges%2C%20INQUIRE%20aims%20to%20bridge%20the%20gap%20between%20AI%20capabilities%0Aand%20the%20needs%20of%20real-world%20scientific%20inquiry%2C%20encouraging%20the%20development%20of%0Aretrieval%20systems%20that%20can%20assist%20with%20accelerating%20ecological%20and%20biodiversity%0Aresearch.%20Our%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//inquire-benchmark.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02537v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DINQUIRE%253A%2520A%2520Natural%2520World%2520Text-to-Image%2520Retrieval%2520Benchmark%26entry.906535625%3DEdward%2520Vendrow%2520and%2520Omiros%2520Pantazis%2520and%2520Alexander%2520Shepard%2520and%2520Gabriel%2520Brostow%2520and%2520Kate%2520E.%2520Jones%2520and%2520Oisin%2520Mac%2520Aodha%2520and%2520Sara%2520Beery%2520and%2520Grant%2520Van%2520Horn%26entry.1292438233%3D%2520%2520We%2520introduce%2520INQUIRE%252C%2520a%2520text-to-image%2520retrieval%2520benchmark%2520designed%2520to%250Achallenge%2520multimodal%2520vision-language%2520models%2520on%2520expert-level%2520queries.%2520INQUIRE%250Aincludes%2520iNaturalist%25202024%2520%2528iNat24%2529%252C%2520a%2520new%2520dataset%2520of%2520five%2520million%2520natural%2520world%250Aimages%252C%2520along%2520with%2520250%2520expert-level%2520retrieval%2520queries.%2520These%2520queries%2520are%2520paired%250Awith%2520all%2520relevant%2520images%2520comprehensively%2520labeled%2520within%2520iNat24%252C%2520comprising%250A33%252C000%2520total%2520matches.%2520Queries%2520span%2520categories%2520such%2520as%2520species%2520identification%252C%250Acontext%252C%2520behavior%252C%2520and%2520appearance%252C%2520emphasizing%2520tasks%2520that%2520require%2520nuanced%2520image%250Aunderstanding%2520and%2520domain%2520expertise.%2520Our%2520benchmark%2520evaluates%2520two%2520core%2520retrieval%250Atasks%253A%2520%25281%2529%2520INQUIRE-Fullrank%252C%2520a%2520full%2520dataset%2520ranking%2520task%252C%2520and%2520%25282%2529%250AINQUIRE-Rerank%252C%2520a%2520reranking%2520task%2520for%2520refining%2520top-100%2520retrievals.%2520Detailed%250Aevaluation%2520of%2520a%2520range%2520of%2520recent%2520multimodal%2520models%2520demonstrates%2520that%2520INQUIRE%250Aposes%2520a%2520significant%2520challenge%252C%2520with%2520the%2520best%2520models%2520failing%2520to%2520achieve%2520an%250AmAP%254050%2520above%252050%2525.%2520In%2520addition%252C%2520we%2520show%2520that%2520reranking%2520with%2520more%2520powerful%250Amultimodal%2520models%2520can%2520enhance%2520retrieval%2520performance%252C%2520yet%2520there%2520remains%2520a%250Asignificant%2520margin%2520for%2520improvement.%2520By%2520focusing%2520on%2520scientifically-motivated%250Aecological%2520challenges%252C%2520INQUIRE%2520aims%2520to%2520bridge%2520the%2520gap%2520between%2520AI%2520capabilities%250Aand%2520the%2520needs%2520of%2520real-world%2520scientific%2520inquiry%252C%2520encouraging%2520the%2520development%2520of%250Aretrieval%2520systems%2520that%2520can%2520assist%2520with%2520accelerating%2520ecological%2520and%2520biodiversity%250Aresearch.%2520Our%2520dataset%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//inquire-benchmark.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02537v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=INQUIRE%3A%20A%20Natural%20World%20Text-to-Image%20Retrieval%20Benchmark&entry.906535625=Edward%20Vendrow%20and%20Omiros%20Pantazis%20and%20Alexander%20Shepard%20and%20Gabriel%20Brostow%20and%20Kate%20E.%20Jones%20and%20Oisin%20Mac%20Aodha%20and%20Sara%20Beery%20and%20Grant%20Van%20Horn&entry.1292438233=%20%20We%20introduce%20INQUIRE%2C%20a%20text-to-image%20retrieval%20benchmark%20designed%20to%0Achallenge%20multimodal%20vision-language%20models%20on%20expert-level%20queries.%20INQUIRE%0Aincludes%20iNaturalist%202024%20%28iNat24%29%2C%20a%20new%20dataset%20of%20five%20million%20natural%20world%0Aimages%2C%20along%20with%20250%20expert-level%20retrieval%20queries.%20These%20queries%20are%20paired%0Awith%20all%20relevant%20images%20comprehensively%20labeled%20within%20iNat24%2C%20comprising%0A33%2C000%20total%20matches.%20Queries%20span%20categories%20such%20as%20species%20identification%2C%0Acontext%2C%20behavior%2C%20and%20appearance%2C%20emphasizing%20tasks%20that%20require%20nuanced%20image%0Aunderstanding%20and%20domain%20expertise.%20Our%20benchmark%20evaluates%20two%20core%20retrieval%0Atasks%3A%20%281%29%20INQUIRE-Fullrank%2C%20a%20full%20dataset%20ranking%20task%2C%20and%20%282%29%0AINQUIRE-Rerank%2C%20a%20reranking%20task%20for%20refining%20top-100%20retrievals.%20Detailed%0Aevaluation%20of%20a%20range%20of%20recent%20multimodal%20models%20demonstrates%20that%20INQUIRE%0Aposes%20a%20significant%20challenge%2C%20with%20the%20best%20models%20failing%20to%20achieve%20an%0AmAP%4050%20above%2050%25.%20In%20addition%2C%20we%20show%20that%20reranking%20with%20more%20powerful%0Amultimodal%20models%20can%20enhance%20retrieval%20performance%2C%20yet%20there%20remains%20a%0Asignificant%20margin%20for%20improvement.%20By%20focusing%20on%20scientifically-motivated%0Aecological%20challenges%2C%20INQUIRE%20aims%20to%20bridge%20the%20gap%20between%20AI%20capabilities%0Aand%20the%20needs%20of%20real-world%20scientific%20inquiry%2C%20encouraging%20the%20development%20of%0Aretrieval%20systems%20that%20can%20assist%20with%20accelerating%20ecological%20and%20biodiversity%0Aresearch.%20Our%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//inquire-benchmark.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02537v3&entry.124074799=Read"},
{"title": "DLCR: A Generative Data Expansion Framework via Diffusion for\n  Clothes-Changing Person Re-ID", "author": "Nyle Siddiqui and Florinel Alin Croitoru and Gaurav Kumar Nayak and Radu Tudor Ionescu and Mubarak Shah", "abstract": "  With the recent exhibited strength of generative diffusion models, an open\nresearch question is \\textit{if images generated by these models can be used to\nlearn better visual representations}. While this generative data expansion may\nsuffice for easier visual tasks, we explore its efficacy on a more difficult\ndiscriminative task: clothes-changing person re-identification (CC-ReID).\nCC-ReID aims to match people appearing in non-overlapping cameras, even when\nthey change their clothes across cameras. Not only are current CC-ReID models\nconstrained by the limited diversity of clothing in current CC-ReID datasets,\nbut generating additional data that retains important personal features for\naccurate identification is a current challenge. To address this issue we\npropose DLCR, a novel data expansion framework that leverages pre-trained\ndiffusion and large language models (LLMs) to accurately generate diverse\nimages of individuals in varied attire. We generate additional data for five\nbenchmark CC-ReID datasets (PRCC, CCVID, LaST, VC-Clothes, and LTCC) and\n\\textbf{increase their clothing diversity by \\boldmath{$10$}x, totaling over\n\\boldmath{$2.1$}M images generated}. DLCR employs diffusion-based text-guided\ninpainting, conditioned on clothing prompts constructed using LLMs, to generate\nsynthetic data that only modifies a subject's clothes while preserving their\npersonally identifiable features. With this massive increase in data, we\nintroduce two novel strategies - progressive learning and test-time prediction\nrefinement - that respectively reduce training time and further boosts CC-ReID\nperformance. On the PRCC dataset, we obtain a large top-1 accuracy improvement\nof $11.3\\%$ by training CAL, a previous state of the art (SOTA) method, with\nDLCR-generated data. We publicly release our code and generated data for each\ndataset here: \\url{https://github.com/CroitoruAlin/dlcr}.\n", "link": "http://arxiv.org/abs/2411.07205v1", "date": "2024-11-11", "relevancy": 2.5169, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6315}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6282}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DLCR%3A%20A%20Generative%20Data%20Expansion%20Framework%20via%20Diffusion%20for%0A%20%20Clothes-Changing%20Person%20Re-ID&body=Title%3A%20DLCR%3A%20A%20Generative%20Data%20Expansion%20Framework%20via%20Diffusion%20for%0A%20%20Clothes-Changing%20Person%20Re-ID%0AAuthor%3A%20Nyle%20Siddiqui%20and%20Florinel%20Alin%20Croitoru%20and%20Gaurav%20Kumar%20Nayak%20and%20Radu%20Tudor%20Ionescu%20and%20Mubarak%20Shah%0AAbstract%3A%20%20%20With%20the%20recent%20exhibited%20strength%20of%20generative%20diffusion%20models%2C%20an%20open%0Aresearch%20question%20is%20%5Ctextit%7Bif%20images%20generated%20by%20these%20models%20can%20be%20used%20to%0Alearn%20better%20visual%20representations%7D.%20While%20this%20generative%20data%20expansion%20may%0Asuffice%20for%20easier%20visual%20tasks%2C%20we%20explore%20its%20efficacy%20on%20a%20more%20difficult%0Adiscriminative%20task%3A%20clothes-changing%20person%20re-identification%20%28CC-ReID%29.%0ACC-ReID%20aims%20to%20match%20people%20appearing%20in%20non-overlapping%20cameras%2C%20even%20when%0Athey%20change%20their%20clothes%20across%20cameras.%20Not%20only%20are%20current%20CC-ReID%20models%0Aconstrained%20by%20the%20limited%20diversity%20of%20clothing%20in%20current%20CC-ReID%20datasets%2C%0Abut%20generating%20additional%20data%20that%20retains%20important%20personal%20features%20for%0Aaccurate%20identification%20is%20a%20current%20challenge.%20To%20address%20this%20issue%20we%0Apropose%20DLCR%2C%20a%20novel%20data%20expansion%20framework%20that%20leverages%20pre-trained%0Adiffusion%20and%20large%20language%20models%20%28LLMs%29%20to%20accurately%20generate%20diverse%0Aimages%20of%20individuals%20in%20varied%20attire.%20We%20generate%20additional%20data%20for%20five%0Abenchmark%20CC-ReID%20datasets%20%28PRCC%2C%20CCVID%2C%20LaST%2C%20VC-Clothes%2C%20and%20LTCC%29%20and%0A%5Ctextbf%7Bincrease%20their%20clothing%20diversity%20by%20%5Cboldmath%7B%2410%24%7Dx%2C%20totaling%20over%0A%5Cboldmath%7B%242.1%24%7DM%20images%20generated%7D.%20DLCR%20employs%20diffusion-based%20text-guided%0Ainpainting%2C%20conditioned%20on%20clothing%20prompts%20constructed%20using%20LLMs%2C%20to%20generate%0Asynthetic%20data%20that%20only%20modifies%20a%20subject%27s%20clothes%20while%20preserving%20their%0Apersonally%20identifiable%20features.%20With%20this%20massive%20increase%20in%20data%2C%20we%0Aintroduce%20two%20novel%20strategies%20-%20progressive%20learning%20and%20test-time%20prediction%0Arefinement%20-%20that%20respectively%20reduce%20training%20time%20and%20further%20boosts%20CC-ReID%0Aperformance.%20On%20the%20PRCC%20dataset%2C%20we%20obtain%20a%20large%20top-1%20accuracy%20improvement%0Aof%20%2411.3%5C%25%24%20by%20training%20CAL%2C%20a%20previous%20state%20of%20the%20art%20%28SOTA%29%20method%2C%20with%0ADLCR-generated%20data.%20We%20publicly%20release%20our%20code%20and%20generated%20data%20for%20each%0Adataset%20here%3A%20%5Curl%7Bhttps%3A//github.com/CroitoruAlin/dlcr%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDLCR%253A%2520A%2520Generative%2520Data%2520Expansion%2520Framework%2520via%2520Diffusion%2520for%250A%2520%2520Clothes-Changing%2520Person%2520Re-ID%26entry.906535625%3DNyle%2520Siddiqui%2520and%2520Florinel%2520Alin%2520Croitoru%2520and%2520Gaurav%2520Kumar%2520Nayak%2520and%2520Radu%2520Tudor%2520Ionescu%2520and%2520Mubarak%2520Shah%26entry.1292438233%3D%2520%2520With%2520the%2520recent%2520exhibited%2520strength%2520of%2520generative%2520diffusion%2520models%252C%2520an%2520open%250Aresearch%2520question%2520is%2520%255Ctextit%257Bif%2520images%2520generated%2520by%2520these%2520models%2520can%2520be%2520used%2520to%250Alearn%2520better%2520visual%2520representations%257D.%2520While%2520this%2520generative%2520data%2520expansion%2520may%250Asuffice%2520for%2520easier%2520visual%2520tasks%252C%2520we%2520explore%2520its%2520efficacy%2520on%2520a%2520more%2520difficult%250Adiscriminative%2520task%253A%2520clothes-changing%2520person%2520re-identification%2520%2528CC-ReID%2529.%250ACC-ReID%2520aims%2520to%2520match%2520people%2520appearing%2520in%2520non-overlapping%2520cameras%252C%2520even%2520when%250Athey%2520change%2520their%2520clothes%2520across%2520cameras.%2520Not%2520only%2520are%2520current%2520CC-ReID%2520models%250Aconstrained%2520by%2520the%2520limited%2520diversity%2520of%2520clothing%2520in%2520current%2520CC-ReID%2520datasets%252C%250Abut%2520generating%2520additional%2520data%2520that%2520retains%2520important%2520personal%2520features%2520for%250Aaccurate%2520identification%2520is%2520a%2520current%2520challenge.%2520To%2520address%2520this%2520issue%2520we%250Apropose%2520DLCR%252C%2520a%2520novel%2520data%2520expansion%2520framework%2520that%2520leverages%2520pre-trained%250Adiffusion%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520accurately%2520generate%2520diverse%250Aimages%2520of%2520individuals%2520in%2520varied%2520attire.%2520We%2520generate%2520additional%2520data%2520for%2520five%250Abenchmark%2520CC-ReID%2520datasets%2520%2528PRCC%252C%2520CCVID%252C%2520LaST%252C%2520VC-Clothes%252C%2520and%2520LTCC%2529%2520and%250A%255Ctextbf%257Bincrease%2520their%2520clothing%2520diversity%2520by%2520%255Cboldmath%257B%252410%2524%257Dx%252C%2520totaling%2520over%250A%255Cboldmath%257B%25242.1%2524%257DM%2520images%2520generated%257D.%2520DLCR%2520employs%2520diffusion-based%2520text-guided%250Ainpainting%252C%2520conditioned%2520on%2520clothing%2520prompts%2520constructed%2520using%2520LLMs%252C%2520to%2520generate%250Asynthetic%2520data%2520that%2520only%2520modifies%2520a%2520subject%2527s%2520clothes%2520while%2520preserving%2520their%250Apersonally%2520identifiable%2520features.%2520With%2520this%2520massive%2520increase%2520in%2520data%252C%2520we%250Aintroduce%2520two%2520novel%2520strategies%2520-%2520progressive%2520learning%2520and%2520test-time%2520prediction%250Arefinement%2520-%2520that%2520respectively%2520reduce%2520training%2520time%2520and%2520further%2520boosts%2520CC-ReID%250Aperformance.%2520On%2520the%2520PRCC%2520dataset%252C%2520we%2520obtain%2520a%2520large%2520top-1%2520accuracy%2520improvement%250Aof%2520%252411.3%255C%2525%2524%2520by%2520training%2520CAL%252C%2520a%2520previous%2520state%2520of%2520the%2520art%2520%2528SOTA%2529%2520method%252C%2520with%250ADLCR-generated%2520data.%2520We%2520publicly%2520release%2520our%2520code%2520and%2520generated%2520data%2520for%2520each%250Adataset%2520here%253A%2520%255Curl%257Bhttps%253A//github.com/CroitoruAlin/dlcr%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DLCR%3A%20A%20Generative%20Data%20Expansion%20Framework%20via%20Diffusion%20for%0A%20%20Clothes-Changing%20Person%20Re-ID&entry.906535625=Nyle%20Siddiqui%20and%20Florinel%20Alin%20Croitoru%20and%20Gaurav%20Kumar%20Nayak%20and%20Radu%20Tudor%20Ionescu%20and%20Mubarak%20Shah&entry.1292438233=%20%20With%20the%20recent%20exhibited%20strength%20of%20generative%20diffusion%20models%2C%20an%20open%0Aresearch%20question%20is%20%5Ctextit%7Bif%20images%20generated%20by%20these%20models%20can%20be%20used%20to%0Alearn%20better%20visual%20representations%7D.%20While%20this%20generative%20data%20expansion%20may%0Asuffice%20for%20easier%20visual%20tasks%2C%20we%20explore%20its%20efficacy%20on%20a%20more%20difficult%0Adiscriminative%20task%3A%20clothes-changing%20person%20re-identification%20%28CC-ReID%29.%0ACC-ReID%20aims%20to%20match%20people%20appearing%20in%20non-overlapping%20cameras%2C%20even%20when%0Athey%20change%20their%20clothes%20across%20cameras.%20Not%20only%20are%20current%20CC-ReID%20models%0Aconstrained%20by%20the%20limited%20diversity%20of%20clothing%20in%20current%20CC-ReID%20datasets%2C%0Abut%20generating%20additional%20data%20that%20retains%20important%20personal%20features%20for%0Aaccurate%20identification%20is%20a%20current%20challenge.%20To%20address%20this%20issue%20we%0Apropose%20DLCR%2C%20a%20novel%20data%20expansion%20framework%20that%20leverages%20pre-trained%0Adiffusion%20and%20large%20language%20models%20%28LLMs%29%20to%20accurately%20generate%20diverse%0Aimages%20of%20individuals%20in%20varied%20attire.%20We%20generate%20additional%20data%20for%20five%0Abenchmark%20CC-ReID%20datasets%20%28PRCC%2C%20CCVID%2C%20LaST%2C%20VC-Clothes%2C%20and%20LTCC%29%20and%0A%5Ctextbf%7Bincrease%20their%20clothing%20diversity%20by%20%5Cboldmath%7B%2410%24%7Dx%2C%20totaling%20over%0A%5Cboldmath%7B%242.1%24%7DM%20images%20generated%7D.%20DLCR%20employs%20diffusion-based%20text-guided%0Ainpainting%2C%20conditioned%20on%20clothing%20prompts%20constructed%20using%20LLMs%2C%20to%20generate%0Asynthetic%20data%20that%20only%20modifies%20a%20subject%27s%20clothes%20while%20preserving%20their%0Apersonally%20identifiable%20features.%20With%20this%20massive%20increase%20in%20data%2C%20we%0Aintroduce%20two%20novel%20strategies%20-%20progressive%20learning%20and%20test-time%20prediction%0Arefinement%20-%20that%20respectively%20reduce%20training%20time%20and%20further%20boosts%20CC-ReID%0Aperformance.%20On%20the%20PRCC%20dataset%2C%20we%20obtain%20a%20large%20top-1%20accuracy%20improvement%0Aof%20%2411.3%5C%25%24%20by%20training%20CAL%2C%20a%20previous%20state%20of%20the%20art%20%28SOTA%29%20method%2C%20with%0ADLCR-generated%20data.%20We%20publicly%20release%20our%20code%20and%20generated%20data%20for%20each%0Adataset%20here%3A%20%5Curl%7Bhttps%3A//github.com/CroitoruAlin/dlcr%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07205v1&entry.124074799=Read"},
{"title": "Visual Data Diagnosis and Debiasing with Concept Graphs", "author": "Rwiddhi Chakraborty and Yinong Wang and Jialu Gao and Runkai Zheng and Cheng Zhang and Fernando De la Torre", "abstract": "  The widespread success of deep learning models today is owed to the curation\nof extensive datasets significant in size and complexity. However, such models\nfrequently pick up inherent biases in the data during the training process,\nleading to unreliable predictions. Diagnosing and debiasing datasets is thus a\nnecessity to ensure reliable model performance. In this paper, we present\nConBias, a novel framework for diagnosing and mitigating Concept co-occurrence\nBiases in visual datasets. ConBias represents visual datasets as knowledge\ngraphs of concepts, enabling meticulous analysis of spurious concept\nco-occurrences to uncover concept imbalances across the whole dataset.\nMoreover, we show that by employing a novel clique-based concept balancing\nstrategy, we can mitigate these imbalances, leading to enhanced performance on\ndownstream tasks. Extensive experiments show that data augmentation based on a\nbalanced concept distribution augmented by Conbias improves generalization\nperformance across multiple datasets compared to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2409.18055v2", "date": "2024-11-11", "relevancy": 2.5078, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Data%20Diagnosis%20and%20Debiasing%20with%20Concept%20Graphs&body=Title%3A%20Visual%20Data%20Diagnosis%20and%20Debiasing%20with%20Concept%20Graphs%0AAuthor%3A%20Rwiddhi%20Chakraborty%20and%20Yinong%20Wang%20and%20Jialu%20Gao%20and%20Runkai%20Zheng%20and%20Cheng%20Zhang%20and%20Fernando%20De%20la%20Torre%0AAbstract%3A%20%20%20The%20widespread%20success%20of%20deep%20learning%20models%20today%20is%20owed%20to%20the%20curation%0Aof%20extensive%20datasets%20significant%20in%20size%20and%20complexity.%20However%2C%20such%20models%0Afrequently%20pick%20up%20inherent%20biases%20in%20the%20data%20during%20the%20training%20process%2C%0Aleading%20to%20unreliable%20predictions.%20Diagnosing%20and%20debiasing%20datasets%20is%20thus%20a%0Anecessity%20to%20ensure%20reliable%20model%20performance.%20In%20this%20paper%2C%20we%20present%0AConBias%2C%20a%20novel%20framework%20for%20diagnosing%20and%20mitigating%20Concept%20co-occurrence%0ABiases%20in%20visual%20datasets.%20ConBias%20represents%20visual%20datasets%20as%20knowledge%0Agraphs%20of%20concepts%2C%20enabling%20meticulous%20analysis%20of%20spurious%20concept%0Aco-occurrences%20to%20uncover%20concept%20imbalances%20across%20the%20whole%20dataset.%0AMoreover%2C%20we%20show%20that%20by%20employing%20a%20novel%20clique-based%20concept%20balancing%0Astrategy%2C%20we%20can%20mitigate%20these%20imbalances%2C%20leading%20to%20enhanced%20performance%20on%0Adownstream%20tasks.%20Extensive%20experiments%20show%20that%20data%20augmentation%20based%20on%20a%0Abalanced%20concept%20distribution%20augmented%20by%20Conbias%20improves%20generalization%0Aperformance%20across%20multiple%20datasets%20compared%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18055v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Data%2520Diagnosis%2520and%2520Debiasing%2520with%2520Concept%2520Graphs%26entry.906535625%3DRwiddhi%2520Chakraborty%2520and%2520Yinong%2520Wang%2520and%2520Jialu%2520Gao%2520and%2520Runkai%2520Zheng%2520and%2520Cheng%2520Zhang%2520and%2520Fernando%2520De%2520la%2520Torre%26entry.1292438233%3D%2520%2520The%2520widespread%2520success%2520of%2520deep%2520learning%2520models%2520today%2520is%2520owed%2520to%2520the%2520curation%250Aof%2520extensive%2520datasets%2520significant%2520in%2520size%2520and%2520complexity.%2520However%252C%2520such%2520models%250Afrequently%2520pick%2520up%2520inherent%2520biases%2520in%2520the%2520data%2520during%2520the%2520training%2520process%252C%250Aleading%2520to%2520unreliable%2520predictions.%2520Diagnosing%2520and%2520debiasing%2520datasets%2520is%2520thus%2520a%250Anecessity%2520to%2520ensure%2520reliable%2520model%2520performance.%2520In%2520this%2520paper%252C%2520we%2520present%250AConBias%252C%2520a%2520novel%2520framework%2520for%2520diagnosing%2520and%2520mitigating%2520Concept%2520co-occurrence%250ABiases%2520in%2520visual%2520datasets.%2520ConBias%2520represents%2520visual%2520datasets%2520as%2520knowledge%250Agraphs%2520of%2520concepts%252C%2520enabling%2520meticulous%2520analysis%2520of%2520spurious%2520concept%250Aco-occurrences%2520to%2520uncover%2520concept%2520imbalances%2520across%2520the%2520whole%2520dataset.%250AMoreover%252C%2520we%2520show%2520that%2520by%2520employing%2520a%2520novel%2520clique-based%2520concept%2520balancing%250Astrategy%252C%2520we%2520can%2520mitigate%2520these%2520imbalances%252C%2520leading%2520to%2520enhanced%2520performance%2520on%250Adownstream%2520tasks.%2520Extensive%2520experiments%2520show%2520that%2520data%2520augmentation%2520based%2520on%2520a%250Abalanced%2520concept%2520distribution%2520augmented%2520by%2520Conbias%2520improves%2520generalization%250Aperformance%2520across%2520multiple%2520datasets%2520compared%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18055v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Data%20Diagnosis%20and%20Debiasing%20with%20Concept%20Graphs&entry.906535625=Rwiddhi%20Chakraborty%20and%20Yinong%20Wang%20and%20Jialu%20Gao%20and%20Runkai%20Zheng%20and%20Cheng%20Zhang%20and%20Fernando%20De%20la%20Torre&entry.1292438233=%20%20The%20widespread%20success%20of%20deep%20learning%20models%20today%20is%20owed%20to%20the%20curation%0Aof%20extensive%20datasets%20significant%20in%20size%20and%20complexity.%20However%2C%20such%20models%0Afrequently%20pick%20up%20inherent%20biases%20in%20the%20data%20during%20the%20training%20process%2C%0Aleading%20to%20unreliable%20predictions.%20Diagnosing%20and%20debiasing%20datasets%20is%20thus%20a%0Anecessity%20to%20ensure%20reliable%20model%20performance.%20In%20this%20paper%2C%20we%20present%0AConBias%2C%20a%20novel%20framework%20for%20diagnosing%20and%20mitigating%20Concept%20co-occurrence%0ABiases%20in%20visual%20datasets.%20ConBias%20represents%20visual%20datasets%20as%20knowledge%0Agraphs%20of%20concepts%2C%20enabling%20meticulous%20analysis%20of%20spurious%20concept%0Aco-occurrences%20to%20uncover%20concept%20imbalances%20across%20the%20whole%20dataset.%0AMoreover%2C%20we%20show%20that%20by%20employing%20a%20novel%20clique-based%20concept%20balancing%0Astrategy%2C%20we%20can%20mitigate%20these%20imbalances%2C%20leading%20to%20enhanced%20performance%20on%0Adownstream%20tasks.%20Extensive%20experiments%20show%20that%20data%20augmentation%20based%20on%20a%0Abalanced%20concept%20distribution%20augmented%20by%20Conbias%20improves%20generalization%0Aperformance%20across%20multiple%20datasets%20compared%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18055v2&entry.124074799=Read"},
{"title": "To Train or Not to Train: Balancing Efficiency and Training Cost in Deep\n  Reinforcement Learning for Mobile Edge Computing", "author": "Maddalena Boscaro and Federico Mason and Federico Chiariotti and Andrea Zanella", "abstract": "  Artificial Intelligence (AI) is a key component of 6G networks, as it enables\ncommunication and computing services to adapt to end users' requirements and\ndemand patterns. The management of Mobile Edge Computing (MEC) is a meaningful\nexample of AI application: computational resources available at the network\nedge need to be carefully allocated to users, whose jobs may have different\npriorities and latency requirements. The research community has developed\nseveral AI algorithms to perform this resource allocation, but it has neglected\na key aspect: learning is itself a computationally demanding task, and\nconsidering free training results in idealized conditions and performance in\nsimulations. In this work, we consider a more realistic case in which the cost\nof learning is specifically accounted for, presenting a new algorithm to\ndynamically select when to train a Deep Reinforcement Learning (DRL) agent that\nallocates resources. Our method is highly general, as it can be directly\napplied to any scenario involving a training overhead, and it can approach the\nsame performance as an ideal learning agent even under realistic training\nconditions.\n", "link": "http://arxiv.org/abs/2411.07086v1", "date": "2024-11-11", "relevancy": 2.4949, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5348}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4912}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Train%20or%20Not%20to%20Train%3A%20Balancing%20Efficiency%20and%20Training%20Cost%20in%20Deep%0A%20%20Reinforcement%20Learning%20for%20Mobile%20Edge%20Computing&body=Title%3A%20To%20Train%20or%20Not%20to%20Train%3A%20Balancing%20Efficiency%20and%20Training%20Cost%20in%20Deep%0A%20%20Reinforcement%20Learning%20for%20Mobile%20Edge%20Computing%0AAuthor%3A%20Maddalena%20Boscaro%20and%20Federico%20Mason%20and%20Federico%20Chiariotti%20and%20Andrea%20Zanella%0AAbstract%3A%20%20%20Artificial%20Intelligence%20%28AI%29%20is%20a%20key%20component%20of%206G%20networks%2C%20as%20it%20enables%0Acommunication%20and%20computing%20services%20to%20adapt%20to%20end%20users%27%20requirements%20and%0Ademand%20patterns.%20The%20management%20of%20Mobile%20Edge%20Computing%20%28MEC%29%20is%20a%20meaningful%0Aexample%20of%20AI%20application%3A%20computational%20resources%20available%20at%20the%20network%0Aedge%20need%20to%20be%20carefully%20allocated%20to%20users%2C%20whose%20jobs%20may%20have%20different%0Apriorities%20and%20latency%20requirements.%20The%20research%20community%20has%20developed%0Aseveral%20AI%20algorithms%20to%20perform%20this%20resource%20allocation%2C%20but%20it%20has%20neglected%0Aa%20key%20aspect%3A%20learning%20is%20itself%20a%20computationally%20demanding%20task%2C%20and%0Aconsidering%20free%20training%20results%20in%20idealized%20conditions%20and%20performance%20in%0Asimulations.%20In%20this%20work%2C%20we%20consider%20a%20more%20realistic%20case%20in%20which%20the%20cost%0Aof%20learning%20is%20specifically%20accounted%20for%2C%20presenting%20a%20new%20algorithm%20to%0Adynamically%20select%20when%20to%20train%20a%20Deep%20Reinforcement%20Learning%20%28DRL%29%20agent%20that%0Aallocates%20resources.%20Our%20method%20is%20highly%20general%2C%20as%20it%20can%20be%20directly%0Aapplied%20to%20any%20scenario%20involving%20a%20training%20overhead%2C%20and%20it%20can%20approach%20the%0Asame%20performance%20as%20an%20ideal%20learning%20agent%20even%20under%20realistic%20training%0Aconditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Train%2520or%2520Not%2520to%2520Train%253A%2520Balancing%2520Efficiency%2520and%2520Training%2520Cost%2520in%2520Deep%250A%2520%2520Reinforcement%2520Learning%2520for%2520Mobile%2520Edge%2520Computing%26entry.906535625%3DMaddalena%2520Boscaro%2520and%2520Federico%2520Mason%2520and%2520Federico%2520Chiariotti%2520and%2520Andrea%2520Zanella%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520%2528AI%2529%2520is%2520a%2520key%2520component%2520of%25206G%2520networks%252C%2520as%2520it%2520enables%250Acommunication%2520and%2520computing%2520services%2520to%2520adapt%2520to%2520end%2520users%2527%2520requirements%2520and%250Ademand%2520patterns.%2520The%2520management%2520of%2520Mobile%2520Edge%2520Computing%2520%2528MEC%2529%2520is%2520a%2520meaningful%250Aexample%2520of%2520AI%2520application%253A%2520computational%2520resources%2520available%2520at%2520the%2520network%250Aedge%2520need%2520to%2520be%2520carefully%2520allocated%2520to%2520users%252C%2520whose%2520jobs%2520may%2520have%2520different%250Apriorities%2520and%2520latency%2520requirements.%2520The%2520research%2520community%2520has%2520developed%250Aseveral%2520AI%2520algorithms%2520to%2520perform%2520this%2520resource%2520allocation%252C%2520but%2520it%2520has%2520neglected%250Aa%2520key%2520aspect%253A%2520learning%2520is%2520itself%2520a%2520computationally%2520demanding%2520task%252C%2520and%250Aconsidering%2520free%2520training%2520results%2520in%2520idealized%2520conditions%2520and%2520performance%2520in%250Asimulations.%2520In%2520this%2520work%252C%2520we%2520consider%2520a%2520more%2520realistic%2520case%2520in%2520which%2520the%2520cost%250Aof%2520learning%2520is%2520specifically%2520accounted%2520for%252C%2520presenting%2520a%2520new%2520algorithm%2520to%250Adynamically%2520select%2520when%2520to%2520train%2520a%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520agent%2520that%250Aallocates%2520resources.%2520Our%2520method%2520is%2520highly%2520general%252C%2520as%2520it%2520can%2520be%2520directly%250Aapplied%2520to%2520any%2520scenario%2520involving%2520a%2520training%2520overhead%252C%2520and%2520it%2520can%2520approach%2520the%250Asame%2520performance%2520as%2520an%2520ideal%2520learning%2520agent%2520even%2520under%2520realistic%2520training%250Aconditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Train%20or%20Not%20to%20Train%3A%20Balancing%20Efficiency%20and%20Training%20Cost%20in%20Deep%0A%20%20Reinforcement%20Learning%20for%20Mobile%20Edge%20Computing&entry.906535625=Maddalena%20Boscaro%20and%20Federico%20Mason%20and%20Federico%20Chiariotti%20and%20Andrea%20Zanella&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20is%20a%20key%20component%20of%206G%20networks%2C%20as%20it%20enables%0Acommunication%20and%20computing%20services%20to%20adapt%20to%20end%20users%27%20requirements%20and%0Ademand%20patterns.%20The%20management%20of%20Mobile%20Edge%20Computing%20%28MEC%29%20is%20a%20meaningful%0Aexample%20of%20AI%20application%3A%20computational%20resources%20available%20at%20the%20network%0Aedge%20need%20to%20be%20carefully%20allocated%20to%20users%2C%20whose%20jobs%20may%20have%20different%0Apriorities%20and%20latency%20requirements.%20The%20research%20community%20has%20developed%0Aseveral%20AI%20algorithms%20to%20perform%20this%20resource%20allocation%2C%20but%20it%20has%20neglected%0Aa%20key%20aspect%3A%20learning%20is%20itself%20a%20computationally%20demanding%20task%2C%20and%0Aconsidering%20free%20training%20results%20in%20idealized%20conditions%20and%20performance%20in%0Asimulations.%20In%20this%20work%2C%20we%20consider%20a%20more%20realistic%20case%20in%20which%20the%20cost%0Aof%20learning%20is%20specifically%20accounted%20for%2C%20presenting%20a%20new%20algorithm%20to%0Adynamically%20select%20when%20to%20train%20a%20Deep%20Reinforcement%20Learning%20%28DRL%29%20agent%20that%0Aallocates%20resources.%20Our%20method%20is%20highly%20general%2C%20as%20it%20can%20be%20directly%0Aapplied%20to%20any%20scenario%20involving%20a%20training%20overhead%2C%20and%20it%20can%20approach%20the%0Asame%20performance%20as%20an%20ideal%20learning%20agent%20even%20under%20realistic%20training%0Aconditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07086v1&entry.124074799=Read"},
{"title": "Electroencephalogram-based Multi-class Decoding of Attended Speakers'\n  Direction with Audio Spatial Spectrum", "author": "Yuanming Zhang and Jing Lu and Zhibin Lin and Fei Chen and Haoliang Du and Xia Gao", "abstract": "  Decoding the directional focus of an attended speaker from listeners'\nelectroencephalogram (EEG) signals is essential for developing brain-computer\ninterfaces to improve the quality of life for individuals with hearing\nimpairment. Previous works have concentrated on binary directional focus\ndecoding, i.e., determining whether the attended speaker is on the left or\nright side of the listener. However, a more precise decoding of the exact\ndirection of the attended speaker is necessary for effective speech processing.\nAdditionally, audio spatial information has not been effectively leveraged,\nresulting in suboptimal decoding results. In this paper, we observe that, on\nour recently presented dataset with 15-class directional focus, models relying\nexclusively on EEG inputs exhibits significantly lower accuracy when decoding\nthe directional focus in both leave-one-subject-out and leave-one-trial-out\nscenarios. By integrating audio spatial spectra with EEG features, the decoding\naccuracy can be effectively improved. We employ the CNN, LSM-CNN, and\nEEG-Deformer models to decode the directional focus from listeners' EEG signals\nwith the auxiliary audio spatial spectra. The proposed Sp-Aux-Deformer model\nachieves notable 15-class decoding accuracies of 57.48% and 61.83% in\nleave-one-subject-out and leave-one-trial-out scenarios, respectively.\n", "link": "http://arxiv.org/abs/2411.06928v1", "date": "2024-11-11", "relevancy": 2.4875, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Electroencephalogram-based%20Multi-class%20Decoding%20of%20Attended%20Speakers%27%0A%20%20Direction%20with%20Audio%20Spatial%20Spectrum&body=Title%3A%20Electroencephalogram-based%20Multi-class%20Decoding%20of%20Attended%20Speakers%27%0A%20%20Direction%20with%20Audio%20Spatial%20Spectrum%0AAuthor%3A%20Yuanming%20Zhang%20and%20Jing%20Lu%20and%20Zhibin%20Lin%20and%20Fei%20Chen%20and%20Haoliang%20Du%20and%20Xia%20Gao%0AAbstract%3A%20%20%20Decoding%20the%20directional%20focus%20of%20an%20attended%20speaker%20from%20listeners%27%0Aelectroencephalogram%20%28EEG%29%20signals%20is%20essential%20for%20developing%20brain-computer%0Ainterfaces%20to%20improve%20the%20quality%20of%20life%20for%20individuals%20with%20hearing%0Aimpairment.%20Previous%20works%20have%20concentrated%20on%20binary%20directional%20focus%0Adecoding%2C%20i.e.%2C%20determining%20whether%20the%20attended%20speaker%20is%20on%20the%20left%20or%0Aright%20side%20of%20the%20listener.%20However%2C%20a%20more%20precise%20decoding%20of%20the%20exact%0Adirection%20of%20the%20attended%20speaker%20is%20necessary%20for%20effective%20speech%20processing.%0AAdditionally%2C%20audio%20spatial%20information%20has%20not%20been%20effectively%20leveraged%2C%0Aresulting%20in%20suboptimal%20decoding%20results.%20In%20this%20paper%2C%20we%20observe%20that%2C%20on%0Aour%20recently%20presented%20dataset%20with%2015-class%20directional%20focus%2C%20models%20relying%0Aexclusively%20on%20EEG%20inputs%20exhibits%20significantly%20lower%20accuracy%20when%20decoding%0Athe%20directional%20focus%20in%20both%20leave-one-subject-out%20and%20leave-one-trial-out%0Ascenarios.%20By%20integrating%20audio%20spatial%20spectra%20with%20EEG%20features%2C%20the%20decoding%0Aaccuracy%20can%20be%20effectively%20improved.%20We%20employ%20the%20CNN%2C%20LSM-CNN%2C%20and%0AEEG-Deformer%20models%20to%20decode%20the%20directional%20focus%20from%20listeners%27%20EEG%20signals%0Awith%20the%20auxiliary%20audio%20spatial%20spectra.%20The%20proposed%20Sp-Aux-Deformer%20model%0Aachieves%20notable%2015-class%20decoding%20accuracies%20of%2057.48%25%20and%2061.83%25%20in%0Aleave-one-subject-out%20and%20leave-one-trial-out%20scenarios%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElectroencephalogram-based%2520Multi-class%2520Decoding%2520of%2520Attended%2520Speakers%2527%250A%2520%2520Direction%2520with%2520Audio%2520Spatial%2520Spectrum%26entry.906535625%3DYuanming%2520Zhang%2520and%2520Jing%2520Lu%2520and%2520Zhibin%2520Lin%2520and%2520Fei%2520Chen%2520and%2520Haoliang%2520Du%2520and%2520Xia%2520Gao%26entry.1292438233%3D%2520%2520Decoding%2520the%2520directional%2520focus%2520of%2520an%2520attended%2520speaker%2520from%2520listeners%2527%250Aelectroencephalogram%2520%2528EEG%2529%2520signals%2520is%2520essential%2520for%2520developing%2520brain-computer%250Ainterfaces%2520to%2520improve%2520the%2520quality%2520of%2520life%2520for%2520individuals%2520with%2520hearing%250Aimpairment.%2520Previous%2520works%2520have%2520concentrated%2520on%2520binary%2520directional%2520focus%250Adecoding%252C%2520i.e.%252C%2520determining%2520whether%2520the%2520attended%2520speaker%2520is%2520on%2520the%2520left%2520or%250Aright%2520side%2520of%2520the%2520listener.%2520However%252C%2520a%2520more%2520precise%2520decoding%2520of%2520the%2520exact%250Adirection%2520of%2520the%2520attended%2520speaker%2520is%2520necessary%2520for%2520effective%2520speech%2520processing.%250AAdditionally%252C%2520audio%2520spatial%2520information%2520has%2520not%2520been%2520effectively%2520leveraged%252C%250Aresulting%2520in%2520suboptimal%2520decoding%2520results.%2520In%2520this%2520paper%252C%2520we%2520observe%2520that%252C%2520on%250Aour%2520recently%2520presented%2520dataset%2520with%252015-class%2520directional%2520focus%252C%2520models%2520relying%250Aexclusively%2520on%2520EEG%2520inputs%2520exhibits%2520significantly%2520lower%2520accuracy%2520when%2520decoding%250Athe%2520directional%2520focus%2520in%2520both%2520leave-one-subject-out%2520and%2520leave-one-trial-out%250Ascenarios.%2520By%2520integrating%2520audio%2520spatial%2520spectra%2520with%2520EEG%2520features%252C%2520the%2520decoding%250Aaccuracy%2520can%2520be%2520effectively%2520improved.%2520We%2520employ%2520the%2520CNN%252C%2520LSM-CNN%252C%2520and%250AEEG-Deformer%2520models%2520to%2520decode%2520the%2520directional%2520focus%2520from%2520listeners%2527%2520EEG%2520signals%250Awith%2520the%2520auxiliary%2520audio%2520spatial%2520spectra.%2520The%2520proposed%2520Sp-Aux-Deformer%2520model%250Aachieves%2520notable%252015-class%2520decoding%2520accuracies%2520of%252057.48%2525%2520and%252061.83%2525%2520in%250Aleave-one-subject-out%2520and%2520leave-one-trial-out%2520scenarios%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Electroencephalogram-based%20Multi-class%20Decoding%20of%20Attended%20Speakers%27%0A%20%20Direction%20with%20Audio%20Spatial%20Spectrum&entry.906535625=Yuanming%20Zhang%20and%20Jing%20Lu%20and%20Zhibin%20Lin%20and%20Fei%20Chen%20and%20Haoliang%20Du%20and%20Xia%20Gao&entry.1292438233=%20%20Decoding%20the%20directional%20focus%20of%20an%20attended%20speaker%20from%20listeners%27%0Aelectroencephalogram%20%28EEG%29%20signals%20is%20essential%20for%20developing%20brain-computer%0Ainterfaces%20to%20improve%20the%20quality%20of%20life%20for%20individuals%20with%20hearing%0Aimpairment.%20Previous%20works%20have%20concentrated%20on%20binary%20directional%20focus%0Adecoding%2C%20i.e.%2C%20determining%20whether%20the%20attended%20speaker%20is%20on%20the%20left%20or%0Aright%20side%20of%20the%20listener.%20However%2C%20a%20more%20precise%20decoding%20of%20the%20exact%0Adirection%20of%20the%20attended%20speaker%20is%20necessary%20for%20effective%20speech%20processing.%0AAdditionally%2C%20audio%20spatial%20information%20has%20not%20been%20effectively%20leveraged%2C%0Aresulting%20in%20suboptimal%20decoding%20results.%20In%20this%20paper%2C%20we%20observe%20that%2C%20on%0Aour%20recently%20presented%20dataset%20with%2015-class%20directional%20focus%2C%20models%20relying%0Aexclusively%20on%20EEG%20inputs%20exhibits%20significantly%20lower%20accuracy%20when%20decoding%0Athe%20directional%20focus%20in%20both%20leave-one-subject-out%20and%20leave-one-trial-out%0Ascenarios.%20By%20integrating%20audio%20spatial%20spectra%20with%20EEG%20features%2C%20the%20decoding%0Aaccuracy%20can%20be%20effectively%20improved.%20We%20employ%20the%20CNN%2C%20LSM-CNN%2C%20and%0AEEG-Deformer%20models%20to%20decode%20the%20directional%20focus%20from%20listeners%27%20EEG%20signals%0Awith%20the%20auxiliary%20audio%20spatial%20spectra.%20The%20proposed%20Sp-Aux-Deformer%20model%0Aachieves%20notable%2015-class%20decoding%20accuracies%20of%2057.48%25%20and%2061.83%25%20in%0Aleave-one-subject-out%20and%20leave-one-trial-out%20scenarios%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06928v1&entry.124074799=Read"},
{"title": "GLinSAT: The General Linear Satisfiability Neural Network Layer By\n  Accelerated Gradient Descent", "author": "Hongtai Zeng and Chao Yang and Yanzhen Zhou and Cheng Yang and Qinglai Guo", "abstract": "  Ensuring that the outputs of neural networks satisfy specific constraints is\ncrucial for applying neural networks to real-life decision-making problems. In\nthis paper, we consider making a batch of neural network outputs satisfy\nbounded and general linear constraints. We first reformulate the neural network\noutput projection problem as an entropy-regularized linear programming problem.\nWe show that such a problem can be equivalently transformed into an\nunconstrained convex optimization problem with Lipschitz continuous gradient\naccording to the duality theorem. Then, based on an accelerated gradient\ndescent algorithm with numerical performance enhancement, we present our\narchitecture, GLinSAT, to solve the problem. To the best of our knowledge, this\nis the first general linear satisfiability layer in which all the operations\nare differentiable and matrix-factorization-free. Despite the fact that we can\nexplicitly perform backpropagation based on automatic differentiation\nmechanism, we also provide an alternative approach in GLinSAT to calculate the\nderivatives based on implicit differentiation of the optimality condition.\nExperimental results on constrained traveling salesman problems, partial graph\nmatching with outliers, predictive portfolio allocation and power system unit\ncommitment demonstrate the advantages of GLinSAT over existing satisfiability\nlayers. Our implementation is available at\n\\url{https://github.com/HunterTracer/GLinSAT}.\n", "link": "http://arxiv.org/abs/2409.17500v2", "date": "2024-11-11", "relevancy": 2.4842, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5048}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5047}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLinSAT%3A%20The%20General%20Linear%20Satisfiability%20Neural%20Network%20Layer%20By%0A%20%20Accelerated%20Gradient%20Descent&body=Title%3A%20GLinSAT%3A%20The%20General%20Linear%20Satisfiability%20Neural%20Network%20Layer%20By%0A%20%20Accelerated%20Gradient%20Descent%0AAuthor%3A%20Hongtai%20Zeng%20and%20Chao%20Yang%20and%20Yanzhen%20Zhou%20and%20Cheng%20Yang%20and%20Qinglai%20Guo%0AAbstract%3A%20%20%20Ensuring%20that%20the%20outputs%20of%20neural%20networks%20satisfy%20specific%20constraints%20is%0Acrucial%20for%20applying%20neural%20networks%20to%20real-life%20decision-making%20problems.%20In%0Athis%20paper%2C%20we%20consider%20making%20a%20batch%20of%20neural%20network%20outputs%20satisfy%0Abounded%20and%20general%20linear%20constraints.%20We%20first%20reformulate%20the%20neural%20network%0Aoutput%20projection%20problem%20as%20an%20entropy-regularized%20linear%20programming%20problem.%0AWe%20show%20that%20such%20a%20problem%20can%20be%20equivalently%20transformed%20into%20an%0Aunconstrained%20convex%20optimization%20problem%20with%20Lipschitz%20continuous%20gradient%0Aaccording%20to%20the%20duality%20theorem.%20Then%2C%20based%20on%20an%20accelerated%20gradient%0Adescent%20algorithm%20with%20numerical%20performance%20enhancement%2C%20we%20present%20our%0Aarchitecture%2C%20GLinSAT%2C%20to%20solve%20the%20problem.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Ais%20the%20first%20general%20linear%20satisfiability%20layer%20in%20which%20all%20the%20operations%0Aare%20differentiable%20and%20matrix-factorization-free.%20Despite%20the%20fact%20that%20we%20can%0Aexplicitly%20perform%20backpropagation%20based%20on%20automatic%20differentiation%0Amechanism%2C%20we%20also%20provide%20an%20alternative%20approach%20in%20GLinSAT%20to%20calculate%20the%0Aderivatives%20based%20on%20implicit%20differentiation%20of%20the%20optimality%20condition.%0AExperimental%20results%20on%20constrained%20traveling%20salesman%20problems%2C%20partial%20graph%0Amatching%20with%20outliers%2C%20predictive%20portfolio%20allocation%20and%20power%20system%20unit%0Acommitment%20demonstrate%20the%20advantages%20of%20GLinSAT%20over%20existing%20satisfiability%0Alayers.%20Our%20implementation%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/HunterTracer/GLinSAT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLinSAT%253A%2520The%2520General%2520Linear%2520Satisfiability%2520Neural%2520Network%2520Layer%2520By%250A%2520%2520Accelerated%2520Gradient%2520Descent%26entry.906535625%3DHongtai%2520Zeng%2520and%2520Chao%2520Yang%2520and%2520Yanzhen%2520Zhou%2520and%2520Cheng%2520Yang%2520and%2520Qinglai%2520Guo%26entry.1292438233%3D%2520%2520Ensuring%2520that%2520the%2520outputs%2520of%2520neural%2520networks%2520satisfy%2520specific%2520constraints%2520is%250Acrucial%2520for%2520applying%2520neural%2520networks%2520to%2520real-life%2520decision-making%2520problems.%2520In%250Athis%2520paper%252C%2520we%2520consider%2520making%2520a%2520batch%2520of%2520neural%2520network%2520outputs%2520satisfy%250Abounded%2520and%2520general%2520linear%2520constraints.%2520We%2520first%2520reformulate%2520the%2520neural%2520network%250Aoutput%2520projection%2520problem%2520as%2520an%2520entropy-regularized%2520linear%2520programming%2520problem.%250AWe%2520show%2520that%2520such%2520a%2520problem%2520can%2520be%2520equivalently%2520transformed%2520into%2520an%250Aunconstrained%2520convex%2520optimization%2520problem%2520with%2520Lipschitz%2520continuous%2520gradient%250Aaccording%2520to%2520the%2520duality%2520theorem.%2520Then%252C%2520based%2520on%2520an%2520accelerated%2520gradient%250Adescent%2520algorithm%2520with%2520numerical%2520performance%2520enhancement%252C%2520we%2520present%2520our%250Aarchitecture%252C%2520GLinSAT%252C%2520to%2520solve%2520the%2520problem.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%250Ais%2520the%2520first%2520general%2520linear%2520satisfiability%2520layer%2520in%2520which%2520all%2520the%2520operations%250Aare%2520differentiable%2520and%2520matrix-factorization-free.%2520Despite%2520the%2520fact%2520that%2520we%2520can%250Aexplicitly%2520perform%2520backpropagation%2520based%2520on%2520automatic%2520differentiation%250Amechanism%252C%2520we%2520also%2520provide%2520an%2520alternative%2520approach%2520in%2520GLinSAT%2520to%2520calculate%2520the%250Aderivatives%2520based%2520on%2520implicit%2520differentiation%2520of%2520the%2520optimality%2520condition.%250AExperimental%2520results%2520on%2520constrained%2520traveling%2520salesman%2520problems%252C%2520partial%2520graph%250Amatching%2520with%2520outliers%252C%2520predictive%2520portfolio%2520allocation%2520and%2520power%2520system%2520unit%250Acommitment%2520demonstrate%2520the%2520advantages%2520of%2520GLinSAT%2520over%2520existing%2520satisfiability%250Alayers.%2520Our%2520implementation%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/HunterTracer/GLinSAT%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLinSAT%3A%20The%20General%20Linear%20Satisfiability%20Neural%20Network%20Layer%20By%0A%20%20Accelerated%20Gradient%20Descent&entry.906535625=Hongtai%20Zeng%20and%20Chao%20Yang%20and%20Yanzhen%20Zhou%20and%20Cheng%20Yang%20and%20Qinglai%20Guo&entry.1292438233=%20%20Ensuring%20that%20the%20outputs%20of%20neural%20networks%20satisfy%20specific%20constraints%20is%0Acrucial%20for%20applying%20neural%20networks%20to%20real-life%20decision-making%20problems.%20In%0Athis%20paper%2C%20we%20consider%20making%20a%20batch%20of%20neural%20network%20outputs%20satisfy%0Abounded%20and%20general%20linear%20constraints.%20We%20first%20reformulate%20the%20neural%20network%0Aoutput%20projection%20problem%20as%20an%20entropy-regularized%20linear%20programming%20problem.%0AWe%20show%20that%20such%20a%20problem%20can%20be%20equivalently%20transformed%20into%20an%0Aunconstrained%20convex%20optimization%20problem%20with%20Lipschitz%20continuous%20gradient%0Aaccording%20to%20the%20duality%20theorem.%20Then%2C%20based%20on%20an%20accelerated%20gradient%0Adescent%20algorithm%20with%20numerical%20performance%20enhancement%2C%20we%20present%20our%0Aarchitecture%2C%20GLinSAT%2C%20to%20solve%20the%20problem.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Ais%20the%20first%20general%20linear%20satisfiability%20layer%20in%20which%20all%20the%20operations%0Aare%20differentiable%20and%20matrix-factorization-free.%20Despite%20the%20fact%20that%20we%20can%0Aexplicitly%20perform%20backpropagation%20based%20on%20automatic%20differentiation%0Amechanism%2C%20we%20also%20provide%20an%20alternative%20approach%20in%20GLinSAT%20to%20calculate%20the%0Aderivatives%20based%20on%20implicit%20differentiation%20of%20the%20optimality%20condition.%0AExperimental%20results%20on%20constrained%20traveling%20salesman%20problems%2C%20partial%20graph%0Amatching%20with%20outliers%2C%20predictive%20portfolio%20allocation%20and%20power%20system%20unit%0Acommitment%20demonstrate%20the%20advantages%20of%20GLinSAT%20over%20existing%20satisfiability%0Alayers.%20Our%20implementation%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/HunterTracer/GLinSAT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17500v2&entry.124074799=Read"},
{"title": "Continual LLaVA: Continual Instruction Tuning in Large Vision-Language\n  Models", "author": "Meng Cao and Yuyang Liu and Yingfei Liu and Tiancai Wang and Jiahua Dong and Henghui Ding and Xiangyu Zhang and Ian Reid and Xiaodan Liang", "abstract": "  Instruction tuning constitutes a prevalent technique for tailoring Large\nVision Language Models (LVLMs) to meet individual task requirements. To date,\nmost of the existing approaches are confined to single-task adaptation, whereas\nthe requirements in real-world scenarios are inherently varied and continually\nevolving. Thus an ideal LVLM should sustain continual instruction tuning in the\nface of stream-task distributions (i.e., different domains, emerging\ncapabilities, and new datasets) while minimizing the forgetting of previously\nacquired knowledge. To achieve this, we propose a new benchmark for COntinuAl\ninStruction Tuning on LVLMs (COAST), which encompasses the aforementioned\ndomain-incremental, capability-incremental, and dataset-incremental\nconfigurations. In terms of methodology, we propose Continual LLaVA, a\nrehearsal-free method tailored for continual instruction tuning in LVLMs. To\ncircumvent the additional overhead associated with experience replay, we freeze\nLVLMs and construct the dual increment embeddings for each input instruction to\nfacilitate parameter-efficient tuning. Specifically, the increment embeddings\ncan be decomposed into two principal components: 1) intrinsic increment\nembeddings to encode task-specific characteristics. To achieve this, we set up\na low-rank pool containing candidate embeddings, from which we select the\nrelevant ones based on their similarity with the user instructions; 2)\ncontextual increment embeddings to investigate the inter-dependencies across\ntasks. In this regard, the low-rank embeddings chosen in the previous tasks are\naggregated via learnable weighted sum to provide complementary hints. Extensive\nexperiments indicate that the proposed Continual LLaVA outperforms previous\nmethods by significantly reducing the forgetting during the continual\ninstruction tuning process.\n", "link": "http://arxiv.org/abs/2411.02564v2", "date": "2024-11-11", "relevancy": 2.4839, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20LLaVA%3A%20Continual%20Instruction%20Tuning%20in%20Large%20Vision-Language%0A%20%20Models&body=Title%3A%20Continual%20LLaVA%3A%20Continual%20Instruction%20Tuning%20in%20Large%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Meng%20Cao%20and%20Yuyang%20Liu%20and%20Yingfei%20Liu%20and%20Tiancai%20Wang%20and%20Jiahua%20Dong%20and%20Henghui%20Ding%20and%20Xiangyu%20Zhang%20and%20Ian%20Reid%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Instruction%20tuning%20constitutes%20a%20prevalent%20technique%20for%20tailoring%20Large%0AVision%20Language%20Models%20%28LVLMs%29%20to%20meet%20individual%20task%20requirements.%20To%20date%2C%0Amost%20of%20the%20existing%20approaches%20are%20confined%20to%20single-task%20adaptation%2C%20whereas%0Athe%20requirements%20in%20real-world%20scenarios%20are%20inherently%20varied%20and%20continually%0Aevolving.%20Thus%20an%20ideal%20LVLM%20should%20sustain%20continual%20instruction%20tuning%20in%20the%0Aface%20of%20stream-task%20distributions%20%28i.e.%2C%20different%20domains%2C%20emerging%0Acapabilities%2C%20and%20new%20datasets%29%20while%20minimizing%20the%20forgetting%20of%20previously%0Aacquired%20knowledge.%20To%20achieve%20this%2C%20we%20propose%20a%20new%20benchmark%20for%20COntinuAl%0AinStruction%20Tuning%20on%20LVLMs%20%28COAST%29%2C%20which%20encompasses%20the%20aforementioned%0Adomain-incremental%2C%20capability-incremental%2C%20and%20dataset-incremental%0Aconfigurations.%20In%20terms%20of%20methodology%2C%20we%20propose%20Continual%20LLaVA%2C%20a%0Arehearsal-free%20method%20tailored%20for%20continual%20instruction%20tuning%20in%20LVLMs.%20To%0Acircumvent%20the%20additional%20overhead%20associated%20with%20experience%20replay%2C%20we%20freeze%0ALVLMs%20and%20construct%20the%20dual%20increment%20embeddings%20for%20each%20input%20instruction%20to%0Afacilitate%20parameter-efficient%20tuning.%20Specifically%2C%20the%20increment%20embeddings%0Acan%20be%20decomposed%20into%20two%20principal%20components%3A%201%29%20intrinsic%20increment%0Aembeddings%20to%20encode%20task-specific%20characteristics.%20To%20achieve%20this%2C%20we%20set%20up%0Aa%20low-rank%20pool%20containing%20candidate%20embeddings%2C%20from%20which%20we%20select%20the%0Arelevant%20ones%20based%20on%20their%20similarity%20with%20the%20user%20instructions%3B%202%29%0Acontextual%20increment%20embeddings%20to%20investigate%20the%20inter-dependencies%20across%0Atasks.%20In%20this%20regard%2C%20the%20low-rank%20embeddings%20chosen%20in%20the%20previous%20tasks%20are%0Aaggregated%20via%20learnable%20weighted%20sum%20to%20provide%20complementary%20hints.%20Extensive%0Aexperiments%20indicate%20that%20the%20proposed%20Continual%20LLaVA%20outperforms%20previous%0Amethods%20by%20significantly%20reducing%20the%20forgetting%20during%20the%20continual%0Ainstruction%20tuning%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02564v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520LLaVA%253A%2520Continual%2520Instruction%2520Tuning%2520in%2520Large%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DMeng%2520Cao%2520and%2520Yuyang%2520Liu%2520and%2520Yingfei%2520Liu%2520and%2520Tiancai%2520Wang%2520and%2520Jiahua%2520Dong%2520and%2520Henghui%2520Ding%2520and%2520Xiangyu%2520Zhang%2520and%2520Ian%2520Reid%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Instruction%2520tuning%2520constitutes%2520a%2520prevalent%2520technique%2520for%2520tailoring%2520Large%250AVision%2520Language%2520Models%2520%2528LVLMs%2529%2520to%2520meet%2520individual%2520task%2520requirements.%2520To%2520date%252C%250Amost%2520of%2520the%2520existing%2520approaches%2520are%2520confined%2520to%2520single-task%2520adaptation%252C%2520whereas%250Athe%2520requirements%2520in%2520real-world%2520scenarios%2520are%2520inherently%2520varied%2520and%2520continually%250Aevolving.%2520Thus%2520an%2520ideal%2520LVLM%2520should%2520sustain%2520continual%2520instruction%2520tuning%2520in%2520the%250Aface%2520of%2520stream-task%2520distributions%2520%2528i.e.%252C%2520different%2520domains%252C%2520emerging%250Acapabilities%252C%2520and%2520new%2520datasets%2529%2520while%2520minimizing%2520the%2520forgetting%2520of%2520previously%250Aacquired%2520knowledge.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%2520new%2520benchmark%2520for%2520COntinuAl%250AinStruction%2520Tuning%2520on%2520LVLMs%2520%2528COAST%2529%252C%2520which%2520encompasses%2520the%2520aforementioned%250Adomain-incremental%252C%2520capability-incremental%252C%2520and%2520dataset-incremental%250Aconfigurations.%2520In%2520terms%2520of%2520methodology%252C%2520we%2520propose%2520Continual%2520LLaVA%252C%2520a%250Arehearsal-free%2520method%2520tailored%2520for%2520continual%2520instruction%2520tuning%2520in%2520LVLMs.%2520To%250Acircumvent%2520the%2520additional%2520overhead%2520associated%2520with%2520experience%2520replay%252C%2520we%2520freeze%250ALVLMs%2520and%2520construct%2520the%2520dual%2520increment%2520embeddings%2520for%2520each%2520input%2520instruction%2520to%250Afacilitate%2520parameter-efficient%2520tuning.%2520Specifically%252C%2520the%2520increment%2520embeddings%250Acan%2520be%2520decomposed%2520into%2520two%2520principal%2520components%253A%25201%2529%2520intrinsic%2520increment%250Aembeddings%2520to%2520encode%2520task-specific%2520characteristics.%2520To%2520achieve%2520this%252C%2520we%2520set%2520up%250Aa%2520low-rank%2520pool%2520containing%2520candidate%2520embeddings%252C%2520from%2520which%2520we%2520select%2520the%250Arelevant%2520ones%2520based%2520on%2520their%2520similarity%2520with%2520the%2520user%2520instructions%253B%25202%2529%250Acontextual%2520increment%2520embeddings%2520to%2520investigate%2520the%2520inter-dependencies%2520across%250Atasks.%2520In%2520this%2520regard%252C%2520the%2520low-rank%2520embeddings%2520chosen%2520in%2520the%2520previous%2520tasks%2520are%250Aaggregated%2520via%2520learnable%2520weighted%2520sum%2520to%2520provide%2520complementary%2520hints.%2520Extensive%250Aexperiments%2520indicate%2520that%2520the%2520proposed%2520Continual%2520LLaVA%2520outperforms%2520previous%250Amethods%2520by%2520significantly%2520reducing%2520the%2520forgetting%2520during%2520the%2520continual%250Ainstruction%2520tuning%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02564v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20LLaVA%3A%20Continual%20Instruction%20Tuning%20in%20Large%20Vision-Language%0A%20%20Models&entry.906535625=Meng%20Cao%20and%20Yuyang%20Liu%20and%20Yingfei%20Liu%20and%20Tiancai%20Wang%20and%20Jiahua%20Dong%20and%20Henghui%20Ding%20and%20Xiangyu%20Zhang%20and%20Ian%20Reid%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Instruction%20tuning%20constitutes%20a%20prevalent%20technique%20for%20tailoring%20Large%0AVision%20Language%20Models%20%28LVLMs%29%20to%20meet%20individual%20task%20requirements.%20To%20date%2C%0Amost%20of%20the%20existing%20approaches%20are%20confined%20to%20single-task%20adaptation%2C%20whereas%0Athe%20requirements%20in%20real-world%20scenarios%20are%20inherently%20varied%20and%20continually%0Aevolving.%20Thus%20an%20ideal%20LVLM%20should%20sustain%20continual%20instruction%20tuning%20in%20the%0Aface%20of%20stream-task%20distributions%20%28i.e.%2C%20different%20domains%2C%20emerging%0Acapabilities%2C%20and%20new%20datasets%29%20while%20minimizing%20the%20forgetting%20of%20previously%0Aacquired%20knowledge.%20To%20achieve%20this%2C%20we%20propose%20a%20new%20benchmark%20for%20COntinuAl%0AinStruction%20Tuning%20on%20LVLMs%20%28COAST%29%2C%20which%20encompasses%20the%20aforementioned%0Adomain-incremental%2C%20capability-incremental%2C%20and%20dataset-incremental%0Aconfigurations.%20In%20terms%20of%20methodology%2C%20we%20propose%20Continual%20LLaVA%2C%20a%0Arehearsal-free%20method%20tailored%20for%20continual%20instruction%20tuning%20in%20LVLMs.%20To%0Acircumvent%20the%20additional%20overhead%20associated%20with%20experience%20replay%2C%20we%20freeze%0ALVLMs%20and%20construct%20the%20dual%20increment%20embeddings%20for%20each%20input%20instruction%20to%0Afacilitate%20parameter-efficient%20tuning.%20Specifically%2C%20the%20increment%20embeddings%0Acan%20be%20decomposed%20into%20two%20principal%20components%3A%201%29%20intrinsic%20increment%0Aembeddings%20to%20encode%20task-specific%20characteristics.%20To%20achieve%20this%2C%20we%20set%20up%0Aa%20low-rank%20pool%20containing%20candidate%20embeddings%2C%20from%20which%20we%20select%20the%0Arelevant%20ones%20based%20on%20their%20similarity%20with%20the%20user%20instructions%3B%202%29%0Acontextual%20increment%20embeddings%20to%20investigate%20the%20inter-dependencies%20across%0Atasks.%20In%20this%20regard%2C%20the%20low-rank%20embeddings%20chosen%20in%20the%20previous%20tasks%20are%0Aaggregated%20via%20learnable%20weighted%20sum%20to%20provide%20complementary%20hints.%20Extensive%0Aexperiments%20indicate%20that%20the%20proposed%20Continual%20LLaVA%20outperforms%20previous%0Amethods%20by%20significantly%20reducing%20the%20forgetting%20during%20the%20continual%0Ainstruction%20tuning%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02564v2&entry.124074799=Read"},
{"title": "Grounding Video Models to Actions through Goal Conditioned Exploration", "author": "Yunhao Luo and Yilun Du", "abstract": "  Large video models, pretrained on massive amounts of Internet video, provide\na rich source of physical knowledge about the dynamics and motions of objects\nand tasks. However, video models are not grounded in the embodiment of an\nagent, and do not describe how to actuate the world to reach the visual states\ndepicted in a video. To tackle this problem, current methods use a separate\nvision-based inverse dynamic model trained on embodiment-specific data to map\nimage states to actions. Gathering data to train such a model is often\nexpensive and challenging, and this model is limited to visual settings similar\nto the ones in which data are available. In this paper, we investigate how to\ndirectly ground video models to continuous actions through self-exploration in\nthe embodied environment -- using generated video states as visual goals for\nexploration. We propose a framework that uses trajectory level action\ngeneration in combination with video guidance to enable an agent to solve\ncomplex tasks without any external supervision, e.g., rewards, action labels,\nor segmentation masks. We validate the proposed approach on 8 tasks in Libero,\n6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual\nNavigation. We show how our approach is on par with or even surpasses multiple\nbehavior cloning baselines trained on expert demonstrations while without\nrequiring any action annotations.\n", "link": "http://arxiv.org/abs/2411.07223v1", "date": "2024-11-11", "relevancy": 2.4667, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6461}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6384}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20Video%20Models%20to%20Actions%20through%20Goal%20Conditioned%20Exploration&body=Title%3A%20Grounding%20Video%20Models%20to%20Actions%20through%20Goal%20Conditioned%20Exploration%0AAuthor%3A%20Yunhao%20Luo%20and%20Yilun%20Du%0AAbstract%3A%20%20%20Large%20video%20models%2C%20pretrained%20on%20massive%20amounts%20of%20Internet%20video%2C%20provide%0Aa%20rich%20source%20of%20physical%20knowledge%20about%20the%20dynamics%20and%20motions%20of%20objects%0Aand%20tasks.%20However%2C%20video%20models%20are%20not%20grounded%20in%20the%20embodiment%20of%20an%0Aagent%2C%20and%20do%20not%20describe%20how%20to%20actuate%20the%20world%20to%20reach%20the%20visual%20states%0Adepicted%20in%20a%20video.%20To%20tackle%20this%20problem%2C%20current%20methods%20use%20a%20separate%0Avision-based%20inverse%20dynamic%20model%20trained%20on%20embodiment-specific%20data%20to%20map%0Aimage%20states%20to%20actions.%20Gathering%20data%20to%20train%20such%20a%20model%20is%20often%0Aexpensive%20and%20challenging%2C%20and%20this%20model%20is%20limited%20to%20visual%20settings%20similar%0Ato%20the%20ones%20in%20which%20data%20are%20available.%20In%20this%20paper%2C%20we%20investigate%20how%20to%0Adirectly%20ground%20video%20models%20to%20continuous%20actions%20through%20self-exploration%20in%0Athe%20embodied%20environment%20--%20using%20generated%20video%20states%20as%20visual%20goals%20for%0Aexploration.%20We%20propose%20a%20framework%20that%20uses%20trajectory%20level%20action%0Ageneration%20in%20combination%20with%20video%20guidance%20to%20enable%20an%20agent%20to%20solve%0Acomplex%20tasks%20without%20any%20external%20supervision%2C%20e.g.%2C%20rewards%2C%20action%20labels%2C%0Aor%20segmentation%20masks.%20We%20validate%20the%20proposed%20approach%20on%208%20tasks%20in%20Libero%2C%0A6%20tasks%20in%20MetaWorld%2C%204%20tasks%20in%20Calvin%2C%20and%2012%20tasks%20in%20iThor%20Visual%0ANavigation.%20We%20show%20how%20our%20approach%20is%20on%20par%20with%20or%20even%20surpasses%20multiple%0Abehavior%20cloning%20baselines%20trained%20on%20expert%20demonstrations%20while%20without%0Arequiring%20any%20action%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520Video%2520Models%2520to%2520Actions%2520through%2520Goal%2520Conditioned%2520Exploration%26entry.906535625%3DYunhao%2520Luo%2520and%2520Yilun%2520Du%26entry.1292438233%3D%2520%2520Large%2520video%2520models%252C%2520pretrained%2520on%2520massive%2520amounts%2520of%2520Internet%2520video%252C%2520provide%250Aa%2520rich%2520source%2520of%2520physical%2520knowledge%2520about%2520the%2520dynamics%2520and%2520motions%2520of%2520objects%250Aand%2520tasks.%2520However%252C%2520video%2520models%2520are%2520not%2520grounded%2520in%2520the%2520embodiment%2520of%2520an%250Aagent%252C%2520and%2520do%2520not%2520describe%2520how%2520to%2520actuate%2520the%2520world%2520to%2520reach%2520the%2520visual%2520states%250Adepicted%2520in%2520a%2520video.%2520To%2520tackle%2520this%2520problem%252C%2520current%2520methods%2520use%2520a%2520separate%250Avision-based%2520inverse%2520dynamic%2520model%2520trained%2520on%2520embodiment-specific%2520data%2520to%2520map%250Aimage%2520states%2520to%2520actions.%2520Gathering%2520data%2520to%2520train%2520such%2520a%2520model%2520is%2520often%250Aexpensive%2520and%2520challenging%252C%2520and%2520this%2520model%2520is%2520limited%2520to%2520visual%2520settings%2520similar%250Ato%2520the%2520ones%2520in%2520which%2520data%2520are%2520available.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520to%250Adirectly%2520ground%2520video%2520models%2520to%2520continuous%2520actions%2520through%2520self-exploration%2520in%250Athe%2520embodied%2520environment%2520--%2520using%2520generated%2520video%2520states%2520as%2520visual%2520goals%2520for%250Aexploration.%2520We%2520propose%2520a%2520framework%2520that%2520uses%2520trajectory%2520level%2520action%250Ageneration%2520in%2520combination%2520with%2520video%2520guidance%2520to%2520enable%2520an%2520agent%2520to%2520solve%250Acomplex%2520tasks%2520without%2520any%2520external%2520supervision%252C%2520e.g.%252C%2520rewards%252C%2520action%2520labels%252C%250Aor%2520segmentation%2520masks.%2520We%2520validate%2520the%2520proposed%2520approach%2520on%25208%2520tasks%2520in%2520Libero%252C%250A6%2520tasks%2520in%2520MetaWorld%252C%25204%2520tasks%2520in%2520Calvin%252C%2520and%252012%2520tasks%2520in%2520iThor%2520Visual%250ANavigation.%2520We%2520show%2520how%2520our%2520approach%2520is%2520on%2520par%2520with%2520or%2520even%2520surpasses%2520multiple%250Abehavior%2520cloning%2520baselines%2520trained%2520on%2520expert%2520demonstrations%2520while%2520without%250Arequiring%2520any%2520action%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20Video%20Models%20to%20Actions%20through%20Goal%20Conditioned%20Exploration&entry.906535625=Yunhao%20Luo%20and%20Yilun%20Du&entry.1292438233=%20%20Large%20video%20models%2C%20pretrained%20on%20massive%20amounts%20of%20Internet%20video%2C%20provide%0Aa%20rich%20source%20of%20physical%20knowledge%20about%20the%20dynamics%20and%20motions%20of%20objects%0Aand%20tasks.%20However%2C%20video%20models%20are%20not%20grounded%20in%20the%20embodiment%20of%20an%0Aagent%2C%20and%20do%20not%20describe%20how%20to%20actuate%20the%20world%20to%20reach%20the%20visual%20states%0Adepicted%20in%20a%20video.%20To%20tackle%20this%20problem%2C%20current%20methods%20use%20a%20separate%0Avision-based%20inverse%20dynamic%20model%20trained%20on%20embodiment-specific%20data%20to%20map%0Aimage%20states%20to%20actions.%20Gathering%20data%20to%20train%20such%20a%20model%20is%20often%0Aexpensive%20and%20challenging%2C%20and%20this%20model%20is%20limited%20to%20visual%20settings%20similar%0Ato%20the%20ones%20in%20which%20data%20are%20available.%20In%20this%20paper%2C%20we%20investigate%20how%20to%0Adirectly%20ground%20video%20models%20to%20continuous%20actions%20through%20self-exploration%20in%0Athe%20embodied%20environment%20--%20using%20generated%20video%20states%20as%20visual%20goals%20for%0Aexploration.%20We%20propose%20a%20framework%20that%20uses%20trajectory%20level%20action%0Ageneration%20in%20combination%20with%20video%20guidance%20to%20enable%20an%20agent%20to%20solve%0Acomplex%20tasks%20without%20any%20external%20supervision%2C%20e.g.%2C%20rewards%2C%20action%20labels%2C%0Aor%20segmentation%20masks.%20We%20validate%20the%20proposed%20approach%20on%208%20tasks%20in%20Libero%2C%0A6%20tasks%20in%20MetaWorld%2C%204%20tasks%20in%20Calvin%2C%20and%2012%20tasks%20in%20iThor%20Visual%0ANavigation.%20We%20show%20how%20our%20approach%20is%20on%20par%20with%20or%20even%20surpasses%20multiple%0Abehavior%20cloning%20baselines%20trained%20on%20expert%20demonstrations%20while%20without%0Arequiring%20any%20action%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07223v1&entry.124074799=Read"},
{"title": "Knowledge Transfer in Deep Reinforcement Learning via an RL-Specific\n  GAN-Based Correspondence Function", "author": "Marko Ruman and Tatiana V. Guy", "abstract": "  Deep reinforcement learning has demonstrated superhuman performance in\ncomplex decision-making tasks, but it struggles with generalization and\nknowledge reuse - key aspects of true intelligence. This article introduces a\nnovel approach that modifies Cycle Generative Adversarial Networks specifically\nfor reinforcement learning, enabling effective one-to-one knowledge transfer\nbetween two tasks. Our method enhances the loss function with two new\ncomponents: model loss, which captures dynamic relationships between source and\ntarget tasks, and Q-loss, which identifies states significantly influencing the\ntarget decision policy. Tested on the 2-D Atari game Pong, our method achieved\n100% knowledge transfer in identical tasks and either 100% knowledge transfer\nor a 30% reduction in training time for a rotated task, depending on the\nnetwork architecture. In contrast, using standard Generative Adversarial\nNetworks or Cycle Generative Adversarial Networks led to worse performance than\ntraining from scratch in the majority of cases. The results demonstrate that\nthe proposed method ensured enhanced knowledge generalization in deep\nreinforcement learning.\n", "link": "http://arxiv.org/abs/2209.06604v2", "date": "2024-11-11", "relevancy": 2.4554, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4957}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4896}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Transfer%20in%20Deep%20Reinforcement%20Learning%20via%20an%20RL-Specific%0A%20%20GAN-Based%20Correspondence%20Function&body=Title%3A%20Knowledge%20Transfer%20in%20Deep%20Reinforcement%20Learning%20via%20an%20RL-Specific%0A%20%20GAN-Based%20Correspondence%20Function%0AAuthor%3A%20Marko%20Ruman%20and%20Tatiana%20V.%20Guy%0AAbstract%3A%20%20%20Deep%20reinforcement%20learning%20has%20demonstrated%20superhuman%20performance%20in%0Acomplex%20decision-making%20tasks%2C%20but%20it%20struggles%20with%20generalization%20and%0Aknowledge%20reuse%20-%20key%20aspects%20of%20true%20intelligence.%20This%20article%20introduces%20a%0Anovel%20approach%20that%20modifies%20Cycle%20Generative%20Adversarial%20Networks%20specifically%0Afor%20reinforcement%20learning%2C%20enabling%20effective%20one-to-one%20knowledge%20transfer%0Abetween%20two%20tasks.%20Our%20method%20enhances%20the%20loss%20function%20with%20two%20new%0Acomponents%3A%20model%20loss%2C%20which%20captures%20dynamic%20relationships%20between%20source%20and%0Atarget%20tasks%2C%20and%20Q-loss%2C%20which%20identifies%20states%20significantly%20influencing%20the%0Atarget%20decision%20policy.%20Tested%20on%20the%202-D%20Atari%20game%20Pong%2C%20our%20method%20achieved%0A100%25%20knowledge%20transfer%20in%20identical%20tasks%20and%20either%20100%25%20knowledge%20transfer%0Aor%20a%2030%25%20reduction%20in%20training%20time%20for%20a%20rotated%20task%2C%20depending%20on%20the%0Anetwork%20architecture.%20In%20contrast%2C%20using%20standard%20Generative%20Adversarial%0ANetworks%20or%20Cycle%20Generative%20Adversarial%20Networks%20led%20to%20worse%20performance%20than%0Atraining%20from%20scratch%20in%20the%20majority%20of%20cases.%20The%20results%20demonstrate%20that%0Athe%20proposed%20method%20ensured%20enhanced%20knowledge%20generalization%20in%20deep%0Areinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.06604v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Transfer%2520in%2520Deep%2520Reinforcement%2520Learning%2520via%2520an%2520RL-Specific%250A%2520%2520GAN-Based%2520Correspondence%2520Function%26entry.906535625%3DMarko%2520Ruman%2520and%2520Tatiana%2520V.%2520Guy%26entry.1292438233%3D%2520%2520Deep%2520reinforcement%2520learning%2520has%2520demonstrated%2520superhuman%2520performance%2520in%250Acomplex%2520decision-making%2520tasks%252C%2520but%2520it%2520struggles%2520with%2520generalization%2520and%250Aknowledge%2520reuse%2520-%2520key%2520aspects%2520of%2520true%2520intelligence.%2520This%2520article%2520introduces%2520a%250Anovel%2520approach%2520that%2520modifies%2520Cycle%2520Generative%2520Adversarial%2520Networks%2520specifically%250Afor%2520reinforcement%2520learning%252C%2520enabling%2520effective%2520one-to-one%2520knowledge%2520transfer%250Abetween%2520two%2520tasks.%2520Our%2520method%2520enhances%2520the%2520loss%2520function%2520with%2520two%2520new%250Acomponents%253A%2520model%2520loss%252C%2520which%2520captures%2520dynamic%2520relationships%2520between%2520source%2520and%250Atarget%2520tasks%252C%2520and%2520Q-loss%252C%2520which%2520identifies%2520states%2520significantly%2520influencing%2520the%250Atarget%2520decision%2520policy.%2520Tested%2520on%2520the%25202-D%2520Atari%2520game%2520Pong%252C%2520our%2520method%2520achieved%250A100%2525%2520knowledge%2520transfer%2520in%2520identical%2520tasks%2520and%2520either%2520100%2525%2520knowledge%2520transfer%250Aor%2520a%252030%2525%2520reduction%2520in%2520training%2520time%2520for%2520a%2520rotated%2520task%252C%2520depending%2520on%2520the%250Anetwork%2520architecture.%2520In%2520contrast%252C%2520using%2520standard%2520Generative%2520Adversarial%250ANetworks%2520or%2520Cycle%2520Generative%2520Adversarial%2520Networks%2520led%2520to%2520worse%2520performance%2520than%250Atraining%2520from%2520scratch%2520in%2520the%2520majority%2520of%2520cases.%2520The%2520results%2520demonstrate%2520that%250Athe%2520proposed%2520method%2520ensured%2520enhanced%2520knowledge%2520generalization%2520in%2520deep%250Areinforcement%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.06604v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Transfer%20in%20Deep%20Reinforcement%20Learning%20via%20an%20RL-Specific%0A%20%20GAN-Based%20Correspondence%20Function&entry.906535625=Marko%20Ruman%20and%20Tatiana%20V.%20Guy&entry.1292438233=%20%20Deep%20reinforcement%20learning%20has%20demonstrated%20superhuman%20performance%20in%0Acomplex%20decision-making%20tasks%2C%20but%20it%20struggles%20with%20generalization%20and%0Aknowledge%20reuse%20-%20key%20aspects%20of%20true%20intelligence.%20This%20article%20introduces%20a%0Anovel%20approach%20that%20modifies%20Cycle%20Generative%20Adversarial%20Networks%20specifically%0Afor%20reinforcement%20learning%2C%20enabling%20effective%20one-to-one%20knowledge%20transfer%0Abetween%20two%20tasks.%20Our%20method%20enhances%20the%20loss%20function%20with%20two%20new%0Acomponents%3A%20model%20loss%2C%20which%20captures%20dynamic%20relationships%20between%20source%20and%0Atarget%20tasks%2C%20and%20Q-loss%2C%20which%20identifies%20states%20significantly%20influencing%20the%0Atarget%20decision%20policy.%20Tested%20on%20the%202-D%20Atari%20game%20Pong%2C%20our%20method%20achieved%0A100%25%20knowledge%20transfer%20in%20identical%20tasks%20and%20either%20100%25%20knowledge%20transfer%0Aor%20a%2030%25%20reduction%20in%20training%20time%20for%20a%20rotated%20task%2C%20depending%20on%20the%0Anetwork%20architecture.%20In%20contrast%2C%20using%20standard%20Generative%20Adversarial%0ANetworks%20or%20Cycle%20Generative%20Adversarial%20Networks%20led%20to%20worse%20performance%20than%0Atraining%20from%20scratch%20in%20the%20majority%20of%20cases.%20The%20results%20demonstrate%20that%0Athe%20proposed%20method%20ensured%20enhanced%20knowledge%20generalization%20in%20deep%0Areinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.06604v2&entry.124074799=Read"},
{"title": "Slowing Down Forgetting in Continual Learning", "author": "Pascal Janetzky and Tobias Schlagenhauf and Stefan Feuerriegel", "abstract": "  A common challenge in continual learning (CL) is catastrophic forgetting,\nwhere the performance on old tasks drops after new, additional tasks are\nlearned. In this paper, we propose a novel framework called ReCL to slow down\nforgetting in CL. Our framework exploits an implicit bias of gradient-based\nneural networks due to which these converge to margin maximization points. Such\nconvergence points allow us to reconstruct old data from previous tasks, which\nwe then combine with the current training data. Our framework is flexible and\ncan be applied on top of existing, state-of-the-art CL methods to slow down\nforgetting. We further demonstrate the performance gain from our framework\nacross a large series of experiments, including different CL scenarios (class\nincremental, domain incremental, task incremental learning) different datasets\n(MNIST, CIFAR10), and different network architectures. Across all experiments,\nwe find large performance gains through ReCL. To the best of our knowledge, our\nframework is the first to address catastrophic forgetting by leveraging models\nin CL as their own memory buffers.\n", "link": "http://arxiv.org/abs/2411.06916v1", "date": "2024-11-11", "relevancy": 2.423, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4941}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Slowing%20Down%20Forgetting%20in%20Continual%20Learning&body=Title%3A%20Slowing%20Down%20Forgetting%20in%20Continual%20Learning%0AAuthor%3A%20Pascal%20Janetzky%20and%20Tobias%20Schlagenhauf%20and%20Stefan%20Feuerriegel%0AAbstract%3A%20%20%20A%20common%20challenge%20in%20continual%20learning%20%28CL%29%20is%20catastrophic%20forgetting%2C%0Awhere%20the%20performance%20on%20old%20tasks%20drops%20after%20new%2C%20additional%20tasks%20are%0Alearned.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20called%20ReCL%20to%20slow%20down%0Aforgetting%20in%20CL.%20Our%20framework%20exploits%20an%20implicit%20bias%20of%20gradient-based%0Aneural%20networks%20due%20to%20which%20these%20converge%20to%20margin%20maximization%20points.%20Such%0Aconvergence%20points%20allow%20us%20to%20reconstruct%20old%20data%20from%20previous%20tasks%2C%20which%0Awe%20then%20combine%20with%20the%20current%20training%20data.%20Our%20framework%20is%20flexible%20and%0Acan%20be%20applied%20on%20top%20of%20existing%2C%20state-of-the-art%20CL%20methods%20to%20slow%20down%0Aforgetting.%20We%20further%20demonstrate%20the%20performance%20gain%20from%20our%20framework%0Aacross%20a%20large%20series%20of%20experiments%2C%20including%20different%20CL%20scenarios%20%28class%0Aincremental%2C%20domain%20incremental%2C%20task%20incremental%20learning%29%20different%20datasets%0A%28MNIST%2C%20CIFAR10%29%2C%20and%20different%20network%20architectures.%20Across%20all%20experiments%2C%0Awe%20find%20large%20performance%20gains%20through%20ReCL.%20To%20the%20best%20of%20our%20knowledge%2C%20our%0Aframework%20is%20the%20first%20to%20address%20catastrophic%20forgetting%20by%20leveraging%20models%0Ain%20CL%20as%20their%20own%20memory%20buffers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlowing%2520Down%2520Forgetting%2520in%2520Continual%2520Learning%26entry.906535625%3DPascal%2520Janetzky%2520and%2520Tobias%2520Schlagenhauf%2520and%2520Stefan%2520Feuerriegel%26entry.1292438233%3D%2520%2520A%2520common%2520challenge%2520in%2520continual%2520learning%2520%2528CL%2529%2520is%2520catastrophic%2520forgetting%252C%250Awhere%2520the%2520performance%2520on%2520old%2520tasks%2520drops%2520after%2520new%252C%2520additional%2520tasks%2520are%250Alearned.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520called%2520ReCL%2520to%2520slow%2520down%250Aforgetting%2520in%2520CL.%2520Our%2520framework%2520exploits%2520an%2520implicit%2520bias%2520of%2520gradient-based%250Aneural%2520networks%2520due%2520to%2520which%2520these%2520converge%2520to%2520margin%2520maximization%2520points.%2520Such%250Aconvergence%2520points%2520allow%2520us%2520to%2520reconstruct%2520old%2520data%2520from%2520previous%2520tasks%252C%2520which%250Awe%2520then%2520combine%2520with%2520the%2520current%2520training%2520data.%2520Our%2520framework%2520is%2520flexible%2520and%250Acan%2520be%2520applied%2520on%2520top%2520of%2520existing%252C%2520state-of-the-art%2520CL%2520methods%2520to%2520slow%2520down%250Aforgetting.%2520We%2520further%2520demonstrate%2520the%2520performance%2520gain%2520from%2520our%2520framework%250Aacross%2520a%2520large%2520series%2520of%2520experiments%252C%2520including%2520different%2520CL%2520scenarios%2520%2528class%250Aincremental%252C%2520domain%2520incremental%252C%2520task%2520incremental%2520learning%2529%2520different%2520datasets%250A%2528MNIST%252C%2520CIFAR10%2529%252C%2520and%2520different%2520network%2520architectures.%2520Across%2520all%2520experiments%252C%250Awe%2520find%2520large%2520performance%2520gains%2520through%2520ReCL.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%250Aframework%2520is%2520the%2520first%2520to%2520address%2520catastrophic%2520forgetting%2520by%2520leveraging%2520models%250Ain%2520CL%2520as%2520their%2520own%2520memory%2520buffers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Slowing%20Down%20Forgetting%20in%20Continual%20Learning&entry.906535625=Pascal%20Janetzky%20and%20Tobias%20Schlagenhauf%20and%20Stefan%20Feuerriegel&entry.1292438233=%20%20A%20common%20challenge%20in%20continual%20learning%20%28CL%29%20is%20catastrophic%20forgetting%2C%0Awhere%20the%20performance%20on%20old%20tasks%20drops%20after%20new%2C%20additional%20tasks%20are%0Alearned.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20called%20ReCL%20to%20slow%20down%0Aforgetting%20in%20CL.%20Our%20framework%20exploits%20an%20implicit%20bias%20of%20gradient-based%0Aneural%20networks%20due%20to%20which%20these%20converge%20to%20margin%20maximization%20points.%20Such%0Aconvergence%20points%20allow%20us%20to%20reconstruct%20old%20data%20from%20previous%20tasks%2C%20which%0Awe%20then%20combine%20with%20the%20current%20training%20data.%20Our%20framework%20is%20flexible%20and%0Acan%20be%20applied%20on%20top%20of%20existing%2C%20state-of-the-art%20CL%20methods%20to%20slow%20down%0Aforgetting.%20We%20further%20demonstrate%20the%20performance%20gain%20from%20our%20framework%0Aacross%20a%20large%20series%20of%20experiments%2C%20including%20different%20CL%20scenarios%20%28class%0Aincremental%2C%20domain%20incremental%2C%20task%20incremental%20learning%29%20different%20datasets%0A%28MNIST%2C%20CIFAR10%29%2C%20and%20different%20network%20architectures.%20Across%20all%20experiments%2C%0Awe%20find%20large%20performance%20gains%20through%20ReCL.%20To%20the%20best%20of%20our%20knowledge%2C%20our%0Aframework%20is%20the%20first%20to%20address%20catastrophic%20forgetting%20by%20leveraging%20models%0Ain%20CL%20as%20their%20own%20memory%20buffers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06916v1&entry.124074799=Read"},
{"title": "PL-FSCIL: Harnessing the Power of Prompts for Few-Shot Class-Incremental\n  Learning", "author": "Songsong Tian and Lusi Li and Weijun Li and Hang Ran and Li Li and Xin Ning", "abstract": "  Few-Shot Class-Incremental Learning (FSCIL) aims to enable deep neural\nnetworks to learn new tasks incrementally from a small number of labeled\nsamples without forgetting previously learned tasks, closely mimicking human\nlearning patterns. In this paper, we propose a novel approach called Prompt\nLearning for FSCIL (PL-FSCIL), which harnesses the power of prompts in\nconjunction with a pre-trained Vision Transformer (ViT) model to address the\nchallenges of FSCIL effectively. Our work pioneers the use of visual prompts in\nFSCIL, which is characterized by its notable simplicity. PL-FSCIL consists of\ntwo distinct prompts: the Domain Prompt and the FSCIL Prompt. Both are vectors\nthat augment the model by embedding themselves into the attention layer of the\nViT model. Specifically, the Domain Prompt assists the ViT model in adapting to\nnew data domains. The task-specific FSCIL Prompt, coupled with a prototype\nclassifier, amplifies the model's ability to effectively handle FSCIL tasks. We\nvalidate the efficacy of PL-FSCIL on widely used benchmark datasets such as\nCIFAR-100 and CUB-200. The results showcase competitive performance,\nunderscoring its promising potential for real-world applications where\nhigh-quality data is often scarce. The source code is available at:\nhttps://github.com/TianSongS/PL-FSCIL.\n", "link": "http://arxiv.org/abs/2401.14807v2", "date": "2024-11-11", "relevancy": 2.4213, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4813}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PL-FSCIL%3A%20Harnessing%20the%20Power%20of%20Prompts%20for%20Few-Shot%20Class-Incremental%0A%20%20Learning&body=Title%3A%20PL-FSCIL%3A%20Harnessing%20the%20Power%20of%20Prompts%20for%20Few-Shot%20Class-Incremental%0A%20%20Learning%0AAuthor%3A%20Songsong%20Tian%20and%20Lusi%20Li%20and%20Weijun%20Li%20and%20Hang%20Ran%20and%20Li%20Li%20and%20Xin%20Ning%0AAbstract%3A%20%20%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20aims%20to%20enable%20deep%20neural%0Anetworks%20to%20learn%20new%20tasks%20incrementally%20from%20a%20small%20number%20of%20labeled%0Asamples%20without%20forgetting%20previously%20learned%20tasks%2C%20closely%20mimicking%20human%0Alearning%20patterns.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20called%20Prompt%0ALearning%20for%20FSCIL%20%28PL-FSCIL%29%2C%20which%20harnesses%20the%20power%20of%20prompts%20in%0Aconjunction%20with%20a%20pre-trained%20Vision%20Transformer%20%28ViT%29%20model%20to%20address%20the%0Achallenges%20of%20FSCIL%20effectively.%20Our%20work%20pioneers%20the%20use%20of%20visual%20prompts%20in%0AFSCIL%2C%20which%20is%20characterized%20by%20its%20notable%20simplicity.%20PL-FSCIL%20consists%20of%0Atwo%20distinct%20prompts%3A%20the%20Domain%20Prompt%20and%20the%20FSCIL%20Prompt.%20Both%20are%20vectors%0Athat%20augment%20the%20model%20by%20embedding%20themselves%20into%20the%20attention%20layer%20of%20the%0AViT%20model.%20Specifically%2C%20the%20Domain%20Prompt%20assists%20the%20ViT%20model%20in%20adapting%20to%0Anew%20data%20domains.%20The%20task-specific%20FSCIL%20Prompt%2C%20coupled%20with%20a%20prototype%0Aclassifier%2C%20amplifies%20the%20model%27s%20ability%20to%20effectively%20handle%20FSCIL%20tasks.%20We%0Avalidate%20the%20efficacy%20of%20PL-FSCIL%20on%20widely%20used%20benchmark%20datasets%20such%20as%0ACIFAR-100%20and%20CUB-200.%20The%20results%20showcase%20competitive%20performance%2C%0Aunderscoring%20its%20promising%20potential%20for%20real-world%20applications%20where%0Ahigh-quality%20data%20is%20often%20scarce.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/TianSongS/PL-FSCIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPL-FSCIL%253A%2520Harnessing%2520the%2520Power%2520of%2520Prompts%2520for%2520Few-Shot%2520Class-Incremental%250A%2520%2520Learning%26entry.906535625%3DSongsong%2520Tian%2520and%2520Lusi%2520Li%2520and%2520Weijun%2520Li%2520and%2520Hang%2520Ran%2520and%2520Li%2520Li%2520and%2520Xin%2520Ning%26entry.1292438233%3D%2520%2520Few-Shot%2520Class-Incremental%2520Learning%2520%2528FSCIL%2529%2520aims%2520to%2520enable%2520deep%2520neural%250Anetworks%2520to%2520learn%2520new%2520tasks%2520incrementally%2520from%2520a%2520small%2520number%2520of%2520labeled%250Asamples%2520without%2520forgetting%2520previously%2520learned%2520tasks%252C%2520closely%2520mimicking%2520human%250Alearning%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520called%2520Prompt%250ALearning%2520for%2520FSCIL%2520%2528PL-FSCIL%2529%252C%2520which%2520harnesses%2520the%2520power%2520of%2520prompts%2520in%250Aconjunction%2520with%2520a%2520pre-trained%2520Vision%2520Transformer%2520%2528ViT%2529%2520model%2520to%2520address%2520the%250Achallenges%2520of%2520FSCIL%2520effectively.%2520Our%2520work%2520pioneers%2520the%2520use%2520of%2520visual%2520prompts%2520in%250AFSCIL%252C%2520which%2520is%2520characterized%2520by%2520its%2520notable%2520simplicity.%2520PL-FSCIL%2520consists%2520of%250Atwo%2520distinct%2520prompts%253A%2520the%2520Domain%2520Prompt%2520and%2520the%2520FSCIL%2520Prompt.%2520Both%2520are%2520vectors%250Athat%2520augment%2520the%2520model%2520by%2520embedding%2520themselves%2520into%2520the%2520attention%2520layer%2520of%2520the%250AViT%2520model.%2520Specifically%252C%2520the%2520Domain%2520Prompt%2520assists%2520the%2520ViT%2520model%2520in%2520adapting%2520to%250Anew%2520data%2520domains.%2520The%2520task-specific%2520FSCIL%2520Prompt%252C%2520coupled%2520with%2520a%2520prototype%250Aclassifier%252C%2520amplifies%2520the%2520model%2527s%2520ability%2520to%2520effectively%2520handle%2520FSCIL%2520tasks.%2520We%250Avalidate%2520the%2520efficacy%2520of%2520PL-FSCIL%2520on%2520widely%2520used%2520benchmark%2520datasets%2520such%2520as%250ACIFAR-100%2520and%2520CUB-200.%2520The%2520results%2520showcase%2520competitive%2520performance%252C%250Aunderscoring%2520its%2520promising%2520potential%2520for%2520real-world%2520applications%2520where%250Ahigh-quality%2520data%2520is%2520often%2520scarce.%2520The%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/TianSongS/PL-FSCIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PL-FSCIL%3A%20Harnessing%20the%20Power%20of%20Prompts%20for%20Few-Shot%20Class-Incremental%0A%20%20Learning&entry.906535625=Songsong%20Tian%20and%20Lusi%20Li%20and%20Weijun%20Li%20and%20Hang%20Ran%20and%20Li%20Li%20and%20Xin%20Ning&entry.1292438233=%20%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20aims%20to%20enable%20deep%20neural%0Anetworks%20to%20learn%20new%20tasks%20incrementally%20from%20a%20small%20number%20of%20labeled%0Asamples%20without%20forgetting%20previously%20learned%20tasks%2C%20closely%20mimicking%20human%0Alearning%20patterns.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20called%20Prompt%0ALearning%20for%20FSCIL%20%28PL-FSCIL%29%2C%20which%20harnesses%20the%20power%20of%20prompts%20in%0Aconjunction%20with%20a%20pre-trained%20Vision%20Transformer%20%28ViT%29%20model%20to%20address%20the%0Achallenges%20of%20FSCIL%20effectively.%20Our%20work%20pioneers%20the%20use%20of%20visual%20prompts%20in%0AFSCIL%2C%20which%20is%20characterized%20by%20its%20notable%20simplicity.%20PL-FSCIL%20consists%20of%0Atwo%20distinct%20prompts%3A%20the%20Domain%20Prompt%20and%20the%20FSCIL%20Prompt.%20Both%20are%20vectors%0Athat%20augment%20the%20model%20by%20embedding%20themselves%20into%20the%20attention%20layer%20of%20the%0AViT%20model.%20Specifically%2C%20the%20Domain%20Prompt%20assists%20the%20ViT%20model%20in%20adapting%20to%0Anew%20data%20domains.%20The%20task-specific%20FSCIL%20Prompt%2C%20coupled%20with%20a%20prototype%0Aclassifier%2C%20amplifies%20the%20model%27s%20ability%20to%20effectively%20handle%20FSCIL%20tasks.%20We%0Avalidate%20the%20efficacy%20of%20PL-FSCIL%20on%20widely%20used%20benchmark%20datasets%20such%20as%0ACIFAR-100%20and%20CUB-200.%20The%20results%20showcase%20competitive%20performance%2C%0Aunderscoring%20its%20promising%20potential%20for%20real-world%20applications%20where%0Ahigh-quality%20data%20is%20often%20scarce.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/TianSongS/PL-FSCIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14807v2&entry.124074799=Read"},
{"title": "HandCraft: Anatomically Correct Restoration of Malformed Hands in\n  Diffusion Generated Images", "author": "Zhenyue Qin and Yiqun Zhang and Yang Liu and Dylan Campbell", "abstract": "  Generative text-to-image models, such as Stable Diffusion, have demonstrated\na remarkable ability to generate diverse, high-quality images. However, they\nare surprisingly inept when it comes to rendering human hands, which are often\nanatomically incorrect or reside in the \"uncanny valley\". In this paper, we\npropose a method HandCraft for restoring such malformed hands. This is achieved\nby automatically constructing masks and depth images for hands as conditioning\nsignals using a parametric model, allowing a diffusion-based image editor to\nfix the hand's anatomy and adjust its pose while seamlessly integrating the\nchanges into the original image, preserving pose, color, and style. Our\nplug-and-play hand restoration solution is compatible with existing pretrained\ndiffusion models, and the restoration process facilitates adoption by eschewing\nany fine-tuning or training requirements for the diffusion models. We also\ncontribute MalHand datasets that contain generated images with a wide variety\nof malformed hands in several styles for hand detector training and hand\nrestoration benchmarking, and demonstrate through qualitative and quantitative\nevaluation that HandCraft not only restores anatomical correctness but also\nmaintains the integrity of the overall image.\n", "link": "http://arxiv.org/abs/2411.04332v2", "date": "2024-11-11", "relevancy": 2.4071, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6168}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.597}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HandCraft%3A%20Anatomically%20Correct%20Restoration%20of%20Malformed%20Hands%20in%0A%20%20Diffusion%20Generated%20Images&body=Title%3A%20HandCraft%3A%20Anatomically%20Correct%20Restoration%20of%20Malformed%20Hands%20in%0A%20%20Diffusion%20Generated%20Images%0AAuthor%3A%20Zhenyue%20Qin%20and%20Yiqun%20Zhang%20and%20Yang%20Liu%20and%20Dylan%20Campbell%0AAbstract%3A%20%20%20Generative%20text-to-image%20models%2C%20such%20as%20Stable%20Diffusion%2C%20have%20demonstrated%0Aa%20remarkable%20ability%20to%20generate%20diverse%2C%20high-quality%20images.%20However%2C%20they%0Aare%20surprisingly%20inept%20when%20it%20comes%20to%20rendering%20human%20hands%2C%20which%20are%20often%0Aanatomically%20incorrect%20or%20reside%20in%20the%20%22uncanny%20valley%22.%20In%20this%20paper%2C%20we%0Apropose%20a%20method%20HandCraft%20for%20restoring%20such%20malformed%20hands.%20This%20is%20achieved%0Aby%20automatically%20constructing%20masks%20and%20depth%20images%20for%20hands%20as%20conditioning%0Asignals%20using%20a%20parametric%20model%2C%20allowing%20a%20diffusion-based%20image%20editor%20to%0Afix%20the%20hand%27s%20anatomy%20and%20adjust%20its%20pose%20while%20seamlessly%20integrating%20the%0Achanges%20into%20the%20original%20image%2C%20preserving%20pose%2C%20color%2C%20and%20style.%20Our%0Aplug-and-play%20hand%20restoration%20solution%20is%20compatible%20with%20existing%20pretrained%0Adiffusion%20models%2C%20and%20the%20restoration%20process%20facilitates%20adoption%20by%20eschewing%0Aany%20fine-tuning%20or%20training%20requirements%20for%20the%20diffusion%20models.%20We%20also%0Acontribute%20MalHand%20datasets%20that%20contain%20generated%20images%20with%20a%20wide%20variety%0Aof%20malformed%20hands%20in%20several%20styles%20for%20hand%20detector%20training%20and%20hand%0Arestoration%20benchmarking%2C%20and%20demonstrate%20through%20qualitative%20and%20quantitative%0Aevaluation%20that%20HandCraft%20not%20only%20restores%20anatomical%20correctness%20but%20also%0Amaintains%20the%20integrity%20of%20the%20overall%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04332v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandCraft%253A%2520Anatomically%2520Correct%2520Restoration%2520of%2520Malformed%2520Hands%2520in%250A%2520%2520Diffusion%2520Generated%2520Images%26entry.906535625%3DZhenyue%2520Qin%2520and%2520Yiqun%2520Zhang%2520and%2520Yang%2520Liu%2520and%2520Dylan%2520Campbell%26entry.1292438233%3D%2520%2520Generative%2520text-to-image%2520models%252C%2520such%2520as%2520Stable%2520Diffusion%252C%2520have%2520demonstrated%250Aa%2520remarkable%2520ability%2520to%2520generate%2520diverse%252C%2520high-quality%2520images.%2520However%252C%2520they%250Aare%2520surprisingly%2520inept%2520when%2520it%2520comes%2520to%2520rendering%2520human%2520hands%252C%2520which%2520are%2520often%250Aanatomically%2520incorrect%2520or%2520reside%2520in%2520the%2520%2522uncanny%2520valley%2522.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520method%2520HandCraft%2520for%2520restoring%2520such%2520malformed%2520hands.%2520This%2520is%2520achieved%250Aby%2520automatically%2520constructing%2520masks%2520and%2520depth%2520images%2520for%2520hands%2520as%2520conditioning%250Asignals%2520using%2520a%2520parametric%2520model%252C%2520allowing%2520a%2520diffusion-based%2520image%2520editor%2520to%250Afix%2520the%2520hand%2527s%2520anatomy%2520and%2520adjust%2520its%2520pose%2520while%2520seamlessly%2520integrating%2520the%250Achanges%2520into%2520the%2520original%2520image%252C%2520preserving%2520pose%252C%2520color%252C%2520and%2520style.%2520Our%250Aplug-and-play%2520hand%2520restoration%2520solution%2520is%2520compatible%2520with%2520existing%2520pretrained%250Adiffusion%2520models%252C%2520and%2520the%2520restoration%2520process%2520facilitates%2520adoption%2520by%2520eschewing%250Aany%2520fine-tuning%2520or%2520training%2520requirements%2520for%2520the%2520diffusion%2520models.%2520We%2520also%250Acontribute%2520MalHand%2520datasets%2520that%2520contain%2520generated%2520images%2520with%2520a%2520wide%2520variety%250Aof%2520malformed%2520hands%2520in%2520several%2520styles%2520for%2520hand%2520detector%2520training%2520and%2520hand%250Arestoration%2520benchmarking%252C%2520and%2520demonstrate%2520through%2520qualitative%2520and%2520quantitative%250Aevaluation%2520that%2520HandCraft%2520not%2520only%2520restores%2520anatomical%2520correctness%2520but%2520also%250Amaintains%2520the%2520integrity%2520of%2520the%2520overall%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04332v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HandCraft%3A%20Anatomically%20Correct%20Restoration%20of%20Malformed%20Hands%20in%0A%20%20Diffusion%20Generated%20Images&entry.906535625=Zhenyue%20Qin%20and%20Yiqun%20Zhang%20and%20Yang%20Liu%20and%20Dylan%20Campbell&entry.1292438233=%20%20Generative%20text-to-image%20models%2C%20such%20as%20Stable%20Diffusion%2C%20have%20demonstrated%0Aa%20remarkable%20ability%20to%20generate%20diverse%2C%20high-quality%20images.%20However%2C%20they%0Aare%20surprisingly%20inept%20when%20it%20comes%20to%20rendering%20human%20hands%2C%20which%20are%20often%0Aanatomically%20incorrect%20or%20reside%20in%20the%20%22uncanny%20valley%22.%20In%20this%20paper%2C%20we%0Apropose%20a%20method%20HandCraft%20for%20restoring%20such%20malformed%20hands.%20This%20is%20achieved%0Aby%20automatically%20constructing%20masks%20and%20depth%20images%20for%20hands%20as%20conditioning%0Asignals%20using%20a%20parametric%20model%2C%20allowing%20a%20diffusion-based%20image%20editor%20to%0Afix%20the%20hand%27s%20anatomy%20and%20adjust%20its%20pose%20while%20seamlessly%20integrating%20the%0Achanges%20into%20the%20original%20image%2C%20preserving%20pose%2C%20color%2C%20and%20style.%20Our%0Aplug-and-play%20hand%20restoration%20solution%20is%20compatible%20with%20existing%20pretrained%0Adiffusion%20models%2C%20and%20the%20restoration%20process%20facilitates%20adoption%20by%20eschewing%0Aany%20fine-tuning%20or%20training%20requirements%20for%20the%20diffusion%20models.%20We%20also%0Acontribute%20MalHand%20datasets%20that%20contain%20generated%20images%20with%20a%20wide%20variety%0Aof%20malformed%20hands%20in%20several%20styles%20for%20hand%20detector%20training%20and%20hand%0Arestoration%20benchmarking%2C%20and%20demonstrate%20through%20qualitative%20and%20quantitative%0Aevaluation%20that%20HandCraft%20not%20only%20restores%20anatomical%20correctness%20but%20also%0Amaintains%20the%20integrity%20of%20the%20overall%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04332v2&entry.124074799=Read"},
{"title": "UniHR: Hierarchical Representation Learning for Unified Knowledge Graph\n  Link Prediction", "author": "Zhiqiang Liu and Mingyang Chen and Yin Hua and Zhuo Chen and Ziqi Liu and Lei Liang and Huajun Chen and Wen Zhang", "abstract": "  Beyond-triple fact representations including hyper-relational facts with\nauxiliary key-value pairs, temporal facts with additional timestamps, and\nnested facts implying relationships between facts, are gaining significant\nattention. However, existing link prediction models are usually designed for\none specific type of facts, making it difficult to generalize to other fact\nrepresentations. To overcome this limitation, we propose a Unified Hierarchical\nRepresentation learning framework (UniHR) for unified knowledge graph link\nprediction. It consists of a unified Hierarchical Data Representation (HiDR)\nmodule and a unified Hierarchical Structure Learning (HiSL) module as graph\nencoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested\nfactual KGs into triple-based representations. Then HiSL incorporates\nintra-fact and inter-fact message passing, focusing on enhancing the semantic\ninformation within individual facts and enriching the structural information\nbetween facts. Experimental results across 7 datasets from 3 types of KGs\ndemonstrate that our UniHR outperforms baselines designed for one specific kind\nof KG, indicating strong generalization capability of HiDR form and the\neffectiveness of HiSL module. Code and data are available at\nhttps://github.com/Lza12a/UniHR.\n", "link": "http://arxiv.org/abs/2411.07019v1", "date": "2024-11-11", "relevancy": 2.3795, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniHR%3A%20Hierarchical%20Representation%20Learning%20for%20Unified%20Knowledge%20Graph%0A%20%20Link%20Prediction&body=Title%3A%20UniHR%3A%20Hierarchical%20Representation%20Learning%20for%20Unified%20Knowledge%20Graph%0A%20%20Link%20Prediction%0AAuthor%3A%20Zhiqiang%20Liu%20and%20Mingyang%20Chen%20and%20Yin%20Hua%20and%20Zhuo%20Chen%20and%20Ziqi%20Liu%20and%20Lei%20Liang%20and%20Huajun%20Chen%20and%20Wen%20Zhang%0AAbstract%3A%20%20%20Beyond-triple%20fact%20representations%20including%20hyper-relational%20facts%20with%0Aauxiliary%20key-value%20pairs%2C%20temporal%20facts%20with%20additional%20timestamps%2C%20and%0Anested%20facts%20implying%20relationships%20between%20facts%2C%20are%20gaining%20significant%0Aattention.%20However%2C%20existing%20link%20prediction%20models%20are%20usually%20designed%20for%0Aone%20specific%20type%20of%20facts%2C%20making%20it%20difficult%20to%20generalize%20to%20other%20fact%0Arepresentations.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20Unified%20Hierarchical%0ARepresentation%20learning%20framework%20%28UniHR%29%20for%20unified%20knowledge%20graph%20link%0Aprediction.%20It%20consists%20of%20a%20unified%20Hierarchical%20Data%20Representation%20%28HiDR%29%0Amodule%20and%20a%20unified%20Hierarchical%20Structure%20Learning%20%28HiSL%29%20module%20as%20graph%0Aencoder.%20The%20HiDR%20module%20unifies%20hyper-relational%20KGs%2C%20temporal%20KGs%2C%20and%20nested%0Afactual%20KGs%20into%20triple-based%20representations.%20Then%20HiSL%20incorporates%0Aintra-fact%20and%20inter-fact%20message%20passing%2C%20focusing%20on%20enhancing%20the%20semantic%0Ainformation%20within%20individual%20facts%20and%20enriching%20the%20structural%20information%0Abetween%20facts.%20Experimental%20results%20across%207%20datasets%20from%203%20types%20of%20KGs%0Ademonstrate%20that%20our%20UniHR%20outperforms%20baselines%20designed%20for%20one%20specific%20kind%0Aof%20KG%2C%20indicating%20strong%20generalization%20capability%20of%20HiDR%20form%20and%20the%0Aeffectiveness%20of%20HiSL%20module.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Lza12a/UniHR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniHR%253A%2520Hierarchical%2520Representation%2520Learning%2520for%2520Unified%2520Knowledge%2520Graph%250A%2520%2520Link%2520Prediction%26entry.906535625%3DZhiqiang%2520Liu%2520and%2520Mingyang%2520Chen%2520and%2520Yin%2520Hua%2520and%2520Zhuo%2520Chen%2520and%2520Ziqi%2520Liu%2520and%2520Lei%2520Liang%2520and%2520Huajun%2520Chen%2520and%2520Wen%2520Zhang%26entry.1292438233%3D%2520%2520Beyond-triple%2520fact%2520representations%2520including%2520hyper-relational%2520facts%2520with%250Aauxiliary%2520key-value%2520pairs%252C%2520temporal%2520facts%2520with%2520additional%2520timestamps%252C%2520and%250Anested%2520facts%2520implying%2520relationships%2520between%2520facts%252C%2520are%2520gaining%2520significant%250Aattention.%2520However%252C%2520existing%2520link%2520prediction%2520models%2520are%2520usually%2520designed%2520for%250Aone%2520specific%2520type%2520of%2520facts%252C%2520making%2520it%2520difficult%2520to%2520generalize%2520to%2520other%2520fact%250Arepresentations.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520Unified%2520Hierarchical%250ARepresentation%2520learning%2520framework%2520%2528UniHR%2529%2520for%2520unified%2520knowledge%2520graph%2520link%250Aprediction.%2520It%2520consists%2520of%2520a%2520unified%2520Hierarchical%2520Data%2520Representation%2520%2528HiDR%2529%250Amodule%2520and%2520a%2520unified%2520Hierarchical%2520Structure%2520Learning%2520%2528HiSL%2529%2520module%2520as%2520graph%250Aencoder.%2520The%2520HiDR%2520module%2520unifies%2520hyper-relational%2520KGs%252C%2520temporal%2520KGs%252C%2520and%2520nested%250Afactual%2520KGs%2520into%2520triple-based%2520representations.%2520Then%2520HiSL%2520incorporates%250Aintra-fact%2520and%2520inter-fact%2520message%2520passing%252C%2520focusing%2520on%2520enhancing%2520the%2520semantic%250Ainformation%2520within%2520individual%2520facts%2520and%2520enriching%2520the%2520structural%2520information%250Abetween%2520facts.%2520Experimental%2520results%2520across%25207%2520datasets%2520from%25203%2520types%2520of%2520KGs%250Ademonstrate%2520that%2520our%2520UniHR%2520outperforms%2520baselines%2520designed%2520for%2520one%2520specific%2520kind%250Aof%2520KG%252C%2520indicating%2520strong%2520generalization%2520capability%2520of%2520HiDR%2520form%2520and%2520the%250Aeffectiveness%2520of%2520HiSL%2520module.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/Lza12a/UniHR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniHR%3A%20Hierarchical%20Representation%20Learning%20for%20Unified%20Knowledge%20Graph%0A%20%20Link%20Prediction&entry.906535625=Zhiqiang%20Liu%20and%20Mingyang%20Chen%20and%20Yin%20Hua%20and%20Zhuo%20Chen%20and%20Ziqi%20Liu%20and%20Lei%20Liang%20and%20Huajun%20Chen%20and%20Wen%20Zhang&entry.1292438233=%20%20Beyond-triple%20fact%20representations%20including%20hyper-relational%20facts%20with%0Aauxiliary%20key-value%20pairs%2C%20temporal%20facts%20with%20additional%20timestamps%2C%20and%0Anested%20facts%20implying%20relationships%20between%20facts%2C%20are%20gaining%20significant%0Aattention.%20However%2C%20existing%20link%20prediction%20models%20are%20usually%20designed%20for%0Aone%20specific%20type%20of%20facts%2C%20making%20it%20difficult%20to%20generalize%20to%20other%20fact%0Arepresentations.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20Unified%20Hierarchical%0ARepresentation%20learning%20framework%20%28UniHR%29%20for%20unified%20knowledge%20graph%20link%0Aprediction.%20It%20consists%20of%20a%20unified%20Hierarchical%20Data%20Representation%20%28HiDR%29%0Amodule%20and%20a%20unified%20Hierarchical%20Structure%20Learning%20%28HiSL%29%20module%20as%20graph%0Aencoder.%20The%20HiDR%20module%20unifies%20hyper-relational%20KGs%2C%20temporal%20KGs%2C%20and%20nested%0Afactual%20KGs%20into%20triple-based%20representations.%20Then%20HiSL%20incorporates%0Aintra-fact%20and%20inter-fact%20message%20passing%2C%20focusing%20on%20enhancing%20the%20semantic%0Ainformation%20within%20individual%20facts%20and%20enriching%20the%20structural%20information%0Abetween%20facts.%20Experimental%20results%20across%207%20datasets%20from%203%20types%20of%20KGs%0Ademonstrate%20that%20our%20UniHR%20outperforms%20baselines%20designed%20for%20one%20specific%20kind%0Aof%20KG%2C%20indicating%20strong%20generalization%20capability%20of%20HiDR%20form%20and%20the%0Aeffectiveness%20of%20HiSL%20module.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Lza12a/UniHR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07019v1&entry.124074799=Read"},
{"title": "Towards Characterizing Cyber Networks with Large Language Models", "author": "Alaric Hartsock and Luiz Manella Pereira and Glenn Fink", "abstract": "  Threat hunting analyzes large, noisy, high-dimensional data to find sparse\nadversarial behavior. We believe adversarial activities, however they are\ndisguised, are extremely difficult to completely obscure in high dimensional\nspace. In this paper, we employ these latent features of cyber data to find\nanomalies via a prototype tool called Cyber Log Embeddings Model (CLEM). CLEM\nwas trained on Zeek network traffic logs from both a real-world production\nnetwork and an from Internet of Things (IoT) cybersecurity testbed. The model\nis deliberately overtrained on a sliding window of data to characterize each\nwindow closely. We use the Adjusted Rand Index (ARI) to comparing the k-means\nclustering of CLEM output to expert labeling of the embeddings. Our approach\ndemonstrates that there is promise in using natural language modeling to\nunderstand cyber data.\n", "link": "http://arxiv.org/abs/2411.07089v1", "date": "2024-11-11", "relevancy": 2.3498, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Characterizing%20Cyber%20Networks%20with%20Large%20Language%20Models&body=Title%3A%20Towards%20Characterizing%20Cyber%20Networks%20with%20Large%20Language%20Models%0AAuthor%3A%20Alaric%20Hartsock%20and%20Luiz%20Manella%20Pereira%20and%20Glenn%20Fink%0AAbstract%3A%20%20%20Threat%20hunting%20analyzes%20large%2C%20noisy%2C%20high-dimensional%20data%20to%20find%20sparse%0Aadversarial%20behavior.%20We%20believe%20adversarial%20activities%2C%20however%20they%20are%0Adisguised%2C%20are%20extremely%20difficult%20to%20completely%20obscure%20in%20high%20dimensional%0Aspace.%20In%20this%20paper%2C%20we%20employ%20these%20latent%20features%20of%20cyber%20data%20to%20find%0Aanomalies%20via%20a%20prototype%20tool%20called%20Cyber%20Log%20Embeddings%20Model%20%28CLEM%29.%20CLEM%0Awas%20trained%20on%20Zeek%20network%20traffic%20logs%20from%20both%20a%20real-world%20production%0Anetwork%20and%20an%20from%20Internet%20of%20Things%20%28IoT%29%20cybersecurity%20testbed.%20The%20model%0Ais%20deliberately%20overtrained%20on%20a%20sliding%20window%20of%20data%20to%20characterize%20each%0Awindow%20closely.%20We%20use%20the%20Adjusted%20Rand%20Index%20%28ARI%29%20to%20comparing%20the%20k-means%0Aclustering%20of%20CLEM%20output%20to%20expert%20labeling%20of%20the%20embeddings.%20Our%20approach%0Ademonstrates%20that%20there%20is%20promise%20in%20using%20natural%20language%20modeling%20to%0Aunderstand%20cyber%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Characterizing%2520Cyber%2520Networks%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DAlaric%2520Hartsock%2520and%2520Luiz%2520Manella%2520Pereira%2520and%2520Glenn%2520Fink%26entry.1292438233%3D%2520%2520Threat%2520hunting%2520analyzes%2520large%252C%2520noisy%252C%2520high-dimensional%2520data%2520to%2520find%2520sparse%250Aadversarial%2520behavior.%2520We%2520believe%2520adversarial%2520activities%252C%2520however%2520they%2520are%250Adisguised%252C%2520are%2520extremely%2520difficult%2520to%2520completely%2520obscure%2520in%2520high%2520dimensional%250Aspace.%2520In%2520this%2520paper%252C%2520we%2520employ%2520these%2520latent%2520features%2520of%2520cyber%2520data%2520to%2520find%250Aanomalies%2520via%2520a%2520prototype%2520tool%2520called%2520Cyber%2520Log%2520Embeddings%2520Model%2520%2528CLEM%2529.%2520CLEM%250Awas%2520trained%2520on%2520Zeek%2520network%2520traffic%2520logs%2520from%2520both%2520a%2520real-world%2520production%250Anetwork%2520and%2520an%2520from%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520cybersecurity%2520testbed.%2520The%2520model%250Ais%2520deliberately%2520overtrained%2520on%2520a%2520sliding%2520window%2520of%2520data%2520to%2520characterize%2520each%250Awindow%2520closely.%2520We%2520use%2520the%2520Adjusted%2520Rand%2520Index%2520%2528ARI%2529%2520to%2520comparing%2520the%2520k-means%250Aclustering%2520of%2520CLEM%2520output%2520to%2520expert%2520labeling%2520of%2520the%2520embeddings.%2520Our%2520approach%250Ademonstrates%2520that%2520there%2520is%2520promise%2520in%2520using%2520natural%2520language%2520modeling%2520to%250Aunderstand%2520cyber%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Characterizing%20Cyber%20Networks%20with%20Large%20Language%20Models&entry.906535625=Alaric%20Hartsock%20and%20Luiz%20Manella%20Pereira%20and%20Glenn%20Fink&entry.1292438233=%20%20Threat%20hunting%20analyzes%20large%2C%20noisy%2C%20high-dimensional%20data%20to%20find%20sparse%0Aadversarial%20behavior.%20We%20believe%20adversarial%20activities%2C%20however%20they%20are%0Adisguised%2C%20are%20extremely%20difficult%20to%20completely%20obscure%20in%20high%20dimensional%0Aspace.%20In%20this%20paper%2C%20we%20employ%20these%20latent%20features%20of%20cyber%20data%20to%20find%0Aanomalies%20via%20a%20prototype%20tool%20called%20Cyber%20Log%20Embeddings%20Model%20%28CLEM%29.%20CLEM%0Awas%20trained%20on%20Zeek%20network%20traffic%20logs%20from%20both%20a%20real-world%20production%0Anetwork%20and%20an%20from%20Internet%20of%20Things%20%28IoT%29%20cybersecurity%20testbed.%20The%20model%0Ais%20deliberately%20overtrained%20on%20a%20sliding%20window%20of%20data%20to%20characterize%20each%0Awindow%20closely.%20We%20use%20the%20Adjusted%20Rand%20Index%20%28ARI%29%20to%20comparing%20the%20k-means%0Aclustering%20of%20CLEM%20output%20to%20expert%20labeling%20of%20the%20embeddings.%20Our%20approach%0Ademonstrates%20that%20there%20is%20promise%20in%20using%20natural%20language%20modeling%20to%0Aunderstand%20cyber%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07089v1&entry.124074799=Read"},
{"title": "UMFC: Unsupervised Multi-Domain Feature Calibration for Vision-Language\n  Models", "author": "Jiachen Liang and Ruibing Hou and Minyang Hu and Hong Chang and Shiguang Shan and Xilin Chen", "abstract": "  Pre-trained vision-language models (e.g., CLIP) have shown powerful zero-shot\ntransfer capabilities. But they still struggle with domain shifts and typically\nrequire labeled data to adapt to downstream tasks, which could be costly. In\nthis work, we aim to leverage unlabeled data that naturally spans multiple\ndomains to enhance the transferability of vision-language models. Under this\nunsupervised multi-domain setting, we have identified inherent model bias\nwithin CLIP, notably in its visual and text encoders. Specifically, we observe\nthat CLIP's visual encoder tends to prioritize encoding domain over\ndiscriminative category information, meanwhile its text encoder exhibits a\npreference for domain-relevant classes. To mitigate this model bias, we propose\na training-free and label-free feature calibration method, Unsupervised\nMulti-domain Feature Calibration (UMFC). UMFC estimates image-level biases from\ndomain-specific features and text-level biases from the direction of domain\ntransition. These biases are subsequently subtracted from original image and\ntext features separately, to render them domain-invariant. We evaluate our\nmethod on multiple settings including transductive learning and test-time\nadaptation. Extensive experiments show that our method outperforms CLIP and\nperforms on par with the state-of-the-arts that need additional annotations or\noptimization. Our code is available at https://github.com/GIT-LJc/UMFC.\n", "link": "http://arxiv.org/abs/2411.06921v1", "date": "2024-11-11", "relevancy": 2.3419, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6199}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UMFC%3A%20Unsupervised%20Multi-Domain%20Feature%20Calibration%20for%20Vision-Language%0A%20%20Models&body=Title%3A%20UMFC%3A%20Unsupervised%20Multi-Domain%20Feature%20Calibration%20for%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Jiachen%20Liang%20and%20Ruibing%20Hou%20and%20Minyang%20Hu%20and%20Hong%20Chang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%20%28e.g.%2C%20CLIP%29%20have%20shown%20powerful%20zero-shot%0Atransfer%20capabilities.%20But%20they%20still%20struggle%20with%20domain%20shifts%20and%20typically%0Arequire%20labeled%20data%20to%20adapt%20to%20downstream%20tasks%2C%20which%20could%20be%20costly.%20In%0Athis%20work%2C%20we%20aim%20to%20leverage%20unlabeled%20data%20that%20naturally%20spans%20multiple%0Adomains%20to%20enhance%20the%20transferability%20of%20vision-language%20models.%20Under%20this%0Aunsupervised%20multi-domain%20setting%2C%20we%20have%20identified%20inherent%20model%20bias%0Awithin%20CLIP%2C%20notably%20in%20its%20visual%20and%20text%20encoders.%20Specifically%2C%20we%20observe%0Athat%20CLIP%27s%20visual%20encoder%20tends%20to%20prioritize%20encoding%20domain%20over%0Adiscriminative%20category%20information%2C%20meanwhile%20its%20text%20encoder%20exhibits%20a%0Apreference%20for%20domain-relevant%20classes.%20To%20mitigate%20this%20model%20bias%2C%20we%20propose%0Aa%20training-free%20and%20label-free%20feature%20calibration%20method%2C%20Unsupervised%0AMulti-domain%20Feature%20Calibration%20%28UMFC%29.%20UMFC%20estimates%20image-level%20biases%20from%0Adomain-specific%20features%20and%20text-level%20biases%20from%20the%20direction%20of%20domain%0Atransition.%20These%20biases%20are%20subsequently%20subtracted%20from%20original%20image%20and%0Atext%20features%20separately%2C%20to%20render%20them%20domain-invariant.%20We%20evaluate%20our%0Amethod%20on%20multiple%20settings%20including%20transductive%20learning%20and%20test-time%0Aadaptation.%20Extensive%20experiments%20show%20that%20our%20method%20outperforms%20CLIP%20and%0Aperforms%20on%20par%20with%20the%20state-of-the-arts%20that%20need%20additional%20annotations%20or%0Aoptimization.%20Our%20code%20is%20available%20at%20https%3A//github.com/GIT-LJc/UMFC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUMFC%253A%2520Unsupervised%2520Multi-Domain%2520Feature%2520Calibration%2520for%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DJiachen%2520Liang%2520and%2520Ruibing%2520Hou%2520and%2520Minyang%2520Hu%2520and%2520Hong%2520Chang%2520and%2520Shiguang%2520Shan%2520and%2520Xilin%2520Chen%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%2520%2528e.g.%252C%2520CLIP%2529%2520have%2520shown%2520powerful%2520zero-shot%250Atransfer%2520capabilities.%2520But%2520they%2520still%2520struggle%2520with%2520domain%2520shifts%2520and%2520typically%250Arequire%2520labeled%2520data%2520to%2520adapt%2520to%2520downstream%2520tasks%252C%2520which%2520could%2520be%2520costly.%2520In%250Athis%2520work%252C%2520we%2520aim%2520to%2520leverage%2520unlabeled%2520data%2520that%2520naturally%2520spans%2520multiple%250Adomains%2520to%2520enhance%2520the%2520transferability%2520of%2520vision-language%2520models.%2520Under%2520this%250Aunsupervised%2520multi-domain%2520setting%252C%2520we%2520have%2520identified%2520inherent%2520model%2520bias%250Awithin%2520CLIP%252C%2520notably%2520in%2520its%2520visual%2520and%2520text%2520encoders.%2520Specifically%252C%2520we%2520observe%250Athat%2520CLIP%2527s%2520visual%2520encoder%2520tends%2520to%2520prioritize%2520encoding%2520domain%2520over%250Adiscriminative%2520category%2520information%252C%2520meanwhile%2520its%2520text%2520encoder%2520exhibits%2520a%250Apreference%2520for%2520domain-relevant%2520classes.%2520To%2520mitigate%2520this%2520model%2520bias%252C%2520we%2520propose%250Aa%2520training-free%2520and%2520label-free%2520feature%2520calibration%2520method%252C%2520Unsupervised%250AMulti-domain%2520Feature%2520Calibration%2520%2528UMFC%2529.%2520UMFC%2520estimates%2520image-level%2520biases%2520from%250Adomain-specific%2520features%2520and%2520text-level%2520biases%2520from%2520the%2520direction%2520of%2520domain%250Atransition.%2520These%2520biases%2520are%2520subsequently%2520subtracted%2520from%2520original%2520image%2520and%250Atext%2520features%2520separately%252C%2520to%2520render%2520them%2520domain-invariant.%2520We%2520evaluate%2520our%250Amethod%2520on%2520multiple%2520settings%2520including%2520transductive%2520learning%2520and%2520test-time%250Aadaptation.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520outperforms%2520CLIP%2520and%250Aperforms%2520on%2520par%2520with%2520the%2520state-of-the-arts%2520that%2520need%2520additional%2520annotations%2520or%250Aoptimization.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/GIT-LJc/UMFC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UMFC%3A%20Unsupervised%20Multi-Domain%20Feature%20Calibration%20for%20Vision-Language%0A%20%20Models&entry.906535625=Jiachen%20Liang%20and%20Ruibing%20Hou%20and%20Minyang%20Hu%20and%20Hong%20Chang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen&entry.1292438233=%20%20Pre-trained%20vision-language%20models%20%28e.g.%2C%20CLIP%29%20have%20shown%20powerful%20zero-shot%0Atransfer%20capabilities.%20But%20they%20still%20struggle%20with%20domain%20shifts%20and%20typically%0Arequire%20labeled%20data%20to%20adapt%20to%20downstream%20tasks%2C%20which%20could%20be%20costly.%20In%0Athis%20work%2C%20we%20aim%20to%20leverage%20unlabeled%20data%20that%20naturally%20spans%20multiple%0Adomains%20to%20enhance%20the%20transferability%20of%20vision-language%20models.%20Under%20this%0Aunsupervised%20multi-domain%20setting%2C%20we%20have%20identified%20inherent%20model%20bias%0Awithin%20CLIP%2C%20notably%20in%20its%20visual%20and%20text%20encoders.%20Specifically%2C%20we%20observe%0Athat%20CLIP%27s%20visual%20encoder%20tends%20to%20prioritize%20encoding%20domain%20over%0Adiscriminative%20category%20information%2C%20meanwhile%20its%20text%20encoder%20exhibits%20a%0Apreference%20for%20domain-relevant%20classes.%20To%20mitigate%20this%20model%20bias%2C%20we%20propose%0Aa%20training-free%20and%20label-free%20feature%20calibration%20method%2C%20Unsupervised%0AMulti-domain%20Feature%20Calibration%20%28UMFC%29.%20UMFC%20estimates%20image-level%20biases%20from%0Adomain-specific%20features%20and%20text-level%20biases%20from%20the%20direction%20of%20domain%0Atransition.%20These%20biases%20are%20subsequently%20subtracted%20from%20original%20image%20and%0Atext%20features%20separately%2C%20to%20render%20them%20domain-invariant.%20We%20evaluate%20our%0Amethod%20on%20multiple%20settings%20including%20transductive%20learning%20and%20test-time%0Aadaptation.%20Extensive%20experiments%20show%20that%20our%20method%20outperforms%20CLIP%20and%0Aperforms%20on%20par%20with%20the%20state-of-the-arts%20that%20need%20additional%20annotations%20or%0Aoptimization.%20Our%20code%20is%20available%20at%20https%3A//github.com/GIT-LJc/UMFC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06921v1&entry.124074799=Read"},
{"title": "StoryAgent: Customized Storytelling Video Generation via Multi-Agent\n  Collaboration", "author": "Panwen Hu and Jin Jiang and Jianqi Chen and Mingfei Han and Shengcai Liao and Xiaojun Chang and Xiaodan Liang", "abstract": "  The advent of AI-Generated Content (AIGC) has spurred research into automated\nvideo generation to streamline conventional processes. However, automating\nstorytelling video production, particularly for customized narratives, remains\nchallenging due to the complexity of maintaining subject consistency across\nshots. While existing approaches like Mora and AesopAgent integrate multiple\nagents for Story-to-Video (S2V) generation, they fall short in preserving\nprotagonist consistency and supporting Customized Storytelling Video Generation\n(CSVG). To address these limitations, we propose StoryAgent, a multi-agent\nframework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasks\nassigned to specialized agents, mirroring the professional production process.\nNotably, our framework includes agents for story design, storyboard generation,\nvideo creation, agent coordination, and result evaluation. Leveraging the\nstrengths of different models, StoryAgent enhances control over the generation\nprocess, significantly improving character consistency. Specifically, we\nintroduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhance\nintra-shot temporal consistency, while a novel storyboard generation pipeline\nis proposed to maintain subject consistency across shots. Extensive experiments\ndemonstrate the effectiveness of our approach in synthesizing highly consistent\nstorytelling videos, outperforming state-of-the-art methods. Our contributions\ninclude the introduction of StoryAgent, a versatile framework for video\ngeneration tasks, and novel techniques for preserving protagonist consistency.\n", "link": "http://arxiv.org/abs/2411.04925v2", "date": "2024-11-11", "relevancy": 2.3295, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6045}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6012}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StoryAgent%3A%20Customized%20Storytelling%20Video%20Generation%20via%20Multi-Agent%0A%20%20Collaboration&body=Title%3A%20StoryAgent%3A%20Customized%20Storytelling%20Video%20Generation%20via%20Multi-Agent%0A%20%20Collaboration%0AAuthor%3A%20Panwen%20Hu%20and%20Jin%20Jiang%20and%20Jianqi%20Chen%20and%20Mingfei%20Han%20and%20Shengcai%20Liao%20and%20Xiaojun%20Chang%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20The%20advent%20of%20AI-Generated%20Content%20%28AIGC%29%20has%20spurred%20research%20into%20automated%0Avideo%20generation%20to%20streamline%20conventional%20processes.%20However%2C%20automating%0Astorytelling%20video%20production%2C%20particularly%20for%20customized%20narratives%2C%20remains%0Achallenging%20due%20to%20the%20complexity%20of%20maintaining%20subject%20consistency%20across%0Ashots.%20While%20existing%20approaches%20like%20Mora%20and%20AesopAgent%20integrate%20multiple%0Aagents%20for%20Story-to-Video%20%28S2V%29%20generation%2C%20they%20fall%20short%20in%20preserving%0Aprotagonist%20consistency%20and%20supporting%20Customized%20Storytelling%20Video%20Generation%0A%28CSVG%29.%20To%20address%20these%20limitations%2C%20we%20propose%20StoryAgent%2C%20a%20multi-agent%0Aframework%20designed%20for%20CSVG.%20StoryAgent%20decomposes%20CSVG%20into%20distinct%20subtasks%0Aassigned%20to%20specialized%20agents%2C%20mirroring%20the%20professional%20production%20process.%0ANotably%2C%20our%20framework%20includes%20agents%20for%20story%20design%2C%20storyboard%20generation%2C%0Avideo%20creation%2C%20agent%20coordination%2C%20and%20result%20evaluation.%20Leveraging%20the%0Astrengths%20of%20different%20models%2C%20StoryAgent%20enhances%20control%20over%20the%20generation%0Aprocess%2C%20significantly%20improving%20character%20consistency.%20Specifically%2C%20we%0Aintroduce%20a%20customized%20Image-to-Video%20%28I2V%29%20method%2C%20LoRA-BE%2C%20to%20enhance%0Aintra-shot%20temporal%20consistency%2C%20while%20a%20novel%20storyboard%20generation%20pipeline%0Ais%20proposed%20to%20maintain%20subject%20consistency%20across%20shots.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20in%20synthesizing%20highly%20consistent%0Astorytelling%20videos%2C%20outperforming%20state-of-the-art%20methods.%20Our%20contributions%0Ainclude%20the%20introduction%20of%20StoryAgent%2C%20a%20versatile%20framework%20for%20video%0Ageneration%20tasks%2C%20and%20novel%20techniques%20for%20preserving%20protagonist%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStoryAgent%253A%2520Customized%2520Storytelling%2520Video%2520Generation%2520via%2520Multi-Agent%250A%2520%2520Collaboration%26entry.906535625%3DPanwen%2520Hu%2520and%2520Jin%2520Jiang%2520and%2520Jianqi%2520Chen%2520and%2520Mingfei%2520Han%2520and%2520Shengcai%2520Liao%2520and%2520Xiaojun%2520Chang%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520AI-Generated%2520Content%2520%2528AIGC%2529%2520has%2520spurred%2520research%2520into%2520automated%250Avideo%2520generation%2520to%2520streamline%2520conventional%2520processes.%2520However%252C%2520automating%250Astorytelling%2520video%2520production%252C%2520particularly%2520for%2520customized%2520narratives%252C%2520remains%250Achallenging%2520due%2520to%2520the%2520complexity%2520of%2520maintaining%2520subject%2520consistency%2520across%250Ashots.%2520While%2520existing%2520approaches%2520like%2520Mora%2520and%2520AesopAgent%2520integrate%2520multiple%250Aagents%2520for%2520Story-to-Video%2520%2528S2V%2529%2520generation%252C%2520they%2520fall%2520short%2520in%2520preserving%250Aprotagonist%2520consistency%2520and%2520supporting%2520Customized%2520Storytelling%2520Video%2520Generation%250A%2528CSVG%2529.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520StoryAgent%252C%2520a%2520multi-agent%250Aframework%2520designed%2520for%2520CSVG.%2520StoryAgent%2520decomposes%2520CSVG%2520into%2520distinct%2520subtasks%250Aassigned%2520to%2520specialized%2520agents%252C%2520mirroring%2520the%2520professional%2520production%2520process.%250ANotably%252C%2520our%2520framework%2520includes%2520agents%2520for%2520story%2520design%252C%2520storyboard%2520generation%252C%250Avideo%2520creation%252C%2520agent%2520coordination%252C%2520and%2520result%2520evaluation.%2520Leveraging%2520the%250Astrengths%2520of%2520different%2520models%252C%2520StoryAgent%2520enhances%2520control%2520over%2520the%2520generation%250Aprocess%252C%2520significantly%2520improving%2520character%2520consistency.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520customized%2520Image-to-Video%2520%2528I2V%2529%2520method%252C%2520LoRA-BE%252C%2520to%2520enhance%250Aintra-shot%2520temporal%2520consistency%252C%2520while%2520a%2520novel%2520storyboard%2520generation%2520pipeline%250Ais%2520proposed%2520to%2520maintain%2520subject%2520consistency%2520across%2520shots.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520synthesizing%2520highly%2520consistent%250Astorytelling%2520videos%252C%2520outperforming%2520state-of-the-art%2520methods.%2520Our%2520contributions%250Ainclude%2520the%2520introduction%2520of%2520StoryAgent%252C%2520a%2520versatile%2520framework%2520for%2520video%250Ageneration%2520tasks%252C%2520and%2520novel%2520techniques%2520for%2520preserving%2520protagonist%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StoryAgent%3A%20Customized%20Storytelling%20Video%20Generation%20via%20Multi-Agent%0A%20%20Collaboration&entry.906535625=Panwen%20Hu%20and%20Jin%20Jiang%20and%20Jianqi%20Chen%20and%20Mingfei%20Han%20and%20Shengcai%20Liao%20and%20Xiaojun%20Chang%20and%20Xiaodan%20Liang&entry.1292438233=%20%20The%20advent%20of%20AI-Generated%20Content%20%28AIGC%29%20has%20spurred%20research%20into%20automated%0Avideo%20generation%20to%20streamline%20conventional%20processes.%20However%2C%20automating%0Astorytelling%20video%20production%2C%20particularly%20for%20customized%20narratives%2C%20remains%0Achallenging%20due%20to%20the%20complexity%20of%20maintaining%20subject%20consistency%20across%0Ashots.%20While%20existing%20approaches%20like%20Mora%20and%20AesopAgent%20integrate%20multiple%0Aagents%20for%20Story-to-Video%20%28S2V%29%20generation%2C%20they%20fall%20short%20in%20preserving%0Aprotagonist%20consistency%20and%20supporting%20Customized%20Storytelling%20Video%20Generation%0A%28CSVG%29.%20To%20address%20these%20limitations%2C%20we%20propose%20StoryAgent%2C%20a%20multi-agent%0Aframework%20designed%20for%20CSVG.%20StoryAgent%20decomposes%20CSVG%20into%20distinct%20subtasks%0Aassigned%20to%20specialized%20agents%2C%20mirroring%20the%20professional%20production%20process.%0ANotably%2C%20our%20framework%20includes%20agents%20for%20story%20design%2C%20storyboard%20generation%2C%0Avideo%20creation%2C%20agent%20coordination%2C%20and%20result%20evaluation.%20Leveraging%20the%0Astrengths%20of%20different%20models%2C%20StoryAgent%20enhances%20control%20over%20the%20generation%0Aprocess%2C%20significantly%20improving%20character%20consistency.%20Specifically%2C%20we%0Aintroduce%20a%20customized%20Image-to-Video%20%28I2V%29%20method%2C%20LoRA-BE%2C%20to%20enhance%0Aintra-shot%20temporal%20consistency%2C%20while%20a%20novel%20storyboard%20generation%20pipeline%0Ais%20proposed%20to%20maintain%20subject%20consistency%20across%20shots.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20in%20synthesizing%20highly%20consistent%0Astorytelling%20videos%2C%20outperforming%20state-of-the-art%20methods.%20Our%20contributions%0Ainclude%20the%20introduction%20of%20StoryAgent%2C%20a%20versatile%20framework%20for%20video%0Ageneration%20tasks%2C%20and%20novel%20techniques%20for%20preserving%20protagonist%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04925v2&entry.124074799=Read"},
{"title": "LMLPA: Language Model Linguistic Personality Assessment", "author": "Jingyao Zheng and Xian Wang and Simo Hosio and Xiaoxian Xu and Lik-Hang Lee", "abstract": "  Large Language Models (LLMs) are increasingly used in everyday life and\nresearch. One of the most common use cases is conversational interactions,\nenabled by the language generation capabilities of LLMs. Just as between two\nhumans, a conversation between an LLM-powered entity and a human depends on the\npersonality of the conversants. However, measuring the personality of a given\nLLM is currently a challenge. This paper introduces the Language Model\nLinguistic Personality Assessment (LMLPA), a system designed to evaluate the\nlinguistic personalities of LLMs. Our system helps to understand LLMs' language\ngeneration capabilities by quantitatively assessing the distinct personality\ntraits reflected in their linguistic outputs. Unlike traditional human-centric\npsychometrics, the LMLPA adapts a personality assessment questionnaire,\nspecifically the Big Five Inventory, to align with the operational capabilities\nof LLMs, and also incorporates the findings from previous language-based\npersonality measurement literature. To mitigate sensitivity to the order of\noptions, our questionnaire is designed to be open-ended, resulting in textual\nanswers. Thus, the AI rater is needed to transform ambiguous personality\ninformation from text responses into clear numerical indicators of personality\ntraits. Utilising Principal Component Analysis and reliability validations, our\nfindings demonstrate that LLMs possess distinct personality traits that can be\neffectively quantified by the LMLPA. This research contributes to\nHuman-Computer Interaction and Human-Centered AI, providing a robust framework\nfor future studies to refine AI personality assessments and expand their\napplications in multiple areas, including education and manufacturing.\n", "link": "http://arxiv.org/abs/2410.17632v2", "date": "2024-11-11", "relevancy": 2.3195, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4678}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMLPA%3A%20Language%20Model%20Linguistic%20Personality%20Assessment&body=Title%3A%20LMLPA%3A%20Language%20Model%20Linguistic%20Personality%20Assessment%0AAuthor%3A%20Jingyao%20Zheng%20and%20Xian%20Wang%20and%20Simo%20Hosio%20and%20Xiaoxian%20Xu%20and%20Lik-Hang%20Lee%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20in%20everyday%20life%20and%0Aresearch.%20One%20of%20the%20most%20common%20use%20cases%20is%20conversational%20interactions%2C%0Aenabled%20by%20the%20language%20generation%20capabilities%20of%20LLMs.%20Just%20as%20between%20two%0Ahumans%2C%20a%20conversation%20between%20an%20LLM-powered%20entity%20and%20a%20human%20depends%20on%20the%0Apersonality%20of%20the%20conversants.%20However%2C%20measuring%20the%20personality%20of%20a%20given%0ALLM%20is%20currently%20a%20challenge.%20This%20paper%20introduces%20the%20Language%20Model%0ALinguistic%20Personality%20Assessment%20%28LMLPA%29%2C%20a%20system%20designed%20to%20evaluate%20the%0Alinguistic%20personalities%20of%20LLMs.%20Our%20system%20helps%20to%20understand%20LLMs%27%20language%0Ageneration%20capabilities%20by%20quantitatively%20assessing%20the%20distinct%20personality%0Atraits%20reflected%20in%20their%20linguistic%20outputs.%20Unlike%20traditional%20human-centric%0Apsychometrics%2C%20the%20LMLPA%20adapts%20a%20personality%20assessment%20questionnaire%2C%0Aspecifically%20the%20Big%20Five%20Inventory%2C%20to%20align%20with%20the%20operational%20capabilities%0Aof%20LLMs%2C%20and%20also%20incorporates%20the%20findings%20from%20previous%20language-based%0Apersonality%20measurement%20literature.%20To%20mitigate%20sensitivity%20to%20the%20order%20of%0Aoptions%2C%20our%20questionnaire%20is%20designed%20to%20be%20open-ended%2C%20resulting%20in%20textual%0Aanswers.%20Thus%2C%20the%20AI%20rater%20is%20needed%20to%20transform%20ambiguous%20personality%0Ainformation%20from%20text%20responses%20into%20clear%20numerical%20indicators%20of%20personality%0Atraits.%20Utilising%20Principal%20Component%20Analysis%20and%20reliability%20validations%2C%20our%0Afindings%20demonstrate%20that%20LLMs%20possess%20distinct%20personality%20traits%20that%20can%20be%0Aeffectively%20quantified%20by%20the%20LMLPA.%20This%20research%20contributes%20to%0AHuman-Computer%20Interaction%20and%20Human-Centered%20AI%2C%20providing%20a%20robust%20framework%0Afor%20future%20studies%20to%20refine%20AI%20personality%20assessments%20and%20expand%20their%0Aapplications%20in%20multiple%20areas%2C%20including%20education%20and%20manufacturing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17632v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMLPA%253A%2520Language%2520Model%2520Linguistic%2520Personality%2520Assessment%26entry.906535625%3DJingyao%2520Zheng%2520and%2520Xian%2520Wang%2520and%2520Simo%2520Hosio%2520and%2520Xiaoxian%2520Xu%2520and%2520Lik-Hang%2520Lee%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520in%2520everyday%2520life%2520and%250Aresearch.%2520One%2520of%2520the%2520most%2520common%2520use%2520cases%2520is%2520conversational%2520interactions%252C%250Aenabled%2520by%2520the%2520language%2520generation%2520capabilities%2520of%2520LLMs.%2520Just%2520as%2520between%2520two%250Ahumans%252C%2520a%2520conversation%2520between%2520an%2520LLM-powered%2520entity%2520and%2520a%2520human%2520depends%2520on%2520the%250Apersonality%2520of%2520the%2520conversants.%2520However%252C%2520measuring%2520the%2520personality%2520of%2520a%2520given%250ALLM%2520is%2520currently%2520a%2520challenge.%2520This%2520paper%2520introduces%2520the%2520Language%2520Model%250ALinguistic%2520Personality%2520Assessment%2520%2528LMLPA%2529%252C%2520a%2520system%2520designed%2520to%2520evaluate%2520the%250Alinguistic%2520personalities%2520of%2520LLMs.%2520Our%2520system%2520helps%2520to%2520understand%2520LLMs%2527%2520language%250Ageneration%2520capabilities%2520by%2520quantitatively%2520assessing%2520the%2520distinct%2520personality%250Atraits%2520reflected%2520in%2520their%2520linguistic%2520outputs.%2520Unlike%2520traditional%2520human-centric%250Apsychometrics%252C%2520the%2520LMLPA%2520adapts%2520a%2520personality%2520assessment%2520questionnaire%252C%250Aspecifically%2520the%2520Big%2520Five%2520Inventory%252C%2520to%2520align%2520with%2520the%2520operational%2520capabilities%250Aof%2520LLMs%252C%2520and%2520also%2520incorporates%2520the%2520findings%2520from%2520previous%2520language-based%250Apersonality%2520measurement%2520literature.%2520To%2520mitigate%2520sensitivity%2520to%2520the%2520order%2520of%250Aoptions%252C%2520our%2520questionnaire%2520is%2520designed%2520to%2520be%2520open-ended%252C%2520resulting%2520in%2520textual%250Aanswers.%2520Thus%252C%2520the%2520AI%2520rater%2520is%2520needed%2520to%2520transform%2520ambiguous%2520personality%250Ainformation%2520from%2520text%2520responses%2520into%2520clear%2520numerical%2520indicators%2520of%2520personality%250Atraits.%2520Utilising%2520Principal%2520Component%2520Analysis%2520and%2520reliability%2520validations%252C%2520our%250Afindings%2520demonstrate%2520that%2520LLMs%2520possess%2520distinct%2520personality%2520traits%2520that%2520can%2520be%250Aeffectively%2520quantified%2520by%2520the%2520LMLPA.%2520This%2520research%2520contributes%2520to%250AHuman-Computer%2520Interaction%2520and%2520Human-Centered%2520AI%252C%2520providing%2520a%2520robust%2520framework%250Afor%2520future%2520studies%2520to%2520refine%2520AI%2520personality%2520assessments%2520and%2520expand%2520their%250Aapplications%2520in%2520multiple%2520areas%252C%2520including%2520education%2520and%2520manufacturing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17632v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMLPA%3A%20Language%20Model%20Linguistic%20Personality%20Assessment&entry.906535625=Jingyao%20Zheng%20and%20Xian%20Wang%20and%20Simo%20Hosio%20and%20Xiaoxian%20Xu%20and%20Lik-Hang%20Lee&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20in%20everyday%20life%20and%0Aresearch.%20One%20of%20the%20most%20common%20use%20cases%20is%20conversational%20interactions%2C%0Aenabled%20by%20the%20language%20generation%20capabilities%20of%20LLMs.%20Just%20as%20between%20two%0Ahumans%2C%20a%20conversation%20between%20an%20LLM-powered%20entity%20and%20a%20human%20depends%20on%20the%0Apersonality%20of%20the%20conversants.%20However%2C%20measuring%20the%20personality%20of%20a%20given%0ALLM%20is%20currently%20a%20challenge.%20This%20paper%20introduces%20the%20Language%20Model%0ALinguistic%20Personality%20Assessment%20%28LMLPA%29%2C%20a%20system%20designed%20to%20evaluate%20the%0Alinguistic%20personalities%20of%20LLMs.%20Our%20system%20helps%20to%20understand%20LLMs%27%20language%0Ageneration%20capabilities%20by%20quantitatively%20assessing%20the%20distinct%20personality%0Atraits%20reflected%20in%20their%20linguistic%20outputs.%20Unlike%20traditional%20human-centric%0Apsychometrics%2C%20the%20LMLPA%20adapts%20a%20personality%20assessment%20questionnaire%2C%0Aspecifically%20the%20Big%20Five%20Inventory%2C%20to%20align%20with%20the%20operational%20capabilities%0Aof%20LLMs%2C%20and%20also%20incorporates%20the%20findings%20from%20previous%20language-based%0Apersonality%20measurement%20literature.%20To%20mitigate%20sensitivity%20to%20the%20order%20of%0Aoptions%2C%20our%20questionnaire%20is%20designed%20to%20be%20open-ended%2C%20resulting%20in%20textual%0Aanswers.%20Thus%2C%20the%20AI%20rater%20is%20needed%20to%20transform%20ambiguous%20personality%0Ainformation%20from%20text%20responses%20into%20clear%20numerical%20indicators%20of%20personality%0Atraits.%20Utilising%20Principal%20Component%20Analysis%20and%20reliability%20validations%2C%20our%0Afindings%20demonstrate%20that%20LLMs%20possess%20distinct%20personality%20traits%20that%20can%20be%0Aeffectively%20quantified%20by%20the%20LMLPA.%20This%20research%20contributes%20to%0AHuman-Computer%20Interaction%20and%20Human-Centered%20AI%2C%20providing%20a%20robust%20framework%0Afor%20future%20studies%20to%20refine%20AI%20personality%20assessments%20and%20expand%20their%0Aapplications%20in%20multiple%20areas%2C%20including%20education%20and%20manufacturing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17632v2&entry.124074799=Read"},
{"title": "Multi-Modal interpretable automatic video captioning", "author": "Antoine Hanna-Asaad and Decky Aspandi and Titus Zaharia", "abstract": "  Video captioning aims to describe video contents using natural language\nformat that involves understanding and interpreting scenes, actions and events\nthat occurs simultaneously on the view. Current approaches have mainly\nconcentrated on visual cues, often neglecting the rich information available\nfrom other important modality of audio information, including their\ninter-dependencies. In this work, we introduce a novel video captioning method\ntrained with multi-modal contrastive loss that emphasizes both multi-modal\nintegration and interpretability. Our approach is designed to capture the\ndependency between these modalities, resulting in more accurate, thus pertinent\ncaptions. Furthermore, we highlight the importance of interpretability,\nemploying multiple attention mechanisms that provide explanation into the\nmodel's decision-making process. Our experimental results demonstrate that our\nproposed method performs favorably against the state-of the-art models on\ncommonly used benchmark datasets of MSR-VTT and VATEX.\n", "link": "http://arxiv.org/abs/2411.06872v1", "date": "2024-11-11", "relevancy": 2.3039, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.59}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5666}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20interpretable%20automatic%20video%20captioning&body=Title%3A%20Multi-Modal%20interpretable%20automatic%20video%20captioning%0AAuthor%3A%20Antoine%20Hanna-Asaad%20and%20Decky%20Aspandi%20and%20Titus%20Zaharia%0AAbstract%3A%20%20%20Video%20captioning%20aims%20to%20describe%20video%20contents%20using%20natural%20language%0Aformat%20that%20involves%20understanding%20and%20interpreting%20scenes%2C%20actions%20and%20events%0Athat%20occurs%20simultaneously%20on%20the%20view.%20Current%20approaches%20have%20mainly%0Aconcentrated%20on%20visual%20cues%2C%20often%20neglecting%20the%20rich%20information%20available%0Afrom%20other%20important%20modality%20of%20audio%20information%2C%20including%20their%0Ainter-dependencies.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20video%20captioning%20method%0Atrained%20with%20multi-modal%20contrastive%20loss%20that%20emphasizes%20both%20multi-modal%0Aintegration%20and%20interpretability.%20Our%20approach%20is%20designed%20to%20capture%20the%0Adependency%20between%20these%20modalities%2C%20resulting%20in%20more%20accurate%2C%20thus%20pertinent%0Acaptions.%20Furthermore%2C%20we%20highlight%20the%20importance%20of%20interpretability%2C%0Aemploying%20multiple%20attention%20mechanisms%20that%20provide%20explanation%20into%20the%0Amodel%27s%20decision-making%20process.%20Our%20experimental%20results%20demonstrate%20that%20our%0Aproposed%20method%20performs%20favorably%20against%20the%20state-of%20the-art%20models%20on%0Acommonly%20used%20benchmark%20datasets%20of%20MSR-VTT%20and%20VATEX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520interpretable%2520automatic%2520video%2520captioning%26entry.906535625%3DAntoine%2520Hanna-Asaad%2520and%2520Decky%2520Aspandi%2520and%2520Titus%2520Zaharia%26entry.1292438233%3D%2520%2520Video%2520captioning%2520aims%2520to%2520describe%2520video%2520contents%2520using%2520natural%2520language%250Aformat%2520that%2520involves%2520understanding%2520and%2520interpreting%2520scenes%252C%2520actions%2520and%2520events%250Athat%2520occurs%2520simultaneously%2520on%2520the%2520view.%2520Current%2520approaches%2520have%2520mainly%250Aconcentrated%2520on%2520visual%2520cues%252C%2520often%2520neglecting%2520the%2520rich%2520information%2520available%250Afrom%2520other%2520important%2520modality%2520of%2520audio%2520information%252C%2520including%2520their%250Ainter-dependencies.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520video%2520captioning%2520method%250Atrained%2520with%2520multi-modal%2520contrastive%2520loss%2520that%2520emphasizes%2520both%2520multi-modal%250Aintegration%2520and%2520interpretability.%2520Our%2520approach%2520is%2520designed%2520to%2520capture%2520the%250Adependency%2520between%2520these%2520modalities%252C%2520resulting%2520in%2520more%2520accurate%252C%2520thus%2520pertinent%250Acaptions.%2520Furthermore%252C%2520we%2520highlight%2520the%2520importance%2520of%2520interpretability%252C%250Aemploying%2520multiple%2520attention%2520mechanisms%2520that%2520provide%2520explanation%2520into%2520the%250Amodel%2527s%2520decision-making%2520process.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520our%250Aproposed%2520method%2520performs%2520favorably%2520against%2520the%2520state-of%2520the-art%2520models%2520on%250Acommonly%2520used%2520benchmark%2520datasets%2520of%2520MSR-VTT%2520and%2520VATEX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20interpretable%20automatic%20video%20captioning&entry.906535625=Antoine%20Hanna-Asaad%20and%20Decky%20Aspandi%20and%20Titus%20Zaharia&entry.1292438233=%20%20Video%20captioning%20aims%20to%20describe%20video%20contents%20using%20natural%20language%0Aformat%20that%20involves%20understanding%20and%20interpreting%20scenes%2C%20actions%20and%20events%0Athat%20occurs%20simultaneously%20on%20the%20view.%20Current%20approaches%20have%20mainly%0Aconcentrated%20on%20visual%20cues%2C%20often%20neglecting%20the%20rich%20information%20available%0Afrom%20other%20important%20modality%20of%20audio%20information%2C%20including%20their%0Ainter-dependencies.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20video%20captioning%20method%0Atrained%20with%20multi-modal%20contrastive%20loss%20that%20emphasizes%20both%20multi-modal%0Aintegration%20and%20interpretability.%20Our%20approach%20is%20designed%20to%20capture%20the%0Adependency%20between%20these%20modalities%2C%20resulting%20in%20more%20accurate%2C%20thus%20pertinent%0Acaptions.%20Furthermore%2C%20we%20highlight%20the%20importance%20of%20interpretability%2C%0Aemploying%20multiple%20attention%20mechanisms%20that%20provide%20explanation%20into%20the%0Amodel%27s%20decision-making%20process.%20Our%20experimental%20results%20demonstrate%20that%20our%0Aproposed%20method%20performs%20favorably%20against%20the%20state-of%20the-art%20models%20on%0Acommonly%20used%20benchmark%20datasets%20of%20MSR-VTT%20and%20VATEX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06872v1&entry.124074799=Read"},
{"title": "Enhanced Textual Feature Extraction for Visual Question Answering: A\n  Simple Convolutional Approach", "author": "Zhilin Zhang", "abstract": "  Visual Question Answering (VQA) has emerged as a highly engaging field in\nrecent years, with increasing research focused on enhancing VQA accuracy\nthrough advanced models such as Transformers. Despite this growing interest,\nlimited work has examined the comparative effectiveness of textual encoders in\nVQA, particularly considering model complexity and computational efficiency. In\nthis work, we conduct a comprehensive comparison between complex textual models\nthat leverage long-range dependencies and simpler models focusing on local\ntextual features within a well-established VQA framework. Our findings reveal\nthat employing complex textual encoders is not invariably the optimal approach\nfor the VQA-v2 dataset. Motivated by this insight, we propose ConvGRU, a model\nthat incorporates convolutional layers to improve text feature representation\nwithout substantially increasing model complexity. Tested on the VQA-v2\ndataset, ConvGRU demonstrates a modest yet consistent improvement over\nbaselines for question types such as Number and Count, which highlights the\npotential of lightweight architectures for VQA tasks, especially when\ncomputational resources are limited.\n", "link": "http://arxiv.org/abs/2405.00479v2", "date": "2024-11-11", "relevancy": 2.275, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.577}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.577}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Textual%20Feature%20Extraction%20for%20Visual%20Question%20Answering%3A%20A%0A%20%20Simple%20Convolutional%20Approach&body=Title%3A%20Enhanced%20Textual%20Feature%20Extraction%20for%20Visual%20Question%20Answering%3A%20A%0A%20%20Simple%20Convolutional%20Approach%0AAuthor%3A%20Zhilin%20Zhang%0AAbstract%3A%20%20%20Visual%20Question%20Answering%20%28VQA%29%20has%20emerged%20as%20a%20highly%20engaging%20field%20in%0Arecent%20years%2C%20with%20increasing%20research%20focused%20on%20enhancing%20VQA%20accuracy%0Athrough%20advanced%20models%20such%20as%20Transformers.%20Despite%20this%20growing%20interest%2C%0Alimited%20work%20has%20examined%20the%20comparative%20effectiveness%20of%20textual%20encoders%20in%0AVQA%2C%20particularly%20considering%20model%20complexity%20and%20computational%20efficiency.%20In%0Athis%20work%2C%20we%20conduct%20a%20comprehensive%20comparison%20between%20complex%20textual%20models%0Athat%20leverage%20long-range%20dependencies%20and%20simpler%20models%20focusing%20on%20local%0Atextual%20features%20within%20a%20well-established%20VQA%20framework.%20Our%20findings%20reveal%0Athat%20employing%20complex%20textual%20encoders%20is%20not%20invariably%20the%20optimal%20approach%0Afor%20the%20VQA-v2%20dataset.%20Motivated%20by%20this%20insight%2C%20we%20propose%20ConvGRU%2C%20a%20model%0Athat%20incorporates%20convolutional%20layers%20to%20improve%20text%20feature%20representation%0Awithout%20substantially%20increasing%20model%20complexity.%20Tested%20on%20the%20VQA-v2%0Adataset%2C%20ConvGRU%20demonstrates%20a%20modest%20yet%20consistent%20improvement%20over%0Abaselines%20for%20question%20types%20such%20as%20Number%20and%20Count%2C%20which%20highlights%20the%0Apotential%20of%20lightweight%20architectures%20for%20VQA%20tasks%2C%20especially%20when%0Acomputational%20resources%20are%20limited.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00479v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Textual%2520Feature%2520Extraction%2520for%2520Visual%2520Question%2520Answering%253A%2520A%250A%2520%2520Simple%2520Convolutional%2520Approach%26entry.906535625%3DZhilin%2520Zhang%26entry.1292438233%3D%2520%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520has%2520emerged%2520as%2520a%2520highly%2520engaging%2520field%2520in%250Arecent%2520years%252C%2520with%2520increasing%2520research%2520focused%2520on%2520enhancing%2520VQA%2520accuracy%250Athrough%2520advanced%2520models%2520such%2520as%2520Transformers.%2520Despite%2520this%2520growing%2520interest%252C%250Alimited%2520work%2520has%2520examined%2520the%2520comparative%2520effectiveness%2520of%2520textual%2520encoders%2520in%250AVQA%252C%2520particularly%2520considering%2520model%2520complexity%2520and%2520computational%2520efficiency.%2520In%250Athis%2520work%252C%2520we%2520conduct%2520a%2520comprehensive%2520comparison%2520between%2520complex%2520textual%2520models%250Athat%2520leverage%2520long-range%2520dependencies%2520and%2520simpler%2520models%2520focusing%2520on%2520local%250Atextual%2520features%2520within%2520a%2520well-established%2520VQA%2520framework.%2520Our%2520findings%2520reveal%250Athat%2520employing%2520complex%2520textual%2520encoders%2520is%2520not%2520invariably%2520the%2520optimal%2520approach%250Afor%2520the%2520VQA-v2%2520dataset.%2520Motivated%2520by%2520this%2520insight%252C%2520we%2520propose%2520ConvGRU%252C%2520a%2520model%250Athat%2520incorporates%2520convolutional%2520layers%2520to%2520improve%2520text%2520feature%2520representation%250Awithout%2520substantially%2520increasing%2520model%2520complexity.%2520Tested%2520on%2520the%2520VQA-v2%250Adataset%252C%2520ConvGRU%2520demonstrates%2520a%2520modest%2520yet%2520consistent%2520improvement%2520over%250Abaselines%2520for%2520question%2520types%2520such%2520as%2520Number%2520and%2520Count%252C%2520which%2520highlights%2520the%250Apotential%2520of%2520lightweight%2520architectures%2520for%2520VQA%2520tasks%252C%2520especially%2520when%250Acomputational%2520resources%2520are%2520limited.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00479v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Textual%20Feature%20Extraction%20for%20Visual%20Question%20Answering%3A%20A%0A%20%20Simple%20Convolutional%20Approach&entry.906535625=Zhilin%20Zhang&entry.1292438233=%20%20Visual%20Question%20Answering%20%28VQA%29%20has%20emerged%20as%20a%20highly%20engaging%20field%20in%0Arecent%20years%2C%20with%20increasing%20research%20focused%20on%20enhancing%20VQA%20accuracy%0Athrough%20advanced%20models%20such%20as%20Transformers.%20Despite%20this%20growing%20interest%2C%0Alimited%20work%20has%20examined%20the%20comparative%20effectiveness%20of%20textual%20encoders%20in%0AVQA%2C%20particularly%20considering%20model%20complexity%20and%20computational%20efficiency.%20In%0Athis%20work%2C%20we%20conduct%20a%20comprehensive%20comparison%20between%20complex%20textual%20models%0Athat%20leverage%20long-range%20dependencies%20and%20simpler%20models%20focusing%20on%20local%0Atextual%20features%20within%20a%20well-established%20VQA%20framework.%20Our%20findings%20reveal%0Athat%20employing%20complex%20textual%20encoders%20is%20not%20invariably%20the%20optimal%20approach%0Afor%20the%20VQA-v2%20dataset.%20Motivated%20by%20this%20insight%2C%20we%20propose%20ConvGRU%2C%20a%20model%0Athat%20incorporates%20convolutional%20layers%20to%20improve%20text%20feature%20representation%0Awithout%20substantially%20increasing%20model%20complexity.%20Tested%20on%20the%20VQA-v2%0Adataset%2C%20ConvGRU%20demonstrates%20a%20modest%20yet%20consistent%20improvement%20over%0Abaselines%20for%20question%20types%20such%20as%20Number%20and%20Count%2C%20which%20highlights%20the%0Apotential%20of%20lightweight%20architectures%20for%20VQA%20tasks%2C%20especially%20when%0Acomputational%20resources%20are%20limited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00479v2&entry.124074799=Read"},
{"title": "Scaling Mesh Generation via Compressive Tokenization", "author": "Haohan Weng and Zibo Zhao and Biwen Lei and Xianghui Yang and Jian Liu and Zeqiang Lai and Zhuo Chen and Yuhong Liu and Jie Jiang and Chunchao Guo and Tong Zhang and Shenghua Gao and C. L. Philip Chen", "abstract": "  We propose a compressive yet effective mesh representation, Blocked and\nPatchified Tokenization (BPT), facilitating the generation of meshes exceeding\n8k faces. BPT compresses mesh sequences by employing block-wise indexing and\npatch aggregation, reducing their length by approximately 75\\% compared to the\noriginal sequences. This compression milestone unlocks the potential to utilize\nmesh data with significantly more faces, thereby enhancing detail richness and\nimproving generation robustness. Empowered with the BPT, we have built a\nfoundation mesh generative model training on scaled mesh data to support\nflexible control for point clouds and images. Our model demonstrates the\ncapability to generate meshes with intricate details and accurate topology,\nachieving SoTA performance on mesh generation and reaching the level for direct\nproduct usage.\n", "link": "http://arxiv.org/abs/2411.07025v1", "date": "2024-11-11", "relevancy": 2.2549, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5877}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5618}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Mesh%20Generation%20via%20Compressive%20Tokenization&body=Title%3A%20Scaling%20Mesh%20Generation%20via%20Compressive%20Tokenization%0AAuthor%3A%20Haohan%20Weng%20and%20Zibo%20Zhao%20and%20Biwen%20Lei%20and%20Xianghui%20Yang%20and%20Jian%20Liu%20and%20Zeqiang%20Lai%20and%20Zhuo%20Chen%20and%20Yuhong%20Liu%20and%20Jie%20Jiang%20and%20Chunchao%20Guo%20and%20Tong%20Zhang%20and%20Shenghua%20Gao%20and%20C.%20L.%20Philip%20Chen%0AAbstract%3A%20%20%20We%20propose%20a%20compressive%20yet%20effective%20mesh%20representation%2C%20Blocked%20and%0APatchified%20Tokenization%20%28BPT%29%2C%20facilitating%20the%20generation%20of%20meshes%20exceeding%0A8k%20faces.%20BPT%20compresses%20mesh%20sequences%20by%20employing%20block-wise%20indexing%20and%0Apatch%20aggregation%2C%20reducing%20their%20length%20by%20approximately%2075%5C%25%20compared%20to%20the%0Aoriginal%20sequences.%20This%20compression%20milestone%20unlocks%20the%20potential%20to%20utilize%0Amesh%20data%20with%20significantly%20more%20faces%2C%20thereby%20enhancing%20detail%20richness%20and%0Aimproving%20generation%20robustness.%20Empowered%20with%20the%20BPT%2C%20we%20have%20built%20a%0Afoundation%20mesh%20generative%20model%20training%20on%20scaled%20mesh%20data%20to%20support%0Aflexible%20control%20for%20point%20clouds%20and%20images.%20Our%20model%20demonstrates%20the%0Acapability%20to%20generate%20meshes%20with%20intricate%20details%20and%20accurate%20topology%2C%0Aachieving%20SoTA%20performance%20on%20mesh%20generation%20and%20reaching%20the%20level%20for%20direct%0Aproduct%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Mesh%2520Generation%2520via%2520Compressive%2520Tokenization%26entry.906535625%3DHaohan%2520Weng%2520and%2520Zibo%2520Zhao%2520and%2520Biwen%2520Lei%2520and%2520Xianghui%2520Yang%2520and%2520Jian%2520Liu%2520and%2520Zeqiang%2520Lai%2520and%2520Zhuo%2520Chen%2520and%2520Yuhong%2520Liu%2520and%2520Jie%2520Jiang%2520and%2520Chunchao%2520Guo%2520and%2520Tong%2520Zhang%2520and%2520Shenghua%2520Gao%2520and%2520C.%2520L.%2520Philip%2520Chen%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520compressive%2520yet%2520effective%2520mesh%2520representation%252C%2520Blocked%2520and%250APatchified%2520Tokenization%2520%2528BPT%2529%252C%2520facilitating%2520the%2520generation%2520of%2520meshes%2520exceeding%250A8k%2520faces.%2520BPT%2520compresses%2520mesh%2520sequences%2520by%2520employing%2520block-wise%2520indexing%2520and%250Apatch%2520aggregation%252C%2520reducing%2520their%2520length%2520by%2520approximately%252075%255C%2525%2520compared%2520to%2520the%250Aoriginal%2520sequences.%2520This%2520compression%2520milestone%2520unlocks%2520the%2520potential%2520to%2520utilize%250Amesh%2520data%2520with%2520significantly%2520more%2520faces%252C%2520thereby%2520enhancing%2520detail%2520richness%2520and%250Aimproving%2520generation%2520robustness.%2520Empowered%2520with%2520the%2520BPT%252C%2520we%2520have%2520built%2520a%250Afoundation%2520mesh%2520generative%2520model%2520training%2520on%2520scaled%2520mesh%2520data%2520to%2520support%250Aflexible%2520control%2520for%2520point%2520clouds%2520and%2520images.%2520Our%2520model%2520demonstrates%2520the%250Acapability%2520to%2520generate%2520meshes%2520with%2520intricate%2520details%2520and%2520accurate%2520topology%252C%250Aachieving%2520SoTA%2520performance%2520on%2520mesh%2520generation%2520and%2520reaching%2520the%2520level%2520for%2520direct%250Aproduct%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Mesh%20Generation%20via%20Compressive%20Tokenization&entry.906535625=Haohan%20Weng%20and%20Zibo%20Zhao%20and%20Biwen%20Lei%20and%20Xianghui%20Yang%20and%20Jian%20Liu%20and%20Zeqiang%20Lai%20and%20Zhuo%20Chen%20and%20Yuhong%20Liu%20and%20Jie%20Jiang%20and%20Chunchao%20Guo%20and%20Tong%20Zhang%20and%20Shenghua%20Gao%20and%20C.%20L.%20Philip%20Chen&entry.1292438233=%20%20We%20propose%20a%20compressive%20yet%20effective%20mesh%20representation%2C%20Blocked%20and%0APatchified%20Tokenization%20%28BPT%29%2C%20facilitating%20the%20generation%20of%20meshes%20exceeding%0A8k%20faces.%20BPT%20compresses%20mesh%20sequences%20by%20employing%20block-wise%20indexing%20and%0Apatch%20aggregation%2C%20reducing%20their%20length%20by%20approximately%2075%5C%25%20compared%20to%20the%0Aoriginal%20sequences.%20This%20compression%20milestone%20unlocks%20the%20potential%20to%20utilize%0Amesh%20data%20with%20significantly%20more%20faces%2C%20thereby%20enhancing%20detail%20richness%20and%0Aimproving%20generation%20robustness.%20Empowered%20with%20the%20BPT%2C%20we%20have%20built%20a%0Afoundation%20mesh%20generative%20model%20training%20on%20scaled%20mesh%20data%20to%20support%0Aflexible%20control%20for%20point%20clouds%20and%20images.%20Our%20model%20demonstrates%20the%0Acapability%20to%20generate%20meshes%20with%20intricate%20details%20and%20accurate%20topology%2C%0Aachieving%20SoTA%20performance%20on%20mesh%20generation%20and%20reaching%20the%20level%20for%20direct%0Aproduct%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07025v1&entry.124074799=Read"},
{"title": "Revisiting Ensembling in One-Shot Federated Learning", "author": "Youssef Allouah and Akash Dhasade and Rachid Guerraoui and Nirupam Gupta and Anne-Marie Kermarrec and Rafael Pinot and Rafael Pires and Rishi Sharma", "abstract": "  Federated learning (FL) is an appealing approach to training machine learning\nmodels without sharing raw data. However, standard FL algorithms are iterative\nand thus induce a significant communication cost. One-shot federated learning\n(OFL) trades the iterative exchange of models between clients and the server\nwith a single round of communication, thereby saving substantially on\ncommunication costs. Not surprisingly, OFL exhibits a performance gap in terms\nof accuracy with respect to FL, especially under high data heterogeneity. We\nintroduce FENS, a novel federated ensembling scheme that approaches the\naccuracy of FL with the communication efficiency of OFL. Learning in FENS\nproceeds in two phases: first, clients train models locally and send them to\nthe server, similar to OFL; second, clients collaboratively train a lightweight\nprediction aggregator model using FL. We showcase the effectiveness of FENS\nthrough exhaustive experiments spanning several datasets and heterogeneity\nlevels. In the particular case of heterogeneously distributed CIFAR-10 dataset,\nFENS achieves up to a 26.9% higher accuracy over state-of-the-art (SOTA) OFL,\nbeing only 3.1% lower than FL. At the same time, FENS incurs at most 4.3x more\ncommunication than OFL, whereas FL is at least 10.9x more\ncommunication-intensive than FENS.\n", "link": "http://arxiv.org/abs/2411.07182v1", "date": "2024-11-11", "relevancy": 2.2526, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.457}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Ensembling%20in%20One-Shot%20Federated%20Learning&body=Title%3A%20Revisiting%20Ensembling%20in%20One-Shot%20Federated%20Learning%0AAuthor%3A%20Youssef%20Allouah%20and%20Akash%20Dhasade%20and%20Rachid%20Guerraoui%20and%20Nirupam%20Gupta%20and%20Anne-Marie%20Kermarrec%20and%20Rafael%20Pinot%20and%20Rafael%20Pires%20and%20Rishi%20Sharma%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20an%20appealing%20approach%20to%20training%20machine%20learning%0Amodels%20without%20sharing%20raw%20data.%20However%2C%20standard%20FL%20algorithms%20are%20iterative%0Aand%20thus%20induce%20a%20significant%20communication%20cost.%20One-shot%20federated%20learning%0A%28OFL%29%20trades%20the%20iterative%20exchange%20of%20models%20between%20clients%20and%20the%20server%0Awith%20a%20single%20round%20of%20communication%2C%20thereby%20saving%20substantially%20on%0Acommunication%20costs.%20Not%20surprisingly%2C%20OFL%20exhibits%20a%20performance%20gap%20in%20terms%0Aof%20accuracy%20with%20respect%20to%20FL%2C%20especially%20under%20high%20data%20heterogeneity.%20We%0Aintroduce%20FENS%2C%20a%20novel%20federated%20ensembling%20scheme%20that%20approaches%20the%0Aaccuracy%20of%20FL%20with%20the%20communication%20efficiency%20of%20OFL.%20Learning%20in%20FENS%0Aproceeds%20in%20two%20phases%3A%20first%2C%20clients%20train%20models%20locally%20and%20send%20them%20to%0Athe%20server%2C%20similar%20to%20OFL%3B%20second%2C%20clients%20collaboratively%20train%20a%20lightweight%0Aprediction%20aggregator%20model%20using%20FL.%20We%20showcase%20the%20effectiveness%20of%20FENS%0Athrough%20exhaustive%20experiments%20spanning%20several%20datasets%20and%20heterogeneity%0Alevels.%20In%20the%20particular%20case%20of%20heterogeneously%20distributed%20CIFAR-10%20dataset%2C%0AFENS%20achieves%20up%20to%20a%2026.9%25%20higher%20accuracy%20over%20state-of-the-art%20%28SOTA%29%20OFL%2C%0Abeing%20only%203.1%25%20lower%20than%20FL.%20At%20the%20same%20time%2C%20FENS%20incurs%20at%20most%204.3x%20more%0Acommunication%20than%20OFL%2C%20whereas%20FL%20is%20at%20least%2010.9x%20more%0Acommunication-intensive%20than%20FENS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Ensembling%2520in%2520One-Shot%2520Federated%2520Learning%26entry.906535625%3DYoussef%2520Allouah%2520and%2520Akash%2520Dhasade%2520and%2520Rachid%2520Guerraoui%2520and%2520Nirupam%2520Gupta%2520and%2520Anne-Marie%2520Kermarrec%2520and%2520Rafael%2520Pinot%2520and%2520Rafael%2520Pires%2520and%2520Rishi%2520Sharma%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520an%2520appealing%2520approach%2520to%2520training%2520machine%2520learning%250Amodels%2520without%2520sharing%2520raw%2520data.%2520However%252C%2520standard%2520FL%2520algorithms%2520are%2520iterative%250Aand%2520thus%2520induce%2520a%2520significant%2520communication%2520cost.%2520One-shot%2520federated%2520learning%250A%2528OFL%2529%2520trades%2520the%2520iterative%2520exchange%2520of%2520models%2520between%2520clients%2520and%2520the%2520server%250Awith%2520a%2520single%2520round%2520of%2520communication%252C%2520thereby%2520saving%2520substantially%2520on%250Acommunication%2520costs.%2520Not%2520surprisingly%252C%2520OFL%2520exhibits%2520a%2520performance%2520gap%2520in%2520terms%250Aof%2520accuracy%2520with%2520respect%2520to%2520FL%252C%2520especially%2520under%2520high%2520data%2520heterogeneity.%2520We%250Aintroduce%2520FENS%252C%2520a%2520novel%2520federated%2520ensembling%2520scheme%2520that%2520approaches%2520the%250Aaccuracy%2520of%2520FL%2520with%2520the%2520communication%2520efficiency%2520of%2520OFL.%2520Learning%2520in%2520FENS%250Aproceeds%2520in%2520two%2520phases%253A%2520first%252C%2520clients%2520train%2520models%2520locally%2520and%2520send%2520them%2520to%250Athe%2520server%252C%2520similar%2520to%2520OFL%253B%2520second%252C%2520clients%2520collaboratively%2520train%2520a%2520lightweight%250Aprediction%2520aggregator%2520model%2520using%2520FL.%2520We%2520showcase%2520the%2520effectiveness%2520of%2520FENS%250Athrough%2520exhaustive%2520experiments%2520spanning%2520several%2520datasets%2520and%2520heterogeneity%250Alevels.%2520In%2520the%2520particular%2520case%2520of%2520heterogeneously%2520distributed%2520CIFAR-10%2520dataset%252C%250AFENS%2520achieves%2520up%2520to%2520a%252026.9%2525%2520higher%2520accuracy%2520over%2520state-of-the-art%2520%2528SOTA%2529%2520OFL%252C%250Abeing%2520only%25203.1%2525%2520lower%2520than%2520FL.%2520At%2520the%2520same%2520time%252C%2520FENS%2520incurs%2520at%2520most%25204.3x%2520more%250Acommunication%2520than%2520OFL%252C%2520whereas%2520FL%2520is%2520at%2520least%252010.9x%2520more%250Acommunication-intensive%2520than%2520FENS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Ensembling%20in%20One-Shot%20Federated%20Learning&entry.906535625=Youssef%20Allouah%20and%20Akash%20Dhasade%20and%20Rachid%20Guerraoui%20and%20Nirupam%20Gupta%20and%20Anne-Marie%20Kermarrec%20and%20Rafael%20Pinot%20and%20Rafael%20Pires%20and%20Rishi%20Sharma&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20an%20appealing%20approach%20to%20training%20machine%20learning%0Amodels%20without%20sharing%20raw%20data.%20However%2C%20standard%20FL%20algorithms%20are%20iterative%0Aand%20thus%20induce%20a%20significant%20communication%20cost.%20One-shot%20federated%20learning%0A%28OFL%29%20trades%20the%20iterative%20exchange%20of%20models%20between%20clients%20and%20the%20server%0Awith%20a%20single%20round%20of%20communication%2C%20thereby%20saving%20substantially%20on%0Acommunication%20costs.%20Not%20surprisingly%2C%20OFL%20exhibits%20a%20performance%20gap%20in%20terms%0Aof%20accuracy%20with%20respect%20to%20FL%2C%20especially%20under%20high%20data%20heterogeneity.%20We%0Aintroduce%20FENS%2C%20a%20novel%20federated%20ensembling%20scheme%20that%20approaches%20the%0Aaccuracy%20of%20FL%20with%20the%20communication%20efficiency%20of%20OFL.%20Learning%20in%20FENS%0Aproceeds%20in%20two%20phases%3A%20first%2C%20clients%20train%20models%20locally%20and%20send%20them%20to%0Athe%20server%2C%20similar%20to%20OFL%3B%20second%2C%20clients%20collaboratively%20train%20a%20lightweight%0Aprediction%20aggregator%20model%20using%20FL.%20We%20showcase%20the%20effectiveness%20of%20FENS%0Athrough%20exhaustive%20experiments%20spanning%20several%20datasets%20and%20heterogeneity%0Alevels.%20In%20the%20particular%20case%20of%20heterogeneously%20distributed%20CIFAR-10%20dataset%2C%0AFENS%20achieves%20up%20to%20a%2026.9%25%20higher%20accuracy%20over%20state-of-the-art%20%28SOTA%29%20OFL%2C%0Abeing%20only%203.1%25%20lower%20than%20FL.%20At%20the%20same%20time%2C%20FENS%20incurs%20at%20most%204.3x%20more%0Acommunication%20than%20OFL%2C%20whereas%20FL%20is%20at%20least%2010.9x%20more%0Acommunication-intensive%20than%20FENS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07182v1&entry.124074799=Read"},
{"title": "Aligning LLMs for FL-free Program Repair", "author": "Junjielong Xu and Ying Fu and Shin Hwei Tan and Pinjia He", "abstract": "  Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.\n", "link": "http://arxiv.org/abs/2404.08877v2", "date": "2024-11-11", "relevancy": 2.2479, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4646}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20LLMs%20for%20FL-free%20Program%20Repair&body=Title%3A%20Aligning%20LLMs%20for%20FL-free%20Program%20Repair%0AAuthor%3A%20Junjielong%20Xu%20and%20Ying%20Fu%20and%20Shin%20Hwei%20Tan%20and%20Pinjia%20He%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20decent%20results%20on%20automated%0Aprogram%20repair%20%28APR%29.%20However%2C%20the%20next%20token%20prediction%20training%20objective%20of%0Adecoder-only%20LLMs%20%28e.g.%2C%20GPT-4%29%20is%20misaligned%20with%20the%20masked%20span%20prediction%0Aobjective%20of%20current%20infilling-style%20methods%2C%20which%20impedes%20LLMs%20from%20fully%0Aleveraging%20pre-trained%20knowledge%20for%20program%20repair.%20In%20addition%2C%20while%20some%0ALLMs%20can%20locate%20and%20repair%20bugs%20in%20certain%20functions%20using%20the%20related%0Aartifacts%20%28e.g.%2C%20test%20cases%29%2C%20existing%20methods%20still%20depend%20on%20statement-level%0Afault%20localization%20methods%20to%20provide%20a%20list%20of%20buggy%20hunks%20for%20repair.%20This%0Arestriction%20hinders%20LLMs%20from%20exploring%20potential%20patches%20beyond%20the%20given%0Alocations.%0A%20%20In%20this%20paper%2C%20we%20investigate%20a%20new%20approach%20to%20adapt%20LLMs%20to%20program%20repair.%0AOur%20core%20insight%20is%20that%20LLM%27s%20APR%20capability%20can%20be%20greatly%20improved%20by%20simply%0Aaligning%20the%20output%20to%20their%20training%20objective%20and%20allowing%20them%20to%20refine%20the%0Awhole%20program%20without%20first%20identifying%20faulty%20statements.%20Based%20on%20this%0Ainsight%2C%20we%20designed%20D4C%2C%20a%20straightforward%20prompting%20framework%20for%20APR.%20D4C%0Acan%20repair%20180%20bugs%20correctly%20in%20Defects4J%2C%20with%20each%20patch%20being%20sampled%20only%0A10%20times.%20This%20surpasses%20the%20SOTA%20APR%20methods%20with%20perfect%20fault%20localization%0Aby%2010%25%20and%20reduces%20the%20patch%20sampling%20number%20by%2090%25.%20Our%20findings%20reveal%20that%0A%281%29%20objective%20alignment%20is%20crucial%20for%20fully%20exploiting%20LLM%27s%20pre-trained%0Acapability%2C%20and%20%282%29%20replacing%20the%20traditional%20localize-buggy-hunks-then-repair%0Aworkflow%20with%20direct%20debugging%20is%20more%20effective%20for%20LLM-based%20APR%20methods.%0AThus%2C%20we%20believe%20this%20paper%20introduces%20a%20new%20mindset%20for%20harnessing%20LLMs%20in%0AAPR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08877v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520LLMs%2520for%2520FL-free%2520Program%2520Repair%26entry.906535625%3DJunjielong%2520Xu%2520and%2520Ying%2520Fu%2520and%2520Shin%2520Hwei%2520Tan%2520and%2520Pinjia%2520He%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520decent%2520results%2520on%2520automated%250Aprogram%2520repair%2520%2528APR%2529.%2520However%252C%2520the%2520next%2520token%2520prediction%2520training%2520objective%2520of%250Adecoder-only%2520LLMs%2520%2528e.g.%252C%2520GPT-4%2529%2520is%2520misaligned%2520with%2520the%2520masked%2520span%2520prediction%250Aobjective%2520of%2520current%2520infilling-style%2520methods%252C%2520which%2520impedes%2520LLMs%2520from%2520fully%250Aleveraging%2520pre-trained%2520knowledge%2520for%2520program%2520repair.%2520In%2520addition%252C%2520while%2520some%250ALLMs%2520can%2520locate%2520and%2520repair%2520bugs%2520in%2520certain%2520functions%2520using%2520the%2520related%250Aartifacts%2520%2528e.g.%252C%2520test%2520cases%2529%252C%2520existing%2520methods%2520still%2520depend%2520on%2520statement-level%250Afault%2520localization%2520methods%2520to%2520provide%2520a%2520list%2520of%2520buggy%2520hunks%2520for%2520repair.%2520This%250Arestriction%2520hinders%2520LLMs%2520from%2520exploring%2520potential%2520patches%2520beyond%2520the%2520given%250Alocations.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520a%2520new%2520approach%2520to%2520adapt%2520LLMs%2520to%2520program%2520repair.%250AOur%2520core%2520insight%2520is%2520that%2520LLM%2527s%2520APR%2520capability%2520can%2520be%2520greatly%2520improved%2520by%2520simply%250Aaligning%2520the%2520output%2520to%2520their%2520training%2520objective%2520and%2520allowing%2520them%2520to%2520refine%2520the%250Awhole%2520program%2520without%2520first%2520identifying%2520faulty%2520statements.%2520Based%2520on%2520this%250Ainsight%252C%2520we%2520designed%2520D4C%252C%2520a%2520straightforward%2520prompting%2520framework%2520for%2520APR.%2520D4C%250Acan%2520repair%2520180%2520bugs%2520correctly%2520in%2520Defects4J%252C%2520with%2520each%2520patch%2520being%2520sampled%2520only%250A10%2520times.%2520This%2520surpasses%2520the%2520SOTA%2520APR%2520methods%2520with%2520perfect%2520fault%2520localization%250Aby%252010%2525%2520and%2520reduces%2520the%2520patch%2520sampling%2520number%2520by%252090%2525.%2520Our%2520findings%2520reveal%2520that%250A%25281%2529%2520objective%2520alignment%2520is%2520crucial%2520for%2520fully%2520exploiting%2520LLM%2527s%2520pre-trained%250Acapability%252C%2520and%2520%25282%2529%2520replacing%2520the%2520traditional%2520localize-buggy-hunks-then-repair%250Aworkflow%2520with%2520direct%2520debugging%2520is%2520more%2520effective%2520for%2520LLM-based%2520APR%2520methods.%250AThus%252C%2520we%2520believe%2520this%2520paper%2520introduces%2520a%2520new%2520mindset%2520for%2520harnessing%2520LLMs%2520in%250AAPR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08877v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20LLMs%20for%20FL-free%20Program%20Repair&entry.906535625=Junjielong%20Xu%20and%20Ying%20Fu%20and%20Shin%20Hwei%20Tan%20and%20Pinjia%20He&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20decent%20results%20on%20automated%0Aprogram%20repair%20%28APR%29.%20However%2C%20the%20next%20token%20prediction%20training%20objective%20of%0Adecoder-only%20LLMs%20%28e.g.%2C%20GPT-4%29%20is%20misaligned%20with%20the%20masked%20span%20prediction%0Aobjective%20of%20current%20infilling-style%20methods%2C%20which%20impedes%20LLMs%20from%20fully%0Aleveraging%20pre-trained%20knowledge%20for%20program%20repair.%20In%20addition%2C%20while%20some%0ALLMs%20can%20locate%20and%20repair%20bugs%20in%20certain%20functions%20using%20the%20related%0Aartifacts%20%28e.g.%2C%20test%20cases%29%2C%20existing%20methods%20still%20depend%20on%20statement-level%0Afault%20localization%20methods%20to%20provide%20a%20list%20of%20buggy%20hunks%20for%20repair.%20This%0Arestriction%20hinders%20LLMs%20from%20exploring%20potential%20patches%20beyond%20the%20given%0Alocations.%0A%20%20In%20this%20paper%2C%20we%20investigate%20a%20new%20approach%20to%20adapt%20LLMs%20to%20program%20repair.%0AOur%20core%20insight%20is%20that%20LLM%27s%20APR%20capability%20can%20be%20greatly%20improved%20by%20simply%0Aaligning%20the%20output%20to%20their%20training%20objective%20and%20allowing%20them%20to%20refine%20the%0Awhole%20program%20without%20first%20identifying%20faulty%20statements.%20Based%20on%20this%0Ainsight%2C%20we%20designed%20D4C%2C%20a%20straightforward%20prompting%20framework%20for%20APR.%20D4C%0Acan%20repair%20180%20bugs%20correctly%20in%20Defects4J%2C%20with%20each%20patch%20being%20sampled%20only%0A10%20times.%20This%20surpasses%20the%20SOTA%20APR%20methods%20with%20perfect%20fault%20localization%0Aby%2010%25%20and%20reduces%20the%20patch%20sampling%20number%20by%2090%25.%20Our%20findings%20reveal%20that%0A%281%29%20objective%20alignment%20is%20crucial%20for%20fully%20exploiting%20LLM%27s%20pre-trained%0Acapability%2C%20and%20%282%29%20replacing%20the%20traditional%20localize-buggy-hunks-then-repair%0Aworkflow%20with%20direct%20debugging%20is%20more%20effective%20for%20LLM-based%20APR%20methods.%0AThus%2C%20we%20believe%20this%20paper%20introduces%20a%20new%20mindset%20for%20harnessing%20LLMs%20in%0AAPR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08877v2&entry.124074799=Read"},
{"title": "Lost in Tracking Translation: A Comprehensive Analysis of Visual SLAM in\n  Human-Centered XR and IoT Ecosystems", "author": "Yasra Chandio and Khotso Selialia and Joseph DeGol and Luis Garcia and Fatima M. Anwar", "abstract": "  Advancements in tracking algorithms have empowered nascent applications\nacross various domains, from steering autonomous vehicles to guiding robots to\nenhancing augmented reality experiences for users. However, these algorithms\nare application-specific and do not work across applications with different\ntypes of motion; even a tracking algorithm designed for a given application\ndoes not work in scenarios deviating from highly standard conditions. For\nexample, a tracking algorithm designed for robot navigation inside a building\nwill not work for tracking the same robot in an outdoor environment. To\ndemonstrate this problem, we evaluate the performance of the state-of-the-art\ntracking methods across various applications and scenarios. To inform our\nanalysis, we first categorize algorithmic, environmental, and\nlocomotion-related challenges faced by tracking algorithms. We quantitatively\nevaluate the performance using multiple tracking algorithms and representative\ndatasets for a wide range of Internet of Things (IoT) and Extended Reality (XR)\napplications, including autonomous vehicles, drones, and humans. Our analysis\nshows that no tracking algorithm works across different applications and\nscenarios within applications. Ultimately, using the insights generated from\nour analysis, we discuss multiple approaches to improving the tracking\nperformance using input data characterization, leveraging intermediate\ninformation, and output evaluation.\n", "link": "http://arxiv.org/abs/2411.07146v1", "date": "2024-11-11", "relevancy": 2.2349, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5776}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5471}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Tracking%20Translation%3A%20A%20Comprehensive%20Analysis%20of%20Visual%20SLAM%20in%0A%20%20Human-Centered%20XR%20and%20IoT%20Ecosystems&body=Title%3A%20Lost%20in%20Tracking%20Translation%3A%20A%20Comprehensive%20Analysis%20of%20Visual%20SLAM%20in%0A%20%20Human-Centered%20XR%20and%20IoT%20Ecosystems%0AAuthor%3A%20Yasra%20Chandio%20and%20Khotso%20Selialia%20and%20Joseph%20DeGol%20and%20Luis%20Garcia%20and%20Fatima%20M.%20Anwar%0AAbstract%3A%20%20%20Advancements%20in%20tracking%20algorithms%20have%20empowered%20nascent%20applications%0Aacross%20various%20domains%2C%20from%20steering%20autonomous%20vehicles%20to%20guiding%20robots%20to%0Aenhancing%20augmented%20reality%20experiences%20for%20users.%20However%2C%20these%20algorithms%0Aare%20application-specific%20and%20do%20not%20work%20across%20applications%20with%20different%0Atypes%20of%20motion%3B%20even%20a%20tracking%20algorithm%20designed%20for%20a%20given%20application%0Adoes%20not%20work%20in%20scenarios%20deviating%20from%20highly%20standard%20conditions.%20For%0Aexample%2C%20a%20tracking%20algorithm%20designed%20for%20robot%20navigation%20inside%20a%20building%0Awill%20not%20work%20for%20tracking%20the%20same%20robot%20in%20an%20outdoor%20environment.%20To%0Ademonstrate%20this%20problem%2C%20we%20evaluate%20the%20performance%20of%20the%20state-of-the-art%0Atracking%20methods%20across%20various%20applications%20and%20scenarios.%20To%20inform%20our%0Aanalysis%2C%20we%20first%20categorize%20algorithmic%2C%20environmental%2C%20and%0Alocomotion-related%20challenges%20faced%20by%20tracking%20algorithms.%20We%20quantitatively%0Aevaluate%20the%20performance%20using%20multiple%20tracking%20algorithms%20and%20representative%0Adatasets%20for%20a%20wide%20range%20of%20Internet%20of%20Things%20%28IoT%29%20and%20Extended%20Reality%20%28XR%29%0Aapplications%2C%20including%20autonomous%20vehicles%2C%20drones%2C%20and%20humans.%20Our%20analysis%0Ashows%20that%20no%20tracking%20algorithm%20works%20across%20different%20applications%20and%0Ascenarios%20within%20applications.%20Ultimately%2C%20using%20the%20insights%20generated%20from%0Aour%20analysis%2C%20we%20discuss%20multiple%20approaches%20to%20improving%20the%20tracking%0Aperformance%20using%20input%20data%20characterization%2C%20leveraging%20intermediate%0Ainformation%2C%20and%20output%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Tracking%2520Translation%253A%2520A%2520Comprehensive%2520Analysis%2520of%2520Visual%2520SLAM%2520in%250A%2520%2520Human-Centered%2520XR%2520and%2520IoT%2520Ecosystems%26entry.906535625%3DYasra%2520Chandio%2520and%2520Khotso%2520Selialia%2520and%2520Joseph%2520DeGol%2520and%2520Luis%2520Garcia%2520and%2520Fatima%2520M.%2520Anwar%26entry.1292438233%3D%2520%2520Advancements%2520in%2520tracking%2520algorithms%2520have%2520empowered%2520nascent%2520applications%250Aacross%2520various%2520domains%252C%2520from%2520steering%2520autonomous%2520vehicles%2520to%2520guiding%2520robots%2520to%250Aenhancing%2520augmented%2520reality%2520experiences%2520for%2520users.%2520However%252C%2520these%2520algorithms%250Aare%2520application-specific%2520and%2520do%2520not%2520work%2520across%2520applications%2520with%2520different%250Atypes%2520of%2520motion%253B%2520even%2520a%2520tracking%2520algorithm%2520designed%2520for%2520a%2520given%2520application%250Adoes%2520not%2520work%2520in%2520scenarios%2520deviating%2520from%2520highly%2520standard%2520conditions.%2520For%250Aexample%252C%2520a%2520tracking%2520algorithm%2520designed%2520for%2520robot%2520navigation%2520inside%2520a%2520building%250Awill%2520not%2520work%2520for%2520tracking%2520the%2520same%2520robot%2520in%2520an%2520outdoor%2520environment.%2520To%250Ademonstrate%2520this%2520problem%252C%2520we%2520evaluate%2520the%2520performance%2520of%2520the%2520state-of-the-art%250Atracking%2520methods%2520across%2520various%2520applications%2520and%2520scenarios.%2520To%2520inform%2520our%250Aanalysis%252C%2520we%2520first%2520categorize%2520algorithmic%252C%2520environmental%252C%2520and%250Alocomotion-related%2520challenges%2520faced%2520by%2520tracking%2520algorithms.%2520We%2520quantitatively%250Aevaluate%2520the%2520performance%2520using%2520multiple%2520tracking%2520algorithms%2520and%2520representative%250Adatasets%2520for%2520a%2520wide%2520range%2520of%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520and%2520Extended%2520Reality%2520%2528XR%2529%250Aapplications%252C%2520including%2520autonomous%2520vehicles%252C%2520drones%252C%2520and%2520humans.%2520Our%2520analysis%250Ashows%2520that%2520no%2520tracking%2520algorithm%2520works%2520across%2520different%2520applications%2520and%250Ascenarios%2520within%2520applications.%2520Ultimately%252C%2520using%2520the%2520insights%2520generated%2520from%250Aour%2520analysis%252C%2520we%2520discuss%2520multiple%2520approaches%2520to%2520improving%2520the%2520tracking%250Aperformance%2520using%2520input%2520data%2520characterization%252C%2520leveraging%2520intermediate%250Ainformation%252C%2520and%2520output%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Tracking%20Translation%3A%20A%20Comprehensive%20Analysis%20of%20Visual%20SLAM%20in%0A%20%20Human-Centered%20XR%20and%20IoT%20Ecosystems&entry.906535625=Yasra%20Chandio%20and%20Khotso%20Selialia%20and%20Joseph%20DeGol%20and%20Luis%20Garcia%20and%20Fatima%20M.%20Anwar&entry.1292438233=%20%20Advancements%20in%20tracking%20algorithms%20have%20empowered%20nascent%20applications%0Aacross%20various%20domains%2C%20from%20steering%20autonomous%20vehicles%20to%20guiding%20robots%20to%0Aenhancing%20augmented%20reality%20experiences%20for%20users.%20However%2C%20these%20algorithms%0Aare%20application-specific%20and%20do%20not%20work%20across%20applications%20with%20different%0Atypes%20of%20motion%3B%20even%20a%20tracking%20algorithm%20designed%20for%20a%20given%20application%0Adoes%20not%20work%20in%20scenarios%20deviating%20from%20highly%20standard%20conditions.%20For%0Aexample%2C%20a%20tracking%20algorithm%20designed%20for%20robot%20navigation%20inside%20a%20building%0Awill%20not%20work%20for%20tracking%20the%20same%20robot%20in%20an%20outdoor%20environment.%20To%0Ademonstrate%20this%20problem%2C%20we%20evaluate%20the%20performance%20of%20the%20state-of-the-art%0Atracking%20methods%20across%20various%20applications%20and%20scenarios.%20To%20inform%20our%0Aanalysis%2C%20we%20first%20categorize%20algorithmic%2C%20environmental%2C%20and%0Alocomotion-related%20challenges%20faced%20by%20tracking%20algorithms.%20We%20quantitatively%0Aevaluate%20the%20performance%20using%20multiple%20tracking%20algorithms%20and%20representative%0Adatasets%20for%20a%20wide%20range%20of%20Internet%20of%20Things%20%28IoT%29%20and%20Extended%20Reality%20%28XR%29%0Aapplications%2C%20including%20autonomous%20vehicles%2C%20drones%2C%20and%20humans.%20Our%20analysis%0Ashows%20that%20no%20tracking%20algorithm%20works%20across%20different%20applications%20and%0Ascenarios%20within%20applications.%20Ultimately%2C%20using%20the%20insights%20generated%20from%0Aour%20analysis%2C%20we%20discuss%20multiple%20approaches%20to%20improving%20the%20tracking%0Aperformance%20using%20input%20data%20characterization%2C%20leveraging%20intermediate%0Ainformation%2C%20and%20output%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07146v1&entry.124074799=Read"},
{"title": "1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of\n  Language, Hate Speech, and Targets using LLMs", "author": "Jebish Purbey and Siddartha Pullakhandam and Kanwal Mehreen and Muhammad Arham and Drishti Sharma and Ashay Srivastava and Ram Mohan Rao Kadiyala", "abstract": "  This paper presents a detailed system description of our entry for the\nCHiPSAL 2025 shared task, focusing on language detection, hate speech\nidentification, and target detection in Devanagari script languages. We\nexperimented with a combination of large language models and their ensembles,\nincluding MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like\nfocal loss to address challenges in the natural understanding of Devanagari\nlanguages, such as multilingual processing and class imbalance. Our approach\nachieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804\nfor Sub-tasks A, B, and C respectively. This work provides insights into the\neffectiveness of transformer models in tasks with domain-specific and\nlinguistic challenges, as well as areas for potential improvement in future\niterations.\n", "link": "http://arxiv.org/abs/2411.06850v1", "date": "2024-11-11", "relevancy": 2.2323, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4667}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%201-800-SHARED-TASKS%20%40%20NLU%20of%20Devanagari%20Script%20Languages%3A%20Detection%20of%0A%20%20Language%2C%20Hate%20Speech%2C%20and%20Targets%20using%20LLMs&body=Title%3A%201-800-SHARED-TASKS%20%40%20NLU%20of%20Devanagari%20Script%20Languages%3A%20Detection%20of%0A%20%20Language%2C%20Hate%20Speech%2C%20and%20Targets%20using%20LLMs%0AAuthor%3A%20Jebish%20Purbey%20and%20Siddartha%20Pullakhandam%20and%20Kanwal%20Mehreen%20and%20Muhammad%20Arham%20and%20Drishti%20Sharma%20and%20Ashay%20Srivastava%20and%20Ram%20Mohan%20Rao%20Kadiyala%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20detailed%20system%20description%20of%20our%20entry%20for%20the%0ACHiPSAL%202025%20shared%20task%2C%20focusing%20on%20language%20detection%2C%20hate%20speech%0Aidentification%2C%20and%20target%20detection%20in%20Devanagari%20script%20languages.%20We%0Aexperimented%20with%20a%20combination%20of%20large%20language%20models%20and%20their%20ensembles%2C%0Aincluding%20MuRIL%2C%20IndicBERT%2C%20and%20Gemma-2%2C%20and%20leveraged%20unique%20techniques%20like%0Afocal%20loss%20to%20address%20challenges%20in%20the%20natural%20understanding%20of%20Devanagari%0Alanguages%2C%20such%20as%20multilingual%20processing%20and%20class%20imbalance.%20Our%20approach%0Aachieved%20competitive%20results%20across%20all%20tasks%3A%20F1%20of%200.9980%2C%200.7652%2C%20and%200.6804%0Afor%20Sub-tasks%20A%2C%20B%2C%20and%20C%20respectively.%20This%20work%20provides%20insights%20into%20the%0Aeffectiveness%20of%20transformer%20models%20in%20tasks%20with%20domain-specific%20and%0Alinguistic%20challenges%2C%20as%20well%20as%20areas%20for%20potential%20improvement%20in%20future%0Aiterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D1-800-SHARED-TASKS%2520%2540%2520NLU%2520of%2520Devanagari%2520Script%2520Languages%253A%2520Detection%2520of%250A%2520%2520Language%252C%2520Hate%2520Speech%252C%2520and%2520Targets%2520using%2520LLMs%26entry.906535625%3DJebish%2520Purbey%2520and%2520Siddartha%2520Pullakhandam%2520and%2520Kanwal%2520Mehreen%2520and%2520Muhammad%2520Arham%2520and%2520Drishti%2520Sharma%2520and%2520Ashay%2520Srivastava%2520and%2520Ram%2520Mohan%2520Rao%2520Kadiyala%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520detailed%2520system%2520description%2520of%2520our%2520entry%2520for%2520the%250ACHiPSAL%25202025%2520shared%2520task%252C%2520focusing%2520on%2520language%2520detection%252C%2520hate%2520speech%250Aidentification%252C%2520and%2520target%2520detection%2520in%2520Devanagari%2520script%2520languages.%2520We%250Aexperimented%2520with%2520a%2520combination%2520of%2520large%2520language%2520models%2520and%2520their%2520ensembles%252C%250Aincluding%2520MuRIL%252C%2520IndicBERT%252C%2520and%2520Gemma-2%252C%2520and%2520leveraged%2520unique%2520techniques%2520like%250Afocal%2520loss%2520to%2520address%2520challenges%2520in%2520the%2520natural%2520understanding%2520of%2520Devanagari%250Alanguages%252C%2520such%2520as%2520multilingual%2520processing%2520and%2520class%2520imbalance.%2520Our%2520approach%250Aachieved%2520competitive%2520results%2520across%2520all%2520tasks%253A%2520F1%2520of%25200.9980%252C%25200.7652%252C%2520and%25200.6804%250Afor%2520Sub-tasks%2520A%252C%2520B%252C%2520and%2520C%2520respectively.%2520This%2520work%2520provides%2520insights%2520into%2520the%250Aeffectiveness%2520of%2520transformer%2520models%2520in%2520tasks%2520with%2520domain-specific%2520and%250Alinguistic%2520challenges%252C%2520as%2520well%2520as%2520areas%2520for%2520potential%2520improvement%2520in%2520future%250Aiterations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=1-800-SHARED-TASKS%20%40%20NLU%20of%20Devanagari%20Script%20Languages%3A%20Detection%20of%0A%20%20Language%2C%20Hate%20Speech%2C%20and%20Targets%20using%20LLMs&entry.906535625=Jebish%20Purbey%20and%20Siddartha%20Pullakhandam%20and%20Kanwal%20Mehreen%20and%20Muhammad%20Arham%20and%20Drishti%20Sharma%20and%20Ashay%20Srivastava%20and%20Ram%20Mohan%20Rao%20Kadiyala&entry.1292438233=%20%20This%20paper%20presents%20a%20detailed%20system%20description%20of%20our%20entry%20for%20the%0ACHiPSAL%202025%20shared%20task%2C%20focusing%20on%20language%20detection%2C%20hate%20speech%0Aidentification%2C%20and%20target%20detection%20in%20Devanagari%20script%20languages.%20We%0Aexperimented%20with%20a%20combination%20of%20large%20language%20models%20and%20their%20ensembles%2C%0Aincluding%20MuRIL%2C%20IndicBERT%2C%20and%20Gemma-2%2C%20and%20leveraged%20unique%20techniques%20like%0Afocal%20loss%20to%20address%20challenges%20in%20the%20natural%20understanding%20of%20Devanagari%0Alanguages%2C%20such%20as%20multilingual%20processing%20and%20class%20imbalance.%20Our%20approach%0Aachieved%20competitive%20results%20across%20all%20tasks%3A%20F1%20of%200.9980%2C%200.7652%2C%20and%200.6804%0Afor%20Sub-tasks%20A%2C%20B%2C%20and%20C%20respectively.%20This%20work%20provides%20insights%20into%20the%0Aeffectiveness%20of%20transformer%20models%20in%20tasks%20with%20domain-specific%20and%0Alinguistic%20challenges%2C%20as%20well%20as%20areas%20for%20potential%20improvement%20in%20future%0Aiterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06850v1&entry.124074799=Read"},
{"title": "Using Large Language Models for Hyperparameter Optimization", "author": "Michael R. Zhang and Nishkrit Desai and Juhan Bae and Jonathan Lorraine and Jimmy Ba", "abstract": "  This paper explores the use of foundational large language models (LLMs) in\nhyperparameter optimization (HPO). Hyperparameters are critical in determining\nthe effectiveness of machine learning models, yet their optimization often\nrelies on manual approaches in limited-budget settings. By prompting LLMs with\ndataset and model descriptions, we develop a methodology where LLMs suggest\nhyperparameter configurations, which are iteratively refined based on model\nperformance. Our empirical evaluations on standard benchmarks reveal that\nwithin constrained search budgets, LLMs can match or outperform traditional HPO\nmethods like Bayesian optimization across different models on standard\nbenchmarks. Furthermore, we propose to treat the code specifying our model as a\nhyperparameter, which the LLM outputs and affords greater flexibility than\nexisting HPO approaches.\n", "link": "http://arxiv.org/abs/2312.04528v2", "date": "2024-11-11", "relevancy": 2.2307, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4552}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Large%20Language%20Models%20for%20Hyperparameter%20Optimization&body=Title%3A%20Using%20Large%20Language%20Models%20for%20Hyperparameter%20Optimization%0AAuthor%3A%20Michael%20R.%20Zhang%20and%20Nishkrit%20Desai%20and%20Juhan%20Bae%20and%20Jonathan%20Lorraine%20and%20Jimmy%20Ba%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20use%20of%20foundational%20large%20language%20models%20%28LLMs%29%20in%0Ahyperparameter%20optimization%20%28HPO%29.%20Hyperparameters%20are%20critical%20in%20determining%0Athe%20effectiveness%20of%20machine%20learning%20models%2C%20yet%20their%20optimization%20often%0Arelies%20on%20manual%20approaches%20in%20limited-budget%20settings.%20By%20prompting%20LLMs%20with%0Adataset%20and%20model%20descriptions%2C%20we%20develop%20a%20methodology%20where%20LLMs%20suggest%0Ahyperparameter%20configurations%2C%20which%20are%20iteratively%20refined%20based%20on%20model%0Aperformance.%20Our%20empirical%20evaluations%20on%20standard%20benchmarks%20reveal%20that%0Awithin%20constrained%20search%20budgets%2C%20LLMs%20can%20match%20or%20outperform%20traditional%20HPO%0Amethods%20like%20Bayesian%20optimization%20across%20different%20models%20on%20standard%0Abenchmarks.%20Furthermore%2C%20we%20propose%20to%20treat%20the%20code%20specifying%20our%20model%20as%20a%0Ahyperparameter%2C%20which%20the%20LLM%20outputs%20and%20affords%20greater%20flexibility%20than%0Aexisting%20HPO%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04528v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Large%2520Language%2520Models%2520for%2520Hyperparameter%2520Optimization%26entry.906535625%3DMichael%2520R.%2520Zhang%2520and%2520Nishkrit%2520Desai%2520and%2520Juhan%2520Bae%2520and%2520Jonathan%2520Lorraine%2520and%2520Jimmy%2520Ba%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520use%2520of%2520foundational%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%250Ahyperparameter%2520optimization%2520%2528HPO%2529.%2520Hyperparameters%2520are%2520critical%2520in%2520determining%250Athe%2520effectiveness%2520of%2520machine%2520learning%2520models%252C%2520yet%2520their%2520optimization%2520often%250Arelies%2520on%2520manual%2520approaches%2520in%2520limited-budget%2520settings.%2520By%2520prompting%2520LLMs%2520with%250Adataset%2520and%2520model%2520descriptions%252C%2520we%2520develop%2520a%2520methodology%2520where%2520LLMs%2520suggest%250Ahyperparameter%2520configurations%252C%2520which%2520are%2520iteratively%2520refined%2520based%2520on%2520model%250Aperformance.%2520Our%2520empirical%2520evaluations%2520on%2520standard%2520benchmarks%2520reveal%2520that%250Awithin%2520constrained%2520search%2520budgets%252C%2520LLMs%2520can%2520match%2520or%2520outperform%2520traditional%2520HPO%250Amethods%2520like%2520Bayesian%2520optimization%2520across%2520different%2520models%2520on%2520standard%250Abenchmarks.%2520Furthermore%252C%2520we%2520propose%2520to%2520treat%2520the%2520code%2520specifying%2520our%2520model%2520as%2520a%250Ahyperparameter%252C%2520which%2520the%2520LLM%2520outputs%2520and%2520affords%2520greater%2520flexibility%2520than%250Aexisting%2520HPO%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04528v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Large%20Language%20Models%20for%20Hyperparameter%20Optimization&entry.906535625=Michael%20R.%20Zhang%20and%20Nishkrit%20Desai%20and%20Juhan%20Bae%20and%20Jonathan%20Lorraine%20and%20Jimmy%20Ba&entry.1292438233=%20%20This%20paper%20explores%20the%20use%20of%20foundational%20large%20language%20models%20%28LLMs%29%20in%0Ahyperparameter%20optimization%20%28HPO%29.%20Hyperparameters%20are%20critical%20in%20determining%0Athe%20effectiveness%20of%20machine%20learning%20models%2C%20yet%20their%20optimization%20often%0Arelies%20on%20manual%20approaches%20in%20limited-budget%20settings.%20By%20prompting%20LLMs%20with%0Adataset%20and%20model%20descriptions%2C%20we%20develop%20a%20methodology%20where%20LLMs%20suggest%0Ahyperparameter%20configurations%2C%20which%20are%20iteratively%20refined%20based%20on%20model%0Aperformance.%20Our%20empirical%20evaluations%20on%20standard%20benchmarks%20reveal%20that%0Awithin%20constrained%20search%20budgets%2C%20LLMs%20can%20match%20or%20outperform%20traditional%20HPO%0Amethods%20like%20Bayesian%20optimization%20across%20different%20models%20on%20standard%0Abenchmarks.%20Furthermore%2C%20we%20propose%20to%20treat%20the%20code%20specifying%20our%20model%20as%20a%0Ahyperparameter%2C%20which%20the%20LLM%20outputs%20and%20affords%20greater%20flexibility%20than%0Aexisting%20HPO%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04528v2&entry.124074799=Read"},
{"title": "Automatic Contact-Based 3D Scanning Using Articulated Robotic Arm", "author": "Shadman Tajwar Shahid and Shah Md. Ahasan Siddique and Md. Humayun Kabir Bhuiyan", "abstract": "  This paper presents an open-loop articulated 6-degree-of-freedom (DoF)\nrobotic system for three-dimensional (3D) scanning of objects by contact-based\nmethod. A digitizer probe was used to detect contact with the object. Inverse\nkinematics (IK) was used to determine the joint angles of the robot\ncorresponding to the probe position and orientation, and straight-line\ntrajectory planning was implemented for motion. The system can take\nsingle-point measurements and 3D scans of freeform surfaces. Specifying the\nscanning area's size, position, and density, the system automatically scans the\ndesignated volume. The system produces 3D scans in Standard Triangle Language\n(STL) format, ensuring compatibility with commonly used 3D software. Tests\nbased on ASME B89.4.22 standards were conducted to quantify accuracy and\nrepeatability. The point cloud from the scans was compared to the original 3D\nmodel of the object.\n", "link": "http://arxiv.org/abs/2411.07047v1", "date": "2024-11-11", "relevancy": 2.2183, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5569}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5569}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Contact-Based%203D%20Scanning%20Using%20Articulated%20Robotic%20Arm&body=Title%3A%20Automatic%20Contact-Based%203D%20Scanning%20Using%20Articulated%20Robotic%20Arm%0AAuthor%3A%20Shadman%20Tajwar%20Shahid%20and%20Shah%20Md.%20Ahasan%20Siddique%20and%20Md.%20Humayun%20Kabir%20Bhuiyan%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20open-loop%20articulated%206-degree-of-freedom%20%28DoF%29%0Arobotic%20system%20for%20three-dimensional%20%283D%29%20scanning%20of%20objects%20by%20contact-based%0Amethod.%20A%20digitizer%20probe%20was%20used%20to%20detect%20contact%20with%20the%20object.%20Inverse%0Akinematics%20%28IK%29%20was%20used%20to%20determine%20the%20joint%20angles%20of%20the%20robot%0Acorresponding%20to%20the%20probe%20position%20and%20orientation%2C%20and%20straight-line%0Atrajectory%20planning%20was%20implemented%20for%20motion.%20The%20system%20can%20take%0Asingle-point%20measurements%20and%203D%20scans%20of%20freeform%20surfaces.%20Specifying%20the%0Ascanning%20area%27s%20size%2C%20position%2C%20and%20density%2C%20the%20system%20automatically%20scans%20the%0Adesignated%20volume.%20The%20system%20produces%203D%20scans%20in%20Standard%20Triangle%20Language%0A%28STL%29%20format%2C%20ensuring%20compatibility%20with%20commonly%20used%203D%20software.%20Tests%0Abased%20on%20ASME%20B89.4.22%20standards%20were%20conducted%20to%20quantify%20accuracy%20and%0Arepeatability.%20The%20point%20cloud%20from%20the%20scans%20was%20compared%20to%20the%20original%203D%0Amodel%20of%20the%20object.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Contact-Based%25203D%2520Scanning%2520Using%2520Articulated%2520Robotic%2520Arm%26entry.906535625%3DShadman%2520Tajwar%2520Shahid%2520and%2520Shah%2520Md.%2520Ahasan%2520Siddique%2520and%2520Md.%2520Humayun%2520Kabir%2520Bhuiyan%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520open-loop%2520articulated%25206-degree-of-freedom%2520%2528DoF%2529%250Arobotic%2520system%2520for%2520three-dimensional%2520%25283D%2529%2520scanning%2520of%2520objects%2520by%2520contact-based%250Amethod.%2520A%2520digitizer%2520probe%2520was%2520used%2520to%2520detect%2520contact%2520with%2520the%2520object.%2520Inverse%250Akinematics%2520%2528IK%2529%2520was%2520used%2520to%2520determine%2520the%2520joint%2520angles%2520of%2520the%2520robot%250Acorresponding%2520to%2520the%2520probe%2520position%2520and%2520orientation%252C%2520and%2520straight-line%250Atrajectory%2520planning%2520was%2520implemented%2520for%2520motion.%2520The%2520system%2520can%2520take%250Asingle-point%2520measurements%2520and%25203D%2520scans%2520of%2520freeform%2520surfaces.%2520Specifying%2520the%250Ascanning%2520area%2527s%2520size%252C%2520position%252C%2520and%2520density%252C%2520the%2520system%2520automatically%2520scans%2520the%250Adesignated%2520volume.%2520The%2520system%2520produces%25203D%2520scans%2520in%2520Standard%2520Triangle%2520Language%250A%2528STL%2529%2520format%252C%2520ensuring%2520compatibility%2520with%2520commonly%2520used%25203D%2520software.%2520Tests%250Abased%2520on%2520ASME%2520B89.4.22%2520standards%2520were%2520conducted%2520to%2520quantify%2520accuracy%2520and%250Arepeatability.%2520The%2520point%2520cloud%2520from%2520the%2520scans%2520was%2520compared%2520to%2520the%2520original%25203D%250Amodel%2520of%2520the%2520object.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Contact-Based%203D%20Scanning%20Using%20Articulated%20Robotic%20Arm&entry.906535625=Shadman%20Tajwar%20Shahid%20and%20Shah%20Md.%20Ahasan%20Siddique%20and%20Md.%20Humayun%20Kabir%20Bhuiyan&entry.1292438233=%20%20This%20paper%20presents%20an%20open-loop%20articulated%206-degree-of-freedom%20%28DoF%29%0Arobotic%20system%20for%20three-dimensional%20%283D%29%20scanning%20of%20objects%20by%20contact-based%0Amethod.%20A%20digitizer%20probe%20was%20used%20to%20detect%20contact%20with%20the%20object.%20Inverse%0Akinematics%20%28IK%29%20was%20used%20to%20determine%20the%20joint%20angles%20of%20the%20robot%0Acorresponding%20to%20the%20probe%20position%20and%20orientation%2C%20and%20straight-line%0Atrajectory%20planning%20was%20implemented%20for%20motion.%20The%20system%20can%20take%0Asingle-point%20measurements%20and%203D%20scans%20of%20freeform%20surfaces.%20Specifying%20the%0Ascanning%20area%27s%20size%2C%20position%2C%20and%20density%2C%20the%20system%20automatically%20scans%20the%0Adesignated%20volume.%20The%20system%20produces%203D%20scans%20in%20Standard%20Triangle%20Language%0A%28STL%29%20format%2C%20ensuring%20compatibility%20with%20commonly%20used%203D%20software.%20Tests%0Abased%20on%20ASME%20B89.4.22%20standards%20were%20conducted%20to%20quantify%20accuracy%20and%0Arepeatability.%20The%20point%20cloud%20from%20the%20scans%20was%20compared%20to%20the%20original%203D%0Amodel%20of%20the%20object.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07047v1&entry.124074799=Read"},
{"title": "Arctique: An artificial histopathological dataset unifying realism and\n  controllability for uncertainty quantification", "author": "Jannik Franzen and Claudia Winklmayr and Vanessa E. Guarino and Christoph Karg and Xiaoyan Yu and Nora Koreuber and Jan P. Albrecht and Philip Bischoff and Dagmar Kainmueller", "abstract": "  Uncertainty Quantification (UQ) is crucial for reliable image segmentation.\nYet, while the field sees continual development of novel methods, a lack of\nagreed-upon benchmarks limits their systematic comparison and evaluation:\nCurrent UQ methods are typically tested either on overly simplistic toy\ndatasets or on complex real-world datasets that do not allow to discern true\nuncertainty. To unify both controllability and complexity, we introduce\nArctique, a procedurally generated dataset modeled after histopathological\ncolon images. We chose histopathological images for two reasons: 1) their\ncomplexity in terms of intricate object structures and highly variable\nappearance, which yields challenging segmentation problems, and 2) their broad\nprevalence for medical diagnosis and respective relevance of high-quality UQ.\nTo generate Arctique, we established a Blender-based framework for 3D scene\ncreation with intrinsic noise manipulation. Arctique contains 50,000 rendered\nimages with precise masks as well as noisy label simulations. We show that by\nindependently controlling the uncertainty in both images and labels, we can\neffectively study the performance of several commonly used UQ methods. Hence,\nArctique serves as a critical resource for benchmarking and advancing UQ\ntechniques and other methodologies in complex, multi-object environments,\nbridging the gap between realism and controllability. All code is publicly\navailable, allowing re-creation and controlled manipulations of our shipped\nimages as well as creation and rendering of new scenes.\n", "link": "http://arxiv.org/abs/2411.07097v1", "date": "2024-11-11", "relevancy": 2.2082, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.563}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5499}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arctique%3A%20An%20artificial%20histopathological%20dataset%20unifying%20realism%20and%0A%20%20controllability%20for%20uncertainty%20quantification&body=Title%3A%20Arctique%3A%20An%20artificial%20histopathological%20dataset%20unifying%20realism%20and%0A%20%20controllability%20for%20uncertainty%20quantification%0AAuthor%3A%20Jannik%20Franzen%20and%20Claudia%20Winklmayr%20and%20Vanessa%20E.%20Guarino%20and%20Christoph%20Karg%20and%20Xiaoyan%20Yu%20and%20Nora%20Koreuber%20and%20Jan%20P.%20Albrecht%20and%20Philip%20Bischoff%20and%20Dagmar%20Kainmueller%0AAbstract%3A%20%20%20Uncertainty%20Quantification%20%28UQ%29%20is%20crucial%20for%20reliable%20image%20segmentation.%0AYet%2C%20while%20the%20field%20sees%20continual%20development%20of%20novel%20methods%2C%20a%20lack%20of%0Aagreed-upon%20benchmarks%20limits%20their%20systematic%20comparison%20and%20evaluation%3A%0ACurrent%20UQ%20methods%20are%20typically%20tested%20either%20on%20overly%20simplistic%20toy%0Adatasets%20or%20on%20complex%20real-world%20datasets%20that%20do%20not%20allow%20to%20discern%20true%0Auncertainty.%20To%20unify%20both%20controllability%20and%20complexity%2C%20we%20introduce%0AArctique%2C%20a%20procedurally%20generated%20dataset%20modeled%20after%20histopathological%0Acolon%20images.%20We%20chose%20histopathological%20images%20for%20two%20reasons%3A%201%29%20their%0Acomplexity%20in%20terms%20of%20intricate%20object%20structures%20and%20highly%20variable%0Aappearance%2C%20which%20yields%20challenging%20segmentation%20problems%2C%20and%202%29%20their%20broad%0Aprevalence%20for%20medical%20diagnosis%20and%20respective%20relevance%20of%20high-quality%20UQ.%0ATo%20generate%20Arctique%2C%20we%20established%20a%20Blender-based%20framework%20for%203D%20scene%0Acreation%20with%20intrinsic%20noise%20manipulation.%20Arctique%20contains%2050%2C000%20rendered%0Aimages%20with%20precise%20masks%20as%20well%20as%20noisy%20label%20simulations.%20We%20show%20that%20by%0Aindependently%20controlling%20the%20uncertainty%20in%20both%20images%20and%20labels%2C%20we%20can%0Aeffectively%20study%20the%20performance%20of%20several%20commonly%20used%20UQ%20methods.%20Hence%2C%0AArctique%20serves%20as%20a%20critical%20resource%20for%20benchmarking%20and%20advancing%20UQ%0Atechniques%20and%20other%20methodologies%20in%20complex%2C%20multi-object%20environments%2C%0Abridging%20the%20gap%20between%20realism%20and%20controllability.%20All%20code%20is%20publicly%0Aavailable%2C%20allowing%20re-creation%20and%20controlled%20manipulations%20of%20our%20shipped%0Aimages%20as%20well%20as%20creation%20and%20rendering%20of%20new%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArctique%253A%2520An%2520artificial%2520histopathological%2520dataset%2520unifying%2520realism%2520and%250A%2520%2520controllability%2520for%2520uncertainty%2520quantification%26entry.906535625%3DJannik%2520Franzen%2520and%2520Claudia%2520Winklmayr%2520and%2520Vanessa%2520E.%2520Guarino%2520and%2520Christoph%2520Karg%2520and%2520Xiaoyan%2520Yu%2520and%2520Nora%2520Koreuber%2520and%2520Jan%2520P.%2520Albrecht%2520and%2520Philip%2520Bischoff%2520and%2520Dagmar%2520Kainmueller%26entry.1292438233%3D%2520%2520Uncertainty%2520Quantification%2520%2528UQ%2529%2520is%2520crucial%2520for%2520reliable%2520image%2520segmentation.%250AYet%252C%2520while%2520the%2520field%2520sees%2520continual%2520development%2520of%2520novel%2520methods%252C%2520a%2520lack%2520of%250Aagreed-upon%2520benchmarks%2520limits%2520their%2520systematic%2520comparison%2520and%2520evaluation%253A%250ACurrent%2520UQ%2520methods%2520are%2520typically%2520tested%2520either%2520on%2520overly%2520simplistic%2520toy%250Adatasets%2520or%2520on%2520complex%2520real-world%2520datasets%2520that%2520do%2520not%2520allow%2520to%2520discern%2520true%250Auncertainty.%2520To%2520unify%2520both%2520controllability%2520and%2520complexity%252C%2520we%2520introduce%250AArctique%252C%2520a%2520procedurally%2520generated%2520dataset%2520modeled%2520after%2520histopathological%250Acolon%2520images.%2520We%2520chose%2520histopathological%2520images%2520for%2520two%2520reasons%253A%25201%2529%2520their%250Acomplexity%2520in%2520terms%2520of%2520intricate%2520object%2520structures%2520and%2520highly%2520variable%250Aappearance%252C%2520which%2520yields%2520challenging%2520segmentation%2520problems%252C%2520and%25202%2529%2520their%2520broad%250Aprevalence%2520for%2520medical%2520diagnosis%2520and%2520respective%2520relevance%2520of%2520high-quality%2520UQ.%250ATo%2520generate%2520Arctique%252C%2520we%2520established%2520a%2520Blender-based%2520framework%2520for%25203D%2520scene%250Acreation%2520with%2520intrinsic%2520noise%2520manipulation.%2520Arctique%2520contains%252050%252C000%2520rendered%250Aimages%2520with%2520precise%2520masks%2520as%2520well%2520as%2520noisy%2520label%2520simulations.%2520We%2520show%2520that%2520by%250Aindependently%2520controlling%2520the%2520uncertainty%2520in%2520both%2520images%2520and%2520labels%252C%2520we%2520can%250Aeffectively%2520study%2520the%2520performance%2520of%2520several%2520commonly%2520used%2520UQ%2520methods.%2520Hence%252C%250AArctique%2520serves%2520as%2520a%2520critical%2520resource%2520for%2520benchmarking%2520and%2520advancing%2520UQ%250Atechniques%2520and%2520other%2520methodologies%2520in%2520complex%252C%2520multi-object%2520environments%252C%250Abridging%2520the%2520gap%2520between%2520realism%2520and%2520controllability.%2520All%2520code%2520is%2520publicly%250Aavailable%252C%2520allowing%2520re-creation%2520and%2520controlled%2520manipulations%2520of%2520our%2520shipped%250Aimages%2520as%2520well%2520as%2520creation%2520and%2520rendering%2520of%2520new%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arctique%3A%20An%20artificial%20histopathological%20dataset%20unifying%20realism%20and%0A%20%20controllability%20for%20uncertainty%20quantification&entry.906535625=Jannik%20Franzen%20and%20Claudia%20Winklmayr%20and%20Vanessa%20E.%20Guarino%20and%20Christoph%20Karg%20and%20Xiaoyan%20Yu%20and%20Nora%20Koreuber%20and%20Jan%20P.%20Albrecht%20and%20Philip%20Bischoff%20and%20Dagmar%20Kainmueller&entry.1292438233=%20%20Uncertainty%20Quantification%20%28UQ%29%20is%20crucial%20for%20reliable%20image%20segmentation.%0AYet%2C%20while%20the%20field%20sees%20continual%20development%20of%20novel%20methods%2C%20a%20lack%20of%0Aagreed-upon%20benchmarks%20limits%20their%20systematic%20comparison%20and%20evaluation%3A%0ACurrent%20UQ%20methods%20are%20typically%20tested%20either%20on%20overly%20simplistic%20toy%0Adatasets%20or%20on%20complex%20real-world%20datasets%20that%20do%20not%20allow%20to%20discern%20true%0Auncertainty.%20To%20unify%20both%20controllability%20and%20complexity%2C%20we%20introduce%0AArctique%2C%20a%20procedurally%20generated%20dataset%20modeled%20after%20histopathological%0Acolon%20images.%20We%20chose%20histopathological%20images%20for%20two%20reasons%3A%201%29%20their%0Acomplexity%20in%20terms%20of%20intricate%20object%20structures%20and%20highly%20variable%0Aappearance%2C%20which%20yields%20challenging%20segmentation%20problems%2C%20and%202%29%20their%20broad%0Aprevalence%20for%20medical%20diagnosis%20and%20respective%20relevance%20of%20high-quality%20UQ.%0ATo%20generate%20Arctique%2C%20we%20established%20a%20Blender-based%20framework%20for%203D%20scene%0Acreation%20with%20intrinsic%20noise%20manipulation.%20Arctique%20contains%2050%2C000%20rendered%0Aimages%20with%20precise%20masks%20as%20well%20as%20noisy%20label%20simulations.%20We%20show%20that%20by%0Aindependently%20controlling%20the%20uncertainty%20in%20both%20images%20and%20labels%2C%20we%20can%0Aeffectively%20study%20the%20performance%20of%20several%20commonly%20used%20UQ%20methods.%20Hence%2C%0AArctique%20serves%20as%20a%20critical%20resource%20for%20benchmarking%20and%20advancing%20UQ%0Atechniques%20and%20other%20methodologies%20in%20complex%2C%20multi-object%20environments%2C%0Abridging%20the%20gap%20between%20realism%20and%20controllability.%20All%20code%20is%20publicly%0Aavailable%2C%20allowing%20re-creation%20and%20controlled%20manipulations%20of%20our%20shipped%0Aimages%20as%20well%20as%20creation%20and%20rendering%20of%20new%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07097v1&entry.124074799=Read"},
{"title": "Efficient Adaptive Optimization via Subset-Norm and Subspace-Momentum:\n  Fast, Memory-Reduced Training with Convergence Guarantees", "author": "Thien Hang Nguyen and Huy Le Nguyen", "abstract": "  We introduce two complementary techniques for efficient adaptive optimization\nthat reduce memory requirements while accelerating training of large-scale\nneural networks. The first technique, Subset-Norm adaptive step size,\ngeneralizes AdaGrad-Norm and AdaGrad(-Coordinate) by reducing the second moment\nterm's memory footprint from $O(d)$ to $O(\\sqrt{d})$ through step-size sharing,\nwhere $d$ is the model size. For non-convex smooth objectives under\ncoordinate-wise sub-gaussian gradient noise, we prove a noise-adapted\nhigh-probability convergence guarantee showing improved dimensional dependence\nover existing methods. Our second technique, Subspace-Momentum, reduces the\nmomentum state's memory footprint by operating in a low-dimensional subspace\nwhile applying standard SGD in the orthogonal complement. We establish\nhigh-probability convergence rates under similar relaxed assumptions. Empirical\nevaluation on LLaMA models from 60M to 1B parameters demonstrates the\neffectiveness of our methods, where combining subset-norm with\nsubspace-momentum achieves Adam's validation perplexity in approximately half\nthe training tokens (6.8B vs 13.1B) while using only 20% of the Adam's\noptimizer-states memory footprint and requiring minimal additional\nhyperparameter tuning.\n", "link": "http://arxiv.org/abs/2411.07120v1", "date": "2024-11-11", "relevancy": 2.1986, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.556}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5484}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Adaptive%20Optimization%20via%20Subset-Norm%20and%20Subspace-Momentum%3A%0A%20%20Fast%2C%20Memory-Reduced%20Training%20with%20Convergence%20Guarantees&body=Title%3A%20Efficient%20Adaptive%20Optimization%20via%20Subset-Norm%20and%20Subspace-Momentum%3A%0A%20%20Fast%2C%20Memory-Reduced%20Training%20with%20Convergence%20Guarantees%0AAuthor%3A%20Thien%20Hang%20Nguyen%20and%20Huy%20Le%20Nguyen%0AAbstract%3A%20%20%20We%20introduce%20two%20complementary%20techniques%20for%20efficient%20adaptive%20optimization%0Athat%20reduce%20memory%20requirements%20while%20accelerating%20training%20of%20large-scale%0Aneural%20networks.%20The%20first%20technique%2C%20Subset-Norm%20adaptive%20step%20size%2C%0Ageneralizes%20AdaGrad-Norm%20and%20AdaGrad%28-Coordinate%29%20by%20reducing%20the%20second%20moment%0Aterm%27s%20memory%20footprint%20from%20%24O%28d%29%24%20to%20%24O%28%5Csqrt%7Bd%7D%29%24%20through%20step-size%20sharing%2C%0Awhere%20%24d%24%20is%20the%20model%20size.%20For%20non-convex%20smooth%20objectives%20under%0Acoordinate-wise%20sub-gaussian%20gradient%20noise%2C%20we%20prove%20a%20noise-adapted%0Ahigh-probability%20convergence%20guarantee%20showing%20improved%20dimensional%20dependence%0Aover%20existing%20methods.%20Our%20second%20technique%2C%20Subspace-Momentum%2C%20reduces%20the%0Amomentum%20state%27s%20memory%20footprint%20by%20operating%20in%20a%20low-dimensional%20subspace%0Awhile%20applying%20standard%20SGD%20in%20the%20orthogonal%20complement.%20We%20establish%0Ahigh-probability%20convergence%20rates%20under%20similar%20relaxed%20assumptions.%20Empirical%0Aevaluation%20on%20LLaMA%20models%20from%2060M%20to%201B%20parameters%20demonstrates%20the%0Aeffectiveness%20of%20our%20methods%2C%20where%20combining%20subset-norm%20with%0Asubspace-momentum%20achieves%20Adam%27s%20validation%20perplexity%20in%20approximately%20half%0Athe%20training%20tokens%20%286.8B%20vs%2013.1B%29%20while%20using%20only%2020%25%20of%20the%20Adam%27s%0Aoptimizer-states%20memory%20footprint%20and%20requiring%20minimal%20additional%0Ahyperparameter%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Adaptive%2520Optimization%2520via%2520Subset-Norm%2520and%2520Subspace-Momentum%253A%250A%2520%2520Fast%252C%2520Memory-Reduced%2520Training%2520with%2520Convergence%2520Guarantees%26entry.906535625%3DThien%2520Hang%2520Nguyen%2520and%2520Huy%2520Le%2520Nguyen%26entry.1292438233%3D%2520%2520We%2520introduce%2520two%2520complementary%2520techniques%2520for%2520efficient%2520adaptive%2520optimization%250Athat%2520reduce%2520memory%2520requirements%2520while%2520accelerating%2520training%2520of%2520large-scale%250Aneural%2520networks.%2520The%2520first%2520technique%252C%2520Subset-Norm%2520adaptive%2520step%2520size%252C%250Ageneralizes%2520AdaGrad-Norm%2520and%2520AdaGrad%2528-Coordinate%2529%2520by%2520reducing%2520the%2520second%2520moment%250Aterm%2527s%2520memory%2520footprint%2520from%2520%2524O%2528d%2529%2524%2520to%2520%2524O%2528%255Csqrt%257Bd%257D%2529%2524%2520through%2520step-size%2520sharing%252C%250Awhere%2520%2524d%2524%2520is%2520the%2520model%2520size.%2520For%2520non-convex%2520smooth%2520objectives%2520under%250Acoordinate-wise%2520sub-gaussian%2520gradient%2520noise%252C%2520we%2520prove%2520a%2520noise-adapted%250Ahigh-probability%2520convergence%2520guarantee%2520showing%2520improved%2520dimensional%2520dependence%250Aover%2520existing%2520methods.%2520Our%2520second%2520technique%252C%2520Subspace-Momentum%252C%2520reduces%2520the%250Amomentum%2520state%2527s%2520memory%2520footprint%2520by%2520operating%2520in%2520a%2520low-dimensional%2520subspace%250Awhile%2520applying%2520standard%2520SGD%2520in%2520the%2520orthogonal%2520complement.%2520We%2520establish%250Ahigh-probability%2520convergence%2520rates%2520under%2520similar%2520relaxed%2520assumptions.%2520Empirical%250Aevaluation%2520on%2520LLaMA%2520models%2520from%252060M%2520to%25201B%2520parameters%2520demonstrates%2520the%250Aeffectiveness%2520of%2520our%2520methods%252C%2520where%2520combining%2520subset-norm%2520with%250Asubspace-momentum%2520achieves%2520Adam%2527s%2520validation%2520perplexity%2520in%2520approximately%2520half%250Athe%2520training%2520tokens%2520%25286.8B%2520vs%252013.1B%2529%2520while%2520using%2520only%252020%2525%2520of%2520the%2520Adam%2527s%250Aoptimizer-states%2520memory%2520footprint%2520and%2520requiring%2520minimal%2520additional%250Ahyperparameter%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Adaptive%20Optimization%20via%20Subset-Norm%20and%20Subspace-Momentum%3A%0A%20%20Fast%2C%20Memory-Reduced%20Training%20with%20Convergence%20Guarantees&entry.906535625=Thien%20Hang%20Nguyen%20and%20Huy%20Le%20Nguyen&entry.1292438233=%20%20We%20introduce%20two%20complementary%20techniques%20for%20efficient%20adaptive%20optimization%0Athat%20reduce%20memory%20requirements%20while%20accelerating%20training%20of%20large-scale%0Aneural%20networks.%20The%20first%20technique%2C%20Subset-Norm%20adaptive%20step%20size%2C%0Ageneralizes%20AdaGrad-Norm%20and%20AdaGrad%28-Coordinate%29%20by%20reducing%20the%20second%20moment%0Aterm%27s%20memory%20footprint%20from%20%24O%28d%29%24%20to%20%24O%28%5Csqrt%7Bd%7D%29%24%20through%20step-size%20sharing%2C%0Awhere%20%24d%24%20is%20the%20model%20size.%20For%20non-convex%20smooth%20objectives%20under%0Acoordinate-wise%20sub-gaussian%20gradient%20noise%2C%20we%20prove%20a%20noise-adapted%0Ahigh-probability%20convergence%20guarantee%20showing%20improved%20dimensional%20dependence%0Aover%20existing%20methods.%20Our%20second%20technique%2C%20Subspace-Momentum%2C%20reduces%20the%0Amomentum%20state%27s%20memory%20footprint%20by%20operating%20in%20a%20low-dimensional%20subspace%0Awhile%20applying%20standard%20SGD%20in%20the%20orthogonal%20complement.%20We%20establish%0Ahigh-probability%20convergence%20rates%20under%20similar%20relaxed%20assumptions.%20Empirical%0Aevaluation%20on%20LLaMA%20models%20from%2060M%20to%201B%20parameters%20demonstrates%20the%0Aeffectiveness%20of%20our%20methods%2C%20where%20combining%20subset-norm%20with%0Asubspace-momentum%20achieves%20Adam%27s%20validation%20perplexity%20in%20approximately%20half%0Athe%20training%20tokens%20%286.8B%20vs%2013.1B%29%20while%20using%20only%2020%25%20of%20the%20Adam%27s%0Aoptimizer-states%20memory%20footprint%20and%20requiring%20minimal%20additional%0Ahyperparameter%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07120v1&entry.124074799=Read"},
{"title": "Learning Collective Dynamics of Multi-Agent Systems using Event-based\n  Vision", "author": "Minah Lee and Uday Kamal and Saibal Mukhopadhyay", "abstract": "  This paper proposes a novel problem: vision-based perception to learn and\npredict the collective dynamics of multi-agent systems, specifically focusing\non interaction strength and convergence time. Multi-agent systems are defined\nas collections of more than ten interacting agents that exhibit complex group\nbehaviors. Unlike prior studies that assume knowledge of agent positions, we\nfocus on deep learning models to directly predict collective dynamics from\nvisual data, captured as frames or events. Due to the lack of relevant\ndatasets, we create a simulated dataset using a state-of-the-art flocking\nsimulator, coupled with a vision-to-event conversion framework. We empirically\ndemonstrate the effectiveness of event-based representation over traditional\nframe-based methods in predicting these collective behaviors. Based on our\nanalysis, we present event-based vision for Multi-Agent dynamic Prediction\n(evMAP), a deep learning architecture designed for real-time, accurate\nunderstanding of interaction strength and collective behavior emergence in\nmulti-agent systems.\n", "link": "http://arxiv.org/abs/2411.07039v1", "date": "2024-11-11", "relevancy": 2.1963, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.61}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Collective%20Dynamics%20of%20Multi-Agent%20Systems%20using%20Event-based%0A%20%20Vision&body=Title%3A%20Learning%20Collective%20Dynamics%20of%20Multi-Agent%20Systems%20using%20Event-based%0A%20%20Vision%0AAuthor%3A%20Minah%20Lee%20and%20Uday%20Kamal%20and%20Saibal%20Mukhopadhyay%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20problem%3A%20vision-based%20perception%20to%20learn%20and%0Apredict%20the%20collective%20dynamics%20of%20multi-agent%20systems%2C%20specifically%20focusing%0Aon%20interaction%20strength%20and%20convergence%20time.%20Multi-agent%20systems%20are%20defined%0Aas%20collections%20of%20more%20than%20ten%20interacting%20agents%20that%20exhibit%20complex%20group%0Abehaviors.%20Unlike%20prior%20studies%20that%20assume%20knowledge%20of%20agent%20positions%2C%20we%0Afocus%20on%20deep%20learning%20models%20to%20directly%20predict%20collective%20dynamics%20from%0Avisual%20data%2C%20captured%20as%20frames%20or%20events.%20Due%20to%20the%20lack%20of%20relevant%0Adatasets%2C%20we%20create%20a%20simulated%20dataset%20using%20a%20state-of-the-art%20flocking%0Asimulator%2C%20coupled%20with%20a%20vision-to-event%20conversion%20framework.%20We%20empirically%0Ademonstrate%20the%20effectiveness%20of%20event-based%20representation%20over%20traditional%0Aframe-based%20methods%20in%20predicting%20these%20collective%20behaviors.%20Based%20on%20our%0Aanalysis%2C%20we%20present%20event-based%20vision%20for%20Multi-Agent%20dynamic%20Prediction%0A%28evMAP%29%2C%20a%20deep%20learning%20architecture%20designed%20for%20real-time%2C%20accurate%0Aunderstanding%20of%20interaction%20strength%20and%20collective%20behavior%20emergence%20in%0Amulti-agent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Collective%2520Dynamics%2520of%2520Multi-Agent%2520Systems%2520using%2520Event-based%250A%2520%2520Vision%26entry.906535625%3DMinah%2520Lee%2520and%2520Uday%2520Kamal%2520and%2520Saibal%2520Mukhopadhyay%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520problem%253A%2520vision-based%2520perception%2520to%2520learn%2520and%250Apredict%2520the%2520collective%2520dynamics%2520of%2520multi-agent%2520systems%252C%2520specifically%2520focusing%250Aon%2520interaction%2520strength%2520and%2520convergence%2520time.%2520Multi-agent%2520systems%2520are%2520defined%250Aas%2520collections%2520of%2520more%2520than%2520ten%2520interacting%2520agents%2520that%2520exhibit%2520complex%2520group%250Abehaviors.%2520Unlike%2520prior%2520studies%2520that%2520assume%2520knowledge%2520of%2520agent%2520positions%252C%2520we%250Afocus%2520on%2520deep%2520learning%2520models%2520to%2520directly%2520predict%2520collective%2520dynamics%2520from%250Avisual%2520data%252C%2520captured%2520as%2520frames%2520or%2520events.%2520Due%2520to%2520the%2520lack%2520of%2520relevant%250Adatasets%252C%2520we%2520create%2520a%2520simulated%2520dataset%2520using%2520a%2520state-of-the-art%2520flocking%250Asimulator%252C%2520coupled%2520with%2520a%2520vision-to-event%2520conversion%2520framework.%2520We%2520empirically%250Ademonstrate%2520the%2520effectiveness%2520of%2520event-based%2520representation%2520over%2520traditional%250Aframe-based%2520methods%2520in%2520predicting%2520these%2520collective%2520behaviors.%2520Based%2520on%2520our%250Aanalysis%252C%2520we%2520present%2520event-based%2520vision%2520for%2520Multi-Agent%2520dynamic%2520Prediction%250A%2528evMAP%2529%252C%2520a%2520deep%2520learning%2520architecture%2520designed%2520for%2520real-time%252C%2520accurate%250Aunderstanding%2520of%2520interaction%2520strength%2520and%2520collective%2520behavior%2520emergence%2520in%250Amulti-agent%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Collective%20Dynamics%20of%20Multi-Agent%20Systems%20using%20Event-based%0A%20%20Vision&entry.906535625=Minah%20Lee%20and%20Uday%20Kamal%20and%20Saibal%20Mukhopadhyay&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20problem%3A%20vision-based%20perception%20to%20learn%20and%0Apredict%20the%20collective%20dynamics%20of%20multi-agent%20systems%2C%20specifically%20focusing%0Aon%20interaction%20strength%20and%20convergence%20time.%20Multi-agent%20systems%20are%20defined%0Aas%20collections%20of%20more%20than%20ten%20interacting%20agents%20that%20exhibit%20complex%20group%0Abehaviors.%20Unlike%20prior%20studies%20that%20assume%20knowledge%20of%20agent%20positions%2C%20we%0Afocus%20on%20deep%20learning%20models%20to%20directly%20predict%20collective%20dynamics%20from%0Avisual%20data%2C%20captured%20as%20frames%20or%20events.%20Due%20to%20the%20lack%20of%20relevant%0Adatasets%2C%20we%20create%20a%20simulated%20dataset%20using%20a%20state-of-the-art%20flocking%0Asimulator%2C%20coupled%20with%20a%20vision-to-event%20conversion%20framework.%20We%20empirically%0Ademonstrate%20the%20effectiveness%20of%20event-based%20representation%20over%20traditional%0Aframe-based%20methods%20in%20predicting%20these%20collective%20behaviors.%20Based%20on%20our%0Aanalysis%2C%20we%20present%20event-based%20vision%20for%20Multi-Agent%20dynamic%20Prediction%0A%28evMAP%29%2C%20a%20deep%20learning%20architecture%20designed%20for%20real-time%2C%20accurate%0Aunderstanding%20of%20interaction%20strength%20and%20collective%20behavior%20emergence%20in%0Amulti-agent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07039v1&entry.124074799=Read"},
{"title": "Concept Drift and Long-Tailed Distribution in Fine-Grained Visual\n  Categorization: Benchmark and Method", "author": "Shuo Ye and Shiming Chen and Ruxin Wang and Tianxu Wu and Jiamiao Xu and Salman Khan and Fahad Shahbaz Khan and Ling Shao", "abstract": "  Data is the foundation for the development of computer vision, and the\nestablishment of datasets plays an important role in advancing the techniques\nof fine-grained visual categorization~(FGVC). In the existing FGVC datasets\nused in computer vision, it is generally assumed that each collected instance\nhas fixed characteristics and the distribution of different categories is\nrelatively balanced. In contrast, the real world scenario reveals the fact that\nthe characteristics of instances tend to vary with time and exhibit a\nlong-tailed distribution. Hence, the collected datasets may mislead the\noptimization of the fine-grained classifiers, resulting in unpleasant\nperformance in real applications. Starting from the real-world conditions and\nto promote the practical progress of fine-grained visual categorization, we\npresent a Concept Drift and Long-Tailed Distribution dataset. Specifically, the\ndataset is collected by gathering 11195 images of 250 instances in different\nspecies for 47 consecutive months in their natural contexts. The collection\nprocess involves dozens of crowd workers for photographing and domain experts\nfor labeling. Meanwhile, we propose a feature recombination framework to\naddress the learning challenges associated with CDLT. Experimental results\nvalidate the efficacy of our method while also highlighting the limitations of\npopular large vision-language models (e.g., CLIP) in the context of long-tailed\ndistributions. This emphasizes the significance of CDLT as a benchmark for\ninvestigating these challenges.\n", "link": "http://arxiv.org/abs/2306.02346v2", "date": "2024-11-11", "relevancy": 2.1879, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5597}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5516}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept%20Drift%20and%20Long-Tailed%20Distribution%20in%20Fine-Grained%20Visual%0A%20%20Categorization%3A%20Benchmark%20and%20Method&body=Title%3A%20Concept%20Drift%20and%20Long-Tailed%20Distribution%20in%20Fine-Grained%20Visual%0A%20%20Categorization%3A%20Benchmark%20and%20Method%0AAuthor%3A%20Shuo%20Ye%20and%20Shiming%20Chen%20and%20Ruxin%20Wang%20and%20Tianxu%20Wu%20and%20Jiamiao%20Xu%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%20and%20Ling%20Shao%0AAbstract%3A%20%20%20Data%20is%20the%20foundation%20for%20the%20development%20of%20computer%20vision%2C%20and%20the%0Aestablishment%20of%20datasets%20plays%20an%20important%20role%20in%20advancing%20the%20techniques%0Aof%20fine-grained%20visual%20categorization~%28FGVC%29.%20In%20the%20existing%20FGVC%20datasets%0Aused%20in%20computer%20vision%2C%20it%20is%20generally%20assumed%20that%20each%20collected%20instance%0Ahas%20fixed%20characteristics%20and%20the%20distribution%20of%20different%20categories%20is%0Arelatively%20balanced.%20In%20contrast%2C%20the%20real%20world%20scenario%20reveals%20the%20fact%20that%0Athe%20characteristics%20of%20instances%20tend%20to%20vary%20with%20time%20and%20exhibit%20a%0Along-tailed%20distribution.%20Hence%2C%20the%20collected%20datasets%20may%20mislead%20the%0Aoptimization%20of%20the%20fine-grained%20classifiers%2C%20resulting%20in%20unpleasant%0Aperformance%20in%20real%20applications.%20Starting%20from%20the%20real-world%20conditions%20and%0Ato%20promote%20the%20practical%20progress%20of%20fine-grained%20visual%20categorization%2C%20we%0Apresent%20a%20Concept%20Drift%20and%20Long-Tailed%20Distribution%20dataset.%20Specifically%2C%20the%0Adataset%20is%20collected%20by%20gathering%2011195%20images%20of%20250%20instances%20in%20different%0Aspecies%20for%2047%20consecutive%20months%20in%20their%20natural%20contexts.%20The%20collection%0Aprocess%20involves%20dozens%20of%20crowd%20workers%20for%20photographing%20and%20domain%20experts%0Afor%20labeling.%20Meanwhile%2C%20we%20propose%20a%20feature%20recombination%20framework%20to%0Aaddress%20the%20learning%20challenges%20associated%20with%20CDLT.%20Experimental%20results%0Avalidate%20the%20efficacy%20of%20our%20method%20while%20also%20highlighting%20the%20limitations%20of%0Apopular%20large%20vision-language%20models%20%28e.g.%2C%20CLIP%29%20in%20the%20context%20of%20long-tailed%0Adistributions.%20This%20emphasizes%20the%20significance%20of%20CDLT%20as%20a%20benchmark%20for%0Ainvestigating%20these%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept%2520Drift%2520and%2520Long-Tailed%2520Distribution%2520in%2520Fine-Grained%2520Visual%250A%2520%2520Categorization%253A%2520Benchmark%2520and%2520Method%26entry.906535625%3DShuo%2520Ye%2520and%2520Shiming%2520Chen%2520and%2520Ruxin%2520Wang%2520and%2520Tianxu%2520Wu%2520and%2520Jiamiao%2520Xu%2520and%2520Salman%2520Khan%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Ling%2520Shao%26entry.1292438233%3D%2520%2520Data%2520is%2520the%2520foundation%2520for%2520the%2520development%2520of%2520computer%2520vision%252C%2520and%2520the%250Aestablishment%2520of%2520datasets%2520plays%2520an%2520important%2520role%2520in%2520advancing%2520the%2520techniques%250Aof%2520fine-grained%2520visual%2520categorization~%2528FGVC%2529.%2520In%2520the%2520existing%2520FGVC%2520datasets%250Aused%2520in%2520computer%2520vision%252C%2520it%2520is%2520generally%2520assumed%2520that%2520each%2520collected%2520instance%250Ahas%2520fixed%2520characteristics%2520and%2520the%2520distribution%2520of%2520different%2520categories%2520is%250Arelatively%2520balanced.%2520In%2520contrast%252C%2520the%2520real%2520world%2520scenario%2520reveals%2520the%2520fact%2520that%250Athe%2520characteristics%2520of%2520instances%2520tend%2520to%2520vary%2520with%2520time%2520and%2520exhibit%2520a%250Along-tailed%2520distribution.%2520Hence%252C%2520the%2520collected%2520datasets%2520may%2520mislead%2520the%250Aoptimization%2520of%2520the%2520fine-grained%2520classifiers%252C%2520resulting%2520in%2520unpleasant%250Aperformance%2520in%2520real%2520applications.%2520Starting%2520from%2520the%2520real-world%2520conditions%2520and%250Ato%2520promote%2520the%2520practical%2520progress%2520of%2520fine-grained%2520visual%2520categorization%252C%2520we%250Apresent%2520a%2520Concept%2520Drift%2520and%2520Long-Tailed%2520Distribution%2520dataset.%2520Specifically%252C%2520the%250Adataset%2520is%2520collected%2520by%2520gathering%252011195%2520images%2520of%2520250%2520instances%2520in%2520different%250Aspecies%2520for%252047%2520consecutive%2520months%2520in%2520their%2520natural%2520contexts.%2520The%2520collection%250Aprocess%2520involves%2520dozens%2520of%2520crowd%2520workers%2520for%2520photographing%2520and%2520domain%2520experts%250Afor%2520labeling.%2520Meanwhile%252C%2520we%2520propose%2520a%2520feature%2520recombination%2520framework%2520to%250Aaddress%2520the%2520learning%2520challenges%2520associated%2520with%2520CDLT.%2520Experimental%2520results%250Avalidate%2520the%2520efficacy%2520of%2520our%2520method%2520while%2520also%2520highlighting%2520the%2520limitations%2520of%250Apopular%2520large%2520vision-language%2520models%2520%2528e.g.%252C%2520CLIP%2529%2520in%2520the%2520context%2520of%2520long-tailed%250Adistributions.%2520This%2520emphasizes%2520the%2520significance%2520of%2520CDLT%2520as%2520a%2520benchmark%2520for%250Ainvestigating%2520these%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.02346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept%20Drift%20and%20Long-Tailed%20Distribution%20in%20Fine-Grained%20Visual%0A%20%20Categorization%3A%20Benchmark%20and%20Method&entry.906535625=Shuo%20Ye%20and%20Shiming%20Chen%20and%20Ruxin%20Wang%20and%20Tianxu%20Wu%20and%20Jiamiao%20Xu%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%20and%20Ling%20Shao&entry.1292438233=%20%20Data%20is%20the%20foundation%20for%20the%20development%20of%20computer%20vision%2C%20and%20the%0Aestablishment%20of%20datasets%20plays%20an%20important%20role%20in%20advancing%20the%20techniques%0Aof%20fine-grained%20visual%20categorization~%28FGVC%29.%20In%20the%20existing%20FGVC%20datasets%0Aused%20in%20computer%20vision%2C%20it%20is%20generally%20assumed%20that%20each%20collected%20instance%0Ahas%20fixed%20characteristics%20and%20the%20distribution%20of%20different%20categories%20is%0Arelatively%20balanced.%20In%20contrast%2C%20the%20real%20world%20scenario%20reveals%20the%20fact%20that%0Athe%20characteristics%20of%20instances%20tend%20to%20vary%20with%20time%20and%20exhibit%20a%0Along-tailed%20distribution.%20Hence%2C%20the%20collected%20datasets%20may%20mislead%20the%0Aoptimization%20of%20the%20fine-grained%20classifiers%2C%20resulting%20in%20unpleasant%0Aperformance%20in%20real%20applications.%20Starting%20from%20the%20real-world%20conditions%20and%0Ato%20promote%20the%20practical%20progress%20of%20fine-grained%20visual%20categorization%2C%20we%0Apresent%20a%20Concept%20Drift%20and%20Long-Tailed%20Distribution%20dataset.%20Specifically%2C%20the%0Adataset%20is%20collected%20by%20gathering%2011195%20images%20of%20250%20instances%20in%20different%0Aspecies%20for%2047%20consecutive%20months%20in%20their%20natural%20contexts.%20The%20collection%0Aprocess%20involves%20dozens%20of%20crowd%20workers%20for%20photographing%20and%20domain%20experts%0Afor%20labeling.%20Meanwhile%2C%20we%20propose%20a%20feature%20recombination%20framework%20to%0Aaddress%20the%20learning%20challenges%20associated%20with%20CDLT.%20Experimental%20results%0Avalidate%20the%20efficacy%20of%20our%20method%20while%20also%20highlighting%20the%20limitations%20of%0Apopular%20large%20vision-language%20models%20%28e.g.%2C%20CLIP%29%20in%20the%20context%20of%20long-tailed%0Adistributions.%20This%20emphasizes%20the%20significance%20of%20CDLT%20as%20a%20benchmark%20for%0Ainvestigating%20these%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02346v2&entry.124074799=Read"},
{"title": "BehaviorGPT: Smart Agent Simulation for Autonomous Driving with\n  Next-Patch Prediction", "author": "Zikang Zhou and Haibo Hu and Xinhong Chen and Jianping Wang and Nan Guan and Kui Wu and Yung-Hui Li and Yu-Kai Huang and Chun Jason Xue", "abstract": "  Simulating realistic behaviors of traffic agents is pivotal for efficiently\nvalidating the safety of autonomous driving systems. Existing data-driven\nsimulators primarily use an encoder-decoder architecture to encode the\nhistorical trajectories before decoding the future. However, the heterogeneity\nbetween encoders and decoders complicates the models, and the manual separation\nof historical and future trajectories leads to low data utilization. Given\nthese limitations, we propose BehaviorGPT, a homogeneous and fully\nautoregressive Transformer designed to simulate the sequential behavior of\nmultiple agents. Crucially, our approach discards the traditional separation\nbetween \"history\" and \"future\" by modeling each time step as the \"current\" one\nfor motion generation, leading to a simpler, more parameter- and data-efficient\nagent simulator. We further introduce the Next-Patch Prediction Paradigm (NP3)\nto mitigate the negative effects of autoregressive modeling, in which models\nare trained to reason at the patch level of trajectories and capture long-range\nspatial-temporal interactions. Despite having merely 3M model parameters,\nBehaviorGPT won first place in the 2024 Waymo Open Sim Agents Challenge with a\nrealism score of 0.7473 and a minADE score of 1.4147, demonstrating its\nexceptional performance in traffic agent simulation.\n", "link": "http://arxiv.org/abs/2405.17372v3", "date": "2024-11-11", "relevancy": 2.1868, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5755}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5544}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BehaviorGPT%3A%20Smart%20Agent%20Simulation%20for%20Autonomous%20Driving%20with%0A%20%20Next-Patch%20Prediction&body=Title%3A%20BehaviorGPT%3A%20Smart%20Agent%20Simulation%20for%20Autonomous%20Driving%20with%0A%20%20Next-Patch%20Prediction%0AAuthor%3A%20Zikang%20Zhou%20and%20Haibo%20Hu%20and%20Xinhong%20Chen%20and%20Jianping%20Wang%20and%20Nan%20Guan%20and%20Kui%20Wu%20and%20Yung-Hui%20Li%20and%20Yu-Kai%20Huang%20and%20Chun%20Jason%20Xue%0AAbstract%3A%20%20%20Simulating%20realistic%20behaviors%20of%20traffic%20agents%20is%20pivotal%20for%20efficiently%0Avalidating%20the%20safety%20of%20autonomous%20driving%20systems.%20Existing%20data-driven%0Asimulators%20primarily%20use%20an%20encoder-decoder%20architecture%20to%20encode%20the%0Ahistorical%20trajectories%20before%20decoding%20the%20future.%20However%2C%20the%20heterogeneity%0Abetween%20encoders%20and%20decoders%20complicates%20the%20models%2C%20and%20the%20manual%20separation%0Aof%20historical%20and%20future%20trajectories%20leads%20to%20low%20data%20utilization.%20Given%0Athese%20limitations%2C%20we%20propose%20BehaviorGPT%2C%20a%20homogeneous%20and%20fully%0Aautoregressive%20Transformer%20designed%20to%20simulate%20the%20sequential%20behavior%20of%0Amultiple%20agents.%20Crucially%2C%20our%20approach%20discards%20the%20traditional%20separation%0Abetween%20%22history%22%20and%20%22future%22%20by%20modeling%20each%20time%20step%20as%20the%20%22current%22%20one%0Afor%20motion%20generation%2C%20leading%20to%20a%20simpler%2C%20more%20parameter-%20and%20data-efficient%0Aagent%20simulator.%20We%20further%20introduce%20the%20Next-Patch%20Prediction%20Paradigm%20%28NP3%29%0Ato%20mitigate%20the%20negative%20effects%20of%20autoregressive%20modeling%2C%20in%20which%20models%0Aare%20trained%20to%20reason%20at%20the%20patch%20level%20of%20trajectories%20and%20capture%20long-range%0Aspatial-temporal%20interactions.%20Despite%20having%20merely%203M%20model%20parameters%2C%0ABehaviorGPT%20won%20first%20place%20in%20the%202024%20Waymo%20Open%20Sim%20Agents%20Challenge%20with%20a%0Arealism%20score%20of%200.7473%20and%20a%20minADE%20score%20of%201.4147%2C%20demonstrating%20its%0Aexceptional%20performance%20in%20traffic%20agent%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17372v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehaviorGPT%253A%2520Smart%2520Agent%2520Simulation%2520for%2520Autonomous%2520Driving%2520with%250A%2520%2520Next-Patch%2520Prediction%26entry.906535625%3DZikang%2520Zhou%2520and%2520Haibo%2520Hu%2520and%2520Xinhong%2520Chen%2520and%2520Jianping%2520Wang%2520and%2520Nan%2520Guan%2520and%2520Kui%2520Wu%2520and%2520Yung-Hui%2520Li%2520and%2520Yu-Kai%2520Huang%2520and%2520Chun%2520Jason%2520Xue%26entry.1292438233%3D%2520%2520Simulating%2520realistic%2520behaviors%2520of%2520traffic%2520agents%2520is%2520pivotal%2520for%2520efficiently%250Avalidating%2520the%2520safety%2520of%2520autonomous%2520driving%2520systems.%2520Existing%2520data-driven%250Asimulators%2520primarily%2520use%2520an%2520encoder-decoder%2520architecture%2520to%2520encode%2520the%250Ahistorical%2520trajectories%2520before%2520decoding%2520the%2520future.%2520However%252C%2520the%2520heterogeneity%250Abetween%2520encoders%2520and%2520decoders%2520complicates%2520the%2520models%252C%2520and%2520the%2520manual%2520separation%250Aof%2520historical%2520and%2520future%2520trajectories%2520leads%2520to%2520low%2520data%2520utilization.%2520Given%250Athese%2520limitations%252C%2520we%2520propose%2520BehaviorGPT%252C%2520a%2520homogeneous%2520and%2520fully%250Aautoregressive%2520Transformer%2520designed%2520to%2520simulate%2520the%2520sequential%2520behavior%2520of%250Amultiple%2520agents.%2520Crucially%252C%2520our%2520approach%2520discards%2520the%2520traditional%2520separation%250Abetween%2520%2522history%2522%2520and%2520%2522future%2522%2520by%2520modeling%2520each%2520time%2520step%2520as%2520the%2520%2522current%2522%2520one%250Afor%2520motion%2520generation%252C%2520leading%2520to%2520a%2520simpler%252C%2520more%2520parameter-%2520and%2520data-efficient%250Aagent%2520simulator.%2520We%2520further%2520introduce%2520the%2520Next-Patch%2520Prediction%2520Paradigm%2520%2528NP3%2529%250Ato%2520mitigate%2520the%2520negative%2520effects%2520of%2520autoregressive%2520modeling%252C%2520in%2520which%2520models%250Aare%2520trained%2520to%2520reason%2520at%2520the%2520patch%2520level%2520of%2520trajectories%2520and%2520capture%2520long-range%250Aspatial-temporal%2520interactions.%2520Despite%2520having%2520merely%25203M%2520model%2520parameters%252C%250ABehaviorGPT%2520won%2520first%2520place%2520in%2520the%25202024%2520Waymo%2520Open%2520Sim%2520Agents%2520Challenge%2520with%2520a%250Arealism%2520score%2520of%25200.7473%2520and%2520a%2520minADE%2520score%2520of%25201.4147%252C%2520demonstrating%2520its%250Aexceptional%2520performance%2520in%2520traffic%2520agent%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17372v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BehaviorGPT%3A%20Smart%20Agent%20Simulation%20for%20Autonomous%20Driving%20with%0A%20%20Next-Patch%20Prediction&entry.906535625=Zikang%20Zhou%20and%20Haibo%20Hu%20and%20Xinhong%20Chen%20and%20Jianping%20Wang%20and%20Nan%20Guan%20and%20Kui%20Wu%20and%20Yung-Hui%20Li%20and%20Yu-Kai%20Huang%20and%20Chun%20Jason%20Xue&entry.1292438233=%20%20Simulating%20realistic%20behaviors%20of%20traffic%20agents%20is%20pivotal%20for%20efficiently%0Avalidating%20the%20safety%20of%20autonomous%20driving%20systems.%20Existing%20data-driven%0Asimulators%20primarily%20use%20an%20encoder-decoder%20architecture%20to%20encode%20the%0Ahistorical%20trajectories%20before%20decoding%20the%20future.%20However%2C%20the%20heterogeneity%0Abetween%20encoders%20and%20decoders%20complicates%20the%20models%2C%20and%20the%20manual%20separation%0Aof%20historical%20and%20future%20trajectories%20leads%20to%20low%20data%20utilization.%20Given%0Athese%20limitations%2C%20we%20propose%20BehaviorGPT%2C%20a%20homogeneous%20and%20fully%0Aautoregressive%20Transformer%20designed%20to%20simulate%20the%20sequential%20behavior%20of%0Amultiple%20agents.%20Crucially%2C%20our%20approach%20discards%20the%20traditional%20separation%0Abetween%20%22history%22%20and%20%22future%22%20by%20modeling%20each%20time%20step%20as%20the%20%22current%22%20one%0Afor%20motion%20generation%2C%20leading%20to%20a%20simpler%2C%20more%20parameter-%20and%20data-efficient%0Aagent%20simulator.%20We%20further%20introduce%20the%20Next-Patch%20Prediction%20Paradigm%20%28NP3%29%0Ato%20mitigate%20the%20negative%20effects%20of%20autoregressive%20modeling%2C%20in%20which%20models%0Aare%20trained%20to%20reason%20at%20the%20patch%20level%20of%20trajectories%20and%20capture%20long-range%0Aspatial-temporal%20interactions.%20Despite%20having%20merely%203M%20model%20parameters%2C%0ABehaviorGPT%20won%20first%20place%20in%20the%202024%20Waymo%20Open%20Sim%20Agents%20Challenge%20with%20a%0Arealism%20score%20of%200.7473%20and%20a%20minADE%20score%20of%201.4147%2C%20demonstrating%20its%0Aexceptional%20performance%20in%20traffic%20agent%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17372v3&entry.124074799=Read"},
{"title": "Meaningful Learning: Enhancing Abstract Reasoning in Large Language\n  Models via Generic Fact Guidance", "author": "Kai Xiong and Xiao Ding and Ting Liu and Bing Qin and Dongliang Xu and Qing Yang and Hongtao Liu and Yixin Cao", "abstract": "  Large language models (LLMs) have developed impressive performance and strong\nexplainability across various reasoning scenarios, marking a significant stride\ntowards mimicking human-like intelligence. Despite this, when tasked with\nseveral simple questions supported by a generic fact, LLMs often struggle to\nabstract and apply the generic fact to provide consistent and precise answers,\nrevealing a deficiency in abstract reasoning abilities. This has sparked a\nvigorous debate about whether LLMs are genuinely reasoning or merely\nmemorizing. In light of this, we design a preliminary study to quantify and\ndelve into the abstract reasoning abilities of existing LLMs. Our findings\nreveal a substantial discrepancy between their general reasoning and abstract\nreasoning performances. To relieve this problem, we tailor an abstract\nreasoning dataset (AbsR) together with a meaningful learning paradigm to teach\nLLMs how to leverage generic facts for reasoning purposes. The results show\nthat our approach not only boosts the general reasoning performance of LLMs but\nalso makes considerable strides towards their capacity for abstract reasoning,\nmoving beyond simple memorization or imitation to a more nuanced understanding\nand application of generic facts. The code is available at\nhttps://github.com/Waste-Wood/MeanLearn.\n", "link": "http://arxiv.org/abs/2403.09085v2", "date": "2024-11-11", "relevancy": 2.1815, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5531}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meaningful%20Learning%3A%20Enhancing%20Abstract%20Reasoning%20in%20Large%20Language%0A%20%20Models%20via%20Generic%20Fact%20Guidance&body=Title%3A%20Meaningful%20Learning%3A%20Enhancing%20Abstract%20Reasoning%20in%20Large%20Language%0A%20%20Models%20via%20Generic%20Fact%20Guidance%0AAuthor%3A%20Kai%20Xiong%20and%20Xiao%20Ding%20and%20Ting%20Liu%20and%20Bing%20Qin%20and%20Dongliang%20Xu%20and%20Qing%20Yang%20and%20Hongtao%20Liu%20and%20Yixin%20Cao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20developed%20impressive%20performance%20and%20strong%0Aexplainability%20across%20various%20reasoning%20scenarios%2C%20marking%20a%20significant%20stride%0Atowards%20mimicking%20human-like%20intelligence.%20Despite%20this%2C%20when%20tasked%20with%0Aseveral%20simple%20questions%20supported%20by%20a%20generic%20fact%2C%20LLMs%20often%20struggle%20to%0Aabstract%20and%20apply%20the%20generic%20fact%20to%20provide%20consistent%20and%20precise%20answers%2C%0Arevealing%20a%20deficiency%20in%20abstract%20reasoning%20abilities.%20This%20has%20sparked%20a%0Avigorous%20debate%20about%20whether%20LLMs%20are%20genuinely%20reasoning%20or%20merely%0Amemorizing.%20In%20light%20of%20this%2C%20we%20design%20a%20preliminary%20study%20to%20quantify%20and%0Adelve%20into%20the%20abstract%20reasoning%20abilities%20of%20existing%20LLMs.%20Our%20findings%0Areveal%20a%20substantial%20discrepancy%20between%20their%20general%20reasoning%20and%20abstract%0Areasoning%20performances.%20To%20relieve%20this%20problem%2C%20we%20tailor%20an%20abstract%0Areasoning%20dataset%20%28AbsR%29%20together%20with%20a%20meaningful%20learning%20paradigm%20to%20teach%0ALLMs%20how%20to%20leverage%20generic%20facts%20for%20reasoning%20purposes.%20The%20results%20show%0Athat%20our%20approach%20not%20only%20boosts%20the%20general%20reasoning%20performance%20of%20LLMs%20but%0Aalso%20makes%20considerable%20strides%20towards%20their%20capacity%20for%20abstract%20reasoning%2C%0Amoving%20beyond%20simple%20memorization%20or%20imitation%20to%20a%20more%20nuanced%20understanding%0Aand%20application%20of%20generic%20facts.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Waste-Wood/MeanLearn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09085v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeaningful%2520Learning%253A%2520Enhancing%2520Abstract%2520Reasoning%2520in%2520Large%2520Language%250A%2520%2520Models%2520via%2520Generic%2520Fact%2520Guidance%26entry.906535625%3DKai%2520Xiong%2520and%2520Xiao%2520Ding%2520and%2520Ting%2520Liu%2520and%2520Bing%2520Qin%2520and%2520Dongliang%2520Xu%2520and%2520Qing%2520Yang%2520and%2520Hongtao%2520Liu%2520and%2520Yixin%2520Cao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520developed%2520impressive%2520performance%2520and%2520strong%250Aexplainability%2520across%2520various%2520reasoning%2520scenarios%252C%2520marking%2520a%2520significant%2520stride%250Atowards%2520mimicking%2520human-like%2520intelligence.%2520Despite%2520this%252C%2520when%2520tasked%2520with%250Aseveral%2520simple%2520questions%2520supported%2520by%2520a%2520generic%2520fact%252C%2520LLMs%2520often%2520struggle%2520to%250Aabstract%2520and%2520apply%2520the%2520generic%2520fact%2520to%2520provide%2520consistent%2520and%2520precise%2520answers%252C%250Arevealing%2520a%2520deficiency%2520in%2520abstract%2520reasoning%2520abilities.%2520This%2520has%2520sparked%2520a%250Avigorous%2520debate%2520about%2520whether%2520LLMs%2520are%2520genuinely%2520reasoning%2520or%2520merely%250Amemorizing.%2520In%2520light%2520of%2520this%252C%2520we%2520design%2520a%2520preliminary%2520study%2520to%2520quantify%2520and%250Adelve%2520into%2520the%2520abstract%2520reasoning%2520abilities%2520of%2520existing%2520LLMs.%2520Our%2520findings%250Areveal%2520a%2520substantial%2520discrepancy%2520between%2520their%2520general%2520reasoning%2520and%2520abstract%250Areasoning%2520performances.%2520To%2520relieve%2520this%2520problem%252C%2520we%2520tailor%2520an%2520abstract%250Areasoning%2520dataset%2520%2528AbsR%2529%2520together%2520with%2520a%2520meaningful%2520learning%2520paradigm%2520to%2520teach%250ALLMs%2520how%2520to%2520leverage%2520generic%2520facts%2520for%2520reasoning%2520purposes.%2520The%2520results%2520show%250Athat%2520our%2520approach%2520not%2520only%2520boosts%2520the%2520general%2520reasoning%2520performance%2520of%2520LLMs%2520but%250Aalso%2520makes%2520considerable%2520strides%2520towards%2520their%2520capacity%2520for%2520abstract%2520reasoning%252C%250Amoving%2520beyond%2520simple%2520memorization%2520or%2520imitation%2520to%2520a%2520more%2520nuanced%2520understanding%250Aand%2520application%2520of%2520generic%2520facts.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Waste-Wood/MeanLearn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09085v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meaningful%20Learning%3A%20Enhancing%20Abstract%20Reasoning%20in%20Large%20Language%0A%20%20Models%20via%20Generic%20Fact%20Guidance&entry.906535625=Kai%20Xiong%20and%20Xiao%20Ding%20and%20Ting%20Liu%20and%20Bing%20Qin%20and%20Dongliang%20Xu%20and%20Qing%20Yang%20and%20Hongtao%20Liu%20and%20Yixin%20Cao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20developed%20impressive%20performance%20and%20strong%0Aexplainability%20across%20various%20reasoning%20scenarios%2C%20marking%20a%20significant%20stride%0Atowards%20mimicking%20human-like%20intelligence.%20Despite%20this%2C%20when%20tasked%20with%0Aseveral%20simple%20questions%20supported%20by%20a%20generic%20fact%2C%20LLMs%20often%20struggle%20to%0Aabstract%20and%20apply%20the%20generic%20fact%20to%20provide%20consistent%20and%20precise%20answers%2C%0Arevealing%20a%20deficiency%20in%20abstract%20reasoning%20abilities.%20This%20has%20sparked%20a%0Avigorous%20debate%20about%20whether%20LLMs%20are%20genuinely%20reasoning%20or%20merely%0Amemorizing.%20In%20light%20of%20this%2C%20we%20design%20a%20preliminary%20study%20to%20quantify%20and%0Adelve%20into%20the%20abstract%20reasoning%20abilities%20of%20existing%20LLMs.%20Our%20findings%0Areveal%20a%20substantial%20discrepancy%20between%20their%20general%20reasoning%20and%20abstract%0Areasoning%20performances.%20To%20relieve%20this%20problem%2C%20we%20tailor%20an%20abstract%0Areasoning%20dataset%20%28AbsR%29%20together%20with%20a%20meaningful%20learning%20paradigm%20to%20teach%0ALLMs%20how%20to%20leverage%20generic%20facts%20for%20reasoning%20purposes.%20The%20results%20show%0Athat%20our%20approach%20not%20only%20boosts%20the%20general%20reasoning%20performance%20of%20LLMs%20but%0Aalso%20makes%20considerable%20strides%20towards%20their%20capacity%20for%20abstract%20reasoning%2C%0Amoving%20beyond%20simple%20memorization%20or%20imitation%20to%20a%20more%20nuanced%20understanding%0Aand%20application%20of%20generic%20facts.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Waste-Wood/MeanLearn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09085v2&entry.124074799=Read"},
{"title": "Stochastic Newton Proximal Extragradient Method", "author": "Ruichen Jiang and Micha\u0142 Derezi\u0144ski and Aryan Mokhtari", "abstract": "  Stochastic second-order methods achieve fast local convergence in strongly\nconvex optimization by using noisy Hessian estimates to precondition the\ngradient. However, these methods typically reach superlinear convergence only\nwhen the stochastic Hessian noise diminishes, increasing per-iteration costs\nover time. Recent work in [arXiv:2204.09266] addressed this with a Hessian\naveraging scheme that achieves superlinear convergence without higher\nper-iteration costs. Nonetheless, the method has slow global convergence,\nrequiring up to $\\tilde{O}(\\kappa^2)$ iterations to reach the superlinear rate\nof $\\tilde{O}((1/t)^{t/2})$, where $\\kappa$ is the problem's condition number.\nIn this paper, we propose a novel stochastic Newton proximal extragradient\nmethod that improves these bounds, achieving a faster global linear rate and\nreaching the same fast superlinear rate in $\\tilde{O}(\\kappa)$ iterations. We\naccomplish this by extending the Hybrid Proximal Extragradient (HPE) framework,\nachieving fast global and local convergence rates for strongly convex functions\nwith access to a noisy Hessian oracle.\n", "link": "http://arxiv.org/abs/2406.01478v2", "date": "2024-11-11", "relevancy": 2.1773, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4455}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.433}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Newton%20Proximal%20Extragradient%20Method&body=Title%3A%20Stochastic%20Newton%20Proximal%20Extragradient%20Method%0AAuthor%3A%20Ruichen%20Jiang%20and%20Micha%C5%82%20Derezi%C5%84ski%20and%20Aryan%20Mokhtari%0AAbstract%3A%20%20%20Stochastic%20second-order%20methods%20achieve%20fast%20local%20convergence%20in%20strongly%0Aconvex%20optimization%20by%20using%20noisy%20Hessian%20estimates%20to%20precondition%20the%0Agradient.%20However%2C%20these%20methods%20typically%20reach%20superlinear%20convergence%20only%0Awhen%20the%20stochastic%20Hessian%20noise%20diminishes%2C%20increasing%20per-iteration%20costs%0Aover%20time.%20Recent%20work%20in%20%5BarXiv%3A2204.09266%5D%20addressed%20this%20with%20a%20Hessian%0Aaveraging%20scheme%20that%20achieves%20superlinear%20convergence%20without%20higher%0Aper-iteration%20costs.%20Nonetheless%2C%20the%20method%20has%20slow%20global%20convergence%2C%0Arequiring%20up%20to%20%24%5Ctilde%7BO%7D%28%5Ckappa%5E2%29%24%20iterations%20to%20reach%20the%20superlinear%20rate%0Aof%20%24%5Ctilde%7BO%7D%28%281/t%29%5E%7Bt/2%7D%29%24%2C%20where%20%24%5Ckappa%24%20is%20the%20problem%27s%20condition%20number.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20stochastic%20Newton%20proximal%20extragradient%0Amethod%20that%20improves%20these%20bounds%2C%20achieving%20a%20faster%20global%20linear%20rate%20and%0Areaching%20the%20same%20fast%20superlinear%20rate%20in%20%24%5Ctilde%7BO%7D%28%5Ckappa%29%24%20iterations.%20We%0Aaccomplish%20this%20by%20extending%20the%20Hybrid%20Proximal%20Extragradient%20%28HPE%29%20framework%2C%0Aachieving%20fast%20global%20and%20local%20convergence%20rates%20for%20strongly%20convex%20functions%0Awith%20access%20to%20a%20noisy%20Hessian%20oracle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01478v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Newton%2520Proximal%2520Extragradient%2520Method%26entry.906535625%3DRuichen%2520Jiang%2520and%2520Micha%25C5%2582%2520Derezi%25C5%2584ski%2520and%2520Aryan%2520Mokhtari%26entry.1292438233%3D%2520%2520Stochastic%2520second-order%2520methods%2520achieve%2520fast%2520local%2520convergence%2520in%2520strongly%250Aconvex%2520optimization%2520by%2520using%2520noisy%2520Hessian%2520estimates%2520to%2520precondition%2520the%250Agradient.%2520However%252C%2520these%2520methods%2520typically%2520reach%2520superlinear%2520convergence%2520only%250Awhen%2520the%2520stochastic%2520Hessian%2520noise%2520diminishes%252C%2520increasing%2520per-iteration%2520costs%250Aover%2520time.%2520Recent%2520work%2520in%2520%255BarXiv%253A2204.09266%255D%2520addressed%2520this%2520with%2520a%2520Hessian%250Aaveraging%2520scheme%2520that%2520achieves%2520superlinear%2520convergence%2520without%2520higher%250Aper-iteration%2520costs.%2520Nonetheless%252C%2520the%2520method%2520has%2520slow%2520global%2520convergence%252C%250Arequiring%2520up%2520to%2520%2524%255Ctilde%257BO%257D%2528%255Ckappa%255E2%2529%2524%2520iterations%2520to%2520reach%2520the%2520superlinear%2520rate%250Aof%2520%2524%255Ctilde%257BO%257D%2528%25281/t%2529%255E%257Bt/2%257D%2529%2524%252C%2520where%2520%2524%255Ckappa%2524%2520is%2520the%2520problem%2527s%2520condition%2520number.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520stochastic%2520Newton%2520proximal%2520extragradient%250Amethod%2520that%2520improves%2520these%2520bounds%252C%2520achieving%2520a%2520faster%2520global%2520linear%2520rate%2520and%250Areaching%2520the%2520same%2520fast%2520superlinear%2520rate%2520in%2520%2524%255Ctilde%257BO%257D%2528%255Ckappa%2529%2524%2520iterations.%2520We%250Aaccomplish%2520this%2520by%2520extending%2520the%2520Hybrid%2520Proximal%2520Extragradient%2520%2528HPE%2529%2520framework%252C%250Aachieving%2520fast%2520global%2520and%2520local%2520convergence%2520rates%2520for%2520strongly%2520convex%2520functions%250Awith%2520access%2520to%2520a%2520noisy%2520Hessian%2520oracle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01478v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Newton%20Proximal%20Extragradient%20Method&entry.906535625=Ruichen%20Jiang%20and%20Micha%C5%82%20Derezi%C5%84ski%20and%20Aryan%20Mokhtari&entry.1292438233=%20%20Stochastic%20second-order%20methods%20achieve%20fast%20local%20convergence%20in%20strongly%0Aconvex%20optimization%20by%20using%20noisy%20Hessian%20estimates%20to%20precondition%20the%0Agradient.%20However%2C%20these%20methods%20typically%20reach%20superlinear%20convergence%20only%0Awhen%20the%20stochastic%20Hessian%20noise%20diminishes%2C%20increasing%20per-iteration%20costs%0Aover%20time.%20Recent%20work%20in%20%5BarXiv%3A2204.09266%5D%20addressed%20this%20with%20a%20Hessian%0Aaveraging%20scheme%20that%20achieves%20superlinear%20convergence%20without%20higher%0Aper-iteration%20costs.%20Nonetheless%2C%20the%20method%20has%20slow%20global%20convergence%2C%0Arequiring%20up%20to%20%24%5Ctilde%7BO%7D%28%5Ckappa%5E2%29%24%20iterations%20to%20reach%20the%20superlinear%20rate%0Aof%20%24%5Ctilde%7BO%7D%28%281/t%29%5E%7Bt/2%7D%29%24%2C%20where%20%24%5Ckappa%24%20is%20the%20problem%27s%20condition%20number.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20stochastic%20Newton%20proximal%20extragradient%0Amethod%20that%20improves%20these%20bounds%2C%20achieving%20a%20faster%20global%20linear%20rate%20and%0Areaching%20the%20same%20fast%20superlinear%20rate%20in%20%24%5Ctilde%7BO%7D%28%5Ckappa%29%24%20iterations.%20We%0Aaccomplish%20this%20by%20extending%20the%20Hybrid%20Proximal%20Extragradient%20%28HPE%29%20framework%2C%0Aachieving%20fast%20global%20and%20local%20convergence%20rates%20for%20strongly%20convex%20functions%0Awith%20access%20to%20a%20noisy%20Hessian%20oracle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01478v2&entry.124074799=Read"},
{"title": "Comparing Bottom-Up and Top-Down Steering Approaches on In-Context\n  Learning Tasks", "author": "Madeline Brumley and Joe Kwon and David Krueger and Dmitrii Krasheninnikov and Usman Anwar", "abstract": "  A key objective of interpretability research on large language models (LLMs)\nis to develop methods for robustly steering models toward desired behaviors. To\nthis end, two distinct approaches to interpretability -- ``bottom-up\" and\n``top-down\" -- have been presented, but there has been little quantitative\ncomparison between them. We present a case study comparing the effectiveness of\nrepresentative vector steering methods from each branch: function vectors (FV;\narXiv:2310.15213), as a bottom-up method, and in-context vectors (ICV;\narXiv:2311.06668) as a top-down method. While both aim to capture compact\nrepresentations of broad in-context learning tasks, we find they are effective\nonly on specific types of tasks: ICVs outperform FVs in behavioral shifting,\nwhereas FVs excel in tasks requiring more precision. We discuss the\nimplications for future evaluations of steering methods and for further\nresearch into top-down and bottom-up steering given these findings.\n", "link": "http://arxiv.org/abs/2411.07213v1", "date": "2024-11-11", "relevancy": 2.1705, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Bottom-Up%20and%20Top-Down%20Steering%20Approaches%20on%20In-Context%0A%20%20Learning%20Tasks&body=Title%3A%20Comparing%20Bottom-Up%20and%20Top-Down%20Steering%20Approaches%20on%20In-Context%0A%20%20Learning%20Tasks%0AAuthor%3A%20Madeline%20Brumley%20and%20Joe%20Kwon%20and%20David%20Krueger%20and%20Dmitrii%20Krasheninnikov%20and%20Usman%20Anwar%0AAbstract%3A%20%20%20A%20key%20objective%20of%20interpretability%20research%20on%20large%20language%20models%20%28LLMs%29%0Ais%20to%20develop%20methods%20for%20robustly%20steering%20models%20toward%20desired%20behaviors.%20To%0Athis%20end%2C%20two%20distinct%20approaches%20to%20interpretability%20--%20%60%60bottom-up%22%20and%0A%60%60top-down%22%20--%20have%20been%20presented%2C%20but%20there%20has%20been%20little%20quantitative%0Acomparison%20between%20them.%20We%20present%20a%20case%20study%20comparing%20the%20effectiveness%20of%0Arepresentative%20vector%20steering%20methods%20from%20each%20branch%3A%20function%20vectors%20%28FV%3B%0AarXiv%3A2310.15213%29%2C%20as%20a%20bottom-up%20method%2C%20and%20in-context%20vectors%20%28ICV%3B%0AarXiv%3A2311.06668%29%20as%20a%20top-down%20method.%20While%20both%20aim%20to%20capture%20compact%0Arepresentations%20of%20broad%20in-context%20learning%20tasks%2C%20we%20find%20they%20are%20effective%0Aonly%20on%20specific%20types%20of%20tasks%3A%20ICVs%20outperform%20FVs%20in%20behavioral%20shifting%2C%0Awhereas%20FVs%20excel%20in%20tasks%20requiring%20more%20precision.%20We%20discuss%20the%0Aimplications%20for%20future%20evaluations%20of%20steering%20methods%20and%20for%20further%0Aresearch%20into%20top-down%20and%20bottom-up%20steering%20given%20these%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Bottom-Up%2520and%2520Top-Down%2520Steering%2520Approaches%2520on%2520In-Context%250A%2520%2520Learning%2520Tasks%26entry.906535625%3DMadeline%2520Brumley%2520and%2520Joe%2520Kwon%2520and%2520David%2520Krueger%2520and%2520Dmitrii%2520Krasheninnikov%2520and%2520Usman%2520Anwar%26entry.1292438233%3D%2520%2520A%2520key%2520objective%2520of%2520interpretability%2520research%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%250Ais%2520to%2520develop%2520methods%2520for%2520robustly%2520steering%2520models%2520toward%2520desired%2520behaviors.%2520To%250Athis%2520end%252C%2520two%2520distinct%2520approaches%2520to%2520interpretability%2520--%2520%2560%2560bottom-up%2522%2520and%250A%2560%2560top-down%2522%2520--%2520have%2520been%2520presented%252C%2520but%2520there%2520has%2520been%2520little%2520quantitative%250Acomparison%2520between%2520them.%2520We%2520present%2520a%2520case%2520study%2520comparing%2520the%2520effectiveness%2520of%250Arepresentative%2520vector%2520steering%2520methods%2520from%2520each%2520branch%253A%2520function%2520vectors%2520%2528FV%253B%250AarXiv%253A2310.15213%2529%252C%2520as%2520a%2520bottom-up%2520method%252C%2520and%2520in-context%2520vectors%2520%2528ICV%253B%250AarXiv%253A2311.06668%2529%2520as%2520a%2520top-down%2520method.%2520While%2520both%2520aim%2520to%2520capture%2520compact%250Arepresentations%2520of%2520broad%2520in-context%2520learning%2520tasks%252C%2520we%2520find%2520they%2520are%2520effective%250Aonly%2520on%2520specific%2520types%2520of%2520tasks%253A%2520ICVs%2520outperform%2520FVs%2520in%2520behavioral%2520shifting%252C%250Awhereas%2520FVs%2520excel%2520in%2520tasks%2520requiring%2520more%2520precision.%2520We%2520discuss%2520the%250Aimplications%2520for%2520future%2520evaluations%2520of%2520steering%2520methods%2520and%2520for%2520further%250Aresearch%2520into%2520top-down%2520and%2520bottom-up%2520steering%2520given%2520these%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Bottom-Up%20and%20Top-Down%20Steering%20Approaches%20on%20In-Context%0A%20%20Learning%20Tasks&entry.906535625=Madeline%20Brumley%20and%20Joe%20Kwon%20and%20David%20Krueger%20and%20Dmitrii%20Krasheninnikov%20and%20Usman%20Anwar&entry.1292438233=%20%20A%20key%20objective%20of%20interpretability%20research%20on%20large%20language%20models%20%28LLMs%29%0Ais%20to%20develop%20methods%20for%20robustly%20steering%20models%20toward%20desired%20behaviors.%20To%0Athis%20end%2C%20two%20distinct%20approaches%20to%20interpretability%20--%20%60%60bottom-up%22%20and%0A%60%60top-down%22%20--%20have%20been%20presented%2C%20but%20there%20has%20been%20little%20quantitative%0Acomparison%20between%20them.%20We%20present%20a%20case%20study%20comparing%20the%20effectiveness%20of%0Arepresentative%20vector%20steering%20methods%20from%20each%20branch%3A%20function%20vectors%20%28FV%3B%0AarXiv%3A2310.15213%29%2C%20as%20a%20bottom-up%20method%2C%20and%20in-context%20vectors%20%28ICV%3B%0AarXiv%3A2311.06668%29%20as%20a%20top-down%20method.%20While%20both%20aim%20to%20capture%20compact%0Arepresentations%20of%20broad%20in-context%20learning%20tasks%2C%20we%20find%20they%20are%20effective%0Aonly%20on%20specific%20types%20of%20tasks%3A%20ICVs%20outperform%20FVs%20in%20behavioral%20shifting%2C%0Awhereas%20FVs%20excel%20in%20tasks%20requiring%20more%20precision.%20We%20discuss%20the%0Aimplications%20for%20future%20evaluations%20of%20steering%20methods%20and%20for%20further%0Aresearch%20into%20top-down%20and%20bottom-up%20steering%20given%20these%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07213v1&entry.124074799=Read"},
{"title": "Spatially Constrained Transformer with Efficient Global Relation\n  Modelling for Spatio-Temporal Prediction", "author": "Ashutosh Sao and Simon Gottschalk", "abstract": "  Accurate spatio-temporal prediction is crucial for the sustainable\ndevelopment of smart cities. However, current approaches often struggle to\ncapture important spatio-temporal relationships, particularly overlooking\nglobal relations among distant city regions. Most existing techniques\npredominantly rely on Convolutional Neural Networks (CNNs) to capture global\nrelations. However, CNNs exhibit neighbourhood bias, making them insufficient\nfor capturing distant relations. To address this limitation, we propose\nST-SampleNet, a novel transformer-based architecture that combines CNNs with\nself-attention mechanisms to capture both local and global relations\neffectively. Moreover, as the number of regions increases, the quadratic\ncomplexity of self-attention becomes a challenge. To tackle this issue, we\nintroduce a lightweight region sampling strategy that prunes non-essential\nregions and enhances the efficiency of our approach. Furthermore, we introduce\na spatially constrained position embedding that incorporates spatial\nneighbourhood information into the self-attention mechanism, aiding in semantic\ninterpretation and improving the performance of ST-SampleNet. Our experimental\nevaluation on three real-world datasets demonstrates the effectiveness of\nST-SampleNet. Additionally, our efficient variant achieves a 40% reduction in\ncomputational costs with only a marginal compromise in performance,\napproximately 1%.\n", "link": "http://arxiv.org/abs/2411.06836v1", "date": "2024-11-11", "relevancy": 2.1694, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5828}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5398}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatially%20Constrained%20Transformer%20with%20Efficient%20Global%20Relation%0A%20%20Modelling%20for%20Spatio-Temporal%20Prediction&body=Title%3A%20Spatially%20Constrained%20Transformer%20with%20Efficient%20Global%20Relation%0A%20%20Modelling%20for%20Spatio-Temporal%20Prediction%0AAuthor%3A%20Ashutosh%20Sao%20and%20Simon%20Gottschalk%0AAbstract%3A%20%20%20Accurate%20spatio-temporal%20prediction%20is%20crucial%20for%20the%20sustainable%0Adevelopment%20of%20smart%20cities.%20However%2C%20current%20approaches%20often%20struggle%20to%0Acapture%20important%20spatio-temporal%20relationships%2C%20particularly%20overlooking%0Aglobal%20relations%20among%20distant%20city%20regions.%20Most%20existing%20techniques%0Apredominantly%20rely%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20to%20capture%20global%0Arelations.%20However%2C%20CNNs%20exhibit%20neighbourhood%20bias%2C%20making%20them%20insufficient%0Afor%20capturing%20distant%20relations.%20To%20address%20this%20limitation%2C%20we%20propose%0AST-SampleNet%2C%20a%20novel%20transformer-based%20architecture%20that%20combines%20CNNs%20with%0Aself-attention%20mechanisms%20to%20capture%20both%20local%20and%20global%20relations%0Aeffectively.%20Moreover%2C%20as%20the%20number%20of%20regions%20increases%2C%20the%20quadratic%0Acomplexity%20of%20self-attention%20becomes%20a%20challenge.%20To%20tackle%20this%20issue%2C%20we%0Aintroduce%20a%20lightweight%20region%20sampling%20strategy%20that%20prunes%20non-essential%0Aregions%20and%20enhances%20the%20efficiency%20of%20our%20approach.%20Furthermore%2C%20we%20introduce%0Aa%20spatially%20constrained%20position%20embedding%20that%20incorporates%20spatial%0Aneighbourhood%20information%20into%20the%20self-attention%20mechanism%2C%20aiding%20in%20semantic%0Ainterpretation%20and%20improving%20the%20performance%20of%20ST-SampleNet.%20Our%20experimental%0Aevaluation%20on%20three%20real-world%20datasets%20demonstrates%20the%20effectiveness%20of%0AST-SampleNet.%20Additionally%2C%20our%20efficient%20variant%20achieves%20a%2040%25%20reduction%20in%0Acomputational%20costs%20with%20only%20a%20marginal%20compromise%20in%20performance%2C%0Aapproximately%201%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatially%2520Constrained%2520Transformer%2520with%2520Efficient%2520Global%2520Relation%250A%2520%2520Modelling%2520for%2520Spatio-Temporal%2520Prediction%26entry.906535625%3DAshutosh%2520Sao%2520and%2520Simon%2520Gottschalk%26entry.1292438233%3D%2520%2520Accurate%2520spatio-temporal%2520prediction%2520is%2520crucial%2520for%2520the%2520sustainable%250Adevelopment%2520of%2520smart%2520cities.%2520However%252C%2520current%2520approaches%2520often%2520struggle%2520to%250Acapture%2520important%2520spatio-temporal%2520relationships%252C%2520particularly%2520overlooking%250Aglobal%2520relations%2520among%2520distant%2520city%2520regions.%2520Most%2520existing%2520techniques%250Apredominantly%2520rely%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520to%2520capture%2520global%250Arelations.%2520However%252C%2520CNNs%2520exhibit%2520neighbourhood%2520bias%252C%2520making%2520them%2520insufficient%250Afor%2520capturing%2520distant%2520relations.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250AST-SampleNet%252C%2520a%2520novel%2520transformer-based%2520architecture%2520that%2520combines%2520CNNs%2520with%250Aself-attention%2520mechanisms%2520to%2520capture%2520both%2520local%2520and%2520global%2520relations%250Aeffectively.%2520Moreover%252C%2520as%2520the%2520number%2520of%2520regions%2520increases%252C%2520the%2520quadratic%250Acomplexity%2520of%2520self-attention%2520becomes%2520a%2520challenge.%2520To%2520tackle%2520this%2520issue%252C%2520we%250Aintroduce%2520a%2520lightweight%2520region%2520sampling%2520strategy%2520that%2520prunes%2520non-essential%250Aregions%2520and%2520enhances%2520the%2520efficiency%2520of%2520our%2520approach.%2520Furthermore%252C%2520we%2520introduce%250Aa%2520spatially%2520constrained%2520position%2520embedding%2520that%2520incorporates%2520spatial%250Aneighbourhood%2520information%2520into%2520the%2520self-attention%2520mechanism%252C%2520aiding%2520in%2520semantic%250Ainterpretation%2520and%2520improving%2520the%2520performance%2520of%2520ST-SampleNet.%2520Our%2520experimental%250Aevaluation%2520on%2520three%2520real-world%2520datasets%2520demonstrates%2520the%2520effectiveness%2520of%250AST-SampleNet.%2520Additionally%252C%2520our%2520efficient%2520variant%2520achieves%2520a%252040%2525%2520reduction%2520in%250Acomputational%2520costs%2520with%2520only%2520a%2520marginal%2520compromise%2520in%2520performance%252C%250Aapproximately%25201%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatially%20Constrained%20Transformer%20with%20Efficient%20Global%20Relation%0A%20%20Modelling%20for%20Spatio-Temporal%20Prediction&entry.906535625=Ashutosh%20Sao%20and%20Simon%20Gottschalk&entry.1292438233=%20%20Accurate%20spatio-temporal%20prediction%20is%20crucial%20for%20the%20sustainable%0Adevelopment%20of%20smart%20cities.%20However%2C%20current%20approaches%20often%20struggle%20to%0Acapture%20important%20spatio-temporal%20relationships%2C%20particularly%20overlooking%0Aglobal%20relations%20among%20distant%20city%20regions.%20Most%20existing%20techniques%0Apredominantly%20rely%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20to%20capture%20global%0Arelations.%20However%2C%20CNNs%20exhibit%20neighbourhood%20bias%2C%20making%20them%20insufficient%0Afor%20capturing%20distant%20relations.%20To%20address%20this%20limitation%2C%20we%20propose%0AST-SampleNet%2C%20a%20novel%20transformer-based%20architecture%20that%20combines%20CNNs%20with%0Aself-attention%20mechanisms%20to%20capture%20both%20local%20and%20global%20relations%0Aeffectively.%20Moreover%2C%20as%20the%20number%20of%20regions%20increases%2C%20the%20quadratic%0Acomplexity%20of%20self-attention%20becomes%20a%20challenge.%20To%20tackle%20this%20issue%2C%20we%0Aintroduce%20a%20lightweight%20region%20sampling%20strategy%20that%20prunes%20non-essential%0Aregions%20and%20enhances%20the%20efficiency%20of%20our%20approach.%20Furthermore%2C%20we%20introduce%0Aa%20spatially%20constrained%20position%20embedding%20that%20incorporates%20spatial%0Aneighbourhood%20information%20into%20the%20self-attention%20mechanism%2C%20aiding%20in%20semantic%0Ainterpretation%20and%20improving%20the%20performance%20of%20ST-SampleNet.%20Our%20experimental%0Aevaluation%20on%20three%20real-world%20datasets%20demonstrates%20the%20effectiveness%20of%0AST-SampleNet.%20Additionally%2C%20our%20efficient%20variant%20achieves%20a%2040%25%20reduction%20in%0Acomputational%20costs%20with%20only%20a%20marginal%20compromise%20in%20performance%2C%0Aapproximately%201%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06836v1&entry.124074799=Read"},
{"title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training", "author": "Elia Cunegatti and Leonardo Lucio Custode and Giovanni Iacca", "abstract": "  Network pruning is a set of computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has focused on pruning and re-training, which\nnowadays is inconvenient due to the vast amount of pre-trained models, which\nare in any case too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAl}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs, that\nmodifies the block-wise and row-wise sparsity ratios to maximize the\n\\emph{neuron alignment} among activations. Moreover, differently from existing\nmethods, our approach adaptively selects the best parameters for the block-wise\nand row-wise sparsity ratios w.r.t. to the model and the desired sparsity\n(given as input), and requires \\emph{no re-training}. We test our method on 4\ndifferent LLM families and 3 different sparsity ratios, showing how it\nconsistently outperforms the latest state-of-the-art techniques. The code is\navailable at https://github.com/eliacunegatti/NeuroAL.\n", "link": "http://arxiv.org/abs/2411.07066v1", "date": "2024-11-11", "relevancy": 2.1682, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.437}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4346}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zeroth-Order%20Adaptive%20Neuron%20Alignment%20Based%20Pruning%20without%20Re-Training&body=Title%3A%20Zeroth-Order%20Adaptive%20Neuron%20Alignment%20Based%20Pruning%20without%20Re-Training%0AAuthor%3A%20Elia%20Cunegatti%20and%20Leonardo%20Lucio%20Custode%20and%20Giovanni%20Iacca%0AAbstract%3A%20%20%20Network%20pruning%20is%20a%20set%20of%20computational%20techniques%20that%20aim%20to%20reduce%20a%0Agiven%20model%27s%20computational%20cost%20by%20removing%20a%20subset%20of%20its%20parameters%20while%0Ahaving%20minimal%20impact%20on%20performance.%20Throughout%20the%20last%20decade%2C%20the%20most%0Awidely%20used%20pruning%20paradigm%20has%20focused%20on%20pruning%20and%20re-training%2C%20which%0Anowadays%20is%20inconvenient%20due%20to%20the%20vast%20amount%20of%20pre-trained%20models%2C%20which%0Aare%20in%20any%20case%20too%20expensive%20to%20re-train.%20In%20this%20paper%2C%20we%20exploit%20functional%0Ainformation%20from%20dense%20pre-trained%20models%2C%20i.e.%2C%20their%20activations%2C%20to%20obtain%0Asparse%20models%20that%20maximize%20the%20activations%27%20alignment%20w.r.t.%20their%0Acorresponding%20dense%20models.%20Hence%2C%20we%20propose%20%5Ctextsc%7BNeuroAl%7D%2C%20a%20%5Cemph%7Btop-up%7D%0Aalgorithm%20that%20can%20be%20used%20on%20top%20of%20any%20given%20pruning%20algorithm%20for%20LLMs%2C%20that%0Amodifies%20the%20block-wise%20and%20row-wise%20sparsity%20ratios%20to%20maximize%20the%0A%5Cemph%7Bneuron%20alignment%7D%20among%20activations.%20Moreover%2C%20differently%20from%20existing%0Amethods%2C%20our%20approach%20adaptively%20selects%20the%20best%20parameters%20for%20the%20block-wise%0Aand%20row-wise%20sparsity%20ratios%20w.r.t.%20to%20the%20model%20and%20the%20desired%20sparsity%0A%28given%20as%20input%29%2C%20and%20requires%20%5Cemph%7Bno%20re-training%7D.%20We%20test%20our%20method%20on%204%0Adifferent%20LLM%20families%20and%203%20different%20sparsity%20ratios%2C%20showing%20how%20it%0Aconsistently%20outperforms%20the%20latest%20state-of-the-art%20techniques.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/eliacunegatti/NeuroAL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZeroth-Order%2520Adaptive%2520Neuron%2520Alignment%2520Based%2520Pruning%2520without%2520Re-Training%26entry.906535625%3DElia%2520Cunegatti%2520and%2520Leonardo%2520Lucio%2520Custode%2520and%2520Giovanni%2520Iacca%26entry.1292438233%3D%2520%2520Network%2520pruning%2520is%2520a%2520set%2520of%2520computational%2520techniques%2520that%2520aim%2520to%2520reduce%2520a%250Agiven%2520model%2527s%2520computational%2520cost%2520by%2520removing%2520a%2520subset%2520of%2520its%2520parameters%2520while%250Ahaving%2520minimal%2520impact%2520on%2520performance.%2520Throughout%2520the%2520last%2520decade%252C%2520the%2520most%250Awidely%2520used%2520pruning%2520paradigm%2520has%2520focused%2520on%2520pruning%2520and%2520re-training%252C%2520which%250Anowadays%2520is%2520inconvenient%2520due%2520to%2520the%2520vast%2520amount%2520of%2520pre-trained%2520models%252C%2520which%250Aare%2520in%2520any%2520case%2520too%2520expensive%2520to%2520re-train.%2520In%2520this%2520paper%252C%2520we%2520exploit%2520functional%250Ainformation%2520from%2520dense%2520pre-trained%2520models%252C%2520i.e.%252C%2520their%2520activations%252C%2520to%2520obtain%250Asparse%2520models%2520that%2520maximize%2520the%2520activations%2527%2520alignment%2520w.r.t.%2520their%250Acorresponding%2520dense%2520models.%2520Hence%252C%2520we%2520propose%2520%255Ctextsc%257BNeuroAl%257D%252C%2520a%2520%255Cemph%257Btop-up%257D%250Aalgorithm%2520that%2520can%2520be%2520used%2520on%2520top%2520of%2520any%2520given%2520pruning%2520algorithm%2520for%2520LLMs%252C%2520that%250Amodifies%2520the%2520block-wise%2520and%2520row-wise%2520sparsity%2520ratios%2520to%2520maximize%2520the%250A%255Cemph%257Bneuron%2520alignment%257D%2520among%2520activations.%2520Moreover%252C%2520differently%2520from%2520existing%250Amethods%252C%2520our%2520approach%2520adaptively%2520selects%2520the%2520best%2520parameters%2520for%2520the%2520block-wise%250Aand%2520row-wise%2520sparsity%2520ratios%2520w.r.t.%2520to%2520the%2520model%2520and%2520the%2520desired%2520sparsity%250A%2528given%2520as%2520input%2529%252C%2520and%2520requires%2520%255Cemph%257Bno%2520re-training%257D.%2520We%2520test%2520our%2520method%2520on%25204%250Adifferent%2520LLM%2520families%2520and%25203%2520different%2520sparsity%2520ratios%252C%2520showing%2520how%2520it%250Aconsistently%2520outperforms%2520the%2520latest%2520state-of-the-art%2520techniques.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/eliacunegatti/NeuroAL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zeroth-Order%20Adaptive%20Neuron%20Alignment%20Based%20Pruning%20without%20Re-Training&entry.906535625=Elia%20Cunegatti%20and%20Leonardo%20Lucio%20Custode%20and%20Giovanni%20Iacca&entry.1292438233=%20%20Network%20pruning%20is%20a%20set%20of%20computational%20techniques%20that%20aim%20to%20reduce%20a%0Agiven%20model%27s%20computational%20cost%20by%20removing%20a%20subset%20of%20its%20parameters%20while%0Ahaving%20minimal%20impact%20on%20performance.%20Throughout%20the%20last%20decade%2C%20the%20most%0Awidely%20used%20pruning%20paradigm%20has%20focused%20on%20pruning%20and%20re-training%2C%20which%0Anowadays%20is%20inconvenient%20due%20to%20the%20vast%20amount%20of%20pre-trained%20models%2C%20which%0Aare%20in%20any%20case%20too%20expensive%20to%20re-train.%20In%20this%20paper%2C%20we%20exploit%20functional%0Ainformation%20from%20dense%20pre-trained%20models%2C%20i.e.%2C%20their%20activations%2C%20to%20obtain%0Asparse%20models%20that%20maximize%20the%20activations%27%20alignment%20w.r.t.%20their%0Acorresponding%20dense%20models.%20Hence%2C%20we%20propose%20%5Ctextsc%7BNeuroAl%7D%2C%20a%20%5Cemph%7Btop-up%7D%0Aalgorithm%20that%20can%20be%20used%20on%20top%20of%20any%20given%20pruning%20algorithm%20for%20LLMs%2C%20that%0Amodifies%20the%20block-wise%20and%20row-wise%20sparsity%20ratios%20to%20maximize%20the%0A%5Cemph%7Bneuron%20alignment%7D%20among%20activations.%20Moreover%2C%20differently%20from%20existing%0Amethods%2C%20our%20approach%20adaptively%20selects%20the%20best%20parameters%20for%20the%20block-wise%0Aand%20row-wise%20sparsity%20ratios%20w.r.t.%20to%20the%20model%20and%20the%20desired%20sparsity%0A%28given%20as%20input%29%2C%20and%20requires%20%5Cemph%7Bno%20re-training%7D.%20We%20test%20our%20method%20on%204%0Adifferent%20LLM%20families%20and%203%20different%20sparsity%20ratios%2C%20showing%20how%20it%0Aconsistently%20outperforms%20the%20latest%20state-of-the-art%20techniques.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/eliacunegatti/NeuroAL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07066v1&entry.124074799=Read"},
{"title": "Exploiting Precision Mapping and Component-Specific Feature Enhancement\n  for Breast Cancer Segmentation and Identification", "author": "Pandiyaraju V and Shravan Venkatraman and Pavan Kumar S and Santhosh Malarvannan and Kannan A", "abstract": "  Breast cancer is a leading cause of mortality worldwide, and demands the\ncritical need for early and accurate diagnostic tools. Ultrasound imaging is a\nwidely used modality for breast cancer screening, yet the precise segmentation\nand classification of tumors in these images are challenging due to variations\nin tumor morphology and image quality. To address these challenges, we propose\nnovel deep learning (DL) frameworks leveraging a precision mapping mechanism\n(PMM) along with a component-specific feature enhancement module (CSFEM) to\nimprove breast cancer lesion segmentation and identification. Our PPM ensures\nthat the segmentation accurately reflects the true shape and extent of the\ntumor by meticulously delineating their boundaries. The CSFEM focuses on\nextracting and amplifying features unique to different tumor types, enabling\nthe model to effectively distinguish between benign, malignant, and normal\ntissues. Integrating PMM and CSFEM into our segmentation model yielded an\naccuracy of 98.1%, an IoU of 96.9%, and a Dice Coefficient of 97.2%. Similarly,\nour classification model achieved an accuracy of 99.2%, with F1-score,\nprecision, and recall values of 99.1%, 99.3%, and 99.1%, respectively. Our\nresults indicate significant improvement in evaluation metrics in comparison to\nstate-of-the-art (SOTA) models, demonstrating the effectiveness of precision\nmapping and component-specific feature enhancement in advancing breast cancer\nlesion analysis.\n", "link": "http://arxiv.org/abs/2407.02844v4", "date": "2024-11-11", "relevancy": 2.1672, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5621}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.542}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Precision%20Mapping%20and%20Component-Specific%20Feature%20Enhancement%0A%20%20for%20Breast%20Cancer%20Segmentation%20and%20Identification&body=Title%3A%20Exploiting%20Precision%20Mapping%20and%20Component-Specific%20Feature%20Enhancement%0A%20%20for%20Breast%20Cancer%20Segmentation%20and%20Identification%0AAuthor%3A%20Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Pavan%20Kumar%20S%20and%20Santhosh%20Malarvannan%20and%20Kannan%20A%0AAbstract%3A%20%20%20Breast%20cancer%20is%20a%20leading%20cause%20of%20mortality%20worldwide%2C%20and%20demands%20the%0Acritical%20need%20for%20early%20and%20accurate%20diagnostic%20tools.%20Ultrasound%20imaging%20is%20a%0Awidely%20used%20modality%20for%20breast%20cancer%20screening%2C%20yet%20the%20precise%20segmentation%0Aand%20classification%20of%20tumors%20in%20these%20images%20are%20challenging%20due%20to%20variations%0Ain%20tumor%20morphology%20and%20image%20quality.%20To%20address%20these%20challenges%2C%20we%20propose%0Anovel%20deep%20learning%20%28DL%29%20frameworks%20leveraging%20a%20precision%20mapping%20mechanism%0A%28PMM%29%20along%20with%20a%20component-specific%20feature%20enhancement%20module%20%28CSFEM%29%20to%0Aimprove%20breast%20cancer%20lesion%20segmentation%20and%20identification.%20Our%20PPM%20ensures%0Athat%20the%20segmentation%20accurately%20reflects%20the%20true%20shape%20and%20extent%20of%20the%0Atumor%20by%20meticulously%20delineating%20their%20boundaries.%20The%20CSFEM%20focuses%20on%0Aextracting%20and%20amplifying%20features%20unique%20to%20different%20tumor%20types%2C%20enabling%0Athe%20model%20to%20effectively%20distinguish%20between%20benign%2C%20malignant%2C%20and%20normal%0Atissues.%20Integrating%20PMM%20and%20CSFEM%20into%20our%20segmentation%20model%20yielded%20an%0Aaccuracy%20of%2098.1%25%2C%20an%20IoU%20of%2096.9%25%2C%20and%20a%20Dice%20Coefficient%20of%2097.2%25.%20Similarly%2C%0Aour%20classification%20model%20achieved%20an%20accuracy%20of%2099.2%25%2C%20with%20F1-score%2C%0Aprecision%2C%20and%20recall%20values%20of%2099.1%25%2C%2099.3%25%2C%20and%2099.1%25%2C%20respectively.%20Our%0Aresults%20indicate%20significant%20improvement%20in%20evaluation%20metrics%20in%20comparison%20to%0Astate-of-the-art%20%28SOTA%29%20models%2C%20demonstrating%20the%20effectiveness%20of%20precision%0Amapping%20and%20component-specific%20feature%20enhancement%20in%20advancing%20breast%20cancer%0Alesion%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02844v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Precision%2520Mapping%2520and%2520Component-Specific%2520Feature%2520Enhancement%250A%2520%2520for%2520Breast%2520Cancer%2520Segmentation%2520and%2520Identification%26entry.906535625%3DPandiyaraju%2520V%2520and%2520Shravan%2520Venkatraman%2520and%2520Pavan%2520Kumar%2520S%2520and%2520Santhosh%2520Malarvannan%2520and%2520Kannan%2520A%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520is%2520a%2520leading%2520cause%2520of%2520mortality%2520worldwide%252C%2520and%2520demands%2520the%250Acritical%2520need%2520for%2520early%2520and%2520accurate%2520diagnostic%2520tools.%2520Ultrasound%2520imaging%2520is%2520a%250Awidely%2520used%2520modality%2520for%2520breast%2520cancer%2520screening%252C%2520yet%2520the%2520precise%2520segmentation%250Aand%2520classification%2520of%2520tumors%2520in%2520these%2520images%2520are%2520challenging%2520due%2520to%2520variations%250Ain%2520tumor%2520morphology%2520and%2520image%2520quality.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250Anovel%2520deep%2520learning%2520%2528DL%2529%2520frameworks%2520leveraging%2520a%2520precision%2520mapping%2520mechanism%250A%2528PMM%2529%2520along%2520with%2520a%2520component-specific%2520feature%2520enhancement%2520module%2520%2528CSFEM%2529%2520to%250Aimprove%2520breast%2520cancer%2520lesion%2520segmentation%2520and%2520identification.%2520Our%2520PPM%2520ensures%250Athat%2520the%2520segmentation%2520accurately%2520reflects%2520the%2520true%2520shape%2520and%2520extent%2520of%2520the%250Atumor%2520by%2520meticulously%2520delineating%2520their%2520boundaries.%2520The%2520CSFEM%2520focuses%2520on%250Aextracting%2520and%2520amplifying%2520features%2520unique%2520to%2520different%2520tumor%2520types%252C%2520enabling%250Athe%2520model%2520to%2520effectively%2520distinguish%2520between%2520benign%252C%2520malignant%252C%2520and%2520normal%250Atissues.%2520Integrating%2520PMM%2520and%2520CSFEM%2520into%2520our%2520segmentation%2520model%2520yielded%2520an%250Aaccuracy%2520of%252098.1%2525%252C%2520an%2520IoU%2520of%252096.9%2525%252C%2520and%2520a%2520Dice%2520Coefficient%2520of%252097.2%2525.%2520Similarly%252C%250Aour%2520classification%2520model%2520achieved%2520an%2520accuracy%2520of%252099.2%2525%252C%2520with%2520F1-score%252C%250Aprecision%252C%2520and%2520recall%2520values%2520of%252099.1%2525%252C%252099.3%2525%252C%2520and%252099.1%2525%252C%2520respectively.%2520Our%250Aresults%2520indicate%2520significant%2520improvement%2520in%2520evaluation%2520metrics%2520in%2520comparison%2520to%250Astate-of-the-art%2520%2528SOTA%2529%2520models%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520precision%250Amapping%2520and%2520component-specific%2520feature%2520enhancement%2520in%2520advancing%2520breast%2520cancer%250Alesion%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02844v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Precision%20Mapping%20and%20Component-Specific%20Feature%20Enhancement%0A%20%20for%20Breast%20Cancer%20Segmentation%20and%20Identification&entry.906535625=Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Pavan%20Kumar%20S%20and%20Santhosh%20Malarvannan%20and%20Kannan%20A&entry.1292438233=%20%20Breast%20cancer%20is%20a%20leading%20cause%20of%20mortality%20worldwide%2C%20and%20demands%20the%0Acritical%20need%20for%20early%20and%20accurate%20diagnostic%20tools.%20Ultrasound%20imaging%20is%20a%0Awidely%20used%20modality%20for%20breast%20cancer%20screening%2C%20yet%20the%20precise%20segmentation%0Aand%20classification%20of%20tumors%20in%20these%20images%20are%20challenging%20due%20to%20variations%0Ain%20tumor%20morphology%20and%20image%20quality.%20To%20address%20these%20challenges%2C%20we%20propose%0Anovel%20deep%20learning%20%28DL%29%20frameworks%20leveraging%20a%20precision%20mapping%20mechanism%0A%28PMM%29%20along%20with%20a%20component-specific%20feature%20enhancement%20module%20%28CSFEM%29%20to%0Aimprove%20breast%20cancer%20lesion%20segmentation%20and%20identification.%20Our%20PPM%20ensures%0Athat%20the%20segmentation%20accurately%20reflects%20the%20true%20shape%20and%20extent%20of%20the%0Atumor%20by%20meticulously%20delineating%20their%20boundaries.%20The%20CSFEM%20focuses%20on%0Aextracting%20and%20amplifying%20features%20unique%20to%20different%20tumor%20types%2C%20enabling%0Athe%20model%20to%20effectively%20distinguish%20between%20benign%2C%20malignant%2C%20and%20normal%0Atissues.%20Integrating%20PMM%20and%20CSFEM%20into%20our%20segmentation%20model%20yielded%20an%0Aaccuracy%20of%2098.1%25%2C%20an%20IoU%20of%2096.9%25%2C%20and%20a%20Dice%20Coefficient%20of%2097.2%25.%20Similarly%2C%0Aour%20classification%20model%20achieved%20an%20accuracy%20of%2099.2%25%2C%20with%20F1-score%2C%0Aprecision%2C%20and%20recall%20values%20of%2099.1%25%2C%2099.3%25%2C%20and%2099.1%25%2C%20respectively.%20Our%0Aresults%20indicate%20significant%20improvement%20in%20evaluation%20metrics%20in%20comparison%20to%0Astate-of-the-art%20%28SOTA%29%20models%2C%20demonstrating%20the%20effectiveness%20of%20precision%0Amapping%20and%20component-specific%20feature%20enhancement%20in%20advancing%20breast%20cancer%0Alesion%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02844v4&entry.124074799=Read"},
{"title": "Robust Nonprehensile Object Transportation with Uncertain Inertial\n  Parameters", "author": "Adam Heins and Angela P. Schoellig", "abstract": "  We consider the nonprehensile object transportation task known as the\nwaiter's problem - in which a robot must move an object balanced on a tray from\none location to another - when the balanced object has uncertain inertial\nparameters. In contrast to existing approaches that completely ignore\nuncertainty in the inertia matrix or which only consider small parameter\nerrors, we are interested in pushing the limits of the amount of inertial\nparameter uncertainty that can be handled. We first show how balancing\nconstraints robust to inertial parameter uncertainty can be incorporated into a\nmotion planning framework to balance objects while moving quickly. Next, we\ndevelop necessary conditions for the inertial parameters to be realizable on a\nbounding shape based on moment relaxations, allowing us to verify whether a\ntrajectory will violate the balancing constraints for any realizable inertial\nparameters. Finally, we demonstrate our approach on a mobile manipulator in\nsimulations and real hardware experiments: our proposed robust constraints\nconsistently balance a 56 cm tall object with substantial inertial parameter\nuncertainty in the real world, while the baseline approaches drop the object\nwhile transporting it.\n", "link": "http://arxiv.org/abs/2411.07079v1", "date": "2024-11-11", "relevancy": 2.1632, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5697}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5373}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Nonprehensile%20Object%20Transportation%20with%20Uncertain%20Inertial%0A%20%20Parameters&body=Title%3A%20Robust%20Nonprehensile%20Object%20Transportation%20with%20Uncertain%20Inertial%0A%20%20Parameters%0AAuthor%3A%20Adam%20Heins%20and%20Angela%20P.%20Schoellig%0AAbstract%3A%20%20%20We%20consider%20the%20nonprehensile%20object%20transportation%20task%20known%20as%20the%0Awaiter%27s%20problem%20-%20in%20which%20a%20robot%20must%20move%20an%20object%20balanced%20on%20a%20tray%20from%0Aone%20location%20to%20another%20-%20when%20the%20balanced%20object%20has%20uncertain%20inertial%0Aparameters.%20In%20contrast%20to%20existing%20approaches%20that%20completely%20ignore%0Auncertainty%20in%20the%20inertia%20matrix%20or%20which%20only%20consider%20small%20parameter%0Aerrors%2C%20we%20are%20interested%20in%20pushing%20the%20limits%20of%20the%20amount%20of%20inertial%0Aparameter%20uncertainty%20that%20can%20be%20handled.%20We%20first%20show%20how%20balancing%0Aconstraints%20robust%20to%20inertial%20parameter%20uncertainty%20can%20be%20incorporated%20into%20a%0Amotion%20planning%20framework%20to%20balance%20objects%20while%20moving%20quickly.%20Next%2C%20we%0Adevelop%20necessary%20conditions%20for%20the%20inertial%20parameters%20to%20be%20realizable%20on%20a%0Abounding%20shape%20based%20on%20moment%20relaxations%2C%20allowing%20us%20to%20verify%20whether%20a%0Atrajectory%20will%20violate%20the%20balancing%20constraints%20for%20any%20realizable%20inertial%0Aparameters.%20Finally%2C%20we%20demonstrate%20our%20approach%20on%20a%20mobile%20manipulator%20in%0Asimulations%20and%20real%20hardware%20experiments%3A%20our%20proposed%20robust%20constraints%0Aconsistently%20balance%20a%2056%20cm%20tall%20object%20with%20substantial%20inertial%20parameter%0Auncertainty%20in%20the%20real%20world%2C%20while%20the%20baseline%20approaches%20drop%20the%20object%0Awhile%20transporting%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Nonprehensile%2520Object%2520Transportation%2520with%2520Uncertain%2520Inertial%250A%2520%2520Parameters%26entry.906535625%3DAdam%2520Heins%2520and%2520Angela%2520P.%2520Schoellig%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520nonprehensile%2520object%2520transportation%2520task%2520known%2520as%2520the%250Awaiter%2527s%2520problem%2520-%2520in%2520which%2520a%2520robot%2520must%2520move%2520an%2520object%2520balanced%2520on%2520a%2520tray%2520from%250Aone%2520location%2520to%2520another%2520-%2520when%2520the%2520balanced%2520object%2520has%2520uncertain%2520inertial%250Aparameters.%2520In%2520contrast%2520to%2520existing%2520approaches%2520that%2520completely%2520ignore%250Auncertainty%2520in%2520the%2520inertia%2520matrix%2520or%2520which%2520only%2520consider%2520small%2520parameter%250Aerrors%252C%2520we%2520are%2520interested%2520in%2520pushing%2520the%2520limits%2520of%2520the%2520amount%2520of%2520inertial%250Aparameter%2520uncertainty%2520that%2520can%2520be%2520handled.%2520We%2520first%2520show%2520how%2520balancing%250Aconstraints%2520robust%2520to%2520inertial%2520parameter%2520uncertainty%2520can%2520be%2520incorporated%2520into%2520a%250Amotion%2520planning%2520framework%2520to%2520balance%2520objects%2520while%2520moving%2520quickly.%2520Next%252C%2520we%250Adevelop%2520necessary%2520conditions%2520for%2520the%2520inertial%2520parameters%2520to%2520be%2520realizable%2520on%2520a%250Abounding%2520shape%2520based%2520on%2520moment%2520relaxations%252C%2520allowing%2520us%2520to%2520verify%2520whether%2520a%250Atrajectory%2520will%2520violate%2520the%2520balancing%2520constraints%2520for%2520any%2520realizable%2520inertial%250Aparameters.%2520Finally%252C%2520we%2520demonstrate%2520our%2520approach%2520on%2520a%2520mobile%2520manipulator%2520in%250Asimulations%2520and%2520real%2520hardware%2520experiments%253A%2520our%2520proposed%2520robust%2520constraints%250Aconsistently%2520balance%2520a%252056%2520cm%2520tall%2520object%2520with%2520substantial%2520inertial%2520parameter%250Auncertainty%2520in%2520the%2520real%2520world%252C%2520while%2520the%2520baseline%2520approaches%2520drop%2520the%2520object%250Awhile%2520transporting%2520it.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Nonprehensile%20Object%20Transportation%20with%20Uncertain%20Inertial%0A%20%20Parameters&entry.906535625=Adam%20Heins%20and%20Angela%20P.%20Schoellig&entry.1292438233=%20%20We%20consider%20the%20nonprehensile%20object%20transportation%20task%20known%20as%20the%0Awaiter%27s%20problem%20-%20in%20which%20a%20robot%20must%20move%20an%20object%20balanced%20on%20a%20tray%20from%0Aone%20location%20to%20another%20-%20when%20the%20balanced%20object%20has%20uncertain%20inertial%0Aparameters.%20In%20contrast%20to%20existing%20approaches%20that%20completely%20ignore%0Auncertainty%20in%20the%20inertia%20matrix%20or%20which%20only%20consider%20small%20parameter%0Aerrors%2C%20we%20are%20interested%20in%20pushing%20the%20limits%20of%20the%20amount%20of%20inertial%0Aparameter%20uncertainty%20that%20can%20be%20handled.%20We%20first%20show%20how%20balancing%0Aconstraints%20robust%20to%20inertial%20parameter%20uncertainty%20can%20be%20incorporated%20into%20a%0Amotion%20planning%20framework%20to%20balance%20objects%20while%20moving%20quickly.%20Next%2C%20we%0Adevelop%20necessary%20conditions%20for%20the%20inertial%20parameters%20to%20be%20realizable%20on%20a%0Abounding%20shape%20based%20on%20moment%20relaxations%2C%20allowing%20us%20to%20verify%20whether%20a%0Atrajectory%20will%20violate%20the%20balancing%20constraints%20for%20any%20realizable%20inertial%0Aparameters.%20Finally%2C%20we%20demonstrate%20our%20approach%20on%20a%20mobile%20manipulator%20in%0Asimulations%20and%20real%20hardware%20experiments%3A%20our%20proposed%20robust%20constraints%0Aconsistently%20balance%20a%2056%20cm%20tall%20object%20with%20substantial%20inertial%20parameter%0Auncertainty%20in%20the%20real%20world%2C%20while%20the%20baseline%20approaches%20drop%20the%20object%0Awhile%20transporting%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07079v1&entry.124074799=Read"},
{"title": "Identifiable Object-Centric Representation Learning via Probabilistic\n  Slot Attention", "author": "Avinash Kori and Francesco Locatello and Ainkaran Santhirasekaram and Francesca Toni and Ben Glocker and Fabio De Sousa Ribeiro", "abstract": "  Learning modular object-centric representations is crucial for systematic\ngeneralization. Existing methods show promising object-binding capabilities\nempirically, but theoretical identifiability guarantees remain relatively\nunderdeveloped. Understanding when object-centric representations can\ntheoretically be identified is crucial for scaling slot-based methods to\nhigh-dimensional images with correctness guarantees. To that end, we propose a\nprobabilistic slot-attention algorithm that imposes an aggregate mixture prior\nover object-centric slot representations, thereby providing slot\nidentifiability guarantees without supervision, up to an equivalence relation.\nWe provide empirical verification of our theoretical identifiability result\nusing both simple 2-dimensional data and high-resolution imaging datasets.\n", "link": "http://arxiv.org/abs/2406.07141v2", "date": "2024-11-11", "relevancy": 2.1574, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5431}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.541}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifiable%20Object-Centric%20Representation%20Learning%20via%20Probabilistic%0A%20%20Slot%20Attention&body=Title%3A%20Identifiable%20Object-Centric%20Representation%20Learning%20via%20Probabilistic%0A%20%20Slot%20Attention%0AAuthor%3A%20Avinash%20Kori%20and%20Francesco%20Locatello%20and%20Ainkaran%20Santhirasekaram%20and%20Francesca%20Toni%20and%20Ben%20Glocker%20and%20Fabio%20De%20Sousa%20Ribeiro%0AAbstract%3A%20%20%20Learning%20modular%20object-centric%20representations%20is%20crucial%20for%20systematic%0Ageneralization.%20Existing%20methods%20show%20promising%20object-binding%20capabilities%0Aempirically%2C%20but%20theoretical%20identifiability%20guarantees%20remain%20relatively%0Aunderdeveloped.%20Understanding%20when%20object-centric%20representations%20can%0Atheoretically%20be%20identified%20is%20crucial%20for%20scaling%20slot-based%20methods%20to%0Ahigh-dimensional%20images%20with%20correctness%20guarantees.%20To%20that%20end%2C%20we%20propose%20a%0Aprobabilistic%20slot-attention%20algorithm%20that%20imposes%20an%20aggregate%20mixture%20prior%0Aover%20object-centric%20slot%20representations%2C%20thereby%20providing%20slot%0Aidentifiability%20guarantees%20without%20supervision%2C%20up%20to%20an%20equivalence%20relation.%0AWe%20provide%20empirical%20verification%20of%20our%20theoretical%20identifiability%20result%0Ausing%20both%20simple%202-dimensional%20data%20and%20high-resolution%20imaging%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifiable%2520Object-Centric%2520Representation%2520Learning%2520via%2520Probabilistic%250A%2520%2520Slot%2520Attention%26entry.906535625%3DAvinash%2520Kori%2520and%2520Francesco%2520Locatello%2520and%2520Ainkaran%2520Santhirasekaram%2520and%2520Francesca%2520Toni%2520and%2520Ben%2520Glocker%2520and%2520Fabio%2520De%2520Sousa%2520Ribeiro%26entry.1292438233%3D%2520%2520Learning%2520modular%2520object-centric%2520representations%2520is%2520crucial%2520for%2520systematic%250Ageneralization.%2520Existing%2520methods%2520show%2520promising%2520object-binding%2520capabilities%250Aempirically%252C%2520but%2520theoretical%2520identifiability%2520guarantees%2520remain%2520relatively%250Aunderdeveloped.%2520Understanding%2520when%2520object-centric%2520representations%2520can%250Atheoretically%2520be%2520identified%2520is%2520crucial%2520for%2520scaling%2520slot-based%2520methods%2520to%250Ahigh-dimensional%2520images%2520with%2520correctness%2520guarantees.%2520To%2520that%2520end%252C%2520we%2520propose%2520a%250Aprobabilistic%2520slot-attention%2520algorithm%2520that%2520imposes%2520an%2520aggregate%2520mixture%2520prior%250Aover%2520object-centric%2520slot%2520representations%252C%2520thereby%2520providing%2520slot%250Aidentifiability%2520guarantees%2520without%2520supervision%252C%2520up%2520to%2520an%2520equivalence%2520relation.%250AWe%2520provide%2520empirical%2520verification%2520of%2520our%2520theoretical%2520identifiability%2520result%250Ausing%2520both%2520simple%25202-dimensional%2520data%2520and%2520high-resolution%2520imaging%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifiable%20Object-Centric%20Representation%20Learning%20via%20Probabilistic%0A%20%20Slot%20Attention&entry.906535625=Avinash%20Kori%20and%20Francesco%20Locatello%20and%20Ainkaran%20Santhirasekaram%20and%20Francesca%20Toni%20and%20Ben%20Glocker%20and%20Fabio%20De%20Sousa%20Ribeiro&entry.1292438233=%20%20Learning%20modular%20object-centric%20representations%20is%20crucial%20for%20systematic%0Ageneralization.%20Existing%20methods%20show%20promising%20object-binding%20capabilities%0Aempirically%2C%20but%20theoretical%20identifiability%20guarantees%20remain%20relatively%0Aunderdeveloped.%20Understanding%20when%20object-centric%20representations%20can%0Atheoretically%20be%20identified%20is%20crucial%20for%20scaling%20slot-based%20methods%20to%0Ahigh-dimensional%20images%20with%20correctness%20guarantees.%20To%20that%20end%2C%20we%20propose%20a%0Aprobabilistic%20slot-attention%20algorithm%20that%20imposes%20an%20aggregate%20mixture%20prior%0Aover%20object-centric%20slot%20representations%2C%20thereby%20providing%20slot%0Aidentifiability%20guarantees%20without%20supervision%2C%20up%20to%20an%20equivalence%20relation.%0AWe%20provide%20empirical%20verification%20of%20our%20theoretical%20identifiability%20result%0Ausing%20both%20simple%202-dimensional%20data%20and%20high-resolution%20imaging%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07141v2&entry.124074799=Read"},
{"title": "ConvMixFormer- A Resource-efficient Convolution Mixer for\n  Transformer-based Dynamic Hand Gesture Recognition", "author": "Mallika Garg and Debashis Ghosh and Pyari Mohan Pradhan", "abstract": "  Transformer models have demonstrated remarkable success in many domains such\nas natural language processing (NLP) and computer vision. With the growing\ninterest in transformer-based architectures, they are now utilized for gesture\nrecognition. So, we also explore and devise a novel ConvMixFormer architecture\nfor dynamic hand gestures. The transformers use quadratic scaling of the\nattention features with the sequential data, due to which these models are\ncomputationally complex and heavy. We have considered this drawback of the\ntransformer and designed a resource-efficient model that replaces the\nself-attention in the transformer with the simple convolutional layer-based\ntoken mixer. The computational cost and the parameters used for the\nconvolution-based mixer are comparatively less than the quadratic\nself-attention. Convolution-mixer helps the model capture the local spatial\nfeatures that self-attention struggles to capture due to their sequential\nprocessing nature. Further, an efficient gate mechanism is employed instead of\na conventional feed-forward network in the transformer to help the model\ncontrol the flow of features within different stages of the proposed model.\nThis design uses fewer learnable parameters which is nearly half the vanilla\ntransformer that helps in fast and efficient training. The proposed method is\nevaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has\nachieved state-of-the-art results on single and multimodal inputs. We have also\nshown the parameter efficiency of the proposed ConvMixFormer model compared to\nother methods. The source code is available at\nhttps://github.com/mallikagarg/ConvMixFormer.\n", "link": "http://arxiv.org/abs/2411.07118v1", "date": "2024-11-11", "relevancy": 2.1553, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5562}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.539}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConvMixFormer-%20A%20Resource-efficient%20Convolution%20Mixer%20for%0A%20%20Transformer-based%20Dynamic%20Hand%20Gesture%20Recognition&body=Title%3A%20ConvMixFormer-%20A%20Resource-efficient%20Convolution%20Mixer%20for%0A%20%20Transformer-based%20Dynamic%20Hand%20Gesture%20Recognition%0AAuthor%3A%20Mallika%20Garg%20and%20Debashis%20Ghosh%20and%20Pyari%20Mohan%20Pradhan%0AAbstract%3A%20%20%20Transformer%20models%20have%20demonstrated%20remarkable%20success%20in%20many%20domains%20such%0Aas%20natural%20language%20processing%20%28NLP%29%20and%20computer%20vision.%20With%20the%20growing%0Ainterest%20in%20transformer-based%20architectures%2C%20they%20are%20now%20utilized%20for%20gesture%0Arecognition.%20So%2C%20we%20also%20explore%20and%20devise%20a%20novel%20ConvMixFormer%20architecture%0Afor%20dynamic%20hand%20gestures.%20The%20transformers%20use%20quadratic%20scaling%20of%20the%0Aattention%20features%20with%20the%20sequential%20data%2C%20due%20to%20which%20these%20models%20are%0Acomputationally%20complex%20and%20heavy.%20We%20have%20considered%20this%20drawback%20of%20the%0Atransformer%20and%20designed%20a%20resource-efficient%20model%20that%20replaces%20the%0Aself-attention%20in%20the%20transformer%20with%20the%20simple%20convolutional%20layer-based%0Atoken%20mixer.%20The%20computational%20cost%20and%20the%20parameters%20used%20for%20the%0Aconvolution-based%20mixer%20are%20comparatively%20less%20than%20the%20quadratic%0Aself-attention.%20Convolution-mixer%20helps%20the%20model%20capture%20the%20local%20spatial%0Afeatures%20that%20self-attention%20struggles%20to%20capture%20due%20to%20their%20sequential%0Aprocessing%20nature.%20Further%2C%20an%20efficient%20gate%20mechanism%20is%20employed%20instead%20of%0Aa%20conventional%20feed-forward%20network%20in%20the%20transformer%20to%20help%20the%20model%0Acontrol%20the%20flow%20of%20features%20within%20different%20stages%20of%20the%20proposed%20model.%0AThis%20design%20uses%20fewer%20learnable%20parameters%20which%20is%20nearly%20half%20the%20vanilla%0Atransformer%20that%20helps%20in%20fast%20and%20efficient%20training.%20The%20proposed%20method%20is%0Aevaluated%20on%20NVidia%20Dynamic%20Hand%20Gesture%20and%20Briareo%20datasets%20and%20our%20model%20has%0Aachieved%20state-of-the-art%20results%20on%20single%20and%20multimodal%20inputs.%20We%20have%20also%0Ashown%20the%20parameter%20efficiency%20of%20the%20proposed%20ConvMixFormer%20model%20compared%20to%0Aother%20methods.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/mallikagarg/ConvMixFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvMixFormer-%2520A%2520Resource-efficient%2520Convolution%2520Mixer%2520for%250A%2520%2520Transformer-based%2520Dynamic%2520Hand%2520Gesture%2520Recognition%26entry.906535625%3DMallika%2520Garg%2520and%2520Debashis%2520Ghosh%2520and%2520Pyari%2520Mohan%2520Pradhan%26entry.1292438233%3D%2520%2520Transformer%2520models%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520many%2520domains%2520such%250Aas%2520natural%2520language%2520processing%2520%2528NLP%2529%2520and%2520computer%2520vision.%2520With%2520the%2520growing%250Ainterest%2520in%2520transformer-based%2520architectures%252C%2520they%2520are%2520now%2520utilized%2520for%2520gesture%250Arecognition.%2520So%252C%2520we%2520also%2520explore%2520and%2520devise%2520a%2520novel%2520ConvMixFormer%2520architecture%250Afor%2520dynamic%2520hand%2520gestures.%2520The%2520transformers%2520use%2520quadratic%2520scaling%2520of%2520the%250Aattention%2520features%2520with%2520the%2520sequential%2520data%252C%2520due%2520to%2520which%2520these%2520models%2520are%250Acomputationally%2520complex%2520and%2520heavy.%2520We%2520have%2520considered%2520this%2520drawback%2520of%2520the%250Atransformer%2520and%2520designed%2520a%2520resource-efficient%2520model%2520that%2520replaces%2520the%250Aself-attention%2520in%2520the%2520transformer%2520with%2520the%2520simple%2520convolutional%2520layer-based%250Atoken%2520mixer.%2520The%2520computational%2520cost%2520and%2520the%2520parameters%2520used%2520for%2520the%250Aconvolution-based%2520mixer%2520are%2520comparatively%2520less%2520than%2520the%2520quadratic%250Aself-attention.%2520Convolution-mixer%2520helps%2520the%2520model%2520capture%2520the%2520local%2520spatial%250Afeatures%2520that%2520self-attention%2520struggles%2520to%2520capture%2520due%2520to%2520their%2520sequential%250Aprocessing%2520nature.%2520Further%252C%2520an%2520efficient%2520gate%2520mechanism%2520is%2520employed%2520instead%2520of%250Aa%2520conventional%2520feed-forward%2520network%2520in%2520the%2520transformer%2520to%2520help%2520the%2520model%250Acontrol%2520the%2520flow%2520of%2520features%2520within%2520different%2520stages%2520of%2520the%2520proposed%2520model.%250AThis%2520design%2520uses%2520fewer%2520learnable%2520parameters%2520which%2520is%2520nearly%2520half%2520the%2520vanilla%250Atransformer%2520that%2520helps%2520in%2520fast%2520and%2520efficient%2520training.%2520The%2520proposed%2520method%2520is%250Aevaluated%2520on%2520NVidia%2520Dynamic%2520Hand%2520Gesture%2520and%2520Briareo%2520datasets%2520and%2520our%2520model%2520has%250Aachieved%2520state-of-the-art%2520results%2520on%2520single%2520and%2520multimodal%2520inputs.%2520We%2520have%2520also%250Ashown%2520the%2520parameter%2520efficiency%2520of%2520the%2520proposed%2520ConvMixFormer%2520model%2520compared%2520to%250Aother%2520methods.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mallikagarg/ConvMixFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConvMixFormer-%20A%20Resource-efficient%20Convolution%20Mixer%20for%0A%20%20Transformer-based%20Dynamic%20Hand%20Gesture%20Recognition&entry.906535625=Mallika%20Garg%20and%20Debashis%20Ghosh%20and%20Pyari%20Mohan%20Pradhan&entry.1292438233=%20%20Transformer%20models%20have%20demonstrated%20remarkable%20success%20in%20many%20domains%20such%0Aas%20natural%20language%20processing%20%28NLP%29%20and%20computer%20vision.%20With%20the%20growing%0Ainterest%20in%20transformer-based%20architectures%2C%20they%20are%20now%20utilized%20for%20gesture%0Arecognition.%20So%2C%20we%20also%20explore%20and%20devise%20a%20novel%20ConvMixFormer%20architecture%0Afor%20dynamic%20hand%20gestures.%20The%20transformers%20use%20quadratic%20scaling%20of%20the%0Aattention%20features%20with%20the%20sequential%20data%2C%20due%20to%20which%20these%20models%20are%0Acomputationally%20complex%20and%20heavy.%20We%20have%20considered%20this%20drawback%20of%20the%0Atransformer%20and%20designed%20a%20resource-efficient%20model%20that%20replaces%20the%0Aself-attention%20in%20the%20transformer%20with%20the%20simple%20convolutional%20layer-based%0Atoken%20mixer.%20The%20computational%20cost%20and%20the%20parameters%20used%20for%20the%0Aconvolution-based%20mixer%20are%20comparatively%20less%20than%20the%20quadratic%0Aself-attention.%20Convolution-mixer%20helps%20the%20model%20capture%20the%20local%20spatial%0Afeatures%20that%20self-attention%20struggles%20to%20capture%20due%20to%20their%20sequential%0Aprocessing%20nature.%20Further%2C%20an%20efficient%20gate%20mechanism%20is%20employed%20instead%20of%0Aa%20conventional%20feed-forward%20network%20in%20the%20transformer%20to%20help%20the%20model%0Acontrol%20the%20flow%20of%20features%20within%20different%20stages%20of%20the%20proposed%20model.%0AThis%20design%20uses%20fewer%20learnable%20parameters%20which%20is%20nearly%20half%20the%20vanilla%0Atransformer%20that%20helps%20in%20fast%20and%20efficient%20training.%20The%20proposed%20method%20is%0Aevaluated%20on%20NVidia%20Dynamic%20Hand%20Gesture%20and%20Briareo%20datasets%20and%20our%20model%20has%0Aachieved%20state-of-the-art%20results%20on%20single%20and%20multimodal%20inputs.%20We%20have%20also%0Ashown%20the%20parameter%20efficiency%20of%20the%20proposed%20ConvMixFormer%20model%20compared%20to%0Aother%20methods.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/mallikagarg/ConvMixFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07118v1&entry.124074799=Read"},
{"title": "E3x: $\\mathrm{E}(3)$-Equivariant Deep Learning Made Easy", "author": "Oliver T. Unke and Hartmut Maennel", "abstract": "  This work introduces E3x, a software package for building neural networks\nthat are equivariant with respect to the Euclidean group $\\mathrm{E}(3)$,\nconsisting of translations, rotations, and reflections of three-dimensional\nspace. Compared to ordinary neural networks, $\\mathrm{E}(3)$-equivariant models\npromise benefits whenever input and/or output data are quantities associated\nwith three-dimensional objects. This is because the numeric values of such\nquantities (e.g. positions) typically depend on the chosen coordinate system.\nUnder transformations of the reference frame, the values change predictably,\nbut the underlying rules can be difficult to learn for ordinary machine\nlearning models. With built-in $\\mathrm{E}(3)$-equivariance, neural networks\nare guaranteed to satisfy the relevant transformation rules exactly, resulting\nin superior data efficiency and accuracy. The code for E3x is available from\nhttps://github.com/google-research/e3x, detailed documentation and usage\nexamples can be found on https://e3x.readthedocs.io.\n", "link": "http://arxiv.org/abs/2401.07595v3", "date": "2024-11-11", "relevancy": 2.1424, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5491}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5366}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E3x%3A%20%24%5Cmathrm%7BE%7D%283%29%24-Equivariant%20Deep%20Learning%20Made%20Easy&body=Title%3A%20E3x%3A%20%24%5Cmathrm%7BE%7D%283%29%24-Equivariant%20Deep%20Learning%20Made%20Easy%0AAuthor%3A%20Oliver%20T.%20Unke%20and%20Hartmut%20Maennel%0AAbstract%3A%20%20%20This%20work%20introduces%20E3x%2C%20a%20software%20package%20for%20building%20neural%20networks%0Athat%20are%20equivariant%20with%20respect%20to%20the%20Euclidean%20group%20%24%5Cmathrm%7BE%7D%283%29%24%2C%0Aconsisting%20of%20translations%2C%20rotations%2C%20and%20reflections%20of%20three-dimensional%0Aspace.%20Compared%20to%20ordinary%20neural%20networks%2C%20%24%5Cmathrm%7BE%7D%283%29%24-equivariant%20models%0Apromise%20benefits%20whenever%20input%20and/or%20output%20data%20are%20quantities%20associated%0Awith%20three-dimensional%20objects.%20This%20is%20because%20the%20numeric%20values%20of%20such%0Aquantities%20%28e.g.%20positions%29%20typically%20depend%20on%20the%20chosen%20coordinate%20system.%0AUnder%20transformations%20of%20the%20reference%20frame%2C%20the%20values%20change%20predictably%2C%0Abut%20the%20underlying%20rules%20can%20be%20difficult%20to%20learn%20for%20ordinary%20machine%0Alearning%20models.%20With%20built-in%20%24%5Cmathrm%7BE%7D%283%29%24-equivariance%2C%20neural%20networks%0Aare%20guaranteed%20to%20satisfy%20the%20relevant%20transformation%20rules%20exactly%2C%20resulting%0Ain%20superior%20data%20efficiency%20and%20accuracy.%20The%20code%20for%20E3x%20is%20available%20from%0Ahttps%3A//github.com/google-research/e3x%2C%20detailed%20documentation%20and%20usage%0Aexamples%20can%20be%20found%20on%20https%3A//e3x.readthedocs.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07595v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE3x%253A%2520%2524%255Cmathrm%257BE%257D%25283%2529%2524-Equivariant%2520Deep%2520Learning%2520Made%2520Easy%26entry.906535625%3DOliver%2520T.%2520Unke%2520and%2520Hartmut%2520Maennel%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520E3x%252C%2520a%2520software%2520package%2520for%2520building%2520neural%2520networks%250Athat%2520are%2520equivariant%2520with%2520respect%2520to%2520the%2520Euclidean%2520group%2520%2524%255Cmathrm%257BE%257D%25283%2529%2524%252C%250Aconsisting%2520of%2520translations%252C%2520rotations%252C%2520and%2520reflections%2520of%2520three-dimensional%250Aspace.%2520Compared%2520to%2520ordinary%2520neural%2520networks%252C%2520%2524%255Cmathrm%257BE%257D%25283%2529%2524-equivariant%2520models%250Apromise%2520benefits%2520whenever%2520input%2520and/or%2520output%2520data%2520are%2520quantities%2520associated%250Awith%2520three-dimensional%2520objects.%2520This%2520is%2520because%2520the%2520numeric%2520values%2520of%2520such%250Aquantities%2520%2528e.g.%2520positions%2529%2520typically%2520depend%2520on%2520the%2520chosen%2520coordinate%2520system.%250AUnder%2520transformations%2520of%2520the%2520reference%2520frame%252C%2520the%2520values%2520change%2520predictably%252C%250Abut%2520the%2520underlying%2520rules%2520can%2520be%2520difficult%2520to%2520learn%2520for%2520ordinary%2520machine%250Alearning%2520models.%2520With%2520built-in%2520%2524%255Cmathrm%257BE%257D%25283%2529%2524-equivariance%252C%2520neural%2520networks%250Aare%2520guaranteed%2520to%2520satisfy%2520the%2520relevant%2520transformation%2520rules%2520exactly%252C%2520resulting%250Ain%2520superior%2520data%2520efficiency%2520and%2520accuracy.%2520The%2520code%2520for%2520E3x%2520is%2520available%2520from%250Ahttps%253A//github.com/google-research/e3x%252C%2520detailed%2520documentation%2520and%2520usage%250Aexamples%2520can%2520be%2520found%2520on%2520https%253A//e3x.readthedocs.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07595v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E3x%3A%20%24%5Cmathrm%7BE%7D%283%29%24-Equivariant%20Deep%20Learning%20Made%20Easy&entry.906535625=Oliver%20T.%20Unke%20and%20Hartmut%20Maennel&entry.1292438233=%20%20This%20work%20introduces%20E3x%2C%20a%20software%20package%20for%20building%20neural%20networks%0Athat%20are%20equivariant%20with%20respect%20to%20the%20Euclidean%20group%20%24%5Cmathrm%7BE%7D%283%29%24%2C%0Aconsisting%20of%20translations%2C%20rotations%2C%20and%20reflections%20of%20three-dimensional%0Aspace.%20Compared%20to%20ordinary%20neural%20networks%2C%20%24%5Cmathrm%7BE%7D%283%29%24-equivariant%20models%0Apromise%20benefits%20whenever%20input%20and/or%20output%20data%20are%20quantities%20associated%0Awith%20three-dimensional%20objects.%20This%20is%20because%20the%20numeric%20values%20of%20such%0Aquantities%20%28e.g.%20positions%29%20typically%20depend%20on%20the%20chosen%20coordinate%20system.%0AUnder%20transformations%20of%20the%20reference%20frame%2C%20the%20values%20change%20predictably%2C%0Abut%20the%20underlying%20rules%20can%20be%20difficult%20to%20learn%20for%20ordinary%20machine%0Alearning%20models.%20With%20built-in%20%24%5Cmathrm%7BE%7D%283%29%24-equivariance%2C%20neural%20networks%0Aare%20guaranteed%20to%20satisfy%20the%20relevant%20transformation%20rules%20exactly%2C%20resulting%0Ain%20superior%20data%20efficiency%20and%20accuracy.%20The%20code%20for%20E3x%20is%20available%20from%0Ahttps%3A//github.com/google-research/e3x%2C%20detailed%20documentation%20and%20usage%0Aexamples%20can%20be%20found%20on%20https%3A//e3x.readthedocs.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07595v3&entry.124074799=Read"},
{"title": "General Geospatial Inference with a Population Dynamics Foundation Model", "author": "Mohit Agarwal and Mimi Sun and Chaitanya Kamath and Arbaaz Muslim and Prithul Sarker and Joydeep Paul and Hector Yee and Marcin Sieniek and Kim Jablonski and Yael Mayer and David Fork and Sheila de Guia and Jamie McPike and Adam Boulanger and Tomer Shekel and David Schottlander and Yao Xiao and Manjit Chakravarthy Manukonda and Yun Liu and Neslihan Bulut and Sami Abu-el-haija and Arno Eigenwillig and Parth Kothari and Bryan Perozzi and Monica Bharel and Von Nguyen and Luke Barrington and Niv Efron and Yossi Matias and Greg Corrado and Krish Eswaran and Shruthi Prabhakara and Shravya Shetty and Gautam Prasad", "abstract": "  Supporting the health and well-being of dynamic populations around the world\nrequires governmental agencies, organizations and researchers to understand and\nreason over complex relationships between human behavior and local contexts in\norder to identify high-risk groups and strategically allocate limited\nresources. Traditional approaches to these classes of problems often entail\ndeveloping manually curated, task-specific features and models to represent\nhuman behavior and the natural and built environment, which can be challenging\nto adapt to new, or even, related tasks. To address this, we introduce a\nPopulation Dynamics Foundation Model (PDFM) that aims to capture the\nrelationships between diverse data modalities and is applicable to a broad\nrange of geospatial tasks. We first construct a geo-indexed dataset for postal\ncodes and counties across the United States, capturing rich aggregated\ninformation on human behavior from maps, busyness, and aggregated search\ntrends, and environmental factors such as weather and air quality. We then\nmodel this data and the complex relationships between locations using a graph\nneural network, producing embeddings that can be adapted to a wide range of\ndownstream tasks using relatively simple models. We evaluate the effectiveness\nof our approach by benchmarking it on 27 downstream tasks spanning three\ndistinct domains: health indicators, socioeconomic factors, and environmental\nmeasurements. The approach achieves state-of-the-art performance on all 27\ngeospatial interpolation tasks, and on 25 out of the 27 extrapolation and\nsuper-resolution tasks. We combined the PDFM with a state-of-the-art\nforecasting foundation model, TimesFM, to predict unemployment and poverty,\nachieving performance that surpasses fully supervised forecasting. The full set\nof embeddings and sample code are publicly available for researchers.\n", "link": "http://arxiv.org/abs/2411.07207v1", "date": "2024-11-11", "relevancy": 2.1193, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5585}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5442}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General%20Geospatial%20Inference%20with%20a%20Population%20Dynamics%20Foundation%20Model&body=Title%3A%20General%20Geospatial%20Inference%20with%20a%20Population%20Dynamics%20Foundation%20Model%0AAuthor%3A%20Mohit%20Agarwal%20and%20Mimi%20Sun%20and%20Chaitanya%20Kamath%20and%20Arbaaz%20Muslim%20and%20Prithul%20Sarker%20and%20Joydeep%20Paul%20and%20Hector%20Yee%20and%20Marcin%20Sieniek%20and%20Kim%20Jablonski%20and%20Yael%20Mayer%20and%20David%20Fork%20and%20Sheila%20de%20Guia%20and%20Jamie%20McPike%20and%20Adam%20Boulanger%20and%20Tomer%20Shekel%20and%20David%20Schottlander%20and%20Yao%20Xiao%20and%20Manjit%20Chakravarthy%20Manukonda%20and%20Yun%20Liu%20and%20Neslihan%20Bulut%20and%20Sami%20Abu-el-haija%20and%20Arno%20Eigenwillig%20and%20Parth%20Kothari%20and%20Bryan%20Perozzi%20and%20Monica%20Bharel%20and%20Von%20Nguyen%20and%20Luke%20Barrington%20and%20Niv%20Efron%20and%20Yossi%20Matias%20and%20Greg%20Corrado%20and%20Krish%20Eswaran%20and%20Shruthi%20Prabhakara%20and%20Shravya%20Shetty%20and%20Gautam%20Prasad%0AAbstract%3A%20%20%20Supporting%20the%20health%20and%20well-being%20of%20dynamic%20populations%20around%20the%20world%0Arequires%20governmental%20agencies%2C%20organizations%20and%20researchers%20to%20understand%20and%0Areason%20over%20complex%20relationships%20between%20human%20behavior%20and%20local%20contexts%20in%0Aorder%20to%20identify%20high-risk%20groups%20and%20strategically%20allocate%20limited%0Aresources.%20Traditional%20approaches%20to%20these%20classes%20of%20problems%20often%20entail%0Adeveloping%20manually%20curated%2C%20task-specific%20features%20and%20models%20to%20represent%0Ahuman%20behavior%20and%20the%20natural%20and%20built%20environment%2C%20which%20can%20be%20challenging%0Ato%20adapt%20to%20new%2C%20or%20even%2C%20related%20tasks.%20To%20address%20this%2C%20we%20introduce%20a%0APopulation%20Dynamics%20Foundation%20Model%20%28PDFM%29%20that%20aims%20to%20capture%20the%0Arelationships%20between%20diverse%20data%20modalities%20and%20is%20applicable%20to%20a%20broad%0Arange%20of%20geospatial%20tasks.%20We%20first%20construct%20a%20geo-indexed%20dataset%20for%20postal%0Acodes%20and%20counties%20across%20the%20United%20States%2C%20capturing%20rich%20aggregated%0Ainformation%20on%20human%20behavior%20from%20maps%2C%20busyness%2C%20and%20aggregated%20search%0Atrends%2C%20and%20environmental%20factors%20such%20as%20weather%20and%20air%20quality.%20We%20then%0Amodel%20this%20data%20and%20the%20complex%20relationships%20between%20locations%20using%20a%20graph%0Aneural%20network%2C%20producing%20embeddings%20that%20can%20be%20adapted%20to%20a%20wide%20range%20of%0Adownstream%20tasks%20using%20relatively%20simple%20models.%20We%20evaluate%20the%20effectiveness%0Aof%20our%20approach%20by%20benchmarking%20it%20on%2027%20downstream%20tasks%20spanning%20three%0Adistinct%20domains%3A%20health%20indicators%2C%20socioeconomic%20factors%2C%20and%20environmental%0Ameasurements.%20The%20approach%20achieves%20state-of-the-art%20performance%20on%20all%2027%0Ageospatial%20interpolation%20tasks%2C%20and%20on%2025%20out%20of%20the%2027%20extrapolation%20and%0Asuper-resolution%20tasks.%20We%20combined%20the%20PDFM%20with%20a%20state-of-the-art%0Aforecasting%20foundation%20model%2C%20TimesFM%2C%20to%20predict%20unemployment%20and%20poverty%2C%0Aachieving%20performance%20that%20surpasses%20fully%20supervised%20forecasting.%20The%20full%20set%0Aof%20embeddings%20and%20sample%20code%20are%20publicly%20available%20for%20researchers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral%2520Geospatial%2520Inference%2520with%2520a%2520Population%2520Dynamics%2520Foundation%2520Model%26entry.906535625%3DMohit%2520Agarwal%2520and%2520Mimi%2520Sun%2520and%2520Chaitanya%2520Kamath%2520and%2520Arbaaz%2520Muslim%2520and%2520Prithul%2520Sarker%2520and%2520Joydeep%2520Paul%2520and%2520Hector%2520Yee%2520and%2520Marcin%2520Sieniek%2520and%2520Kim%2520Jablonski%2520and%2520Yael%2520Mayer%2520and%2520David%2520Fork%2520and%2520Sheila%2520de%2520Guia%2520and%2520Jamie%2520McPike%2520and%2520Adam%2520Boulanger%2520and%2520Tomer%2520Shekel%2520and%2520David%2520Schottlander%2520and%2520Yao%2520Xiao%2520and%2520Manjit%2520Chakravarthy%2520Manukonda%2520and%2520Yun%2520Liu%2520and%2520Neslihan%2520Bulut%2520and%2520Sami%2520Abu-el-haija%2520and%2520Arno%2520Eigenwillig%2520and%2520Parth%2520Kothari%2520and%2520Bryan%2520Perozzi%2520and%2520Monica%2520Bharel%2520and%2520Von%2520Nguyen%2520and%2520Luke%2520Barrington%2520and%2520Niv%2520Efron%2520and%2520Yossi%2520Matias%2520and%2520Greg%2520Corrado%2520and%2520Krish%2520Eswaran%2520and%2520Shruthi%2520Prabhakara%2520and%2520Shravya%2520Shetty%2520and%2520Gautam%2520Prasad%26entry.1292438233%3D%2520%2520Supporting%2520the%2520health%2520and%2520well-being%2520of%2520dynamic%2520populations%2520around%2520the%2520world%250Arequires%2520governmental%2520agencies%252C%2520organizations%2520and%2520researchers%2520to%2520understand%2520and%250Areason%2520over%2520complex%2520relationships%2520between%2520human%2520behavior%2520and%2520local%2520contexts%2520in%250Aorder%2520to%2520identify%2520high-risk%2520groups%2520and%2520strategically%2520allocate%2520limited%250Aresources.%2520Traditional%2520approaches%2520to%2520these%2520classes%2520of%2520problems%2520often%2520entail%250Adeveloping%2520manually%2520curated%252C%2520task-specific%2520features%2520and%2520models%2520to%2520represent%250Ahuman%2520behavior%2520and%2520the%2520natural%2520and%2520built%2520environment%252C%2520which%2520can%2520be%2520challenging%250Ato%2520adapt%2520to%2520new%252C%2520or%2520even%252C%2520related%2520tasks.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%250APopulation%2520Dynamics%2520Foundation%2520Model%2520%2528PDFM%2529%2520that%2520aims%2520to%2520capture%2520the%250Arelationships%2520between%2520diverse%2520data%2520modalities%2520and%2520is%2520applicable%2520to%2520a%2520broad%250Arange%2520of%2520geospatial%2520tasks.%2520We%2520first%2520construct%2520a%2520geo-indexed%2520dataset%2520for%2520postal%250Acodes%2520and%2520counties%2520across%2520the%2520United%2520States%252C%2520capturing%2520rich%2520aggregated%250Ainformation%2520on%2520human%2520behavior%2520from%2520maps%252C%2520busyness%252C%2520and%2520aggregated%2520search%250Atrends%252C%2520and%2520environmental%2520factors%2520such%2520as%2520weather%2520and%2520air%2520quality.%2520We%2520then%250Amodel%2520this%2520data%2520and%2520the%2520complex%2520relationships%2520between%2520locations%2520using%2520a%2520graph%250Aneural%2520network%252C%2520producing%2520embeddings%2520that%2520can%2520be%2520adapted%2520to%2520a%2520wide%2520range%2520of%250Adownstream%2520tasks%2520using%2520relatively%2520simple%2520models.%2520We%2520evaluate%2520the%2520effectiveness%250Aof%2520our%2520approach%2520by%2520benchmarking%2520it%2520on%252027%2520downstream%2520tasks%2520spanning%2520three%250Adistinct%2520domains%253A%2520health%2520indicators%252C%2520socioeconomic%2520factors%252C%2520and%2520environmental%250Ameasurements.%2520The%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520all%252027%250Ageospatial%2520interpolation%2520tasks%252C%2520and%2520on%252025%2520out%2520of%2520the%252027%2520extrapolation%2520and%250Asuper-resolution%2520tasks.%2520We%2520combined%2520the%2520PDFM%2520with%2520a%2520state-of-the-art%250Aforecasting%2520foundation%2520model%252C%2520TimesFM%252C%2520to%2520predict%2520unemployment%2520and%2520poverty%252C%250Aachieving%2520performance%2520that%2520surpasses%2520fully%2520supervised%2520forecasting.%2520The%2520full%2520set%250Aof%2520embeddings%2520and%2520sample%2520code%2520are%2520publicly%2520available%2520for%2520researchers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General%20Geospatial%20Inference%20with%20a%20Population%20Dynamics%20Foundation%20Model&entry.906535625=Mohit%20Agarwal%20and%20Mimi%20Sun%20and%20Chaitanya%20Kamath%20and%20Arbaaz%20Muslim%20and%20Prithul%20Sarker%20and%20Joydeep%20Paul%20and%20Hector%20Yee%20and%20Marcin%20Sieniek%20and%20Kim%20Jablonski%20and%20Yael%20Mayer%20and%20David%20Fork%20and%20Sheila%20de%20Guia%20and%20Jamie%20McPike%20and%20Adam%20Boulanger%20and%20Tomer%20Shekel%20and%20David%20Schottlander%20and%20Yao%20Xiao%20and%20Manjit%20Chakravarthy%20Manukonda%20and%20Yun%20Liu%20and%20Neslihan%20Bulut%20and%20Sami%20Abu-el-haija%20and%20Arno%20Eigenwillig%20and%20Parth%20Kothari%20and%20Bryan%20Perozzi%20and%20Monica%20Bharel%20and%20Von%20Nguyen%20and%20Luke%20Barrington%20and%20Niv%20Efron%20and%20Yossi%20Matias%20and%20Greg%20Corrado%20and%20Krish%20Eswaran%20and%20Shruthi%20Prabhakara%20and%20Shravya%20Shetty%20and%20Gautam%20Prasad&entry.1292438233=%20%20Supporting%20the%20health%20and%20well-being%20of%20dynamic%20populations%20around%20the%20world%0Arequires%20governmental%20agencies%2C%20organizations%20and%20researchers%20to%20understand%20and%0Areason%20over%20complex%20relationships%20between%20human%20behavior%20and%20local%20contexts%20in%0Aorder%20to%20identify%20high-risk%20groups%20and%20strategically%20allocate%20limited%0Aresources.%20Traditional%20approaches%20to%20these%20classes%20of%20problems%20often%20entail%0Adeveloping%20manually%20curated%2C%20task-specific%20features%20and%20models%20to%20represent%0Ahuman%20behavior%20and%20the%20natural%20and%20built%20environment%2C%20which%20can%20be%20challenging%0Ato%20adapt%20to%20new%2C%20or%20even%2C%20related%20tasks.%20To%20address%20this%2C%20we%20introduce%20a%0APopulation%20Dynamics%20Foundation%20Model%20%28PDFM%29%20that%20aims%20to%20capture%20the%0Arelationships%20between%20diverse%20data%20modalities%20and%20is%20applicable%20to%20a%20broad%0Arange%20of%20geospatial%20tasks.%20We%20first%20construct%20a%20geo-indexed%20dataset%20for%20postal%0Acodes%20and%20counties%20across%20the%20United%20States%2C%20capturing%20rich%20aggregated%0Ainformation%20on%20human%20behavior%20from%20maps%2C%20busyness%2C%20and%20aggregated%20search%0Atrends%2C%20and%20environmental%20factors%20such%20as%20weather%20and%20air%20quality.%20We%20then%0Amodel%20this%20data%20and%20the%20complex%20relationships%20between%20locations%20using%20a%20graph%0Aneural%20network%2C%20producing%20embeddings%20that%20can%20be%20adapted%20to%20a%20wide%20range%20of%0Adownstream%20tasks%20using%20relatively%20simple%20models.%20We%20evaluate%20the%20effectiveness%0Aof%20our%20approach%20by%20benchmarking%20it%20on%2027%20downstream%20tasks%20spanning%20three%0Adistinct%20domains%3A%20health%20indicators%2C%20socioeconomic%20factors%2C%20and%20environmental%0Ameasurements.%20The%20approach%20achieves%20state-of-the-art%20performance%20on%20all%2027%0Ageospatial%20interpolation%20tasks%2C%20and%20on%2025%20out%20of%20the%2027%20extrapolation%20and%0Asuper-resolution%20tasks.%20We%20combined%20the%20PDFM%20with%20a%20state-of-the-art%0Aforecasting%20foundation%20model%2C%20TimesFM%2C%20to%20predict%20unemployment%20and%20poverty%2C%0Aachieving%20performance%20that%20surpasses%20fully%20supervised%20forecasting.%20The%20full%20set%0Aof%20embeddings%20and%20sample%20code%20are%20publicly%20available%20for%20researchers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07207v1&entry.124074799=Read"},
{"title": "Feature Selection Based on Wasserstein Distance", "author": "Fuwei Li", "abstract": "  In this paper, we present a novel feature selection method based on the\nWasserstein distance. Feature selection plays a critical role in reducing the\ndimensionality of input data, thereby improving machine learning efficiency and\ngeneralization performance. Unlike traditional feature selection approaches\nthat rely on criteria such as correlation or KL divergence, our method\nleverages the Wasserstein distance to measure the similarity between\ndistributions of selected features and original features. This approach\ninherently accounts for similarities between classes, making it robust in\nscenarios involving noisy labels. Experimental results demonstrate that our\nmethod outperforms traditional approaches, particularly in challenging settings\ninvolving noisy labeled data.\n", "link": "http://arxiv.org/abs/2411.07217v1", "date": "2024-11-11", "relevancy": 2.1152, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4266}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4231}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Selection%20Based%20on%20Wasserstein%20Distance&body=Title%3A%20Feature%20Selection%20Based%20on%20Wasserstein%20Distance%0AAuthor%3A%20Fuwei%20Li%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20feature%20selection%20method%20based%20on%20the%0AWasserstein%20distance.%20Feature%20selection%20plays%20a%20critical%20role%20in%20reducing%20the%0Adimensionality%20of%20input%20data%2C%20thereby%20improving%20machine%20learning%20efficiency%20and%0Ageneralization%20performance.%20Unlike%20traditional%20feature%20selection%20approaches%0Athat%20rely%20on%20criteria%20such%20as%20correlation%20or%20KL%20divergence%2C%20our%20method%0Aleverages%20the%20Wasserstein%20distance%20to%20measure%20the%20similarity%20between%0Adistributions%20of%20selected%20features%20and%20original%20features.%20This%20approach%0Ainherently%20accounts%20for%20similarities%20between%20classes%2C%20making%20it%20robust%20in%0Ascenarios%20involving%20noisy%20labels.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20outperforms%20traditional%20approaches%2C%20particularly%20in%20challenging%20settings%0Ainvolving%20noisy%20labeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Selection%2520Based%2520on%2520Wasserstein%2520Distance%26entry.906535625%3DFuwei%2520Li%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520feature%2520selection%2520method%2520based%2520on%2520the%250AWasserstein%2520distance.%2520Feature%2520selection%2520plays%2520a%2520critical%2520role%2520in%2520reducing%2520the%250Adimensionality%2520of%2520input%2520data%252C%2520thereby%2520improving%2520machine%2520learning%2520efficiency%2520and%250Ageneralization%2520performance.%2520Unlike%2520traditional%2520feature%2520selection%2520approaches%250Athat%2520rely%2520on%2520criteria%2520such%2520as%2520correlation%2520or%2520KL%2520divergence%252C%2520our%2520method%250Aleverages%2520the%2520Wasserstein%2520distance%2520to%2520measure%2520the%2520similarity%2520between%250Adistributions%2520of%2520selected%2520features%2520and%2520original%2520features.%2520This%2520approach%250Ainherently%2520accounts%2520for%2520similarities%2520between%2520classes%252C%2520making%2520it%2520robust%2520in%250Ascenarios%2520involving%2520noisy%2520labels.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Amethod%2520outperforms%2520traditional%2520approaches%252C%2520particularly%2520in%2520challenging%2520settings%250Ainvolving%2520noisy%2520labeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Selection%20Based%20on%20Wasserstein%20Distance&entry.906535625=Fuwei%20Li&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20feature%20selection%20method%20based%20on%20the%0AWasserstein%20distance.%20Feature%20selection%20plays%20a%20critical%20role%20in%20reducing%20the%0Adimensionality%20of%20input%20data%2C%20thereby%20improving%20machine%20learning%20efficiency%20and%0Ageneralization%20performance.%20Unlike%20traditional%20feature%20selection%20approaches%0Athat%20rely%20on%20criteria%20such%20as%20correlation%20or%20KL%20divergence%2C%20our%20method%0Aleverages%20the%20Wasserstein%20distance%20to%20measure%20the%20similarity%20between%0Adistributions%20of%20selected%20features%20and%20original%20features.%20This%20approach%0Ainherently%20accounts%20for%20similarities%20between%20classes%2C%20making%20it%20robust%20in%0Ascenarios%20involving%20noisy%20labels.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20outperforms%20traditional%20approaches%2C%20particularly%20in%20challenging%20settings%0Ainvolving%20noisy%20labeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07217v1&entry.124074799=Read"},
{"title": "Robust Fine-tuning of Zero-shot Models via Variance Reduction", "author": "Beier Zhu and Jiequan Cui and Hanwang Zhang", "abstract": "  When fine-tuning zero-shot models like CLIP, our desideratum is for the\nfine-tuned model to excel in both in-distribution (ID) and out-of-distribution\n(OOD). Recently, ensemble-based models (ESM) have been shown to offer\nsignificant robustness improvement, while preserving high ID accuracy. However,\nour study finds that ESMs do not solve the ID-OOD trade-offs: they achieve peak\nperformance for ID and OOD accuracy at different mixing coefficients. When\noptimized for OOD accuracy, the ensemble model exhibits a noticeable decline in\nID accuracy, and vice versa. In contrast, we propose a sample-wise ensembling\ntechnique that can simultaneously attain the best ID and OOD accuracy without\nthe trade-offs. Specifically, we construct a Zero-Shot Failure (ZSF) set\ncontaining training samples incorrectly predicted by the zero-shot model. For\neach test sample, we calculate its distance to the ZSF set and assign a higher\nweight to the fine-tuned model in the ensemble if the distance is small. We\nterm our method Variance Reduction Fine-tuning (VRF), as it effectively reduces\nthe variance in ensemble predictions, thereby decreasing residual error. On\nImageNet and five derived distribution shifts, our VRF further improves the OOD\naccuracy by 1.5 - 2.0 pp over the ensemble baselines while maintaining or\nincreasing ID accuracy. VRF achieves similar large robustness gains (0.9 - 3.1\npp) on other distribution shifts benchmarks. Codes are available in\nhttps://github.com/BeierZhu/VRF.\n", "link": "http://arxiv.org/abs/2411.06966v1", "date": "2024-11-11", "relevancy": 2.1149, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5455}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5204}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Fine-tuning%20of%20Zero-shot%20Models%20via%20Variance%20Reduction&body=Title%3A%20Robust%20Fine-tuning%20of%20Zero-shot%20Models%20via%20Variance%20Reduction%0AAuthor%3A%20Beier%20Zhu%20and%20Jiequan%20Cui%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20When%20fine-tuning%20zero-shot%20models%20like%20CLIP%2C%20our%20desideratum%20is%20for%20the%0Afine-tuned%20model%20to%20excel%20in%20both%20in-distribution%20%28ID%29%20and%20out-of-distribution%0A%28OOD%29.%20Recently%2C%20ensemble-based%20models%20%28ESM%29%20have%20been%20shown%20to%20offer%0Asignificant%20robustness%20improvement%2C%20while%20preserving%20high%20ID%20accuracy.%20However%2C%0Aour%20study%20finds%20that%20ESMs%20do%20not%20solve%20the%20ID-OOD%20trade-offs%3A%20they%20achieve%20peak%0Aperformance%20for%20ID%20and%20OOD%20accuracy%20at%20different%20mixing%20coefficients.%20When%0Aoptimized%20for%20OOD%20accuracy%2C%20the%20ensemble%20model%20exhibits%20a%20noticeable%20decline%20in%0AID%20accuracy%2C%20and%20vice%20versa.%20In%20contrast%2C%20we%20propose%20a%20sample-wise%20ensembling%0Atechnique%20that%20can%20simultaneously%20attain%20the%20best%20ID%20and%20OOD%20accuracy%20without%0Athe%20trade-offs.%20Specifically%2C%20we%20construct%20a%20Zero-Shot%20Failure%20%28ZSF%29%20set%0Acontaining%20training%20samples%20incorrectly%20predicted%20by%20the%20zero-shot%20model.%20For%0Aeach%20test%20sample%2C%20we%20calculate%20its%20distance%20to%20the%20ZSF%20set%20and%20assign%20a%20higher%0Aweight%20to%20the%20fine-tuned%20model%20in%20the%20ensemble%20if%20the%20distance%20is%20small.%20We%0Aterm%20our%20method%20Variance%20Reduction%20Fine-tuning%20%28VRF%29%2C%20as%20it%20effectively%20reduces%0Athe%20variance%20in%20ensemble%20predictions%2C%20thereby%20decreasing%20residual%20error.%20On%0AImageNet%20and%20five%20derived%20distribution%20shifts%2C%20our%20VRF%20further%20improves%20the%20OOD%0Aaccuracy%20by%201.5%20-%202.0%20pp%20over%20the%20ensemble%20baselines%20while%20maintaining%20or%0Aincreasing%20ID%20accuracy.%20VRF%20achieves%20similar%20large%20robustness%20gains%20%280.9%20-%203.1%0App%29%20on%20other%20distribution%20shifts%20benchmarks.%20Codes%20are%20available%20in%0Ahttps%3A//github.com/BeierZhu/VRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Fine-tuning%2520of%2520Zero-shot%2520Models%2520via%2520Variance%2520Reduction%26entry.906535625%3DBeier%2520Zhu%2520and%2520Jiequan%2520Cui%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520When%2520fine-tuning%2520zero-shot%2520models%2520like%2520CLIP%252C%2520our%2520desideratum%2520is%2520for%2520the%250Afine-tuned%2520model%2520to%2520excel%2520in%2520both%2520in-distribution%2520%2528ID%2529%2520and%2520out-of-distribution%250A%2528OOD%2529.%2520Recently%252C%2520ensemble-based%2520models%2520%2528ESM%2529%2520have%2520been%2520shown%2520to%2520offer%250Asignificant%2520robustness%2520improvement%252C%2520while%2520preserving%2520high%2520ID%2520accuracy.%2520However%252C%250Aour%2520study%2520finds%2520that%2520ESMs%2520do%2520not%2520solve%2520the%2520ID-OOD%2520trade-offs%253A%2520they%2520achieve%2520peak%250Aperformance%2520for%2520ID%2520and%2520OOD%2520accuracy%2520at%2520different%2520mixing%2520coefficients.%2520When%250Aoptimized%2520for%2520OOD%2520accuracy%252C%2520the%2520ensemble%2520model%2520exhibits%2520a%2520noticeable%2520decline%2520in%250AID%2520accuracy%252C%2520and%2520vice%2520versa.%2520In%2520contrast%252C%2520we%2520propose%2520a%2520sample-wise%2520ensembling%250Atechnique%2520that%2520can%2520simultaneously%2520attain%2520the%2520best%2520ID%2520and%2520OOD%2520accuracy%2520without%250Athe%2520trade-offs.%2520Specifically%252C%2520we%2520construct%2520a%2520Zero-Shot%2520Failure%2520%2528ZSF%2529%2520set%250Acontaining%2520training%2520samples%2520incorrectly%2520predicted%2520by%2520the%2520zero-shot%2520model.%2520For%250Aeach%2520test%2520sample%252C%2520we%2520calculate%2520its%2520distance%2520to%2520the%2520ZSF%2520set%2520and%2520assign%2520a%2520higher%250Aweight%2520to%2520the%2520fine-tuned%2520model%2520in%2520the%2520ensemble%2520if%2520the%2520distance%2520is%2520small.%2520We%250Aterm%2520our%2520method%2520Variance%2520Reduction%2520Fine-tuning%2520%2528VRF%2529%252C%2520as%2520it%2520effectively%2520reduces%250Athe%2520variance%2520in%2520ensemble%2520predictions%252C%2520thereby%2520decreasing%2520residual%2520error.%2520On%250AImageNet%2520and%2520five%2520derived%2520distribution%2520shifts%252C%2520our%2520VRF%2520further%2520improves%2520the%2520OOD%250Aaccuracy%2520by%25201.5%2520-%25202.0%2520pp%2520over%2520the%2520ensemble%2520baselines%2520while%2520maintaining%2520or%250Aincreasing%2520ID%2520accuracy.%2520VRF%2520achieves%2520similar%2520large%2520robustness%2520gains%2520%25280.9%2520-%25203.1%250App%2529%2520on%2520other%2520distribution%2520shifts%2520benchmarks.%2520Codes%2520are%2520available%2520in%250Ahttps%253A//github.com/BeierZhu/VRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Fine-tuning%20of%20Zero-shot%20Models%20via%20Variance%20Reduction&entry.906535625=Beier%20Zhu%20and%20Jiequan%20Cui%20and%20Hanwang%20Zhang&entry.1292438233=%20%20When%20fine-tuning%20zero-shot%20models%20like%20CLIP%2C%20our%20desideratum%20is%20for%20the%0Afine-tuned%20model%20to%20excel%20in%20both%20in-distribution%20%28ID%29%20and%20out-of-distribution%0A%28OOD%29.%20Recently%2C%20ensemble-based%20models%20%28ESM%29%20have%20been%20shown%20to%20offer%0Asignificant%20robustness%20improvement%2C%20while%20preserving%20high%20ID%20accuracy.%20However%2C%0Aour%20study%20finds%20that%20ESMs%20do%20not%20solve%20the%20ID-OOD%20trade-offs%3A%20they%20achieve%20peak%0Aperformance%20for%20ID%20and%20OOD%20accuracy%20at%20different%20mixing%20coefficients.%20When%0Aoptimized%20for%20OOD%20accuracy%2C%20the%20ensemble%20model%20exhibits%20a%20noticeable%20decline%20in%0AID%20accuracy%2C%20and%20vice%20versa.%20In%20contrast%2C%20we%20propose%20a%20sample-wise%20ensembling%0Atechnique%20that%20can%20simultaneously%20attain%20the%20best%20ID%20and%20OOD%20accuracy%20without%0Athe%20trade-offs.%20Specifically%2C%20we%20construct%20a%20Zero-Shot%20Failure%20%28ZSF%29%20set%0Acontaining%20training%20samples%20incorrectly%20predicted%20by%20the%20zero-shot%20model.%20For%0Aeach%20test%20sample%2C%20we%20calculate%20its%20distance%20to%20the%20ZSF%20set%20and%20assign%20a%20higher%0Aweight%20to%20the%20fine-tuned%20model%20in%20the%20ensemble%20if%20the%20distance%20is%20small.%20We%0Aterm%20our%20method%20Variance%20Reduction%20Fine-tuning%20%28VRF%29%2C%20as%20it%20effectively%20reduces%0Athe%20variance%20in%20ensemble%20predictions%2C%20thereby%20decreasing%20residual%20error.%20On%0AImageNet%20and%20five%20derived%20distribution%20shifts%2C%20our%20VRF%20further%20improves%20the%20OOD%0Aaccuracy%20by%201.5%20-%202.0%20pp%20over%20the%20ensemble%20baselines%20while%20maintaining%20or%0Aincreasing%20ID%20accuracy.%20VRF%20achieves%20similar%20large%20robustness%20gains%20%280.9%20-%203.1%0App%29%20on%20other%20distribution%20shifts%20benchmarks.%20Codes%20are%20available%20in%0Ahttps%3A//github.com/BeierZhu/VRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06966v1&entry.124074799=Read"},
{"title": "Permutative redundancy and uncertainty of the objective in deep learning", "author": "Vacslav Glukhov", "abstract": "  Implications of uncertain objective functions and permutative symmetry of\ntraditional deep learning architectures are discussed. It is shown that\ntraditional architectures are polluted by an astronomical number of equivalent\nglobal and local optima. Uncertainty of the objective makes local optima\nunattainable, and, as the size of the network grows, the global optimization\nlandscape likely becomes a tangled web of valleys and ridges. Some remedies\nwhich reduce or eliminate ghost optima are discussed including forced\npre-pruning, re-ordering, ortho-polynomial activations, and modular\nbio-inspired architectures.\n", "link": "http://arxiv.org/abs/2411.07008v1", "date": "2024-11-11", "relevancy": 2.114, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5379}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5311}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Permutative%20redundancy%20and%20uncertainty%20of%20the%20objective%20in%20deep%20learning&body=Title%3A%20Permutative%20redundancy%20and%20uncertainty%20of%20the%20objective%20in%20deep%20learning%0AAuthor%3A%20Vacslav%20Glukhov%0AAbstract%3A%20%20%20Implications%20of%20uncertain%20objective%20functions%20and%20permutative%20symmetry%20of%0Atraditional%20deep%20learning%20architectures%20are%20discussed.%20It%20is%20shown%20that%0Atraditional%20architectures%20are%20polluted%20by%20an%20astronomical%20number%20of%20equivalent%0Aglobal%20and%20local%20optima.%20Uncertainty%20of%20the%20objective%20makes%20local%20optima%0Aunattainable%2C%20and%2C%20as%20the%20size%20of%20the%20network%20grows%2C%20the%20global%20optimization%0Alandscape%20likely%20becomes%20a%20tangled%20web%20of%20valleys%20and%20ridges.%20Some%20remedies%0Awhich%20reduce%20or%20eliminate%20ghost%20optima%20are%20discussed%20including%20forced%0Apre-pruning%2C%20re-ordering%2C%20ortho-polynomial%20activations%2C%20and%20modular%0Abio-inspired%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPermutative%2520redundancy%2520and%2520uncertainty%2520of%2520the%2520objective%2520in%2520deep%2520learning%26entry.906535625%3DVacslav%2520Glukhov%26entry.1292438233%3D%2520%2520Implications%2520of%2520uncertain%2520objective%2520functions%2520and%2520permutative%2520symmetry%2520of%250Atraditional%2520deep%2520learning%2520architectures%2520are%2520discussed.%2520It%2520is%2520shown%2520that%250Atraditional%2520architectures%2520are%2520polluted%2520by%2520an%2520astronomical%2520number%2520of%2520equivalent%250Aglobal%2520and%2520local%2520optima.%2520Uncertainty%2520of%2520the%2520objective%2520makes%2520local%2520optima%250Aunattainable%252C%2520and%252C%2520as%2520the%2520size%2520of%2520the%2520network%2520grows%252C%2520the%2520global%2520optimization%250Alandscape%2520likely%2520becomes%2520a%2520tangled%2520web%2520of%2520valleys%2520and%2520ridges.%2520Some%2520remedies%250Awhich%2520reduce%2520or%2520eliminate%2520ghost%2520optima%2520are%2520discussed%2520including%2520forced%250Apre-pruning%252C%2520re-ordering%252C%2520ortho-polynomial%2520activations%252C%2520and%2520modular%250Abio-inspired%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Permutative%20redundancy%20and%20uncertainty%20of%20the%20objective%20in%20deep%20learning&entry.906535625=Vacslav%20Glukhov&entry.1292438233=%20%20Implications%20of%20uncertain%20objective%20functions%20and%20permutative%20symmetry%20of%0Atraditional%20deep%20learning%20architectures%20are%20discussed.%20It%20is%20shown%20that%0Atraditional%20architectures%20are%20polluted%20by%20an%20astronomical%20number%20of%20equivalent%0Aglobal%20and%20local%20optima.%20Uncertainty%20of%20the%20objective%20makes%20local%20optima%0Aunattainable%2C%20and%2C%20as%20the%20size%20of%20the%20network%20grows%2C%20the%20global%20optimization%0Alandscape%20likely%20becomes%20a%20tangled%20web%20of%20valleys%20and%20ridges.%20Some%20remedies%0Awhich%20reduce%20or%20eliminate%20ghost%20optima%20are%20discussed%20including%20forced%0Apre-pruning%2C%20re-ordering%2C%20ortho-polynomial%20activations%2C%20and%20modular%0Abio-inspired%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07008v1&entry.124074799=Read"},
{"title": "Efficient Unsupervised Domain Adaptation Regression for Spatial-Temporal\n  Air Quality Sensor Fusion", "author": "Keivan Faghih Niresi and Ismail Nejjar and Olga Fink", "abstract": "  The deployment of affordable Internet of Things (IoT) sensors for air\npollution monitoring has increased in recent years due to their scalability and\ncost-effectiveness. However, accurately calibrating these sensors in\nuncontrolled environments remains a significant challenge. While expensive\nreference sensors can provide accurate ground truth data, they are often\ndeployed on a limited scale due to high costs, leading to a scarcity of labeled\ndata. In diverse urban environments, data distributions constantly shift due to\nvarying factors such as traffic patterns, industrial activities, and weather\nconditions, which impact sensor readings. Consequently, traditional machine\nlearning models -- despite their increasing deployment for environmental sensor\ncalibration -- often struggle to provide reliable pollutant measurements across\ndifferent locations due to domain shifts. To address these challenges, we\npropose a novel unsupervised domain adaptation (UDA) method specifically\ntailored for regression tasks on graph-structured data. Our approach leverages\nGraph Neural Networks (GNNs) to model the relationships between sensors. To\neffectively capture critical spatial-temporal interactions, we incorporate\nspatial-temporal graph neural networks (STGNNs), which extend GNNs by\nincorporating temporal dynamics. To handle the resulting larger embeddings, we\npropose a domain adaptation method using a closed-form solution inspired by the\nTikhonov-regularized least-squares problem. This method leverages Cholesky\ndecomposition and power iteration to align the subspaces between source and\ntarget domains. By aligning these subspaces, our approach allows low-cost IoT\nsensors to learn calibration parameters from expensive reference sensors. This\nfacilitates reliable pollutant measurements in new locations without the need\nfor additional costly equipment.\n", "link": "http://arxiv.org/abs/2411.06917v1", "date": "2024-11-11", "relevancy": 2.1067, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5306}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5244}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Unsupervised%20Domain%20Adaptation%20Regression%20for%20Spatial-Temporal%0A%20%20Air%20Quality%20Sensor%20Fusion&body=Title%3A%20Efficient%20Unsupervised%20Domain%20Adaptation%20Regression%20for%20Spatial-Temporal%0A%20%20Air%20Quality%20Sensor%20Fusion%0AAuthor%3A%20Keivan%20Faghih%20Niresi%20and%20Ismail%20Nejjar%20and%20Olga%20Fink%0AAbstract%3A%20%20%20The%20deployment%20of%20affordable%20Internet%20of%20Things%20%28IoT%29%20sensors%20for%20air%0Apollution%20monitoring%20has%20increased%20in%20recent%20years%20due%20to%20their%20scalability%20and%0Acost-effectiveness.%20However%2C%20accurately%20calibrating%20these%20sensors%20in%0Auncontrolled%20environments%20remains%20a%20significant%20challenge.%20While%20expensive%0Areference%20sensors%20can%20provide%20accurate%20ground%20truth%20data%2C%20they%20are%20often%0Adeployed%20on%20a%20limited%20scale%20due%20to%20high%20costs%2C%20leading%20to%20a%20scarcity%20of%20labeled%0Adata.%20In%20diverse%20urban%20environments%2C%20data%20distributions%20constantly%20shift%20due%20to%0Avarying%20factors%20such%20as%20traffic%20patterns%2C%20industrial%20activities%2C%20and%20weather%0Aconditions%2C%20which%20impact%20sensor%20readings.%20Consequently%2C%20traditional%20machine%0Alearning%20models%20--%20despite%20their%20increasing%20deployment%20for%20environmental%20sensor%0Acalibration%20--%20often%20struggle%20to%20provide%20reliable%20pollutant%20measurements%20across%0Adifferent%20locations%20due%20to%20domain%20shifts.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20novel%20unsupervised%20domain%20adaptation%20%28UDA%29%20method%20specifically%0Atailored%20for%20regression%20tasks%20on%20graph-structured%20data.%20Our%20approach%20leverages%0AGraph%20Neural%20Networks%20%28GNNs%29%20to%20model%20the%20relationships%20between%20sensors.%20To%0Aeffectively%20capture%20critical%20spatial-temporal%20interactions%2C%20we%20incorporate%0Aspatial-temporal%20graph%20neural%20networks%20%28STGNNs%29%2C%20which%20extend%20GNNs%20by%0Aincorporating%20temporal%20dynamics.%20To%20handle%20the%20resulting%20larger%20embeddings%2C%20we%0Apropose%20a%20domain%20adaptation%20method%20using%20a%20closed-form%20solution%20inspired%20by%20the%0ATikhonov-regularized%20least-squares%20problem.%20This%20method%20leverages%20Cholesky%0Adecomposition%20and%20power%20iteration%20to%20align%20the%20subspaces%20between%20source%20and%0Atarget%20domains.%20By%20aligning%20these%20subspaces%2C%20our%20approach%20allows%20low-cost%20IoT%0Asensors%20to%20learn%20calibration%20parameters%20from%20expensive%20reference%20sensors.%20This%0Afacilitates%20reliable%20pollutant%20measurements%20in%20new%20locations%20without%20the%20need%0Afor%20additional%20costly%20equipment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Unsupervised%2520Domain%2520Adaptation%2520Regression%2520for%2520Spatial-Temporal%250A%2520%2520Air%2520Quality%2520Sensor%2520Fusion%26entry.906535625%3DKeivan%2520Faghih%2520Niresi%2520and%2520Ismail%2520Nejjar%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520affordable%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520sensors%2520for%2520air%250Apollution%2520monitoring%2520has%2520increased%2520in%2520recent%2520years%2520due%2520to%2520their%2520scalability%2520and%250Acost-effectiveness.%2520However%252C%2520accurately%2520calibrating%2520these%2520sensors%2520in%250Auncontrolled%2520environments%2520remains%2520a%2520significant%2520challenge.%2520While%2520expensive%250Areference%2520sensors%2520can%2520provide%2520accurate%2520ground%2520truth%2520data%252C%2520they%2520are%2520often%250Adeployed%2520on%2520a%2520limited%2520scale%2520due%2520to%2520high%2520costs%252C%2520leading%2520to%2520a%2520scarcity%2520of%2520labeled%250Adata.%2520In%2520diverse%2520urban%2520environments%252C%2520data%2520distributions%2520constantly%2520shift%2520due%2520to%250Avarying%2520factors%2520such%2520as%2520traffic%2520patterns%252C%2520industrial%2520activities%252C%2520and%2520weather%250Aconditions%252C%2520which%2520impact%2520sensor%2520readings.%2520Consequently%252C%2520traditional%2520machine%250Alearning%2520models%2520--%2520despite%2520their%2520increasing%2520deployment%2520for%2520environmental%2520sensor%250Acalibration%2520--%2520often%2520struggle%2520to%2520provide%2520reliable%2520pollutant%2520measurements%2520across%250Adifferent%2520locations%2520due%2520to%2520domain%2520shifts.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520novel%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520method%2520specifically%250Atailored%2520for%2520regression%2520tasks%2520on%2520graph-structured%2520data.%2520Our%2520approach%2520leverages%250AGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%2520model%2520the%2520relationships%2520between%2520sensors.%2520To%250Aeffectively%2520capture%2520critical%2520spatial-temporal%2520interactions%252C%2520we%2520incorporate%250Aspatial-temporal%2520graph%2520neural%2520networks%2520%2528STGNNs%2529%252C%2520which%2520extend%2520GNNs%2520by%250Aincorporating%2520temporal%2520dynamics.%2520To%2520handle%2520the%2520resulting%2520larger%2520embeddings%252C%2520we%250Apropose%2520a%2520domain%2520adaptation%2520method%2520using%2520a%2520closed-form%2520solution%2520inspired%2520by%2520the%250ATikhonov-regularized%2520least-squares%2520problem.%2520This%2520method%2520leverages%2520Cholesky%250Adecomposition%2520and%2520power%2520iteration%2520to%2520align%2520the%2520subspaces%2520between%2520source%2520and%250Atarget%2520domains.%2520By%2520aligning%2520these%2520subspaces%252C%2520our%2520approach%2520allows%2520low-cost%2520IoT%250Asensors%2520to%2520learn%2520calibration%2520parameters%2520from%2520expensive%2520reference%2520sensors.%2520This%250Afacilitates%2520reliable%2520pollutant%2520measurements%2520in%2520new%2520locations%2520without%2520the%2520need%250Afor%2520additional%2520costly%2520equipment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Unsupervised%20Domain%20Adaptation%20Regression%20for%20Spatial-Temporal%0A%20%20Air%20Quality%20Sensor%20Fusion&entry.906535625=Keivan%20Faghih%20Niresi%20and%20Ismail%20Nejjar%20and%20Olga%20Fink&entry.1292438233=%20%20The%20deployment%20of%20affordable%20Internet%20of%20Things%20%28IoT%29%20sensors%20for%20air%0Apollution%20monitoring%20has%20increased%20in%20recent%20years%20due%20to%20their%20scalability%20and%0Acost-effectiveness.%20However%2C%20accurately%20calibrating%20these%20sensors%20in%0Auncontrolled%20environments%20remains%20a%20significant%20challenge.%20While%20expensive%0Areference%20sensors%20can%20provide%20accurate%20ground%20truth%20data%2C%20they%20are%20often%0Adeployed%20on%20a%20limited%20scale%20due%20to%20high%20costs%2C%20leading%20to%20a%20scarcity%20of%20labeled%0Adata.%20In%20diverse%20urban%20environments%2C%20data%20distributions%20constantly%20shift%20due%20to%0Avarying%20factors%20such%20as%20traffic%20patterns%2C%20industrial%20activities%2C%20and%20weather%0Aconditions%2C%20which%20impact%20sensor%20readings.%20Consequently%2C%20traditional%20machine%0Alearning%20models%20--%20despite%20their%20increasing%20deployment%20for%20environmental%20sensor%0Acalibration%20--%20often%20struggle%20to%20provide%20reliable%20pollutant%20measurements%20across%0Adifferent%20locations%20due%20to%20domain%20shifts.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20novel%20unsupervised%20domain%20adaptation%20%28UDA%29%20method%20specifically%0Atailored%20for%20regression%20tasks%20on%20graph-structured%20data.%20Our%20approach%20leverages%0AGraph%20Neural%20Networks%20%28GNNs%29%20to%20model%20the%20relationships%20between%20sensors.%20To%0Aeffectively%20capture%20critical%20spatial-temporal%20interactions%2C%20we%20incorporate%0Aspatial-temporal%20graph%20neural%20networks%20%28STGNNs%29%2C%20which%20extend%20GNNs%20by%0Aincorporating%20temporal%20dynamics.%20To%20handle%20the%20resulting%20larger%20embeddings%2C%20we%0Apropose%20a%20domain%20adaptation%20method%20using%20a%20closed-form%20solution%20inspired%20by%20the%0ATikhonov-regularized%20least-squares%20problem.%20This%20method%20leverages%20Cholesky%0Adecomposition%20and%20power%20iteration%20to%20align%20the%20subspaces%20between%20source%20and%0Atarget%20domains.%20By%20aligning%20these%20subspaces%2C%20our%20approach%20allows%20low-cost%20IoT%0Asensors%20to%20learn%20calibration%20parameters%20from%20expensive%20reference%20sensors.%20This%0Afacilitates%20reliable%20pollutant%20measurements%20in%20new%20locations%20without%20the%20need%0Afor%20additional%20costly%20equipment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06917v1&entry.124074799=Read"},
{"title": "Distributed Spatial Awareness for Robot Swarms", "author": "Simon Jones and Sabine Hauert", "abstract": "  Building a distributed spatial awareness within a swarm of locally sensing\nand communicating robots enables new swarm algorithms. We use local\nobservations by robots of each other and Gaussian Belief Propagation message\npassing combined with continuous swarm movement to build a global and\ndistributed swarm-centric frame of reference. With low bandwidth and\ncomputation requirements, this shared reference frame allows new swarm\nalgorithms. We characterise the system in simulation and demonstrate two\nexample algorithms.\n", "link": "http://arxiv.org/abs/2411.07056v1", "date": "2024-11-11", "relevancy": 2.1045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5409}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5252}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Spatial%20Awareness%20for%20Robot%20Swarms&body=Title%3A%20Distributed%20Spatial%20Awareness%20for%20Robot%20Swarms%0AAuthor%3A%20Simon%20Jones%20and%20Sabine%20Hauert%0AAbstract%3A%20%20%20Building%20a%20distributed%20spatial%20awareness%20within%20a%20swarm%20of%20locally%20sensing%0Aand%20communicating%20robots%20enables%20new%20swarm%20algorithms.%20We%20use%20local%0Aobservations%20by%20robots%20of%20each%20other%20and%20Gaussian%20Belief%20Propagation%20message%0Apassing%20combined%20with%20continuous%20swarm%20movement%20to%20build%20a%20global%20and%0Adistributed%20swarm-centric%20frame%20of%20reference.%20With%20low%20bandwidth%20and%0Acomputation%20requirements%2C%20this%20shared%20reference%20frame%20allows%20new%20swarm%0Aalgorithms.%20We%20characterise%20the%20system%20in%20simulation%20and%20demonstrate%20two%0Aexample%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Spatial%2520Awareness%2520for%2520Robot%2520Swarms%26entry.906535625%3DSimon%2520Jones%2520and%2520Sabine%2520Hauert%26entry.1292438233%3D%2520%2520Building%2520a%2520distributed%2520spatial%2520awareness%2520within%2520a%2520swarm%2520of%2520locally%2520sensing%250Aand%2520communicating%2520robots%2520enables%2520new%2520swarm%2520algorithms.%2520We%2520use%2520local%250Aobservations%2520by%2520robots%2520of%2520each%2520other%2520and%2520Gaussian%2520Belief%2520Propagation%2520message%250Apassing%2520combined%2520with%2520continuous%2520swarm%2520movement%2520to%2520build%2520a%2520global%2520and%250Adistributed%2520swarm-centric%2520frame%2520of%2520reference.%2520With%2520low%2520bandwidth%2520and%250Acomputation%2520requirements%252C%2520this%2520shared%2520reference%2520frame%2520allows%2520new%2520swarm%250Aalgorithms.%2520We%2520characterise%2520the%2520system%2520in%2520simulation%2520and%2520demonstrate%2520two%250Aexample%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Spatial%20Awareness%20for%20Robot%20Swarms&entry.906535625=Simon%20Jones%20and%20Sabine%20Hauert&entry.1292438233=%20%20Building%20a%20distributed%20spatial%20awareness%20within%20a%20swarm%20of%20locally%20sensing%0Aand%20communicating%20robots%20enables%20new%20swarm%20algorithms.%20We%20use%20local%0Aobservations%20by%20robots%20of%20each%20other%20and%20Gaussian%20Belief%20Propagation%20message%0Apassing%20combined%20with%20continuous%20swarm%20movement%20to%20build%20a%20global%20and%0Adistributed%20swarm-centric%20frame%20of%20reference.%20With%20low%20bandwidth%20and%0Acomputation%20requirements%2C%20this%20shared%20reference%20frame%20allows%20new%20swarm%0Aalgorithms.%20We%20characterise%20the%20system%20in%20simulation%20and%20demonstrate%20two%0Aexample%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07056v1&entry.124074799=Read"},
{"title": "Content-Style Learning from Unaligned Domains: Identifiability under\n  Unknown Latent Dimensions", "author": "Sagar Shrestha and Xiao Fu", "abstract": "  Understanding identifiability of latent content and style variables from\nunaligned multi-domain data is essential for tasks such as domain translation\nand data generation. Existing works on content-style identification were often\ndeveloped under somewhat stringent conditions, e.g., that all latent components\nare mutually independent and that the dimensions of the content and style\nvariables are known. We introduce a new analytical framework via cross-domain\n\\textit{latent distribution matching} (LDM), which establishes content-style\nidentifiability under substantially more relaxed conditions. Specifically, we\nshow that restrictive assumptions such as component-wise independence of the\nlatent variables can be removed. Most notably, we prove that prior knowledge of\nthe content and style dimensions is not necessary for ensuring identifiability,\nif sparsity constraints are properly imposed onto the learned latent\nrepresentations. Bypassing the knowledge of the exact latent dimension has been\na longstanding aspiration in unsupervised representation learning -- our\nanalysis is the first to underpin its theoretical and practical viability. On\nthe implementation side, we recast the LDM formulation into a regularized\nmulti-domain GAN loss with coupled latent variables. We show that the\nreformulation is equivalent to LDM under mild conditions -- yet requiring\nconsiderably less computational resource. Experiments corroborate with our\ntheoretical claims.\n", "link": "http://arxiv.org/abs/2411.03755v2", "date": "2024-11-11", "relevancy": 2.0961, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5286}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5238}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Content-Style%20Learning%20from%20Unaligned%20Domains%3A%20Identifiability%20under%0A%20%20Unknown%20Latent%20Dimensions&body=Title%3A%20Content-Style%20Learning%20from%20Unaligned%20Domains%3A%20Identifiability%20under%0A%20%20Unknown%20Latent%20Dimensions%0AAuthor%3A%20Sagar%20Shrestha%20and%20Xiao%20Fu%0AAbstract%3A%20%20%20Understanding%20identifiability%20of%20latent%20content%20and%20style%20variables%20from%0Aunaligned%20multi-domain%20data%20is%20essential%20for%20tasks%20such%20as%20domain%20translation%0Aand%20data%20generation.%20Existing%20works%20on%20content-style%20identification%20were%20often%0Adeveloped%20under%20somewhat%20stringent%20conditions%2C%20e.g.%2C%20that%20all%20latent%20components%0Aare%20mutually%20independent%20and%20that%20the%20dimensions%20of%20the%20content%20and%20style%0Avariables%20are%20known.%20We%20introduce%20a%20new%20analytical%20framework%20via%20cross-domain%0A%5Ctextit%7Blatent%20distribution%20matching%7D%20%28LDM%29%2C%20which%20establishes%20content-style%0Aidentifiability%20under%20substantially%20more%20relaxed%20conditions.%20Specifically%2C%20we%0Ashow%20that%20restrictive%20assumptions%20such%20as%20component-wise%20independence%20of%20the%0Alatent%20variables%20can%20be%20removed.%20Most%20notably%2C%20we%20prove%20that%20prior%20knowledge%20of%0Athe%20content%20and%20style%20dimensions%20is%20not%20necessary%20for%20ensuring%20identifiability%2C%0Aif%20sparsity%20constraints%20are%20properly%20imposed%20onto%20the%20learned%20latent%0Arepresentations.%20Bypassing%20the%20knowledge%20of%20the%20exact%20latent%20dimension%20has%20been%0Aa%20longstanding%20aspiration%20in%20unsupervised%20representation%20learning%20--%20our%0Aanalysis%20is%20the%20first%20to%20underpin%20its%20theoretical%20and%20practical%20viability.%20On%0Athe%20implementation%20side%2C%20we%20recast%20the%20LDM%20formulation%20into%20a%20regularized%0Amulti-domain%20GAN%20loss%20with%20coupled%20latent%20variables.%20We%20show%20that%20the%0Areformulation%20is%20equivalent%20to%20LDM%20under%20mild%20conditions%20--%20yet%20requiring%0Aconsiderably%20less%20computational%20resource.%20Experiments%20corroborate%20with%20our%0Atheoretical%20claims.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContent-Style%2520Learning%2520from%2520Unaligned%2520Domains%253A%2520Identifiability%2520under%250A%2520%2520Unknown%2520Latent%2520Dimensions%26entry.906535625%3DSagar%2520Shrestha%2520and%2520Xiao%2520Fu%26entry.1292438233%3D%2520%2520Understanding%2520identifiability%2520of%2520latent%2520content%2520and%2520style%2520variables%2520from%250Aunaligned%2520multi-domain%2520data%2520is%2520essential%2520for%2520tasks%2520such%2520as%2520domain%2520translation%250Aand%2520data%2520generation.%2520Existing%2520works%2520on%2520content-style%2520identification%2520were%2520often%250Adeveloped%2520under%2520somewhat%2520stringent%2520conditions%252C%2520e.g.%252C%2520that%2520all%2520latent%2520components%250Aare%2520mutually%2520independent%2520and%2520that%2520the%2520dimensions%2520of%2520the%2520content%2520and%2520style%250Avariables%2520are%2520known.%2520We%2520introduce%2520a%2520new%2520analytical%2520framework%2520via%2520cross-domain%250A%255Ctextit%257Blatent%2520distribution%2520matching%257D%2520%2528LDM%2529%252C%2520which%2520establishes%2520content-style%250Aidentifiability%2520under%2520substantially%2520more%2520relaxed%2520conditions.%2520Specifically%252C%2520we%250Ashow%2520that%2520restrictive%2520assumptions%2520such%2520as%2520component-wise%2520independence%2520of%2520the%250Alatent%2520variables%2520can%2520be%2520removed.%2520Most%2520notably%252C%2520we%2520prove%2520that%2520prior%2520knowledge%2520of%250Athe%2520content%2520and%2520style%2520dimensions%2520is%2520not%2520necessary%2520for%2520ensuring%2520identifiability%252C%250Aif%2520sparsity%2520constraints%2520are%2520properly%2520imposed%2520onto%2520the%2520learned%2520latent%250Arepresentations.%2520Bypassing%2520the%2520knowledge%2520of%2520the%2520exact%2520latent%2520dimension%2520has%2520been%250Aa%2520longstanding%2520aspiration%2520in%2520unsupervised%2520representation%2520learning%2520--%2520our%250Aanalysis%2520is%2520the%2520first%2520to%2520underpin%2520its%2520theoretical%2520and%2520practical%2520viability.%2520On%250Athe%2520implementation%2520side%252C%2520we%2520recast%2520the%2520LDM%2520formulation%2520into%2520a%2520regularized%250Amulti-domain%2520GAN%2520loss%2520with%2520coupled%2520latent%2520variables.%2520We%2520show%2520that%2520the%250Areformulation%2520is%2520equivalent%2520to%2520LDM%2520under%2520mild%2520conditions%2520--%2520yet%2520requiring%250Aconsiderably%2520less%2520computational%2520resource.%2520Experiments%2520corroborate%2520with%2520our%250Atheoretical%2520claims.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Content-Style%20Learning%20from%20Unaligned%20Domains%3A%20Identifiability%20under%0A%20%20Unknown%20Latent%20Dimensions&entry.906535625=Sagar%20Shrestha%20and%20Xiao%20Fu&entry.1292438233=%20%20Understanding%20identifiability%20of%20latent%20content%20and%20style%20variables%20from%0Aunaligned%20multi-domain%20data%20is%20essential%20for%20tasks%20such%20as%20domain%20translation%0Aand%20data%20generation.%20Existing%20works%20on%20content-style%20identification%20were%20often%0Adeveloped%20under%20somewhat%20stringent%20conditions%2C%20e.g.%2C%20that%20all%20latent%20components%0Aare%20mutually%20independent%20and%20that%20the%20dimensions%20of%20the%20content%20and%20style%0Avariables%20are%20known.%20We%20introduce%20a%20new%20analytical%20framework%20via%20cross-domain%0A%5Ctextit%7Blatent%20distribution%20matching%7D%20%28LDM%29%2C%20which%20establishes%20content-style%0Aidentifiability%20under%20substantially%20more%20relaxed%20conditions.%20Specifically%2C%20we%0Ashow%20that%20restrictive%20assumptions%20such%20as%20component-wise%20independence%20of%20the%0Alatent%20variables%20can%20be%20removed.%20Most%20notably%2C%20we%20prove%20that%20prior%20knowledge%20of%0Athe%20content%20and%20style%20dimensions%20is%20not%20necessary%20for%20ensuring%20identifiability%2C%0Aif%20sparsity%20constraints%20are%20properly%20imposed%20onto%20the%20learned%20latent%0Arepresentations.%20Bypassing%20the%20knowledge%20of%20the%20exact%20latent%20dimension%20has%20been%0Aa%20longstanding%20aspiration%20in%20unsupervised%20representation%20learning%20--%20our%0Aanalysis%20is%20the%20first%20to%20underpin%20its%20theoretical%20and%20practical%20viability.%20On%0Athe%20implementation%20side%2C%20we%20recast%20the%20LDM%20formulation%20into%20a%20regularized%0Amulti-domain%20GAN%20loss%20with%20coupled%20latent%20variables.%20We%20show%20that%20the%0Areformulation%20is%20equivalent%20to%20LDM%20under%20mild%20conditions%20--%20yet%20requiring%0Aconsiderably%20less%20computational%20resource.%20Experiments%20corroborate%20with%20our%0Atheoretical%20claims.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03755v2&entry.124074799=Read"},
{"title": "ActionAtlas: A VideoQA Benchmark for Domain-specialized Action\n  Recognition", "author": "Mohammadreza Salehi and Jae Sung Park and Tanush Yadav and Aditya Kusupati and Ranjay Krishna and Yejin Choi and Hannaneh Hajishirzi and Ali Farhadi", "abstract": "  Our world is full of varied actions and moves across specialized domains that\nwe, as humans, strive to identify and understand. Within any single domain,\nactions can often appear quite similar, making it challenging for deep models\nto distinguish them accurately. To evaluate the effectiveness of multimodal\nfoundation models in helping us recognize such actions, we present ActionAtlas\nv1.0, a multiple-choice video question answering benchmark featuring short\nvideos across various sports. Each video in the dataset is paired with a\nquestion and four or five choices. The question pinpoints specific individuals,\nasking which choice \"best\" describes their action within a certain temporal\ncontext. Overall, the dataset includes 934 videos showcasing 580 unique actions\nacross 56 sports, with a total of 1896 actions within choices. Unlike most\nexisting video question answering benchmarks that only cover simplistic\nactions, often identifiable from a single frame, ActionAtlas focuses on\nintricate movements and rigorously tests the model's capability to discern\nsubtle differences between moves that look similar within each domain. We\nevaluate open and proprietary foundation models on this benchmark, finding that\nthe best model, GPT-4o, achieves a maximum accuracy of 45.52%. Meanwhile,\nNon-expert crowd workers, provided with action description for each choice,\nachieve 61.64% accuracy, where random chance is approximately 21%. Our findings\nwith state-of-the-art models indicate that having a high frame sampling rate is\nimportant for accurately recognizing actions in ActionAtlas, a feature that\nsome leading proprietary video models, such as Gemini, do not include in their\ndefault configuration.\n", "link": "http://arxiv.org/abs/2410.05774v4", "date": "2024-11-11", "relevancy": 2.0932, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ActionAtlas%3A%20A%20VideoQA%20Benchmark%20for%20Domain-specialized%20Action%0A%20%20Recognition&body=Title%3A%20ActionAtlas%3A%20A%20VideoQA%20Benchmark%20for%20Domain-specialized%20Action%0A%20%20Recognition%0AAuthor%3A%20Mohammadreza%20Salehi%20and%20Jae%20Sung%20Park%20and%20Tanush%20Yadav%20and%20Aditya%20Kusupati%20and%20Ranjay%20Krishna%20and%20Yejin%20Choi%20and%20Hannaneh%20Hajishirzi%20and%20Ali%20Farhadi%0AAbstract%3A%20%20%20Our%20world%20is%20full%20of%20varied%20actions%20and%20moves%20across%20specialized%20domains%20that%0Awe%2C%20as%20humans%2C%20strive%20to%20identify%20and%20understand.%20Within%20any%20single%20domain%2C%0Aactions%20can%20often%20appear%20quite%20similar%2C%20making%20it%20challenging%20for%20deep%20models%0Ato%20distinguish%20them%20accurately.%20To%20evaluate%20the%20effectiveness%20of%20multimodal%0Afoundation%20models%20in%20helping%20us%20recognize%20such%20actions%2C%20we%20present%20ActionAtlas%0Av1.0%2C%20a%20multiple-choice%20video%20question%20answering%20benchmark%20featuring%20short%0Avideos%20across%20various%20sports.%20Each%20video%20in%20the%20dataset%20is%20paired%20with%20a%0Aquestion%20and%20four%20or%20five%20choices.%20The%20question%20pinpoints%20specific%20individuals%2C%0Aasking%20which%20choice%20%22best%22%20describes%20their%20action%20within%20a%20certain%20temporal%0Acontext.%20Overall%2C%20the%20dataset%20includes%20934%20videos%20showcasing%20580%20unique%20actions%0Aacross%2056%20sports%2C%20with%20a%20total%20of%201896%20actions%20within%20choices.%20Unlike%20most%0Aexisting%20video%20question%20answering%20benchmarks%20that%20only%20cover%20simplistic%0Aactions%2C%20often%20identifiable%20from%20a%20single%20frame%2C%20ActionAtlas%20focuses%20on%0Aintricate%20movements%20and%20rigorously%20tests%20the%20model%27s%20capability%20to%20discern%0Asubtle%20differences%20between%20moves%20that%20look%20similar%20within%20each%20domain.%20We%0Aevaluate%20open%20and%20proprietary%20foundation%20models%20on%20this%20benchmark%2C%20finding%20that%0Athe%20best%20model%2C%20GPT-4o%2C%20achieves%20a%20maximum%20accuracy%20of%2045.52%25.%20Meanwhile%2C%0ANon-expert%20crowd%20workers%2C%20provided%20with%20action%20description%20for%20each%20choice%2C%0Aachieve%2061.64%25%20accuracy%2C%20where%20random%20chance%20is%20approximately%2021%25.%20Our%20findings%0Awith%20state-of-the-art%20models%20indicate%20that%20having%20a%20high%20frame%20sampling%20rate%20is%0Aimportant%20for%20accurately%20recognizing%20actions%20in%20ActionAtlas%2C%20a%20feature%20that%0Asome%20leading%20proprietary%20video%20models%2C%20such%20as%20Gemini%2C%20do%20not%20include%20in%20their%0Adefault%20configuration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05774v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActionAtlas%253A%2520A%2520VideoQA%2520Benchmark%2520for%2520Domain-specialized%2520Action%250A%2520%2520Recognition%26entry.906535625%3DMohammadreza%2520Salehi%2520and%2520Jae%2520Sung%2520Park%2520and%2520Tanush%2520Yadav%2520and%2520Aditya%2520Kusupati%2520and%2520Ranjay%2520Krishna%2520and%2520Yejin%2520Choi%2520and%2520Hannaneh%2520Hajishirzi%2520and%2520Ali%2520Farhadi%26entry.1292438233%3D%2520%2520Our%2520world%2520is%2520full%2520of%2520varied%2520actions%2520and%2520moves%2520across%2520specialized%2520domains%2520that%250Awe%252C%2520as%2520humans%252C%2520strive%2520to%2520identify%2520and%2520understand.%2520Within%2520any%2520single%2520domain%252C%250Aactions%2520can%2520often%2520appear%2520quite%2520similar%252C%2520making%2520it%2520challenging%2520for%2520deep%2520models%250Ato%2520distinguish%2520them%2520accurately.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520multimodal%250Afoundation%2520models%2520in%2520helping%2520us%2520recognize%2520such%2520actions%252C%2520we%2520present%2520ActionAtlas%250Av1.0%252C%2520a%2520multiple-choice%2520video%2520question%2520answering%2520benchmark%2520featuring%2520short%250Avideos%2520across%2520various%2520sports.%2520Each%2520video%2520in%2520the%2520dataset%2520is%2520paired%2520with%2520a%250Aquestion%2520and%2520four%2520or%2520five%2520choices.%2520The%2520question%2520pinpoints%2520specific%2520individuals%252C%250Aasking%2520which%2520choice%2520%2522best%2522%2520describes%2520their%2520action%2520within%2520a%2520certain%2520temporal%250Acontext.%2520Overall%252C%2520the%2520dataset%2520includes%2520934%2520videos%2520showcasing%2520580%2520unique%2520actions%250Aacross%252056%2520sports%252C%2520with%2520a%2520total%2520of%25201896%2520actions%2520within%2520choices.%2520Unlike%2520most%250Aexisting%2520video%2520question%2520answering%2520benchmarks%2520that%2520only%2520cover%2520simplistic%250Aactions%252C%2520often%2520identifiable%2520from%2520a%2520single%2520frame%252C%2520ActionAtlas%2520focuses%2520on%250Aintricate%2520movements%2520and%2520rigorously%2520tests%2520the%2520model%2527s%2520capability%2520to%2520discern%250Asubtle%2520differences%2520between%2520moves%2520that%2520look%2520similar%2520within%2520each%2520domain.%2520We%250Aevaluate%2520open%2520and%2520proprietary%2520foundation%2520models%2520on%2520this%2520benchmark%252C%2520finding%2520that%250Athe%2520best%2520model%252C%2520GPT-4o%252C%2520achieves%2520a%2520maximum%2520accuracy%2520of%252045.52%2525.%2520Meanwhile%252C%250ANon-expert%2520crowd%2520workers%252C%2520provided%2520with%2520action%2520description%2520for%2520each%2520choice%252C%250Aachieve%252061.64%2525%2520accuracy%252C%2520where%2520random%2520chance%2520is%2520approximately%252021%2525.%2520Our%2520findings%250Awith%2520state-of-the-art%2520models%2520indicate%2520that%2520having%2520a%2520high%2520frame%2520sampling%2520rate%2520is%250Aimportant%2520for%2520accurately%2520recognizing%2520actions%2520in%2520ActionAtlas%252C%2520a%2520feature%2520that%250Asome%2520leading%2520proprietary%2520video%2520models%252C%2520such%2520as%2520Gemini%252C%2520do%2520not%2520include%2520in%2520their%250Adefault%2520configuration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05774v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActionAtlas%3A%20A%20VideoQA%20Benchmark%20for%20Domain-specialized%20Action%0A%20%20Recognition&entry.906535625=Mohammadreza%20Salehi%20and%20Jae%20Sung%20Park%20and%20Tanush%20Yadav%20and%20Aditya%20Kusupati%20and%20Ranjay%20Krishna%20and%20Yejin%20Choi%20and%20Hannaneh%20Hajishirzi%20and%20Ali%20Farhadi&entry.1292438233=%20%20Our%20world%20is%20full%20of%20varied%20actions%20and%20moves%20across%20specialized%20domains%20that%0Awe%2C%20as%20humans%2C%20strive%20to%20identify%20and%20understand.%20Within%20any%20single%20domain%2C%0Aactions%20can%20often%20appear%20quite%20similar%2C%20making%20it%20challenging%20for%20deep%20models%0Ato%20distinguish%20them%20accurately.%20To%20evaluate%20the%20effectiveness%20of%20multimodal%0Afoundation%20models%20in%20helping%20us%20recognize%20such%20actions%2C%20we%20present%20ActionAtlas%0Av1.0%2C%20a%20multiple-choice%20video%20question%20answering%20benchmark%20featuring%20short%0Avideos%20across%20various%20sports.%20Each%20video%20in%20the%20dataset%20is%20paired%20with%20a%0Aquestion%20and%20four%20or%20five%20choices.%20The%20question%20pinpoints%20specific%20individuals%2C%0Aasking%20which%20choice%20%22best%22%20describes%20their%20action%20within%20a%20certain%20temporal%0Acontext.%20Overall%2C%20the%20dataset%20includes%20934%20videos%20showcasing%20580%20unique%20actions%0Aacross%2056%20sports%2C%20with%20a%20total%20of%201896%20actions%20within%20choices.%20Unlike%20most%0Aexisting%20video%20question%20answering%20benchmarks%20that%20only%20cover%20simplistic%0Aactions%2C%20often%20identifiable%20from%20a%20single%20frame%2C%20ActionAtlas%20focuses%20on%0Aintricate%20movements%20and%20rigorously%20tests%20the%20model%27s%20capability%20to%20discern%0Asubtle%20differences%20between%20moves%20that%20look%20similar%20within%20each%20domain.%20We%0Aevaluate%20open%20and%20proprietary%20foundation%20models%20on%20this%20benchmark%2C%20finding%20that%0Athe%20best%20model%2C%20GPT-4o%2C%20achieves%20a%20maximum%20accuracy%20of%2045.52%25.%20Meanwhile%2C%0ANon-expert%20crowd%20workers%2C%20provided%20with%20action%20description%20for%20each%20choice%2C%0Aachieve%2061.64%25%20accuracy%2C%20where%20random%20chance%20is%20approximately%2021%25.%20Our%20findings%0Awith%20state-of-the-art%20models%20indicate%20that%20having%20a%20high%20frame%20sampling%20rate%20is%0Aimportant%20for%20accurately%20recognizing%20actions%20in%20ActionAtlas%2C%20a%20feature%20that%0Asome%20leading%20proprietary%20video%20models%2C%20such%20as%20Gemini%2C%20do%20not%20include%20in%20their%0Adefault%20configuration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05774v4&entry.124074799=Read"},
{"title": "Deep Riemannian Networks for End-to-End EEG Decoding", "author": "Daniel Wilson and Robin Tibor Schirrmeister and Lukas Alexander Wilhelm Gemein and Tonio Ball", "abstract": "  State-of-the-art performance in electroencephalography (EEG) decoding tasks\nis currently often achieved with either Deep-Learning (DL) or\nRiemannian-Geometry-based decoders (RBDs). Recently, there is growing interest\nin Deep Riemannian Networks (DRNs) possibly combining the advantages of both\nprevious classes of methods. However, there are still a range of topics where\nadditional insight is needed to pave the way for a more widespread application\nof DRNs in EEG. These include architecture design questions such as network\nsize and end-to-end ability. How these factors affect model performance has not\nbeen explored. Additionally, it is not clear how the data within these networks\nis transformed, and whether this would correlate with traditional EEG decoding.\nOur study aims to lay the groundwork in the area of these topics through the\nanalysis of DRNs for EEG with a wide range of hyperparameters. Networks were\ntested on five public EEG datasets and compared with state-of-the-art ConvNets.\n  Here we propose EE(G)-SPDNet, and we show that this wide, end-to-end DRN can\noutperform the ConvNets, and in doing so use physiologically plausible\nfrequency regions. We also show that the end-to-end approach learns more\ncomplex filters than traditional band-pass filters targeting the classical\nalpha, beta, and gamma frequency bands of the EEG, and that performance can\nbenefit from channel specific filtering approaches. Additionally, architectural\nanalysis revealed areas for further improvement due to the possible under\nutilisation of Riemannian specific information throughout the network. Our\nstudy thus shows how to design and train DRNs to infer task-related information\nfrom the raw EEG without the need of handcrafted filterbanks and highlights the\npotential of end-to-end DRNs such as EE(G)-SPDNet for high-performance EEG\ndecoding.\n", "link": "http://arxiv.org/abs/2212.10426v7", "date": "2024-11-11", "relevancy": 2.0885, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.553}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5006}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Riemannian%20Networks%20for%20End-to-End%20EEG%20Decoding&body=Title%3A%20Deep%20Riemannian%20Networks%20for%20End-to-End%20EEG%20Decoding%0AAuthor%3A%20Daniel%20Wilson%20and%20Robin%20Tibor%20Schirrmeister%20and%20Lukas%20Alexander%20Wilhelm%20Gemein%20and%20Tonio%20Ball%0AAbstract%3A%20%20%20State-of-the-art%20performance%20in%20electroencephalography%20%28EEG%29%20decoding%20tasks%0Ais%20currently%20often%20achieved%20with%20either%20Deep-Learning%20%28DL%29%20or%0ARiemannian-Geometry-based%20decoders%20%28RBDs%29.%20Recently%2C%20there%20is%20growing%20interest%0Ain%20Deep%20Riemannian%20Networks%20%28DRNs%29%20possibly%20combining%20the%20advantages%20of%20both%0Aprevious%20classes%20of%20methods.%20However%2C%20there%20are%20still%20a%20range%20of%20topics%20where%0Aadditional%20insight%20is%20needed%20to%20pave%20the%20way%20for%20a%20more%20widespread%20application%0Aof%20DRNs%20in%20EEG.%20These%20include%20architecture%20design%20questions%20such%20as%20network%0Asize%20and%20end-to-end%20ability.%20How%20these%20factors%20affect%20model%20performance%20has%20not%0Abeen%20explored.%20Additionally%2C%20it%20is%20not%20clear%20how%20the%20data%20within%20these%20networks%0Ais%20transformed%2C%20and%20whether%20this%20would%20correlate%20with%20traditional%20EEG%20decoding.%0AOur%20study%20aims%20to%20lay%20the%20groundwork%20in%20the%20area%20of%20these%20topics%20through%20the%0Aanalysis%20of%20DRNs%20for%20EEG%20with%20a%20wide%20range%20of%20hyperparameters.%20Networks%20were%0Atested%20on%20five%20public%20EEG%20datasets%20and%20compared%20with%20state-of-the-art%20ConvNets.%0A%20%20Here%20we%20propose%20EE%28G%29-SPDNet%2C%20and%20we%20show%20that%20this%20wide%2C%20end-to-end%20DRN%20can%0Aoutperform%20the%20ConvNets%2C%20and%20in%20doing%20so%20use%20physiologically%20plausible%0Afrequency%20regions.%20We%20also%20show%20that%20the%20end-to-end%20approach%20learns%20more%0Acomplex%20filters%20than%20traditional%20band-pass%20filters%20targeting%20the%20classical%0Aalpha%2C%20beta%2C%20and%20gamma%20frequency%20bands%20of%20the%20EEG%2C%20and%20that%20performance%20can%0Abenefit%20from%20channel%20specific%20filtering%20approaches.%20Additionally%2C%20architectural%0Aanalysis%20revealed%20areas%20for%20further%20improvement%20due%20to%20the%20possible%20under%0Autilisation%20of%20Riemannian%20specific%20information%20throughout%20the%20network.%20Our%0Astudy%20thus%20shows%20how%20to%20design%20and%20train%20DRNs%20to%20infer%20task-related%20information%0Afrom%20the%20raw%20EEG%20without%20the%20need%20of%20handcrafted%20filterbanks%20and%20highlights%20the%0Apotential%20of%20end-to-end%20DRNs%20such%20as%20EE%28G%29-SPDNet%20for%20high-performance%20EEG%0Adecoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.10426v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Riemannian%2520Networks%2520for%2520End-to-End%2520EEG%2520Decoding%26entry.906535625%3DDaniel%2520Wilson%2520and%2520Robin%2520Tibor%2520Schirrmeister%2520and%2520Lukas%2520Alexander%2520Wilhelm%2520Gemein%2520and%2520Tonio%2520Ball%26entry.1292438233%3D%2520%2520State-of-the-art%2520performance%2520in%2520electroencephalography%2520%2528EEG%2529%2520decoding%2520tasks%250Ais%2520currently%2520often%2520achieved%2520with%2520either%2520Deep-Learning%2520%2528DL%2529%2520or%250ARiemannian-Geometry-based%2520decoders%2520%2528RBDs%2529.%2520Recently%252C%2520there%2520is%2520growing%2520interest%250Ain%2520Deep%2520Riemannian%2520Networks%2520%2528DRNs%2529%2520possibly%2520combining%2520the%2520advantages%2520of%2520both%250Aprevious%2520classes%2520of%2520methods.%2520However%252C%2520there%2520are%2520still%2520a%2520range%2520of%2520topics%2520where%250Aadditional%2520insight%2520is%2520needed%2520to%2520pave%2520the%2520way%2520for%2520a%2520more%2520widespread%2520application%250Aof%2520DRNs%2520in%2520EEG.%2520These%2520include%2520architecture%2520design%2520questions%2520such%2520as%2520network%250Asize%2520and%2520end-to-end%2520ability.%2520How%2520these%2520factors%2520affect%2520model%2520performance%2520has%2520not%250Abeen%2520explored.%2520Additionally%252C%2520it%2520is%2520not%2520clear%2520how%2520the%2520data%2520within%2520these%2520networks%250Ais%2520transformed%252C%2520and%2520whether%2520this%2520would%2520correlate%2520with%2520traditional%2520EEG%2520decoding.%250AOur%2520study%2520aims%2520to%2520lay%2520the%2520groundwork%2520in%2520the%2520area%2520of%2520these%2520topics%2520through%2520the%250Aanalysis%2520of%2520DRNs%2520for%2520EEG%2520with%2520a%2520wide%2520range%2520of%2520hyperparameters.%2520Networks%2520were%250Atested%2520on%2520five%2520public%2520EEG%2520datasets%2520and%2520compared%2520with%2520state-of-the-art%2520ConvNets.%250A%2520%2520Here%2520we%2520propose%2520EE%2528G%2529-SPDNet%252C%2520and%2520we%2520show%2520that%2520this%2520wide%252C%2520end-to-end%2520DRN%2520can%250Aoutperform%2520the%2520ConvNets%252C%2520and%2520in%2520doing%2520so%2520use%2520physiologically%2520plausible%250Afrequency%2520regions.%2520We%2520also%2520show%2520that%2520the%2520end-to-end%2520approach%2520learns%2520more%250Acomplex%2520filters%2520than%2520traditional%2520band-pass%2520filters%2520targeting%2520the%2520classical%250Aalpha%252C%2520beta%252C%2520and%2520gamma%2520frequency%2520bands%2520of%2520the%2520EEG%252C%2520and%2520that%2520performance%2520can%250Abenefit%2520from%2520channel%2520specific%2520filtering%2520approaches.%2520Additionally%252C%2520architectural%250Aanalysis%2520revealed%2520areas%2520for%2520further%2520improvement%2520due%2520to%2520the%2520possible%2520under%250Autilisation%2520of%2520Riemannian%2520specific%2520information%2520throughout%2520the%2520network.%2520Our%250Astudy%2520thus%2520shows%2520how%2520to%2520design%2520and%2520train%2520DRNs%2520to%2520infer%2520task-related%2520information%250Afrom%2520the%2520raw%2520EEG%2520without%2520the%2520need%2520of%2520handcrafted%2520filterbanks%2520and%2520highlights%2520the%250Apotential%2520of%2520end-to-end%2520DRNs%2520such%2520as%2520EE%2528G%2529-SPDNet%2520for%2520high-performance%2520EEG%250Adecoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.10426v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Riemannian%20Networks%20for%20End-to-End%20EEG%20Decoding&entry.906535625=Daniel%20Wilson%20and%20Robin%20Tibor%20Schirrmeister%20and%20Lukas%20Alexander%20Wilhelm%20Gemein%20and%20Tonio%20Ball&entry.1292438233=%20%20State-of-the-art%20performance%20in%20electroencephalography%20%28EEG%29%20decoding%20tasks%0Ais%20currently%20often%20achieved%20with%20either%20Deep-Learning%20%28DL%29%20or%0ARiemannian-Geometry-based%20decoders%20%28RBDs%29.%20Recently%2C%20there%20is%20growing%20interest%0Ain%20Deep%20Riemannian%20Networks%20%28DRNs%29%20possibly%20combining%20the%20advantages%20of%20both%0Aprevious%20classes%20of%20methods.%20However%2C%20there%20are%20still%20a%20range%20of%20topics%20where%0Aadditional%20insight%20is%20needed%20to%20pave%20the%20way%20for%20a%20more%20widespread%20application%0Aof%20DRNs%20in%20EEG.%20These%20include%20architecture%20design%20questions%20such%20as%20network%0Asize%20and%20end-to-end%20ability.%20How%20these%20factors%20affect%20model%20performance%20has%20not%0Abeen%20explored.%20Additionally%2C%20it%20is%20not%20clear%20how%20the%20data%20within%20these%20networks%0Ais%20transformed%2C%20and%20whether%20this%20would%20correlate%20with%20traditional%20EEG%20decoding.%0AOur%20study%20aims%20to%20lay%20the%20groundwork%20in%20the%20area%20of%20these%20topics%20through%20the%0Aanalysis%20of%20DRNs%20for%20EEG%20with%20a%20wide%20range%20of%20hyperparameters.%20Networks%20were%0Atested%20on%20five%20public%20EEG%20datasets%20and%20compared%20with%20state-of-the-art%20ConvNets.%0A%20%20Here%20we%20propose%20EE%28G%29-SPDNet%2C%20and%20we%20show%20that%20this%20wide%2C%20end-to-end%20DRN%20can%0Aoutperform%20the%20ConvNets%2C%20and%20in%20doing%20so%20use%20physiologically%20plausible%0Afrequency%20regions.%20We%20also%20show%20that%20the%20end-to-end%20approach%20learns%20more%0Acomplex%20filters%20than%20traditional%20band-pass%20filters%20targeting%20the%20classical%0Aalpha%2C%20beta%2C%20and%20gamma%20frequency%20bands%20of%20the%20EEG%2C%20and%20that%20performance%20can%0Abenefit%20from%20channel%20specific%20filtering%20approaches.%20Additionally%2C%20architectural%0Aanalysis%20revealed%20areas%20for%20further%20improvement%20due%20to%20the%20possible%20under%0Autilisation%20of%20Riemannian%20specific%20information%20throughout%20the%20network.%20Our%0Astudy%20thus%20shows%20how%20to%20design%20and%20train%20DRNs%20to%20infer%20task-related%20information%0Afrom%20the%20raw%20EEG%20without%20the%20need%20of%20handcrafted%20filterbanks%20and%20highlights%20the%0Apotential%20of%20end-to-end%20DRNs%20such%20as%20EE%28G%29-SPDNet%20for%20high-performance%20EEG%0Adecoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.10426v7&entry.124074799=Read"},
{"title": "FuseAnyPart: Diffusion-Driven Facial Parts Swapping via Multiple\n  Reference Images", "author": "Zheng Yu and Yaohua Wang and Siying Cui and Aixi Zhang and Wei-Long Zheng and Senzhang Wang", "abstract": "  Facial parts swapping aims to selectively transfer regions of interest from\nthe source image onto the target image while maintaining the rest of the target\nimage unchanged. Most studies on face swapping designed specifically for\nfull-face swapping, are either unable or significantly limited when it comes to\nswapping individual facial parts, which hinders fine-grained and customized\ncharacter designs. However, designing such an approach specifically for facial\nparts swapping is challenged by a reasonable multiple reference feature fusion,\nwhich needs to be both efficient and effective. To overcome this challenge,\nFuseAnyPart is proposed to facilitate the seamless \"fuse-any-part\"\ncustomization of the face. In FuseAnyPart, facial parts from different people\nare assembled into a complete face in latent space within the Mask-based Fusion\nModule. Subsequently, the consolidated feature is dispatched to the\nAddition-based Injection Module for fusion within the UNet of the diffusion\nmodel to create novel characters. Extensive experiments qualitatively and\nquantitatively validate the superiority and robustness of FuseAnyPart. Source\ncodes are available at https://github.com/Thomas-wyh/FuseAnyPart.\n", "link": "http://arxiv.org/abs/2410.22771v2", "date": "2024-11-11", "relevancy": 2.0831, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5435}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5201}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FuseAnyPart%3A%20Diffusion-Driven%20Facial%20Parts%20Swapping%20via%20Multiple%0A%20%20Reference%20Images&body=Title%3A%20FuseAnyPart%3A%20Diffusion-Driven%20Facial%20Parts%20Swapping%20via%20Multiple%0A%20%20Reference%20Images%0AAuthor%3A%20Zheng%20Yu%20and%20Yaohua%20Wang%20and%20Siying%20Cui%20and%20Aixi%20Zhang%20and%20Wei-Long%20Zheng%20and%20Senzhang%20Wang%0AAbstract%3A%20%20%20Facial%20parts%20swapping%20aims%20to%20selectively%20transfer%20regions%20of%20interest%20from%0Athe%20source%20image%20onto%20the%20target%20image%20while%20maintaining%20the%20rest%20of%20the%20target%0Aimage%20unchanged.%20Most%20studies%20on%20face%20swapping%20designed%20specifically%20for%0Afull-face%20swapping%2C%20are%20either%20unable%20or%20significantly%20limited%20when%20it%20comes%20to%0Aswapping%20individual%20facial%20parts%2C%20which%20hinders%20fine-grained%20and%20customized%0Acharacter%20designs.%20However%2C%20designing%20such%20an%20approach%20specifically%20for%20facial%0Aparts%20swapping%20is%20challenged%20by%20a%20reasonable%20multiple%20reference%20feature%20fusion%2C%0Awhich%20needs%20to%20be%20both%20efficient%20and%20effective.%20To%20overcome%20this%20challenge%2C%0AFuseAnyPart%20is%20proposed%20to%20facilitate%20the%20seamless%20%22fuse-any-part%22%0Acustomization%20of%20the%20face.%20In%20FuseAnyPart%2C%20facial%20parts%20from%20different%20people%0Aare%20assembled%20into%20a%20complete%20face%20in%20latent%20space%20within%20the%20Mask-based%20Fusion%0AModule.%20Subsequently%2C%20the%20consolidated%20feature%20is%20dispatched%20to%20the%0AAddition-based%20Injection%20Module%20for%20fusion%20within%20the%20UNet%20of%20the%20diffusion%0Amodel%20to%20create%20novel%20characters.%20Extensive%20experiments%20qualitatively%20and%0Aquantitatively%20validate%20the%20superiority%20and%20robustness%20of%20FuseAnyPart.%20Source%0Acodes%20are%20available%20at%20https%3A//github.com/Thomas-wyh/FuseAnyPart.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuseAnyPart%253A%2520Diffusion-Driven%2520Facial%2520Parts%2520Swapping%2520via%2520Multiple%250A%2520%2520Reference%2520Images%26entry.906535625%3DZheng%2520Yu%2520and%2520Yaohua%2520Wang%2520and%2520Siying%2520Cui%2520and%2520Aixi%2520Zhang%2520and%2520Wei-Long%2520Zheng%2520and%2520Senzhang%2520Wang%26entry.1292438233%3D%2520%2520Facial%2520parts%2520swapping%2520aims%2520to%2520selectively%2520transfer%2520regions%2520of%2520interest%2520from%250Athe%2520source%2520image%2520onto%2520the%2520target%2520image%2520while%2520maintaining%2520the%2520rest%2520of%2520the%2520target%250Aimage%2520unchanged.%2520Most%2520studies%2520on%2520face%2520swapping%2520designed%2520specifically%2520for%250Afull-face%2520swapping%252C%2520are%2520either%2520unable%2520or%2520significantly%2520limited%2520when%2520it%2520comes%2520to%250Aswapping%2520individual%2520facial%2520parts%252C%2520which%2520hinders%2520fine-grained%2520and%2520customized%250Acharacter%2520designs.%2520However%252C%2520designing%2520such%2520an%2520approach%2520specifically%2520for%2520facial%250Aparts%2520swapping%2520is%2520challenged%2520by%2520a%2520reasonable%2520multiple%2520reference%2520feature%2520fusion%252C%250Awhich%2520needs%2520to%2520be%2520both%2520efficient%2520and%2520effective.%2520To%2520overcome%2520this%2520challenge%252C%250AFuseAnyPart%2520is%2520proposed%2520to%2520facilitate%2520the%2520seamless%2520%2522fuse-any-part%2522%250Acustomization%2520of%2520the%2520face.%2520In%2520FuseAnyPart%252C%2520facial%2520parts%2520from%2520different%2520people%250Aare%2520assembled%2520into%2520a%2520complete%2520face%2520in%2520latent%2520space%2520within%2520the%2520Mask-based%2520Fusion%250AModule.%2520Subsequently%252C%2520the%2520consolidated%2520feature%2520is%2520dispatched%2520to%2520the%250AAddition-based%2520Injection%2520Module%2520for%2520fusion%2520within%2520the%2520UNet%2520of%2520the%2520diffusion%250Amodel%2520to%2520create%2520novel%2520characters.%2520Extensive%2520experiments%2520qualitatively%2520and%250Aquantitatively%2520validate%2520the%2520superiority%2520and%2520robustness%2520of%2520FuseAnyPart.%2520Source%250Acodes%2520are%2520available%2520at%2520https%253A//github.com/Thomas-wyh/FuseAnyPart.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FuseAnyPart%3A%20Diffusion-Driven%20Facial%20Parts%20Swapping%20via%20Multiple%0A%20%20Reference%20Images&entry.906535625=Zheng%20Yu%20and%20Yaohua%20Wang%20and%20Siying%20Cui%20and%20Aixi%20Zhang%20and%20Wei-Long%20Zheng%20and%20Senzhang%20Wang&entry.1292438233=%20%20Facial%20parts%20swapping%20aims%20to%20selectively%20transfer%20regions%20of%20interest%20from%0Athe%20source%20image%20onto%20the%20target%20image%20while%20maintaining%20the%20rest%20of%20the%20target%0Aimage%20unchanged.%20Most%20studies%20on%20face%20swapping%20designed%20specifically%20for%0Afull-face%20swapping%2C%20are%20either%20unable%20or%20significantly%20limited%20when%20it%20comes%20to%0Aswapping%20individual%20facial%20parts%2C%20which%20hinders%20fine-grained%20and%20customized%0Acharacter%20designs.%20However%2C%20designing%20such%20an%20approach%20specifically%20for%20facial%0Aparts%20swapping%20is%20challenged%20by%20a%20reasonable%20multiple%20reference%20feature%20fusion%2C%0Awhich%20needs%20to%20be%20both%20efficient%20and%20effective.%20To%20overcome%20this%20challenge%2C%0AFuseAnyPart%20is%20proposed%20to%20facilitate%20the%20seamless%20%22fuse-any-part%22%0Acustomization%20of%20the%20face.%20In%20FuseAnyPart%2C%20facial%20parts%20from%20different%20people%0Aare%20assembled%20into%20a%20complete%20face%20in%20latent%20space%20within%20the%20Mask-based%20Fusion%0AModule.%20Subsequently%2C%20the%20consolidated%20feature%20is%20dispatched%20to%20the%0AAddition-based%20Injection%20Module%20for%20fusion%20within%20the%20UNet%20of%20the%20diffusion%0Amodel%20to%20create%20novel%20characters.%20Extensive%20experiments%20qualitatively%20and%0Aquantitatively%20validate%20the%20superiority%20and%20robustness%20of%20FuseAnyPart.%20Source%0Acodes%20are%20available%20at%20https%3A//github.com/Thomas-wyh/FuseAnyPart.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22771v2&entry.124074799=Read"},
{"title": "Evaluating Large Language Models on Financial Report Summarization: An\n  Empirical Study", "author": "Xinqi Yang and Scott Zang and Yong Ren and Dingjie Peng and Zheng Wen", "abstract": "  In recent years, Large Language Models (LLMs) have demonstrated remarkable\nversatility across various applications, including natural language\nunderstanding, domain-specific knowledge tasks, etc. However, applying LLMs to\ncomplex, high-stakes domains like finance requires rigorous evaluation to\nensure reliability, accuracy, and compliance with industry standards. To\naddress this need, we conduct a comprehensive and comparative study on three\nstate-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their\neffectiveness in generating automated financial reports. Our primary motivation\nis to explore how these models can be harnessed within finance, a field\ndemanding precision, contextual relevance, and robustness against erroneous or\nmisleading information. By examining each model's capabilities, we aim to\nprovide an insightful assessment of their strengths and limitations. Our paper\noffers benchmarks for financial report analysis, encompassing proposed metrics\nsuch as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative\nevaluation framework that integrates both quantitative metrics (e.g.,\nprecision, recall) and qualitative analyses (e.g., contextual fit, consistency)\nto provide a holistic view of each model's output quality. Additionally, we\nmake our financial dataset publicly available, inviting researchers and\npractitioners to leverage, scrutinize, and enhance our findings through broader\ncommunity engagement and collaborative improvement. Our dataset is available on\nhuggingface.\n", "link": "http://arxiv.org/abs/2411.06852v1", "date": "2024-11-11", "relevancy": 2.0671, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Large%20Language%20Models%20on%20Financial%20Report%20Summarization%3A%20An%0A%20%20Empirical%20Study&body=Title%3A%20Evaluating%20Large%20Language%20Models%20on%20Financial%20Report%20Summarization%3A%20An%0A%20%20Empirical%20Study%0AAuthor%3A%20Xinqi%20Yang%20and%20Scott%20Zang%20and%20Yong%20Ren%20and%20Dingjie%20Peng%20and%20Zheng%20Wen%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%0Aversatility%20across%20various%20applications%2C%20including%20natural%20language%0Aunderstanding%2C%20domain-specific%20knowledge%20tasks%2C%20etc.%20However%2C%20applying%20LLMs%20to%0Acomplex%2C%20high-stakes%20domains%20like%20finance%20requires%20rigorous%20evaluation%20to%0Aensure%20reliability%2C%20accuracy%2C%20and%20compliance%20with%20industry%20standards.%20To%0Aaddress%20this%20need%2C%20we%20conduct%20a%20comprehensive%20and%20comparative%20study%20on%20three%0Astate-of-the-art%20LLMs%2C%20GLM-4%2C%20Mistral-NeMo%2C%20and%20LLaMA3.1%2C%20focusing%20on%20their%0Aeffectiveness%20in%20generating%20automated%20financial%20reports.%20Our%20primary%20motivation%0Ais%20to%20explore%20how%20these%20models%20can%20be%20harnessed%20within%20finance%2C%20a%20field%0Ademanding%20precision%2C%20contextual%20relevance%2C%20and%20robustness%20against%20erroneous%20or%0Amisleading%20information.%20By%20examining%20each%20model%27s%20capabilities%2C%20we%20aim%20to%0Aprovide%20an%20insightful%20assessment%20of%20their%20strengths%20and%20limitations.%20Our%20paper%0Aoffers%20benchmarks%20for%20financial%20report%20analysis%2C%20encompassing%20proposed%20metrics%0Asuch%20as%20ROUGE-1%2C%20BERT%20Score%2C%20and%20LLM%20Score.%20We%20introduce%20an%20innovative%0Aevaluation%20framework%20that%20integrates%20both%20quantitative%20metrics%20%28e.g.%2C%0Aprecision%2C%20recall%29%20and%20qualitative%20analyses%20%28e.g.%2C%20contextual%20fit%2C%20consistency%29%0Ato%20provide%20a%20holistic%20view%20of%20each%20model%27s%20output%20quality.%20Additionally%2C%20we%0Amake%20our%20financial%20dataset%20publicly%20available%2C%20inviting%20researchers%20and%0Apractitioners%20to%20leverage%2C%20scrutinize%2C%20and%20enhance%20our%20findings%20through%20broader%0Acommunity%20engagement%20and%20collaborative%20improvement.%20Our%20dataset%20is%20available%20on%0Ahuggingface.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Large%2520Language%2520Models%2520on%2520Financial%2520Report%2520Summarization%253A%2520An%250A%2520%2520Empirical%2520Study%26entry.906535625%3DXinqi%2520Yang%2520and%2520Scott%2520Zang%2520and%2520Yong%2520Ren%2520and%2520Dingjie%2520Peng%2520and%2520Zheng%2520Wen%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%250Aversatility%2520across%2520various%2520applications%252C%2520including%2520natural%2520language%250Aunderstanding%252C%2520domain-specific%2520knowledge%2520tasks%252C%2520etc.%2520However%252C%2520applying%2520LLMs%2520to%250Acomplex%252C%2520high-stakes%2520domains%2520like%2520finance%2520requires%2520rigorous%2520evaluation%2520to%250Aensure%2520reliability%252C%2520accuracy%252C%2520and%2520compliance%2520with%2520industry%2520standards.%2520To%250Aaddress%2520this%2520need%252C%2520we%2520conduct%2520a%2520comprehensive%2520and%2520comparative%2520study%2520on%2520three%250Astate-of-the-art%2520LLMs%252C%2520GLM-4%252C%2520Mistral-NeMo%252C%2520and%2520LLaMA3.1%252C%2520focusing%2520on%2520their%250Aeffectiveness%2520in%2520generating%2520automated%2520financial%2520reports.%2520Our%2520primary%2520motivation%250Ais%2520to%2520explore%2520how%2520these%2520models%2520can%2520be%2520harnessed%2520within%2520finance%252C%2520a%2520field%250Ademanding%2520precision%252C%2520contextual%2520relevance%252C%2520and%2520robustness%2520against%2520erroneous%2520or%250Amisleading%2520information.%2520By%2520examining%2520each%2520model%2527s%2520capabilities%252C%2520we%2520aim%2520to%250Aprovide%2520an%2520insightful%2520assessment%2520of%2520their%2520strengths%2520and%2520limitations.%2520Our%2520paper%250Aoffers%2520benchmarks%2520for%2520financial%2520report%2520analysis%252C%2520encompassing%2520proposed%2520metrics%250Asuch%2520as%2520ROUGE-1%252C%2520BERT%2520Score%252C%2520and%2520LLM%2520Score.%2520We%2520introduce%2520an%2520innovative%250Aevaluation%2520framework%2520that%2520integrates%2520both%2520quantitative%2520metrics%2520%2528e.g.%252C%250Aprecision%252C%2520recall%2529%2520and%2520qualitative%2520analyses%2520%2528e.g.%252C%2520contextual%2520fit%252C%2520consistency%2529%250Ato%2520provide%2520a%2520holistic%2520view%2520of%2520each%2520model%2527s%2520output%2520quality.%2520Additionally%252C%2520we%250Amake%2520our%2520financial%2520dataset%2520publicly%2520available%252C%2520inviting%2520researchers%2520and%250Apractitioners%2520to%2520leverage%252C%2520scrutinize%252C%2520and%2520enhance%2520our%2520findings%2520through%2520broader%250Acommunity%2520engagement%2520and%2520collaborative%2520improvement.%2520Our%2520dataset%2520is%2520available%2520on%250Ahuggingface.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Large%20Language%20Models%20on%20Financial%20Report%20Summarization%3A%20An%0A%20%20Empirical%20Study&entry.906535625=Xinqi%20Yang%20and%20Scott%20Zang%20and%20Yong%20Ren%20and%20Dingjie%20Peng%20and%20Zheng%20Wen&entry.1292438233=%20%20In%20recent%20years%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%0Aversatility%20across%20various%20applications%2C%20including%20natural%20language%0Aunderstanding%2C%20domain-specific%20knowledge%20tasks%2C%20etc.%20However%2C%20applying%20LLMs%20to%0Acomplex%2C%20high-stakes%20domains%20like%20finance%20requires%20rigorous%20evaluation%20to%0Aensure%20reliability%2C%20accuracy%2C%20and%20compliance%20with%20industry%20standards.%20To%0Aaddress%20this%20need%2C%20we%20conduct%20a%20comprehensive%20and%20comparative%20study%20on%20three%0Astate-of-the-art%20LLMs%2C%20GLM-4%2C%20Mistral-NeMo%2C%20and%20LLaMA3.1%2C%20focusing%20on%20their%0Aeffectiveness%20in%20generating%20automated%20financial%20reports.%20Our%20primary%20motivation%0Ais%20to%20explore%20how%20these%20models%20can%20be%20harnessed%20within%20finance%2C%20a%20field%0Ademanding%20precision%2C%20contextual%20relevance%2C%20and%20robustness%20against%20erroneous%20or%0Amisleading%20information.%20By%20examining%20each%20model%27s%20capabilities%2C%20we%20aim%20to%0Aprovide%20an%20insightful%20assessment%20of%20their%20strengths%20and%20limitations.%20Our%20paper%0Aoffers%20benchmarks%20for%20financial%20report%20analysis%2C%20encompassing%20proposed%20metrics%0Asuch%20as%20ROUGE-1%2C%20BERT%20Score%2C%20and%20LLM%20Score.%20We%20introduce%20an%20innovative%0Aevaluation%20framework%20that%20integrates%20both%20quantitative%20metrics%20%28e.g.%2C%0Aprecision%2C%20recall%29%20and%20qualitative%20analyses%20%28e.g.%2C%20contextual%20fit%2C%20consistency%29%0Ato%20provide%20a%20holistic%20view%20of%20each%20model%27s%20output%20quality.%20Additionally%2C%20we%0Amake%20our%20financial%20dataset%20publicly%20available%2C%20inviting%20researchers%20and%0Apractitioners%20to%20leverage%2C%20scrutinize%2C%20and%20enhance%20our%20findings%20through%20broader%0Acommunity%20engagement%20and%20collaborative%20improvement.%20Our%20dataset%20is%20available%20on%0Ahuggingface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06852v1&entry.124074799=Read"},
{"title": "Evaluation of Environmental Conditions on Object Detection using\n  Oriented Bounding Boxes for AR Applications", "author": "Vladislav Li and Barbara Villarini and Jean-Christophe Nebel and Thomas Lagkas and Panagiotis Sarigiannidis and Vasileios Argyriou", "abstract": "  The objective of augmented reality (AR) is to add digital content to natural\nimages and videos to create an interactive experience between the user and the\nenvironment. Scene analysis and object recognition play a crucial role in AR,\nas they must be performed quickly and accurately. In this study, a new approach\nis proposed that involves using oriented bounding boxes with a detection and\nrecognition deep network to improve performance and processing time. The\napproach is evaluated using two datasets: a real image dataset (DOTA dataset)\ncommonly used for computer vision tasks, and a synthetic dataset that simulates\ndifferent environmental, lighting, and acquisition conditions. The focus of the\nevaluation is on small objects, which are difficult to detect and recognise.\nThe results indicate that the proposed approach tends to produce better Average\nPrecision and greater accuracy for small objects in most of the tested\nconditions.\n", "link": "http://arxiv.org/abs/2306.16798v2", "date": "2024-11-11", "relevancy": 2.0643, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5162}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Environmental%20Conditions%20on%20Object%20Detection%20using%0A%20%20Oriented%20Bounding%20Boxes%20for%20AR%20Applications&body=Title%3A%20Evaluation%20of%20Environmental%20Conditions%20on%20Object%20Detection%20using%0A%20%20Oriented%20Bounding%20Boxes%20for%20AR%20Applications%0AAuthor%3A%20Vladislav%20Li%20and%20Barbara%20Villarini%20and%20Jean-Christophe%20Nebel%20and%20Thomas%20Lagkas%20and%20Panagiotis%20Sarigiannidis%20and%20Vasileios%20Argyriou%0AAbstract%3A%20%20%20The%20objective%20of%20augmented%20reality%20%28AR%29%20is%20to%20add%20digital%20content%20to%20natural%0Aimages%20and%20videos%20to%20create%20an%20interactive%20experience%20between%20the%20user%20and%20the%0Aenvironment.%20Scene%20analysis%20and%20object%20recognition%20play%20a%20crucial%20role%20in%20AR%2C%0Aas%20they%20must%20be%20performed%20quickly%20and%20accurately.%20In%20this%20study%2C%20a%20new%20approach%0Ais%20proposed%20that%20involves%20using%20oriented%20bounding%20boxes%20with%20a%20detection%20and%0Arecognition%20deep%20network%20to%20improve%20performance%20and%20processing%20time.%20The%0Aapproach%20is%20evaluated%20using%20two%20datasets%3A%20a%20real%20image%20dataset%20%28DOTA%20dataset%29%0Acommonly%20used%20for%20computer%20vision%20tasks%2C%20and%20a%20synthetic%20dataset%20that%20simulates%0Adifferent%20environmental%2C%20lighting%2C%20and%20acquisition%20conditions.%20The%20focus%20of%20the%0Aevaluation%20is%20on%20small%20objects%2C%20which%20are%20difficult%20to%20detect%20and%20recognise.%0AThe%20results%20indicate%20that%20the%20proposed%20approach%20tends%20to%20produce%20better%20Average%0APrecision%20and%20greater%20accuracy%20for%20small%20objects%20in%20most%20of%20the%20tested%0Aconditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.16798v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Environmental%2520Conditions%2520on%2520Object%2520Detection%2520using%250A%2520%2520Oriented%2520Bounding%2520Boxes%2520for%2520AR%2520Applications%26entry.906535625%3DVladislav%2520Li%2520and%2520Barbara%2520Villarini%2520and%2520Jean-Christophe%2520Nebel%2520and%2520Thomas%2520Lagkas%2520and%2520Panagiotis%2520Sarigiannidis%2520and%2520Vasileios%2520Argyriou%26entry.1292438233%3D%2520%2520The%2520objective%2520of%2520augmented%2520reality%2520%2528AR%2529%2520is%2520to%2520add%2520digital%2520content%2520to%2520natural%250Aimages%2520and%2520videos%2520to%2520create%2520an%2520interactive%2520experience%2520between%2520the%2520user%2520and%2520the%250Aenvironment.%2520Scene%2520analysis%2520and%2520object%2520recognition%2520play%2520a%2520crucial%2520role%2520in%2520AR%252C%250Aas%2520they%2520must%2520be%2520performed%2520quickly%2520and%2520accurately.%2520In%2520this%2520study%252C%2520a%2520new%2520approach%250Ais%2520proposed%2520that%2520involves%2520using%2520oriented%2520bounding%2520boxes%2520with%2520a%2520detection%2520and%250Arecognition%2520deep%2520network%2520to%2520improve%2520performance%2520and%2520processing%2520time.%2520The%250Aapproach%2520is%2520evaluated%2520using%2520two%2520datasets%253A%2520a%2520real%2520image%2520dataset%2520%2528DOTA%2520dataset%2529%250Acommonly%2520used%2520for%2520computer%2520vision%2520tasks%252C%2520and%2520a%2520synthetic%2520dataset%2520that%2520simulates%250Adifferent%2520environmental%252C%2520lighting%252C%2520and%2520acquisition%2520conditions.%2520The%2520focus%2520of%2520the%250Aevaluation%2520is%2520on%2520small%2520objects%252C%2520which%2520are%2520difficult%2520to%2520detect%2520and%2520recognise.%250AThe%2520results%2520indicate%2520that%2520the%2520proposed%2520approach%2520tends%2520to%2520produce%2520better%2520Average%250APrecision%2520and%2520greater%2520accuracy%2520for%2520small%2520objects%2520in%2520most%2520of%2520the%2520tested%250Aconditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.16798v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Environmental%20Conditions%20on%20Object%20Detection%20using%0A%20%20Oriented%20Bounding%20Boxes%20for%20AR%20Applications&entry.906535625=Vladislav%20Li%20and%20Barbara%20Villarini%20and%20Jean-Christophe%20Nebel%20and%20Thomas%20Lagkas%20and%20Panagiotis%20Sarigiannidis%20and%20Vasileios%20Argyriou&entry.1292438233=%20%20The%20objective%20of%20augmented%20reality%20%28AR%29%20is%20to%20add%20digital%20content%20to%20natural%0Aimages%20and%20videos%20to%20create%20an%20interactive%20experience%20between%20the%20user%20and%20the%0Aenvironment.%20Scene%20analysis%20and%20object%20recognition%20play%20a%20crucial%20role%20in%20AR%2C%0Aas%20they%20must%20be%20performed%20quickly%20and%20accurately.%20In%20this%20study%2C%20a%20new%20approach%0Ais%20proposed%20that%20involves%20using%20oriented%20bounding%20boxes%20with%20a%20detection%20and%0Arecognition%20deep%20network%20to%20improve%20performance%20and%20processing%20time.%20The%0Aapproach%20is%20evaluated%20using%20two%20datasets%3A%20a%20real%20image%20dataset%20%28DOTA%20dataset%29%0Acommonly%20used%20for%20computer%20vision%20tasks%2C%20and%20a%20synthetic%20dataset%20that%20simulates%0Adifferent%20environmental%2C%20lighting%2C%20and%20acquisition%20conditions.%20The%20focus%20of%20the%0Aevaluation%20is%20on%20small%20objects%2C%20which%20are%20difficult%20to%20detect%20and%20recognise.%0AThe%20results%20indicate%20that%20the%20proposed%20approach%20tends%20to%20produce%20better%20Average%0APrecision%20and%20greater%20accuracy%20for%20small%20objects%20in%20most%20of%20the%20tested%0Aconditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.16798v2&entry.124074799=Read"},
{"title": "LongSafetyBench: Long-Context LLMs Struggle with Safety Issues", "author": "Mianqiu Huang and Xiaoran Liu and Shaojun Zhou and Mozhi Zhang and Chenkun Tan and Pengyu Wang and Qipeng Guo and Zhe Xu and Linyang Li and Zhikai Lei and Linlin Li and Qun Liu and Yaqian Zhou and Xipeng Qiu and Xuanjing Huang", "abstract": "  With the development of large language models (LLMs), the sequence length of\nthese models continues to increase, drawing significant attention to\nlong-context language models. However, the evaluation of these models has been\nprimarily limited to their capabilities, with a lack of research focusing on\ntheir safety. Existing work, such as ManyShotJailbreak, has to some extent\ndemonstrated that long-context language models can exhibit safety concerns.\nHowever, the methods used are limited and lack comprehensiveness. In response,\nwe introduce \\textbf{LongSafetyBench}, the first benchmark designed to\nobjectively and comprehensively evaluate the safety of long-context models.\nLongSafetyBench consists of 10 task categories, with an average length of\n41,889 words. After testing eight long-context language models on\nLongSafetyBench, we found that existing models generally exhibit insufficient\nsafety capabilities. The proportion of safe responses from most mainstream\nlong-context LLMs is below 50\\%. Moreover, models' safety performance in\nlong-context scenarios does not always align with that in short-context\nscenarios. Further investigation revealed that long-context models tend to\noverlook harmful content within lengthy texts. We also proposed a simple yet\neffective solution, allowing open-source models to achieve performance\ncomparable to that of top-tier closed-source models. We believe that\nLongSafetyBench can serve as a valuable benchmark for evaluating the safety\ncapabilities of long-context language models. We hope that our work will\nencourage the broader community to pay attention to the safety of long-context\nmodels and contribute to the development of solutions to improve the safety of\nlong-context LLMs.\n", "link": "http://arxiv.org/abs/2411.06899v1", "date": "2024-11-11", "relevancy": 2.0569, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongSafetyBench%3A%20Long-Context%20LLMs%20Struggle%20with%20Safety%20Issues&body=Title%3A%20LongSafetyBench%3A%20Long-Context%20LLMs%20Struggle%20with%20Safety%20Issues%0AAuthor%3A%20Mianqiu%20Huang%20and%20Xiaoran%20Liu%20and%20Shaojun%20Zhou%20and%20Mozhi%20Zhang%20and%20Chenkun%20Tan%20and%20Pengyu%20Wang%20and%20Qipeng%20Guo%20and%20Zhe%20Xu%20and%20Linyang%20Li%20and%20Zhikai%20Lei%20and%20Linlin%20Li%20and%20Qun%20Liu%20and%20Yaqian%20Zhou%20and%20Xipeng%20Qiu%20and%20Xuanjing%20Huang%0AAbstract%3A%20%20%20With%20the%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20the%20sequence%20length%20of%0Athese%20models%20continues%20to%20increase%2C%20drawing%20significant%20attention%20to%0Along-context%20language%20models.%20However%2C%20the%20evaluation%20of%20these%20models%20has%20been%0Aprimarily%20limited%20to%20their%20capabilities%2C%20with%20a%20lack%20of%20research%20focusing%20on%0Atheir%20safety.%20Existing%20work%2C%20such%20as%20ManyShotJailbreak%2C%20has%20to%20some%20extent%0Ademonstrated%20that%20long-context%20language%20models%20can%20exhibit%20safety%20concerns.%0AHowever%2C%20the%20methods%20used%20are%20limited%20and%20lack%20comprehensiveness.%20In%20response%2C%0Awe%20introduce%20%5Ctextbf%7BLongSafetyBench%7D%2C%20the%20first%20benchmark%20designed%20to%0Aobjectively%20and%20comprehensively%20evaluate%20the%20safety%20of%20long-context%20models.%0ALongSafetyBench%20consists%20of%2010%20task%20categories%2C%20with%20an%20average%20length%20of%0A41%2C889%20words.%20After%20testing%20eight%20long-context%20language%20models%20on%0ALongSafetyBench%2C%20we%20found%20that%20existing%20models%20generally%20exhibit%20insufficient%0Asafety%20capabilities.%20The%20proportion%20of%20safe%20responses%20from%20most%20mainstream%0Along-context%20LLMs%20is%20below%2050%5C%25.%20Moreover%2C%20models%27%20safety%20performance%20in%0Along-context%20scenarios%20does%20not%20always%20align%20with%20that%20in%20short-context%0Ascenarios.%20Further%20investigation%20revealed%20that%20long-context%20models%20tend%20to%0Aoverlook%20harmful%20content%20within%20lengthy%20texts.%20We%20also%20proposed%20a%20simple%20yet%0Aeffective%20solution%2C%20allowing%20open-source%20models%20to%20achieve%20performance%0Acomparable%20to%20that%20of%20top-tier%20closed-source%20models.%20We%20believe%20that%0ALongSafetyBench%20can%20serve%20as%20a%20valuable%20benchmark%20for%20evaluating%20the%20safety%0Acapabilities%20of%20long-context%20language%20models.%20We%20hope%20that%20our%20work%20will%0Aencourage%20the%20broader%20community%20to%20pay%20attention%20to%20the%20safety%20of%20long-context%0Amodels%20and%20contribute%20to%20the%20development%20of%20solutions%20to%20improve%20the%20safety%20of%0Along-context%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongSafetyBench%253A%2520Long-Context%2520LLMs%2520Struggle%2520with%2520Safety%2520Issues%26entry.906535625%3DMianqiu%2520Huang%2520and%2520Xiaoran%2520Liu%2520and%2520Shaojun%2520Zhou%2520and%2520Mozhi%2520Zhang%2520and%2520Chenkun%2520Tan%2520and%2520Pengyu%2520Wang%2520and%2520Qipeng%2520Guo%2520and%2520Zhe%2520Xu%2520and%2520Linyang%2520Li%2520and%2520Zhikai%2520Lei%2520and%2520Linlin%2520Li%2520and%2520Qun%2520Liu%2520and%2520Yaqian%2520Zhou%2520and%2520Xipeng%2520Qiu%2520and%2520Xuanjing%2520Huang%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520the%2520sequence%2520length%2520of%250Athese%2520models%2520continues%2520to%2520increase%252C%2520drawing%2520significant%2520attention%2520to%250Along-context%2520language%2520models.%2520However%252C%2520the%2520evaluation%2520of%2520these%2520models%2520has%2520been%250Aprimarily%2520limited%2520to%2520their%2520capabilities%252C%2520with%2520a%2520lack%2520of%2520research%2520focusing%2520on%250Atheir%2520safety.%2520Existing%2520work%252C%2520such%2520as%2520ManyShotJailbreak%252C%2520has%2520to%2520some%2520extent%250Ademonstrated%2520that%2520long-context%2520language%2520models%2520can%2520exhibit%2520safety%2520concerns.%250AHowever%252C%2520the%2520methods%2520used%2520are%2520limited%2520and%2520lack%2520comprehensiveness.%2520In%2520response%252C%250Awe%2520introduce%2520%255Ctextbf%257BLongSafetyBench%257D%252C%2520the%2520first%2520benchmark%2520designed%2520to%250Aobjectively%2520and%2520comprehensively%2520evaluate%2520the%2520safety%2520of%2520long-context%2520models.%250ALongSafetyBench%2520consists%2520of%252010%2520task%2520categories%252C%2520with%2520an%2520average%2520length%2520of%250A41%252C889%2520words.%2520After%2520testing%2520eight%2520long-context%2520language%2520models%2520on%250ALongSafetyBench%252C%2520we%2520found%2520that%2520existing%2520models%2520generally%2520exhibit%2520insufficient%250Asafety%2520capabilities.%2520The%2520proportion%2520of%2520safe%2520responses%2520from%2520most%2520mainstream%250Along-context%2520LLMs%2520is%2520below%252050%255C%2525.%2520Moreover%252C%2520models%2527%2520safety%2520performance%2520in%250Along-context%2520scenarios%2520does%2520not%2520always%2520align%2520with%2520that%2520in%2520short-context%250Ascenarios.%2520Further%2520investigation%2520revealed%2520that%2520long-context%2520models%2520tend%2520to%250Aoverlook%2520harmful%2520content%2520within%2520lengthy%2520texts.%2520We%2520also%2520proposed%2520a%2520simple%2520yet%250Aeffective%2520solution%252C%2520allowing%2520open-source%2520models%2520to%2520achieve%2520performance%250Acomparable%2520to%2520that%2520of%2520top-tier%2520closed-source%2520models.%2520We%2520believe%2520that%250ALongSafetyBench%2520can%2520serve%2520as%2520a%2520valuable%2520benchmark%2520for%2520evaluating%2520the%2520safety%250Acapabilities%2520of%2520long-context%2520language%2520models.%2520We%2520hope%2520that%2520our%2520work%2520will%250Aencourage%2520the%2520broader%2520community%2520to%2520pay%2520attention%2520to%2520the%2520safety%2520of%2520long-context%250Amodels%2520and%2520contribute%2520to%2520the%2520development%2520of%2520solutions%2520to%2520improve%2520the%2520safety%2520of%250Along-context%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongSafetyBench%3A%20Long-Context%20LLMs%20Struggle%20with%20Safety%20Issues&entry.906535625=Mianqiu%20Huang%20and%20Xiaoran%20Liu%20and%20Shaojun%20Zhou%20and%20Mozhi%20Zhang%20and%20Chenkun%20Tan%20and%20Pengyu%20Wang%20and%20Qipeng%20Guo%20and%20Zhe%20Xu%20and%20Linyang%20Li%20and%20Zhikai%20Lei%20and%20Linlin%20Li%20and%20Qun%20Liu%20and%20Yaqian%20Zhou%20and%20Xipeng%20Qiu%20and%20Xuanjing%20Huang&entry.1292438233=%20%20With%20the%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20the%20sequence%20length%20of%0Athese%20models%20continues%20to%20increase%2C%20drawing%20significant%20attention%20to%0Along-context%20language%20models.%20However%2C%20the%20evaluation%20of%20these%20models%20has%20been%0Aprimarily%20limited%20to%20their%20capabilities%2C%20with%20a%20lack%20of%20research%20focusing%20on%0Atheir%20safety.%20Existing%20work%2C%20such%20as%20ManyShotJailbreak%2C%20has%20to%20some%20extent%0Ademonstrated%20that%20long-context%20language%20models%20can%20exhibit%20safety%20concerns.%0AHowever%2C%20the%20methods%20used%20are%20limited%20and%20lack%20comprehensiveness.%20In%20response%2C%0Awe%20introduce%20%5Ctextbf%7BLongSafetyBench%7D%2C%20the%20first%20benchmark%20designed%20to%0Aobjectively%20and%20comprehensively%20evaluate%20the%20safety%20of%20long-context%20models.%0ALongSafetyBench%20consists%20of%2010%20task%20categories%2C%20with%20an%20average%20length%20of%0A41%2C889%20words.%20After%20testing%20eight%20long-context%20language%20models%20on%0ALongSafetyBench%2C%20we%20found%20that%20existing%20models%20generally%20exhibit%20insufficient%0Asafety%20capabilities.%20The%20proportion%20of%20safe%20responses%20from%20most%20mainstream%0Along-context%20LLMs%20is%20below%2050%5C%25.%20Moreover%2C%20models%27%20safety%20performance%20in%0Along-context%20scenarios%20does%20not%20always%20align%20with%20that%20in%20short-context%0Ascenarios.%20Further%20investigation%20revealed%20that%20long-context%20models%20tend%20to%0Aoverlook%20harmful%20content%20within%20lengthy%20texts.%20We%20also%20proposed%20a%20simple%20yet%0Aeffective%20solution%2C%20allowing%20open-source%20models%20to%20achieve%20performance%0Acomparable%20to%20that%20of%20top-tier%20closed-source%20models.%20We%20believe%20that%0ALongSafetyBench%20can%20serve%20as%20a%20valuable%20benchmark%20for%20evaluating%20the%20safety%0Acapabilities%20of%20long-context%20language%20models.%20We%20hope%20that%20our%20work%20will%0Aencourage%20the%20broader%20community%20to%20pay%20attention%20to%20the%20safety%20of%20long-context%0Amodels%20and%20contribute%20to%20the%20development%20of%20solutions%20to%20improve%20the%20safety%20of%0Along-context%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06899v1&entry.124074799=Read"},
{"title": "DeepONet as a Multi-Operator Extrapolation Model: Distributed\n  Pretraining with Physics-Informed Fine-Tuning", "author": "Zecheng Zhang and Christian Moya and Lu Lu and Guang Lin and Hayden Schaeffer", "abstract": "  We propose a novel fine-tuning method to achieve multi-operator learning\nthrough training a distributed neural operator with diverse function data and\nthen zero-shot fine-tuning the neural network using physics-informed losses for\ndownstream tasks. Operator learning effectively approximates solution operators\nfor PDEs and various PDE-related problems, yet it often struggles to generalize\nto new tasks. To address this, we investigate fine-tuning a pretrained model,\nwhile carefully selecting an initialization that enables rapid adaptation to\nnew tasks with minimal data. Our approach combines distributed learning to\nintegrate data from various operators in pre-training, while physics-informed\nmethods enable zero-shot fine-tuning, minimizing the reliance on downstream\ndata. We investigate standard fine-tuning and Low-Rank Adaptation fine-tuning,\napplying both to train complex nonlinear target operators that are difficult to\nlearn only using random initialization. Through comprehensive numerical\nexamples, we demonstrate the advantages of our approach, showcasing significant\nimprovements in accuracy. Our findings provide a robust framework for advancing\nmulti-operator learning and highlight the potential of transfer learning\ntechniques in this domain.\n", "link": "http://arxiv.org/abs/2411.07239v1", "date": "2024-11-11", "relevancy": 2.0525, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5285}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5053}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepONet%20as%20a%20Multi-Operator%20Extrapolation%20Model%3A%20Distributed%0A%20%20Pretraining%20with%20Physics-Informed%20Fine-Tuning&body=Title%3A%20DeepONet%20as%20a%20Multi-Operator%20Extrapolation%20Model%3A%20Distributed%0A%20%20Pretraining%20with%20Physics-Informed%20Fine-Tuning%0AAuthor%3A%20Zecheng%20Zhang%20and%20Christian%20Moya%20and%20Lu%20Lu%20and%20Guang%20Lin%20and%20Hayden%20Schaeffer%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20fine-tuning%20method%20to%20achieve%20multi-operator%20learning%0Athrough%20training%20a%20distributed%20neural%20operator%20with%20diverse%20function%20data%20and%0Athen%20zero-shot%20fine-tuning%20the%20neural%20network%20using%20physics-informed%20losses%20for%0Adownstream%20tasks.%20Operator%20learning%20effectively%20approximates%20solution%20operators%0Afor%20PDEs%20and%20various%20PDE-related%20problems%2C%20yet%20it%20often%20struggles%20to%20generalize%0Ato%20new%20tasks.%20To%20address%20this%2C%20we%20investigate%20fine-tuning%20a%20pretrained%20model%2C%0Awhile%20carefully%20selecting%20an%20initialization%20that%20enables%20rapid%20adaptation%20to%0Anew%20tasks%20with%20minimal%20data.%20Our%20approach%20combines%20distributed%20learning%20to%0Aintegrate%20data%20from%20various%20operators%20in%20pre-training%2C%20while%20physics-informed%0Amethods%20enable%20zero-shot%20fine-tuning%2C%20minimizing%20the%20reliance%20on%20downstream%0Adata.%20We%20investigate%20standard%20fine-tuning%20and%20Low-Rank%20Adaptation%20fine-tuning%2C%0Aapplying%20both%20to%20train%20complex%20nonlinear%20target%20operators%20that%20are%20difficult%20to%0Alearn%20only%20using%20random%20initialization.%20Through%20comprehensive%20numerical%0Aexamples%2C%20we%20demonstrate%20the%20advantages%20of%20our%20approach%2C%20showcasing%20significant%0Aimprovements%20in%20accuracy.%20Our%20findings%20provide%20a%20robust%20framework%20for%20advancing%0Amulti-operator%20learning%20and%20highlight%20the%20potential%20of%20transfer%20learning%0Atechniques%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepONet%2520as%2520a%2520Multi-Operator%2520Extrapolation%2520Model%253A%2520Distributed%250A%2520%2520Pretraining%2520with%2520Physics-Informed%2520Fine-Tuning%26entry.906535625%3DZecheng%2520Zhang%2520and%2520Christian%2520Moya%2520and%2520Lu%2520Lu%2520and%2520Guang%2520Lin%2520and%2520Hayden%2520Schaeffer%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520fine-tuning%2520method%2520to%2520achieve%2520multi-operator%2520learning%250Athrough%2520training%2520a%2520distributed%2520neural%2520operator%2520with%2520diverse%2520function%2520data%2520and%250Athen%2520zero-shot%2520fine-tuning%2520the%2520neural%2520network%2520using%2520physics-informed%2520losses%2520for%250Adownstream%2520tasks.%2520Operator%2520learning%2520effectively%2520approximates%2520solution%2520operators%250Afor%2520PDEs%2520and%2520various%2520PDE-related%2520problems%252C%2520yet%2520it%2520often%2520struggles%2520to%2520generalize%250Ato%2520new%2520tasks.%2520To%2520address%2520this%252C%2520we%2520investigate%2520fine-tuning%2520a%2520pretrained%2520model%252C%250Awhile%2520carefully%2520selecting%2520an%2520initialization%2520that%2520enables%2520rapid%2520adaptation%2520to%250Anew%2520tasks%2520with%2520minimal%2520data.%2520Our%2520approach%2520combines%2520distributed%2520learning%2520to%250Aintegrate%2520data%2520from%2520various%2520operators%2520in%2520pre-training%252C%2520while%2520physics-informed%250Amethods%2520enable%2520zero-shot%2520fine-tuning%252C%2520minimizing%2520the%2520reliance%2520on%2520downstream%250Adata.%2520We%2520investigate%2520standard%2520fine-tuning%2520and%2520Low-Rank%2520Adaptation%2520fine-tuning%252C%250Aapplying%2520both%2520to%2520train%2520complex%2520nonlinear%2520target%2520operators%2520that%2520are%2520difficult%2520to%250Alearn%2520only%2520using%2520random%2520initialization.%2520Through%2520comprehensive%2520numerical%250Aexamples%252C%2520we%2520demonstrate%2520the%2520advantages%2520of%2520our%2520approach%252C%2520showcasing%2520significant%250Aimprovements%2520in%2520accuracy.%2520Our%2520findings%2520provide%2520a%2520robust%2520framework%2520for%2520advancing%250Amulti-operator%2520learning%2520and%2520highlight%2520the%2520potential%2520of%2520transfer%2520learning%250Atechniques%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepONet%20as%20a%20Multi-Operator%20Extrapolation%20Model%3A%20Distributed%0A%20%20Pretraining%20with%20Physics-Informed%20Fine-Tuning&entry.906535625=Zecheng%20Zhang%20and%20Christian%20Moya%20and%20Lu%20Lu%20and%20Guang%20Lin%20and%20Hayden%20Schaeffer&entry.1292438233=%20%20We%20propose%20a%20novel%20fine-tuning%20method%20to%20achieve%20multi-operator%20learning%0Athrough%20training%20a%20distributed%20neural%20operator%20with%20diverse%20function%20data%20and%0Athen%20zero-shot%20fine-tuning%20the%20neural%20network%20using%20physics-informed%20losses%20for%0Adownstream%20tasks.%20Operator%20learning%20effectively%20approximates%20solution%20operators%0Afor%20PDEs%20and%20various%20PDE-related%20problems%2C%20yet%20it%20often%20struggles%20to%20generalize%0Ato%20new%20tasks.%20To%20address%20this%2C%20we%20investigate%20fine-tuning%20a%20pretrained%20model%2C%0Awhile%20carefully%20selecting%20an%20initialization%20that%20enables%20rapid%20adaptation%20to%0Anew%20tasks%20with%20minimal%20data.%20Our%20approach%20combines%20distributed%20learning%20to%0Aintegrate%20data%20from%20various%20operators%20in%20pre-training%2C%20while%20physics-informed%0Amethods%20enable%20zero-shot%20fine-tuning%2C%20minimizing%20the%20reliance%20on%20downstream%0Adata.%20We%20investigate%20standard%20fine-tuning%20and%20Low-Rank%20Adaptation%20fine-tuning%2C%0Aapplying%20both%20to%20train%20complex%20nonlinear%20target%20operators%20that%20are%20difficult%20to%0Alearn%20only%20using%20random%20initialization.%20Through%20comprehensive%20numerical%0Aexamples%2C%20we%20demonstrate%20the%20advantages%20of%20our%20approach%2C%20showcasing%20significant%0Aimprovements%20in%20accuracy.%20Our%20findings%20provide%20a%20robust%20framework%20for%20advancing%0Amulti-operator%20learning%20and%20highlight%20the%20potential%20of%20transfer%20learning%0Atechniques%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07239v1&entry.124074799=Read"},
{"title": "EVQAScore: Efficient Video Question Answering Data Evaluation", "author": "Hao Liang and Zirong Chen and Wentao Zhang", "abstract": "  Video question-answering (QA) is a core task in video understanding.\nEvaluating the quality of video QA and video caption data quality for training\nvideo large language models (VideoLLMs) is an essential challenge. Although\nvarious methods have been proposed for assessing video caption quality, there\nremains a lack of dedicated evaluation methods for Video QA. To address this\ngap, we introduce EVQAScore, a reference-free method that leverages keyword\nextraction to assess both video caption and video QA data quality.\nAdditionally, we incorporate frame sampling and rescaling techniques to enhance\nthe efficiency and robustness of our evaluation, this enables our score to\nevaluate the quality of extremely long videos. Our approach achieves\nstate-of-the-art (SOTA) performance (32.8 for Kendall correlation and 42.3 for\nSpearman correlation, 4.7 and 5.9 higher than the previous method PAC-S++) on\nthe VATEX-EVAL benchmark for video caption evaluation. Furthermore, by using\nEVQAScore for data selection, we achieved SOTA results with only 12.5\\% of the\noriginal data volume, outperforming the previous SOTA method PAC-S and 100\\% of\ndata.\n", "link": "http://arxiv.org/abs/2411.06908v1", "date": "2024-11-11", "relevancy": 2.0503, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVQAScore%3A%20Efficient%20Video%20Question%20Answering%20Data%20Evaluation&body=Title%3A%20EVQAScore%3A%20Efficient%20Video%20Question%20Answering%20Data%20Evaluation%0AAuthor%3A%20Hao%20Liang%20and%20Zirong%20Chen%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Video%20question-answering%20%28QA%29%20is%20a%20core%20task%20in%20video%20understanding.%0AEvaluating%20the%20quality%20of%20video%20QA%20and%20video%20caption%20data%20quality%20for%20training%0Avideo%20large%20language%20models%20%28VideoLLMs%29%20is%20an%20essential%20challenge.%20Although%0Avarious%20methods%20have%20been%20proposed%20for%20assessing%20video%20caption%20quality%2C%20there%0Aremains%20a%20lack%20of%20dedicated%20evaluation%20methods%20for%20Video%20QA.%20To%20address%20this%0Agap%2C%20we%20introduce%20EVQAScore%2C%20a%20reference-free%20method%20that%20leverages%20keyword%0Aextraction%20to%20assess%20both%20video%20caption%20and%20video%20QA%20data%20quality.%0AAdditionally%2C%20we%20incorporate%20frame%20sampling%20and%20rescaling%20techniques%20to%20enhance%0Athe%20efficiency%20and%20robustness%20of%20our%20evaluation%2C%20this%20enables%20our%20score%20to%0Aevaluate%20the%20quality%20of%20extremely%20long%20videos.%20Our%20approach%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20%2832.8%20for%20Kendall%20correlation%20and%2042.3%20for%0ASpearman%20correlation%2C%204.7%20and%205.9%20higher%20than%20the%20previous%20method%20PAC-S%2B%2B%29%20on%0Athe%20VATEX-EVAL%20benchmark%20for%20video%20caption%20evaluation.%20Furthermore%2C%20by%20using%0AEVQAScore%20for%20data%20selection%2C%20we%20achieved%20SOTA%20results%20with%20only%2012.5%5C%25%20of%20the%0Aoriginal%20data%20volume%2C%20outperforming%20the%20previous%20SOTA%20method%20PAC-S%20and%20100%5C%25%20of%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVQAScore%253A%2520Efficient%2520Video%2520Question%2520Answering%2520Data%2520Evaluation%26entry.906535625%3DHao%2520Liang%2520and%2520Zirong%2520Chen%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520question-answering%2520%2528QA%2529%2520is%2520a%2520core%2520task%2520in%2520video%2520understanding.%250AEvaluating%2520the%2520quality%2520of%2520video%2520QA%2520and%2520video%2520caption%2520data%2520quality%2520for%2520training%250Avideo%2520large%2520language%2520models%2520%2528VideoLLMs%2529%2520is%2520an%2520essential%2520challenge.%2520Although%250Avarious%2520methods%2520have%2520been%2520proposed%2520for%2520assessing%2520video%2520caption%2520quality%252C%2520there%250Aremains%2520a%2520lack%2520of%2520dedicated%2520evaluation%2520methods%2520for%2520Video%2520QA.%2520To%2520address%2520this%250Agap%252C%2520we%2520introduce%2520EVQAScore%252C%2520a%2520reference-free%2520method%2520that%2520leverages%2520keyword%250Aextraction%2520to%2520assess%2520both%2520video%2520caption%2520and%2520video%2520QA%2520data%2520quality.%250AAdditionally%252C%2520we%2520incorporate%2520frame%2520sampling%2520and%2520rescaling%2520techniques%2520to%2520enhance%250Athe%2520efficiency%2520and%2520robustness%2520of%2520our%2520evaluation%252C%2520this%2520enables%2520our%2520score%2520to%250Aevaluate%2520the%2520quality%2520of%2520extremely%2520long%2520videos.%2520Our%2520approach%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520%252832.8%2520for%2520Kendall%2520correlation%2520and%252042.3%2520for%250ASpearman%2520correlation%252C%25204.7%2520and%25205.9%2520higher%2520than%2520the%2520previous%2520method%2520PAC-S%252B%252B%2529%2520on%250Athe%2520VATEX-EVAL%2520benchmark%2520for%2520video%2520caption%2520evaluation.%2520Furthermore%252C%2520by%2520using%250AEVQAScore%2520for%2520data%2520selection%252C%2520we%2520achieved%2520SOTA%2520results%2520with%2520only%252012.5%255C%2525%2520of%2520the%250Aoriginal%2520data%2520volume%252C%2520outperforming%2520the%2520previous%2520SOTA%2520method%2520PAC-S%2520and%2520100%255C%2525%2520of%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVQAScore%3A%20Efficient%20Video%20Question%20Answering%20Data%20Evaluation&entry.906535625=Hao%20Liang%20and%20Zirong%20Chen%20and%20Wentao%20Zhang&entry.1292438233=%20%20Video%20question-answering%20%28QA%29%20is%20a%20core%20task%20in%20video%20understanding.%0AEvaluating%20the%20quality%20of%20video%20QA%20and%20video%20caption%20data%20quality%20for%20training%0Avideo%20large%20language%20models%20%28VideoLLMs%29%20is%20an%20essential%20challenge.%20Although%0Avarious%20methods%20have%20been%20proposed%20for%20assessing%20video%20caption%20quality%2C%20there%0Aremains%20a%20lack%20of%20dedicated%20evaluation%20methods%20for%20Video%20QA.%20To%20address%20this%0Agap%2C%20we%20introduce%20EVQAScore%2C%20a%20reference-free%20method%20that%20leverages%20keyword%0Aextraction%20to%20assess%20both%20video%20caption%20and%20video%20QA%20data%20quality.%0AAdditionally%2C%20we%20incorporate%20frame%20sampling%20and%20rescaling%20techniques%20to%20enhance%0Athe%20efficiency%20and%20robustness%20of%20our%20evaluation%2C%20this%20enables%20our%20score%20to%0Aevaluate%20the%20quality%20of%20extremely%20long%20videos.%20Our%20approach%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20%2832.8%20for%20Kendall%20correlation%20and%2042.3%20for%0ASpearman%20correlation%2C%204.7%20and%205.9%20higher%20than%20the%20previous%20method%20PAC-S%2B%2B%29%20on%0Athe%20VATEX-EVAL%20benchmark%20for%20video%20caption%20evaluation.%20Furthermore%2C%20by%20using%0AEVQAScore%20for%20data%20selection%2C%20we%20achieved%20SOTA%20results%20with%20only%2012.5%5C%25%20of%20the%0Aoriginal%20data%20volume%2C%20outperforming%20the%20previous%20SOTA%20method%20PAC-S%20and%20100%5C%25%20of%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06908v1&entry.124074799=Read"},
{"title": "Bipedal walking with continuously compliant robotic legs", "author": "Robin Bendfeld and C. David Remy", "abstract": "  In biomechanics and robotics, elasticity plays a crucial role in enhancing\nlocomotion efficiency and stability. Traditional approaches in legged robots\noften employ series elastic actuators (SEA) with discrete rigid components,\nwhich, while effective, add weight and complexity. This paper presents an\ninnovative alternative by integrating continuously compliant structures into\nthe lower legs of a bipedal robot, fundamentally transforming the SEA concept.\nOur approach replaces traditional rigid segments with lightweight, deformable\nmaterials, reducing overall mass and simplifying the actuation design. This\nnovel design introduces unique challenges in modeling, sensing, and control,\ndue to the infinite dimensionality of continuously compliant elements. We\naddress these challenges through effective approximations and control\nstrategies. The paper details the design and modeling of the compliant leg\nstructure, presents low-level force and kinematics controllers, and introduces\na high-level posture controller with a gait scheduler. Experimental results\ndemonstrate successful bipedal walking using this new design.\n", "link": "http://arxiv.org/abs/2411.06948v1", "date": "2024-11-11", "relevancy": 1.5601, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5609}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5156}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bipedal%20walking%20with%20continuously%20compliant%20robotic%20legs&body=Title%3A%20Bipedal%20walking%20with%20continuously%20compliant%20robotic%20legs%0AAuthor%3A%20Robin%20Bendfeld%20and%20C.%20David%20Remy%0AAbstract%3A%20%20%20In%20biomechanics%20and%20robotics%2C%20elasticity%20plays%20a%20crucial%20role%20in%20enhancing%0Alocomotion%20efficiency%20and%20stability.%20Traditional%20approaches%20in%20legged%20robots%0Aoften%20employ%20series%20elastic%20actuators%20%28SEA%29%20with%20discrete%20rigid%20components%2C%0Awhich%2C%20while%20effective%2C%20add%20weight%20and%20complexity.%20This%20paper%20presents%20an%0Ainnovative%20alternative%20by%20integrating%20continuously%20compliant%20structures%20into%0Athe%20lower%20legs%20of%20a%20bipedal%20robot%2C%20fundamentally%20transforming%20the%20SEA%20concept.%0AOur%20approach%20replaces%20traditional%20rigid%20segments%20with%20lightweight%2C%20deformable%0Amaterials%2C%20reducing%20overall%20mass%20and%20simplifying%20the%20actuation%20design.%20This%0Anovel%20design%20introduces%20unique%20challenges%20in%20modeling%2C%20sensing%2C%20and%20control%2C%0Adue%20to%20the%20infinite%20dimensionality%20of%20continuously%20compliant%20elements.%20We%0Aaddress%20these%20challenges%20through%20effective%20approximations%20and%20control%0Astrategies.%20The%20paper%20details%20the%20design%20and%20modeling%20of%20the%20compliant%20leg%0Astructure%2C%20presents%20low-level%20force%20and%20kinematics%20controllers%2C%20and%20introduces%0Aa%20high-level%20posture%20controller%20with%20a%20gait%20scheduler.%20Experimental%20results%0Ademonstrate%20successful%20bipedal%20walking%20using%20this%20new%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBipedal%2520walking%2520with%2520continuously%2520compliant%2520robotic%2520legs%26entry.906535625%3DRobin%2520Bendfeld%2520and%2520C.%2520David%2520Remy%26entry.1292438233%3D%2520%2520In%2520biomechanics%2520and%2520robotics%252C%2520elasticity%2520plays%2520a%2520crucial%2520role%2520in%2520enhancing%250Alocomotion%2520efficiency%2520and%2520stability.%2520Traditional%2520approaches%2520in%2520legged%2520robots%250Aoften%2520employ%2520series%2520elastic%2520actuators%2520%2528SEA%2529%2520with%2520discrete%2520rigid%2520components%252C%250Awhich%252C%2520while%2520effective%252C%2520add%2520weight%2520and%2520complexity.%2520This%2520paper%2520presents%2520an%250Ainnovative%2520alternative%2520by%2520integrating%2520continuously%2520compliant%2520structures%2520into%250Athe%2520lower%2520legs%2520of%2520a%2520bipedal%2520robot%252C%2520fundamentally%2520transforming%2520the%2520SEA%2520concept.%250AOur%2520approach%2520replaces%2520traditional%2520rigid%2520segments%2520with%2520lightweight%252C%2520deformable%250Amaterials%252C%2520reducing%2520overall%2520mass%2520and%2520simplifying%2520the%2520actuation%2520design.%2520This%250Anovel%2520design%2520introduces%2520unique%2520challenges%2520in%2520modeling%252C%2520sensing%252C%2520and%2520control%252C%250Adue%2520to%2520the%2520infinite%2520dimensionality%2520of%2520continuously%2520compliant%2520elements.%2520We%250Aaddress%2520these%2520challenges%2520through%2520effective%2520approximations%2520and%2520control%250Astrategies.%2520The%2520paper%2520details%2520the%2520design%2520and%2520modeling%2520of%2520the%2520compliant%2520leg%250Astructure%252C%2520presents%2520low-level%2520force%2520and%2520kinematics%2520controllers%252C%2520and%2520introduces%250Aa%2520high-level%2520posture%2520controller%2520with%2520a%2520gait%2520scheduler.%2520Experimental%2520results%250Ademonstrate%2520successful%2520bipedal%2520walking%2520using%2520this%2520new%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bipedal%20walking%20with%20continuously%20compliant%20robotic%20legs&entry.906535625=Robin%20Bendfeld%20and%20C.%20David%20Remy&entry.1292438233=%20%20In%20biomechanics%20and%20robotics%2C%20elasticity%20plays%20a%20crucial%20role%20in%20enhancing%0Alocomotion%20efficiency%20and%20stability.%20Traditional%20approaches%20in%20legged%20robots%0Aoften%20employ%20series%20elastic%20actuators%20%28SEA%29%20with%20discrete%20rigid%20components%2C%0Awhich%2C%20while%20effective%2C%20add%20weight%20and%20complexity.%20This%20paper%20presents%20an%0Ainnovative%20alternative%20by%20integrating%20continuously%20compliant%20structures%20into%0Athe%20lower%20legs%20of%20a%20bipedal%20robot%2C%20fundamentally%20transforming%20the%20SEA%20concept.%0AOur%20approach%20replaces%20traditional%20rigid%20segments%20with%20lightweight%2C%20deformable%0Amaterials%2C%20reducing%20overall%20mass%20and%20simplifying%20the%20actuation%20design.%20This%0Anovel%20design%20introduces%20unique%20challenges%20in%20modeling%2C%20sensing%2C%20and%20control%2C%0Adue%20to%20the%20infinite%20dimensionality%20of%20continuously%20compliant%20elements.%20We%0Aaddress%20these%20challenges%20through%20effective%20approximations%20and%20control%0Astrategies.%20The%20paper%20details%20the%20design%20and%20modeling%20of%20the%20compliant%20leg%0Astructure%2C%20presents%20low-level%20force%20and%20kinematics%20controllers%2C%20and%20introduces%0Aa%20high-level%20posture%20controller%20with%20a%20gait%20scheduler.%20Experimental%20results%0Ademonstrate%20successful%20bipedal%20walking%20using%20this%20new%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06948v1&entry.124074799=Read"},
{"title": "Increasing Rosacea Awareness Among Population Using Deep Learning and\n  Statistical Approaches", "author": "Chengyu Yang and Chengjun Liu", "abstract": "  Approximately 16 million Americans suffer from rosacea according to the\nNational Rosacea Society. To increase rosacea awareness, automatic rosacea\ndetection methods using deep learning and explainable statistical approaches\nare presented in this paper. The deep learning method applies the ResNet-18 for\nrosacea detection, and the statistical approaches utilize the means of the two\nclasses, namely, the rosacea class vs. the normal class, and the principal\ncomponent analysis to extract features from the facial images for automatic\nrosacea detection. The contributions of the proposed methods are three-fold.\nFirst, the proposed methods are able to automatically distinguish patients who\nare suffering from rosacea from people who are clean of this disease. Second,\nthe statistical approaches address the explainability issue that allows doctors\nand patients to understand and trust the results. And finally, the proposed\nmethods will not only help increase rosacea awareness in the general population\nbut also help remind the patients who suffer from this disease of possible\nearly treatment since rosacea is more treatable at its early stages. The code\nand data are available at https://github.com/cyang322/rosacea_detection.git.\n", "link": "http://arxiv.org/abs/2411.07074v1", "date": "2024-11-11", "relevancy": 0.9319, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4693}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4647}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Increasing%20Rosacea%20Awareness%20Among%20Population%20Using%20Deep%20Learning%20and%0A%20%20Statistical%20Approaches&body=Title%3A%20Increasing%20Rosacea%20Awareness%20Among%20Population%20Using%20Deep%20Learning%20and%0A%20%20Statistical%20Approaches%0AAuthor%3A%20Chengyu%20Yang%20and%20Chengjun%20Liu%0AAbstract%3A%20%20%20Approximately%2016%20million%20Americans%20suffer%20from%20rosacea%20according%20to%20the%0ANational%20Rosacea%20Society.%20To%20increase%20rosacea%20awareness%2C%20automatic%20rosacea%0Adetection%20methods%20using%20deep%20learning%20and%20explainable%20statistical%20approaches%0Aare%20presented%20in%20this%20paper.%20The%20deep%20learning%20method%20applies%20the%20ResNet-18%20for%0Arosacea%20detection%2C%20and%20the%20statistical%20approaches%20utilize%20the%20means%20of%20the%20two%0Aclasses%2C%20namely%2C%20the%20rosacea%20class%20vs.%20the%20normal%20class%2C%20and%20the%20principal%0Acomponent%20analysis%20to%20extract%20features%20from%20the%20facial%20images%20for%20automatic%0Arosacea%20detection.%20The%20contributions%20of%20the%20proposed%20methods%20are%20three-fold.%0AFirst%2C%20the%20proposed%20methods%20are%20able%20to%20automatically%20distinguish%20patients%20who%0Aare%20suffering%20from%20rosacea%20from%20people%20who%20are%20clean%20of%20this%20disease.%20Second%2C%0Athe%20statistical%20approaches%20address%20the%20explainability%20issue%20that%20allows%20doctors%0Aand%20patients%20to%20understand%20and%20trust%20the%20results.%20And%20finally%2C%20the%20proposed%0Amethods%20will%20not%20only%20help%20increase%20rosacea%20awareness%20in%20the%20general%20population%0Abut%20also%20help%20remind%20the%20patients%20who%20suffer%20from%20this%20disease%20of%20possible%0Aearly%20treatment%20since%20rosacea%20is%20more%20treatable%20at%20its%20early%20stages.%20The%20code%0Aand%20data%20are%20available%20at%20https%3A//github.com/cyang322/rosacea_detection.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncreasing%2520Rosacea%2520Awareness%2520Among%2520Population%2520Using%2520Deep%2520Learning%2520and%250A%2520%2520Statistical%2520Approaches%26entry.906535625%3DChengyu%2520Yang%2520and%2520Chengjun%2520Liu%26entry.1292438233%3D%2520%2520Approximately%252016%2520million%2520Americans%2520suffer%2520from%2520rosacea%2520according%2520to%2520the%250ANational%2520Rosacea%2520Society.%2520To%2520increase%2520rosacea%2520awareness%252C%2520automatic%2520rosacea%250Adetection%2520methods%2520using%2520deep%2520learning%2520and%2520explainable%2520statistical%2520approaches%250Aare%2520presented%2520in%2520this%2520paper.%2520The%2520deep%2520learning%2520method%2520applies%2520the%2520ResNet-18%2520for%250Arosacea%2520detection%252C%2520and%2520the%2520statistical%2520approaches%2520utilize%2520the%2520means%2520of%2520the%2520two%250Aclasses%252C%2520namely%252C%2520the%2520rosacea%2520class%2520vs.%2520the%2520normal%2520class%252C%2520and%2520the%2520principal%250Acomponent%2520analysis%2520to%2520extract%2520features%2520from%2520the%2520facial%2520images%2520for%2520automatic%250Arosacea%2520detection.%2520The%2520contributions%2520of%2520the%2520proposed%2520methods%2520are%2520three-fold.%250AFirst%252C%2520the%2520proposed%2520methods%2520are%2520able%2520to%2520automatically%2520distinguish%2520patients%2520who%250Aare%2520suffering%2520from%2520rosacea%2520from%2520people%2520who%2520are%2520clean%2520of%2520this%2520disease.%2520Second%252C%250Athe%2520statistical%2520approaches%2520address%2520the%2520explainability%2520issue%2520that%2520allows%2520doctors%250Aand%2520patients%2520to%2520understand%2520and%2520trust%2520the%2520results.%2520And%2520finally%252C%2520the%2520proposed%250Amethods%2520will%2520not%2520only%2520help%2520increase%2520rosacea%2520awareness%2520in%2520the%2520general%2520population%250Abut%2520also%2520help%2520remind%2520the%2520patients%2520who%2520suffer%2520from%2520this%2520disease%2520of%2520possible%250Aearly%2520treatment%2520since%2520rosacea%2520is%2520more%2520treatable%2520at%2520its%2520early%2520stages.%2520The%2520code%250Aand%2520data%2520are%2520available%2520at%2520https%253A//github.com/cyang322/rosacea_detection.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Increasing%20Rosacea%20Awareness%20Among%20Population%20Using%20Deep%20Learning%20and%0A%20%20Statistical%20Approaches&entry.906535625=Chengyu%20Yang%20and%20Chengjun%20Liu&entry.1292438233=%20%20Approximately%2016%20million%20Americans%20suffer%20from%20rosacea%20according%20to%20the%0ANational%20Rosacea%20Society.%20To%20increase%20rosacea%20awareness%2C%20automatic%20rosacea%0Adetection%20methods%20using%20deep%20learning%20and%20explainable%20statistical%20approaches%0Aare%20presented%20in%20this%20paper.%20The%20deep%20learning%20method%20applies%20the%20ResNet-18%20for%0Arosacea%20detection%2C%20and%20the%20statistical%20approaches%20utilize%20the%20means%20of%20the%20two%0Aclasses%2C%20namely%2C%20the%20rosacea%20class%20vs.%20the%20normal%20class%2C%20and%20the%20principal%0Acomponent%20analysis%20to%20extract%20features%20from%20the%20facial%20images%20for%20automatic%0Arosacea%20detection.%20The%20contributions%20of%20the%20proposed%20methods%20are%20three-fold.%0AFirst%2C%20the%20proposed%20methods%20are%20able%20to%20automatically%20distinguish%20patients%20who%0Aare%20suffering%20from%20rosacea%20from%20people%20who%20are%20clean%20of%20this%20disease.%20Second%2C%0Athe%20statistical%20approaches%20address%20the%20explainability%20issue%20that%20allows%20doctors%0Aand%20patients%20to%20understand%20and%20trust%20the%20results.%20And%20finally%2C%20the%20proposed%0Amethods%20will%20not%20only%20help%20increase%20rosacea%20awareness%20in%20the%20general%20population%0Abut%20also%20help%20remind%20the%20patients%20who%20suffer%20from%20this%20disease%20of%20possible%0Aearly%20treatment%20since%20rosacea%20is%20more%20treatable%20at%20its%20early%20stages.%20The%20code%0Aand%20data%20are%20available%20at%20https%3A//github.com/cyang322/rosacea_detection.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07074v1&entry.124074799=Read"},
{"title": "The Super Weight in Large Language Models", "author": "Mengxia Yu and De Wang and Qi Shan and Colorado Reed and Alvin Wan", "abstract": "  Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs.\n", "link": "http://arxiv.org/abs/2411.07191v1", "date": "2024-11-11", "relevancy": 1.8334, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4747}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Super%20Weight%20in%20Large%20Language%20Models&body=Title%3A%20The%20Super%20Weight%20in%20Large%20Language%20Models%0AAuthor%3A%20Mengxia%20Yu%20and%20De%20Wang%20and%20Qi%20Shan%20and%20Colorado%20Reed%20and%20Alvin%20Wan%0AAbstract%3A%20%20%20Recent%20works%20have%20shown%20a%20surprising%20result%3A%20a%20small%20fraction%20of%20Large%0ALanguage%20Model%20%28LLM%29%20parameter%20outliers%20are%20disproportionately%20important%20to%20the%0Aquality%20of%20the%20model.%20LLMs%20contain%20billions%20of%20parameters%2C%20so%20these%20small%0Afractions%2C%20such%20as%200.01%25%2C%20translate%20to%20hundreds%20of%20thousands%20of%20parameters.%20In%0Athis%20work%2C%20we%20present%20an%20even%20more%20surprising%20finding%3A%20Pruning%20as%20few%20as%20a%0Asingle%20parameter%20can%20destroy%20an%20LLM%27s%20ability%20to%20generate%20text%20--%20increasing%0Aperplexity%20by%203%20orders%20of%20magnitude%20and%20reducing%20zero-shot%20accuracy%20to%0Aguessing.%20We%20propose%20a%20data-free%20method%20for%20identifying%20such%20parameters%2C%20termed%0Asuper%20weights%2C%20using%20a%20single%20forward%20pass%20through%20the%20model.%20We%20additionally%0Afind%20that%20these%20super%20weights%20induce%20correspondingly%20rare%20and%20large%20activation%0Aoutliers%2C%20termed%20super%20activations.%20When%20preserved%20with%20high%20precision%2C%20super%0Aactivations%20can%20improve%20simple%20round-to-nearest%20quantization%20to%20become%0Acompetitive%20with%20state-of-the-art%20methods.%20For%20weight%20quantization%2C%20we%0Asimilarly%20find%20that%20by%20preserving%20the%20super%20weight%20and%20clipping%20other%20weight%0Aoutliers%2C%20round-to-nearest%20quantization%20can%20scale%20to%20much%20larger%20block%20sizes%0Athan%20previously%20considered.%20To%20facilitate%20further%20research%20into%20super%20weights%2C%0Awe%20provide%20an%20index%20of%20super%20weight%20coordinates%20for%20common%2C%20openly%20available%0ALLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Super%2520Weight%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DMengxia%2520Yu%2520and%2520De%2520Wang%2520and%2520Qi%2520Shan%2520and%2520Colorado%2520Reed%2520and%2520Alvin%2520Wan%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520shown%2520a%2520surprising%2520result%253A%2520a%2520small%2520fraction%2520of%2520Large%250ALanguage%2520Model%2520%2528LLM%2529%2520parameter%2520outliers%2520are%2520disproportionately%2520important%2520to%2520the%250Aquality%2520of%2520the%2520model.%2520LLMs%2520contain%2520billions%2520of%2520parameters%252C%2520so%2520these%2520small%250Afractions%252C%2520such%2520as%25200.01%2525%252C%2520translate%2520to%2520hundreds%2520of%2520thousands%2520of%2520parameters.%2520In%250Athis%2520work%252C%2520we%2520present%2520an%2520even%2520more%2520surprising%2520finding%253A%2520Pruning%2520as%2520few%2520as%2520a%250Asingle%2520parameter%2520can%2520destroy%2520an%2520LLM%2527s%2520ability%2520to%2520generate%2520text%2520--%2520increasing%250Aperplexity%2520by%25203%2520orders%2520of%2520magnitude%2520and%2520reducing%2520zero-shot%2520accuracy%2520to%250Aguessing.%2520We%2520propose%2520a%2520data-free%2520method%2520for%2520identifying%2520such%2520parameters%252C%2520termed%250Asuper%2520weights%252C%2520using%2520a%2520single%2520forward%2520pass%2520through%2520the%2520model.%2520We%2520additionally%250Afind%2520that%2520these%2520super%2520weights%2520induce%2520correspondingly%2520rare%2520and%2520large%2520activation%250Aoutliers%252C%2520termed%2520super%2520activations.%2520When%2520preserved%2520with%2520high%2520precision%252C%2520super%250Aactivations%2520can%2520improve%2520simple%2520round-to-nearest%2520quantization%2520to%2520become%250Acompetitive%2520with%2520state-of-the-art%2520methods.%2520For%2520weight%2520quantization%252C%2520we%250Asimilarly%2520find%2520that%2520by%2520preserving%2520the%2520super%2520weight%2520and%2520clipping%2520other%2520weight%250Aoutliers%252C%2520round-to-nearest%2520quantization%2520can%2520scale%2520to%2520much%2520larger%2520block%2520sizes%250Athan%2520previously%2520considered.%2520To%2520facilitate%2520further%2520research%2520into%2520super%2520weights%252C%250Awe%2520provide%2520an%2520index%2520of%2520super%2520weight%2520coordinates%2520for%2520common%252C%2520openly%2520available%250ALLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Super%20Weight%20in%20Large%20Language%20Models&entry.906535625=Mengxia%20Yu%20and%20De%20Wang%20and%20Qi%20Shan%20and%20Colorado%20Reed%20and%20Alvin%20Wan&entry.1292438233=%20%20Recent%20works%20have%20shown%20a%20surprising%20result%3A%20a%20small%20fraction%20of%20Large%0ALanguage%20Model%20%28LLM%29%20parameter%20outliers%20are%20disproportionately%20important%20to%20the%0Aquality%20of%20the%20model.%20LLMs%20contain%20billions%20of%20parameters%2C%20so%20these%20small%0Afractions%2C%20such%20as%200.01%25%2C%20translate%20to%20hundreds%20of%20thousands%20of%20parameters.%20In%0Athis%20work%2C%20we%20present%20an%20even%20more%20surprising%20finding%3A%20Pruning%20as%20few%20as%20a%0Asingle%20parameter%20can%20destroy%20an%20LLM%27s%20ability%20to%20generate%20text%20--%20increasing%0Aperplexity%20by%203%20orders%20of%20magnitude%20and%20reducing%20zero-shot%20accuracy%20to%0Aguessing.%20We%20propose%20a%20data-free%20method%20for%20identifying%20such%20parameters%2C%20termed%0Asuper%20weights%2C%20using%20a%20single%20forward%20pass%20through%20the%20model.%20We%20additionally%0Afind%20that%20these%20super%20weights%20induce%20correspondingly%20rare%20and%20large%20activation%0Aoutliers%2C%20termed%20super%20activations.%20When%20preserved%20with%20high%20precision%2C%20super%0Aactivations%20can%20improve%20simple%20round-to-nearest%20quantization%20to%20become%0Acompetitive%20with%20state-of-the-art%20methods.%20For%20weight%20quantization%2C%20we%0Asimilarly%20find%20that%20by%20preserving%20the%20super%20weight%20and%20clipping%20other%20weight%0Aoutliers%2C%20round-to-nearest%20quantization%20can%20scale%20to%20much%20larger%20block%20sizes%0Athan%20previously%20considered.%20To%20facilitate%20further%20research%20into%20super%20weights%2C%0Awe%20provide%20an%20index%20of%20super%20weight%20coordinates%20for%20common%2C%20openly%20available%0ALLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07191v1&entry.124074799=Read"},
{"title": "Examining Attacks on Consensus and Incentive Systems in Proof-of-Work\n  Blockchains: A Systematic Literature Review", "author": "Dinitha Wijewardhana and Sugandima Vidanagamachchi and Nalin Arachchilage", "abstract": "  Cryptocurrencies have gained popularity due to their transparency, security,\nand accessibility compared to traditional financial systems, with Bitcoin,\nintroduced in 2009, leading the market. Bitcoin's security relies on blockchain\ntechnology - a decentralized ledger consisting of a consensus and an incentive\nmechanism. The consensus mechanism, Proof of Work (PoW), requires miners to\nsolve difficult cryptographic puzzles to add new blocks, while the incentive\nmechanism rewards them with newly minted bitcoins. However, as Bitcoin's\nacceptance grows, it faces increasing threats from attacks targeting these\nmechanisms, such as selfish mining, double-spending, and block withholding.\nThese attacks compromise security, efficiency, and reward distribution. Recent\nresearch shows that these attacks can be combined with each other or with\neither malicious strategies, such as network-layer attacks, or non-malicious\nstrategies, like honest mining. These combinations lead to more sophisticated\nattacks, increasing the attacker's success rates and profitability. Therefore,\nunderstanding and evaluating these attacks is essential for developing\neffective countermeasures and ensuring long-term security. This paper begins by\nexamining individual attacks executed in isolation and their profitability. It\nthen explores how combining these attacks with each other or with other\nmalicious and non-malicious strategies can enhance their overall effectiveness\nand profitability. The analysis further explores how the deployment of attacks\nsuch as selfish mining and block withholding by multiple competing mining pools\nagainst each other impacts their economic returns. Lastly, a set of design\nguidelines is provided, outlining areas future work should focus on to prevent\nor mitigate the identified threats.\n", "link": "http://arxiv.org/abs/2411.00349v2", "date": "2024-11-11", "relevancy": 1.563, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3096}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Examining%20Attacks%20on%20Consensus%20and%20Incentive%20Systems%20in%20Proof-of-Work%0A%20%20Blockchains%3A%20A%20Systematic%20Literature%20Review&body=Title%3A%20Examining%20Attacks%20on%20Consensus%20and%20Incentive%20Systems%20in%20Proof-of-Work%0A%20%20Blockchains%3A%20A%20Systematic%20Literature%20Review%0AAuthor%3A%20Dinitha%20Wijewardhana%20and%20Sugandima%20Vidanagamachchi%20and%20Nalin%20Arachchilage%0AAbstract%3A%20%20%20Cryptocurrencies%20have%20gained%20popularity%20due%20to%20their%20transparency%2C%20security%2C%0Aand%20accessibility%20compared%20to%20traditional%20financial%20systems%2C%20with%20Bitcoin%2C%0Aintroduced%20in%202009%2C%20leading%20the%20market.%20Bitcoin%27s%20security%20relies%20on%20blockchain%0Atechnology%20-%20a%20decentralized%20ledger%20consisting%20of%20a%20consensus%20and%20an%20incentive%0Amechanism.%20The%20consensus%20mechanism%2C%20Proof%20of%20Work%20%28PoW%29%2C%20requires%20miners%20to%0Asolve%20difficult%20cryptographic%20puzzles%20to%20add%20new%20blocks%2C%20while%20the%20incentive%0Amechanism%20rewards%20them%20with%20newly%20minted%20bitcoins.%20However%2C%20as%20Bitcoin%27s%0Aacceptance%20grows%2C%20it%20faces%20increasing%20threats%20from%20attacks%20targeting%20these%0Amechanisms%2C%20such%20as%20selfish%20mining%2C%20double-spending%2C%20and%20block%20withholding.%0AThese%20attacks%20compromise%20security%2C%20efficiency%2C%20and%20reward%20distribution.%20Recent%0Aresearch%20shows%20that%20these%20attacks%20can%20be%20combined%20with%20each%20other%20or%20with%0Aeither%20malicious%20strategies%2C%20such%20as%20network-layer%20attacks%2C%20or%20non-malicious%0Astrategies%2C%20like%20honest%20mining.%20These%20combinations%20lead%20to%20more%20sophisticated%0Aattacks%2C%20increasing%20the%20attacker%27s%20success%20rates%20and%20profitability.%20Therefore%2C%0Aunderstanding%20and%20evaluating%20these%20attacks%20is%20essential%20for%20developing%0Aeffective%20countermeasures%20and%20ensuring%20long-term%20security.%20This%20paper%20begins%20by%0Aexamining%20individual%20attacks%20executed%20in%20isolation%20and%20their%20profitability.%20It%0Athen%20explores%20how%20combining%20these%20attacks%20with%20each%20other%20or%20with%20other%0Amalicious%20and%20non-malicious%20strategies%20can%20enhance%20their%20overall%20effectiveness%0Aand%20profitability.%20The%20analysis%20further%20explores%20how%20the%20deployment%20of%20attacks%0Asuch%20as%20selfish%20mining%20and%20block%20withholding%20by%20multiple%20competing%20mining%20pools%0Aagainst%20each%20other%20impacts%20their%20economic%20returns.%20Lastly%2C%20a%20set%20of%20design%0Aguidelines%20is%20provided%2C%20outlining%20areas%20future%20work%20should%20focus%20on%20to%20prevent%0Aor%20mitigate%20the%20identified%20threats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExamining%2520Attacks%2520on%2520Consensus%2520and%2520Incentive%2520Systems%2520in%2520Proof-of-Work%250A%2520%2520Blockchains%253A%2520A%2520Systematic%2520Literature%2520Review%26entry.906535625%3DDinitha%2520Wijewardhana%2520and%2520Sugandima%2520Vidanagamachchi%2520and%2520Nalin%2520Arachchilage%26entry.1292438233%3D%2520%2520Cryptocurrencies%2520have%2520gained%2520popularity%2520due%2520to%2520their%2520transparency%252C%2520security%252C%250Aand%2520accessibility%2520compared%2520to%2520traditional%2520financial%2520systems%252C%2520with%2520Bitcoin%252C%250Aintroduced%2520in%25202009%252C%2520leading%2520the%2520market.%2520Bitcoin%2527s%2520security%2520relies%2520on%2520blockchain%250Atechnology%2520-%2520a%2520decentralized%2520ledger%2520consisting%2520of%2520a%2520consensus%2520and%2520an%2520incentive%250Amechanism.%2520The%2520consensus%2520mechanism%252C%2520Proof%2520of%2520Work%2520%2528PoW%2529%252C%2520requires%2520miners%2520to%250Asolve%2520difficult%2520cryptographic%2520puzzles%2520to%2520add%2520new%2520blocks%252C%2520while%2520the%2520incentive%250Amechanism%2520rewards%2520them%2520with%2520newly%2520minted%2520bitcoins.%2520However%252C%2520as%2520Bitcoin%2527s%250Aacceptance%2520grows%252C%2520it%2520faces%2520increasing%2520threats%2520from%2520attacks%2520targeting%2520these%250Amechanisms%252C%2520such%2520as%2520selfish%2520mining%252C%2520double-spending%252C%2520and%2520block%2520withholding.%250AThese%2520attacks%2520compromise%2520security%252C%2520efficiency%252C%2520and%2520reward%2520distribution.%2520Recent%250Aresearch%2520shows%2520that%2520these%2520attacks%2520can%2520be%2520combined%2520with%2520each%2520other%2520or%2520with%250Aeither%2520malicious%2520strategies%252C%2520such%2520as%2520network-layer%2520attacks%252C%2520or%2520non-malicious%250Astrategies%252C%2520like%2520honest%2520mining.%2520These%2520combinations%2520lead%2520to%2520more%2520sophisticated%250Aattacks%252C%2520increasing%2520the%2520attacker%2527s%2520success%2520rates%2520and%2520profitability.%2520Therefore%252C%250Aunderstanding%2520and%2520evaluating%2520these%2520attacks%2520is%2520essential%2520for%2520developing%250Aeffective%2520countermeasures%2520and%2520ensuring%2520long-term%2520security.%2520This%2520paper%2520begins%2520by%250Aexamining%2520individual%2520attacks%2520executed%2520in%2520isolation%2520and%2520their%2520profitability.%2520It%250Athen%2520explores%2520how%2520combining%2520these%2520attacks%2520with%2520each%2520other%2520or%2520with%2520other%250Amalicious%2520and%2520non-malicious%2520strategies%2520can%2520enhance%2520their%2520overall%2520effectiveness%250Aand%2520profitability.%2520The%2520analysis%2520further%2520explores%2520how%2520the%2520deployment%2520of%2520attacks%250Asuch%2520as%2520selfish%2520mining%2520and%2520block%2520withholding%2520by%2520multiple%2520competing%2520mining%2520pools%250Aagainst%2520each%2520other%2520impacts%2520their%2520economic%2520returns.%2520Lastly%252C%2520a%2520set%2520of%2520design%250Aguidelines%2520is%2520provided%252C%2520outlining%2520areas%2520future%2520work%2520should%2520focus%2520on%2520to%2520prevent%250Aor%2520mitigate%2520the%2520identified%2520threats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Examining%20Attacks%20on%20Consensus%20and%20Incentive%20Systems%20in%20Proof-of-Work%0A%20%20Blockchains%3A%20A%20Systematic%20Literature%20Review&entry.906535625=Dinitha%20Wijewardhana%20and%20Sugandima%20Vidanagamachchi%20and%20Nalin%20Arachchilage&entry.1292438233=%20%20Cryptocurrencies%20have%20gained%20popularity%20due%20to%20their%20transparency%2C%20security%2C%0Aand%20accessibility%20compared%20to%20traditional%20financial%20systems%2C%20with%20Bitcoin%2C%0Aintroduced%20in%202009%2C%20leading%20the%20market.%20Bitcoin%27s%20security%20relies%20on%20blockchain%0Atechnology%20-%20a%20decentralized%20ledger%20consisting%20of%20a%20consensus%20and%20an%20incentive%0Amechanism.%20The%20consensus%20mechanism%2C%20Proof%20of%20Work%20%28PoW%29%2C%20requires%20miners%20to%0Asolve%20difficult%20cryptographic%20puzzles%20to%20add%20new%20blocks%2C%20while%20the%20incentive%0Amechanism%20rewards%20them%20with%20newly%20minted%20bitcoins.%20However%2C%20as%20Bitcoin%27s%0Aacceptance%20grows%2C%20it%20faces%20increasing%20threats%20from%20attacks%20targeting%20these%0Amechanisms%2C%20such%20as%20selfish%20mining%2C%20double-spending%2C%20and%20block%20withholding.%0AThese%20attacks%20compromise%20security%2C%20efficiency%2C%20and%20reward%20distribution.%20Recent%0Aresearch%20shows%20that%20these%20attacks%20can%20be%20combined%20with%20each%20other%20or%20with%0Aeither%20malicious%20strategies%2C%20such%20as%20network-layer%20attacks%2C%20or%20non-malicious%0Astrategies%2C%20like%20honest%20mining.%20These%20combinations%20lead%20to%20more%20sophisticated%0Aattacks%2C%20increasing%20the%20attacker%27s%20success%20rates%20and%20profitability.%20Therefore%2C%0Aunderstanding%20and%20evaluating%20these%20attacks%20is%20essential%20for%20developing%0Aeffective%20countermeasures%20and%20ensuring%20long-term%20security.%20This%20paper%20begins%20by%0Aexamining%20individual%20attacks%20executed%20in%20isolation%20and%20their%20profitability.%20It%0Athen%20explores%20how%20combining%20these%20attacks%20with%20each%20other%20or%20with%20other%0Amalicious%20and%20non-malicious%20strategies%20can%20enhance%20their%20overall%20effectiveness%0Aand%20profitability.%20The%20analysis%20further%20explores%20how%20the%20deployment%20of%20attacks%0Asuch%20as%20selfish%20mining%20and%20block%20withholding%20by%20multiple%20competing%20mining%20pools%0Aagainst%20each%20other%20impacts%20their%20economic%20returns.%20Lastly%2C%20a%20set%20of%20design%0Aguidelines%20is%20provided%2C%20outlining%20areas%20future%20work%20should%20focus%20on%20to%20prevent%0Aor%20mitigate%20the%20identified%20threats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00349v2&entry.124074799=Read"},
{"title": "Enhancing Phishing Detection through Feature Importance Analysis and\n  Explainable AI: A Comparative Study of CatBoost, XGBoost, and EBM Models", "author": "Abdullah Fajar and Setiadi Yazid and Indra Budi", "abstract": "  Phishing attacks remain a persistent threat to online security, demanding\nrobust detection methods. This study investigates the use of machine learning\nto identify phishing URLs, emphasizing the crucial role of feature selection\nand model interpretability for improved performance. Employing Recursive\nFeature Elimination, the research pinpointed key features like \"length_url,\"\n\"time_domain_activation\" and \"Page_rank\" as strong indicators of phishing\nattempts. The study evaluated various algorithms, including CatBoost, XGBoost,\nand Explainable Boosting Machine, assessing their robustness and scalability.\nXGBoost emerged as highly efficient in terms of runtime, making it well-suited\nfor large datasets. CatBoost, on the other hand, demonstrated resilience by\nmaintaining high accuracy even with reduced features. To enhance transparency\nand trustworthiness, Explainable AI techniques, such as SHAP, were employed to\nprovide insights into feature importance. The study's findings highlight that\neffective feature selection and model interpretability can significantly\nbolster phishing detection systems, paving the way for more efficient and\nadaptable defenses against evolving cyber threats\n", "link": "http://arxiv.org/abs/2411.06860v1", "date": "2024-11-11", "relevancy": 1.8007, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4553}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4505}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Phishing%20Detection%20through%20Feature%20Importance%20Analysis%20and%0A%20%20Explainable%20AI%3A%20A%20Comparative%20Study%20of%20CatBoost%2C%20XGBoost%2C%20and%20EBM%20Models&body=Title%3A%20Enhancing%20Phishing%20Detection%20through%20Feature%20Importance%20Analysis%20and%0A%20%20Explainable%20AI%3A%20A%20Comparative%20Study%20of%20CatBoost%2C%20XGBoost%2C%20and%20EBM%20Models%0AAuthor%3A%20Abdullah%20Fajar%20and%20Setiadi%20Yazid%20and%20Indra%20Budi%0AAbstract%3A%20%20%20Phishing%20attacks%20remain%20a%20persistent%20threat%20to%20online%20security%2C%20demanding%0Arobust%20detection%20methods.%20This%20study%20investigates%20the%20use%20of%20machine%20learning%0Ato%20identify%20phishing%20URLs%2C%20emphasizing%20the%20crucial%20role%20of%20feature%20selection%0Aand%20model%20interpretability%20for%20improved%20performance.%20Employing%20Recursive%0AFeature%20Elimination%2C%20the%20research%20pinpointed%20key%20features%20like%20%22length_url%2C%22%0A%22time_domain_activation%22%20and%20%22Page_rank%22%20as%20strong%20indicators%20of%20phishing%0Aattempts.%20The%20study%20evaluated%20various%20algorithms%2C%20including%20CatBoost%2C%20XGBoost%2C%0Aand%20Explainable%20Boosting%20Machine%2C%20assessing%20their%20robustness%20and%20scalability.%0AXGBoost%20emerged%20as%20highly%20efficient%20in%20terms%20of%20runtime%2C%20making%20it%20well-suited%0Afor%20large%20datasets.%20CatBoost%2C%20on%20the%20other%20hand%2C%20demonstrated%20resilience%20by%0Amaintaining%20high%20accuracy%20even%20with%20reduced%20features.%20To%20enhance%20transparency%0Aand%20trustworthiness%2C%20Explainable%20AI%20techniques%2C%20such%20as%20SHAP%2C%20were%20employed%20to%0Aprovide%20insights%20into%20feature%20importance.%20The%20study%27s%20findings%20highlight%20that%0Aeffective%20feature%20selection%20and%20model%20interpretability%20can%20significantly%0Abolster%20phishing%20detection%20systems%2C%20paving%20the%20way%20for%20more%20efficient%20and%0Aadaptable%20defenses%20against%20evolving%20cyber%20threats%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Phishing%2520Detection%2520through%2520Feature%2520Importance%2520Analysis%2520and%250A%2520%2520Explainable%2520AI%253A%2520A%2520Comparative%2520Study%2520of%2520CatBoost%252C%2520XGBoost%252C%2520and%2520EBM%2520Models%26entry.906535625%3DAbdullah%2520Fajar%2520and%2520Setiadi%2520Yazid%2520and%2520Indra%2520Budi%26entry.1292438233%3D%2520%2520Phishing%2520attacks%2520remain%2520a%2520persistent%2520threat%2520to%2520online%2520security%252C%2520demanding%250Arobust%2520detection%2520methods.%2520This%2520study%2520investigates%2520the%2520use%2520of%2520machine%2520learning%250Ato%2520identify%2520phishing%2520URLs%252C%2520emphasizing%2520the%2520crucial%2520role%2520of%2520feature%2520selection%250Aand%2520model%2520interpretability%2520for%2520improved%2520performance.%2520Employing%2520Recursive%250AFeature%2520Elimination%252C%2520the%2520research%2520pinpointed%2520key%2520features%2520like%2520%2522length_url%252C%2522%250A%2522time_domain_activation%2522%2520and%2520%2522Page_rank%2522%2520as%2520strong%2520indicators%2520of%2520phishing%250Aattempts.%2520The%2520study%2520evaluated%2520various%2520algorithms%252C%2520including%2520CatBoost%252C%2520XGBoost%252C%250Aand%2520Explainable%2520Boosting%2520Machine%252C%2520assessing%2520their%2520robustness%2520and%2520scalability.%250AXGBoost%2520emerged%2520as%2520highly%2520efficient%2520in%2520terms%2520of%2520runtime%252C%2520making%2520it%2520well-suited%250Afor%2520large%2520datasets.%2520CatBoost%252C%2520on%2520the%2520other%2520hand%252C%2520demonstrated%2520resilience%2520by%250Amaintaining%2520high%2520accuracy%2520even%2520with%2520reduced%2520features.%2520To%2520enhance%2520transparency%250Aand%2520trustworthiness%252C%2520Explainable%2520AI%2520techniques%252C%2520such%2520as%2520SHAP%252C%2520were%2520employed%2520to%250Aprovide%2520insights%2520into%2520feature%2520importance.%2520The%2520study%2527s%2520findings%2520highlight%2520that%250Aeffective%2520feature%2520selection%2520and%2520model%2520interpretability%2520can%2520significantly%250Abolster%2520phishing%2520detection%2520systems%252C%2520paving%2520the%2520way%2520for%2520more%2520efficient%2520and%250Aadaptable%2520defenses%2520against%2520evolving%2520cyber%2520threats%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Phishing%20Detection%20through%20Feature%20Importance%20Analysis%20and%0A%20%20Explainable%20AI%3A%20A%20Comparative%20Study%20of%20CatBoost%2C%20XGBoost%2C%20and%20EBM%20Models&entry.906535625=Abdullah%20Fajar%20and%20Setiadi%20Yazid%20and%20Indra%20Budi&entry.1292438233=%20%20Phishing%20attacks%20remain%20a%20persistent%20threat%20to%20online%20security%2C%20demanding%0Arobust%20detection%20methods.%20This%20study%20investigates%20the%20use%20of%20machine%20learning%0Ato%20identify%20phishing%20URLs%2C%20emphasizing%20the%20crucial%20role%20of%20feature%20selection%0Aand%20model%20interpretability%20for%20improved%20performance.%20Employing%20Recursive%0AFeature%20Elimination%2C%20the%20research%20pinpointed%20key%20features%20like%20%22length_url%2C%22%0A%22time_domain_activation%22%20and%20%22Page_rank%22%20as%20strong%20indicators%20of%20phishing%0Aattempts.%20The%20study%20evaluated%20various%20algorithms%2C%20including%20CatBoost%2C%20XGBoost%2C%0Aand%20Explainable%20Boosting%20Machine%2C%20assessing%20their%20robustness%20and%20scalability.%0AXGBoost%20emerged%20as%20highly%20efficient%20in%20terms%20of%20runtime%2C%20making%20it%20well-suited%0Afor%20large%20datasets.%20CatBoost%2C%20on%20the%20other%20hand%2C%20demonstrated%20resilience%20by%0Amaintaining%20high%20accuracy%20even%20with%20reduced%20features.%20To%20enhance%20transparency%0Aand%20trustworthiness%2C%20Explainable%20AI%20techniques%2C%20such%20as%20SHAP%2C%20were%20employed%20to%0Aprovide%20insights%20into%20feature%20importance.%20The%20study%27s%20findings%20highlight%20that%0Aeffective%20feature%20selection%20and%20model%20interpretability%20can%20significantly%0Abolster%20phishing%20detection%20systems%2C%20paving%20the%20way%20for%20more%20efficient%20and%0Aadaptable%20defenses%20against%20evolving%20cyber%20threats%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06860v1&entry.124074799=Read"},
{"title": "Adaptive and Optimal Second-order Optimistic Methods for Minimax\n  Optimization", "author": "Ruichen Jiang and Ali Kavis and Qiujiang Jin and Sujay Sanghavi and Aryan Mokhtari", "abstract": "  We propose adaptive, line search-free second-order methods with optimal rate\nof convergence for solving convex-concave min-max problems. By means of an\nadaptive step size, our algorithms feature a simple update rule that requires\nsolving only one linear system per iteration, eliminating the need for line\nsearch or backtracking mechanisms. Specifically, we base our algorithms on the\noptimistic method and appropriately combine it with second-order information.\nMoreover, distinct from common adaptive schemes, we define the step size\nrecursively as a function of the gradient norm and the prediction error in the\noptimistic update. We first analyze a variant where the step size requires\nknowledge of the Lipschitz constant of the Hessian. Under the additional\nassumption of Lipschitz continuous gradients, we further design a\nparameter-free version by tracking the Hessian Lipschitz constant locally and\nensuring the iterates remain bounded. We also evaluate the practical\nperformance of our algorithm by comparing it to existing second-order\nalgorithms for minimax optimization.\n", "link": "http://arxiv.org/abs/2406.02016v2", "date": "2024-11-11", "relevancy": 1.6854, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4381}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4267}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20and%20Optimal%20Second-order%20Optimistic%20Methods%20for%20Minimax%0A%20%20Optimization&body=Title%3A%20Adaptive%20and%20Optimal%20Second-order%20Optimistic%20Methods%20for%20Minimax%0A%20%20Optimization%0AAuthor%3A%20Ruichen%20Jiang%20and%20Ali%20Kavis%20and%20Qiujiang%20Jin%20and%20Sujay%20Sanghavi%20and%20Aryan%20Mokhtari%0AAbstract%3A%20%20%20We%20propose%20adaptive%2C%20line%20search-free%20second-order%20methods%20with%20optimal%20rate%0Aof%20convergence%20for%20solving%20convex-concave%20min-max%20problems.%20By%20means%20of%20an%0Aadaptive%20step%20size%2C%20our%20algorithms%20feature%20a%20simple%20update%20rule%20that%20requires%0Asolving%20only%20one%20linear%20system%20per%20iteration%2C%20eliminating%20the%20need%20for%20line%0Asearch%20or%20backtracking%20mechanisms.%20Specifically%2C%20we%20base%20our%20algorithms%20on%20the%0Aoptimistic%20method%20and%20appropriately%20combine%20it%20with%20second-order%20information.%0AMoreover%2C%20distinct%20from%20common%20adaptive%20schemes%2C%20we%20define%20the%20step%20size%0Arecursively%20as%20a%20function%20of%20the%20gradient%20norm%20and%20the%20prediction%20error%20in%20the%0Aoptimistic%20update.%20We%20first%20analyze%20a%20variant%20where%20the%20step%20size%20requires%0Aknowledge%20of%20the%20Lipschitz%20constant%20of%20the%20Hessian.%20Under%20the%20additional%0Aassumption%20of%20Lipschitz%20continuous%20gradients%2C%20we%20further%20design%20a%0Aparameter-free%20version%20by%20tracking%20the%20Hessian%20Lipschitz%20constant%20locally%20and%0Aensuring%20the%20iterates%20remain%20bounded.%20We%20also%20evaluate%20the%20practical%0Aperformance%20of%20our%20algorithm%20by%20comparing%20it%20to%20existing%20second-order%0Aalgorithms%20for%20minimax%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02016v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520and%2520Optimal%2520Second-order%2520Optimistic%2520Methods%2520for%2520Minimax%250A%2520%2520Optimization%26entry.906535625%3DRuichen%2520Jiang%2520and%2520Ali%2520Kavis%2520and%2520Qiujiang%2520Jin%2520and%2520Sujay%2520Sanghavi%2520and%2520Aryan%2520Mokhtari%26entry.1292438233%3D%2520%2520We%2520propose%2520adaptive%252C%2520line%2520search-free%2520second-order%2520methods%2520with%2520optimal%2520rate%250Aof%2520convergence%2520for%2520solving%2520convex-concave%2520min-max%2520problems.%2520By%2520means%2520of%2520an%250Aadaptive%2520step%2520size%252C%2520our%2520algorithms%2520feature%2520a%2520simple%2520update%2520rule%2520that%2520requires%250Asolving%2520only%2520one%2520linear%2520system%2520per%2520iteration%252C%2520eliminating%2520the%2520need%2520for%2520line%250Asearch%2520or%2520backtracking%2520mechanisms.%2520Specifically%252C%2520we%2520base%2520our%2520algorithms%2520on%2520the%250Aoptimistic%2520method%2520and%2520appropriately%2520combine%2520it%2520with%2520second-order%2520information.%250AMoreover%252C%2520distinct%2520from%2520common%2520adaptive%2520schemes%252C%2520we%2520define%2520the%2520step%2520size%250Arecursively%2520as%2520a%2520function%2520of%2520the%2520gradient%2520norm%2520and%2520the%2520prediction%2520error%2520in%2520the%250Aoptimistic%2520update.%2520We%2520first%2520analyze%2520a%2520variant%2520where%2520the%2520step%2520size%2520requires%250Aknowledge%2520of%2520the%2520Lipschitz%2520constant%2520of%2520the%2520Hessian.%2520Under%2520the%2520additional%250Aassumption%2520of%2520Lipschitz%2520continuous%2520gradients%252C%2520we%2520further%2520design%2520a%250Aparameter-free%2520version%2520by%2520tracking%2520the%2520Hessian%2520Lipschitz%2520constant%2520locally%2520and%250Aensuring%2520the%2520iterates%2520remain%2520bounded.%2520We%2520also%2520evaluate%2520the%2520practical%250Aperformance%2520of%2520our%2520algorithm%2520by%2520comparing%2520it%2520to%2520existing%2520second-order%250Aalgorithms%2520for%2520minimax%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02016v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20and%20Optimal%20Second-order%20Optimistic%20Methods%20for%20Minimax%0A%20%20Optimization&entry.906535625=Ruichen%20Jiang%20and%20Ali%20Kavis%20and%20Qiujiang%20Jin%20and%20Sujay%20Sanghavi%20and%20Aryan%20Mokhtari&entry.1292438233=%20%20We%20propose%20adaptive%2C%20line%20search-free%20second-order%20methods%20with%20optimal%20rate%0Aof%20convergence%20for%20solving%20convex-concave%20min-max%20problems.%20By%20means%20of%20an%0Aadaptive%20step%20size%2C%20our%20algorithms%20feature%20a%20simple%20update%20rule%20that%20requires%0Asolving%20only%20one%20linear%20system%20per%20iteration%2C%20eliminating%20the%20need%20for%20line%0Asearch%20or%20backtracking%20mechanisms.%20Specifically%2C%20we%20base%20our%20algorithms%20on%20the%0Aoptimistic%20method%20and%20appropriately%20combine%20it%20with%20second-order%20information.%0AMoreover%2C%20distinct%20from%20common%20adaptive%20schemes%2C%20we%20define%20the%20step%20size%0Arecursively%20as%20a%20function%20of%20the%20gradient%20norm%20and%20the%20prediction%20error%20in%20the%0Aoptimistic%20update.%20We%20first%20analyze%20a%20variant%20where%20the%20step%20size%20requires%0Aknowledge%20of%20the%20Lipschitz%20constant%20of%20the%20Hessian.%20Under%20the%20additional%0Aassumption%20of%20Lipschitz%20continuous%20gradients%2C%20we%20further%20design%20a%0Aparameter-free%20version%20by%20tracking%20the%20Hessian%20Lipschitz%20constant%20locally%20and%0Aensuring%20the%20iterates%20remain%20bounded.%20We%20also%20evaluate%20the%20practical%0Aperformance%20of%20our%20algorithm%20by%20comparing%20it%20to%20existing%20second-order%0Aalgorithms%20for%20minimax%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02016v2&entry.124074799=Read"},
{"title": "AI-Native Multi-Access Future Networks -- The REASON Architecture", "author": "Konstantinos Katsaros and Ioannis Mavromatis and Kostantinos Antonakoglou and Saptarshi Ghosh and Dritan Kaleshi and Toktam Mahmoodi and Hamid Asgari and Anastasios Karousos and Iman Tavakkolnia and Hossein Safi and Harald Hass and Constantinos Vrontos and Amin Emami and Juan Parra Ullauri and Shadi Moazzeni and Dimitra Simeonidou", "abstract": "  The development of the sixth generation of communication networks (6G) has\nbeen gaining momentum over the past years, with a target of being introduced by\n2030. Several initiatives worldwide are developing innovative solutions and\nsetting the direction for the key features of these networks. Some common\nemerging themes are the tight integration of AI, the convergence of multiple\naccess technologies and sustainable operation, aiming to meet stringent\nperformance and societal requirements. To that end, we are introducing REASON -\nRealising Enabling Architectures and Solutions for Open Networks. The REASON\nproject aims to address technical challenges in future network deployments,\nsuch as E2E service orchestration, sustainability, security and trust\nmanagement, and policy management, utilising AI-native principles, considering\nmultiple access technologies and cloud-native solutions.\n  This paper presents REASON's architecture and the identified requirements for\nfuture networks. The architecture is meticulously designed for modularity,\ninteroperability, scalability, simplified troubleshooting, flexibility, and\nenhanced security, taking into consideration current and future standardisation\nefforts, and the ease of implementation and training. It is structured into\nfour horizontal layers: Physical Infrastructure, Network Service, Knowledge,\nand End-User Application, complemented by two vertical layers: Management and\nOrchestration, and E2E Security. This layered approach ensures a robust,\nadaptable framework to support the diverse and evolving requirements of 6G\nnetworks, fostering innovation and facilitating seamless integration of\nadvanced technologies.\n", "link": "http://arxiv.org/abs/2411.06870v1", "date": "2024-11-11", "relevancy": 1.175, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3996}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Native%20Multi-Access%20Future%20Networks%20--%20The%20REASON%20Architecture&body=Title%3A%20AI-Native%20Multi-Access%20Future%20Networks%20--%20The%20REASON%20Architecture%0AAuthor%3A%20Konstantinos%20Katsaros%20and%20Ioannis%20Mavromatis%20and%20Kostantinos%20Antonakoglou%20and%20Saptarshi%20Ghosh%20and%20Dritan%20Kaleshi%20and%20Toktam%20Mahmoodi%20and%20Hamid%20Asgari%20and%20Anastasios%20Karousos%20and%20Iman%20Tavakkolnia%20and%20Hossein%20Safi%20and%20Harald%20Hass%20and%20Constantinos%20Vrontos%20and%20Amin%20Emami%20and%20Juan%20Parra%20Ullauri%20and%20Shadi%20Moazzeni%20and%20Dimitra%20Simeonidou%0AAbstract%3A%20%20%20The%20development%20of%20the%20sixth%20generation%20of%20communication%20networks%20%286G%29%20has%0Abeen%20gaining%20momentum%20over%20the%20past%20years%2C%20with%20a%20target%20of%20being%20introduced%20by%0A2030.%20Several%20initiatives%20worldwide%20are%20developing%20innovative%20solutions%20and%0Asetting%20the%20direction%20for%20the%20key%20features%20of%20these%20networks.%20Some%20common%0Aemerging%20themes%20are%20the%20tight%20integration%20of%20AI%2C%20the%20convergence%20of%20multiple%0Aaccess%20technologies%20and%20sustainable%20operation%2C%20aiming%20to%20meet%20stringent%0Aperformance%20and%20societal%20requirements.%20To%20that%20end%2C%20we%20are%20introducing%20REASON%20-%0ARealising%20Enabling%20Architectures%20and%20Solutions%20for%20Open%20Networks.%20The%20REASON%0Aproject%20aims%20to%20address%20technical%20challenges%20in%20future%20network%20deployments%2C%0Asuch%20as%20E2E%20service%20orchestration%2C%20sustainability%2C%20security%20and%20trust%0Amanagement%2C%20and%20policy%20management%2C%20utilising%20AI-native%20principles%2C%20considering%0Amultiple%20access%20technologies%20and%20cloud-native%20solutions.%0A%20%20This%20paper%20presents%20REASON%27s%20architecture%20and%20the%20identified%20requirements%20for%0Afuture%20networks.%20The%20architecture%20is%20meticulously%20designed%20for%20modularity%2C%0Ainteroperability%2C%20scalability%2C%20simplified%20troubleshooting%2C%20flexibility%2C%20and%0Aenhanced%20security%2C%20taking%20into%20consideration%20current%20and%20future%20standardisation%0Aefforts%2C%20and%20the%20ease%20of%20implementation%20and%20training.%20It%20is%20structured%20into%0Afour%20horizontal%20layers%3A%20Physical%20Infrastructure%2C%20Network%20Service%2C%20Knowledge%2C%0Aand%20End-User%20Application%2C%20complemented%20by%20two%20vertical%20layers%3A%20Management%20and%0AOrchestration%2C%20and%20E2E%20Security.%20This%20layered%20approach%20ensures%20a%20robust%2C%0Aadaptable%20framework%20to%20support%20the%20diverse%20and%20evolving%20requirements%20of%206G%0Anetworks%2C%20fostering%20innovation%20and%20facilitating%20seamless%20integration%20of%0Aadvanced%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Native%2520Multi-Access%2520Future%2520Networks%2520--%2520The%2520REASON%2520Architecture%26entry.906535625%3DKonstantinos%2520Katsaros%2520and%2520Ioannis%2520Mavromatis%2520and%2520Kostantinos%2520Antonakoglou%2520and%2520Saptarshi%2520Ghosh%2520and%2520Dritan%2520Kaleshi%2520and%2520Toktam%2520Mahmoodi%2520and%2520Hamid%2520Asgari%2520and%2520Anastasios%2520Karousos%2520and%2520Iman%2520Tavakkolnia%2520and%2520Hossein%2520Safi%2520and%2520Harald%2520Hass%2520and%2520Constantinos%2520Vrontos%2520and%2520Amin%2520Emami%2520and%2520Juan%2520Parra%2520Ullauri%2520and%2520Shadi%2520Moazzeni%2520and%2520Dimitra%2520Simeonidou%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520the%2520sixth%2520generation%2520of%2520communication%2520networks%2520%25286G%2529%2520has%250Abeen%2520gaining%2520momentum%2520over%2520the%2520past%2520years%252C%2520with%2520a%2520target%2520of%2520being%2520introduced%2520by%250A2030.%2520Several%2520initiatives%2520worldwide%2520are%2520developing%2520innovative%2520solutions%2520and%250Asetting%2520the%2520direction%2520for%2520the%2520key%2520features%2520of%2520these%2520networks.%2520Some%2520common%250Aemerging%2520themes%2520are%2520the%2520tight%2520integration%2520of%2520AI%252C%2520the%2520convergence%2520of%2520multiple%250Aaccess%2520technologies%2520and%2520sustainable%2520operation%252C%2520aiming%2520to%2520meet%2520stringent%250Aperformance%2520and%2520societal%2520requirements.%2520To%2520that%2520end%252C%2520we%2520are%2520introducing%2520REASON%2520-%250ARealising%2520Enabling%2520Architectures%2520and%2520Solutions%2520for%2520Open%2520Networks.%2520The%2520REASON%250Aproject%2520aims%2520to%2520address%2520technical%2520challenges%2520in%2520future%2520network%2520deployments%252C%250Asuch%2520as%2520E2E%2520service%2520orchestration%252C%2520sustainability%252C%2520security%2520and%2520trust%250Amanagement%252C%2520and%2520policy%2520management%252C%2520utilising%2520AI-native%2520principles%252C%2520considering%250Amultiple%2520access%2520technologies%2520and%2520cloud-native%2520solutions.%250A%2520%2520This%2520paper%2520presents%2520REASON%2527s%2520architecture%2520and%2520the%2520identified%2520requirements%2520for%250Afuture%2520networks.%2520The%2520architecture%2520is%2520meticulously%2520designed%2520for%2520modularity%252C%250Ainteroperability%252C%2520scalability%252C%2520simplified%2520troubleshooting%252C%2520flexibility%252C%2520and%250Aenhanced%2520security%252C%2520taking%2520into%2520consideration%2520current%2520and%2520future%2520standardisation%250Aefforts%252C%2520and%2520the%2520ease%2520of%2520implementation%2520and%2520training.%2520It%2520is%2520structured%2520into%250Afour%2520horizontal%2520layers%253A%2520Physical%2520Infrastructure%252C%2520Network%2520Service%252C%2520Knowledge%252C%250Aand%2520End-User%2520Application%252C%2520complemented%2520by%2520two%2520vertical%2520layers%253A%2520Management%2520and%250AOrchestration%252C%2520and%2520E2E%2520Security.%2520This%2520layered%2520approach%2520ensures%2520a%2520robust%252C%250Aadaptable%2520framework%2520to%2520support%2520the%2520diverse%2520and%2520evolving%2520requirements%2520of%25206G%250Anetworks%252C%2520fostering%2520innovation%2520and%2520facilitating%2520seamless%2520integration%2520of%250Aadvanced%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Native%20Multi-Access%20Future%20Networks%20--%20The%20REASON%20Architecture&entry.906535625=Konstantinos%20Katsaros%20and%20Ioannis%20Mavromatis%20and%20Kostantinos%20Antonakoglou%20and%20Saptarshi%20Ghosh%20and%20Dritan%20Kaleshi%20and%20Toktam%20Mahmoodi%20and%20Hamid%20Asgari%20and%20Anastasios%20Karousos%20and%20Iman%20Tavakkolnia%20and%20Hossein%20Safi%20and%20Harald%20Hass%20and%20Constantinos%20Vrontos%20and%20Amin%20Emami%20and%20Juan%20Parra%20Ullauri%20and%20Shadi%20Moazzeni%20and%20Dimitra%20Simeonidou&entry.1292438233=%20%20The%20development%20of%20the%20sixth%20generation%20of%20communication%20networks%20%286G%29%20has%0Abeen%20gaining%20momentum%20over%20the%20past%20years%2C%20with%20a%20target%20of%20being%20introduced%20by%0A2030.%20Several%20initiatives%20worldwide%20are%20developing%20innovative%20solutions%20and%0Asetting%20the%20direction%20for%20the%20key%20features%20of%20these%20networks.%20Some%20common%0Aemerging%20themes%20are%20the%20tight%20integration%20of%20AI%2C%20the%20convergence%20of%20multiple%0Aaccess%20technologies%20and%20sustainable%20operation%2C%20aiming%20to%20meet%20stringent%0Aperformance%20and%20societal%20requirements.%20To%20that%20end%2C%20we%20are%20introducing%20REASON%20-%0ARealising%20Enabling%20Architectures%20and%20Solutions%20for%20Open%20Networks.%20The%20REASON%0Aproject%20aims%20to%20address%20technical%20challenges%20in%20future%20network%20deployments%2C%0Asuch%20as%20E2E%20service%20orchestration%2C%20sustainability%2C%20security%20and%20trust%0Amanagement%2C%20and%20policy%20management%2C%20utilising%20AI-native%20principles%2C%20considering%0Amultiple%20access%20technologies%20and%20cloud-native%20solutions.%0A%20%20This%20paper%20presents%20REASON%27s%20architecture%20and%20the%20identified%20requirements%20for%0Afuture%20networks.%20The%20architecture%20is%20meticulously%20designed%20for%20modularity%2C%0Ainteroperability%2C%20scalability%2C%20simplified%20troubleshooting%2C%20flexibility%2C%20and%0Aenhanced%20security%2C%20taking%20into%20consideration%20current%20and%20future%20standardisation%0Aefforts%2C%20and%20the%20ease%20of%20implementation%20and%20training.%20It%20is%20structured%20into%0Afour%20horizontal%20layers%3A%20Physical%20Infrastructure%2C%20Network%20Service%2C%20Knowledge%2C%0Aand%20End-User%20Application%2C%20complemented%20by%20two%20vertical%20layers%3A%20Management%20and%0AOrchestration%2C%20and%20E2E%20Security.%20This%20layered%20approach%20ensures%20a%20robust%2C%0Aadaptable%20framework%20to%20support%20the%20diverse%20and%20evolving%20requirements%20of%206G%0Anetworks%2C%20fostering%20innovation%20and%20facilitating%20seamless%20integration%20of%0Aadvanced%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06870v1&entry.124074799=Read"},
{"title": "On Active Privacy Auditing in Supervised Fine-tuning for White-Box\n  Language Models", "author": "Qian Sun and Hanpeng Wu and Xi Sheryl Zhang", "abstract": "  The pretraining and fine-tuning approach has become the leading technique for\nvarious NLP applications. However, recent studies reveal that fine-tuning data,\ndue to their sensitive nature, domain-specific characteristics, and\nidentifiability, pose significant privacy concerns. To help develop more\nprivacy-resilient fine-tuning models, we introduce a novel active privacy\nauditing framework, dubbed Parsing, designed to identify and quantify privacy\nleakage risks during the supervised fine-tuning (SFT) of language models (LMs).\nThe framework leverages improved white-box membership inference attacks (MIAs)\nas the core technology, utilizing novel learning objectives and a two-stage\npipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the\nexposure of privacy risks. Additionally, we have improved the effectiveness of\nMIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our\nresearch aims to provide the SFT community of LMs with a reliable, ready-to-use\nprivacy auditing tool, and to offer valuable insights into safeguarding privacy\nduring the fine-tuning process. Experimental results confirm the framework's\nefficiency across various models and tasks, emphasizing notable privacy\nconcerns in the fine-tuning process. Project code available for\nhttps://github.com/mapleleavesss/PARSING.\n", "link": "http://arxiv.org/abs/2411.07070v1", "date": "2024-11-11", "relevancy": 1.9464, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4962}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4813}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Active%20Privacy%20Auditing%20in%20Supervised%20Fine-tuning%20for%20White-Box%0A%20%20Language%20Models&body=Title%3A%20On%20Active%20Privacy%20Auditing%20in%20Supervised%20Fine-tuning%20for%20White-Box%0A%20%20Language%20Models%0AAuthor%3A%20Qian%20Sun%20and%20Hanpeng%20Wu%20and%20Xi%20Sheryl%20Zhang%0AAbstract%3A%20%20%20The%20pretraining%20and%20fine-tuning%20approach%20has%20become%20the%20leading%20technique%20for%0Avarious%20NLP%20applications.%20However%2C%20recent%20studies%20reveal%20that%20fine-tuning%20data%2C%0Adue%20to%20their%20sensitive%20nature%2C%20domain-specific%20characteristics%2C%20and%0Aidentifiability%2C%20pose%20significant%20privacy%20concerns.%20To%20help%20develop%20more%0Aprivacy-resilient%20fine-tuning%20models%2C%20we%20introduce%20a%20novel%20active%20privacy%0Aauditing%20framework%2C%20dubbed%20Parsing%2C%20designed%20to%20identify%20and%20quantify%20privacy%0Aleakage%20risks%20during%20the%20supervised%20fine-tuning%20%28SFT%29%20of%20language%20models%20%28LMs%29.%0AThe%20framework%20leverages%20improved%20white-box%20membership%20inference%20attacks%20%28MIAs%29%0Aas%20the%20core%20technology%2C%20utilizing%20novel%20learning%20objectives%20and%20a%20two-stage%0Apipeline%20to%20monitor%20the%20privacy%20of%20the%20LMs%27%20fine-tuning%20process%2C%20maximizing%20the%0Aexposure%20of%20privacy%20risks.%20Additionally%2C%20we%20have%20improved%20the%20effectiveness%20of%0AMIAs%20on%20large%20LMs%20including%20GPT-2%2C%20Llama2%2C%20and%20certain%20variants%20of%20them.%20Our%0Aresearch%20aims%20to%20provide%20the%20SFT%20community%20of%20LMs%20with%20a%20reliable%2C%20ready-to-use%0Aprivacy%20auditing%20tool%2C%20and%20to%20offer%20valuable%20insights%20into%20safeguarding%20privacy%0Aduring%20the%20fine-tuning%20process.%20Experimental%20results%20confirm%20the%20framework%27s%0Aefficiency%20across%20various%20models%20and%20tasks%2C%20emphasizing%20notable%20privacy%0Aconcerns%20in%20the%20fine-tuning%20process.%20Project%20code%20available%20for%0Ahttps%3A//github.com/mapleleavesss/PARSING.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Active%2520Privacy%2520Auditing%2520in%2520Supervised%2520Fine-tuning%2520for%2520White-Box%250A%2520%2520Language%2520Models%26entry.906535625%3DQian%2520Sun%2520and%2520Hanpeng%2520Wu%2520and%2520Xi%2520Sheryl%2520Zhang%26entry.1292438233%3D%2520%2520The%2520pretraining%2520and%2520fine-tuning%2520approach%2520has%2520become%2520the%2520leading%2520technique%2520for%250Avarious%2520NLP%2520applications.%2520However%252C%2520recent%2520studies%2520reveal%2520that%2520fine-tuning%2520data%252C%250Adue%2520to%2520their%2520sensitive%2520nature%252C%2520domain-specific%2520characteristics%252C%2520and%250Aidentifiability%252C%2520pose%2520significant%2520privacy%2520concerns.%2520To%2520help%2520develop%2520more%250Aprivacy-resilient%2520fine-tuning%2520models%252C%2520we%2520introduce%2520a%2520novel%2520active%2520privacy%250Aauditing%2520framework%252C%2520dubbed%2520Parsing%252C%2520designed%2520to%2520identify%2520and%2520quantify%2520privacy%250Aleakage%2520risks%2520during%2520the%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520of%2520language%2520models%2520%2528LMs%2529.%250AThe%2520framework%2520leverages%2520improved%2520white-box%2520membership%2520inference%2520attacks%2520%2528MIAs%2529%250Aas%2520the%2520core%2520technology%252C%2520utilizing%2520novel%2520learning%2520objectives%2520and%2520a%2520two-stage%250Apipeline%2520to%2520monitor%2520the%2520privacy%2520of%2520the%2520LMs%2527%2520fine-tuning%2520process%252C%2520maximizing%2520the%250Aexposure%2520of%2520privacy%2520risks.%2520Additionally%252C%2520we%2520have%2520improved%2520the%2520effectiveness%2520of%250AMIAs%2520on%2520large%2520LMs%2520including%2520GPT-2%252C%2520Llama2%252C%2520and%2520certain%2520variants%2520of%2520them.%2520Our%250Aresearch%2520aims%2520to%2520provide%2520the%2520SFT%2520community%2520of%2520LMs%2520with%2520a%2520reliable%252C%2520ready-to-use%250Aprivacy%2520auditing%2520tool%252C%2520and%2520to%2520offer%2520valuable%2520insights%2520into%2520safeguarding%2520privacy%250Aduring%2520the%2520fine-tuning%2520process.%2520Experimental%2520results%2520confirm%2520the%2520framework%2527s%250Aefficiency%2520across%2520various%2520models%2520and%2520tasks%252C%2520emphasizing%2520notable%2520privacy%250Aconcerns%2520in%2520the%2520fine-tuning%2520process.%2520Project%2520code%2520available%2520for%250Ahttps%253A//github.com/mapleleavesss/PARSING.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Active%20Privacy%20Auditing%20in%20Supervised%20Fine-tuning%20for%20White-Box%0A%20%20Language%20Models&entry.906535625=Qian%20Sun%20and%20Hanpeng%20Wu%20and%20Xi%20Sheryl%20Zhang&entry.1292438233=%20%20The%20pretraining%20and%20fine-tuning%20approach%20has%20become%20the%20leading%20technique%20for%0Avarious%20NLP%20applications.%20However%2C%20recent%20studies%20reveal%20that%20fine-tuning%20data%2C%0Adue%20to%20their%20sensitive%20nature%2C%20domain-specific%20characteristics%2C%20and%0Aidentifiability%2C%20pose%20significant%20privacy%20concerns.%20To%20help%20develop%20more%0Aprivacy-resilient%20fine-tuning%20models%2C%20we%20introduce%20a%20novel%20active%20privacy%0Aauditing%20framework%2C%20dubbed%20Parsing%2C%20designed%20to%20identify%20and%20quantify%20privacy%0Aleakage%20risks%20during%20the%20supervised%20fine-tuning%20%28SFT%29%20of%20language%20models%20%28LMs%29.%0AThe%20framework%20leverages%20improved%20white-box%20membership%20inference%20attacks%20%28MIAs%29%0Aas%20the%20core%20technology%2C%20utilizing%20novel%20learning%20objectives%20and%20a%20two-stage%0Apipeline%20to%20monitor%20the%20privacy%20of%20the%20LMs%27%20fine-tuning%20process%2C%20maximizing%20the%0Aexposure%20of%20privacy%20risks.%20Additionally%2C%20we%20have%20improved%20the%20effectiveness%20of%0AMIAs%20on%20large%20LMs%20including%20GPT-2%2C%20Llama2%2C%20and%20certain%20variants%20of%20them.%20Our%0Aresearch%20aims%20to%20provide%20the%20SFT%20community%20of%20LMs%20with%20a%20reliable%2C%20ready-to-use%0Aprivacy%20auditing%20tool%2C%20and%20to%20offer%20valuable%20insights%20into%20safeguarding%20privacy%0Aduring%20the%20fine-tuning%20process.%20Experimental%20results%20confirm%20the%20framework%27s%0Aefficiency%20across%20various%20models%20and%20tasks%2C%20emphasizing%20notable%20privacy%0Aconcerns%20in%20the%20fine-tuning%20process.%20Project%20code%20available%20for%0Ahttps%3A//github.com/mapleleavesss/PARSING.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07070v1&entry.124074799=Read"},
{"title": "A Hyperspectral Imaging Dataset and Methodology for Intraoperative\n  Pixel-Wise Classification of Metastatic Colon Cancer in the Liver", "author": "Ivica Kopriva and Dario Sitnik and Laura-Isabelle Dion-Bertrand and Marija Milkovi\u0107 Peri\u0161a and Mirko Had\u017eija and Marijana Popovi\u0107 Had\u017eija", "abstract": "  Hyperspectral imaging (HSI) holds significant potential for transforming the\nfield of computational pathology. However, there is currently a shortage of\npixel-wise annotated HSI data necessary for training deep learning (DL) models.\nAdditionally, the number of HSI-based research studies remains limited, and in\nmany cases, the advantages of HSI over traditional RGB imaging have not been\nconclusively demonstrated, particularly for specimens collected\nintraoperatively. To address these challenges we present a database consisted\nof 27 HSIs of hematoxylin-eosin stained frozen sections, collected from 14\npatients with colon adenocarcinoma metastasized to the liver. It is aimed to\nvalidate pixel-wise classification for intraoperative tumor resection. The HSIs\nwere acquired in the spectral range of 450 to 800 nm, with a resolution of 1\nnm, resulting in images of 1384x1035 pixels. Pixel-wise annotations were\nperformed by three pathologists. To overcome challenges such as experimental\nvariability and the lack of annotated data, we combined label-propagation-based\nsemi-supervised learning (SSL) with spectral-spatial features extracted by: the\nmultiscale principle of relevant information (MPRI) method and tensor singular\nspectrum analysis method. Using only 1% of labeled pixels per class the\nSSL-MPRI method achieved a micro balanced accuracy (BACC) of 0.9313 and a micro\nF1-score of 0.9235 on the HSI dataset. The performance on corresponding RGB\nimages was lower, with a micro BACC of 0.8809 and a micro F1-score of 0.8688.\nThese improvements are statistically significant. The SSL-MPRI approach\noutperformed six DL architectures trained with 63% of labeled pixels. Data and\ncode are available at: https://github.com/ikopriva/ColonCancerHSI.\n", "link": "http://arxiv.org/abs/2411.06969v1", "date": "2024-11-11", "relevancy": 1.3392, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4497}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4426}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hyperspectral%20Imaging%20Dataset%20and%20Methodology%20for%20Intraoperative%0A%20%20Pixel-Wise%20Classification%20of%20Metastatic%20Colon%20Cancer%20in%20the%20Liver&body=Title%3A%20A%20Hyperspectral%20Imaging%20Dataset%20and%20Methodology%20for%20Intraoperative%0A%20%20Pixel-Wise%20Classification%20of%20Metastatic%20Colon%20Cancer%20in%20the%20Liver%0AAuthor%3A%20Ivica%20Kopriva%20and%20Dario%20Sitnik%20and%20Laura-Isabelle%20Dion-Bertrand%20and%20Marija%20Milkovi%C4%87%20Peri%C5%A1a%20and%20Mirko%20Had%C5%BEija%20and%20Marijana%20Popovi%C4%87%20Had%C5%BEija%0AAbstract%3A%20%20%20Hyperspectral%20imaging%20%28HSI%29%20holds%20significant%20potential%20for%20transforming%20the%0Afield%20of%20computational%20pathology.%20However%2C%20there%20is%20currently%20a%20shortage%20of%0Apixel-wise%20annotated%20HSI%20data%20necessary%20for%20training%20deep%20learning%20%28DL%29%20models.%0AAdditionally%2C%20the%20number%20of%20HSI-based%20research%20studies%20remains%20limited%2C%20and%20in%0Amany%20cases%2C%20the%20advantages%20of%20HSI%20over%20traditional%20RGB%20imaging%20have%20not%20been%0Aconclusively%20demonstrated%2C%20particularly%20for%20specimens%20collected%0Aintraoperatively.%20To%20address%20these%20challenges%20we%20present%20a%20database%20consisted%0Aof%2027%20HSIs%20of%20hematoxylin-eosin%20stained%20frozen%20sections%2C%20collected%20from%2014%0Apatients%20with%20colon%20adenocarcinoma%20metastasized%20to%20the%20liver.%20It%20is%20aimed%20to%0Avalidate%20pixel-wise%20classification%20for%20intraoperative%20tumor%20resection.%20The%20HSIs%0Awere%20acquired%20in%20the%20spectral%20range%20of%20450%20to%20800%20nm%2C%20with%20a%20resolution%20of%201%0Anm%2C%20resulting%20in%20images%20of%201384x1035%20pixels.%20Pixel-wise%20annotations%20were%0Aperformed%20by%20three%20pathologists.%20To%20overcome%20challenges%20such%20as%20experimental%0Avariability%20and%20the%20lack%20of%20annotated%20data%2C%20we%20combined%20label-propagation-based%0Asemi-supervised%20learning%20%28SSL%29%20with%20spectral-spatial%20features%20extracted%20by%3A%20the%0Amultiscale%20principle%20of%20relevant%20information%20%28MPRI%29%20method%20and%20tensor%20singular%0Aspectrum%20analysis%20method.%20Using%20only%201%25%20of%20labeled%20pixels%20per%20class%20the%0ASSL-MPRI%20method%20achieved%20a%20micro%20balanced%20accuracy%20%28BACC%29%20of%200.9313%20and%20a%20micro%0AF1-score%20of%200.9235%20on%20the%20HSI%20dataset.%20The%20performance%20on%20corresponding%20RGB%0Aimages%20was%20lower%2C%20with%20a%20micro%20BACC%20of%200.8809%20and%20a%20micro%20F1-score%20of%200.8688.%0AThese%20improvements%20are%20statistically%20significant.%20The%20SSL-MPRI%20approach%0Aoutperformed%20six%20DL%20architectures%20trained%20with%2063%25%20of%20labeled%20pixels.%20Data%20and%0Acode%20are%20available%20at%3A%20https%3A//github.com/ikopriva/ColonCancerHSI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hyperspectral%2520Imaging%2520Dataset%2520and%2520Methodology%2520for%2520Intraoperative%250A%2520%2520Pixel-Wise%2520Classification%2520of%2520Metastatic%2520Colon%2520Cancer%2520in%2520the%2520Liver%26entry.906535625%3DIvica%2520Kopriva%2520and%2520Dario%2520Sitnik%2520and%2520Laura-Isabelle%2520Dion-Bertrand%2520and%2520Marija%2520Milkovi%25C4%2587%2520Peri%25C5%25A1a%2520and%2520Mirko%2520Had%25C5%25BEija%2520and%2520Marijana%2520Popovi%25C4%2587%2520Had%25C5%25BEija%26entry.1292438233%3D%2520%2520Hyperspectral%2520imaging%2520%2528HSI%2529%2520holds%2520significant%2520potential%2520for%2520transforming%2520the%250Afield%2520of%2520computational%2520pathology.%2520However%252C%2520there%2520is%2520currently%2520a%2520shortage%2520of%250Apixel-wise%2520annotated%2520HSI%2520data%2520necessary%2520for%2520training%2520deep%2520learning%2520%2528DL%2529%2520models.%250AAdditionally%252C%2520the%2520number%2520of%2520HSI-based%2520research%2520studies%2520remains%2520limited%252C%2520and%2520in%250Amany%2520cases%252C%2520the%2520advantages%2520of%2520HSI%2520over%2520traditional%2520RGB%2520imaging%2520have%2520not%2520been%250Aconclusively%2520demonstrated%252C%2520particularly%2520for%2520specimens%2520collected%250Aintraoperatively.%2520To%2520address%2520these%2520challenges%2520we%2520present%2520a%2520database%2520consisted%250Aof%252027%2520HSIs%2520of%2520hematoxylin-eosin%2520stained%2520frozen%2520sections%252C%2520collected%2520from%252014%250Apatients%2520with%2520colon%2520adenocarcinoma%2520metastasized%2520to%2520the%2520liver.%2520It%2520is%2520aimed%2520to%250Avalidate%2520pixel-wise%2520classification%2520for%2520intraoperative%2520tumor%2520resection.%2520The%2520HSIs%250Awere%2520acquired%2520in%2520the%2520spectral%2520range%2520of%2520450%2520to%2520800%2520nm%252C%2520with%2520a%2520resolution%2520of%25201%250Anm%252C%2520resulting%2520in%2520images%2520of%25201384x1035%2520pixels.%2520Pixel-wise%2520annotations%2520were%250Aperformed%2520by%2520three%2520pathologists.%2520To%2520overcome%2520challenges%2520such%2520as%2520experimental%250Avariability%2520and%2520the%2520lack%2520of%2520annotated%2520data%252C%2520we%2520combined%2520label-propagation-based%250Asemi-supervised%2520learning%2520%2528SSL%2529%2520with%2520spectral-spatial%2520features%2520extracted%2520by%253A%2520the%250Amultiscale%2520principle%2520of%2520relevant%2520information%2520%2528MPRI%2529%2520method%2520and%2520tensor%2520singular%250Aspectrum%2520analysis%2520method.%2520Using%2520only%25201%2525%2520of%2520labeled%2520pixels%2520per%2520class%2520the%250ASSL-MPRI%2520method%2520achieved%2520a%2520micro%2520balanced%2520accuracy%2520%2528BACC%2529%2520of%25200.9313%2520and%2520a%2520micro%250AF1-score%2520of%25200.9235%2520on%2520the%2520HSI%2520dataset.%2520The%2520performance%2520on%2520corresponding%2520RGB%250Aimages%2520was%2520lower%252C%2520with%2520a%2520micro%2520BACC%2520of%25200.8809%2520and%2520a%2520micro%2520F1-score%2520of%25200.8688.%250AThese%2520improvements%2520are%2520statistically%2520significant.%2520The%2520SSL-MPRI%2520approach%250Aoutperformed%2520six%2520DL%2520architectures%2520trained%2520with%252063%2525%2520of%2520labeled%2520pixels.%2520Data%2520and%250Acode%2520are%2520available%2520at%253A%2520https%253A//github.com/ikopriva/ColonCancerHSI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hyperspectral%20Imaging%20Dataset%20and%20Methodology%20for%20Intraoperative%0A%20%20Pixel-Wise%20Classification%20of%20Metastatic%20Colon%20Cancer%20in%20the%20Liver&entry.906535625=Ivica%20Kopriva%20and%20Dario%20Sitnik%20and%20Laura-Isabelle%20Dion-Bertrand%20and%20Marija%20Milkovi%C4%87%20Peri%C5%A1a%20and%20Mirko%20Had%C5%BEija%20and%20Marijana%20Popovi%C4%87%20Had%C5%BEija&entry.1292438233=%20%20Hyperspectral%20imaging%20%28HSI%29%20holds%20significant%20potential%20for%20transforming%20the%0Afield%20of%20computational%20pathology.%20However%2C%20there%20is%20currently%20a%20shortage%20of%0Apixel-wise%20annotated%20HSI%20data%20necessary%20for%20training%20deep%20learning%20%28DL%29%20models.%0AAdditionally%2C%20the%20number%20of%20HSI-based%20research%20studies%20remains%20limited%2C%20and%20in%0Amany%20cases%2C%20the%20advantages%20of%20HSI%20over%20traditional%20RGB%20imaging%20have%20not%20been%0Aconclusively%20demonstrated%2C%20particularly%20for%20specimens%20collected%0Aintraoperatively.%20To%20address%20these%20challenges%20we%20present%20a%20database%20consisted%0Aof%2027%20HSIs%20of%20hematoxylin-eosin%20stained%20frozen%20sections%2C%20collected%20from%2014%0Apatients%20with%20colon%20adenocarcinoma%20metastasized%20to%20the%20liver.%20It%20is%20aimed%20to%0Avalidate%20pixel-wise%20classification%20for%20intraoperative%20tumor%20resection.%20The%20HSIs%0Awere%20acquired%20in%20the%20spectral%20range%20of%20450%20to%20800%20nm%2C%20with%20a%20resolution%20of%201%0Anm%2C%20resulting%20in%20images%20of%201384x1035%20pixels.%20Pixel-wise%20annotations%20were%0Aperformed%20by%20three%20pathologists.%20To%20overcome%20challenges%20such%20as%20experimental%0Avariability%20and%20the%20lack%20of%20annotated%20data%2C%20we%20combined%20label-propagation-based%0Asemi-supervised%20learning%20%28SSL%29%20with%20spectral-spatial%20features%20extracted%20by%3A%20the%0Amultiscale%20principle%20of%20relevant%20information%20%28MPRI%29%20method%20and%20tensor%20singular%0Aspectrum%20analysis%20method.%20Using%20only%201%25%20of%20labeled%20pixels%20per%20class%20the%0ASSL-MPRI%20method%20achieved%20a%20micro%20balanced%20accuracy%20%28BACC%29%20of%200.9313%20and%20a%20micro%0AF1-score%20of%200.9235%20on%20the%20HSI%20dataset.%20The%20performance%20on%20corresponding%20RGB%0Aimages%20was%20lower%2C%20with%20a%20micro%20BACC%20of%200.8809%20and%20a%20micro%20F1-score%20of%200.8688.%0AThese%20improvements%20are%20statistically%20significant.%20The%20SSL-MPRI%20approach%0Aoutperformed%20six%20DL%20architectures%20trained%20with%2063%25%20of%20labeled%20pixels.%20Data%20and%0Acode%20are%20available%20at%3A%20https%3A//github.com/ikopriva/ColonCancerHSI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06969v1&entry.124074799=Read"},
{"title": "Computing excited states of molecules using normalizing flows", "author": "Yahya Saleh and \u00c1lvaro Fern\u00e1ndez Corral and Emil Vogt and Armin Iske and Jochen K\u00fcpper and Andrey Yachmenev", "abstract": "  Calculations of highly excited and delocalized molecular vibrational states\nis a computationally challenging task, which strongly depends on the choice of\ncoordinates for describing vibrational motions. We introduce a new method that\nutilizes normalizing flows (parametrized invertible functions) to optimize\nvibrational coordinates to satisfy the variational principle. This approach\nproduces coordinates specifically tailored to the vibrational problem at hand,\nsignificantly increasing the accuracy and enhancing basis set convergence of\ncalculated energy spectrum. The efficiency of the method is demonstrated in\ncalculations of the 100 lowest excited vibrational states of H$_2$S, H$_2$CO,\nand HCN/CNH. The method effectively captures the essential vibrational behavior\nof molecules by enhancing the separability of the Hamiltonian. We further\ndemonstrate that the optimized coordinates are transferable across different\nlevels of basis set truncation, enabling a cost-efficient protocol for\ncomputing vibrational spectra of high-dimensional systems.\n", "link": "http://arxiv.org/abs/2308.16468v2", "date": "2024-11-11", "relevancy": 1.2164, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4168}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4073}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computing%20excited%20states%20of%20molecules%20using%20normalizing%20flows&body=Title%3A%20Computing%20excited%20states%20of%20molecules%20using%20normalizing%20flows%0AAuthor%3A%20Yahya%20Saleh%20and%20%C3%81lvaro%20Fern%C3%A1ndez%20Corral%20and%20Emil%20Vogt%20and%20Armin%20Iske%20and%20Jochen%20K%C3%BCpper%20and%20Andrey%20Yachmenev%0AAbstract%3A%20%20%20Calculations%20of%20highly%20excited%20and%20delocalized%20molecular%20vibrational%20states%0Ais%20a%20computationally%20challenging%20task%2C%20which%20strongly%20depends%20on%20the%20choice%20of%0Acoordinates%20for%20describing%20vibrational%20motions.%20We%20introduce%20a%20new%20method%20that%0Autilizes%20normalizing%20flows%20%28parametrized%20invertible%20functions%29%20to%20optimize%0Avibrational%20coordinates%20to%20satisfy%20the%20variational%20principle.%20This%20approach%0Aproduces%20coordinates%20specifically%20tailored%20to%20the%20vibrational%20problem%20at%20hand%2C%0Asignificantly%20increasing%20the%20accuracy%20and%20enhancing%20basis%20set%20convergence%20of%0Acalculated%20energy%20spectrum.%20The%20efficiency%20of%20the%20method%20is%20demonstrated%20in%0Acalculations%20of%20the%20100%20lowest%20excited%20vibrational%20states%20of%20H%24_2%24S%2C%20H%24_2%24CO%2C%0Aand%20HCN/CNH.%20The%20method%20effectively%20captures%20the%20essential%20vibrational%20behavior%0Aof%20molecules%20by%20enhancing%20the%20separability%20of%20the%20Hamiltonian.%20We%20further%0Ademonstrate%20that%20the%20optimized%20coordinates%20are%20transferable%20across%20different%0Alevels%20of%20basis%20set%20truncation%2C%20enabling%20a%20cost-efficient%20protocol%20for%0Acomputing%20vibrational%20spectra%20of%20high-dimensional%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.16468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputing%2520excited%2520states%2520of%2520molecules%2520using%2520normalizing%2520flows%26entry.906535625%3DYahya%2520Saleh%2520and%2520%25C3%2581lvaro%2520Fern%25C3%25A1ndez%2520Corral%2520and%2520Emil%2520Vogt%2520and%2520Armin%2520Iske%2520and%2520Jochen%2520K%25C3%25BCpper%2520and%2520Andrey%2520Yachmenev%26entry.1292438233%3D%2520%2520Calculations%2520of%2520highly%2520excited%2520and%2520delocalized%2520molecular%2520vibrational%2520states%250Ais%2520a%2520computationally%2520challenging%2520task%252C%2520which%2520strongly%2520depends%2520on%2520the%2520choice%2520of%250Acoordinates%2520for%2520describing%2520vibrational%2520motions.%2520We%2520introduce%2520a%2520new%2520method%2520that%250Autilizes%2520normalizing%2520flows%2520%2528parametrized%2520invertible%2520functions%2529%2520to%2520optimize%250Avibrational%2520coordinates%2520to%2520satisfy%2520the%2520variational%2520principle.%2520This%2520approach%250Aproduces%2520coordinates%2520specifically%2520tailored%2520to%2520the%2520vibrational%2520problem%2520at%2520hand%252C%250Asignificantly%2520increasing%2520the%2520accuracy%2520and%2520enhancing%2520basis%2520set%2520convergence%2520of%250Acalculated%2520energy%2520spectrum.%2520The%2520efficiency%2520of%2520the%2520method%2520is%2520demonstrated%2520in%250Acalculations%2520of%2520the%2520100%2520lowest%2520excited%2520vibrational%2520states%2520of%2520H%2524_2%2524S%252C%2520H%2524_2%2524CO%252C%250Aand%2520HCN/CNH.%2520The%2520method%2520effectively%2520captures%2520the%2520essential%2520vibrational%2520behavior%250Aof%2520molecules%2520by%2520enhancing%2520the%2520separability%2520of%2520the%2520Hamiltonian.%2520We%2520further%250Ademonstrate%2520that%2520the%2520optimized%2520coordinates%2520are%2520transferable%2520across%2520different%250Alevels%2520of%2520basis%2520set%2520truncation%252C%2520enabling%2520a%2520cost-efficient%2520protocol%2520for%250Acomputing%2520vibrational%2520spectra%2520of%2520high-dimensional%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.16468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computing%20excited%20states%20of%20molecules%20using%20normalizing%20flows&entry.906535625=Yahya%20Saleh%20and%20%C3%81lvaro%20Fern%C3%A1ndez%20Corral%20and%20Emil%20Vogt%20and%20Armin%20Iske%20and%20Jochen%20K%C3%BCpper%20and%20Andrey%20Yachmenev&entry.1292438233=%20%20Calculations%20of%20highly%20excited%20and%20delocalized%20molecular%20vibrational%20states%0Ais%20a%20computationally%20challenging%20task%2C%20which%20strongly%20depends%20on%20the%20choice%20of%0Acoordinates%20for%20describing%20vibrational%20motions.%20We%20introduce%20a%20new%20method%20that%0Autilizes%20normalizing%20flows%20%28parametrized%20invertible%20functions%29%20to%20optimize%0Avibrational%20coordinates%20to%20satisfy%20the%20variational%20principle.%20This%20approach%0Aproduces%20coordinates%20specifically%20tailored%20to%20the%20vibrational%20problem%20at%20hand%2C%0Asignificantly%20increasing%20the%20accuracy%20and%20enhancing%20basis%20set%20convergence%20of%0Acalculated%20energy%20spectrum.%20The%20efficiency%20of%20the%20method%20is%20demonstrated%20in%0Acalculations%20of%20the%20100%20lowest%20excited%20vibrational%20states%20of%20H%24_2%24S%2C%20H%24_2%24CO%2C%0Aand%20HCN/CNH.%20The%20method%20effectively%20captures%20the%20essential%20vibrational%20behavior%0Aof%20molecules%20by%20enhancing%20the%20separability%20of%20the%20Hamiltonian.%20We%20further%0Ademonstrate%20that%20the%20optimized%20coordinates%20are%20transferable%20across%20different%0Alevels%20of%20basis%20set%20truncation%2C%20enabling%20a%20cost-efficient%20protocol%20for%0Acomputing%20vibrational%20spectra%20of%20high-dimensional%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.16468v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


