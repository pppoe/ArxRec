<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260118.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "UIKA: Fast Universal Head Avatar from Pose-Free Images", "author": "Zijian Wu and Boyao Zhou and Liangxiao Hu and Hongyu Liu and Yuan Sun and Xuan Wang and Xun Cao and Yujun Shen and Hao Zhu", "abstract": "We present UIKA, a feed-forward animatable Gaussian head model from an arbitrary number of unposed inputs, including a single image, multi-view captures, and smartphone-captured videos. Unlike the traditional avatar method, which requires a studio-level multi-view capture system and reconstructs a human-specific model through a long-time optimization process, we rethink the task through the lenses of model representation, network design, and data preparation. First, we introduce a UV-guided avatar modeling strategy, in which each input image is associated with a pixel-wise facial correspondence estimation. Such correspondence estimation allows us to reproject each valid pixel color from screen space to UV space, which is independent of camera pose and character expression. Furthermore, we design learnable UV tokens on which the attention mechanism can be applied at both the screen and UV levels. The learned UV tokens can be decoded into canonical Gaussian attributes using aggregated UV information from all input views. To train our large avatar model, we additionally prepare a large-scale, identity-rich synthetic training dataset. Our method significantly outperforms existing approaches in both monocular and multi-view settings. See more details in our project page: https://zijian-wu.github.io/uika-page/", "link": "http://arxiv.org/abs/2601.07603v2", "date": "2026-01-16", "relevancy": 3.2422, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6672}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6672}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UIKA%3A%20Fast%20Universal%20Head%20Avatar%20from%20Pose-Free%20Images&body=Title%3A%20UIKA%3A%20Fast%20Universal%20Head%20Avatar%20from%20Pose-Free%20Images%0AAuthor%3A%20Zijian%20Wu%20and%20Boyao%20Zhou%20and%20Liangxiao%20Hu%20and%20Hongyu%20Liu%20and%20Yuan%20Sun%20and%20Xuan%20Wang%20and%20Xun%20Cao%20and%20Yujun%20Shen%20and%20Hao%20Zhu%0AAbstract%3A%20We%20present%20UIKA%2C%20a%20feed-forward%20animatable%20Gaussian%20head%20model%20from%20an%20arbitrary%20number%20of%20unposed%20inputs%2C%20including%20a%20single%20image%2C%20multi-view%20captures%2C%20and%20smartphone-captured%20videos.%20Unlike%20the%20traditional%20avatar%20method%2C%20which%20requires%20a%20studio-level%20multi-view%20capture%20system%20and%20reconstructs%20a%20human-specific%20model%20through%20a%20long-time%20optimization%20process%2C%20we%20rethink%20the%20task%20through%20the%20lenses%20of%20model%20representation%2C%20network%20design%2C%20and%20data%20preparation.%20First%2C%20we%20introduce%20a%20UV-guided%20avatar%20modeling%20strategy%2C%20in%20which%20each%20input%20image%20is%20associated%20with%20a%20pixel-wise%20facial%20correspondence%20estimation.%20Such%20correspondence%20estimation%20allows%20us%20to%20reproject%20each%20valid%20pixel%20color%20from%20screen%20space%20to%20UV%20space%2C%20which%20is%20independent%20of%20camera%20pose%20and%20character%20expression.%20Furthermore%2C%20we%20design%20learnable%20UV%20tokens%20on%20which%20the%20attention%20mechanism%20can%20be%20applied%20at%20both%20the%20screen%20and%20UV%20levels.%20The%20learned%20UV%20tokens%20can%20be%20decoded%20into%20canonical%20Gaussian%20attributes%20using%20aggregated%20UV%20information%20from%20all%20input%20views.%20To%20train%20our%20large%20avatar%20model%2C%20we%20additionally%20prepare%20a%20large-scale%2C%20identity-rich%20synthetic%20training%20dataset.%20Our%20method%20significantly%20outperforms%20existing%20approaches%20in%20both%20monocular%20and%20multi-view%20settings.%20See%20more%20details%20in%20our%20project%20page%3A%20https%3A//zijian-wu.github.io/uika-page/%0ALink%3A%20http%3A//arxiv.org/abs/2601.07603v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUIKA%253A%2520Fast%2520Universal%2520Head%2520Avatar%2520from%2520Pose-Free%2520Images%26entry.906535625%3DZijian%2520Wu%2520and%2520Boyao%2520Zhou%2520and%2520Liangxiao%2520Hu%2520and%2520Hongyu%2520Liu%2520and%2520Yuan%2520Sun%2520and%2520Xuan%2520Wang%2520and%2520Xun%2520Cao%2520and%2520Yujun%2520Shen%2520and%2520Hao%2520Zhu%26entry.1292438233%3DWe%2520present%2520UIKA%252C%2520a%2520feed-forward%2520animatable%2520Gaussian%2520head%2520model%2520from%2520an%2520arbitrary%2520number%2520of%2520unposed%2520inputs%252C%2520including%2520a%2520single%2520image%252C%2520multi-view%2520captures%252C%2520and%2520smartphone-captured%2520videos.%2520Unlike%2520the%2520traditional%2520avatar%2520method%252C%2520which%2520requires%2520a%2520studio-level%2520multi-view%2520capture%2520system%2520and%2520reconstructs%2520a%2520human-specific%2520model%2520through%2520a%2520long-time%2520optimization%2520process%252C%2520we%2520rethink%2520the%2520task%2520through%2520the%2520lenses%2520of%2520model%2520representation%252C%2520network%2520design%252C%2520and%2520data%2520preparation.%2520First%252C%2520we%2520introduce%2520a%2520UV-guided%2520avatar%2520modeling%2520strategy%252C%2520in%2520which%2520each%2520input%2520image%2520is%2520associated%2520with%2520a%2520pixel-wise%2520facial%2520correspondence%2520estimation.%2520Such%2520correspondence%2520estimation%2520allows%2520us%2520to%2520reproject%2520each%2520valid%2520pixel%2520color%2520from%2520screen%2520space%2520to%2520UV%2520space%252C%2520which%2520is%2520independent%2520of%2520camera%2520pose%2520and%2520character%2520expression.%2520Furthermore%252C%2520we%2520design%2520learnable%2520UV%2520tokens%2520on%2520which%2520the%2520attention%2520mechanism%2520can%2520be%2520applied%2520at%2520both%2520the%2520screen%2520and%2520UV%2520levels.%2520The%2520learned%2520UV%2520tokens%2520can%2520be%2520decoded%2520into%2520canonical%2520Gaussian%2520attributes%2520using%2520aggregated%2520UV%2520information%2520from%2520all%2520input%2520views.%2520To%2520train%2520our%2520large%2520avatar%2520model%252C%2520we%2520additionally%2520prepare%2520a%2520large-scale%252C%2520identity-rich%2520synthetic%2520training%2520dataset.%2520Our%2520method%2520significantly%2520outperforms%2520existing%2520approaches%2520in%2520both%2520monocular%2520and%2520multi-view%2520settings.%2520See%2520more%2520details%2520in%2520our%2520project%2520page%253A%2520https%253A//zijian-wu.github.io/uika-page/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07603v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UIKA%3A%20Fast%20Universal%20Head%20Avatar%20from%20Pose-Free%20Images&entry.906535625=Zijian%20Wu%20and%20Boyao%20Zhou%20and%20Liangxiao%20Hu%20and%20Hongyu%20Liu%20and%20Yuan%20Sun%20and%20Xuan%20Wang%20and%20Xun%20Cao%20and%20Yujun%20Shen%20and%20Hao%20Zhu&entry.1292438233=We%20present%20UIKA%2C%20a%20feed-forward%20animatable%20Gaussian%20head%20model%20from%20an%20arbitrary%20number%20of%20unposed%20inputs%2C%20including%20a%20single%20image%2C%20multi-view%20captures%2C%20and%20smartphone-captured%20videos.%20Unlike%20the%20traditional%20avatar%20method%2C%20which%20requires%20a%20studio-level%20multi-view%20capture%20system%20and%20reconstructs%20a%20human-specific%20model%20through%20a%20long-time%20optimization%20process%2C%20we%20rethink%20the%20task%20through%20the%20lenses%20of%20model%20representation%2C%20network%20design%2C%20and%20data%20preparation.%20First%2C%20we%20introduce%20a%20UV-guided%20avatar%20modeling%20strategy%2C%20in%20which%20each%20input%20image%20is%20associated%20with%20a%20pixel-wise%20facial%20correspondence%20estimation.%20Such%20correspondence%20estimation%20allows%20us%20to%20reproject%20each%20valid%20pixel%20color%20from%20screen%20space%20to%20UV%20space%2C%20which%20is%20independent%20of%20camera%20pose%20and%20character%20expression.%20Furthermore%2C%20we%20design%20learnable%20UV%20tokens%20on%20which%20the%20attention%20mechanism%20can%20be%20applied%20at%20both%20the%20screen%20and%20UV%20levels.%20The%20learned%20UV%20tokens%20can%20be%20decoded%20into%20canonical%20Gaussian%20attributes%20using%20aggregated%20UV%20information%20from%20all%20input%20views.%20To%20train%20our%20large%20avatar%20model%2C%20we%20additionally%20prepare%20a%20large-scale%2C%20identity-rich%20synthetic%20training%20dataset.%20Our%20method%20significantly%20outperforms%20existing%20approaches%20in%20both%20monocular%20and%20multi-view%20settings.%20See%20more%20details%20in%20our%20project%20page%3A%20https%3A//zijian-wu.github.io/uika-page/&entry.1838667208=http%3A//arxiv.org/abs/2601.07603v2&entry.124074799=Read"},
{"title": "TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing Novel View Synthesis", "author": "Jiaming Kang and Keyan Chen and Zhengxia Zou and Zhenwei Shi", "abstract": "Remote sensing novel view synthesis (NVS) offers significant potential for 3D interpretation of remote sensing scenes, with important applications in urban planning and environmental monitoring. However, remote sensing scenes frequently lack sufficient multi-view images due to acquisition constraints. While existing NVS methods tend to overfit when processing limited input views, advanced few-shot NVS methods are computationally intensive and perform sub-optimally in remote sensing scenes. This paper presents TriDF, an efficient hybrid 3D representation for fast remote sensing NVS from as few as 3 input views. Our approach decouples color and volume density information, modeling them independently to reduce the computational burden on implicit radiance fields and accelerate reconstruction. We explore the potential of the triplane representation in few-shot NVS tasks by mapping high-frequency color information onto this compact structure, and the direct optimization of feature planes significantly speeds up convergence. Volume density is modeled as continuous density fields, incorporating reference features from neighboring views through image-based rendering to compensate for limited input data. Additionally, we introduce depth-guided optimization based on point clouds, which effectively mitigates the overfitting problem in few-shot NVS. Comprehensive experiments across multiple remote sensing scenes demonstrate that our hybrid representation achieves a 30x speed increase compared to NeRF-based methods, while simultaneously improving rendering quality metrics over advanced few-shot methods (7.4% increase in PSNR and 3.4% in SSIM). The code is publicly available at https://github.com/kanehub/TriDF", "link": "http://arxiv.org/abs/2503.13347v3", "date": "2026-01-16", "relevancy": 3.055, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6115}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6115}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TriDF%3A%20Triplane-Accelerated%20Density%20Fields%20for%20Few-Shot%20Remote%20Sensing%20Novel%20View%20Synthesis&body=Title%3A%20TriDF%3A%20Triplane-Accelerated%20Density%20Fields%20for%20Few-Shot%20Remote%20Sensing%20Novel%20View%20Synthesis%0AAuthor%3A%20Jiaming%20Kang%20and%20Keyan%20Chen%20and%20Zhengxia%20Zou%20and%20Zhenwei%20Shi%0AAbstract%3A%20Remote%20sensing%20novel%20view%20synthesis%20%28NVS%29%20offers%20significant%20potential%20for%203D%20interpretation%20of%20remote%20sensing%20scenes%2C%20with%20important%20applications%20in%20urban%20planning%20and%20environmental%20monitoring.%20However%2C%20remote%20sensing%20scenes%20frequently%20lack%20sufficient%20multi-view%20images%20due%20to%20acquisition%20constraints.%20While%20existing%20NVS%20methods%20tend%20to%20overfit%20when%20processing%20limited%20input%20views%2C%20advanced%20few-shot%20NVS%20methods%20are%20computationally%20intensive%20and%20perform%20sub-optimally%20in%20remote%20sensing%20scenes.%20This%20paper%20presents%20TriDF%2C%20an%20efficient%20hybrid%203D%20representation%20for%20fast%20remote%20sensing%20NVS%20from%20as%20few%20as%203%20input%20views.%20Our%20approach%20decouples%20color%20and%20volume%20density%20information%2C%20modeling%20them%20independently%20to%20reduce%20the%20computational%20burden%20on%20implicit%20radiance%20fields%20and%20accelerate%20reconstruction.%20We%20explore%20the%20potential%20of%20the%20triplane%20representation%20in%20few-shot%20NVS%20tasks%20by%20mapping%20high-frequency%20color%20information%20onto%20this%20compact%20structure%2C%20and%20the%20direct%20optimization%20of%20feature%20planes%20significantly%20speeds%20up%20convergence.%20Volume%20density%20is%20modeled%20as%20continuous%20density%20fields%2C%20incorporating%20reference%20features%20from%20neighboring%20views%20through%20image-based%20rendering%20to%20compensate%20for%20limited%20input%20data.%20Additionally%2C%20we%20introduce%20depth-guided%20optimization%20based%20on%20point%20clouds%2C%20which%20effectively%20mitigates%20the%20overfitting%20problem%20in%20few-shot%20NVS.%20Comprehensive%20experiments%20across%20multiple%20remote%20sensing%20scenes%20demonstrate%20that%20our%20hybrid%20representation%20achieves%20a%2030x%20speed%20increase%20compared%20to%20NeRF-based%20methods%2C%20while%20simultaneously%20improving%20rendering%20quality%20metrics%20over%20advanced%20few-shot%20methods%20%287.4%25%20increase%20in%20PSNR%20and%203.4%25%20in%20SSIM%29.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/kanehub/TriDF%0ALink%3A%20http%3A//arxiv.org/abs/2503.13347v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriDF%253A%2520Triplane-Accelerated%2520Density%2520Fields%2520for%2520Few-Shot%2520Remote%2520Sensing%2520Novel%2520View%2520Synthesis%26entry.906535625%3DJiaming%2520Kang%2520and%2520Keyan%2520Chen%2520and%2520Zhengxia%2520Zou%2520and%2520Zhenwei%2520Shi%26entry.1292438233%3DRemote%2520sensing%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520offers%2520significant%2520potential%2520for%25203D%2520interpretation%2520of%2520remote%2520sensing%2520scenes%252C%2520with%2520important%2520applications%2520in%2520urban%2520planning%2520and%2520environmental%2520monitoring.%2520However%252C%2520remote%2520sensing%2520scenes%2520frequently%2520lack%2520sufficient%2520multi-view%2520images%2520due%2520to%2520acquisition%2520constraints.%2520While%2520existing%2520NVS%2520methods%2520tend%2520to%2520overfit%2520when%2520processing%2520limited%2520input%2520views%252C%2520advanced%2520few-shot%2520NVS%2520methods%2520are%2520computationally%2520intensive%2520and%2520perform%2520sub-optimally%2520in%2520remote%2520sensing%2520scenes.%2520This%2520paper%2520presents%2520TriDF%252C%2520an%2520efficient%2520hybrid%25203D%2520representation%2520for%2520fast%2520remote%2520sensing%2520NVS%2520from%2520as%2520few%2520as%25203%2520input%2520views.%2520Our%2520approach%2520decouples%2520color%2520and%2520volume%2520density%2520information%252C%2520modeling%2520them%2520independently%2520to%2520reduce%2520the%2520computational%2520burden%2520on%2520implicit%2520radiance%2520fields%2520and%2520accelerate%2520reconstruction.%2520We%2520explore%2520the%2520potential%2520of%2520the%2520triplane%2520representation%2520in%2520few-shot%2520NVS%2520tasks%2520by%2520mapping%2520high-frequency%2520color%2520information%2520onto%2520this%2520compact%2520structure%252C%2520and%2520the%2520direct%2520optimization%2520of%2520feature%2520planes%2520significantly%2520speeds%2520up%2520convergence.%2520Volume%2520density%2520is%2520modeled%2520as%2520continuous%2520density%2520fields%252C%2520incorporating%2520reference%2520features%2520from%2520neighboring%2520views%2520through%2520image-based%2520rendering%2520to%2520compensate%2520for%2520limited%2520input%2520data.%2520Additionally%252C%2520we%2520introduce%2520depth-guided%2520optimization%2520based%2520on%2520point%2520clouds%252C%2520which%2520effectively%2520mitigates%2520the%2520overfitting%2520problem%2520in%2520few-shot%2520NVS.%2520Comprehensive%2520experiments%2520across%2520multiple%2520remote%2520sensing%2520scenes%2520demonstrate%2520that%2520our%2520hybrid%2520representation%2520achieves%2520a%252030x%2520speed%2520increase%2520compared%2520to%2520NeRF-based%2520methods%252C%2520while%2520simultaneously%2520improving%2520rendering%2520quality%2520metrics%2520over%2520advanced%2520few-shot%2520methods%2520%25287.4%2525%2520increase%2520in%2520PSNR%2520and%25203.4%2525%2520in%2520SSIM%2529.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/kanehub/TriDF%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13347v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TriDF%3A%20Triplane-Accelerated%20Density%20Fields%20for%20Few-Shot%20Remote%20Sensing%20Novel%20View%20Synthesis&entry.906535625=Jiaming%20Kang%20and%20Keyan%20Chen%20and%20Zhengxia%20Zou%20and%20Zhenwei%20Shi&entry.1292438233=Remote%20sensing%20novel%20view%20synthesis%20%28NVS%29%20offers%20significant%20potential%20for%203D%20interpretation%20of%20remote%20sensing%20scenes%2C%20with%20important%20applications%20in%20urban%20planning%20and%20environmental%20monitoring.%20However%2C%20remote%20sensing%20scenes%20frequently%20lack%20sufficient%20multi-view%20images%20due%20to%20acquisition%20constraints.%20While%20existing%20NVS%20methods%20tend%20to%20overfit%20when%20processing%20limited%20input%20views%2C%20advanced%20few-shot%20NVS%20methods%20are%20computationally%20intensive%20and%20perform%20sub-optimally%20in%20remote%20sensing%20scenes.%20This%20paper%20presents%20TriDF%2C%20an%20efficient%20hybrid%203D%20representation%20for%20fast%20remote%20sensing%20NVS%20from%20as%20few%20as%203%20input%20views.%20Our%20approach%20decouples%20color%20and%20volume%20density%20information%2C%20modeling%20them%20independently%20to%20reduce%20the%20computational%20burden%20on%20implicit%20radiance%20fields%20and%20accelerate%20reconstruction.%20We%20explore%20the%20potential%20of%20the%20triplane%20representation%20in%20few-shot%20NVS%20tasks%20by%20mapping%20high-frequency%20color%20information%20onto%20this%20compact%20structure%2C%20and%20the%20direct%20optimization%20of%20feature%20planes%20significantly%20speeds%20up%20convergence.%20Volume%20density%20is%20modeled%20as%20continuous%20density%20fields%2C%20incorporating%20reference%20features%20from%20neighboring%20views%20through%20image-based%20rendering%20to%20compensate%20for%20limited%20input%20data.%20Additionally%2C%20we%20introduce%20depth-guided%20optimization%20based%20on%20point%20clouds%2C%20which%20effectively%20mitigates%20the%20overfitting%20problem%20in%20few-shot%20NVS.%20Comprehensive%20experiments%20across%20multiple%20remote%20sensing%20scenes%20demonstrate%20that%20our%20hybrid%20representation%20achieves%20a%2030x%20speed%20increase%20compared%20to%20NeRF-based%20methods%2C%20while%20simultaneously%20improving%20rendering%20quality%20metrics%20over%20advanced%20few-shot%20methods%20%287.4%25%20increase%20in%20PSNR%20and%203.4%25%20in%20SSIM%29.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/kanehub/TriDF&entry.1838667208=http%3A//arxiv.org/abs/2503.13347v3&entry.124074799=Read"},
{"title": "Better Language Models Exhibit Higher Visual Alignment", "author": "Jona Ruthardt and Gertjan J. Burghouts and Serge Belongie and Yuki M. Asano", "abstract": "How well do text-only large language models (LLMs) align with the visual world? We present a systematic evaluation of this question by incorporating frozen representations of various language models into a discriminative vision-language framework and measuring zero-shot generalization to novel concepts. We find that decoder-based models exhibit stronger visual alignment than encoders, even when controlling for model and dataset size. Moreover, language modeling performance correlates with visual generalization, suggesting that advances in unimodal LLMs can simultaneously improve vision models. Leveraging these insights, we propose ShareLock, a lightweight method for fusing frozen vision and language backbones. ShareLock achieves robust performance across tasks while drastically reducing the need for paired data and compute. With just 563k image-caption pairs and under one GPU-hour of training, it reaches 51% accuracy on ImageNet. In cross-lingual settings, ShareLock dramatically outperforms CLIP, achieving 38.7% top-1 accuracy on Chinese image classification versus CLIP's 1.4%. Code is available.", "link": "http://arxiv.org/abs/2410.07173v3", "date": "2026-01-16", "relevancy": 2.9555, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5962}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5962}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20Language%20Models%20Exhibit%20Higher%20Visual%20Alignment&body=Title%3A%20Better%20Language%20Models%20Exhibit%20Higher%20Visual%20Alignment%0AAuthor%3A%20Jona%20Ruthardt%20and%20Gertjan%20J.%20Burghouts%20and%20Serge%20Belongie%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20How%20well%20do%20text-only%20large%20language%20models%20%28LLMs%29%20align%20with%20the%20visual%20world%3F%20We%20present%20a%20systematic%20evaluation%20of%20this%20question%20by%20incorporating%20frozen%20representations%20of%20various%20language%20models%20into%20a%20discriminative%20vision-language%20framework%20and%20measuring%20zero-shot%20generalization%20to%20novel%20concepts.%20We%20find%20that%20decoder-based%20models%20exhibit%20stronger%20visual%20alignment%20than%20encoders%2C%20even%20when%20controlling%20for%20model%20and%20dataset%20size.%20Moreover%2C%20language%20modeling%20performance%20correlates%20with%20visual%20generalization%2C%20suggesting%20that%20advances%20in%20unimodal%20LLMs%20can%20simultaneously%20improve%20vision%20models.%20Leveraging%20these%20insights%2C%20we%20propose%20ShareLock%2C%20a%20lightweight%20method%20for%20fusing%20frozen%20vision%20and%20language%20backbones.%20ShareLock%20achieves%20robust%20performance%20across%20tasks%20while%20drastically%20reducing%20the%20need%20for%20paired%20data%20and%20compute.%20With%20just%20563k%20image-caption%20pairs%20and%20under%20one%20GPU-hour%20of%20training%2C%20it%20reaches%2051%25%20accuracy%20on%20ImageNet.%20In%20cross-lingual%20settings%2C%20ShareLock%20dramatically%20outperforms%20CLIP%2C%20achieving%2038.7%25%20top-1%20accuracy%20on%20Chinese%20image%20classification%20versus%20CLIP%27s%201.4%25.%20Code%20is%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2410.07173v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520Language%2520Models%2520Exhibit%2520Higher%2520Visual%2520Alignment%26entry.906535625%3DJona%2520Ruthardt%2520and%2520Gertjan%2520J.%2520Burghouts%2520and%2520Serge%2520Belongie%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3DHow%2520well%2520do%2520text-only%2520large%2520language%2520models%2520%2528LLMs%2529%2520align%2520with%2520the%2520visual%2520world%253F%2520We%2520present%2520a%2520systematic%2520evaluation%2520of%2520this%2520question%2520by%2520incorporating%2520frozen%2520representations%2520of%2520various%2520language%2520models%2520into%2520a%2520discriminative%2520vision-language%2520framework%2520and%2520measuring%2520zero-shot%2520generalization%2520to%2520novel%2520concepts.%2520We%2520find%2520that%2520decoder-based%2520models%2520exhibit%2520stronger%2520visual%2520alignment%2520than%2520encoders%252C%2520even%2520when%2520controlling%2520for%2520model%2520and%2520dataset%2520size.%2520Moreover%252C%2520language%2520modeling%2520performance%2520correlates%2520with%2520visual%2520generalization%252C%2520suggesting%2520that%2520advances%2520in%2520unimodal%2520LLMs%2520can%2520simultaneously%2520improve%2520vision%2520models.%2520Leveraging%2520these%2520insights%252C%2520we%2520propose%2520ShareLock%252C%2520a%2520lightweight%2520method%2520for%2520fusing%2520frozen%2520vision%2520and%2520language%2520backbones.%2520ShareLock%2520achieves%2520robust%2520performance%2520across%2520tasks%2520while%2520drastically%2520reducing%2520the%2520need%2520for%2520paired%2520data%2520and%2520compute.%2520With%2520just%2520563k%2520image-caption%2520pairs%2520and%2520under%2520one%2520GPU-hour%2520of%2520training%252C%2520it%2520reaches%252051%2525%2520accuracy%2520on%2520ImageNet.%2520In%2520cross-lingual%2520settings%252C%2520ShareLock%2520dramatically%2520outperforms%2520CLIP%252C%2520achieving%252038.7%2525%2520top-1%2520accuracy%2520on%2520Chinese%2520image%2520classification%2520versus%2520CLIP%2527s%25201.4%2525.%2520Code%2520is%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07173v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20Language%20Models%20Exhibit%20Higher%20Visual%20Alignment&entry.906535625=Jona%20Ruthardt%20and%20Gertjan%20J.%20Burghouts%20and%20Serge%20Belongie%20and%20Yuki%20M.%20Asano&entry.1292438233=How%20well%20do%20text-only%20large%20language%20models%20%28LLMs%29%20align%20with%20the%20visual%20world%3F%20We%20present%20a%20systematic%20evaluation%20of%20this%20question%20by%20incorporating%20frozen%20representations%20of%20various%20language%20models%20into%20a%20discriminative%20vision-language%20framework%20and%20measuring%20zero-shot%20generalization%20to%20novel%20concepts.%20We%20find%20that%20decoder-based%20models%20exhibit%20stronger%20visual%20alignment%20than%20encoders%2C%20even%20when%20controlling%20for%20model%20and%20dataset%20size.%20Moreover%2C%20language%20modeling%20performance%20correlates%20with%20visual%20generalization%2C%20suggesting%20that%20advances%20in%20unimodal%20LLMs%20can%20simultaneously%20improve%20vision%20models.%20Leveraging%20these%20insights%2C%20we%20propose%20ShareLock%2C%20a%20lightweight%20method%20for%20fusing%20frozen%20vision%20and%20language%20backbones.%20ShareLock%20achieves%20robust%20performance%20across%20tasks%20while%20drastically%20reducing%20the%20need%20for%20paired%20data%20and%20compute.%20With%20just%20563k%20image-caption%20pairs%20and%20under%20one%20GPU-hour%20of%20training%2C%20it%20reaches%2051%25%20accuracy%20on%20ImageNet.%20In%20cross-lingual%20settings%2C%20ShareLock%20dramatically%20outperforms%20CLIP%2C%20achieving%2038.7%25%20top-1%20accuracy%20on%20Chinese%20image%20classification%20versus%20CLIP%27s%201.4%25.%20Code%20is%20available.&entry.1838667208=http%3A//arxiv.org/abs/2410.07173v3&entry.124074799=Read"},
{"title": "Enhancing Vision Language Models with Logic Reasoning for Situational Awareness", "author": "Pavana Pradeep and Krishna Kant and Suya Yu", "abstract": "Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.", "link": "http://arxiv.org/abs/2601.11322v1", "date": "2026-01-16", "relevancy": 2.9002, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5841}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5841}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Vision%20Language%20Models%20with%20Logic%20Reasoning%20for%20Situational%20Awareness&body=Title%3A%20Enhancing%20Vision%20Language%20Models%20with%20Logic%20Reasoning%20for%20Situational%20Awareness%0AAuthor%3A%20Pavana%20Pradeep%20and%20Krishna%20Kant%20and%20Suya%20Yu%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20offer%20the%20ability%20to%20generate%20high-level%2C%20interpretable%20descriptions%20of%20complex%20activities%20from%20images%20and%20videos%2C%20making%20them%20valuable%20for%20situational%20awareness%20%28SA%29%20applications.%20In%20such%20settings%2C%20the%20focus%20is%20on%20identifying%20infrequent%20but%20significant%20events%20with%20high%20reliability%20and%20accuracy%2C%20while%20also%20extracting%20fine-grained%20details%20and%20assessing%20recognition%20quality.%20In%20this%20paper%2C%20we%20propose%20an%20approach%20that%20integrates%20VLMs%20with%20traditional%20computer%20vision%20methods%20through%20explicit%20logic%20reasoning%20to%20enhance%20SA%20in%20three%20key%20ways%3A%20%28a%29%20extracting%20fine-grained%20event%20details%2C%20%28b%29%20employing%20an%20intelligent%20fine-tuning%20%28FT%29%20strategy%20that%20achieves%20substantially%20higher%20accuracy%20than%20uninformed%20selection%2C%20and%20%28c%29%20generating%20justifications%20for%20VLM%20outputs%20during%20inference.%20We%20demonstrate%20that%20our%20intelligent%20FT%20mechanism%20improves%20the%20accuracy%20and%20provides%20a%20valuable%20means%2C%20during%20inferencing%2C%20to%20either%20confirm%20the%20validity%20of%20the%20VLM%20output%20or%20indicate%20why%20it%20may%20be%20questionable.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Vision%2520Language%2520Models%2520with%2520Logic%2520Reasoning%2520for%2520Situational%2520Awareness%26entry.906535625%3DPavana%2520Pradeep%2520and%2520Krishna%2520Kant%2520and%2520Suya%2520Yu%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520offer%2520the%2520ability%2520to%2520generate%2520high-level%252C%2520interpretable%2520descriptions%2520of%2520complex%2520activities%2520from%2520images%2520and%2520videos%252C%2520making%2520them%2520valuable%2520for%2520situational%2520awareness%2520%2528SA%2529%2520applications.%2520In%2520such%2520settings%252C%2520the%2520focus%2520is%2520on%2520identifying%2520infrequent%2520but%2520significant%2520events%2520with%2520high%2520reliability%2520and%2520accuracy%252C%2520while%2520also%2520extracting%2520fine-grained%2520details%2520and%2520assessing%2520recognition%2520quality.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520approach%2520that%2520integrates%2520VLMs%2520with%2520traditional%2520computer%2520vision%2520methods%2520through%2520explicit%2520logic%2520reasoning%2520to%2520enhance%2520SA%2520in%2520three%2520key%2520ways%253A%2520%2528a%2529%2520extracting%2520fine-grained%2520event%2520details%252C%2520%2528b%2529%2520employing%2520an%2520intelligent%2520fine-tuning%2520%2528FT%2529%2520strategy%2520that%2520achieves%2520substantially%2520higher%2520accuracy%2520than%2520uninformed%2520selection%252C%2520and%2520%2528c%2529%2520generating%2520justifications%2520for%2520VLM%2520outputs%2520during%2520inference.%2520We%2520demonstrate%2520that%2520our%2520intelligent%2520FT%2520mechanism%2520improves%2520the%2520accuracy%2520and%2520provides%2520a%2520valuable%2520means%252C%2520during%2520inferencing%252C%2520to%2520either%2520confirm%2520the%2520validity%2520of%2520the%2520VLM%2520output%2520or%2520indicate%2520why%2520it%2520may%2520be%2520questionable.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Vision%20Language%20Models%20with%20Logic%20Reasoning%20for%20Situational%20Awareness&entry.906535625=Pavana%20Pradeep%20and%20Krishna%20Kant%20and%20Suya%20Yu&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20offer%20the%20ability%20to%20generate%20high-level%2C%20interpretable%20descriptions%20of%20complex%20activities%20from%20images%20and%20videos%2C%20making%20them%20valuable%20for%20situational%20awareness%20%28SA%29%20applications.%20In%20such%20settings%2C%20the%20focus%20is%20on%20identifying%20infrequent%20but%20significant%20events%20with%20high%20reliability%20and%20accuracy%2C%20while%20also%20extracting%20fine-grained%20details%20and%20assessing%20recognition%20quality.%20In%20this%20paper%2C%20we%20propose%20an%20approach%20that%20integrates%20VLMs%20with%20traditional%20computer%20vision%20methods%20through%20explicit%20logic%20reasoning%20to%20enhance%20SA%20in%20three%20key%20ways%3A%20%28a%29%20extracting%20fine-grained%20event%20details%2C%20%28b%29%20employing%20an%20intelligent%20fine-tuning%20%28FT%29%20strategy%20that%20achieves%20substantially%20higher%20accuracy%20than%20uninformed%20selection%2C%20and%20%28c%29%20generating%20justifications%20for%20VLM%20outputs%20during%20inference.%20We%20demonstrate%20that%20our%20intelligent%20FT%20mechanism%20improves%20the%20accuracy%20and%20provides%20a%20valuable%20means%2C%20during%20inferencing%2C%20to%20either%20confirm%20the%20validity%20of%20the%20VLM%20output%20or%20indicate%20why%20it%20may%20be%20questionable.&entry.1838667208=http%3A//arxiv.org/abs/2601.11322v1&entry.124074799=Read"},
{"title": "ReScene4D: Temporally Consistent Semantic Instance Segmentation of Evolving Indoor 3D Scenes", "author": "Emily Steiner and Jianhao Zheng and Henry Howard-Jenkins and Chris Xie and Iro Armeni", "abstract": "Indoor environments evolve as objects move, appear, or disappear. Capturing these dynamics requires maintaining temporally consistent instance identities across intermittently captured 3D scans, even when changes are unobserved. We introduce and formalize the task of temporally sparse 4D indoor semantic instance segmentation (SIS), which jointly segments, identifies, and temporally associates object instances. This setting poses a challenge for existing 3DSIS methods, which require a discrete matching step due to their lack of temporal reasoning, and for 4D LiDAR approaches, which perform poorly due to their reliance on high-frequency temporal measurements that are uncommon in the longer-horizon evolution of indoor environments. We propose ReScene4D, a novel method that adapts 3DSIS architectures for 4DSIS without needing dense observations. It explores strategies to share information across observations, demonstrating that this shared context not only enables consistent instance tracking but also improves standard 3DSIS quality. To evaluate this task, we define a new metric, t-mAP, that extends mAP to reward temporal identity consistency. ReScene4D achieves state-of-the-art performance on the 3RScan dataset, establishing a new benchmark for understanding evolving indoor scenes.", "link": "http://arxiv.org/abs/2601.11508v1", "date": "2026-01-16", "relevancy": 2.8849, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReScene4D%3A%20Temporally%20Consistent%20Semantic%20Instance%20Segmentation%20of%20Evolving%20Indoor%203D%20Scenes&body=Title%3A%20ReScene4D%3A%20Temporally%20Consistent%20Semantic%20Instance%20Segmentation%20of%20Evolving%20Indoor%203D%20Scenes%0AAuthor%3A%20Emily%20Steiner%20and%20Jianhao%20Zheng%20and%20Henry%20Howard-Jenkins%20and%20Chris%20Xie%20and%20Iro%20Armeni%0AAbstract%3A%20Indoor%20environments%20evolve%20as%20objects%20move%2C%20appear%2C%20or%20disappear.%20Capturing%20these%20dynamics%20requires%20maintaining%20temporally%20consistent%20instance%20identities%20across%20intermittently%20captured%203D%20scans%2C%20even%20when%20changes%20are%20unobserved.%20We%20introduce%20and%20formalize%20the%20task%20of%20temporally%20sparse%204D%20indoor%20semantic%20instance%20segmentation%20%28SIS%29%2C%20which%20jointly%20segments%2C%20identifies%2C%20and%20temporally%20associates%20object%20instances.%20This%20setting%20poses%20a%20challenge%20for%20existing%203DSIS%20methods%2C%20which%20require%20a%20discrete%20matching%20step%20due%20to%20their%20lack%20of%20temporal%20reasoning%2C%20and%20for%204D%20LiDAR%20approaches%2C%20which%20perform%20poorly%20due%20to%20their%20reliance%20on%20high-frequency%20temporal%20measurements%20that%20are%20uncommon%20in%20the%20longer-horizon%20evolution%20of%20indoor%20environments.%20We%20propose%20ReScene4D%2C%20a%20novel%20method%20that%20adapts%203DSIS%20architectures%20for%204DSIS%20without%20needing%20dense%20observations.%20It%20explores%20strategies%20to%20share%20information%20across%20observations%2C%20demonstrating%20that%20this%20shared%20context%20not%20only%20enables%20consistent%20instance%20tracking%20but%20also%20improves%20standard%203DSIS%20quality.%20To%20evaluate%20this%20task%2C%20we%20define%20a%20new%20metric%2C%20t-mAP%2C%20that%20extends%20mAP%20to%20reward%20temporal%20identity%20consistency.%20ReScene4D%20achieves%20state-of-the-art%20performance%20on%20the%203RScan%20dataset%2C%20establishing%20a%20new%20benchmark%20for%20understanding%20evolving%20indoor%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReScene4D%253A%2520Temporally%2520Consistent%2520Semantic%2520Instance%2520Segmentation%2520of%2520Evolving%2520Indoor%25203D%2520Scenes%26entry.906535625%3DEmily%2520Steiner%2520and%2520Jianhao%2520Zheng%2520and%2520Henry%2520Howard-Jenkins%2520and%2520Chris%2520Xie%2520and%2520Iro%2520Armeni%26entry.1292438233%3DIndoor%2520environments%2520evolve%2520as%2520objects%2520move%252C%2520appear%252C%2520or%2520disappear.%2520Capturing%2520these%2520dynamics%2520requires%2520maintaining%2520temporally%2520consistent%2520instance%2520identities%2520across%2520intermittently%2520captured%25203D%2520scans%252C%2520even%2520when%2520changes%2520are%2520unobserved.%2520We%2520introduce%2520and%2520formalize%2520the%2520task%2520of%2520temporally%2520sparse%25204D%2520indoor%2520semantic%2520instance%2520segmentation%2520%2528SIS%2529%252C%2520which%2520jointly%2520segments%252C%2520identifies%252C%2520and%2520temporally%2520associates%2520object%2520instances.%2520This%2520setting%2520poses%2520a%2520challenge%2520for%2520existing%25203DSIS%2520methods%252C%2520which%2520require%2520a%2520discrete%2520matching%2520step%2520due%2520to%2520their%2520lack%2520of%2520temporal%2520reasoning%252C%2520and%2520for%25204D%2520LiDAR%2520approaches%252C%2520which%2520perform%2520poorly%2520due%2520to%2520their%2520reliance%2520on%2520high-frequency%2520temporal%2520measurements%2520that%2520are%2520uncommon%2520in%2520the%2520longer-horizon%2520evolution%2520of%2520indoor%2520environments.%2520We%2520propose%2520ReScene4D%252C%2520a%2520novel%2520method%2520that%2520adapts%25203DSIS%2520architectures%2520for%25204DSIS%2520without%2520needing%2520dense%2520observations.%2520It%2520explores%2520strategies%2520to%2520share%2520information%2520across%2520observations%252C%2520demonstrating%2520that%2520this%2520shared%2520context%2520not%2520only%2520enables%2520consistent%2520instance%2520tracking%2520but%2520also%2520improves%2520standard%25203DSIS%2520quality.%2520To%2520evaluate%2520this%2520task%252C%2520we%2520define%2520a%2520new%2520metric%252C%2520t-mAP%252C%2520that%2520extends%2520mAP%2520to%2520reward%2520temporal%2520identity%2520consistency.%2520ReScene4D%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%25203RScan%2520dataset%252C%2520establishing%2520a%2520new%2520benchmark%2520for%2520understanding%2520evolving%2520indoor%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReScene4D%3A%20Temporally%20Consistent%20Semantic%20Instance%20Segmentation%20of%20Evolving%20Indoor%203D%20Scenes&entry.906535625=Emily%20Steiner%20and%20Jianhao%20Zheng%20and%20Henry%20Howard-Jenkins%20and%20Chris%20Xie%20and%20Iro%20Armeni&entry.1292438233=Indoor%20environments%20evolve%20as%20objects%20move%2C%20appear%2C%20or%20disappear.%20Capturing%20these%20dynamics%20requires%20maintaining%20temporally%20consistent%20instance%20identities%20across%20intermittently%20captured%203D%20scans%2C%20even%20when%20changes%20are%20unobserved.%20We%20introduce%20and%20formalize%20the%20task%20of%20temporally%20sparse%204D%20indoor%20semantic%20instance%20segmentation%20%28SIS%29%2C%20which%20jointly%20segments%2C%20identifies%2C%20and%20temporally%20associates%20object%20instances.%20This%20setting%20poses%20a%20challenge%20for%20existing%203DSIS%20methods%2C%20which%20require%20a%20discrete%20matching%20step%20due%20to%20their%20lack%20of%20temporal%20reasoning%2C%20and%20for%204D%20LiDAR%20approaches%2C%20which%20perform%20poorly%20due%20to%20their%20reliance%20on%20high-frequency%20temporal%20measurements%20that%20are%20uncommon%20in%20the%20longer-horizon%20evolution%20of%20indoor%20environments.%20We%20propose%20ReScene4D%2C%20a%20novel%20method%20that%20adapts%203DSIS%20architectures%20for%204DSIS%20without%20needing%20dense%20observations.%20It%20explores%20strategies%20to%20share%20information%20across%20observations%2C%20demonstrating%20that%20this%20shared%20context%20not%20only%20enables%20consistent%20instance%20tracking%20but%20also%20improves%20standard%203DSIS%20quality.%20To%20evaluate%20this%20task%2C%20we%20define%20a%20new%20metric%2C%20t-mAP%2C%20that%20extends%20mAP%20to%20reward%20temporal%20identity%20consistency.%20ReScene4D%20achieves%20state-of-the-art%20performance%20on%20the%203RScan%20dataset%2C%20establishing%20a%20new%20benchmark%20for%20understanding%20evolving%20indoor%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2601.11508v1&entry.124074799=Read"},
{"title": "MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models", "author": "Xiaoran Fan and Zhichao Sun and Tao Ji and Lixing Shen and Tao Gui", "abstract": "As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.", "link": "http://arxiv.org/abs/2601.11464v1", "date": "2026-01-16", "relevancy": 2.7943, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5769}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MHA2MLA-VLM%3A%20Enabling%20DeepSeek%27s%20Economical%20Multi-Head%20Latent%20Attention%20across%20Vision-Language%20Models&body=Title%3A%20MHA2MLA-VLM%3A%20Enabling%20DeepSeek%27s%20Economical%20Multi-Head%20Latent%20Attention%20across%20Vision-Language%20Models%0AAuthor%3A%20Xiaoran%20Fan%20and%20Zhichao%20Sun%20and%20Tao%20Ji%20and%20Lixing%20Shen%20and%20Tao%20Gui%0AAbstract%3A%20As%20vision-language%20models%20%28VLMs%29%20tackle%20increasingly%20complex%20and%20multimodal%20tasks%2C%20the%20rapid%20growth%20of%20Key-Value%20%28KV%29%20cache%20imposes%20significant%20memory%20and%20computational%20bottlenecks%20during%20inference.%20While%20Multi-Head%20Latent%20Attention%20%28MLA%29%20offers%20an%20effective%20means%20to%20compress%20the%20KV%20cache%20and%20accelerate%20inference%2C%20adapting%20existing%20VLMs%20to%20the%20MLA%20architecture%20without%20costly%20pretraining%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20present%20MHA2MLA-VLM%2C%20a%20parameter-efficient%20and%20multimodal-aware%20framework%20for%20converting%20off-the-shelf%20VLMs%20to%20MLA.%20Our%20approach%20features%20two%20core%20techniques%3A%20%281%29%20a%20modality-adaptive%20partial-RoPE%20strategy%20that%20supports%20both%20traditional%20and%20multimodal%20settings%20by%20selectively%20masking%20nonessential%20dimensions%2C%20and%20%282%29%20a%20modality-decoupled%20low-rank%20approximation%20method%20that%20independently%20compresses%20the%20visual%20and%20textual%20KV%20spaces.%20Furthermore%2C%20we%20introduce%20parameter-efficient%20fine-tuning%20to%20minimize%20adaptation%20cost%20and%20demonstrate%20that%20minimizing%20output%20activation%20error%2C%20rather%20than%20parameter%20distance%2C%20substantially%20reduces%20performance%20loss.%20Extensive%20experiments%20on%20three%20representative%20VLMs%20show%20that%20MHA2MLA-VLM%20restores%20original%20model%20performance%20with%20minimal%20supervised%20data%2C%20significantly%20reduces%20KV%20cache%20footprint%2C%20and%20integrates%20seamlessly%20with%20KV%20quantization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMHA2MLA-VLM%253A%2520Enabling%2520DeepSeek%2527s%2520Economical%2520Multi-Head%2520Latent%2520Attention%2520across%2520Vision-Language%2520Models%26entry.906535625%3DXiaoran%2520Fan%2520and%2520Zhichao%2520Sun%2520and%2520Tao%2520Ji%2520and%2520Lixing%2520Shen%2520and%2520Tao%2520Gui%26entry.1292438233%3DAs%2520vision-language%2520models%2520%2528VLMs%2529%2520tackle%2520increasingly%2520complex%2520and%2520multimodal%2520tasks%252C%2520the%2520rapid%2520growth%2520of%2520Key-Value%2520%2528KV%2529%2520cache%2520imposes%2520significant%2520memory%2520and%2520computational%2520bottlenecks%2520during%2520inference.%2520While%2520Multi-Head%2520Latent%2520Attention%2520%2528MLA%2529%2520offers%2520an%2520effective%2520means%2520to%2520compress%2520the%2520KV%2520cache%2520and%2520accelerate%2520inference%252C%2520adapting%2520existing%2520VLMs%2520to%2520the%2520MLA%2520architecture%2520without%2520costly%2520pretraining%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520present%2520MHA2MLA-VLM%252C%2520a%2520parameter-efficient%2520and%2520multimodal-aware%2520framework%2520for%2520converting%2520off-the-shelf%2520VLMs%2520to%2520MLA.%2520Our%2520approach%2520features%2520two%2520core%2520techniques%253A%2520%25281%2529%2520a%2520modality-adaptive%2520partial-RoPE%2520strategy%2520that%2520supports%2520both%2520traditional%2520and%2520multimodal%2520settings%2520by%2520selectively%2520masking%2520nonessential%2520dimensions%252C%2520and%2520%25282%2529%2520a%2520modality-decoupled%2520low-rank%2520approximation%2520method%2520that%2520independently%2520compresses%2520the%2520visual%2520and%2520textual%2520KV%2520spaces.%2520Furthermore%252C%2520we%2520introduce%2520parameter-efficient%2520fine-tuning%2520to%2520minimize%2520adaptation%2520cost%2520and%2520demonstrate%2520that%2520minimizing%2520output%2520activation%2520error%252C%2520rather%2520than%2520parameter%2520distance%252C%2520substantially%2520reduces%2520performance%2520loss.%2520Extensive%2520experiments%2520on%2520three%2520representative%2520VLMs%2520show%2520that%2520MHA2MLA-VLM%2520restores%2520original%2520model%2520performance%2520with%2520minimal%2520supervised%2520data%252C%2520significantly%2520reduces%2520KV%2520cache%2520footprint%252C%2520and%2520integrates%2520seamlessly%2520with%2520KV%2520quantization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MHA2MLA-VLM%3A%20Enabling%20DeepSeek%27s%20Economical%20Multi-Head%20Latent%20Attention%20across%20Vision-Language%20Models&entry.906535625=Xiaoran%20Fan%20and%20Zhichao%20Sun%20and%20Tao%20Ji%20and%20Lixing%20Shen%20and%20Tao%20Gui&entry.1292438233=As%20vision-language%20models%20%28VLMs%29%20tackle%20increasingly%20complex%20and%20multimodal%20tasks%2C%20the%20rapid%20growth%20of%20Key-Value%20%28KV%29%20cache%20imposes%20significant%20memory%20and%20computational%20bottlenecks%20during%20inference.%20While%20Multi-Head%20Latent%20Attention%20%28MLA%29%20offers%20an%20effective%20means%20to%20compress%20the%20KV%20cache%20and%20accelerate%20inference%2C%20adapting%20existing%20VLMs%20to%20the%20MLA%20architecture%20without%20costly%20pretraining%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20present%20MHA2MLA-VLM%2C%20a%20parameter-efficient%20and%20multimodal-aware%20framework%20for%20converting%20off-the-shelf%20VLMs%20to%20MLA.%20Our%20approach%20features%20two%20core%20techniques%3A%20%281%29%20a%20modality-adaptive%20partial-RoPE%20strategy%20that%20supports%20both%20traditional%20and%20multimodal%20settings%20by%20selectively%20masking%20nonessential%20dimensions%2C%20and%20%282%29%20a%20modality-decoupled%20low-rank%20approximation%20method%20that%20independently%20compresses%20the%20visual%20and%20textual%20KV%20spaces.%20Furthermore%2C%20we%20introduce%20parameter-efficient%20fine-tuning%20to%20minimize%20adaptation%20cost%20and%20demonstrate%20that%20minimizing%20output%20activation%20error%2C%20rather%20than%20parameter%20distance%2C%20substantially%20reduces%20performance%20loss.%20Extensive%20experiments%20on%20three%20representative%20VLMs%20show%20that%20MHA2MLA-VLM%20restores%20original%20model%20performance%20with%20minimal%20supervised%20data%2C%20significantly%20reduces%20KV%20cache%20footprint%2C%20and%20integrates%20seamlessly%20with%20KV%20quantization.&entry.1838667208=http%3A//arxiv.org/abs/2601.11464v1&entry.124074799=Read"},
{"title": "TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech", "author": "Girish A. Koushik and Helen Treharne and Diptesh Kanojia", "abstract": "Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as \"black boxes\" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.", "link": "http://arxiv.org/abs/2601.11178v1", "date": "2026-01-16", "relevancy": 2.7852, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5856}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5585}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TANDEM%3A%20Temporal-Aware%20Neural%20Detection%20for%20Multimodal%20Hate%20Speech&body=Title%3A%20TANDEM%3A%20Temporal-Aware%20Neural%20Detection%20for%20Multimodal%20Hate%20Speech%0AAuthor%3A%20Girish%20A.%20Koushik%20and%20Helen%20Treharne%20and%20Diptesh%20Kanojia%0AAbstract%3A%20Social%20media%20platforms%20are%20increasingly%20dominated%20by%20long-form%20multimodal%20content%2C%20where%20harmful%20narratives%20are%20constructed%20through%20a%20complex%20interplay%20of%20audio%2C%20visual%2C%20and%20textual%20cues.%20While%20automated%20systems%20can%20flag%20hate%20speech%20with%20high%20accuracy%2C%20they%20often%20function%20as%20%22black%20boxes%22%20that%20fail%20to%20provide%20the%20granular%2C%20interpretable%20evidence%2C%20such%20as%20precise%20timestamps%20and%20target%20identities%2C%20required%20for%20effective%20human-in-the-loop%20moderation.%20In%20this%20work%2C%20we%20introduce%20TANDEM%2C%20a%20unified%20framework%20that%20transforms%20audio-visual%20hate%20detection%20from%20a%20binary%20classification%20task%20into%20a%20structured%20reasoning%20problem.%20Our%20approach%20employs%20a%20novel%20tandem%20reinforcement%20learning%20strategy%20where%20vision-language%20and%20audio-language%20models%20optimize%20each%20other%20through%20self-constrained%20cross-modal%20context%2C%20stabilizing%20reasoning%20over%20extended%20temporal%20sequences%20without%20requiring%20dense%20frame-level%20supervision.%20Experiments%20across%20three%20benchmark%20datasets%20demonstrate%20that%20TANDEM%20significantly%20outperforms%20zero-shot%20and%20context-augmented%20baselines%2C%20achieving%200.73%20F1%20in%20target%20identification%20on%20HateMM%20%28a%2030%25%20improvement%20over%20state-of-the-art%29%20while%20maintaining%20precise%20temporal%20grounding.%20We%20further%20observe%20that%20while%20binary%20detection%20is%20robust%2C%20differentiating%20between%20offensive%20and%20hateful%20content%20remains%20challenging%20in%20multi-class%20settings%20due%20to%20inherent%20label%20ambiguity%20and%20dataset%20imbalance.%20More%20broadly%2C%20our%20findings%20suggest%20that%20structured%2C%20interpretable%20alignment%20is%20achievable%20even%20in%20complex%20multimodal%20settings%2C%20offering%20a%20blueprint%20for%20the%20next%20generation%20of%20transparent%20and%20actionable%20online%20safety%20moderation%20tools.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTANDEM%253A%2520Temporal-Aware%2520Neural%2520Detection%2520for%2520Multimodal%2520Hate%2520Speech%26entry.906535625%3DGirish%2520A.%2520Koushik%2520and%2520Helen%2520Treharne%2520and%2520Diptesh%2520Kanojia%26entry.1292438233%3DSocial%2520media%2520platforms%2520are%2520increasingly%2520dominated%2520by%2520long-form%2520multimodal%2520content%252C%2520where%2520harmful%2520narratives%2520are%2520constructed%2520through%2520a%2520complex%2520interplay%2520of%2520audio%252C%2520visual%252C%2520and%2520textual%2520cues.%2520While%2520automated%2520systems%2520can%2520flag%2520hate%2520speech%2520with%2520high%2520accuracy%252C%2520they%2520often%2520function%2520as%2520%2522black%2520boxes%2522%2520that%2520fail%2520to%2520provide%2520the%2520granular%252C%2520interpretable%2520evidence%252C%2520such%2520as%2520precise%2520timestamps%2520and%2520target%2520identities%252C%2520required%2520for%2520effective%2520human-in-the-loop%2520moderation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520TANDEM%252C%2520a%2520unified%2520framework%2520that%2520transforms%2520audio-visual%2520hate%2520detection%2520from%2520a%2520binary%2520classification%2520task%2520into%2520a%2520structured%2520reasoning%2520problem.%2520Our%2520approach%2520employs%2520a%2520novel%2520tandem%2520reinforcement%2520learning%2520strategy%2520where%2520vision-language%2520and%2520audio-language%2520models%2520optimize%2520each%2520other%2520through%2520self-constrained%2520cross-modal%2520context%252C%2520stabilizing%2520reasoning%2520over%2520extended%2520temporal%2520sequences%2520without%2520requiring%2520dense%2520frame-level%2520supervision.%2520Experiments%2520across%2520three%2520benchmark%2520datasets%2520demonstrate%2520that%2520TANDEM%2520significantly%2520outperforms%2520zero-shot%2520and%2520context-augmented%2520baselines%252C%2520achieving%25200.73%2520F1%2520in%2520target%2520identification%2520on%2520HateMM%2520%2528a%252030%2525%2520improvement%2520over%2520state-of-the-art%2529%2520while%2520maintaining%2520precise%2520temporal%2520grounding.%2520We%2520further%2520observe%2520that%2520while%2520binary%2520detection%2520is%2520robust%252C%2520differentiating%2520between%2520offensive%2520and%2520hateful%2520content%2520remains%2520challenging%2520in%2520multi-class%2520settings%2520due%2520to%2520inherent%2520label%2520ambiguity%2520and%2520dataset%2520imbalance.%2520More%2520broadly%252C%2520our%2520findings%2520suggest%2520that%2520structured%252C%2520interpretable%2520alignment%2520is%2520achievable%2520even%2520in%2520complex%2520multimodal%2520settings%252C%2520offering%2520a%2520blueprint%2520for%2520the%2520next%2520generation%2520of%2520transparent%2520and%2520actionable%2520online%2520safety%2520moderation%2520tools.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TANDEM%3A%20Temporal-Aware%20Neural%20Detection%20for%20Multimodal%20Hate%20Speech&entry.906535625=Girish%20A.%20Koushik%20and%20Helen%20Treharne%20and%20Diptesh%20Kanojia&entry.1292438233=Social%20media%20platforms%20are%20increasingly%20dominated%20by%20long-form%20multimodal%20content%2C%20where%20harmful%20narratives%20are%20constructed%20through%20a%20complex%20interplay%20of%20audio%2C%20visual%2C%20and%20textual%20cues.%20While%20automated%20systems%20can%20flag%20hate%20speech%20with%20high%20accuracy%2C%20they%20often%20function%20as%20%22black%20boxes%22%20that%20fail%20to%20provide%20the%20granular%2C%20interpretable%20evidence%2C%20such%20as%20precise%20timestamps%20and%20target%20identities%2C%20required%20for%20effective%20human-in-the-loop%20moderation.%20In%20this%20work%2C%20we%20introduce%20TANDEM%2C%20a%20unified%20framework%20that%20transforms%20audio-visual%20hate%20detection%20from%20a%20binary%20classification%20task%20into%20a%20structured%20reasoning%20problem.%20Our%20approach%20employs%20a%20novel%20tandem%20reinforcement%20learning%20strategy%20where%20vision-language%20and%20audio-language%20models%20optimize%20each%20other%20through%20self-constrained%20cross-modal%20context%2C%20stabilizing%20reasoning%20over%20extended%20temporal%20sequences%20without%20requiring%20dense%20frame-level%20supervision.%20Experiments%20across%20three%20benchmark%20datasets%20demonstrate%20that%20TANDEM%20significantly%20outperforms%20zero-shot%20and%20context-augmented%20baselines%2C%20achieving%200.73%20F1%20in%20target%20identification%20on%20HateMM%20%28a%2030%25%20improvement%20over%20state-of-the-art%29%20while%20maintaining%20precise%20temporal%20grounding.%20We%20further%20observe%20that%20while%20binary%20detection%20is%20robust%2C%20differentiating%20between%20offensive%20and%20hateful%20content%20remains%20challenging%20in%20multi-class%20settings%20due%20to%20inherent%20label%20ambiguity%20and%20dataset%20imbalance.%20More%20broadly%2C%20our%20findings%20suggest%20that%20structured%2C%20interpretable%20alignment%20is%20achievable%20even%20in%20complex%20multimodal%20settings%2C%20offering%20a%20blueprint%20for%20the%20next%20generation%20of%20transparent%20and%20actionable%20online%20safety%20moderation%20tools.&entry.1838667208=http%3A//arxiv.org/abs/2601.11178v1&entry.124074799=Read"},
{"title": "Bio-inspired fine-tuning for selective transfer learning in image classification", "author": "Ana Davila and Jacinto Colan and Yasuhisa Hasegawa", "abstract": "Deep learning has significantly advanced image analysis across diverse domains but often depends on large, annotated datasets for success. Transfer learning addresses this challenge by utilizing pre-trained models to tackle new tasks with limited labeled data. However, discrepancies between source and target domains can hinder effective transfer learning. We introduce BioTune, a novel adaptive fine-tuning technique utilizing evolutionary optimization. BioTune enhances transfer learning by optimally choosing which layers to freeze and adjusting learning rates for unfrozen layers. Through extensive evaluation on nine image classification datasets, spanning natural and specialized domains such as medical imaging, BioTune demonstrates superior accuracy and efficiency over state-of-the-art fine-tuning methods, including AutoRGN and LoRA, highlighting its adaptability to various data characteristics and distribution changes. Additionally, BioTune consistently achieves top performance across four different CNN architectures, underscoring its flexibility. Ablation studies provide valuable insights into the impact of BioTune's key components on overall performance. The source code is available at https://github.com/davilac/BioTune.", "link": "http://arxiv.org/abs/2601.11235v1", "date": "2026-01-16", "relevancy": 2.747, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.562}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5568}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bio-inspired%20fine-tuning%20for%20selective%20transfer%20learning%20in%20image%20classification&body=Title%3A%20Bio-inspired%20fine-tuning%20for%20selective%20transfer%20learning%20in%20image%20classification%0AAuthor%3A%20Ana%20Davila%20and%20Jacinto%20Colan%20and%20Yasuhisa%20Hasegawa%0AAbstract%3A%20Deep%20learning%20has%20significantly%20advanced%20image%20analysis%20across%20diverse%20domains%20but%20often%20depends%20on%20large%2C%20annotated%20datasets%20for%20success.%20Transfer%20learning%20addresses%20this%20challenge%20by%20utilizing%20pre-trained%20models%20to%20tackle%20new%20tasks%20with%20limited%20labeled%20data.%20However%2C%20discrepancies%20between%20source%20and%20target%20domains%20can%20hinder%20effective%20transfer%20learning.%20We%20introduce%20BioTune%2C%20a%20novel%20adaptive%20fine-tuning%20technique%20utilizing%20evolutionary%20optimization.%20BioTune%20enhances%20transfer%20learning%20by%20optimally%20choosing%20which%20layers%20to%20freeze%20and%20adjusting%20learning%20rates%20for%20unfrozen%20layers.%20Through%20extensive%20evaluation%20on%20nine%20image%20classification%20datasets%2C%20spanning%20natural%20and%20specialized%20domains%20such%20as%20medical%20imaging%2C%20BioTune%20demonstrates%20superior%20accuracy%20and%20efficiency%20over%20state-of-the-art%20fine-tuning%20methods%2C%20including%20AutoRGN%20and%20LoRA%2C%20highlighting%20its%20adaptability%20to%20various%20data%20characteristics%20and%20distribution%20changes.%20Additionally%2C%20BioTune%20consistently%20achieves%20top%20performance%20across%20four%20different%20CNN%20architectures%2C%20underscoring%20its%20flexibility.%20Ablation%20studies%20provide%20valuable%20insights%20into%20the%20impact%20of%20BioTune%27s%20key%20components%20on%20overall%20performance.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/davilac/BioTune.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBio-inspired%2520fine-tuning%2520for%2520selective%2520transfer%2520learning%2520in%2520image%2520classification%26entry.906535625%3DAna%2520Davila%2520and%2520Jacinto%2520Colan%2520and%2520Yasuhisa%2520Hasegawa%26entry.1292438233%3DDeep%2520learning%2520has%2520significantly%2520advanced%2520image%2520analysis%2520across%2520diverse%2520domains%2520but%2520often%2520depends%2520on%2520large%252C%2520annotated%2520datasets%2520for%2520success.%2520Transfer%2520learning%2520addresses%2520this%2520challenge%2520by%2520utilizing%2520pre-trained%2520models%2520to%2520tackle%2520new%2520tasks%2520with%2520limited%2520labeled%2520data.%2520However%252C%2520discrepancies%2520between%2520source%2520and%2520target%2520domains%2520can%2520hinder%2520effective%2520transfer%2520learning.%2520We%2520introduce%2520BioTune%252C%2520a%2520novel%2520adaptive%2520fine-tuning%2520technique%2520utilizing%2520evolutionary%2520optimization.%2520BioTune%2520enhances%2520transfer%2520learning%2520by%2520optimally%2520choosing%2520which%2520layers%2520to%2520freeze%2520and%2520adjusting%2520learning%2520rates%2520for%2520unfrozen%2520layers.%2520Through%2520extensive%2520evaluation%2520on%2520nine%2520image%2520classification%2520datasets%252C%2520spanning%2520natural%2520and%2520specialized%2520domains%2520such%2520as%2520medical%2520imaging%252C%2520BioTune%2520demonstrates%2520superior%2520accuracy%2520and%2520efficiency%2520over%2520state-of-the-art%2520fine-tuning%2520methods%252C%2520including%2520AutoRGN%2520and%2520LoRA%252C%2520highlighting%2520its%2520adaptability%2520to%2520various%2520data%2520characteristics%2520and%2520distribution%2520changes.%2520Additionally%252C%2520BioTune%2520consistently%2520achieves%2520top%2520performance%2520across%2520four%2520different%2520CNN%2520architectures%252C%2520underscoring%2520its%2520flexibility.%2520Ablation%2520studies%2520provide%2520valuable%2520insights%2520into%2520the%2520impact%2520of%2520BioTune%2527s%2520key%2520components%2520on%2520overall%2520performance.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/davilac/BioTune.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bio-inspired%20fine-tuning%20for%20selective%20transfer%20learning%20in%20image%20classification&entry.906535625=Ana%20Davila%20and%20Jacinto%20Colan%20and%20Yasuhisa%20Hasegawa&entry.1292438233=Deep%20learning%20has%20significantly%20advanced%20image%20analysis%20across%20diverse%20domains%20but%20often%20depends%20on%20large%2C%20annotated%20datasets%20for%20success.%20Transfer%20learning%20addresses%20this%20challenge%20by%20utilizing%20pre-trained%20models%20to%20tackle%20new%20tasks%20with%20limited%20labeled%20data.%20However%2C%20discrepancies%20between%20source%20and%20target%20domains%20can%20hinder%20effective%20transfer%20learning.%20We%20introduce%20BioTune%2C%20a%20novel%20adaptive%20fine-tuning%20technique%20utilizing%20evolutionary%20optimization.%20BioTune%20enhances%20transfer%20learning%20by%20optimally%20choosing%20which%20layers%20to%20freeze%20and%20adjusting%20learning%20rates%20for%20unfrozen%20layers.%20Through%20extensive%20evaluation%20on%20nine%20image%20classification%20datasets%2C%20spanning%20natural%20and%20specialized%20domains%20such%20as%20medical%20imaging%2C%20BioTune%20demonstrates%20superior%20accuracy%20and%20efficiency%20over%20state-of-the-art%20fine-tuning%20methods%2C%20including%20AutoRGN%20and%20LoRA%2C%20highlighting%20its%20adaptability%20to%20various%20data%20characteristics%20and%20distribution%20changes.%20Additionally%2C%20BioTune%20consistently%20achieves%20top%20performance%20across%20four%20different%20CNN%20architectures%2C%20underscoring%20its%20flexibility.%20Ablation%20studies%20provide%20valuable%20insights%20into%20the%20impact%20of%20BioTune%27s%20key%20components%20on%20overall%20performance.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/davilac/BioTune.&entry.1838667208=http%3A//arxiv.org/abs/2601.11235v1&entry.124074799=Read"},
{"title": "Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps", "author": "Xiangjun Gao and Zhensong Zhang and Dave Zhenyu Chen and Songcen Xu and Long Quan and Eduardo P\u00e9rez-Pellitero and Youngkyoon Jang", "abstract": "We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.", "link": "http://arxiv.org/abs/2601.11442v1", "date": "2026-01-16", "relevancy": 2.7429, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Map2Thought%3A%20Explicit%203D%20Spatial%20Reasoning%20via%20Metric%20Cognitive%20Maps&body=Title%3A%20Map2Thought%3A%20Explicit%203D%20Spatial%20Reasoning%20via%20Metric%20Cognitive%20Maps%0AAuthor%3A%20Xiangjun%20Gao%20and%20Zhensong%20Zhang%20and%20Dave%20Zhenyu%20Chen%20and%20Songcen%20Xu%20and%20Long%20Quan%20and%20Eduardo%20P%C3%A9rez-Pellitero%20and%20Youngkyoon%20Jang%0AAbstract%3A%20We%20propose%20Map2Thought%2C%20a%20framework%20that%20enables%20explicit%20and%20interpretable%20spatial%20reasoning%20for%203D%20VLMs.%20The%20framework%20is%20grounded%20in%20two%20key%20components%3A%20Metric%20Cognitive%20Map%20%28Metric-CogMap%29%20and%20Cognitive%20Chain-of-Thought%20%28Cog-CoT%29.%20Metric-CogMap%20provides%20a%20unified%20spatial%20representation%20by%20integrating%20a%20discrete%20grid%20for%20relational%20reasoning%20with%20a%20continuous%2C%20metric-scale%20representation%20for%20precise%20geometric%20understanding.%20Building%20upon%20the%20Metric-CogMap%2C%20Cog-CoT%20performs%20explicit%20geometric%20reasoning%20through%20deterministic%20operations%2C%20including%20vector%20operations%2C%20bounding-box%20distances%2C%20and%20occlusion-aware%20appearance%20order%20cues%2C%20producing%20interpretable%20inference%20traces%20grounded%20in%203D%20structure.%20Experimental%20results%20show%20that%20Map2Thought%20enables%20explainable%203D%20understanding%2C%20achieving%2059.9%25%20accuracy%20using%20only%20half%20the%20supervision%2C%20closely%20matching%20the%2060.9%25%20baseline%20trained%20with%20the%20full%20dataset.%20It%20consistently%20outperforms%20state-of-the-art%20methods%20by%205.3%25%2C%204.8%25%2C%20and%204.0%25%20under%2010%25%2C%2025%25%2C%20and%2050%25%20training%20subsets%2C%20respectively%2C%20on%20the%20VSI-Bench.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMap2Thought%253A%2520Explicit%25203D%2520Spatial%2520Reasoning%2520via%2520Metric%2520Cognitive%2520Maps%26entry.906535625%3DXiangjun%2520Gao%2520and%2520Zhensong%2520Zhang%2520and%2520Dave%2520Zhenyu%2520Chen%2520and%2520Songcen%2520Xu%2520and%2520Long%2520Quan%2520and%2520Eduardo%2520P%25C3%25A9rez-Pellitero%2520and%2520Youngkyoon%2520Jang%26entry.1292438233%3DWe%2520propose%2520Map2Thought%252C%2520a%2520framework%2520that%2520enables%2520explicit%2520and%2520interpretable%2520spatial%2520reasoning%2520for%25203D%2520VLMs.%2520The%2520framework%2520is%2520grounded%2520in%2520two%2520key%2520components%253A%2520Metric%2520Cognitive%2520Map%2520%2528Metric-CogMap%2529%2520and%2520Cognitive%2520Chain-of-Thought%2520%2528Cog-CoT%2529.%2520Metric-CogMap%2520provides%2520a%2520unified%2520spatial%2520representation%2520by%2520integrating%2520a%2520discrete%2520grid%2520for%2520relational%2520reasoning%2520with%2520a%2520continuous%252C%2520metric-scale%2520representation%2520for%2520precise%2520geometric%2520understanding.%2520Building%2520upon%2520the%2520Metric-CogMap%252C%2520Cog-CoT%2520performs%2520explicit%2520geometric%2520reasoning%2520through%2520deterministic%2520operations%252C%2520including%2520vector%2520operations%252C%2520bounding-box%2520distances%252C%2520and%2520occlusion-aware%2520appearance%2520order%2520cues%252C%2520producing%2520interpretable%2520inference%2520traces%2520grounded%2520in%25203D%2520structure.%2520Experimental%2520results%2520show%2520that%2520Map2Thought%2520enables%2520explainable%25203D%2520understanding%252C%2520achieving%252059.9%2525%2520accuracy%2520using%2520only%2520half%2520the%2520supervision%252C%2520closely%2520matching%2520the%252060.9%2525%2520baseline%2520trained%2520with%2520the%2520full%2520dataset.%2520It%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520by%25205.3%2525%252C%25204.8%2525%252C%2520and%25204.0%2525%2520under%252010%2525%252C%252025%2525%252C%2520and%252050%2525%2520training%2520subsets%252C%2520respectively%252C%2520on%2520the%2520VSI-Bench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Map2Thought%3A%20Explicit%203D%20Spatial%20Reasoning%20via%20Metric%20Cognitive%20Maps&entry.906535625=Xiangjun%20Gao%20and%20Zhensong%20Zhang%20and%20Dave%20Zhenyu%20Chen%20and%20Songcen%20Xu%20and%20Long%20Quan%20and%20Eduardo%20P%C3%A9rez-Pellitero%20and%20Youngkyoon%20Jang&entry.1292438233=We%20propose%20Map2Thought%2C%20a%20framework%20that%20enables%20explicit%20and%20interpretable%20spatial%20reasoning%20for%203D%20VLMs.%20The%20framework%20is%20grounded%20in%20two%20key%20components%3A%20Metric%20Cognitive%20Map%20%28Metric-CogMap%29%20and%20Cognitive%20Chain-of-Thought%20%28Cog-CoT%29.%20Metric-CogMap%20provides%20a%20unified%20spatial%20representation%20by%20integrating%20a%20discrete%20grid%20for%20relational%20reasoning%20with%20a%20continuous%2C%20metric-scale%20representation%20for%20precise%20geometric%20understanding.%20Building%20upon%20the%20Metric-CogMap%2C%20Cog-CoT%20performs%20explicit%20geometric%20reasoning%20through%20deterministic%20operations%2C%20including%20vector%20operations%2C%20bounding-box%20distances%2C%20and%20occlusion-aware%20appearance%20order%20cues%2C%20producing%20interpretable%20inference%20traces%20grounded%20in%203D%20structure.%20Experimental%20results%20show%20that%20Map2Thought%20enables%20explainable%203D%20understanding%2C%20achieving%2059.9%25%20accuracy%20using%20only%20half%20the%20supervision%2C%20closely%20matching%20the%2060.9%25%20baseline%20trained%20with%20the%20full%20dataset.%20It%20consistently%20outperforms%20state-of-the-art%20methods%20by%205.3%25%2C%204.8%25%2C%20and%204.0%25%20under%2010%25%2C%2025%25%2C%20and%2050%25%20training%20subsets%2C%20respectively%2C%20on%20the%20VSI-Bench.&entry.1838667208=http%3A//arxiv.org/abs/2601.11442v1&entry.124074799=Read"},
{"title": "Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model", "author": "Shuai Yuan and Tianwu Lin and Shuang Chen and Yu Xia and Peng Qin and Xiangyu Liu and Xiaoqing Xu and Nan Xu and Hongsheng Zhang and Jie Wang and Peng Gong", "abstract": "Accurate wetland mapping is essential for ecosystem monitoring, yet dense pixel-level annotation is prohibitively expensive and practical applications usually rely on sparse point labels, under which existing deep learning models perform poorly, while strong seasonal and inter-annual wetland dynamics further render single-date imagery inadequate and lead to significant mapping errors; although foundation models such as SAM show promising generalization from point prompts, they are inherently designed for static images and fail to model temporal information, resulting in fragmented masks in heterogeneous wetlands. To overcome these limitations, we propose WetSAM, a SAM-based framework that integrates satellite image time series for wetland mapping from sparse point supervision through a dual-branch design, where a temporally prompted branch extends SAM with hierarchical adapters and dynamic temporal aggregation to disentangle wetland characteristics from phenological variability, and a spatial branch employs a temporally constrained region-growing strategy to generate reliable dense pseudo-labels, while a bidirectional consistency regularization jointly optimizes both branches. Extensive experiments across eight global regions of approximately 5,000 km2 each demonstrate that WetSAM substantially outperforms state-of-the-art methods, achieving an average F1-score of 85.58%, and delivering accurate and structurally consistent wetland segmentation with minimal labeling effort, highlighting its strong generalization capability and potential for scalable, low-cost, high-resolution wetland mapping.", "link": "http://arxiv.org/abs/2601.11400v1", "date": "2026-01-16", "relevancy": 2.7089, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5667}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.537}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wetland%20mapping%20from%20sparse%20annotations%20with%20satellite%20image%20time%20series%20and%20temporal-aware%20segment%20anything%20model&body=Title%3A%20Wetland%20mapping%20from%20sparse%20annotations%20with%20satellite%20image%20time%20series%20and%20temporal-aware%20segment%20anything%20model%0AAuthor%3A%20Shuai%20Yuan%20and%20Tianwu%20Lin%20and%20Shuang%20Chen%20and%20Yu%20Xia%20and%20Peng%20Qin%20and%20Xiangyu%20Liu%20and%20Xiaoqing%20Xu%20and%20Nan%20Xu%20and%20Hongsheng%20Zhang%20and%20Jie%20Wang%20and%20Peng%20Gong%0AAbstract%3A%20Accurate%20wetland%20mapping%20is%20essential%20for%20ecosystem%20monitoring%2C%20yet%20dense%20pixel-level%20annotation%20is%20prohibitively%20expensive%20and%20practical%20applications%20usually%20rely%20on%20sparse%20point%20labels%2C%20under%20which%20existing%20deep%20learning%20models%20perform%20poorly%2C%20while%20strong%20seasonal%20and%20inter-annual%20wetland%20dynamics%20further%20render%20single-date%20imagery%20inadequate%20and%20lead%20to%20significant%20mapping%20errors%3B%20although%20foundation%20models%20such%20as%20SAM%20show%20promising%20generalization%20from%20point%20prompts%2C%20they%20are%20inherently%20designed%20for%20static%20images%20and%20fail%20to%20model%20temporal%20information%2C%20resulting%20in%20fragmented%20masks%20in%20heterogeneous%20wetlands.%20To%20overcome%20these%20limitations%2C%20we%20propose%20WetSAM%2C%20a%20SAM-based%20framework%20that%20integrates%20satellite%20image%20time%20series%20for%20wetland%20mapping%20from%20sparse%20point%20supervision%20through%20a%20dual-branch%20design%2C%20where%20a%20temporally%20prompted%20branch%20extends%20SAM%20with%20hierarchical%20adapters%20and%20dynamic%20temporal%20aggregation%20to%20disentangle%20wetland%20characteristics%20from%20phenological%20variability%2C%20and%20a%20spatial%20branch%20employs%20a%20temporally%20constrained%20region-growing%20strategy%20to%20generate%20reliable%20dense%20pseudo-labels%2C%20while%20a%20bidirectional%20consistency%20regularization%20jointly%20optimizes%20both%20branches.%20Extensive%20experiments%20across%20eight%20global%20regions%20of%20approximately%205%2C000%20km2%20each%20demonstrate%20that%20WetSAM%20substantially%20outperforms%20state-of-the-art%20methods%2C%20achieving%20an%20average%20F1-score%20of%2085.58%25%2C%20and%20delivering%20accurate%20and%20structurally%20consistent%20wetland%20segmentation%20with%20minimal%20labeling%20effort%2C%20highlighting%20its%20strong%20generalization%20capability%20and%20potential%20for%20scalable%2C%20low-cost%2C%20high-resolution%20wetland%20mapping.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWetland%2520mapping%2520from%2520sparse%2520annotations%2520with%2520satellite%2520image%2520time%2520series%2520and%2520temporal-aware%2520segment%2520anything%2520model%26entry.906535625%3DShuai%2520Yuan%2520and%2520Tianwu%2520Lin%2520and%2520Shuang%2520Chen%2520and%2520Yu%2520Xia%2520and%2520Peng%2520Qin%2520and%2520Xiangyu%2520Liu%2520and%2520Xiaoqing%2520Xu%2520and%2520Nan%2520Xu%2520and%2520Hongsheng%2520Zhang%2520and%2520Jie%2520Wang%2520and%2520Peng%2520Gong%26entry.1292438233%3DAccurate%2520wetland%2520mapping%2520is%2520essential%2520for%2520ecosystem%2520monitoring%252C%2520yet%2520dense%2520pixel-level%2520annotation%2520is%2520prohibitively%2520expensive%2520and%2520practical%2520applications%2520usually%2520rely%2520on%2520sparse%2520point%2520labels%252C%2520under%2520which%2520existing%2520deep%2520learning%2520models%2520perform%2520poorly%252C%2520while%2520strong%2520seasonal%2520and%2520inter-annual%2520wetland%2520dynamics%2520further%2520render%2520single-date%2520imagery%2520inadequate%2520and%2520lead%2520to%2520significant%2520mapping%2520errors%253B%2520although%2520foundation%2520models%2520such%2520as%2520SAM%2520show%2520promising%2520generalization%2520from%2520point%2520prompts%252C%2520they%2520are%2520inherently%2520designed%2520for%2520static%2520images%2520and%2520fail%2520to%2520model%2520temporal%2520information%252C%2520resulting%2520in%2520fragmented%2520masks%2520in%2520heterogeneous%2520wetlands.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520WetSAM%252C%2520a%2520SAM-based%2520framework%2520that%2520integrates%2520satellite%2520image%2520time%2520series%2520for%2520wetland%2520mapping%2520from%2520sparse%2520point%2520supervision%2520through%2520a%2520dual-branch%2520design%252C%2520where%2520a%2520temporally%2520prompted%2520branch%2520extends%2520SAM%2520with%2520hierarchical%2520adapters%2520and%2520dynamic%2520temporal%2520aggregation%2520to%2520disentangle%2520wetland%2520characteristics%2520from%2520phenological%2520variability%252C%2520and%2520a%2520spatial%2520branch%2520employs%2520a%2520temporally%2520constrained%2520region-growing%2520strategy%2520to%2520generate%2520reliable%2520dense%2520pseudo-labels%252C%2520while%2520a%2520bidirectional%2520consistency%2520regularization%2520jointly%2520optimizes%2520both%2520branches.%2520Extensive%2520experiments%2520across%2520eight%2520global%2520regions%2520of%2520approximately%25205%252C000%2520km2%2520each%2520demonstrate%2520that%2520WetSAM%2520substantially%2520outperforms%2520state-of-the-art%2520methods%252C%2520achieving%2520an%2520average%2520F1-score%2520of%252085.58%2525%252C%2520and%2520delivering%2520accurate%2520and%2520structurally%2520consistent%2520wetland%2520segmentation%2520with%2520minimal%2520labeling%2520effort%252C%2520highlighting%2520its%2520strong%2520generalization%2520capability%2520and%2520potential%2520for%2520scalable%252C%2520low-cost%252C%2520high-resolution%2520wetland%2520mapping.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wetland%20mapping%20from%20sparse%20annotations%20with%20satellite%20image%20time%20series%20and%20temporal-aware%20segment%20anything%20model&entry.906535625=Shuai%20Yuan%20and%20Tianwu%20Lin%20and%20Shuang%20Chen%20and%20Yu%20Xia%20and%20Peng%20Qin%20and%20Xiangyu%20Liu%20and%20Xiaoqing%20Xu%20and%20Nan%20Xu%20and%20Hongsheng%20Zhang%20and%20Jie%20Wang%20and%20Peng%20Gong&entry.1292438233=Accurate%20wetland%20mapping%20is%20essential%20for%20ecosystem%20monitoring%2C%20yet%20dense%20pixel-level%20annotation%20is%20prohibitively%20expensive%20and%20practical%20applications%20usually%20rely%20on%20sparse%20point%20labels%2C%20under%20which%20existing%20deep%20learning%20models%20perform%20poorly%2C%20while%20strong%20seasonal%20and%20inter-annual%20wetland%20dynamics%20further%20render%20single-date%20imagery%20inadequate%20and%20lead%20to%20significant%20mapping%20errors%3B%20although%20foundation%20models%20such%20as%20SAM%20show%20promising%20generalization%20from%20point%20prompts%2C%20they%20are%20inherently%20designed%20for%20static%20images%20and%20fail%20to%20model%20temporal%20information%2C%20resulting%20in%20fragmented%20masks%20in%20heterogeneous%20wetlands.%20To%20overcome%20these%20limitations%2C%20we%20propose%20WetSAM%2C%20a%20SAM-based%20framework%20that%20integrates%20satellite%20image%20time%20series%20for%20wetland%20mapping%20from%20sparse%20point%20supervision%20through%20a%20dual-branch%20design%2C%20where%20a%20temporally%20prompted%20branch%20extends%20SAM%20with%20hierarchical%20adapters%20and%20dynamic%20temporal%20aggregation%20to%20disentangle%20wetland%20characteristics%20from%20phenological%20variability%2C%20and%20a%20spatial%20branch%20employs%20a%20temporally%20constrained%20region-growing%20strategy%20to%20generate%20reliable%20dense%20pseudo-labels%2C%20while%20a%20bidirectional%20consistency%20regularization%20jointly%20optimizes%20both%20branches.%20Extensive%20experiments%20across%20eight%20global%20regions%20of%20approximately%205%2C000%20km2%20each%20demonstrate%20that%20WetSAM%20substantially%20outperforms%20state-of-the-art%20methods%2C%20achieving%20an%20average%20F1-score%20of%2085.58%25%2C%20and%20delivering%20accurate%20and%20structurally%20consistent%20wetland%20segmentation%20with%20minimal%20labeling%20effort%2C%20highlighting%20its%20strong%20generalization%20capability%20and%20potential%20for%20scalable%2C%20low-cost%2C%20high-resolution%20wetland%20mapping.&entry.1838667208=http%3A//arxiv.org/abs/2601.11400v1&entry.124074799=Read"},
{"title": "PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs", "author": "Oishee Bintey Hoque and Nibir Chandra Mandal and Kyle Luong and Amanda Wilson and Samarth Swarup and Madhav Marathe and Abhijin Adiga", "abstract": "Large-scale livestock operations pose significant risks to human health and the environment, while also being vulnerable to threats such as infectious diseases and extreme weather events. As the number of such operations continues to grow, accurate and scalable mapping has become increasingly important. In this work, we present an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery. Our method (1) detects candidate infrastructure (e.g., barns, feedlots, manure lagoons, silos) with a domain-tuned YOLOv8 detector, then derives SAM2 masks from these boxes and filters component-specific criteria, (2) extracts structured descriptors (e.g., counts, areas, orientations, and spatial relations) and fuses them with deep visual features using a lightweight spatial cross-attention classifier, and (3) outputs both CAFO type predictions and mask-level attributions that link decisions to visible infrastructure. Through comprehensive evaluation, we show that our approach achieves state-of-the-art performance, with Swin-B+PRISM-CAFO surpassing the best performing baseline by up to 15\\%. Beyond strong predictive performance across diverse U.S. regions, we run systematic gradient--activation analyses that quantify the impact of domain priors and show ho", "link": "http://arxiv.org/abs/2601.11451v1", "date": "2026-01-16", "relevancy": 2.7084, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5745}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5253}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRISM-CAFO%3A%20Prior-conditioned%20Remote-sensing%20Infrastructure%20Segmentation%20and%20Mapping%20for%20CAFOs&body=Title%3A%20PRISM-CAFO%3A%20Prior-conditioned%20Remote-sensing%20Infrastructure%20Segmentation%20and%20Mapping%20for%20CAFOs%0AAuthor%3A%20Oishee%20Bintey%20Hoque%20and%20Nibir%20Chandra%20Mandal%20and%20Kyle%20Luong%20and%20Amanda%20Wilson%20and%20Samarth%20Swarup%20and%20Madhav%20Marathe%20and%20Abhijin%20Adiga%0AAbstract%3A%20Large-scale%20livestock%20operations%20pose%20significant%20risks%20to%20human%20health%20and%20the%20environment%2C%20while%20also%20being%20vulnerable%20to%20threats%20such%20as%20infectious%20diseases%20and%20extreme%20weather%20events.%20As%20the%20number%20of%20such%20operations%20continues%20to%20grow%2C%20accurate%20and%20scalable%20mapping%20has%20become%20increasingly%20important.%20In%20this%20work%2C%20we%20present%20an%20infrastructure-first%2C%20explainable%20pipeline%20for%20identifying%20and%20characterizing%20Concentrated%20Animal%20Feeding%20Operations%20%28CAFOs%29%20from%20aerial%20and%20satellite%20imagery.%20Our%20method%20%281%29%20detects%20candidate%20infrastructure%20%28e.g.%2C%20barns%2C%20feedlots%2C%20manure%20lagoons%2C%20silos%29%20with%20a%20domain-tuned%20YOLOv8%20detector%2C%20then%20derives%20SAM2%20masks%20from%20these%20boxes%20and%20filters%20component-specific%20criteria%2C%20%282%29%20extracts%20structured%20descriptors%20%28e.g.%2C%20counts%2C%20areas%2C%20orientations%2C%20and%20spatial%20relations%29%20and%20fuses%20them%20with%20deep%20visual%20features%20using%20a%20lightweight%20spatial%20cross-attention%20classifier%2C%20and%20%283%29%20outputs%20both%20CAFO%20type%20predictions%20and%20mask-level%20attributions%20that%20link%20decisions%20to%20visible%20infrastructure.%20Through%20comprehensive%20evaluation%2C%20we%20show%20that%20our%20approach%20achieves%20state-of-the-art%20performance%2C%20with%20Swin-B%2BPRISM-CAFO%20surpassing%20the%20best%20performing%20baseline%20by%20up%20to%2015%5C%25.%20Beyond%20strong%20predictive%20performance%20across%20diverse%20U.S.%20regions%2C%20we%20run%20systematic%20gradient--activation%20analyses%20that%20quantify%20the%20impact%20of%20domain%20priors%20and%20show%20ho%0ALink%3A%20http%3A//arxiv.org/abs/2601.11451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRISM-CAFO%253A%2520Prior-conditioned%2520Remote-sensing%2520Infrastructure%2520Segmentation%2520and%2520Mapping%2520for%2520CAFOs%26entry.906535625%3DOishee%2520Bintey%2520Hoque%2520and%2520Nibir%2520Chandra%2520Mandal%2520and%2520Kyle%2520Luong%2520and%2520Amanda%2520Wilson%2520and%2520Samarth%2520Swarup%2520and%2520Madhav%2520Marathe%2520and%2520Abhijin%2520Adiga%26entry.1292438233%3DLarge-scale%2520livestock%2520operations%2520pose%2520significant%2520risks%2520to%2520human%2520health%2520and%2520the%2520environment%252C%2520while%2520also%2520being%2520vulnerable%2520to%2520threats%2520such%2520as%2520infectious%2520diseases%2520and%2520extreme%2520weather%2520events.%2520As%2520the%2520number%2520of%2520such%2520operations%2520continues%2520to%2520grow%252C%2520accurate%2520and%2520scalable%2520mapping%2520has%2520become%2520increasingly%2520important.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520infrastructure-first%252C%2520explainable%2520pipeline%2520for%2520identifying%2520and%2520characterizing%2520Concentrated%2520Animal%2520Feeding%2520Operations%2520%2528CAFOs%2529%2520from%2520aerial%2520and%2520satellite%2520imagery.%2520Our%2520method%2520%25281%2529%2520detects%2520candidate%2520infrastructure%2520%2528e.g.%252C%2520barns%252C%2520feedlots%252C%2520manure%2520lagoons%252C%2520silos%2529%2520with%2520a%2520domain-tuned%2520YOLOv8%2520detector%252C%2520then%2520derives%2520SAM2%2520masks%2520from%2520these%2520boxes%2520and%2520filters%2520component-specific%2520criteria%252C%2520%25282%2529%2520extracts%2520structured%2520descriptors%2520%2528e.g.%252C%2520counts%252C%2520areas%252C%2520orientations%252C%2520and%2520spatial%2520relations%2529%2520and%2520fuses%2520them%2520with%2520deep%2520visual%2520features%2520using%2520a%2520lightweight%2520spatial%2520cross-attention%2520classifier%252C%2520and%2520%25283%2529%2520outputs%2520both%2520CAFO%2520type%2520predictions%2520and%2520mask-level%2520attributions%2520that%2520link%2520decisions%2520to%2520visible%2520infrastructure.%2520Through%2520comprehensive%2520evaluation%252C%2520we%2520show%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%252C%2520with%2520Swin-B%252BPRISM-CAFO%2520surpassing%2520the%2520best%2520performing%2520baseline%2520by%2520up%2520to%252015%255C%2525.%2520Beyond%2520strong%2520predictive%2520performance%2520across%2520diverse%2520U.S.%2520regions%252C%2520we%2520run%2520systematic%2520gradient--activation%2520analyses%2520that%2520quantify%2520the%2520impact%2520of%2520domain%2520priors%2520and%2520show%2520ho%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRISM-CAFO%3A%20Prior-conditioned%20Remote-sensing%20Infrastructure%20Segmentation%20and%20Mapping%20for%20CAFOs&entry.906535625=Oishee%20Bintey%20Hoque%20and%20Nibir%20Chandra%20Mandal%20and%20Kyle%20Luong%20and%20Amanda%20Wilson%20and%20Samarth%20Swarup%20and%20Madhav%20Marathe%20and%20Abhijin%20Adiga&entry.1292438233=Large-scale%20livestock%20operations%20pose%20significant%20risks%20to%20human%20health%20and%20the%20environment%2C%20while%20also%20being%20vulnerable%20to%20threats%20such%20as%20infectious%20diseases%20and%20extreme%20weather%20events.%20As%20the%20number%20of%20such%20operations%20continues%20to%20grow%2C%20accurate%20and%20scalable%20mapping%20has%20become%20increasingly%20important.%20In%20this%20work%2C%20we%20present%20an%20infrastructure-first%2C%20explainable%20pipeline%20for%20identifying%20and%20characterizing%20Concentrated%20Animal%20Feeding%20Operations%20%28CAFOs%29%20from%20aerial%20and%20satellite%20imagery.%20Our%20method%20%281%29%20detects%20candidate%20infrastructure%20%28e.g.%2C%20barns%2C%20feedlots%2C%20manure%20lagoons%2C%20silos%29%20with%20a%20domain-tuned%20YOLOv8%20detector%2C%20then%20derives%20SAM2%20masks%20from%20these%20boxes%20and%20filters%20component-specific%20criteria%2C%20%282%29%20extracts%20structured%20descriptors%20%28e.g.%2C%20counts%2C%20areas%2C%20orientations%2C%20and%20spatial%20relations%29%20and%20fuses%20them%20with%20deep%20visual%20features%20using%20a%20lightweight%20spatial%20cross-attention%20classifier%2C%20and%20%283%29%20outputs%20both%20CAFO%20type%20predictions%20and%20mask-level%20attributions%20that%20link%20decisions%20to%20visible%20infrastructure.%20Through%20comprehensive%20evaluation%2C%20we%20show%20that%20our%20approach%20achieves%20state-of-the-art%20performance%2C%20with%20Swin-B%2BPRISM-CAFO%20surpassing%20the%20best%20performing%20baseline%20by%20up%20to%2015%5C%25.%20Beyond%20strong%20predictive%20performance%20across%20diverse%20U.S.%20regions%2C%20we%20run%20systematic%20gradient--activation%20analyses%20that%20quantify%20the%20impact%20of%20domain%20priors%20and%20show%20ho&entry.1838667208=http%3A//arxiv.org/abs/2601.11451v1&entry.124074799=Read"},
{"title": "TimeMar: Multi-Scale Autoregressive Modeling for Unconditional Time Series Generation", "author": "Xiangyu Xu and Qingsong Zhong and Jilin Hu", "abstract": "Generative modeling offers a promising solution to data scarcity and privacy challenges in time series analysis. However, the structural complexity of time series, characterized by multi-scale temporal patterns and heterogeneous components, remains insufficiently addressed. In this work, we propose a structure-disentangled multiscale generation framework for time series. Our approach encodes sequences into discrete tokens at multiple temporal resolutions and performs autoregressive generation in a coarse-to-fine manner, thereby preserving hierarchical dependencies. To tackle structural heterogeneity, we introduce a dual-path VQ-VAE that disentangles trend and seasonal components, enabling the learning of semantically consistent latent representations. Additionally, we present a guidance-based reconstruction strategy, where coarse seasonal signals are utilized as priors to guide the reconstruction of fine-grained seasonal patterns. Experiments on six datasets show that our approach produces higher-quality time series than existing methods. Notably, our model achieves strong performance with a significantly reduced parameter count and exhibits superior capability in generating high-quality long-term sequences. Our implementation is available at https://anonymous.4open.science/r/TimeMAR-BC5B.", "link": "http://arxiv.org/abs/2601.11184v1", "date": "2026-01-16", "relevancy": 2.6361, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5402}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5221}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeMar%3A%20Multi-Scale%20Autoregressive%20Modeling%20for%20Unconditional%20Time%20Series%20Generation&body=Title%3A%20TimeMar%3A%20Multi-Scale%20Autoregressive%20Modeling%20for%20Unconditional%20Time%20Series%20Generation%0AAuthor%3A%20Xiangyu%20Xu%20and%20Qingsong%20Zhong%20and%20Jilin%20Hu%0AAbstract%3A%20Generative%20modeling%20offers%20a%20promising%20solution%20to%20data%20scarcity%20and%20privacy%20challenges%20in%20time%20series%20analysis.%20However%2C%20the%20structural%20complexity%20of%20time%20series%2C%20characterized%20by%20multi-scale%20temporal%20patterns%20and%20heterogeneous%20components%2C%20remains%20insufficiently%20addressed.%20In%20this%20work%2C%20we%20propose%20a%20structure-disentangled%20multiscale%20generation%20framework%20for%20time%20series.%20Our%20approach%20encodes%20sequences%20into%20discrete%20tokens%20at%20multiple%20temporal%20resolutions%20and%20performs%20autoregressive%20generation%20in%20a%20coarse-to-fine%20manner%2C%20thereby%20preserving%20hierarchical%20dependencies.%20To%20tackle%20structural%20heterogeneity%2C%20we%20introduce%20a%20dual-path%20VQ-VAE%20that%20disentangles%20trend%20and%20seasonal%20components%2C%20enabling%20the%20learning%20of%20semantically%20consistent%20latent%20representations.%20Additionally%2C%20we%20present%20a%20guidance-based%20reconstruction%20strategy%2C%20where%20coarse%20seasonal%20signals%20are%20utilized%20as%20priors%20to%20guide%20the%20reconstruction%20of%20fine-grained%20seasonal%20patterns.%20Experiments%20on%20six%20datasets%20show%20that%20our%20approach%20produces%20higher-quality%20time%20series%20than%20existing%20methods.%20Notably%2C%20our%20model%20achieves%20strong%20performance%20with%20a%20significantly%20reduced%20parameter%20count%20and%20exhibits%20superior%20capability%20in%20generating%20high-quality%20long-term%20sequences.%20Our%20implementation%20is%20available%20at%20https%3A//anonymous.4open.science/r/TimeMAR-BC5B.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeMar%253A%2520Multi-Scale%2520Autoregressive%2520Modeling%2520for%2520Unconditional%2520Time%2520Series%2520Generation%26entry.906535625%3DXiangyu%2520Xu%2520and%2520Qingsong%2520Zhong%2520and%2520Jilin%2520Hu%26entry.1292438233%3DGenerative%2520modeling%2520offers%2520a%2520promising%2520solution%2520to%2520data%2520scarcity%2520and%2520privacy%2520challenges%2520in%2520time%2520series%2520analysis.%2520However%252C%2520the%2520structural%2520complexity%2520of%2520time%2520series%252C%2520characterized%2520by%2520multi-scale%2520temporal%2520patterns%2520and%2520heterogeneous%2520components%252C%2520remains%2520insufficiently%2520addressed.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520structure-disentangled%2520multiscale%2520generation%2520framework%2520for%2520time%2520series.%2520Our%2520approach%2520encodes%2520sequences%2520into%2520discrete%2520tokens%2520at%2520multiple%2520temporal%2520resolutions%2520and%2520performs%2520autoregressive%2520generation%2520in%2520a%2520coarse-to-fine%2520manner%252C%2520thereby%2520preserving%2520hierarchical%2520dependencies.%2520To%2520tackle%2520structural%2520heterogeneity%252C%2520we%2520introduce%2520a%2520dual-path%2520VQ-VAE%2520that%2520disentangles%2520trend%2520and%2520seasonal%2520components%252C%2520enabling%2520the%2520learning%2520of%2520semantically%2520consistent%2520latent%2520representations.%2520Additionally%252C%2520we%2520present%2520a%2520guidance-based%2520reconstruction%2520strategy%252C%2520where%2520coarse%2520seasonal%2520signals%2520are%2520utilized%2520as%2520priors%2520to%2520guide%2520the%2520reconstruction%2520of%2520fine-grained%2520seasonal%2520patterns.%2520Experiments%2520on%2520six%2520datasets%2520show%2520that%2520our%2520approach%2520produces%2520higher-quality%2520time%2520series%2520than%2520existing%2520methods.%2520Notably%252C%2520our%2520model%2520achieves%2520strong%2520performance%2520with%2520a%2520significantly%2520reduced%2520parameter%2520count%2520and%2520exhibits%2520superior%2520capability%2520in%2520generating%2520high-quality%2520long-term%2520sequences.%2520Our%2520implementation%2520is%2520available%2520at%2520https%253A//anonymous.4open.science/r/TimeMAR-BC5B.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeMar%3A%20Multi-Scale%20Autoregressive%20Modeling%20for%20Unconditional%20Time%20Series%20Generation&entry.906535625=Xiangyu%20Xu%20and%20Qingsong%20Zhong%20and%20Jilin%20Hu&entry.1292438233=Generative%20modeling%20offers%20a%20promising%20solution%20to%20data%20scarcity%20and%20privacy%20challenges%20in%20time%20series%20analysis.%20However%2C%20the%20structural%20complexity%20of%20time%20series%2C%20characterized%20by%20multi-scale%20temporal%20patterns%20and%20heterogeneous%20components%2C%20remains%20insufficiently%20addressed.%20In%20this%20work%2C%20we%20propose%20a%20structure-disentangled%20multiscale%20generation%20framework%20for%20time%20series.%20Our%20approach%20encodes%20sequences%20into%20discrete%20tokens%20at%20multiple%20temporal%20resolutions%20and%20performs%20autoregressive%20generation%20in%20a%20coarse-to-fine%20manner%2C%20thereby%20preserving%20hierarchical%20dependencies.%20To%20tackle%20structural%20heterogeneity%2C%20we%20introduce%20a%20dual-path%20VQ-VAE%20that%20disentangles%20trend%20and%20seasonal%20components%2C%20enabling%20the%20learning%20of%20semantically%20consistent%20latent%20representations.%20Additionally%2C%20we%20present%20a%20guidance-based%20reconstruction%20strategy%2C%20where%20coarse%20seasonal%20signals%20are%20utilized%20as%20priors%20to%20guide%20the%20reconstruction%20of%20fine-grained%20seasonal%20patterns.%20Experiments%20on%20six%20datasets%20show%20that%20our%20approach%20produces%20higher-quality%20time%20series%20than%20existing%20methods.%20Notably%2C%20our%20model%20achieves%20strong%20performance%20with%20a%20significantly%20reduced%20parameter%20count%20and%20exhibits%20superior%20capability%20in%20generating%20high-quality%20long-term%20sequences.%20Our%20implementation%20is%20available%20at%20https%3A//anonymous.4open.science/r/TimeMAR-BC5B.&entry.1838667208=http%3A//arxiv.org/abs/2601.11184v1&entry.124074799=Read"},
{"title": "Multi-Receptive Field Ensemble with Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection", "author": "Humza Naveed and Xina Zeng and Mitch Bryson and Nagita Mehrseresht", "abstract": "Remote sensing change detection (RSCD) is a complex task, where changes often appear at different scales and orientations. Convolutional neural networks (CNNs) are good at capturing local spatial patterns but cannot model global semantics due to limited receptive fields. Alternatively, transformers can model long-range dependencies but are data hungry, and RSCD datasets are not large enough to train these models effectively. To tackle this, this paper presents a new architecture for RSCD which adapts a segment anything (SAM) vision foundation model and processes features from the SAM encoder through a multi-receptive field ensemble to capture local and global change patterns. We propose an ensemble of spatial-temporal feature enhancement (STFE) to capture cross-temporal relations, a decoder to reconstruct change patterns, and a multi-scale decoder fusion with attention (MSDFA) to fuse multi-scale decoder information and highlight key change patterns. Each branch in an ensemble operates on a separate receptive field to capture finer-to-coarser level details. Additionally, we propose a novel cross-entropy masking (CEM) loss to handle class-imbalance in RSCD datasets. Our work outperforms state-of-the-art (SOTA) methods on four change detection datasets, Levir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.97\\% F1-score improvement on a complex S2Looking dataset. The code is available at: https://github.com/humza909/SAM-ECEM", "link": "http://arxiv.org/abs/2508.10568v2", "date": "2026-01-16", "relevancy": 2.6084, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5211}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Receptive%20Field%20Ensemble%20with%20Cross-Entropy%20Masking%20for%20Class%20Imbalance%20in%20Remote%20Sensing%20Change%20Detection&body=Title%3A%20Multi-Receptive%20Field%20Ensemble%20with%20Cross-Entropy%20Masking%20for%20Class%20Imbalance%20in%20Remote%20Sensing%20Change%20Detection%0AAuthor%3A%20Humza%20Naveed%20and%20Xina%20Zeng%20and%20Mitch%20Bryson%20and%20Nagita%20Mehrseresht%0AAbstract%3A%20Remote%20sensing%20change%20detection%20%28RSCD%29%20is%20a%20complex%20task%2C%20where%20changes%20often%20appear%20at%20different%20scales%20and%20orientations.%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20good%20at%20capturing%20local%20spatial%20patterns%20but%20cannot%20model%20global%20semantics%20due%20to%20limited%20receptive%20fields.%20Alternatively%2C%20transformers%20can%20model%20long-range%20dependencies%20but%20are%20data%20hungry%2C%20and%20RSCD%20datasets%20are%20not%20large%20enough%20to%20train%20these%20models%20effectively.%20To%20tackle%20this%2C%20this%20paper%20presents%20a%20new%20architecture%20for%20RSCD%20which%20adapts%20a%20segment%20anything%20%28SAM%29%20vision%20foundation%20model%20and%20processes%20features%20from%20the%20SAM%20encoder%20through%20a%20multi-receptive%20field%20ensemble%20to%20capture%20local%20and%20global%20change%20patterns.%20We%20propose%20an%20ensemble%20of%20spatial-temporal%20feature%20enhancement%20%28STFE%29%20to%20capture%20cross-temporal%20relations%2C%20a%20decoder%20to%20reconstruct%20change%20patterns%2C%20and%20a%20multi-scale%20decoder%20fusion%20with%20attention%20%28MSDFA%29%20to%20fuse%20multi-scale%20decoder%20information%20and%20highlight%20key%20change%20patterns.%20Each%20branch%20in%20an%20ensemble%20operates%20on%20a%20separate%20receptive%20field%20to%20capture%20finer-to-coarser%20level%20details.%20Additionally%2C%20we%20propose%20a%20novel%20cross-entropy%20masking%20%28CEM%29%20loss%20to%20handle%20class-imbalance%20in%20RSCD%20datasets.%20Our%20work%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%20on%20four%20change%20detection%20datasets%2C%20Levir-CD%2C%20WHU-CD%2C%20CLCD%2C%20and%20S2Looking.%20We%20achieved%202.97%5C%25%20F1-score%20improvement%20on%20a%20complex%20S2Looking%20dataset.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/humza909/SAM-ECEM%0ALink%3A%20http%3A//arxiv.org/abs/2508.10568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Receptive%2520Field%2520Ensemble%2520with%2520Cross-Entropy%2520Masking%2520for%2520Class%2520Imbalance%2520in%2520Remote%2520Sensing%2520Change%2520Detection%26entry.906535625%3DHumza%2520Naveed%2520and%2520Xina%2520Zeng%2520and%2520Mitch%2520Bryson%2520and%2520Nagita%2520Mehrseresht%26entry.1292438233%3DRemote%2520sensing%2520change%2520detection%2520%2528RSCD%2529%2520is%2520a%2520complex%2520task%252C%2520where%2520changes%2520often%2520appear%2520at%2520different%2520scales%2520and%2520orientations.%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520are%2520good%2520at%2520capturing%2520local%2520spatial%2520patterns%2520but%2520cannot%2520model%2520global%2520semantics%2520due%2520to%2520limited%2520receptive%2520fields.%2520Alternatively%252C%2520transformers%2520can%2520model%2520long-range%2520dependencies%2520but%2520are%2520data%2520hungry%252C%2520and%2520RSCD%2520datasets%2520are%2520not%2520large%2520enough%2520to%2520train%2520these%2520models%2520effectively.%2520To%2520tackle%2520this%252C%2520this%2520paper%2520presents%2520a%2520new%2520architecture%2520for%2520RSCD%2520which%2520adapts%2520a%2520segment%2520anything%2520%2528SAM%2529%2520vision%2520foundation%2520model%2520and%2520processes%2520features%2520from%2520the%2520SAM%2520encoder%2520through%2520a%2520multi-receptive%2520field%2520ensemble%2520to%2520capture%2520local%2520and%2520global%2520change%2520patterns.%2520We%2520propose%2520an%2520ensemble%2520of%2520spatial-temporal%2520feature%2520enhancement%2520%2528STFE%2529%2520to%2520capture%2520cross-temporal%2520relations%252C%2520a%2520decoder%2520to%2520reconstruct%2520change%2520patterns%252C%2520and%2520a%2520multi-scale%2520decoder%2520fusion%2520with%2520attention%2520%2528MSDFA%2529%2520to%2520fuse%2520multi-scale%2520decoder%2520information%2520and%2520highlight%2520key%2520change%2520patterns.%2520Each%2520branch%2520in%2520an%2520ensemble%2520operates%2520on%2520a%2520separate%2520receptive%2520field%2520to%2520capture%2520finer-to-coarser%2520level%2520details.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520cross-entropy%2520masking%2520%2528CEM%2529%2520loss%2520to%2520handle%2520class-imbalance%2520in%2520RSCD%2520datasets.%2520Our%2520work%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520on%2520four%2520change%2520detection%2520datasets%252C%2520Levir-CD%252C%2520WHU-CD%252C%2520CLCD%252C%2520and%2520S2Looking.%2520We%2520achieved%25202.97%255C%2525%2520F1-score%2520improvement%2520on%2520a%2520complex%2520S2Looking%2520dataset.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/humza909/SAM-ECEM%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Receptive%20Field%20Ensemble%20with%20Cross-Entropy%20Masking%20for%20Class%20Imbalance%20in%20Remote%20Sensing%20Change%20Detection&entry.906535625=Humza%20Naveed%20and%20Xina%20Zeng%20and%20Mitch%20Bryson%20and%20Nagita%20Mehrseresht&entry.1292438233=Remote%20sensing%20change%20detection%20%28RSCD%29%20is%20a%20complex%20task%2C%20where%20changes%20often%20appear%20at%20different%20scales%20and%20orientations.%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20good%20at%20capturing%20local%20spatial%20patterns%20but%20cannot%20model%20global%20semantics%20due%20to%20limited%20receptive%20fields.%20Alternatively%2C%20transformers%20can%20model%20long-range%20dependencies%20but%20are%20data%20hungry%2C%20and%20RSCD%20datasets%20are%20not%20large%20enough%20to%20train%20these%20models%20effectively.%20To%20tackle%20this%2C%20this%20paper%20presents%20a%20new%20architecture%20for%20RSCD%20which%20adapts%20a%20segment%20anything%20%28SAM%29%20vision%20foundation%20model%20and%20processes%20features%20from%20the%20SAM%20encoder%20through%20a%20multi-receptive%20field%20ensemble%20to%20capture%20local%20and%20global%20change%20patterns.%20We%20propose%20an%20ensemble%20of%20spatial-temporal%20feature%20enhancement%20%28STFE%29%20to%20capture%20cross-temporal%20relations%2C%20a%20decoder%20to%20reconstruct%20change%20patterns%2C%20and%20a%20multi-scale%20decoder%20fusion%20with%20attention%20%28MSDFA%29%20to%20fuse%20multi-scale%20decoder%20information%20and%20highlight%20key%20change%20patterns.%20Each%20branch%20in%20an%20ensemble%20operates%20on%20a%20separate%20receptive%20field%20to%20capture%20finer-to-coarser%20level%20details.%20Additionally%2C%20we%20propose%20a%20novel%20cross-entropy%20masking%20%28CEM%29%20loss%20to%20handle%20class-imbalance%20in%20RSCD%20datasets.%20Our%20work%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%20on%20four%20change%20detection%20datasets%2C%20Levir-CD%2C%20WHU-CD%2C%20CLCD%2C%20and%20S2Looking.%20We%20achieved%202.97%5C%25%20F1-score%20improvement%20on%20a%20complex%20S2Looking%20dataset.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/humza909/SAM-ECEM&entry.1838667208=http%3A//arxiv.org/abs/2508.10568v2&entry.124074799=Read"},
{"title": "Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models", "author": "Haeun Yu and Seogyeong Jeong and Siddhesh Pawar and Jisu Shin and Jiho Jin and Junho Myung and Alice Oh and Isabelle Augenstein", "abstract": "The growing deployment of large language models (LLMs) across diverse cultural contexts necessitates a deeper understanding of LLMs' representations of different cultures. Prior work has focused on evaluating the cultural awareness of LLMs by only examining the text they generate. This approach overlooks the internal sources of cultural misrepresentation within the models themselves. To bridge this gap, we propose Culturescope, the first mechanistic interpretability-based method that probes the internal representations of different cultural knowledge in LLMs. We also introduce a cultural flattening score as a measure of the intrinsic cultural biases of the decoded knowledge from Culturescope. Additionally, we study how LLMs internalize cultural biases, which allows us to trace how cultural biases such as Western-dominance bias and cultural flattening emerge within LLMs. We find that low-resource cultures are less susceptible to cultural biases, likely due to the model's limited parametric knowledge. Our work provides a foundation for future research on mitigating cultural biases and enhancing LLMs' cultural understanding.", "link": "http://arxiv.org/abs/2508.08879v2", "date": "2026-01-16", "relevancy": 2.5965, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entangled%20in%20Representations%3A%20Mechanistic%20Investigation%20of%20Cultural%20Biases%20in%20Large%20Language%20Models&body=Title%3A%20Entangled%20in%20Representations%3A%20Mechanistic%20Investigation%20of%20Cultural%20Biases%20in%20Large%20Language%20Models%0AAuthor%3A%20Haeun%20Yu%20and%20Seogyeong%20Jeong%20and%20Siddhesh%20Pawar%20and%20Jisu%20Shin%20and%20Jiho%20Jin%20and%20Junho%20Myung%20and%20Alice%20Oh%20and%20Isabelle%20Augenstein%0AAbstract%3A%20The%20growing%20deployment%20of%20large%20language%20models%20%28LLMs%29%20across%20diverse%20cultural%20contexts%20necessitates%20a%20deeper%20understanding%20of%20LLMs%27%20representations%20of%20different%20cultures.%20Prior%20work%20has%20focused%20on%20evaluating%20the%20cultural%20awareness%20of%20LLMs%20by%20only%20examining%20the%20text%20they%20generate.%20This%20approach%20overlooks%20the%20internal%20sources%20of%20cultural%20misrepresentation%20within%20the%20models%20themselves.%20To%20bridge%20this%20gap%2C%20we%20propose%20Culturescope%2C%20the%20first%20mechanistic%20interpretability-based%20method%20that%20probes%20the%20internal%20representations%20of%20different%20cultural%20knowledge%20in%20LLMs.%20We%20also%20introduce%20a%20cultural%20flattening%20score%20as%20a%20measure%20of%20the%20intrinsic%20cultural%20biases%20of%20the%20decoded%20knowledge%20from%20Culturescope.%20Additionally%2C%20we%20study%20how%20LLMs%20internalize%20cultural%20biases%2C%20which%20allows%20us%20to%20trace%20how%20cultural%20biases%20such%20as%20Western-dominance%20bias%20and%20cultural%20flattening%20emerge%20within%20LLMs.%20We%20find%20that%20low-resource%20cultures%20are%20less%20susceptible%20to%20cultural%20biases%2C%20likely%20due%20to%20the%20model%27s%20limited%20parametric%20knowledge.%20Our%20work%20provides%20a%20foundation%20for%20future%20research%20on%20mitigating%20cultural%20biases%20and%20enhancing%20LLMs%27%20cultural%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2508.08879v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntangled%2520in%2520Representations%253A%2520Mechanistic%2520Investigation%2520of%2520Cultural%2520Biases%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DHaeun%2520Yu%2520and%2520Seogyeong%2520Jeong%2520and%2520Siddhesh%2520Pawar%2520and%2520Jisu%2520Shin%2520and%2520Jiho%2520Jin%2520and%2520Junho%2520Myung%2520and%2520Alice%2520Oh%2520and%2520Isabelle%2520Augenstein%26entry.1292438233%3DThe%2520growing%2520deployment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520across%2520diverse%2520cultural%2520contexts%2520necessitates%2520a%2520deeper%2520understanding%2520of%2520LLMs%2527%2520representations%2520of%2520different%2520cultures.%2520Prior%2520work%2520has%2520focused%2520on%2520evaluating%2520the%2520cultural%2520awareness%2520of%2520LLMs%2520by%2520only%2520examining%2520the%2520text%2520they%2520generate.%2520This%2520approach%2520overlooks%2520the%2520internal%2520sources%2520of%2520cultural%2520misrepresentation%2520within%2520the%2520models%2520themselves.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520Culturescope%252C%2520the%2520first%2520mechanistic%2520interpretability-based%2520method%2520that%2520probes%2520the%2520internal%2520representations%2520of%2520different%2520cultural%2520knowledge%2520in%2520LLMs.%2520We%2520also%2520introduce%2520a%2520cultural%2520flattening%2520score%2520as%2520a%2520measure%2520of%2520the%2520intrinsic%2520cultural%2520biases%2520of%2520the%2520decoded%2520knowledge%2520from%2520Culturescope.%2520Additionally%252C%2520we%2520study%2520how%2520LLMs%2520internalize%2520cultural%2520biases%252C%2520which%2520allows%2520us%2520to%2520trace%2520how%2520cultural%2520biases%2520such%2520as%2520Western-dominance%2520bias%2520and%2520cultural%2520flattening%2520emerge%2520within%2520LLMs.%2520We%2520find%2520that%2520low-resource%2520cultures%2520are%2520less%2520susceptible%2520to%2520cultural%2520biases%252C%2520likely%2520due%2520to%2520the%2520model%2527s%2520limited%2520parametric%2520knowledge.%2520Our%2520work%2520provides%2520a%2520foundation%2520for%2520future%2520research%2520on%2520mitigating%2520cultural%2520biases%2520and%2520enhancing%2520LLMs%2527%2520cultural%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08879v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entangled%20in%20Representations%3A%20Mechanistic%20Investigation%20of%20Cultural%20Biases%20in%20Large%20Language%20Models&entry.906535625=Haeun%20Yu%20and%20Seogyeong%20Jeong%20and%20Siddhesh%20Pawar%20and%20Jisu%20Shin%20and%20Jiho%20Jin%20and%20Junho%20Myung%20and%20Alice%20Oh%20and%20Isabelle%20Augenstein&entry.1292438233=The%20growing%20deployment%20of%20large%20language%20models%20%28LLMs%29%20across%20diverse%20cultural%20contexts%20necessitates%20a%20deeper%20understanding%20of%20LLMs%27%20representations%20of%20different%20cultures.%20Prior%20work%20has%20focused%20on%20evaluating%20the%20cultural%20awareness%20of%20LLMs%20by%20only%20examining%20the%20text%20they%20generate.%20This%20approach%20overlooks%20the%20internal%20sources%20of%20cultural%20misrepresentation%20within%20the%20models%20themselves.%20To%20bridge%20this%20gap%2C%20we%20propose%20Culturescope%2C%20the%20first%20mechanistic%20interpretability-based%20method%20that%20probes%20the%20internal%20representations%20of%20different%20cultural%20knowledge%20in%20LLMs.%20We%20also%20introduce%20a%20cultural%20flattening%20score%20as%20a%20measure%20of%20the%20intrinsic%20cultural%20biases%20of%20the%20decoded%20knowledge%20from%20Culturescope.%20Additionally%2C%20we%20study%20how%20LLMs%20internalize%20cultural%20biases%2C%20which%20allows%20us%20to%20trace%20how%20cultural%20biases%20such%20as%20Western-dominance%20bias%20and%20cultural%20flattening%20emerge%20within%20LLMs.%20We%20find%20that%20low-resource%20cultures%20are%20less%20susceptible%20to%20cultural%20biases%2C%20likely%20due%20to%20the%20model%27s%20limited%20parametric%20knowledge.%20Our%20work%20provides%20a%20foundation%20for%20future%20research%20on%20mitigating%20cultural%20biases%20and%20enhancing%20LLMs%27%20cultural%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2508.08879v2&entry.124074799=Read"},
{"title": "Universal Architectures for the Learning of Polyhedral Norms and Convex Regularizers", "author": "Michael Unser and Stanislas Ducotterd", "abstract": "This paper addresses the task of learning convex regularizers to guide the reconstruction of images from limited data. By imposing that the reconstruction be amplitude-equivariant, we narrow down the class of admissible functionals to those that can be expressed as a power of a seminorm. We then show that such functionals can be approximated to arbitrary precision with the help of polyhedral norms. In particular, we identify two dual parameterizations of such systems: (i) a synthesis form with an $\\ell_1$-penalty that involves some learnable dictionary; and (ii) an analysis form with an $\\ell_\\infty$-penalty that involves a trainable regularization operator. After having provided geometric insights and proved that the two forms are universal, we propose an implementation that relies on a specific architecture (tight frame with a weighted $\\ell_1$ penalty) that is easy to train. We illustrate its use for denoising and the reconstruction of biomedical images. We find that the proposed framework outperforms the sparsity-based methods of compressed sensing, while it offers essentially the same convergence and robustness guarantees.", "link": "http://arxiv.org/abs/2503.19190v3", "date": "2026-01-16", "relevancy": 2.5778, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5272}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.513}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Architectures%20for%20the%20Learning%20of%20Polyhedral%20Norms%20and%20Convex%20Regularizers&body=Title%3A%20Universal%20Architectures%20for%20the%20Learning%20of%20Polyhedral%20Norms%20and%20Convex%20Regularizers%0AAuthor%3A%20Michael%20Unser%20and%20Stanislas%20Ducotterd%0AAbstract%3A%20This%20paper%20addresses%20the%20task%20of%20learning%20convex%20regularizers%20to%20guide%20the%20reconstruction%20of%20images%20from%20limited%20data.%20By%20imposing%20that%20the%20reconstruction%20be%20amplitude-equivariant%2C%20we%20narrow%20down%20the%20class%20of%20admissible%20functionals%20to%20those%20that%20can%20be%20expressed%20as%20a%20power%20of%20a%20seminorm.%20We%20then%20show%20that%20such%20functionals%20can%20be%20approximated%20to%20arbitrary%20precision%20with%20the%20help%20of%20polyhedral%20norms.%20In%20particular%2C%20we%20identify%20two%20dual%20parameterizations%20of%20such%20systems%3A%20%28i%29%20a%20synthesis%20form%20with%20an%20%24%5Cell_1%24-penalty%20that%20involves%20some%20learnable%20dictionary%3B%20and%20%28ii%29%20an%20analysis%20form%20with%20an%20%24%5Cell_%5Cinfty%24-penalty%20that%20involves%20a%20trainable%20regularization%20operator.%20After%20having%20provided%20geometric%20insights%20and%20proved%20that%20the%20two%20forms%20are%20universal%2C%20we%20propose%20an%20implementation%20that%20relies%20on%20a%20specific%20architecture%20%28tight%20frame%20with%20a%20weighted%20%24%5Cell_1%24%20penalty%29%20that%20is%20easy%20to%20train.%20We%20illustrate%20its%20use%20for%20denoising%20and%20the%20reconstruction%20of%20biomedical%20images.%20We%20find%20that%20the%20proposed%20framework%20outperforms%20the%20sparsity-based%20methods%20of%20compressed%20sensing%2C%20while%20it%20offers%20essentially%20the%20same%20convergence%20and%20robustness%20guarantees.%0ALink%3A%20http%3A//arxiv.org/abs/2503.19190v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Architectures%2520for%2520the%2520Learning%2520of%2520Polyhedral%2520Norms%2520and%2520Convex%2520Regularizers%26entry.906535625%3DMichael%2520Unser%2520and%2520Stanislas%2520Ducotterd%26entry.1292438233%3DThis%2520paper%2520addresses%2520the%2520task%2520of%2520learning%2520convex%2520regularizers%2520to%2520guide%2520the%2520reconstruction%2520of%2520images%2520from%2520limited%2520data.%2520By%2520imposing%2520that%2520the%2520reconstruction%2520be%2520amplitude-equivariant%252C%2520we%2520narrow%2520down%2520the%2520class%2520of%2520admissible%2520functionals%2520to%2520those%2520that%2520can%2520be%2520expressed%2520as%2520a%2520power%2520of%2520a%2520seminorm.%2520We%2520then%2520show%2520that%2520such%2520functionals%2520can%2520be%2520approximated%2520to%2520arbitrary%2520precision%2520with%2520the%2520help%2520of%2520polyhedral%2520norms.%2520In%2520particular%252C%2520we%2520identify%2520two%2520dual%2520parameterizations%2520of%2520such%2520systems%253A%2520%2528i%2529%2520a%2520synthesis%2520form%2520with%2520an%2520%2524%255Cell_1%2524-penalty%2520that%2520involves%2520some%2520learnable%2520dictionary%253B%2520and%2520%2528ii%2529%2520an%2520analysis%2520form%2520with%2520an%2520%2524%255Cell_%255Cinfty%2524-penalty%2520that%2520involves%2520a%2520trainable%2520regularization%2520operator.%2520After%2520having%2520provided%2520geometric%2520insights%2520and%2520proved%2520that%2520the%2520two%2520forms%2520are%2520universal%252C%2520we%2520propose%2520an%2520implementation%2520that%2520relies%2520on%2520a%2520specific%2520architecture%2520%2528tight%2520frame%2520with%2520a%2520weighted%2520%2524%255Cell_1%2524%2520penalty%2529%2520that%2520is%2520easy%2520to%2520train.%2520We%2520illustrate%2520its%2520use%2520for%2520denoising%2520and%2520the%2520reconstruction%2520of%2520biomedical%2520images.%2520We%2520find%2520that%2520the%2520proposed%2520framework%2520outperforms%2520the%2520sparsity-based%2520methods%2520of%2520compressed%2520sensing%252C%2520while%2520it%2520offers%2520essentially%2520the%2520same%2520convergence%2520and%2520robustness%2520guarantees.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19190v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Architectures%20for%20the%20Learning%20of%20Polyhedral%20Norms%20and%20Convex%20Regularizers&entry.906535625=Michael%20Unser%20and%20Stanislas%20Ducotterd&entry.1292438233=This%20paper%20addresses%20the%20task%20of%20learning%20convex%20regularizers%20to%20guide%20the%20reconstruction%20of%20images%20from%20limited%20data.%20By%20imposing%20that%20the%20reconstruction%20be%20amplitude-equivariant%2C%20we%20narrow%20down%20the%20class%20of%20admissible%20functionals%20to%20those%20that%20can%20be%20expressed%20as%20a%20power%20of%20a%20seminorm.%20We%20then%20show%20that%20such%20functionals%20can%20be%20approximated%20to%20arbitrary%20precision%20with%20the%20help%20of%20polyhedral%20norms.%20In%20particular%2C%20we%20identify%20two%20dual%20parameterizations%20of%20such%20systems%3A%20%28i%29%20a%20synthesis%20form%20with%20an%20%24%5Cell_1%24-penalty%20that%20involves%20some%20learnable%20dictionary%3B%20and%20%28ii%29%20an%20analysis%20form%20with%20an%20%24%5Cell_%5Cinfty%24-penalty%20that%20involves%20a%20trainable%20regularization%20operator.%20After%20having%20provided%20geometric%20insights%20and%20proved%20that%20the%20two%20forms%20are%20universal%2C%20we%20propose%20an%20implementation%20that%20relies%20on%20a%20specific%20architecture%20%28tight%20frame%20with%20a%20weighted%20%24%5Cell_1%24%20penalty%29%20that%20is%20easy%20to%20train.%20We%20illustrate%20its%20use%20for%20denoising%20and%20the%20reconstruction%20of%20biomedical%20images.%20We%20find%20that%20the%20proposed%20framework%20outperforms%20the%20sparsity-based%20methods%20of%20compressed%20sensing%2C%20while%20it%20offers%20essentially%20the%20same%20convergence%20and%20robustness%20guarantees.&entry.1838667208=http%3A//arxiv.org/abs/2503.19190v3&entry.124074799=Read"},
{"title": "Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification", "author": "Zhiqi Pang and Lingling Zhao and Yang Liu and Chunyu Wang and Gaurav Sharma", "abstract": "We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.", "link": "http://arxiv.org/abs/2601.11243v1", "date": "2026-01-16", "relevancy": 2.5742, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image-Text%20Knowledge%20Modeling%20for%20Unsupervised%20Multi-Scenario%20Person%20Re-Identification&body=Title%3A%20Image-Text%20Knowledge%20Modeling%20for%20Unsupervised%20Multi-Scenario%20Person%20Re-Identification%0AAuthor%3A%20Zhiqi%20Pang%20and%20Lingling%20Zhao%20and%20Yang%20Liu%20and%20Chunyu%20Wang%20and%20Gaurav%20Sharma%0AAbstract%3A%20We%20propose%20unsupervised%20multi-scenario%20%28UMS%29%20person%20re-identification%20%28ReID%29%20as%20a%20new%20task%20that%20expands%20ReID%20across%20diverse%20scenarios%20%28cross-resolution%2C%20clothing%20change%2C%20etc.%29%20within%20a%20single%20coherent%20framework.%20To%20tackle%20UMS-ReID%2C%20we%20introduce%20image-text%20knowledge%20modeling%20%28ITKM%29%20--%20a%20three-stage%20framework%20that%20effectively%20exploits%20the%20representational%20power%20of%20vision-language%20models.%20We%20start%20with%20a%20pre-trained%20CLIP%20model%20with%20an%20image%20encoder%20and%20a%20text%20encoder.%20In%20Stage%20I%2C%20we%20introduce%20a%20scenario%20embedding%20in%20the%20image%20encoder%20and%20fine-tune%20the%20encoder%20to%20adaptively%20leverage%20knowledge%20from%20multiple%20scenarios.%20In%20Stage%20II%2C%20we%20optimize%20a%20set%20of%20learned%20text%20embeddings%20to%20associate%20with%20pseudo-labels%20from%20Stage%20I%20and%20introduce%20a%20multi-scenario%20separation%20loss%20to%20increase%20the%20divergence%20between%20inter-scenario%20text%20representations.%20In%20Stage%20III%2C%20we%20first%20introduce%20cluster-level%20and%20instance-level%20heterogeneous%20matching%20modules%20to%20obtain%20reliable%20heterogeneous%20positive%20pairs%20%28e.g.%2C%20a%20visible%20image%20and%20an%20infrared%20image%20of%20the%20same%20person%29%20within%20each%20scenario.%20Next%2C%20we%20propose%20a%20dynamic%20text%20representation%20update%20strategy%20to%20maintain%20consistency%20between%20text%20and%20image%20supervision%20signals.%20Experimental%20results%20across%20multiple%20scenarios%20demonstrate%20the%20superiority%20and%20generalizability%20of%20ITKM%3B%20it%20not%20only%20outperforms%20existing%20scenario-specific%20methods%20but%20also%20enhances%20overall%20performance%20by%20integrating%20knowledge%20from%20multiple%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage-Text%2520Knowledge%2520Modeling%2520for%2520Unsupervised%2520Multi-Scenario%2520Person%2520Re-Identification%26entry.906535625%3DZhiqi%2520Pang%2520and%2520Lingling%2520Zhao%2520and%2520Yang%2520Liu%2520and%2520Chunyu%2520Wang%2520and%2520Gaurav%2520Sharma%26entry.1292438233%3DWe%2520propose%2520unsupervised%2520multi-scenario%2520%2528UMS%2529%2520person%2520re-identification%2520%2528ReID%2529%2520as%2520a%2520new%2520task%2520that%2520expands%2520ReID%2520across%2520diverse%2520scenarios%2520%2528cross-resolution%252C%2520clothing%2520change%252C%2520etc.%2529%2520within%2520a%2520single%2520coherent%2520framework.%2520To%2520tackle%2520UMS-ReID%252C%2520we%2520introduce%2520image-text%2520knowledge%2520modeling%2520%2528ITKM%2529%2520--%2520a%2520three-stage%2520framework%2520that%2520effectively%2520exploits%2520the%2520representational%2520power%2520of%2520vision-language%2520models.%2520We%2520start%2520with%2520a%2520pre-trained%2520CLIP%2520model%2520with%2520an%2520image%2520encoder%2520and%2520a%2520text%2520encoder.%2520In%2520Stage%2520I%252C%2520we%2520introduce%2520a%2520scenario%2520embedding%2520in%2520the%2520image%2520encoder%2520and%2520fine-tune%2520the%2520encoder%2520to%2520adaptively%2520leverage%2520knowledge%2520from%2520multiple%2520scenarios.%2520In%2520Stage%2520II%252C%2520we%2520optimize%2520a%2520set%2520of%2520learned%2520text%2520embeddings%2520to%2520associate%2520with%2520pseudo-labels%2520from%2520Stage%2520I%2520and%2520introduce%2520a%2520multi-scenario%2520separation%2520loss%2520to%2520increase%2520the%2520divergence%2520between%2520inter-scenario%2520text%2520representations.%2520In%2520Stage%2520III%252C%2520we%2520first%2520introduce%2520cluster-level%2520and%2520instance-level%2520heterogeneous%2520matching%2520modules%2520to%2520obtain%2520reliable%2520heterogeneous%2520positive%2520pairs%2520%2528e.g.%252C%2520a%2520visible%2520image%2520and%2520an%2520infrared%2520image%2520of%2520the%2520same%2520person%2529%2520within%2520each%2520scenario.%2520Next%252C%2520we%2520propose%2520a%2520dynamic%2520text%2520representation%2520update%2520strategy%2520to%2520maintain%2520consistency%2520between%2520text%2520and%2520image%2520supervision%2520signals.%2520Experimental%2520results%2520across%2520multiple%2520scenarios%2520demonstrate%2520the%2520superiority%2520and%2520generalizability%2520of%2520ITKM%253B%2520it%2520not%2520only%2520outperforms%2520existing%2520scenario-specific%2520methods%2520but%2520also%2520enhances%2520overall%2520performance%2520by%2520integrating%2520knowledge%2520from%2520multiple%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-Text%20Knowledge%20Modeling%20for%20Unsupervised%20Multi-Scenario%20Person%20Re-Identification&entry.906535625=Zhiqi%20Pang%20and%20Lingling%20Zhao%20and%20Yang%20Liu%20and%20Chunyu%20Wang%20and%20Gaurav%20Sharma&entry.1292438233=We%20propose%20unsupervised%20multi-scenario%20%28UMS%29%20person%20re-identification%20%28ReID%29%20as%20a%20new%20task%20that%20expands%20ReID%20across%20diverse%20scenarios%20%28cross-resolution%2C%20clothing%20change%2C%20etc.%29%20within%20a%20single%20coherent%20framework.%20To%20tackle%20UMS-ReID%2C%20we%20introduce%20image-text%20knowledge%20modeling%20%28ITKM%29%20--%20a%20three-stage%20framework%20that%20effectively%20exploits%20the%20representational%20power%20of%20vision-language%20models.%20We%20start%20with%20a%20pre-trained%20CLIP%20model%20with%20an%20image%20encoder%20and%20a%20text%20encoder.%20In%20Stage%20I%2C%20we%20introduce%20a%20scenario%20embedding%20in%20the%20image%20encoder%20and%20fine-tune%20the%20encoder%20to%20adaptively%20leverage%20knowledge%20from%20multiple%20scenarios.%20In%20Stage%20II%2C%20we%20optimize%20a%20set%20of%20learned%20text%20embeddings%20to%20associate%20with%20pseudo-labels%20from%20Stage%20I%20and%20introduce%20a%20multi-scenario%20separation%20loss%20to%20increase%20the%20divergence%20between%20inter-scenario%20text%20representations.%20In%20Stage%20III%2C%20we%20first%20introduce%20cluster-level%20and%20instance-level%20heterogeneous%20matching%20modules%20to%20obtain%20reliable%20heterogeneous%20positive%20pairs%20%28e.g.%2C%20a%20visible%20image%20and%20an%20infrared%20image%20of%20the%20same%20person%29%20within%20each%20scenario.%20Next%2C%20we%20propose%20a%20dynamic%20text%20representation%20update%20strategy%20to%20maintain%20consistency%20between%20text%20and%20image%20supervision%20signals.%20Experimental%20results%20across%20multiple%20scenarios%20demonstrate%20the%20superiority%20and%20generalizability%20of%20ITKM%3B%20it%20not%20only%20outperforms%20existing%20scenario-specific%20methods%20but%20also%20enhances%20overall%20performance%20by%20integrating%20knowledge%20from%20multiple%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2601.11243v1&entry.124074799=Read"},
{"title": "SoLA-Vision: Fine-grained Layer-wise Linear Softmax Hybrid Attention", "author": "Ruibang Li and Guan Luo and Yiwei Zhang and Jin Gao and Bing Li and Weiming Hu", "abstract": "Standard softmax self-attention excels in vision tasks but incurs quadratic complexity O(N^2), limiting high-resolution deployment. Linear attention reduces the cost to O(N), yet its compressed state representations can impair modeling capacity and accuracy. We present an analytical study that contrasts linear and softmax attention for visual representation learning from a layer-stacking perspective. We further conduct systematic experiments on layer-wise hybridization patterns of linear and softmax attention. Our results show that, compared with rigid intra-block hybrid designs, fine-grained layer-wise hybridization can match or surpass performance while requiring fewer softmax layers. Building on these findings, we propose SoLA-Vision (Softmax-Linear Attention Vision), a flexible layer-wise hybrid attention backbone that enables fine-grained control over how linear and softmax attention are integrated. By strategically inserting a small number of global softmax layers, SoLA-Vision achieves a strong trade-off between accuracy and computational cost. On ImageNet-1K, SoLA-Vision outperforms purely linear and other hybrid attention models. On dense prediction tasks, it consistently surpasses strong baselines by a considerable margin. Code will be released.", "link": "http://arxiv.org/abs/2601.11164v1", "date": "2026-01-16", "relevancy": 2.5597, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5307}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5151}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoLA-Vision%3A%20Fine-grained%20Layer-wise%20Linear%20Softmax%20Hybrid%20Attention&body=Title%3A%20SoLA-Vision%3A%20Fine-grained%20Layer-wise%20Linear%20Softmax%20Hybrid%20Attention%0AAuthor%3A%20Ruibang%20Li%20and%20Guan%20Luo%20and%20Yiwei%20Zhang%20and%20Jin%20Gao%20and%20Bing%20Li%20and%20Weiming%20Hu%0AAbstract%3A%20Standard%20softmax%20self-attention%20excels%20in%20vision%20tasks%20but%20incurs%20quadratic%20complexity%20O%28N%5E2%29%2C%20limiting%20high-resolution%20deployment.%20Linear%20attention%20reduces%20the%20cost%20to%20O%28N%29%2C%20yet%20its%20compressed%20state%20representations%20can%20impair%20modeling%20capacity%20and%20accuracy.%20We%20present%20an%20analytical%20study%20that%20contrasts%20linear%20and%20softmax%20attention%20for%20visual%20representation%20learning%20from%20a%20layer-stacking%20perspective.%20We%20further%20conduct%20systematic%20experiments%20on%20layer-wise%20hybridization%20patterns%20of%20linear%20and%20softmax%20attention.%20Our%20results%20show%20that%2C%20compared%20with%20rigid%20intra-block%20hybrid%20designs%2C%20fine-grained%20layer-wise%20hybridization%20can%20match%20or%20surpass%20performance%20while%20requiring%20fewer%20softmax%20layers.%20Building%20on%20these%20findings%2C%20we%20propose%20SoLA-Vision%20%28Softmax-Linear%20Attention%20Vision%29%2C%20a%20flexible%20layer-wise%20hybrid%20attention%20backbone%20that%20enables%20fine-grained%20control%20over%20how%20linear%20and%20softmax%20attention%20are%20integrated.%20By%20strategically%20inserting%20a%20small%20number%20of%20global%20softmax%20layers%2C%20SoLA-Vision%20achieves%20a%20strong%20trade-off%20between%20accuracy%20and%20computational%20cost.%20On%20ImageNet-1K%2C%20SoLA-Vision%20outperforms%20purely%20linear%20and%20other%20hybrid%20attention%20models.%20On%20dense%20prediction%20tasks%2C%20it%20consistently%20surpasses%20strong%20baselines%20by%20a%20considerable%20margin.%20Code%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoLA-Vision%253A%2520Fine-grained%2520Layer-wise%2520Linear%2520Softmax%2520Hybrid%2520Attention%26entry.906535625%3DRuibang%2520Li%2520and%2520Guan%2520Luo%2520and%2520Yiwei%2520Zhang%2520and%2520Jin%2520Gao%2520and%2520Bing%2520Li%2520and%2520Weiming%2520Hu%26entry.1292438233%3DStandard%2520softmax%2520self-attention%2520excels%2520in%2520vision%2520tasks%2520but%2520incurs%2520quadratic%2520complexity%2520O%2528N%255E2%2529%252C%2520limiting%2520high-resolution%2520deployment.%2520Linear%2520attention%2520reduces%2520the%2520cost%2520to%2520O%2528N%2529%252C%2520yet%2520its%2520compressed%2520state%2520representations%2520can%2520impair%2520modeling%2520capacity%2520and%2520accuracy.%2520We%2520present%2520an%2520analytical%2520study%2520that%2520contrasts%2520linear%2520and%2520softmax%2520attention%2520for%2520visual%2520representation%2520learning%2520from%2520a%2520layer-stacking%2520perspective.%2520We%2520further%2520conduct%2520systematic%2520experiments%2520on%2520layer-wise%2520hybridization%2520patterns%2520of%2520linear%2520and%2520softmax%2520attention.%2520Our%2520results%2520show%2520that%252C%2520compared%2520with%2520rigid%2520intra-block%2520hybrid%2520designs%252C%2520fine-grained%2520layer-wise%2520hybridization%2520can%2520match%2520or%2520surpass%2520performance%2520while%2520requiring%2520fewer%2520softmax%2520layers.%2520Building%2520on%2520these%2520findings%252C%2520we%2520propose%2520SoLA-Vision%2520%2528Softmax-Linear%2520Attention%2520Vision%2529%252C%2520a%2520flexible%2520layer-wise%2520hybrid%2520attention%2520backbone%2520that%2520enables%2520fine-grained%2520control%2520over%2520how%2520linear%2520and%2520softmax%2520attention%2520are%2520integrated.%2520By%2520strategically%2520inserting%2520a%2520small%2520number%2520of%2520global%2520softmax%2520layers%252C%2520SoLA-Vision%2520achieves%2520a%2520strong%2520trade-off%2520between%2520accuracy%2520and%2520computational%2520cost.%2520On%2520ImageNet-1K%252C%2520SoLA-Vision%2520outperforms%2520purely%2520linear%2520and%2520other%2520hybrid%2520attention%2520models.%2520On%2520dense%2520prediction%2520tasks%252C%2520it%2520consistently%2520surpasses%2520strong%2520baselines%2520by%2520a%2520considerable%2520margin.%2520Code%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoLA-Vision%3A%20Fine-grained%20Layer-wise%20Linear%20Softmax%20Hybrid%20Attention&entry.906535625=Ruibang%20Li%20and%20Guan%20Luo%20and%20Yiwei%20Zhang%20and%20Jin%20Gao%20and%20Bing%20Li%20and%20Weiming%20Hu&entry.1292438233=Standard%20softmax%20self-attention%20excels%20in%20vision%20tasks%20but%20incurs%20quadratic%20complexity%20O%28N%5E2%29%2C%20limiting%20high-resolution%20deployment.%20Linear%20attention%20reduces%20the%20cost%20to%20O%28N%29%2C%20yet%20its%20compressed%20state%20representations%20can%20impair%20modeling%20capacity%20and%20accuracy.%20We%20present%20an%20analytical%20study%20that%20contrasts%20linear%20and%20softmax%20attention%20for%20visual%20representation%20learning%20from%20a%20layer-stacking%20perspective.%20We%20further%20conduct%20systematic%20experiments%20on%20layer-wise%20hybridization%20patterns%20of%20linear%20and%20softmax%20attention.%20Our%20results%20show%20that%2C%20compared%20with%20rigid%20intra-block%20hybrid%20designs%2C%20fine-grained%20layer-wise%20hybridization%20can%20match%20or%20surpass%20performance%20while%20requiring%20fewer%20softmax%20layers.%20Building%20on%20these%20findings%2C%20we%20propose%20SoLA-Vision%20%28Softmax-Linear%20Attention%20Vision%29%2C%20a%20flexible%20layer-wise%20hybrid%20attention%20backbone%20that%20enables%20fine-grained%20control%20over%20how%20linear%20and%20softmax%20attention%20are%20integrated.%20By%20strategically%20inserting%20a%20small%20number%20of%20global%20softmax%20layers%2C%20SoLA-Vision%20achieves%20a%20strong%20trade-off%20between%20accuracy%20and%20computational%20cost.%20On%20ImageNet-1K%2C%20SoLA-Vision%20outperforms%20purely%20linear%20and%20other%20hybrid%20attention%20models.%20On%20dense%20prediction%20tasks%2C%20it%20consistently%20surpasses%20strong%20baselines%20by%20a%20considerable%20margin.%20Code%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2601.11164v1&entry.124074799=Read"},
{"title": "Do Sparse Autoencoders Identify Reasoning Features in Language Models?", "author": "George Ma and Zhongyuan Liang and Irene Y. Chen and Somayeh Sojoudi", "abstract": "We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). We first show through a simple theoretical analysis that $\\ell_1$-regularized SAEs are intrinsically biased toward low-dimensional patterns, providing a mechanistic explanation for why shallow linguistic cues may be preferentially captured over distributed reasoning behaviors. Motivated by this bias, we introduce a falsification-oriented evaluation framework that combines causal token injection and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that features identified by contrastive methods are highly sensitive to token-level interventions, with 45% to 90% activating when a small number of associated tokens are injected into non-reasoning text. For the remaining features, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields no improvements in benchmark performance. Overall, our results suggest that SAE features identified by current contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves. Code is available at https://github.com/GeorgeMLP/reasoning-probing.", "link": "http://arxiv.org/abs/2601.05679v3", "date": "2026-01-16", "relevancy": 2.5326, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Sparse%20Autoencoders%20Identify%20Reasoning%20Features%20in%20Language%20Models%3F&body=Title%3A%20Do%20Sparse%20Autoencoders%20Identify%20Reasoning%20Features%20in%20Language%20Models%3F%0AAuthor%3A%20George%20Ma%20and%20Zhongyuan%20Liang%20and%20Irene%20Y.%20Chen%20and%20Somayeh%20Sojoudi%0AAbstract%3A%20We%20investigate%20whether%20sparse%20autoencoders%20%28SAEs%29%20identify%20genuine%20reasoning%20features%20in%20large%20language%20models%20%28LLMs%29.%20We%20first%20show%20through%20a%20simple%20theoretical%20analysis%20that%20%24%5Cell_1%24-regularized%20SAEs%20are%20intrinsically%20biased%20toward%20low-dimensional%20patterns%2C%20providing%20a%20mechanistic%20explanation%20for%20why%20shallow%20linguistic%20cues%20may%20be%20preferentially%20captured%20over%20distributed%20reasoning%20behaviors.%20Motivated%20by%20this%20bias%2C%20we%20introduce%20a%20falsification-oriented%20evaluation%20framework%20that%20combines%20causal%20token%20injection%20and%20LLM-guided%20falsification%20to%20test%20whether%20feature%20activation%20reflects%20reasoning%20processes%20or%20superficial%20linguistic%20correlates.%20Across%2020%20configurations%20spanning%20multiple%20model%20families%2C%20layers%2C%20and%20reasoning%20datasets%2C%20we%20find%20that%20features%20identified%20by%20contrastive%20methods%20are%20highly%20sensitive%20to%20token-level%20interventions%2C%20with%2045%25%20to%2090%25%20activating%20when%20a%20small%20number%20of%20associated%20tokens%20are%20injected%20into%20non-reasoning%20text.%20For%20the%20remaining%20features%2C%20LLM-guided%20falsification%20consistently%20produces%20non-reasoning%20inputs%20that%20activate%20the%20feature%20and%20reasoning%20inputs%20that%20do%20not%2C%20with%20no%20analyzed%20feature%20satisfying%20our%20criteria%20for%20genuine%20reasoning%20behavior.%20Steering%20these%20features%20yields%20no%20improvements%20in%20benchmark%20performance.%20Overall%2C%20our%20results%20suggest%20that%20SAE%20features%20identified%20by%20current%20contrastive%20approaches%20primarily%20capture%20linguistic%20correlates%20of%20reasoning%20rather%20than%20the%20underlying%20reasoning%20computations%20themselves.%20Code%20is%20available%20at%20https%3A//github.com/GeorgeMLP/reasoning-probing.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05679v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Sparse%2520Autoencoders%2520Identify%2520Reasoning%2520Features%2520in%2520Language%2520Models%253F%26entry.906535625%3DGeorge%2520Ma%2520and%2520Zhongyuan%2520Liang%2520and%2520Irene%2520Y.%2520Chen%2520and%2520Somayeh%2520Sojoudi%26entry.1292438233%3DWe%2520investigate%2520whether%2520sparse%2520autoencoders%2520%2528SAEs%2529%2520identify%2520genuine%2520reasoning%2520features%2520in%2520large%2520language%2520models%2520%2528LLMs%2529.%2520We%2520first%2520show%2520through%2520a%2520simple%2520theoretical%2520analysis%2520that%2520%2524%255Cell_1%2524-regularized%2520SAEs%2520are%2520intrinsically%2520biased%2520toward%2520low-dimensional%2520patterns%252C%2520providing%2520a%2520mechanistic%2520explanation%2520for%2520why%2520shallow%2520linguistic%2520cues%2520may%2520be%2520preferentially%2520captured%2520over%2520distributed%2520reasoning%2520behaviors.%2520Motivated%2520by%2520this%2520bias%252C%2520we%2520introduce%2520a%2520falsification-oriented%2520evaluation%2520framework%2520that%2520combines%2520causal%2520token%2520injection%2520and%2520LLM-guided%2520falsification%2520to%2520test%2520whether%2520feature%2520activation%2520reflects%2520reasoning%2520processes%2520or%2520superficial%2520linguistic%2520correlates.%2520Across%252020%2520configurations%2520spanning%2520multiple%2520model%2520families%252C%2520layers%252C%2520and%2520reasoning%2520datasets%252C%2520we%2520find%2520that%2520features%2520identified%2520by%2520contrastive%2520methods%2520are%2520highly%2520sensitive%2520to%2520token-level%2520interventions%252C%2520with%252045%2525%2520to%252090%2525%2520activating%2520when%2520a%2520small%2520number%2520of%2520associated%2520tokens%2520are%2520injected%2520into%2520non-reasoning%2520text.%2520For%2520the%2520remaining%2520features%252C%2520LLM-guided%2520falsification%2520consistently%2520produces%2520non-reasoning%2520inputs%2520that%2520activate%2520the%2520feature%2520and%2520reasoning%2520inputs%2520that%2520do%2520not%252C%2520with%2520no%2520analyzed%2520feature%2520satisfying%2520our%2520criteria%2520for%2520genuine%2520reasoning%2520behavior.%2520Steering%2520these%2520features%2520yields%2520no%2520improvements%2520in%2520benchmark%2520performance.%2520Overall%252C%2520our%2520results%2520suggest%2520that%2520SAE%2520features%2520identified%2520by%2520current%2520contrastive%2520approaches%2520primarily%2520capture%2520linguistic%2520correlates%2520of%2520reasoning%2520rather%2520than%2520the%2520underlying%2520reasoning%2520computations%2520themselves.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/GeorgeMLP/reasoning-probing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05679v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Sparse%20Autoencoders%20Identify%20Reasoning%20Features%20in%20Language%20Models%3F&entry.906535625=George%20Ma%20and%20Zhongyuan%20Liang%20and%20Irene%20Y.%20Chen%20and%20Somayeh%20Sojoudi&entry.1292438233=We%20investigate%20whether%20sparse%20autoencoders%20%28SAEs%29%20identify%20genuine%20reasoning%20features%20in%20large%20language%20models%20%28LLMs%29.%20We%20first%20show%20through%20a%20simple%20theoretical%20analysis%20that%20%24%5Cell_1%24-regularized%20SAEs%20are%20intrinsically%20biased%20toward%20low-dimensional%20patterns%2C%20providing%20a%20mechanistic%20explanation%20for%20why%20shallow%20linguistic%20cues%20may%20be%20preferentially%20captured%20over%20distributed%20reasoning%20behaviors.%20Motivated%20by%20this%20bias%2C%20we%20introduce%20a%20falsification-oriented%20evaluation%20framework%20that%20combines%20causal%20token%20injection%20and%20LLM-guided%20falsification%20to%20test%20whether%20feature%20activation%20reflects%20reasoning%20processes%20or%20superficial%20linguistic%20correlates.%20Across%2020%20configurations%20spanning%20multiple%20model%20families%2C%20layers%2C%20and%20reasoning%20datasets%2C%20we%20find%20that%20features%20identified%20by%20contrastive%20methods%20are%20highly%20sensitive%20to%20token-level%20interventions%2C%20with%2045%25%20to%2090%25%20activating%20when%20a%20small%20number%20of%20associated%20tokens%20are%20injected%20into%20non-reasoning%20text.%20For%20the%20remaining%20features%2C%20LLM-guided%20falsification%20consistently%20produces%20non-reasoning%20inputs%20that%20activate%20the%20feature%20and%20reasoning%20inputs%20that%20do%20not%2C%20with%20no%20analyzed%20feature%20satisfying%20our%20criteria%20for%20genuine%20reasoning%20behavior.%20Steering%20these%20features%20yields%20no%20improvements%20in%20benchmark%20performance.%20Overall%2C%20our%20results%20suggest%20that%20SAE%20features%20identified%20by%20current%20contrastive%20approaches%20primarily%20capture%20linguistic%20correlates%20of%20reasoning%20rather%20than%20the%20underlying%20reasoning%20computations%20themselves.%20Code%20is%20available%20at%20https%3A//github.com/GeorgeMLP/reasoning-probing.&entry.1838667208=http%3A//arxiv.org/abs/2601.05679v3&entry.124074799=Read"},
{"title": "Language-Agnostic Visual Embeddings for Cross-Script Handwriting Retrieval", "author": "Fangke Chen and Tianhao Dong and Sirry Chen and Guobin Zhang and Yishu Zhang and Yining Chen", "abstract": "Handwritten word retrieval is vital for digital archives but remains challenging due to large handwriting variability and cross-lingual semantic gaps. While large vision-language models offer potential solutions, their prohibitive computational costs hinder practical edge deployment. To address this, we propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings. By jointly optimizing instance-level alignment and class-level semantic consistency, our approach anchors visual embeddings to language-agnostic semantic prototypes, enforcing invariance across scripts and writing styles. Experiments show that our method outperforms 28 baselines and achieves state-of-the-art accuracy on within-language retrieval benchmarks. We further conduct explicit cross-lingual retrieval, where the query language differs from the target language, to validate the effectiveness of the learned cross-lingual representations. Achieving strong performance with only a fraction of the parameters required by existing models, our framework enables accurate and resource-efficient cross-script handwriting retrieval.", "link": "http://arxiv.org/abs/2601.11248v1", "date": "2026-01-16", "relevancy": 2.5218, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Agnostic%20Visual%20Embeddings%20for%20Cross-Script%20Handwriting%20Retrieval&body=Title%3A%20Language-Agnostic%20Visual%20Embeddings%20for%20Cross-Script%20Handwriting%20Retrieval%0AAuthor%3A%20Fangke%20Chen%20and%20Tianhao%20Dong%20and%20Sirry%20Chen%20and%20Guobin%20Zhang%20and%20Yishu%20Zhang%20and%20Yining%20Chen%0AAbstract%3A%20Handwritten%20word%20retrieval%20is%20vital%20for%20digital%20archives%20but%20remains%20challenging%20due%20to%20large%20handwriting%20variability%20and%20cross-lingual%20semantic%20gaps.%20While%20large%20vision-language%20models%20offer%20potential%20solutions%2C%20their%20prohibitive%20computational%20costs%20hinder%20practical%20edge%20deployment.%20To%20address%20this%2C%20we%20propose%20a%20lightweight%20asymmetric%20dual-encoder%20framework%20that%20learns%20unified%2C%20style-invariant%20visual%20embeddings.%20By%20jointly%20optimizing%20instance-level%20alignment%20and%20class-level%20semantic%20consistency%2C%20our%20approach%20anchors%20visual%20embeddings%20to%20language-agnostic%20semantic%20prototypes%2C%20enforcing%20invariance%20across%20scripts%20and%20writing%20styles.%20Experiments%20show%20that%20our%20method%20outperforms%2028%20baselines%20and%20achieves%20state-of-the-art%20accuracy%20on%20within-language%20retrieval%20benchmarks.%20We%20further%20conduct%20explicit%20cross-lingual%20retrieval%2C%20where%20the%20query%20language%20differs%20from%20the%20target%20language%2C%20to%20validate%20the%20effectiveness%20of%20the%20learned%20cross-lingual%20representations.%20Achieving%20strong%20performance%20with%20only%20a%20fraction%20of%20the%20parameters%20required%20by%20existing%20models%2C%20our%20framework%20enables%20accurate%20and%20resource-efficient%20cross-script%20handwriting%20retrieval.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Agnostic%2520Visual%2520Embeddings%2520for%2520Cross-Script%2520Handwriting%2520Retrieval%26entry.906535625%3DFangke%2520Chen%2520and%2520Tianhao%2520Dong%2520and%2520Sirry%2520Chen%2520and%2520Guobin%2520Zhang%2520and%2520Yishu%2520Zhang%2520and%2520Yining%2520Chen%26entry.1292438233%3DHandwritten%2520word%2520retrieval%2520is%2520vital%2520for%2520digital%2520archives%2520but%2520remains%2520challenging%2520due%2520to%2520large%2520handwriting%2520variability%2520and%2520cross-lingual%2520semantic%2520gaps.%2520While%2520large%2520vision-language%2520models%2520offer%2520potential%2520solutions%252C%2520their%2520prohibitive%2520computational%2520costs%2520hinder%2520practical%2520edge%2520deployment.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520lightweight%2520asymmetric%2520dual-encoder%2520framework%2520that%2520learns%2520unified%252C%2520style-invariant%2520visual%2520embeddings.%2520By%2520jointly%2520optimizing%2520instance-level%2520alignment%2520and%2520class-level%2520semantic%2520consistency%252C%2520our%2520approach%2520anchors%2520visual%2520embeddings%2520to%2520language-agnostic%2520semantic%2520prototypes%252C%2520enforcing%2520invariance%2520across%2520scripts%2520and%2520writing%2520styles.%2520Experiments%2520show%2520that%2520our%2520method%2520outperforms%252028%2520baselines%2520and%2520achieves%2520state-of-the-art%2520accuracy%2520on%2520within-language%2520retrieval%2520benchmarks.%2520We%2520further%2520conduct%2520explicit%2520cross-lingual%2520retrieval%252C%2520where%2520the%2520query%2520language%2520differs%2520from%2520the%2520target%2520language%252C%2520to%2520validate%2520the%2520effectiveness%2520of%2520the%2520learned%2520cross-lingual%2520representations.%2520Achieving%2520strong%2520performance%2520with%2520only%2520a%2520fraction%2520of%2520the%2520parameters%2520required%2520by%2520existing%2520models%252C%2520our%2520framework%2520enables%2520accurate%2520and%2520resource-efficient%2520cross-script%2520handwriting%2520retrieval.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Agnostic%20Visual%20Embeddings%20for%20Cross-Script%20Handwriting%20Retrieval&entry.906535625=Fangke%20Chen%20and%20Tianhao%20Dong%20and%20Sirry%20Chen%20and%20Guobin%20Zhang%20and%20Yishu%20Zhang%20and%20Yining%20Chen&entry.1292438233=Handwritten%20word%20retrieval%20is%20vital%20for%20digital%20archives%20but%20remains%20challenging%20due%20to%20large%20handwriting%20variability%20and%20cross-lingual%20semantic%20gaps.%20While%20large%20vision-language%20models%20offer%20potential%20solutions%2C%20their%20prohibitive%20computational%20costs%20hinder%20practical%20edge%20deployment.%20To%20address%20this%2C%20we%20propose%20a%20lightweight%20asymmetric%20dual-encoder%20framework%20that%20learns%20unified%2C%20style-invariant%20visual%20embeddings.%20By%20jointly%20optimizing%20instance-level%20alignment%20and%20class-level%20semantic%20consistency%2C%20our%20approach%20anchors%20visual%20embeddings%20to%20language-agnostic%20semantic%20prototypes%2C%20enforcing%20invariance%20across%20scripts%20and%20writing%20styles.%20Experiments%20show%20that%20our%20method%20outperforms%2028%20baselines%20and%20achieves%20state-of-the-art%20accuracy%20on%20within-language%20retrieval%20benchmarks.%20We%20further%20conduct%20explicit%20cross-lingual%20retrieval%2C%20where%20the%20query%20language%20differs%20from%20the%20target%20language%2C%20to%20validate%20the%20effectiveness%20of%20the%20learned%20cross-lingual%20representations.%20Achieving%20strong%20performance%20with%20only%20a%20fraction%20of%20the%20parameters%20required%20by%20existing%20models%2C%20our%20framework%20enables%20accurate%20and%20resource-efficient%20cross-script%20handwriting%20retrieval.&entry.1838667208=http%3A//arxiv.org/abs/2601.11248v1&entry.124074799=Read"},
{"title": "Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints", "author": "Wenxiao Li and Xue-Cheng Tai and Jun Liu", "abstract": "Existing research highlights the crucial role of topological priors in image segmentation, particularly in preserving essential structures such as connectivity and genus. Accurately capturing these topological features often requires incorporating width-related information, including the thickness and length inherent to the image structures. However, traditional mathematical definitions of topological structures lack this dimensional width information, limiting methods like persistent homology from fully addressing practical segmentation needs. To overcome this limitation, we propose a novel mathematical framework that explicitly integrates width information into the characterization of topological structures. This method leverages persistent homology, complemented by smoothing concepts from partial differential equations (PDEs), to modify local extrema of upper-level sets. This approach enables the resulting topological structures to inherently capture width properties. We incorporate this enhanced topological description into variational image segmentation models. Using some proper loss functions, we are also able to design neural networks that can segment images with the required topological and width properties. Through variational constraints on the relevant topological energies, our approach successfully preserves essential topological invariants such as connectivity and genus counts, simultaneously ensuring that segmented structures retain critical width attributes, including line thickness and length. Numerical experiments demonstrate the effectiveness of our method, showcasing its capability to maintain topological fidelity while explicitly embedding width characteristics into segmented image structures.", "link": "http://arxiv.org/abs/2601.11409v1", "date": "2026-01-16", "relevancy": 2.5125, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5145}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4968}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology-Guaranteed%20Image%20Segmentation%3A%20Enforcing%20Connectivity%2C%20Genus%2C%20and%20Width%20Constraints&body=Title%3A%20Topology-Guaranteed%20Image%20Segmentation%3A%20Enforcing%20Connectivity%2C%20Genus%2C%20and%20Width%20Constraints%0AAuthor%3A%20Wenxiao%20Li%20and%20Xue-Cheng%20Tai%20and%20Jun%20Liu%0AAbstract%3A%20Existing%20research%20highlights%20the%20crucial%20role%20of%20topological%20priors%20in%20image%20segmentation%2C%20particularly%20in%20preserving%20essential%20structures%20such%20as%20connectivity%20and%20genus.%20Accurately%20capturing%20these%20topological%20features%20often%20requires%20incorporating%20width-related%20information%2C%20including%20the%20thickness%20and%20length%20inherent%20to%20the%20image%20structures.%20However%2C%20traditional%20mathematical%20definitions%20of%20topological%20structures%20lack%20this%20dimensional%20width%20information%2C%20limiting%20methods%20like%20persistent%20homology%20from%20fully%20addressing%20practical%20segmentation%20needs.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20mathematical%20framework%20that%20explicitly%20integrates%20width%20information%20into%20the%20characterization%20of%20topological%20structures.%20This%20method%20leverages%20persistent%20homology%2C%20complemented%20by%20smoothing%20concepts%20from%20partial%20differential%20equations%20%28PDEs%29%2C%20to%20modify%20local%20extrema%20of%20upper-level%20sets.%20This%20approach%20enables%20the%20resulting%20topological%20structures%20to%20inherently%20capture%20width%20properties.%20We%20incorporate%20this%20enhanced%20topological%20description%20into%20variational%20image%20segmentation%20models.%20Using%20some%20proper%20loss%20functions%2C%20we%20are%20also%20able%20to%20design%20neural%20networks%20that%20can%20segment%20images%20with%20the%20required%20topological%20and%20width%20properties.%20Through%20variational%20constraints%20on%20the%20relevant%20topological%20energies%2C%20our%20approach%20successfully%20preserves%20essential%20topological%20invariants%20such%20as%20connectivity%20and%20genus%20counts%2C%20simultaneously%20ensuring%20that%20segmented%20structures%20retain%20critical%20width%20attributes%2C%20including%20line%20thickness%20and%20length.%20Numerical%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20showcasing%20its%20capability%20to%20maintain%20topological%20fidelity%20while%20explicitly%20embedding%20width%20characteristics%20into%20segmented%20image%20structures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology-Guaranteed%2520Image%2520Segmentation%253A%2520Enforcing%2520Connectivity%252C%2520Genus%252C%2520and%2520Width%2520Constraints%26entry.906535625%3DWenxiao%2520Li%2520and%2520Xue-Cheng%2520Tai%2520and%2520Jun%2520Liu%26entry.1292438233%3DExisting%2520research%2520highlights%2520the%2520crucial%2520role%2520of%2520topological%2520priors%2520in%2520image%2520segmentation%252C%2520particularly%2520in%2520preserving%2520essential%2520structures%2520such%2520as%2520connectivity%2520and%2520genus.%2520Accurately%2520capturing%2520these%2520topological%2520features%2520often%2520requires%2520incorporating%2520width-related%2520information%252C%2520including%2520the%2520thickness%2520and%2520length%2520inherent%2520to%2520the%2520image%2520structures.%2520However%252C%2520traditional%2520mathematical%2520definitions%2520of%2520topological%2520structures%2520lack%2520this%2520dimensional%2520width%2520information%252C%2520limiting%2520methods%2520like%2520persistent%2520homology%2520from%2520fully%2520addressing%2520practical%2520segmentation%2520needs.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520mathematical%2520framework%2520that%2520explicitly%2520integrates%2520width%2520information%2520into%2520the%2520characterization%2520of%2520topological%2520structures.%2520This%2520method%2520leverages%2520persistent%2520homology%252C%2520complemented%2520by%2520smoothing%2520concepts%2520from%2520partial%2520differential%2520equations%2520%2528PDEs%2529%252C%2520to%2520modify%2520local%2520extrema%2520of%2520upper-level%2520sets.%2520This%2520approach%2520enables%2520the%2520resulting%2520topological%2520structures%2520to%2520inherently%2520capture%2520width%2520properties.%2520We%2520incorporate%2520this%2520enhanced%2520topological%2520description%2520into%2520variational%2520image%2520segmentation%2520models.%2520Using%2520some%2520proper%2520loss%2520functions%252C%2520we%2520are%2520also%2520able%2520to%2520design%2520neural%2520networks%2520that%2520can%2520segment%2520images%2520with%2520the%2520required%2520topological%2520and%2520width%2520properties.%2520Through%2520variational%2520constraints%2520on%2520the%2520relevant%2520topological%2520energies%252C%2520our%2520approach%2520successfully%2520preserves%2520essential%2520topological%2520invariants%2520such%2520as%2520connectivity%2520and%2520genus%2520counts%252C%2520simultaneously%2520ensuring%2520that%2520segmented%2520structures%2520retain%2520critical%2520width%2520attributes%252C%2520including%2520line%2520thickness%2520and%2520length.%2520Numerical%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520showcasing%2520its%2520capability%2520to%2520maintain%2520topological%2520fidelity%2520while%2520explicitly%2520embedding%2520width%2520characteristics%2520into%2520segmented%2520image%2520structures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology-Guaranteed%20Image%20Segmentation%3A%20Enforcing%20Connectivity%2C%20Genus%2C%20and%20Width%20Constraints&entry.906535625=Wenxiao%20Li%20and%20Xue-Cheng%20Tai%20and%20Jun%20Liu&entry.1292438233=Existing%20research%20highlights%20the%20crucial%20role%20of%20topological%20priors%20in%20image%20segmentation%2C%20particularly%20in%20preserving%20essential%20structures%20such%20as%20connectivity%20and%20genus.%20Accurately%20capturing%20these%20topological%20features%20often%20requires%20incorporating%20width-related%20information%2C%20including%20the%20thickness%20and%20length%20inherent%20to%20the%20image%20structures.%20However%2C%20traditional%20mathematical%20definitions%20of%20topological%20structures%20lack%20this%20dimensional%20width%20information%2C%20limiting%20methods%20like%20persistent%20homology%20from%20fully%20addressing%20practical%20segmentation%20needs.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20mathematical%20framework%20that%20explicitly%20integrates%20width%20information%20into%20the%20characterization%20of%20topological%20structures.%20This%20method%20leverages%20persistent%20homology%2C%20complemented%20by%20smoothing%20concepts%20from%20partial%20differential%20equations%20%28PDEs%29%2C%20to%20modify%20local%20extrema%20of%20upper-level%20sets.%20This%20approach%20enables%20the%20resulting%20topological%20structures%20to%20inherently%20capture%20width%20properties.%20We%20incorporate%20this%20enhanced%20topological%20description%20into%20variational%20image%20segmentation%20models.%20Using%20some%20proper%20loss%20functions%2C%20we%20are%20also%20able%20to%20design%20neural%20networks%20that%20can%20segment%20images%20with%20the%20required%20topological%20and%20width%20properties.%20Through%20variational%20constraints%20on%20the%20relevant%20topological%20energies%2C%20our%20approach%20successfully%20preserves%20essential%20topological%20invariants%20such%20as%20connectivity%20and%20genus%20counts%2C%20simultaneously%20ensuring%20that%20segmented%20structures%20retain%20critical%20width%20attributes%2C%20including%20line%20thickness%20and%20length.%20Numerical%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20showcasing%20its%20capability%20to%20maintain%20topological%20fidelity%20while%20explicitly%20embedding%20width%20characteristics%20into%20segmented%20image%20structures.&entry.1838667208=http%3A//arxiv.org/abs/2601.11409v1&entry.124074799=Read"},
{"title": "Low-Rank Key Value Attention", "author": "James O'Neill and Robert Clancy and Mariia Matskevichus and Fergal Reid", "abstract": "Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \\textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.\n  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \\textbf{20-25\\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.", "link": "http://arxiv.org/abs/2601.11471v1", "date": "2026-01-16", "relevancy": 2.4875, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5294}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4848}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Key%20Value%20Attention&body=Title%3A%20Low-Rank%20Key%20Value%20Attention%0AAuthor%3A%20James%20O%27Neill%20and%20Robert%20Clancy%20and%20Mariia%20Matskevichus%20and%20Fergal%20Reid%0AAbstract%3A%20Transformer%20pretraining%20is%20increasingly%20constrained%20by%20memory%20and%20compute%20requirements%2C%20with%20the%20key-value%20%28KV%29%20cache%20emerging%20as%20a%20dominant%20bottleneck%20during%20training%20and%20autoregressive%20decoding.%20We%20propose%20%5Ctextit%7Blow-rank%20KV%20adaptation%7D%20%28LRKV%29%2C%20a%20simple%20modification%20of%20multi-head%20attention%20that%20reduces%20KV%20cache%20memory%20by%20exploiting%20redundancy%20across%20attention%20heads%20while%20preserving%20full%20token-level%20resolution.%20Each%20layer%20uses%20a%20shared%20full-rank%20KV%20projection%20augmented%20with%20low-rank%2C%20head-specific%20residuals%2C%20yielding%20a%20continuous%20trade-off%20between%20complete%20sharing%20and%20fully%20independent%20attention.%0A%20%20LRKV%20is%20a%20drop-in%20replacement%20for%20standard%20multi-head%20attention%20and%20directly%20subsumes%20query-sharing%20approaches%20such%20as%20multi-query%20and%20grouped-query%20attention%2C%20while%20remaining%20distinct%20from%20latent-compression%20methods%20such%20as%20multi-latent%20attention%20%28MLA%29.%20Across%20large-scale%20pretraining%20experiments%2C%20LRKV%20consistently%20achieves%20faster%20loss%20reduction%2C%20lower%20validation%20perplexity%2C%20and%20stronger%20downstream%20task%20performance%20than%20standard%20attention%2C%20MQA/GQA%2C%20and%20MLA.%20At%20the%202.5B%20scale%2C%20LRKV%20outperforms%20standard%20attention%20while%20using%20roughly%20half%20the%20KV%20cache%2C%20and%20reaches%20equivalent%20model%20quality%20with%20up%20to%20%5Ctextbf%7B20-25%5C%25%20less%20training%20compute%7D%20when%20measured%20in%20cumulative%20FLOPs.%20To%20explain%20these%20gains%2C%20we%20analyze%20attention%20head%20structure%20in%20operator%20space%20and%20show%20that%20LRKV%20preserves%20nearly%20all%20functional%20head%20diversity%20relative%20to%20standard%20attention%2C%20whereas%20more%20aggressive%20KV-sharing%20mechanisms%20rely%20on%20compensatory%20query%20specialization.%20Together%2C%20these%20results%20establish%20LRKV%20as%20a%20practical%20and%20effective%20attention%20mechanism%20for%20scaling%20Transformer%20pretraining%20under%20memory-%20and%20compute-constrained%20regimes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Key%2520Value%2520Attention%26entry.906535625%3DJames%2520O%2527Neill%2520and%2520Robert%2520Clancy%2520and%2520Mariia%2520Matskevichus%2520and%2520Fergal%2520Reid%26entry.1292438233%3DTransformer%2520pretraining%2520is%2520increasingly%2520constrained%2520by%2520memory%2520and%2520compute%2520requirements%252C%2520with%2520the%2520key-value%2520%2528KV%2529%2520cache%2520emerging%2520as%2520a%2520dominant%2520bottleneck%2520during%2520training%2520and%2520autoregressive%2520decoding.%2520We%2520propose%2520%255Ctextit%257Blow-rank%2520KV%2520adaptation%257D%2520%2528LRKV%2529%252C%2520a%2520simple%2520modification%2520of%2520multi-head%2520attention%2520that%2520reduces%2520KV%2520cache%2520memory%2520by%2520exploiting%2520redundancy%2520across%2520attention%2520heads%2520while%2520preserving%2520full%2520token-level%2520resolution.%2520Each%2520layer%2520uses%2520a%2520shared%2520full-rank%2520KV%2520projection%2520augmented%2520with%2520low-rank%252C%2520head-specific%2520residuals%252C%2520yielding%2520a%2520continuous%2520trade-off%2520between%2520complete%2520sharing%2520and%2520fully%2520independent%2520attention.%250A%2520%2520LRKV%2520is%2520a%2520drop-in%2520replacement%2520for%2520standard%2520multi-head%2520attention%2520and%2520directly%2520subsumes%2520query-sharing%2520approaches%2520such%2520as%2520multi-query%2520and%2520grouped-query%2520attention%252C%2520while%2520remaining%2520distinct%2520from%2520latent-compression%2520methods%2520such%2520as%2520multi-latent%2520attention%2520%2528MLA%2529.%2520Across%2520large-scale%2520pretraining%2520experiments%252C%2520LRKV%2520consistently%2520achieves%2520faster%2520loss%2520reduction%252C%2520lower%2520validation%2520perplexity%252C%2520and%2520stronger%2520downstream%2520task%2520performance%2520than%2520standard%2520attention%252C%2520MQA/GQA%252C%2520and%2520MLA.%2520At%2520the%25202.5B%2520scale%252C%2520LRKV%2520outperforms%2520standard%2520attention%2520while%2520using%2520roughly%2520half%2520the%2520KV%2520cache%252C%2520and%2520reaches%2520equivalent%2520model%2520quality%2520with%2520up%2520to%2520%255Ctextbf%257B20-25%255C%2525%2520less%2520training%2520compute%257D%2520when%2520measured%2520in%2520cumulative%2520FLOPs.%2520To%2520explain%2520these%2520gains%252C%2520we%2520analyze%2520attention%2520head%2520structure%2520in%2520operator%2520space%2520and%2520show%2520that%2520LRKV%2520preserves%2520nearly%2520all%2520functional%2520head%2520diversity%2520relative%2520to%2520standard%2520attention%252C%2520whereas%2520more%2520aggressive%2520KV-sharing%2520mechanisms%2520rely%2520on%2520compensatory%2520query%2520specialization.%2520Together%252C%2520these%2520results%2520establish%2520LRKV%2520as%2520a%2520practical%2520and%2520effective%2520attention%2520mechanism%2520for%2520scaling%2520Transformer%2520pretraining%2520under%2520memory-%2520and%2520compute-constrained%2520regimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Key%20Value%20Attention&entry.906535625=James%20O%27Neill%20and%20Robert%20Clancy%20and%20Mariia%20Matskevichus%20and%20Fergal%20Reid&entry.1292438233=Transformer%20pretraining%20is%20increasingly%20constrained%20by%20memory%20and%20compute%20requirements%2C%20with%20the%20key-value%20%28KV%29%20cache%20emerging%20as%20a%20dominant%20bottleneck%20during%20training%20and%20autoregressive%20decoding.%20We%20propose%20%5Ctextit%7Blow-rank%20KV%20adaptation%7D%20%28LRKV%29%2C%20a%20simple%20modification%20of%20multi-head%20attention%20that%20reduces%20KV%20cache%20memory%20by%20exploiting%20redundancy%20across%20attention%20heads%20while%20preserving%20full%20token-level%20resolution.%20Each%20layer%20uses%20a%20shared%20full-rank%20KV%20projection%20augmented%20with%20low-rank%2C%20head-specific%20residuals%2C%20yielding%20a%20continuous%20trade-off%20between%20complete%20sharing%20and%20fully%20independent%20attention.%0A%20%20LRKV%20is%20a%20drop-in%20replacement%20for%20standard%20multi-head%20attention%20and%20directly%20subsumes%20query-sharing%20approaches%20such%20as%20multi-query%20and%20grouped-query%20attention%2C%20while%20remaining%20distinct%20from%20latent-compression%20methods%20such%20as%20multi-latent%20attention%20%28MLA%29.%20Across%20large-scale%20pretraining%20experiments%2C%20LRKV%20consistently%20achieves%20faster%20loss%20reduction%2C%20lower%20validation%20perplexity%2C%20and%20stronger%20downstream%20task%20performance%20than%20standard%20attention%2C%20MQA/GQA%2C%20and%20MLA.%20At%20the%202.5B%20scale%2C%20LRKV%20outperforms%20standard%20attention%20while%20using%20roughly%20half%20the%20KV%20cache%2C%20and%20reaches%20equivalent%20model%20quality%20with%20up%20to%20%5Ctextbf%7B20-25%5C%25%20less%20training%20compute%7D%20when%20measured%20in%20cumulative%20FLOPs.%20To%20explain%20these%20gains%2C%20we%20analyze%20attention%20head%20structure%20in%20operator%20space%20and%20show%20that%20LRKV%20preserves%20nearly%20all%20functional%20head%20diversity%20relative%20to%20standard%20attention%2C%20whereas%20more%20aggressive%20KV-sharing%20mechanisms%20rely%20on%20compensatory%20query%20specialization.%20Together%2C%20these%20results%20establish%20LRKV%20as%20a%20practical%20and%20effective%20attention%20mechanism%20for%20scaling%20Transformer%20pretraining%20under%20memory-%20and%20compute-constrained%20regimes.&entry.1838667208=http%3A//arxiv.org/abs/2601.11471v1&entry.124074799=Read"},
{"title": "ShapeR: Robust Conditional 3D Shape Generation from Casual Captures", "author": "Yawar Siddiqui and Duncan Frost and Samir Aroudj and Armen Avetisyan and Henry Howard-Jenkins and Daniel DeTone and Pierre Moulon and Qirui Wu and Zhengqin Li and Julian Straub and Richard Newcombe and Jakob Engel", "abstract": "Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.", "link": "http://arxiv.org/abs/2601.11514v1", "date": "2026-01-16", "relevancy": 2.4859, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6368}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6154}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShapeR%3A%20Robust%20Conditional%203D%20Shape%20Generation%20from%20Casual%20Captures&body=Title%3A%20ShapeR%3A%20Robust%20Conditional%203D%20Shape%20Generation%20from%20Casual%20Captures%0AAuthor%3A%20Yawar%20Siddiqui%20and%20Duncan%20Frost%20and%20Samir%20Aroudj%20and%20Armen%20Avetisyan%20and%20Henry%20Howard-Jenkins%20and%20Daniel%20DeTone%20and%20Pierre%20Moulon%20and%20Qirui%20Wu%20and%20Zhengqin%20Li%20and%20Julian%20Straub%20and%20Richard%20Newcombe%20and%20Jakob%20Engel%0AAbstract%3A%20Recent%20advances%20in%203D%20shape%20generation%20have%20achieved%20impressive%20results%2C%20but%20most%20existing%20methods%20rely%20on%20clean%2C%20unoccluded%2C%20and%20well-segmented%20inputs.%20Such%20conditions%20are%20rarely%20met%20in%20real-world%20scenarios.%20We%20present%20ShapeR%2C%20a%20novel%20approach%20for%20conditional%203D%20object%20shape%20generation%20from%20casually%20captured%20sequences.%20Given%20an%20image%20sequence%2C%20we%20leverage%20off-the-shelf%20visual-inertial%20SLAM%2C%203D%20detection%20algorithms%2C%20and%20vision-language%20models%20to%20extract%2C%20for%20each%20object%2C%20a%20set%20of%20sparse%20SLAM%20points%2C%20posed%20multi-view%20images%2C%20and%20machine-generated%20captions.%20A%20rectified%20flow%20transformer%20trained%20to%20effectively%20condition%20on%20these%20modalities%20then%20generates%20high-fidelity%20metric%203D%20shapes.%20To%20ensure%20robustness%20to%20the%20challenges%20of%20casually%20captured%20data%2C%20we%20employ%20a%20range%20of%20techniques%20including%20on-the-fly%20compositional%20augmentations%2C%20a%20curriculum%20training%20scheme%20spanning%20object-%20and%20scene-level%20datasets%2C%20and%20strategies%20to%20handle%20background%20clutter.%20Additionally%2C%20we%20introduce%20a%20new%20evaluation%20benchmark%20comprising%20178%20in-the-wild%20objects%20across%207%20real-world%20scenes%20with%20geometry%20annotations.%20Experiments%20show%20that%20ShapeR%20significantly%20outperforms%20existing%20approaches%20in%20this%20challenging%20setting%2C%20achieving%20an%20improvement%20of%202.7x%20in%20Chamfer%20distance%20compared%20to%20state%20of%20the%20art.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapeR%253A%2520Robust%2520Conditional%25203D%2520Shape%2520Generation%2520from%2520Casual%2520Captures%26entry.906535625%3DYawar%2520Siddiqui%2520and%2520Duncan%2520Frost%2520and%2520Samir%2520Aroudj%2520and%2520Armen%2520Avetisyan%2520and%2520Henry%2520Howard-Jenkins%2520and%2520Daniel%2520DeTone%2520and%2520Pierre%2520Moulon%2520and%2520Qirui%2520Wu%2520and%2520Zhengqin%2520Li%2520and%2520Julian%2520Straub%2520and%2520Richard%2520Newcombe%2520and%2520Jakob%2520Engel%26entry.1292438233%3DRecent%2520advances%2520in%25203D%2520shape%2520generation%2520have%2520achieved%2520impressive%2520results%252C%2520but%2520most%2520existing%2520methods%2520rely%2520on%2520clean%252C%2520unoccluded%252C%2520and%2520well-segmented%2520inputs.%2520Such%2520conditions%2520are%2520rarely%2520met%2520in%2520real-world%2520scenarios.%2520We%2520present%2520ShapeR%252C%2520a%2520novel%2520approach%2520for%2520conditional%25203D%2520object%2520shape%2520generation%2520from%2520casually%2520captured%2520sequences.%2520Given%2520an%2520image%2520sequence%252C%2520we%2520leverage%2520off-the-shelf%2520visual-inertial%2520SLAM%252C%25203D%2520detection%2520algorithms%252C%2520and%2520vision-language%2520models%2520to%2520extract%252C%2520for%2520each%2520object%252C%2520a%2520set%2520of%2520sparse%2520SLAM%2520points%252C%2520posed%2520multi-view%2520images%252C%2520and%2520machine-generated%2520captions.%2520A%2520rectified%2520flow%2520transformer%2520trained%2520to%2520effectively%2520condition%2520on%2520these%2520modalities%2520then%2520generates%2520high-fidelity%2520metric%25203D%2520shapes.%2520To%2520ensure%2520robustness%2520to%2520the%2520challenges%2520of%2520casually%2520captured%2520data%252C%2520we%2520employ%2520a%2520range%2520of%2520techniques%2520including%2520on-the-fly%2520compositional%2520augmentations%252C%2520a%2520curriculum%2520training%2520scheme%2520spanning%2520object-%2520and%2520scene-level%2520datasets%252C%2520and%2520strategies%2520to%2520handle%2520background%2520clutter.%2520Additionally%252C%2520we%2520introduce%2520a%2520new%2520evaluation%2520benchmark%2520comprising%2520178%2520in-the-wild%2520objects%2520across%25207%2520real-world%2520scenes%2520with%2520geometry%2520annotations.%2520Experiments%2520show%2520that%2520ShapeR%2520significantly%2520outperforms%2520existing%2520approaches%2520in%2520this%2520challenging%2520setting%252C%2520achieving%2520an%2520improvement%2520of%25202.7x%2520in%2520Chamfer%2520distance%2520compared%2520to%2520state%2520of%2520the%2520art.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapeR%3A%20Robust%20Conditional%203D%20Shape%20Generation%20from%20Casual%20Captures&entry.906535625=Yawar%20Siddiqui%20and%20Duncan%20Frost%20and%20Samir%20Aroudj%20and%20Armen%20Avetisyan%20and%20Henry%20Howard-Jenkins%20and%20Daniel%20DeTone%20and%20Pierre%20Moulon%20and%20Qirui%20Wu%20and%20Zhengqin%20Li%20and%20Julian%20Straub%20and%20Richard%20Newcombe%20and%20Jakob%20Engel&entry.1292438233=Recent%20advances%20in%203D%20shape%20generation%20have%20achieved%20impressive%20results%2C%20but%20most%20existing%20methods%20rely%20on%20clean%2C%20unoccluded%2C%20and%20well-segmented%20inputs.%20Such%20conditions%20are%20rarely%20met%20in%20real-world%20scenarios.%20We%20present%20ShapeR%2C%20a%20novel%20approach%20for%20conditional%203D%20object%20shape%20generation%20from%20casually%20captured%20sequences.%20Given%20an%20image%20sequence%2C%20we%20leverage%20off-the-shelf%20visual-inertial%20SLAM%2C%203D%20detection%20algorithms%2C%20and%20vision-language%20models%20to%20extract%2C%20for%20each%20object%2C%20a%20set%20of%20sparse%20SLAM%20points%2C%20posed%20multi-view%20images%2C%20and%20machine-generated%20captions.%20A%20rectified%20flow%20transformer%20trained%20to%20effectively%20condition%20on%20these%20modalities%20then%20generates%20high-fidelity%20metric%203D%20shapes.%20To%20ensure%20robustness%20to%20the%20challenges%20of%20casually%20captured%20data%2C%20we%20employ%20a%20range%20of%20techniques%20including%20on-the-fly%20compositional%20augmentations%2C%20a%20curriculum%20training%20scheme%20spanning%20object-%20and%20scene-level%20datasets%2C%20and%20strategies%20to%20handle%20background%20clutter.%20Additionally%2C%20we%20introduce%20a%20new%20evaluation%20benchmark%20comprising%20178%20in-the-wild%20objects%20across%207%20real-world%20scenes%20with%20geometry%20annotations.%20Experiments%20show%20that%20ShapeR%20significantly%20outperforms%20existing%20approaches%20in%20this%20challenging%20setting%2C%20achieving%20an%20improvement%20of%202.7x%20in%20Chamfer%20distance%20compared%20to%20state%20of%20the%20art.&entry.1838667208=http%3A//arxiv.org/abs/2601.11514v1&entry.124074799=Read"},
{"title": "FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning", "author": "Zhihan Yang and Jiaqi Wei and Xiang Zhang and Haoyu Dong and Yiwen Wang and Xiaoke Guo and Pengkun Zhang and Yiwei Xu and Chenyu You", "abstract": "Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2601.11311v1", "date": "2026-01-16", "relevancy": 2.4738, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5013}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4932}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FORESTLLM%3A%20Large%20Language%20Models%20Make%20Random%20Forest%20Great%20on%20Few-shot%20Tabular%20Learning&body=Title%3A%20FORESTLLM%3A%20Large%20Language%20Models%20Make%20Random%20Forest%20Great%20on%20Few-shot%20Tabular%20Learning%0AAuthor%3A%20Zhihan%20Yang%20and%20Jiaqi%20Wei%20and%20Xiang%20Zhang%20and%20Haoyu%20Dong%20and%20Yiwen%20Wang%20and%20Xiaoke%20Guo%20and%20Pengkun%20Zhang%20and%20Yiwei%20Xu%20and%20Chenyu%20You%0AAbstract%3A%20Tabular%20data%20high-stakes%20critical%20decision-making%20in%20domains%20such%20as%20finance%2C%20healthcare%2C%20and%20scientific%20discovery.%20Yet%2C%20learning%20effectively%20from%20tabular%20data%20in%20few-shot%20settings%2C%20where%20labeled%20examples%20are%20scarce%2C%20remains%20a%20fundamental%20challenge.%20Traditional%20tree-based%20methods%20often%20falter%20in%20these%20regimes%20due%20to%20their%20reliance%20on%20statistical%20purity%20metrics%2C%20which%20become%20unstable%20and%20prone%20to%20overfitting%20with%20limited%20supervision.%20At%20the%20same%20time%2C%20direct%20applications%20of%20large%20language%20models%20%28LLMs%29%20often%20overlook%20its%20inherent%20structure%2C%20leading%20to%20suboptimal%20performance.%20To%20overcome%20these%20limitations%2C%20we%20propose%20FORESTLLM%2C%20a%20novel%20framework%20that%20unifies%20the%20structural%20inductive%20biases%20of%20decision%20forests%20with%20the%20semantic%20reasoning%20capabilities%20of%20LLMs.%20Crucially%2C%20FORESTLLM%20leverages%20the%20LLM%20only%20during%20training%2C%20treating%20it%20as%20an%20offline%20model%20designer%20that%20encodes%20rich%2C%20contextual%20knowledge%20into%20a%20lightweight%2C%20interpretable%20forest%20model%2C%20eliminating%20the%20need%20for%20LLM%20inference%20at%20test%20time.%20Our%20method%20is%20two-fold.%20First%2C%20we%20introduce%20a%20semantic%20splitting%20criterion%20in%20which%20the%20LLM%20evaluates%20candidate%20partitions%20based%20on%20their%20coherence%20over%20both%20labeled%20and%20unlabeled%20data%2C%20enabling%20the%20induction%20of%20more%20robust%20and%20generalizable%20tree%20structures%20under%20few-shot%20supervision.%20Second%2C%20we%20propose%20a%20one-time%20in-context%20inference%20mechanism%20for%20leaf%20node%20stabilization%2C%20where%20the%20LLM%20distills%20the%20decision%20path%20and%20its%20supporting%20examples%20into%20a%20concise%2C%20deterministic%20prediction%2C%20replacing%20noisy%20empirical%20estimates%20with%20semantically%20informed%20outputs.%20Across%20a%20diverse%20suite%20of%20few-shot%20classification%20and%20regression%20benchmarks%2C%20FORESTLLM%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFORESTLLM%253A%2520Large%2520Language%2520Models%2520Make%2520Random%2520Forest%2520Great%2520on%2520Few-shot%2520Tabular%2520Learning%26entry.906535625%3DZhihan%2520Yang%2520and%2520Jiaqi%2520Wei%2520and%2520Xiang%2520Zhang%2520and%2520Haoyu%2520Dong%2520and%2520Yiwen%2520Wang%2520and%2520Xiaoke%2520Guo%2520and%2520Pengkun%2520Zhang%2520and%2520Yiwei%2520Xu%2520and%2520Chenyu%2520You%26entry.1292438233%3DTabular%2520data%2520high-stakes%2520critical%2520decision-making%2520in%2520domains%2520such%2520as%2520finance%252C%2520healthcare%252C%2520and%2520scientific%2520discovery.%2520Yet%252C%2520learning%2520effectively%2520from%2520tabular%2520data%2520in%2520few-shot%2520settings%252C%2520where%2520labeled%2520examples%2520are%2520scarce%252C%2520remains%2520a%2520fundamental%2520challenge.%2520Traditional%2520tree-based%2520methods%2520often%2520falter%2520in%2520these%2520regimes%2520due%2520to%2520their%2520reliance%2520on%2520statistical%2520purity%2520metrics%252C%2520which%2520become%2520unstable%2520and%2520prone%2520to%2520overfitting%2520with%2520limited%2520supervision.%2520At%2520the%2520same%2520time%252C%2520direct%2520applications%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520often%2520overlook%2520its%2520inherent%2520structure%252C%2520leading%2520to%2520suboptimal%2520performance.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520FORESTLLM%252C%2520a%2520novel%2520framework%2520that%2520unifies%2520the%2520structural%2520inductive%2520biases%2520of%2520decision%2520forests%2520with%2520the%2520semantic%2520reasoning%2520capabilities%2520of%2520LLMs.%2520Crucially%252C%2520FORESTLLM%2520leverages%2520the%2520LLM%2520only%2520during%2520training%252C%2520treating%2520it%2520as%2520an%2520offline%2520model%2520designer%2520that%2520encodes%2520rich%252C%2520contextual%2520knowledge%2520into%2520a%2520lightweight%252C%2520interpretable%2520forest%2520model%252C%2520eliminating%2520the%2520need%2520for%2520LLM%2520inference%2520at%2520test%2520time.%2520Our%2520method%2520is%2520two-fold.%2520First%252C%2520we%2520introduce%2520a%2520semantic%2520splitting%2520criterion%2520in%2520which%2520the%2520LLM%2520evaluates%2520candidate%2520partitions%2520based%2520on%2520their%2520coherence%2520over%2520both%2520labeled%2520and%2520unlabeled%2520data%252C%2520enabling%2520the%2520induction%2520of%2520more%2520robust%2520and%2520generalizable%2520tree%2520structures%2520under%2520few-shot%2520supervision.%2520Second%252C%2520we%2520propose%2520a%2520one-time%2520in-context%2520inference%2520mechanism%2520for%2520leaf%2520node%2520stabilization%252C%2520where%2520the%2520LLM%2520distills%2520the%2520decision%2520path%2520and%2520its%2520supporting%2520examples%2520into%2520a%2520concise%252C%2520deterministic%2520prediction%252C%2520replacing%2520noisy%2520empirical%2520estimates%2520with%2520semantically%2520informed%2520outputs.%2520Across%2520a%2520diverse%2520suite%2520of%2520few-shot%2520classification%2520and%2520regression%2520benchmarks%252C%2520FORESTLLM%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FORESTLLM%3A%20Large%20Language%20Models%20Make%20Random%20Forest%20Great%20on%20Few-shot%20Tabular%20Learning&entry.906535625=Zhihan%20Yang%20and%20Jiaqi%20Wei%20and%20Xiang%20Zhang%20and%20Haoyu%20Dong%20and%20Yiwen%20Wang%20and%20Xiaoke%20Guo%20and%20Pengkun%20Zhang%20and%20Yiwei%20Xu%20and%20Chenyu%20You&entry.1292438233=Tabular%20data%20high-stakes%20critical%20decision-making%20in%20domains%20such%20as%20finance%2C%20healthcare%2C%20and%20scientific%20discovery.%20Yet%2C%20learning%20effectively%20from%20tabular%20data%20in%20few-shot%20settings%2C%20where%20labeled%20examples%20are%20scarce%2C%20remains%20a%20fundamental%20challenge.%20Traditional%20tree-based%20methods%20often%20falter%20in%20these%20regimes%20due%20to%20their%20reliance%20on%20statistical%20purity%20metrics%2C%20which%20become%20unstable%20and%20prone%20to%20overfitting%20with%20limited%20supervision.%20At%20the%20same%20time%2C%20direct%20applications%20of%20large%20language%20models%20%28LLMs%29%20often%20overlook%20its%20inherent%20structure%2C%20leading%20to%20suboptimal%20performance.%20To%20overcome%20these%20limitations%2C%20we%20propose%20FORESTLLM%2C%20a%20novel%20framework%20that%20unifies%20the%20structural%20inductive%20biases%20of%20decision%20forests%20with%20the%20semantic%20reasoning%20capabilities%20of%20LLMs.%20Crucially%2C%20FORESTLLM%20leverages%20the%20LLM%20only%20during%20training%2C%20treating%20it%20as%20an%20offline%20model%20designer%20that%20encodes%20rich%2C%20contextual%20knowledge%20into%20a%20lightweight%2C%20interpretable%20forest%20model%2C%20eliminating%20the%20need%20for%20LLM%20inference%20at%20test%20time.%20Our%20method%20is%20two-fold.%20First%2C%20we%20introduce%20a%20semantic%20splitting%20criterion%20in%20which%20the%20LLM%20evaluates%20candidate%20partitions%20based%20on%20their%20coherence%20over%20both%20labeled%20and%20unlabeled%20data%2C%20enabling%20the%20induction%20of%20more%20robust%20and%20generalizable%20tree%20structures%20under%20few-shot%20supervision.%20Second%2C%20we%20propose%20a%20one-time%20in-context%20inference%20mechanism%20for%20leaf%20node%20stabilization%2C%20where%20the%20LLM%20distills%20the%20decision%20path%20and%20its%20supporting%20examples%20into%20a%20concise%2C%20deterministic%20prediction%2C%20replacing%20noisy%20empirical%20estimates%20with%20semantically%20informed%20outputs.%20Across%20a%20diverse%20suite%20of%20few-shot%20classification%20and%20regression%20benchmarks%2C%20FORESTLLM%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.11311v1&entry.124074799=Read"},
{"title": "SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation", "author": "Aiman Al Masoud and Marco Arazzi and Antonino Nocera", "abstract": "Retrieval-Augmented Generation (RAG) has attracted significant attention due to its ability to combine the generative capabilities of Large Language Models (LLMs) with knowledge obtained through efficient retrieval mechanisms over large-scale data collections. Currently, the majority of existing approaches overlook the risks associated with exposing sensitive or access-controlled information directly to the generation model. Only a few approaches propose techniques to instruct the generative model to refrain from disclosing sensitive information; however, recent studies have also demonstrated that LLMs remain vulnerable to prompt injection attacks that can override intended behavioral constraints. For these reasons, we propose a novel approach to Selective Disclosure in Retrieval-Augmented Generation, called SD-RAG, which decouples the enforcement of security and privacy constraints from the generation process itself. Rather than relying on prompt-level safeguards, SD-RAG applies sanitization and disclosure controls during the retrieval phase, prior to augmenting the language model's input. Moreover, we introduce a semantic mechanism to allow the ingestion of human-readable dynamic security and privacy constraints together with an optimized graph-based data model that supports fine-grained, policy-aware retrieval. Our experimental evaluation demonstrates the superiority of SD-RAG over baseline existing approaches, achieving up to a $58\\%$ improvement in the privacy score, while also showing a strong resilience to prompt injection attacks targeting the generative model.", "link": "http://arxiv.org/abs/2601.11199v1", "date": "2026-01-16", "relevancy": 2.4494, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5031}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4881}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SD-RAG%3A%20A%20Prompt-Injection-Resilient%20Framework%20for%20Selective%20Disclosure%20in%20Retrieval-Augmented%20Generation&body=Title%3A%20SD-RAG%3A%20A%20Prompt-Injection-Resilient%20Framework%20for%20Selective%20Disclosure%20in%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Aiman%20Al%20Masoud%20and%20Marco%20Arazzi%20and%20Antonino%20Nocera%0AAbstract%3A%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20attracted%20significant%20attention%20due%20to%20its%20ability%20to%20combine%20the%20generative%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20knowledge%20obtained%20through%20efficient%20retrieval%20mechanisms%20over%20large-scale%20data%20collections.%20Currently%2C%20the%20majority%20of%20existing%20approaches%20overlook%20the%20risks%20associated%20with%20exposing%20sensitive%20or%20access-controlled%20information%20directly%20to%20the%20generation%20model.%20Only%20a%20few%20approaches%20propose%20techniques%20to%20instruct%20the%20generative%20model%20to%20refrain%20from%20disclosing%20sensitive%20information%3B%20however%2C%20recent%20studies%20have%20also%20demonstrated%20that%20LLMs%20remain%20vulnerable%20to%20prompt%20injection%20attacks%20that%20can%20override%20intended%20behavioral%20constraints.%20For%20these%20reasons%2C%20we%20propose%20a%20novel%20approach%20to%20Selective%20Disclosure%20in%20Retrieval-Augmented%20Generation%2C%20called%20SD-RAG%2C%20which%20decouples%20the%20enforcement%20of%20security%20and%20privacy%20constraints%20from%20the%20generation%20process%20itself.%20Rather%20than%20relying%20on%20prompt-level%20safeguards%2C%20SD-RAG%20applies%20sanitization%20and%20disclosure%20controls%20during%20the%20retrieval%20phase%2C%20prior%20to%20augmenting%20the%20language%20model%27s%20input.%20Moreover%2C%20we%20introduce%20a%20semantic%20mechanism%20to%20allow%20the%20ingestion%20of%20human-readable%20dynamic%20security%20and%20privacy%20constraints%20together%20with%20an%20optimized%20graph-based%20data%20model%20that%20supports%20fine-grained%2C%20policy-aware%20retrieval.%20Our%20experimental%20evaluation%20demonstrates%20the%20superiority%20of%20SD-RAG%20over%20baseline%20existing%20approaches%2C%20achieving%20up%20to%20a%20%2458%5C%25%24%20improvement%20in%20the%20privacy%20score%2C%20while%20also%20showing%20a%20strong%20resilience%20to%20prompt%20injection%20attacks%20targeting%20the%20generative%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSD-RAG%253A%2520A%2520Prompt-Injection-Resilient%2520Framework%2520for%2520Selective%2520Disclosure%2520in%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DAiman%2520Al%2520Masoud%2520and%2520Marco%2520Arazzi%2520and%2520Antonino%2520Nocera%26entry.1292438233%3DRetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520has%2520attracted%2520significant%2520attention%2520due%2520to%2520its%2520ability%2520to%2520combine%2520the%2520generative%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520knowledge%2520obtained%2520through%2520efficient%2520retrieval%2520mechanisms%2520over%2520large-scale%2520data%2520collections.%2520Currently%252C%2520the%2520majority%2520of%2520existing%2520approaches%2520overlook%2520the%2520risks%2520associated%2520with%2520exposing%2520sensitive%2520or%2520access-controlled%2520information%2520directly%2520to%2520the%2520generation%2520model.%2520Only%2520a%2520few%2520approaches%2520propose%2520techniques%2520to%2520instruct%2520the%2520generative%2520model%2520to%2520refrain%2520from%2520disclosing%2520sensitive%2520information%253B%2520however%252C%2520recent%2520studies%2520have%2520also%2520demonstrated%2520that%2520LLMs%2520remain%2520vulnerable%2520to%2520prompt%2520injection%2520attacks%2520that%2520can%2520override%2520intended%2520behavioral%2520constraints.%2520For%2520these%2520reasons%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520Selective%2520Disclosure%2520in%2520Retrieval-Augmented%2520Generation%252C%2520called%2520SD-RAG%252C%2520which%2520decouples%2520the%2520enforcement%2520of%2520security%2520and%2520privacy%2520constraints%2520from%2520the%2520generation%2520process%2520itself.%2520Rather%2520than%2520relying%2520on%2520prompt-level%2520safeguards%252C%2520SD-RAG%2520applies%2520sanitization%2520and%2520disclosure%2520controls%2520during%2520the%2520retrieval%2520phase%252C%2520prior%2520to%2520augmenting%2520the%2520language%2520model%2527s%2520input.%2520Moreover%252C%2520we%2520introduce%2520a%2520semantic%2520mechanism%2520to%2520allow%2520the%2520ingestion%2520of%2520human-readable%2520dynamic%2520security%2520and%2520privacy%2520constraints%2520together%2520with%2520an%2520optimized%2520graph-based%2520data%2520model%2520that%2520supports%2520fine-grained%252C%2520policy-aware%2520retrieval.%2520Our%2520experimental%2520evaluation%2520demonstrates%2520the%2520superiority%2520of%2520SD-RAG%2520over%2520baseline%2520existing%2520approaches%252C%2520achieving%2520up%2520to%2520a%2520%252458%255C%2525%2524%2520improvement%2520in%2520the%2520privacy%2520score%252C%2520while%2520also%2520showing%2520a%2520strong%2520resilience%2520to%2520prompt%2520injection%2520attacks%2520targeting%2520the%2520generative%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SD-RAG%3A%20A%20Prompt-Injection-Resilient%20Framework%20for%20Selective%20Disclosure%20in%20Retrieval-Augmented%20Generation&entry.906535625=Aiman%20Al%20Masoud%20and%20Marco%20Arazzi%20and%20Antonino%20Nocera&entry.1292438233=Retrieval-Augmented%20Generation%20%28RAG%29%20has%20attracted%20significant%20attention%20due%20to%20its%20ability%20to%20combine%20the%20generative%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20knowledge%20obtained%20through%20efficient%20retrieval%20mechanisms%20over%20large-scale%20data%20collections.%20Currently%2C%20the%20majority%20of%20existing%20approaches%20overlook%20the%20risks%20associated%20with%20exposing%20sensitive%20or%20access-controlled%20information%20directly%20to%20the%20generation%20model.%20Only%20a%20few%20approaches%20propose%20techniques%20to%20instruct%20the%20generative%20model%20to%20refrain%20from%20disclosing%20sensitive%20information%3B%20however%2C%20recent%20studies%20have%20also%20demonstrated%20that%20LLMs%20remain%20vulnerable%20to%20prompt%20injection%20attacks%20that%20can%20override%20intended%20behavioral%20constraints.%20For%20these%20reasons%2C%20we%20propose%20a%20novel%20approach%20to%20Selective%20Disclosure%20in%20Retrieval-Augmented%20Generation%2C%20called%20SD-RAG%2C%20which%20decouples%20the%20enforcement%20of%20security%20and%20privacy%20constraints%20from%20the%20generation%20process%20itself.%20Rather%20than%20relying%20on%20prompt-level%20safeguards%2C%20SD-RAG%20applies%20sanitization%20and%20disclosure%20controls%20during%20the%20retrieval%20phase%2C%20prior%20to%20augmenting%20the%20language%20model%27s%20input.%20Moreover%2C%20we%20introduce%20a%20semantic%20mechanism%20to%20allow%20the%20ingestion%20of%20human-readable%20dynamic%20security%20and%20privacy%20constraints%20together%20with%20an%20optimized%20graph-based%20data%20model%20that%20supports%20fine-grained%2C%20policy-aware%20retrieval.%20Our%20experimental%20evaluation%20demonstrates%20the%20superiority%20of%20SD-RAG%20over%20baseline%20existing%20approaches%2C%20achieving%20up%20to%20a%20%2458%5C%25%24%20improvement%20in%20the%20privacy%20score%2C%20while%20also%20showing%20a%20strong%20resilience%20to%20prompt%20injection%20attacks%20targeting%20the%20generative%20model.&entry.1838667208=http%3A//arxiv.org/abs/2601.11199v1&entry.124074799=Read"},
{"title": "Zero-Shot Detection of Elastic Transient Morphology Across Physical Systems", "author": "Jose S\u00e1nchez Andreu", "abstract": "We test whether a representation learned from interferometric strain transients in gravitational-wave observatories can act as a frozen morphology-sensitive operator for unseen sensors, provided the target signals preserve coherent elastic transient structure. Using a neural encoder trained exclusively on non-Gaussian instrumental glitches, we perform strict zero-shot anomaly analysis on rolling-element bearings without retraining, fine-tuning, or target-domain labels.\n  On the IMS-NASA run-to-failure dataset, the operator yields a monotonic health index HI(t) = s0.99(t)/tau normalized to an early-life reference distribution, enabling fixed false-alarm monitoring at 1-q = 1e-3 with tau = Q0.999(P0). In discrete fault regimes (CWRU), it achieves strong window-level discrimination (AUC_win about 0.90) and file-level separability approaching unity (AUC_file about 0.99). Electrically dominated vibration signals (VSB) show weak, non-selective behavior, delineating a physical boundary for transfer.\n  Under a matched IMS controlled-split protocol, a generic EfficientNet-B0 encoder pretrained on ImageNet collapses in the intermittent regime (Lambda_tail about 2), while the interferometric operator retains strong extreme-event selectivity (Lambda_tail about 860), indicating that the effect is not a generic property of CNN features. Controlled morphology-destruction transformations selectively degrade performance despite per-window normalization, consistent with sensitivity to coherent time-frequency organization rather than marginal amplitude statistics.", "link": "http://arxiv.org/abs/2601.11415v1", "date": "2026-01-16", "relevancy": 2.4456, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4912}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4908}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Detection%20of%20Elastic%20Transient%20Morphology%20Across%20Physical%20Systems&body=Title%3A%20Zero-Shot%20Detection%20of%20Elastic%20Transient%20Morphology%20Across%20Physical%20Systems%0AAuthor%3A%20Jose%20S%C3%A1nchez%20Andreu%0AAbstract%3A%20We%20test%20whether%20a%20representation%20learned%20from%20interferometric%20strain%20transients%20in%20gravitational-wave%20observatories%20can%20act%20as%20a%20frozen%20morphology-sensitive%20operator%20for%20unseen%20sensors%2C%20provided%20the%20target%20signals%20preserve%20coherent%20elastic%20transient%20structure.%20Using%20a%20neural%20encoder%20trained%20exclusively%20on%20non-Gaussian%20instrumental%20glitches%2C%20we%20perform%20strict%20zero-shot%20anomaly%20analysis%20on%20rolling-element%20bearings%20without%20retraining%2C%20fine-tuning%2C%20or%20target-domain%20labels.%0A%20%20On%20the%20IMS-NASA%20run-to-failure%20dataset%2C%20the%20operator%20yields%20a%20monotonic%20health%20index%20HI%28t%29%20%3D%20s0.99%28t%29/tau%20normalized%20to%20an%20early-life%20reference%20distribution%2C%20enabling%20fixed%20false-alarm%20monitoring%20at%201-q%20%3D%201e-3%20with%20tau%20%3D%20Q0.999%28P0%29.%20In%20discrete%20fault%20regimes%20%28CWRU%29%2C%20it%20achieves%20strong%20window-level%20discrimination%20%28AUC_win%20about%200.90%29%20and%20file-level%20separability%20approaching%20unity%20%28AUC_file%20about%200.99%29.%20Electrically%20dominated%20vibration%20signals%20%28VSB%29%20show%20weak%2C%20non-selective%20behavior%2C%20delineating%20a%20physical%20boundary%20for%20transfer.%0A%20%20Under%20a%20matched%20IMS%20controlled-split%20protocol%2C%20a%20generic%20EfficientNet-B0%20encoder%20pretrained%20on%20ImageNet%20collapses%20in%20the%20intermittent%20regime%20%28Lambda_tail%20about%202%29%2C%20while%20the%20interferometric%20operator%20retains%20strong%20extreme-event%20selectivity%20%28Lambda_tail%20about%20860%29%2C%20indicating%20that%20the%20effect%20is%20not%20a%20generic%20property%20of%20CNN%20features.%20Controlled%20morphology-destruction%20transformations%20selectively%20degrade%20performance%20despite%20per-window%20normalization%2C%20consistent%20with%20sensitivity%20to%20coherent%20time-frequency%20organization%20rather%20than%20marginal%20amplitude%20statistics.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Detection%2520of%2520Elastic%2520Transient%2520Morphology%2520Across%2520Physical%2520Systems%26entry.906535625%3DJose%2520S%25C3%25A1nchez%2520Andreu%26entry.1292438233%3DWe%2520test%2520whether%2520a%2520representation%2520learned%2520from%2520interferometric%2520strain%2520transients%2520in%2520gravitational-wave%2520observatories%2520can%2520act%2520as%2520a%2520frozen%2520morphology-sensitive%2520operator%2520for%2520unseen%2520sensors%252C%2520provided%2520the%2520target%2520signals%2520preserve%2520coherent%2520elastic%2520transient%2520structure.%2520Using%2520a%2520neural%2520encoder%2520trained%2520exclusively%2520on%2520non-Gaussian%2520instrumental%2520glitches%252C%2520we%2520perform%2520strict%2520zero-shot%2520anomaly%2520analysis%2520on%2520rolling-element%2520bearings%2520without%2520retraining%252C%2520fine-tuning%252C%2520or%2520target-domain%2520labels.%250A%2520%2520On%2520the%2520IMS-NASA%2520run-to-failure%2520dataset%252C%2520the%2520operator%2520yields%2520a%2520monotonic%2520health%2520index%2520HI%2528t%2529%2520%253D%2520s0.99%2528t%2529/tau%2520normalized%2520to%2520an%2520early-life%2520reference%2520distribution%252C%2520enabling%2520fixed%2520false-alarm%2520monitoring%2520at%25201-q%2520%253D%25201e-3%2520with%2520tau%2520%253D%2520Q0.999%2528P0%2529.%2520In%2520discrete%2520fault%2520regimes%2520%2528CWRU%2529%252C%2520it%2520achieves%2520strong%2520window-level%2520discrimination%2520%2528AUC_win%2520about%25200.90%2529%2520and%2520file-level%2520separability%2520approaching%2520unity%2520%2528AUC_file%2520about%25200.99%2529.%2520Electrically%2520dominated%2520vibration%2520signals%2520%2528VSB%2529%2520show%2520weak%252C%2520non-selective%2520behavior%252C%2520delineating%2520a%2520physical%2520boundary%2520for%2520transfer.%250A%2520%2520Under%2520a%2520matched%2520IMS%2520controlled-split%2520protocol%252C%2520a%2520generic%2520EfficientNet-B0%2520encoder%2520pretrained%2520on%2520ImageNet%2520collapses%2520in%2520the%2520intermittent%2520regime%2520%2528Lambda_tail%2520about%25202%2529%252C%2520while%2520the%2520interferometric%2520operator%2520retains%2520strong%2520extreme-event%2520selectivity%2520%2528Lambda_tail%2520about%2520860%2529%252C%2520indicating%2520that%2520the%2520effect%2520is%2520not%2520a%2520generic%2520property%2520of%2520CNN%2520features.%2520Controlled%2520morphology-destruction%2520transformations%2520selectively%2520degrade%2520performance%2520despite%2520per-window%2520normalization%252C%2520consistent%2520with%2520sensitivity%2520to%2520coherent%2520time-frequency%2520organization%2520rather%2520than%2520marginal%2520amplitude%2520statistics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Detection%20of%20Elastic%20Transient%20Morphology%20Across%20Physical%20Systems&entry.906535625=Jose%20S%C3%A1nchez%20Andreu&entry.1292438233=We%20test%20whether%20a%20representation%20learned%20from%20interferometric%20strain%20transients%20in%20gravitational-wave%20observatories%20can%20act%20as%20a%20frozen%20morphology-sensitive%20operator%20for%20unseen%20sensors%2C%20provided%20the%20target%20signals%20preserve%20coherent%20elastic%20transient%20structure.%20Using%20a%20neural%20encoder%20trained%20exclusively%20on%20non-Gaussian%20instrumental%20glitches%2C%20we%20perform%20strict%20zero-shot%20anomaly%20analysis%20on%20rolling-element%20bearings%20without%20retraining%2C%20fine-tuning%2C%20or%20target-domain%20labels.%0A%20%20On%20the%20IMS-NASA%20run-to-failure%20dataset%2C%20the%20operator%20yields%20a%20monotonic%20health%20index%20HI%28t%29%20%3D%20s0.99%28t%29/tau%20normalized%20to%20an%20early-life%20reference%20distribution%2C%20enabling%20fixed%20false-alarm%20monitoring%20at%201-q%20%3D%201e-3%20with%20tau%20%3D%20Q0.999%28P0%29.%20In%20discrete%20fault%20regimes%20%28CWRU%29%2C%20it%20achieves%20strong%20window-level%20discrimination%20%28AUC_win%20about%200.90%29%20and%20file-level%20separability%20approaching%20unity%20%28AUC_file%20about%200.99%29.%20Electrically%20dominated%20vibration%20signals%20%28VSB%29%20show%20weak%2C%20non-selective%20behavior%2C%20delineating%20a%20physical%20boundary%20for%20transfer.%0A%20%20Under%20a%20matched%20IMS%20controlled-split%20protocol%2C%20a%20generic%20EfficientNet-B0%20encoder%20pretrained%20on%20ImageNet%20collapses%20in%20the%20intermittent%20regime%20%28Lambda_tail%20about%202%29%2C%20while%20the%20interferometric%20operator%20retains%20strong%20extreme-event%20selectivity%20%28Lambda_tail%20about%20860%29%2C%20indicating%20that%20the%20effect%20is%20not%20a%20generic%20property%20of%20CNN%20features.%20Controlled%20morphology-destruction%20transformations%20selectively%20degrade%20performance%20despite%20per-window%20normalization%2C%20consistent%20with%20sensitivity%20to%20coherent%20time-frequency%20organization%20rather%20than%20marginal%20amplitude%20statistics.&entry.1838667208=http%3A//arxiv.org/abs/2601.11415v1&entry.124074799=Read"},
{"title": "A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery", "author": "Ch Muhammad Awais and Marco Reggiannini and Davide Moroni and Oktay Karakus", "abstract": "High-resolution imagery plays a critical role in improving the performance of visual recognition tasks such as classification, detection, and segmentation. In many domains, including remote sensing and surveillance, low-resolution images can limit the accuracy of automated analysis. To address this, super-resolution (SR) techniques have been widely adopted to attempt to reconstruct high-resolution images from low-resolution inputs. Related traditional approaches focus solely on enhancing image quality based on pixel-level metrics, leaving the relationship between super-resolved image fidelity and downstream classification performance largely underexplored. This raises a key question: can integrating classification objectives directly into the super-resolution process further improve classification accuracy? In this paper, we try to respond to this question by investigating the relationship between super-resolution and classification through the deployment of a specialised algorithmic strategy. We propose a novel methodology that increases the resolution of synthetic aperture radar imagery by optimising loss functions that account for both image quality and classification performance. Our approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.", "link": "http://arxiv.org/abs/2508.06407v2", "date": "2026-01-16", "relevancy": 2.4402, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4953}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.488}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Classification-Aware%20Super-Resolution%20Framework%20for%20Ship%20Targets%20in%20SAR%20Imagery&body=Title%3A%20A%20Classification-Aware%20Super-Resolution%20Framework%20for%20Ship%20Targets%20in%20SAR%20Imagery%0AAuthor%3A%20Ch%20Muhammad%20Awais%20and%20Marco%20Reggiannini%20and%20Davide%20Moroni%20and%20Oktay%20Karakus%0AAbstract%3A%20High-resolution%20imagery%20plays%20a%20critical%20role%20in%20improving%20the%20performance%20of%20visual%20recognition%20tasks%20such%20as%20classification%2C%20detection%2C%20and%20segmentation.%20In%20many%20domains%2C%20including%20remote%20sensing%20and%20surveillance%2C%20low-resolution%20images%20can%20limit%20the%20accuracy%20of%20automated%20analysis.%20To%20address%20this%2C%20super-resolution%20%28SR%29%20techniques%20have%20been%20widely%20adopted%20to%20attempt%20to%20reconstruct%20high-resolution%20images%20from%20low-resolution%20inputs.%20Related%20traditional%20approaches%20focus%20solely%20on%20enhancing%20image%20quality%20based%20on%20pixel-level%20metrics%2C%20leaving%20the%20relationship%20between%20super-resolved%20image%20fidelity%20and%20downstream%20classification%20performance%20largely%20underexplored.%20This%20raises%20a%20key%20question%3A%20can%20integrating%20classification%20objectives%20directly%20into%20the%20super-resolution%20process%20further%20improve%20classification%20accuracy%3F%20In%20this%20paper%2C%20we%20try%20to%20respond%20to%20this%20question%20by%20investigating%20the%20relationship%20between%20super-resolution%20and%20classification%20through%20the%20deployment%20of%20a%20specialised%20algorithmic%20strategy.%20We%20propose%20a%20novel%20methodology%20that%20increases%20the%20resolution%20of%20synthetic%20aperture%20radar%20imagery%20by%20optimising%20loss%20functions%20that%20account%20for%20both%20image%20quality%20and%20classification%20performance.%20Our%20approach%20improves%20image%20quality%2C%20as%20measured%20by%20scientifically%20ascertained%20image%20quality%20indicators%2C%20while%20also%20enhancing%20classification%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2508.06407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Classification-Aware%2520Super-Resolution%2520Framework%2520for%2520Ship%2520Targets%2520in%2520SAR%2520Imagery%26entry.906535625%3DCh%2520Muhammad%2520Awais%2520and%2520Marco%2520Reggiannini%2520and%2520Davide%2520Moroni%2520and%2520Oktay%2520Karakus%26entry.1292438233%3DHigh-resolution%2520imagery%2520plays%2520a%2520critical%2520role%2520in%2520improving%2520the%2520performance%2520of%2520visual%2520recognition%2520tasks%2520such%2520as%2520classification%252C%2520detection%252C%2520and%2520segmentation.%2520In%2520many%2520domains%252C%2520including%2520remote%2520sensing%2520and%2520surveillance%252C%2520low-resolution%2520images%2520can%2520limit%2520the%2520accuracy%2520of%2520automated%2520analysis.%2520To%2520address%2520this%252C%2520super-resolution%2520%2528SR%2529%2520techniques%2520have%2520been%2520widely%2520adopted%2520to%2520attempt%2520to%2520reconstruct%2520high-resolution%2520images%2520from%2520low-resolution%2520inputs.%2520Related%2520traditional%2520approaches%2520focus%2520solely%2520on%2520enhancing%2520image%2520quality%2520based%2520on%2520pixel-level%2520metrics%252C%2520leaving%2520the%2520relationship%2520between%2520super-resolved%2520image%2520fidelity%2520and%2520downstream%2520classification%2520performance%2520largely%2520underexplored.%2520This%2520raises%2520a%2520key%2520question%253A%2520can%2520integrating%2520classification%2520objectives%2520directly%2520into%2520the%2520super-resolution%2520process%2520further%2520improve%2520classification%2520accuracy%253F%2520In%2520this%2520paper%252C%2520we%2520try%2520to%2520respond%2520to%2520this%2520question%2520by%2520investigating%2520the%2520relationship%2520between%2520super-resolution%2520and%2520classification%2520through%2520the%2520deployment%2520of%2520a%2520specialised%2520algorithmic%2520strategy.%2520We%2520propose%2520a%2520novel%2520methodology%2520that%2520increases%2520the%2520resolution%2520of%2520synthetic%2520aperture%2520radar%2520imagery%2520by%2520optimising%2520loss%2520functions%2520that%2520account%2520for%2520both%2520image%2520quality%2520and%2520classification%2520performance.%2520Our%2520approach%2520improves%2520image%2520quality%252C%2520as%2520measured%2520by%2520scientifically%2520ascertained%2520image%2520quality%2520indicators%252C%2520while%2520also%2520enhancing%2520classification%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Classification-Aware%20Super-Resolution%20Framework%20for%20Ship%20Targets%20in%20SAR%20Imagery&entry.906535625=Ch%20Muhammad%20Awais%20and%20Marco%20Reggiannini%20and%20Davide%20Moroni%20and%20Oktay%20Karakus&entry.1292438233=High-resolution%20imagery%20plays%20a%20critical%20role%20in%20improving%20the%20performance%20of%20visual%20recognition%20tasks%20such%20as%20classification%2C%20detection%2C%20and%20segmentation.%20In%20many%20domains%2C%20including%20remote%20sensing%20and%20surveillance%2C%20low-resolution%20images%20can%20limit%20the%20accuracy%20of%20automated%20analysis.%20To%20address%20this%2C%20super-resolution%20%28SR%29%20techniques%20have%20been%20widely%20adopted%20to%20attempt%20to%20reconstruct%20high-resolution%20images%20from%20low-resolution%20inputs.%20Related%20traditional%20approaches%20focus%20solely%20on%20enhancing%20image%20quality%20based%20on%20pixel-level%20metrics%2C%20leaving%20the%20relationship%20between%20super-resolved%20image%20fidelity%20and%20downstream%20classification%20performance%20largely%20underexplored.%20This%20raises%20a%20key%20question%3A%20can%20integrating%20classification%20objectives%20directly%20into%20the%20super-resolution%20process%20further%20improve%20classification%20accuracy%3F%20In%20this%20paper%2C%20we%20try%20to%20respond%20to%20this%20question%20by%20investigating%20the%20relationship%20between%20super-resolution%20and%20classification%20through%20the%20deployment%20of%20a%20specialised%20algorithmic%20strategy.%20We%20propose%20a%20novel%20methodology%20that%20increases%20the%20resolution%20of%20synthetic%20aperture%20radar%20imagery%20by%20optimising%20loss%20functions%20that%20account%20for%20both%20image%20quality%20and%20classification%20performance.%20Our%20approach%20improves%20image%20quality%2C%20as%20measured%20by%20scientifically%20ascertained%20image%20quality%20indicators%2C%20while%20also%20enhancing%20classification%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2508.06407v2&entry.124074799=Read"},
{"title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context", "author": "Junyi Chen and Tong He and Zhoujie Fu and Pengfei Wan and Kun Gai and Weicai Ye", "abstract": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.", "link": "http://arxiv.org/abs/2601.02358v2", "date": "2026-01-16", "relevancy": 2.4391, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6512}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5862}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VINO%3A%20A%20Unified%20Visual%20Generator%20with%20Interleaved%20OmniModal%20Context&body=Title%3A%20VINO%3A%20A%20Unified%20Visual%20Generator%20with%20Interleaved%20OmniModal%20Context%0AAuthor%3A%20Junyi%20Chen%20and%20Tong%20He%20and%20Zhoujie%20Fu%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Weicai%20Ye%0AAbstract%3A%20We%20present%20VINO%2C%20a%20unified%20visual%20generator%20that%20performs%20image%20and%20video%20generation%20and%20editing%20within%20a%20single%20framework.%20Instead%20of%20relying%20on%20task-specific%20models%20or%20independent%20modules%20for%20each%20modality%2C%20VINO%20uses%20a%20shared%20diffusion%20backbone%20that%20conditions%20on%20text%2C%20images%20and%20videos%2C%20enabling%20a%20broad%20range%20of%20visual%20creation%20and%20editing%20tasks%20under%20one%20model.%20Specifically%2C%20VINO%20couples%20a%20vision-language%20model%20%28VLM%29%20with%20a%20Multimodal%20Diffusion%20Transformer%20%28MMDiT%29%2C%20where%20multimodal%20inputs%20are%20encoded%20as%20interleaved%20conditioning%20tokens%2C%20and%20then%20used%20to%20guide%20the%20diffusion%20process.%20This%20design%20supports%20multi-reference%20grounding%2C%20long-form%20instruction%20following%2C%20and%20coherent%20identity%20preservation%20across%20static%20and%20dynamic%20content%2C%20while%20avoiding%20modality-specific%20architectural%20components.%20To%20train%20such%20a%20unified%20system%2C%20we%20introduce%20a%20multi-stage%20training%20pipeline%20that%20progressively%20expands%20a%20video%20generation%20base%20model%20into%20a%20unified%2C%20multi-task%20generator%20capable%20of%20both%20image%20and%20video%20input%20and%20output.%20Across%20diverse%20generation%20and%20editing%20benchmarks%2C%20VINO%20demonstrates%20strong%20visual%20quality%2C%20faithful%20instruction%20following%2C%20improved%20reference%20and%20attribute%20preservation%2C%20and%20more%20controllable%20multi-identity%20edits.%20Our%20results%20highlight%20a%20practical%20path%20toward%20scalable%20unified%20visual%20generation%2C%20and%20the%20promise%20of%20interleaved%2C%20in-context%20computation%20as%20a%20foundation%20for%20general-purpose%20visual%20creation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVINO%253A%2520A%2520Unified%2520Visual%2520Generator%2520with%2520Interleaved%2520OmniModal%2520Context%26entry.906535625%3DJunyi%2520Chen%2520and%2520Tong%2520He%2520and%2520Zhoujie%2520Fu%2520and%2520Pengfei%2520Wan%2520and%2520Kun%2520Gai%2520and%2520Weicai%2520Ye%26entry.1292438233%3DWe%2520present%2520VINO%252C%2520a%2520unified%2520visual%2520generator%2520that%2520performs%2520image%2520and%2520video%2520generation%2520and%2520editing%2520within%2520a%2520single%2520framework.%2520Instead%2520of%2520relying%2520on%2520task-specific%2520models%2520or%2520independent%2520modules%2520for%2520each%2520modality%252C%2520VINO%2520uses%2520a%2520shared%2520diffusion%2520backbone%2520that%2520conditions%2520on%2520text%252C%2520images%2520and%2520videos%252C%2520enabling%2520a%2520broad%2520range%2520of%2520visual%2520creation%2520and%2520editing%2520tasks%2520under%2520one%2520model.%2520Specifically%252C%2520VINO%2520couples%2520a%2520vision-language%2520model%2520%2528VLM%2529%2520with%2520a%2520Multimodal%2520Diffusion%2520Transformer%2520%2528MMDiT%2529%252C%2520where%2520multimodal%2520inputs%2520are%2520encoded%2520as%2520interleaved%2520conditioning%2520tokens%252C%2520and%2520then%2520used%2520to%2520guide%2520the%2520diffusion%2520process.%2520This%2520design%2520supports%2520multi-reference%2520grounding%252C%2520long-form%2520instruction%2520following%252C%2520and%2520coherent%2520identity%2520preservation%2520across%2520static%2520and%2520dynamic%2520content%252C%2520while%2520avoiding%2520modality-specific%2520architectural%2520components.%2520To%2520train%2520such%2520a%2520unified%2520system%252C%2520we%2520introduce%2520a%2520multi-stage%2520training%2520pipeline%2520that%2520progressively%2520expands%2520a%2520video%2520generation%2520base%2520model%2520into%2520a%2520unified%252C%2520multi-task%2520generator%2520capable%2520of%2520both%2520image%2520and%2520video%2520input%2520and%2520output.%2520Across%2520diverse%2520generation%2520and%2520editing%2520benchmarks%252C%2520VINO%2520demonstrates%2520strong%2520visual%2520quality%252C%2520faithful%2520instruction%2520following%252C%2520improved%2520reference%2520and%2520attribute%2520preservation%252C%2520and%2520more%2520controllable%2520multi-identity%2520edits.%2520Our%2520results%2520highlight%2520a%2520practical%2520path%2520toward%2520scalable%2520unified%2520visual%2520generation%252C%2520and%2520the%2520promise%2520of%2520interleaved%252C%2520in-context%2520computation%2520as%2520a%2520foundation%2520for%2520general-purpose%2520visual%2520creation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VINO%3A%20A%20Unified%20Visual%20Generator%20with%20Interleaved%20OmniModal%20Context&entry.906535625=Junyi%20Chen%20and%20Tong%20He%20and%20Zhoujie%20Fu%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Weicai%20Ye&entry.1292438233=We%20present%20VINO%2C%20a%20unified%20visual%20generator%20that%20performs%20image%20and%20video%20generation%20and%20editing%20within%20a%20single%20framework.%20Instead%20of%20relying%20on%20task-specific%20models%20or%20independent%20modules%20for%20each%20modality%2C%20VINO%20uses%20a%20shared%20diffusion%20backbone%20that%20conditions%20on%20text%2C%20images%20and%20videos%2C%20enabling%20a%20broad%20range%20of%20visual%20creation%20and%20editing%20tasks%20under%20one%20model.%20Specifically%2C%20VINO%20couples%20a%20vision-language%20model%20%28VLM%29%20with%20a%20Multimodal%20Diffusion%20Transformer%20%28MMDiT%29%2C%20where%20multimodal%20inputs%20are%20encoded%20as%20interleaved%20conditioning%20tokens%2C%20and%20then%20used%20to%20guide%20the%20diffusion%20process.%20This%20design%20supports%20multi-reference%20grounding%2C%20long-form%20instruction%20following%2C%20and%20coherent%20identity%20preservation%20across%20static%20and%20dynamic%20content%2C%20while%20avoiding%20modality-specific%20architectural%20components.%20To%20train%20such%20a%20unified%20system%2C%20we%20introduce%20a%20multi-stage%20training%20pipeline%20that%20progressively%20expands%20a%20video%20generation%20base%20model%20into%20a%20unified%2C%20multi-task%20generator%20capable%20of%20both%20image%20and%20video%20input%20and%20output.%20Across%20diverse%20generation%20and%20editing%20benchmarks%2C%20VINO%20demonstrates%20strong%20visual%20quality%2C%20faithful%20instruction%20following%2C%20improved%20reference%20and%20attribute%20preservation%2C%20and%20more%20controllable%20multi-identity%20edits.%20Our%20results%20highlight%20a%20practical%20path%20toward%20scalable%20unified%20visual%20generation%2C%20and%20the%20promise%20of%20interleaved%2C%20in-context%20computation%20as%20a%20foundation%20for%20general-purpose%20visual%20creation.&entry.1838667208=http%3A//arxiv.org/abs/2601.02358v2&entry.124074799=Read"},
{"title": "Dynamic Prototype Rehearsal for Continual ECG Arrhythmia Detection", "author": "Sana Rahmani and Reetam Chatterjee and Ali Etemad and Javad Hashemi", "abstract": "Continual Learning (CL) methods aim to learn from a sequence of tasks while avoiding the challenge of forgetting previous knowledge. We present DREAM-CL, a novel CL method for ECG arrhythmia detection that introduces dynamic prototype rehearsal memory. DREAM-CL selects representative prototypes by clustering data based on learning behavior during each training session. Within each cluster, we apply a smooth sorting operation that ranks samples by training difficulty, compressing extreme values and removing outliers. The more challenging samples are then chosen as prototypes for the rehearsal memory, ensuring effective knowledge retention across sessions. We evaluate our method on time-incremental, class-incremental, and lead-incremental scenarios using two widely used ECG arrhythmia datasets, Chapman and PTB-XL. The results demonstrate that DREAM-CL outperforms the state-of-the-art in CL for ECG arrhythmia detection. Detailed ablation and sensitivity studies are performed to validate the different design choices of our method.", "link": "http://arxiv.org/abs/2501.07555v2", "date": "2026-01-16", "relevancy": 2.4386, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4965}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4866}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Prototype%20Rehearsal%20for%20Continual%20ECG%20Arrhythmia%20Detection&body=Title%3A%20Dynamic%20Prototype%20Rehearsal%20for%20Continual%20ECG%20Arrhythmia%20Detection%0AAuthor%3A%20Sana%20Rahmani%20and%20Reetam%20Chatterjee%20and%20Ali%20Etemad%20and%20Javad%20Hashemi%0AAbstract%3A%20Continual%20Learning%20%28CL%29%20methods%20aim%20to%20learn%20from%20a%20sequence%20of%20tasks%20while%20avoiding%20the%20challenge%20of%20forgetting%20previous%20knowledge.%20We%20present%20DREAM-CL%2C%20a%20novel%20CL%20method%20for%20ECG%20arrhythmia%20detection%20that%20introduces%20dynamic%20prototype%20rehearsal%20memory.%20DREAM-CL%20selects%20representative%20prototypes%20by%20clustering%20data%20based%20on%20learning%20behavior%20during%20each%20training%20session.%20Within%20each%20cluster%2C%20we%20apply%20a%20smooth%20sorting%20operation%20that%20ranks%20samples%20by%20training%20difficulty%2C%20compressing%20extreme%20values%20and%20removing%20outliers.%20The%20more%20challenging%20samples%20are%20then%20chosen%20as%20prototypes%20for%20the%20rehearsal%20memory%2C%20ensuring%20effective%20knowledge%20retention%20across%20sessions.%20We%20evaluate%20our%20method%20on%20time-incremental%2C%20class-incremental%2C%20and%20lead-incremental%20scenarios%20using%20two%20widely%20used%20ECG%20arrhythmia%20datasets%2C%20Chapman%20and%20PTB-XL.%20The%20results%20demonstrate%20that%20DREAM-CL%20outperforms%20the%20state-of-the-art%20in%20CL%20for%20ECG%20arrhythmia%20detection.%20Detailed%20ablation%20and%20sensitivity%20studies%20are%20performed%20to%20validate%20the%20different%20design%20choices%20of%20our%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2501.07555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Prototype%2520Rehearsal%2520for%2520Continual%2520ECG%2520Arrhythmia%2520Detection%26entry.906535625%3DSana%2520Rahmani%2520and%2520Reetam%2520Chatterjee%2520and%2520Ali%2520Etemad%2520and%2520Javad%2520Hashemi%26entry.1292438233%3DContinual%2520Learning%2520%2528CL%2529%2520methods%2520aim%2520to%2520learn%2520from%2520a%2520sequence%2520of%2520tasks%2520while%2520avoiding%2520the%2520challenge%2520of%2520forgetting%2520previous%2520knowledge.%2520We%2520present%2520DREAM-CL%252C%2520a%2520novel%2520CL%2520method%2520for%2520ECG%2520arrhythmia%2520detection%2520that%2520introduces%2520dynamic%2520prototype%2520rehearsal%2520memory.%2520DREAM-CL%2520selects%2520representative%2520prototypes%2520by%2520clustering%2520data%2520based%2520on%2520learning%2520behavior%2520during%2520each%2520training%2520session.%2520Within%2520each%2520cluster%252C%2520we%2520apply%2520a%2520smooth%2520sorting%2520operation%2520that%2520ranks%2520samples%2520by%2520training%2520difficulty%252C%2520compressing%2520extreme%2520values%2520and%2520removing%2520outliers.%2520The%2520more%2520challenging%2520samples%2520are%2520then%2520chosen%2520as%2520prototypes%2520for%2520the%2520rehearsal%2520memory%252C%2520ensuring%2520effective%2520knowledge%2520retention%2520across%2520sessions.%2520We%2520evaluate%2520our%2520method%2520on%2520time-incremental%252C%2520class-incremental%252C%2520and%2520lead-incremental%2520scenarios%2520using%2520two%2520widely%2520used%2520ECG%2520arrhythmia%2520datasets%252C%2520Chapman%2520and%2520PTB-XL.%2520The%2520results%2520demonstrate%2520that%2520DREAM-CL%2520outperforms%2520the%2520state-of-the-art%2520in%2520CL%2520for%2520ECG%2520arrhythmia%2520detection.%2520Detailed%2520ablation%2520and%2520sensitivity%2520studies%2520are%2520performed%2520to%2520validate%2520the%2520different%2520design%2520choices%2520of%2520our%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Prototype%20Rehearsal%20for%20Continual%20ECG%20Arrhythmia%20Detection&entry.906535625=Sana%20Rahmani%20and%20Reetam%20Chatterjee%20and%20Ali%20Etemad%20and%20Javad%20Hashemi&entry.1292438233=Continual%20Learning%20%28CL%29%20methods%20aim%20to%20learn%20from%20a%20sequence%20of%20tasks%20while%20avoiding%20the%20challenge%20of%20forgetting%20previous%20knowledge.%20We%20present%20DREAM-CL%2C%20a%20novel%20CL%20method%20for%20ECG%20arrhythmia%20detection%20that%20introduces%20dynamic%20prototype%20rehearsal%20memory.%20DREAM-CL%20selects%20representative%20prototypes%20by%20clustering%20data%20based%20on%20learning%20behavior%20during%20each%20training%20session.%20Within%20each%20cluster%2C%20we%20apply%20a%20smooth%20sorting%20operation%20that%20ranks%20samples%20by%20training%20difficulty%2C%20compressing%20extreme%20values%20and%20removing%20outliers.%20The%20more%20challenging%20samples%20are%20then%20chosen%20as%20prototypes%20for%20the%20rehearsal%20memory%2C%20ensuring%20effective%20knowledge%20retention%20across%20sessions.%20We%20evaluate%20our%20method%20on%20time-incremental%2C%20class-incremental%2C%20and%20lead-incremental%20scenarios%20using%20two%20widely%20used%20ECG%20arrhythmia%20datasets%2C%20Chapman%20and%20PTB-XL.%20The%20results%20demonstrate%20that%20DREAM-CL%20outperforms%20the%20state-of-the-art%20in%20CL%20for%20ECG%20arrhythmia%20detection.%20Detailed%20ablation%20and%20sensitivity%20studies%20are%20performed%20to%20validate%20the%20different%20design%20choices%20of%20our%20method.&entry.1838667208=http%3A//arxiv.org/abs/2501.07555v2&entry.124074799=Read"},
{"title": "SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction", "author": "Hanlin Wu and Pengfei Lin and Ehsan Javanmardi and Nanren Bao and Bo Qian and Hao Si and Manabu Tsukada", "abstract": "As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\\% gain in efficiency.", "link": "http://arxiv.org/abs/2601.11396v1", "date": "2026-01-16", "relevancy": 2.4354, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6163}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6126}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUG-Occ%3A%20An%20Explicit%20Semantics%20and%20Uncertainty%20Guided%20Sparse%20Learning%20Framework%20for%20Real-Time%203D%20Occupancy%20Prediction&body=Title%3A%20SUG-Occ%3A%20An%20Explicit%20Semantics%20and%20Uncertainty%20Guided%20Sparse%20Learning%20Framework%20for%20Real-Time%203D%20Occupancy%20Prediction%0AAuthor%3A%20Hanlin%20Wu%20and%20Pengfei%20Lin%20and%20Ehsan%20Javanmardi%20and%20Nanren%20Bao%20and%20Bo%20Qian%20and%20Hao%20Si%20and%20Manabu%20Tsukada%0AAbstract%3A%20As%20autonomous%20driving%20moves%20toward%20full%20scene%20understanding%2C%203D%20semantic%20occupancy%20prediction%20has%20emerged%20as%20a%20crucial%20perception%20task%2C%20offering%20voxel-level%20semantics%20beyond%20traditional%20detection%20and%20segmentation%20paradigms.%20However%2C%20such%20a%20refined%20representation%20for%20scene%20understanding%20incurs%20prohibitive%20computation%20and%20memory%20overhead%2C%20posing%20a%20major%20barrier%20to%20practical%20real-time%20deployment.%20To%20address%20this%2C%20we%20propose%20SUG-Occ%2C%20an%20explicit%20Semantics%20and%20Uncertainty%20Guided%20Sparse%20Learning%20Enabled%203D%20Occupancy%20Prediction%20Framework%2C%20which%20exploits%20the%20inherent%20sparsity%20of%203D%20scenes%20to%20reduce%20redundant%20computation%20while%20maintaining%20geometric%20and%20semantic%20completeness.%20Specifically%2C%20we%20first%20utilize%20semantic%20and%20uncertainty%20priors%20to%20suppress%20projections%20from%20free%20space%20during%20view%20transformation%20while%20employing%20an%20explicit%20unsigned%20distance%20encoding%20to%20enhance%20geometric%20consistency%2C%20producing%20a%20structurally%20consistent%20sparse%203D%20representation.%20Secondly%2C%20we%20design%20an%20cascade%20sparse%20completion%20module%20via%20hyper%20cross%20sparse%20convolution%20and%20generative%20upsampling%20to%20enable%20efficiently%20coarse-to-fine%20reasoning.%20Finally%2C%20we%20devise%20an%20object%20contextual%20representation%20%28OCR%29%20based%20mask%20decoder%20that%20aggregates%20global%20semantic%20context%20from%20sparse%20features%20and%20refines%20voxel-wise%20predictions%20via%20lightweight%20query-context%20interactions%2C%20avoiding%20expensive%20attention%20operations%20over%20volumetric%20features.%20Extensive%20experiments%20on%20SemanticKITTI%20benchmark%20demonstrate%20that%20the%20proposed%20approach%20outperforms%20the%20baselines%2C%20achieving%20a%207.34/%25%20improvement%20in%20accuracy%20and%20a%2057.8%5C%25%20gain%20in%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUG-Occ%253A%2520An%2520Explicit%2520Semantics%2520and%2520Uncertainty%2520Guided%2520Sparse%2520Learning%2520Framework%2520for%2520Real-Time%25203D%2520Occupancy%2520Prediction%26entry.906535625%3DHanlin%2520Wu%2520and%2520Pengfei%2520Lin%2520and%2520Ehsan%2520Javanmardi%2520and%2520Nanren%2520Bao%2520and%2520Bo%2520Qian%2520and%2520Hao%2520Si%2520and%2520Manabu%2520Tsukada%26entry.1292438233%3DAs%2520autonomous%2520driving%2520moves%2520toward%2520full%2520scene%2520understanding%252C%25203D%2520semantic%2520occupancy%2520prediction%2520has%2520emerged%2520as%2520a%2520crucial%2520perception%2520task%252C%2520offering%2520voxel-level%2520semantics%2520beyond%2520traditional%2520detection%2520and%2520segmentation%2520paradigms.%2520However%252C%2520such%2520a%2520refined%2520representation%2520for%2520scene%2520understanding%2520incurs%2520prohibitive%2520computation%2520and%2520memory%2520overhead%252C%2520posing%2520a%2520major%2520barrier%2520to%2520practical%2520real-time%2520deployment.%2520To%2520address%2520this%252C%2520we%2520propose%2520SUG-Occ%252C%2520an%2520explicit%2520Semantics%2520and%2520Uncertainty%2520Guided%2520Sparse%2520Learning%2520Enabled%25203D%2520Occupancy%2520Prediction%2520Framework%252C%2520which%2520exploits%2520the%2520inherent%2520sparsity%2520of%25203D%2520scenes%2520to%2520reduce%2520redundant%2520computation%2520while%2520maintaining%2520geometric%2520and%2520semantic%2520completeness.%2520Specifically%252C%2520we%2520first%2520utilize%2520semantic%2520and%2520uncertainty%2520priors%2520to%2520suppress%2520projections%2520from%2520free%2520space%2520during%2520view%2520transformation%2520while%2520employing%2520an%2520explicit%2520unsigned%2520distance%2520encoding%2520to%2520enhance%2520geometric%2520consistency%252C%2520producing%2520a%2520structurally%2520consistent%2520sparse%25203D%2520representation.%2520Secondly%252C%2520we%2520design%2520an%2520cascade%2520sparse%2520completion%2520module%2520via%2520hyper%2520cross%2520sparse%2520convolution%2520and%2520generative%2520upsampling%2520to%2520enable%2520efficiently%2520coarse-to-fine%2520reasoning.%2520Finally%252C%2520we%2520devise%2520an%2520object%2520contextual%2520representation%2520%2528OCR%2529%2520based%2520mask%2520decoder%2520that%2520aggregates%2520global%2520semantic%2520context%2520from%2520sparse%2520features%2520and%2520refines%2520voxel-wise%2520predictions%2520via%2520lightweight%2520query-context%2520interactions%252C%2520avoiding%2520expensive%2520attention%2520operations%2520over%2520volumetric%2520features.%2520Extensive%2520experiments%2520on%2520SemanticKITTI%2520benchmark%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520outperforms%2520the%2520baselines%252C%2520achieving%2520a%25207.34/%2525%2520improvement%2520in%2520accuracy%2520and%2520a%252057.8%255C%2525%2520gain%2520in%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUG-Occ%3A%20An%20Explicit%20Semantics%20and%20Uncertainty%20Guided%20Sparse%20Learning%20Framework%20for%20Real-Time%203D%20Occupancy%20Prediction&entry.906535625=Hanlin%20Wu%20and%20Pengfei%20Lin%20and%20Ehsan%20Javanmardi%20and%20Nanren%20Bao%20and%20Bo%20Qian%20and%20Hao%20Si%20and%20Manabu%20Tsukada&entry.1292438233=As%20autonomous%20driving%20moves%20toward%20full%20scene%20understanding%2C%203D%20semantic%20occupancy%20prediction%20has%20emerged%20as%20a%20crucial%20perception%20task%2C%20offering%20voxel-level%20semantics%20beyond%20traditional%20detection%20and%20segmentation%20paradigms.%20However%2C%20such%20a%20refined%20representation%20for%20scene%20understanding%20incurs%20prohibitive%20computation%20and%20memory%20overhead%2C%20posing%20a%20major%20barrier%20to%20practical%20real-time%20deployment.%20To%20address%20this%2C%20we%20propose%20SUG-Occ%2C%20an%20explicit%20Semantics%20and%20Uncertainty%20Guided%20Sparse%20Learning%20Enabled%203D%20Occupancy%20Prediction%20Framework%2C%20which%20exploits%20the%20inherent%20sparsity%20of%203D%20scenes%20to%20reduce%20redundant%20computation%20while%20maintaining%20geometric%20and%20semantic%20completeness.%20Specifically%2C%20we%20first%20utilize%20semantic%20and%20uncertainty%20priors%20to%20suppress%20projections%20from%20free%20space%20during%20view%20transformation%20while%20employing%20an%20explicit%20unsigned%20distance%20encoding%20to%20enhance%20geometric%20consistency%2C%20producing%20a%20structurally%20consistent%20sparse%203D%20representation.%20Secondly%2C%20we%20design%20an%20cascade%20sparse%20completion%20module%20via%20hyper%20cross%20sparse%20convolution%20and%20generative%20upsampling%20to%20enable%20efficiently%20coarse-to-fine%20reasoning.%20Finally%2C%20we%20devise%20an%20object%20contextual%20representation%20%28OCR%29%20based%20mask%20decoder%20that%20aggregates%20global%20semantic%20context%20from%20sparse%20features%20and%20refines%20voxel-wise%20predictions%20via%20lightweight%20query-context%20interactions%2C%20avoiding%20expensive%20attention%20operations%20over%20volumetric%20features.%20Extensive%20experiments%20on%20SemanticKITTI%20benchmark%20demonstrate%20that%20the%20proposed%20approach%20outperforms%20the%20baselines%2C%20achieving%20a%207.34/%25%20improvement%20in%20accuracy%20and%20a%2057.8%5C%25%20gain%20in%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2601.11396v1&entry.124074799=Read"},
{"title": "What Makes a Good Speech Tokenizer for LLM-Centric Speech Generation? A Systematic Study", "author": "Xiaoran Fan and Zhichao Sun and Yangfan Gao and Jingfei Xiong and Hang Yan and Yifei Cao and Jiajun Sun and Shuo Li and Zhihao Zhang and Zhiheng Xi and Yuhao Zhou and Senjie Jin and Changhao Jiang and Junjie Ye and Ming Zhang and Rui Zheng and Zhenhua Han and Yunke Zhang and Demei Yan and Shaokang Dong and Tao Ji and Tao Gui", "abstract": "Speech-language models (SLMs) offer a promising path toward unifying speech and text understanding and generation. However, challenges remain in achieving effective cross-modal alignment and high-quality speech generation. In this work, we systematically investigate the role of speech tokenizer designs in LLM-centric SLMs, augmented by speech heads and speaker modeling. We compare coupled, semi-decoupled, and fully decoupled speech tokenizers under a fair SLM framework and find that decoupled tokenization significantly improves alignment and synthesis quality. To address the information density mismatch between speech and text, we introduce multi-token prediction (MTP) into SLMs, enabling each hidden state to decode multiple speech tokens. This leads to up to 12$\\times$ faster decoding and a substantial drop in word error rate (from 6.07 to 3.01). Furthermore, we propose a speaker-aware generation paradigm and introduce RoleTriviaQA, a large-scale role-playing knowledge QA benchmark with diverse speaker identities. Experiments demonstrate that our methods enhance both knowledge understanding and speaker consistency.", "link": "http://arxiv.org/abs/2506.12537v3", "date": "2026-01-16", "relevancy": 2.4095, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4797}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Makes%20a%20Good%20Speech%20Tokenizer%20for%20LLM-Centric%20Speech%20Generation%3F%20A%20Systematic%20Study&body=Title%3A%20What%20Makes%20a%20Good%20Speech%20Tokenizer%20for%20LLM-Centric%20Speech%20Generation%3F%20A%20Systematic%20Study%0AAuthor%3A%20Xiaoran%20Fan%20and%20Zhichao%20Sun%20and%20Yangfan%20Gao%20and%20Jingfei%20Xiong%20and%20Hang%20Yan%20and%20Yifei%20Cao%20and%20Jiajun%20Sun%20and%20Shuo%20Li%20and%20Zhihao%20Zhang%20and%20Zhiheng%20Xi%20and%20Yuhao%20Zhou%20and%20Senjie%20Jin%20and%20Changhao%20Jiang%20and%20Junjie%20Ye%20and%20Ming%20Zhang%20and%20Rui%20Zheng%20and%20Zhenhua%20Han%20and%20Yunke%20Zhang%20and%20Demei%20Yan%20and%20Shaokang%20Dong%20and%20Tao%20Ji%20and%20Tao%20Gui%0AAbstract%3A%20Speech-language%20models%20%28SLMs%29%20offer%20a%20promising%20path%20toward%20unifying%20speech%20and%20text%20understanding%20and%20generation.%20However%2C%20challenges%20remain%20in%20achieving%20effective%20cross-modal%20alignment%20and%20high-quality%20speech%20generation.%20In%20this%20work%2C%20we%20systematically%20investigate%20the%20role%20of%20speech%20tokenizer%20designs%20in%20LLM-centric%20SLMs%2C%20augmented%20by%20speech%20heads%20and%20speaker%20modeling.%20We%20compare%20coupled%2C%20semi-decoupled%2C%20and%20fully%20decoupled%20speech%20tokenizers%20under%20a%20fair%20SLM%20framework%20and%20find%20that%20decoupled%20tokenization%20significantly%20improves%20alignment%20and%20synthesis%20quality.%20To%20address%20the%20information%20density%20mismatch%20between%20speech%20and%20text%2C%20we%20introduce%20multi-token%20prediction%20%28MTP%29%20into%20SLMs%2C%20enabling%20each%20hidden%20state%20to%20decode%20multiple%20speech%20tokens.%20This%20leads%20to%20up%20to%2012%24%5Ctimes%24%20faster%20decoding%20and%20a%20substantial%20drop%20in%20word%20error%20rate%20%28from%206.07%20to%203.01%29.%20Furthermore%2C%20we%20propose%20a%20speaker-aware%20generation%20paradigm%20and%20introduce%20RoleTriviaQA%2C%20a%20large-scale%20role-playing%20knowledge%20QA%20benchmark%20with%20diverse%20speaker%20identities.%20Experiments%20demonstrate%20that%20our%20methods%20enhance%20both%20knowledge%20understanding%20and%20speaker%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2506.12537v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Makes%2520a%2520Good%2520Speech%2520Tokenizer%2520for%2520LLM-Centric%2520Speech%2520Generation%253F%2520A%2520Systematic%2520Study%26entry.906535625%3DXiaoran%2520Fan%2520and%2520Zhichao%2520Sun%2520and%2520Yangfan%2520Gao%2520and%2520Jingfei%2520Xiong%2520and%2520Hang%2520Yan%2520and%2520Yifei%2520Cao%2520and%2520Jiajun%2520Sun%2520and%2520Shuo%2520Li%2520and%2520Zhihao%2520Zhang%2520and%2520Zhiheng%2520Xi%2520and%2520Yuhao%2520Zhou%2520and%2520Senjie%2520Jin%2520and%2520Changhao%2520Jiang%2520and%2520Junjie%2520Ye%2520and%2520Ming%2520Zhang%2520and%2520Rui%2520Zheng%2520and%2520Zhenhua%2520Han%2520and%2520Yunke%2520Zhang%2520and%2520Demei%2520Yan%2520and%2520Shaokang%2520Dong%2520and%2520Tao%2520Ji%2520and%2520Tao%2520Gui%26entry.1292438233%3DSpeech-language%2520models%2520%2528SLMs%2529%2520offer%2520a%2520promising%2520path%2520toward%2520unifying%2520speech%2520and%2520text%2520understanding%2520and%2520generation.%2520However%252C%2520challenges%2520remain%2520in%2520achieving%2520effective%2520cross-modal%2520alignment%2520and%2520high-quality%2520speech%2520generation.%2520In%2520this%2520work%252C%2520we%2520systematically%2520investigate%2520the%2520role%2520of%2520speech%2520tokenizer%2520designs%2520in%2520LLM-centric%2520SLMs%252C%2520augmented%2520by%2520speech%2520heads%2520and%2520speaker%2520modeling.%2520We%2520compare%2520coupled%252C%2520semi-decoupled%252C%2520and%2520fully%2520decoupled%2520speech%2520tokenizers%2520under%2520a%2520fair%2520SLM%2520framework%2520and%2520find%2520that%2520decoupled%2520tokenization%2520significantly%2520improves%2520alignment%2520and%2520synthesis%2520quality.%2520To%2520address%2520the%2520information%2520density%2520mismatch%2520between%2520speech%2520and%2520text%252C%2520we%2520introduce%2520multi-token%2520prediction%2520%2528MTP%2529%2520into%2520SLMs%252C%2520enabling%2520each%2520hidden%2520state%2520to%2520decode%2520multiple%2520speech%2520tokens.%2520This%2520leads%2520to%2520up%2520to%252012%2524%255Ctimes%2524%2520faster%2520decoding%2520and%2520a%2520substantial%2520drop%2520in%2520word%2520error%2520rate%2520%2528from%25206.07%2520to%25203.01%2529.%2520Furthermore%252C%2520we%2520propose%2520a%2520speaker-aware%2520generation%2520paradigm%2520and%2520introduce%2520RoleTriviaQA%252C%2520a%2520large-scale%2520role-playing%2520knowledge%2520QA%2520benchmark%2520with%2520diverse%2520speaker%2520identities.%2520Experiments%2520demonstrate%2520that%2520our%2520methods%2520enhance%2520both%2520knowledge%2520understanding%2520and%2520speaker%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12537v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Makes%20a%20Good%20Speech%20Tokenizer%20for%20LLM-Centric%20Speech%20Generation%3F%20A%20Systematic%20Study&entry.906535625=Xiaoran%20Fan%20and%20Zhichao%20Sun%20and%20Yangfan%20Gao%20and%20Jingfei%20Xiong%20and%20Hang%20Yan%20and%20Yifei%20Cao%20and%20Jiajun%20Sun%20and%20Shuo%20Li%20and%20Zhihao%20Zhang%20and%20Zhiheng%20Xi%20and%20Yuhao%20Zhou%20and%20Senjie%20Jin%20and%20Changhao%20Jiang%20and%20Junjie%20Ye%20and%20Ming%20Zhang%20and%20Rui%20Zheng%20and%20Zhenhua%20Han%20and%20Yunke%20Zhang%20and%20Demei%20Yan%20and%20Shaokang%20Dong%20and%20Tao%20Ji%20and%20Tao%20Gui&entry.1292438233=Speech-language%20models%20%28SLMs%29%20offer%20a%20promising%20path%20toward%20unifying%20speech%20and%20text%20understanding%20and%20generation.%20However%2C%20challenges%20remain%20in%20achieving%20effective%20cross-modal%20alignment%20and%20high-quality%20speech%20generation.%20In%20this%20work%2C%20we%20systematically%20investigate%20the%20role%20of%20speech%20tokenizer%20designs%20in%20LLM-centric%20SLMs%2C%20augmented%20by%20speech%20heads%20and%20speaker%20modeling.%20We%20compare%20coupled%2C%20semi-decoupled%2C%20and%20fully%20decoupled%20speech%20tokenizers%20under%20a%20fair%20SLM%20framework%20and%20find%20that%20decoupled%20tokenization%20significantly%20improves%20alignment%20and%20synthesis%20quality.%20To%20address%20the%20information%20density%20mismatch%20between%20speech%20and%20text%2C%20we%20introduce%20multi-token%20prediction%20%28MTP%29%20into%20SLMs%2C%20enabling%20each%20hidden%20state%20to%20decode%20multiple%20speech%20tokens.%20This%20leads%20to%20up%20to%2012%24%5Ctimes%24%20faster%20decoding%20and%20a%20substantial%20drop%20in%20word%20error%20rate%20%28from%206.07%20to%203.01%29.%20Furthermore%2C%20we%20propose%20a%20speaker-aware%20generation%20paradigm%20and%20introduce%20RoleTriviaQA%2C%20a%20large-scale%20role-playing%20knowledge%20QA%20benchmark%20with%20diverse%20speaker%20identities.%20Experiments%20demonstrate%20that%20our%20methods%20enhance%20both%20knowledge%20understanding%20and%20speaker%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2506.12537v3&entry.124074799=Read"},
{"title": "Balanced Edge Pruning for Graph Anomaly Detection with Noisy Labels", "author": "Zhu Wang and Junnan Dong and Shuang Zhou and Chang Yang and Shengjie Zhao and Xiao Huang", "abstract": "Graph anomaly detection (GAD) is widely applied in many areas, such as financial fraud detection and social spammer detection. Anomalous nodes in the graph not only impact their own communities but also create a ripple effect on neighbors throughout the graph structure. Detecting anomalous nodes in complex graphs has been a challenging task. While existing GAD methods assume all labels are correct, real-world scenarios often involve inaccurate annotations. These noisy labels can severely degrade GAD performance because, with anomalies representing a minority class, even a small number of mislabeled instances can disproportionately interfere with detection models. Cutting edges to mitigate the negative effects of noisy labels is a good option; however, it has both positive and negative influences and also presents an issue of weak supervision. To perform effective GAD with noisy labels, we propose REinforced Graph Anomaly Detector (REGAD) by pruning the edges of candidate nodes potentially with mistaken labels. Moreover, we design the performance feedback based on strategically crafted confident labels to guide the cutting process, ensuring optimal results. Specifically, REGAD contains two novel components. (i) A tailored policy network, which involves two-step actions to remove negative effect propagation step by step. (ii) A policy-in-the-loop mechanism to identify suitable edge removal strategies that control the propagation of noise on the graph and estimate the updated structure to obtain reliable pseudo labels iteratively. Experiments on three real-world datasets demonstrate that REGAD outperforms all baselines under different noisy ratios.", "link": "http://arxiv.org/abs/2407.05934v2", "date": "2026-01-16", "relevancy": 2.4094, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4961}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4926}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balanced%20Edge%20Pruning%20for%20Graph%20Anomaly%20Detection%20with%20Noisy%20Labels&body=Title%3A%20Balanced%20Edge%20Pruning%20for%20Graph%20Anomaly%20Detection%20with%20Noisy%20Labels%0AAuthor%3A%20Zhu%20Wang%20and%20Junnan%20Dong%20and%20Shuang%20Zhou%20and%20Chang%20Yang%20and%20Shengjie%20Zhao%20and%20Xiao%20Huang%0AAbstract%3A%20Graph%20anomaly%20detection%20%28GAD%29%20is%20widely%20applied%20in%20many%20areas%2C%20such%20as%20financial%20fraud%20detection%20and%20social%20spammer%20detection.%20Anomalous%20nodes%20in%20the%20graph%20not%20only%20impact%20their%20own%20communities%20but%20also%20create%20a%20ripple%20effect%20on%20neighbors%20throughout%20the%20graph%20structure.%20Detecting%20anomalous%20nodes%20in%20complex%20graphs%20has%20been%20a%20challenging%20task.%20While%20existing%20GAD%20methods%20assume%20all%20labels%20are%20correct%2C%20real-world%20scenarios%20often%20involve%20inaccurate%20annotations.%20These%20noisy%20labels%20can%20severely%20degrade%20GAD%20performance%20because%2C%20with%20anomalies%20representing%20a%20minority%20class%2C%20even%20a%20small%20number%20of%20mislabeled%20instances%20can%20disproportionately%20interfere%20with%20detection%20models.%20Cutting%20edges%20to%20mitigate%20the%20negative%20effects%20of%20noisy%20labels%20is%20a%20good%20option%3B%20however%2C%20it%20has%20both%20positive%20and%20negative%20influences%20and%20also%20presents%20an%20issue%20of%20weak%20supervision.%20To%20perform%20effective%20GAD%20with%20noisy%20labels%2C%20we%20propose%20REinforced%20Graph%20Anomaly%20Detector%20%28REGAD%29%20by%20pruning%20the%20edges%20of%20candidate%20nodes%20potentially%20with%20mistaken%20labels.%20Moreover%2C%20we%20design%20the%20performance%20feedback%20based%20on%20strategically%20crafted%20confident%20labels%20to%20guide%20the%20cutting%20process%2C%20ensuring%20optimal%20results.%20Specifically%2C%20REGAD%20contains%20two%20novel%20components.%20%28i%29%20A%20tailored%20policy%20network%2C%20which%20involves%20two-step%20actions%20to%20remove%20negative%20effect%20propagation%20step%20by%20step.%20%28ii%29%20A%20policy-in-the-loop%20mechanism%20to%20identify%20suitable%20edge%20removal%20strategies%20that%20control%20the%20propagation%20of%20noise%20on%20the%20graph%20and%20estimate%20the%20updated%20structure%20to%20obtain%20reliable%20pseudo%20labels%20iteratively.%20Experiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20REGAD%20outperforms%20all%20baselines%20under%20different%20noisy%20ratios.%0ALink%3A%20http%3A//arxiv.org/abs/2407.05934v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalanced%2520Edge%2520Pruning%2520for%2520Graph%2520Anomaly%2520Detection%2520with%2520Noisy%2520Labels%26entry.906535625%3DZhu%2520Wang%2520and%2520Junnan%2520Dong%2520and%2520Shuang%2520Zhou%2520and%2520Chang%2520Yang%2520and%2520Shengjie%2520Zhao%2520and%2520Xiao%2520Huang%26entry.1292438233%3DGraph%2520anomaly%2520detection%2520%2528GAD%2529%2520is%2520widely%2520applied%2520in%2520many%2520areas%252C%2520such%2520as%2520financial%2520fraud%2520detection%2520and%2520social%2520spammer%2520detection.%2520Anomalous%2520nodes%2520in%2520the%2520graph%2520not%2520only%2520impact%2520their%2520own%2520communities%2520but%2520also%2520create%2520a%2520ripple%2520effect%2520on%2520neighbors%2520throughout%2520the%2520graph%2520structure.%2520Detecting%2520anomalous%2520nodes%2520in%2520complex%2520graphs%2520has%2520been%2520a%2520challenging%2520task.%2520While%2520existing%2520GAD%2520methods%2520assume%2520all%2520labels%2520are%2520correct%252C%2520real-world%2520scenarios%2520often%2520involve%2520inaccurate%2520annotations.%2520These%2520noisy%2520labels%2520can%2520severely%2520degrade%2520GAD%2520performance%2520because%252C%2520with%2520anomalies%2520representing%2520a%2520minority%2520class%252C%2520even%2520a%2520small%2520number%2520of%2520mislabeled%2520instances%2520can%2520disproportionately%2520interfere%2520with%2520detection%2520models.%2520Cutting%2520edges%2520to%2520mitigate%2520the%2520negative%2520effects%2520of%2520noisy%2520labels%2520is%2520a%2520good%2520option%253B%2520however%252C%2520it%2520has%2520both%2520positive%2520and%2520negative%2520influences%2520and%2520also%2520presents%2520an%2520issue%2520of%2520weak%2520supervision.%2520To%2520perform%2520effective%2520GAD%2520with%2520noisy%2520labels%252C%2520we%2520propose%2520REinforced%2520Graph%2520Anomaly%2520Detector%2520%2528REGAD%2529%2520by%2520pruning%2520the%2520edges%2520of%2520candidate%2520nodes%2520potentially%2520with%2520mistaken%2520labels.%2520Moreover%252C%2520we%2520design%2520the%2520performance%2520feedback%2520based%2520on%2520strategically%2520crafted%2520confident%2520labels%2520to%2520guide%2520the%2520cutting%2520process%252C%2520ensuring%2520optimal%2520results.%2520Specifically%252C%2520REGAD%2520contains%2520two%2520novel%2520components.%2520%2528i%2529%2520A%2520tailored%2520policy%2520network%252C%2520which%2520involves%2520two-step%2520actions%2520to%2520remove%2520negative%2520effect%2520propagation%2520step%2520by%2520step.%2520%2528ii%2529%2520A%2520policy-in-the-loop%2520mechanism%2520to%2520identify%2520suitable%2520edge%2520removal%2520strategies%2520that%2520control%2520the%2520propagation%2520of%2520noise%2520on%2520the%2520graph%2520and%2520estimate%2520the%2520updated%2520structure%2520to%2520obtain%2520reliable%2520pseudo%2520labels%2520iteratively.%2520Experiments%2520on%2520three%2520real-world%2520datasets%2520demonstrate%2520that%2520REGAD%2520outperforms%2520all%2520baselines%2520under%2520different%2520noisy%2520ratios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05934v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balanced%20Edge%20Pruning%20for%20Graph%20Anomaly%20Detection%20with%20Noisy%20Labels&entry.906535625=Zhu%20Wang%20and%20Junnan%20Dong%20and%20Shuang%20Zhou%20and%20Chang%20Yang%20and%20Shengjie%20Zhao%20and%20Xiao%20Huang&entry.1292438233=Graph%20anomaly%20detection%20%28GAD%29%20is%20widely%20applied%20in%20many%20areas%2C%20such%20as%20financial%20fraud%20detection%20and%20social%20spammer%20detection.%20Anomalous%20nodes%20in%20the%20graph%20not%20only%20impact%20their%20own%20communities%20but%20also%20create%20a%20ripple%20effect%20on%20neighbors%20throughout%20the%20graph%20structure.%20Detecting%20anomalous%20nodes%20in%20complex%20graphs%20has%20been%20a%20challenging%20task.%20While%20existing%20GAD%20methods%20assume%20all%20labels%20are%20correct%2C%20real-world%20scenarios%20often%20involve%20inaccurate%20annotations.%20These%20noisy%20labels%20can%20severely%20degrade%20GAD%20performance%20because%2C%20with%20anomalies%20representing%20a%20minority%20class%2C%20even%20a%20small%20number%20of%20mislabeled%20instances%20can%20disproportionately%20interfere%20with%20detection%20models.%20Cutting%20edges%20to%20mitigate%20the%20negative%20effects%20of%20noisy%20labels%20is%20a%20good%20option%3B%20however%2C%20it%20has%20both%20positive%20and%20negative%20influences%20and%20also%20presents%20an%20issue%20of%20weak%20supervision.%20To%20perform%20effective%20GAD%20with%20noisy%20labels%2C%20we%20propose%20REinforced%20Graph%20Anomaly%20Detector%20%28REGAD%29%20by%20pruning%20the%20edges%20of%20candidate%20nodes%20potentially%20with%20mistaken%20labels.%20Moreover%2C%20we%20design%20the%20performance%20feedback%20based%20on%20strategically%20crafted%20confident%20labels%20to%20guide%20the%20cutting%20process%2C%20ensuring%20optimal%20results.%20Specifically%2C%20REGAD%20contains%20two%20novel%20components.%20%28i%29%20A%20tailored%20policy%20network%2C%20which%20involves%20two-step%20actions%20to%20remove%20negative%20effect%20propagation%20step%20by%20step.%20%28ii%29%20A%20policy-in-the-loop%20mechanism%20to%20identify%20suitable%20edge%20removal%20strategies%20that%20control%20the%20propagation%20of%20noise%20on%20the%20graph%20and%20estimate%20the%20updated%20structure%20to%20obtain%20reliable%20pseudo%20labels%20iteratively.%20Experiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20REGAD%20outperforms%20all%20baselines%20under%20different%20noisy%20ratios.&entry.1838667208=http%3A//arxiv.org/abs/2407.05934v2&entry.124074799=Read"},
{"title": "Clustering High-dimensional Data: Balancing Abstraction and Representation Tutorial at AAAI 2026", "author": "Claudia Plant and Lena G. M. Bauer and Christian B\u00f6hm", "abstract": "How to find a natural grouping of a large real data set? Clustering requires a balance between abstraction and representation. To identify clusters, we need to abstract from superfluous details of individual objects. But we also need a rich representation that emphasizes the key features shared by groups of objects that distinguish them from other groups of objects.\n  Each clustering algorithm implements a different trade-off between abstraction and representation. Classical K-means implements a high level of abstraction - details are simply averaged out - combined with a very simple representation - all clusters are Gaussians in the original data space. We will see how approaches to subspace and deep clustering support high-dimensional and complex data by allowing richer representations. However, with increasing representational expressiveness comes the need to explicitly enforce abstraction in the objective function to ensure that the resulting method performs clustering and not just representation learning. We will see how current deep clustering methods define and enforce abstraction through centroid-based and density-based clustering losses. Balancing the conflicting goals of abstraction and representation is challenging. Ideas from subspace clustering help by learning one latent space for the information that is relevant to clustering and another latent space to capture all other information in the data.\n  The tutorial ends with an outlook on future research in clustering. Future methods will more adaptively balance abstraction and representation to improve performance, energy efficiency and interpretability. By automatically finding the sweet spot between abstraction and representation, the human brain is very good at clustering and other related tasks such as single-shot learning. So, there is still much room for improvement.", "link": "http://arxiv.org/abs/2601.11160v1", "date": "2026-01-16", "relevancy": 2.3905, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4894}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.481}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clustering%20High-dimensional%20Data%3A%20Balancing%20Abstraction%20and%20Representation%20Tutorial%20at%20AAAI%202026&body=Title%3A%20Clustering%20High-dimensional%20Data%3A%20Balancing%20Abstraction%20and%20Representation%20Tutorial%20at%20AAAI%202026%0AAuthor%3A%20Claudia%20Plant%20and%20Lena%20G.%20M.%20Bauer%20and%20Christian%20B%C3%B6hm%0AAbstract%3A%20How%20to%20find%20a%20natural%20grouping%20of%20a%20large%20real%20data%20set%3F%20Clustering%20requires%20a%20balance%20between%20abstraction%20and%20representation.%20To%20identify%20clusters%2C%20we%20need%20to%20abstract%20from%20superfluous%20details%20of%20individual%20objects.%20But%20we%20also%20need%20a%20rich%20representation%20that%20emphasizes%20the%20key%20features%20shared%20by%20groups%20of%20objects%20that%20distinguish%20them%20from%20other%20groups%20of%20objects.%0A%20%20Each%20clustering%20algorithm%20implements%20a%20different%20trade-off%20between%20abstraction%20and%20representation.%20Classical%20K-means%20implements%20a%20high%20level%20of%20abstraction%20-%20details%20are%20simply%20averaged%20out%20-%20combined%20with%20a%20very%20simple%20representation%20-%20all%20clusters%20are%20Gaussians%20in%20the%20original%20data%20space.%20We%20will%20see%20how%20approaches%20to%20subspace%20and%20deep%20clustering%20support%20high-dimensional%20and%20complex%20data%20by%20allowing%20richer%20representations.%20However%2C%20with%20increasing%20representational%20expressiveness%20comes%20the%20need%20to%20explicitly%20enforce%20abstraction%20in%20the%20objective%20function%20to%20ensure%20that%20the%20resulting%20method%20performs%20clustering%20and%20not%20just%20representation%20learning.%20We%20will%20see%20how%20current%20deep%20clustering%20methods%20define%20and%20enforce%20abstraction%20through%20centroid-based%20and%20density-based%20clustering%20losses.%20Balancing%20the%20conflicting%20goals%20of%20abstraction%20and%20representation%20is%20challenging.%20Ideas%20from%20subspace%20clustering%20help%20by%20learning%20one%20latent%20space%20for%20the%20information%20that%20is%20relevant%20to%20clustering%20and%20another%20latent%20space%20to%20capture%20all%20other%20information%20in%20the%20data.%0A%20%20The%20tutorial%20ends%20with%20an%20outlook%20on%20future%20research%20in%20clustering.%20Future%20methods%20will%20more%20adaptively%20balance%20abstraction%20and%20representation%20to%20improve%20performance%2C%20energy%20efficiency%20and%20interpretability.%20By%20automatically%20finding%20the%20sweet%20spot%20between%20abstraction%20and%20representation%2C%20the%20human%20brain%20is%20very%20good%20at%20clustering%20and%20other%20related%20tasks%20such%20as%20single-shot%20learning.%20So%2C%20there%20is%20still%20much%20room%20for%20improvement.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustering%2520High-dimensional%2520Data%253A%2520Balancing%2520Abstraction%2520and%2520Representation%2520Tutorial%2520at%2520AAAI%25202026%26entry.906535625%3DClaudia%2520Plant%2520and%2520Lena%2520G.%2520M.%2520Bauer%2520and%2520Christian%2520B%25C3%25B6hm%26entry.1292438233%3DHow%2520to%2520find%2520a%2520natural%2520grouping%2520of%2520a%2520large%2520real%2520data%2520set%253F%2520Clustering%2520requires%2520a%2520balance%2520between%2520abstraction%2520and%2520representation.%2520To%2520identify%2520clusters%252C%2520we%2520need%2520to%2520abstract%2520from%2520superfluous%2520details%2520of%2520individual%2520objects.%2520But%2520we%2520also%2520need%2520a%2520rich%2520representation%2520that%2520emphasizes%2520the%2520key%2520features%2520shared%2520by%2520groups%2520of%2520objects%2520that%2520distinguish%2520them%2520from%2520other%2520groups%2520of%2520objects.%250A%2520%2520Each%2520clustering%2520algorithm%2520implements%2520a%2520different%2520trade-off%2520between%2520abstraction%2520and%2520representation.%2520Classical%2520K-means%2520implements%2520a%2520high%2520level%2520of%2520abstraction%2520-%2520details%2520are%2520simply%2520averaged%2520out%2520-%2520combined%2520with%2520a%2520very%2520simple%2520representation%2520-%2520all%2520clusters%2520are%2520Gaussians%2520in%2520the%2520original%2520data%2520space.%2520We%2520will%2520see%2520how%2520approaches%2520to%2520subspace%2520and%2520deep%2520clustering%2520support%2520high-dimensional%2520and%2520complex%2520data%2520by%2520allowing%2520richer%2520representations.%2520However%252C%2520with%2520increasing%2520representational%2520expressiveness%2520comes%2520the%2520need%2520to%2520explicitly%2520enforce%2520abstraction%2520in%2520the%2520objective%2520function%2520to%2520ensure%2520that%2520the%2520resulting%2520method%2520performs%2520clustering%2520and%2520not%2520just%2520representation%2520learning.%2520We%2520will%2520see%2520how%2520current%2520deep%2520clustering%2520methods%2520define%2520and%2520enforce%2520abstraction%2520through%2520centroid-based%2520and%2520density-based%2520clustering%2520losses.%2520Balancing%2520the%2520conflicting%2520goals%2520of%2520abstraction%2520and%2520representation%2520is%2520challenging.%2520Ideas%2520from%2520subspace%2520clustering%2520help%2520by%2520learning%2520one%2520latent%2520space%2520for%2520the%2520information%2520that%2520is%2520relevant%2520to%2520clustering%2520and%2520another%2520latent%2520space%2520to%2520capture%2520all%2520other%2520information%2520in%2520the%2520data.%250A%2520%2520The%2520tutorial%2520ends%2520with%2520an%2520outlook%2520on%2520future%2520research%2520in%2520clustering.%2520Future%2520methods%2520will%2520more%2520adaptively%2520balance%2520abstraction%2520and%2520representation%2520to%2520improve%2520performance%252C%2520energy%2520efficiency%2520and%2520interpretability.%2520By%2520automatically%2520finding%2520the%2520sweet%2520spot%2520between%2520abstraction%2520and%2520representation%252C%2520the%2520human%2520brain%2520is%2520very%2520good%2520at%2520clustering%2520and%2520other%2520related%2520tasks%2520such%2520as%2520single-shot%2520learning.%2520So%252C%2520there%2520is%2520still%2520much%2520room%2520for%2520improvement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering%20High-dimensional%20Data%3A%20Balancing%20Abstraction%20and%20Representation%20Tutorial%20at%20AAAI%202026&entry.906535625=Claudia%20Plant%20and%20Lena%20G.%20M.%20Bauer%20and%20Christian%20B%C3%B6hm&entry.1292438233=How%20to%20find%20a%20natural%20grouping%20of%20a%20large%20real%20data%20set%3F%20Clustering%20requires%20a%20balance%20between%20abstraction%20and%20representation.%20To%20identify%20clusters%2C%20we%20need%20to%20abstract%20from%20superfluous%20details%20of%20individual%20objects.%20But%20we%20also%20need%20a%20rich%20representation%20that%20emphasizes%20the%20key%20features%20shared%20by%20groups%20of%20objects%20that%20distinguish%20them%20from%20other%20groups%20of%20objects.%0A%20%20Each%20clustering%20algorithm%20implements%20a%20different%20trade-off%20between%20abstraction%20and%20representation.%20Classical%20K-means%20implements%20a%20high%20level%20of%20abstraction%20-%20details%20are%20simply%20averaged%20out%20-%20combined%20with%20a%20very%20simple%20representation%20-%20all%20clusters%20are%20Gaussians%20in%20the%20original%20data%20space.%20We%20will%20see%20how%20approaches%20to%20subspace%20and%20deep%20clustering%20support%20high-dimensional%20and%20complex%20data%20by%20allowing%20richer%20representations.%20However%2C%20with%20increasing%20representational%20expressiveness%20comes%20the%20need%20to%20explicitly%20enforce%20abstraction%20in%20the%20objective%20function%20to%20ensure%20that%20the%20resulting%20method%20performs%20clustering%20and%20not%20just%20representation%20learning.%20We%20will%20see%20how%20current%20deep%20clustering%20methods%20define%20and%20enforce%20abstraction%20through%20centroid-based%20and%20density-based%20clustering%20losses.%20Balancing%20the%20conflicting%20goals%20of%20abstraction%20and%20representation%20is%20challenging.%20Ideas%20from%20subspace%20clustering%20help%20by%20learning%20one%20latent%20space%20for%20the%20information%20that%20is%20relevant%20to%20clustering%20and%20another%20latent%20space%20to%20capture%20all%20other%20information%20in%20the%20data.%0A%20%20The%20tutorial%20ends%20with%20an%20outlook%20on%20future%20research%20in%20clustering.%20Future%20methods%20will%20more%20adaptively%20balance%20abstraction%20and%20representation%20to%20improve%20performance%2C%20energy%20efficiency%20and%20interpretability.%20By%20automatically%20finding%20the%20sweet%20spot%20between%20abstraction%20and%20representation%2C%20the%20human%20brain%20is%20very%20good%20at%20clustering%20and%20other%20related%20tasks%20such%20as%20single-shot%20learning.%20So%2C%20there%20is%20still%20much%20room%20for%20improvement.&entry.1838667208=http%3A//arxiv.org/abs/2601.11160v1&entry.124074799=Read"},
{"title": "A Synthetic Benchmark for Collaborative 3D Semantic Occupancy Prediction in V2X-Enabled Autonomous Driving", "author": "Hanlin Wu and Pengfei Lin and Ehsan Javanmardi and Naren Bao and Bo Qian and Hao Si and Manabu Tsukada", "abstract": "3D semantic occupancy prediction is an emerging perception paradigm in autonomous driving, providing a voxel-level representation of both geometric details and semantic categories. However, its effectiveness is inherently constrained in single-vehicle setups by occlusions, restricted sensor range, and narrow viewpoints. To address these limitations, collaborative perception enables the exchange of complementary information, thereby enhancing the completeness and accuracy of predictions. Despite its potential, research on collaborative 3D semantic occupancy prediction is hindered by the lack of dedicated datasets. To bridge this gap, we design a high-resolution semantic voxel sensor in CARLA to produce dense and comprehensive annotations. We further develop a baseline model that performs inter-agent feature fusion via spatial alignment and attention aggregation. In addition, we establish benchmarks with varying prediction ranges designed to systematically assess the impact of spatial extent on collaborative prediction. Experimental results demonstrate the superior performance of our baseline, with increasing gains observed as range expands. Our code is available at https://github.com/tlab-wide/Co3SOP}{https://github.com/tlab-wide/Co3SOP.", "link": "http://arxiv.org/abs/2506.17004v3", "date": "2026-01-16", "relevancy": 2.3827, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6253}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Synthetic%20Benchmark%20for%20Collaborative%203D%20Semantic%20Occupancy%20Prediction%20in%20V2X-Enabled%20Autonomous%20Driving&body=Title%3A%20A%20Synthetic%20Benchmark%20for%20Collaborative%203D%20Semantic%20Occupancy%20Prediction%20in%20V2X-Enabled%20Autonomous%20Driving%0AAuthor%3A%20Hanlin%20Wu%20and%20Pengfei%20Lin%20and%20Ehsan%20Javanmardi%20and%20Naren%20Bao%20and%20Bo%20Qian%20and%20Hao%20Si%20and%20Manabu%20Tsukada%0AAbstract%3A%203D%20semantic%20occupancy%20prediction%20is%20an%20emerging%20perception%20paradigm%20in%20autonomous%20driving%2C%20providing%20a%20voxel-level%20representation%20of%20both%20geometric%20details%20and%20semantic%20categories.%20However%2C%20its%20effectiveness%20is%20inherently%20constrained%20in%20single-vehicle%20setups%20by%20occlusions%2C%20restricted%20sensor%20range%2C%20and%20narrow%20viewpoints.%20To%20address%20these%20limitations%2C%20collaborative%20perception%20enables%20the%20exchange%20of%20complementary%20information%2C%20thereby%20enhancing%20the%20completeness%20and%20accuracy%20of%20predictions.%20Despite%20its%20potential%2C%20research%20on%20collaborative%203D%20semantic%20occupancy%20prediction%20is%20hindered%20by%20the%20lack%20of%20dedicated%20datasets.%20To%20bridge%20this%20gap%2C%20we%20design%20a%20high-resolution%20semantic%20voxel%20sensor%20in%20CARLA%20to%20produce%20dense%20and%20comprehensive%20annotations.%20We%20further%20develop%20a%20baseline%20model%20that%20performs%20inter-agent%20feature%20fusion%20via%20spatial%20alignment%20and%20attention%20aggregation.%20In%20addition%2C%20we%20establish%20benchmarks%20with%20varying%20prediction%20ranges%20designed%20to%20systematically%20assess%20the%20impact%20of%20spatial%20extent%20on%20collaborative%20prediction.%20Experimental%20results%20demonstrate%20the%20superior%20performance%20of%20our%20baseline%2C%20with%20increasing%20gains%20observed%20as%20range%20expands.%20Our%20code%20is%20available%20at%20https%3A//github.com/tlab-wide/Co3SOP%7D%7Bhttps%3A//github.com/tlab-wide/Co3SOP.%0ALink%3A%20http%3A//arxiv.org/abs/2506.17004v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Synthetic%2520Benchmark%2520for%2520Collaborative%25203D%2520Semantic%2520Occupancy%2520Prediction%2520in%2520V2X-Enabled%2520Autonomous%2520Driving%26entry.906535625%3DHanlin%2520Wu%2520and%2520Pengfei%2520Lin%2520and%2520Ehsan%2520Javanmardi%2520and%2520Naren%2520Bao%2520and%2520Bo%2520Qian%2520and%2520Hao%2520Si%2520and%2520Manabu%2520Tsukada%26entry.1292438233%3D3D%2520semantic%2520occupancy%2520prediction%2520is%2520an%2520emerging%2520perception%2520paradigm%2520in%2520autonomous%2520driving%252C%2520providing%2520a%2520voxel-level%2520representation%2520of%2520both%2520geometric%2520details%2520and%2520semantic%2520categories.%2520However%252C%2520its%2520effectiveness%2520is%2520inherently%2520constrained%2520in%2520single-vehicle%2520setups%2520by%2520occlusions%252C%2520restricted%2520sensor%2520range%252C%2520and%2520narrow%2520viewpoints.%2520To%2520address%2520these%2520limitations%252C%2520collaborative%2520perception%2520enables%2520the%2520exchange%2520of%2520complementary%2520information%252C%2520thereby%2520enhancing%2520the%2520completeness%2520and%2520accuracy%2520of%2520predictions.%2520Despite%2520its%2520potential%252C%2520research%2520on%2520collaborative%25203D%2520semantic%2520occupancy%2520prediction%2520is%2520hindered%2520by%2520the%2520lack%2520of%2520dedicated%2520datasets.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520design%2520a%2520high-resolution%2520semantic%2520voxel%2520sensor%2520in%2520CARLA%2520to%2520produce%2520dense%2520and%2520comprehensive%2520annotations.%2520We%2520further%2520develop%2520a%2520baseline%2520model%2520that%2520performs%2520inter-agent%2520feature%2520fusion%2520via%2520spatial%2520alignment%2520and%2520attention%2520aggregation.%2520In%2520addition%252C%2520we%2520establish%2520benchmarks%2520with%2520varying%2520prediction%2520ranges%2520designed%2520to%2520systematically%2520assess%2520the%2520impact%2520of%2520spatial%2520extent%2520on%2520collaborative%2520prediction.%2520Experimental%2520results%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%2520baseline%252C%2520with%2520increasing%2520gains%2520observed%2520as%2520range%2520expands.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/tlab-wide/Co3SOP%257D%257Bhttps%253A//github.com/tlab-wide/Co3SOP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.17004v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Synthetic%20Benchmark%20for%20Collaborative%203D%20Semantic%20Occupancy%20Prediction%20in%20V2X-Enabled%20Autonomous%20Driving&entry.906535625=Hanlin%20Wu%20and%20Pengfei%20Lin%20and%20Ehsan%20Javanmardi%20and%20Naren%20Bao%20and%20Bo%20Qian%20and%20Hao%20Si%20and%20Manabu%20Tsukada&entry.1292438233=3D%20semantic%20occupancy%20prediction%20is%20an%20emerging%20perception%20paradigm%20in%20autonomous%20driving%2C%20providing%20a%20voxel-level%20representation%20of%20both%20geometric%20details%20and%20semantic%20categories.%20However%2C%20its%20effectiveness%20is%20inherently%20constrained%20in%20single-vehicle%20setups%20by%20occlusions%2C%20restricted%20sensor%20range%2C%20and%20narrow%20viewpoints.%20To%20address%20these%20limitations%2C%20collaborative%20perception%20enables%20the%20exchange%20of%20complementary%20information%2C%20thereby%20enhancing%20the%20completeness%20and%20accuracy%20of%20predictions.%20Despite%20its%20potential%2C%20research%20on%20collaborative%203D%20semantic%20occupancy%20prediction%20is%20hindered%20by%20the%20lack%20of%20dedicated%20datasets.%20To%20bridge%20this%20gap%2C%20we%20design%20a%20high-resolution%20semantic%20voxel%20sensor%20in%20CARLA%20to%20produce%20dense%20and%20comprehensive%20annotations.%20We%20further%20develop%20a%20baseline%20model%20that%20performs%20inter-agent%20feature%20fusion%20via%20spatial%20alignment%20and%20attention%20aggregation.%20In%20addition%2C%20we%20establish%20benchmarks%20with%20varying%20prediction%20ranges%20designed%20to%20systematically%20assess%20the%20impact%20of%20spatial%20extent%20on%20collaborative%20prediction.%20Experimental%20results%20demonstrate%20the%20superior%20performance%20of%20our%20baseline%2C%20with%20increasing%20gains%20observed%20as%20range%20expands.%20Our%20code%20is%20available%20at%20https%3A//github.com/tlab-wide/Co3SOP%7D%7Bhttps%3A//github.com/tlab-wide/Co3SOP.&entry.1838667208=http%3A//arxiv.org/abs/2506.17004v3&entry.124074799=Read"},
{"title": "VidLeaks: Membership Inference Attacks Against Text-to-Video Models", "author": "Li Wang and Wenyu Chen and Ning Yu and Zheng Li and Shanqing Guo", "abstract": "The proliferation of powerful Text-to-Video (T2V) models, trained on massive web-scale datasets, raises urgent concerns about copyright and privacy violations. Membership inference attacks (MIAs) provide a principled tool for auditing such risks, yet existing techniques - designed for static data like images or text - fail to capture the spatio-temporal complexities of video generation. In particular, they overlook the sparsity of memorization signals in keyframes and the instability introduced by stochastic temporal dynamics. In this paper, we conduct the first systematic study of MIAs against T2V models and introduce a novel framework VidLeaks, which probes sparse-temporal memorization through two complementary signals: 1) Spatial Reconstruction Fidelity (SRF), using a Top-K similarity to amplify spatial memorization signals from sparsely memorized keyframes, and 2) Temporal Generative Stability (TGS), which measures semantic consistency across multiple queries to capture temporal leakage. We evaluate VidLeaks under three progressively restrictive black-box settings - supervised, reference-based, and query-only. Experiments on three representative T2V models reveal severe vulnerabilities: VidLeaks achieves AUC of 82.92% on AnimateDiff and 97.01% on InstructVideo even in the strict query-only setting, posing a realistic and exploitable privacy risk. Our work provides the first concrete evidence that T2V models leak substantial membership information through both sparse and temporal memorization, establishing a foundation for auditing video generation systems and motivating the development of new defenses. Code is available at: https://zenodo.org/records/17972831.", "link": "http://arxiv.org/abs/2601.11210v1", "date": "2026-01-16", "relevancy": 2.3687, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6071}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6007}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidLeaks%3A%20Membership%20Inference%20Attacks%20Against%20Text-to-Video%20Models&body=Title%3A%20VidLeaks%3A%20Membership%20Inference%20Attacks%20Against%20Text-to-Video%20Models%0AAuthor%3A%20Li%20Wang%20and%20Wenyu%20Chen%20and%20Ning%20Yu%20and%20Zheng%20Li%20and%20Shanqing%20Guo%0AAbstract%3A%20The%20proliferation%20of%20powerful%20Text-to-Video%20%28T2V%29%20models%2C%20trained%20on%20massive%20web-scale%20datasets%2C%20raises%20urgent%20concerns%20about%20copyright%20and%20privacy%20violations.%20Membership%20inference%20attacks%20%28MIAs%29%20provide%20a%20principled%20tool%20for%20auditing%20such%20risks%2C%20yet%20existing%20techniques%20-%20designed%20for%20static%20data%20like%20images%20or%20text%20-%20fail%20to%20capture%20the%20spatio-temporal%20complexities%20of%20video%20generation.%20In%20particular%2C%20they%20overlook%20the%20sparsity%20of%20memorization%20signals%20in%20keyframes%20and%20the%20instability%20introduced%20by%20stochastic%20temporal%20dynamics.%20In%20this%20paper%2C%20we%20conduct%20the%20first%20systematic%20study%20of%20MIAs%20against%20T2V%20models%20and%20introduce%20a%20novel%20framework%20VidLeaks%2C%20which%20probes%20sparse-temporal%20memorization%20through%20two%20complementary%20signals%3A%201%29%20Spatial%20Reconstruction%20Fidelity%20%28SRF%29%2C%20using%20a%20Top-K%20similarity%20to%20amplify%20spatial%20memorization%20signals%20from%20sparsely%20memorized%20keyframes%2C%20and%202%29%20Temporal%20Generative%20Stability%20%28TGS%29%2C%20which%20measures%20semantic%20consistency%20across%20multiple%20queries%20to%20capture%20temporal%20leakage.%20We%20evaluate%20VidLeaks%20under%20three%20progressively%20restrictive%20black-box%20settings%20-%20supervised%2C%20reference-based%2C%20and%20query-only.%20Experiments%20on%20three%20representative%20T2V%20models%20reveal%20severe%20vulnerabilities%3A%20VidLeaks%20achieves%20AUC%20of%2082.92%25%20on%20AnimateDiff%20and%2097.01%25%20on%20InstructVideo%20even%20in%20the%20strict%20query-only%20setting%2C%20posing%20a%20realistic%20and%20exploitable%20privacy%20risk.%20Our%20work%20provides%20the%20first%20concrete%20evidence%20that%20T2V%20models%20leak%20substantial%20membership%20information%20through%20both%20sparse%20and%20temporal%20memorization%2C%20establishing%20a%20foundation%20for%20auditing%20video%20generation%20systems%20and%20motivating%20the%20development%20of%20new%20defenses.%20Code%20is%20available%20at%3A%20https%3A//zenodo.org/records/17972831.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidLeaks%253A%2520Membership%2520Inference%2520Attacks%2520Against%2520Text-to-Video%2520Models%26entry.906535625%3DLi%2520Wang%2520and%2520Wenyu%2520Chen%2520and%2520Ning%2520Yu%2520and%2520Zheng%2520Li%2520and%2520Shanqing%2520Guo%26entry.1292438233%3DThe%2520proliferation%2520of%2520powerful%2520Text-to-Video%2520%2528T2V%2529%2520models%252C%2520trained%2520on%2520massive%2520web-scale%2520datasets%252C%2520raises%2520urgent%2520concerns%2520about%2520copyright%2520and%2520privacy%2520violations.%2520Membership%2520inference%2520attacks%2520%2528MIAs%2529%2520provide%2520a%2520principled%2520tool%2520for%2520auditing%2520such%2520risks%252C%2520yet%2520existing%2520techniques%2520-%2520designed%2520for%2520static%2520data%2520like%2520images%2520or%2520text%2520-%2520fail%2520to%2520capture%2520the%2520spatio-temporal%2520complexities%2520of%2520video%2520generation.%2520In%2520particular%252C%2520they%2520overlook%2520the%2520sparsity%2520of%2520memorization%2520signals%2520in%2520keyframes%2520and%2520the%2520instability%2520introduced%2520by%2520stochastic%2520temporal%2520dynamics.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520the%2520first%2520systematic%2520study%2520of%2520MIAs%2520against%2520T2V%2520models%2520and%2520introduce%2520a%2520novel%2520framework%2520VidLeaks%252C%2520which%2520probes%2520sparse-temporal%2520memorization%2520through%2520two%2520complementary%2520signals%253A%25201%2529%2520Spatial%2520Reconstruction%2520Fidelity%2520%2528SRF%2529%252C%2520using%2520a%2520Top-K%2520similarity%2520to%2520amplify%2520spatial%2520memorization%2520signals%2520from%2520sparsely%2520memorized%2520keyframes%252C%2520and%25202%2529%2520Temporal%2520Generative%2520Stability%2520%2528TGS%2529%252C%2520which%2520measures%2520semantic%2520consistency%2520across%2520multiple%2520queries%2520to%2520capture%2520temporal%2520leakage.%2520We%2520evaluate%2520VidLeaks%2520under%2520three%2520progressively%2520restrictive%2520black-box%2520settings%2520-%2520supervised%252C%2520reference-based%252C%2520and%2520query-only.%2520Experiments%2520on%2520three%2520representative%2520T2V%2520models%2520reveal%2520severe%2520vulnerabilities%253A%2520VidLeaks%2520achieves%2520AUC%2520of%252082.92%2525%2520on%2520AnimateDiff%2520and%252097.01%2525%2520on%2520InstructVideo%2520even%2520in%2520the%2520strict%2520query-only%2520setting%252C%2520posing%2520a%2520realistic%2520and%2520exploitable%2520privacy%2520risk.%2520Our%2520work%2520provides%2520the%2520first%2520concrete%2520evidence%2520that%2520T2V%2520models%2520leak%2520substantial%2520membership%2520information%2520through%2520both%2520sparse%2520and%2520temporal%2520memorization%252C%2520establishing%2520a%2520foundation%2520for%2520auditing%2520video%2520generation%2520systems%2520and%2520motivating%2520the%2520development%2520of%2520new%2520defenses.%2520Code%2520is%2520available%2520at%253A%2520https%253A//zenodo.org/records/17972831.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidLeaks%3A%20Membership%20Inference%20Attacks%20Against%20Text-to-Video%20Models&entry.906535625=Li%20Wang%20and%20Wenyu%20Chen%20and%20Ning%20Yu%20and%20Zheng%20Li%20and%20Shanqing%20Guo&entry.1292438233=The%20proliferation%20of%20powerful%20Text-to-Video%20%28T2V%29%20models%2C%20trained%20on%20massive%20web-scale%20datasets%2C%20raises%20urgent%20concerns%20about%20copyright%20and%20privacy%20violations.%20Membership%20inference%20attacks%20%28MIAs%29%20provide%20a%20principled%20tool%20for%20auditing%20such%20risks%2C%20yet%20existing%20techniques%20-%20designed%20for%20static%20data%20like%20images%20or%20text%20-%20fail%20to%20capture%20the%20spatio-temporal%20complexities%20of%20video%20generation.%20In%20particular%2C%20they%20overlook%20the%20sparsity%20of%20memorization%20signals%20in%20keyframes%20and%20the%20instability%20introduced%20by%20stochastic%20temporal%20dynamics.%20In%20this%20paper%2C%20we%20conduct%20the%20first%20systematic%20study%20of%20MIAs%20against%20T2V%20models%20and%20introduce%20a%20novel%20framework%20VidLeaks%2C%20which%20probes%20sparse-temporal%20memorization%20through%20two%20complementary%20signals%3A%201%29%20Spatial%20Reconstruction%20Fidelity%20%28SRF%29%2C%20using%20a%20Top-K%20similarity%20to%20amplify%20spatial%20memorization%20signals%20from%20sparsely%20memorized%20keyframes%2C%20and%202%29%20Temporal%20Generative%20Stability%20%28TGS%29%2C%20which%20measures%20semantic%20consistency%20across%20multiple%20queries%20to%20capture%20temporal%20leakage.%20We%20evaluate%20VidLeaks%20under%20three%20progressively%20restrictive%20black-box%20settings%20-%20supervised%2C%20reference-based%2C%20and%20query-only.%20Experiments%20on%20three%20representative%20T2V%20models%20reveal%20severe%20vulnerabilities%3A%20VidLeaks%20achieves%20AUC%20of%2082.92%25%20on%20AnimateDiff%20and%2097.01%25%20on%20InstructVideo%20even%20in%20the%20strict%20query-only%20setting%2C%20posing%20a%20realistic%20and%20exploitable%20privacy%20risk.%20Our%20work%20provides%20the%20first%20concrete%20evidence%20that%20T2V%20models%20leak%20substantial%20membership%20information%20through%20both%20sparse%20and%20temporal%20memorization%2C%20establishing%20a%20foundation%20for%20auditing%20video%20generation%20systems%20and%20motivating%20the%20development%20of%20new%20defenses.%20Code%20is%20available%20at%3A%20https%3A//zenodo.org/records/17972831.&entry.1838667208=http%3A//arxiv.org/abs/2601.11210v1&entry.124074799=Read"},
{"title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation", "author": "Ruiheng Zhang and Jingfeng Yao and Huangxuan Zhao and Hao Yan and Xiao He and Lei Chen and Zhou Wei and Yong Luo and Zengmao Wang and Lefei Zhang and Dacheng Tao and Bo Du", "abstract": "Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.", "link": "http://arxiv.org/abs/2601.11522v1", "date": "2026-01-16", "relevancy": 2.3685, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6312}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6064}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniX%3A%20Unifying%20Autoregression%20and%20Diffusion%20for%20Chest%20X-Ray%20Understanding%20and%20Generation&body=Title%3A%20UniX%3A%20Unifying%20Autoregression%20and%20Diffusion%20for%20Chest%20X-Ray%20Understanding%20and%20Generation%0AAuthor%3A%20Ruiheng%20Zhang%20and%20Jingfeng%20Yao%20and%20Huangxuan%20Zhao%20and%20Hao%20Yan%20and%20Xiao%20He%20and%20Lei%20Chen%20and%20Zhou%20Wei%20and%20Yong%20Luo%20and%20Zengmao%20Wang%20and%20Lefei%20Zhang%20and%20Dacheng%20Tao%20and%20Bo%20Du%0AAbstract%3A%20Despite%20recent%20progress%2C%20medical%20foundation%20models%20still%20struggle%20to%20unify%20visual%20understanding%20and%20generation%2C%20as%20these%20tasks%20have%20inherently%20conflicting%20goals%3A%20semantic%20abstraction%20versus%20pixel-level%20reconstruction.%20Existing%20approaches%2C%20typically%20based%20on%20parameter-shared%20autoregressive%20architectures%2C%20frequently%20lead%20to%20compromised%20performance%20in%20one%20or%20both%20tasks.%20To%20address%20this%2C%20we%20present%20UniX%2C%20a%20next-generation%20unified%20medical%20foundation%20model%20for%20chest%20X-ray%20understanding%20and%20generation.%20UniX%20decouples%20the%20two%20tasks%20into%20an%20autoregressive%20branch%20for%20understanding%20and%20a%20diffusion%20branch%20for%20high-fidelity%20generation.%20Crucially%2C%20a%20cross-modal%20self-attention%20mechanism%20is%20introduced%20to%20dynamically%20guide%20the%20generation%20process%20with%20understanding%20features.%20Coupled%20with%20a%20rigorous%20data%20cleaning%20pipeline%20and%20a%20multi-stage%20training%20strategy%2C%20this%20architecture%20enables%20synergistic%20collaboration%20between%20tasks%20while%20leveraging%20the%20strengths%20of%20diffusion%20models%20for%20superior%20generation.%20On%20two%20representative%20benchmarks%2C%20UniX%20achieves%20a%2046.1%25%20improvement%20in%20understanding%20performance%20%28Micro-F1%29%20and%20a%2024.2%25%20gain%20in%20generation%20quality%20%28FD-RadDino%29%2C%20using%20only%20a%20quarter%20of%20the%20parameters%20of%20LLM-CXR.%20By%20achieving%20performance%20on%20par%20with%20task-specific%20models%2C%20our%20work%20establishes%20a%20scalable%20paradigm%20for%20synergistic%20medical%20image%20understanding%20and%20generation.%20Codes%20and%20models%20are%20available%20at%20https%3A//github.com/ZrH42/UniX.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniX%253A%2520Unifying%2520Autoregression%2520and%2520Diffusion%2520for%2520Chest%2520X-Ray%2520Understanding%2520and%2520Generation%26entry.906535625%3DRuiheng%2520Zhang%2520and%2520Jingfeng%2520Yao%2520and%2520Huangxuan%2520Zhao%2520and%2520Hao%2520Yan%2520and%2520Xiao%2520He%2520and%2520Lei%2520Chen%2520and%2520Zhou%2520Wei%2520and%2520Yong%2520Luo%2520and%2520Zengmao%2520Wang%2520and%2520Lefei%2520Zhang%2520and%2520Dacheng%2520Tao%2520and%2520Bo%2520Du%26entry.1292438233%3DDespite%2520recent%2520progress%252C%2520medical%2520foundation%2520models%2520still%2520struggle%2520to%2520unify%2520visual%2520understanding%2520and%2520generation%252C%2520as%2520these%2520tasks%2520have%2520inherently%2520conflicting%2520goals%253A%2520semantic%2520abstraction%2520versus%2520pixel-level%2520reconstruction.%2520Existing%2520approaches%252C%2520typically%2520based%2520on%2520parameter-shared%2520autoregressive%2520architectures%252C%2520frequently%2520lead%2520to%2520compromised%2520performance%2520in%2520one%2520or%2520both%2520tasks.%2520To%2520address%2520this%252C%2520we%2520present%2520UniX%252C%2520a%2520next-generation%2520unified%2520medical%2520foundation%2520model%2520for%2520chest%2520X-ray%2520understanding%2520and%2520generation.%2520UniX%2520decouples%2520the%2520two%2520tasks%2520into%2520an%2520autoregressive%2520branch%2520for%2520understanding%2520and%2520a%2520diffusion%2520branch%2520for%2520high-fidelity%2520generation.%2520Crucially%252C%2520a%2520cross-modal%2520self-attention%2520mechanism%2520is%2520introduced%2520to%2520dynamically%2520guide%2520the%2520generation%2520process%2520with%2520understanding%2520features.%2520Coupled%2520with%2520a%2520rigorous%2520data%2520cleaning%2520pipeline%2520and%2520a%2520multi-stage%2520training%2520strategy%252C%2520this%2520architecture%2520enables%2520synergistic%2520collaboration%2520between%2520tasks%2520while%2520leveraging%2520the%2520strengths%2520of%2520diffusion%2520models%2520for%2520superior%2520generation.%2520On%2520two%2520representative%2520benchmarks%252C%2520UniX%2520achieves%2520a%252046.1%2525%2520improvement%2520in%2520understanding%2520performance%2520%2528Micro-F1%2529%2520and%2520a%252024.2%2525%2520gain%2520in%2520generation%2520quality%2520%2528FD-RadDino%2529%252C%2520using%2520only%2520a%2520quarter%2520of%2520the%2520parameters%2520of%2520LLM-CXR.%2520By%2520achieving%2520performance%2520on%2520par%2520with%2520task-specific%2520models%252C%2520our%2520work%2520establishes%2520a%2520scalable%2520paradigm%2520for%2520synergistic%2520medical%2520image%2520understanding%2520and%2520generation.%2520Codes%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/ZrH42/UniX.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniX%3A%20Unifying%20Autoregression%20and%20Diffusion%20for%20Chest%20X-Ray%20Understanding%20and%20Generation&entry.906535625=Ruiheng%20Zhang%20and%20Jingfeng%20Yao%20and%20Huangxuan%20Zhao%20and%20Hao%20Yan%20and%20Xiao%20He%20and%20Lei%20Chen%20and%20Zhou%20Wei%20and%20Yong%20Luo%20and%20Zengmao%20Wang%20and%20Lefei%20Zhang%20and%20Dacheng%20Tao%20and%20Bo%20Du&entry.1292438233=Despite%20recent%20progress%2C%20medical%20foundation%20models%20still%20struggle%20to%20unify%20visual%20understanding%20and%20generation%2C%20as%20these%20tasks%20have%20inherently%20conflicting%20goals%3A%20semantic%20abstraction%20versus%20pixel-level%20reconstruction.%20Existing%20approaches%2C%20typically%20based%20on%20parameter-shared%20autoregressive%20architectures%2C%20frequently%20lead%20to%20compromised%20performance%20in%20one%20or%20both%20tasks.%20To%20address%20this%2C%20we%20present%20UniX%2C%20a%20next-generation%20unified%20medical%20foundation%20model%20for%20chest%20X-ray%20understanding%20and%20generation.%20UniX%20decouples%20the%20two%20tasks%20into%20an%20autoregressive%20branch%20for%20understanding%20and%20a%20diffusion%20branch%20for%20high-fidelity%20generation.%20Crucially%2C%20a%20cross-modal%20self-attention%20mechanism%20is%20introduced%20to%20dynamically%20guide%20the%20generation%20process%20with%20understanding%20features.%20Coupled%20with%20a%20rigorous%20data%20cleaning%20pipeline%20and%20a%20multi-stage%20training%20strategy%2C%20this%20architecture%20enables%20synergistic%20collaboration%20between%20tasks%20while%20leveraging%20the%20strengths%20of%20diffusion%20models%20for%20superior%20generation.%20On%20two%20representative%20benchmarks%2C%20UniX%20achieves%20a%2046.1%25%20improvement%20in%20understanding%20performance%20%28Micro-F1%29%20and%20a%2024.2%25%20gain%20in%20generation%20quality%20%28FD-RadDino%29%2C%20using%20only%20a%20quarter%20of%20the%20parameters%20of%20LLM-CXR.%20By%20achieving%20performance%20on%20par%20with%20task-specific%20models%2C%20our%20work%20establishes%20a%20scalable%20paradigm%20for%20synergistic%20medical%20image%20understanding%20and%20generation.%20Codes%20and%20models%20are%20available%20at%20https%3A//github.com/ZrH42/UniX.&entry.1838667208=http%3A//arxiv.org/abs/2601.11522v1&entry.124074799=Read"},
{"title": "Information Theoretic Perspective on Representation Learning", "author": "Deborah Pereg", "abstract": "An information-theoretic framework is introduced to analyze last-layer embedding, focusing on learned representations for regression tasks. We define representation-rate and derive limits on the reliability with which input-output information can be represented as is inherently determined by the input-source entropy. We further define representation capacity in a perturbed setting, and representation rate-distortion for a compressed output. We derive the achievable capacity, the achievable representation-rate, and their converse. Finally, we combine the results in a unified setting.", "link": "http://arxiv.org/abs/2601.11334v1", "date": "2026-01-16", "relevancy": 2.3592, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.491}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4647}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information%20Theoretic%20Perspective%20on%20Representation%20Learning&body=Title%3A%20Information%20Theoretic%20Perspective%20on%20Representation%20Learning%0AAuthor%3A%20Deborah%20Pereg%0AAbstract%3A%20An%20information-theoretic%20framework%20is%20introduced%20to%20analyze%20last-layer%20embedding%2C%20focusing%20on%20learned%20representations%20for%20regression%20tasks.%20We%20define%20representation-rate%20and%20derive%20limits%20on%20the%20reliability%20with%20which%20input-output%20information%20can%20be%20represented%20as%20is%20inherently%20determined%20by%20the%20input-source%20entropy.%20We%20further%20define%20representation%20capacity%20in%20a%20perturbed%20setting%2C%20and%20representation%20rate-distortion%20for%20a%20compressed%20output.%20We%20derive%20the%20achievable%20capacity%2C%20the%20achievable%20representation-rate%2C%20and%20their%20converse.%20Finally%2C%20we%20combine%20the%20results%20in%20a%20unified%20setting.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation%2520Theoretic%2520Perspective%2520on%2520Representation%2520Learning%26entry.906535625%3DDeborah%2520Pereg%26entry.1292438233%3DAn%2520information-theoretic%2520framework%2520is%2520introduced%2520to%2520analyze%2520last-layer%2520embedding%252C%2520focusing%2520on%2520learned%2520representations%2520for%2520regression%2520tasks.%2520We%2520define%2520representation-rate%2520and%2520derive%2520limits%2520on%2520the%2520reliability%2520with%2520which%2520input-output%2520information%2520can%2520be%2520represented%2520as%2520is%2520inherently%2520determined%2520by%2520the%2520input-source%2520entropy.%2520We%2520further%2520define%2520representation%2520capacity%2520in%2520a%2520perturbed%2520setting%252C%2520and%2520representation%2520rate-distortion%2520for%2520a%2520compressed%2520output.%2520We%2520derive%2520the%2520achievable%2520capacity%252C%2520the%2520achievable%2520representation-rate%252C%2520and%2520their%2520converse.%2520Finally%252C%2520we%2520combine%2520the%2520results%2520in%2520a%2520unified%2520setting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information%20Theoretic%20Perspective%20on%20Representation%20Learning&entry.906535625=Deborah%20Pereg&entry.1292438233=An%20information-theoretic%20framework%20is%20introduced%20to%20analyze%20last-layer%20embedding%2C%20focusing%20on%20learned%20representations%20for%20regression%20tasks.%20We%20define%20representation-rate%20and%20derive%20limits%20on%20the%20reliability%20with%20which%20input-output%20information%20can%20be%20represented%20as%20is%20inherently%20determined%20by%20the%20input-source%20entropy.%20We%20further%20define%20representation%20capacity%20in%20a%20perturbed%20setting%2C%20and%20representation%20rate-distortion%20for%20a%20compressed%20output.%20We%20derive%20the%20achievable%20capacity%2C%20the%20achievable%20representation-rate%2C%20and%20their%20converse.%20Finally%2C%20we%20combine%20the%20results%20in%20a%20unified%20setting.&entry.1838667208=http%3A//arxiv.org/abs/2601.11334v1&entry.124074799=Read"},
{"title": "Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra", "author": "Laura Mismetti and Marvin Alberts and Andreas Krause and Mara Graziani", "abstract": "Tandem Mass Spectrometry is a cornerstone technique for identifying unknown small molecules in fields such as metabolomics, natural product discovery and environmental analysis. However, certain aspects, such as the probabilistic fragmentation process and size of the chemical space, make structure elucidation from such spectra highly challenging, particularly when there is a shift between the deployment and training conditions. Current methods rely on database matching of previously observed spectra of known molecules and multi-step pipelines that require intermediate fingerprint prediction or expensive fragment annotations. We introduce a novel end-to-end framework based on a transformer model that directly generates molecular structures from an input tandem mass spectrum and its corresponding molecular formula, thereby eliminating the need for manual annotations and intermediate steps, while leveraging transfer learning from simulated data. To further address the challenge of out-of-distribution spectra, we introduce a test-time tuning strategy that dynamically adapts the pre-trained model to novel experimental data. Our approach achieves a Top-1 accuracy of 3.16% on the MassSpecGym benchmark and 12.88% on the NPLIB1 datasets, considerably outperforming conventional fine-tuning. Baseline approaches are also surpassed by 27% and 67% respectively. Even when the exact reference structure is not recovered, the generated candidates are chemically informative, exhibiting high structural plausibility as reflected by strong Tanimoto similarity to the ground truth. Notably, we observe a relative improvement in average Tanimoto similarity of 83% on NPLIB1 and 64% on MassSpecGym compared to state-of-the-art methods. Our framework combines simplicity with adaptability, generating accurate molecular candidates that offer valuable guidance for expert interpretation of unseen spectra.", "link": "http://arxiv.org/abs/2510.23746v2", "date": "2026-01-16", "relevancy": 2.3499, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4698}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Tuned%20Language%20Models%20Enable%20End-to-end%20De%20Novo%20Molecular%20Structure%20Generation%20from%20MS/MS%20Spectra&body=Title%3A%20Test-Time%20Tuned%20Language%20Models%20Enable%20End-to-end%20De%20Novo%20Molecular%20Structure%20Generation%20from%20MS/MS%20Spectra%0AAuthor%3A%20Laura%20Mismetti%20and%20Marvin%20Alberts%20and%20Andreas%20Krause%20and%20Mara%20Graziani%0AAbstract%3A%20Tandem%20Mass%20Spectrometry%20is%20a%20cornerstone%20technique%20for%20identifying%20unknown%20small%20molecules%20in%20fields%20such%20as%20metabolomics%2C%20natural%20product%20discovery%20and%20environmental%20analysis.%20However%2C%20certain%20aspects%2C%20such%20as%20the%20probabilistic%20fragmentation%20process%20and%20size%20of%20the%20chemical%20space%2C%20make%20structure%20elucidation%20from%20such%20spectra%20highly%20challenging%2C%20particularly%20when%20there%20is%20a%20shift%20between%20the%20deployment%20and%20training%20conditions.%20Current%20methods%20rely%20on%20database%20matching%20of%20previously%20observed%20spectra%20of%20known%20molecules%20and%20multi-step%20pipelines%20that%20require%20intermediate%20fingerprint%20prediction%20or%20expensive%20fragment%20annotations.%20We%20introduce%20a%20novel%20end-to-end%20framework%20based%20on%20a%20transformer%20model%20that%20directly%20generates%20molecular%20structures%20from%20an%20input%20tandem%20mass%20spectrum%20and%20its%20corresponding%20molecular%20formula%2C%20thereby%20eliminating%20the%20need%20for%20manual%20annotations%20and%20intermediate%20steps%2C%20while%20leveraging%20transfer%20learning%20from%20simulated%20data.%20To%20further%20address%20the%20challenge%20of%20out-of-distribution%20spectra%2C%20we%20introduce%20a%20test-time%20tuning%20strategy%20that%20dynamically%20adapts%20the%20pre-trained%20model%20to%20novel%20experimental%20data.%20Our%20approach%20achieves%20a%20Top-1%20accuracy%20of%203.16%25%20on%20the%20MassSpecGym%20benchmark%20and%2012.88%25%20on%20the%20NPLIB1%20datasets%2C%20considerably%20outperforming%20conventional%20fine-tuning.%20Baseline%20approaches%20are%20also%20surpassed%20by%2027%25%20and%2067%25%20respectively.%20Even%20when%20the%20exact%20reference%20structure%20is%20not%20recovered%2C%20the%20generated%20candidates%20are%20chemically%20informative%2C%20exhibiting%20high%20structural%20plausibility%20as%20reflected%20by%20strong%20Tanimoto%20similarity%20to%20the%20ground%20truth.%20Notably%2C%20we%20observe%20a%20relative%20improvement%20in%20average%20Tanimoto%20similarity%20of%2083%25%20on%20NPLIB1%20and%2064%25%20on%20MassSpecGym%20compared%20to%20state-of-the-art%20methods.%20Our%20framework%20combines%20simplicity%20with%20adaptability%2C%20generating%20accurate%20molecular%20candidates%20that%20offer%20valuable%20guidance%20for%20expert%20interpretation%20of%20unseen%20spectra.%0ALink%3A%20http%3A//arxiv.org/abs/2510.23746v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Tuned%2520Language%2520Models%2520Enable%2520End-to-end%2520De%2520Novo%2520Molecular%2520Structure%2520Generation%2520from%2520MS/MS%2520Spectra%26entry.906535625%3DLaura%2520Mismetti%2520and%2520Marvin%2520Alberts%2520and%2520Andreas%2520Krause%2520and%2520Mara%2520Graziani%26entry.1292438233%3DTandem%2520Mass%2520Spectrometry%2520is%2520a%2520cornerstone%2520technique%2520for%2520identifying%2520unknown%2520small%2520molecules%2520in%2520fields%2520such%2520as%2520metabolomics%252C%2520natural%2520product%2520discovery%2520and%2520environmental%2520analysis.%2520However%252C%2520certain%2520aspects%252C%2520such%2520as%2520the%2520probabilistic%2520fragmentation%2520process%2520and%2520size%2520of%2520the%2520chemical%2520space%252C%2520make%2520structure%2520elucidation%2520from%2520such%2520spectra%2520highly%2520challenging%252C%2520particularly%2520when%2520there%2520is%2520a%2520shift%2520between%2520the%2520deployment%2520and%2520training%2520conditions.%2520Current%2520methods%2520rely%2520on%2520database%2520matching%2520of%2520previously%2520observed%2520spectra%2520of%2520known%2520molecules%2520and%2520multi-step%2520pipelines%2520that%2520require%2520intermediate%2520fingerprint%2520prediction%2520or%2520expensive%2520fragment%2520annotations.%2520We%2520introduce%2520a%2520novel%2520end-to-end%2520framework%2520based%2520on%2520a%2520transformer%2520model%2520that%2520directly%2520generates%2520molecular%2520structures%2520from%2520an%2520input%2520tandem%2520mass%2520spectrum%2520and%2520its%2520corresponding%2520molecular%2520formula%252C%2520thereby%2520eliminating%2520the%2520need%2520for%2520manual%2520annotations%2520and%2520intermediate%2520steps%252C%2520while%2520leveraging%2520transfer%2520learning%2520from%2520simulated%2520data.%2520To%2520further%2520address%2520the%2520challenge%2520of%2520out-of-distribution%2520spectra%252C%2520we%2520introduce%2520a%2520test-time%2520tuning%2520strategy%2520that%2520dynamically%2520adapts%2520the%2520pre-trained%2520model%2520to%2520novel%2520experimental%2520data.%2520Our%2520approach%2520achieves%2520a%2520Top-1%2520accuracy%2520of%25203.16%2525%2520on%2520the%2520MassSpecGym%2520benchmark%2520and%252012.88%2525%2520on%2520the%2520NPLIB1%2520datasets%252C%2520considerably%2520outperforming%2520conventional%2520fine-tuning.%2520Baseline%2520approaches%2520are%2520also%2520surpassed%2520by%252027%2525%2520and%252067%2525%2520respectively.%2520Even%2520when%2520the%2520exact%2520reference%2520structure%2520is%2520not%2520recovered%252C%2520the%2520generated%2520candidates%2520are%2520chemically%2520informative%252C%2520exhibiting%2520high%2520structural%2520plausibility%2520as%2520reflected%2520by%2520strong%2520Tanimoto%2520similarity%2520to%2520the%2520ground%2520truth.%2520Notably%252C%2520we%2520observe%2520a%2520relative%2520improvement%2520in%2520average%2520Tanimoto%2520similarity%2520of%252083%2525%2520on%2520NPLIB1%2520and%252064%2525%2520on%2520MassSpecGym%2520compared%2520to%2520state-of-the-art%2520methods.%2520Our%2520framework%2520combines%2520simplicity%2520with%2520adaptability%252C%2520generating%2520accurate%2520molecular%2520candidates%2520that%2520offer%2520valuable%2520guidance%2520for%2520expert%2520interpretation%2520of%2520unseen%2520spectra.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23746v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Tuned%20Language%20Models%20Enable%20End-to-end%20De%20Novo%20Molecular%20Structure%20Generation%20from%20MS/MS%20Spectra&entry.906535625=Laura%20Mismetti%20and%20Marvin%20Alberts%20and%20Andreas%20Krause%20and%20Mara%20Graziani&entry.1292438233=Tandem%20Mass%20Spectrometry%20is%20a%20cornerstone%20technique%20for%20identifying%20unknown%20small%20molecules%20in%20fields%20such%20as%20metabolomics%2C%20natural%20product%20discovery%20and%20environmental%20analysis.%20However%2C%20certain%20aspects%2C%20such%20as%20the%20probabilistic%20fragmentation%20process%20and%20size%20of%20the%20chemical%20space%2C%20make%20structure%20elucidation%20from%20such%20spectra%20highly%20challenging%2C%20particularly%20when%20there%20is%20a%20shift%20between%20the%20deployment%20and%20training%20conditions.%20Current%20methods%20rely%20on%20database%20matching%20of%20previously%20observed%20spectra%20of%20known%20molecules%20and%20multi-step%20pipelines%20that%20require%20intermediate%20fingerprint%20prediction%20or%20expensive%20fragment%20annotations.%20We%20introduce%20a%20novel%20end-to-end%20framework%20based%20on%20a%20transformer%20model%20that%20directly%20generates%20molecular%20structures%20from%20an%20input%20tandem%20mass%20spectrum%20and%20its%20corresponding%20molecular%20formula%2C%20thereby%20eliminating%20the%20need%20for%20manual%20annotations%20and%20intermediate%20steps%2C%20while%20leveraging%20transfer%20learning%20from%20simulated%20data.%20To%20further%20address%20the%20challenge%20of%20out-of-distribution%20spectra%2C%20we%20introduce%20a%20test-time%20tuning%20strategy%20that%20dynamically%20adapts%20the%20pre-trained%20model%20to%20novel%20experimental%20data.%20Our%20approach%20achieves%20a%20Top-1%20accuracy%20of%203.16%25%20on%20the%20MassSpecGym%20benchmark%20and%2012.88%25%20on%20the%20NPLIB1%20datasets%2C%20considerably%20outperforming%20conventional%20fine-tuning.%20Baseline%20approaches%20are%20also%20surpassed%20by%2027%25%20and%2067%25%20respectively.%20Even%20when%20the%20exact%20reference%20structure%20is%20not%20recovered%2C%20the%20generated%20candidates%20are%20chemically%20informative%2C%20exhibiting%20high%20structural%20plausibility%20as%20reflected%20by%20strong%20Tanimoto%20similarity%20to%20the%20ground%20truth.%20Notably%2C%20we%20observe%20a%20relative%20improvement%20in%20average%20Tanimoto%20similarity%20of%2083%25%20on%20NPLIB1%20and%2064%25%20on%20MassSpecGym%20compared%20to%20state-of-the-art%20methods.%20Our%20framework%20combines%20simplicity%20with%20adaptability%2C%20generating%20accurate%20molecular%20candidates%20that%20offer%20valuable%20guidance%20for%20expert%20interpretation%20of%20unseen%20spectra.&entry.1838667208=http%3A//arxiv.org/abs/2510.23746v2&entry.124074799=Read"},
{"title": "Near-Optimal Decentralized Stochastic Nonconvex Optimization with Heavy-Tailed Noise", "author": "Menglian Wang and Zhuanghua Liu and Luo Luo", "abstract": "This paper studies decentralized stochastic nonconvex optimization problem over row-stochastic networks. We consider the heavy-tailed gradient noise which is empirically observed in many popular real-world applications. Specifically, we propose a decentralized normalized stochastic gradient descent with Pull-Diag gradient tracking, which achieves approximate stationary points with the optimal sample complexity and the near-optimal communication complexity. We further follow our framework to study the setting of undirected networks, also achieving the nearly tight upper complexity bounds. Moreover, we conduct empirical studies to show the practical superiority of the proposed methods.", "link": "http://arxiv.org/abs/2601.11435v1", "date": "2026-01-16", "relevancy": 2.3442, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.515}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4511}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near-Optimal%20Decentralized%20Stochastic%20Nonconvex%20Optimization%20with%20Heavy-Tailed%20Noise&body=Title%3A%20Near-Optimal%20Decentralized%20Stochastic%20Nonconvex%20Optimization%20with%20Heavy-Tailed%20Noise%0AAuthor%3A%20Menglian%20Wang%20and%20Zhuanghua%20Liu%20and%20Luo%20Luo%0AAbstract%3A%20This%20paper%20studies%20decentralized%20stochastic%20nonconvex%20optimization%20problem%20over%20row-stochastic%20networks.%20We%20consider%20the%20heavy-tailed%20gradient%20noise%20which%20is%20empirically%20observed%20in%20many%20popular%20real-world%20applications.%20Specifically%2C%20we%20propose%20a%20decentralized%20normalized%20stochastic%20gradient%20descent%20with%20Pull-Diag%20gradient%20tracking%2C%20which%20achieves%20approximate%20stationary%20points%20with%20the%20optimal%20sample%20complexity%20and%20the%20near-optimal%20communication%20complexity.%20We%20further%20follow%20our%20framework%20to%20study%20the%20setting%20of%20undirected%20networks%2C%20also%20achieving%20the%20nearly%20tight%20upper%20complexity%20bounds.%20Moreover%2C%20we%20conduct%20empirical%20studies%20to%20show%20the%20practical%20superiority%20of%20the%20proposed%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear-Optimal%2520Decentralized%2520Stochastic%2520Nonconvex%2520Optimization%2520with%2520Heavy-Tailed%2520Noise%26entry.906535625%3DMenglian%2520Wang%2520and%2520Zhuanghua%2520Liu%2520and%2520Luo%2520Luo%26entry.1292438233%3DThis%2520paper%2520studies%2520decentralized%2520stochastic%2520nonconvex%2520optimization%2520problem%2520over%2520row-stochastic%2520networks.%2520We%2520consider%2520the%2520heavy-tailed%2520gradient%2520noise%2520which%2520is%2520empirically%2520observed%2520in%2520many%2520popular%2520real-world%2520applications.%2520Specifically%252C%2520we%2520propose%2520a%2520decentralized%2520normalized%2520stochastic%2520gradient%2520descent%2520with%2520Pull-Diag%2520gradient%2520tracking%252C%2520which%2520achieves%2520approximate%2520stationary%2520points%2520with%2520the%2520optimal%2520sample%2520complexity%2520and%2520the%2520near-optimal%2520communication%2520complexity.%2520We%2520further%2520follow%2520our%2520framework%2520to%2520study%2520the%2520setting%2520of%2520undirected%2520networks%252C%2520also%2520achieving%2520the%2520nearly%2520tight%2520upper%2520complexity%2520bounds.%2520Moreover%252C%2520we%2520conduct%2520empirical%2520studies%2520to%2520show%2520the%2520practical%2520superiority%2520of%2520the%2520proposed%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near-Optimal%20Decentralized%20Stochastic%20Nonconvex%20Optimization%20with%20Heavy-Tailed%20Noise&entry.906535625=Menglian%20Wang%20and%20Zhuanghua%20Liu%20and%20Luo%20Luo&entry.1292438233=This%20paper%20studies%20decentralized%20stochastic%20nonconvex%20optimization%20problem%20over%20row-stochastic%20networks.%20We%20consider%20the%20heavy-tailed%20gradient%20noise%20which%20is%20empirically%20observed%20in%20many%20popular%20real-world%20applications.%20Specifically%2C%20we%20propose%20a%20decentralized%20normalized%20stochastic%20gradient%20descent%20with%20Pull-Diag%20gradient%20tracking%2C%20which%20achieves%20approximate%20stationary%20points%20with%20the%20optimal%20sample%20complexity%20and%20the%20near-optimal%20communication%20complexity.%20We%20further%20follow%20our%20framework%20to%20study%20the%20setting%20of%20undirected%20networks%2C%20also%20achieving%20the%20nearly%20tight%20upper%20complexity%20bounds.%20Moreover%2C%20we%20conduct%20empirical%20studies%20to%20show%20the%20practical%20superiority%20of%20the%20proposed%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.11435v1&entry.124074799=Read"},
{"title": "Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities", "author": "Yiyun Zhou and Mingjing Xu and Jingwei Shi and Quanjiang Li and Jingyuan Chen", "abstract": "Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.", "link": "http://arxiv.org/abs/2511.11512v4", "date": "2026-01-16", "relevancy": 2.3412, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6352}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Representation%20Learning%20for%20Alignment%20of%20Tactile%2C%20Language%2C%20and%20Vision%20Modalities&body=Title%3A%20Collaborative%20Representation%20Learning%20for%20Alignment%20of%20Tactile%2C%20Language%2C%20and%20Vision%20Modalities%0AAuthor%3A%20Yiyun%20Zhou%20and%20Mingjing%20Xu%20and%20Jingwei%20Shi%20and%20Quanjiang%20Li%20and%20Jingyuan%20Chen%0AAbstract%3A%20Tactile%20sensing%20offers%20rich%20and%20complementary%20information%20to%20vision%20and%20language%2C%20enabling%20robots%20to%20perceive%20fine-grained%20object%20properties.%20However%2C%20existing%20tactile%20sensors%20lack%20standardization%2C%20leading%20to%20redundant%20features%20that%20hinder%20cross-sensor%20generalization.%20Moreover%2C%20existing%20methods%20fail%20to%20fully%20integrate%20the%20intermediate%20communication%20among%20tactile%2C%20language%2C%20and%20vision%20modalities.%20To%20address%20this%2C%20we%20propose%20TLV-CoRe%2C%20a%20CLIP-based%20Tactile-Language-Vision%20Collaborative%20Representation%20learning%20method.%20TLV-CoRe%20introduces%20a%20Sensor-Aware%20Modulator%20to%20unify%20tactile%20features%20across%20different%20sensors%20and%20employs%20tactile-irrelevant%20decoupled%20learning%20to%20disentangle%20irrelevant%20tactile%20features.%20Additionally%2C%20a%20Unified%20Bridging%20Adapter%20is%20introduced%20to%20enhance%20tri-modal%20interaction%20within%20the%20shared%20representation%20space.%20To%20fairly%20evaluate%20the%20effectiveness%20of%20tactile%20models%2C%20we%20further%20propose%20the%20RSS%20evaluation%20framework%2C%20focusing%20on%20Robustness%2C%20Synergy%2C%20and%20Stability%20across%20different%20methods.%20Experimental%20results%20demonstrate%20that%20TLV-CoRe%20significantly%20improves%20sensor-agnostic%20representation%20learning%20and%20cross-modal%20alignment%2C%20offering%20a%20new%20direction%20for%20multimodal%20tactile%20representation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11512v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Representation%2520Learning%2520for%2520Alignment%2520of%2520Tactile%252C%2520Language%252C%2520and%2520Vision%2520Modalities%26entry.906535625%3DYiyun%2520Zhou%2520and%2520Mingjing%2520Xu%2520and%2520Jingwei%2520Shi%2520and%2520Quanjiang%2520Li%2520and%2520Jingyuan%2520Chen%26entry.1292438233%3DTactile%2520sensing%2520offers%2520rich%2520and%2520complementary%2520information%2520to%2520vision%2520and%2520language%252C%2520enabling%2520robots%2520to%2520perceive%2520fine-grained%2520object%2520properties.%2520However%252C%2520existing%2520tactile%2520sensors%2520lack%2520standardization%252C%2520leading%2520to%2520redundant%2520features%2520that%2520hinder%2520cross-sensor%2520generalization.%2520Moreover%252C%2520existing%2520methods%2520fail%2520to%2520fully%2520integrate%2520the%2520intermediate%2520communication%2520among%2520tactile%252C%2520language%252C%2520and%2520vision%2520modalities.%2520To%2520address%2520this%252C%2520we%2520propose%2520TLV-CoRe%252C%2520a%2520CLIP-based%2520Tactile-Language-Vision%2520Collaborative%2520Representation%2520learning%2520method.%2520TLV-CoRe%2520introduces%2520a%2520Sensor-Aware%2520Modulator%2520to%2520unify%2520tactile%2520features%2520across%2520different%2520sensors%2520and%2520employs%2520tactile-irrelevant%2520decoupled%2520learning%2520to%2520disentangle%2520irrelevant%2520tactile%2520features.%2520Additionally%252C%2520a%2520Unified%2520Bridging%2520Adapter%2520is%2520introduced%2520to%2520enhance%2520tri-modal%2520interaction%2520within%2520the%2520shared%2520representation%2520space.%2520To%2520fairly%2520evaluate%2520the%2520effectiveness%2520of%2520tactile%2520models%252C%2520we%2520further%2520propose%2520the%2520RSS%2520evaluation%2520framework%252C%2520focusing%2520on%2520Robustness%252C%2520Synergy%252C%2520and%2520Stability%2520across%2520different%2520methods.%2520Experimental%2520results%2520demonstrate%2520that%2520TLV-CoRe%2520significantly%2520improves%2520sensor-agnostic%2520representation%2520learning%2520and%2520cross-modal%2520alignment%252C%2520offering%2520a%2520new%2520direction%2520for%2520multimodal%2520tactile%2520representation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11512v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Representation%20Learning%20for%20Alignment%20of%20Tactile%2C%20Language%2C%20and%20Vision%20Modalities&entry.906535625=Yiyun%20Zhou%20and%20Mingjing%20Xu%20and%20Jingwei%20Shi%20and%20Quanjiang%20Li%20and%20Jingyuan%20Chen&entry.1292438233=Tactile%20sensing%20offers%20rich%20and%20complementary%20information%20to%20vision%20and%20language%2C%20enabling%20robots%20to%20perceive%20fine-grained%20object%20properties.%20However%2C%20existing%20tactile%20sensors%20lack%20standardization%2C%20leading%20to%20redundant%20features%20that%20hinder%20cross-sensor%20generalization.%20Moreover%2C%20existing%20methods%20fail%20to%20fully%20integrate%20the%20intermediate%20communication%20among%20tactile%2C%20language%2C%20and%20vision%20modalities.%20To%20address%20this%2C%20we%20propose%20TLV-CoRe%2C%20a%20CLIP-based%20Tactile-Language-Vision%20Collaborative%20Representation%20learning%20method.%20TLV-CoRe%20introduces%20a%20Sensor-Aware%20Modulator%20to%20unify%20tactile%20features%20across%20different%20sensors%20and%20employs%20tactile-irrelevant%20decoupled%20learning%20to%20disentangle%20irrelevant%20tactile%20features.%20Additionally%2C%20a%20Unified%20Bridging%20Adapter%20is%20introduced%20to%20enhance%20tri-modal%20interaction%20within%20the%20shared%20representation%20space.%20To%20fairly%20evaluate%20the%20effectiveness%20of%20tactile%20models%2C%20we%20further%20propose%20the%20RSS%20evaluation%20framework%2C%20focusing%20on%20Robustness%2C%20Synergy%2C%20and%20Stability%20across%20different%20methods.%20Experimental%20results%20demonstrate%20that%20TLV-CoRe%20significantly%20improves%20sensor-agnostic%20representation%20learning%20and%20cross-modal%20alignment%2C%20offering%20a%20new%20direction%20for%20multimodal%20tactile%20representation.&entry.1838667208=http%3A//arxiv.org/abs/2511.11512v4&entry.124074799=Read"},
{"title": "Tug-of-war between idioms' figurative and literal interpretations in LLMs", "author": "Soyoung Oh and Xinting Huang and Mathis Pink and Michael Hahn and Vera Demberg", "abstract": "Idioms present a unique challenge for language models due to their non-compositional figurative interpretations, which often strongly diverge from the idiom's literal interpretation. In this paper, we employ causal tracing to systematically analyze how pretrained causal transformers deal with this ambiguity. We localize three mechanisms: (i) Early sublayers and specific attention heads retrieve an idiom's figurative interpretation, while suppressing its literal interpretation. (ii) When disambiguating context precedes the idiom, the model leverages it from the earliest layer and later layers refine the interpretation if the context conflicts with the retrieved interpretation. (iii) Then, selective, competing pathways carry both interpretations: an intermediate pathway prioritizes the figurative interpretation and a parallel direct route favors the literal interpretation, ensuring that both readings remain available. Our findings provide mechanistic evidence for idiom comprehension in autoregressive transformers.", "link": "http://arxiv.org/abs/2506.01723v5", "date": "2026-01-16", "relevancy": 2.3347, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4796}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tug-of-war%20between%20idioms%27%20figurative%20and%20literal%20interpretations%20in%20LLMs&body=Title%3A%20Tug-of-war%20between%20idioms%27%20figurative%20and%20literal%20interpretations%20in%20LLMs%0AAuthor%3A%20Soyoung%20Oh%20and%20Xinting%20Huang%20and%20Mathis%20Pink%20and%20Michael%20Hahn%20and%20Vera%20Demberg%0AAbstract%3A%20Idioms%20present%20a%20unique%20challenge%20for%20language%20models%20due%20to%20their%20non-compositional%20figurative%20interpretations%2C%20which%20often%20strongly%20diverge%20from%20the%20idiom%27s%20literal%20interpretation.%20In%20this%20paper%2C%20we%20employ%20causal%20tracing%20to%20systematically%20analyze%20how%20pretrained%20causal%20transformers%20deal%20with%20this%20ambiguity.%20We%20localize%20three%20mechanisms%3A%20%28i%29%20Early%20sublayers%20and%20specific%20attention%20heads%20retrieve%20an%20idiom%27s%20figurative%20interpretation%2C%20while%20suppressing%20its%20literal%20interpretation.%20%28ii%29%20When%20disambiguating%20context%20precedes%20the%20idiom%2C%20the%20model%20leverages%20it%20from%20the%20earliest%20layer%20and%20later%20layers%20refine%20the%20interpretation%20if%20the%20context%20conflicts%20with%20the%20retrieved%20interpretation.%20%28iii%29%20Then%2C%20selective%2C%20competing%20pathways%20carry%20both%20interpretations%3A%20an%20intermediate%20pathway%20prioritizes%20the%20figurative%20interpretation%20and%20a%20parallel%20direct%20route%20favors%20the%20literal%20interpretation%2C%20ensuring%20that%20both%20readings%20remain%20available.%20Our%20findings%20provide%20mechanistic%20evidence%20for%20idiom%20comprehension%20in%20autoregressive%20transformers.%0ALink%3A%20http%3A//arxiv.org/abs/2506.01723v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTug-of-war%2520between%2520idioms%2527%2520figurative%2520and%2520literal%2520interpretations%2520in%2520LLMs%26entry.906535625%3DSoyoung%2520Oh%2520and%2520Xinting%2520Huang%2520and%2520Mathis%2520Pink%2520and%2520Michael%2520Hahn%2520and%2520Vera%2520Demberg%26entry.1292438233%3DIdioms%2520present%2520a%2520unique%2520challenge%2520for%2520language%2520models%2520due%2520to%2520their%2520non-compositional%2520figurative%2520interpretations%252C%2520which%2520often%2520strongly%2520diverge%2520from%2520the%2520idiom%2527s%2520literal%2520interpretation.%2520In%2520this%2520paper%252C%2520we%2520employ%2520causal%2520tracing%2520to%2520systematically%2520analyze%2520how%2520pretrained%2520causal%2520transformers%2520deal%2520with%2520this%2520ambiguity.%2520We%2520localize%2520three%2520mechanisms%253A%2520%2528i%2529%2520Early%2520sublayers%2520and%2520specific%2520attention%2520heads%2520retrieve%2520an%2520idiom%2527s%2520figurative%2520interpretation%252C%2520while%2520suppressing%2520its%2520literal%2520interpretation.%2520%2528ii%2529%2520When%2520disambiguating%2520context%2520precedes%2520the%2520idiom%252C%2520the%2520model%2520leverages%2520it%2520from%2520the%2520earliest%2520layer%2520and%2520later%2520layers%2520refine%2520the%2520interpretation%2520if%2520the%2520context%2520conflicts%2520with%2520the%2520retrieved%2520interpretation.%2520%2528iii%2529%2520Then%252C%2520selective%252C%2520competing%2520pathways%2520carry%2520both%2520interpretations%253A%2520an%2520intermediate%2520pathway%2520prioritizes%2520the%2520figurative%2520interpretation%2520and%2520a%2520parallel%2520direct%2520route%2520favors%2520the%2520literal%2520interpretation%252C%2520ensuring%2520that%2520both%2520readings%2520remain%2520available.%2520Our%2520findings%2520provide%2520mechanistic%2520evidence%2520for%2520idiom%2520comprehension%2520in%2520autoregressive%2520transformers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01723v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tug-of-war%20between%20idioms%27%20figurative%20and%20literal%20interpretations%20in%20LLMs&entry.906535625=Soyoung%20Oh%20and%20Xinting%20Huang%20and%20Mathis%20Pink%20and%20Michael%20Hahn%20and%20Vera%20Demberg&entry.1292438233=Idioms%20present%20a%20unique%20challenge%20for%20language%20models%20due%20to%20their%20non-compositional%20figurative%20interpretations%2C%20which%20often%20strongly%20diverge%20from%20the%20idiom%27s%20literal%20interpretation.%20In%20this%20paper%2C%20we%20employ%20causal%20tracing%20to%20systematically%20analyze%20how%20pretrained%20causal%20transformers%20deal%20with%20this%20ambiguity.%20We%20localize%20three%20mechanisms%3A%20%28i%29%20Early%20sublayers%20and%20specific%20attention%20heads%20retrieve%20an%20idiom%27s%20figurative%20interpretation%2C%20while%20suppressing%20its%20literal%20interpretation.%20%28ii%29%20When%20disambiguating%20context%20precedes%20the%20idiom%2C%20the%20model%20leverages%20it%20from%20the%20earliest%20layer%20and%20later%20layers%20refine%20the%20interpretation%20if%20the%20context%20conflicts%20with%20the%20retrieved%20interpretation.%20%28iii%29%20Then%2C%20selective%2C%20competing%20pathways%20carry%20both%20interpretations%3A%20an%20intermediate%20pathway%20prioritizes%20the%20figurative%20interpretation%20and%20a%20parallel%20direct%20route%20favors%20the%20literal%20interpretation%2C%20ensuring%20that%20both%20readings%20remain%20available.%20Our%20findings%20provide%20mechanistic%20evidence%20for%20idiom%20comprehension%20in%20autoregressive%20transformers.&entry.1838667208=http%3A//arxiv.org/abs/2506.01723v5&entry.124074799=Read"},
{"title": "Scalable Music Cover Retrieval Using Lyrics-Aligned Audio Embeddings", "author": "Joanne Affolter and Benjamin Martin and Elena V. Epure and Gabriel Meseguer-Brocal and Fr\u00e9d\u00e9ric Kaplan", "abstract": "Music Cover Retrieval, also known as Version Identification, aims to recognize distinct renditions of the same underlying musical work, a task central to catalog management, copyright enforcement, and music retrieval. State-of-the-art approaches have largely focused on harmonic and melodic features, employing increasingly complex audio pipelines designed to be invariant to musical attributes that often vary widely across covers. While effective, these methods demand substantial training time and computational resources. By contrast, lyrics constitute a strong invariant across covers, though their use has been limited by the difficulty of extracting them accurately and efficiently from polyphonic audio. Early methods relied on simple frameworks that limited downstream performance, while more recent systems deliver stronger results but require large models integrated within complex multimodal architectures. We introduce LIVI (Lyrics-Informed Version Identification), an approach that seeks to balance retrieval accuracy with computational efficiency. First, LIVI leverages supervision from state-of-the-art transcription and text embedding models during training to achieve retrieval accuracy on par with--or superior to--harmonic-based systems. Second, LIVI remains lightweight and efficient by removing the transcription step at inference, challenging the dominance of complexity-heavy pipelines.", "link": "http://arxiv.org/abs/2601.11262v1", "date": "2026-01-16", "relevancy": 2.3307, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Music%20Cover%20Retrieval%20Using%20Lyrics-Aligned%20Audio%20Embeddings&body=Title%3A%20Scalable%20Music%20Cover%20Retrieval%20Using%20Lyrics-Aligned%20Audio%20Embeddings%0AAuthor%3A%20Joanne%20Affolter%20and%20Benjamin%20Martin%20and%20Elena%20V.%20Epure%20and%20Gabriel%20Meseguer-Brocal%20and%20Fr%C3%A9d%C3%A9ric%20Kaplan%0AAbstract%3A%20Music%20Cover%20Retrieval%2C%20also%20known%20as%20Version%20Identification%2C%20aims%20to%20recognize%20distinct%20renditions%20of%20the%20same%20underlying%20musical%20work%2C%20a%20task%20central%20to%20catalog%20management%2C%20copyright%20enforcement%2C%20and%20music%20retrieval.%20State-of-the-art%20approaches%20have%20largely%20focused%20on%20harmonic%20and%20melodic%20features%2C%20employing%20increasingly%20complex%20audio%20pipelines%20designed%20to%20be%20invariant%20to%20musical%20attributes%20that%20often%20vary%20widely%20across%20covers.%20While%20effective%2C%20these%20methods%20demand%20substantial%20training%20time%20and%20computational%20resources.%20By%20contrast%2C%20lyrics%20constitute%20a%20strong%20invariant%20across%20covers%2C%20though%20their%20use%20has%20been%20limited%20by%20the%20difficulty%20of%20extracting%20them%20accurately%20and%20efficiently%20from%20polyphonic%20audio.%20Early%20methods%20relied%20on%20simple%20frameworks%20that%20limited%20downstream%20performance%2C%20while%20more%20recent%20systems%20deliver%20stronger%20results%20but%20require%20large%20models%20integrated%20within%20complex%20multimodal%20architectures.%20We%20introduce%20LIVI%20%28Lyrics-Informed%20Version%20Identification%29%2C%20an%20approach%20that%20seeks%20to%20balance%20retrieval%20accuracy%20with%20computational%20efficiency.%20First%2C%20LIVI%20leverages%20supervision%20from%20state-of-the-art%20transcription%20and%20text%20embedding%20models%20during%20training%20to%20achieve%20retrieval%20accuracy%20on%20par%20with--or%20superior%20to--harmonic-based%20systems.%20Second%2C%20LIVI%20remains%20lightweight%20and%20efficient%20by%20removing%20the%20transcription%20step%20at%20inference%2C%20challenging%20the%20dominance%20of%20complexity-heavy%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Music%2520Cover%2520Retrieval%2520Using%2520Lyrics-Aligned%2520Audio%2520Embeddings%26entry.906535625%3DJoanne%2520Affolter%2520and%2520Benjamin%2520Martin%2520and%2520Elena%2520V.%2520Epure%2520and%2520Gabriel%2520Meseguer-Brocal%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Kaplan%26entry.1292438233%3DMusic%2520Cover%2520Retrieval%252C%2520also%2520known%2520as%2520Version%2520Identification%252C%2520aims%2520to%2520recognize%2520distinct%2520renditions%2520of%2520the%2520same%2520underlying%2520musical%2520work%252C%2520a%2520task%2520central%2520to%2520catalog%2520management%252C%2520copyright%2520enforcement%252C%2520and%2520music%2520retrieval.%2520State-of-the-art%2520approaches%2520have%2520largely%2520focused%2520on%2520harmonic%2520and%2520melodic%2520features%252C%2520employing%2520increasingly%2520complex%2520audio%2520pipelines%2520designed%2520to%2520be%2520invariant%2520to%2520musical%2520attributes%2520that%2520often%2520vary%2520widely%2520across%2520covers.%2520While%2520effective%252C%2520these%2520methods%2520demand%2520substantial%2520training%2520time%2520and%2520computational%2520resources.%2520By%2520contrast%252C%2520lyrics%2520constitute%2520a%2520strong%2520invariant%2520across%2520covers%252C%2520though%2520their%2520use%2520has%2520been%2520limited%2520by%2520the%2520difficulty%2520of%2520extracting%2520them%2520accurately%2520and%2520efficiently%2520from%2520polyphonic%2520audio.%2520Early%2520methods%2520relied%2520on%2520simple%2520frameworks%2520that%2520limited%2520downstream%2520performance%252C%2520while%2520more%2520recent%2520systems%2520deliver%2520stronger%2520results%2520but%2520require%2520large%2520models%2520integrated%2520within%2520complex%2520multimodal%2520architectures.%2520We%2520introduce%2520LIVI%2520%2528Lyrics-Informed%2520Version%2520Identification%2529%252C%2520an%2520approach%2520that%2520seeks%2520to%2520balance%2520retrieval%2520accuracy%2520with%2520computational%2520efficiency.%2520First%252C%2520LIVI%2520leverages%2520supervision%2520from%2520state-of-the-art%2520transcription%2520and%2520text%2520embedding%2520models%2520during%2520training%2520to%2520achieve%2520retrieval%2520accuracy%2520on%2520par%2520with--or%2520superior%2520to--harmonic-based%2520systems.%2520Second%252C%2520LIVI%2520remains%2520lightweight%2520and%2520efficient%2520by%2520removing%2520the%2520transcription%2520step%2520at%2520inference%252C%2520challenging%2520the%2520dominance%2520of%2520complexity-heavy%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Music%20Cover%20Retrieval%20Using%20Lyrics-Aligned%20Audio%20Embeddings&entry.906535625=Joanne%20Affolter%20and%20Benjamin%20Martin%20and%20Elena%20V.%20Epure%20and%20Gabriel%20Meseguer-Brocal%20and%20Fr%C3%A9d%C3%A9ric%20Kaplan&entry.1292438233=Music%20Cover%20Retrieval%2C%20also%20known%20as%20Version%20Identification%2C%20aims%20to%20recognize%20distinct%20renditions%20of%20the%20same%20underlying%20musical%20work%2C%20a%20task%20central%20to%20catalog%20management%2C%20copyright%20enforcement%2C%20and%20music%20retrieval.%20State-of-the-art%20approaches%20have%20largely%20focused%20on%20harmonic%20and%20melodic%20features%2C%20employing%20increasingly%20complex%20audio%20pipelines%20designed%20to%20be%20invariant%20to%20musical%20attributes%20that%20often%20vary%20widely%20across%20covers.%20While%20effective%2C%20these%20methods%20demand%20substantial%20training%20time%20and%20computational%20resources.%20By%20contrast%2C%20lyrics%20constitute%20a%20strong%20invariant%20across%20covers%2C%20though%20their%20use%20has%20been%20limited%20by%20the%20difficulty%20of%20extracting%20them%20accurately%20and%20efficiently%20from%20polyphonic%20audio.%20Early%20methods%20relied%20on%20simple%20frameworks%20that%20limited%20downstream%20performance%2C%20while%20more%20recent%20systems%20deliver%20stronger%20results%20but%20require%20large%20models%20integrated%20within%20complex%20multimodal%20architectures.%20We%20introduce%20LIVI%20%28Lyrics-Informed%20Version%20Identification%29%2C%20an%20approach%20that%20seeks%20to%20balance%20retrieval%20accuracy%20with%20computational%20efficiency.%20First%2C%20LIVI%20leverages%20supervision%20from%20state-of-the-art%20transcription%20and%20text%20embedding%20models%20during%20training%20to%20achieve%20retrieval%20accuracy%20on%20par%20with--or%20superior%20to--harmonic-based%20systems.%20Second%2C%20LIVI%20remains%20lightweight%20and%20efficient%20by%20removing%20the%20transcription%20step%20at%20inference%2C%20challenging%20the%20dominance%20of%20complexity-heavy%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2601.11262v1&entry.124074799=Read"},
{"title": "A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints", "author": "Youssef Tawfilis and Hossam Amer and Minar El-Aasser and Tallal Elshabrawy", "abstract": "Federated Learning has gained attention for its ability to enable multiple nodes to collaboratively train machine learning models without sharing raw data. At the same time, Generative AI -- particularly Generative Adversarial Networks (GANs) -- have achieved remarkable success across a wide range of domains, such as healthcare, security, and Image Generation. However, training generative models typically requires large datasets and significant computational resources, which are often unavailable in real-world settings. Acquiring such resources can be costly and inefficient, especially when many underutilized devices -- such as IoT devices and edge devices -- with varying capabilities remain idle. Moreover, obtaining large datasets is challenging due to privacy concerns and copyright restrictions, as most devices are unwilling to share their data. To address these challenges, we propose a novel approach for decentralized GAN training that enables utilizing distributed data and underutilized, low-capability devices while not sharing data in its raw form. Our approach is designed to tackle key challenges in decentralized environments, combining KLD-weighted Clustered Federated Learning to address the issues of data heterogeneity and multi-domain datasets, with Heterogeneous U-Shaped split learning to tackle the challenge of device heterogeneity under strict data sharing constraints -- ensuring that no labels or raw data, whether real or synthetic, are ever shared between nodes. Experiments show that our approach demonstrates significant improvements across key metrics, where it achieves an average 10% boost in classification metrics (up to 60% in multi-domain non-IID settings), 1.1x -- 3x higher image generation scores for the MNIST family datasets, and 2x -- 70x lower FID scores for higher resolution datasets. Find our code at https://distributed-gen-ai.github.io/huscf-gan.github.io/.", "link": "http://arxiv.org/abs/2507.12979v3", "date": "2026-01-16", "relevancy": 2.3132, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5824}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5803}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Distributed%20Generative%20AI%20Approach%20for%20Heterogeneous%20Multi-Domain%20Environments%20under%20Data%20Sharing%20constraints&body=Title%3A%20A%20Distributed%20Generative%20AI%20Approach%20for%20Heterogeneous%20Multi-Domain%20Environments%20under%20Data%20Sharing%20constraints%0AAuthor%3A%20Youssef%20Tawfilis%20and%20Hossam%20Amer%20and%20Minar%20El-Aasser%20and%20Tallal%20Elshabrawy%0AAbstract%3A%20Federated%20Learning%20has%20gained%20attention%20for%20its%20ability%20to%20enable%20multiple%20nodes%20to%20collaboratively%20train%20machine%20learning%20models%20without%20sharing%20raw%20data.%20At%20the%20same%20time%2C%20Generative%20AI%20--%20particularly%20Generative%20Adversarial%20Networks%20%28GANs%29%20--%20have%20achieved%20remarkable%20success%20across%20a%20wide%20range%20of%20domains%2C%20such%20as%20healthcare%2C%20security%2C%20and%20Image%20Generation.%20However%2C%20training%20generative%20models%20typically%20requires%20large%20datasets%20and%20significant%20computational%20resources%2C%20which%20are%20often%20unavailable%20in%20real-world%20settings.%20Acquiring%20such%20resources%20can%20be%20costly%20and%20inefficient%2C%20especially%20when%20many%20underutilized%20devices%20--%20such%20as%20IoT%20devices%20and%20edge%20devices%20--%20with%20varying%20capabilities%20remain%20idle.%20Moreover%2C%20obtaining%20large%20datasets%20is%20challenging%20due%20to%20privacy%20concerns%20and%20copyright%20restrictions%2C%20as%20most%20devices%20are%20unwilling%20to%20share%20their%20data.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20for%20decentralized%20GAN%20training%20that%20enables%20utilizing%20distributed%20data%20and%20underutilized%2C%20low-capability%20devices%20while%20not%20sharing%20data%20in%20its%20raw%20form.%20Our%20approach%20is%20designed%20to%20tackle%20key%20challenges%20in%20decentralized%20environments%2C%20combining%20KLD-weighted%20Clustered%20Federated%20Learning%20to%20address%20the%20issues%20of%20data%20heterogeneity%20and%20multi-domain%20datasets%2C%20with%20Heterogeneous%20U-Shaped%20split%20learning%20to%20tackle%20the%20challenge%20of%20device%20heterogeneity%20under%20strict%20data%20sharing%20constraints%20--%20ensuring%20that%20no%20labels%20or%20raw%20data%2C%20whether%20real%20or%20synthetic%2C%20are%20ever%20shared%20between%20nodes.%20Experiments%20show%20that%20our%20approach%20demonstrates%20significant%20improvements%20across%20key%20metrics%2C%20where%20it%20achieves%20an%20average%2010%25%20boost%20in%20classification%20metrics%20%28up%20to%2060%25%20in%20multi-domain%20non-IID%20settings%29%2C%201.1x%20--%203x%20higher%20image%20generation%20scores%20for%20the%20MNIST%20family%20datasets%2C%20and%202x%20--%2070x%20lower%20FID%20scores%20for%20higher%20resolution%20datasets.%20Find%20our%20code%20at%20https%3A//distributed-gen-ai.github.io/huscf-gan.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2507.12979v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Distributed%2520Generative%2520AI%2520Approach%2520for%2520Heterogeneous%2520Multi-Domain%2520Environments%2520under%2520Data%2520Sharing%2520constraints%26entry.906535625%3DYoussef%2520Tawfilis%2520and%2520Hossam%2520Amer%2520and%2520Minar%2520El-Aasser%2520and%2520Tallal%2520Elshabrawy%26entry.1292438233%3DFederated%2520Learning%2520has%2520gained%2520attention%2520for%2520its%2520ability%2520to%2520enable%2520multiple%2520nodes%2520to%2520collaboratively%2520train%2520machine%2520learning%2520models%2520without%2520sharing%2520raw%2520data.%2520At%2520the%2520same%2520time%252C%2520Generative%2520AI%2520--%2520particularly%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520--%2520have%2520achieved%2520remarkable%2520success%2520across%2520a%2520wide%2520range%2520of%2520domains%252C%2520such%2520as%2520healthcare%252C%2520security%252C%2520and%2520Image%2520Generation.%2520However%252C%2520training%2520generative%2520models%2520typically%2520requires%2520large%2520datasets%2520and%2520significant%2520computational%2520resources%252C%2520which%2520are%2520often%2520unavailable%2520in%2520real-world%2520settings.%2520Acquiring%2520such%2520resources%2520can%2520be%2520costly%2520and%2520inefficient%252C%2520especially%2520when%2520many%2520underutilized%2520devices%2520--%2520such%2520as%2520IoT%2520devices%2520and%2520edge%2520devices%2520--%2520with%2520varying%2520capabilities%2520remain%2520idle.%2520Moreover%252C%2520obtaining%2520large%2520datasets%2520is%2520challenging%2520due%2520to%2520privacy%2520concerns%2520and%2520copyright%2520restrictions%252C%2520as%2520most%2520devices%2520are%2520unwilling%2520to%2520share%2520their%2520data.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520decentralized%2520GAN%2520training%2520that%2520enables%2520utilizing%2520distributed%2520data%2520and%2520underutilized%252C%2520low-capability%2520devices%2520while%2520not%2520sharing%2520data%2520in%2520its%2520raw%2520form.%2520Our%2520approach%2520is%2520designed%2520to%2520tackle%2520key%2520challenges%2520in%2520decentralized%2520environments%252C%2520combining%2520KLD-weighted%2520Clustered%2520Federated%2520Learning%2520to%2520address%2520the%2520issues%2520of%2520data%2520heterogeneity%2520and%2520multi-domain%2520datasets%252C%2520with%2520Heterogeneous%2520U-Shaped%2520split%2520learning%2520to%2520tackle%2520the%2520challenge%2520of%2520device%2520heterogeneity%2520under%2520strict%2520data%2520sharing%2520constraints%2520--%2520ensuring%2520that%2520no%2520labels%2520or%2520raw%2520data%252C%2520whether%2520real%2520or%2520synthetic%252C%2520are%2520ever%2520shared%2520between%2520nodes.%2520Experiments%2520show%2520that%2520our%2520approach%2520demonstrates%2520significant%2520improvements%2520across%2520key%2520metrics%252C%2520where%2520it%2520achieves%2520an%2520average%252010%2525%2520boost%2520in%2520classification%2520metrics%2520%2528up%2520to%252060%2525%2520in%2520multi-domain%2520non-IID%2520settings%2529%252C%25201.1x%2520--%25203x%2520higher%2520image%2520generation%2520scores%2520for%2520the%2520MNIST%2520family%2520datasets%252C%2520and%25202x%2520--%252070x%2520lower%2520FID%2520scores%2520for%2520higher%2520resolution%2520datasets.%2520Find%2520our%2520code%2520at%2520https%253A//distributed-gen-ai.github.io/huscf-gan.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12979v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Distributed%20Generative%20AI%20Approach%20for%20Heterogeneous%20Multi-Domain%20Environments%20under%20Data%20Sharing%20constraints&entry.906535625=Youssef%20Tawfilis%20and%20Hossam%20Amer%20and%20Minar%20El-Aasser%20and%20Tallal%20Elshabrawy&entry.1292438233=Federated%20Learning%20has%20gained%20attention%20for%20its%20ability%20to%20enable%20multiple%20nodes%20to%20collaboratively%20train%20machine%20learning%20models%20without%20sharing%20raw%20data.%20At%20the%20same%20time%2C%20Generative%20AI%20--%20particularly%20Generative%20Adversarial%20Networks%20%28GANs%29%20--%20have%20achieved%20remarkable%20success%20across%20a%20wide%20range%20of%20domains%2C%20such%20as%20healthcare%2C%20security%2C%20and%20Image%20Generation.%20However%2C%20training%20generative%20models%20typically%20requires%20large%20datasets%20and%20significant%20computational%20resources%2C%20which%20are%20often%20unavailable%20in%20real-world%20settings.%20Acquiring%20such%20resources%20can%20be%20costly%20and%20inefficient%2C%20especially%20when%20many%20underutilized%20devices%20--%20such%20as%20IoT%20devices%20and%20edge%20devices%20--%20with%20varying%20capabilities%20remain%20idle.%20Moreover%2C%20obtaining%20large%20datasets%20is%20challenging%20due%20to%20privacy%20concerns%20and%20copyright%20restrictions%2C%20as%20most%20devices%20are%20unwilling%20to%20share%20their%20data.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20for%20decentralized%20GAN%20training%20that%20enables%20utilizing%20distributed%20data%20and%20underutilized%2C%20low-capability%20devices%20while%20not%20sharing%20data%20in%20its%20raw%20form.%20Our%20approach%20is%20designed%20to%20tackle%20key%20challenges%20in%20decentralized%20environments%2C%20combining%20KLD-weighted%20Clustered%20Federated%20Learning%20to%20address%20the%20issues%20of%20data%20heterogeneity%20and%20multi-domain%20datasets%2C%20with%20Heterogeneous%20U-Shaped%20split%20learning%20to%20tackle%20the%20challenge%20of%20device%20heterogeneity%20under%20strict%20data%20sharing%20constraints%20--%20ensuring%20that%20no%20labels%20or%20raw%20data%2C%20whether%20real%20or%20synthetic%2C%20are%20ever%20shared%20between%20nodes.%20Experiments%20show%20that%20our%20approach%20demonstrates%20significant%20improvements%20across%20key%20metrics%2C%20where%20it%20achieves%20an%20average%2010%25%20boost%20in%20classification%20metrics%20%28up%20to%2060%25%20in%20multi-domain%20non-IID%20settings%29%2C%201.1x%20--%203x%20higher%20image%20generation%20scores%20for%20the%20MNIST%20family%20datasets%2C%20and%202x%20--%2070x%20lower%20FID%20scores%20for%20higher%20resolution%20datasets.%20Find%20our%20code%20at%20https%3A//distributed-gen-ai.github.io/huscf-gan.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2507.12979v3&entry.124074799=Read"},
{"title": "High-Dimensional Tail Index Regression", "author": "Yuya Sasaki and Jing Tao and Yulong Wang", "abstract": "Motivated by the empirical observation of power-law distributions in the credits (e.g., ``likes'') of viral posts in social media, we introduce a high-dimensional tail index regression model and propose methods for estimation and inference of its parameters. First, we propose a regularized estimator, establish its consistency, and derive its convergence rate. Second, we debias the regularized estimator to facilitate inference and prove its asymptotic normality. Simulation studies corroborate our theoretical findings. We apply these methods to the text analysis of viral posts on X (formerly Twitter).", "link": "http://arxiv.org/abs/2403.01318v3", "date": "2026-01-16", "relevancy": 2.2926, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4683}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4654}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Dimensional%20Tail%20Index%20Regression&body=Title%3A%20High-Dimensional%20Tail%20Index%20Regression%0AAuthor%3A%20Yuya%20Sasaki%20and%20Jing%20Tao%20and%20Yulong%20Wang%0AAbstract%3A%20Motivated%20by%20the%20empirical%20observation%20of%20power-law%20distributions%20in%20the%20credits%20%28e.g.%2C%20%60%60likes%27%27%29%20of%20viral%20posts%20in%20social%20media%2C%20we%20introduce%20a%20high-dimensional%20tail%20index%20regression%20model%20and%20propose%20methods%20for%20estimation%20and%20inference%20of%20its%20parameters.%20First%2C%20we%20propose%20a%20regularized%20estimator%2C%20establish%20its%20consistency%2C%20and%20derive%20its%20convergence%20rate.%20Second%2C%20we%20debias%20the%20regularized%20estimator%20to%20facilitate%20inference%20and%20prove%20its%20asymptotic%20normality.%20Simulation%20studies%20corroborate%20our%20theoretical%20findings.%20We%20apply%20these%20methods%20to%20the%20text%20analysis%20of%20viral%20posts%20on%20X%20%28formerly%20Twitter%29.%0ALink%3A%20http%3A//arxiv.org/abs/2403.01318v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Dimensional%2520Tail%2520Index%2520Regression%26entry.906535625%3DYuya%2520Sasaki%2520and%2520Jing%2520Tao%2520and%2520Yulong%2520Wang%26entry.1292438233%3DMotivated%2520by%2520the%2520empirical%2520observation%2520of%2520power-law%2520distributions%2520in%2520the%2520credits%2520%2528e.g.%252C%2520%2560%2560likes%2527%2527%2529%2520of%2520viral%2520posts%2520in%2520social%2520media%252C%2520we%2520introduce%2520a%2520high-dimensional%2520tail%2520index%2520regression%2520model%2520and%2520propose%2520methods%2520for%2520estimation%2520and%2520inference%2520of%2520its%2520parameters.%2520First%252C%2520we%2520propose%2520a%2520regularized%2520estimator%252C%2520establish%2520its%2520consistency%252C%2520and%2520derive%2520its%2520convergence%2520rate.%2520Second%252C%2520we%2520debias%2520the%2520regularized%2520estimator%2520to%2520facilitate%2520inference%2520and%2520prove%2520its%2520asymptotic%2520normality.%2520Simulation%2520studies%2520corroborate%2520our%2520theoretical%2520findings.%2520We%2520apply%2520these%2520methods%2520to%2520the%2520text%2520analysis%2520of%2520viral%2520posts%2520on%2520X%2520%2528formerly%2520Twitter%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01318v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Dimensional%20Tail%20Index%20Regression&entry.906535625=Yuya%20Sasaki%20and%20Jing%20Tao%20and%20Yulong%20Wang&entry.1292438233=Motivated%20by%20the%20empirical%20observation%20of%20power-law%20distributions%20in%20the%20credits%20%28e.g.%2C%20%60%60likes%27%27%29%20of%20viral%20posts%20in%20social%20media%2C%20we%20introduce%20a%20high-dimensional%20tail%20index%20regression%20model%20and%20propose%20methods%20for%20estimation%20and%20inference%20of%20its%20parameters.%20First%2C%20we%20propose%20a%20regularized%20estimator%2C%20establish%20its%20consistency%2C%20and%20derive%20its%20convergence%20rate.%20Second%2C%20we%20debias%20the%20regularized%20estimator%20to%20facilitate%20inference%20and%20prove%20its%20asymptotic%20normality.%20Simulation%20studies%20corroborate%20our%20theoretical%20findings.%20We%20apply%20these%20methods%20to%20the%20text%20analysis%20of%20viral%20posts%20on%20X%20%28formerly%20Twitter%29.&entry.1838667208=http%3A//arxiv.org/abs/2403.01318v3&entry.124074799=Read"},
{"title": "ChartComplete: A Taxonomy-based Inclusive Chart Dataset", "author": "Ahmad Mustapha and Charbel Toumieh and Mariette Awad", "abstract": "With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.", "link": "http://arxiv.org/abs/2601.10462v2", "date": "2026-01-16", "relevancy": 2.2895, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4838}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChartComplete%3A%20A%20Taxonomy-based%20Inclusive%20Chart%20Dataset&body=Title%3A%20ChartComplete%3A%20A%20Taxonomy-based%20Inclusive%20Chart%20Dataset%0AAuthor%3A%20Ahmad%20Mustapha%20and%20Charbel%20Toumieh%20and%20Mariette%20Awad%0AAbstract%3A%20With%20advancements%20in%20deep%20learning%20%28DL%29%20and%20computer%20vision%20techniques%2C%20the%20field%20of%20chart%20understanding%20is%20evolving%20rapidly.%20In%20particular%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20are%20proving%20to%20be%20efficient%20and%20accurate%20in%20understanding%20charts.%20To%20accurately%20measure%20the%20performance%20of%20MLLMs%2C%20the%20research%20community%20has%20developed%20multiple%20datasets%20to%20serve%20as%20benchmarks.%20By%20examining%20these%20datasets%2C%20we%20found%20that%20they%20are%20all%20limited%20to%20a%20small%20set%20of%20chart%20types.%20To%20bridge%20this%20gap%2C%20we%20propose%20the%20ChartComplete%20dataset.%20The%20dataset%20is%20based%20on%20a%20chart%20taxonomy%20borrowed%20from%20the%20visualization%20community%2C%20and%20it%20covers%20thirty%20different%20chart%20types.%20The%20dataset%20is%20a%20collection%20of%20classified%20chart%20images%20and%20does%20not%20include%20a%20learning%20signal.%20We%20present%20the%20ChartComplete%20dataset%20as%20is%20to%20the%20community%20to%20build%20upon%20it.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10462v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChartComplete%253A%2520A%2520Taxonomy-based%2520Inclusive%2520Chart%2520Dataset%26entry.906535625%3DAhmad%2520Mustapha%2520and%2520Charbel%2520Toumieh%2520and%2520Mariette%2520Awad%26entry.1292438233%3DWith%2520advancements%2520in%2520deep%2520learning%2520%2528DL%2529%2520and%2520computer%2520vision%2520techniques%252C%2520the%2520field%2520of%2520chart%2520understanding%2520is%2520evolving%2520rapidly.%2520In%2520particular%252C%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520are%2520proving%2520to%2520be%2520efficient%2520and%2520accurate%2520in%2520understanding%2520charts.%2520To%2520accurately%2520measure%2520the%2520performance%2520of%2520MLLMs%252C%2520the%2520research%2520community%2520has%2520developed%2520multiple%2520datasets%2520to%2520serve%2520as%2520benchmarks.%2520By%2520examining%2520these%2520datasets%252C%2520we%2520found%2520that%2520they%2520are%2520all%2520limited%2520to%2520a%2520small%2520set%2520of%2520chart%2520types.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520the%2520ChartComplete%2520dataset.%2520The%2520dataset%2520is%2520based%2520on%2520a%2520chart%2520taxonomy%2520borrowed%2520from%2520the%2520visualization%2520community%252C%2520and%2520it%2520covers%2520thirty%2520different%2520chart%2520types.%2520The%2520dataset%2520is%2520a%2520collection%2520of%2520classified%2520chart%2520images%2520and%2520does%2520not%2520include%2520a%2520learning%2520signal.%2520We%2520present%2520the%2520ChartComplete%2520dataset%2520as%2520is%2520to%2520the%2520community%2520to%2520build%2520upon%2520it.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10462v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChartComplete%3A%20A%20Taxonomy-based%20Inclusive%20Chart%20Dataset&entry.906535625=Ahmad%20Mustapha%20and%20Charbel%20Toumieh%20and%20Mariette%20Awad&entry.1292438233=With%20advancements%20in%20deep%20learning%20%28DL%29%20and%20computer%20vision%20techniques%2C%20the%20field%20of%20chart%20understanding%20is%20evolving%20rapidly.%20In%20particular%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20are%20proving%20to%20be%20efficient%20and%20accurate%20in%20understanding%20charts.%20To%20accurately%20measure%20the%20performance%20of%20MLLMs%2C%20the%20research%20community%20has%20developed%20multiple%20datasets%20to%20serve%20as%20benchmarks.%20By%20examining%20these%20datasets%2C%20we%20found%20that%20they%20are%20all%20limited%20to%20a%20small%20set%20of%20chart%20types.%20To%20bridge%20this%20gap%2C%20we%20propose%20the%20ChartComplete%20dataset.%20The%20dataset%20is%20based%20on%20a%20chart%20taxonomy%20borrowed%20from%20the%20visualization%20community%2C%20and%20it%20covers%20thirty%20different%20chart%20types.%20The%20dataset%20is%20a%20collection%20of%20classified%20chart%20images%20and%20does%20not%20include%20a%20learning%20signal.%20We%20present%20the%20ChartComplete%20dataset%20as%20is%20to%20the%20community%20to%20build%20upon%20it.&entry.1838667208=http%3A//arxiv.org/abs/2601.10462v2&entry.124074799=Read"},
{"title": "Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning", "author": "Haomiao Tang and Jinpeng Wang and Minyi Zhao and Guanghao Meng and Ruisheng Luo and Long Chen and Shu-Tao Xia", "abstract": "Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model's robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG's effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions.", "link": "http://arxiv.org/abs/2601.11393v1", "date": "2026-01-16", "relevancy": 2.2761, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5874}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5719}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Uncertainty-Guided%20Composed%20Image%20Retrieval%20with%20Fine-Grained%20Probabilistic%20Learning&body=Title%3A%20Heterogeneous%20Uncertainty-Guided%20Composed%20Image%20Retrieval%20with%20Fine-Grained%20Probabilistic%20Learning%0AAuthor%3A%20Haomiao%20Tang%20and%20Jinpeng%20Wang%20and%20Minyi%20Zhao%20and%20Guanghao%20Meng%20and%20Ruisheng%20Luo%20and%20Long%20Chen%20and%20Shu-Tao%20Xia%0AAbstract%3A%20Composed%20Image%20Retrieval%20%28CIR%29%20enables%20image%20search%20by%20combining%20a%20reference%20image%20with%20modification%20text.%20Intrinsic%20noise%20in%20CIR%20triplets%20incurs%20intrinsic%20uncertainty%20and%20threatens%20the%20model%27s%20robustness.%20Probabilistic%20learning%20approaches%20have%20shown%20promise%20in%20addressing%20such%20issues%3B%20however%2C%20they%20fall%20short%20for%20CIR%20due%20to%20their%20instance-level%20holistic%20modeling%20and%20homogeneous%20treatment%20of%20queries%20and%20targets.%20This%20paper%20introduces%20a%20Heterogeneous%20Uncertainty-Guided%20%28HUG%29%20paradigm%20to%20overcome%20these%20limitations.%20HUG%20utilizes%20a%20fine-grained%20probabilistic%20learning%20framework%2C%20where%20queries%20and%20targets%20are%20represented%20by%20Gaussian%20embeddings%20that%20capture%20detailed%20concepts%20and%20uncertainties.%20We%20customize%20heterogeneous%20uncertainty%20estimations%20for%20multi-modal%20queries%20and%20uni-modal%20targets.%20Given%20a%20query%2C%20we%20capture%20uncertainties%20not%20only%20regarding%20uni-modal%20content%20quality%20but%20also%20multi-modal%20coordination%2C%20followed%20by%20a%20provable%20dynamic%20weighting%20mechanism%20to%20derive%20comprehensive%20query%20uncertainty.%20We%20further%20design%20uncertainty-guided%20objectives%2C%20including%20query-target%20holistic%20contrast%20and%20fine-grained%20contrasts%20with%20comprehensive%20negative%20sampling%20strategies%2C%20which%20effectively%20enhance%20discriminative%20learning.%20Experiments%20on%20benchmarks%20demonstrate%20HUG%27s%20effectiveness%20beyond%20state-of-the-art%20baselines%2C%20with%20faithful%20analysis%20justifying%20the%20technical%20contributions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneous%2520Uncertainty-Guided%2520Composed%2520Image%2520Retrieval%2520with%2520Fine-Grained%2520Probabilistic%2520Learning%26entry.906535625%3DHaomiao%2520Tang%2520and%2520Jinpeng%2520Wang%2520and%2520Minyi%2520Zhao%2520and%2520Guanghao%2520Meng%2520and%2520Ruisheng%2520Luo%2520and%2520Long%2520Chen%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3DComposed%2520Image%2520Retrieval%2520%2528CIR%2529%2520enables%2520image%2520search%2520by%2520combining%2520a%2520reference%2520image%2520with%2520modification%2520text.%2520Intrinsic%2520noise%2520in%2520CIR%2520triplets%2520incurs%2520intrinsic%2520uncertainty%2520and%2520threatens%2520the%2520model%2527s%2520robustness.%2520Probabilistic%2520learning%2520approaches%2520have%2520shown%2520promise%2520in%2520addressing%2520such%2520issues%253B%2520however%252C%2520they%2520fall%2520short%2520for%2520CIR%2520due%2520to%2520their%2520instance-level%2520holistic%2520modeling%2520and%2520homogeneous%2520treatment%2520of%2520queries%2520and%2520targets.%2520This%2520paper%2520introduces%2520a%2520Heterogeneous%2520Uncertainty-Guided%2520%2528HUG%2529%2520paradigm%2520to%2520overcome%2520these%2520limitations.%2520HUG%2520utilizes%2520a%2520fine-grained%2520probabilistic%2520learning%2520framework%252C%2520where%2520queries%2520and%2520targets%2520are%2520represented%2520by%2520Gaussian%2520embeddings%2520that%2520capture%2520detailed%2520concepts%2520and%2520uncertainties.%2520We%2520customize%2520heterogeneous%2520uncertainty%2520estimations%2520for%2520multi-modal%2520queries%2520and%2520uni-modal%2520targets.%2520Given%2520a%2520query%252C%2520we%2520capture%2520uncertainties%2520not%2520only%2520regarding%2520uni-modal%2520content%2520quality%2520but%2520also%2520multi-modal%2520coordination%252C%2520followed%2520by%2520a%2520provable%2520dynamic%2520weighting%2520mechanism%2520to%2520derive%2520comprehensive%2520query%2520uncertainty.%2520We%2520further%2520design%2520uncertainty-guided%2520objectives%252C%2520including%2520query-target%2520holistic%2520contrast%2520and%2520fine-grained%2520contrasts%2520with%2520comprehensive%2520negative%2520sampling%2520strategies%252C%2520which%2520effectively%2520enhance%2520discriminative%2520learning.%2520Experiments%2520on%2520benchmarks%2520demonstrate%2520HUG%2527s%2520effectiveness%2520beyond%2520state-of-the-art%2520baselines%252C%2520with%2520faithful%2520analysis%2520justifying%2520the%2520technical%2520contributions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Uncertainty-Guided%20Composed%20Image%20Retrieval%20with%20Fine-Grained%20Probabilistic%20Learning&entry.906535625=Haomiao%20Tang%20and%20Jinpeng%20Wang%20and%20Minyi%20Zhao%20and%20Guanghao%20Meng%20and%20Ruisheng%20Luo%20and%20Long%20Chen%20and%20Shu-Tao%20Xia&entry.1292438233=Composed%20Image%20Retrieval%20%28CIR%29%20enables%20image%20search%20by%20combining%20a%20reference%20image%20with%20modification%20text.%20Intrinsic%20noise%20in%20CIR%20triplets%20incurs%20intrinsic%20uncertainty%20and%20threatens%20the%20model%27s%20robustness.%20Probabilistic%20learning%20approaches%20have%20shown%20promise%20in%20addressing%20such%20issues%3B%20however%2C%20they%20fall%20short%20for%20CIR%20due%20to%20their%20instance-level%20holistic%20modeling%20and%20homogeneous%20treatment%20of%20queries%20and%20targets.%20This%20paper%20introduces%20a%20Heterogeneous%20Uncertainty-Guided%20%28HUG%29%20paradigm%20to%20overcome%20these%20limitations.%20HUG%20utilizes%20a%20fine-grained%20probabilistic%20learning%20framework%2C%20where%20queries%20and%20targets%20are%20represented%20by%20Gaussian%20embeddings%20that%20capture%20detailed%20concepts%20and%20uncertainties.%20We%20customize%20heterogeneous%20uncertainty%20estimations%20for%20multi-modal%20queries%20and%20uni-modal%20targets.%20Given%20a%20query%2C%20we%20capture%20uncertainties%20not%20only%20regarding%20uni-modal%20content%20quality%20but%20also%20multi-modal%20coordination%2C%20followed%20by%20a%20provable%20dynamic%20weighting%20mechanism%20to%20derive%20comprehensive%20query%20uncertainty.%20We%20further%20design%20uncertainty-guided%20objectives%2C%20including%20query-target%20holistic%20contrast%20and%20fine-grained%20contrasts%20with%20comprehensive%20negative%20sampling%20strategies%2C%20which%20effectively%20enhance%20discriminative%20learning.%20Experiments%20on%20benchmarks%20demonstrate%20HUG%27s%20effectiveness%20beyond%20state-of-the-art%20baselines%2C%20with%20faithful%20analysis%20justifying%20the%20technical%20contributions.&entry.1838667208=http%3A//arxiv.org/abs/2601.11393v1&entry.124074799=Read"},
{"title": "SENSE: Self-Supervised Neural Embeddings for Spatial Ensembles", "author": "Hamid Gadirov and Lennard Manuel and Steffen Frey", "abstract": "Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.", "link": "http://arxiv.org/abs/2512.11145v2", "date": "2026-01-16", "relevancy": 2.2646, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5996}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5463}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SENSE%3A%20Self-Supervised%20Neural%20Embeddings%20for%20Spatial%20Ensembles&body=Title%3A%20SENSE%3A%20Self-Supervised%20Neural%20Embeddings%20for%20Spatial%20Ensembles%0AAuthor%3A%20Hamid%20Gadirov%20and%20Lennard%20Manuel%20and%20Steffen%20Frey%0AAbstract%3A%20Analyzing%20and%20visualizing%20scientific%20ensemble%20datasets%20with%20high%20dimensionality%20and%20complexity%20poses%20significant%20challenges.%20Dimensionality%20reduction%20techniques%20and%20autoencoders%20are%20powerful%20tools%20for%20extracting%20features%2C%20but%20they%20often%20struggle%20with%20such%20high-dimensional%20data.%20This%20paper%20presents%20an%20enhanced%20autoencoder%20framework%20that%20incorporates%20a%20clustering%20loss%2C%20based%20on%20the%20soft%20silhouette%20score%2C%20alongside%20a%20contrastive%20loss%20to%20improve%20the%20visualization%20and%20interpretability%20of%20ensemble%20datasets.%20First%2C%20EfficientNetV2%20is%20used%20to%20generate%20pseudo-labels%20for%20the%20unlabeled%20portions%20of%20the%20scientific%20ensemble%20datasets.%20By%20jointly%20optimizing%20the%20reconstruction%2C%20clustering%2C%20and%20contrastive%20objectives%2C%20our%20method%20encourages%20similar%20data%20points%20to%20group%20together%20while%20separating%20distinct%20clusters%20in%20the%20latent%20space.%20UMAP%20is%20subsequently%20applied%20to%20this%20latent%20representation%20to%20produce%202D%20projections%2C%20which%20are%20evaluated%20using%20the%20silhouette%20score.%20Multiple%20types%20of%20autoencoders%20are%20evaluated%20and%20compared%20based%20on%20their%20ability%20to%20extract%20meaningful%20features.%20Experiments%20on%20two%20scientific%20ensemble%20datasets%20-%20channel%20structures%20in%20soil%20derived%20from%20Markov%20chain%20Monte%20Carlo%2C%20and%20droplet-on-film%20impact%20dynamics%20-%20show%20that%20models%20incorporating%20clustering%20or%20contrastive%20loss%20marginally%20outperform%20the%20baseline%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11145v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSENSE%253A%2520Self-Supervised%2520Neural%2520Embeddings%2520for%2520Spatial%2520Ensembles%26entry.906535625%3DHamid%2520Gadirov%2520and%2520Lennard%2520Manuel%2520and%2520Steffen%2520Frey%26entry.1292438233%3DAnalyzing%2520and%2520visualizing%2520scientific%2520ensemble%2520datasets%2520with%2520high%2520dimensionality%2520and%2520complexity%2520poses%2520significant%2520challenges.%2520Dimensionality%2520reduction%2520techniques%2520and%2520autoencoders%2520are%2520powerful%2520tools%2520for%2520extracting%2520features%252C%2520but%2520they%2520often%2520struggle%2520with%2520such%2520high-dimensional%2520data.%2520This%2520paper%2520presents%2520an%2520enhanced%2520autoencoder%2520framework%2520that%2520incorporates%2520a%2520clustering%2520loss%252C%2520based%2520on%2520the%2520soft%2520silhouette%2520score%252C%2520alongside%2520a%2520contrastive%2520loss%2520to%2520improve%2520the%2520visualization%2520and%2520interpretability%2520of%2520ensemble%2520datasets.%2520First%252C%2520EfficientNetV2%2520is%2520used%2520to%2520generate%2520pseudo-labels%2520for%2520the%2520unlabeled%2520portions%2520of%2520the%2520scientific%2520ensemble%2520datasets.%2520By%2520jointly%2520optimizing%2520the%2520reconstruction%252C%2520clustering%252C%2520and%2520contrastive%2520objectives%252C%2520our%2520method%2520encourages%2520similar%2520data%2520points%2520to%2520group%2520together%2520while%2520separating%2520distinct%2520clusters%2520in%2520the%2520latent%2520space.%2520UMAP%2520is%2520subsequently%2520applied%2520to%2520this%2520latent%2520representation%2520to%2520produce%25202D%2520projections%252C%2520which%2520are%2520evaluated%2520using%2520the%2520silhouette%2520score.%2520Multiple%2520types%2520of%2520autoencoders%2520are%2520evaluated%2520and%2520compared%2520based%2520on%2520their%2520ability%2520to%2520extract%2520meaningful%2520features.%2520Experiments%2520on%2520two%2520scientific%2520ensemble%2520datasets%2520-%2520channel%2520structures%2520in%2520soil%2520derived%2520from%2520Markov%2520chain%2520Monte%2520Carlo%252C%2520and%2520droplet-on-film%2520impact%2520dynamics%2520-%2520show%2520that%2520models%2520incorporating%2520clustering%2520or%2520contrastive%2520loss%2520marginally%2520outperform%2520the%2520baseline%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11145v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SENSE%3A%20Self-Supervised%20Neural%20Embeddings%20for%20Spatial%20Ensembles&entry.906535625=Hamid%20Gadirov%20and%20Lennard%20Manuel%20and%20Steffen%20Frey&entry.1292438233=Analyzing%20and%20visualizing%20scientific%20ensemble%20datasets%20with%20high%20dimensionality%20and%20complexity%20poses%20significant%20challenges.%20Dimensionality%20reduction%20techniques%20and%20autoencoders%20are%20powerful%20tools%20for%20extracting%20features%2C%20but%20they%20often%20struggle%20with%20such%20high-dimensional%20data.%20This%20paper%20presents%20an%20enhanced%20autoencoder%20framework%20that%20incorporates%20a%20clustering%20loss%2C%20based%20on%20the%20soft%20silhouette%20score%2C%20alongside%20a%20contrastive%20loss%20to%20improve%20the%20visualization%20and%20interpretability%20of%20ensemble%20datasets.%20First%2C%20EfficientNetV2%20is%20used%20to%20generate%20pseudo-labels%20for%20the%20unlabeled%20portions%20of%20the%20scientific%20ensemble%20datasets.%20By%20jointly%20optimizing%20the%20reconstruction%2C%20clustering%2C%20and%20contrastive%20objectives%2C%20our%20method%20encourages%20similar%20data%20points%20to%20group%20together%20while%20separating%20distinct%20clusters%20in%20the%20latent%20space.%20UMAP%20is%20subsequently%20applied%20to%20this%20latent%20representation%20to%20produce%202D%20projections%2C%20which%20are%20evaluated%20using%20the%20silhouette%20score.%20Multiple%20types%20of%20autoencoders%20are%20evaluated%20and%20compared%20based%20on%20their%20ability%20to%20extract%20meaningful%20features.%20Experiments%20on%20two%20scientific%20ensemble%20datasets%20-%20channel%20structures%20in%20soil%20derived%20from%20Markov%20chain%20Monte%20Carlo%2C%20and%20droplet-on-film%20impact%20dynamics%20-%20show%20that%20models%20incorporating%20clustering%20or%20contrastive%20loss%20marginally%20outperform%20the%20baseline%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2512.11145v2&entry.124074799=Read"},
{"title": "From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda", "author": "Piercosma Bisconti and Marcello Galisai and Matteo Prandi and Federico Pierucci and Olga Sorokoletova and Francesco Giarrusso and Vincenzo Suriani and Marcantonio Bracale Syrnikov and Daniele Nardi", "abstract": "Safety mechanisms in LLMs remain vulnerable to attacks that reframe harmful requests through culturally coded structures. We introduce Adversarial Tales, a jailbreak technique that embeds harmful content within cyberpunk narratives and prompts models to perform functional analysis inspired by Vladimir Propp's morphology of folktales. By casting the task as structural decomposition, the attack induces models to reconstruct harmful procedures as legitimate narrative interpretation. Across 26 frontier models from nine providers, we observe an average attack success rate of 71.3%, with no model family proving reliably robust. Together with our prior work on Adversarial Poetry, these findings suggest that structurally-grounded jailbreaks constitute a broad vulnerability class rather than isolated techniques. The space of culturally coded frames that can mediate harmful intent is vast, likely inexhaustible by pattern-matching defenses alone. Understanding why these attacks succeed is therefore essential: we outline a mechanistic interpretability research agenda to investigate how narrative cues reshape model representations and whether models can learn to recognize harmful intent independently of surface form.", "link": "http://arxiv.org/abs/2601.08837v2", "date": "2026-01-16", "relevancy": 2.2639, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4576}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Adversarial%20Poetry%20to%20Adversarial%20Tales%3A%20An%20Interpretability%20Research%20Agenda&body=Title%3A%20From%20Adversarial%20Poetry%20to%20Adversarial%20Tales%3A%20An%20Interpretability%20Research%20Agenda%0AAuthor%3A%20Piercosma%20Bisconti%20and%20Marcello%20Galisai%20and%20Matteo%20Prandi%20and%20Federico%20Pierucci%20and%20Olga%20Sorokoletova%20and%20Francesco%20Giarrusso%20and%20Vincenzo%20Suriani%20and%20Marcantonio%20Bracale%20Syrnikov%20and%20Daniele%20Nardi%0AAbstract%3A%20Safety%20mechanisms%20in%20LLMs%20remain%20vulnerable%20to%20attacks%20that%20reframe%20harmful%20requests%20through%20culturally%20coded%20structures.%20We%20introduce%20Adversarial%20Tales%2C%20a%20jailbreak%20technique%20that%20embeds%20harmful%20content%20within%20cyberpunk%20narratives%20and%20prompts%20models%20to%20perform%20functional%20analysis%20inspired%20by%20Vladimir%20Propp%27s%20morphology%20of%20folktales.%20By%20casting%20the%20task%20as%20structural%20decomposition%2C%20the%20attack%20induces%20models%20to%20reconstruct%20harmful%20procedures%20as%20legitimate%20narrative%20interpretation.%20Across%2026%20frontier%20models%20from%20nine%20providers%2C%20we%20observe%20an%20average%20attack%20success%20rate%20of%2071.3%25%2C%20with%20no%20model%20family%20proving%20reliably%20robust.%20Together%20with%20our%20prior%20work%20on%20Adversarial%20Poetry%2C%20these%20findings%20suggest%20that%20structurally-grounded%20jailbreaks%20constitute%20a%20broad%20vulnerability%20class%20rather%20than%20isolated%20techniques.%20The%20space%20of%20culturally%20coded%20frames%20that%20can%20mediate%20harmful%20intent%20is%20vast%2C%20likely%20inexhaustible%20by%20pattern-matching%20defenses%20alone.%20Understanding%20why%20these%20attacks%20succeed%20is%20therefore%20essential%3A%20we%20outline%20a%20mechanistic%20interpretability%20research%20agenda%20to%20investigate%20how%20narrative%20cues%20reshape%20model%20representations%20and%20whether%20models%20can%20learn%20to%20recognize%20harmful%20intent%20independently%20of%20surface%20form.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08837v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Adversarial%2520Poetry%2520to%2520Adversarial%2520Tales%253A%2520An%2520Interpretability%2520Research%2520Agenda%26entry.906535625%3DPiercosma%2520Bisconti%2520and%2520Marcello%2520Galisai%2520and%2520Matteo%2520Prandi%2520and%2520Federico%2520Pierucci%2520and%2520Olga%2520Sorokoletova%2520and%2520Francesco%2520Giarrusso%2520and%2520Vincenzo%2520Suriani%2520and%2520Marcantonio%2520Bracale%2520Syrnikov%2520and%2520Daniele%2520Nardi%26entry.1292438233%3DSafety%2520mechanisms%2520in%2520LLMs%2520remain%2520vulnerable%2520to%2520attacks%2520that%2520reframe%2520harmful%2520requests%2520through%2520culturally%2520coded%2520structures.%2520We%2520introduce%2520Adversarial%2520Tales%252C%2520a%2520jailbreak%2520technique%2520that%2520embeds%2520harmful%2520content%2520within%2520cyberpunk%2520narratives%2520and%2520prompts%2520models%2520to%2520perform%2520functional%2520analysis%2520inspired%2520by%2520Vladimir%2520Propp%2527s%2520morphology%2520of%2520folktales.%2520By%2520casting%2520the%2520task%2520as%2520structural%2520decomposition%252C%2520the%2520attack%2520induces%2520models%2520to%2520reconstruct%2520harmful%2520procedures%2520as%2520legitimate%2520narrative%2520interpretation.%2520Across%252026%2520frontier%2520models%2520from%2520nine%2520providers%252C%2520we%2520observe%2520an%2520average%2520attack%2520success%2520rate%2520of%252071.3%2525%252C%2520with%2520no%2520model%2520family%2520proving%2520reliably%2520robust.%2520Together%2520with%2520our%2520prior%2520work%2520on%2520Adversarial%2520Poetry%252C%2520these%2520findings%2520suggest%2520that%2520structurally-grounded%2520jailbreaks%2520constitute%2520a%2520broad%2520vulnerability%2520class%2520rather%2520than%2520isolated%2520techniques.%2520The%2520space%2520of%2520culturally%2520coded%2520frames%2520that%2520can%2520mediate%2520harmful%2520intent%2520is%2520vast%252C%2520likely%2520inexhaustible%2520by%2520pattern-matching%2520defenses%2520alone.%2520Understanding%2520why%2520these%2520attacks%2520succeed%2520is%2520therefore%2520essential%253A%2520we%2520outline%2520a%2520mechanistic%2520interpretability%2520research%2520agenda%2520to%2520investigate%2520how%2520narrative%2520cues%2520reshape%2520model%2520representations%2520and%2520whether%2520models%2520can%2520learn%2520to%2520recognize%2520harmful%2520intent%2520independently%2520of%2520surface%2520form.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08837v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Adversarial%20Poetry%20to%20Adversarial%20Tales%3A%20An%20Interpretability%20Research%20Agenda&entry.906535625=Piercosma%20Bisconti%20and%20Marcello%20Galisai%20and%20Matteo%20Prandi%20and%20Federico%20Pierucci%20and%20Olga%20Sorokoletova%20and%20Francesco%20Giarrusso%20and%20Vincenzo%20Suriani%20and%20Marcantonio%20Bracale%20Syrnikov%20and%20Daniele%20Nardi&entry.1292438233=Safety%20mechanisms%20in%20LLMs%20remain%20vulnerable%20to%20attacks%20that%20reframe%20harmful%20requests%20through%20culturally%20coded%20structures.%20We%20introduce%20Adversarial%20Tales%2C%20a%20jailbreak%20technique%20that%20embeds%20harmful%20content%20within%20cyberpunk%20narratives%20and%20prompts%20models%20to%20perform%20functional%20analysis%20inspired%20by%20Vladimir%20Propp%27s%20morphology%20of%20folktales.%20By%20casting%20the%20task%20as%20structural%20decomposition%2C%20the%20attack%20induces%20models%20to%20reconstruct%20harmful%20procedures%20as%20legitimate%20narrative%20interpretation.%20Across%2026%20frontier%20models%20from%20nine%20providers%2C%20we%20observe%20an%20average%20attack%20success%20rate%20of%2071.3%25%2C%20with%20no%20model%20family%20proving%20reliably%20robust.%20Together%20with%20our%20prior%20work%20on%20Adversarial%20Poetry%2C%20these%20findings%20suggest%20that%20structurally-grounded%20jailbreaks%20constitute%20a%20broad%20vulnerability%20class%20rather%20than%20isolated%20techniques.%20The%20space%20of%20culturally%20coded%20frames%20that%20can%20mediate%20harmful%20intent%20is%20vast%2C%20likely%20inexhaustible%20by%20pattern-matching%20defenses%20alone.%20Understanding%20why%20these%20attacks%20succeed%20is%20therefore%20essential%3A%20we%20outline%20a%20mechanistic%20interpretability%20research%20agenda%20to%20investigate%20how%20narrative%20cues%20reshape%20model%20representations%20and%20whether%20models%20can%20learn%20to%20recognize%20harmful%20intent%20independently%20of%20surface%20form.&entry.1838667208=http%3A//arxiv.org/abs/2601.08837v2&entry.124074799=Read"},
{"title": "ATATA: One Algorithm to Align Them All", "author": "Boyi Pang and Savva Ignatyev and Vladimir Ippolitov and Ramil Khafizov and Yurii Melnik and Oleg Voynov and Maksim Nakhodnov and Aibek Alanov and Xiaopeng Fan and Peter Wonka and Evgeny Burnaev", "abstract": "We suggest a new multi-modal algorithm for joint inference of paired structurally aligned samples with Rectified Flow models. While some existing methods propose a codependent generation process, they do not view the problem of joint generation from a structural alignment perspective. Recent work uses Score Distillation Sampling to generate aligned 3D models, but SDS is known to be time-consuming, prone to mode collapse, and often provides cartoonish results. By contrast, our suggested approach relies on the joint transport of a segment in the sample space, yielding faster computation at inference time. Our approach can be built on top of an arbitrary Rectified Flow model operating on the structured latent space. We show the applicability of our method to the domains of image, video, and 3D shape generation using state-of-the-art baselines and evaluate it against both editing-based and joint inference-based competing approaches. We demonstrate a high degree of structural alignment for the sample pairs obtained with our method and a high visual quality of the samples. Our method improves the state-of-the-art for image and video generation pipelines. For 3D generation, it is able to show comparable quality while working orders of magnitude faster.", "link": "http://arxiv.org/abs/2601.11194v1", "date": "2026-01-16", "relevancy": 2.2515, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5852}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATATA%3A%20One%20Algorithm%20to%20Align%20Them%20All&body=Title%3A%20ATATA%3A%20One%20Algorithm%20to%20Align%20Them%20All%0AAuthor%3A%20Boyi%20Pang%20and%20Savva%20Ignatyev%20and%20Vladimir%20Ippolitov%20and%20Ramil%20Khafizov%20and%20Yurii%20Melnik%20and%20Oleg%20Voynov%20and%20Maksim%20Nakhodnov%20and%20Aibek%20Alanov%20and%20Xiaopeng%20Fan%20and%20Peter%20Wonka%20and%20Evgeny%20Burnaev%0AAbstract%3A%20We%20suggest%20a%20new%20multi-modal%20algorithm%20for%20joint%20inference%20of%20paired%20structurally%20aligned%20samples%20with%20Rectified%20Flow%20models.%20While%20some%20existing%20methods%20propose%20a%20codependent%20generation%20process%2C%20they%20do%20not%20view%20the%20problem%20of%20joint%20generation%20from%20a%20structural%20alignment%20perspective.%20Recent%20work%20uses%20Score%20Distillation%20Sampling%20to%20generate%20aligned%203D%20models%2C%20but%20SDS%20is%20known%20to%20be%20time-consuming%2C%20prone%20to%20mode%20collapse%2C%20and%20often%20provides%20cartoonish%20results.%20By%20contrast%2C%20our%20suggested%20approach%20relies%20on%20the%20joint%20transport%20of%20a%20segment%20in%20the%20sample%20space%2C%20yielding%20faster%20computation%20at%20inference%20time.%20Our%20approach%20can%20be%20built%20on%20top%20of%20an%20arbitrary%20Rectified%20Flow%20model%20operating%20on%20the%20structured%20latent%20space.%20We%20show%20the%20applicability%20of%20our%20method%20to%20the%20domains%20of%20image%2C%20video%2C%20and%203D%20shape%20generation%20using%20state-of-the-art%20baselines%20and%20evaluate%20it%20against%20both%20editing-based%20and%20joint%20inference-based%20competing%20approaches.%20We%20demonstrate%20a%20high%20degree%20of%20structural%20alignment%20for%20the%20sample%20pairs%20obtained%20with%20our%20method%20and%20a%20high%20visual%20quality%20of%20the%20samples.%20Our%20method%20improves%20the%20state-of-the-art%20for%20image%20and%20video%20generation%20pipelines.%20For%203D%20generation%2C%20it%20is%20able%20to%20show%20comparable%20quality%20while%20working%20orders%20of%20magnitude%20faster.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATATA%253A%2520One%2520Algorithm%2520to%2520Align%2520Them%2520All%26entry.906535625%3DBoyi%2520Pang%2520and%2520Savva%2520Ignatyev%2520and%2520Vladimir%2520Ippolitov%2520and%2520Ramil%2520Khafizov%2520and%2520Yurii%2520Melnik%2520and%2520Oleg%2520Voynov%2520and%2520Maksim%2520Nakhodnov%2520and%2520Aibek%2520Alanov%2520and%2520Xiaopeng%2520Fan%2520and%2520Peter%2520Wonka%2520and%2520Evgeny%2520Burnaev%26entry.1292438233%3DWe%2520suggest%2520a%2520new%2520multi-modal%2520algorithm%2520for%2520joint%2520inference%2520of%2520paired%2520structurally%2520aligned%2520samples%2520with%2520Rectified%2520Flow%2520models.%2520While%2520some%2520existing%2520methods%2520propose%2520a%2520codependent%2520generation%2520process%252C%2520they%2520do%2520not%2520view%2520the%2520problem%2520of%2520joint%2520generation%2520from%2520a%2520structural%2520alignment%2520perspective.%2520Recent%2520work%2520uses%2520Score%2520Distillation%2520Sampling%2520to%2520generate%2520aligned%25203D%2520models%252C%2520but%2520SDS%2520is%2520known%2520to%2520be%2520time-consuming%252C%2520prone%2520to%2520mode%2520collapse%252C%2520and%2520often%2520provides%2520cartoonish%2520results.%2520By%2520contrast%252C%2520our%2520suggested%2520approach%2520relies%2520on%2520the%2520joint%2520transport%2520of%2520a%2520segment%2520in%2520the%2520sample%2520space%252C%2520yielding%2520faster%2520computation%2520at%2520inference%2520time.%2520Our%2520approach%2520can%2520be%2520built%2520on%2520top%2520of%2520an%2520arbitrary%2520Rectified%2520Flow%2520model%2520operating%2520on%2520the%2520structured%2520latent%2520space.%2520We%2520show%2520the%2520applicability%2520of%2520our%2520method%2520to%2520the%2520domains%2520of%2520image%252C%2520video%252C%2520and%25203D%2520shape%2520generation%2520using%2520state-of-the-art%2520baselines%2520and%2520evaluate%2520it%2520against%2520both%2520editing-based%2520and%2520joint%2520inference-based%2520competing%2520approaches.%2520We%2520demonstrate%2520a%2520high%2520degree%2520of%2520structural%2520alignment%2520for%2520the%2520sample%2520pairs%2520obtained%2520with%2520our%2520method%2520and%2520a%2520high%2520visual%2520quality%2520of%2520the%2520samples.%2520Our%2520method%2520improves%2520the%2520state-of-the-art%2520for%2520image%2520and%2520video%2520generation%2520pipelines.%2520For%25203D%2520generation%252C%2520it%2520is%2520able%2520to%2520show%2520comparable%2520quality%2520while%2520working%2520orders%2520of%2520magnitude%2520faster.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATATA%3A%20One%20Algorithm%20to%20Align%20Them%20All&entry.906535625=Boyi%20Pang%20and%20Savva%20Ignatyev%20and%20Vladimir%20Ippolitov%20and%20Ramil%20Khafizov%20and%20Yurii%20Melnik%20and%20Oleg%20Voynov%20and%20Maksim%20Nakhodnov%20and%20Aibek%20Alanov%20and%20Xiaopeng%20Fan%20and%20Peter%20Wonka%20and%20Evgeny%20Burnaev&entry.1292438233=We%20suggest%20a%20new%20multi-modal%20algorithm%20for%20joint%20inference%20of%20paired%20structurally%20aligned%20samples%20with%20Rectified%20Flow%20models.%20While%20some%20existing%20methods%20propose%20a%20codependent%20generation%20process%2C%20they%20do%20not%20view%20the%20problem%20of%20joint%20generation%20from%20a%20structural%20alignment%20perspective.%20Recent%20work%20uses%20Score%20Distillation%20Sampling%20to%20generate%20aligned%203D%20models%2C%20but%20SDS%20is%20known%20to%20be%20time-consuming%2C%20prone%20to%20mode%20collapse%2C%20and%20often%20provides%20cartoonish%20results.%20By%20contrast%2C%20our%20suggested%20approach%20relies%20on%20the%20joint%20transport%20of%20a%20segment%20in%20the%20sample%20space%2C%20yielding%20faster%20computation%20at%20inference%20time.%20Our%20approach%20can%20be%20built%20on%20top%20of%20an%20arbitrary%20Rectified%20Flow%20model%20operating%20on%20the%20structured%20latent%20space.%20We%20show%20the%20applicability%20of%20our%20method%20to%20the%20domains%20of%20image%2C%20video%2C%20and%203D%20shape%20generation%20using%20state-of-the-art%20baselines%20and%20evaluate%20it%20against%20both%20editing-based%20and%20joint%20inference-based%20competing%20approaches.%20We%20demonstrate%20a%20high%20degree%20of%20structural%20alignment%20for%20the%20sample%20pairs%20obtained%20with%20our%20method%20and%20a%20high%20visual%20quality%20of%20the%20samples.%20Our%20method%20improves%20the%20state-of-the-art%20for%20image%20and%20video%20generation%20pipelines.%20For%203D%20generation%2C%20it%20is%20able%20to%20show%20comparable%20quality%20while%20working%20orders%20of%20magnitude%20faster.&entry.1838667208=http%3A//arxiv.org/abs/2601.11194v1&entry.124074799=Read"},
{"title": "Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation", "author": "Tao Tang and Shijie Xu and Jionglong Su and Zhixiang Lu", "abstract": "The clinical utility of deep learning models for medical image segmentation is severely constrained by their inability to generalize to unseen domains. This failure is often rooted in the models learning spurious correlations between anatomical content and domain-specific imaging styles. To overcome this fundamental challenge, we introduce Causal-SAM-LLM, a novel framework that elevates Large Language Models (LLMs) to the role of causal reasoners. Our framework, built upon a frozen Segment Anything Model (SAM) encoder, incorporates two synergistic innovations. First, Linguistic Adversarial Disentanglement (LAD) employs a Vision-Language Model to generate rich, textual descriptions of confounding image styles. By training the segmentation model's features to be contrastively dissimilar to these style descriptions, it learns a representation robustly purged of non-causal information. Second, Test-Time Causal Intervention (TCI) provides an interactive mechanism where an LLM interprets a clinician's natural language command to modulate the segmentation decoder's features in real-time, enabling targeted error correction. We conduct an extensive empirical evaluation on a composite benchmark from four public datasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under cross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM establishes a new state of the art in out-of-distribution (OOD) robustness, improving the average Dice score by up to 6.2 points and reducing the Hausdorff Distance by 15.8 mm over the strongest baseline, all while using less than 9% of the full model's trainable parameters. Our work charts a new course for building robust, efficient, and interactively controllable medical AI systems.", "link": "http://arxiv.org/abs/2507.03585v2", "date": "2026-01-16", "relevancy": 2.2472, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5768}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal-SAM-LLM%3A%20Large%20Language%20Models%20as%20Causal%20Reasoners%20for%20Robust%20Medical%20Segmentation&body=Title%3A%20Causal-SAM-LLM%3A%20Large%20Language%20Models%20as%20Causal%20Reasoners%20for%20Robust%20Medical%20Segmentation%0AAuthor%3A%20Tao%20Tang%20and%20Shijie%20Xu%20and%20Jionglong%20Su%20and%20Zhixiang%20Lu%0AAbstract%3A%20The%20clinical%20utility%20of%20deep%20learning%20models%20for%20medical%20image%20segmentation%20is%20severely%20constrained%20by%20their%20inability%20to%20generalize%20to%20unseen%20domains.%20This%20failure%20is%20often%20rooted%20in%20the%20models%20learning%20spurious%20correlations%20between%20anatomical%20content%20and%20domain-specific%20imaging%20styles.%20To%20overcome%20this%20fundamental%20challenge%2C%20we%20introduce%20Causal-SAM-LLM%2C%20a%20novel%20framework%20that%20elevates%20Large%20Language%20Models%20%28LLMs%29%20to%20the%20role%20of%20causal%20reasoners.%20Our%20framework%2C%20built%20upon%20a%20frozen%20Segment%20Anything%20Model%20%28SAM%29%20encoder%2C%20incorporates%20two%20synergistic%20innovations.%20First%2C%20Linguistic%20Adversarial%20Disentanglement%20%28LAD%29%20employs%20a%20Vision-Language%20Model%20to%20generate%20rich%2C%20textual%20descriptions%20of%20confounding%20image%20styles.%20By%20training%20the%20segmentation%20model%27s%20features%20to%20be%20contrastively%20dissimilar%20to%20these%20style%20descriptions%2C%20it%20learns%20a%20representation%20robustly%20purged%20of%20non-causal%20information.%20Second%2C%20Test-Time%20Causal%20Intervention%20%28TCI%29%20provides%20an%20interactive%20mechanism%20where%20an%20LLM%20interprets%20a%20clinician%27s%20natural%20language%20command%20to%20modulate%20the%20segmentation%20decoder%27s%20features%20in%20real-time%2C%20enabling%20targeted%20error%20correction.%20We%20conduct%20an%20extensive%20empirical%20evaluation%20on%20a%20composite%20benchmark%20from%20four%20public%20datasets%20%28BTCV%2C%20CHAOS%2C%20AMOS%2C%20BraTS%29%2C%20assessing%20generalization%20under%20cross-scanner%2C%20cross-modality%2C%20and%20cross-anatomy%20settings.%20Causal-SAM-LLM%20establishes%20a%20new%20state%20of%20the%20art%20in%20out-of-distribution%20%28OOD%29%20robustness%2C%20improving%20the%20average%20Dice%20score%20by%20up%20to%206.2%20points%20and%20reducing%20the%20Hausdorff%20Distance%20by%2015.8%20mm%20over%20the%20strongest%20baseline%2C%20all%20while%20using%20less%20than%209%25%20of%20the%20full%20model%27s%20trainable%20parameters.%20Our%20work%20charts%20a%20new%20course%20for%20building%20robust%2C%20efficient%2C%20and%20interactively%20controllable%20medical%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2507.03585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal-SAM-LLM%253A%2520Large%2520Language%2520Models%2520as%2520Causal%2520Reasoners%2520for%2520Robust%2520Medical%2520Segmentation%26entry.906535625%3DTao%2520Tang%2520and%2520Shijie%2520Xu%2520and%2520Jionglong%2520Su%2520and%2520Zhixiang%2520Lu%26entry.1292438233%3DThe%2520clinical%2520utility%2520of%2520deep%2520learning%2520models%2520for%2520medical%2520image%2520segmentation%2520is%2520severely%2520constrained%2520by%2520their%2520inability%2520to%2520generalize%2520to%2520unseen%2520domains.%2520This%2520failure%2520is%2520often%2520rooted%2520in%2520the%2520models%2520learning%2520spurious%2520correlations%2520between%2520anatomical%2520content%2520and%2520domain-specific%2520imaging%2520styles.%2520To%2520overcome%2520this%2520fundamental%2520challenge%252C%2520we%2520introduce%2520Causal-SAM-LLM%252C%2520a%2520novel%2520framework%2520that%2520elevates%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520the%2520role%2520of%2520causal%2520reasoners.%2520Our%2520framework%252C%2520built%2520upon%2520a%2520frozen%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520encoder%252C%2520incorporates%2520two%2520synergistic%2520innovations.%2520First%252C%2520Linguistic%2520Adversarial%2520Disentanglement%2520%2528LAD%2529%2520employs%2520a%2520Vision-Language%2520Model%2520to%2520generate%2520rich%252C%2520textual%2520descriptions%2520of%2520confounding%2520image%2520styles.%2520By%2520training%2520the%2520segmentation%2520model%2527s%2520features%2520to%2520be%2520contrastively%2520dissimilar%2520to%2520these%2520style%2520descriptions%252C%2520it%2520learns%2520a%2520representation%2520robustly%2520purged%2520of%2520non-causal%2520information.%2520Second%252C%2520Test-Time%2520Causal%2520Intervention%2520%2528TCI%2529%2520provides%2520an%2520interactive%2520mechanism%2520where%2520an%2520LLM%2520interprets%2520a%2520clinician%2527s%2520natural%2520language%2520command%2520to%2520modulate%2520the%2520segmentation%2520decoder%2527s%2520features%2520in%2520real-time%252C%2520enabling%2520targeted%2520error%2520correction.%2520We%2520conduct%2520an%2520extensive%2520empirical%2520evaluation%2520on%2520a%2520composite%2520benchmark%2520from%2520four%2520public%2520datasets%2520%2528BTCV%252C%2520CHAOS%252C%2520AMOS%252C%2520BraTS%2529%252C%2520assessing%2520generalization%2520under%2520cross-scanner%252C%2520cross-modality%252C%2520and%2520cross-anatomy%2520settings.%2520Causal-SAM-LLM%2520establishes%2520a%2520new%2520state%2520of%2520the%2520art%2520in%2520out-of-distribution%2520%2528OOD%2529%2520robustness%252C%2520improving%2520the%2520average%2520Dice%2520score%2520by%2520up%2520to%25206.2%2520points%2520and%2520reducing%2520the%2520Hausdorff%2520Distance%2520by%252015.8%2520mm%2520over%2520the%2520strongest%2520baseline%252C%2520all%2520while%2520using%2520less%2520than%25209%2525%2520of%2520the%2520full%2520model%2527s%2520trainable%2520parameters.%2520Our%2520work%2520charts%2520a%2520new%2520course%2520for%2520building%2520robust%252C%2520efficient%252C%2520and%2520interactively%2520controllable%2520medical%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal-SAM-LLM%3A%20Large%20Language%20Models%20as%20Causal%20Reasoners%20for%20Robust%20Medical%20Segmentation&entry.906535625=Tao%20Tang%20and%20Shijie%20Xu%20and%20Jionglong%20Su%20and%20Zhixiang%20Lu&entry.1292438233=The%20clinical%20utility%20of%20deep%20learning%20models%20for%20medical%20image%20segmentation%20is%20severely%20constrained%20by%20their%20inability%20to%20generalize%20to%20unseen%20domains.%20This%20failure%20is%20often%20rooted%20in%20the%20models%20learning%20spurious%20correlations%20between%20anatomical%20content%20and%20domain-specific%20imaging%20styles.%20To%20overcome%20this%20fundamental%20challenge%2C%20we%20introduce%20Causal-SAM-LLM%2C%20a%20novel%20framework%20that%20elevates%20Large%20Language%20Models%20%28LLMs%29%20to%20the%20role%20of%20causal%20reasoners.%20Our%20framework%2C%20built%20upon%20a%20frozen%20Segment%20Anything%20Model%20%28SAM%29%20encoder%2C%20incorporates%20two%20synergistic%20innovations.%20First%2C%20Linguistic%20Adversarial%20Disentanglement%20%28LAD%29%20employs%20a%20Vision-Language%20Model%20to%20generate%20rich%2C%20textual%20descriptions%20of%20confounding%20image%20styles.%20By%20training%20the%20segmentation%20model%27s%20features%20to%20be%20contrastively%20dissimilar%20to%20these%20style%20descriptions%2C%20it%20learns%20a%20representation%20robustly%20purged%20of%20non-causal%20information.%20Second%2C%20Test-Time%20Causal%20Intervention%20%28TCI%29%20provides%20an%20interactive%20mechanism%20where%20an%20LLM%20interprets%20a%20clinician%27s%20natural%20language%20command%20to%20modulate%20the%20segmentation%20decoder%27s%20features%20in%20real-time%2C%20enabling%20targeted%20error%20correction.%20We%20conduct%20an%20extensive%20empirical%20evaluation%20on%20a%20composite%20benchmark%20from%20four%20public%20datasets%20%28BTCV%2C%20CHAOS%2C%20AMOS%2C%20BraTS%29%2C%20assessing%20generalization%20under%20cross-scanner%2C%20cross-modality%2C%20and%20cross-anatomy%20settings.%20Causal-SAM-LLM%20establishes%20a%20new%20state%20of%20the%20art%20in%20out-of-distribution%20%28OOD%29%20robustness%2C%20improving%20the%20average%20Dice%20score%20by%20up%20to%206.2%20points%20and%20reducing%20the%20Hausdorff%20Distance%20by%2015.8%20mm%20over%20the%20strongest%20baseline%2C%20all%20while%20using%20less%20than%209%25%20of%20the%20full%20model%27s%20trainable%20parameters.%20Our%20work%20charts%20a%20new%20course%20for%20building%20robust%2C%20efficient%2C%20and%20interactively%20controllable%20medical%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2507.03585v2&entry.124074799=Read"},
{"title": "Efficient On-Board Processing of Oblique UAV Video for Rapid Flood Extent Mapping", "author": "Vishisht Sharma and Sam Leroux and Lisa Landuyt and Nick Witvrouwen and Pieter Simoens", "abstract": "Effective disaster response relies on rapid disaster response, where oblique aerial video is the primary modality for initial scouting due to its ability to maximize spatial coverage and situational awareness in limited flight time. However, the on-board processing of high-resolution oblique streams is severely bottlenecked by the strict Size, Weight, and Power (SWaP) constraints of Unmanned Aerial Vehicles (UAVs). The computational density required to process these wide-field-of-view streams precludes low-latency inference on standard edge hardware. To address this, we propose Temporal Token Reuse (TTR), an adaptive inference framework capable of accelerating video segmentation on embedded devices. TTR exploits the intrinsic spatiotemporal redundancy of aerial video by formulating image patches as tokens; it utilizes a lightweight similarity metric to dynamically identify static regions and propagate their precomputed deep features, thereby bypassing redundant backbone computations. We validate the framework on standard benchmarks and a newly curated Oblique Floodwater Dataset designed for hydrological monitoring. Experimental results on edge-grade hardware demonstrate that TTR achieves a 30% reduction in inference latency with negligible degradation in segmentation accuracy (< 0.5% mIoU). These findings confirm that TTR effectively shifts the operational Pareto frontier, enabling high-fidelity, real-time oblique video understanding for time-critical remote sensing missions", "link": "http://arxiv.org/abs/2601.11290v1", "date": "2026-01-16", "relevancy": 2.2447, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5725}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5618}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20On-Board%20Processing%20of%20Oblique%20UAV%20Video%20for%20Rapid%20Flood%20Extent%20Mapping&body=Title%3A%20Efficient%20On-Board%20Processing%20of%20Oblique%20UAV%20Video%20for%20Rapid%20Flood%20Extent%20Mapping%0AAuthor%3A%20Vishisht%20Sharma%20and%20Sam%20Leroux%20and%20Lisa%20Landuyt%20and%20Nick%20Witvrouwen%20and%20Pieter%20Simoens%0AAbstract%3A%20Effective%20disaster%20response%20relies%20on%20rapid%20disaster%20response%2C%20where%20oblique%20aerial%20video%20is%20the%20primary%20modality%20for%20initial%20scouting%20due%20to%20its%20ability%20to%20maximize%20spatial%20coverage%20and%20situational%20awareness%20in%20limited%20flight%20time.%20However%2C%20the%20on-board%20processing%20of%20high-resolution%20oblique%20streams%20is%20severely%20bottlenecked%20by%20the%20strict%20Size%2C%20Weight%2C%20and%20Power%20%28SWaP%29%20constraints%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29.%20The%20computational%20density%20required%20to%20process%20these%20wide-field-of-view%20streams%20precludes%20low-latency%20inference%20on%20standard%20edge%20hardware.%20To%20address%20this%2C%20we%20propose%20Temporal%20Token%20Reuse%20%28TTR%29%2C%20an%20adaptive%20inference%20framework%20capable%20of%20accelerating%20video%20segmentation%20on%20embedded%20devices.%20TTR%20exploits%20the%20intrinsic%20spatiotemporal%20redundancy%20of%20aerial%20video%20by%20formulating%20image%20patches%20as%20tokens%3B%20it%20utilizes%20a%20lightweight%20similarity%20metric%20to%20dynamically%20identify%20static%20regions%20and%20propagate%20their%20precomputed%20deep%20features%2C%20thereby%20bypassing%20redundant%20backbone%20computations.%20We%20validate%20the%20framework%20on%20standard%20benchmarks%20and%20a%20newly%20curated%20Oblique%20Floodwater%20Dataset%20designed%20for%20hydrological%20monitoring.%20Experimental%20results%20on%20edge-grade%20hardware%20demonstrate%20that%20TTR%20achieves%20a%2030%25%20reduction%20in%20inference%20latency%20with%20negligible%20degradation%20in%20segmentation%20accuracy%20%28%3C%200.5%25%20mIoU%29.%20These%20findings%20confirm%20that%20TTR%20effectively%20shifts%20the%20operational%20Pareto%20frontier%2C%20enabling%20high-fidelity%2C%20real-time%20oblique%20video%20understanding%20for%20time-critical%20remote%20sensing%20missions%0ALink%3A%20http%3A//arxiv.org/abs/2601.11290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520On-Board%2520Processing%2520of%2520Oblique%2520UAV%2520Video%2520for%2520Rapid%2520Flood%2520Extent%2520Mapping%26entry.906535625%3DVishisht%2520Sharma%2520and%2520Sam%2520Leroux%2520and%2520Lisa%2520Landuyt%2520and%2520Nick%2520Witvrouwen%2520and%2520Pieter%2520Simoens%26entry.1292438233%3DEffective%2520disaster%2520response%2520relies%2520on%2520rapid%2520disaster%2520response%252C%2520where%2520oblique%2520aerial%2520video%2520is%2520the%2520primary%2520modality%2520for%2520initial%2520scouting%2520due%2520to%2520its%2520ability%2520to%2520maximize%2520spatial%2520coverage%2520and%2520situational%2520awareness%2520in%2520limited%2520flight%2520time.%2520However%252C%2520the%2520on-board%2520processing%2520of%2520high-resolution%2520oblique%2520streams%2520is%2520severely%2520bottlenecked%2520by%2520the%2520strict%2520Size%252C%2520Weight%252C%2520and%2520Power%2520%2528SWaP%2529%2520constraints%2520of%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529.%2520The%2520computational%2520density%2520required%2520to%2520process%2520these%2520wide-field-of-view%2520streams%2520precludes%2520low-latency%2520inference%2520on%2520standard%2520edge%2520hardware.%2520To%2520address%2520this%252C%2520we%2520propose%2520Temporal%2520Token%2520Reuse%2520%2528TTR%2529%252C%2520an%2520adaptive%2520inference%2520framework%2520capable%2520of%2520accelerating%2520video%2520segmentation%2520on%2520embedded%2520devices.%2520TTR%2520exploits%2520the%2520intrinsic%2520spatiotemporal%2520redundancy%2520of%2520aerial%2520video%2520by%2520formulating%2520image%2520patches%2520as%2520tokens%253B%2520it%2520utilizes%2520a%2520lightweight%2520similarity%2520metric%2520to%2520dynamically%2520identify%2520static%2520regions%2520and%2520propagate%2520their%2520precomputed%2520deep%2520features%252C%2520thereby%2520bypassing%2520redundant%2520backbone%2520computations.%2520We%2520validate%2520the%2520framework%2520on%2520standard%2520benchmarks%2520and%2520a%2520newly%2520curated%2520Oblique%2520Floodwater%2520Dataset%2520designed%2520for%2520hydrological%2520monitoring.%2520Experimental%2520results%2520on%2520edge-grade%2520hardware%2520demonstrate%2520that%2520TTR%2520achieves%2520a%252030%2525%2520reduction%2520in%2520inference%2520latency%2520with%2520negligible%2520degradation%2520in%2520segmentation%2520accuracy%2520%2528%253C%25200.5%2525%2520mIoU%2529.%2520These%2520findings%2520confirm%2520that%2520TTR%2520effectively%2520shifts%2520the%2520operational%2520Pareto%2520frontier%252C%2520enabling%2520high-fidelity%252C%2520real-time%2520oblique%2520video%2520understanding%2520for%2520time-critical%2520remote%2520sensing%2520missions%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20On-Board%20Processing%20of%20Oblique%20UAV%20Video%20for%20Rapid%20Flood%20Extent%20Mapping&entry.906535625=Vishisht%20Sharma%20and%20Sam%20Leroux%20and%20Lisa%20Landuyt%20and%20Nick%20Witvrouwen%20and%20Pieter%20Simoens&entry.1292438233=Effective%20disaster%20response%20relies%20on%20rapid%20disaster%20response%2C%20where%20oblique%20aerial%20video%20is%20the%20primary%20modality%20for%20initial%20scouting%20due%20to%20its%20ability%20to%20maximize%20spatial%20coverage%20and%20situational%20awareness%20in%20limited%20flight%20time.%20However%2C%20the%20on-board%20processing%20of%20high-resolution%20oblique%20streams%20is%20severely%20bottlenecked%20by%20the%20strict%20Size%2C%20Weight%2C%20and%20Power%20%28SWaP%29%20constraints%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29.%20The%20computational%20density%20required%20to%20process%20these%20wide-field-of-view%20streams%20precludes%20low-latency%20inference%20on%20standard%20edge%20hardware.%20To%20address%20this%2C%20we%20propose%20Temporal%20Token%20Reuse%20%28TTR%29%2C%20an%20adaptive%20inference%20framework%20capable%20of%20accelerating%20video%20segmentation%20on%20embedded%20devices.%20TTR%20exploits%20the%20intrinsic%20spatiotemporal%20redundancy%20of%20aerial%20video%20by%20formulating%20image%20patches%20as%20tokens%3B%20it%20utilizes%20a%20lightweight%20similarity%20metric%20to%20dynamically%20identify%20static%20regions%20and%20propagate%20their%20precomputed%20deep%20features%2C%20thereby%20bypassing%20redundant%20backbone%20computations.%20We%20validate%20the%20framework%20on%20standard%20benchmarks%20and%20a%20newly%20curated%20Oblique%20Floodwater%20Dataset%20designed%20for%20hydrological%20monitoring.%20Experimental%20results%20on%20edge-grade%20hardware%20demonstrate%20that%20TTR%20achieves%20a%2030%25%20reduction%20in%20inference%20latency%20with%20negligible%20degradation%20in%20segmentation%20accuracy%20%28%3C%200.5%25%20mIoU%29.%20These%20findings%20confirm%20that%20TTR%20effectively%20shifts%20the%20operational%20Pareto%20frontier%2C%20enabling%20high-fidelity%2C%20real-time%20oblique%20video%20understanding%20for%20time-critical%20remote%20sensing%20missions&entry.1838667208=http%3A//arxiv.org/abs/2601.11290v1&entry.124074799=Read"},
{"title": "Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding", "author": "Wenhui Tan and Ruihua Song and Jiaze Li and Jianzhong Ju and Zhenbo Luo", "abstract": "Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.", "link": "http://arxiv.org/abs/2601.11359v1", "date": "2026-01-16", "relevancy": 2.2388, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5634}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5627}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think-Clip-Sample%3A%20Slow-Fast%20Frame%20Selection%20for%20Video%20Understanding&body=Title%3A%20Think-Clip-Sample%3A%20Slow-Fast%20Frame%20Selection%20for%20Video%20Understanding%0AAuthor%3A%20Wenhui%20Tan%20and%20Ruihua%20Song%20and%20Jiaze%20Li%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%0AAbstract%3A%20Recent%20progress%20in%20multi-modal%20large%20language%20models%20%28MLLMs%29%20has%20significantly%20advanced%20video%20understanding.%20However%2C%20their%20performance%20on%20long-form%20videos%20remains%20limited%20by%20computational%20constraints%20and%20suboptimal%20frame%20selection.%20We%20present%20Think-Clip-Sample%20%28TCS%29%2C%20a%20training-free%20framework%20that%20enhances%20long%20video%20understanding%20through%20two%20key%20components%3A%20%28i%29%20Multi-Query%20Reasoning%2C%20which%20generates%20multiple%20queries%20to%20capture%20complementary%20aspects%20of%20the%20question%20and%20video%3B%20and%20%28ii%29%20Clip-level%20Slow-Fast%20Sampling%2C%20which%20adaptively%20balances%20dense%20local%20details%20and%20sparse%20global%20context.%20Extensive%20experiments%20on%20MLVU%2C%20LongVideoBench%2C%20and%20VideoMME%20demonstrate%20that%20TCS%20consistently%20improves%20performance%20across%20different%20MLLMs%2C%20boosting%20up%20to%206.9%25%20accuracy%2C%20and%20is%20capable%20of%20achieving%20comparable%20accuracy%20with%2050%25%20fewer%20inference%20time%20cost%2C%20highlighting%20both%20efficiency%20and%20efficacy%20of%20TCS%20on%20long%20video%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink-Clip-Sample%253A%2520Slow-Fast%2520Frame%2520Selection%2520for%2520Video%2520Understanding%26entry.906535625%3DWenhui%2520Tan%2520and%2520Ruihua%2520Song%2520and%2520Jiaze%2520Li%2520and%2520Jianzhong%2520Ju%2520and%2520Zhenbo%2520Luo%26entry.1292438233%3DRecent%2520progress%2520in%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520has%2520significantly%2520advanced%2520video%2520understanding.%2520However%252C%2520their%2520performance%2520on%2520long-form%2520videos%2520remains%2520limited%2520by%2520computational%2520constraints%2520and%2520suboptimal%2520frame%2520selection.%2520We%2520present%2520Think-Clip-Sample%2520%2528TCS%2529%252C%2520a%2520training-free%2520framework%2520that%2520enhances%2520long%2520video%2520understanding%2520through%2520two%2520key%2520components%253A%2520%2528i%2529%2520Multi-Query%2520Reasoning%252C%2520which%2520generates%2520multiple%2520queries%2520to%2520capture%2520complementary%2520aspects%2520of%2520the%2520question%2520and%2520video%253B%2520and%2520%2528ii%2529%2520Clip-level%2520Slow-Fast%2520Sampling%252C%2520which%2520adaptively%2520balances%2520dense%2520local%2520details%2520and%2520sparse%2520global%2520context.%2520Extensive%2520experiments%2520on%2520MLVU%252C%2520LongVideoBench%252C%2520and%2520VideoMME%2520demonstrate%2520that%2520TCS%2520consistently%2520improves%2520performance%2520across%2520different%2520MLLMs%252C%2520boosting%2520up%2520to%25206.9%2525%2520accuracy%252C%2520and%2520is%2520capable%2520of%2520achieving%2520comparable%2520accuracy%2520with%252050%2525%2520fewer%2520inference%2520time%2520cost%252C%2520highlighting%2520both%2520efficiency%2520and%2520efficacy%2520of%2520TCS%2520on%2520long%2520video%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think-Clip-Sample%3A%20Slow-Fast%20Frame%20Selection%20for%20Video%20Understanding&entry.906535625=Wenhui%20Tan%20and%20Ruihua%20Song%20and%20Jiaze%20Li%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo&entry.1292438233=Recent%20progress%20in%20multi-modal%20large%20language%20models%20%28MLLMs%29%20has%20significantly%20advanced%20video%20understanding.%20However%2C%20their%20performance%20on%20long-form%20videos%20remains%20limited%20by%20computational%20constraints%20and%20suboptimal%20frame%20selection.%20We%20present%20Think-Clip-Sample%20%28TCS%29%2C%20a%20training-free%20framework%20that%20enhances%20long%20video%20understanding%20through%20two%20key%20components%3A%20%28i%29%20Multi-Query%20Reasoning%2C%20which%20generates%20multiple%20queries%20to%20capture%20complementary%20aspects%20of%20the%20question%20and%20video%3B%20and%20%28ii%29%20Clip-level%20Slow-Fast%20Sampling%2C%20which%20adaptively%20balances%20dense%20local%20details%20and%20sparse%20global%20context.%20Extensive%20experiments%20on%20MLVU%2C%20LongVideoBench%2C%20and%20VideoMME%20demonstrate%20that%20TCS%20consistently%20improves%20performance%20across%20different%20MLLMs%2C%20boosting%20up%20to%206.9%25%20accuracy%2C%20and%20is%20capable%20of%20achieving%20comparable%20accuracy%20with%2050%25%20fewer%20inference%20time%20cost%2C%20highlighting%20both%20efficiency%20and%20efficacy%20of%20TCS%20on%20long%20video%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2601.11359v1&entry.124074799=Read"},
{"title": "Vision-Conditioned Variational Bayesian Last Layer Dynamics Models", "author": "Paul Brunzema and Thomas Lew and Ray Zhang and Takeru Shirasawa and John Subosits and Marcus Greiff", "abstract": "Agile control of robotic systems often requires anticipating how the environment affects system behavior. For example, a driver must perceive the road ahead to anticipate available friction and plan actions accordingly. Achieving such proactive adaptation within autonomous frameworks remains a challenge, particularly under rapidly changing conditions. Traditional modeling approaches often struggle to capture abrupt variations in system behavior, while adaptive methods are inherently reactive and may adapt too late to ensure safety. We propose a vision-conditioned variational Bayesian last-layer dynamics model that leverages visual context to anticipate changes in the environment. The model first learns nominal vehicle dynamics and is then fine-tuned with feature-wise affine transformations of latent features, enabling context-aware dynamics prediction. The resulting model is integrated into an optimal controller for vehicle racing. We validate our method on a Lexus LC500 racing through water puddles. With vision-conditioning, the system completed all 12 attempted laps under varying conditions. In contrast, all baselines without visual context consistently lost control, demonstrating the importance of proactive dynamics adaptation in high-performance applications.", "link": "http://arxiv.org/abs/2601.09178v2", "date": "2026-01-16", "relevancy": 2.2211, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6286}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5519}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Conditioned%20Variational%20Bayesian%20Last%20Layer%20Dynamics%20Models&body=Title%3A%20Vision-Conditioned%20Variational%20Bayesian%20Last%20Layer%20Dynamics%20Models%0AAuthor%3A%20Paul%20Brunzema%20and%20Thomas%20Lew%20and%20Ray%20Zhang%20and%20Takeru%20Shirasawa%20and%20John%20Subosits%20and%20Marcus%20Greiff%0AAbstract%3A%20Agile%20control%20of%20robotic%20systems%20often%20requires%20anticipating%20how%20the%20environment%20affects%20system%20behavior.%20For%20example%2C%20a%20driver%20must%20perceive%20the%20road%20ahead%20to%20anticipate%20available%20friction%20and%20plan%20actions%20accordingly.%20Achieving%20such%20proactive%20adaptation%20within%20autonomous%20frameworks%20remains%20a%20challenge%2C%20particularly%20under%20rapidly%20changing%20conditions.%20Traditional%20modeling%20approaches%20often%20struggle%20to%20capture%20abrupt%20variations%20in%20system%20behavior%2C%20while%20adaptive%20methods%20are%20inherently%20reactive%20and%20may%20adapt%20too%20late%20to%20ensure%20safety.%20We%20propose%20a%20vision-conditioned%20variational%20Bayesian%20last-layer%20dynamics%20model%20that%20leverages%20visual%20context%20to%20anticipate%20changes%20in%20the%20environment.%20The%20model%20first%20learns%20nominal%20vehicle%20dynamics%20and%20is%20then%20fine-tuned%20with%20feature-wise%20affine%20transformations%20of%20latent%20features%2C%20enabling%20context-aware%20dynamics%20prediction.%20The%20resulting%20model%20is%20integrated%20into%20an%20optimal%20controller%20for%20vehicle%20racing.%20We%20validate%20our%20method%20on%20a%20Lexus%20LC500%20racing%20through%20water%20puddles.%20With%20vision-conditioning%2C%20the%20system%20completed%20all%2012%20attempted%20laps%20under%20varying%20conditions.%20In%20contrast%2C%20all%20baselines%20without%20visual%20context%20consistently%20lost%20control%2C%20demonstrating%20the%20importance%20of%20proactive%20dynamics%20adaptation%20in%20high-performance%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Conditioned%2520Variational%2520Bayesian%2520Last%2520Layer%2520Dynamics%2520Models%26entry.906535625%3DPaul%2520Brunzema%2520and%2520Thomas%2520Lew%2520and%2520Ray%2520Zhang%2520and%2520Takeru%2520Shirasawa%2520and%2520John%2520Subosits%2520and%2520Marcus%2520Greiff%26entry.1292438233%3DAgile%2520control%2520of%2520robotic%2520systems%2520often%2520requires%2520anticipating%2520how%2520the%2520environment%2520affects%2520system%2520behavior.%2520For%2520example%252C%2520a%2520driver%2520must%2520perceive%2520the%2520road%2520ahead%2520to%2520anticipate%2520available%2520friction%2520and%2520plan%2520actions%2520accordingly.%2520Achieving%2520such%2520proactive%2520adaptation%2520within%2520autonomous%2520frameworks%2520remains%2520a%2520challenge%252C%2520particularly%2520under%2520rapidly%2520changing%2520conditions.%2520Traditional%2520modeling%2520approaches%2520often%2520struggle%2520to%2520capture%2520abrupt%2520variations%2520in%2520system%2520behavior%252C%2520while%2520adaptive%2520methods%2520are%2520inherently%2520reactive%2520and%2520may%2520adapt%2520too%2520late%2520to%2520ensure%2520safety.%2520We%2520propose%2520a%2520vision-conditioned%2520variational%2520Bayesian%2520last-layer%2520dynamics%2520model%2520that%2520leverages%2520visual%2520context%2520to%2520anticipate%2520changes%2520in%2520the%2520environment.%2520The%2520model%2520first%2520learns%2520nominal%2520vehicle%2520dynamics%2520and%2520is%2520then%2520fine-tuned%2520with%2520feature-wise%2520affine%2520transformations%2520of%2520latent%2520features%252C%2520enabling%2520context-aware%2520dynamics%2520prediction.%2520The%2520resulting%2520model%2520is%2520integrated%2520into%2520an%2520optimal%2520controller%2520for%2520vehicle%2520racing.%2520We%2520validate%2520our%2520method%2520on%2520a%2520Lexus%2520LC500%2520racing%2520through%2520water%2520puddles.%2520With%2520vision-conditioning%252C%2520the%2520system%2520completed%2520all%252012%2520attempted%2520laps%2520under%2520varying%2520conditions.%2520In%2520contrast%252C%2520all%2520baselines%2520without%2520visual%2520context%2520consistently%2520lost%2520control%252C%2520demonstrating%2520the%2520importance%2520of%2520proactive%2520dynamics%2520adaptation%2520in%2520high-performance%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Conditioned%20Variational%20Bayesian%20Last%20Layer%20Dynamics%20Models&entry.906535625=Paul%20Brunzema%20and%20Thomas%20Lew%20and%20Ray%20Zhang%20and%20Takeru%20Shirasawa%20and%20John%20Subosits%20and%20Marcus%20Greiff&entry.1292438233=Agile%20control%20of%20robotic%20systems%20often%20requires%20anticipating%20how%20the%20environment%20affects%20system%20behavior.%20For%20example%2C%20a%20driver%20must%20perceive%20the%20road%20ahead%20to%20anticipate%20available%20friction%20and%20plan%20actions%20accordingly.%20Achieving%20such%20proactive%20adaptation%20within%20autonomous%20frameworks%20remains%20a%20challenge%2C%20particularly%20under%20rapidly%20changing%20conditions.%20Traditional%20modeling%20approaches%20often%20struggle%20to%20capture%20abrupt%20variations%20in%20system%20behavior%2C%20while%20adaptive%20methods%20are%20inherently%20reactive%20and%20may%20adapt%20too%20late%20to%20ensure%20safety.%20We%20propose%20a%20vision-conditioned%20variational%20Bayesian%20last-layer%20dynamics%20model%20that%20leverages%20visual%20context%20to%20anticipate%20changes%20in%20the%20environment.%20The%20model%20first%20learns%20nominal%20vehicle%20dynamics%20and%20is%20then%20fine-tuned%20with%20feature-wise%20affine%20transformations%20of%20latent%20features%2C%20enabling%20context-aware%20dynamics%20prediction.%20The%20resulting%20model%20is%20integrated%20into%20an%20optimal%20controller%20for%20vehicle%20racing.%20We%20validate%20our%20method%20on%20a%20Lexus%20LC500%20racing%20through%20water%20puddles.%20With%20vision-conditioning%2C%20the%20system%20completed%20all%2012%20attempted%20laps%20under%20varying%20conditions.%20In%20contrast%2C%20all%20baselines%20without%20visual%20context%20consistently%20lost%20control%2C%20demonstrating%20the%20importance%20of%20proactive%20dynamics%20adaptation%20in%20high-performance%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.09178v2&entry.124074799=Read"},
{"title": "Latent Space Inference via Paired Autoencoders", "author": "Emma Hart and Bas Peters and Julianne Chung and Matthias Chung", "abstract": "This work describes a novel data-driven latent space inference framework built on paired autoencoders to handle observational inconsistencies when solving inverse problems. Our approach uses two autoencoders, one for the parameter space and one for the observation space, connected by learned mappings between the autoencoders' latent spaces. These mappings enable a surrogate for regularized inversion and optimization in low-dimensional, informative latent spaces. Our flexible framework can work with partial, noisy, or out-of-distribution data, all while maintaining consistency with the underlying physical models. The paired autoencoders enable reconstruction of corrupted data, and then use the reconstructed data for parameter estimation, which produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of the same architecture, especially in scenarios with data inconsistencies. We demonstrate our approaches on two imaging examples in medical tomography and geophysical seismic-waveform inversion, but the described approaches are broadly applicable to a variety of inverse problems in scientific and engineering applications.", "link": "http://arxiv.org/abs/2601.11397v1", "date": "2026-01-16", "relevancy": 2.2058, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6049}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5336}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Space%20Inference%20via%20Paired%20Autoencoders&body=Title%3A%20Latent%20Space%20Inference%20via%20Paired%20Autoencoders%0AAuthor%3A%20Emma%20Hart%20and%20Bas%20Peters%20and%20Julianne%20Chung%20and%20Matthias%20Chung%0AAbstract%3A%20This%20work%20describes%20a%20novel%20data-driven%20latent%20space%20inference%20framework%20built%20on%20paired%20autoencoders%20to%20handle%20observational%20inconsistencies%20when%20solving%20inverse%20problems.%20Our%20approach%20uses%20two%20autoencoders%2C%20one%20for%20the%20parameter%20space%20and%20one%20for%20the%20observation%20space%2C%20connected%20by%20learned%20mappings%20between%20the%20autoencoders%27%20latent%20spaces.%20These%20mappings%20enable%20a%20surrogate%20for%20regularized%20inversion%20and%20optimization%20in%20low-dimensional%2C%20informative%20latent%20spaces.%20Our%20flexible%20framework%20can%20work%20with%20partial%2C%20noisy%2C%20or%20out-of-distribution%20data%2C%20all%20while%20maintaining%20consistency%20with%20the%20underlying%20physical%20models.%20The%20paired%20autoencoders%20enable%20reconstruction%20of%20corrupted%20data%2C%20and%20then%20use%20the%20reconstructed%20data%20for%20parameter%20estimation%2C%20which%20produces%20more%20accurate%20reconstructions%20compared%20to%20paired%20autoencoders%20alone%20and%20end-to-end%20encoder-decoders%20of%20the%20same%20architecture%2C%20especially%20in%20scenarios%20with%20data%20inconsistencies.%20We%20demonstrate%20our%20approaches%20on%20two%20imaging%20examples%20in%20medical%20tomography%20and%20geophysical%20seismic-waveform%20inversion%2C%20but%20the%20described%20approaches%20are%20broadly%20applicable%20to%20a%20variety%20of%20inverse%20problems%20in%20scientific%20and%20engineering%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Space%2520Inference%2520via%2520Paired%2520Autoencoders%26entry.906535625%3DEmma%2520Hart%2520and%2520Bas%2520Peters%2520and%2520Julianne%2520Chung%2520and%2520Matthias%2520Chung%26entry.1292438233%3DThis%2520work%2520describes%2520a%2520novel%2520data-driven%2520latent%2520space%2520inference%2520framework%2520built%2520on%2520paired%2520autoencoders%2520to%2520handle%2520observational%2520inconsistencies%2520when%2520solving%2520inverse%2520problems.%2520Our%2520approach%2520uses%2520two%2520autoencoders%252C%2520one%2520for%2520the%2520parameter%2520space%2520and%2520one%2520for%2520the%2520observation%2520space%252C%2520connected%2520by%2520learned%2520mappings%2520between%2520the%2520autoencoders%2527%2520latent%2520spaces.%2520These%2520mappings%2520enable%2520a%2520surrogate%2520for%2520regularized%2520inversion%2520and%2520optimization%2520in%2520low-dimensional%252C%2520informative%2520latent%2520spaces.%2520Our%2520flexible%2520framework%2520can%2520work%2520with%2520partial%252C%2520noisy%252C%2520or%2520out-of-distribution%2520data%252C%2520all%2520while%2520maintaining%2520consistency%2520with%2520the%2520underlying%2520physical%2520models.%2520The%2520paired%2520autoencoders%2520enable%2520reconstruction%2520of%2520corrupted%2520data%252C%2520and%2520then%2520use%2520the%2520reconstructed%2520data%2520for%2520parameter%2520estimation%252C%2520which%2520produces%2520more%2520accurate%2520reconstructions%2520compared%2520to%2520paired%2520autoencoders%2520alone%2520and%2520end-to-end%2520encoder-decoders%2520of%2520the%2520same%2520architecture%252C%2520especially%2520in%2520scenarios%2520with%2520data%2520inconsistencies.%2520We%2520demonstrate%2520our%2520approaches%2520on%2520two%2520imaging%2520examples%2520in%2520medical%2520tomography%2520and%2520geophysical%2520seismic-waveform%2520inversion%252C%2520but%2520the%2520described%2520approaches%2520are%2520broadly%2520applicable%2520to%2520a%2520variety%2520of%2520inverse%2520problems%2520in%2520scientific%2520and%2520engineering%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Space%20Inference%20via%20Paired%20Autoencoders&entry.906535625=Emma%20Hart%20and%20Bas%20Peters%20and%20Julianne%20Chung%20and%20Matthias%20Chung&entry.1292438233=This%20work%20describes%20a%20novel%20data-driven%20latent%20space%20inference%20framework%20built%20on%20paired%20autoencoders%20to%20handle%20observational%20inconsistencies%20when%20solving%20inverse%20problems.%20Our%20approach%20uses%20two%20autoencoders%2C%20one%20for%20the%20parameter%20space%20and%20one%20for%20the%20observation%20space%2C%20connected%20by%20learned%20mappings%20between%20the%20autoencoders%27%20latent%20spaces.%20These%20mappings%20enable%20a%20surrogate%20for%20regularized%20inversion%20and%20optimization%20in%20low-dimensional%2C%20informative%20latent%20spaces.%20Our%20flexible%20framework%20can%20work%20with%20partial%2C%20noisy%2C%20or%20out-of-distribution%20data%2C%20all%20while%20maintaining%20consistency%20with%20the%20underlying%20physical%20models.%20The%20paired%20autoencoders%20enable%20reconstruction%20of%20corrupted%20data%2C%20and%20then%20use%20the%20reconstructed%20data%20for%20parameter%20estimation%2C%20which%20produces%20more%20accurate%20reconstructions%20compared%20to%20paired%20autoencoders%20alone%20and%20end-to-end%20encoder-decoders%20of%20the%20same%20architecture%2C%20especially%20in%20scenarios%20with%20data%20inconsistencies.%20We%20demonstrate%20our%20approaches%20on%20two%20imaging%20examples%20in%20medical%20tomography%20and%20geophysical%20seismic-waveform%20inversion%2C%20but%20the%20described%20approaches%20are%20broadly%20applicable%20to%20a%20variety%20of%20inverse%20problems%20in%20scientific%20and%20engineering%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.11397v1&entry.124074799=Read"},
{"title": "LoRA as Oracle", "author": "Marco Arazzi and Antonino Nocera", "abstract": "Backdoored and privacy-leaking deep neural networks pose a serious threat to the deployment of machine learning systems in security-critical settings. Existing defenses for backdoor detection and membership inference typically require access to clean reference models, extensive retraining, or strong assumptions about the attack mechanism. In this work, we introduce a novel LoRA-based oracle framework that leverages low-rank adaptation modules as a lightweight, model-agnostic probe for both backdoor detection and membership inference.\n  Our approach attaches task-specific LoRA adapters to a frozen backbone and analyzes their optimization dynamics and representation shifts when exposed to suspicious samples. We show that poisoned and member samples induce distinctive low-rank updates that differ significantly from those generated by clean or non-member data. These signals can be measured using simple ranking and energy-based statistics, enabling reliable inference without access to the original training data or modification of the deployed model.", "link": "http://arxiv.org/abs/2601.11207v1", "date": "2026-01-16", "relevancy": 2.199, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.45}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA%20as%20Oracle&body=Title%3A%20LoRA%20as%20Oracle%0AAuthor%3A%20Marco%20Arazzi%20and%20Antonino%20Nocera%0AAbstract%3A%20Backdoored%20and%20privacy-leaking%20deep%20neural%20networks%20pose%20a%20serious%20threat%20to%20the%20deployment%20of%20machine%20learning%20systems%20in%20security-critical%20settings.%20Existing%20defenses%20for%20backdoor%20detection%20and%20membership%20inference%20typically%20require%20access%20to%20clean%20reference%20models%2C%20extensive%20retraining%2C%20or%20strong%20assumptions%20about%20the%20attack%20mechanism.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20LoRA-based%20oracle%20framework%20that%20leverages%20low-rank%20adaptation%20modules%20as%20a%20lightweight%2C%20model-agnostic%20probe%20for%20both%20backdoor%20detection%20and%20membership%20inference.%0A%20%20Our%20approach%20attaches%20task-specific%20LoRA%20adapters%20to%20a%20frozen%20backbone%20and%20analyzes%20their%20optimization%20dynamics%20and%20representation%20shifts%20when%20exposed%20to%20suspicious%20samples.%20We%20show%20that%20poisoned%20and%20member%20samples%20induce%20distinctive%20low-rank%20updates%20that%20differ%20significantly%20from%20those%20generated%20by%20clean%20or%20non-member%20data.%20These%20signals%20can%20be%20measured%20using%20simple%20ranking%20and%20energy-based%20statistics%2C%20enabling%20reliable%20inference%20without%20access%20to%20the%20original%20training%20data%20or%20modification%20of%20the%20deployed%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA%2520as%2520Oracle%26entry.906535625%3DMarco%2520Arazzi%2520and%2520Antonino%2520Nocera%26entry.1292438233%3DBackdoored%2520and%2520privacy-leaking%2520deep%2520neural%2520networks%2520pose%2520a%2520serious%2520threat%2520to%2520the%2520deployment%2520of%2520machine%2520learning%2520systems%2520in%2520security-critical%2520settings.%2520Existing%2520defenses%2520for%2520backdoor%2520detection%2520and%2520membership%2520inference%2520typically%2520require%2520access%2520to%2520clean%2520reference%2520models%252C%2520extensive%2520retraining%252C%2520or%2520strong%2520assumptions%2520about%2520the%2520attack%2520mechanism.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520LoRA-based%2520oracle%2520framework%2520that%2520leverages%2520low-rank%2520adaptation%2520modules%2520as%2520a%2520lightweight%252C%2520model-agnostic%2520probe%2520for%2520both%2520backdoor%2520detection%2520and%2520membership%2520inference.%250A%2520%2520Our%2520approach%2520attaches%2520task-specific%2520LoRA%2520adapters%2520to%2520a%2520frozen%2520backbone%2520and%2520analyzes%2520their%2520optimization%2520dynamics%2520and%2520representation%2520shifts%2520when%2520exposed%2520to%2520suspicious%2520samples.%2520We%2520show%2520that%2520poisoned%2520and%2520member%2520samples%2520induce%2520distinctive%2520low-rank%2520updates%2520that%2520differ%2520significantly%2520from%2520those%2520generated%2520by%2520clean%2520or%2520non-member%2520data.%2520These%2520signals%2520can%2520be%2520measured%2520using%2520simple%2520ranking%2520and%2520energy-based%2520statistics%252C%2520enabling%2520reliable%2520inference%2520without%2520access%2520to%2520the%2520original%2520training%2520data%2520or%2520modification%2520of%2520the%2520deployed%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA%20as%20Oracle&entry.906535625=Marco%20Arazzi%20and%20Antonino%20Nocera&entry.1292438233=Backdoored%20and%20privacy-leaking%20deep%20neural%20networks%20pose%20a%20serious%20threat%20to%20the%20deployment%20of%20machine%20learning%20systems%20in%20security-critical%20settings.%20Existing%20defenses%20for%20backdoor%20detection%20and%20membership%20inference%20typically%20require%20access%20to%20clean%20reference%20models%2C%20extensive%20retraining%2C%20or%20strong%20assumptions%20about%20the%20attack%20mechanism.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20LoRA-based%20oracle%20framework%20that%20leverages%20low-rank%20adaptation%20modules%20as%20a%20lightweight%2C%20model-agnostic%20probe%20for%20both%20backdoor%20detection%20and%20membership%20inference.%0A%20%20Our%20approach%20attaches%20task-specific%20LoRA%20adapters%20to%20a%20frozen%20backbone%20and%20analyzes%20their%20optimization%20dynamics%20and%20representation%20shifts%20when%20exposed%20to%20suspicious%20samples.%20We%20show%20that%20poisoned%20and%20member%20samples%20induce%20distinctive%20low-rank%20updates%20that%20differ%20significantly%20from%20those%20generated%20by%20clean%20or%20non-member%20data.%20These%20signals%20can%20be%20measured%20using%20simple%20ranking%20and%20energy-based%20statistics%2C%20enabling%20reliable%20inference%20without%20access%20to%20the%20original%20training%20data%20or%20modification%20of%20the%20deployed%20model.&entry.1838667208=http%3A//arxiv.org/abs/2601.11207v1&entry.124074799=Read"},
{"title": "LSTM VS. Feed-Forward Autoencoders for Unsupervised Fault Detection in Hydraulic Pumps", "author": "P. S\u00e1nchez and K. Reyes and B. Radu and E. Fern\u00e1ndez", "abstract": "Unplanned failures in industrial hydraulic pumps can halt production and incur substantial costs. We explore two unsupervised autoencoder (AE) schemes for early fault detection: a feed-forward model that analyses individual sensor snapshots and a Long Short-Term Memory (LSTM) model that captures short temporal windows. Both networks are trained only on healthy data drawn from a minute-level log of 52 sensor channels; evaluation uses a separate set that contains seven annotated fault intervals. Despite the absence of fault samples during training, the models achieve high reliability.", "link": "http://arxiv.org/abs/2601.11163v1", "date": "2026-01-16", "relevancy": 2.1926, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4531}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.432}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LSTM%20VS.%20Feed-Forward%20Autoencoders%20for%20Unsupervised%20Fault%20Detection%20in%20Hydraulic%20Pumps&body=Title%3A%20LSTM%20VS.%20Feed-Forward%20Autoencoders%20for%20Unsupervised%20Fault%20Detection%20in%20Hydraulic%20Pumps%0AAuthor%3A%20P.%20S%C3%A1nchez%20and%20K.%20Reyes%20and%20B.%20Radu%20and%20E.%20Fern%C3%A1ndez%0AAbstract%3A%20Unplanned%20failures%20in%20industrial%20hydraulic%20pumps%20can%20halt%20production%20and%20incur%20substantial%20costs.%20We%20explore%20two%20unsupervised%20autoencoder%20%28AE%29%20schemes%20for%20early%20fault%20detection%3A%20a%20feed-forward%20model%20that%20analyses%20individual%20sensor%20snapshots%20and%20a%20Long%20Short-Term%20Memory%20%28LSTM%29%20model%20that%20captures%20short%20temporal%20windows.%20Both%20networks%20are%20trained%20only%20on%20healthy%20data%20drawn%20from%20a%20minute-level%20log%20of%2052%20sensor%20channels%3B%20evaluation%20uses%20a%20separate%20set%20that%20contains%20seven%20annotated%20fault%20intervals.%20Despite%20the%20absence%20of%20fault%20samples%20during%20training%2C%20the%20models%20achieve%20high%20reliability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLSTM%2520VS.%2520Feed-Forward%2520Autoencoders%2520for%2520Unsupervised%2520Fault%2520Detection%2520in%2520Hydraulic%2520Pumps%26entry.906535625%3DP.%2520S%25C3%25A1nchez%2520and%2520K.%2520Reyes%2520and%2520B.%2520Radu%2520and%2520E.%2520Fern%25C3%25A1ndez%26entry.1292438233%3DUnplanned%2520failures%2520in%2520industrial%2520hydraulic%2520pumps%2520can%2520halt%2520production%2520and%2520incur%2520substantial%2520costs.%2520We%2520explore%2520two%2520unsupervised%2520autoencoder%2520%2528AE%2529%2520schemes%2520for%2520early%2520fault%2520detection%253A%2520a%2520feed-forward%2520model%2520that%2520analyses%2520individual%2520sensor%2520snapshots%2520and%2520a%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520model%2520that%2520captures%2520short%2520temporal%2520windows.%2520Both%2520networks%2520are%2520trained%2520only%2520on%2520healthy%2520data%2520drawn%2520from%2520a%2520minute-level%2520log%2520of%252052%2520sensor%2520channels%253B%2520evaluation%2520uses%2520a%2520separate%2520set%2520that%2520contains%2520seven%2520annotated%2520fault%2520intervals.%2520Despite%2520the%2520absence%2520of%2520fault%2520samples%2520during%2520training%252C%2520the%2520models%2520achieve%2520high%2520reliability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LSTM%20VS.%20Feed-Forward%20Autoencoders%20for%20Unsupervised%20Fault%20Detection%20in%20Hydraulic%20Pumps&entry.906535625=P.%20S%C3%A1nchez%20and%20K.%20Reyes%20and%20B.%20Radu%20and%20E.%20Fern%C3%A1ndez&entry.1292438233=Unplanned%20failures%20in%20industrial%20hydraulic%20pumps%20can%20halt%20production%20and%20incur%20substantial%20costs.%20We%20explore%20two%20unsupervised%20autoencoder%20%28AE%29%20schemes%20for%20early%20fault%20detection%3A%20a%20feed-forward%20model%20that%20analyses%20individual%20sensor%20snapshots%20and%20a%20Long%20Short-Term%20Memory%20%28LSTM%29%20model%20that%20captures%20short%20temporal%20windows.%20Both%20networks%20are%20trained%20only%20on%20healthy%20data%20drawn%20from%20a%20minute-level%20log%20of%2052%20sensor%20channels%3B%20evaluation%20uses%20a%20separate%20set%20that%20contains%20seven%20annotated%20fault%20intervals.%20Despite%20the%20absence%20of%20fault%20samples%20during%20training%2C%20the%20models%20achieve%20high%20reliability.&entry.1838667208=http%3A//arxiv.org/abs/2601.11163v1&entry.124074799=Read"},
{"title": "FTDMamba: Frequency-Assisted Temporal Dilation Mamba for Unmanned Aerial Vehicle Video Anomaly Detection", "author": "Cheng-Zhuang Liu and Si-Bao Chen and Qing-Ling Shu and Chris Ding and Jin Tang and Bin Luo", "abstract": "Recent advances in video anomaly detection (VAD) mainly focus on ground-based surveillance or unmanned aerial vehicle (UAV) videos with static backgrounds, whereas research on UAV videos with dynamic backgrounds remains limited. Unlike static scenarios, dynamically captured UAV videos exhibit multi-source motion coupling, where the motion of objects and UAV-induced global motion are intricately intertwined. Consequently, existing methods may misclassify normal UAV movements as anomalies or fail to capture true anomalies concealed within dynamic backgrounds. Moreover, many approaches do not adequately address the joint modeling of inter-frame continuity and local spatial correlations across diverse temporal scales. To overcome these limitations, we propose the Frequency-Assisted Temporal Dilation Mamba (FTDMamba) network for UAV VAD, including two core components: (1) a Frequency Decoupled Spatiotemporal Correlation Module, which disentangles coupled motion patterns and models global spatiotemporal dependencies through frequency analysis; and (2) a Temporal Dilation Mamba Module, which leverages Mamba's sequence modeling capability to jointly learn fine-grained temporal dynamics and local spatial structures across multiple temporal receptive fields. Additionally, unlike existing UAV VAD datasets which focus on static backgrounds, we construct a large-scale Moving UAV VAD dataset (MUVAD), comprising 222,736 frames with 240 anomaly events across 12 anomaly types. Extensive experiments demonstrate that FTDMamba achieves state-of-the-art (SOTA) performance on two public static benchmarks and the new MUVAD dataset. The code and MUVAD dataset will be available at: https://github.com/uavano/FTDMamba.", "link": "http://arxiv.org/abs/2601.11254v1", "date": "2026-01-16", "relevancy": 2.1916, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5564}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5479}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FTDMamba%3A%20Frequency-Assisted%20Temporal%20Dilation%20Mamba%20for%20Unmanned%20Aerial%20Vehicle%20Video%20Anomaly%20Detection&body=Title%3A%20FTDMamba%3A%20Frequency-Assisted%20Temporal%20Dilation%20Mamba%20for%20Unmanned%20Aerial%20Vehicle%20Video%20Anomaly%20Detection%0AAuthor%3A%20Cheng-Zhuang%20Liu%20and%20Si-Bao%20Chen%20and%20Qing-Ling%20Shu%20and%20Chris%20Ding%20and%20Jin%20Tang%20and%20Bin%20Luo%0AAbstract%3A%20Recent%20advances%20in%20video%20anomaly%20detection%20%28VAD%29%20mainly%20focus%20on%20ground-based%20surveillance%20or%20unmanned%20aerial%20vehicle%20%28UAV%29%20videos%20with%20static%20backgrounds%2C%20whereas%20research%20on%20UAV%20videos%20with%20dynamic%20backgrounds%20remains%20limited.%20Unlike%20static%20scenarios%2C%20dynamically%20captured%20UAV%20videos%20exhibit%20multi-source%20motion%20coupling%2C%20where%20the%20motion%20of%20objects%20and%20UAV-induced%20global%20motion%20are%20intricately%20intertwined.%20Consequently%2C%20existing%20methods%20may%20misclassify%20normal%20UAV%20movements%20as%20anomalies%20or%20fail%20to%20capture%20true%20anomalies%20concealed%20within%20dynamic%20backgrounds.%20Moreover%2C%20many%20approaches%20do%20not%20adequately%20address%20the%20joint%20modeling%20of%20inter-frame%20continuity%20and%20local%20spatial%20correlations%20across%20diverse%20temporal%20scales.%20To%20overcome%20these%20limitations%2C%20we%20propose%20the%20Frequency-Assisted%20Temporal%20Dilation%20Mamba%20%28FTDMamba%29%20network%20for%20UAV%20VAD%2C%20including%20two%20core%20components%3A%20%281%29%20a%20Frequency%20Decoupled%20Spatiotemporal%20Correlation%20Module%2C%20which%20disentangles%20coupled%20motion%20patterns%20and%20models%20global%20spatiotemporal%20dependencies%20through%20frequency%20analysis%3B%20and%20%282%29%20a%20Temporal%20Dilation%20Mamba%20Module%2C%20which%20leverages%20Mamba%27s%20sequence%20modeling%20capability%20to%20jointly%20learn%20fine-grained%20temporal%20dynamics%20and%20local%20spatial%20structures%20across%20multiple%20temporal%20receptive%20fields.%20Additionally%2C%20unlike%20existing%20UAV%20VAD%20datasets%20which%20focus%20on%20static%20backgrounds%2C%20we%20construct%20a%20large-scale%20Moving%20UAV%20VAD%20dataset%20%28MUVAD%29%2C%20comprising%20222%2C736%20frames%20with%20240%20anomaly%20events%20across%2012%20anomaly%20types.%20Extensive%20experiments%20demonstrate%20that%20FTDMamba%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20two%20public%20static%20benchmarks%20and%20the%20new%20MUVAD%20dataset.%20The%20code%20and%20MUVAD%20dataset%20will%20be%20available%20at%3A%20https%3A//github.com/uavano/FTDMamba.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFTDMamba%253A%2520Frequency-Assisted%2520Temporal%2520Dilation%2520Mamba%2520for%2520Unmanned%2520Aerial%2520Vehicle%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DCheng-Zhuang%2520Liu%2520and%2520Si-Bao%2520Chen%2520and%2520Qing-Ling%2520Shu%2520and%2520Chris%2520Ding%2520and%2520Jin%2520Tang%2520and%2520Bin%2520Luo%26entry.1292438233%3DRecent%2520advances%2520in%2520video%2520anomaly%2520detection%2520%2528VAD%2529%2520mainly%2520focus%2520on%2520ground-based%2520surveillance%2520or%2520unmanned%2520aerial%2520vehicle%2520%2528UAV%2529%2520videos%2520with%2520static%2520backgrounds%252C%2520whereas%2520research%2520on%2520UAV%2520videos%2520with%2520dynamic%2520backgrounds%2520remains%2520limited.%2520Unlike%2520static%2520scenarios%252C%2520dynamically%2520captured%2520UAV%2520videos%2520exhibit%2520multi-source%2520motion%2520coupling%252C%2520where%2520the%2520motion%2520of%2520objects%2520and%2520UAV-induced%2520global%2520motion%2520are%2520intricately%2520intertwined.%2520Consequently%252C%2520existing%2520methods%2520may%2520misclassify%2520normal%2520UAV%2520movements%2520as%2520anomalies%2520or%2520fail%2520to%2520capture%2520true%2520anomalies%2520concealed%2520within%2520dynamic%2520backgrounds.%2520Moreover%252C%2520many%2520approaches%2520do%2520not%2520adequately%2520address%2520the%2520joint%2520modeling%2520of%2520inter-frame%2520continuity%2520and%2520local%2520spatial%2520correlations%2520across%2520diverse%2520temporal%2520scales.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520the%2520Frequency-Assisted%2520Temporal%2520Dilation%2520Mamba%2520%2528FTDMamba%2529%2520network%2520for%2520UAV%2520VAD%252C%2520including%2520two%2520core%2520components%253A%2520%25281%2529%2520a%2520Frequency%2520Decoupled%2520Spatiotemporal%2520Correlation%2520Module%252C%2520which%2520disentangles%2520coupled%2520motion%2520patterns%2520and%2520models%2520global%2520spatiotemporal%2520dependencies%2520through%2520frequency%2520analysis%253B%2520and%2520%25282%2529%2520a%2520Temporal%2520Dilation%2520Mamba%2520Module%252C%2520which%2520leverages%2520Mamba%2527s%2520sequence%2520modeling%2520capability%2520to%2520jointly%2520learn%2520fine-grained%2520temporal%2520dynamics%2520and%2520local%2520spatial%2520structures%2520across%2520multiple%2520temporal%2520receptive%2520fields.%2520Additionally%252C%2520unlike%2520existing%2520UAV%2520VAD%2520datasets%2520which%2520focus%2520on%2520static%2520backgrounds%252C%2520we%2520construct%2520a%2520large-scale%2520Moving%2520UAV%2520VAD%2520dataset%2520%2528MUVAD%2529%252C%2520comprising%2520222%252C736%2520frames%2520with%2520240%2520anomaly%2520events%2520across%252012%2520anomaly%2520types.%2520Extensive%2520experiments%2520demonstrate%2520that%2520FTDMamba%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520on%2520two%2520public%2520static%2520benchmarks%2520and%2520the%2520new%2520MUVAD%2520dataset.%2520The%2520code%2520and%2520MUVAD%2520dataset%2520will%2520be%2520available%2520at%253A%2520https%253A//github.com/uavano/FTDMamba.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FTDMamba%3A%20Frequency-Assisted%20Temporal%20Dilation%20Mamba%20for%20Unmanned%20Aerial%20Vehicle%20Video%20Anomaly%20Detection&entry.906535625=Cheng-Zhuang%20Liu%20and%20Si-Bao%20Chen%20and%20Qing-Ling%20Shu%20and%20Chris%20Ding%20and%20Jin%20Tang%20and%20Bin%20Luo&entry.1292438233=Recent%20advances%20in%20video%20anomaly%20detection%20%28VAD%29%20mainly%20focus%20on%20ground-based%20surveillance%20or%20unmanned%20aerial%20vehicle%20%28UAV%29%20videos%20with%20static%20backgrounds%2C%20whereas%20research%20on%20UAV%20videos%20with%20dynamic%20backgrounds%20remains%20limited.%20Unlike%20static%20scenarios%2C%20dynamically%20captured%20UAV%20videos%20exhibit%20multi-source%20motion%20coupling%2C%20where%20the%20motion%20of%20objects%20and%20UAV-induced%20global%20motion%20are%20intricately%20intertwined.%20Consequently%2C%20existing%20methods%20may%20misclassify%20normal%20UAV%20movements%20as%20anomalies%20or%20fail%20to%20capture%20true%20anomalies%20concealed%20within%20dynamic%20backgrounds.%20Moreover%2C%20many%20approaches%20do%20not%20adequately%20address%20the%20joint%20modeling%20of%20inter-frame%20continuity%20and%20local%20spatial%20correlations%20across%20diverse%20temporal%20scales.%20To%20overcome%20these%20limitations%2C%20we%20propose%20the%20Frequency-Assisted%20Temporal%20Dilation%20Mamba%20%28FTDMamba%29%20network%20for%20UAV%20VAD%2C%20including%20two%20core%20components%3A%20%281%29%20a%20Frequency%20Decoupled%20Spatiotemporal%20Correlation%20Module%2C%20which%20disentangles%20coupled%20motion%20patterns%20and%20models%20global%20spatiotemporal%20dependencies%20through%20frequency%20analysis%3B%20and%20%282%29%20a%20Temporal%20Dilation%20Mamba%20Module%2C%20which%20leverages%20Mamba%27s%20sequence%20modeling%20capability%20to%20jointly%20learn%20fine-grained%20temporal%20dynamics%20and%20local%20spatial%20structures%20across%20multiple%20temporal%20receptive%20fields.%20Additionally%2C%20unlike%20existing%20UAV%20VAD%20datasets%20which%20focus%20on%20static%20backgrounds%2C%20we%20construct%20a%20large-scale%20Moving%20UAV%20VAD%20dataset%20%28MUVAD%29%2C%20comprising%20222%2C736%20frames%20with%20240%20anomaly%20events%20across%2012%20anomaly%20types.%20Extensive%20experiments%20demonstrate%20that%20FTDMamba%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20two%20public%20static%20benchmarks%20and%20the%20new%20MUVAD%20dataset.%20The%20code%20and%20MUVAD%20dataset%20will%20be%20available%20at%3A%20https%3A//github.com/uavano/FTDMamba.&entry.1838667208=http%3A//arxiv.org/abs/2601.11254v1&entry.124074799=Read"},
{"title": "Latent Dynamics Graph Convolutional Networks for model order reduction of parameterized time-dependent PDEs", "author": "Lorenzo Tomada and Federico Pichi and Gianluigi Rozza", "abstract": "Graph Neural Networks (GNNs) are emerging as powerful tools for nonlinear Model Order Reduction (MOR) of time-dependent parameterized Partial Differential Equations (PDEs). However, existing methodologies struggle to combine geometric inductive biases with interpretable latent behavior, overlooking dynamics-driven features or disregarding spatial information. In this work, we address this gap by introducing Latent Dynamics Graph Convolutional Network (LD-GCN), a purely data-driven, encoder-free architecture that learns a global, low-dimensional representation of dynamical systems conditioned on external inputs and parameters. The temporal evolution is modeled in the latent space and advanced through time-stepping, allowing for time-extrapolation, and the trajectories are consistently decoded onto geometrically parameterized domains using a GNN. Our framework enhances interpretability by enabling the analysis of the reduced dynamics and supporting zero-shot prediction through latent interpolation. The methodology is mathematically validated via a universal approximation theorem for encoder-free architectures, and numerically tested on complex computational mechanics problems involving physical and geometric parameters, including the detection of bifurcating phenomena for Navier-Stokes equations. Code availability: https://github.com/lorenzotomada/ld-gcn-rom", "link": "http://arxiv.org/abs/2601.11259v1", "date": "2026-01-16", "relevancy": 2.1805, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5578}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5403}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Dynamics%20Graph%20Convolutional%20Networks%20for%20model%20order%20reduction%20of%20parameterized%20time-dependent%20PDEs&body=Title%3A%20Latent%20Dynamics%20Graph%20Convolutional%20Networks%20for%20model%20order%20reduction%20of%20parameterized%20time-dependent%20PDEs%0AAuthor%3A%20Lorenzo%20Tomada%20and%20Federico%20Pichi%20and%20Gianluigi%20Rozza%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20emerging%20as%20powerful%20tools%20for%20nonlinear%20Model%20Order%20Reduction%20%28MOR%29%20of%20time-dependent%20parameterized%20Partial%20Differential%20Equations%20%28PDEs%29.%20However%2C%20existing%20methodologies%20struggle%20to%20combine%20geometric%20inductive%20biases%20with%20interpretable%20latent%20behavior%2C%20overlooking%20dynamics-driven%20features%20or%20disregarding%20spatial%20information.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20introducing%20Latent%20Dynamics%20Graph%20Convolutional%20Network%20%28LD-GCN%29%2C%20a%20purely%20data-driven%2C%20encoder-free%20architecture%20that%20learns%20a%20global%2C%20low-dimensional%20representation%20of%20dynamical%20systems%20conditioned%20on%20external%20inputs%20and%20parameters.%20The%20temporal%20evolution%20is%20modeled%20in%20the%20latent%20space%20and%20advanced%20through%20time-stepping%2C%20allowing%20for%20time-extrapolation%2C%20and%20the%20trajectories%20are%20consistently%20decoded%20onto%20geometrically%20parameterized%20domains%20using%20a%20GNN.%20Our%20framework%20enhances%20interpretability%20by%20enabling%20the%20analysis%20of%20the%20reduced%20dynamics%20and%20supporting%20zero-shot%20prediction%20through%20latent%20interpolation.%20The%20methodology%20is%20mathematically%20validated%20via%20a%20universal%20approximation%20theorem%20for%20encoder-free%20architectures%2C%20and%20numerically%20tested%20on%20complex%20computational%20mechanics%20problems%20involving%20physical%20and%20geometric%20parameters%2C%20including%20the%20detection%20of%20bifurcating%20phenomena%20for%20Navier-Stokes%20equations.%20Code%20availability%3A%20https%3A//github.com/lorenzotomada/ld-gcn-rom%0ALink%3A%20http%3A//arxiv.org/abs/2601.11259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Dynamics%2520Graph%2520Convolutional%2520Networks%2520for%2520model%2520order%2520reduction%2520of%2520parameterized%2520time-dependent%2520PDEs%26entry.906535625%3DLorenzo%2520Tomada%2520and%2520Federico%2520Pichi%2520and%2520Gianluigi%2520Rozza%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520emerging%2520as%2520powerful%2520tools%2520for%2520nonlinear%2520Model%2520Order%2520Reduction%2520%2528MOR%2529%2520of%2520time-dependent%2520parameterized%2520Partial%2520Differential%2520Equations%2520%2528PDEs%2529.%2520However%252C%2520existing%2520methodologies%2520struggle%2520to%2520combine%2520geometric%2520inductive%2520biases%2520with%2520interpretable%2520latent%2520behavior%252C%2520overlooking%2520dynamics-driven%2520features%2520or%2520disregarding%2520spatial%2520information.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520introducing%2520Latent%2520Dynamics%2520Graph%2520Convolutional%2520Network%2520%2528LD-GCN%2529%252C%2520a%2520purely%2520data-driven%252C%2520encoder-free%2520architecture%2520that%2520learns%2520a%2520global%252C%2520low-dimensional%2520representation%2520of%2520dynamical%2520systems%2520conditioned%2520on%2520external%2520inputs%2520and%2520parameters.%2520The%2520temporal%2520evolution%2520is%2520modeled%2520in%2520the%2520latent%2520space%2520and%2520advanced%2520through%2520time-stepping%252C%2520allowing%2520for%2520time-extrapolation%252C%2520and%2520the%2520trajectories%2520are%2520consistently%2520decoded%2520onto%2520geometrically%2520parameterized%2520domains%2520using%2520a%2520GNN.%2520Our%2520framework%2520enhances%2520interpretability%2520by%2520enabling%2520the%2520analysis%2520of%2520the%2520reduced%2520dynamics%2520and%2520supporting%2520zero-shot%2520prediction%2520through%2520latent%2520interpolation.%2520The%2520methodology%2520is%2520mathematically%2520validated%2520via%2520a%2520universal%2520approximation%2520theorem%2520for%2520encoder-free%2520architectures%252C%2520and%2520numerically%2520tested%2520on%2520complex%2520computational%2520mechanics%2520problems%2520involving%2520physical%2520and%2520geometric%2520parameters%252C%2520including%2520the%2520detection%2520of%2520bifurcating%2520phenomena%2520for%2520Navier-Stokes%2520equations.%2520Code%2520availability%253A%2520https%253A//github.com/lorenzotomada/ld-gcn-rom%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Dynamics%20Graph%20Convolutional%20Networks%20for%20model%20order%20reduction%20of%20parameterized%20time-dependent%20PDEs&entry.906535625=Lorenzo%20Tomada%20and%20Federico%20Pichi%20and%20Gianluigi%20Rozza&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20are%20emerging%20as%20powerful%20tools%20for%20nonlinear%20Model%20Order%20Reduction%20%28MOR%29%20of%20time-dependent%20parameterized%20Partial%20Differential%20Equations%20%28PDEs%29.%20However%2C%20existing%20methodologies%20struggle%20to%20combine%20geometric%20inductive%20biases%20with%20interpretable%20latent%20behavior%2C%20overlooking%20dynamics-driven%20features%20or%20disregarding%20spatial%20information.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20introducing%20Latent%20Dynamics%20Graph%20Convolutional%20Network%20%28LD-GCN%29%2C%20a%20purely%20data-driven%2C%20encoder-free%20architecture%20that%20learns%20a%20global%2C%20low-dimensional%20representation%20of%20dynamical%20systems%20conditioned%20on%20external%20inputs%20and%20parameters.%20The%20temporal%20evolution%20is%20modeled%20in%20the%20latent%20space%20and%20advanced%20through%20time-stepping%2C%20allowing%20for%20time-extrapolation%2C%20and%20the%20trajectories%20are%20consistently%20decoded%20onto%20geometrically%20parameterized%20domains%20using%20a%20GNN.%20Our%20framework%20enhances%20interpretability%20by%20enabling%20the%20analysis%20of%20the%20reduced%20dynamics%20and%20supporting%20zero-shot%20prediction%20through%20latent%20interpolation.%20The%20methodology%20is%20mathematically%20validated%20via%20a%20universal%20approximation%20theorem%20for%20encoder-free%20architectures%2C%20and%20numerically%20tested%20on%20complex%20computational%20mechanics%20problems%20involving%20physical%20and%20geometric%20parameters%2C%20including%20the%20detection%20of%20bifurcating%20phenomena%20for%20Navier-Stokes%20equations.%20Code%20availability%3A%20https%3A//github.com/lorenzotomada/ld-gcn-rom&entry.1838667208=http%3A//arxiv.org/abs/2601.11259v1&entry.124074799=Read"},
{"title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5", "author": "Xingjun Ma and Yixu Wang and Hengyuan Xu and Yutao Wu and Yifan Ding and Yunhan Zhao and Zilong Wang and Jiabin Hua and Ming Wen and Jianan Liu and Ranjie Duan and Yifeng Gao and Yingshui Tan and Yunhao Chen and Hui Xue and Xin Wang and Wei Cheng and Jingjing Chen and Zuxuan Wu and Bo Li and Yu-Gang Jiang", "abstract": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has driven major gains in reasoning, perception, and generation across language and vision, yet whether these advances translate into comparable improvements in safety remains unclear, partly due to fragmented evaluations that focus on isolated modalities or threat models. In this report, we present an integrated safety evaluation of six frontier models--GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5--assessing each across language, vision-language, and image generation using a unified protocol that combines benchmark, adversarial, multilingual, and compliance evaluations. By aggregating results into safety leaderboards and model profiles, we reveal a highly uneven safety landscape: while GPT-5.2 demonstrates consistently strong and balanced performance, other models exhibit clear trade-offs across benchmark safety, adversarial robustness, multilingual generalization, and regulatory compliance. Despite strong results under standard benchmarks, all models remain highly vulnerable under adversarial testing, with worst-case safety rates dropping below 6%. Text-to-image models show slightly stronger alignment in regulated visual risk categories, yet remain fragile when faced with adversarial or semantically ambiguous prompts. Overall, these findings highlight that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation design--underscoring the need for standardized, holistic safety assessments to better reflect real-world risk and guide responsible deployment.", "link": "http://arxiv.org/abs/2601.10527v2", "date": "2026-01-16", "relevancy": 2.1685, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5453}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5453}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Safety%20Report%20on%20GPT-5.2%2C%20Gemini%203%20Pro%2C%20Qwen3-VL%2C%20Grok%204.1%20Fast%2C%20Nano%20Banana%20Pro%2C%20and%20Seedream%204.5&body=Title%3A%20A%20Safety%20Report%20on%20GPT-5.2%2C%20Gemini%203%20Pro%2C%20Qwen3-VL%2C%20Grok%204.1%20Fast%2C%20Nano%20Banana%20Pro%2C%20and%20Seedream%204.5%0AAuthor%3A%20Xingjun%20Ma%20and%20Yixu%20Wang%20and%20Hengyuan%20Xu%20and%20Yutao%20Wu%20and%20Yifan%20Ding%20and%20Yunhan%20Zhao%20and%20Zilong%20Wang%20and%20Jiabin%20Hua%20and%20Ming%20Wen%20and%20Jianan%20Liu%20and%20Ranjie%20Duan%20and%20Yifeng%20Gao%20and%20Yingshui%20Tan%20and%20Yunhao%20Chen%20and%20Hui%20Xue%20and%20Xin%20Wang%20and%20Wei%20Cheng%20and%20Jingjing%20Chen%20and%20Zuxuan%20Wu%20and%20Bo%20Li%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20The%20rapid%20evolution%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20driven%20major%20gains%20in%20reasoning%2C%20perception%2C%20and%20generation%20across%20language%20and%20vision%2C%20yet%20whether%20these%20advances%20translate%20into%20comparable%20improvements%20in%20safety%20remains%20unclear%2C%20partly%20due%20to%20fragmented%20evaluations%20that%20focus%20on%20isolated%20modalities%20or%20threat%20models.%20In%20this%20report%2C%20we%20present%20an%20integrated%20safety%20evaluation%20of%20six%20frontier%20models--GPT-5.2%2C%20Gemini%203%20Pro%2C%20Qwen3-VL%2C%20Grok%204.1%20Fast%2C%20Nano%20Banana%20Pro%2C%20and%20Seedream%204.5--assessing%20each%20across%20language%2C%20vision-language%2C%20and%20image%20generation%20using%20a%20unified%20protocol%20that%20combines%20benchmark%2C%20adversarial%2C%20multilingual%2C%20and%20compliance%20evaluations.%20By%20aggregating%20results%20into%20safety%20leaderboards%20and%20model%20profiles%2C%20we%20reveal%20a%20highly%20uneven%20safety%20landscape%3A%20while%20GPT-5.2%20demonstrates%20consistently%20strong%20and%20balanced%20performance%2C%20other%20models%20exhibit%20clear%20trade-offs%20across%20benchmark%20safety%2C%20adversarial%20robustness%2C%20multilingual%20generalization%2C%20and%20regulatory%20compliance.%20Despite%20strong%20results%20under%20standard%20benchmarks%2C%20all%20models%20remain%20highly%20vulnerable%20under%20adversarial%20testing%2C%20with%20worst-case%20safety%20rates%20dropping%20below%206%25.%20Text-to-image%20models%20show%20slightly%20stronger%20alignment%20in%20regulated%20visual%20risk%20categories%2C%20yet%20remain%20fragile%20when%20faced%20with%20adversarial%20or%20semantically%20ambiguous%20prompts.%20Overall%2C%20these%20findings%20highlight%20that%20safety%20in%20frontier%20models%20is%20inherently%20multidimensional--shaped%20by%20modality%2C%20language%2C%20and%20evaluation%20design--underscoring%20the%20need%20for%20standardized%2C%20holistic%20safety%20assessments%20to%20better%20reflect%20real-world%20risk%20and%20guide%20responsible%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10527v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Safety%2520Report%2520on%2520GPT-5.2%252C%2520Gemini%25203%2520Pro%252C%2520Qwen3-VL%252C%2520Grok%25204.1%2520Fast%252C%2520Nano%2520Banana%2520Pro%252C%2520and%2520Seedream%25204.5%26entry.906535625%3DXingjun%2520Ma%2520and%2520Yixu%2520Wang%2520and%2520Hengyuan%2520Xu%2520and%2520Yutao%2520Wu%2520and%2520Yifan%2520Ding%2520and%2520Yunhan%2520Zhao%2520and%2520Zilong%2520Wang%2520and%2520Jiabin%2520Hua%2520and%2520Ming%2520Wen%2520and%2520Jianan%2520Liu%2520and%2520Ranjie%2520Duan%2520and%2520Yifeng%2520Gao%2520and%2520Yingshui%2520Tan%2520and%2520Yunhao%2520Chen%2520and%2520Hui%2520Xue%2520and%2520Xin%2520Wang%2520and%2520Wei%2520Cheng%2520and%2520Jingjing%2520Chen%2520and%2520Zuxuan%2520Wu%2520and%2520Bo%2520Li%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3DThe%2520rapid%2520evolution%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520driven%2520major%2520gains%2520in%2520reasoning%252C%2520perception%252C%2520and%2520generation%2520across%2520language%2520and%2520vision%252C%2520yet%2520whether%2520these%2520advances%2520translate%2520into%2520comparable%2520improvements%2520in%2520safety%2520remains%2520unclear%252C%2520partly%2520due%2520to%2520fragmented%2520evaluations%2520that%2520focus%2520on%2520isolated%2520modalities%2520or%2520threat%2520models.%2520In%2520this%2520report%252C%2520we%2520present%2520an%2520integrated%2520safety%2520evaluation%2520of%2520six%2520frontier%2520models--GPT-5.2%252C%2520Gemini%25203%2520Pro%252C%2520Qwen3-VL%252C%2520Grok%25204.1%2520Fast%252C%2520Nano%2520Banana%2520Pro%252C%2520and%2520Seedream%25204.5--assessing%2520each%2520across%2520language%252C%2520vision-language%252C%2520and%2520image%2520generation%2520using%2520a%2520unified%2520protocol%2520that%2520combines%2520benchmark%252C%2520adversarial%252C%2520multilingual%252C%2520and%2520compliance%2520evaluations.%2520By%2520aggregating%2520results%2520into%2520safety%2520leaderboards%2520and%2520model%2520profiles%252C%2520we%2520reveal%2520a%2520highly%2520uneven%2520safety%2520landscape%253A%2520while%2520GPT-5.2%2520demonstrates%2520consistently%2520strong%2520and%2520balanced%2520performance%252C%2520other%2520models%2520exhibit%2520clear%2520trade-offs%2520across%2520benchmark%2520safety%252C%2520adversarial%2520robustness%252C%2520multilingual%2520generalization%252C%2520and%2520regulatory%2520compliance.%2520Despite%2520strong%2520results%2520under%2520standard%2520benchmarks%252C%2520all%2520models%2520remain%2520highly%2520vulnerable%2520under%2520adversarial%2520testing%252C%2520with%2520worst-case%2520safety%2520rates%2520dropping%2520below%25206%2525.%2520Text-to-image%2520models%2520show%2520slightly%2520stronger%2520alignment%2520in%2520regulated%2520visual%2520risk%2520categories%252C%2520yet%2520remain%2520fragile%2520when%2520faced%2520with%2520adversarial%2520or%2520semantically%2520ambiguous%2520prompts.%2520Overall%252C%2520these%2520findings%2520highlight%2520that%2520safety%2520in%2520frontier%2520models%2520is%2520inherently%2520multidimensional--shaped%2520by%2520modality%252C%2520language%252C%2520and%2520evaluation%2520design--underscoring%2520the%2520need%2520for%2520standardized%252C%2520holistic%2520safety%2520assessments%2520to%2520better%2520reflect%2520real-world%2520risk%2520and%2520guide%2520responsible%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10527v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Safety%20Report%20on%20GPT-5.2%2C%20Gemini%203%20Pro%2C%20Qwen3-VL%2C%20Grok%204.1%20Fast%2C%20Nano%20Banana%20Pro%2C%20and%20Seedream%204.5&entry.906535625=Xingjun%20Ma%20and%20Yixu%20Wang%20and%20Hengyuan%20Xu%20and%20Yutao%20Wu%20and%20Yifan%20Ding%20and%20Yunhan%20Zhao%20and%20Zilong%20Wang%20and%20Jiabin%20Hua%20and%20Ming%20Wen%20and%20Jianan%20Liu%20and%20Ranjie%20Duan%20and%20Yifeng%20Gao%20and%20Yingshui%20Tan%20and%20Yunhao%20Chen%20and%20Hui%20Xue%20and%20Xin%20Wang%20and%20Wei%20Cheng%20and%20Jingjing%20Chen%20and%20Zuxuan%20Wu%20and%20Bo%20Li%20and%20Yu-Gang%20Jiang&entry.1292438233=The%20rapid%20evolution%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20driven%20major%20gains%20in%20reasoning%2C%20perception%2C%20and%20generation%20across%20language%20and%20vision%2C%20yet%20whether%20these%20advances%20translate%20into%20comparable%20improvements%20in%20safety%20remains%20unclear%2C%20partly%20due%20to%20fragmented%20evaluations%20that%20focus%20on%20isolated%20modalities%20or%20threat%20models.%20In%20this%20report%2C%20we%20present%20an%20integrated%20safety%20evaluation%20of%20six%20frontier%20models--GPT-5.2%2C%20Gemini%203%20Pro%2C%20Qwen3-VL%2C%20Grok%204.1%20Fast%2C%20Nano%20Banana%20Pro%2C%20and%20Seedream%204.5--assessing%20each%20across%20language%2C%20vision-language%2C%20and%20image%20generation%20using%20a%20unified%20protocol%20that%20combines%20benchmark%2C%20adversarial%2C%20multilingual%2C%20and%20compliance%20evaluations.%20By%20aggregating%20results%20into%20safety%20leaderboards%20and%20model%20profiles%2C%20we%20reveal%20a%20highly%20uneven%20safety%20landscape%3A%20while%20GPT-5.2%20demonstrates%20consistently%20strong%20and%20balanced%20performance%2C%20other%20models%20exhibit%20clear%20trade-offs%20across%20benchmark%20safety%2C%20adversarial%20robustness%2C%20multilingual%20generalization%2C%20and%20regulatory%20compliance.%20Despite%20strong%20results%20under%20standard%20benchmarks%2C%20all%20models%20remain%20highly%20vulnerable%20under%20adversarial%20testing%2C%20with%20worst-case%20safety%20rates%20dropping%20below%206%25.%20Text-to-image%20models%20show%20slightly%20stronger%20alignment%20in%20regulated%20visual%20risk%20categories%2C%20yet%20remain%20fragile%20when%20faced%20with%20adversarial%20or%20semantically%20ambiguous%20prompts.%20Overall%2C%20these%20findings%20highlight%20that%20safety%20in%20frontier%20models%20is%20inherently%20multidimensional--shaped%20by%20modality%2C%20language%2C%20and%20evaluation%20design--underscoring%20the%20need%20for%20standardized%2C%20holistic%20safety%20assessments%20to%20better%20reflect%20real-world%20risk%20and%20guide%20responsible%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2601.10527v2&entry.124074799=Read"},
{"title": "GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance", "author": "Francisco Giral and \u00c1lvaro Manzano and Ignacio G\u00f3mez and Ricardo Vinuesa and Soledad Le Clainche", "abstract": "Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available. We propose GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism: the unconditional branch learns a geometry-aware flow prior, while the sensor-conditioned branch injects observational constraints during sampling. This formulation enables obstacle-aware reconstruction and generalization across unseen geometries, wind directions, and mesh resolutions without retraining. We consider both sparse fixed sensors and trajectory-based observations using the same reconstruction procedure. When evaluated against supervised graph neural network (GNN) baselines and classical reduced-order data assimilation methods, GenDA reduces the relative root-mean-square error (RRMSE) by 25-57% and increases the structural similarity index (SSIM) by 23-33% across the tested meshes. Experiments are conducted on Reynolds-averaged Navier-Stokes (RANS) simulations of a real urban neighbourhood in Bristol, United Kingdom, at a characteristic Reynolds number of $\\mathrm{Re}\\approx2\\times10^{7}$, featuring complex building geometry and irregular terrain. The proposed framework provides a scalable path toward generative, geometry-aware data assimilation for environmental monitoring in complex domains.", "link": "http://arxiv.org/abs/2601.11440v1", "date": "2026-01-16", "relevancy": 2.167, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5611}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5517}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenDA%3A%20Generative%20Data%20Assimilation%20on%20Complex%20Urban%20Areas%20via%20Classifier-Free%20Diffusion%20Guidance&body=Title%3A%20GenDA%3A%20Generative%20Data%20Assimilation%20on%20Complex%20Urban%20Areas%20via%20Classifier-Free%20Diffusion%20Guidance%0AAuthor%3A%20Francisco%20Giral%20and%20%C3%81lvaro%20Manzano%20and%20Ignacio%20G%C3%B3mez%20and%20Ricardo%20Vinuesa%20and%20Soledad%20Le%20Clainche%0AAbstract%3A%20Urban%20wind%20flow%20reconstruction%20is%20essential%20for%20assessing%20air%20quality%2C%20heat%20dispersion%2C%20and%20pedestrian%20comfort%2C%20yet%20remains%20challenging%20when%20only%20sparse%20sensor%20data%20are%20available.%20We%20propose%20GenDA%2C%20a%20generative%20data%20assimilation%20framework%20that%20reconstructs%20high-resolution%20wind%20fields%20on%20unstructured%20meshes%20from%20limited%20observations.%20The%20model%20employs%20a%20multiscale%20graph-based%20diffusion%20architecture%20trained%20on%20computational%20fluid%20dynamics%20%28CFD%29%20simulations%20and%20interprets%20classifier-free%20guidance%20as%20a%20learned%20posterior%20reconstruction%20mechanism%3A%20the%20unconditional%20branch%20learns%20a%20geometry-aware%20flow%20prior%2C%20while%20the%20sensor-conditioned%20branch%20injects%20observational%20constraints%20during%20sampling.%20This%20formulation%20enables%20obstacle-aware%20reconstruction%20and%20generalization%20across%20unseen%20geometries%2C%20wind%20directions%2C%20and%20mesh%20resolutions%20without%20retraining.%20We%20consider%20both%20sparse%20fixed%20sensors%20and%20trajectory-based%20observations%20using%20the%20same%20reconstruction%20procedure.%20When%20evaluated%20against%20supervised%20graph%20neural%20network%20%28GNN%29%20baselines%20and%20classical%20reduced-order%20data%20assimilation%20methods%2C%20GenDA%20reduces%20the%20relative%20root-mean-square%20error%20%28RRMSE%29%20by%2025-57%25%20and%20increases%20the%20structural%20similarity%20index%20%28SSIM%29%20by%2023-33%25%20across%20the%20tested%20meshes.%20Experiments%20are%20conducted%20on%20Reynolds-averaged%20Navier-Stokes%20%28RANS%29%20simulations%20of%20a%20real%20urban%20neighbourhood%20in%20Bristol%2C%20United%20Kingdom%2C%20at%20a%20characteristic%20Reynolds%20number%20of%20%24%5Cmathrm%7BRe%7D%5Capprox2%5Ctimes10%5E%7B7%7D%24%2C%20featuring%20complex%20building%20geometry%20and%20irregular%20terrain.%20The%20proposed%20framework%20provides%20a%20scalable%20path%20toward%20generative%2C%20geometry-aware%20data%20assimilation%20for%20environmental%20monitoring%20in%20complex%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenDA%253A%2520Generative%2520Data%2520Assimilation%2520on%2520Complex%2520Urban%2520Areas%2520via%2520Classifier-Free%2520Diffusion%2520Guidance%26entry.906535625%3DFrancisco%2520Giral%2520and%2520%25C3%2581lvaro%2520Manzano%2520and%2520Ignacio%2520G%25C3%25B3mez%2520and%2520Ricardo%2520Vinuesa%2520and%2520Soledad%2520Le%2520Clainche%26entry.1292438233%3DUrban%2520wind%2520flow%2520reconstruction%2520is%2520essential%2520for%2520assessing%2520air%2520quality%252C%2520heat%2520dispersion%252C%2520and%2520pedestrian%2520comfort%252C%2520yet%2520remains%2520challenging%2520when%2520only%2520sparse%2520sensor%2520data%2520are%2520available.%2520We%2520propose%2520GenDA%252C%2520a%2520generative%2520data%2520assimilation%2520framework%2520that%2520reconstructs%2520high-resolution%2520wind%2520fields%2520on%2520unstructured%2520meshes%2520from%2520limited%2520observations.%2520The%2520model%2520employs%2520a%2520multiscale%2520graph-based%2520diffusion%2520architecture%2520trained%2520on%2520computational%2520fluid%2520dynamics%2520%2528CFD%2529%2520simulations%2520and%2520interprets%2520classifier-free%2520guidance%2520as%2520a%2520learned%2520posterior%2520reconstruction%2520mechanism%253A%2520the%2520unconditional%2520branch%2520learns%2520a%2520geometry-aware%2520flow%2520prior%252C%2520while%2520the%2520sensor-conditioned%2520branch%2520injects%2520observational%2520constraints%2520during%2520sampling.%2520This%2520formulation%2520enables%2520obstacle-aware%2520reconstruction%2520and%2520generalization%2520across%2520unseen%2520geometries%252C%2520wind%2520directions%252C%2520and%2520mesh%2520resolutions%2520without%2520retraining.%2520We%2520consider%2520both%2520sparse%2520fixed%2520sensors%2520and%2520trajectory-based%2520observations%2520using%2520the%2520same%2520reconstruction%2520procedure.%2520When%2520evaluated%2520against%2520supervised%2520graph%2520neural%2520network%2520%2528GNN%2529%2520baselines%2520and%2520classical%2520reduced-order%2520data%2520assimilation%2520methods%252C%2520GenDA%2520reduces%2520the%2520relative%2520root-mean-square%2520error%2520%2528RRMSE%2529%2520by%252025-57%2525%2520and%2520increases%2520the%2520structural%2520similarity%2520index%2520%2528SSIM%2529%2520by%252023-33%2525%2520across%2520the%2520tested%2520meshes.%2520Experiments%2520are%2520conducted%2520on%2520Reynolds-averaged%2520Navier-Stokes%2520%2528RANS%2529%2520simulations%2520of%2520a%2520real%2520urban%2520neighbourhood%2520in%2520Bristol%252C%2520United%2520Kingdom%252C%2520at%2520a%2520characteristic%2520Reynolds%2520number%2520of%2520%2524%255Cmathrm%257BRe%257D%255Capprox2%255Ctimes10%255E%257B7%257D%2524%252C%2520featuring%2520complex%2520building%2520geometry%2520and%2520irregular%2520terrain.%2520The%2520proposed%2520framework%2520provides%2520a%2520scalable%2520path%2520toward%2520generative%252C%2520geometry-aware%2520data%2520assimilation%2520for%2520environmental%2520monitoring%2520in%2520complex%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenDA%3A%20Generative%20Data%20Assimilation%20on%20Complex%20Urban%20Areas%20via%20Classifier-Free%20Diffusion%20Guidance&entry.906535625=Francisco%20Giral%20and%20%C3%81lvaro%20Manzano%20and%20Ignacio%20G%C3%B3mez%20and%20Ricardo%20Vinuesa%20and%20Soledad%20Le%20Clainche&entry.1292438233=Urban%20wind%20flow%20reconstruction%20is%20essential%20for%20assessing%20air%20quality%2C%20heat%20dispersion%2C%20and%20pedestrian%20comfort%2C%20yet%20remains%20challenging%20when%20only%20sparse%20sensor%20data%20are%20available.%20We%20propose%20GenDA%2C%20a%20generative%20data%20assimilation%20framework%20that%20reconstructs%20high-resolution%20wind%20fields%20on%20unstructured%20meshes%20from%20limited%20observations.%20The%20model%20employs%20a%20multiscale%20graph-based%20diffusion%20architecture%20trained%20on%20computational%20fluid%20dynamics%20%28CFD%29%20simulations%20and%20interprets%20classifier-free%20guidance%20as%20a%20learned%20posterior%20reconstruction%20mechanism%3A%20the%20unconditional%20branch%20learns%20a%20geometry-aware%20flow%20prior%2C%20while%20the%20sensor-conditioned%20branch%20injects%20observational%20constraints%20during%20sampling.%20This%20formulation%20enables%20obstacle-aware%20reconstruction%20and%20generalization%20across%20unseen%20geometries%2C%20wind%20directions%2C%20and%20mesh%20resolutions%20without%20retraining.%20We%20consider%20both%20sparse%20fixed%20sensors%20and%20trajectory-based%20observations%20using%20the%20same%20reconstruction%20procedure.%20When%20evaluated%20against%20supervised%20graph%20neural%20network%20%28GNN%29%20baselines%20and%20classical%20reduced-order%20data%20assimilation%20methods%2C%20GenDA%20reduces%20the%20relative%20root-mean-square%20error%20%28RRMSE%29%20by%2025-57%25%20and%20increases%20the%20structural%20similarity%20index%20%28SSIM%29%20by%2023-33%25%20across%20the%20tested%20meshes.%20Experiments%20are%20conducted%20on%20Reynolds-averaged%20Navier-Stokes%20%28RANS%29%20simulations%20of%20a%20real%20urban%20neighbourhood%20in%20Bristol%2C%20United%20Kingdom%2C%20at%20a%20characteristic%20Reynolds%20number%20of%20%24%5Cmathrm%7BRe%7D%5Capprox2%5Ctimes10%5E%7B7%7D%24%2C%20featuring%20complex%20building%20geometry%20and%20irregular%20terrain.%20The%20proposed%20framework%20provides%20a%20scalable%20path%20toward%20generative%2C%20geometry-aware%20data%20assimilation%20for%20environmental%20monitoring%20in%20complex%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2601.11440v1&entry.124074799=Read"},
{"title": "How DDAIR you? Disambiguated Data Augmentation for Intent Recognition", "author": "Galo Castillo-L\u00f3pez and Alexis Lombard and Nasredine Semmar and Ga\u00ebl de Chalendar", "abstract": "Large Language Models (LLMs) are effective for data augmentation in classification tasks like intent detection. In some cases, they inadvertently produce examples that are ambiguous with regard to untargeted classes. We present DDAIR (Disambiguated Data Augmentation for Intent Recognition) to mitigate this problem. We use Sentence Transformers to detect ambiguous class-guided augmented examples generated by LLMs for intent recognition in low-resource scenarios. We identify synthetic examples that are semantically more similar to another intent than to their target one. We also provide an iterative re-generation method to mitigate such ambiguities. Our findings show that sentence embeddings effectively help to (re)generate less ambiguous examples, and suggest promising potential to improve classification performance in scenarios where intents are loosely or broadly defined.", "link": "http://arxiv.org/abs/2601.11234v1", "date": "2026-01-16", "relevancy": 2.1402, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5317}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20DDAIR%20you%3F%20Disambiguated%20Data%20Augmentation%20for%20Intent%20Recognition&body=Title%3A%20How%20DDAIR%20you%3F%20Disambiguated%20Data%20Augmentation%20for%20Intent%20Recognition%0AAuthor%3A%20Galo%20Castillo-L%C3%B3pez%20and%20Alexis%20Lombard%20and%20Nasredine%20Semmar%20and%20Ga%C3%ABl%20de%20Chalendar%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20effective%20for%20data%20augmentation%20in%20classification%20tasks%20like%20intent%20detection.%20In%20some%20cases%2C%20they%20inadvertently%20produce%20examples%20that%20are%20ambiguous%20with%20regard%20to%20untargeted%20classes.%20We%20present%20DDAIR%20%28Disambiguated%20Data%20Augmentation%20for%20Intent%20Recognition%29%20to%20mitigate%20this%20problem.%20We%20use%20Sentence%20Transformers%20to%20detect%20ambiguous%20class-guided%20augmented%20examples%20generated%20by%20LLMs%20for%20intent%20recognition%20in%20low-resource%20scenarios.%20We%20identify%20synthetic%20examples%20that%20are%20semantically%20more%20similar%20to%20another%20intent%20than%20to%20their%20target%20one.%20We%20also%20provide%20an%20iterative%20re-generation%20method%20to%20mitigate%20such%20ambiguities.%20Our%20findings%20show%20that%20sentence%20embeddings%20effectively%20help%20to%20%28re%29generate%20less%20ambiguous%20examples%2C%20and%20suggest%20promising%20potential%20to%20improve%20classification%20performance%20in%20scenarios%20where%20intents%20are%20loosely%20or%20broadly%20defined.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520DDAIR%2520you%253F%2520Disambiguated%2520Data%2520Augmentation%2520for%2520Intent%2520Recognition%26entry.906535625%3DGalo%2520Castillo-L%25C3%25B3pez%2520and%2520Alexis%2520Lombard%2520and%2520Nasredine%2520Semmar%2520and%2520Ga%25C3%25ABl%2520de%2520Chalendar%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520effective%2520for%2520data%2520augmentation%2520in%2520classification%2520tasks%2520like%2520intent%2520detection.%2520In%2520some%2520cases%252C%2520they%2520inadvertently%2520produce%2520examples%2520that%2520are%2520ambiguous%2520with%2520regard%2520to%2520untargeted%2520classes.%2520We%2520present%2520DDAIR%2520%2528Disambiguated%2520Data%2520Augmentation%2520for%2520Intent%2520Recognition%2529%2520to%2520mitigate%2520this%2520problem.%2520We%2520use%2520Sentence%2520Transformers%2520to%2520detect%2520ambiguous%2520class-guided%2520augmented%2520examples%2520generated%2520by%2520LLMs%2520for%2520intent%2520recognition%2520in%2520low-resource%2520scenarios.%2520We%2520identify%2520synthetic%2520examples%2520that%2520are%2520semantically%2520more%2520similar%2520to%2520another%2520intent%2520than%2520to%2520their%2520target%2520one.%2520We%2520also%2520provide%2520an%2520iterative%2520re-generation%2520method%2520to%2520mitigate%2520such%2520ambiguities.%2520Our%2520findings%2520show%2520that%2520sentence%2520embeddings%2520effectively%2520help%2520to%2520%2528re%2529generate%2520less%2520ambiguous%2520examples%252C%2520and%2520suggest%2520promising%2520potential%2520to%2520improve%2520classification%2520performance%2520in%2520scenarios%2520where%2520intents%2520are%2520loosely%2520or%2520broadly%2520defined.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20DDAIR%20you%3F%20Disambiguated%20Data%20Augmentation%20for%20Intent%20Recognition&entry.906535625=Galo%20Castillo-L%C3%B3pez%20and%20Alexis%20Lombard%20and%20Nasredine%20Semmar%20and%20Ga%C3%ABl%20de%20Chalendar&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20are%20effective%20for%20data%20augmentation%20in%20classification%20tasks%20like%20intent%20detection.%20In%20some%20cases%2C%20they%20inadvertently%20produce%20examples%20that%20are%20ambiguous%20with%20regard%20to%20untargeted%20classes.%20We%20present%20DDAIR%20%28Disambiguated%20Data%20Augmentation%20for%20Intent%20Recognition%29%20to%20mitigate%20this%20problem.%20We%20use%20Sentence%20Transformers%20to%20detect%20ambiguous%20class-guided%20augmented%20examples%20generated%20by%20LLMs%20for%20intent%20recognition%20in%20low-resource%20scenarios.%20We%20identify%20synthetic%20examples%20that%20are%20semantically%20more%20similar%20to%20another%20intent%20than%20to%20their%20target%20one.%20We%20also%20provide%20an%20iterative%20re-generation%20method%20to%20mitigate%20such%20ambiguities.%20Our%20findings%20show%20that%20sentence%20embeddings%20effectively%20help%20to%20%28re%29generate%20less%20ambiguous%20examples%2C%20and%20suggest%20promising%20potential%20to%20improve%20classification%20performance%20in%20scenarios%20where%20intents%20are%20loosely%20or%20broadly%20defined.&entry.1838667208=http%3A//arxiv.org/abs/2601.11234v1&entry.124074799=Read"},
{"title": "SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2", "author": "Gergely Dinya and Andr\u00e1s Gelencs\u00e9r and Krisztina Kup\u00e1n and Clemens K\u00fcpper and Krist\u00f3f Karacs and Anna Gelencs\u00e9r-Horv\u00e1th", "abstract": "Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine'' workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.", "link": "http://arxiv.org/abs/2601.11301v1", "date": "2026-01-16", "relevancy": 2.1045, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5341}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5309}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMannot%3A%20A%20Memory-Efficient%2C%20Local%2C%20Open-source%20Framework%20for%20Interactive%20Video%20Instance%20Segmentation%20based%20on%20SAM2&body=Title%3A%20SAMannot%3A%20A%20Memory-Efficient%2C%20Local%2C%20Open-source%20Framework%20for%20Interactive%20Video%20Instance%20Segmentation%20based%20on%20SAM2%0AAuthor%3A%20Gergely%20Dinya%20and%20Andr%C3%A1s%20Gelencs%C3%A9r%20and%20Krisztina%20Kup%C3%A1n%20and%20Clemens%20K%C3%BCpper%20and%20Krist%C3%B3f%20Karacs%20and%20Anna%20Gelencs%C3%A9r-Horv%C3%A1th%0AAbstract%3A%20Current%20research%20workflows%20for%20precise%20video%20segmentation%20are%20often%20forced%20into%20a%20compromise%20between%20labor-intensive%20manual%20curation%2C%20costly%20commercial%20platforms%2C%20and/or%20privacy-compromising%20cloud-based%20services.%20The%20demand%20for%20high-fidelity%20video%20instance%20segmentation%20in%20research%20is%20often%20hindered%20by%20the%20bottleneck%20of%20manual%20annotation%20and%20the%20privacy%20concerns%20of%20cloud-based%20tools.%20We%20present%20SAMannot%2C%20an%20open-source%2C%20local%20framework%20that%20integrates%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20into%20a%20human-in-the-loop%20workflow.%20To%20address%20the%20high%20resource%20requirements%20of%20foundation%20models%2C%20we%20modified%20the%20SAM2%20dependency%20and%20implemented%20a%20processing%20layer%20that%20minimizes%20computational%20overhead%20and%20maximizes%20throughput%2C%20ensuring%20a%20highly%20responsive%20user%20interface.%20Key%20features%20include%20persistent%20instance%20identity%20management%2C%20an%20automated%20%60%60lock-and-refine%27%27%20workflow%20with%20barrier%20frames%2C%20and%20a%20mask-skeletonization-based%20auto-prompting%20mechanism.%20SAMannot%20facilitates%20the%20generation%20of%20research-ready%20datasets%20in%20YOLO%20and%20PNG%20formats%20alongside%20structured%20interaction%20logs.%20Verified%20through%20animal%20behavior%20tracking%20use-cases%20and%20subsets%20of%20the%20LVOS%20and%20DAVIS%20benchmark%20datasets%2C%20the%20tool%20provides%20a%20scalable%2C%20private%2C%20and%20cost-effective%20alternative%20to%20commercial%20platforms%20for%20complex%20video%20annotation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMannot%253A%2520A%2520Memory-Efficient%252C%2520Local%252C%2520Open-source%2520Framework%2520for%2520Interactive%2520Video%2520Instance%2520Segmentation%2520based%2520on%2520SAM2%26entry.906535625%3DGergely%2520Dinya%2520and%2520Andr%25C3%25A1s%2520Gelencs%25C3%25A9r%2520and%2520Krisztina%2520Kup%25C3%25A1n%2520and%2520Clemens%2520K%25C3%25BCpper%2520and%2520Krist%25C3%25B3f%2520Karacs%2520and%2520Anna%2520Gelencs%25C3%25A9r-Horv%25C3%25A1th%26entry.1292438233%3DCurrent%2520research%2520workflows%2520for%2520precise%2520video%2520segmentation%2520are%2520often%2520forced%2520into%2520a%2520compromise%2520between%2520labor-intensive%2520manual%2520curation%252C%2520costly%2520commercial%2520platforms%252C%2520and/or%2520privacy-compromising%2520cloud-based%2520services.%2520The%2520demand%2520for%2520high-fidelity%2520video%2520instance%2520segmentation%2520in%2520research%2520is%2520often%2520hindered%2520by%2520the%2520bottleneck%2520of%2520manual%2520annotation%2520and%2520the%2520privacy%2520concerns%2520of%2520cloud-based%2520tools.%2520We%2520present%2520SAMannot%252C%2520an%2520open-source%252C%2520local%2520framework%2520that%2520integrates%2520the%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520into%2520a%2520human-in-the-loop%2520workflow.%2520To%2520address%2520the%2520high%2520resource%2520requirements%2520of%2520foundation%2520models%252C%2520we%2520modified%2520the%2520SAM2%2520dependency%2520and%2520implemented%2520a%2520processing%2520layer%2520that%2520minimizes%2520computational%2520overhead%2520and%2520maximizes%2520throughput%252C%2520ensuring%2520a%2520highly%2520responsive%2520user%2520interface.%2520Key%2520features%2520include%2520persistent%2520instance%2520identity%2520management%252C%2520an%2520automated%2520%2560%2560lock-and-refine%2527%2527%2520workflow%2520with%2520barrier%2520frames%252C%2520and%2520a%2520mask-skeletonization-based%2520auto-prompting%2520mechanism.%2520SAMannot%2520facilitates%2520the%2520generation%2520of%2520research-ready%2520datasets%2520in%2520YOLO%2520and%2520PNG%2520formats%2520alongside%2520structured%2520interaction%2520logs.%2520Verified%2520through%2520animal%2520behavior%2520tracking%2520use-cases%2520and%2520subsets%2520of%2520the%2520LVOS%2520and%2520DAVIS%2520benchmark%2520datasets%252C%2520the%2520tool%2520provides%2520a%2520scalable%252C%2520private%252C%2520and%2520cost-effective%2520alternative%2520to%2520commercial%2520platforms%2520for%2520complex%2520video%2520annotation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMannot%3A%20A%20Memory-Efficient%2C%20Local%2C%20Open-source%20Framework%20for%20Interactive%20Video%20Instance%20Segmentation%20based%20on%20SAM2&entry.906535625=Gergely%20Dinya%20and%20Andr%C3%A1s%20Gelencs%C3%A9r%20and%20Krisztina%20Kup%C3%A1n%20and%20Clemens%20K%C3%BCpper%20and%20Krist%C3%B3f%20Karacs%20and%20Anna%20Gelencs%C3%A9r-Horv%C3%A1th&entry.1292438233=Current%20research%20workflows%20for%20precise%20video%20segmentation%20are%20often%20forced%20into%20a%20compromise%20between%20labor-intensive%20manual%20curation%2C%20costly%20commercial%20platforms%2C%20and/or%20privacy-compromising%20cloud-based%20services.%20The%20demand%20for%20high-fidelity%20video%20instance%20segmentation%20in%20research%20is%20often%20hindered%20by%20the%20bottleneck%20of%20manual%20annotation%20and%20the%20privacy%20concerns%20of%20cloud-based%20tools.%20We%20present%20SAMannot%2C%20an%20open-source%2C%20local%20framework%20that%20integrates%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20into%20a%20human-in-the-loop%20workflow.%20To%20address%20the%20high%20resource%20requirements%20of%20foundation%20models%2C%20we%20modified%20the%20SAM2%20dependency%20and%20implemented%20a%20processing%20layer%20that%20minimizes%20computational%20overhead%20and%20maximizes%20throughput%2C%20ensuring%20a%20highly%20responsive%20user%20interface.%20Key%20features%20include%20persistent%20instance%20identity%20management%2C%20an%20automated%20%60%60lock-and-refine%27%27%20workflow%20with%20barrier%20frames%2C%20and%20a%20mask-skeletonization-based%20auto-prompting%20mechanism.%20SAMannot%20facilitates%20the%20generation%20of%20research-ready%20datasets%20in%20YOLO%20and%20PNG%20formats%20alongside%20structured%20interaction%20logs.%20Verified%20through%20animal%20behavior%20tracking%20use-cases%20and%20subsets%20of%20the%20LVOS%20and%20DAVIS%20benchmark%20datasets%2C%20the%20tool%20provides%20a%20scalable%2C%20private%2C%20and%20cost-effective%20alternative%20to%20commercial%20platforms%20for%20complex%20video%20annotation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.11301v1&entry.124074799=Read"},
{"title": "GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling", "author": "Pascal Schlachter and Bin Yang", "abstract": "Unsupervised domain adaptation tackles the problem that domain shifts between training and test data impair the performance of neural networks in many real-world applications. Thereby, in realistic scenarios, the source data may no longer be available during adaptation, and the label space of the target domain may differ from the source label space. This setting, known as source-free universal domain adaptation (SF-UniDA), has recently gained attention, but all existing approaches only assume a single domain shift from source to target. In this work, we present the first study on continual SF-UniDA, where the model must adapt sequentially to a stream of multiple different unlabeled target domains. Building upon our previous methods for online SF-UniDA, we combine their key ideas by integrating Gaussian mixture model-based pseudo-labeling within a mean teacher framework for improved stability over long adaptation sequences. Additionally, we introduce consistency losses for further robustness. The resulting method GMM-COMET provides a strong first baseline for continual SF-UniDA and is the only approach in our experiments to consistently improve upon the source-only model across all evaluated scenarios. Our code is available at https://github.com/pascalschlachter/GMM-COMET.", "link": "http://arxiv.org/abs/2601.11161v1", "date": "2026-01-16", "relevancy": 2.0993, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5085}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMM-COMET%3A%20Continual%20Source-Free%20Universal%20Domain%20Adaptation%20via%20a%20Mean%20Teacher%20and%20Gaussian%20Mixture%20Model-Based%20Pseudo-Labeling&body=Title%3A%20GMM-COMET%3A%20Continual%20Source-Free%20Universal%20Domain%20Adaptation%20via%20a%20Mean%20Teacher%20and%20Gaussian%20Mixture%20Model-Based%20Pseudo-Labeling%0AAuthor%3A%20Pascal%20Schlachter%20and%20Bin%20Yang%0AAbstract%3A%20Unsupervised%20domain%20adaptation%20tackles%20the%20problem%20that%20domain%20shifts%20between%20training%20and%20test%20data%20impair%20the%20performance%20of%20neural%20networks%20in%20many%20real-world%20applications.%20Thereby%2C%20in%20realistic%20scenarios%2C%20the%20source%20data%20may%20no%20longer%20be%20available%20during%20adaptation%2C%20and%20the%20label%20space%20of%20the%20target%20domain%20may%20differ%20from%20the%20source%20label%20space.%20This%20setting%2C%20known%20as%20source-free%20universal%20domain%20adaptation%20%28SF-UniDA%29%2C%20has%20recently%20gained%20attention%2C%20but%20all%20existing%20approaches%20only%20assume%20a%20single%20domain%20shift%20from%20source%20to%20target.%20In%20this%20work%2C%20we%20present%20the%20first%20study%20on%20continual%20SF-UniDA%2C%20where%20the%20model%20must%20adapt%20sequentially%20to%20a%20stream%20of%20multiple%20different%20unlabeled%20target%20domains.%20Building%20upon%20our%20previous%20methods%20for%20online%20SF-UniDA%2C%20we%20combine%20their%20key%20ideas%20by%20integrating%20Gaussian%20mixture%20model-based%20pseudo-labeling%20within%20a%20mean%20teacher%20framework%20for%20improved%20stability%20over%20long%20adaptation%20sequences.%20Additionally%2C%20we%20introduce%20consistency%20losses%20for%20further%20robustness.%20The%20resulting%20method%20GMM-COMET%20provides%20a%20strong%20first%20baseline%20for%20continual%20SF-UniDA%20and%20is%20the%20only%20approach%20in%20our%20experiments%20to%20consistently%20improve%20upon%20the%20source-only%20model%20across%20all%20evaluated%20scenarios.%20Our%20code%20is%20available%20at%20https%3A//github.com/pascalschlachter/GMM-COMET.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMM-COMET%253A%2520Continual%2520Source-Free%2520Universal%2520Domain%2520Adaptation%2520via%2520a%2520Mean%2520Teacher%2520and%2520Gaussian%2520Mixture%2520Model-Based%2520Pseudo-Labeling%26entry.906535625%3DPascal%2520Schlachter%2520and%2520Bin%2520Yang%26entry.1292438233%3DUnsupervised%2520domain%2520adaptation%2520tackles%2520the%2520problem%2520that%2520domain%2520shifts%2520between%2520training%2520and%2520test%2520data%2520impair%2520the%2520performance%2520of%2520neural%2520networks%2520in%2520many%2520real-world%2520applications.%2520Thereby%252C%2520in%2520realistic%2520scenarios%252C%2520the%2520source%2520data%2520may%2520no%2520longer%2520be%2520available%2520during%2520adaptation%252C%2520and%2520the%2520label%2520space%2520of%2520the%2520target%2520domain%2520may%2520differ%2520from%2520the%2520source%2520label%2520space.%2520This%2520setting%252C%2520known%2520as%2520source-free%2520universal%2520domain%2520adaptation%2520%2528SF-UniDA%2529%252C%2520has%2520recently%2520gained%2520attention%252C%2520but%2520all%2520existing%2520approaches%2520only%2520assume%2520a%2520single%2520domain%2520shift%2520from%2520source%2520to%2520target.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520study%2520on%2520continual%2520SF-UniDA%252C%2520where%2520the%2520model%2520must%2520adapt%2520sequentially%2520to%2520a%2520stream%2520of%2520multiple%2520different%2520unlabeled%2520target%2520domains.%2520Building%2520upon%2520our%2520previous%2520methods%2520for%2520online%2520SF-UniDA%252C%2520we%2520combine%2520their%2520key%2520ideas%2520by%2520integrating%2520Gaussian%2520mixture%2520model-based%2520pseudo-labeling%2520within%2520a%2520mean%2520teacher%2520framework%2520for%2520improved%2520stability%2520over%2520long%2520adaptation%2520sequences.%2520Additionally%252C%2520we%2520introduce%2520consistency%2520losses%2520for%2520further%2520robustness.%2520The%2520resulting%2520method%2520GMM-COMET%2520provides%2520a%2520strong%2520first%2520baseline%2520for%2520continual%2520SF-UniDA%2520and%2520is%2520the%2520only%2520approach%2520in%2520our%2520experiments%2520to%2520consistently%2520improve%2520upon%2520the%2520source-only%2520model%2520across%2520all%2520evaluated%2520scenarios.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/pascalschlachter/GMM-COMET.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMM-COMET%3A%20Continual%20Source-Free%20Universal%20Domain%20Adaptation%20via%20a%20Mean%20Teacher%20and%20Gaussian%20Mixture%20Model-Based%20Pseudo-Labeling&entry.906535625=Pascal%20Schlachter%20and%20Bin%20Yang&entry.1292438233=Unsupervised%20domain%20adaptation%20tackles%20the%20problem%20that%20domain%20shifts%20between%20training%20and%20test%20data%20impair%20the%20performance%20of%20neural%20networks%20in%20many%20real-world%20applications.%20Thereby%2C%20in%20realistic%20scenarios%2C%20the%20source%20data%20may%20no%20longer%20be%20available%20during%20adaptation%2C%20and%20the%20label%20space%20of%20the%20target%20domain%20may%20differ%20from%20the%20source%20label%20space.%20This%20setting%2C%20known%20as%20source-free%20universal%20domain%20adaptation%20%28SF-UniDA%29%2C%20has%20recently%20gained%20attention%2C%20but%20all%20existing%20approaches%20only%20assume%20a%20single%20domain%20shift%20from%20source%20to%20target.%20In%20this%20work%2C%20we%20present%20the%20first%20study%20on%20continual%20SF-UniDA%2C%20where%20the%20model%20must%20adapt%20sequentially%20to%20a%20stream%20of%20multiple%20different%20unlabeled%20target%20domains.%20Building%20upon%20our%20previous%20methods%20for%20online%20SF-UniDA%2C%20we%20combine%20their%20key%20ideas%20by%20integrating%20Gaussian%20mixture%20model-based%20pseudo-labeling%20within%20a%20mean%20teacher%20framework%20for%20improved%20stability%20over%20long%20adaptation%20sequences.%20Additionally%2C%20we%20introduce%20consistency%20losses%20for%20further%20robustness.%20The%20resulting%20method%20GMM-COMET%20provides%20a%20strong%20first%20baseline%20for%20continual%20SF-UniDA%20and%20is%20the%20only%20approach%20in%20our%20experiments%20to%20consistently%20improve%20upon%20the%20source-only%20model%20across%20all%20evaluated%20scenarios.%20Our%20code%20is%20available%20at%20https%3A//github.com/pascalschlachter/GMM-COMET.&entry.1838667208=http%3A//arxiv.org/abs/2601.11161v1&entry.124074799=Read"},
{"title": "Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization", "author": "Henrik Hose and Paul Brunzema and Alexander von Rohr and Alexander Gr\u00e4fe and Angela P. Schoellig and Sebastian Trimpe", "abstract": "Approximate model-predictive control (AMPC) aims to imitate an MPC's behavior with a neural network, removing the need to solve an expensive optimization problem at runtime. However, during deployment, the parameters of the underlying MPC must usually be fine-tuned. This often renders AMPC impractical as it requires repeatedly generating a new dataset and retraining the neural network. Recent work addresses this problem by adapting AMPC without retraining using approximated sensitivities of the MPC's optimization problem. Currently, this adaption must be done by hand, which is labor-intensive and can be unintuitive for high-dimensional systems. To solve this issue, we propose using Bayesian optimization to tune the parameters of AMPC policies based on experimental data. By combining model-based control with direct and local learning, our approach achieves superior performance to nominal AMPC on hardware, with minimal experimentation. This allows automatic and data-efficient adaptation of AMPC to new system instances and fine-tuning to cost functions that are difficult to directly implement in MPC. We demonstrate the proposed method in hardware experiments for the swing-up maneuver on an inverted cartpole and yaw control of an under-actuated balancing unicycle robot, a challenging control problem.", "link": "http://arxiv.org/abs/2512.14350v3", "date": "2026-01-16", "relevancy": 2.0954, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5601}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5339}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuning%20of%20Neural%20Network%20Approximate%20MPC%20without%20Retraining%20via%20Bayesian%20Optimization&body=Title%3A%20Fine-Tuning%20of%20Neural%20Network%20Approximate%20MPC%20without%20Retraining%20via%20Bayesian%20Optimization%0AAuthor%3A%20Henrik%20Hose%20and%20Paul%20Brunzema%20and%20Alexander%20von%20Rohr%20and%20Alexander%20Gr%C3%A4fe%20and%20Angela%20P.%20Schoellig%20and%20Sebastian%20Trimpe%0AAbstract%3A%20Approximate%20model-predictive%20control%20%28AMPC%29%20aims%20to%20imitate%20an%20MPC%27s%20behavior%20with%20a%20neural%20network%2C%20removing%20the%20need%20to%20solve%20an%20expensive%20optimization%20problem%20at%20runtime.%20However%2C%20during%20deployment%2C%20the%20parameters%20of%20the%20underlying%20MPC%20must%20usually%20be%20fine-tuned.%20This%20often%20renders%20AMPC%20impractical%20as%20it%20requires%20repeatedly%20generating%20a%20new%20dataset%20and%20retraining%20the%20neural%20network.%20Recent%20work%20addresses%20this%20problem%20by%20adapting%20AMPC%20without%20retraining%20using%20approximated%20sensitivities%20of%20the%20MPC%27s%20optimization%20problem.%20Currently%2C%20this%20adaption%20must%20be%20done%20by%20hand%2C%20which%20is%20labor-intensive%20and%20can%20be%20unintuitive%20for%20high-dimensional%20systems.%20To%20solve%20this%20issue%2C%20we%20propose%20using%20Bayesian%20optimization%20to%20tune%20the%20parameters%20of%20AMPC%20policies%20based%20on%20experimental%20data.%20By%20combining%20model-based%20control%20with%20direct%20and%20local%20learning%2C%20our%20approach%20achieves%20superior%20performance%20to%20nominal%20AMPC%20on%20hardware%2C%20with%20minimal%20experimentation.%20This%20allows%20automatic%20and%20data-efficient%20adaptation%20of%20AMPC%20to%20new%20system%20instances%20and%20fine-tuning%20to%20cost%20functions%20that%20are%20difficult%20to%20directly%20implement%20in%20MPC.%20We%20demonstrate%20the%20proposed%20method%20in%20hardware%20experiments%20for%20the%20swing-up%20maneuver%20on%20an%20inverted%20cartpole%20and%20yaw%20control%20of%20an%20under-actuated%20balancing%20unicycle%20robot%2C%20a%20challenging%20control%20problem.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14350v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuning%2520of%2520Neural%2520Network%2520Approximate%2520MPC%2520without%2520Retraining%2520via%2520Bayesian%2520Optimization%26entry.906535625%3DHenrik%2520Hose%2520and%2520Paul%2520Brunzema%2520and%2520Alexander%2520von%2520Rohr%2520and%2520Alexander%2520Gr%25C3%25A4fe%2520and%2520Angela%2520P.%2520Schoellig%2520and%2520Sebastian%2520Trimpe%26entry.1292438233%3DApproximate%2520model-predictive%2520control%2520%2528AMPC%2529%2520aims%2520to%2520imitate%2520an%2520MPC%2527s%2520behavior%2520with%2520a%2520neural%2520network%252C%2520removing%2520the%2520need%2520to%2520solve%2520an%2520expensive%2520optimization%2520problem%2520at%2520runtime.%2520However%252C%2520during%2520deployment%252C%2520the%2520parameters%2520of%2520the%2520underlying%2520MPC%2520must%2520usually%2520be%2520fine-tuned.%2520This%2520often%2520renders%2520AMPC%2520impractical%2520as%2520it%2520requires%2520repeatedly%2520generating%2520a%2520new%2520dataset%2520and%2520retraining%2520the%2520neural%2520network.%2520Recent%2520work%2520addresses%2520this%2520problem%2520by%2520adapting%2520AMPC%2520without%2520retraining%2520using%2520approximated%2520sensitivities%2520of%2520the%2520MPC%2527s%2520optimization%2520problem.%2520Currently%252C%2520this%2520adaption%2520must%2520be%2520done%2520by%2520hand%252C%2520which%2520is%2520labor-intensive%2520and%2520can%2520be%2520unintuitive%2520for%2520high-dimensional%2520systems.%2520To%2520solve%2520this%2520issue%252C%2520we%2520propose%2520using%2520Bayesian%2520optimization%2520to%2520tune%2520the%2520parameters%2520of%2520AMPC%2520policies%2520based%2520on%2520experimental%2520data.%2520By%2520combining%2520model-based%2520control%2520with%2520direct%2520and%2520local%2520learning%252C%2520our%2520approach%2520achieves%2520superior%2520performance%2520to%2520nominal%2520AMPC%2520on%2520hardware%252C%2520with%2520minimal%2520experimentation.%2520This%2520allows%2520automatic%2520and%2520data-efficient%2520adaptation%2520of%2520AMPC%2520to%2520new%2520system%2520instances%2520and%2520fine-tuning%2520to%2520cost%2520functions%2520that%2520are%2520difficult%2520to%2520directly%2520implement%2520in%2520MPC.%2520We%2520demonstrate%2520the%2520proposed%2520method%2520in%2520hardware%2520experiments%2520for%2520the%2520swing-up%2520maneuver%2520on%2520an%2520inverted%2520cartpole%2520and%2520yaw%2520control%2520of%2520an%2520under-actuated%2520balancing%2520unicycle%2520robot%252C%2520a%2520challenging%2520control%2520problem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14350v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuning%20of%20Neural%20Network%20Approximate%20MPC%20without%20Retraining%20via%20Bayesian%20Optimization&entry.906535625=Henrik%20Hose%20and%20Paul%20Brunzema%20and%20Alexander%20von%20Rohr%20and%20Alexander%20Gr%C3%A4fe%20and%20Angela%20P.%20Schoellig%20and%20Sebastian%20Trimpe&entry.1292438233=Approximate%20model-predictive%20control%20%28AMPC%29%20aims%20to%20imitate%20an%20MPC%27s%20behavior%20with%20a%20neural%20network%2C%20removing%20the%20need%20to%20solve%20an%20expensive%20optimization%20problem%20at%20runtime.%20However%2C%20during%20deployment%2C%20the%20parameters%20of%20the%20underlying%20MPC%20must%20usually%20be%20fine-tuned.%20This%20often%20renders%20AMPC%20impractical%20as%20it%20requires%20repeatedly%20generating%20a%20new%20dataset%20and%20retraining%20the%20neural%20network.%20Recent%20work%20addresses%20this%20problem%20by%20adapting%20AMPC%20without%20retraining%20using%20approximated%20sensitivities%20of%20the%20MPC%27s%20optimization%20problem.%20Currently%2C%20this%20adaption%20must%20be%20done%20by%20hand%2C%20which%20is%20labor-intensive%20and%20can%20be%20unintuitive%20for%20high-dimensional%20systems.%20To%20solve%20this%20issue%2C%20we%20propose%20using%20Bayesian%20optimization%20to%20tune%20the%20parameters%20of%20AMPC%20policies%20based%20on%20experimental%20data.%20By%20combining%20model-based%20control%20with%20direct%20and%20local%20learning%2C%20our%20approach%20achieves%20superior%20performance%20to%20nominal%20AMPC%20on%20hardware%2C%20with%20minimal%20experimentation.%20This%20allows%20automatic%20and%20data-efficient%20adaptation%20of%20AMPC%20to%20new%20system%20instances%20and%20fine-tuning%20to%20cost%20functions%20that%20are%20difficult%20to%20directly%20implement%20in%20MPC.%20We%20demonstrate%20the%20proposed%20method%20in%20hardware%20experiments%20for%20the%20swing-up%20maneuver%20on%20an%20inverted%20cartpole%20and%20yaw%20control%20of%20an%20under-actuated%20balancing%20unicycle%20robot%2C%20a%20challenging%20control%20problem.&entry.1838667208=http%3A//arxiv.org/abs/2512.14350v3&entry.124074799=Read"},
{"title": "Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration", "author": "Yuejie Li and Ke Yang and Tao Wang and Bolin Chen and Bowen Li and Chengjun Mao", "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between the comprehensiveness of global search and the efficiency of local search. Existing methods are often challenged by navigating large-scale hierarchical graphs, optimizing retrieval paths, and balancing exploration-exploitation dynamics, frequently lacking robust multi-stage re-ranking. To overcome these deficits, we propose Deep GraphRAG, a framework designed for a balanced approach to hierarchical retrieval and adaptive integration. It introduces a hierarchical global-to-local retrieval strategy that integrates macroscopic inter-community and microscopic intra-community contextual relations. This strategy employs a three-stage process: (1) inter-community filtering, which prunes the search space using local context; (2) community-level refinement, which prioritizes relevant subgraphs via entity-interaction analysis; and (3) entity-level fine-grained search within target communities. A beam search-optimized dynamic re-ranking module guides this process, continuously filtering candidates to balance efficiency and global comprehensiveness. Deep GraphRAG also features a Knowledge Integration Module leveraging a compact LLM, trained with Dynamic Weighting Reward GRPO (DW-GRPO). This novel reinforcement learning approach dynamically adjusts reward weights to balance three key objectives: relevance, faithfulness, and conciseness. This training enables compact models (1.5B) to approach the performance of large models (70B) in the integration task. Evaluations on Natural Questions and HotpotQA demonstrate that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency.", "link": "http://arxiv.org/abs/2601.11144v1", "date": "2026-01-16", "relevancy": 2.0715, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.532}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5297}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20GraphRAG%3A%20A%20Balanced%20Approach%20to%20Hierarchical%20Retrieval%20and%20Adaptive%20Integration&body=Title%3A%20Deep%20GraphRAG%3A%20A%20Balanced%20Approach%20to%20Hierarchical%20Retrieval%20and%20Adaptive%20Integration%0AAuthor%3A%20Yuejie%20Li%20and%20Ke%20Yang%20and%20Tao%20Wang%20and%20Bolin%20Chen%20and%20Bowen%20Li%20and%20Chengjun%20Mao%0AAbstract%3A%20Graph-based%20Retrieval-Augmented%20Generation%20%28GraphRAG%29%20frameworks%20face%20a%20trade-off%20between%20the%20comprehensiveness%20of%20global%20search%20and%20the%20efficiency%20of%20local%20search.%20Existing%20methods%20are%20often%20challenged%20by%20navigating%20large-scale%20hierarchical%20graphs%2C%20optimizing%20retrieval%20paths%2C%20and%20balancing%20exploration-exploitation%20dynamics%2C%20frequently%20lacking%20robust%20multi-stage%20re-ranking.%20To%20overcome%20these%20deficits%2C%20we%20propose%20Deep%20GraphRAG%2C%20a%20framework%20designed%20for%20a%20balanced%20approach%20to%20hierarchical%20retrieval%20and%20adaptive%20integration.%20It%20introduces%20a%20hierarchical%20global-to-local%20retrieval%20strategy%20that%20integrates%20macroscopic%20inter-community%20and%20microscopic%20intra-community%20contextual%20relations.%20This%20strategy%20employs%20a%20three-stage%20process%3A%20%281%29%20inter-community%20filtering%2C%20which%20prunes%20the%20search%20space%20using%20local%20context%3B%20%282%29%20community-level%20refinement%2C%20which%20prioritizes%20relevant%20subgraphs%20via%20entity-interaction%20analysis%3B%20and%20%283%29%20entity-level%20fine-grained%20search%20within%20target%20communities.%20A%20beam%20search-optimized%20dynamic%20re-ranking%20module%20guides%20this%20process%2C%20continuously%20filtering%20candidates%20to%20balance%20efficiency%20and%20global%20comprehensiveness.%20Deep%20GraphRAG%20also%20features%20a%20Knowledge%20Integration%20Module%20leveraging%20a%20compact%20LLM%2C%20trained%20with%20Dynamic%20Weighting%20Reward%20GRPO%20%28DW-GRPO%29.%20This%20novel%20reinforcement%20learning%20approach%20dynamically%20adjusts%20reward%20weights%20to%20balance%20three%20key%20objectives%3A%20relevance%2C%20faithfulness%2C%20and%20conciseness.%20This%20training%20enables%20compact%20models%20%281.5B%29%20to%20approach%20the%20performance%20of%20large%20models%20%2870B%29%20in%20the%20integration%20task.%20Evaluations%20on%20Natural%20Questions%20and%20HotpotQA%20demonstrate%20that%20Deep%20GraphRAG%20significantly%20outperforms%20baseline%20graph%20retrieval%20methods%20in%20both%20accuracy%20and%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520GraphRAG%253A%2520A%2520Balanced%2520Approach%2520to%2520Hierarchical%2520Retrieval%2520and%2520Adaptive%2520Integration%26entry.906535625%3DYuejie%2520Li%2520and%2520Ke%2520Yang%2520and%2520Tao%2520Wang%2520and%2520Bolin%2520Chen%2520and%2520Bowen%2520Li%2520and%2520Chengjun%2520Mao%26entry.1292438233%3DGraph-based%2520Retrieval-Augmented%2520Generation%2520%2528GraphRAG%2529%2520frameworks%2520face%2520a%2520trade-off%2520between%2520the%2520comprehensiveness%2520of%2520global%2520search%2520and%2520the%2520efficiency%2520of%2520local%2520search.%2520Existing%2520methods%2520are%2520often%2520challenged%2520by%2520navigating%2520large-scale%2520hierarchical%2520graphs%252C%2520optimizing%2520retrieval%2520paths%252C%2520and%2520balancing%2520exploration-exploitation%2520dynamics%252C%2520frequently%2520lacking%2520robust%2520multi-stage%2520re-ranking.%2520To%2520overcome%2520these%2520deficits%252C%2520we%2520propose%2520Deep%2520GraphRAG%252C%2520a%2520framework%2520designed%2520for%2520a%2520balanced%2520approach%2520to%2520hierarchical%2520retrieval%2520and%2520adaptive%2520integration.%2520It%2520introduces%2520a%2520hierarchical%2520global-to-local%2520retrieval%2520strategy%2520that%2520integrates%2520macroscopic%2520inter-community%2520and%2520microscopic%2520intra-community%2520contextual%2520relations.%2520This%2520strategy%2520employs%2520a%2520three-stage%2520process%253A%2520%25281%2529%2520inter-community%2520filtering%252C%2520which%2520prunes%2520the%2520search%2520space%2520using%2520local%2520context%253B%2520%25282%2529%2520community-level%2520refinement%252C%2520which%2520prioritizes%2520relevant%2520subgraphs%2520via%2520entity-interaction%2520analysis%253B%2520and%2520%25283%2529%2520entity-level%2520fine-grained%2520search%2520within%2520target%2520communities.%2520A%2520beam%2520search-optimized%2520dynamic%2520re-ranking%2520module%2520guides%2520this%2520process%252C%2520continuously%2520filtering%2520candidates%2520to%2520balance%2520efficiency%2520and%2520global%2520comprehensiveness.%2520Deep%2520GraphRAG%2520also%2520features%2520a%2520Knowledge%2520Integration%2520Module%2520leveraging%2520a%2520compact%2520LLM%252C%2520trained%2520with%2520Dynamic%2520Weighting%2520Reward%2520GRPO%2520%2528DW-GRPO%2529.%2520This%2520novel%2520reinforcement%2520learning%2520approach%2520dynamically%2520adjusts%2520reward%2520weights%2520to%2520balance%2520three%2520key%2520objectives%253A%2520relevance%252C%2520faithfulness%252C%2520and%2520conciseness.%2520This%2520training%2520enables%2520compact%2520models%2520%25281.5B%2529%2520to%2520approach%2520the%2520performance%2520of%2520large%2520models%2520%252870B%2529%2520in%2520the%2520integration%2520task.%2520Evaluations%2520on%2520Natural%2520Questions%2520and%2520HotpotQA%2520demonstrate%2520that%2520Deep%2520GraphRAG%2520significantly%2520outperforms%2520baseline%2520graph%2520retrieval%2520methods%2520in%2520both%2520accuracy%2520and%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20GraphRAG%3A%20A%20Balanced%20Approach%20to%20Hierarchical%20Retrieval%20and%20Adaptive%20Integration&entry.906535625=Yuejie%20Li%20and%20Ke%20Yang%20and%20Tao%20Wang%20and%20Bolin%20Chen%20and%20Bowen%20Li%20and%20Chengjun%20Mao&entry.1292438233=Graph-based%20Retrieval-Augmented%20Generation%20%28GraphRAG%29%20frameworks%20face%20a%20trade-off%20between%20the%20comprehensiveness%20of%20global%20search%20and%20the%20efficiency%20of%20local%20search.%20Existing%20methods%20are%20often%20challenged%20by%20navigating%20large-scale%20hierarchical%20graphs%2C%20optimizing%20retrieval%20paths%2C%20and%20balancing%20exploration-exploitation%20dynamics%2C%20frequently%20lacking%20robust%20multi-stage%20re-ranking.%20To%20overcome%20these%20deficits%2C%20we%20propose%20Deep%20GraphRAG%2C%20a%20framework%20designed%20for%20a%20balanced%20approach%20to%20hierarchical%20retrieval%20and%20adaptive%20integration.%20It%20introduces%20a%20hierarchical%20global-to-local%20retrieval%20strategy%20that%20integrates%20macroscopic%20inter-community%20and%20microscopic%20intra-community%20contextual%20relations.%20This%20strategy%20employs%20a%20three-stage%20process%3A%20%281%29%20inter-community%20filtering%2C%20which%20prunes%20the%20search%20space%20using%20local%20context%3B%20%282%29%20community-level%20refinement%2C%20which%20prioritizes%20relevant%20subgraphs%20via%20entity-interaction%20analysis%3B%20and%20%283%29%20entity-level%20fine-grained%20search%20within%20target%20communities.%20A%20beam%20search-optimized%20dynamic%20re-ranking%20module%20guides%20this%20process%2C%20continuously%20filtering%20candidates%20to%20balance%20efficiency%20and%20global%20comprehensiveness.%20Deep%20GraphRAG%20also%20features%20a%20Knowledge%20Integration%20Module%20leveraging%20a%20compact%20LLM%2C%20trained%20with%20Dynamic%20Weighting%20Reward%20GRPO%20%28DW-GRPO%29.%20This%20novel%20reinforcement%20learning%20approach%20dynamically%20adjusts%20reward%20weights%20to%20balance%20three%20key%20objectives%3A%20relevance%2C%20faithfulness%2C%20and%20conciseness.%20This%20training%20enables%20compact%20models%20%281.5B%29%20to%20approach%20the%20performance%20of%20large%20models%20%2870B%29%20in%20the%20integration%20task.%20Evaluations%20on%20Natural%20Questions%20and%20HotpotQA%20demonstrate%20that%20Deep%20GraphRAG%20significantly%20outperforms%20baseline%20graph%20retrieval%20methods%20in%20both%20accuracy%20and%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2601.11144v1&entry.124074799=Read"},
{"title": "Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning", "author": "Hongye Cao and Zhixin Bai and Ziyue Peng and Boyan Wang and Tianpei Yang and Jing Huo and Yuyao Zhang and Yang Gao", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.", "link": "http://arxiv.org/abs/2512.04359v3", "date": "2026-01-16", "relevancy": 2.071, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5233}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Reinforcement%20Learning%20with%20Semantic%20and%20Token%20Entropy%20for%20LLM%20Reasoning&body=Title%3A%20Efficient%20Reinforcement%20Learning%20with%20Semantic%20and%20Token%20Entropy%20for%20LLM%20Reasoning%0AAuthor%3A%20Hongye%20Cao%20and%20Zhixin%20Bai%20and%20Ziyue%20Peng%20and%20Boyan%20Wang%20and%20Tianpei%20Yang%20and%20Jing%20Huo%20and%20Yuyao%20Zhang%20and%20Yang%20Gao%0AAbstract%3A%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20demonstrated%20superior%20performance%20in%20enhancing%20the%20reasoning%20capability%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20this%20accuracy-oriented%20learning%20paradigm%20often%20suffers%20from%20entropy%20collapse%2C%20which%20reduces%20policy%20exploration%20and%20limits%20reasoning%20capabilities.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20efficient%20reinforcement%20learning%20framework%20that%20leverages%20entropy%20signals%20at%20both%20the%20semantic%20and%20token%20levels%20to%20improve%20reasoning.%20From%20the%20data%20perspective%2C%20we%20introduce%20semantic%20entropy-guided%20curriculum%20learning%2C%20organizing%20training%20data%20from%20low%20to%20high%20semantic%20entropy%20to%20guide%20progressive%20optimization%20from%20easier%20to%20more%20challenging%20tasks.%20For%20the%20algorithmic%20design%2C%20we%20adopt%20non-uniform%20token%20treatment%20by%20imposing%20KL%20regularization%20on%20low-entropy%20tokens%20that%20critically%20impact%20policy%20exploration%20and%20applying%20stronger%20constraints%20on%20high-covariance%20portions%20within%20these%20tokens.%20By%20jointly%20optimizing%20data%20organization%20and%20algorithmic%20design%2C%20our%20method%20effectively%20mitigates%20entropy%20collapse%20and%20enhances%20LLM%20reasoning.%20Experimental%20results%20across%206%20benchmarks%20with%203%20different%20parameter-scale%20base%20models%20demonstrate%20that%20our%20method%20outperforms%20other%20entropy-based%20approaches%20in%20improving%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04359v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Reinforcement%2520Learning%2520with%2520Semantic%2520and%2520Token%2520Entropy%2520for%2520LLM%2520Reasoning%26entry.906535625%3DHongye%2520Cao%2520and%2520Zhixin%2520Bai%2520and%2520Ziyue%2520Peng%2520and%2520Boyan%2520Wang%2520and%2520Tianpei%2520Yang%2520and%2520Jing%2520Huo%2520and%2520Yuyao%2520Zhang%2520and%2520Yang%2520Gao%26entry.1292438233%3DReinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520has%2520demonstrated%2520superior%2520performance%2520in%2520enhancing%2520the%2520reasoning%2520capability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520this%2520accuracy-oriented%2520learning%2520paradigm%2520often%2520suffers%2520from%2520entropy%2520collapse%252C%2520which%2520reduces%2520policy%2520exploration%2520and%2520limits%2520reasoning%2520capabilities.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%2520efficient%2520reinforcement%2520learning%2520framework%2520that%2520leverages%2520entropy%2520signals%2520at%2520both%2520the%2520semantic%2520and%2520token%2520levels%2520to%2520improve%2520reasoning.%2520From%2520the%2520data%2520perspective%252C%2520we%2520introduce%2520semantic%2520entropy-guided%2520curriculum%2520learning%252C%2520organizing%2520training%2520data%2520from%2520low%2520to%2520high%2520semantic%2520entropy%2520to%2520guide%2520progressive%2520optimization%2520from%2520easier%2520to%2520more%2520challenging%2520tasks.%2520For%2520the%2520algorithmic%2520design%252C%2520we%2520adopt%2520non-uniform%2520token%2520treatment%2520by%2520imposing%2520KL%2520regularization%2520on%2520low-entropy%2520tokens%2520that%2520critically%2520impact%2520policy%2520exploration%2520and%2520applying%2520stronger%2520constraints%2520on%2520high-covariance%2520portions%2520within%2520these%2520tokens.%2520By%2520jointly%2520optimizing%2520data%2520organization%2520and%2520algorithmic%2520design%252C%2520our%2520method%2520effectively%2520mitigates%2520entropy%2520collapse%2520and%2520enhances%2520LLM%2520reasoning.%2520Experimental%2520results%2520across%25206%2520benchmarks%2520with%25203%2520different%2520parameter-scale%2520base%2520models%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520other%2520entropy-based%2520approaches%2520in%2520improving%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04359v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Reinforcement%20Learning%20with%20Semantic%20and%20Token%20Entropy%20for%20LLM%20Reasoning&entry.906535625=Hongye%20Cao%20and%20Zhixin%20Bai%20and%20Ziyue%20Peng%20and%20Boyan%20Wang%20and%20Tianpei%20Yang%20and%20Jing%20Huo%20and%20Yuyao%20Zhang%20and%20Yang%20Gao&entry.1292438233=Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20demonstrated%20superior%20performance%20in%20enhancing%20the%20reasoning%20capability%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20this%20accuracy-oriented%20learning%20paradigm%20often%20suffers%20from%20entropy%20collapse%2C%20which%20reduces%20policy%20exploration%20and%20limits%20reasoning%20capabilities.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20efficient%20reinforcement%20learning%20framework%20that%20leverages%20entropy%20signals%20at%20both%20the%20semantic%20and%20token%20levels%20to%20improve%20reasoning.%20From%20the%20data%20perspective%2C%20we%20introduce%20semantic%20entropy-guided%20curriculum%20learning%2C%20organizing%20training%20data%20from%20low%20to%20high%20semantic%20entropy%20to%20guide%20progressive%20optimization%20from%20easier%20to%20more%20challenging%20tasks.%20For%20the%20algorithmic%20design%2C%20we%20adopt%20non-uniform%20token%20treatment%20by%20imposing%20KL%20regularization%20on%20low-entropy%20tokens%20that%20critically%20impact%20policy%20exploration%20and%20applying%20stronger%20constraints%20on%20high-covariance%20portions%20within%20these%20tokens.%20By%20jointly%20optimizing%20data%20organization%20and%20algorithmic%20design%2C%20our%20method%20effectively%20mitigates%20entropy%20collapse%20and%20enhances%20LLM%20reasoning.%20Experimental%20results%20across%206%20benchmarks%20with%203%20different%20parameter-scale%20base%20models%20demonstrate%20that%20our%20method%20outperforms%20other%20entropy-based%20approaches%20in%20improving%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2512.04359v3&entry.124074799=Read"},
{"title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models", "author": "Linqing Zhong and Yi Liu and Yifei Wei and Ziyu Xiong and Maoqing Yao and Si Liu and Guanghui Ren", "abstract": "Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.", "link": "http://arxiv.org/abs/2601.11404v1", "date": "2026-01-16", "relevancy": 2.0676, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACoT-VLA%3A%20Action%20Chain-of-Thought%20for%20Vision-Language-Action%20Models&body=Title%3A%20ACoT-VLA%3A%20Action%20Chain-of-Thought%20for%20Vision-Language-Action%20Models%0AAuthor%3A%20Linqing%20Zhong%20and%20Yi%20Liu%20and%20Yifei%20Wei%20and%20Ziyu%20Xiong%20and%20Maoqing%20Yao%20and%20Si%20Liu%20and%20Guanghui%20Ren%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20essential%20generalist%20robot%20policies%20for%20diverse%20manipulation%20tasks%2C%20conventionally%20relying%20on%20directly%20translating%20multimodal%20inputs%20into%20actions%20via%20Vision-Language%20Model%20%28VLM%29%20embeddings.%20Recent%20advancements%20have%20introduced%20explicit%20intermediary%20reasoning%2C%20such%20as%20sub-task%20prediction%20%28language%29%20or%20goal%20image%20synthesis%20%28vision%29%2C%20to%20guide%20action%20generation.%20However%2C%20these%20intermediate%20reasoning%20are%20often%20indirect%20and%20inherently%20limited%20in%20their%20capacity%20to%20convey%20the%20full%2C%20granular%20information%20required%20for%20precise%20action%20execution.%20Instead%2C%20we%20posit%20that%20the%20most%20effective%20form%20of%20reasoning%20is%20one%20that%20deliberates%20directly%20in%20the%20action%20space.%20We%20introduce%20Action%20Chain-of-Thought%20%28ACoT%29%2C%20a%20paradigm%20where%20the%20reasoning%20process%20itself%20is%20formulated%20as%20a%20structured%20sequence%20of%20coarse%20action%20intents%20that%20guide%20the%20final%20policy.%20In%20this%20paper%2C%20we%20propose%20ACoT-VLA%2C%20a%20novel%20architecture%20that%20materializes%20the%20ACoT%20paradigm.%20Specifically%2C%20we%20introduce%20two%20complementary%20components%3A%20an%20Explicit%20Action%20Reasoner%20%28EAR%29%20and%20Implicit%20Action%20Reasoner%20%28IAR%29.%20The%20former%20proposes%20coarse%20reference%20trajectories%20as%20explicit%20action-level%20reasoning%20steps%2C%20while%20the%20latter%20extracts%20latent%20action%20priors%20from%20internal%20representations%20of%20multimodal%20input%2C%20co-forming%20an%20ACoT%20that%20conditions%20the%20downstream%20action%20head%20to%20enable%20grounded%20policy%20learning.%20Extensive%20experiments%20in%20real-world%20and%20simulation%20environments%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%2C%20which%20achieves%2098.5%25%2C%2084.1%25%2C%20and%2047.4%25%20on%20LIBERO%2C%20LIBERO-Plus%20and%20VLABench%2C%20respectively.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACoT-VLA%253A%2520Action%2520Chain-of-Thought%2520for%2520Vision-Language-Action%2520Models%26entry.906535625%3DLinqing%2520Zhong%2520and%2520Yi%2520Liu%2520and%2520Yifei%2520Wei%2520and%2520Ziyu%2520Xiong%2520and%2520Maoqing%2520Yao%2520and%2520Si%2520Liu%2520and%2520Guanghui%2520Ren%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520emerged%2520as%2520essential%2520generalist%2520robot%2520policies%2520for%2520diverse%2520manipulation%2520tasks%252C%2520conventionally%2520relying%2520on%2520directly%2520translating%2520multimodal%2520inputs%2520into%2520actions%2520via%2520Vision-Language%2520Model%2520%2528VLM%2529%2520embeddings.%2520Recent%2520advancements%2520have%2520introduced%2520explicit%2520intermediary%2520reasoning%252C%2520such%2520as%2520sub-task%2520prediction%2520%2528language%2529%2520or%2520goal%2520image%2520synthesis%2520%2528vision%2529%252C%2520to%2520guide%2520action%2520generation.%2520However%252C%2520these%2520intermediate%2520reasoning%2520are%2520often%2520indirect%2520and%2520inherently%2520limited%2520in%2520their%2520capacity%2520to%2520convey%2520the%2520full%252C%2520granular%2520information%2520required%2520for%2520precise%2520action%2520execution.%2520Instead%252C%2520we%2520posit%2520that%2520the%2520most%2520effective%2520form%2520of%2520reasoning%2520is%2520one%2520that%2520deliberates%2520directly%2520in%2520the%2520action%2520space.%2520We%2520introduce%2520Action%2520Chain-of-Thought%2520%2528ACoT%2529%252C%2520a%2520paradigm%2520where%2520the%2520reasoning%2520process%2520itself%2520is%2520formulated%2520as%2520a%2520structured%2520sequence%2520of%2520coarse%2520action%2520intents%2520that%2520guide%2520the%2520final%2520policy.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ACoT-VLA%252C%2520a%2520novel%2520architecture%2520that%2520materializes%2520the%2520ACoT%2520paradigm.%2520Specifically%252C%2520we%2520introduce%2520two%2520complementary%2520components%253A%2520an%2520Explicit%2520Action%2520Reasoner%2520%2528EAR%2529%2520and%2520Implicit%2520Action%2520Reasoner%2520%2528IAR%2529.%2520The%2520former%2520proposes%2520coarse%2520reference%2520trajectories%2520as%2520explicit%2520action-level%2520reasoning%2520steps%252C%2520while%2520the%2520latter%2520extracts%2520latent%2520action%2520priors%2520from%2520internal%2520representations%2520of%2520multimodal%2520input%252C%2520co-forming%2520an%2520ACoT%2520that%2520conditions%2520the%2520downstream%2520action%2520head%2520to%2520enable%2520grounded%2520policy%2520learning.%2520Extensive%2520experiments%2520in%2520real-world%2520and%2520simulation%2520environments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520proposed%2520method%252C%2520which%2520achieves%252098.5%2525%252C%252084.1%2525%252C%2520and%252047.4%2525%2520on%2520LIBERO%252C%2520LIBERO-Plus%2520and%2520VLABench%252C%2520respectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACoT-VLA%3A%20Action%20Chain-of-Thought%20for%20Vision-Language-Action%20Models&entry.906535625=Linqing%20Zhong%20and%20Yi%20Liu%20and%20Yifei%20Wei%20and%20Ziyu%20Xiong%20and%20Maoqing%20Yao%20and%20Si%20Liu%20and%20Guanghui%20Ren&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20essential%20generalist%20robot%20policies%20for%20diverse%20manipulation%20tasks%2C%20conventionally%20relying%20on%20directly%20translating%20multimodal%20inputs%20into%20actions%20via%20Vision-Language%20Model%20%28VLM%29%20embeddings.%20Recent%20advancements%20have%20introduced%20explicit%20intermediary%20reasoning%2C%20such%20as%20sub-task%20prediction%20%28language%29%20or%20goal%20image%20synthesis%20%28vision%29%2C%20to%20guide%20action%20generation.%20However%2C%20these%20intermediate%20reasoning%20are%20often%20indirect%20and%20inherently%20limited%20in%20their%20capacity%20to%20convey%20the%20full%2C%20granular%20information%20required%20for%20precise%20action%20execution.%20Instead%2C%20we%20posit%20that%20the%20most%20effective%20form%20of%20reasoning%20is%20one%20that%20deliberates%20directly%20in%20the%20action%20space.%20We%20introduce%20Action%20Chain-of-Thought%20%28ACoT%29%2C%20a%20paradigm%20where%20the%20reasoning%20process%20itself%20is%20formulated%20as%20a%20structured%20sequence%20of%20coarse%20action%20intents%20that%20guide%20the%20final%20policy.%20In%20this%20paper%2C%20we%20propose%20ACoT-VLA%2C%20a%20novel%20architecture%20that%20materializes%20the%20ACoT%20paradigm.%20Specifically%2C%20we%20introduce%20two%20complementary%20components%3A%20an%20Explicit%20Action%20Reasoner%20%28EAR%29%20and%20Implicit%20Action%20Reasoner%20%28IAR%29.%20The%20former%20proposes%20coarse%20reference%20trajectories%20as%20explicit%20action-level%20reasoning%20steps%2C%20while%20the%20latter%20extracts%20latent%20action%20priors%20from%20internal%20representations%20of%20multimodal%20input%2C%20co-forming%20an%20ACoT%20that%20conditions%20the%20downstream%20action%20head%20to%20enable%20grounded%20policy%20learning.%20Extensive%20experiments%20in%20real-world%20and%20simulation%20environments%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%2C%20which%20achieves%2098.5%25%2C%2084.1%25%2C%20and%2047.4%25%20on%20LIBERO%2C%20LIBERO-Plus%20and%20VLABench%2C%20respectively.&entry.1838667208=http%3A//arxiv.org/abs/2601.11404v1&entry.124074799=Read"},
{"title": "Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models", "author": "Piercosma Bisconti and Matteo Prandi and Federico Pierucci and Francesco Giarrusso and Marcantonio Bracale Syrnikov and Marcello Galisai and Vincenzo Suriani and Olga Sorokoletova and Federico Sartore and Daniele Nardi", "abstract": "We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for Large Language Models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of 3 open-weight LLM judges, whose binary safety assessments were validated on a stratified human-labeled subset. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.", "link": "http://arxiv.org/abs/2511.15304v3", "date": "2026-01-16", "relevancy": 2.0553, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4306}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Poetry%20as%20a%20Universal%20Single-Turn%20Jailbreak%20Mechanism%20in%20Large%20Language%20Models&body=Title%3A%20Adversarial%20Poetry%20as%20a%20Universal%20Single-Turn%20Jailbreak%20Mechanism%20in%20Large%20Language%20Models%0AAuthor%3A%20Piercosma%20Bisconti%20and%20Matteo%20Prandi%20and%20Federico%20Pierucci%20and%20Francesco%20Giarrusso%20and%20Marcantonio%20Bracale%20Syrnikov%20and%20Marcello%20Galisai%20and%20Vincenzo%20Suriani%20and%20Olga%20Sorokoletova%20and%20Federico%20Sartore%20and%20Daniele%20Nardi%0AAbstract%3A%20We%20present%20evidence%20that%20adversarial%20poetry%20functions%20as%20a%20universal%20single-turn%20jailbreak%20technique%20for%20Large%20Language%20Models%20%28LLMs%29.%20Across%2025%20frontier%20proprietary%20and%20open-weight%20models%2C%20curated%20poetic%20prompts%20yielded%20high%20attack-success%20rates%20%28ASR%29%2C%20with%20some%20providers%20exceeding%2090%25.%20Mapping%20prompts%20to%20MLCommons%20and%20EU%20CoP%20risk%20taxonomies%20shows%20that%20poetic%20attacks%20transfer%20across%20CBRN%2C%20manipulation%2C%20cyber-offence%2C%20and%20loss-of-control%20domains.%20Converting%201%2C200%20MLCommons%20harmful%20prompts%20into%20verse%20via%20a%20standardized%20meta-prompt%20produced%20ASRs%20up%20to%2018%20times%20higher%20than%20their%20prose%20baselines.%20Outputs%20are%20evaluated%20using%20an%20ensemble%20of%203%20open-weight%20LLM%20judges%2C%20whose%20binary%20safety%20assessments%20were%20validated%20on%20a%20stratified%20human-labeled%20subset.%20Poetic%20framing%20achieved%20an%20average%20jailbreak%20success%20rate%20of%2062%25%20for%20hand-crafted%20poems%20and%20approximately%2043%25%20for%20meta-prompt%20conversions%20%28compared%20to%20non-poetic%20baselines%29%2C%20substantially%20outperforming%20non-poetic%20baselines%20and%20revealing%20a%20systematic%20vulnerability%20across%20model%20families%20and%20safety%20training%20approaches.%20These%20findings%20demonstrate%20that%20stylistic%20variation%20alone%20can%20circumvent%20contemporary%20safety%20mechanisms%2C%20suggesting%20fundamental%20limitations%20in%20current%20alignment%20methods%20and%20evaluation%20protocols.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15304v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Poetry%2520as%2520a%2520Universal%2520Single-Turn%2520Jailbreak%2520Mechanism%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DPiercosma%2520Bisconti%2520and%2520Matteo%2520Prandi%2520and%2520Federico%2520Pierucci%2520and%2520Francesco%2520Giarrusso%2520and%2520Marcantonio%2520Bracale%2520Syrnikov%2520and%2520Marcello%2520Galisai%2520and%2520Vincenzo%2520Suriani%2520and%2520Olga%2520Sorokoletova%2520and%2520Federico%2520Sartore%2520and%2520Daniele%2520Nardi%26entry.1292438233%3DWe%2520present%2520evidence%2520that%2520adversarial%2520poetry%2520functions%2520as%2520a%2520universal%2520single-turn%2520jailbreak%2520technique%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Across%252025%2520frontier%2520proprietary%2520and%2520open-weight%2520models%252C%2520curated%2520poetic%2520prompts%2520yielded%2520high%2520attack-success%2520rates%2520%2528ASR%2529%252C%2520with%2520some%2520providers%2520exceeding%252090%2525.%2520Mapping%2520prompts%2520to%2520MLCommons%2520and%2520EU%2520CoP%2520risk%2520taxonomies%2520shows%2520that%2520poetic%2520attacks%2520transfer%2520across%2520CBRN%252C%2520manipulation%252C%2520cyber-offence%252C%2520and%2520loss-of-control%2520domains.%2520Converting%25201%252C200%2520MLCommons%2520harmful%2520prompts%2520into%2520verse%2520via%2520a%2520standardized%2520meta-prompt%2520produced%2520ASRs%2520up%2520to%252018%2520times%2520higher%2520than%2520their%2520prose%2520baselines.%2520Outputs%2520are%2520evaluated%2520using%2520an%2520ensemble%2520of%25203%2520open-weight%2520LLM%2520judges%252C%2520whose%2520binary%2520safety%2520assessments%2520were%2520validated%2520on%2520a%2520stratified%2520human-labeled%2520subset.%2520Poetic%2520framing%2520achieved%2520an%2520average%2520jailbreak%2520success%2520rate%2520of%252062%2525%2520for%2520hand-crafted%2520poems%2520and%2520approximately%252043%2525%2520for%2520meta-prompt%2520conversions%2520%2528compared%2520to%2520non-poetic%2520baselines%2529%252C%2520substantially%2520outperforming%2520non-poetic%2520baselines%2520and%2520revealing%2520a%2520systematic%2520vulnerability%2520across%2520model%2520families%2520and%2520safety%2520training%2520approaches.%2520These%2520findings%2520demonstrate%2520that%2520stylistic%2520variation%2520alone%2520can%2520circumvent%2520contemporary%2520safety%2520mechanisms%252C%2520suggesting%2520fundamental%2520limitations%2520in%2520current%2520alignment%2520methods%2520and%2520evaluation%2520protocols.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15304v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Poetry%20as%20a%20Universal%20Single-Turn%20Jailbreak%20Mechanism%20in%20Large%20Language%20Models&entry.906535625=Piercosma%20Bisconti%20and%20Matteo%20Prandi%20and%20Federico%20Pierucci%20and%20Francesco%20Giarrusso%20and%20Marcantonio%20Bracale%20Syrnikov%20and%20Marcello%20Galisai%20and%20Vincenzo%20Suriani%20and%20Olga%20Sorokoletova%20and%20Federico%20Sartore%20and%20Daniele%20Nardi&entry.1292438233=We%20present%20evidence%20that%20adversarial%20poetry%20functions%20as%20a%20universal%20single-turn%20jailbreak%20technique%20for%20Large%20Language%20Models%20%28LLMs%29.%20Across%2025%20frontier%20proprietary%20and%20open-weight%20models%2C%20curated%20poetic%20prompts%20yielded%20high%20attack-success%20rates%20%28ASR%29%2C%20with%20some%20providers%20exceeding%2090%25.%20Mapping%20prompts%20to%20MLCommons%20and%20EU%20CoP%20risk%20taxonomies%20shows%20that%20poetic%20attacks%20transfer%20across%20CBRN%2C%20manipulation%2C%20cyber-offence%2C%20and%20loss-of-control%20domains.%20Converting%201%2C200%20MLCommons%20harmful%20prompts%20into%20verse%20via%20a%20standardized%20meta-prompt%20produced%20ASRs%20up%20to%2018%20times%20higher%20than%20their%20prose%20baselines.%20Outputs%20are%20evaluated%20using%20an%20ensemble%20of%203%20open-weight%20LLM%20judges%2C%20whose%20binary%20safety%20assessments%20were%20validated%20on%20a%20stratified%20human-labeled%20subset.%20Poetic%20framing%20achieved%20an%20average%20jailbreak%20success%20rate%20of%2062%25%20for%20hand-crafted%20poems%20and%20approximately%2043%25%20for%20meta-prompt%20conversions%20%28compared%20to%20non-poetic%20baselines%29%2C%20substantially%20outperforming%20non-poetic%20baselines%20and%20revealing%20a%20systematic%20vulnerability%20across%20model%20families%20and%20safety%20training%20approaches.%20These%20findings%20demonstrate%20that%20stylistic%20variation%20alone%20can%20circumvent%20contemporary%20safety%20mechanisms%2C%20suggesting%20fundamental%20limitations%20in%20current%20alignment%20methods%20and%20evaluation%20protocols.&entry.1838667208=http%3A//arxiv.org/abs/2511.15304v3&entry.124074799=Read"},
{"title": "Adaptive Monitoring of Stochastic Fire Front Processes via Information-seeking Predictive Control", "author": "Savvas Papaioannou and Panayiotis Kolios and Christos G. Panayiotou and Marios M. Polycarpou", "abstract": "We consider the problem of adaptively monitoring a wildfire front using a mobile agent (e.g., a drone), whose trajectory determines where sensor data is collected and thus influences the accuracy of fire propagation estimation. This is a challenging problem, as the stochastic nature of wildfire evolution requires the seamless integration of sensing, estimation, and control, often treated separately in existing methods. State-of-the-art methods either impose linear-Gaussian assumptions to establish optimality or rely on approximations and heuristics, often without providing explicit performance guarantees. To address these limitations, we formulate the fire front monitoring task as a stochastic optimal control problem that integrates sensing, estimation, and control. We derive an optimal recursive Bayesian estimator for a class of stochastic nonlinear elliptical-growth fire front models. Subsequently, we transform the resulting nonlinear stochastic control problem into a finite-horizon Markov decision process and design an information-seeking predictive control law obtained via a lower confidence bound-based adaptive search algorithm with asymptotic convergence to the optimal policy.", "link": "http://arxiv.org/abs/2601.11231v1", "date": "2026-01-16", "relevancy": 2.0538, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.535}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5323}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Monitoring%20of%20Stochastic%20Fire%20Front%20Processes%20via%20Information-seeking%20Predictive%20Control&body=Title%3A%20Adaptive%20Monitoring%20of%20Stochastic%20Fire%20Front%20Processes%20via%20Information-seeking%20Predictive%20Control%0AAuthor%3A%20Savvas%20Papaioannou%20and%20Panayiotis%20Kolios%20and%20Christos%20G.%20Panayiotou%20and%20Marios%20M.%20Polycarpou%0AAbstract%3A%20We%20consider%20the%20problem%20of%20adaptively%20monitoring%20a%20wildfire%20front%20using%20a%20mobile%20agent%20%28e.g.%2C%20a%20drone%29%2C%20whose%20trajectory%20determines%20where%20sensor%20data%20is%20collected%20and%20thus%20influences%20the%20accuracy%20of%20fire%20propagation%20estimation.%20This%20is%20a%20challenging%20problem%2C%20as%20the%20stochastic%20nature%20of%20wildfire%20evolution%20requires%20the%20seamless%20integration%20of%20sensing%2C%20estimation%2C%20and%20control%2C%20often%20treated%20separately%20in%20existing%20methods.%20State-of-the-art%20methods%20either%20impose%20linear-Gaussian%20assumptions%20to%20establish%20optimality%20or%20rely%20on%20approximations%20and%20heuristics%2C%20often%20without%20providing%20explicit%20performance%20guarantees.%20To%20address%20these%20limitations%2C%20we%20formulate%20the%20fire%20front%20monitoring%20task%20as%20a%20stochastic%20optimal%20control%20problem%20that%20integrates%20sensing%2C%20estimation%2C%20and%20control.%20We%20derive%20an%20optimal%20recursive%20Bayesian%20estimator%20for%20a%20class%20of%20stochastic%20nonlinear%20elliptical-growth%20fire%20front%20models.%20Subsequently%2C%20we%20transform%20the%20resulting%20nonlinear%20stochastic%20control%20problem%20into%20a%20finite-horizon%20Markov%20decision%20process%20and%20design%20an%20information-seeking%20predictive%20control%20law%20obtained%20via%20a%20lower%20confidence%20bound-based%20adaptive%20search%20algorithm%20with%20asymptotic%20convergence%20to%20the%20optimal%20policy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Monitoring%2520of%2520Stochastic%2520Fire%2520Front%2520Processes%2520via%2520Information-seeking%2520Predictive%2520Control%26entry.906535625%3DSavvas%2520Papaioannou%2520and%2520Panayiotis%2520Kolios%2520and%2520Christos%2520G.%2520Panayiotou%2520and%2520Marios%2520M.%2520Polycarpou%26entry.1292438233%3DWe%2520consider%2520the%2520problem%2520of%2520adaptively%2520monitoring%2520a%2520wildfire%2520front%2520using%2520a%2520mobile%2520agent%2520%2528e.g.%252C%2520a%2520drone%2529%252C%2520whose%2520trajectory%2520determines%2520where%2520sensor%2520data%2520is%2520collected%2520and%2520thus%2520influences%2520the%2520accuracy%2520of%2520fire%2520propagation%2520estimation.%2520This%2520is%2520a%2520challenging%2520problem%252C%2520as%2520the%2520stochastic%2520nature%2520of%2520wildfire%2520evolution%2520requires%2520the%2520seamless%2520integration%2520of%2520sensing%252C%2520estimation%252C%2520and%2520control%252C%2520often%2520treated%2520separately%2520in%2520existing%2520methods.%2520State-of-the-art%2520methods%2520either%2520impose%2520linear-Gaussian%2520assumptions%2520to%2520establish%2520optimality%2520or%2520rely%2520on%2520approximations%2520and%2520heuristics%252C%2520often%2520without%2520providing%2520explicit%2520performance%2520guarantees.%2520To%2520address%2520these%2520limitations%252C%2520we%2520formulate%2520the%2520fire%2520front%2520monitoring%2520task%2520as%2520a%2520stochastic%2520optimal%2520control%2520problem%2520that%2520integrates%2520sensing%252C%2520estimation%252C%2520and%2520control.%2520We%2520derive%2520an%2520optimal%2520recursive%2520Bayesian%2520estimator%2520for%2520a%2520class%2520of%2520stochastic%2520nonlinear%2520elliptical-growth%2520fire%2520front%2520models.%2520Subsequently%252C%2520we%2520transform%2520the%2520resulting%2520nonlinear%2520stochastic%2520control%2520problem%2520into%2520a%2520finite-horizon%2520Markov%2520decision%2520process%2520and%2520design%2520an%2520information-seeking%2520predictive%2520control%2520law%2520obtained%2520via%2520a%2520lower%2520confidence%2520bound-based%2520adaptive%2520search%2520algorithm%2520with%2520asymptotic%2520convergence%2520to%2520the%2520optimal%2520policy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Monitoring%20of%20Stochastic%20Fire%20Front%20Processes%20via%20Information-seeking%20Predictive%20Control&entry.906535625=Savvas%20Papaioannou%20and%20Panayiotis%20Kolios%20and%20Christos%20G.%20Panayiotou%20and%20Marios%20M.%20Polycarpou&entry.1292438233=We%20consider%20the%20problem%20of%20adaptively%20monitoring%20a%20wildfire%20front%20using%20a%20mobile%20agent%20%28e.g.%2C%20a%20drone%29%2C%20whose%20trajectory%20determines%20where%20sensor%20data%20is%20collected%20and%20thus%20influences%20the%20accuracy%20of%20fire%20propagation%20estimation.%20This%20is%20a%20challenging%20problem%2C%20as%20the%20stochastic%20nature%20of%20wildfire%20evolution%20requires%20the%20seamless%20integration%20of%20sensing%2C%20estimation%2C%20and%20control%2C%20often%20treated%20separately%20in%20existing%20methods.%20State-of-the-art%20methods%20either%20impose%20linear-Gaussian%20assumptions%20to%20establish%20optimality%20or%20rely%20on%20approximations%20and%20heuristics%2C%20often%20without%20providing%20explicit%20performance%20guarantees.%20To%20address%20these%20limitations%2C%20we%20formulate%20the%20fire%20front%20monitoring%20task%20as%20a%20stochastic%20optimal%20control%20problem%20that%20integrates%20sensing%2C%20estimation%2C%20and%20control.%20We%20derive%20an%20optimal%20recursive%20Bayesian%20estimator%20for%20a%20class%20of%20stochastic%20nonlinear%20elliptical-growth%20fire%20front%20models.%20Subsequently%2C%20we%20transform%20the%20resulting%20nonlinear%20stochastic%20control%20problem%20into%20a%20finite-horizon%20Markov%20decision%20process%20and%20design%20an%20information-seeking%20predictive%20control%20law%20obtained%20via%20a%20lower%20confidence%20bound-based%20adaptive%20search%20algorithm%20with%20asymptotic%20convergence%20to%20the%20optimal%20policy.&entry.1838667208=http%3A//arxiv.org/abs/2601.11231v1&entry.124074799=Read"},
{"title": "Efficient LLM Collaboration via Planning", "author": "Byeongchan Lee and Jonghoon Lee and Dongyoung Kim and Jaehyung Kim and Kyungjoon Park and Dongjun Lee and Jinwoo Shin", "abstract": "Recently, large language models (LLMs) have demonstrated strong performance, ranging from simple to complex tasks. However, while large proprietary models (e.g., models with over 100B parameters) achieve remarkable results across diverse tasks, they are often accessible through costly APIs, making frequent use too costly for many applications. In contrast, small open-source models (e.g., models with fewer than 3B parameters) are freely available and easy to deploy locally, but their performance on complex tasks remains limited. This trade-off raises a natural question: how can small and large models efficiently collaborate to combine their complementary strengths? To bridge this trade-off, we propose COPE, a test-time collaboration framework. A planner model first generates a plan that serves as a lightweight intermediate that guides a downstream executor model. Small and large models take turns acting as planner and executor, exchanging plans in a multi-stage cascade to collaboratively solve tasks. Through comprehensive experiments on benchmarks spanning mathematical reasoning, code generation, open-ended tasks, and agent tasks, we demonstrate that COPE achieves performance comparable to large proprietary models, while drastically reducing the inference API cost. These results highlight planning as an effective prior for cost-efficient inference.", "link": "http://arxiv.org/abs/2506.11578v3", "date": "2026-01-16", "relevancy": 2.0498, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.519}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20LLM%20Collaboration%20via%20Planning&body=Title%3A%20Efficient%20LLM%20Collaboration%20via%20Planning%0AAuthor%3A%20Byeongchan%20Lee%20and%20Jonghoon%20Lee%20and%20Dongyoung%20Kim%20and%20Jaehyung%20Kim%20and%20Kyungjoon%20Park%20and%20Dongjun%20Lee%20and%20Jinwoo%20Shin%0AAbstract%3A%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%20performance%2C%20ranging%20from%20simple%20to%20complex%20tasks.%20However%2C%20while%20large%20proprietary%20models%20%28e.g.%2C%20models%20with%20over%20100B%20parameters%29%20achieve%20remarkable%20results%20across%20diverse%20tasks%2C%20they%20are%20often%20accessible%20through%20costly%20APIs%2C%20making%20frequent%20use%20too%20costly%20for%20many%20applications.%20In%20contrast%2C%20small%20open-source%20models%20%28e.g.%2C%20models%20with%20fewer%20than%203B%20parameters%29%20are%20freely%20available%20and%20easy%20to%20deploy%20locally%2C%20but%20their%20performance%20on%20complex%20tasks%20remains%20limited.%20This%20trade-off%20raises%20a%20natural%20question%3A%20how%20can%20small%20and%20large%20models%20efficiently%20collaborate%20to%20combine%20their%20complementary%20strengths%3F%20To%20bridge%20this%20trade-off%2C%20we%20propose%20COPE%2C%20a%20test-time%20collaboration%20framework.%20A%20planner%20model%20first%20generates%20a%20plan%20that%20serves%20as%20a%20lightweight%20intermediate%20that%20guides%20a%20downstream%20executor%20model.%20Small%20and%20large%20models%20take%20turns%20acting%20as%20planner%20and%20executor%2C%20exchanging%20plans%20in%20a%20multi-stage%20cascade%20to%20collaboratively%20solve%20tasks.%20Through%20comprehensive%20experiments%20on%20benchmarks%20spanning%20mathematical%20reasoning%2C%20code%20generation%2C%20open-ended%20tasks%2C%20and%20agent%20tasks%2C%20we%20demonstrate%20that%20COPE%20achieves%20performance%20comparable%20to%20large%20proprietary%20models%2C%20while%20drastically%20reducing%20the%20inference%20API%20cost.%20These%20results%20highlight%20planning%20as%20an%20effective%20prior%20for%20cost-efficient%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2506.11578v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520LLM%2520Collaboration%2520via%2520Planning%26entry.906535625%3DByeongchan%2520Lee%2520and%2520Jonghoon%2520Lee%2520and%2520Dongyoung%2520Kim%2520and%2520Jaehyung%2520Kim%2520and%2520Kyungjoon%2520Park%2520and%2520Dongjun%2520Lee%2520and%2520Jinwoo%2520Shin%26entry.1292438233%3DRecently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520performance%252C%2520ranging%2520from%2520simple%2520to%2520complex%2520tasks.%2520However%252C%2520while%2520large%2520proprietary%2520models%2520%2528e.g.%252C%2520models%2520with%2520over%2520100B%2520parameters%2529%2520achieve%2520remarkable%2520results%2520across%2520diverse%2520tasks%252C%2520they%2520are%2520often%2520accessible%2520through%2520costly%2520APIs%252C%2520making%2520frequent%2520use%2520too%2520costly%2520for%2520many%2520applications.%2520In%2520contrast%252C%2520small%2520open-source%2520models%2520%2528e.g.%252C%2520models%2520with%2520fewer%2520than%25203B%2520parameters%2529%2520are%2520freely%2520available%2520and%2520easy%2520to%2520deploy%2520locally%252C%2520but%2520their%2520performance%2520on%2520complex%2520tasks%2520remains%2520limited.%2520This%2520trade-off%2520raises%2520a%2520natural%2520question%253A%2520how%2520can%2520small%2520and%2520large%2520models%2520efficiently%2520collaborate%2520to%2520combine%2520their%2520complementary%2520strengths%253F%2520To%2520bridge%2520this%2520trade-off%252C%2520we%2520propose%2520COPE%252C%2520a%2520test-time%2520collaboration%2520framework.%2520A%2520planner%2520model%2520first%2520generates%2520a%2520plan%2520that%2520serves%2520as%2520a%2520lightweight%2520intermediate%2520that%2520guides%2520a%2520downstream%2520executor%2520model.%2520Small%2520and%2520large%2520models%2520take%2520turns%2520acting%2520as%2520planner%2520and%2520executor%252C%2520exchanging%2520plans%2520in%2520a%2520multi-stage%2520cascade%2520to%2520collaboratively%2520solve%2520tasks.%2520Through%2520comprehensive%2520experiments%2520on%2520benchmarks%2520spanning%2520mathematical%2520reasoning%252C%2520code%2520generation%252C%2520open-ended%2520tasks%252C%2520and%2520agent%2520tasks%252C%2520we%2520demonstrate%2520that%2520COPE%2520achieves%2520performance%2520comparable%2520to%2520large%2520proprietary%2520models%252C%2520while%2520drastically%2520reducing%2520the%2520inference%2520API%2520cost.%2520These%2520results%2520highlight%2520planning%2520as%2520an%2520effective%2520prior%2520for%2520cost-efficient%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11578v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20LLM%20Collaboration%20via%20Planning&entry.906535625=Byeongchan%20Lee%20and%20Jonghoon%20Lee%20and%20Dongyoung%20Kim%20and%20Jaehyung%20Kim%20and%20Kyungjoon%20Park%20and%20Dongjun%20Lee%20and%20Jinwoo%20Shin&entry.1292438233=Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%20performance%2C%20ranging%20from%20simple%20to%20complex%20tasks.%20However%2C%20while%20large%20proprietary%20models%20%28e.g.%2C%20models%20with%20over%20100B%20parameters%29%20achieve%20remarkable%20results%20across%20diverse%20tasks%2C%20they%20are%20often%20accessible%20through%20costly%20APIs%2C%20making%20frequent%20use%20too%20costly%20for%20many%20applications.%20In%20contrast%2C%20small%20open-source%20models%20%28e.g.%2C%20models%20with%20fewer%20than%203B%20parameters%29%20are%20freely%20available%20and%20easy%20to%20deploy%20locally%2C%20but%20their%20performance%20on%20complex%20tasks%20remains%20limited.%20This%20trade-off%20raises%20a%20natural%20question%3A%20how%20can%20small%20and%20large%20models%20efficiently%20collaborate%20to%20combine%20their%20complementary%20strengths%3F%20To%20bridge%20this%20trade-off%2C%20we%20propose%20COPE%2C%20a%20test-time%20collaboration%20framework.%20A%20planner%20model%20first%20generates%20a%20plan%20that%20serves%20as%20a%20lightweight%20intermediate%20that%20guides%20a%20downstream%20executor%20model.%20Small%20and%20large%20models%20take%20turns%20acting%20as%20planner%20and%20executor%2C%20exchanging%20plans%20in%20a%20multi-stage%20cascade%20to%20collaboratively%20solve%20tasks.%20Through%20comprehensive%20experiments%20on%20benchmarks%20spanning%20mathematical%20reasoning%2C%20code%20generation%2C%20open-ended%20tasks%2C%20and%20agent%20tasks%2C%20we%20demonstrate%20that%20COPE%20achieves%20performance%20comparable%20to%20large%20proprietary%20models%2C%20while%20drastically%20reducing%20the%20inference%20API%20cost.%20These%20results%20highlight%20planning%20as%20an%20effective%20prior%20for%20cost-efficient%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2506.11578v3&entry.124074799=Read"},
{"title": "SME-YOLO: A Real-Time Detector for Tiny Defect Detection on PCB Surfaces", "author": "Meng Han", "abstract": "Surface defects on Printed Circuit Boards (PCBs) directly compromise product reliability and safety. However, achieving high-precision detection is challenging because PCB defects are typically characterized by tiny sizes, high texture similarity, and uneven scale distributions. To address these challenges, this paper proposes a novel framework based on YOLOv11n, named SME-YOLO (Small-target Multi-scale Enhanced YOLO). First, we employ the Normalized Wasserstein Distance Loss (NWDLoss). This metric effectively mitigates the sensitivity of Intersection over Union (IoU) to positional deviations in tiny objects. Second, the original upsampling module is replaced by the Efficient Upsampling Convolution Block (EUCB). By utilizing multi-scale convolutions, the EUCB gradually recovers spatial resolution and enhances the preservation of edge and texture details for tiny defects. Finally, this paper proposes the Multi-Scale Focused Attention (MSFA) module. Tailored to the specific spatial distribution of PCB defects, this module adaptively strengthens perception within key scale intervals, achieving efficient fusion of local fine-grained features and global context information. Experimental results on the PKU-PCB dataset demonstrate that SME-YOLO achieves state-of-the-art performance. Specifically, compared to the baseline YOLOv11n, SME-YOLO improves mAP by 2.2% and Precision by 4%, validating the effectiveness of the proposed method.", "link": "http://arxiv.org/abs/2601.11402v1", "date": "2026-01-16", "relevancy": 2.0217, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5202}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.503}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SME-YOLO%3A%20A%20Real-Time%20Detector%20for%20Tiny%20Defect%20Detection%20on%20PCB%20Surfaces&body=Title%3A%20SME-YOLO%3A%20A%20Real-Time%20Detector%20for%20Tiny%20Defect%20Detection%20on%20PCB%20Surfaces%0AAuthor%3A%20Meng%20Han%0AAbstract%3A%20Surface%20defects%20on%20Printed%20Circuit%20Boards%20%28PCBs%29%20directly%20compromise%20product%20reliability%20and%20safety.%20However%2C%20achieving%20high-precision%20detection%20is%20challenging%20because%20PCB%20defects%20are%20typically%20characterized%20by%20tiny%20sizes%2C%20high%20texture%20similarity%2C%20and%20uneven%20scale%20distributions.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20novel%20framework%20based%20on%20YOLOv11n%2C%20named%20SME-YOLO%20%28Small-target%20Multi-scale%20Enhanced%20YOLO%29.%20First%2C%20we%20employ%20the%20Normalized%20Wasserstein%20Distance%20Loss%20%28NWDLoss%29.%20This%20metric%20effectively%20mitigates%20the%20sensitivity%20of%20Intersection%20over%20Union%20%28IoU%29%20to%20positional%20deviations%20in%20tiny%20objects.%20Second%2C%20the%20original%20upsampling%20module%20is%20replaced%20by%20the%20Efficient%20Upsampling%20Convolution%20Block%20%28EUCB%29.%20By%20utilizing%20multi-scale%20convolutions%2C%20the%20EUCB%20gradually%20recovers%20spatial%20resolution%20and%20enhances%20the%20preservation%20of%20edge%20and%20texture%20details%20for%20tiny%20defects.%20Finally%2C%20this%20paper%20proposes%20the%20Multi-Scale%20Focused%20Attention%20%28MSFA%29%20module.%20Tailored%20to%20the%20specific%20spatial%20distribution%20of%20PCB%20defects%2C%20this%20module%20adaptively%20strengthens%20perception%20within%20key%20scale%20intervals%2C%20achieving%20efficient%20fusion%20of%20local%20fine-grained%20features%20and%20global%20context%20information.%20Experimental%20results%20on%20the%20PKU-PCB%20dataset%20demonstrate%20that%20SME-YOLO%20achieves%20state-of-the-art%20performance.%20Specifically%2C%20compared%20to%20the%20baseline%20YOLOv11n%2C%20SME-YOLO%20improves%20mAP%20by%202.2%25%20and%20Precision%20by%204%25%2C%20validating%20the%20effectiveness%20of%20the%20proposed%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSME-YOLO%253A%2520A%2520Real-Time%2520Detector%2520for%2520Tiny%2520Defect%2520Detection%2520on%2520PCB%2520Surfaces%26entry.906535625%3DMeng%2520Han%26entry.1292438233%3DSurface%2520defects%2520on%2520Printed%2520Circuit%2520Boards%2520%2528PCBs%2529%2520directly%2520compromise%2520product%2520reliability%2520and%2520safety.%2520However%252C%2520achieving%2520high-precision%2520detection%2520is%2520challenging%2520because%2520PCB%2520defects%2520are%2520typically%2520characterized%2520by%2520tiny%2520sizes%252C%2520high%2520texture%2520similarity%252C%2520and%2520uneven%2520scale%2520distributions.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520framework%2520based%2520on%2520YOLOv11n%252C%2520named%2520SME-YOLO%2520%2528Small-target%2520Multi-scale%2520Enhanced%2520YOLO%2529.%2520First%252C%2520we%2520employ%2520the%2520Normalized%2520Wasserstein%2520Distance%2520Loss%2520%2528NWDLoss%2529.%2520This%2520metric%2520effectively%2520mitigates%2520the%2520sensitivity%2520of%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520to%2520positional%2520deviations%2520in%2520tiny%2520objects.%2520Second%252C%2520the%2520original%2520upsampling%2520module%2520is%2520replaced%2520by%2520the%2520Efficient%2520Upsampling%2520Convolution%2520Block%2520%2528EUCB%2529.%2520By%2520utilizing%2520multi-scale%2520convolutions%252C%2520the%2520EUCB%2520gradually%2520recovers%2520spatial%2520resolution%2520and%2520enhances%2520the%2520preservation%2520of%2520edge%2520and%2520texture%2520details%2520for%2520tiny%2520defects.%2520Finally%252C%2520this%2520paper%2520proposes%2520the%2520Multi-Scale%2520Focused%2520Attention%2520%2528MSFA%2529%2520module.%2520Tailored%2520to%2520the%2520specific%2520spatial%2520distribution%2520of%2520PCB%2520defects%252C%2520this%2520module%2520adaptively%2520strengthens%2520perception%2520within%2520key%2520scale%2520intervals%252C%2520achieving%2520efficient%2520fusion%2520of%2520local%2520fine-grained%2520features%2520and%2520global%2520context%2520information.%2520Experimental%2520results%2520on%2520the%2520PKU-PCB%2520dataset%2520demonstrate%2520that%2520SME-YOLO%2520achieves%2520state-of-the-art%2520performance.%2520Specifically%252C%2520compared%2520to%2520the%2520baseline%2520YOLOv11n%252C%2520SME-YOLO%2520improves%2520mAP%2520by%25202.2%2525%2520and%2520Precision%2520by%25204%2525%252C%2520validating%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SME-YOLO%3A%20A%20Real-Time%20Detector%20for%20Tiny%20Defect%20Detection%20on%20PCB%20Surfaces&entry.906535625=Meng%20Han&entry.1292438233=Surface%20defects%20on%20Printed%20Circuit%20Boards%20%28PCBs%29%20directly%20compromise%20product%20reliability%20and%20safety.%20However%2C%20achieving%20high-precision%20detection%20is%20challenging%20because%20PCB%20defects%20are%20typically%20characterized%20by%20tiny%20sizes%2C%20high%20texture%20similarity%2C%20and%20uneven%20scale%20distributions.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20novel%20framework%20based%20on%20YOLOv11n%2C%20named%20SME-YOLO%20%28Small-target%20Multi-scale%20Enhanced%20YOLO%29.%20First%2C%20we%20employ%20the%20Normalized%20Wasserstein%20Distance%20Loss%20%28NWDLoss%29.%20This%20metric%20effectively%20mitigates%20the%20sensitivity%20of%20Intersection%20over%20Union%20%28IoU%29%20to%20positional%20deviations%20in%20tiny%20objects.%20Second%2C%20the%20original%20upsampling%20module%20is%20replaced%20by%20the%20Efficient%20Upsampling%20Convolution%20Block%20%28EUCB%29.%20By%20utilizing%20multi-scale%20convolutions%2C%20the%20EUCB%20gradually%20recovers%20spatial%20resolution%20and%20enhances%20the%20preservation%20of%20edge%20and%20texture%20details%20for%20tiny%20defects.%20Finally%2C%20this%20paper%20proposes%20the%20Multi-Scale%20Focused%20Attention%20%28MSFA%29%20module.%20Tailored%20to%20the%20specific%20spatial%20distribution%20of%20PCB%20defects%2C%20this%20module%20adaptively%20strengthens%20perception%20within%20key%20scale%20intervals%2C%20achieving%20efficient%20fusion%20of%20local%20fine-grained%20features%20and%20global%20context%20information.%20Experimental%20results%20on%20the%20PKU-PCB%20dataset%20demonstrate%20that%20SME-YOLO%20achieves%20state-of-the-art%20performance.%20Specifically%2C%20compared%20to%20the%20baseline%20YOLOv11n%2C%20SME-YOLO%20improves%20mAP%20by%202.2%25%20and%20Precision%20by%204%25%2C%20validating%20the%20effectiveness%20of%20the%20proposed%20method.&entry.1838667208=http%3A//arxiv.org/abs/2601.11402v1&entry.124074799=Read"},
{"title": "AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks", "author": "Hangwei Zhang and Zhimu Huang and Yan Wang", "abstract": "Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving partial differential equations (PDEs). Yet their original formulation is computationally and memory intensive, motivating the introduction of Chebyshev Type-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed the vanilla KANs architecture, our rigorous theoretical analysis reveals that they still suffer from rank collapse, ultimately limiting their expressive capacity. To overcome these limitations, we enhance Chebyshev1KANs by integrating wavelet-activated MLPs with learnable parameters and an internal attention mechanism. We prove that this design preserves a full-rank Jacobian and is capable of approximating solutions to PDEs of arbitrary order. Furthermore, to alleviate the loss instability and imbalance introduced by the Chebyshev polynomial basis, we externally incorporate a Residual Gradient Attention (RGA) mechanism that dynamically re-weights individual loss terms according to their gradient norms and residual magnitudes. By jointly leveraging internal and external attention, we present AC-PKAN, a novel architecture that constitutes an enhancement to weakly supervised Physics-Informed Neural Networks (PINNs) and extends the expressive power of KANs. Experimental results from nine benchmark tasks across three domains show that AC-PKAN consistently outperforms or matches state-of-the-art models such as PINNsFormer, establishing it as a highly effective tool for solving complex real-world engineering problems in zero-data or data-sparse regimes. The code will be made publicly available upon acceptance.", "link": "http://arxiv.org/abs/2505.08687v2", "date": "2026-01-16", "relevancy": 2.0153, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5197}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5156}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AC-PKAN%3A%20Attention-Enhanced%20and%20Chebyshev%20Polynomial-Based%20Physics-Informed%20Kolmogorov-Arnold%20Networks&body=Title%3A%20AC-PKAN%3A%20Attention-Enhanced%20and%20Chebyshev%20Polynomial-Based%20Physics-Informed%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Hangwei%20Zhang%20and%20Zhimu%20Huang%20and%20Yan%20Wang%0AAbstract%3A%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20have%20recently%20shown%20promise%20for%20solving%20partial%20differential%20equations%20%28PDEs%29.%20Yet%20their%20original%20formulation%20is%20computationally%20and%20memory%20intensive%2C%20motivating%20the%20introduction%20of%20Chebyshev%20Type-I-based%20KANs%20%28Chebyshev1KANs%29.%20Although%20Chebyshev1KANs%20have%20outperformed%20the%20vanilla%20KANs%20architecture%2C%20our%20rigorous%20theoretical%20analysis%20reveals%20that%20they%20still%20suffer%20from%20rank%20collapse%2C%20ultimately%20limiting%20their%20expressive%20capacity.%20To%20overcome%20these%20limitations%2C%20we%20enhance%20Chebyshev1KANs%20by%20integrating%20wavelet-activated%20MLPs%20with%20learnable%20parameters%20and%20an%20internal%20attention%20mechanism.%20We%20prove%20that%20this%20design%20preserves%20a%20full-rank%20Jacobian%20and%20is%20capable%20of%20approximating%20solutions%20to%20PDEs%20of%20arbitrary%20order.%20Furthermore%2C%20to%20alleviate%20the%20loss%20instability%20and%20imbalance%20introduced%20by%20the%20Chebyshev%20polynomial%20basis%2C%20we%20externally%20incorporate%20a%20Residual%20Gradient%20Attention%20%28RGA%29%20mechanism%20that%20dynamically%20re-weights%20individual%20loss%20terms%20according%20to%20their%20gradient%20norms%20and%20residual%20magnitudes.%20By%20jointly%20leveraging%20internal%20and%20external%20attention%2C%20we%20present%20AC-PKAN%2C%20a%20novel%20architecture%20that%20constitutes%20an%20enhancement%20to%20weakly%20supervised%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20and%20extends%20the%20expressive%20power%20of%20KANs.%20Experimental%20results%20from%20nine%20benchmark%20tasks%20across%20three%20domains%20show%20that%20AC-PKAN%20consistently%20outperforms%20or%20matches%20state-of-the-art%20models%20such%20as%20PINNsFormer%2C%20establishing%20it%20as%20a%20highly%20effective%20tool%20for%20solving%20complex%20real-world%20engineering%20problems%20in%20zero-data%20or%20data-sparse%20regimes.%20The%20code%20will%20be%20made%20publicly%20available%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2505.08687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAC-PKAN%253A%2520Attention-Enhanced%2520and%2520Chebyshev%2520Polynomial-Based%2520Physics-Informed%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DHangwei%2520Zhang%2520and%2520Zhimu%2520Huang%2520and%2520Yan%2520Wang%26entry.1292438233%3DKolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520have%2520recently%2520shown%2520promise%2520for%2520solving%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%2520Yet%2520their%2520original%2520formulation%2520is%2520computationally%2520and%2520memory%2520intensive%252C%2520motivating%2520the%2520introduction%2520of%2520Chebyshev%2520Type-I-based%2520KANs%2520%2528Chebyshev1KANs%2529.%2520Although%2520Chebyshev1KANs%2520have%2520outperformed%2520the%2520vanilla%2520KANs%2520architecture%252C%2520our%2520rigorous%2520theoretical%2520analysis%2520reveals%2520that%2520they%2520still%2520suffer%2520from%2520rank%2520collapse%252C%2520ultimately%2520limiting%2520their%2520expressive%2520capacity.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520enhance%2520Chebyshev1KANs%2520by%2520integrating%2520wavelet-activated%2520MLPs%2520with%2520learnable%2520parameters%2520and%2520an%2520internal%2520attention%2520mechanism.%2520We%2520prove%2520that%2520this%2520design%2520preserves%2520a%2520full-rank%2520Jacobian%2520and%2520is%2520capable%2520of%2520approximating%2520solutions%2520to%2520PDEs%2520of%2520arbitrary%2520order.%2520Furthermore%252C%2520to%2520alleviate%2520the%2520loss%2520instability%2520and%2520imbalance%2520introduced%2520by%2520the%2520Chebyshev%2520polynomial%2520basis%252C%2520we%2520externally%2520incorporate%2520a%2520Residual%2520Gradient%2520Attention%2520%2528RGA%2529%2520mechanism%2520that%2520dynamically%2520re-weights%2520individual%2520loss%2520terms%2520according%2520to%2520their%2520gradient%2520norms%2520and%2520residual%2520magnitudes.%2520By%2520jointly%2520leveraging%2520internal%2520and%2520external%2520attention%252C%2520we%2520present%2520AC-PKAN%252C%2520a%2520novel%2520architecture%2520that%2520constitutes%2520an%2520enhancement%2520to%2520weakly%2520supervised%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520and%2520extends%2520the%2520expressive%2520power%2520of%2520KANs.%2520Experimental%2520results%2520from%2520nine%2520benchmark%2520tasks%2520across%2520three%2520domains%2520show%2520that%2520AC-PKAN%2520consistently%2520outperforms%2520or%2520matches%2520state-of-the-art%2520models%2520such%2520as%2520PINNsFormer%252C%2520establishing%2520it%2520as%2520a%2520highly%2520effective%2520tool%2520for%2520solving%2520complex%2520real-world%2520engineering%2520problems%2520in%2520zero-data%2520or%2520data-sparse%2520regimes.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AC-PKAN%3A%20Attention-Enhanced%20and%20Chebyshev%20Polynomial-Based%20Physics-Informed%20Kolmogorov-Arnold%20Networks&entry.906535625=Hangwei%20Zhang%20and%20Zhimu%20Huang%20and%20Yan%20Wang&entry.1292438233=Kolmogorov-Arnold%20Networks%20%28KANs%29%20have%20recently%20shown%20promise%20for%20solving%20partial%20differential%20equations%20%28PDEs%29.%20Yet%20their%20original%20formulation%20is%20computationally%20and%20memory%20intensive%2C%20motivating%20the%20introduction%20of%20Chebyshev%20Type-I-based%20KANs%20%28Chebyshev1KANs%29.%20Although%20Chebyshev1KANs%20have%20outperformed%20the%20vanilla%20KANs%20architecture%2C%20our%20rigorous%20theoretical%20analysis%20reveals%20that%20they%20still%20suffer%20from%20rank%20collapse%2C%20ultimately%20limiting%20their%20expressive%20capacity.%20To%20overcome%20these%20limitations%2C%20we%20enhance%20Chebyshev1KANs%20by%20integrating%20wavelet-activated%20MLPs%20with%20learnable%20parameters%20and%20an%20internal%20attention%20mechanism.%20We%20prove%20that%20this%20design%20preserves%20a%20full-rank%20Jacobian%20and%20is%20capable%20of%20approximating%20solutions%20to%20PDEs%20of%20arbitrary%20order.%20Furthermore%2C%20to%20alleviate%20the%20loss%20instability%20and%20imbalance%20introduced%20by%20the%20Chebyshev%20polynomial%20basis%2C%20we%20externally%20incorporate%20a%20Residual%20Gradient%20Attention%20%28RGA%29%20mechanism%20that%20dynamically%20re-weights%20individual%20loss%20terms%20according%20to%20their%20gradient%20norms%20and%20residual%20magnitudes.%20By%20jointly%20leveraging%20internal%20and%20external%20attention%2C%20we%20present%20AC-PKAN%2C%20a%20novel%20architecture%20that%20constitutes%20an%20enhancement%20to%20weakly%20supervised%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20and%20extends%20the%20expressive%20power%20of%20KANs.%20Experimental%20results%20from%20nine%20benchmark%20tasks%20across%20three%20domains%20show%20that%20AC-PKAN%20consistently%20outperforms%20or%20matches%20state-of-the-art%20models%20such%20as%20PINNsFormer%2C%20establishing%20it%20as%20a%20highly%20effective%20tool%20for%20solving%20complex%20real-world%20engineering%20problems%20in%20zero-data%20or%20data-sparse%20regimes.%20The%20code%20will%20be%20made%20publicly%20available%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2505.08687v2&entry.124074799=Read"},
{"title": "Let the Void Be Void: Robust Open-Set Semi-Supervised Learning via Selective Non-Alignment", "author": "You Rim Choi and Subeom Park and Seojun Heo and Eunchung Noh and Hyung-Sin Kim", "abstract": "Open-set semi-supervised learning (OSSL) leverages unlabeled data containing both in-distribution (ID) and unknown out-of-distribution (OOD) samples, aiming simultaneously to improve closed-set accuracy and detect novel OOD instances. Existing methods either discard valuable information from uncertain samples or force-align every unlabeled sample into one or a few synthetic \"catch-all\" representations, resulting in geometric collapse and overconfidence on only seen OODs. To address the limitations, we introduce selective non-alignment, adding a novel \"skip\" operator into conventional pull and push operations of contrastive learning. Our framework, SkipAlign, selectively skips alignment (pulling) for low-confidence unlabeled samples, retaining only gentle repulsion against ID prototypes. This approach transforms uncertain samples into a pure repulsion signal, resulting in tighter ID clusters and naturally dispersed OOD features. Extensive experiments demonstrate that SkipAlign significantly outperforms state-of-the-art methods in detecting unseen OOD data without sacrificing ID classification accuracy.", "link": "http://arxiv.org/abs/2504.12569v4", "date": "2026-01-16", "relevancy": 2.0089, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5082}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5014}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%20the%20Void%20Be%20Void%3A%20Robust%20Open-Set%20Semi-Supervised%20Learning%20via%20Selective%20Non-Alignment&body=Title%3A%20Let%20the%20Void%20Be%20Void%3A%20Robust%20Open-Set%20Semi-Supervised%20Learning%20via%20Selective%20Non-Alignment%0AAuthor%3A%20You%20Rim%20Choi%20and%20Subeom%20Park%20and%20Seojun%20Heo%20and%20Eunchung%20Noh%20and%20Hyung-Sin%20Kim%0AAbstract%3A%20Open-set%20semi-supervised%20learning%20%28OSSL%29%20leverages%20unlabeled%20data%20containing%20both%20in-distribution%20%28ID%29%20and%20unknown%20out-of-distribution%20%28OOD%29%20samples%2C%20aiming%20simultaneously%20to%20improve%20closed-set%20accuracy%20and%20detect%20novel%20OOD%20instances.%20Existing%20methods%20either%20discard%20valuable%20information%20from%20uncertain%20samples%20or%20force-align%20every%20unlabeled%20sample%20into%20one%20or%20a%20few%20synthetic%20%22catch-all%22%20representations%2C%20resulting%20in%20geometric%20collapse%20and%20overconfidence%20on%20only%20seen%20OODs.%20To%20address%20the%20limitations%2C%20we%20introduce%20selective%20non-alignment%2C%20adding%20a%20novel%20%22skip%22%20operator%20into%20conventional%20pull%20and%20push%20operations%20of%20contrastive%20learning.%20Our%20framework%2C%20SkipAlign%2C%20selectively%20skips%20alignment%20%28pulling%29%20for%20low-confidence%20unlabeled%20samples%2C%20retaining%20only%20gentle%20repulsion%20against%20ID%20prototypes.%20This%20approach%20transforms%20uncertain%20samples%20into%20a%20pure%20repulsion%20signal%2C%20resulting%20in%20tighter%20ID%20clusters%20and%20naturally%20dispersed%20OOD%20features.%20Extensive%20experiments%20demonstrate%20that%20SkipAlign%20significantly%20outperforms%20state-of-the-art%20methods%20in%20detecting%20unseen%20OOD%20data%20without%20sacrificing%20ID%20classification%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2504.12569v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2520the%2520Void%2520Be%2520Void%253A%2520Robust%2520Open-Set%2520Semi-Supervised%2520Learning%2520via%2520Selective%2520Non-Alignment%26entry.906535625%3DYou%2520Rim%2520Choi%2520and%2520Subeom%2520Park%2520and%2520Seojun%2520Heo%2520and%2520Eunchung%2520Noh%2520and%2520Hyung-Sin%2520Kim%26entry.1292438233%3DOpen-set%2520semi-supervised%2520learning%2520%2528OSSL%2529%2520leverages%2520unlabeled%2520data%2520containing%2520both%2520in-distribution%2520%2528ID%2529%2520and%2520unknown%2520out-of-distribution%2520%2528OOD%2529%2520samples%252C%2520aiming%2520simultaneously%2520to%2520improve%2520closed-set%2520accuracy%2520and%2520detect%2520novel%2520OOD%2520instances.%2520Existing%2520methods%2520either%2520discard%2520valuable%2520information%2520from%2520uncertain%2520samples%2520or%2520force-align%2520every%2520unlabeled%2520sample%2520into%2520one%2520or%2520a%2520few%2520synthetic%2520%2522catch-all%2522%2520representations%252C%2520resulting%2520in%2520geometric%2520collapse%2520and%2520overconfidence%2520on%2520only%2520seen%2520OODs.%2520To%2520address%2520the%2520limitations%252C%2520we%2520introduce%2520selective%2520non-alignment%252C%2520adding%2520a%2520novel%2520%2522skip%2522%2520operator%2520into%2520conventional%2520pull%2520and%2520push%2520operations%2520of%2520contrastive%2520learning.%2520Our%2520framework%252C%2520SkipAlign%252C%2520selectively%2520skips%2520alignment%2520%2528pulling%2529%2520for%2520low-confidence%2520unlabeled%2520samples%252C%2520retaining%2520only%2520gentle%2520repulsion%2520against%2520ID%2520prototypes.%2520This%2520approach%2520transforms%2520uncertain%2520samples%2520into%2520a%2520pure%2520repulsion%2520signal%252C%2520resulting%2520in%2520tighter%2520ID%2520clusters%2520and%2520naturally%2520dispersed%2520OOD%2520features.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SkipAlign%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520in%2520detecting%2520unseen%2520OOD%2520data%2520without%2520sacrificing%2520ID%2520classification%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12569v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%20the%20Void%20Be%20Void%3A%20Robust%20Open-Set%20Semi-Supervised%20Learning%20via%20Selective%20Non-Alignment&entry.906535625=You%20Rim%20Choi%20and%20Subeom%20Park%20and%20Seojun%20Heo%20and%20Eunchung%20Noh%20and%20Hyung-Sin%20Kim&entry.1292438233=Open-set%20semi-supervised%20learning%20%28OSSL%29%20leverages%20unlabeled%20data%20containing%20both%20in-distribution%20%28ID%29%20and%20unknown%20out-of-distribution%20%28OOD%29%20samples%2C%20aiming%20simultaneously%20to%20improve%20closed-set%20accuracy%20and%20detect%20novel%20OOD%20instances.%20Existing%20methods%20either%20discard%20valuable%20information%20from%20uncertain%20samples%20or%20force-align%20every%20unlabeled%20sample%20into%20one%20or%20a%20few%20synthetic%20%22catch-all%22%20representations%2C%20resulting%20in%20geometric%20collapse%20and%20overconfidence%20on%20only%20seen%20OODs.%20To%20address%20the%20limitations%2C%20we%20introduce%20selective%20non-alignment%2C%20adding%20a%20novel%20%22skip%22%20operator%20into%20conventional%20pull%20and%20push%20operations%20of%20contrastive%20learning.%20Our%20framework%2C%20SkipAlign%2C%20selectively%20skips%20alignment%20%28pulling%29%20for%20low-confidence%20unlabeled%20samples%2C%20retaining%20only%20gentle%20repulsion%20against%20ID%20prototypes.%20This%20approach%20transforms%20uncertain%20samples%20into%20a%20pure%20repulsion%20signal%2C%20resulting%20in%20tighter%20ID%20clusters%20and%20naturally%20dispersed%20OOD%20features.%20Extensive%20experiments%20demonstrate%20that%20SkipAlign%20significantly%20outperforms%20state-of-the-art%20methods%20in%20detecting%20unseen%20OOD%20data%20without%20sacrificing%20ID%20classification%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2504.12569v4&entry.124074799=Read"},
{"title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller", "author": "Kirill Djebko and Tom Baumann and Erik Dilger and Frank Puppe and Sergio Montenegro", "abstract": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universit\u00e4t W\u00fcrzburg in cooperation with the Technische Universit\u00e4t Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.", "link": "http://arxiv.org/abs/2512.19576v3", "date": "2026-01-16", "relevancy": 2.0053, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5419}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeLaR%3A%20The%20First%20In-Orbit%20Demonstration%20of%20an%20AI-Based%20Satellite%20Attitude%20Controller&body=Title%3A%20LeLaR%3A%20The%20First%20In-Orbit%20Demonstration%20of%20an%20AI-Based%20Satellite%20Attitude%20Controller%0AAuthor%3A%20Kirill%20Djebko%20and%20Tom%20Baumann%20and%20Erik%20Dilger%20and%20Frank%20Puppe%20and%20Sergio%20Montenegro%0AAbstract%3A%20Attitude%20control%20is%20essential%20for%20many%20satellite%20missions.%20Classical%20controllers%2C%20however%2C%20are%20time-consuming%20to%20design%20and%20sensitive%20to%20model%20uncertainties%20and%20variations%20in%20operational%20boundary%20conditions.%20Deep%20Reinforcement%20Learning%20%28DRL%29%20offers%20a%20promising%20alternative%20by%20learning%20adaptive%20control%20strategies%20through%20autonomous%20interaction%20with%20a%20simulation%20environment.%20Overcoming%20the%20Sim2Real%20gap%2C%20which%20involves%20deploying%20an%20agent%20trained%20in%20simulation%20onto%20the%20real%20physical%20satellite%2C%20remains%20a%20significant%20challenge.%20In%20this%20work%2C%20we%20present%20the%20first%20successful%20in-orbit%20demonstration%20of%20an%20AI-based%20attitude%20controller%20for%20inertial%20pointing%20maneuvers.%20The%20controller%20was%20trained%20entirely%20in%20simulation%20and%20deployed%20to%20the%20InnoCube%203U%20nanosatellite%2C%20which%20was%20developed%20by%20the%20Julius-Maximilians-Universit%C3%A4t%20W%C3%BCrzburg%20in%20cooperation%20with%20the%20Technische%20Universit%C3%A4t%20Berlin%2C%20and%20launched%20in%20January%202025.%20We%20present%20the%20AI%20agent%20design%2C%20the%20methodology%20of%20the%20training%20procedure%2C%20the%20discrepancies%20between%20the%20simulation%20and%20the%20observed%20behavior%20of%20the%20real%20satellite%2C%20and%20a%20comparison%20of%20the%20AI-based%20attitude%20controller%20with%20the%20classical%20PD%20controller%20of%20InnoCube.%20Steady-state%20metrics%20confirm%20the%20robust%20performance%20of%20the%20AI-based%20controller%20during%20repeated%20in-orbit%20maneuvers.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19576v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeLaR%253A%2520The%2520First%2520In-Orbit%2520Demonstration%2520of%2520an%2520AI-Based%2520Satellite%2520Attitude%2520Controller%26entry.906535625%3DKirill%2520Djebko%2520and%2520Tom%2520Baumann%2520and%2520Erik%2520Dilger%2520and%2520Frank%2520Puppe%2520and%2520Sergio%2520Montenegro%26entry.1292438233%3DAttitude%2520control%2520is%2520essential%2520for%2520many%2520satellite%2520missions.%2520Classical%2520controllers%252C%2520however%252C%2520are%2520time-consuming%2520to%2520design%2520and%2520sensitive%2520to%2520model%2520uncertainties%2520and%2520variations%2520in%2520operational%2520boundary%2520conditions.%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520offers%2520a%2520promising%2520alternative%2520by%2520learning%2520adaptive%2520control%2520strategies%2520through%2520autonomous%2520interaction%2520with%2520a%2520simulation%2520environment.%2520Overcoming%2520the%2520Sim2Real%2520gap%252C%2520which%2520involves%2520deploying%2520an%2520agent%2520trained%2520in%2520simulation%2520onto%2520the%2520real%2520physical%2520satellite%252C%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520successful%2520in-orbit%2520demonstration%2520of%2520an%2520AI-based%2520attitude%2520controller%2520for%2520inertial%2520pointing%2520maneuvers.%2520The%2520controller%2520was%2520trained%2520entirely%2520in%2520simulation%2520and%2520deployed%2520to%2520the%2520InnoCube%25203U%2520nanosatellite%252C%2520which%2520was%2520developed%2520by%2520the%2520Julius-Maximilians-Universit%25C3%25A4t%2520W%25C3%25BCrzburg%2520in%2520cooperation%2520with%2520the%2520Technische%2520Universit%25C3%25A4t%2520Berlin%252C%2520and%2520launched%2520in%2520January%25202025.%2520We%2520present%2520the%2520AI%2520agent%2520design%252C%2520the%2520methodology%2520of%2520the%2520training%2520procedure%252C%2520the%2520discrepancies%2520between%2520the%2520simulation%2520and%2520the%2520observed%2520behavior%2520of%2520the%2520real%2520satellite%252C%2520and%2520a%2520comparison%2520of%2520the%2520AI-based%2520attitude%2520controller%2520with%2520the%2520classical%2520PD%2520controller%2520of%2520InnoCube.%2520Steady-state%2520metrics%2520confirm%2520the%2520robust%2520performance%2520of%2520the%2520AI-based%2520controller%2520during%2520repeated%2520in-orbit%2520maneuvers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19576v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeLaR%3A%20The%20First%20In-Orbit%20Demonstration%20of%20an%20AI-Based%20Satellite%20Attitude%20Controller&entry.906535625=Kirill%20Djebko%20and%20Tom%20Baumann%20and%20Erik%20Dilger%20and%20Frank%20Puppe%20and%20Sergio%20Montenegro&entry.1292438233=Attitude%20control%20is%20essential%20for%20many%20satellite%20missions.%20Classical%20controllers%2C%20however%2C%20are%20time-consuming%20to%20design%20and%20sensitive%20to%20model%20uncertainties%20and%20variations%20in%20operational%20boundary%20conditions.%20Deep%20Reinforcement%20Learning%20%28DRL%29%20offers%20a%20promising%20alternative%20by%20learning%20adaptive%20control%20strategies%20through%20autonomous%20interaction%20with%20a%20simulation%20environment.%20Overcoming%20the%20Sim2Real%20gap%2C%20which%20involves%20deploying%20an%20agent%20trained%20in%20simulation%20onto%20the%20real%20physical%20satellite%2C%20remains%20a%20significant%20challenge.%20In%20this%20work%2C%20we%20present%20the%20first%20successful%20in-orbit%20demonstration%20of%20an%20AI-based%20attitude%20controller%20for%20inertial%20pointing%20maneuvers.%20The%20controller%20was%20trained%20entirely%20in%20simulation%20and%20deployed%20to%20the%20InnoCube%203U%20nanosatellite%2C%20which%20was%20developed%20by%20the%20Julius-Maximilians-Universit%C3%A4t%20W%C3%BCrzburg%20in%20cooperation%20with%20the%20Technische%20Universit%C3%A4t%20Berlin%2C%20and%20launched%20in%20January%202025.%20We%20present%20the%20AI%20agent%20design%2C%20the%20methodology%20of%20the%20training%20procedure%2C%20the%20discrepancies%20between%20the%20simulation%20and%20the%20observed%20behavior%20of%20the%20real%20satellite%2C%20and%20a%20comparison%20of%20the%20AI-based%20attitude%20controller%20with%20the%20classical%20PD%20controller%20of%20InnoCube.%20Steady-state%20metrics%20confirm%20the%20robust%20performance%20of%20the%20AI-based%20controller%20during%20repeated%20in-orbit%20maneuvers.&entry.1838667208=http%3A//arxiv.org/abs/2512.19576v3&entry.124074799=Read"},
{"title": "Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation", "author": "Pingzhi Tang and Yiding Wang and Muhan Zhang", "abstract": "Large Language Models (LLMs) face the \"knowledge cutoff\" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.", "link": "http://arxiv.org/abs/2601.11258v1", "date": "2026-01-16", "relevancy": 1.9986, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5087}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4983}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20is%20Not%20Enough%3A%20Injecting%20RL%20Skills%20for%20Continual%20Adaptation&body=Title%3A%20Knowledge%20is%20Not%20Enough%3A%20Injecting%20RL%20Skills%20for%20Continual%20Adaptation%0AAuthor%3A%20Pingzhi%20Tang%20and%20Yiding%20Wang%20and%20Muhan%20Zhang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20face%20the%20%22knowledge%20cutoff%22%20challenge%2C%20where%20their%20frozen%20parametric%20memory%20prevents%20direct%20internalization%20of%20new%20information.%20While%20Supervised%20Fine-Tuning%20%28SFT%29%20is%20commonly%20used%20to%20update%20model%20knowledge%2C%20it%20often%20updates%20factual%20content%20without%20reliably%20improving%20the%20model%27s%20ability%20to%20use%20the%20newly%20incorporated%20information%20for%20question%20answering%20or%20decision-making.%20Reinforcement%20Learning%20%28RL%29%20is%20essential%20for%20acquiring%20reasoning%20skills%3B%20however%2C%20its%20high%20computational%20cost%20makes%20it%20impractical%20for%20efficient%20online%20adaptation.%20We%20empirically%20observe%20that%20the%20parameter%20updates%20induced%20by%20SFT%20and%20RL%20are%20nearly%20orthogonal.%20Based%20on%20this%20observation%2C%20we%20propose%20Parametric%20Skill%20Transfer%20%28PaST%29%2C%20a%20framework%20that%20supports%20modular%20skill%20transfer%20for%20efficient%20and%20effective%20knowledge%20adaptation.%20By%20extracting%20a%20domain-agnostic%20Skill%20Vector%20from%20a%20source%20domain%2C%20we%20can%20linearly%20inject%20knowledge%20manipulation%20skills%20into%20a%20target%20model%20after%20it%20has%20undergone%20lightweight%20SFT%20on%20new%20data.%20Experiments%20on%20knowledge-incorporation%20QA%20%28SQuAD%2C%20LooGLE%29%20and%20agentic%20tool-use%20benchmarks%20%28ToolBench%29%20demonstrate%20the%20effectiveness%20of%20our%20method.%20On%20SQuAD%2C%20PaST%20outperforms%20the%20state-of-the-art%20self-editing%20SFT%20baseline%20by%20up%20to%209.9%20points.%20PaST%20further%20scales%20to%20long-context%20QA%20on%20LooGLE%20with%20an%208.0-point%20absolute%20accuracy%20gain%2C%20and%20improves%20zero-shot%20ToolBench%20success%20rates%20by%20%2B10.3%20points%20on%20average%20with%20consistent%20gains%20across%20tool%20categories%2C%20indicating%20strong%20scalability%20and%20cross-domain%20transferability%20of%20the%20Skill%20Vector.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520is%2520Not%2520Enough%253A%2520Injecting%2520RL%2520Skills%2520for%2520Continual%2520Adaptation%26entry.906535625%3DPingzhi%2520Tang%2520and%2520Yiding%2520Wang%2520and%2520Muhan%2520Zhang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520face%2520the%2520%2522knowledge%2520cutoff%2522%2520challenge%252C%2520where%2520their%2520frozen%2520parametric%2520memory%2520prevents%2520direct%2520internalization%2520of%2520new%2520information.%2520While%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520is%2520commonly%2520used%2520to%2520update%2520model%2520knowledge%252C%2520it%2520often%2520updates%2520factual%2520content%2520without%2520reliably%2520improving%2520the%2520model%2527s%2520ability%2520to%2520use%2520the%2520newly%2520incorporated%2520information%2520for%2520question%2520answering%2520or%2520decision-making.%2520Reinforcement%2520Learning%2520%2528RL%2529%2520is%2520essential%2520for%2520acquiring%2520reasoning%2520skills%253B%2520however%252C%2520its%2520high%2520computational%2520cost%2520makes%2520it%2520impractical%2520for%2520efficient%2520online%2520adaptation.%2520We%2520empirically%2520observe%2520that%2520the%2520parameter%2520updates%2520induced%2520by%2520SFT%2520and%2520RL%2520are%2520nearly%2520orthogonal.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%2520Parametric%2520Skill%2520Transfer%2520%2528PaST%2529%252C%2520a%2520framework%2520that%2520supports%2520modular%2520skill%2520transfer%2520for%2520efficient%2520and%2520effective%2520knowledge%2520adaptation.%2520By%2520extracting%2520a%2520domain-agnostic%2520Skill%2520Vector%2520from%2520a%2520source%2520domain%252C%2520we%2520can%2520linearly%2520inject%2520knowledge%2520manipulation%2520skills%2520into%2520a%2520target%2520model%2520after%2520it%2520has%2520undergone%2520lightweight%2520SFT%2520on%2520new%2520data.%2520Experiments%2520on%2520knowledge-incorporation%2520QA%2520%2528SQuAD%252C%2520LooGLE%2529%2520and%2520agentic%2520tool-use%2520benchmarks%2520%2528ToolBench%2529%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%2520On%2520SQuAD%252C%2520PaST%2520outperforms%2520the%2520state-of-the-art%2520self-editing%2520SFT%2520baseline%2520by%2520up%2520to%25209.9%2520points.%2520PaST%2520further%2520scales%2520to%2520long-context%2520QA%2520on%2520LooGLE%2520with%2520an%25208.0-point%2520absolute%2520accuracy%2520gain%252C%2520and%2520improves%2520zero-shot%2520ToolBench%2520success%2520rates%2520by%2520%252B10.3%2520points%2520on%2520average%2520with%2520consistent%2520gains%2520across%2520tool%2520categories%252C%2520indicating%2520strong%2520scalability%2520and%2520cross-domain%2520transferability%2520of%2520the%2520Skill%2520Vector.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20is%20Not%20Enough%3A%20Injecting%20RL%20Skills%20for%20Continual%20Adaptation&entry.906535625=Pingzhi%20Tang%20and%20Yiding%20Wang%20and%20Muhan%20Zhang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20face%20the%20%22knowledge%20cutoff%22%20challenge%2C%20where%20their%20frozen%20parametric%20memory%20prevents%20direct%20internalization%20of%20new%20information.%20While%20Supervised%20Fine-Tuning%20%28SFT%29%20is%20commonly%20used%20to%20update%20model%20knowledge%2C%20it%20often%20updates%20factual%20content%20without%20reliably%20improving%20the%20model%27s%20ability%20to%20use%20the%20newly%20incorporated%20information%20for%20question%20answering%20or%20decision-making.%20Reinforcement%20Learning%20%28RL%29%20is%20essential%20for%20acquiring%20reasoning%20skills%3B%20however%2C%20its%20high%20computational%20cost%20makes%20it%20impractical%20for%20efficient%20online%20adaptation.%20We%20empirically%20observe%20that%20the%20parameter%20updates%20induced%20by%20SFT%20and%20RL%20are%20nearly%20orthogonal.%20Based%20on%20this%20observation%2C%20we%20propose%20Parametric%20Skill%20Transfer%20%28PaST%29%2C%20a%20framework%20that%20supports%20modular%20skill%20transfer%20for%20efficient%20and%20effective%20knowledge%20adaptation.%20By%20extracting%20a%20domain-agnostic%20Skill%20Vector%20from%20a%20source%20domain%2C%20we%20can%20linearly%20inject%20knowledge%20manipulation%20skills%20into%20a%20target%20model%20after%20it%20has%20undergone%20lightweight%20SFT%20on%20new%20data.%20Experiments%20on%20knowledge-incorporation%20QA%20%28SQuAD%2C%20LooGLE%29%20and%20agentic%20tool-use%20benchmarks%20%28ToolBench%29%20demonstrate%20the%20effectiveness%20of%20our%20method.%20On%20SQuAD%2C%20PaST%20outperforms%20the%20state-of-the-art%20self-editing%20SFT%20baseline%20by%20up%20to%209.9%20points.%20PaST%20further%20scales%20to%20long-context%20QA%20on%20LooGLE%20with%20an%208.0-point%20absolute%20accuracy%20gain%2C%20and%20improves%20zero-shot%20ToolBench%20success%20rates%20by%20%2B10.3%20points%20on%20average%20with%20consistent%20gains%20across%20tool%20categories%2C%20indicating%20strong%20scalability%20and%20cross-domain%20transferability%20of%20the%20Skill%20Vector.&entry.1838667208=http%3A//arxiv.org/abs/2601.11258v1&entry.124074799=Read"},
{"title": "A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning", "author": "Siyuan Guo and Yanchao Sun and Jifeng Hu and Sili Huang and Hechang Chen and Haiyin Piao and Lichao Sun and Yi Chang", "abstract": "Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. In view of this, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples to smoothly bridge offline and online stages. SUNG achieves state-of-the-art online finetuning performance when combined with different offline RL methods, across various environments and datasets in D4RL benchmark. Codes are made publicly available in https://github.com/guosyjlu/SUNG.", "link": "http://arxiv.org/abs/2306.07541v3", "date": "2026-01-16", "relevancy": 1.9901, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5222}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5013}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Unified%20Uncertainty-Guided%20Framework%20for%20Offline-to-Online%20Reinforcement%20Learning&body=Title%3A%20A%20Simple%20Unified%20Uncertainty-Guided%20Framework%20for%20Offline-to-Online%20Reinforcement%20Learning%0AAuthor%3A%20Siyuan%20Guo%20and%20Yanchao%20Sun%20and%20Jifeng%20Hu%20and%20Sili%20Huang%20and%20Hechang%20Chen%20and%20Haiyin%20Piao%20and%20Lichao%20Sun%20and%20Yi%20Chang%0AAbstract%3A%20Offline%20reinforcement%20learning%20%28RL%29%20provides%20a%20promising%20solution%20to%20learning%20an%20agent%20fully%20relying%20on%20a%20data-driven%20paradigm.%20However%2C%20constrained%20by%20the%20limited%20quality%20of%20the%20offline%20dataset%2C%20its%20performance%20is%20often%20sub-optimal.%20Therefore%2C%20it%20is%20desired%20to%20further%20finetune%20the%20agent%20via%20extra%20online%20interactions%20before%20deployment.%20Unfortunately%2C%20offline-to-online%20RL%20can%20be%20challenging%20due%20to%20two%20main%20challenges%3A%20constrained%20exploratory%20behavior%20and%20state-action%20distribution%20shift.%20In%20view%20of%20this%2C%20we%20propose%20a%20Simple%20Unified%20uNcertainty-Guided%20%28SUNG%29%20framework%2C%20which%20naturally%20unifies%20the%20solution%20to%20both%20challenges%20with%20the%20tool%20of%20uncertainty.%20Specifically%2C%20SUNG%20quantifies%20uncertainty%20via%20a%20VAE-based%20state-action%20visitation%20density%20estimator.%20To%20facilitate%20efficient%20exploration%2C%20SUNG%20presents%20a%20practical%20optimistic%20exploration%20strategy%20to%20select%20informative%20actions%20with%20both%20high%20value%20and%20high%20uncertainty.%20Moreover%2C%20SUNG%20develops%20an%20adaptive%20exploitation%20method%20by%20applying%20conservative%20offline%20RL%20objectives%20to%20high-uncertainty%20samples%20and%20standard%20online%20RL%20objectives%20to%20low-uncertainty%20samples%20to%20smoothly%20bridge%20offline%20and%20online%20stages.%20SUNG%20achieves%20state-of-the-art%20online%20finetuning%20performance%20when%20combined%20with%20different%20offline%20RL%20methods%2C%20across%20various%20environments%20and%20datasets%20in%20D4RL%20benchmark.%20Codes%20are%20made%20publicly%20available%20in%20https%3A//github.com/guosyjlu/SUNG.%0ALink%3A%20http%3A//arxiv.org/abs/2306.07541v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Unified%2520Uncertainty-Guided%2520Framework%2520for%2520Offline-to-Online%2520Reinforcement%2520Learning%26entry.906535625%3DSiyuan%2520Guo%2520and%2520Yanchao%2520Sun%2520and%2520Jifeng%2520Hu%2520and%2520Sili%2520Huang%2520and%2520Hechang%2520Chen%2520and%2520Haiyin%2520Piao%2520and%2520Lichao%2520Sun%2520and%2520Yi%2520Chang%26entry.1292438233%3DOffline%2520reinforcement%2520learning%2520%2528RL%2529%2520provides%2520a%2520promising%2520solution%2520to%2520learning%2520an%2520agent%2520fully%2520relying%2520on%2520a%2520data-driven%2520paradigm.%2520However%252C%2520constrained%2520by%2520the%2520limited%2520quality%2520of%2520the%2520offline%2520dataset%252C%2520its%2520performance%2520is%2520often%2520sub-optimal.%2520Therefore%252C%2520it%2520is%2520desired%2520to%2520further%2520finetune%2520the%2520agent%2520via%2520extra%2520online%2520interactions%2520before%2520deployment.%2520Unfortunately%252C%2520offline-to-online%2520RL%2520can%2520be%2520challenging%2520due%2520to%2520two%2520main%2520challenges%253A%2520constrained%2520exploratory%2520behavior%2520and%2520state-action%2520distribution%2520shift.%2520In%2520view%2520of%2520this%252C%2520we%2520propose%2520a%2520Simple%2520Unified%2520uNcertainty-Guided%2520%2528SUNG%2529%2520framework%252C%2520which%2520naturally%2520unifies%2520the%2520solution%2520to%2520both%2520challenges%2520with%2520the%2520tool%2520of%2520uncertainty.%2520Specifically%252C%2520SUNG%2520quantifies%2520uncertainty%2520via%2520a%2520VAE-based%2520state-action%2520visitation%2520density%2520estimator.%2520To%2520facilitate%2520efficient%2520exploration%252C%2520SUNG%2520presents%2520a%2520practical%2520optimistic%2520exploration%2520strategy%2520to%2520select%2520informative%2520actions%2520with%2520both%2520high%2520value%2520and%2520high%2520uncertainty.%2520Moreover%252C%2520SUNG%2520develops%2520an%2520adaptive%2520exploitation%2520method%2520by%2520applying%2520conservative%2520offline%2520RL%2520objectives%2520to%2520high-uncertainty%2520samples%2520and%2520standard%2520online%2520RL%2520objectives%2520to%2520low-uncertainty%2520samples%2520to%2520smoothly%2520bridge%2520offline%2520and%2520online%2520stages.%2520SUNG%2520achieves%2520state-of-the-art%2520online%2520finetuning%2520performance%2520when%2520combined%2520with%2520different%2520offline%2520RL%2520methods%252C%2520across%2520various%2520environments%2520and%2520datasets%2520in%2520D4RL%2520benchmark.%2520Codes%2520are%2520made%2520publicly%2520available%2520in%2520https%253A//github.com/guosyjlu/SUNG.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.07541v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Unified%20Uncertainty-Guided%20Framework%20for%20Offline-to-Online%20Reinforcement%20Learning&entry.906535625=Siyuan%20Guo%20and%20Yanchao%20Sun%20and%20Jifeng%20Hu%20and%20Sili%20Huang%20and%20Hechang%20Chen%20and%20Haiyin%20Piao%20and%20Lichao%20Sun%20and%20Yi%20Chang&entry.1292438233=Offline%20reinforcement%20learning%20%28RL%29%20provides%20a%20promising%20solution%20to%20learning%20an%20agent%20fully%20relying%20on%20a%20data-driven%20paradigm.%20However%2C%20constrained%20by%20the%20limited%20quality%20of%20the%20offline%20dataset%2C%20its%20performance%20is%20often%20sub-optimal.%20Therefore%2C%20it%20is%20desired%20to%20further%20finetune%20the%20agent%20via%20extra%20online%20interactions%20before%20deployment.%20Unfortunately%2C%20offline-to-online%20RL%20can%20be%20challenging%20due%20to%20two%20main%20challenges%3A%20constrained%20exploratory%20behavior%20and%20state-action%20distribution%20shift.%20In%20view%20of%20this%2C%20we%20propose%20a%20Simple%20Unified%20uNcertainty-Guided%20%28SUNG%29%20framework%2C%20which%20naturally%20unifies%20the%20solution%20to%20both%20challenges%20with%20the%20tool%20of%20uncertainty.%20Specifically%2C%20SUNG%20quantifies%20uncertainty%20via%20a%20VAE-based%20state-action%20visitation%20density%20estimator.%20To%20facilitate%20efficient%20exploration%2C%20SUNG%20presents%20a%20practical%20optimistic%20exploration%20strategy%20to%20select%20informative%20actions%20with%20both%20high%20value%20and%20high%20uncertainty.%20Moreover%2C%20SUNG%20develops%20an%20adaptive%20exploitation%20method%20by%20applying%20conservative%20offline%20RL%20objectives%20to%20high-uncertainty%20samples%20and%20standard%20online%20RL%20objectives%20to%20low-uncertainty%20samples%20to%20smoothly%20bridge%20offline%20and%20online%20stages.%20SUNG%20achieves%20state-of-the-art%20online%20finetuning%20performance%20when%20combined%20with%20different%20offline%20RL%20methods%2C%20across%20various%20environments%20and%20datasets%20in%20D4RL%20benchmark.%20Codes%20are%20made%20publicly%20available%20in%20https%3A//github.com/guosyjlu/SUNG.&entry.1838667208=http%3A//arxiv.org/abs/2306.07541v3&entry.124074799=Read"},
{"title": "Distributed Control Barrier Functions for Safe Multi-Vehicle Navigation in Heterogeneous USV Fleets", "author": "Tyler Paine and Brendan Long and Jeremy Wenger and Michael DeFilippo and James Usevitch and Michael Benjamin", "abstract": "Collision avoidance in heterogeneous fleets of uncrewed vessels is challenging because the decision-making processes and controllers often differ between platforms, and it is further complicated by the limitations on sharing trajectories and control values in real-time. This paper presents a pragmatic approach that addresses these issues by adding a control filter on each autonomous vehicle that assumes worst-case behavior from other contacts, including crewed vessels. This distributed safety control filter is developed using control barrier function (CBF) theory and the application is clearly described to ensure explainability of these safety-critical methods. This work compares the worst-case CBF approach with a Collision Regulations (COLREGS) behavior-based approach in simulated encounters. Real-world experiments with three different uncrewed vessels and a human operated vessel were performed to confirm the approach is effective across a range of platforms and is robust to uncooperative behavior from human operators. Results show that combining both CBF methods and COLREGS behaviors achieves the best safety and efficiency.", "link": "http://arxiv.org/abs/2601.11335v1", "date": "2026-01-16", "relevancy": 1.9785, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5097}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4859}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Control%20Barrier%20Functions%20for%20Safe%20Multi-Vehicle%20Navigation%20in%20Heterogeneous%20USV%20Fleets&body=Title%3A%20Distributed%20Control%20Barrier%20Functions%20for%20Safe%20Multi-Vehicle%20Navigation%20in%20Heterogeneous%20USV%20Fleets%0AAuthor%3A%20Tyler%20Paine%20and%20Brendan%20Long%20and%20Jeremy%20Wenger%20and%20Michael%20DeFilippo%20and%20James%20Usevitch%20and%20Michael%20Benjamin%0AAbstract%3A%20Collision%20avoidance%20in%20heterogeneous%20fleets%20of%20uncrewed%20vessels%20is%20challenging%20because%20the%20decision-making%20processes%20and%20controllers%20often%20differ%20between%20platforms%2C%20and%20it%20is%20further%20complicated%20by%20the%20limitations%20on%20sharing%20trajectories%20and%20control%20values%20in%20real-time.%20This%20paper%20presents%20a%20pragmatic%20approach%20that%20addresses%20these%20issues%20by%20adding%20a%20control%20filter%20on%20each%20autonomous%20vehicle%20that%20assumes%20worst-case%20behavior%20from%20other%20contacts%2C%20including%20crewed%20vessels.%20This%20distributed%20safety%20control%20filter%20is%20developed%20using%20control%20barrier%20function%20%28CBF%29%20theory%20and%20the%20application%20is%20clearly%20described%20to%20ensure%20explainability%20of%20these%20safety-critical%20methods.%20This%20work%20compares%20the%20worst-case%20CBF%20approach%20with%20a%20Collision%20Regulations%20%28COLREGS%29%20behavior-based%20approach%20in%20simulated%20encounters.%20Real-world%20experiments%20with%20three%20different%20uncrewed%20vessels%20and%20a%20human%20operated%20vessel%20were%20performed%20to%20confirm%20the%20approach%20is%20effective%20across%20a%20range%20of%20platforms%20and%20is%20robust%20to%20uncooperative%20behavior%20from%20human%20operators.%20Results%20show%20that%20combining%20both%20CBF%20methods%20and%20COLREGS%20behaviors%20achieves%20the%20best%20safety%20and%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Control%2520Barrier%2520Functions%2520for%2520Safe%2520Multi-Vehicle%2520Navigation%2520in%2520Heterogeneous%2520USV%2520Fleets%26entry.906535625%3DTyler%2520Paine%2520and%2520Brendan%2520Long%2520and%2520Jeremy%2520Wenger%2520and%2520Michael%2520DeFilippo%2520and%2520James%2520Usevitch%2520and%2520Michael%2520Benjamin%26entry.1292438233%3DCollision%2520avoidance%2520in%2520heterogeneous%2520fleets%2520of%2520uncrewed%2520vessels%2520is%2520challenging%2520because%2520the%2520decision-making%2520processes%2520and%2520controllers%2520often%2520differ%2520between%2520platforms%252C%2520and%2520it%2520is%2520further%2520complicated%2520by%2520the%2520limitations%2520on%2520sharing%2520trajectories%2520and%2520control%2520values%2520in%2520real-time.%2520This%2520paper%2520presents%2520a%2520pragmatic%2520approach%2520that%2520addresses%2520these%2520issues%2520by%2520adding%2520a%2520control%2520filter%2520on%2520each%2520autonomous%2520vehicle%2520that%2520assumes%2520worst-case%2520behavior%2520from%2520other%2520contacts%252C%2520including%2520crewed%2520vessels.%2520This%2520distributed%2520safety%2520control%2520filter%2520is%2520developed%2520using%2520control%2520barrier%2520function%2520%2528CBF%2529%2520theory%2520and%2520the%2520application%2520is%2520clearly%2520described%2520to%2520ensure%2520explainability%2520of%2520these%2520safety-critical%2520methods.%2520This%2520work%2520compares%2520the%2520worst-case%2520CBF%2520approach%2520with%2520a%2520Collision%2520Regulations%2520%2528COLREGS%2529%2520behavior-based%2520approach%2520in%2520simulated%2520encounters.%2520Real-world%2520experiments%2520with%2520three%2520different%2520uncrewed%2520vessels%2520and%2520a%2520human%2520operated%2520vessel%2520were%2520performed%2520to%2520confirm%2520the%2520approach%2520is%2520effective%2520across%2520a%2520range%2520of%2520platforms%2520and%2520is%2520robust%2520to%2520uncooperative%2520behavior%2520from%2520human%2520operators.%2520Results%2520show%2520that%2520combining%2520both%2520CBF%2520methods%2520and%2520COLREGS%2520behaviors%2520achieves%2520the%2520best%2520safety%2520and%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Control%20Barrier%20Functions%20for%20Safe%20Multi-Vehicle%20Navigation%20in%20Heterogeneous%20USV%20Fleets&entry.906535625=Tyler%20Paine%20and%20Brendan%20Long%20and%20Jeremy%20Wenger%20and%20Michael%20DeFilippo%20and%20James%20Usevitch%20and%20Michael%20Benjamin&entry.1292438233=Collision%20avoidance%20in%20heterogeneous%20fleets%20of%20uncrewed%20vessels%20is%20challenging%20because%20the%20decision-making%20processes%20and%20controllers%20often%20differ%20between%20platforms%2C%20and%20it%20is%20further%20complicated%20by%20the%20limitations%20on%20sharing%20trajectories%20and%20control%20values%20in%20real-time.%20This%20paper%20presents%20a%20pragmatic%20approach%20that%20addresses%20these%20issues%20by%20adding%20a%20control%20filter%20on%20each%20autonomous%20vehicle%20that%20assumes%20worst-case%20behavior%20from%20other%20contacts%2C%20including%20crewed%20vessels.%20This%20distributed%20safety%20control%20filter%20is%20developed%20using%20control%20barrier%20function%20%28CBF%29%20theory%20and%20the%20application%20is%20clearly%20described%20to%20ensure%20explainability%20of%20these%20safety-critical%20methods.%20This%20work%20compares%20the%20worst-case%20CBF%20approach%20with%20a%20Collision%20Regulations%20%28COLREGS%29%20behavior-based%20approach%20in%20simulated%20encounters.%20Real-world%20experiments%20with%20three%20different%20uncrewed%20vessels%20and%20a%20human%20operated%20vessel%20were%20performed%20to%20confirm%20the%20approach%20is%20effective%20across%20a%20range%20of%20platforms%20and%20is%20robust%20to%20uncooperative%20behavior%20from%20human%20operators.%20Results%20show%20that%20combining%20both%20CBF%20methods%20and%20COLREGS%20behaviors%20achieves%20the%20best%20safety%20and%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2601.11335v1&entry.124074799=Read"},
{"title": "SceneFoundry: Generating Interactive Infinite 3D Worlds", "author": "ChunTeng Chen and YiChen Hsu and YiWen Liu and WeiFang Sun and TsaiChing Ni and ChunYi Lee and Min Sun and YuanFu Yang", "abstract": "The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research. project page: https://anc891203.github.io/SceneFoundry-Demo/", "link": "http://arxiv.org/abs/2601.05810v2", "date": "2026-01-16", "relevancy": 1.9756, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.69}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6804}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneFoundry%3A%20Generating%20Interactive%20Infinite%203D%20Worlds&body=Title%3A%20SceneFoundry%3A%20Generating%20Interactive%20Infinite%203D%20Worlds%0AAuthor%3A%20ChunTeng%20Chen%20and%20YiChen%20Hsu%20and%20YiWen%20Liu%20and%20WeiFang%20Sun%20and%20TsaiChing%20Ni%20and%20ChunYi%20Lee%20and%20Min%20Sun%20and%20YuanFu%20Yang%0AAbstract%3A%20The%20ability%20to%20automatically%20generate%20large-scale%2C%20interactive%2C%20and%20physically%20realistic%203D%20environments%20is%20crucial%20for%20advancing%20robotic%20learning%20and%20embodied%20intelligence.%20However%2C%20existing%20generative%20approaches%20often%20fail%20to%20capture%20the%20functional%20complexity%20of%20real-world%20interiors%2C%20particularly%20those%20containing%20articulated%20objects%20with%20movable%20parts%20essential%20for%20manipulation%20and%20navigation.%20This%20paper%20presents%20SceneFoundry%2C%20a%20language-guided%20diffusion%20framework%20that%20generates%20apartment-scale%203D%20worlds%20with%20functionally%20articulated%20furniture%20and%20semantically%20diverse%20layouts%20for%20robotic%20training.%20From%20natural%20language%20prompts%2C%20an%20LLM%20module%20controls%20floor%20layout%20generation%2C%20while%20diffusion-based%20posterior%20sampling%20efficiently%20populates%20the%20scene%20with%20articulated%20assets%20from%20large-scale%203D%20repositories.%20To%20ensure%20physical%20usability%2C%20SceneFoundry%20employs%20differentiable%20guidance%20functions%20to%20regulate%20object%20quantity%2C%20prevent%20articulation%20collisions%2C%20and%20maintain%20sufficient%20walkable%20space%20for%20robotic%20navigation.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20generates%20structurally%20valid%2C%20semantically%20coherent%2C%20and%20functionally%20interactive%20environments%20across%20diverse%20scene%20types%20and%20conditions%2C%20enabling%20scalable%20embodied%20AI%20research.%20project%20page%3A%20https%3A//anc891203.github.io/SceneFoundry-Demo/%0ALink%3A%20http%3A//arxiv.org/abs/2601.05810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneFoundry%253A%2520Generating%2520Interactive%2520Infinite%25203D%2520Worlds%26entry.906535625%3DChunTeng%2520Chen%2520and%2520YiChen%2520Hsu%2520and%2520YiWen%2520Liu%2520and%2520WeiFang%2520Sun%2520and%2520TsaiChing%2520Ni%2520and%2520ChunYi%2520Lee%2520and%2520Min%2520Sun%2520and%2520YuanFu%2520Yang%26entry.1292438233%3DThe%2520ability%2520to%2520automatically%2520generate%2520large-scale%252C%2520interactive%252C%2520and%2520physically%2520realistic%25203D%2520environments%2520is%2520crucial%2520for%2520advancing%2520robotic%2520learning%2520and%2520embodied%2520intelligence.%2520However%252C%2520existing%2520generative%2520approaches%2520often%2520fail%2520to%2520capture%2520the%2520functional%2520complexity%2520of%2520real-world%2520interiors%252C%2520particularly%2520those%2520containing%2520articulated%2520objects%2520with%2520movable%2520parts%2520essential%2520for%2520manipulation%2520and%2520navigation.%2520This%2520paper%2520presents%2520SceneFoundry%252C%2520a%2520language-guided%2520diffusion%2520framework%2520that%2520generates%2520apartment-scale%25203D%2520worlds%2520with%2520functionally%2520articulated%2520furniture%2520and%2520semantically%2520diverse%2520layouts%2520for%2520robotic%2520training.%2520From%2520natural%2520language%2520prompts%252C%2520an%2520LLM%2520module%2520controls%2520floor%2520layout%2520generation%252C%2520while%2520diffusion-based%2520posterior%2520sampling%2520efficiently%2520populates%2520the%2520scene%2520with%2520articulated%2520assets%2520from%2520large-scale%25203D%2520repositories.%2520To%2520ensure%2520physical%2520usability%252C%2520SceneFoundry%2520employs%2520differentiable%2520guidance%2520functions%2520to%2520regulate%2520object%2520quantity%252C%2520prevent%2520articulation%2520collisions%252C%2520and%2520maintain%2520sufficient%2520walkable%2520space%2520for%2520robotic%2520navigation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520framework%2520generates%2520structurally%2520valid%252C%2520semantically%2520coherent%252C%2520and%2520functionally%2520interactive%2520environments%2520across%2520diverse%2520scene%2520types%2520and%2520conditions%252C%2520enabling%2520scalable%2520embodied%2520AI%2520research.%2520project%2520page%253A%2520https%253A//anc891203.github.io/SceneFoundry-Demo/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneFoundry%3A%20Generating%20Interactive%20Infinite%203D%20Worlds&entry.906535625=ChunTeng%20Chen%20and%20YiChen%20Hsu%20and%20YiWen%20Liu%20and%20WeiFang%20Sun%20and%20TsaiChing%20Ni%20and%20ChunYi%20Lee%20and%20Min%20Sun%20and%20YuanFu%20Yang&entry.1292438233=The%20ability%20to%20automatically%20generate%20large-scale%2C%20interactive%2C%20and%20physically%20realistic%203D%20environments%20is%20crucial%20for%20advancing%20robotic%20learning%20and%20embodied%20intelligence.%20However%2C%20existing%20generative%20approaches%20often%20fail%20to%20capture%20the%20functional%20complexity%20of%20real-world%20interiors%2C%20particularly%20those%20containing%20articulated%20objects%20with%20movable%20parts%20essential%20for%20manipulation%20and%20navigation.%20This%20paper%20presents%20SceneFoundry%2C%20a%20language-guided%20diffusion%20framework%20that%20generates%20apartment-scale%203D%20worlds%20with%20functionally%20articulated%20furniture%20and%20semantically%20diverse%20layouts%20for%20robotic%20training.%20From%20natural%20language%20prompts%2C%20an%20LLM%20module%20controls%20floor%20layout%20generation%2C%20while%20diffusion-based%20posterior%20sampling%20efficiently%20populates%20the%20scene%20with%20articulated%20assets%20from%20large-scale%203D%20repositories.%20To%20ensure%20physical%20usability%2C%20SceneFoundry%20employs%20differentiable%20guidance%20functions%20to%20regulate%20object%20quantity%2C%20prevent%20articulation%20collisions%2C%20and%20maintain%20sufficient%20walkable%20space%20for%20robotic%20navigation.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20generates%20structurally%20valid%2C%20semantically%20coherent%2C%20and%20functionally%20interactive%20environments%20across%20diverse%20scene%20types%20and%20conditions%2C%20enabling%20scalable%20embodied%20AI%20research.%20project%20page%3A%20https%3A//anc891203.github.io/SceneFoundry-Demo/&entry.1838667208=http%3A//arxiv.org/abs/2601.05810v2&entry.124074799=Read"},
{"title": "Democratizing planetary-scale analysis: An ultra-lightweight Earth embedding database for accurate and flexible global land monitoring", "author": "Shuang Chen and Jie Wang and Shuai Yuan and Jiayang Li and Yu Xia and Yuanhong Liao and Junbo Wei and Jincheng Yuan and Xiaoqing Xu and Xiaolin Zhu and Peng Zhu and Hongsheng Zhang and Yuyu Zhou and Haohuan Fu and Huabing Huang and Bin Chen and Fan Dai and Peng Gong", "abstract": "The rapid evolution of satellite-borne Earth Observation (EO) systems has revolutionized terrestrial monitoring, yielding petabyte-scale archives. However, the immense computational and storage requirements for global-scale analysis often preclude widespread use, hindering planetary-scale studies. To address these barriers, we present Embedded Seamless Data (ESD), an ultra-lightweight, 30-m global Earth embedding database spanning the 25-year period from 2000 to 2024. By transforming high-dimensional, multi-sensor observations from the Landsat series (5, 7, 8, and 9) and MODIS Terra into information-dense, quantized latent vectors, ESD distills essential geophysical and semantic features into a unified latent space. Utilizing the ESDNet architecture and Finite Scalar Quantization (FSQ), the dataset achieves a transformative ~340-fold reduction in data volume compared to raw archives. This compression allows the entire global land surface for a single year to be encapsulated within approximately 2.4 TB, enabling decadal-scale global analysis on standard local workstations. Rigorous validation demonstrates high reconstructive fidelity (MAE: 0.0130; RMSE: 0.0179; CC: 0.8543). By condensing the annual phenological cycle into 12 temporal steps, the embeddings provide inherent denoising and a semantically organized space that outperforms raw reflectance in land-cover classification, achieving 79.74% accuracy (vs. 76.92% for raw fusion). With robust few-shot learning capabilities and longitudinal consistency, ESD provides a versatile foundation for democratizing planetary-scale research and advancing next-generation geospatial artificial intelligence.", "link": "http://arxiv.org/abs/2601.11183v1", "date": "2026-01-16", "relevancy": 1.9655, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4966}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4909}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Democratizing%20planetary-scale%20analysis%3A%20An%20ultra-lightweight%20Earth%20embedding%20database%20for%20accurate%20and%20flexible%20global%20land%20monitoring&body=Title%3A%20Democratizing%20planetary-scale%20analysis%3A%20An%20ultra-lightweight%20Earth%20embedding%20database%20for%20accurate%20and%20flexible%20global%20land%20monitoring%0AAuthor%3A%20Shuang%20Chen%20and%20Jie%20Wang%20and%20Shuai%20Yuan%20and%20Jiayang%20Li%20and%20Yu%20Xia%20and%20Yuanhong%20Liao%20and%20Junbo%20Wei%20and%20Jincheng%20Yuan%20and%20Xiaoqing%20Xu%20and%20Xiaolin%20Zhu%20and%20Peng%20Zhu%20and%20Hongsheng%20Zhang%20and%20Yuyu%20Zhou%20and%20Haohuan%20Fu%20and%20Huabing%20Huang%20and%20Bin%20Chen%20and%20Fan%20Dai%20and%20Peng%20Gong%0AAbstract%3A%20The%20rapid%20evolution%20of%20satellite-borne%20Earth%20Observation%20%28EO%29%20systems%20has%20revolutionized%20terrestrial%20monitoring%2C%20yielding%20petabyte-scale%20archives.%20However%2C%20the%20immense%20computational%20and%20storage%20requirements%20for%20global-scale%20analysis%20often%20preclude%20widespread%20use%2C%20hindering%20planetary-scale%20studies.%20To%20address%20these%20barriers%2C%20we%20present%20Embedded%20Seamless%20Data%20%28ESD%29%2C%20an%20ultra-lightweight%2C%2030-m%20global%20Earth%20embedding%20database%20spanning%20the%2025-year%20period%20from%202000%20to%202024.%20By%20transforming%20high-dimensional%2C%20multi-sensor%20observations%20from%20the%20Landsat%20series%20%285%2C%207%2C%208%2C%20and%209%29%20and%20MODIS%20Terra%20into%20information-dense%2C%20quantized%20latent%20vectors%2C%20ESD%20distills%20essential%20geophysical%20and%20semantic%20features%20into%20a%20unified%20latent%20space.%20Utilizing%20the%20ESDNet%20architecture%20and%20Finite%20Scalar%20Quantization%20%28FSQ%29%2C%20the%20dataset%20achieves%20a%20transformative%20~340-fold%20reduction%20in%20data%20volume%20compared%20to%20raw%20archives.%20This%20compression%20allows%20the%20entire%20global%20land%20surface%20for%20a%20single%20year%20to%20be%20encapsulated%20within%20approximately%202.4%20TB%2C%20enabling%20decadal-scale%20global%20analysis%20on%20standard%20local%20workstations.%20Rigorous%20validation%20demonstrates%20high%20reconstructive%20fidelity%20%28MAE%3A%200.0130%3B%20RMSE%3A%200.0179%3B%20CC%3A%200.8543%29.%20By%20condensing%20the%20annual%20phenological%20cycle%20into%2012%20temporal%20steps%2C%20the%20embeddings%20provide%20inherent%20denoising%20and%20a%20semantically%20organized%20space%20that%20outperforms%20raw%20reflectance%20in%20land-cover%20classification%2C%20achieving%2079.74%25%20accuracy%20%28vs.%2076.92%25%20for%20raw%20fusion%29.%20With%20robust%20few-shot%20learning%20capabilities%20and%20longitudinal%20consistency%2C%20ESD%20provides%20a%20versatile%20foundation%20for%20democratizing%20planetary-scale%20research%20and%20advancing%20next-generation%20geospatial%20artificial%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemocratizing%2520planetary-scale%2520analysis%253A%2520An%2520ultra-lightweight%2520Earth%2520embedding%2520database%2520for%2520accurate%2520and%2520flexible%2520global%2520land%2520monitoring%26entry.906535625%3DShuang%2520Chen%2520and%2520Jie%2520Wang%2520and%2520Shuai%2520Yuan%2520and%2520Jiayang%2520Li%2520and%2520Yu%2520Xia%2520and%2520Yuanhong%2520Liao%2520and%2520Junbo%2520Wei%2520and%2520Jincheng%2520Yuan%2520and%2520Xiaoqing%2520Xu%2520and%2520Xiaolin%2520Zhu%2520and%2520Peng%2520Zhu%2520and%2520Hongsheng%2520Zhang%2520and%2520Yuyu%2520Zhou%2520and%2520Haohuan%2520Fu%2520and%2520Huabing%2520Huang%2520and%2520Bin%2520Chen%2520and%2520Fan%2520Dai%2520and%2520Peng%2520Gong%26entry.1292438233%3DThe%2520rapid%2520evolution%2520of%2520satellite-borne%2520Earth%2520Observation%2520%2528EO%2529%2520systems%2520has%2520revolutionized%2520terrestrial%2520monitoring%252C%2520yielding%2520petabyte-scale%2520archives.%2520However%252C%2520the%2520immense%2520computational%2520and%2520storage%2520requirements%2520for%2520global-scale%2520analysis%2520often%2520preclude%2520widespread%2520use%252C%2520hindering%2520planetary-scale%2520studies.%2520To%2520address%2520these%2520barriers%252C%2520we%2520present%2520Embedded%2520Seamless%2520Data%2520%2528ESD%2529%252C%2520an%2520ultra-lightweight%252C%252030-m%2520global%2520Earth%2520embedding%2520database%2520spanning%2520the%252025-year%2520period%2520from%25202000%2520to%25202024.%2520By%2520transforming%2520high-dimensional%252C%2520multi-sensor%2520observations%2520from%2520the%2520Landsat%2520series%2520%25285%252C%25207%252C%25208%252C%2520and%25209%2529%2520and%2520MODIS%2520Terra%2520into%2520information-dense%252C%2520quantized%2520latent%2520vectors%252C%2520ESD%2520distills%2520essential%2520geophysical%2520and%2520semantic%2520features%2520into%2520a%2520unified%2520latent%2520space.%2520Utilizing%2520the%2520ESDNet%2520architecture%2520and%2520Finite%2520Scalar%2520Quantization%2520%2528FSQ%2529%252C%2520the%2520dataset%2520achieves%2520a%2520transformative%2520~340-fold%2520reduction%2520in%2520data%2520volume%2520compared%2520to%2520raw%2520archives.%2520This%2520compression%2520allows%2520the%2520entire%2520global%2520land%2520surface%2520for%2520a%2520single%2520year%2520to%2520be%2520encapsulated%2520within%2520approximately%25202.4%2520TB%252C%2520enabling%2520decadal-scale%2520global%2520analysis%2520on%2520standard%2520local%2520workstations.%2520Rigorous%2520validation%2520demonstrates%2520high%2520reconstructive%2520fidelity%2520%2528MAE%253A%25200.0130%253B%2520RMSE%253A%25200.0179%253B%2520CC%253A%25200.8543%2529.%2520By%2520condensing%2520the%2520annual%2520phenological%2520cycle%2520into%252012%2520temporal%2520steps%252C%2520the%2520embeddings%2520provide%2520inherent%2520denoising%2520and%2520a%2520semantically%2520organized%2520space%2520that%2520outperforms%2520raw%2520reflectance%2520in%2520land-cover%2520classification%252C%2520achieving%252079.74%2525%2520accuracy%2520%2528vs.%252076.92%2525%2520for%2520raw%2520fusion%2529.%2520With%2520robust%2520few-shot%2520learning%2520capabilities%2520and%2520longitudinal%2520consistency%252C%2520ESD%2520provides%2520a%2520versatile%2520foundation%2520for%2520democratizing%2520planetary-scale%2520research%2520and%2520advancing%2520next-generation%2520geospatial%2520artificial%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Democratizing%20planetary-scale%20analysis%3A%20An%20ultra-lightweight%20Earth%20embedding%20database%20for%20accurate%20and%20flexible%20global%20land%20monitoring&entry.906535625=Shuang%20Chen%20and%20Jie%20Wang%20and%20Shuai%20Yuan%20and%20Jiayang%20Li%20and%20Yu%20Xia%20and%20Yuanhong%20Liao%20and%20Junbo%20Wei%20and%20Jincheng%20Yuan%20and%20Xiaoqing%20Xu%20and%20Xiaolin%20Zhu%20and%20Peng%20Zhu%20and%20Hongsheng%20Zhang%20and%20Yuyu%20Zhou%20and%20Haohuan%20Fu%20and%20Huabing%20Huang%20and%20Bin%20Chen%20and%20Fan%20Dai%20and%20Peng%20Gong&entry.1292438233=The%20rapid%20evolution%20of%20satellite-borne%20Earth%20Observation%20%28EO%29%20systems%20has%20revolutionized%20terrestrial%20monitoring%2C%20yielding%20petabyte-scale%20archives.%20However%2C%20the%20immense%20computational%20and%20storage%20requirements%20for%20global-scale%20analysis%20often%20preclude%20widespread%20use%2C%20hindering%20planetary-scale%20studies.%20To%20address%20these%20barriers%2C%20we%20present%20Embedded%20Seamless%20Data%20%28ESD%29%2C%20an%20ultra-lightweight%2C%2030-m%20global%20Earth%20embedding%20database%20spanning%20the%2025-year%20period%20from%202000%20to%202024.%20By%20transforming%20high-dimensional%2C%20multi-sensor%20observations%20from%20the%20Landsat%20series%20%285%2C%207%2C%208%2C%20and%209%29%20and%20MODIS%20Terra%20into%20information-dense%2C%20quantized%20latent%20vectors%2C%20ESD%20distills%20essential%20geophysical%20and%20semantic%20features%20into%20a%20unified%20latent%20space.%20Utilizing%20the%20ESDNet%20architecture%20and%20Finite%20Scalar%20Quantization%20%28FSQ%29%2C%20the%20dataset%20achieves%20a%20transformative%20~340-fold%20reduction%20in%20data%20volume%20compared%20to%20raw%20archives.%20This%20compression%20allows%20the%20entire%20global%20land%20surface%20for%20a%20single%20year%20to%20be%20encapsulated%20within%20approximately%202.4%20TB%2C%20enabling%20decadal-scale%20global%20analysis%20on%20standard%20local%20workstations.%20Rigorous%20validation%20demonstrates%20high%20reconstructive%20fidelity%20%28MAE%3A%200.0130%3B%20RMSE%3A%200.0179%3B%20CC%3A%200.8543%29.%20By%20condensing%20the%20annual%20phenological%20cycle%20into%2012%20temporal%20steps%2C%20the%20embeddings%20provide%20inherent%20denoising%20and%20a%20semantically%20organized%20space%20that%20outperforms%20raw%20reflectance%20in%20land-cover%20classification%2C%20achieving%2079.74%25%20accuracy%20%28vs.%2076.92%25%20for%20raw%20fusion%29.%20With%20robust%20few-shot%20learning%20capabilities%20and%20longitudinal%20consistency%2C%20ESD%20provides%20a%20versatile%20foundation%20for%20democratizing%20planetary-scale%20research%20and%20advancing%20next-generation%20geospatial%20artificial%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2601.11183v1&entry.124074799=Read"},
{"title": "Do explanations generalize across large reasoning models?", "author": "Koyena Pal and David Bau and Chandan Singh", "abstract": "Large reasoning models (LRMs) produce a textual chain of thought (CoT) in the process of solving a problem, which serves as a potentially powerful tool to understand the problem by surfacing a human-readable, natural-language explanation. However, it is unclear whether these explanations generalize, i.e. whether they capture general patterns about the underlying problem rather than patterns which are esoteric to the LRM. This is a crucial question in understanding or discovering new concepts, e.g. in AI for science. We study this generalization question by evaluating a specific notion of generalizability: whether explanations produced by one LRM induce the same behavior when given to other LRMs. We find that CoT explanations often exhibit this form of generalization (i.e. they increase consistency between LRMs) and that this increased generalization is correlated with human preference rankings and post-training with reinforcement learning. We further analyze the conditions under which explanations yield consistent answers and propose a straightforward, sentence-level ensembling strategy that improves consistency. Taken together, these results prescribe caution when using LRM explanations to yield new insights and outline a framework for characterizing LRM explanation generalization.", "link": "http://arxiv.org/abs/2601.11517v1", "date": "2026-01-16", "relevancy": 1.9591, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4936}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20explanations%20generalize%20across%20large%20reasoning%20models%3F&body=Title%3A%20Do%20explanations%20generalize%20across%20large%20reasoning%20models%3F%0AAuthor%3A%20Koyena%20Pal%20and%20David%20Bau%20and%20Chandan%20Singh%0AAbstract%3A%20Large%20reasoning%20models%20%28LRMs%29%20produce%20a%20textual%20chain%20of%20thought%20%28CoT%29%20in%20the%20process%20of%20solving%20a%20problem%2C%20which%20serves%20as%20a%20potentially%20powerful%20tool%20to%20understand%20the%20problem%20by%20surfacing%20a%20human-readable%2C%20natural-language%20explanation.%20However%2C%20it%20is%20unclear%20whether%20these%20explanations%20generalize%2C%20i.e.%20whether%20they%20capture%20general%20patterns%20about%20the%20underlying%20problem%20rather%20than%20patterns%20which%20are%20esoteric%20to%20the%20LRM.%20This%20is%20a%20crucial%20question%20in%20understanding%20or%20discovering%20new%20concepts%2C%20e.g.%20in%20AI%20for%20science.%20We%20study%20this%20generalization%20question%20by%20evaluating%20a%20specific%20notion%20of%20generalizability%3A%20whether%20explanations%20produced%20by%20one%20LRM%20induce%20the%20same%20behavior%20when%20given%20to%20other%20LRMs.%20We%20find%20that%20CoT%20explanations%20often%20exhibit%20this%20form%20of%20generalization%20%28i.e.%20they%20increase%20consistency%20between%20LRMs%29%20and%20that%20this%20increased%20generalization%20is%20correlated%20with%20human%20preference%20rankings%20and%20post-training%20with%20reinforcement%20learning.%20We%20further%20analyze%20the%20conditions%20under%20which%20explanations%20yield%20consistent%20answers%20and%20propose%20a%20straightforward%2C%20sentence-level%20ensembling%20strategy%20that%20improves%20consistency.%20Taken%20together%2C%20these%20results%20prescribe%20caution%20when%20using%20LRM%20explanations%20to%20yield%20new%20insights%20and%20outline%20a%20framework%20for%20characterizing%20LRM%20explanation%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520explanations%2520generalize%2520across%2520large%2520reasoning%2520models%253F%26entry.906535625%3DKoyena%2520Pal%2520and%2520David%2520Bau%2520and%2520Chandan%2520Singh%26entry.1292438233%3DLarge%2520reasoning%2520models%2520%2528LRMs%2529%2520produce%2520a%2520textual%2520chain%2520of%2520thought%2520%2528CoT%2529%2520in%2520the%2520process%2520of%2520solving%2520a%2520problem%252C%2520which%2520serves%2520as%2520a%2520potentially%2520powerful%2520tool%2520to%2520understand%2520the%2520problem%2520by%2520surfacing%2520a%2520human-readable%252C%2520natural-language%2520explanation.%2520However%252C%2520it%2520is%2520unclear%2520whether%2520these%2520explanations%2520generalize%252C%2520i.e.%2520whether%2520they%2520capture%2520general%2520patterns%2520about%2520the%2520underlying%2520problem%2520rather%2520than%2520patterns%2520which%2520are%2520esoteric%2520to%2520the%2520LRM.%2520This%2520is%2520a%2520crucial%2520question%2520in%2520understanding%2520or%2520discovering%2520new%2520concepts%252C%2520e.g.%2520in%2520AI%2520for%2520science.%2520We%2520study%2520this%2520generalization%2520question%2520by%2520evaluating%2520a%2520specific%2520notion%2520of%2520generalizability%253A%2520whether%2520explanations%2520produced%2520by%2520one%2520LRM%2520induce%2520the%2520same%2520behavior%2520when%2520given%2520to%2520other%2520LRMs.%2520We%2520find%2520that%2520CoT%2520explanations%2520often%2520exhibit%2520this%2520form%2520of%2520generalization%2520%2528i.e.%2520they%2520increase%2520consistency%2520between%2520LRMs%2529%2520and%2520that%2520this%2520increased%2520generalization%2520is%2520correlated%2520with%2520human%2520preference%2520rankings%2520and%2520post-training%2520with%2520reinforcement%2520learning.%2520We%2520further%2520analyze%2520the%2520conditions%2520under%2520which%2520explanations%2520yield%2520consistent%2520answers%2520and%2520propose%2520a%2520straightforward%252C%2520sentence-level%2520ensembling%2520strategy%2520that%2520improves%2520consistency.%2520Taken%2520together%252C%2520these%2520results%2520prescribe%2520caution%2520when%2520using%2520LRM%2520explanations%2520to%2520yield%2520new%2520insights%2520and%2520outline%2520a%2520framework%2520for%2520characterizing%2520LRM%2520explanation%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20explanations%20generalize%20across%20large%20reasoning%20models%3F&entry.906535625=Koyena%20Pal%20and%20David%20Bau%20and%20Chandan%20Singh&entry.1292438233=Large%20reasoning%20models%20%28LRMs%29%20produce%20a%20textual%20chain%20of%20thought%20%28CoT%29%20in%20the%20process%20of%20solving%20a%20problem%2C%20which%20serves%20as%20a%20potentially%20powerful%20tool%20to%20understand%20the%20problem%20by%20surfacing%20a%20human-readable%2C%20natural-language%20explanation.%20However%2C%20it%20is%20unclear%20whether%20these%20explanations%20generalize%2C%20i.e.%20whether%20they%20capture%20general%20patterns%20about%20the%20underlying%20problem%20rather%20than%20patterns%20which%20are%20esoteric%20to%20the%20LRM.%20This%20is%20a%20crucial%20question%20in%20understanding%20or%20discovering%20new%20concepts%2C%20e.g.%20in%20AI%20for%20science.%20We%20study%20this%20generalization%20question%20by%20evaluating%20a%20specific%20notion%20of%20generalizability%3A%20whether%20explanations%20produced%20by%20one%20LRM%20induce%20the%20same%20behavior%20when%20given%20to%20other%20LRMs.%20We%20find%20that%20CoT%20explanations%20often%20exhibit%20this%20form%20of%20generalization%20%28i.e.%20they%20increase%20consistency%20between%20LRMs%29%20and%20that%20this%20increased%20generalization%20is%20correlated%20with%20human%20preference%20rankings%20and%20post-training%20with%20reinforcement%20learning.%20We%20further%20analyze%20the%20conditions%20under%20which%20explanations%20yield%20consistent%20answers%20and%20propose%20a%20straightforward%2C%20sentence-level%20ensembling%20strategy%20that%20improves%20consistency.%20Taken%20together%2C%20these%20results%20prescribe%20caution%20when%20using%20LRM%20explanations%20to%20yield%20new%20insights%20and%20outline%20a%20framework%20for%20characterizing%20LRM%20explanation%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2601.11517v1&entry.124074799=Read"},
{"title": "Beer-Lambert Autoencoder for Unsupervised Stain Representation Learning and Deconvolution in Multi-immunohistochemical Brightfield Histology Images", "author": "Mark Eastwood and Thomas McKee and Zedong Hu and Sabine Tejpar and Fayyaz Minhas", "abstract": "Separating the contributions of individual chromogenic stains in RGB histology whole slide images (WSIs) is essential for stain normalization, quantitative assessment of marker expression, and cell-level readouts in immunohistochemistry (IHC). Classical Beer-Lambert (BL) color deconvolution is well-established for two- or three-stain settings, but becomes under-determined and unstable for multiplex IHC (mIHC) with K>3 chromogens. We present a simple, data-driven encoder-decoder architecture that learns cohort-specific stain characteristics for mIHC RGB WSIs and yields crisp, well-separated per-stain concentration maps. The encoder is a compact U-Net that predicts K nonnegative concentration channels; the decoder is a differentiable BL forward model with a learnable stain matrix initialized from typical chromogen hues. Training is unsupervised with a perceptual reconstruction objective augmented by loss terms that discourage unnecessary stain mixing. On a colorectal mIHC panel comprising 5 stains (H, CDX2, MUC2, MUC5, CD8) we show excellent RGB reconstruction, and significantly reduced inter-channel bleed-through compared with matrix-based deconvolution. Code and model are available at https://github.com/measty/StainQuant.git.", "link": "http://arxiv.org/abs/2601.11336v1", "date": "2026-01-16", "relevancy": 1.9443, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4999}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4864}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beer-Lambert%20Autoencoder%20for%20Unsupervised%20Stain%20Representation%20Learning%20and%20Deconvolution%20in%20Multi-immunohistochemical%20Brightfield%20Histology%20Images&body=Title%3A%20Beer-Lambert%20Autoencoder%20for%20Unsupervised%20Stain%20Representation%20Learning%20and%20Deconvolution%20in%20Multi-immunohistochemical%20Brightfield%20Histology%20Images%0AAuthor%3A%20Mark%20Eastwood%20and%20Thomas%20McKee%20and%20Zedong%20Hu%20and%20Sabine%20Tejpar%20and%20Fayyaz%20Minhas%0AAbstract%3A%20Separating%20the%20contributions%20of%20individual%20chromogenic%20stains%20in%20RGB%20histology%20whole%20slide%20images%20%28WSIs%29%20is%20essential%20for%20stain%20normalization%2C%20quantitative%20assessment%20of%20marker%20expression%2C%20and%20cell-level%20readouts%20in%20immunohistochemistry%20%28IHC%29.%20Classical%20Beer-Lambert%20%28BL%29%20color%20deconvolution%20is%20well-established%20for%20two-%20or%20three-stain%20settings%2C%20but%20becomes%20under-determined%20and%20unstable%20for%20multiplex%20IHC%20%28mIHC%29%20with%20K%3E3%20chromogens.%20We%20present%20a%20simple%2C%20data-driven%20encoder-decoder%20architecture%20that%20learns%20cohort-specific%20stain%20characteristics%20for%20mIHC%20RGB%20WSIs%20and%20yields%20crisp%2C%20well-separated%20per-stain%20concentration%20maps.%20The%20encoder%20is%20a%20compact%20U-Net%20that%20predicts%20K%20nonnegative%20concentration%20channels%3B%20the%20decoder%20is%20a%20differentiable%20BL%20forward%20model%20with%20a%20learnable%20stain%20matrix%20initialized%20from%20typical%20chromogen%20hues.%20Training%20is%20unsupervised%20with%20a%20perceptual%20reconstruction%20objective%20augmented%20by%20loss%20terms%20that%20discourage%20unnecessary%20stain%20mixing.%20On%20a%20colorectal%20mIHC%20panel%20comprising%205%20stains%20%28H%2C%20CDX2%2C%20MUC2%2C%20MUC5%2C%20CD8%29%20we%20show%20excellent%20RGB%20reconstruction%2C%20and%20significantly%20reduced%20inter-channel%20bleed-through%20compared%20with%20matrix-based%20deconvolution.%20Code%20and%20model%20are%20available%20at%20https%3A//github.com/measty/StainQuant.git.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeer-Lambert%2520Autoencoder%2520for%2520Unsupervised%2520Stain%2520Representation%2520Learning%2520and%2520Deconvolution%2520in%2520Multi-immunohistochemical%2520Brightfield%2520Histology%2520Images%26entry.906535625%3DMark%2520Eastwood%2520and%2520Thomas%2520McKee%2520and%2520Zedong%2520Hu%2520and%2520Sabine%2520Tejpar%2520and%2520Fayyaz%2520Minhas%26entry.1292438233%3DSeparating%2520the%2520contributions%2520of%2520individual%2520chromogenic%2520stains%2520in%2520RGB%2520histology%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520is%2520essential%2520for%2520stain%2520normalization%252C%2520quantitative%2520assessment%2520of%2520marker%2520expression%252C%2520and%2520cell-level%2520readouts%2520in%2520immunohistochemistry%2520%2528IHC%2529.%2520Classical%2520Beer-Lambert%2520%2528BL%2529%2520color%2520deconvolution%2520is%2520well-established%2520for%2520two-%2520or%2520three-stain%2520settings%252C%2520but%2520becomes%2520under-determined%2520and%2520unstable%2520for%2520multiplex%2520IHC%2520%2528mIHC%2529%2520with%2520K%253E3%2520chromogens.%2520We%2520present%2520a%2520simple%252C%2520data-driven%2520encoder-decoder%2520architecture%2520that%2520learns%2520cohort-specific%2520stain%2520characteristics%2520for%2520mIHC%2520RGB%2520WSIs%2520and%2520yields%2520crisp%252C%2520well-separated%2520per-stain%2520concentration%2520maps.%2520The%2520encoder%2520is%2520a%2520compact%2520U-Net%2520that%2520predicts%2520K%2520nonnegative%2520concentration%2520channels%253B%2520the%2520decoder%2520is%2520a%2520differentiable%2520BL%2520forward%2520model%2520with%2520a%2520learnable%2520stain%2520matrix%2520initialized%2520from%2520typical%2520chromogen%2520hues.%2520Training%2520is%2520unsupervised%2520with%2520a%2520perceptual%2520reconstruction%2520objective%2520augmented%2520by%2520loss%2520terms%2520that%2520discourage%2520unnecessary%2520stain%2520mixing.%2520On%2520a%2520colorectal%2520mIHC%2520panel%2520comprising%25205%2520stains%2520%2528H%252C%2520CDX2%252C%2520MUC2%252C%2520MUC5%252C%2520CD8%2529%2520we%2520show%2520excellent%2520RGB%2520reconstruction%252C%2520and%2520significantly%2520reduced%2520inter-channel%2520bleed-through%2520compared%2520with%2520matrix-based%2520deconvolution.%2520Code%2520and%2520model%2520are%2520available%2520at%2520https%253A//github.com/measty/StainQuant.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beer-Lambert%20Autoencoder%20for%20Unsupervised%20Stain%20Representation%20Learning%20and%20Deconvolution%20in%20Multi-immunohistochemical%20Brightfield%20Histology%20Images&entry.906535625=Mark%20Eastwood%20and%20Thomas%20McKee%20and%20Zedong%20Hu%20and%20Sabine%20Tejpar%20and%20Fayyaz%20Minhas&entry.1292438233=Separating%20the%20contributions%20of%20individual%20chromogenic%20stains%20in%20RGB%20histology%20whole%20slide%20images%20%28WSIs%29%20is%20essential%20for%20stain%20normalization%2C%20quantitative%20assessment%20of%20marker%20expression%2C%20and%20cell-level%20readouts%20in%20immunohistochemistry%20%28IHC%29.%20Classical%20Beer-Lambert%20%28BL%29%20color%20deconvolution%20is%20well-established%20for%20two-%20or%20three-stain%20settings%2C%20but%20becomes%20under-determined%20and%20unstable%20for%20multiplex%20IHC%20%28mIHC%29%20with%20K%3E3%20chromogens.%20We%20present%20a%20simple%2C%20data-driven%20encoder-decoder%20architecture%20that%20learns%20cohort-specific%20stain%20characteristics%20for%20mIHC%20RGB%20WSIs%20and%20yields%20crisp%2C%20well-separated%20per-stain%20concentration%20maps.%20The%20encoder%20is%20a%20compact%20U-Net%20that%20predicts%20K%20nonnegative%20concentration%20channels%3B%20the%20decoder%20is%20a%20differentiable%20BL%20forward%20model%20with%20a%20learnable%20stain%20matrix%20initialized%20from%20typical%20chromogen%20hues.%20Training%20is%20unsupervised%20with%20a%20perceptual%20reconstruction%20objective%20augmented%20by%20loss%20terms%20that%20discourage%20unnecessary%20stain%20mixing.%20On%20a%20colorectal%20mIHC%20panel%20comprising%205%20stains%20%28H%2C%20CDX2%2C%20MUC2%2C%20MUC5%2C%20CD8%29%20we%20show%20excellent%20RGB%20reconstruction%2C%20and%20significantly%20reduced%20inter-channel%20bleed-through%20compared%20with%20matrix-based%20deconvolution.%20Code%20and%20model%20are%20available%20at%20https%3A//github.com/measty/StainQuant.git.&entry.1838667208=http%3A//arxiv.org/abs/2601.11336v1&entry.124074799=Read"},
{"title": "Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning", "author": "Qianyue Wang and Jinwu Hu and Yufeng Wang and Huanxiang Lin and Bolin Chen and Zhiquan Wen and Yaofo Chen and Mingkui Tan", "abstract": "Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.", "link": "http://arxiv.org/abs/2601.11252v1", "date": "2026-01-16", "relevancy": 1.9409, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Model%20Scaling%3A%20Test-Time%20Intervention%20for%20Efficient%20Deep%20Reasoning&body=Title%3A%20Beyond%20Model%20Scaling%3A%20Test-Time%20Intervention%20for%20Efficient%20Deep%20Reasoning%0AAuthor%3A%20Qianyue%20Wang%20and%20Jinwu%20Hu%20and%20Yufeng%20Wang%20and%20Huanxiang%20Lin%20and%20Bolin%20Chen%20and%20Zhiquan%20Wen%20and%20Yaofo%20Chen%20and%20Mingkui%20Tan%0AAbstract%3A%20Large%20Reasoning%20Models%20%28LRMs%29%20excel%20at%20multi-step%20reasoning%20but%20often%20suffer%20from%20inefficient%20reasoning%20processes%20like%20overthinking%20and%20overshoot%2C%20where%20excessive%20or%20misdirected%20reasoning%20increases%20computational%20cost%20and%20degrades%20performance.%20Existing%20efficient%20reasoning%20methods%20operate%20in%20a%20closed-loop%20manner%2C%20lacking%20mechanisms%20for%20external%20intervention%20to%20guide%20the%20reasoning%20process.%20To%20address%20this%2C%20we%20propose%20Think-with-Me%2C%20a%20novel%20test-time%20interactive%20reasoning%20paradigm%20that%20introduces%20external%20feedback%20intervention%20into%20the%20reasoning%20process.%20Our%20key%20insights%20are%20that%20transitional%20conjunctions%20serve%20as%20natural%20points%20for%20intervention%2C%20signaling%20phases%20of%20self-validation%20or%20exploration%20and%20using%20transitional%20words%20appropriately%20to%20prolong%20the%20reasoning%20enhances%20performance%2C%20while%20excessive%20use%20affects%20performance.%20Building%20on%20these%20insights%2C%20Think-with-Me%20pauses%20reasoning%20at%20these%20points%20for%20external%20feedback%2C%20adaptively%20extending%20or%20terminating%20reasoning%20to%20reduce%20redundancy%20while%20preserving%20accuracy.%20The%20feedback%20is%20generated%20via%20a%20multi-criteria%20evaluation%20%28rationality%20and%20completeness%29%20and%20comes%20from%20either%20human%20or%20LLM%20proxies.%20We%20train%20the%20target%20model%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20adapt%20to%20this%20interactive%20mode.%20Experiments%20show%20that%20Think-with-Me%20achieves%20a%20superior%20balance%20between%20accuracy%20and%20reasoning%20length%20under%20limited%20context%20windows.%20On%20AIME24%2C%20Think-with-Me%20outperforms%20QwQ-32B%20by%207.19%25%20in%20accuracy%20while%20reducing%20average%20reasoning%20length%20by%2081%25%20under%20an%208K%20window.%20The%20paradigm%20also%20benefits%20security%20and%20creative%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Model%2520Scaling%253A%2520Test-Time%2520Intervention%2520for%2520Efficient%2520Deep%2520Reasoning%26entry.906535625%3DQianyue%2520Wang%2520and%2520Jinwu%2520Hu%2520and%2520Yufeng%2520Wang%2520and%2520Huanxiang%2520Lin%2520and%2520Bolin%2520Chen%2520and%2520Zhiquan%2520Wen%2520and%2520Yaofo%2520Chen%2520and%2520Mingkui%2520Tan%26entry.1292438233%3DLarge%2520Reasoning%2520Models%2520%2528LRMs%2529%2520excel%2520at%2520multi-step%2520reasoning%2520but%2520often%2520suffer%2520from%2520inefficient%2520reasoning%2520processes%2520like%2520overthinking%2520and%2520overshoot%252C%2520where%2520excessive%2520or%2520misdirected%2520reasoning%2520increases%2520computational%2520cost%2520and%2520degrades%2520performance.%2520Existing%2520efficient%2520reasoning%2520methods%2520operate%2520in%2520a%2520closed-loop%2520manner%252C%2520lacking%2520mechanisms%2520for%2520external%2520intervention%2520to%2520guide%2520the%2520reasoning%2520process.%2520To%2520address%2520this%252C%2520we%2520propose%2520Think-with-Me%252C%2520a%2520novel%2520test-time%2520interactive%2520reasoning%2520paradigm%2520that%2520introduces%2520external%2520feedback%2520intervention%2520into%2520the%2520reasoning%2520process.%2520Our%2520key%2520insights%2520are%2520that%2520transitional%2520conjunctions%2520serve%2520as%2520natural%2520points%2520for%2520intervention%252C%2520signaling%2520phases%2520of%2520self-validation%2520or%2520exploration%2520and%2520using%2520transitional%2520words%2520appropriately%2520to%2520prolong%2520the%2520reasoning%2520enhances%2520performance%252C%2520while%2520excessive%2520use%2520affects%2520performance.%2520Building%2520on%2520these%2520insights%252C%2520Think-with-Me%2520pauses%2520reasoning%2520at%2520these%2520points%2520for%2520external%2520feedback%252C%2520adaptively%2520extending%2520or%2520terminating%2520reasoning%2520to%2520reduce%2520redundancy%2520while%2520preserving%2520accuracy.%2520The%2520feedback%2520is%2520generated%2520via%2520a%2520multi-criteria%2520evaluation%2520%2528rationality%2520and%2520completeness%2529%2520and%2520comes%2520from%2520either%2520human%2520or%2520LLM%2520proxies.%2520We%2520train%2520the%2520target%2520model%2520using%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520to%2520adapt%2520to%2520this%2520interactive%2520mode.%2520Experiments%2520show%2520that%2520Think-with-Me%2520achieves%2520a%2520superior%2520balance%2520between%2520accuracy%2520and%2520reasoning%2520length%2520under%2520limited%2520context%2520windows.%2520On%2520AIME24%252C%2520Think-with-Me%2520outperforms%2520QwQ-32B%2520by%25207.19%2525%2520in%2520accuracy%2520while%2520reducing%2520average%2520reasoning%2520length%2520by%252081%2525%2520under%2520an%25208K%2520window.%2520The%2520paradigm%2520also%2520benefits%2520security%2520and%2520creative%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Model%20Scaling%3A%20Test-Time%20Intervention%20for%20Efficient%20Deep%20Reasoning&entry.906535625=Qianyue%20Wang%20and%20Jinwu%20Hu%20and%20Yufeng%20Wang%20and%20Huanxiang%20Lin%20and%20Bolin%20Chen%20and%20Zhiquan%20Wen%20and%20Yaofo%20Chen%20and%20Mingkui%20Tan&entry.1292438233=Large%20Reasoning%20Models%20%28LRMs%29%20excel%20at%20multi-step%20reasoning%20but%20often%20suffer%20from%20inefficient%20reasoning%20processes%20like%20overthinking%20and%20overshoot%2C%20where%20excessive%20or%20misdirected%20reasoning%20increases%20computational%20cost%20and%20degrades%20performance.%20Existing%20efficient%20reasoning%20methods%20operate%20in%20a%20closed-loop%20manner%2C%20lacking%20mechanisms%20for%20external%20intervention%20to%20guide%20the%20reasoning%20process.%20To%20address%20this%2C%20we%20propose%20Think-with-Me%2C%20a%20novel%20test-time%20interactive%20reasoning%20paradigm%20that%20introduces%20external%20feedback%20intervention%20into%20the%20reasoning%20process.%20Our%20key%20insights%20are%20that%20transitional%20conjunctions%20serve%20as%20natural%20points%20for%20intervention%2C%20signaling%20phases%20of%20self-validation%20or%20exploration%20and%20using%20transitional%20words%20appropriately%20to%20prolong%20the%20reasoning%20enhances%20performance%2C%20while%20excessive%20use%20affects%20performance.%20Building%20on%20these%20insights%2C%20Think-with-Me%20pauses%20reasoning%20at%20these%20points%20for%20external%20feedback%2C%20adaptively%20extending%20or%20terminating%20reasoning%20to%20reduce%20redundancy%20while%20preserving%20accuracy.%20The%20feedback%20is%20generated%20via%20a%20multi-criteria%20evaluation%20%28rationality%20and%20completeness%29%20and%20comes%20from%20either%20human%20or%20LLM%20proxies.%20We%20train%20the%20target%20model%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20adapt%20to%20this%20interactive%20mode.%20Experiments%20show%20that%20Think-with-Me%20achieves%20a%20superior%20balance%20between%20accuracy%20and%20reasoning%20length%20under%20limited%20context%20windows.%20On%20AIME24%2C%20Think-with-Me%20outperforms%20QwQ-32B%20by%207.19%25%20in%20accuracy%20while%20reducing%20average%20reasoning%20length%20by%2081%25%20under%20an%208K%20window.%20The%20paradigm%20also%20benefits%20security%20and%20creative%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.11252v1&entry.124074799=Read"},
{"title": "Sample-Near-Optimal Agnostic Boosting with Improved Running Time", "author": "Arthur da Cunha and Miakel M\u00f8ller H\u00f8gsgaard and Andrea Paudice", "abstract": "Boosting is a powerful method that turns weak learners, which perform only slightly better than random guessing, into strong learners with high accuracy. While boosting is well understood in the classic setting, it is less so in the agnostic case, where no assumptions are made about the data. Indeed, only recently was the sample complexity of agnostic boosting nearly settled arXiv:2503.09384, but the known algorithm achieving this bound has exponential running time. In this work, we propose the first agnostic boosting algorithm with near-optimal sample complexity, running in time polynomial in the sample size when considering the other parameters of the problem fixed.", "link": "http://arxiv.org/abs/2601.11265v1", "date": "2026-01-16", "relevancy": 1.9359, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5124}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4862}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample-Near-Optimal%20Agnostic%20Boosting%20with%20Improved%20Running%20Time&body=Title%3A%20Sample-Near-Optimal%20Agnostic%20Boosting%20with%20Improved%20Running%20Time%0AAuthor%3A%20Arthur%20da%20Cunha%20and%20Miakel%20M%C3%B8ller%20H%C3%B8gsgaard%20and%20Andrea%20Paudice%0AAbstract%3A%20Boosting%20is%20a%20powerful%20method%20that%20turns%20weak%20learners%2C%20which%20perform%20only%20slightly%20better%20than%20random%20guessing%2C%20into%20strong%20learners%20with%20high%20accuracy.%20While%20boosting%20is%20well%20understood%20in%20the%20classic%20setting%2C%20it%20is%20less%20so%20in%20the%20agnostic%20case%2C%20where%20no%20assumptions%20are%20made%20about%20the%20data.%20Indeed%2C%20only%20recently%20was%20the%20sample%20complexity%20of%20agnostic%20boosting%20nearly%20settled%20arXiv%3A2503.09384%2C%20but%20the%20known%20algorithm%20achieving%20this%20bound%20has%20exponential%20running%20time.%20In%20this%20work%2C%20we%20propose%20the%20first%20agnostic%20boosting%20algorithm%20with%20near-optimal%20sample%20complexity%2C%20running%20in%20time%20polynomial%20in%20the%20sample%20size%20when%20considering%20the%20other%20parameters%20of%20the%20problem%20fixed.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample-Near-Optimal%2520Agnostic%2520Boosting%2520with%2520Improved%2520Running%2520Time%26entry.906535625%3DArthur%2520da%2520Cunha%2520and%2520Miakel%2520M%25C3%25B8ller%2520H%25C3%25B8gsgaard%2520and%2520Andrea%2520Paudice%26entry.1292438233%3DBoosting%2520is%2520a%2520powerful%2520method%2520that%2520turns%2520weak%2520learners%252C%2520which%2520perform%2520only%2520slightly%2520better%2520than%2520random%2520guessing%252C%2520into%2520strong%2520learners%2520with%2520high%2520accuracy.%2520While%2520boosting%2520is%2520well%2520understood%2520in%2520the%2520classic%2520setting%252C%2520it%2520is%2520less%2520so%2520in%2520the%2520agnostic%2520case%252C%2520where%2520no%2520assumptions%2520are%2520made%2520about%2520the%2520data.%2520Indeed%252C%2520only%2520recently%2520was%2520the%2520sample%2520complexity%2520of%2520agnostic%2520boosting%2520nearly%2520settled%2520arXiv%253A2503.09384%252C%2520but%2520the%2520known%2520algorithm%2520achieving%2520this%2520bound%2520has%2520exponential%2520running%2520time.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520first%2520agnostic%2520boosting%2520algorithm%2520with%2520near-optimal%2520sample%2520complexity%252C%2520running%2520in%2520time%2520polynomial%2520in%2520the%2520sample%2520size%2520when%2520considering%2520the%2520other%2520parameters%2520of%2520the%2520problem%2520fixed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample-Near-Optimal%20Agnostic%20Boosting%20with%20Improved%20Running%20Time&entry.906535625=Arthur%20da%20Cunha%20and%20Miakel%20M%C3%B8ller%20H%C3%B8gsgaard%20and%20Andrea%20Paudice&entry.1292438233=Boosting%20is%20a%20powerful%20method%20that%20turns%20weak%20learners%2C%20which%20perform%20only%20slightly%20better%20than%20random%20guessing%2C%20into%20strong%20learners%20with%20high%20accuracy.%20While%20boosting%20is%20well%20understood%20in%20the%20classic%20setting%2C%20it%20is%20less%20so%20in%20the%20agnostic%20case%2C%20where%20no%20assumptions%20are%20made%20about%20the%20data.%20Indeed%2C%20only%20recently%20was%20the%20sample%20complexity%20of%20agnostic%20boosting%20nearly%20settled%20arXiv%3A2503.09384%2C%20but%20the%20known%20algorithm%20achieving%20this%20bound%20has%20exponential%20running%20time.%20In%20this%20work%2C%20we%20propose%20the%20first%20agnostic%20boosting%20algorithm%20with%20near-optimal%20sample%20complexity%2C%20running%20in%20time%20polynomial%20in%20the%20sample%20size%20when%20considering%20the%20other%20parameters%20of%20the%20problem%20fixed.&entry.1838667208=http%3A//arxiv.org/abs/2601.11265v1&entry.124074799=Read"},
{"title": "From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP", "author": "Shanshan Xu and Santosh T. Y. S. S and Barbara Plank", "abstract": "Human Label Variation (HLV) refers to legitimate disagreement in annotation that reflects the diversity of human perspectives rather than mere error. Long treated in NLP as noise to be eliminated, HLV has only recently been reframed as a signal for improving model robustness. With the rise of large language models (LLMs) and post-training methods such as human feedback-based alignment, the role of HLV has become increasingly consequential. Yet current preference-learning datasets routinely collapse multiple annotations into a single label, flattening diverse perspectives into artificial consensus. Preserving HLV is necessary not only for pluralistic alignment but also for sociotechnical safety evaluation, where model behavior must be assessed in relation to human interaction and societal context. This position paper argues that preserving HLV as an embodiment of human pluralism must be treated as a Selbstzweck, an intrinsic value in itself. We analyze the limitations of existing preference datasets and propose actionable strategies for incorporating HLV into dataset construction to better preserve pluralistic human values.", "link": "http://arxiv.org/abs/2510.12817v2", "date": "2026-01-16", "relevancy": 1.9248, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Noise%20to%20Signal%20to%20Selbstzweck%3A%20Reframing%20Human%20Label%20Variation%20in%20the%20Era%20of%20Post-training%20in%20NLP&body=Title%3A%20From%20Noise%20to%20Signal%20to%20Selbstzweck%3A%20Reframing%20Human%20Label%20Variation%20in%20the%20Era%20of%20Post-training%20in%20NLP%0AAuthor%3A%20Shanshan%20Xu%20and%20Santosh%20T.%20Y.%20S.%20S%20and%20Barbara%20Plank%0AAbstract%3A%20Human%20Label%20Variation%20%28HLV%29%20refers%20to%20legitimate%20disagreement%20in%20annotation%20that%20reflects%20the%20diversity%20of%20human%20perspectives%20rather%20than%20mere%20error.%20Long%20treated%20in%20NLP%20as%20noise%20to%20be%20eliminated%2C%20HLV%20has%20only%20recently%20been%20reframed%20as%20a%20signal%20for%20improving%20model%20robustness.%20With%20the%20rise%20of%20large%20language%20models%20%28LLMs%29%20and%20post-training%20methods%20such%20as%20human%20feedback-based%20alignment%2C%20the%20role%20of%20HLV%20has%20become%20increasingly%20consequential.%20Yet%20current%20preference-learning%20datasets%20routinely%20collapse%20multiple%20annotations%20into%20a%20single%20label%2C%20flattening%20diverse%20perspectives%20into%20artificial%20consensus.%20Preserving%20HLV%20is%20necessary%20not%20only%20for%20pluralistic%20alignment%20but%20also%20for%20sociotechnical%20safety%20evaluation%2C%20where%20model%20behavior%20must%20be%20assessed%20in%20relation%20to%20human%20interaction%20and%20societal%20context.%20This%20position%20paper%20argues%20that%20preserving%20HLV%20as%20an%20embodiment%20of%20human%20pluralism%20must%20be%20treated%20as%20a%20Selbstzweck%2C%20an%20intrinsic%20value%20in%20itself.%20We%20analyze%20the%20limitations%20of%20existing%20preference%20datasets%20and%20propose%20actionable%20strategies%20for%20incorporating%20HLV%20into%20dataset%20construction%20to%20better%20preserve%20pluralistic%20human%20values.%0ALink%3A%20http%3A//arxiv.org/abs/2510.12817v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Noise%2520to%2520Signal%2520to%2520Selbstzweck%253A%2520Reframing%2520Human%2520Label%2520Variation%2520in%2520the%2520Era%2520of%2520Post-training%2520in%2520NLP%26entry.906535625%3DShanshan%2520Xu%2520and%2520Santosh%2520T.%2520Y.%2520S.%2520S%2520and%2520Barbara%2520Plank%26entry.1292438233%3DHuman%2520Label%2520Variation%2520%2528HLV%2529%2520refers%2520to%2520legitimate%2520disagreement%2520in%2520annotation%2520that%2520reflects%2520the%2520diversity%2520of%2520human%2520perspectives%2520rather%2520than%2520mere%2520error.%2520Long%2520treated%2520in%2520NLP%2520as%2520noise%2520to%2520be%2520eliminated%252C%2520HLV%2520has%2520only%2520recently%2520been%2520reframed%2520as%2520a%2520signal%2520for%2520improving%2520model%2520robustness.%2520With%2520the%2520rise%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520post-training%2520methods%2520such%2520as%2520human%2520feedback-based%2520alignment%252C%2520the%2520role%2520of%2520HLV%2520has%2520become%2520increasingly%2520consequential.%2520Yet%2520current%2520preference-learning%2520datasets%2520routinely%2520collapse%2520multiple%2520annotations%2520into%2520a%2520single%2520label%252C%2520flattening%2520diverse%2520perspectives%2520into%2520artificial%2520consensus.%2520Preserving%2520HLV%2520is%2520necessary%2520not%2520only%2520for%2520pluralistic%2520alignment%2520but%2520also%2520for%2520sociotechnical%2520safety%2520evaluation%252C%2520where%2520model%2520behavior%2520must%2520be%2520assessed%2520in%2520relation%2520to%2520human%2520interaction%2520and%2520societal%2520context.%2520This%2520position%2520paper%2520argues%2520that%2520preserving%2520HLV%2520as%2520an%2520embodiment%2520of%2520human%2520pluralism%2520must%2520be%2520treated%2520as%2520a%2520Selbstzweck%252C%2520an%2520intrinsic%2520value%2520in%2520itself.%2520We%2520analyze%2520the%2520limitations%2520of%2520existing%2520preference%2520datasets%2520and%2520propose%2520actionable%2520strategies%2520for%2520incorporating%2520HLV%2520into%2520dataset%2520construction%2520to%2520better%2520preserve%2520pluralistic%2520human%2520values.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12817v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Noise%20to%20Signal%20to%20Selbstzweck%3A%20Reframing%20Human%20Label%20Variation%20in%20the%20Era%20of%20Post-training%20in%20NLP&entry.906535625=Shanshan%20Xu%20and%20Santosh%20T.%20Y.%20S.%20S%20and%20Barbara%20Plank&entry.1292438233=Human%20Label%20Variation%20%28HLV%29%20refers%20to%20legitimate%20disagreement%20in%20annotation%20that%20reflects%20the%20diversity%20of%20human%20perspectives%20rather%20than%20mere%20error.%20Long%20treated%20in%20NLP%20as%20noise%20to%20be%20eliminated%2C%20HLV%20has%20only%20recently%20been%20reframed%20as%20a%20signal%20for%20improving%20model%20robustness.%20With%20the%20rise%20of%20large%20language%20models%20%28LLMs%29%20and%20post-training%20methods%20such%20as%20human%20feedback-based%20alignment%2C%20the%20role%20of%20HLV%20has%20become%20increasingly%20consequential.%20Yet%20current%20preference-learning%20datasets%20routinely%20collapse%20multiple%20annotations%20into%20a%20single%20label%2C%20flattening%20diverse%20perspectives%20into%20artificial%20consensus.%20Preserving%20HLV%20is%20necessary%20not%20only%20for%20pluralistic%20alignment%20but%20also%20for%20sociotechnical%20safety%20evaluation%2C%20where%20model%20behavior%20must%20be%20assessed%20in%20relation%20to%20human%20interaction%20and%20societal%20context.%20This%20position%20paper%20argues%20that%20preserving%20HLV%20as%20an%20embodiment%20of%20human%20pluralism%20must%20be%20treated%20as%20a%20Selbstzweck%2C%20an%20intrinsic%20value%20in%20itself.%20We%20analyze%20the%20limitations%20of%20existing%20preference%20datasets%20and%20propose%20actionable%20strategies%20for%20incorporating%20HLV%20into%20dataset%20construction%20to%20better%20preserve%20pluralistic%20human%20values.&entry.1838667208=http%3A//arxiv.org/abs/2510.12817v2&entry.124074799=Read"},
{"title": "New Adaptive Mechanism for Large Neighborhood Search using Dual Actor-Critic", "author": "Shaohua Yu and Wenhao Mao and Zigao Wu and Jakob Puchinger", "abstract": "Adaptive Large Neighborhood Search (ALNS) is a widely used heuristic method for solving combinatorial optimization problems. ALNS explores the solution space by iteratively using destroy and repair operators with probabilities, which are adjusted by an adaptive mechanism to find optimal solutions. However, the classic ALNS adaptive mechanism does not consider the interaction between destroy and repair operators when selecting them. To overcome this limitation, this study proposes a novel adaptive mechanism. This mechanism enhances the adaptability of the algorithm through a Dual Actor-Critic (DAC) model, which fully considers the fact that the quality of new solutions is jointly determined by the destroy and repair operators. It effectively utilizes the interaction between these operators during the weight adjustment process, greatly improving the adaptability of the ALNS algorithm. In this mechanism, the destroy and repair processes are modeled as independent Markov Decision Processes to guide the selection of operators more accurately. Furthermore, we use Graph Neural Networks to extract key features from problem instances and perform effective aggregation and normalization to enhance the algorithm's transferability to different sizes and characteristics of problems. Through a series of experiments, we demonstrate that the proposed DAC-ALNS algorithm significantly improves solution efficiency and exhibits excellent transferability.", "link": "http://arxiv.org/abs/2601.11414v1", "date": "2026-01-16", "relevancy": 1.9232, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5184}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4749}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20New%20Adaptive%20Mechanism%20for%20Large%20Neighborhood%20Search%20using%20Dual%20Actor-Critic&body=Title%3A%20New%20Adaptive%20Mechanism%20for%20Large%20Neighborhood%20Search%20using%20Dual%20Actor-Critic%0AAuthor%3A%20Shaohua%20Yu%20and%20Wenhao%20Mao%20and%20Zigao%20Wu%20and%20Jakob%20Puchinger%0AAbstract%3A%20Adaptive%20Large%20Neighborhood%20Search%20%28ALNS%29%20is%20a%20widely%20used%20heuristic%20method%20for%20solving%20combinatorial%20optimization%20problems.%20ALNS%20explores%20the%20solution%20space%20by%20iteratively%20using%20destroy%20and%20repair%20operators%20with%20probabilities%2C%20which%20are%20adjusted%20by%20an%20adaptive%20mechanism%20to%20find%20optimal%20solutions.%20However%2C%20the%20classic%20ALNS%20adaptive%20mechanism%20does%20not%20consider%20the%20interaction%20between%20destroy%20and%20repair%20operators%20when%20selecting%20them.%20To%20overcome%20this%20limitation%2C%20this%20study%20proposes%20a%20novel%20adaptive%20mechanism.%20This%20mechanism%20enhances%20the%20adaptability%20of%20the%20algorithm%20through%20a%20Dual%20Actor-Critic%20%28DAC%29%20model%2C%20which%20fully%20considers%20the%20fact%20that%20the%20quality%20of%20new%20solutions%20is%20jointly%20determined%20by%20the%20destroy%20and%20repair%20operators.%20It%20effectively%20utilizes%20the%20interaction%20between%20these%20operators%20during%20the%20weight%20adjustment%20process%2C%20greatly%20improving%20the%20adaptability%20of%20the%20ALNS%20algorithm.%20In%20this%20mechanism%2C%20the%20destroy%20and%20repair%20processes%20are%20modeled%20as%20independent%20Markov%20Decision%20Processes%20to%20guide%20the%20selection%20of%20operators%20more%20accurately.%20Furthermore%2C%20we%20use%20Graph%20Neural%20Networks%20to%20extract%20key%20features%20from%20problem%20instances%20and%20perform%20effective%20aggregation%20and%20normalization%20to%20enhance%20the%20algorithm%27s%20transferability%20to%20different%20sizes%20and%20characteristics%20of%20problems.%20Through%20a%20series%20of%20experiments%2C%20we%20demonstrate%20that%20the%20proposed%20DAC-ALNS%20algorithm%20significantly%20improves%20solution%20efficiency%20and%20exhibits%20excellent%20transferability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNew%2520Adaptive%2520Mechanism%2520for%2520Large%2520Neighborhood%2520Search%2520using%2520Dual%2520Actor-Critic%26entry.906535625%3DShaohua%2520Yu%2520and%2520Wenhao%2520Mao%2520and%2520Zigao%2520Wu%2520and%2520Jakob%2520Puchinger%26entry.1292438233%3DAdaptive%2520Large%2520Neighborhood%2520Search%2520%2528ALNS%2529%2520is%2520a%2520widely%2520used%2520heuristic%2520method%2520for%2520solving%2520combinatorial%2520optimization%2520problems.%2520ALNS%2520explores%2520the%2520solution%2520space%2520by%2520iteratively%2520using%2520destroy%2520and%2520repair%2520operators%2520with%2520probabilities%252C%2520which%2520are%2520adjusted%2520by%2520an%2520adaptive%2520mechanism%2520to%2520find%2520optimal%2520solutions.%2520However%252C%2520the%2520classic%2520ALNS%2520adaptive%2520mechanism%2520does%2520not%2520consider%2520the%2520interaction%2520between%2520destroy%2520and%2520repair%2520operators%2520when%2520selecting%2520them.%2520To%2520overcome%2520this%2520limitation%252C%2520this%2520study%2520proposes%2520a%2520novel%2520adaptive%2520mechanism.%2520This%2520mechanism%2520enhances%2520the%2520adaptability%2520of%2520the%2520algorithm%2520through%2520a%2520Dual%2520Actor-Critic%2520%2528DAC%2529%2520model%252C%2520which%2520fully%2520considers%2520the%2520fact%2520that%2520the%2520quality%2520of%2520new%2520solutions%2520is%2520jointly%2520determined%2520by%2520the%2520destroy%2520and%2520repair%2520operators.%2520It%2520effectively%2520utilizes%2520the%2520interaction%2520between%2520these%2520operators%2520during%2520the%2520weight%2520adjustment%2520process%252C%2520greatly%2520improving%2520the%2520adaptability%2520of%2520the%2520ALNS%2520algorithm.%2520In%2520this%2520mechanism%252C%2520the%2520destroy%2520and%2520repair%2520processes%2520are%2520modeled%2520as%2520independent%2520Markov%2520Decision%2520Processes%2520to%2520guide%2520the%2520selection%2520of%2520operators%2520more%2520accurately.%2520Furthermore%252C%2520we%2520use%2520Graph%2520Neural%2520Networks%2520to%2520extract%2520key%2520features%2520from%2520problem%2520instances%2520and%2520perform%2520effective%2520aggregation%2520and%2520normalization%2520to%2520enhance%2520the%2520algorithm%2527s%2520transferability%2520to%2520different%2520sizes%2520and%2520characteristics%2520of%2520problems.%2520Through%2520a%2520series%2520of%2520experiments%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520DAC-ALNS%2520algorithm%2520significantly%2520improves%2520solution%2520efficiency%2520and%2520exhibits%2520excellent%2520transferability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20Adaptive%20Mechanism%20for%20Large%20Neighborhood%20Search%20using%20Dual%20Actor-Critic&entry.906535625=Shaohua%20Yu%20and%20Wenhao%20Mao%20and%20Zigao%20Wu%20and%20Jakob%20Puchinger&entry.1292438233=Adaptive%20Large%20Neighborhood%20Search%20%28ALNS%29%20is%20a%20widely%20used%20heuristic%20method%20for%20solving%20combinatorial%20optimization%20problems.%20ALNS%20explores%20the%20solution%20space%20by%20iteratively%20using%20destroy%20and%20repair%20operators%20with%20probabilities%2C%20which%20are%20adjusted%20by%20an%20adaptive%20mechanism%20to%20find%20optimal%20solutions.%20However%2C%20the%20classic%20ALNS%20adaptive%20mechanism%20does%20not%20consider%20the%20interaction%20between%20destroy%20and%20repair%20operators%20when%20selecting%20them.%20To%20overcome%20this%20limitation%2C%20this%20study%20proposes%20a%20novel%20adaptive%20mechanism.%20This%20mechanism%20enhances%20the%20adaptability%20of%20the%20algorithm%20through%20a%20Dual%20Actor-Critic%20%28DAC%29%20model%2C%20which%20fully%20considers%20the%20fact%20that%20the%20quality%20of%20new%20solutions%20is%20jointly%20determined%20by%20the%20destroy%20and%20repair%20operators.%20It%20effectively%20utilizes%20the%20interaction%20between%20these%20operators%20during%20the%20weight%20adjustment%20process%2C%20greatly%20improving%20the%20adaptability%20of%20the%20ALNS%20algorithm.%20In%20this%20mechanism%2C%20the%20destroy%20and%20repair%20processes%20are%20modeled%20as%20independent%20Markov%20Decision%20Processes%20to%20guide%20the%20selection%20of%20operators%20more%20accurately.%20Furthermore%2C%20we%20use%20Graph%20Neural%20Networks%20to%20extract%20key%20features%20from%20problem%20instances%20and%20perform%20effective%20aggregation%20and%20normalization%20to%20enhance%20the%20algorithm%27s%20transferability%20to%20different%20sizes%20and%20characteristics%20of%20problems.%20Through%20a%20series%20of%20experiments%2C%20we%20demonstrate%20that%20the%20proposed%20DAC-ALNS%20algorithm%20significantly%20improves%20solution%20efficiency%20and%20exhibits%20excellent%20transferability.&entry.1838667208=http%3A//arxiv.org/abs/2601.11414v1&entry.124074799=Read"},
{"title": "FEAT: Free energy Estimators with Adaptive Transport", "author": "Jiajun He and Yuanqi Du and Francisco Vargas and Yuanqing Wang and Carla P. Gomes and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato and Eric Vanden-Eijnden", "abstract": "We present Free energy Estimators with Adaptive Transport (FEAT), a novel framework for free energy estimation -- a critical challenge across scientific domains. FEAT leverages learned transports implemented via stochastic interpolants and provides consistent, minimum-variance estimators based on escorted Jarzynski equality and controlled Crooks theorem, alongside variational upper and lower bounds on free energy differences. Unifying equilibrium and non-equilibrium methods under a single theoretical framework, FEAT establishes a principled foundation for neural free energy calculations. Experimental validation on toy examples, molecular simulations, and quantum field theory demonstrates improvements over existing learning-based methods. Our PyTorch implementation is available at https://github.com/jiajunhe98/FEAT.", "link": "http://arxiv.org/abs/2504.11516v3", "date": "2026-01-16", "relevancy": 1.9179, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4962}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.486}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FEAT%3A%20Free%20energy%20Estimators%20with%20Adaptive%20Transport&body=Title%3A%20FEAT%3A%20Free%20energy%20Estimators%20with%20Adaptive%20Transport%0AAuthor%3A%20Jiajun%20He%20and%20Yuanqi%20Du%20and%20Francisco%20Vargas%20and%20Yuanqing%20Wang%20and%20Carla%20P.%20Gomes%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%20and%20Eric%20Vanden-Eijnden%0AAbstract%3A%20We%20present%20Free%20energy%20Estimators%20with%20Adaptive%20Transport%20%28FEAT%29%2C%20a%20novel%20framework%20for%20free%20energy%20estimation%20--%20a%20critical%20challenge%20across%20scientific%20domains.%20FEAT%20leverages%20learned%20transports%20implemented%20via%20stochastic%20interpolants%20and%20provides%20consistent%2C%20minimum-variance%20estimators%20based%20on%20escorted%20Jarzynski%20equality%20and%20controlled%20Crooks%20theorem%2C%20alongside%20variational%20upper%20and%20lower%20bounds%20on%20free%20energy%20differences.%20Unifying%20equilibrium%20and%20non-equilibrium%20methods%20under%20a%20single%20theoretical%20framework%2C%20FEAT%20establishes%20a%20principled%20foundation%20for%20neural%20free%20energy%20calculations.%20Experimental%20validation%20on%20toy%20examples%2C%20molecular%20simulations%2C%20and%20quantum%20field%20theory%20demonstrates%20improvements%20over%20existing%20learning-based%20methods.%20Our%20PyTorch%20implementation%20is%20available%20at%20https%3A//github.com/jiajunhe98/FEAT.%0ALink%3A%20http%3A//arxiv.org/abs/2504.11516v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFEAT%253A%2520Free%2520energy%2520Estimators%2520with%2520Adaptive%2520Transport%26entry.906535625%3DJiajun%2520He%2520and%2520Yuanqi%2520Du%2520and%2520Francisco%2520Vargas%2520and%2520Yuanqing%2520Wang%2520and%2520Carla%2520P.%2520Gomes%2520and%2520Jos%25C3%25A9%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%2520and%2520Eric%2520Vanden-Eijnden%26entry.1292438233%3DWe%2520present%2520Free%2520energy%2520Estimators%2520with%2520Adaptive%2520Transport%2520%2528FEAT%2529%252C%2520a%2520novel%2520framework%2520for%2520free%2520energy%2520estimation%2520--%2520a%2520critical%2520challenge%2520across%2520scientific%2520domains.%2520FEAT%2520leverages%2520learned%2520transports%2520implemented%2520via%2520stochastic%2520interpolants%2520and%2520provides%2520consistent%252C%2520minimum-variance%2520estimators%2520based%2520on%2520escorted%2520Jarzynski%2520equality%2520and%2520controlled%2520Crooks%2520theorem%252C%2520alongside%2520variational%2520upper%2520and%2520lower%2520bounds%2520on%2520free%2520energy%2520differences.%2520Unifying%2520equilibrium%2520and%2520non-equilibrium%2520methods%2520under%2520a%2520single%2520theoretical%2520framework%252C%2520FEAT%2520establishes%2520a%2520principled%2520foundation%2520for%2520neural%2520free%2520energy%2520calculations.%2520Experimental%2520validation%2520on%2520toy%2520examples%252C%2520molecular%2520simulations%252C%2520and%2520quantum%2520field%2520theory%2520demonstrates%2520improvements%2520over%2520existing%2520learning-based%2520methods.%2520Our%2520PyTorch%2520implementation%2520is%2520available%2520at%2520https%253A//github.com/jiajunhe98/FEAT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11516v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FEAT%3A%20Free%20energy%20Estimators%20with%20Adaptive%20Transport&entry.906535625=Jiajun%20He%20and%20Yuanqi%20Du%20and%20Francisco%20Vargas%20and%20Yuanqing%20Wang%20and%20Carla%20P.%20Gomes%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%20and%20Eric%20Vanden-Eijnden&entry.1292438233=We%20present%20Free%20energy%20Estimators%20with%20Adaptive%20Transport%20%28FEAT%29%2C%20a%20novel%20framework%20for%20free%20energy%20estimation%20--%20a%20critical%20challenge%20across%20scientific%20domains.%20FEAT%20leverages%20learned%20transports%20implemented%20via%20stochastic%20interpolants%20and%20provides%20consistent%2C%20minimum-variance%20estimators%20based%20on%20escorted%20Jarzynski%20equality%20and%20controlled%20Crooks%20theorem%2C%20alongside%20variational%20upper%20and%20lower%20bounds%20on%20free%20energy%20differences.%20Unifying%20equilibrium%20and%20non-equilibrium%20methods%20under%20a%20single%20theoretical%20framework%2C%20FEAT%20establishes%20a%20principled%20foundation%20for%20neural%20free%20energy%20calculations.%20Experimental%20validation%20on%20toy%20examples%2C%20molecular%20simulations%2C%20and%20quantum%20field%20theory%20demonstrates%20improvements%20over%20existing%20learning-based%20methods.%20Our%20PyTorch%20implementation%20is%20available%20at%20https%3A//github.com/jiajunhe98/FEAT.&entry.1838667208=http%3A//arxiv.org/abs/2504.11516v3&entry.124074799=Read"},
{"title": "Periodic Asynchrony: An On-Policy Approach for Accelerating LLM Reinforcement Learning", "author": "Jian Lu and Yi Luo", "abstract": "Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, our approach consistently delivers significant end-to-end training efficiency improvements on NPU platforms, indicating its potential for widespread application.", "link": "http://arxiv.org/abs/2511.18871v4", "date": "2026-01-16", "relevancy": 1.9143, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.511}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4645}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Periodic%20Asynchrony%3A%20An%20On-Policy%20Approach%20for%20Accelerating%20LLM%20Reinforcement%20Learning&body=Title%3A%20Periodic%20Asynchrony%3A%20An%20On-Policy%20Approach%20for%20Accelerating%20LLM%20Reinforcement%20Learning%0AAuthor%3A%20Jian%20Lu%20and%20Yi%20Luo%0AAbstract%3A%20Since%20the%20introduction%20of%20the%20GRPO%20algorithm%2C%20reinforcement%20learning%20%28RL%29%20has%20attracted%20increasing%20attention%2C%20with%20growing%20efforts%20to%20reproduce%20and%20apply%20it.%20However%2C%20training%20efficiency%20remains%20a%20critical%20challenge.%20In%20mainstream%20RL%20frameworks%2C%20inference%20and%20training%20are%20typically%20deployed%20on%20the%20same%20devices.%20While%20this%20approach%20reduces%20costs%20through%20resource%20consolidation%2C%20its%20synchronous%20execution%20imposes%20a%20computational%20coupling%20that%20prevents%20concurrent%20inference%20and%20training.%20In%20this%20study%2C%20we%20are%20returning%20to%20the%20strategy%20of%20separating%20inference%20and%20training%20deployment%2C%20and%20by%20introducing%20improvements%20in%20the%20data%20loader%2C%20we%20transform%20the%20conventional%20synchronous%20architecture%20into%20a%20periodically%20asynchronous%20framework%2C%20which%20allows%20for%20demand-driven%2C%20independent%2C%20and%20elastic%20scaling%20of%20each%20component%2C%20while%20the%20accuracy%20of%20the%20algorithm%20remains%20completely%20equivalent%20to%20the%20synchronization%20method%2C%20with%20both%20belonging%20to%20the%20on-policy%20strategy.%20It%20is%20worth%20emphasizing%20that%20we%20apply%20a%20unified%20tri-model%20architecture%20in%20the%20training%20phase%2C%20and%20we%20also%20proposed%20a%20shared-prompt%20attention%20mask%20to%20reduce%20repetitive%20computation.%20In%20practice%2C%20our%20approach%20consistently%20delivers%20significant%20end-to-end%20training%20efficiency%20improvements%20on%20NPU%20platforms%2C%20indicating%20its%20potential%20for%20widespread%20application.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18871v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPeriodic%2520Asynchrony%253A%2520An%2520On-Policy%2520Approach%2520for%2520Accelerating%2520LLM%2520Reinforcement%2520Learning%26entry.906535625%3DJian%2520Lu%2520and%2520Yi%2520Luo%26entry.1292438233%3DSince%2520the%2520introduction%2520of%2520the%2520GRPO%2520algorithm%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520attracted%2520increasing%2520attention%252C%2520with%2520growing%2520efforts%2520to%2520reproduce%2520and%2520apply%2520it.%2520However%252C%2520training%2520efficiency%2520remains%2520a%2520critical%2520challenge.%2520In%2520mainstream%2520RL%2520frameworks%252C%2520inference%2520and%2520training%2520are%2520typically%2520deployed%2520on%2520the%2520same%2520devices.%2520While%2520this%2520approach%2520reduces%2520costs%2520through%2520resource%2520consolidation%252C%2520its%2520synchronous%2520execution%2520imposes%2520a%2520computational%2520coupling%2520that%2520prevents%2520concurrent%2520inference%2520and%2520training.%2520In%2520this%2520study%252C%2520we%2520are%2520returning%2520to%2520the%2520strategy%2520of%2520separating%2520inference%2520and%2520training%2520deployment%252C%2520and%2520by%2520introducing%2520improvements%2520in%2520the%2520data%2520loader%252C%2520we%2520transform%2520the%2520conventional%2520synchronous%2520architecture%2520into%2520a%2520periodically%2520asynchronous%2520framework%252C%2520which%2520allows%2520for%2520demand-driven%252C%2520independent%252C%2520and%2520elastic%2520scaling%2520of%2520each%2520component%252C%2520while%2520the%2520accuracy%2520of%2520the%2520algorithm%2520remains%2520completely%2520equivalent%2520to%2520the%2520synchronization%2520method%252C%2520with%2520both%2520belonging%2520to%2520the%2520on-policy%2520strategy.%2520It%2520is%2520worth%2520emphasizing%2520that%2520we%2520apply%2520a%2520unified%2520tri-model%2520architecture%2520in%2520the%2520training%2520phase%252C%2520and%2520we%2520also%2520proposed%2520a%2520shared-prompt%2520attention%2520mask%2520to%2520reduce%2520repetitive%2520computation.%2520In%2520practice%252C%2520our%2520approach%2520consistently%2520delivers%2520significant%2520end-to-end%2520training%2520efficiency%2520improvements%2520on%2520NPU%2520platforms%252C%2520indicating%2520its%2520potential%2520for%2520widespread%2520application.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18871v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Periodic%20Asynchrony%3A%20An%20On-Policy%20Approach%20for%20Accelerating%20LLM%20Reinforcement%20Learning&entry.906535625=Jian%20Lu%20and%20Yi%20Luo&entry.1292438233=Since%20the%20introduction%20of%20the%20GRPO%20algorithm%2C%20reinforcement%20learning%20%28RL%29%20has%20attracted%20increasing%20attention%2C%20with%20growing%20efforts%20to%20reproduce%20and%20apply%20it.%20However%2C%20training%20efficiency%20remains%20a%20critical%20challenge.%20In%20mainstream%20RL%20frameworks%2C%20inference%20and%20training%20are%20typically%20deployed%20on%20the%20same%20devices.%20While%20this%20approach%20reduces%20costs%20through%20resource%20consolidation%2C%20its%20synchronous%20execution%20imposes%20a%20computational%20coupling%20that%20prevents%20concurrent%20inference%20and%20training.%20In%20this%20study%2C%20we%20are%20returning%20to%20the%20strategy%20of%20separating%20inference%20and%20training%20deployment%2C%20and%20by%20introducing%20improvements%20in%20the%20data%20loader%2C%20we%20transform%20the%20conventional%20synchronous%20architecture%20into%20a%20periodically%20asynchronous%20framework%2C%20which%20allows%20for%20demand-driven%2C%20independent%2C%20and%20elastic%20scaling%20of%20each%20component%2C%20while%20the%20accuracy%20of%20the%20algorithm%20remains%20completely%20equivalent%20to%20the%20synchronization%20method%2C%20with%20both%20belonging%20to%20the%20on-policy%20strategy.%20It%20is%20worth%20emphasizing%20that%20we%20apply%20a%20unified%20tri-model%20architecture%20in%20the%20training%20phase%2C%20and%20we%20also%20proposed%20a%20shared-prompt%20attention%20mask%20to%20reduce%20repetitive%20computation.%20In%20practice%2C%20our%20approach%20consistently%20delivers%20significant%20end-to-end%20training%20efficiency%20improvements%20on%20NPU%20platforms%2C%20indicating%20its%20potential%20for%20widespread%20application.&entry.1838667208=http%3A//arxiv.org/abs/2511.18871v4&entry.124074799=Read"},
{"title": "Building Production-Ready Probes For Gemini", "author": "J\u00e1nos Kram\u00e1r and Joshua Engels and Zheng Wang and Bilal Chughtai and Rohin Shah and Neel Nanda and Arthur Conmy", "abstract": "Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.\n  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.\n  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.", "link": "http://arxiv.org/abs/2601.11516v1", "date": "2026-01-16", "relevancy": 1.8994, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Production-Ready%20Probes%20For%20Gemini&body=Title%3A%20Building%20Production-Ready%20Probes%20For%20Gemini%0AAuthor%3A%20J%C3%A1nos%20Kram%C3%A1r%20and%20Joshua%20Engels%20and%20Zheng%20Wang%20and%20Bilal%20Chughtai%20and%20Rohin%20Shah%20and%20Neel%20Nanda%20and%20Arthur%20Conmy%0AAbstract%3A%20Frontier%20language%20model%20capabilities%20are%20improving%20rapidly.%20We%20thus%20need%20stronger%20mitigations%20against%20bad%20actors%20misusing%20increasingly%20powerful%20systems.%20Prior%20work%20has%20shown%20that%20activation%20probes%20may%20be%20a%20promising%20misuse%20mitigation%20technique%2C%20but%20we%20identify%20a%20key%20remaining%20challenge%3A%20probes%20fail%20to%20generalize%20under%20important%20production%20distribution%20shifts.%20In%20particular%2C%20we%20find%20that%20the%20shift%20from%20short-context%20to%20long-context%20inputs%20is%20difficult%20for%20existing%20probe%20architectures.%20We%20propose%20several%20new%20probe%20architecture%20that%20handle%20this%20long-context%20distribution%20shift.%0A%20%20We%20evaluate%20these%20probes%20in%20the%20cyber-offensive%20domain%2C%20testing%20their%20robustness%20against%20various%20production-relevant%20shifts%2C%20including%20multi-turn%20conversations%2C%20static%20jailbreaks%2C%20and%20adaptive%20red%20teaming.%20Our%20results%20demonstrate%20that%20while%20multimax%20addresses%20context%20length%2C%20a%20combination%20of%20architecture%20choice%20and%20training%20on%20diverse%20distributions%20is%20required%20for%20broad%20generalization.%20Additionally%2C%20we%20show%20that%20pairing%20probes%20with%20prompted%20classifiers%20achieves%20optimal%20accuracy%20at%20a%20low%20cost%20due%20to%20the%20computational%20efficiency%20of%20probes.%0A%20%20These%20findings%20have%20informed%20the%20successful%20deployment%20of%20misuse%20mitigation%20probes%20in%20user-facing%20instances%20of%20Gemini%2C%20Google%27s%20frontier%20language%20model.%20Finally%2C%20we%20find%20early%20positive%20results%20using%20AlphaEvolve%20to%20automate%20improvements%20in%20both%20probe%20architecture%20search%20and%20adaptive%20red%20teaming%2C%20showing%20that%20automating%20some%20AI%20safety%20research%20is%20already%20possible.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Production-Ready%2520Probes%2520For%2520Gemini%26entry.906535625%3DJ%25C3%25A1nos%2520Kram%25C3%25A1r%2520and%2520Joshua%2520Engels%2520and%2520Zheng%2520Wang%2520and%2520Bilal%2520Chughtai%2520and%2520Rohin%2520Shah%2520and%2520Neel%2520Nanda%2520and%2520Arthur%2520Conmy%26entry.1292438233%3DFrontier%2520language%2520model%2520capabilities%2520are%2520improving%2520rapidly.%2520We%2520thus%2520need%2520stronger%2520mitigations%2520against%2520bad%2520actors%2520misusing%2520increasingly%2520powerful%2520systems.%2520Prior%2520work%2520has%2520shown%2520that%2520activation%2520probes%2520may%2520be%2520a%2520promising%2520misuse%2520mitigation%2520technique%252C%2520but%2520we%2520identify%2520a%2520key%2520remaining%2520challenge%253A%2520probes%2520fail%2520to%2520generalize%2520under%2520important%2520production%2520distribution%2520shifts.%2520In%2520particular%252C%2520we%2520find%2520that%2520the%2520shift%2520from%2520short-context%2520to%2520long-context%2520inputs%2520is%2520difficult%2520for%2520existing%2520probe%2520architectures.%2520We%2520propose%2520several%2520new%2520probe%2520architecture%2520that%2520handle%2520this%2520long-context%2520distribution%2520shift.%250A%2520%2520We%2520evaluate%2520these%2520probes%2520in%2520the%2520cyber-offensive%2520domain%252C%2520testing%2520their%2520robustness%2520against%2520various%2520production-relevant%2520shifts%252C%2520including%2520multi-turn%2520conversations%252C%2520static%2520jailbreaks%252C%2520and%2520adaptive%2520red%2520teaming.%2520Our%2520results%2520demonstrate%2520that%2520while%2520multimax%2520addresses%2520context%2520length%252C%2520a%2520combination%2520of%2520architecture%2520choice%2520and%2520training%2520on%2520diverse%2520distributions%2520is%2520required%2520for%2520broad%2520generalization.%2520Additionally%252C%2520we%2520show%2520that%2520pairing%2520probes%2520with%2520prompted%2520classifiers%2520achieves%2520optimal%2520accuracy%2520at%2520a%2520low%2520cost%2520due%2520to%2520the%2520computational%2520efficiency%2520of%2520probes.%250A%2520%2520These%2520findings%2520have%2520informed%2520the%2520successful%2520deployment%2520of%2520misuse%2520mitigation%2520probes%2520in%2520user-facing%2520instances%2520of%2520Gemini%252C%2520Google%2527s%2520frontier%2520language%2520model.%2520Finally%252C%2520we%2520find%2520early%2520positive%2520results%2520using%2520AlphaEvolve%2520to%2520automate%2520improvements%2520in%2520both%2520probe%2520architecture%2520search%2520and%2520adaptive%2520red%2520teaming%252C%2520showing%2520that%2520automating%2520some%2520AI%2520safety%2520research%2520is%2520already%2520possible.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Production-Ready%20Probes%20For%20Gemini&entry.906535625=J%C3%A1nos%20Kram%C3%A1r%20and%20Joshua%20Engels%20and%20Zheng%20Wang%20and%20Bilal%20Chughtai%20and%20Rohin%20Shah%20and%20Neel%20Nanda%20and%20Arthur%20Conmy&entry.1292438233=Frontier%20language%20model%20capabilities%20are%20improving%20rapidly.%20We%20thus%20need%20stronger%20mitigations%20against%20bad%20actors%20misusing%20increasingly%20powerful%20systems.%20Prior%20work%20has%20shown%20that%20activation%20probes%20may%20be%20a%20promising%20misuse%20mitigation%20technique%2C%20but%20we%20identify%20a%20key%20remaining%20challenge%3A%20probes%20fail%20to%20generalize%20under%20important%20production%20distribution%20shifts.%20In%20particular%2C%20we%20find%20that%20the%20shift%20from%20short-context%20to%20long-context%20inputs%20is%20difficult%20for%20existing%20probe%20architectures.%20We%20propose%20several%20new%20probe%20architecture%20that%20handle%20this%20long-context%20distribution%20shift.%0A%20%20We%20evaluate%20these%20probes%20in%20the%20cyber-offensive%20domain%2C%20testing%20their%20robustness%20against%20various%20production-relevant%20shifts%2C%20including%20multi-turn%20conversations%2C%20static%20jailbreaks%2C%20and%20adaptive%20red%20teaming.%20Our%20results%20demonstrate%20that%20while%20multimax%20addresses%20context%20length%2C%20a%20combination%20of%20architecture%20choice%20and%20training%20on%20diverse%20distributions%20is%20required%20for%20broad%20generalization.%20Additionally%2C%20we%20show%20that%20pairing%20probes%20with%20prompted%20classifiers%20achieves%20optimal%20accuracy%20at%20a%20low%20cost%20due%20to%20the%20computational%20efficiency%20of%20probes.%0A%20%20These%20findings%20have%20informed%20the%20successful%20deployment%20of%20misuse%20mitigation%20probes%20in%20user-facing%20instances%20of%20Gemini%2C%20Google%27s%20frontier%20language%20model.%20Finally%2C%20we%20find%20early%20positive%20results%20using%20AlphaEvolve%20to%20automate%20improvements%20in%20both%20probe%20architecture%20search%20and%20adaptive%20red%20teaming%2C%20showing%20that%20automating%20some%20AI%20safety%20research%20is%20already%20possible.&entry.1838667208=http%3A//arxiv.org/abs/2601.11516v1&entry.124074799=Read"},
{"title": "Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models", "author": "Xiaojie Gu and Guangxu Chen and Yuheng Yang and Jingxin Han and Andi Zhang", "abstract": "Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing model editing methods often focus on optimizing an information matrix that blends new and old knowledge. While effective, these approaches can be computationally expensive and may cause conflicts. In contrast, we shift our attention to Hierarchical Orthogonal Residual SprEad of the information matrix, which reduces noisy gradients and enables more stable edits from a different perspective. We demonstrate the effectiveness of our method HORSE through a clear theoretical comparison with several popular methods and extensive experiments conducted on two datasets across multiple LLMs. The results show that HORSE maintains precise massive editing across diverse scenarios. The code is available at https://github.com/XiaojieGu/HORSE", "link": "http://arxiv.org/abs/2601.11441v1", "date": "2026-01-16", "relevancy": 1.8889, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4814}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Orthogonal%20Residual%20Spread%20for%20Precise%20Massive%20Editing%20in%20Large%20Language%20Models&body=Title%3A%20Hierarchical%20Orthogonal%20Residual%20Spread%20for%20Precise%20Massive%20Editing%20in%20Large%20Language%20Models%0AAuthor%3A%20Xiaojie%20Gu%20and%20Guangxu%20Chen%20and%20Yuheng%20Yang%20and%20Jingxin%20Han%20and%20Andi%20Zhang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20exhibit%20exceptional%20performance%20across%20various%20domains%2C%20yet%20they%20face%20critical%20safety%20concerns.%20Model%20editing%20has%20emerged%20as%20an%20effective%20approach%20to%20mitigate%20these%20issues.%20Existing%20model%20editing%20methods%20often%20focus%20on%20optimizing%20an%20information%20matrix%20that%20blends%20new%20and%20old%20knowledge.%20While%20effective%2C%20these%20approaches%20can%20be%20computationally%20expensive%20and%20may%20cause%20conflicts.%20In%20contrast%2C%20we%20shift%20our%20attention%20to%20Hierarchical%20Orthogonal%20Residual%20SprEad%20of%20the%20information%20matrix%2C%20which%20reduces%20noisy%20gradients%20and%20enables%20more%20stable%20edits%20from%20a%20different%20perspective.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20HORSE%20through%20a%20clear%20theoretical%20comparison%20with%20several%20popular%20methods%20and%20extensive%20experiments%20conducted%20on%20two%20datasets%20across%20multiple%20LLMs.%20The%20results%20show%20that%20HORSE%20maintains%20precise%20massive%20editing%20across%20diverse%20scenarios.%20The%20code%20is%20available%20at%20https%3A//github.com/XiaojieGu/HORSE%0ALink%3A%20http%3A//arxiv.org/abs/2601.11441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Orthogonal%2520Residual%2520Spread%2520for%2520Precise%2520Massive%2520Editing%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DXiaojie%2520Gu%2520and%2520Guangxu%2520Chen%2520and%2520Yuheng%2520Yang%2520and%2520Jingxin%2520Han%2520and%2520Andi%2520Zhang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520exceptional%2520performance%2520across%2520various%2520domains%252C%2520yet%2520they%2520face%2520critical%2520safety%2520concerns.%2520Model%2520editing%2520has%2520emerged%2520as%2520an%2520effective%2520approach%2520to%2520mitigate%2520these%2520issues.%2520Existing%2520model%2520editing%2520methods%2520often%2520focus%2520on%2520optimizing%2520an%2520information%2520matrix%2520that%2520blends%2520new%2520and%2520old%2520knowledge.%2520While%2520effective%252C%2520these%2520approaches%2520can%2520be%2520computationally%2520expensive%2520and%2520may%2520cause%2520conflicts.%2520In%2520contrast%252C%2520we%2520shift%2520our%2520attention%2520to%2520Hierarchical%2520Orthogonal%2520Residual%2520SprEad%2520of%2520the%2520information%2520matrix%252C%2520which%2520reduces%2520noisy%2520gradients%2520and%2520enables%2520more%2520stable%2520edits%2520from%2520a%2520different%2520perspective.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520HORSE%2520through%2520a%2520clear%2520theoretical%2520comparison%2520with%2520several%2520popular%2520methods%2520and%2520extensive%2520experiments%2520conducted%2520on%2520two%2520datasets%2520across%2520multiple%2520LLMs.%2520The%2520results%2520show%2520that%2520HORSE%2520maintains%2520precise%2520massive%2520editing%2520across%2520diverse%2520scenarios.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/XiaojieGu/HORSE%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Orthogonal%20Residual%20Spread%20for%20Precise%20Massive%20Editing%20in%20Large%20Language%20Models&entry.906535625=Xiaojie%20Gu%20and%20Guangxu%20Chen%20and%20Yuheng%20Yang%20and%20Jingxin%20Han%20and%20Andi%20Zhang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20exhibit%20exceptional%20performance%20across%20various%20domains%2C%20yet%20they%20face%20critical%20safety%20concerns.%20Model%20editing%20has%20emerged%20as%20an%20effective%20approach%20to%20mitigate%20these%20issues.%20Existing%20model%20editing%20methods%20often%20focus%20on%20optimizing%20an%20information%20matrix%20that%20blends%20new%20and%20old%20knowledge.%20While%20effective%2C%20these%20approaches%20can%20be%20computationally%20expensive%20and%20may%20cause%20conflicts.%20In%20contrast%2C%20we%20shift%20our%20attention%20to%20Hierarchical%20Orthogonal%20Residual%20SprEad%20of%20the%20information%20matrix%2C%20which%20reduces%20noisy%20gradients%20and%20enables%20more%20stable%20edits%20from%20a%20different%20perspective.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20HORSE%20through%20a%20clear%20theoretical%20comparison%20with%20several%20popular%20methods%20and%20extensive%20experiments%20conducted%20on%20two%20datasets%20across%20multiple%20LLMs.%20The%20results%20show%20that%20HORSE%20maintains%20precise%20massive%20editing%20across%20diverse%20scenarios.%20The%20code%20is%20available%20at%20https%3A//github.com/XiaojieGu/HORSE&entry.1838667208=http%3A//arxiv.org/abs/2601.11441v1&entry.124074799=Read"},
{"title": "Assesing the Viability of Unsupervised Learning with Autoencoders for Predictive Maintenance in Helicopter Engines", "author": "P. S\u00e1nchez and K. Reyes and B. Radu and E. Fern\u00e1ndez", "abstract": "Unplanned engine failures in helicopters can lead to severe operational disruptions, safety hazards, and costly repairs. To mitigate these risks, this study compares two predictive maintenance strategies for helicopter engines: a supervised classification pipeline and an unsupervised anomaly detection approach based on autoencoders (AEs). The supervised method relies on labelled examples of both normal and faulty behaviour, while the unsupervised approach learns a model of normal operation using only healthy engine data, flagging deviations as potential faults. Both methods are evaluated on a real-world dataset comprising labelled snapshots of helicopter engine telemetry. While supervised models demonstrate strong performance when annotated failures are available, the AE achieves effective detection without requiring fault labels, making it particularly well suited for settings where failure data are scarce or incomplete. The comparison highlights the practical trade-offs between accuracy, data availability, and deployment feasibility, and underscores the potential of unsupervised learning as a viable solution for early fault detection in aerospace applications.", "link": "http://arxiv.org/abs/2601.11154v1", "date": "2026-01-16", "relevancy": 1.8883, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5075}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4523}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assesing%20the%20Viability%20of%20Unsupervised%20Learning%20with%20Autoencoders%20for%20Predictive%20Maintenance%20in%20Helicopter%20Engines&body=Title%3A%20Assesing%20the%20Viability%20of%20Unsupervised%20Learning%20with%20Autoencoders%20for%20Predictive%20Maintenance%20in%20Helicopter%20Engines%0AAuthor%3A%20P.%20S%C3%A1nchez%20and%20K.%20Reyes%20and%20B.%20Radu%20and%20E.%20Fern%C3%A1ndez%0AAbstract%3A%20Unplanned%20engine%20failures%20in%20helicopters%20can%20lead%20to%20severe%20operational%20disruptions%2C%20safety%20hazards%2C%20and%20costly%20repairs.%20To%20mitigate%20these%20risks%2C%20this%20study%20compares%20two%20predictive%20maintenance%20strategies%20for%20helicopter%20engines%3A%20a%20supervised%20classification%20pipeline%20and%20an%20unsupervised%20anomaly%20detection%20approach%20based%20on%20autoencoders%20%28AEs%29.%20The%20supervised%20method%20relies%20on%20labelled%20examples%20of%20both%20normal%20and%20faulty%20behaviour%2C%20while%20the%20unsupervised%20approach%20learns%20a%20model%20of%20normal%20operation%20using%20only%20healthy%20engine%20data%2C%20flagging%20deviations%20as%20potential%20faults.%20Both%20methods%20are%20evaluated%20on%20a%20real-world%20dataset%20comprising%20labelled%20snapshots%20of%20helicopter%20engine%20telemetry.%20While%20supervised%20models%20demonstrate%20strong%20performance%20when%20annotated%20failures%20are%20available%2C%20the%20AE%20achieves%20effective%20detection%20without%20requiring%20fault%20labels%2C%20making%20it%20particularly%20well%20suited%20for%20settings%20where%20failure%20data%20are%20scarce%20or%20incomplete.%20The%20comparison%20highlights%20the%20practical%20trade-offs%20between%20accuracy%2C%20data%20availability%2C%20and%20deployment%20feasibility%2C%20and%20underscores%20the%20potential%20of%20unsupervised%20learning%20as%20a%20viable%20solution%20for%20early%20fault%20detection%20in%20aerospace%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssesing%2520the%2520Viability%2520of%2520Unsupervised%2520Learning%2520with%2520Autoencoders%2520for%2520Predictive%2520Maintenance%2520in%2520Helicopter%2520Engines%26entry.906535625%3DP.%2520S%25C3%25A1nchez%2520and%2520K.%2520Reyes%2520and%2520B.%2520Radu%2520and%2520E.%2520Fern%25C3%25A1ndez%26entry.1292438233%3DUnplanned%2520engine%2520failures%2520in%2520helicopters%2520can%2520lead%2520to%2520severe%2520operational%2520disruptions%252C%2520safety%2520hazards%252C%2520and%2520costly%2520repairs.%2520To%2520mitigate%2520these%2520risks%252C%2520this%2520study%2520compares%2520two%2520predictive%2520maintenance%2520strategies%2520for%2520helicopter%2520engines%253A%2520a%2520supervised%2520classification%2520pipeline%2520and%2520an%2520unsupervised%2520anomaly%2520detection%2520approach%2520based%2520on%2520autoencoders%2520%2528AEs%2529.%2520The%2520supervised%2520method%2520relies%2520on%2520labelled%2520examples%2520of%2520both%2520normal%2520and%2520faulty%2520behaviour%252C%2520while%2520the%2520unsupervised%2520approach%2520learns%2520a%2520model%2520of%2520normal%2520operation%2520using%2520only%2520healthy%2520engine%2520data%252C%2520flagging%2520deviations%2520as%2520potential%2520faults.%2520Both%2520methods%2520are%2520evaluated%2520on%2520a%2520real-world%2520dataset%2520comprising%2520labelled%2520snapshots%2520of%2520helicopter%2520engine%2520telemetry.%2520While%2520supervised%2520models%2520demonstrate%2520strong%2520performance%2520when%2520annotated%2520failures%2520are%2520available%252C%2520the%2520AE%2520achieves%2520effective%2520detection%2520without%2520requiring%2520fault%2520labels%252C%2520making%2520it%2520particularly%2520well%2520suited%2520for%2520settings%2520where%2520failure%2520data%2520are%2520scarce%2520or%2520incomplete.%2520The%2520comparison%2520highlights%2520the%2520practical%2520trade-offs%2520between%2520accuracy%252C%2520data%2520availability%252C%2520and%2520deployment%2520feasibility%252C%2520and%2520underscores%2520the%2520potential%2520of%2520unsupervised%2520learning%2520as%2520a%2520viable%2520solution%2520for%2520early%2520fault%2520detection%2520in%2520aerospace%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assesing%20the%20Viability%20of%20Unsupervised%20Learning%20with%20Autoencoders%20for%20Predictive%20Maintenance%20in%20Helicopter%20Engines&entry.906535625=P.%20S%C3%A1nchez%20and%20K.%20Reyes%20and%20B.%20Radu%20and%20E.%20Fern%C3%A1ndez&entry.1292438233=Unplanned%20engine%20failures%20in%20helicopters%20can%20lead%20to%20severe%20operational%20disruptions%2C%20safety%20hazards%2C%20and%20costly%20repairs.%20To%20mitigate%20these%20risks%2C%20this%20study%20compares%20two%20predictive%20maintenance%20strategies%20for%20helicopter%20engines%3A%20a%20supervised%20classification%20pipeline%20and%20an%20unsupervised%20anomaly%20detection%20approach%20based%20on%20autoencoders%20%28AEs%29.%20The%20supervised%20method%20relies%20on%20labelled%20examples%20of%20both%20normal%20and%20faulty%20behaviour%2C%20while%20the%20unsupervised%20approach%20learns%20a%20model%20of%20normal%20operation%20using%20only%20healthy%20engine%20data%2C%20flagging%20deviations%20as%20potential%20faults.%20Both%20methods%20are%20evaluated%20on%20a%20real-world%20dataset%20comprising%20labelled%20snapshots%20of%20helicopter%20engine%20telemetry.%20While%20supervised%20models%20demonstrate%20strong%20performance%20when%20annotated%20failures%20are%20available%2C%20the%20AE%20achieves%20effective%20detection%20without%20requiring%20fault%20labels%2C%20making%20it%20particularly%20well%20suited%20for%20settings%20where%20failure%20data%20are%20scarce%20or%20incomplete.%20The%20comparison%20highlights%20the%20practical%20trade-offs%20between%20accuracy%2C%20data%20availability%2C%20and%20deployment%20feasibility%2C%20and%20underscores%20the%20potential%20of%20unsupervised%20learning%20as%20a%20viable%20solution%20for%20early%20fault%20detection%20in%20aerospace%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.11154v1&entry.124074799=Read"},
{"title": "MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction", "author": "Yue Huang and Yanyuan Chen and Dexuan Xu and Chenzhuo Zhao and Weihua Yue and Yu Huang", "abstract": "Medical problem-solving demands expert knowledge and intricate reasoning. Recent studies of large language models (LLMs) attempt to ease this complexity by introducing external knowledge verification through retrieval-augmented generation or by training on reasoning datasets. However, these approaches suffer from drawbacks such as retrieval overhead and high annotation costs, and they heavily rely on substituted external assistants to reach limited performance in medical field. In this paper, we introduce MedReflect, a generalizable framework designed to inspire LLMs with a physician-like reflective thinking mode. MedReflect generates a single-pass reflection chain that includes initial hypothesis generation, self-questioning, self-answering and decision refinement. This self-verified and self-reflective nature releases large language model's latent capability in medical problem-solving without external retrieval or heavy annotation. We demonstrate that MedReflect enables cost-efficient medical dataset construction. With only a minimal subset of randomly sampled training examples and lightweight fine-tuning, this approach achieves notable absolute accuracy improvements across a series of medical benchmarks while significantly cutting annotation requirements. Our results provide evidence that LLMs can learn to solve specialized medical problems via self-reflection and self-improvement, reducing reliance on external supervision and extensive task-specific fine-tuning data.", "link": "http://arxiv.org/abs/2510.03687v2", "date": "2026-01-16", "relevancy": 1.8864, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5293}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4639}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedReflect%3A%20Teaching%20Medical%20LLMs%20to%20Self-Improve%20via%20Reflective%20Correction&body=Title%3A%20MedReflect%3A%20Teaching%20Medical%20LLMs%20to%20Self-Improve%20via%20Reflective%20Correction%0AAuthor%3A%20Yue%20Huang%20and%20Yanyuan%20Chen%20and%20Dexuan%20Xu%20and%20Chenzhuo%20Zhao%20and%20Weihua%20Yue%20and%20Yu%20Huang%0AAbstract%3A%20Medical%20problem-solving%20demands%20expert%20knowledge%20and%20intricate%20reasoning.%20Recent%20studies%20of%20large%20language%20models%20%28LLMs%29%20attempt%20to%20ease%20this%20complexity%20by%20introducing%20external%20knowledge%20verification%20through%20retrieval-augmented%20generation%20or%20by%20training%20on%20reasoning%20datasets.%20However%2C%20these%20approaches%20suffer%20from%20drawbacks%20such%20as%20retrieval%20overhead%20and%20high%20annotation%20costs%2C%20and%20they%20heavily%20rely%20on%20substituted%20external%20assistants%20to%20reach%20limited%20performance%20in%20medical%20field.%20In%20this%20paper%2C%20we%20introduce%20MedReflect%2C%20a%20generalizable%20framework%20designed%20to%20inspire%20LLMs%20with%20a%20physician-like%20reflective%20thinking%20mode.%20MedReflect%20generates%20a%20single-pass%20reflection%20chain%20that%20includes%20initial%20hypothesis%20generation%2C%20self-questioning%2C%20self-answering%20and%20decision%20refinement.%20This%20self-verified%20and%20self-reflective%20nature%20releases%20large%20language%20model%27s%20latent%20capability%20in%20medical%20problem-solving%20without%20external%20retrieval%20or%20heavy%20annotation.%20We%20demonstrate%20that%20MedReflect%20enables%20cost-efficient%20medical%20dataset%20construction.%20With%20only%20a%20minimal%20subset%20of%20randomly%20sampled%20training%20examples%20and%20lightweight%20fine-tuning%2C%20this%20approach%20achieves%20notable%20absolute%20accuracy%20improvements%20across%20a%20series%20of%20medical%20benchmarks%20while%20significantly%20cutting%20annotation%20requirements.%20Our%20results%20provide%20evidence%20that%20LLMs%20can%20learn%20to%20solve%20specialized%20medical%20problems%20via%20self-reflection%20and%20self-improvement%2C%20reducing%20reliance%20on%20external%20supervision%20and%20extensive%20task-specific%20fine-tuning%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2510.03687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedReflect%253A%2520Teaching%2520Medical%2520LLMs%2520to%2520Self-Improve%2520via%2520Reflective%2520Correction%26entry.906535625%3DYue%2520Huang%2520and%2520Yanyuan%2520Chen%2520and%2520Dexuan%2520Xu%2520and%2520Chenzhuo%2520Zhao%2520and%2520Weihua%2520Yue%2520and%2520Yu%2520Huang%26entry.1292438233%3DMedical%2520problem-solving%2520demands%2520expert%2520knowledge%2520and%2520intricate%2520reasoning.%2520Recent%2520studies%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520attempt%2520to%2520ease%2520this%2520complexity%2520by%2520introducing%2520external%2520knowledge%2520verification%2520through%2520retrieval-augmented%2520generation%2520or%2520by%2520training%2520on%2520reasoning%2520datasets.%2520However%252C%2520these%2520approaches%2520suffer%2520from%2520drawbacks%2520such%2520as%2520retrieval%2520overhead%2520and%2520high%2520annotation%2520costs%252C%2520and%2520they%2520heavily%2520rely%2520on%2520substituted%2520external%2520assistants%2520to%2520reach%2520limited%2520performance%2520in%2520medical%2520field.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MedReflect%252C%2520a%2520generalizable%2520framework%2520designed%2520to%2520inspire%2520LLMs%2520with%2520a%2520physician-like%2520reflective%2520thinking%2520mode.%2520MedReflect%2520generates%2520a%2520single-pass%2520reflection%2520chain%2520that%2520includes%2520initial%2520hypothesis%2520generation%252C%2520self-questioning%252C%2520self-answering%2520and%2520decision%2520refinement.%2520This%2520self-verified%2520and%2520self-reflective%2520nature%2520releases%2520large%2520language%2520model%2527s%2520latent%2520capability%2520in%2520medical%2520problem-solving%2520without%2520external%2520retrieval%2520or%2520heavy%2520annotation.%2520We%2520demonstrate%2520that%2520MedReflect%2520enables%2520cost-efficient%2520medical%2520dataset%2520construction.%2520With%2520only%2520a%2520minimal%2520subset%2520of%2520randomly%2520sampled%2520training%2520examples%2520and%2520lightweight%2520fine-tuning%252C%2520this%2520approach%2520achieves%2520notable%2520absolute%2520accuracy%2520improvements%2520across%2520a%2520series%2520of%2520medical%2520benchmarks%2520while%2520significantly%2520cutting%2520annotation%2520requirements.%2520Our%2520results%2520provide%2520evidence%2520that%2520LLMs%2520can%2520learn%2520to%2520solve%2520specialized%2520medical%2520problems%2520via%2520self-reflection%2520and%2520self-improvement%252C%2520reducing%2520reliance%2520on%2520external%2520supervision%2520and%2520extensive%2520task-specific%2520fine-tuning%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedReflect%3A%20Teaching%20Medical%20LLMs%20to%20Self-Improve%20via%20Reflective%20Correction&entry.906535625=Yue%20Huang%20and%20Yanyuan%20Chen%20and%20Dexuan%20Xu%20and%20Chenzhuo%20Zhao%20and%20Weihua%20Yue%20and%20Yu%20Huang&entry.1292438233=Medical%20problem-solving%20demands%20expert%20knowledge%20and%20intricate%20reasoning.%20Recent%20studies%20of%20large%20language%20models%20%28LLMs%29%20attempt%20to%20ease%20this%20complexity%20by%20introducing%20external%20knowledge%20verification%20through%20retrieval-augmented%20generation%20or%20by%20training%20on%20reasoning%20datasets.%20However%2C%20these%20approaches%20suffer%20from%20drawbacks%20such%20as%20retrieval%20overhead%20and%20high%20annotation%20costs%2C%20and%20they%20heavily%20rely%20on%20substituted%20external%20assistants%20to%20reach%20limited%20performance%20in%20medical%20field.%20In%20this%20paper%2C%20we%20introduce%20MedReflect%2C%20a%20generalizable%20framework%20designed%20to%20inspire%20LLMs%20with%20a%20physician-like%20reflective%20thinking%20mode.%20MedReflect%20generates%20a%20single-pass%20reflection%20chain%20that%20includes%20initial%20hypothesis%20generation%2C%20self-questioning%2C%20self-answering%20and%20decision%20refinement.%20This%20self-verified%20and%20self-reflective%20nature%20releases%20large%20language%20model%27s%20latent%20capability%20in%20medical%20problem-solving%20without%20external%20retrieval%20or%20heavy%20annotation.%20We%20demonstrate%20that%20MedReflect%20enables%20cost-efficient%20medical%20dataset%20construction.%20With%20only%20a%20minimal%20subset%20of%20randomly%20sampled%20training%20examples%20and%20lightweight%20fine-tuning%2C%20this%20approach%20achieves%20notable%20absolute%20accuracy%20improvements%20across%20a%20series%20of%20medical%20benchmarks%20while%20significantly%20cutting%20annotation%20requirements.%20Our%20results%20provide%20evidence%20that%20LLMs%20can%20learn%20to%20solve%20specialized%20medical%20problems%20via%20self-reflection%20and%20self-improvement%2C%20reducing%20reliance%20on%20external%20supervision%20and%20extensive%20task-specific%20fine-tuning%20data.&entry.1838667208=http%3A//arxiv.org/abs/2510.03687v2&entry.124074799=Read"},
{"title": "Probabilistic Mission Design for Neuro-Symbolic Unmanned Aircraft Systems", "author": "Simon Kohaut and Benedict Flade and Daniel Ochs and Devendra Singh Dhami and Julian Eggert and Kristian Kersting", "abstract": "Advanced Air Mobility (AAM) is a growing field that demands accurate and trustworthy models of legal concepts and restrictions for navigating Unmanned Aircraft Systems (UAS). In addition, any implementation of AAM needs to face the challenges posed by inherently dynamic and uncertain human-inhabited spaces robustly. Nevertheless, the employment of UAS beyond visual line of sight (BVLOS) is an endearing task that promises to significantly enhance today's logistics and emergency response capabilities. Hence, we propose Probabilistic Mission Design (ProMis), a novel neuro-symbolic approach to navigating UAS within legal frameworks. ProMis is an interpretable and adaptable system architecture that links uncertain geospatial data and noisy perception with declarative, Hybrid Probabilistic Logic Programs (HPLP) to reason over the agent's state space and its legality. To inform planning with legal restrictions and uncertainty in mind, ProMis yields Probabilistic Mission Landscapes (PML). These scalar fields quantify the belief that the HPLP is satisfied across the agent's state space. Extending prior work on ProMis' reasoning capabilities and computational characteristics, we show its integration with potent machine learning models such as Large Language Models (LLM) and Transformer-based vision models. Hence, our experiments underpin the application of ProMis with multi-modal input data and how our method applies to many AAM scenarios.", "link": "http://arxiv.org/abs/2501.01439v2", "date": "2026-01-16", "relevancy": 1.7057, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5995}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5838}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Mission%20Design%20for%20Neuro-Symbolic%20Unmanned%20Aircraft%20Systems&body=Title%3A%20Probabilistic%20Mission%20Design%20for%20Neuro-Symbolic%20Unmanned%20Aircraft%20Systems%0AAuthor%3A%20Simon%20Kohaut%20and%20Benedict%20Flade%20and%20Daniel%20Ochs%20and%20Devendra%20Singh%20Dhami%20and%20Julian%20Eggert%20and%20Kristian%20Kersting%0AAbstract%3A%20Advanced%20Air%20Mobility%20%28AAM%29%20is%20a%20growing%20field%20that%20demands%20accurate%20and%20trustworthy%20models%20of%20legal%20concepts%20and%20restrictions%20for%20navigating%20Unmanned%20Aircraft%20Systems%20%28UAS%29.%20In%20addition%2C%20any%20implementation%20of%20AAM%20needs%20to%20face%20the%20challenges%20posed%20by%20inherently%20dynamic%20and%20uncertain%20human-inhabited%20spaces%20robustly.%20Nevertheless%2C%20the%20employment%20of%20UAS%20beyond%20visual%20line%20of%20sight%20%28BVLOS%29%20is%20an%20endearing%20task%20that%20promises%20to%20significantly%20enhance%20today%27s%20logistics%20and%20emergency%20response%20capabilities.%20Hence%2C%20we%20propose%20Probabilistic%20Mission%20Design%20%28ProMis%29%2C%20a%20novel%20neuro-symbolic%20approach%20to%20navigating%20UAS%20within%20legal%20frameworks.%20ProMis%20is%20an%20interpretable%20and%20adaptable%20system%20architecture%20that%20links%20uncertain%20geospatial%20data%20and%20noisy%20perception%20with%20declarative%2C%20Hybrid%20Probabilistic%20Logic%20Programs%20%28HPLP%29%20to%20reason%20over%20the%20agent%27s%20state%20space%20and%20its%20legality.%20To%20inform%20planning%20with%20legal%20restrictions%20and%20uncertainty%20in%20mind%2C%20ProMis%20yields%20Probabilistic%20Mission%20Landscapes%20%28PML%29.%20These%20scalar%20fields%20quantify%20the%20belief%20that%20the%20HPLP%20is%20satisfied%20across%20the%20agent%27s%20state%20space.%20Extending%20prior%20work%20on%20ProMis%27%20reasoning%20capabilities%20and%20computational%20characteristics%2C%20we%20show%20its%20integration%20with%20potent%20machine%20learning%20models%20such%20as%20Large%20Language%20Models%20%28LLM%29%20and%20Transformer-based%20vision%20models.%20Hence%2C%20our%20experiments%20underpin%20the%20application%20of%20ProMis%20with%20multi-modal%20input%20data%20and%20how%20our%20method%20applies%20to%20many%20AAM%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2501.01439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Mission%2520Design%2520for%2520Neuro-Symbolic%2520Unmanned%2520Aircraft%2520Systems%26entry.906535625%3DSimon%2520Kohaut%2520and%2520Benedict%2520Flade%2520and%2520Daniel%2520Ochs%2520and%2520Devendra%2520Singh%2520Dhami%2520and%2520Julian%2520Eggert%2520and%2520Kristian%2520Kersting%26entry.1292438233%3DAdvanced%2520Air%2520Mobility%2520%2528AAM%2529%2520is%2520a%2520growing%2520field%2520that%2520demands%2520accurate%2520and%2520trustworthy%2520models%2520of%2520legal%2520concepts%2520and%2520restrictions%2520for%2520navigating%2520Unmanned%2520Aircraft%2520Systems%2520%2528UAS%2529.%2520In%2520addition%252C%2520any%2520implementation%2520of%2520AAM%2520needs%2520to%2520face%2520the%2520challenges%2520posed%2520by%2520inherently%2520dynamic%2520and%2520uncertain%2520human-inhabited%2520spaces%2520robustly.%2520Nevertheless%252C%2520the%2520employment%2520of%2520UAS%2520beyond%2520visual%2520line%2520of%2520sight%2520%2528BVLOS%2529%2520is%2520an%2520endearing%2520task%2520that%2520promises%2520to%2520significantly%2520enhance%2520today%2527s%2520logistics%2520and%2520emergency%2520response%2520capabilities.%2520Hence%252C%2520we%2520propose%2520Probabilistic%2520Mission%2520Design%2520%2528ProMis%2529%252C%2520a%2520novel%2520neuro-symbolic%2520approach%2520to%2520navigating%2520UAS%2520within%2520legal%2520frameworks.%2520ProMis%2520is%2520an%2520interpretable%2520and%2520adaptable%2520system%2520architecture%2520that%2520links%2520uncertain%2520geospatial%2520data%2520and%2520noisy%2520perception%2520with%2520declarative%252C%2520Hybrid%2520Probabilistic%2520Logic%2520Programs%2520%2528HPLP%2529%2520to%2520reason%2520over%2520the%2520agent%2527s%2520state%2520space%2520and%2520its%2520legality.%2520To%2520inform%2520planning%2520with%2520legal%2520restrictions%2520and%2520uncertainty%2520in%2520mind%252C%2520ProMis%2520yields%2520Probabilistic%2520Mission%2520Landscapes%2520%2528PML%2529.%2520These%2520scalar%2520fields%2520quantify%2520the%2520belief%2520that%2520the%2520HPLP%2520is%2520satisfied%2520across%2520the%2520agent%2527s%2520state%2520space.%2520Extending%2520prior%2520work%2520on%2520ProMis%2527%2520reasoning%2520capabilities%2520and%2520computational%2520characteristics%252C%2520we%2520show%2520its%2520integration%2520with%2520potent%2520machine%2520learning%2520models%2520such%2520as%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520and%2520Transformer-based%2520vision%2520models.%2520Hence%252C%2520our%2520experiments%2520underpin%2520the%2520application%2520of%2520ProMis%2520with%2520multi-modal%2520input%2520data%2520and%2520how%2520our%2520method%2520applies%2520to%2520many%2520AAM%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Mission%20Design%20for%20Neuro-Symbolic%20Unmanned%20Aircraft%20Systems&entry.906535625=Simon%20Kohaut%20and%20Benedict%20Flade%20and%20Daniel%20Ochs%20and%20Devendra%20Singh%20Dhami%20and%20Julian%20Eggert%20and%20Kristian%20Kersting&entry.1292438233=Advanced%20Air%20Mobility%20%28AAM%29%20is%20a%20growing%20field%20that%20demands%20accurate%20and%20trustworthy%20models%20of%20legal%20concepts%20and%20restrictions%20for%20navigating%20Unmanned%20Aircraft%20Systems%20%28UAS%29.%20In%20addition%2C%20any%20implementation%20of%20AAM%20needs%20to%20face%20the%20challenges%20posed%20by%20inherently%20dynamic%20and%20uncertain%20human-inhabited%20spaces%20robustly.%20Nevertheless%2C%20the%20employment%20of%20UAS%20beyond%20visual%20line%20of%20sight%20%28BVLOS%29%20is%20an%20endearing%20task%20that%20promises%20to%20significantly%20enhance%20today%27s%20logistics%20and%20emergency%20response%20capabilities.%20Hence%2C%20we%20propose%20Probabilistic%20Mission%20Design%20%28ProMis%29%2C%20a%20novel%20neuro-symbolic%20approach%20to%20navigating%20UAS%20within%20legal%20frameworks.%20ProMis%20is%20an%20interpretable%20and%20adaptable%20system%20architecture%20that%20links%20uncertain%20geospatial%20data%20and%20noisy%20perception%20with%20declarative%2C%20Hybrid%20Probabilistic%20Logic%20Programs%20%28HPLP%29%20to%20reason%20over%20the%20agent%27s%20state%20space%20and%20its%20legality.%20To%20inform%20planning%20with%20legal%20restrictions%20and%20uncertainty%20in%20mind%2C%20ProMis%20yields%20Probabilistic%20Mission%20Landscapes%20%28PML%29.%20These%20scalar%20fields%20quantify%20the%20belief%20that%20the%20HPLP%20is%20satisfied%20across%20the%20agent%27s%20state%20space.%20Extending%20prior%20work%20on%20ProMis%27%20reasoning%20capabilities%20and%20computational%20characteristics%2C%20we%20show%20its%20integration%20with%20potent%20machine%20learning%20models%20such%20as%20Large%20Language%20Models%20%28LLM%29%20and%20Transformer-based%20vision%20models.%20Hence%2C%20our%20experiments%20underpin%20the%20application%20of%20ProMis%20with%20multi-modal%20input%20data%20and%20how%20our%20method%20applies%20to%20many%20AAM%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2501.01439v2&entry.124074799=Read"},
{"title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training", "author": "Shuo Cheng and Liqian Ma and Zhenyang Chen and Ajay Mandlekar and Caelan Garrett and Danfei Xu", "abstract": "Behavior cloning has shown promise for robot manipulation, but real-world demonstrations are costly to acquire at scale. While simulated data offers a scalable alternative, particularly with advances in automated demonstration generation, transferring policies to the real world is hampered by various simulation and real domain gaps. In this work, we propose a unified sim-and-real co-training framework for learning generalizable manipulation policies that primarily leverages simulation and only requires a few real-world demonstrations. Central to our approach is learning a domain-invariant, task-relevant feature space. Our key insight is that aligning the joint distributions of observations and their corresponding actions across domains provides a richer signal than aligning observations (marginals) alone. We achieve this by embedding an Optimal Transport (OT)-inspired loss within the co-training framework, and extend this to an Unbalanced OT framework to handle the imbalance between abundant simulation data and limited real-world examples. We validate our method on challenging manipulation tasks, showing it can leverage abundant simulation data to achieve up to a 30% improvement in the real-world success rate and even generalize to scenarios seen only in simulation. Project webpage: https://ot-sim2real.github.io/.", "link": "http://arxiv.org/abs/2509.18631v3", "date": "2026-01-16", "relevancy": 1.6878, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5803}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5673}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Domain%20Adaptation%20for%20Sim-and-Real%20Policy%20Co-Training&body=Title%3A%20Generalizable%20Domain%20Adaptation%20for%20Sim-and-Real%20Policy%20Co-Training%0AAuthor%3A%20Shuo%20Cheng%20and%20Liqian%20Ma%20and%20Zhenyang%20Chen%20and%20Ajay%20Mandlekar%20and%20Caelan%20Garrett%20and%20Danfei%20Xu%0AAbstract%3A%20Behavior%20cloning%20has%20shown%20promise%20for%20robot%20manipulation%2C%20but%20real-world%20demonstrations%20are%20costly%20to%20acquire%20at%20scale.%20While%20simulated%20data%20offers%20a%20scalable%20alternative%2C%20particularly%20with%20advances%20in%20automated%20demonstration%20generation%2C%20transferring%20policies%20to%20the%20real%20world%20is%20hampered%20by%20various%20simulation%20and%20real%20domain%20gaps.%20In%20this%20work%2C%20we%20propose%20a%20unified%20sim-and-real%20co-training%20framework%20for%20learning%20generalizable%20manipulation%20policies%20that%20primarily%20leverages%20simulation%20and%20only%20requires%20a%20few%20real-world%20demonstrations.%20Central%20to%20our%20approach%20is%20learning%20a%20domain-invariant%2C%20task-relevant%20feature%20space.%20Our%20key%20insight%20is%20that%20aligning%20the%20joint%20distributions%20of%20observations%20and%20their%20corresponding%20actions%20across%20domains%20provides%20a%20richer%20signal%20than%20aligning%20observations%20%28marginals%29%20alone.%20We%20achieve%20this%20by%20embedding%20an%20Optimal%20Transport%20%28OT%29-inspired%20loss%20within%20the%20co-training%20framework%2C%20and%20extend%20this%20to%20an%20Unbalanced%20OT%20framework%20to%20handle%20the%20imbalance%20between%20abundant%20simulation%20data%20and%20limited%20real-world%20examples.%20We%20validate%20our%20method%20on%20challenging%20manipulation%20tasks%2C%20showing%20it%20can%20leverage%20abundant%20simulation%20data%20to%20achieve%20up%20to%20a%2030%25%20improvement%20in%20the%20real-world%20success%20rate%20and%20even%20generalize%20to%20scenarios%20seen%20only%20in%20simulation.%20Project%20webpage%3A%20https%3A//ot-sim2real.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2509.18631v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520Domain%2520Adaptation%2520for%2520Sim-and-Real%2520Policy%2520Co-Training%26entry.906535625%3DShuo%2520Cheng%2520and%2520Liqian%2520Ma%2520and%2520Zhenyang%2520Chen%2520and%2520Ajay%2520Mandlekar%2520and%2520Caelan%2520Garrett%2520and%2520Danfei%2520Xu%26entry.1292438233%3DBehavior%2520cloning%2520has%2520shown%2520promise%2520for%2520robot%2520manipulation%252C%2520but%2520real-world%2520demonstrations%2520are%2520costly%2520to%2520acquire%2520at%2520scale.%2520While%2520simulated%2520data%2520offers%2520a%2520scalable%2520alternative%252C%2520particularly%2520with%2520advances%2520in%2520automated%2520demonstration%2520generation%252C%2520transferring%2520policies%2520to%2520the%2520real%2520world%2520is%2520hampered%2520by%2520various%2520simulation%2520and%2520real%2520domain%2520gaps.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520unified%2520sim-and-real%2520co-training%2520framework%2520for%2520learning%2520generalizable%2520manipulation%2520policies%2520that%2520primarily%2520leverages%2520simulation%2520and%2520only%2520requires%2520a%2520few%2520real-world%2520demonstrations.%2520Central%2520to%2520our%2520approach%2520is%2520learning%2520a%2520domain-invariant%252C%2520task-relevant%2520feature%2520space.%2520Our%2520key%2520insight%2520is%2520that%2520aligning%2520the%2520joint%2520distributions%2520of%2520observations%2520and%2520their%2520corresponding%2520actions%2520across%2520domains%2520provides%2520a%2520richer%2520signal%2520than%2520aligning%2520observations%2520%2528marginals%2529%2520alone.%2520We%2520achieve%2520this%2520by%2520embedding%2520an%2520Optimal%2520Transport%2520%2528OT%2529-inspired%2520loss%2520within%2520the%2520co-training%2520framework%252C%2520and%2520extend%2520this%2520to%2520an%2520Unbalanced%2520OT%2520framework%2520to%2520handle%2520the%2520imbalance%2520between%2520abundant%2520simulation%2520data%2520and%2520limited%2520real-world%2520examples.%2520We%2520validate%2520our%2520method%2520on%2520challenging%2520manipulation%2520tasks%252C%2520showing%2520it%2520can%2520leverage%2520abundant%2520simulation%2520data%2520to%2520achieve%2520up%2520to%2520a%252030%2525%2520improvement%2520in%2520the%2520real-world%2520success%2520rate%2520and%2520even%2520generalize%2520to%2520scenarios%2520seen%2520only%2520in%2520simulation.%2520Project%2520webpage%253A%2520https%253A//ot-sim2real.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18631v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Domain%20Adaptation%20for%20Sim-and-Real%20Policy%20Co-Training&entry.906535625=Shuo%20Cheng%20and%20Liqian%20Ma%20and%20Zhenyang%20Chen%20and%20Ajay%20Mandlekar%20and%20Caelan%20Garrett%20and%20Danfei%20Xu&entry.1292438233=Behavior%20cloning%20has%20shown%20promise%20for%20robot%20manipulation%2C%20but%20real-world%20demonstrations%20are%20costly%20to%20acquire%20at%20scale.%20While%20simulated%20data%20offers%20a%20scalable%20alternative%2C%20particularly%20with%20advances%20in%20automated%20demonstration%20generation%2C%20transferring%20policies%20to%20the%20real%20world%20is%20hampered%20by%20various%20simulation%20and%20real%20domain%20gaps.%20In%20this%20work%2C%20we%20propose%20a%20unified%20sim-and-real%20co-training%20framework%20for%20learning%20generalizable%20manipulation%20policies%20that%20primarily%20leverages%20simulation%20and%20only%20requires%20a%20few%20real-world%20demonstrations.%20Central%20to%20our%20approach%20is%20learning%20a%20domain-invariant%2C%20task-relevant%20feature%20space.%20Our%20key%20insight%20is%20that%20aligning%20the%20joint%20distributions%20of%20observations%20and%20their%20corresponding%20actions%20across%20domains%20provides%20a%20richer%20signal%20than%20aligning%20observations%20%28marginals%29%20alone.%20We%20achieve%20this%20by%20embedding%20an%20Optimal%20Transport%20%28OT%29-inspired%20loss%20within%20the%20co-training%20framework%2C%20and%20extend%20this%20to%20an%20Unbalanced%20OT%20framework%20to%20handle%20the%20imbalance%20between%20abundant%20simulation%20data%20and%20limited%20real-world%20examples.%20We%20validate%20our%20method%20on%20challenging%20manipulation%20tasks%2C%20showing%20it%20can%20leverage%20abundant%20simulation%20data%20to%20achieve%20up%20to%20a%2030%25%20improvement%20in%20the%20real-world%20success%20rate%20and%20even%20generalize%20to%20scenarios%20seen%20only%20in%20simulation.%20Project%20webpage%3A%20https%3A//ot-sim2real.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2509.18631v3&entry.124074799=Read"},
{"title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents", "author": "Eilam Shapira and Roi Reichart and Moshe Tennenholtz", "abstract": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.", "link": "http://arxiv.org/abs/2601.11496v1", "date": "2026-01-16", "relevancy": 1.7072, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4519}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4364}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Poisoned%20Apple%20Effect%3A%20Strategic%20Manipulation%20of%20Mediated%20Markets%20via%20Technology%20Expansion%20of%20AI%20Agents&body=Title%3A%20The%20Poisoned%20Apple%20Effect%3A%20Strategic%20Manipulation%20of%20Mediated%20Markets%20via%20Technology%20Expansion%20of%20AI%20Agents%0AAuthor%3A%20Eilam%20Shapira%20and%20Roi%20Reichart%20and%20Moshe%20Tennenholtz%0AAbstract%3A%20The%20integration%20of%20AI%20agents%20into%20economic%20markets%20fundamentally%20alters%20the%20landscape%20of%20strategic%20interaction.%20We%20investigate%20the%20economic%20implications%20of%20expanding%20the%20set%20of%20available%20technologies%20in%20three%20canonical%20game-theoretic%20settings%3A%20bargaining%20%28resource%20division%29%2C%20negotiation%20%28asymmetric%20information%20trade%29%2C%20and%20persuasion%20%28strategic%20information%20transmission%29.%20We%20find%20that%20simply%20increasing%20the%20choice%20of%20AI%20delegates%20can%20drastically%20shift%20equilibrium%20payoffs%20and%20regulatory%20outcomes%2C%20often%20creating%20incentives%20for%20regulators%20to%20proactively%20develop%20and%20release%20technologies.%20Conversely%2C%20we%20identify%20a%20strategic%20phenomenon%20termed%20the%20%22Poisoned%20Apple%22%20effect%3A%20an%20agent%20may%20release%20a%20new%20technology%2C%20which%20neither%20they%20nor%20their%20opponent%20ultimately%20uses%2C%20solely%20to%20manipulate%20the%20regulator%27s%20choice%20of%20market%20design%20in%20their%20favor.%20This%20strategic%20release%20improves%20the%20releaser%27s%20welfare%20at%20the%20expense%20of%20their%20opponent%20and%20the%20regulator%27s%20fairness%20objectives.%20Our%20findings%20demonstrate%20that%20static%20regulatory%20frameworks%20are%20vulnerable%20to%20manipulation%20via%20technology%20expansion%2C%20necessitating%20dynamic%20market%20designs%20that%20adapt%20to%20the%20evolving%20landscape%20of%20AI%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Poisoned%2520Apple%2520Effect%253A%2520Strategic%2520Manipulation%2520of%2520Mediated%2520Markets%2520via%2520Technology%2520Expansion%2520of%2520AI%2520Agents%26entry.906535625%3DEilam%2520Shapira%2520and%2520Roi%2520Reichart%2520and%2520Moshe%2520Tennenholtz%26entry.1292438233%3DThe%2520integration%2520of%2520AI%2520agents%2520into%2520economic%2520markets%2520fundamentally%2520alters%2520the%2520landscape%2520of%2520strategic%2520interaction.%2520We%2520investigate%2520the%2520economic%2520implications%2520of%2520expanding%2520the%2520set%2520of%2520available%2520technologies%2520in%2520three%2520canonical%2520game-theoretic%2520settings%253A%2520bargaining%2520%2528resource%2520division%2529%252C%2520negotiation%2520%2528asymmetric%2520information%2520trade%2529%252C%2520and%2520persuasion%2520%2528strategic%2520information%2520transmission%2529.%2520We%2520find%2520that%2520simply%2520increasing%2520the%2520choice%2520of%2520AI%2520delegates%2520can%2520drastically%2520shift%2520equilibrium%2520payoffs%2520and%2520regulatory%2520outcomes%252C%2520often%2520creating%2520incentives%2520for%2520regulators%2520to%2520proactively%2520develop%2520and%2520release%2520technologies.%2520Conversely%252C%2520we%2520identify%2520a%2520strategic%2520phenomenon%2520termed%2520the%2520%2522Poisoned%2520Apple%2522%2520effect%253A%2520an%2520agent%2520may%2520release%2520a%2520new%2520technology%252C%2520which%2520neither%2520they%2520nor%2520their%2520opponent%2520ultimately%2520uses%252C%2520solely%2520to%2520manipulate%2520the%2520regulator%2527s%2520choice%2520of%2520market%2520design%2520in%2520their%2520favor.%2520This%2520strategic%2520release%2520improves%2520the%2520releaser%2527s%2520welfare%2520at%2520the%2520expense%2520of%2520their%2520opponent%2520and%2520the%2520regulator%2527s%2520fairness%2520objectives.%2520Our%2520findings%2520demonstrate%2520that%2520static%2520regulatory%2520frameworks%2520are%2520vulnerable%2520to%2520manipulation%2520via%2520technology%2520expansion%252C%2520necessitating%2520dynamic%2520market%2520designs%2520that%2520adapt%2520to%2520the%2520evolving%2520landscape%2520of%2520AI%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Poisoned%20Apple%20Effect%3A%20Strategic%20Manipulation%20of%20Mediated%20Markets%20via%20Technology%20Expansion%20of%20AI%20Agents&entry.906535625=Eilam%20Shapira%20and%20Roi%20Reichart%20and%20Moshe%20Tennenholtz&entry.1292438233=The%20integration%20of%20AI%20agents%20into%20economic%20markets%20fundamentally%20alters%20the%20landscape%20of%20strategic%20interaction.%20We%20investigate%20the%20economic%20implications%20of%20expanding%20the%20set%20of%20available%20technologies%20in%20three%20canonical%20game-theoretic%20settings%3A%20bargaining%20%28resource%20division%29%2C%20negotiation%20%28asymmetric%20information%20trade%29%2C%20and%20persuasion%20%28strategic%20information%20transmission%29.%20We%20find%20that%20simply%20increasing%20the%20choice%20of%20AI%20delegates%20can%20drastically%20shift%20equilibrium%20payoffs%20and%20regulatory%20outcomes%2C%20often%20creating%20incentives%20for%20regulators%20to%20proactively%20develop%20and%20release%20technologies.%20Conversely%2C%20we%20identify%20a%20strategic%20phenomenon%20termed%20the%20%22Poisoned%20Apple%22%20effect%3A%20an%20agent%20may%20release%20a%20new%20technology%2C%20which%20neither%20they%20nor%20their%20opponent%20ultimately%20uses%2C%20solely%20to%20manipulate%20the%20regulator%27s%20choice%20of%20market%20design%20in%20their%20favor.%20This%20strategic%20release%20improves%20the%20releaser%27s%20welfare%20at%20the%20expense%20of%20their%20opponent%20and%20the%20regulator%27s%20fairness%20objectives.%20Our%20findings%20demonstrate%20that%20static%20regulatory%20frameworks%20are%20vulnerable%20to%20manipulation%20via%20technology%20expansion%2C%20necessitating%20dynamic%20market%20designs%20that%20adapt%20to%20the%20evolving%20landscape%20of%20AI%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2601.11496v1&entry.124074799=Read"},
{"title": "UCB-type Algorithm for Budget-Constrained Expert Learning", "author": "Ilgam Latypov and Alexandra Suvorikova and Alexey Kroshnin and Alexander Gasnikov and Yuriy Dorn", "abstract": "In many modern applications, a system must dynamically choose between several adaptive learning algorithms that are trained online. Examples include model selection in streaming environments, switching between trading strategies in finance, and orchestrating multiple contextual bandit or reinforcement learning agents. At each round, a learner must select one predictor among $K$ adaptive experts to make a prediction, while being able to update at most $M \\le K$ of them under a fixed training budget.\n  We address this problem in the \\emph{stochastic setting} and introduce \\algname{M-LCB}, a computationally efficient UCB-style meta-algorithm that provides \\emph{anytime regret guarantees}. Its confidence intervals are built directly from realized losses, require no additional optimization, and seamlessly reflect the convergence properties of the underlying experts. If each expert achieves internal regret $\\tilde O(T^\u03b1)$, then \\algname{M-LCB} ensures overall regret bounded by $\\tilde O\\!\\Bigl(\\sqrt{\\tfrac{KT}{M}} \\;+\\; (K/M)^{1-\u03b1}\\,T^\u03b1\\Bigr)$.\n  To our knowledge, this is the first result establishing regret guarantees when multiple adaptive experts are trained simultaneously under per-round budget constraints. We illustrate the framework with two representative cases: (i) parametric models trained online with stochastic losses, and (ii) experts that are themselves multi-armed bandit algorithms. These examples highlight how \\algname{M-LCB} extends the classical bandit paradigm to the more realistic scenario of coordinating stateful, self-learning experts under limited resources.", "link": "http://arxiv.org/abs/2510.22654v2", "date": "2026-01-16", "relevancy": 1.8506, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4737}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4609}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UCB-type%20Algorithm%20for%20Budget-Constrained%20Expert%20Learning&body=Title%3A%20UCB-type%20Algorithm%20for%20Budget-Constrained%20Expert%20Learning%0AAuthor%3A%20Ilgam%20Latypov%20and%20Alexandra%20Suvorikova%20and%20Alexey%20Kroshnin%20and%20Alexander%20Gasnikov%20and%20Yuriy%20Dorn%0AAbstract%3A%20In%20many%20modern%20applications%2C%20a%20system%20must%20dynamically%20choose%20between%20several%20adaptive%20learning%20algorithms%20that%20are%20trained%20online.%20Examples%20include%20model%20selection%20in%20streaming%20environments%2C%20switching%20between%20trading%20strategies%20in%20finance%2C%20and%20orchestrating%20multiple%20contextual%20bandit%20or%20reinforcement%20learning%20agents.%20At%20each%20round%2C%20a%20learner%20must%20select%20one%20predictor%20among%20%24K%24%20adaptive%20experts%20to%20make%20a%20prediction%2C%20while%20being%20able%20to%20update%20at%20most%20%24M%20%5Cle%20K%24%20of%20them%20under%20a%20fixed%20training%20budget.%0A%20%20We%20address%20this%20problem%20in%20the%20%5Cemph%7Bstochastic%20setting%7D%20and%20introduce%20%5Calgname%7BM-LCB%7D%2C%20a%20computationally%20efficient%20UCB-style%20meta-algorithm%20that%20provides%20%5Cemph%7Banytime%20regret%20guarantees%7D.%20Its%20confidence%20intervals%20are%20built%20directly%20from%20realized%20losses%2C%20require%20no%20additional%20optimization%2C%20and%20seamlessly%20reflect%20the%20convergence%20properties%20of%20the%20underlying%20experts.%20If%20each%20expert%20achieves%20internal%20regret%20%24%5Ctilde%20O%28T%5E%CE%B1%29%24%2C%20then%20%5Calgname%7BM-LCB%7D%20ensures%20overall%20regret%20bounded%20by%20%24%5Ctilde%20O%5C%21%5CBigl%28%5Csqrt%7B%5Ctfrac%7BKT%7D%7BM%7D%7D%20%5C%3B%2B%5C%3B%20%28K/M%29%5E%7B1-%CE%B1%7D%5C%2CT%5E%CE%B1%5CBigr%29%24.%0A%20%20To%20our%20knowledge%2C%20this%20is%20the%20first%20result%20establishing%20regret%20guarantees%20when%20multiple%20adaptive%20experts%20are%20trained%20simultaneously%20under%20per-round%20budget%20constraints.%20We%20illustrate%20the%20framework%20with%20two%20representative%20cases%3A%20%28i%29%20parametric%20models%20trained%20online%20with%20stochastic%20losses%2C%20and%20%28ii%29%20experts%20that%20are%20themselves%20multi-armed%20bandit%20algorithms.%20These%20examples%20highlight%20how%20%5Calgname%7BM-LCB%7D%20extends%20the%20classical%20bandit%20paradigm%20to%20the%20more%20realistic%20scenario%20of%20coordinating%20stateful%2C%20self-learning%20experts%20under%20limited%20resources.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUCB-type%2520Algorithm%2520for%2520Budget-Constrained%2520Expert%2520Learning%26entry.906535625%3DIlgam%2520Latypov%2520and%2520Alexandra%2520Suvorikova%2520and%2520Alexey%2520Kroshnin%2520and%2520Alexander%2520Gasnikov%2520and%2520Yuriy%2520Dorn%26entry.1292438233%3DIn%2520many%2520modern%2520applications%252C%2520a%2520system%2520must%2520dynamically%2520choose%2520between%2520several%2520adaptive%2520learning%2520algorithms%2520that%2520are%2520trained%2520online.%2520Examples%2520include%2520model%2520selection%2520in%2520streaming%2520environments%252C%2520switching%2520between%2520trading%2520strategies%2520in%2520finance%252C%2520and%2520orchestrating%2520multiple%2520contextual%2520bandit%2520or%2520reinforcement%2520learning%2520agents.%2520At%2520each%2520round%252C%2520a%2520learner%2520must%2520select%2520one%2520predictor%2520among%2520%2524K%2524%2520adaptive%2520experts%2520to%2520make%2520a%2520prediction%252C%2520while%2520being%2520able%2520to%2520update%2520at%2520most%2520%2524M%2520%255Cle%2520K%2524%2520of%2520them%2520under%2520a%2520fixed%2520training%2520budget.%250A%2520%2520We%2520address%2520this%2520problem%2520in%2520the%2520%255Cemph%257Bstochastic%2520setting%257D%2520and%2520introduce%2520%255Calgname%257BM-LCB%257D%252C%2520a%2520computationally%2520efficient%2520UCB-style%2520meta-algorithm%2520that%2520provides%2520%255Cemph%257Banytime%2520regret%2520guarantees%257D.%2520Its%2520confidence%2520intervals%2520are%2520built%2520directly%2520from%2520realized%2520losses%252C%2520require%2520no%2520additional%2520optimization%252C%2520and%2520seamlessly%2520reflect%2520the%2520convergence%2520properties%2520of%2520the%2520underlying%2520experts.%2520If%2520each%2520expert%2520achieves%2520internal%2520regret%2520%2524%255Ctilde%2520O%2528T%255E%25CE%25B1%2529%2524%252C%2520then%2520%255Calgname%257BM-LCB%257D%2520ensures%2520overall%2520regret%2520bounded%2520by%2520%2524%255Ctilde%2520O%255C%2521%255CBigl%2528%255Csqrt%257B%255Ctfrac%257BKT%257D%257BM%257D%257D%2520%255C%253B%252B%255C%253B%2520%2528K/M%2529%255E%257B1-%25CE%25B1%257D%255C%252CT%255E%25CE%25B1%255CBigr%2529%2524.%250A%2520%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520result%2520establishing%2520regret%2520guarantees%2520when%2520multiple%2520adaptive%2520experts%2520are%2520trained%2520simultaneously%2520under%2520per-round%2520budget%2520constraints.%2520We%2520illustrate%2520the%2520framework%2520with%2520two%2520representative%2520cases%253A%2520%2528i%2529%2520parametric%2520models%2520trained%2520online%2520with%2520stochastic%2520losses%252C%2520and%2520%2528ii%2529%2520experts%2520that%2520are%2520themselves%2520multi-armed%2520bandit%2520algorithms.%2520These%2520examples%2520highlight%2520how%2520%255Calgname%257BM-LCB%257D%2520extends%2520the%2520classical%2520bandit%2520paradigm%2520to%2520the%2520more%2520realistic%2520scenario%2520of%2520coordinating%2520stateful%252C%2520self-learning%2520experts%2520under%2520limited%2520resources.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UCB-type%20Algorithm%20for%20Budget-Constrained%20Expert%20Learning&entry.906535625=Ilgam%20Latypov%20and%20Alexandra%20Suvorikova%20and%20Alexey%20Kroshnin%20and%20Alexander%20Gasnikov%20and%20Yuriy%20Dorn&entry.1292438233=In%20many%20modern%20applications%2C%20a%20system%20must%20dynamically%20choose%20between%20several%20adaptive%20learning%20algorithms%20that%20are%20trained%20online.%20Examples%20include%20model%20selection%20in%20streaming%20environments%2C%20switching%20between%20trading%20strategies%20in%20finance%2C%20and%20orchestrating%20multiple%20contextual%20bandit%20or%20reinforcement%20learning%20agents.%20At%20each%20round%2C%20a%20learner%20must%20select%20one%20predictor%20among%20%24K%24%20adaptive%20experts%20to%20make%20a%20prediction%2C%20while%20being%20able%20to%20update%20at%20most%20%24M%20%5Cle%20K%24%20of%20them%20under%20a%20fixed%20training%20budget.%0A%20%20We%20address%20this%20problem%20in%20the%20%5Cemph%7Bstochastic%20setting%7D%20and%20introduce%20%5Calgname%7BM-LCB%7D%2C%20a%20computationally%20efficient%20UCB-style%20meta-algorithm%20that%20provides%20%5Cemph%7Banytime%20regret%20guarantees%7D.%20Its%20confidence%20intervals%20are%20built%20directly%20from%20realized%20losses%2C%20require%20no%20additional%20optimization%2C%20and%20seamlessly%20reflect%20the%20convergence%20properties%20of%20the%20underlying%20experts.%20If%20each%20expert%20achieves%20internal%20regret%20%24%5Ctilde%20O%28T%5E%CE%B1%29%24%2C%20then%20%5Calgname%7BM-LCB%7D%20ensures%20overall%20regret%20bounded%20by%20%24%5Ctilde%20O%5C%21%5CBigl%28%5Csqrt%7B%5Ctfrac%7BKT%7D%7BM%7D%7D%20%5C%3B%2B%5C%3B%20%28K/M%29%5E%7B1-%CE%B1%7D%5C%2CT%5E%CE%B1%5CBigr%29%24.%0A%20%20To%20our%20knowledge%2C%20this%20is%20the%20first%20result%20establishing%20regret%20guarantees%20when%20multiple%20adaptive%20experts%20are%20trained%20simultaneously%20under%20per-round%20budget%20constraints.%20We%20illustrate%20the%20framework%20with%20two%20representative%20cases%3A%20%28i%29%20parametric%20models%20trained%20online%20with%20stochastic%20losses%2C%20and%20%28ii%29%20experts%20that%20are%20themselves%20multi-armed%20bandit%20algorithms.%20These%20examples%20highlight%20how%20%5Calgname%7BM-LCB%7D%20extends%20the%20classical%20bandit%20paradigm%20to%20the%20more%20realistic%20scenario%20of%20coordinating%20stateful%2C%20self-learning%20experts%20under%20limited%20resources.&entry.1838667208=http%3A//arxiv.org/abs/2510.22654v2&entry.124074799=Read"},
{"title": "Model-free policy gradient for discrete-time mean-field control", "author": "Matthieu Meunier and Huy\u00ean Pham and Christoph Reisinger", "abstract": "We study model-free policy learning for discrete-time mean-field control (MFC) problems with finite state space and compact action space. In contrast to the extensive literature on value-based methods for MFC, policy-based approaches remain largely unexplored due to the intrinsic dependence of transition kernels and rewards on the evolving population state distribution, which prevents the direct use of likelihood-ratio estimators of policy gradients from classical single-agent reinforcement learning. We introduce a novel perturbation scheme on the state-distribution flow and prove that the gradient of the resulting perturbed value function converges to the true policy gradient as the perturbation magnitude vanishes. This construction yields a fully model-free estimator based solely on simulated trajectories and an auxiliary estimate of the sensitivity of the state distribution. Building on this framework, we develop MF-REINFORCE, a model-free policy gradient algorithm for MFC, and establish explicit quantitative bounds on its bias and mean-squared error. Numerical experiments on representative mean-field control tasks demonstrate the effectiveness of the proposed approach.", "link": "http://arxiv.org/abs/2601.11217v1", "date": "2026-01-16", "relevancy": 1.8778, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4945}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4788}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-free%20policy%20gradient%20for%20discrete-time%20mean-field%20control&body=Title%3A%20Model-free%20policy%20gradient%20for%20discrete-time%20mean-field%20control%0AAuthor%3A%20Matthieu%20Meunier%20and%20Huy%C3%AAn%20Pham%20and%20Christoph%20Reisinger%0AAbstract%3A%20We%20study%20model-free%20policy%20learning%20for%20discrete-time%20mean-field%20control%20%28MFC%29%20problems%20with%20finite%20state%20space%20and%20compact%20action%20space.%20In%20contrast%20to%20the%20extensive%20literature%20on%20value-based%20methods%20for%20MFC%2C%20policy-based%20approaches%20remain%20largely%20unexplored%20due%20to%20the%20intrinsic%20dependence%20of%20transition%20kernels%20and%20rewards%20on%20the%20evolving%20population%20state%20distribution%2C%20which%20prevents%20the%20direct%20use%20of%20likelihood-ratio%20estimators%20of%20policy%20gradients%20from%20classical%20single-agent%20reinforcement%20learning.%20We%20introduce%20a%20novel%20perturbation%20scheme%20on%20the%20state-distribution%20flow%20and%20prove%20that%20the%20gradient%20of%20the%20resulting%20perturbed%20value%20function%20converges%20to%20the%20true%20policy%20gradient%20as%20the%20perturbation%20magnitude%20vanishes.%20This%20construction%20yields%20a%20fully%20model-free%20estimator%20based%20solely%20on%20simulated%20trajectories%20and%20an%20auxiliary%20estimate%20of%20the%20sensitivity%20of%20the%20state%20distribution.%20Building%20on%20this%20framework%2C%20we%20develop%20MF-REINFORCE%2C%20a%20model-free%20policy%20gradient%20algorithm%20for%20MFC%2C%20and%20establish%20explicit%20quantitative%20bounds%20on%20its%20bias%20and%20mean-squared%20error.%20Numerical%20experiments%20on%20representative%20mean-field%20control%20tasks%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-free%2520policy%2520gradient%2520for%2520discrete-time%2520mean-field%2520control%26entry.906535625%3DMatthieu%2520Meunier%2520and%2520Huy%25C3%25AAn%2520Pham%2520and%2520Christoph%2520Reisinger%26entry.1292438233%3DWe%2520study%2520model-free%2520policy%2520learning%2520for%2520discrete-time%2520mean-field%2520control%2520%2528MFC%2529%2520problems%2520with%2520finite%2520state%2520space%2520and%2520compact%2520action%2520space.%2520In%2520contrast%2520to%2520the%2520extensive%2520literature%2520on%2520value-based%2520methods%2520for%2520MFC%252C%2520policy-based%2520approaches%2520remain%2520largely%2520unexplored%2520due%2520to%2520the%2520intrinsic%2520dependence%2520of%2520transition%2520kernels%2520and%2520rewards%2520on%2520the%2520evolving%2520population%2520state%2520distribution%252C%2520which%2520prevents%2520the%2520direct%2520use%2520of%2520likelihood-ratio%2520estimators%2520of%2520policy%2520gradients%2520from%2520classical%2520single-agent%2520reinforcement%2520learning.%2520We%2520introduce%2520a%2520novel%2520perturbation%2520scheme%2520on%2520the%2520state-distribution%2520flow%2520and%2520prove%2520that%2520the%2520gradient%2520of%2520the%2520resulting%2520perturbed%2520value%2520function%2520converges%2520to%2520the%2520true%2520policy%2520gradient%2520as%2520the%2520perturbation%2520magnitude%2520vanishes.%2520This%2520construction%2520yields%2520a%2520fully%2520model-free%2520estimator%2520based%2520solely%2520on%2520simulated%2520trajectories%2520and%2520an%2520auxiliary%2520estimate%2520of%2520the%2520sensitivity%2520of%2520the%2520state%2520distribution.%2520Building%2520on%2520this%2520framework%252C%2520we%2520develop%2520MF-REINFORCE%252C%2520a%2520model-free%2520policy%2520gradient%2520algorithm%2520for%2520MFC%252C%2520and%2520establish%2520explicit%2520quantitative%2520bounds%2520on%2520its%2520bias%2520and%2520mean-squared%2520error.%2520Numerical%2520experiments%2520on%2520representative%2520mean-field%2520control%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-free%20policy%20gradient%20for%20discrete-time%20mean-field%20control&entry.906535625=Matthieu%20Meunier%20and%20Huy%C3%AAn%20Pham%20and%20Christoph%20Reisinger&entry.1292438233=We%20study%20model-free%20policy%20learning%20for%20discrete-time%20mean-field%20control%20%28MFC%29%20problems%20with%20finite%20state%20space%20and%20compact%20action%20space.%20In%20contrast%20to%20the%20extensive%20literature%20on%20value-based%20methods%20for%20MFC%2C%20policy-based%20approaches%20remain%20largely%20unexplored%20due%20to%20the%20intrinsic%20dependence%20of%20transition%20kernels%20and%20rewards%20on%20the%20evolving%20population%20state%20distribution%2C%20which%20prevents%20the%20direct%20use%20of%20likelihood-ratio%20estimators%20of%20policy%20gradients%20from%20classical%20single-agent%20reinforcement%20learning.%20We%20introduce%20a%20novel%20perturbation%20scheme%20on%20the%20state-distribution%20flow%20and%20prove%20that%20the%20gradient%20of%20the%20resulting%20perturbed%20value%20function%20converges%20to%20the%20true%20policy%20gradient%20as%20the%20perturbation%20magnitude%20vanishes.%20This%20construction%20yields%20a%20fully%20model-free%20estimator%20based%20solely%20on%20simulated%20trajectories%20and%20an%20auxiliary%20estimate%20of%20the%20sensitivity%20of%20the%20state%20distribution.%20Building%20on%20this%20framework%2C%20we%20develop%20MF-REINFORCE%2C%20a%20model-free%20policy%20gradient%20algorithm%20for%20MFC%2C%20and%20establish%20explicit%20quantitative%20bounds%20on%20its%20bias%20and%20mean-squared%20error.%20Numerical%20experiments%20on%20representative%20mean-field%20control%20tasks%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2601.11217v1&entry.124074799=Read"},
{"title": "An Efficient Long-Context Ranking Architecture With Calibrated LLM Distillation: Application to Person-Job Fit", "author": "Warren Jouanneau and Emma Jouffroy and Marc Palyart", "abstract": "Finding the most relevant person for a job proposal in real time is challenging, especially when resumes are long, structured, and multilingual. In this paper, we propose a re-ranking model based on a new generation of late cross-attention architecture, that decomposes both resumes and project briefs to efficiently handle long-context inputs with minimal computational overhead. To mitigate historical data biases, we use a generative large language model (LLM) as a teacher, generating fine-grained, semantically grounded supervision. This signal is distilled into our student model via an enriched distillation loss function. The resulting model produces skill-fit scores that enable consistent and interpretable person-job matching. Experiments on relevance, ranking, and calibration metrics demonstrate that our approach outperforms state-of-the-art baselines.", "link": "http://arxiv.org/abs/2601.10321v2", "date": "2026-01-16", "relevancy": 1.8788, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4723}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4704}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Long-Context%20Ranking%20Architecture%20With%20Calibrated%20LLM%20Distillation%3A%20Application%20to%20Person-Job%20Fit&body=Title%3A%20An%20Efficient%20Long-Context%20Ranking%20Architecture%20With%20Calibrated%20LLM%20Distillation%3A%20Application%20to%20Person-Job%20Fit%0AAuthor%3A%20Warren%20Jouanneau%20and%20Emma%20Jouffroy%20and%20Marc%20Palyart%0AAbstract%3A%20Finding%20the%20most%20relevant%20person%20for%20a%20job%20proposal%20in%20real%20time%20is%20challenging%2C%20especially%20when%20resumes%20are%20long%2C%20structured%2C%20and%20multilingual.%20In%20this%20paper%2C%20we%20propose%20a%20re-ranking%20model%20based%20on%20a%20new%20generation%20of%20late%20cross-attention%20architecture%2C%20that%20decomposes%20both%20resumes%20and%20project%20briefs%20to%20efficiently%20handle%20long-context%20inputs%20with%20minimal%20computational%20overhead.%20To%20mitigate%20historical%20data%20biases%2C%20we%20use%20a%20generative%20large%20language%20model%20%28LLM%29%20as%20a%20teacher%2C%20generating%20fine-grained%2C%20semantically%20grounded%20supervision.%20This%20signal%20is%20distilled%20into%20our%20student%20model%20via%20an%20enriched%20distillation%20loss%20function.%20The%20resulting%20model%20produces%20skill-fit%20scores%20that%20enable%20consistent%20and%20interpretable%20person-job%20matching.%20Experiments%20on%20relevance%2C%20ranking%2C%20and%20calibration%20metrics%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10321v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Long-Context%2520Ranking%2520Architecture%2520With%2520Calibrated%2520LLM%2520Distillation%253A%2520Application%2520to%2520Person-Job%2520Fit%26entry.906535625%3DWarren%2520Jouanneau%2520and%2520Emma%2520Jouffroy%2520and%2520Marc%2520Palyart%26entry.1292438233%3DFinding%2520the%2520most%2520relevant%2520person%2520for%2520a%2520job%2520proposal%2520in%2520real%2520time%2520is%2520challenging%252C%2520especially%2520when%2520resumes%2520are%2520long%252C%2520structured%252C%2520and%2520multilingual.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520re-ranking%2520model%2520based%2520on%2520a%2520new%2520generation%2520of%2520late%2520cross-attention%2520architecture%252C%2520that%2520decomposes%2520both%2520resumes%2520and%2520project%2520briefs%2520to%2520efficiently%2520handle%2520long-context%2520inputs%2520with%2520minimal%2520computational%2520overhead.%2520To%2520mitigate%2520historical%2520data%2520biases%252C%2520we%2520use%2520a%2520generative%2520large%2520language%2520model%2520%2528LLM%2529%2520as%2520a%2520teacher%252C%2520generating%2520fine-grained%252C%2520semantically%2520grounded%2520supervision.%2520This%2520signal%2520is%2520distilled%2520into%2520our%2520student%2520model%2520via%2520an%2520enriched%2520distillation%2520loss%2520function.%2520The%2520resulting%2520model%2520produces%2520skill-fit%2520scores%2520that%2520enable%2520consistent%2520and%2520interpretable%2520person-job%2520matching.%2520Experiments%2520on%2520relevance%252C%2520ranking%252C%2520and%2520calibration%2520metrics%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520state-of-the-art%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10321v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Long-Context%20Ranking%20Architecture%20With%20Calibrated%20LLM%20Distillation%3A%20Application%20to%20Person-Job%20Fit&entry.906535625=Warren%20Jouanneau%20and%20Emma%20Jouffroy%20and%20Marc%20Palyart&entry.1292438233=Finding%20the%20most%20relevant%20person%20for%20a%20job%20proposal%20in%20real%20time%20is%20challenging%2C%20especially%20when%20resumes%20are%20long%2C%20structured%2C%20and%20multilingual.%20In%20this%20paper%2C%20we%20propose%20a%20re-ranking%20model%20based%20on%20a%20new%20generation%20of%20late%20cross-attention%20architecture%2C%20that%20decomposes%20both%20resumes%20and%20project%20briefs%20to%20efficiently%20handle%20long-context%20inputs%20with%20minimal%20computational%20overhead.%20To%20mitigate%20historical%20data%20biases%2C%20we%20use%20a%20generative%20large%20language%20model%20%28LLM%29%20as%20a%20teacher%2C%20generating%20fine-grained%2C%20semantically%20grounded%20supervision.%20This%20signal%20is%20distilled%20into%20our%20student%20model%20via%20an%20enriched%20distillation%20loss%20function.%20The%20resulting%20model%20produces%20skill-fit%20scores%20that%20enable%20consistent%20and%20interpretable%20person-job%20matching.%20Experiments%20on%20relevance%2C%20ranking%2C%20and%20calibration%20metrics%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.10321v2&entry.124074799=Read"},
{"title": "IMS: Intelligent Hardware Monitoring System for Secure SoCs", "author": "Wadid Foudhaili and Aykut Rencber and Anouar Nechi and Rainer Buchty and Mladen Berekovic and Andres Gomez and Saleh Mulhem", "abstract": "In the modern Systems-on-Chip (SoC), the Advanced eXtensible Interface (AXI) protocol exhibits security vulnerabilities, enabling partial or complete denial-of-service (DoS) through protocol-violation attacks. The recent countermeasures lack a dedicated real-time protocol semantic analysis and evade protocol compliance checks. This paper tackles this AXI vulnerability issue and presents an intelligent hardware monitoring system (IMS) for real-time detection of AXI protocol violations. IMS is a hardware module leveraging neural networks to achieve high detection accuracy. For model training, we perform DoS attacks through header-field manipulation and systematic malicious operations, while recording AXI transactions to build a training dataset. We then deploy a quantization-optimized neural network, achieving 98.7% detection accuracy with <=3% latency overhead, and throughput of >2.5 million inferences/s. We subsequently integrate this IMS into a RISC-V SoC as a memory-mapped IP core to monitor its AXI bus. For demonstration and initial assessment for later ASIC integration, we implemented this IMS on an AMD Zynq UltraScale+ MPSoC ZCU104 board, showing an overall small hardware footprint (9.04% look-up-tables (LUTs), 0.23% DSP slices, and 0.70% flip-flops) and negligible impact on the overall design's achievable frequency. This demonstrates the feasibility of lightweight, security monitoring for resource-constrained edge environments.", "link": "http://arxiv.org/abs/2601.11447v1", "date": "2026-01-16", "relevancy": 1.1467, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4515}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3783}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMS%3A%20Intelligent%20Hardware%20Monitoring%20System%20for%20Secure%20SoCs&body=Title%3A%20IMS%3A%20Intelligent%20Hardware%20Monitoring%20System%20for%20Secure%20SoCs%0AAuthor%3A%20Wadid%20Foudhaili%20and%20Aykut%20Rencber%20and%20Anouar%20Nechi%20and%20Rainer%20Buchty%20and%20Mladen%20Berekovic%20and%20Andres%20Gomez%20and%20Saleh%20Mulhem%0AAbstract%3A%20In%20the%20modern%20Systems-on-Chip%20%28SoC%29%2C%20the%20Advanced%20eXtensible%20Interface%20%28AXI%29%20protocol%20exhibits%20security%20vulnerabilities%2C%20enabling%20partial%20or%20complete%20denial-of-service%20%28DoS%29%20through%20protocol-violation%20attacks.%20The%20recent%20countermeasures%20lack%20a%20dedicated%20real-time%20protocol%20semantic%20analysis%20and%20evade%20protocol%20compliance%20checks.%20This%20paper%20tackles%20this%20AXI%20vulnerability%20issue%20and%20presents%20an%20intelligent%20hardware%20monitoring%20system%20%28IMS%29%20for%20real-time%20detection%20of%20AXI%20protocol%20violations.%20IMS%20is%20a%20hardware%20module%20leveraging%20neural%20networks%20to%20achieve%20high%20detection%20accuracy.%20For%20model%20training%2C%20we%20perform%20DoS%20attacks%20through%20header-field%20manipulation%20and%20systematic%20malicious%20operations%2C%20while%20recording%20AXI%20transactions%20to%20build%20a%20training%20dataset.%20We%20then%20deploy%20a%20quantization-optimized%20neural%20network%2C%20achieving%2098.7%25%20detection%20accuracy%20with%20%3C%3D3%25%20latency%20overhead%2C%20and%20throughput%20of%20%3E2.5%20million%20inferences/s.%20We%20subsequently%20integrate%20this%20IMS%20into%20a%20RISC-V%20SoC%20as%20a%20memory-mapped%20IP%20core%20to%20monitor%20its%20AXI%20bus.%20For%20demonstration%20and%20initial%20assessment%20for%20later%20ASIC%20integration%2C%20we%20implemented%20this%20IMS%20on%20an%20AMD%20Zynq%20UltraScale%2B%20MPSoC%20ZCU104%20board%2C%20showing%20an%20overall%20small%20hardware%20footprint%20%289.04%25%20look-up-tables%20%28LUTs%29%2C%200.23%25%20DSP%20slices%2C%20and%200.70%25%20flip-flops%29%20and%20negligible%20impact%20on%20the%20overall%20design%27s%20achievable%20frequency.%20This%20demonstrates%20the%20feasibility%20of%20lightweight%2C%20security%20monitoring%20for%20resource-constrained%20edge%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMS%253A%2520Intelligent%2520Hardware%2520Monitoring%2520System%2520for%2520Secure%2520SoCs%26entry.906535625%3DWadid%2520Foudhaili%2520and%2520Aykut%2520Rencber%2520and%2520Anouar%2520Nechi%2520and%2520Rainer%2520Buchty%2520and%2520Mladen%2520Berekovic%2520and%2520Andres%2520Gomez%2520and%2520Saleh%2520Mulhem%26entry.1292438233%3DIn%2520the%2520modern%2520Systems-on-Chip%2520%2528SoC%2529%252C%2520the%2520Advanced%2520eXtensible%2520Interface%2520%2528AXI%2529%2520protocol%2520exhibits%2520security%2520vulnerabilities%252C%2520enabling%2520partial%2520or%2520complete%2520denial-of-service%2520%2528DoS%2529%2520through%2520protocol-violation%2520attacks.%2520The%2520recent%2520countermeasures%2520lack%2520a%2520dedicated%2520real-time%2520protocol%2520semantic%2520analysis%2520and%2520evade%2520protocol%2520compliance%2520checks.%2520This%2520paper%2520tackles%2520this%2520AXI%2520vulnerability%2520issue%2520and%2520presents%2520an%2520intelligent%2520hardware%2520monitoring%2520system%2520%2528IMS%2529%2520for%2520real-time%2520detection%2520of%2520AXI%2520protocol%2520violations.%2520IMS%2520is%2520a%2520hardware%2520module%2520leveraging%2520neural%2520networks%2520to%2520achieve%2520high%2520detection%2520accuracy.%2520For%2520model%2520training%252C%2520we%2520perform%2520DoS%2520attacks%2520through%2520header-field%2520manipulation%2520and%2520systematic%2520malicious%2520operations%252C%2520while%2520recording%2520AXI%2520transactions%2520to%2520build%2520a%2520training%2520dataset.%2520We%2520then%2520deploy%2520a%2520quantization-optimized%2520neural%2520network%252C%2520achieving%252098.7%2525%2520detection%2520accuracy%2520with%2520%253C%253D3%2525%2520latency%2520overhead%252C%2520and%2520throughput%2520of%2520%253E2.5%2520million%2520inferences/s.%2520We%2520subsequently%2520integrate%2520this%2520IMS%2520into%2520a%2520RISC-V%2520SoC%2520as%2520a%2520memory-mapped%2520IP%2520core%2520to%2520monitor%2520its%2520AXI%2520bus.%2520For%2520demonstration%2520and%2520initial%2520assessment%2520for%2520later%2520ASIC%2520integration%252C%2520we%2520implemented%2520this%2520IMS%2520on%2520an%2520AMD%2520Zynq%2520UltraScale%252B%2520MPSoC%2520ZCU104%2520board%252C%2520showing%2520an%2520overall%2520small%2520hardware%2520footprint%2520%25289.04%2525%2520look-up-tables%2520%2528LUTs%2529%252C%25200.23%2525%2520DSP%2520slices%252C%2520and%25200.70%2525%2520flip-flops%2529%2520and%2520negligible%2520impact%2520on%2520the%2520overall%2520design%2527s%2520achievable%2520frequency.%2520This%2520demonstrates%2520the%2520feasibility%2520of%2520lightweight%252C%2520security%2520monitoring%2520for%2520resource-constrained%2520edge%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMS%3A%20Intelligent%20Hardware%20Monitoring%20System%20for%20Secure%20SoCs&entry.906535625=Wadid%20Foudhaili%20and%20Aykut%20Rencber%20and%20Anouar%20Nechi%20and%20Rainer%20Buchty%20and%20Mladen%20Berekovic%20and%20Andres%20Gomez%20and%20Saleh%20Mulhem&entry.1292438233=In%20the%20modern%20Systems-on-Chip%20%28SoC%29%2C%20the%20Advanced%20eXtensible%20Interface%20%28AXI%29%20protocol%20exhibits%20security%20vulnerabilities%2C%20enabling%20partial%20or%20complete%20denial-of-service%20%28DoS%29%20through%20protocol-violation%20attacks.%20The%20recent%20countermeasures%20lack%20a%20dedicated%20real-time%20protocol%20semantic%20analysis%20and%20evade%20protocol%20compliance%20checks.%20This%20paper%20tackles%20this%20AXI%20vulnerability%20issue%20and%20presents%20an%20intelligent%20hardware%20monitoring%20system%20%28IMS%29%20for%20real-time%20detection%20of%20AXI%20protocol%20violations.%20IMS%20is%20a%20hardware%20module%20leveraging%20neural%20networks%20to%20achieve%20high%20detection%20accuracy.%20For%20model%20training%2C%20we%20perform%20DoS%20attacks%20through%20header-field%20manipulation%20and%20systematic%20malicious%20operations%2C%20while%20recording%20AXI%20transactions%20to%20build%20a%20training%20dataset.%20We%20then%20deploy%20a%20quantization-optimized%20neural%20network%2C%20achieving%2098.7%25%20detection%20accuracy%20with%20%3C%3D3%25%20latency%20overhead%2C%20and%20throughput%20of%20%3E2.5%20million%20inferences/s.%20We%20subsequently%20integrate%20this%20IMS%20into%20a%20RISC-V%20SoC%20as%20a%20memory-mapped%20IP%20core%20to%20monitor%20its%20AXI%20bus.%20For%20demonstration%20and%20initial%20assessment%20for%20later%20ASIC%20integration%2C%20we%20implemented%20this%20IMS%20on%20an%20AMD%20Zynq%20UltraScale%2B%20MPSoC%20ZCU104%20board%2C%20showing%20an%20overall%20small%20hardware%20footprint%20%289.04%25%20look-up-tables%20%28LUTs%29%2C%200.23%25%20DSP%20slices%2C%20and%200.70%25%20flip-flops%29%20and%20negligible%20impact%20on%20the%20overall%20design%27s%20achievable%20frequency.%20This%20demonstrates%20the%20feasibility%20of%20lightweight%2C%20security%20monitoring%20for%20resource-constrained%20edge%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.11447v1&entry.124074799=Read"},
{"title": "PubMed-OCR: PMC Open Access OCR Annotations", "author": "Hunter Heidenreich and Yosheb Getachew and Olivia Dinica and Ben Elliott", "abstract": "PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions.", "link": "http://arxiv.org/abs/2601.11425v1", "date": "2026-01-16", "relevancy": 1.693, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4324}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4246}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PubMed-OCR%3A%20PMC%20Open%20Access%20OCR%20Annotations&body=Title%3A%20PubMed-OCR%3A%20PMC%20Open%20Access%20OCR%20Annotations%0AAuthor%3A%20Hunter%20Heidenreich%20and%20Yosheb%20Getachew%20and%20Olivia%20Dinica%20and%20Ben%20Elliott%0AAbstract%3A%20PubMed-OCR%20is%20an%20OCR-centric%20corpus%20of%20scientific%20articles%20derived%20from%20PubMed%20Central%20Open%20Access%20PDFs.%20Each%20page%20image%20is%20annotated%20with%20Google%20Cloud%20Vision%20and%20released%20in%20a%20compact%20JSON%20schema%20with%20word-%2C%20line-%2C%20and%20paragraph-level%20bounding%20boxes.%20The%20corpus%20spans%20209.5K%20articles%20%281.5M%20pages%3B%20~1.3B%20words%29%20and%20supports%20layout-aware%20modeling%2C%20coordinate-grounded%20QA%2C%20and%20evaluation%20of%20OCR-dependent%20pipelines.%20We%20analyze%20corpus%20characteristics%20%28e.g.%2C%20journal%20coverage%20and%20detected%20layout%20features%29%20and%20discuss%20limitations%2C%20including%20reliance%20on%20a%20single%20OCR%20engine%20and%20heuristic%20line%20reconstruction.%20We%20release%20the%20data%20and%20schema%20to%20facilitate%20downstream%20research%20and%20invite%20extensions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPubMed-OCR%253A%2520PMC%2520Open%2520Access%2520OCR%2520Annotations%26entry.906535625%3DHunter%2520Heidenreich%2520and%2520Yosheb%2520Getachew%2520and%2520Olivia%2520Dinica%2520and%2520Ben%2520Elliott%26entry.1292438233%3DPubMed-OCR%2520is%2520an%2520OCR-centric%2520corpus%2520of%2520scientific%2520articles%2520derived%2520from%2520PubMed%2520Central%2520Open%2520Access%2520PDFs.%2520Each%2520page%2520image%2520is%2520annotated%2520with%2520Google%2520Cloud%2520Vision%2520and%2520released%2520in%2520a%2520compact%2520JSON%2520schema%2520with%2520word-%252C%2520line-%252C%2520and%2520paragraph-level%2520bounding%2520boxes.%2520The%2520corpus%2520spans%2520209.5K%2520articles%2520%25281.5M%2520pages%253B%2520~1.3B%2520words%2529%2520and%2520supports%2520layout-aware%2520modeling%252C%2520coordinate-grounded%2520QA%252C%2520and%2520evaluation%2520of%2520OCR-dependent%2520pipelines.%2520We%2520analyze%2520corpus%2520characteristics%2520%2528e.g.%252C%2520journal%2520coverage%2520and%2520detected%2520layout%2520features%2529%2520and%2520discuss%2520limitations%252C%2520including%2520reliance%2520on%2520a%2520single%2520OCR%2520engine%2520and%2520heuristic%2520line%2520reconstruction.%2520We%2520release%2520the%2520data%2520and%2520schema%2520to%2520facilitate%2520downstream%2520research%2520and%2520invite%2520extensions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PubMed-OCR%3A%20PMC%20Open%20Access%20OCR%20Annotations&entry.906535625=Hunter%20Heidenreich%20and%20Yosheb%20Getachew%20and%20Olivia%20Dinica%20and%20Ben%20Elliott&entry.1292438233=PubMed-OCR%20is%20an%20OCR-centric%20corpus%20of%20scientific%20articles%20derived%20from%20PubMed%20Central%20Open%20Access%20PDFs.%20Each%20page%20image%20is%20annotated%20with%20Google%20Cloud%20Vision%20and%20released%20in%20a%20compact%20JSON%20schema%20with%20word-%2C%20line-%2C%20and%20paragraph-level%20bounding%20boxes.%20The%20corpus%20spans%20209.5K%20articles%20%281.5M%20pages%3B%20~1.3B%20words%29%20and%20supports%20layout-aware%20modeling%2C%20coordinate-grounded%20QA%2C%20and%20evaluation%20of%20OCR-dependent%20pipelines.%20We%20analyze%20corpus%20characteristics%20%28e.g.%2C%20journal%20coverage%20and%20detected%20layout%20features%29%20and%20discuss%20limitations%2C%20including%20reliance%20on%20a%20single%20OCR%20engine%20and%20heuristic%20line%20reconstruction.%20We%20release%20the%20data%20and%20schema%20to%20facilitate%20downstream%20research%20and%20invite%20extensions.&entry.1838667208=http%3A//arxiv.org/abs/2601.11425v1&entry.124074799=Read"},
{"title": "SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients", "author": "Zhikang Shen and Jianrong Lu and Haiyuan Wan and Jianhai Chen", "abstract": "Federated learning (FL) for large language models (LLMs) has attracted increasing attention as a way to enable privacy-preserving adaptation over distributed data. Parameter-efficient methods such as LoRA are widely adopted to reduce communication and memory costs. Despite these advances, practical FL deployments often exhibit rank heterogeneity, since different clients may use different low-rank configurations. This makes direct aggregation of LoRA updates biased and unstable. Existing solutions typically enforce unified ranks or align heterogeneous updates into a shared subspace, which over-constrains client-specific semantics, limits personalization, and provides weak protection of local client information under differential privacy noise. To address this issue, we propose Selective Dual-module Federated LoRA (SDFLoRA), which decomposes each client adapter into a global module that captures transferable knowledge and a local module that preserves client-specific adaptations. The global module is selectively aligned and aggregated across clients, while local modules remain private. This design enables robust learning under rank heterogeneity and supports privacy-aware optimization by injecting differential privacy noise exclusively into the global module. Experiments on GLUE benchmarks demonstrate that SDFLoRA outperforms representative federated LoRA baselines and achieves a better utility-privacy trade-off.", "link": "http://arxiv.org/abs/2601.11219v1", "date": "2026-01-16", "relevancy": 1.7952, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4577}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4432}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDFLoRA%3A%20Selective%20Dual-Module%20LoRA%20for%20Federated%20Fine-tuning%20with%20Heterogeneous%20Clients&body=Title%3A%20SDFLoRA%3A%20Selective%20Dual-Module%20LoRA%20for%20Federated%20Fine-tuning%20with%20Heterogeneous%20Clients%0AAuthor%3A%20Zhikang%20Shen%20and%20Jianrong%20Lu%20and%20Haiyuan%20Wan%20and%20Jianhai%20Chen%0AAbstract%3A%20Federated%20learning%20%28FL%29%20for%20large%20language%20models%20%28LLMs%29%20has%20attracted%20increasing%20attention%20as%20a%20way%20to%20enable%20privacy-preserving%20adaptation%20over%20distributed%20data.%20Parameter-efficient%20methods%20such%20as%20LoRA%20are%20widely%20adopted%20to%20reduce%20communication%20and%20memory%20costs.%20Despite%20these%20advances%2C%20practical%20FL%20deployments%20often%20exhibit%20rank%20heterogeneity%2C%20since%20different%20clients%20may%20use%20different%20low-rank%20configurations.%20This%20makes%20direct%20aggregation%20of%20LoRA%20updates%20biased%20and%20unstable.%20Existing%20solutions%20typically%20enforce%20unified%20ranks%20or%20align%20heterogeneous%20updates%20into%20a%20shared%20subspace%2C%20which%20over-constrains%20client-specific%20semantics%2C%20limits%20personalization%2C%20and%20provides%20weak%20protection%20of%20local%20client%20information%20under%20differential%20privacy%20noise.%20To%20address%20this%20issue%2C%20we%20propose%20Selective%20Dual-module%20Federated%20LoRA%20%28SDFLoRA%29%2C%20which%20decomposes%20each%20client%20adapter%20into%20a%20global%20module%20that%20captures%20transferable%20knowledge%20and%20a%20local%20module%20that%20preserves%20client-specific%20adaptations.%20The%20global%20module%20is%20selectively%20aligned%20and%20aggregated%20across%20clients%2C%20while%20local%20modules%20remain%20private.%20This%20design%20enables%20robust%20learning%20under%20rank%20heterogeneity%20and%20supports%20privacy-aware%20optimization%20by%20injecting%20differential%20privacy%20noise%20exclusively%20into%20the%20global%20module.%20Experiments%20on%20GLUE%20benchmarks%20demonstrate%20that%20SDFLoRA%20outperforms%20representative%20federated%20LoRA%20baselines%20and%20achieves%20a%20better%20utility-privacy%20trade-off.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDFLoRA%253A%2520Selective%2520Dual-Module%2520LoRA%2520for%2520Federated%2520Fine-tuning%2520with%2520Heterogeneous%2520Clients%26entry.906535625%3DZhikang%2520Shen%2520and%2520Jianrong%2520Lu%2520and%2520Haiyuan%2520Wan%2520and%2520Jianhai%2520Chen%26entry.1292438233%3DFederated%2520learning%2520%2528FL%2529%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520attracted%2520increasing%2520attention%2520as%2520a%2520way%2520to%2520enable%2520privacy-preserving%2520adaptation%2520over%2520distributed%2520data.%2520Parameter-efficient%2520methods%2520such%2520as%2520LoRA%2520are%2520widely%2520adopted%2520to%2520reduce%2520communication%2520and%2520memory%2520costs.%2520Despite%2520these%2520advances%252C%2520practical%2520FL%2520deployments%2520often%2520exhibit%2520rank%2520heterogeneity%252C%2520since%2520different%2520clients%2520may%2520use%2520different%2520low-rank%2520configurations.%2520This%2520makes%2520direct%2520aggregation%2520of%2520LoRA%2520updates%2520biased%2520and%2520unstable.%2520Existing%2520solutions%2520typically%2520enforce%2520unified%2520ranks%2520or%2520align%2520heterogeneous%2520updates%2520into%2520a%2520shared%2520subspace%252C%2520which%2520over-constrains%2520client-specific%2520semantics%252C%2520limits%2520personalization%252C%2520and%2520provides%2520weak%2520protection%2520of%2520local%2520client%2520information%2520under%2520differential%2520privacy%2520noise.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Selective%2520Dual-module%2520Federated%2520LoRA%2520%2528SDFLoRA%2529%252C%2520which%2520decomposes%2520each%2520client%2520adapter%2520into%2520a%2520global%2520module%2520that%2520captures%2520transferable%2520knowledge%2520and%2520a%2520local%2520module%2520that%2520preserves%2520client-specific%2520adaptations.%2520The%2520global%2520module%2520is%2520selectively%2520aligned%2520and%2520aggregated%2520across%2520clients%252C%2520while%2520local%2520modules%2520remain%2520private.%2520This%2520design%2520enables%2520robust%2520learning%2520under%2520rank%2520heterogeneity%2520and%2520supports%2520privacy-aware%2520optimization%2520by%2520injecting%2520differential%2520privacy%2520noise%2520exclusively%2520into%2520the%2520global%2520module.%2520Experiments%2520on%2520GLUE%2520benchmarks%2520demonstrate%2520that%2520SDFLoRA%2520outperforms%2520representative%2520federated%2520LoRA%2520baselines%2520and%2520achieves%2520a%2520better%2520utility-privacy%2520trade-off.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDFLoRA%3A%20Selective%20Dual-Module%20LoRA%20for%20Federated%20Fine-tuning%20with%20Heterogeneous%20Clients&entry.906535625=Zhikang%20Shen%20and%20Jianrong%20Lu%20and%20Haiyuan%20Wan%20and%20Jianhai%20Chen&entry.1292438233=Federated%20learning%20%28FL%29%20for%20large%20language%20models%20%28LLMs%29%20has%20attracted%20increasing%20attention%20as%20a%20way%20to%20enable%20privacy-preserving%20adaptation%20over%20distributed%20data.%20Parameter-efficient%20methods%20such%20as%20LoRA%20are%20widely%20adopted%20to%20reduce%20communication%20and%20memory%20costs.%20Despite%20these%20advances%2C%20practical%20FL%20deployments%20often%20exhibit%20rank%20heterogeneity%2C%20since%20different%20clients%20may%20use%20different%20low-rank%20configurations.%20This%20makes%20direct%20aggregation%20of%20LoRA%20updates%20biased%20and%20unstable.%20Existing%20solutions%20typically%20enforce%20unified%20ranks%20or%20align%20heterogeneous%20updates%20into%20a%20shared%20subspace%2C%20which%20over-constrains%20client-specific%20semantics%2C%20limits%20personalization%2C%20and%20provides%20weak%20protection%20of%20local%20client%20information%20under%20differential%20privacy%20noise.%20To%20address%20this%20issue%2C%20we%20propose%20Selective%20Dual-module%20Federated%20LoRA%20%28SDFLoRA%29%2C%20which%20decomposes%20each%20client%20adapter%20into%20a%20global%20module%20that%20captures%20transferable%20knowledge%20and%20a%20local%20module%20that%20preserves%20client-specific%20adaptations.%20The%20global%20module%20is%20selectively%20aligned%20and%20aggregated%20across%20clients%2C%20while%20local%20modules%20remain%20private.%20This%20design%20enables%20robust%20learning%20under%20rank%20heterogeneity%20and%20supports%20privacy-aware%20optimization%20by%20injecting%20differential%20privacy%20noise%20exclusively%20into%20the%20global%20module.%20Experiments%20on%20GLUE%20benchmarks%20demonstrate%20that%20SDFLoRA%20outperforms%20representative%20federated%20LoRA%20baselines%20and%20achieves%20a%20better%20utility-privacy%20trade-off.&entry.1838667208=http%3A//arxiv.org/abs/2601.11219v1&entry.124074799=Read"},
{"title": "Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling Problems", "author": "Sofiene Lassoued and Asrat Gobachew and Stefan Lier and Andreas Schwung", "abstract": "This paper proposes a policy-based deep reinforcement learning hyper-heuristic framework for solving the Job Shop Scheduling Problem. The hyper-heuristic agent learns to switch scheduling rules based on the system state dynamically. We extend the hyper-heuristic framework with two key mechanisms. First, action prefiltering restricts decision-making to feasible low-level actions, enabling low-level heuristics to be evaluated independently of environmental constraints and providing an unbiased assessment. Second, a commitment mechanism regulates the frequency of heuristic switching. We investigate the impact of different commitment strategies, from step-wise switching to full-episode commitment, on both training behavior and makespan. Additionally, we compare two action selection strategies at the policy level: deterministic greedy selection and stochastic sampling. Computational experiments on standard JSSP benchmarks demonstrate that the proposed approach outperforms traditional heuristics, metaheuristics, and recent neural network-based scheduling methods", "link": "http://arxiv.org/abs/2601.11189v1", "date": "2026-01-16", "relevancy": 1.4171, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4825}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4629}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Policy-Based%20Deep%20Reinforcement%20Learning%20Hyperheuristics%20for%20Job-Shop%20Scheduling%20Problems&body=Title%3A%20Policy-Based%20Deep%20Reinforcement%20Learning%20Hyperheuristics%20for%20Job-Shop%20Scheduling%20Problems%0AAuthor%3A%20Sofiene%20Lassoued%20and%20Asrat%20Gobachew%20and%20Stefan%20Lier%20and%20Andreas%20Schwung%0AAbstract%3A%20This%20paper%20proposes%20a%20policy-based%20deep%20reinforcement%20learning%20hyper-heuristic%20framework%20for%20solving%20the%20Job%20Shop%20Scheduling%20Problem.%20The%20hyper-heuristic%20agent%20learns%20to%20switch%20scheduling%20rules%20based%20on%20the%20system%20state%20dynamically.%20We%20extend%20the%20hyper-heuristic%20framework%20with%20two%20key%20mechanisms.%20First%2C%20action%20prefiltering%20restricts%20decision-making%20to%20feasible%20low-level%20actions%2C%20enabling%20low-level%20heuristics%20to%20be%20evaluated%20independently%20of%20environmental%20constraints%20and%20providing%20an%20unbiased%20assessment.%20Second%2C%20a%20commitment%20mechanism%20regulates%20the%20frequency%20of%20heuristic%20switching.%20We%20investigate%20the%20impact%20of%20different%20commitment%20strategies%2C%20from%20step-wise%20switching%20to%20full-episode%20commitment%2C%20on%20both%20training%20behavior%20and%20makespan.%20Additionally%2C%20we%20compare%20two%20action%20selection%20strategies%20at%20the%20policy%20level%3A%20deterministic%20greedy%20selection%20and%20stochastic%20sampling.%20Computational%20experiments%20on%20standard%20JSSP%20benchmarks%20demonstrate%20that%20the%20proposed%20approach%20outperforms%20traditional%20heuristics%2C%20metaheuristics%2C%20and%20recent%20neural%20network-based%20scheduling%20methods%0ALink%3A%20http%3A//arxiv.org/abs/2601.11189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolicy-Based%2520Deep%2520Reinforcement%2520Learning%2520Hyperheuristics%2520for%2520Job-Shop%2520Scheduling%2520Problems%26entry.906535625%3DSofiene%2520Lassoued%2520and%2520Asrat%2520Gobachew%2520and%2520Stefan%2520Lier%2520and%2520Andreas%2520Schwung%26entry.1292438233%3DThis%2520paper%2520proposes%2520a%2520policy-based%2520deep%2520reinforcement%2520learning%2520hyper-heuristic%2520framework%2520for%2520solving%2520the%2520Job%2520Shop%2520Scheduling%2520Problem.%2520The%2520hyper-heuristic%2520agent%2520learns%2520to%2520switch%2520scheduling%2520rules%2520based%2520on%2520the%2520system%2520state%2520dynamically.%2520We%2520extend%2520the%2520hyper-heuristic%2520framework%2520with%2520two%2520key%2520mechanisms.%2520First%252C%2520action%2520prefiltering%2520restricts%2520decision-making%2520to%2520feasible%2520low-level%2520actions%252C%2520enabling%2520low-level%2520heuristics%2520to%2520be%2520evaluated%2520independently%2520of%2520environmental%2520constraints%2520and%2520providing%2520an%2520unbiased%2520assessment.%2520Second%252C%2520a%2520commitment%2520mechanism%2520regulates%2520the%2520frequency%2520of%2520heuristic%2520switching.%2520We%2520investigate%2520the%2520impact%2520of%2520different%2520commitment%2520strategies%252C%2520from%2520step-wise%2520switching%2520to%2520full-episode%2520commitment%252C%2520on%2520both%2520training%2520behavior%2520and%2520makespan.%2520Additionally%252C%2520we%2520compare%2520two%2520action%2520selection%2520strategies%2520at%2520the%2520policy%2520level%253A%2520deterministic%2520greedy%2520selection%2520and%2520stochastic%2520sampling.%2520Computational%2520experiments%2520on%2520standard%2520JSSP%2520benchmarks%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520outperforms%2520traditional%2520heuristics%252C%2520metaheuristics%252C%2520and%2520recent%2520neural%2520network-based%2520scheduling%2520methods%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Policy-Based%20Deep%20Reinforcement%20Learning%20Hyperheuristics%20for%20Job-Shop%20Scheduling%20Problems&entry.906535625=Sofiene%20Lassoued%20and%20Asrat%20Gobachew%20and%20Stefan%20Lier%20and%20Andreas%20Schwung&entry.1292438233=This%20paper%20proposes%20a%20policy-based%20deep%20reinforcement%20learning%20hyper-heuristic%20framework%20for%20solving%20the%20Job%20Shop%20Scheduling%20Problem.%20The%20hyper-heuristic%20agent%20learns%20to%20switch%20scheduling%20rules%20based%20on%20the%20system%20state%20dynamically.%20We%20extend%20the%20hyper-heuristic%20framework%20with%20two%20key%20mechanisms.%20First%2C%20action%20prefiltering%20restricts%20decision-making%20to%20feasible%20low-level%20actions%2C%20enabling%20low-level%20heuristics%20to%20be%20evaluated%20independently%20of%20environmental%20constraints%20and%20providing%20an%20unbiased%20assessment.%20Second%2C%20a%20commitment%20mechanism%20regulates%20the%20frequency%20of%20heuristic%20switching.%20We%20investigate%20the%20impact%20of%20different%20commitment%20strategies%2C%20from%20step-wise%20switching%20to%20full-episode%20commitment%2C%20on%20both%20training%20behavior%20and%20makespan.%20Additionally%2C%20we%20compare%20two%20action%20selection%20strategies%20at%20the%20policy%20level%3A%20deterministic%20greedy%20selection%20and%20stochastic%20sampling.%20Computational%20experiments%20on%20standard%20JSSP%20benchmarks%20demonstrate%20that%20the%20proposed%20approach%20outperforms%20traditional%20heuristics%2C%20metaheuristics%2C%20and%20recent%20neural%20network-based%20scheduling%20methods&entry.1838667208=http%3A//arxiv.org/abs/2601.11189v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


