<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240908.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "author": "Lorenza Prospero and Abdullah Hamdi and Joao F. Henriques and Christian Rupprecht", "abstract": "  Reconstructing realistic 3D human models from monocular images has\nsignificant applications in creative industries, human-computer interfaces, and\nhealthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene\nrepresentation composed of a mixture of Gaussians. Predicting such mixtures for\na human from a single input image is challenging, as it is a non-uniform\ndensity (with a many-to-one relationship with input pixels) with strict\nphysical constraints. At the same time, it needs to be flexible to accommodate\na variety of clothes and poses. Our key observation is that the vertices of\nstandardized human meshes (such as SMPL) can provide an adequate density and\napproximate initial position for Gaussians. We can then train a transformer\nmodel to jointly predict comparatively small adjustments to these positions, as\nwell as the other Gaussians' attributes and the SMPL parameters. We show\nempirically that this combination (using only multi-view supervision) can\nachieve fast inference of 3D human models from a single image without test-time\noptimization, expensive diffusion models, or 3D points supervision. We also\nshow that it can improve 3D pose estimation by better fitting human models that\naccount for clothes and other variations. The code is available on the project\nwebsite https://abdullahamdi.com/gst/ .\n", "link": "http://arxiv.org/abs/2409.04196v1", "date": "2024-09-06", "relevancy": 3.2779, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6898}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6605}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GST%3A%20Precise%203D%20Human%20Body%20from%20a%20Single%20Image%20with%20Gaussian%20Splatting%0A%20%20Transformers&body=Title%3A%20GST%3A%20Precise%203D%20Human%20Body%20from%20a%20Single%20Image%20with%20Gaussian%20Splatting%0A%20%20Transformers%0AAuthor%3A%20Lorenza%20Prospero%20and%20Abdullah%20Hamdi%20and%20Joao%20F.%20Henriques%20and%20Christian%20Rupprecht%0AAbstract%3A%20%20%20Reconstructing%20realistic%203D%20human%20models%20from%20monocular%20images%20has%0Asignificant%20applications%20in%20creative%20industries%2C%20human-computer%20interfaces%2C%20and%0Ahealthcare.%20We%20base%20our%20work%20on%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20a%20scene%0Arepresentation%20composed%20of%20a%20mixture%20of%20Gaussians.%20Predicting%20such%20mixtures%20for%0Aa%20human%20from%20a%20single%20input%20image%20is%20challenging%2C%20as%20it%20is%20a%20non-uniform%0Adensity%20%28with%20a%20many-to-one%20relationship%20with%20input%20pixels%29%20with%20strict%0Aphysical%20constraints.%20At%20the%20same%20time%2C%20it%20needs%20to%20be%20flexible%20to%20accommodate%0Aa%20variety%20of%20clothes%20and%20poses.%20Our%20key%20observation%20is%20that%20the%20vertices%20of%0Astandardized%20human%20meshes%20%28such%20as%20SMPL%29%20can%20provide%20an%20adequate%20density%20and%0Aapproximate%20initial%20position%20for%20Gaussians.%20We%20can%20then%20train%20a%20transformer%0Amodel%20to%20jointly%20predict%20comparatively%20small%20adjustments%20to%20these%20positions%2C%20as%0Awell%20as%20the%20other%20Gaussians%27%20attributes%20and%20the%20SMPL%20parameters.%20We%20show%0Aempirically%20that%20this%20combination%20%28using%20only%20multi-view%20supervision%29%20can%0Aachieve%20fast%20inference%20of%203D%20human%20models%20from%20a%20single%20image%20without%20test-time%0Aoptimization%2C%20expensive%20diffusion%20models%2C%20or%203D%20points%20supervision.%20We%20also%0Ashow%20that%20it%20can%20improve%203D%20pose%20estimation%20by%20better%20fitting%20human%20models%20that%0Aaccount%20for%20clothes%20and%20other%20variations.%20The%20code%20is%20available%20on%20the%20project%0Awebsite%20https%3A//abdullahamdi.com/gst/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGST%253A%2520Precise%25203D%2520Human%2520Body%2520from%2520a%2520Single%2520Image%2520with%2520Gaussian%2520Splatting%250A%2520%2520Transformers%26entry.906535625%3DLorenza%2520Prospero%2520and%2520Abdullah%2520Hamdi%2520and%2520Joao%2520F.%2520Henriques%2520and%2520Christian%2520Rupprecht%26entry.1292438233%3D%2520%2520Reconstructing%2520realistic%25203D%2520human%2520models%2520from%2520monocular%2520images%2520has%250Asignificant%2520applications%2520in%2520creative%2520industries%252C%2520human-computer%2520interfaces%252C%2520and%250Ahealthcare.%2520We%2520base%2520our%2520work%2520on%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520a%2520scene%250Arepresentation%2520composed%2520of%2520a%2520mixture%2520of%2520Gaussians.%2520Predicting%2520such%2520mixtures%2520for%250Aa%2520human%2520from%2520a%2520single%2520input%2520image%2520is%2520challenging%252C%2520as%2520it%2520is%2520a%2520non-uniform%250Adensity%2520%2528with%2520a%2520many-to-one%2520relationship%2520with%2520input%2520pixels%2529%2520with%2520strict%250Aphysical%2520constraints.%2520At%2520the%2520same%2520time%252C%2520it%2520needs%2520to%2520be%2520flexible%2520to%2520accommodate%250Aa%2520variety%2520of%2520clothes%2520and%2520poses.%2520Our%2520key%2520observation%2520is%2520that%2520the%2520vertices%2520of%250Astandardized%2520human%2520meshes%2520%2528such%2520as%2520SMPL%2529%2520can%2520provide%2520an%2520adequate%2520density%2520and%250Aapproximate%2520initial%2520position%2520for%2520Gaussians.%2520We%2520can%2520then%2520train%2520a%2520transformer%250Amodel%2520to%2520jointly%2520predict%2520comparatively%2520small%2520adjustments%2520to%2520these%2520positions%252C%2520as%250Awell%2520as%2520the%2520other%2520Gaussians%2527%2520attributes%2520and%2520the%2520SMPL%2520parameters.%2520We%2520show%250Aempirically%2520that%2520this%2520combination%2520%2528using%2520only%2520multi-view%2520supervision%2529%2520can%250Aachieve%2520fast%2520inference%2520of%25203D%2520human%2520models%2520from%2520a%2520single%2520image%2520without%2520test-time%250Aoptimization%252C%2520expensive%2520diffusion%2520models%252C%2520or%25203D%2520points%2520supervision.%2520We%2520also%250Ashow%2520that%2520it%2520can%2520improve%25203D%2520pose%2520estimation%2520by%2520better%2520fitting%2520human%2520models%2520that%250Aaccount%2520for%2520clothes%2520and%2520other%2520variations.%2520The%2520code%2520is%2520available%2520on%2520the%2520project%250Awebsite%2520https%253A//abdullahamdi.com/gst/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GST%3A%20Precise%203D%20Human%20Body%20from%20a%20Single%20Image%20with%20Gaussian%20Splatting%0A%20%20Transformers&entry.906535625=Lorenza%20Prospero%20and%20Abdullah%20Hamdi%20and%20Joao%20F.%20Henriques%20and%20Christian%20Rupprecht&entry.1292438233=%20%20Reconstructing%20realistic%203D%20human%20models%20from%20monocular%20images%20has%0Asignificant%20applications%20in%20creative%20industries%2C%20human-computer%20interfaces%2C%20and%0Ahealthcare.%20We%20base%20our%20work%20on%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20a%20scene%0Arepresentation%20composed%20of%20a%20mixture%20of%20Gaussians.%20Predicting%20such%20mixtures%20for%0Aa%20human%20from%20a%20single%20input%20image%20is%20challenging%2C%20as%20it%20is%20a%20non-uniform%0Adensity%20%28with%20a%20many-to-one%20relationship%20with%20input%20pixels%29%20with%20strict%0Aphysical%20constraints.%20At%20the%20same%20time%2C%20it%20needs%20to%20be%20flexible%20to%20accommodate%0Aa%20variety%20of%20clothes%20and%20poses.%20Our%20key%20observation%20is%20that%20the%20vertices%20of%0Astandardized%20human%20meshes%20%28such%20as%20SMPL%29%20can%20provide%20an%20adequate%20density%20and%0Aapproximate%20initial%20position%20for%20Gaussians.%20We%20can%20then%20train%20a%20transformer%0Amodel%20to%20jointly%20predict%20comparatively%20small%20adjustments%20to%20these%20positions%2C%20as%0Awell%20as%20the%20other%20Gaussians%27%20attributes%20and%20the%20SMPL%20parameters.%20We%20show%0Aempirically%20that%20this%20combination%20%28using%20only%20multi-view%20supervision%29%20can%0Aachieve%20fast%20inference%20of%203D%20human%20models%20from%20a%20single%20image%20without%20test-time%0Aoptimization%2C%20expensive%20diffusion%20models%2C%20or%203D%20points%20supervision.%20We%20also%0Ashow%20that%20it%20can%20improve%203D%20pose%20estimation%20by%20better%20fitting%20human%20models%20that%0Aaccount%20for%20clothes%20and%20other%20variations.%20The%20code%20is%20available%20on%20the%20project%0Awebsite%20https%3A//abdullahamdi.com/gst/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04196v1&entry.124074799=Read"},
{"title": "3D Gaussian Splatting for Large-scale 3D Surface Reconstruction from\n  Aerial Images", "author": "YuanZheng Wu and Jin Liu and Shunping Ji", "abstract": "  Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention.\nHowever, the unstructured nature of 3DGS poses challenges for large-scale\nsurface reconstruction from aerial images. To address this gap, we propose the\nfirst large-scale surface reconstruction method for multi-view stereo (MVS)\naerial images based on 3DGS, named Aerial Gaussian Splatting (AGS). Initially,\nwe introduce a data chunking method tailored for large-scale aerial imagery,\nmaking the modern 3DGS technology feasible for surface reconstruction over\nextensive scenes. Additionally, we integrate the Ray-Gaussian Intersection\nmethod to obtain normal and depth information, facilitating geometric\nconstraints. Finally, we introduce a multi-view geometric consistency\nconstraint to enhance global geometric consistency and improve reconstruction\naccuracy. Our experiments on multiple datasets demonstrate for the first time\nthat the GS-based technique can match traditional aerial MVS methods on\ngeometric accuracy, and beat state-of-the-art GS-based methods on geometry and\nrendering quality.\n", "link": "http://arxiv.org/abs/2409.00381v2", "date": "2024-09-06", "relevancy": 3.2016, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7291}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6394}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Gaussian%20Splatting%20for%20Large-scale%203D%20Surface%20Reconstruction%20from%0A%20%20Aerial%20Images&body=Title%3A%203D%20Gaussian%20Splatting%20for%20Large-scale%203D%20Surface%20Reconstruction%20from%0A%20%20Aerial%20Images%0AAuthor%3A%20YuanZheng%20Wu%20and%20Jin%20Liu%20and%20Shunping%20Ji%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20garnered%20significant%20attention.%0AHowever%2C%20the%20unstructured%20nature%20of%203DGS%20poses%20challenges%20for%20large-scale%0Asurface%20reconstruction%20from%20aerial%20images.%20To%20address%20this%20gap%2C%20we%20propose%20the%0Afirst%20large-scale%20surface%20reconstruction%20method%20for%20multi-view%20stereo%20%28MVS%29%0Aaerial%20images%20based%20on%203DGS%2C%20named%20Aerial%20Gaussian%20Splatting%20%28AGS%29.%20Initially%2C%0Awe%20introduce%20a%20data%20chunking%20method%20tailored%20for%20large-scale%20aerial%20imagery%2C%0Amaking%20the%20modern%203DGS%20technology%20feasible%20for%20surface%20reconstruction%20over%0Aextensive%20scenes.%20Additionally%2C%20we%20integrate%20the%20Ray-Gaussian%20Intersection%0Amethod%20to%20obtain%20normal%20and%20depth%20information%2C%20facilitating%20geometric%0Aconstraints.%20Finally%2C%20we%20introduce%20a%20multi-view%20geometric%20consistency%0Aconstraint%20to%20enhance%20global%20geometric%20consistency%20and%20improve%20reconstruction%0Aaccuracy.%20Our%20experiments%20on%20multiple%20datasets%20demonstrate%20for%20the%20first%20time%0Athat%20the%20GS-based%20technique%20can%20match%20traditional%20aerial%20MVS%20methods%20on%0Ageometric%20accuracy%2C%20and%20beat%20state-of-the-art%20GS-based%20methods%20on%20geometry%20and%0Arendering%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00381v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Gaussian%2520Splatting%2520for%2520Large-scale%25203D%2520Surface%2520Reconstruction%2520from%250A%2520%2520Aerial%2520Images%26entry.906535625%3DYuanZheng%2520Wu%2520and%2520Jin%2520Liu%2520and%2520Shunping%2520Ji%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520garnered%2520significant%2520attention.%250AHowever%252C%2520the%2520unstructured%2520nature%2520of%25203DGS%2520poses%2520challenges%2520for%2520large-scale%250Asurface%2520reconstruction%2520from%2520aerial%2520images.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520the%250Afirst%2520large-scale%2520surface%2520reconstruction%2520method%2520for%2520multi-view%2520stereo%2520%2528MVS%2529%250Aaerial%2520images%2520based%2520on%25203DGS%252C%2520named%2520Aerial%2520Gaussian%2520Splatting%2520%2528AGS%2529.%2520Initially%252C%250Awe%2520introduce%2520a%2520data%2520chunking%2520method%2520tailored%2520for%2520large-scale%2520aerial%2520imagery%252C%250Amaking%2520the%2520modern%25203DGS%2520technology%2520feasible%2520for%2520surface%2520reconstruction%2520over%250Aextensive%2520scenes.%2520Additionally%252C%2520we%2520integrate%2520the%2520Ray-Gaussian%2520Intersection%250Amethod%2520to%2520obtain%2520normal%2520and%2520depth%2520information%252C%2520facilitating%2520geometric%250Aconstraints.%2520Finally%252C%2520we%2520introduce%2520a%2520multi-view%2520geometric%2520consistency%250Aconstraint%2520to%2520enhance%2520global%2520geometric%2520consistency%2520and%2520improve%2520reconstruction%250Aaccuracy.%2520Our%2520experiments%2520on%2520multiple%2520datasets%2520demonstrate%2520for%2520the%2520first%2520time%250Athat%2520the%2520GS-based%2520technique%2520can%2520match%2520traditional%2520aerial%2520MVS%2520methods%2520on%250Ageometric%2520accuracy%252C%2520and%2520beat%2520state-of-the-art%2520GS-based%2520methods%2520on%2520geometry%2520and%250Arendering%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00381v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Gaussian%20Splatting%20for%20Large-scale%203D%20Surface%20Reconstruction%20from%0A%20%20Aerial%20Images&entry.906535625=YuanZheng%20Wu%20and%20Jin%20Liu%20and%20Shunping%20Ji&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20garnered%20significant%20attention.%0AHowever%2C%20the%20unstructured%20nature%20of%203DGS%20poses%20challenges%20for%20large-scale%0Asurface%20reconstruction%20from%20aerial%20images.%20To%20address%20this%20gap%2C%20we%20propose%20the%0Afirst%20large-scale%20surface%20reconstruction%20method%20for%20multi-view%20stereo%20%28MVS%29%0Aaerial%20images%20based%20on%203DGS%2C%20named%20Aerial%20Gaussian%20Splatting%20%28AGS%29.%20Initially%2C%0Awe%20introduce%20a%20data%20chunking%20method%20tailored%20for%20large-scale%20aerial%20imagery%2C%0Amaking%20the%20modern%203DGS%20technology%20feasible%20for%20surface%20reconstruction%20over%0Aextensive%20scenes.%20Additionally%2C%20we%20integrate%20the%20Ray-Gaussian%20Intersection%0Amethod%20to%20obtain%20normal%20and%20depth%20information%2C%20facilitating%20geometric%0Aconstraints.%20Finally%2C%20we%20introduce%20a%20multi-view%20geometric%20consistency%0Aconstraint%20to%20enhance%20global%20geometric%20consistency%20and%20improve%20reconstruction%0Aaccuracy.%20Our%20experiments%20on%20multiple%20datasets%20demonstrate%20for%20the%20first%20time%0Athat%20the%20GS-based%20technique%20can%20match%20traditional%20aerial%20MVS%20methods%20on%0Ageometric%20accuracy%2C%20and%20beat%20state-of-the-art%20GS-based%20methods%20on%20geometry%20and%0Arendering%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00381v2&entry.124074799=Read"},
{"title": "Gaussian Splatting in Style", "author": "Abhishek Saroha and Mariia Gladkova and Cecilia Curreli and Dominik Muhle and Tarun Yenamandra and Daniel Cremers", "abstract": "  3D scene stylization extends the work of neural style transfer to 3D. A vital\nchallenge in this problem is to maintain the uniformity of the stylized\nappearance across multiple views. A vast majority of the previous works achieve\nthis by training a 3D model for every stylized image and a set of multi-view\nimages. In contrast, we propose a novel architecture trained on a collection of\nstyle images that, at test time, produces real time high-quality stylized novel\nviews. We choose the underlying 3D scene representation for our model as 3D\nGaussian splatting. We take the 3D Gaussians and process them using a\nmulti-resolution hash grid and a tiny MLP to obtain stylized views. The MLP is\nconditioned on different style codes for generalization to different styles\nduring test time. The explicit nature of 3D Gaussians gives us inherent\nadvantages over NeRF-based methods, including geometric consistency and a fast\ntraining and rendering regime. This enables our method to be useful for various\npractical use cases, such as augmented or virtual reality. We demonstrate that\nour method achieves state-of-the-art performance with superior visual quality\non various indoor and outdoor real-world data.\n", "link": "http://arxiv.org/abs/2403.08498v2", "date": "2024-09-06", "relevancy": 3.1502, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6801}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.605}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting%20in%20Style&body=Title%3A%20Gaussian%20Splatting%20in%20Style%0AAuthor%3A%20Abhishek%20Saroha%20and%20Mariia%20Gladkova%20and%20Cecilia%20Curreli%20and%20Dominik%20Muhle%20and%20Tarun%20Yenamandra%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%203D%20scene%20stylization%20extends%20the%20work%20of%20neural%20style%20transfer%20to%203D.%20A%20vital%0Achallenge%20in%20this%20problem%20is%20to%20maintain%20the%20uniformity%20of%20the%20stylized%0Aappearance%20across%20multiple%20views.%20A%20vast%20majority%20of%20the%20previous%20works%20achieve%0Athis%20by%20training%20a%203D%20model%20for%20every%20stylized%20image%20and%20a%20set%20of%20multi-view%0Aimages.%20In%20contrast%2C%20we%20propose%20a%20novel%20architecture%20trained%20on%20a%20collection%20of%0Astyle%20images%20that%2C%20at%20test%20time%2C%20produces%20real%20time%20high-quality%20stylized%20novel%0Aviews.%20We%20choose%20the%20underlying%203D%20scene%20representation%20for%20our%20model%20as%203D%0AGaussian%20splatting.%20We%20take%20the%203D%20Gaussians%20and%20process%20them%20using%20a%0Amulti-resolution%20hash%20grid%20and%20a%20tiny%20MLP%20to%20obtain%20stylized%20views.%20The%20MLP%20is%0Aconditioned%20on%20different%20style%20codes%20for%20generalization%20to%20different%20styles%0Aduring%20test%20time.%20The%20explicit%20nature%20of%203D%20Gaussians%20gives%20us%20inherent%0Aadvantages%20over%20NeRF-based%20methods%2C%20including%20geometric%20consistency%20and%20a%20fast%0Atraining%20and%20rendering%20regime.%20This%20enables%20our%20method%20to%20be%20useful%20for%20various%0Apractical%20use%20cases%2C%20such%20as%20augmented%20or%20virtual%20reality.%20We%20demonstrate%20that%0Aour%20method%20achieves%20state-of-the-art%20performance%20with%20superior%20visual%20quality%0Aon%20various%20indoor%20and%20outdoor%20real-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08498v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting%2520in%2520Style%26entry.906535625%3DAbhishek%2520Saroha%2520and%2520Mariia%2520Gladkova%2520and%2520Cecilia%2520Curreli%2520and%2520Dominik%2520Muhle%2520and%2520Tarun%2520Yenamandra%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%25203D%2520scene%2520stylization%2520extends%2520the%2520work%2520of%2520neural%2520style%2520transfer%2520to%25203D.%2520A%2520vital%250Achallenge%2520in%2520this%2520problem%2520is%2520to%2520maintain%2520the%2520uniformity%2520of%2520the%2520stylized%250Aappearance%2520across%2520multiple%2520views.%2520A%2520vast%2520majority%2520of%2520the%2520previous%2520works%2520achieve%250Athis%2520by%2520training%2520a%25203D%2520model%2520for%2520every%2520stylized%2520image%2520and%2520a%2520set%2520of%2520multi-view%250Aimages.%2520In%2520contrast%252C%2520we%2520propose%2520a%2520novel%2520architecture%2520trained%2520on%2520a%2520collection%2520of%250Astyle%2520images%2520that%252C%2520at%2520test%2520time%252C%2520produces%2520real%2520time%2520high-quality%2520stylized%2520novel%250Aviews.%2520We%2520choose%2520the%2520underlying%25203D%2520scene%2520representation%2520for%2520our%2520model%2520as%25203D%250AGaussian%2520splatting.%2520We%2520take%2520the%25203D%2520Gaussians%2520and%2520process%2520them%2520using%2520a%250Amulti-resolution%2520hash%2520grid%2520and%2520a%2520tiny%2520MLP%2520to%2520obtain%2520stylized%2520views.%2520The%2520MLP%2520is%250Aconditioned%2520on%2520different%2520style%2520codes%2520for%2520generalization%2520to%2520different%2520styles%250Aduring%2520test%2520time.%2520The%2520explicit%2520nature%2520of%25203D%2520Gaussians%2520gives%2520us%2520inherent%250Aadvantages%2520over%2520NeRF-based%2520methods%252C%2520including%2520geometric%2520consistency%2520and%2520a%2520fast%250Atraining%2520and%2520rendering%2520regime.%2520This%2520enables%2520our%2520method%2520to%2520be%2520useful%2520for%2520various%250Apractical%2520use%2520cases%252C%2520such%2520as%2520augmented%2520or%2520virtual%2520reality.%2520We%2520demonstrate%2520that%250Aour%2520method%2520achieves%2520state-of-the-art%2520performance%2520with%2520superior%2520visual%2520quality%250Aon%2520various%2520indoor%2520and%2520outdoor%2520real-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08498v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting%20in%20Style&entry.906535625=Abhishek%20Saroha%20and%20Mariia%20Gladkova%20and%20Cecilia%20Curreli%20and%20Dominik%20Muhle%20and%20Tarun%20Yenamandra%20and%20Daniel%20Cremers&entry.1292438233=%20%203D%20scene%20stylization%20extends%20the%20work%20of%20neural%20style%20transfer%20to%203D.%20A%20vital%0Achallenge%20in%20this%20problem%20is%20to%20maintain%20the%20uniformity%20of%20the%20stylized%0Aappearance%20across%20multiple%20views.%20A%20vast%20majority%20of%20the%20previous%20works%20achieve%0Athis%20by%20training%20a%203D%20model%20for%20every%20stylized%20image%20and%20a%20set%20of%20multi-view%0Aimages.%20In%20contrast%2C%20we%20propose%20a%20novel%20architecture%20trained%20on%20a%20collection%20of%0Astyle%20images%20that%2C%20at%20test%20time%2C%20produces%20real%20time%20high-quality%20stylized%20novel%0Aviews.%20We%20choose%20the%20underlying%203D%20scene%20representation%20for%20our%20model%20as%203D%0AGaussian%20splatting.%20We%20take%20the%203D%20Gaussians%20and%20process%20them%20using%20a%0Amulti-resolution%20hash%20grid%20and%20a%20tiny%20MLP%20to%20obtain%20stylized%20views.%20The%20MLP%20is%0Aconditioned%20on%20different%20style%20codes%20for%20generalization%20to%20different%20styles%0Aduring%20test%20time.%20The%20explicit%20nature%20of%203D%20Gaussians%20gives%20us%20inherent%0Aadvantages%20over%20NeRF-based%20methods%2C%20including%20geometric%20consistency%20and%20a%20fast%0Atraining%20and%20rendering%20regime.%20This%20enables%20our%20method%20to%20be%20useful%20for%20various%0Apractical%20use%20cases%2C%20such%20as%20augmented%20or%20virtual%20reality.%20We%20demonstrate%20that%0Aour%20method%20achieves%20state-of-the-art%20performance%20with%20superior%20visual%20quality%0Aon%20various%20indoor%20and%20outdoor%20real-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08498v2&entry.124074799=Read"},
{"title": "TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object\n  Detection", "author": "Hanning Chen and Wenjun Huang and Yang Ni and Sanggeon Yun and Yezi Liu and Fei Wen and Alvaro Velasquez and Hugo Latapie and Mohsen Imani", "abstract": "  Task-oriented object detection aims to find objects suitable for\naccomplishing specific tasks. As a challenging task, it requires simultaneous\nvisual data processing and reasoning under ambiguous semantics. Recent\nsolutions are mainly all-in-one models. However, the object detection backbones\nare pre-trained without text supervision. Thus, to incorporate task\nrequirements, their intricate models undergo extensive learning on a highly\nimbalanced and scarce dataset, resulting in capped performance, laborious\ntraining, and poor generalizability. In contrast, we propose TaskCLIP, a more\nnatural two-stage design composed of general object detection and task-guided\nobject selection. Particularly for the latter, we resort to the recently\nsuccessful large Vision-Language Models (VLMs) as our backbone, which provides\nrich semantic knowledge and a uniform embedding space for images and texts.\nNevertheless, the naive application of VLMs leads to sub-optimal quality, due\nto the misalignment between embeddings of object images and their visual\nattributes, which are mainly adjective phrases. To this end, we design a\ntransformer-based aligner after the pre-trained VLMs to re-calibrate both\nembeddings. Finally, we employ a trainable score function to post-process the\nVLM matching results for object selection. Experimental results demonstrate\nthat our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by\n3.5% and only requires a single NVIDIA RTX 4090 for both training and\ninference.\n", "link": "http://arxiv.org/abs/2403.08108v2", "date": "2024-09-06", "relevancy": 3.0641, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5952}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TaskCLIP%3A%20Extend%20Large%20Vision-Language%20Model%20for%20Task%20Oriented%20Object%0A%20%20Detection&body=Title%3A%20TaskCLIP%3A%20Extend%20Large%20Vision-Language%20Model%20for%20Task%20Oriented%20Object%0A%20%20Detection%0AAuthor%3A%20Hanning%20Chen%20and%20Wenjun%20Huang%20and%20Yang%20Ni%20and%20Sanggeon%20Yun%20and%20Yezi%20Liu%20and%20Fei%20Wen%20and%20Alvaro%20Velasquez%20and%20Hugo%20Latapie%20and%20Mohsen%20Imani%0AAbstract%3A%20%20%20Task-oriented%20object%20detection%20aims%20to%20find%20objects%20suitable%20for%0Aaccomplishing%20specific%20tasks.%20As%20a%20challenging%20task%2C%20it%20requires%20simultaneous%0Avisual%20data%20processing%20and%20reasoning%20under%20ambiguous%20semantics.%20Recent%0Asolutions%20are%20mainly%20all-in-one%20models.%20However%2C%20the%20object%20detection%20backbones%0Aare%20pre-trained%20without%20text%20supervision.%20Thus%2C%20to%20incorporate%20task%0Arequirements%2C%20their%20intricate%20models%20undergo%20extensive%20learning%20on%20a%20highly%0Aimbalanced%20and%20scarce%20dataset%2C%20resulting%20in%20capped%20performance%2C%20laborious%0Atraining%2C%20and%20poor%20generalizability.%20In%20contrast%2C%20we%20propose%20TaskCLIP%2C%20a%20more%0Anatural%20two-stage%20design%20composed%20of%20general%20object%20detection%20and%20task-guided%0Aobject%20selection.%20Particularly%20for%20the%20latter%2C%20we%20resort%20to%20the%20recently%0Asuccessful%20large%20Vision-Language%20Models%20%28VLMs%29%20as%20our%20backbone%2C%20which%20provides%0Arich%20semantic%20knowledge%20and%20a%20uniform%20embedding%20space%20for%20images%20and%20texts.%0ANevertheless%2C%20the%20naive%20application%20of%20VLMs%20leads%20to%20sub-optimal%20quality%2C%20due%0Ato%20the%20misalignment%20between%20embeddings%20of%20object%20images%20and%20their%20visual%0Aattributes%2C%20which%20are%20mainly%20adjective%20phrases.%20To%20this%20end%2C%20we%20design%20a%0Atransformer-based%20aligner%20after%20the%20pre-trained%20VLMs%20to%20re-calibrate%20both%0Aembeddings.%20Finally%2C%20we%20employ%20a%20trainable%20score%20function%20to%20post-process%20the%0AVLM%20matching%20results%20for%20object%20selection.%20Experimental%20results%20demonstrate%0Athat%20our%20TaskCLIP%20outperforms%20the%20state-of-the-art%20DETR-based%20model%20TOIST%20by%0A3.5%25%20and%20only%20requires%20a%20single%20NVIDIA%20RTX%204090%20for%20both%20training%20and%0Ainference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08108v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaskCLIP%253A%2520Extend%2520Large%2520Vision-Language%2520Model%2520for%2520Task%2520Oriented%2520Object%250A%2520%2520Detection%26entry.906535625%3DHanning%2520Chen%2520and%2520Wenjun%2520Huang%2520and%2520Yang%2520Ni%2520and%2520Sanggeon%2520Yun%2520and%2520Yezi%2520Liu%2520and%2520Fei%2520Wen%2520and%2520Alvaro%2520Velasquez%2520and%2520Hugo%2520Latapie%2520and%2520Mohsen%2520Imani%26entry.1292438233%3D%2520%2520Task-oriented%2520object%2520detection%2520aims%2520to%2520find%2520objects%2520suitable%2520for%250Aaccomplishing%2520specific%2520tasks.%2520As%2520a%2520challenging%2520task%252C%2520it%2520requires%2520simultaneous%250Avisual%2520data%2520processing%2520and%2520reasoning%2520under%2520ambiguous%2520semantics.%2520Recent%250Asolutions%2520are%2520mainly%2520all-in-one%2520models.%2520However%252C%2520the%2520object%2520detection%2520backbones%250Aare%2520pre-trained%2520without%2520text%2520supervision.%2520Thus%252C%2520to%2520incorporate%2520task%250Arequirements%252C%2520their%2520intricate%2520models%2520undergo%2520extensive%2520learning%2520on%2520a%2520highly%250Aimbalanced%2520and%2520scarce%2520dataset%252C%2520resulting%2520in%2520capped%2520performance%252C%2520laborious%250Atraining%252C%2520and%2520poor%2520generalizability.%2520In%2520contrast%252C%2520we%2520propose%2520TaskCLIP%252C%2520a%2520more%250Anatural%2520two-stage%2520design%2520composed%2520of%2520general%2520object%2520detection%2520and%2520task-guided%250Aobject%2520selection.%2520Particularly%2520for%2520the%2520latter%252C%2520we%2520resort%2520to%2520the%2520recently%250Asuccessful%2520large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520as%2520our%2520backbone%252C%2520which%2520provides%250Arich%2520semantic%2520knowledge%2520and%2520a%2520uniform%2520embedding%2520space%2520for%2520images%2520and%2520texts.%250ANevertheless%252C%2520the%2520naive%2520application%2520of%2520VLMs%2520leads%2520to%2520sub-optimal%2520quality%252C%2520due%250Ato%2520the%2520misalignment%2520between%2520embeddings%2520of%2520object%2520images%2520and%2520their%2520visual%250Aattributes%252C%2520which%2520are%2520mainly%2520adjective%2520phrases.%2520To%2520this%2520end%252C%2520we%2520design%2520a%250Atransformer-based%2520aligner%2520after%2520the%2520pre-trained%2520VLMs%2520to%2520re-calibrate%2520both%250Aembeddings.%2520Finally%252C%2520we%2520employ%2520a%2520trainable%2520score%2520function%2520to%2520post-process%2520the%250AVLM%2520matching%2520results%2520for%2520object%2520selection.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520TaskCLIP%2520outperforms%2520the%2520state-of-the-art%2520DETR-based%2520model%2520TOIST%2520by%250A3.5%2525%2520and%2520only%2520requires%2520a%2520single%2520NVIDIA%2520RTX%25204090%2520for%2520both%2520training%2520and%250Ainference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08108v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaskCLIP%3A%20Extend%20Large%20Vision-Language%20Model%20for%20Task%20Oriented%20Object%0A%20%20Detection&entry.906535625=Hanning%20Chen%20and%20Wenjun%20Huang%20and%20Yang%20Ni%20and%20Sanggeon%20Yun%20and%20Yezi%20Liu%20and%20Fei%20Wen%20and%20Alvaro%20Velasquez%20and%20Hugo%20Latapie%20and%20Mohsen%20Imani&entry.1292438233=%20%20Task-oriented%20object%20detection%20aims%20to%20find%20objects%20suitable%20for%0Aaccomplishing%20specific%20tasks.%20As%20a%20challenging%20task%2C%20it%20requires%20simultaneous%0Avisual%20data%20processing%20and%20reasoning%20under%20ambiguous%20semantics.%20Recent%0Asolutions%20are%20mainly%20all-in-one%20models.%20However%2C%20the%20object%20detection%20backbones%0Aare%20pre-trained%20without%20text%20supervision.%20Thus%2C%20to%20incorporate%20task%0Arequirements%2C%20their%20intricate%20models%20undergo%20extensive%20learning%20on%20a%20highly%0Aimbalanced%20and%20scarce%20dataset%2C%20resulting%20in%20capped%20performance%2C%20laborious%0Atraining%2C%20and%20poor%20generalizability.%20In%20contrast%2C%20we%20propose%20TaskCLIP%2C%20a%20more%0Anatural%20two-stage%20design%20composed%20of%20general%20object%20detection%20and%20task-guided%0Aobject%20selection.%20Particularly%20for%20the%20latter%2C%20we%20resort%20to%20the%20recently%0Asuccessful%20large%20Vision-Language%20Models%20%28VLMs%29%20as%20our%20backbone%2C%20which%20provides%0Arich%20semantic%20knowledge%20and%20a%20uniform%20embedding%20space%20for%20images%20and%20texts.%0ANevertheless%2C%20the%20naive%20application%20of%20VLMs%20leads%20to%20sub-optimal%20quality%2C%20due%0Ato%20the%20misalignment%20between%20embeddings%20of%20object%20images%20and%20their%20visual%0Aattributes%2C%20which%20are%20mainly%20adjective%20phrases.%20To%20this%20end%2C%20we%20design%20a%0Atransformer-based%20aligner%20after%20the%20pre-trained%20VLMs%20to%20re-calibrate%20both%0Aembeddings.%20Finally%2C%20we%20employ%20a%20trainable%20score%20function%20to%20post-process%20the%0AVLM%20matching%20results%20for%20object%20selection.%20Experimental%20results%20demonstrate%0Athat%20our%20TaskCLIP%20outperforms%20the%20state-of-the-art%20DETR-based%20model%20TOIST%20by%0A3.5%25%20and%20only%20requires%20a%20single%20NVIDIA%20RTX%204090%20for%20both%20training%20and%0Ainference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08108v2&entry.124074799=Read"},
{"title": "Geospecific View Generation -- Geometry-Context Aware High-resolution\n  Ground View Inference from Satellite Views", "author": "Ningli Xu and Rongjun Qin", "abstract": "  Predicting realistic ground views from satellite imagery in urban scenes is a\nchallenging task due to the significant view gaps between satellite and\nground-view images. We propose a novel pipeline to tackle this challenge, by\ngenerating geospecifc views that maximally respect the weak geometry and\ntexture from multi-view satellite images. Different from existing approaches\nthat hallucinate images from cues such as partial semantics or geometry from\noverhead satellite images, our method directly predicts ground-view images at\ngeolocation by using a comprehensive set of information from the satellite\nimage, resulting in ground-level images with a resolution boost at a factor of\nten or more. We leverage a novel building refinement method to reduce geometric\ndistortions in satellite data at ground level, which ensures the creation of\naccurate conditions for view synthesis using diffusion networks. Moreover, we\nproposed a novel geospecific prior, which prompts distribution learning of\ndiffusion models to respect image samples that are closer to the geolocation of\nthe predicted images. We demonstrate our pipeline is the first to generate\nclose-to-real and geospecific ground views merely based on satellite images.\n", "link": "http://arxiv.org/abs/2407.08061v3", "date": "2024-09-06", "relevancy": 3.0179, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6149}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6149}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geospecific%20View%20Generation%20--%20Geometry-Context%20Aware%20High-resolution%0A%20%20Ground%20View%20Inference%20from%20Satellite%20Views&body=Title%3A%20Geospecific%20View%20Generation%20--%20Geometry-Context%20Aware%20High-resolution%0A%20%20Ground%20View%20Inference%20from%20Satellite%20Views%0AAuthor%3A%20Ningli%20Xu%20and%20Rongjun%20Qin%0AAbstract%3A%20%20%20Predicting%20realistic%20ground%20views%20from%20satellite%20imagery%20in%20urban%20scenes%20is%20a%0Achallenging%20task%20due%20to%20the%20significant%20view%20gaps%20between%20satellite%20and%0Aground-view%20images.%20We%20propose%20a%20novel%20pipeline%20to%20tackle%20this%20challenge%2C%20by%0Agenerating%20geospecifc%20views%20that%20maximally%20respect%20the%20weak%20geometry%20and%0Atexture%20from%20multi-view%20satellite%20images.%20Different%20from%20existing%20approaches%0Athat%20hallucinate%20images%20from%20cues%20such%20as%20partial%20semantics%20or%20geometry%20from%0Aoverhead%20satellite%20images%2C%20our%20method%20directly%20predicts%20ground-view%20images%20at%0Ageolocation%20by%20using%20a%20comprehensive%20set%20of%20information%20from%20the%20satellite%0Aimage%2C%20resulting%20in%20ground-level%20images%20with%20a%20resolution%20boost%20at%20a%20factor%20of%0Aten%20or%20more.%20We%20leverage%20a%20novel%20building%20refinement%20method%20to%20reduce%20geometric%0Adistortions%20in%20satellite%20data%20at%20ground%20level%2C%20which%20ensures%20the%20creation%20of%0Aaccurate%20conditions%20for%20view%20synthesis%20using%20diffusion%20networks.%20Moreover%2C%20we%0Aproposed%20a%20novel%20geospecific%20prior%2C%20which%20prompts%20distribution%20learning%20of%0Adiffusion%20models%20to%20respect%20image%20samples%20that%20are%20closer%20to%20the%20geolocation%20of%0Athe%20predicted%20images.%20We%20demonstrate%20our%20pipeline%20is%20the%20first%20to%20generate%0Aclose-to-real%20and%20geospecific%20ground%20views%20merely%20based%20on%20satellite%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08061v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeospecific%2520View%2520Generation%2520--%2520Geometry-Context%2520Aware%2520High-resolution%250A%2520%2520Ground%2520View%2520Inference%2520from%2520Satellite%2520Views%26entry.906535625%3DNingli%2520Xu%2520and%2520Rongjun%2520Qin%26entry.1292438233%3D%2520%2520Predicting%2520realistic%2520ground%2520views%2520from%2520satellite%2520imagery%2520in%2520urban%2520scenes%2520is%2520a%250Achallenging%2520task%2520due%2520to%2520the%2520significant%2520view%2520gaps%2520between%2520satellite%2520and%250Aground-view%2520images.%2520We%2520propose%2520a%2520novel%2520pipeline%2520to%2520tackle%2520this%2520challenge%252C%2520by%250Agenerating%2520geospecifc%2520views%2520that%2520maximally%2520respect%2520the%2520weak%2520geometry%2520and%250Atexture%2520from%2520multi-view%2520satellite%2520images.%2520Different%2520from%2520existing%2520approaches%250Athat%2520hallucinate%2520images%2520from%2520cues%2520such%2520as%2520partial%2520semantics%2520or%2520geometry%2520from%250Aoverhead%2520satellite%2520images%252C%2520our%2520method%2520directly%2520predicts%2520ground-view%2520images%2520at%250Ageolocation%2520by%2520using%2520a%2520comprehensive%2520set%2520of%2520information%2520from%2520the%2520satellite%250Aimage%252C%2520resulting%2520in%2520ground-level%2520images%2520with%2520a%2520resolution%2520boost%2520at%2520a%2520factor%2520of%250Aten%2520or%2520more.%2520We%2520leverage%2520a%2520novel%2520building%2520refinement%2520method%2520to%2520reduce%2520geometric%250Adistortions%2520in%2520satellite%2520data%2520at%2520ground%2520level%252C%2520which%2520ensures%2520the%2520creation%2520of%250Aaccurate%2520conditions%2520for%2520view%2520synthesis%2520using%2520diffusion%2520networks.%2520Moreover%252C%2520we%250Aproposed%2520a%2520novel%2520geospecific%2520prior%252C%2520which%2520prompts%2520distribution%2520learning%2520of%250Adiffusion%2520models%2520to%2520respect%2520image%2520samples%2520that%2520are%2520closer%2520to%2520the%2520geolocation%2520of%250Athe%2520predicted%2520images.%2520We%2520demonstrate%2520our%2520pipeline%2520is%2520the%2520first%2520to%2520generate%250Aclose-to-real%2520and%2520geospecific%2520ground%2520views%2520merely%2520based%2520on%2520satellite%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08061v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geospecific%20View%20Generation%20--%20Geometry-Context%20Aware%20High-resolution%0A%20%20Ground%20View%20Inference%20from%20Satellite%20Views&entry.906535625=Ningli%20Xu%20and%20Rongjun%20Qin&entry.1292438233=%20%20Predicting%20realistic%20ground%20views%20from%20satellite%20imagery%20in%20urban%20scenes%20is%20a%0Achallenging%20task%20due%20to%20the%20significant%20view%20gaps%20between%20satellite%20and%0Aground-view%20images.%20We%20propose%20a%20novel%20pipeline%20to%20tackle%20this%20challenge%2C%20by%0Agenerating%20geospecifc%20views%20that%20maximally%20respect%20the%20weak%20geometry%20and%0Atexture%20from%20multi-view%20satellite%20images.%20Different%20from%20existing%20approaches%0Athat%20hallucinate%20images%20from%20cues%20such%20as%20partial%20semantics%20or%20geometry%20from%0Aoverhead%20satellite%20images%2C%20our%20method%20directly%20predicts%20ground-view%20images%20at%0Ageolocation%20by%20using%20a%20comprehensive%20set%20of%20information%20from%20the%20satellite%0Aimage%2C%20resulting%20in%20ground-level%20images%20with%20a%20resolution%20boost%20at%20a%20factor%20of%0Aten%20or%20more.%20We%20leverage%20a%20novel%20building%20refinement%20method%20to%20reduce%20geometric%0Adistortions%20in%20satellite%20data%20at%20ground%20level%2C%20which%20ensures%20the%20creation%20of%0Aaccurate%20conditions%20for%20view%20synthesis%20using%20diffusion%20networks.%20Moreover%2C%20we%0Aproposed%20a%20novel%20geospecific%20prior%2C%20which%20prompts%20distribution%20learning%20of%0Adiffusion%20models%20to%20respect%20image%20samples%20that%20are%20closer%20to%20the%20geolocation%20of%0Athe%20predicted%20images.%20We%20demonstrate%20our%20pipeline%20is%20the%20first%20to%20generate%0Aclose-to-real%20and%20geospecific%20ground%20views%20merely%20based%20on%20satellite%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08061v3&entry.124074799=Read"},
{"title": "VILA-U: a Unified Foundation Model Integrating Visual Understanding and\n  Generation", "author": "Yecheng Wu and Zhuoyang Zhang and Junyu Chen and Haotian Tang and Dacheng Li and Yunhao Fang and Ligeng Zhu and Enze Xie and Hongxu Yin and Li Yi and Song Han and Yao Lu", "abstract": "  VILA-U is a Unified foundation model that integrates Video, Image, Language\nunderstanding and generation. Traditional visual language models (VLMs) use\nseparate modules for understanding and generating visual content, which can\nlead to misalignment and increased complexity. In contrast, VILA-U employs a\nsingle autoregressive next-token prediction framework for both tasks,\neliminating the need for additional components like diffusion models. This\napproach not only simplifies the model but also achieves near state-of-the-art\nperformance in visual language understanding and generation. The success of\nVILA-U is attributed to two main factors: the unified vision tower that aligns\ndiscrete visual tokens with textual inputs during pretraining, which enhances\nvisual perception, and autoregressive image generation can achieve similar\nquality as diffusion models with high-quality dataset. This allows VILA-U to\nperform comparably to more complex models using a fully token-based\nautoregressive framework.\n", "link": "http://arxiv.org/abs/2409.04429v1", "date": "2024-09-06", "relevancy": 2.9466, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.587}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VILA-U%3A%20a%20Unified%20Foundation%20Model%20Integrating%20Visual%20Understanding%20and%0A%20%20Generation&body=Title%3A%20VILA-U%3A%20a%20Unified%20Foundation%20Model%20Integrating%20Visual%20Understanding%20and%0A%20%20Generation%0AAuthor%3A%20Yecheng%20Wu%20and%20Zhuoyang%20Zhang%20and%20Junyu%20Chen%20and%20Haotian%20Tang%20and%20Dacheng%20Li%20and%20Yunhao%20Fang%20and%20Ligeng%20Zhu%20and%20Enze%20Xie%20and%20Hongxu%20Yin%20and%20Li%20Yi%20and%20Song%20Han%20and%20Yao%20Lu%0AAbstract%3A%20%20%20VILA-U%20is%20a%20Unified%20foundation%20model%20that%20integrates%20Video%2C%20Image%2C%20Language%0Aunderstanding%20and%20generation.%20Traditional%20visual%20language%20models%20%28VLMs%29%20use%0Aseparate%20modules%20for%20understanding%20and%20generating%20visual%20content%2C%20which%20can%0Alead%20to%20misalignment%20and%20increased%20complexity.%20In%20contrast%2C%20VILA-U%20employs%20a%0Asingle%20autoregressive%20next-token%20prediction%20framework%20for%20both%20tasks%2C%0Aeliminating%20the%20need%20for%20additional%20components%20like%20diffusion%20models.%20This%0Aapproach%20not%20only%20simplifies%20the%20model%20but%20also%20achieves%20near%20state-of-the-art%0Aperformance%20in%20visual%20language%20understanding%20and%20generation.%20The%20success%20of%0AVILA-U%20is%20attributed%20to%20two%20main%20factors%3A%20the%20unified%20vision%20tower%20that%20aligns%0Adiscrete%20visual%20tokens%20with%20textual%20inputs%20during%20pretraining%2C%20which%20enhances%0Avisual%20perception%2C%20and%20autoregressive%20image%20generation%20can%20achieve%20similar%0Aquality%20as%20diffusion%20models%20with%20high-quality%20dataset.%20This%20allows%20VILA-U%20to%0Aperform%20comparably%20to%20more%20complex%20models%20using%20a%20fully%20token-based%0Aautoregressive%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVILA-U%253A%2520a%2520Unified%2520Foundation%2520Model%2520Integrating%2520Visual%2520Understanding%2520and%250A%2520%2520Generation%26entry.906535625%3DYecheng%2520Wu%2520and%2520Zhuoyang%2520Zhang%2520and%2520Junyu%2520Chen%2520and%2520Haotian%2520Tang%2520and%2520Dacheng%2520Li%2520and%2520Yunhao%2520Fang%2520and%2520Ligeng%2520Zhu%2520and%2520Enze%2520Xie%2520and%2520Hongxu%2520Yin%2520and%2520Li%2520Yi%2520and%2520Song%2520Han%2520and%2520Yao%2520Lu%26entry.1292438233%3D%2520%2520VILA-U%2520is%2520a%2520Unified%2520foundation%2520model%2520that%2520integrates%2520Video%252C%2520Image%252C%2520Language%250Aunderstanding%2520and%2520generation.%2520Traditional%2520visual%2520language%2520models%2520%2528VLMs%2529%2520use%250Aseparate%2520modules%2520for%2520understanding%2520and%2520generating%2520visual%2520content%252C%2520which%2520can%250Alead%2520to%2520misalignment%2520and%2520increased%2520complexity.%2520In%2520contrast%252C%2520VILA-U%2520employs%2520a%250Asingle%2520autoregressive%2520next-token%2520prediction%2520framework%2520for%2520both%2520tasks%252C%250Aeliminating%2520the%2520need%2520for%2520additional%2520components%2520like%2520diffusion%2520models.%2520This%250Aapproach%2520not%2520only%2520simplifies%2520the%2520model%2520but%2520also%2520achieves%2520near%2520state-of-the-art%250Aperformance%2520in%2520visual%2520language%2520understanding%2520and%2520generation.%2520The%2520success%2520of%250AVILA-U%2520is%2520attributed%2520to%2520two%2520main%2520factors%253A%2520the%2520unified%2520vision%2520tower%2520that%2520aligns%250Adiscrete%2520visual%2520tokens%2520with%2520textual%2520inputs%2520during%2520pretraining%252C%2520which%2520enhances%250Avisual%2520perception%252C%2520and%2520autoregressive%2520image%2520generation%2520can%2520achieve%2520similar%250Aquality%2520as%2520diffusion%2520models%2520with%2520high-quality%2520dataset.%2520This%2520allows%2520VILA-U%2520to%250Aperform%2520comparably%2520to%2520more%2520complex%2520models%2520using%2520a%2520fully%2520token-based%250Aautoregressive%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VILA-U%3A%20a%20Unified%20Foundation%20Model%20Integrating%20Visual%20Understanding%20and%0A%20%20Generation&entry.906535625=Yecheng%20Wu%20and%20Zhuoyang%20Zhang%20and%20Junyu%20Chen%20and%20Haotian%20Tang%20and%20Dacheng%20Li%20and%20Yunhao%20Fang%20and%20Ligeng%20Zhu%20and%20Enze%20Xie%20and%20Hongxu%20Yin%20and%20Li%20Yi%20and%20Song%20Han%20and%20Yao%20Lu&entry.1292438233=%20%20VILA-U%20is%20a%20Unified%20foundation%20model%20that%20integrates%20Video%2C%20Image%2C%20Language%0Aunderstanding%20and%20generation.%20Traditional%20visual%20language%20models%20%28VLMs%29%20use%0Aseparate%20modules%20for%20understanding%20and%20generating%20visual%20content%2C%20which%20can%0Alead%20to%20misalignment%20and%20increased%20complexity.%20In%20contrast%2C%20VILA-U%20employs%20a%0Asingle%20autoregressive%20next-token%20prediction%20framework%20for%20both%20tasks%2C%0Aeliminating%20the%20need%20for%20additional%20components%20like%20diffusion%20models.%20This%0Aapproach%20not%20only%20simplifies%20the%20model%20but%20also%20achieves%20near%20state-of-the-art%0Aperformance%20in%20visual%20language%20understanding%20and%20generation.%20The%20success%20of%0AVILA-U%20is%20attributed%20to%20two%20main%20factors%3A%20the%20unified%20vision%20tower%20that%20aligns%0Adiscrete%20visual%20tokens%20with%20textual%20inputs%20during%20pretraining%2C%20which%20enhances%0Avisual%20perception%2C%20and%20autoregressive%20image%20generation%20can%20achieve%20similar%0Aquality%20as%20diffusion%20models%20with%20high-quality%20dataset.%20This%20allows%20VILA-U%20to%0Aperform%20comparably%20to%20more%20complex%20models%20using%20a%20fully%20token-based%0Aautoregressive%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04429v1&entry.124074799=Read"},
{"title": "AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation", "author": "Zanlin Ni and Yulin Wang and Renping Zhou and Rui Lu and Jiayi Guo and Jinyi Hu and Zhiyuan Liu and Yuan Yao and Gao Huang", "abstract": "  Recent studies have demonstrated the effectiveness of token-based methods for\nvisual content generation. As a representative work, non-autoregressive\nTransformers (NATs) are able to synthesize images with decent quality in a\nsmall number of steps. However, NATs usually necessitate configuring a\ncomplicated generation policy comprising multiple manually-designed scheduling\nrules. These heuristic-driven rules are prone to sub-optimality and come with\nthe requirements of expert knowledge and labor-intensive efforts. Moreover,\ntheir one-size-fits-all nature cannot flexibly adapt to the diverse\ncharacteristics of each individual sample. To address these issues, we propose\nAdaNAT, a learnable approach that automatically configures a suitable policy\ntailored for every sample to be generated. In specific, we formulate the\ndetermination of generation policies as a Markov decision process. Under this\nframework, a lightweight policy network for generation can be learned via\nreinforcement learning. Importantly, we demonstrate that simple reward designs\nsuch as FID or pre-trained reward models, may not reliably guarantee the\ndesired quality or diversity of generated samples. Therefore, we propose an\nadversarial reward design to guide the training of policy networks effectively.\nComprehensive experiments on four benchmark datasets, i.e., ImageNet-256 & 512,\nMS-COCO, and CC3M, validate the effectiveness of AdaNAT. Code and pre-trained\nmodels will be released at https://github.com/LeapLabTHU/AdaNAT.\n", "link": "http://arxiv.org/abs/2409.00342v2", "date": "2024-09-06", "relevancy": 2.9285, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6183}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5731}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaNAT%3A%20Exploring%20Adaptive%20Policy%20for%20Token-Based%20Image%20Generation&body=Title%3A%20AdaNAT%3A%20Exploring%20Adaptive%20Policy%20for%20Token-Based%20Image%20Generation%0AAuthor%3A%20Zanlin%20Ni%20and%20Yulin%20Wang%20and%20Renping%20Zhou%20and%20Rui%20Lu%20and%20Jiayi%20Guo%20and%20Jinyi%20Hu%20and%20Zhiyuan%20Liu%20and%20Yuan%20Yao%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Recent%20studies%20have%20demonstrated%20the%20effectiveness%20of%20token-based%20methods%20for%0Avisual%20content%20generation.%20As%20a%20representative%20work%2C%20non-autoregressive%0ATransformers%20%28NATs%29%20are%20able%20to%20synthesize%20images%20with%20decent%20quality%20in%20a%0Asmall%20number%20of%20steps.%20However%2C%20NATs%20usually%20necessitate%20configuring%20a%0Acomplicated%20generation%20policy%20comprising%20multiple%20manually-designed%20scheduling%0Arules.%20These%20heuristic-driven%20rules%20are%20prone%20to%20sub-optimality%20and%20come%20with%0Athe%20requirements%20of%20expert%20knowledge%20and%20labor-intensive%20efforts.%20Moreover%2C%0Atheir%20one-size-fits-all%20nature%20cannot%20flexibly%20adapt%20to%20the%20diverse%0Acharacteristics%20of%20each%20individual%20sample.%20To%20address%20these%20issues%2C%20we%20propose%0AAdaNAT%2C%20a%20learnable%20approach%20that%20automatically%20configures%20a%20suitable%20policy%0Atailored%20for%20every%20sample%20to%20be%20generated.%20In%20specific%2C%20we%20formulate%20the%0Adetermination%20of%20generation%20policies%20as%20a%20Markov%20decision%20process.%20Under%20this%0Aframework%2C%20a%20lightweight%20policy%20network%20for%20generation%20can%20be%20learned%20via%0Areinforcement%20learning.%20Importantly%2C%20we%20demonstrate%20that%20simple%20reward%20designs%0Asuch%20as%20FID%20or%20pre-trained%20reward%20models%2C%20may%20not%20reliably%20guarantee%20the%0Adesired%20quality%20or%20diversity%20of%20generated%20samples.%20Therefore%2C%20we%20propose%20an%0Aadversarial%20reward%20design%20to%20guide%20the%20training%20of%20policy%20networks%20effectively.%0AComprehensive%20experiments%20on%20four%20benchmark%20datasets%2C%20i.e.%2C%20ImageNet-256%20%26%20512%2C%0AMS-COCO%2C%20and%20CC3M%2C%20validate%20the%20effectiveness%20of%20AdaNAT.%20Code%20and%20pre-trained%0Amodels%20will%20be%20released%20at%20https%3A//github.com/LeapLabTHU/AdaNAT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaNAT%253A%2520Exploring%2520Adaptive%2520Policy%2520for%2520Token-Based%2520Image%2520Generation%26entry.906535625%3DZanlin%2520Ni%2520and%2520Yulin%2520Wang%2520and%2520Renping%2520Zhou%2520and%2520Rui%2520Lu%2520and%2520Jiayi%2520Guo%2520and%2520Jinyi%2520Hu%2520and%2520Zhiyuan%2520Liu%2520and%2520Yuan%2520Yao%2520and%2520Gao%2520Huang%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520demonstrated%2520the%2520effectiveness%2520of%2520token-based%2520methods%2520for%250Avisual%2520content%2520generation.%2520As%2520a%2520representative%2520work%252C%2520non-autoregressive%250ATransformers%2520%2528NATs%2529%2520are%2520able%2520to%2520synthesize%2520images%2520with%2520decent%2520quality%2520in%2520a%250Asmall%2520number%2520of%2520steps.%2520However%252C%2520NATs%2520usually%2520necessitate%2520configuring%2520a%250Acomplicated%2520generation%2520policy%2520comprising%2520multiple%2520manually-designed%2520scheduling%250Arules.%2520These%2520heuristic-driven%2520rules%2520are%2520prone%2520to%2520sub-optimality%2520and%2520come%2520with%250Athe%2520requirements%2520of%2520expert%2520knowledge%2520and%2520labor-intensive%2520efforts.%2520Moreover%252C%250Atheir%2520one-size-fits-all%2520nature%2520cannot%2520flexibly%2520adapt%2520to%2520the%2520diverse%250Acharacteristics%2520of%2520each%2520individual%2520sample.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250AAdaNAT%252C%2520a%2520learnable%2520approach%2520that%2520automatically%2520configures%2520a%2520suitable%2520policy%250Atailored%2520for%2520every%2520sample%2520to%2520be%2520generated.%2520In%2520specific%252C%2520we%2520formulate%2520the%250Adetermination%2520of%2520generation%2520policies%2520as%2520a%2520Markov%2520decision%2520process.%2520Under%2520this%250Aframework%252C%2520a%2520lightweight%2520policy%2520network%2520for%2520generation%2520can%2520be%2520learned%2520via%250Areinforcement%2520learning.%2520Importantly%252C%2520we%2520demonstrate%2520that%2520simple%2520reward%2520designs%250Asuch%2520as%2520FID%2520or%2520pre-trained%2520reward%2520models%252C%2520may%2520not%2520reliably%2520guarantee%2520the%250Adesired%2520quality%2520or%2520diversity%2520of%2520generated%2520samples.%2520Therefore%252C%2520we%2520propose%2520an%250Aadversarial%2520reward%2520design%2520to%2520guide%2520the%2520training%2520of%2520policy%2520networks%2520effectively.%250AComprehensive%2520experiments%2520on%2520four%2520benchmark%2520datasets%252C%2520i.e.%252C%2520ImageNet-256%2520%2526%2520512%252C%250AMS-COCO%252C%2520and%2520CC3M%252C%2520validate%2520the%2520effectiveness%2520of%2520AdaNAT.%2520Code%2520and%2520pre-trained%250Amodels%2520will%2520be%2520released%2520at%2520https%253A//github.com/LeapLabTHU/AdaNAT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaNAT%3A%20Exploring%20Adaptive%20Policy%20for%20Token-Based%20Image%20Generation&entry.906535625=Zanlin%20Ni%20and%20Yulin%20Wang%20and%20Renping%20Zhou%20and%20Rui%20Lu%20and%20Jiayi%20Guo%20and%20Jinyi%20Hu%20and%20Zhiyuan%20Liu%20and%20Yuan%20Yao%20and%20Gao%20Huang&entry.1292438233=%20%20Recent%20studies%20have%20demonstrated%20the%20effectiveness%20of%20token-based%20methods%20for%0Avisual%20content%20generation.%20As%20a%20representative%20work%2C%20non-autoregressive%0ATransformers%20%28NATs%29%20are%20able%20to%20synthesize%20images%20with%20decent%20quality%20in%20a%0Asmall%20number%20of%20steps.%20However%2C%20NATs%20usually%20necessitate%20configuring%20a%0Acomplicated%20generation%20policy%20comprising%20multiple%20manually-designed%20scheduling%0Arules.%20These%20heuristic-driven%20rules%20are%20prone%20to%20sub-optimality%20and%20come%20with%0Athe%20requirements%20of%20expert%20knowledge%20and%20labor-intensive%20efforts.%20Moreover%2C%0Atheir%20one-size-fits-all%20nature%20cannot%20flexibly%20adapt%20to%20the%20diverse%0Acharacteristics%20of%20each%20individual%20sample.%20To%20address%20these%20issues%2C%20we%20propose%0AAdaNAT%2C%20a%20learnable%20approach%20that%20automatically%20configures%20a%20suitable%20policy%0Atailored%20for%20every%20sample%20to%20be%20generated.%20In%20specific%2C%20we%20formulate%20the%0Adetermination%20of%20generation%20policies%20as%20a%20Markov%20decision%20process.%20Under%20this%0Aframework%2C%20a%20lightweight%20policy%20network%20for%20generation%20can%20be%20learned%20via%0Areinforcement%20learning.%20Importantly%2C%20we%20demonstrate%20that%20simple%20reward%20designs%0Asuch%20as%20FID%20or%20pre-trained%20reward%20models%2C%20may%20not%20reliably%20guarantee%20the%0Adesired%20quality%20or%20diversity%20of%20generated%20samples.%20Therefore%2C%20we%20propose%20an%0Aadversarial%20reward%20design%20to%20guide%20the%20training%20of%20policy%20networks%20effectively.%0AComprehensive%20experiments%20on%20four%20benchmark%20datasets%2C%20i.e.%2C%20ImageNet-256%20%26%20512%2C%0AMS-COCO%2C%20and%20CC3M%2C%20validate%20the%20effectiveness%20of%20AdaNAT.%20Code%20and%20pre-trained%0Amodels%20will%20be%20released%20at%20https%3A//github.com/LeapLabTHU/AdaNAT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00342v2&entry.124074799=Read"},
{"title": "Reprojection Errors as Prompts for Efficient Scene Coordinate Regression", "author": "Ting-Ru Liu and Hsuan-Kung Yang and Jou-Min Liu and Chun-Wei Huang and Tsung-Chih Chiang and Quan Kong and Norimasa Kobori and Chun-Yi Lee", "abstract": "  Scene coordinate regression (SCR) methods have emerged as a promising area of\nresearch due to their potential for accurate visual localization. However, many\nexisting SCR approaches train on samples from all image regions, including\ndynamic objects and texture-less areas. Utilizing these areas for optimization\nduring training can potentially hamper the overall performance and efficiency\nof the model. In this study, we first perform an in-depth analysis to validate\nthe adverse impacts of these areas. Drawing inspiration from our analysis, we\nthen introduce an error-guided feature selection (EGFS) mechanism, in tandem\nwith the use of the Segment Anything Model (SAM). This mechanism seeds low\nreprojection areas as prompts and expands them into error-guided masks, and\nthen utilizes these masks to sample points and filter out problematic areas in\nan iterative manner. The experiments demonstrate that our method outperforms\nexisting SCR approaches that do not rely on 3D information on the Cambridge\nLandmarks and Indoor6 datasets.\n", "link": "http://arxiv.org/abs/2409.04178v1", "date": "2024-09-06", "relevancy": 2.9069, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reprojection%20Errors%20as%20Prompts%20for%20Efficient%20Scene%20Coordinate%20Regression&body=Title%3A%20Reprojection%20Errors%20as%20Prompts%20for%20Efficient%20Scene%20Coordinate%20Regression%0AAuthor%3A%20Ting-Ru%20Liu%20and%20Hsuan-Kung%20Yang%20and%20Jou-Min%20Liu%20and%20Chun-Wei%20Huang%20and%20Tsung-Chih%20Chiang%20and%20Quan%20Kong%20and%20Norimasa%20Kobori%20and%20Chun-Yi%20Lee%0AAbstract%3A%20%20%20Scene%20coordinate%20regression%20%28SCR%29%20methods%20have%20emerged%20as%20a%20promising%20area%20of%0Aresearch%20due%20to%20their%20potential%20for%20accurate%20visual%20localization.%20However%2C%20many%0Aexisting%20SCR%20approaches%20train%20on%20samples%20from%20all%20image%20regions%2C%20including%0Adynamic%20objects%20and%20texture-less%20areas.%20Utilizing%20these%20areas%20for%20optimization%0Aduring%20training%20can%20potentially%20hamper%20the%20overall%20performance%20and%20efficiency%0Aof%20the%20model.%20In%20this%20study%2C%20we%20first%20perform%20an%20in-depth%20analysis%20to%20validate%0Athe%20adverse%20impacts%20of%20these%20areas.%20Drawing%20inspiration%20from%20our%20analysis%2C%20we%0Athen%20introduce%20an%20error-guided%20feature%20selection%20%28EGFS%29%20mechanism%2C%20in%20tandem%0Awith%20the%20use%20of%20the%20Segment%20Anything%20Model%20%28SAM%29.%20This%20mechanism%20seeds%20low%0Areprojection%20areas%20as%20prompts%20and%20expands%20them%20into%20error-guided%20masks%2C%20and%0Athen%20utilizes%20these%20masks%20to%20sample%20points%20and%20filter%20out%20problematic%20areas%20in%0Aan%20iterative%20manner.%20The%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Aexisting%20SCR%20approaches%20that%20do%20not%20rely%20on%203D%20information%20on%20the%20Cambridge%0ALandmarks%20and%20Indoor6%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReprojection%2520Errors%2520as%2520Prompts%2520for%2520Efficient%2520Scene%2520Coordinate%2520Regression%26entry.906535625%3DTing-Ru%2520Liu%2520and%2520Hsuan-Kung%2520Yang%2520and%2520Jou-Min%2520Liu%2520and%2520Chun-Wei%2520Huang%2520and%2520Tsung-Chih%2520Chiang%2520and%2520Quan%2520Kong%2520and%2520Norimasa%2520Kobori%2520and%2520Chun-Yi%2520Lee%26entry.1292438233%3D%2520%2520Scene%2520coordinate%2520regression%2520%2528SCR%2529%2520methods%2520have%2520emerged%2520as%2520a%2520promising%2520area%2520of%250Aresearch%2520due%2520to%2520their%2520potential%2520for%2520accurate%2520visual%2520localization.%2520However%252C%2520many%250Aexisting%2520SCR%2520approaches%2520train%2520on%2520samples%2520from%2520all%2520image%2520regions%252C%2520including%250Adynamic%2520objects%2520and%2520texture-less%2520areas.%2520Utilizing%2520these%2520areas%2520for%2520optimization%250Aduring%2520training%2520can%2520potentially%2520hamper%2520the%2520overall%2520performance%2520and%2520efficiency%250Aof%2520the%2520model.%2520In%2520this%2520study%252C%2520we%2520first%2520perform%2520an%2520in-depth%2520analysis%2520to%2520validate%250Athe%2520adverse%2520impacts%2520of%2520these%2520areas.%2520Drawing%2520inspiration%2520from%2520our%2520analysis%252C%2520we%250Athen%2520introduce%2520an%2520error-guided%2520feature%2520selection%2520%2528EGFS%2529%2520mechanism%252C%2520in%2520tandem%250Awith%2520the%2520use%2520of%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529.%2520This%2520mechanism%2520seeds%2520low%250Areprojection%2520areas%2520as%2520prompts%2520and%2520expands%2520them%2520into%2520error-guided%2520masks%252C%2520and%250Athen%2520utilizes%2520these%2520masks%2520to%2520sample%2520points%2520and%2520filter%2520out%2520problematic%2520areas%2520in%250Aan%2520iterative%2520manner.%2520The%2520experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Aexisting%2520SCR%2520approaches%2520that%2520do%2520not%2520rely%2520on%25203D%2520information%2520on%2520the%2520Cambridge%250ALandmarks%2520and%2520Indoor6%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reprojection%20Errors%20as%20Prompts%20for%20Efficient%20Scene%20Coordinate%20Regression&entry.906535625=Ting-Ru%20Liu%20and%20Hsuan-Kung%20Yang%20and%20Jou-Min%20Liu%20and%20Chun-Wei%20Huang%20and%20Tsung-Chih%20Chiang%20and%20Quan%20Kong%20and%20Norimasa%20Kobori%20and%20Chun-Yi%20Lee&entry.1292438233=%20%20Scene%20coordinate%20regression%20%28SCR%29%20methods%20have%20emerged%20as%20a%20promising%20area%20of%0Aresearch%20due%20to%20their%20potential%20for%20accurate%20visual%20localization.%20However%2C%20many%0Aexisting%20SCR%20approaches%20train%20on%20samples%20from%20all%20image%20regions%2C%20including%0Adynamic%20objects%20and%20texture-less%20areas.%20Utilizing%20these%20areas%20for%20optimization%0Aduring%20training%20can%20potentially%20hamper%20the%20overall%20performance%20and%20efficiency%0Aof%20the%20model.%20In%20this%20study%2C%20we%20first%20perform%20an%20in-depth%20analysis%20to%20validate%0Athe%20adverse%20impacts%20of%20these%20areas.%20Drawing%20inspiration%20from%20our%20analysis%2C%20we%0Athen%20introduce%20an%20error-guided%20feature%20selection%20%28EGFS%29%20mechanism%2C%20in%20tandem%0Awith%20the%20use%20of%20the%20Segment%20Anything%20Model%20%28SAM%29.%20This%20mechanism%20seeds%20low%0Areprojection%20areas%20as%20prompts%20and%20expands%20them%20into%20error-guided%20masks%2C%20and%0Athen%20utilizes%20these%20masks%20to%20sample%20points%20and%20filter%20out%20problematic%20areas%20in%0Aan%20iterative%20manner.%20The%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Aexisting%20SCR%20approaches%20that%20do%20not%20rely%20on%203D%20information%20on%20the%20Cambridge%0ALandmarks%20and%20Indoor6%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04178v1&entry.124074799=Read"},
{"title": "Diagram Formalization Enhanced Multi-Modal Geometry Problem Solver", "author": "Zeren Zhang and Jo-Ku Cheng and Jingyang Deng and Lu Tian and Jinwen Ma and Ziran Qin and Xiaokai Zhang and Na Zhu and Tuo Leng", "abstract": "  Mathematical reasoning remains an ongoing challenge for AI models, especially\nfor geometry problems that require both linguistic and visual signals. As the\nvision encoders of most MLLMs are trained on natural scenes, they often\nstruggle to understand geometric diagrams, performing no better in geometry\nproblem solving than LLMs that only process text. This limitation is amplified\nby the lack of effective methods for representing geometric relationships. To\naddress these issues, we introduce the Diagram Formalization Enhanced Geometry\nProblem Solver (DFE-GPS), a new framework that integrates visual features,\ngeometric formal language, and natural language representations. We propose a\nnovel synthetic data approach and create a large-scale geometric dataset,\nSynthGeo228K, annotated with both formal and natural language captions,\ndesigned to enhance the vision encoder for a better understanding of geometric\nstructures. Our framework improves MLLMs' ability to process geometric diagrams\nand extends their application to open-ended tasks on the formalgeo7k dataset.\n", "link": "http://arxiv.org/abs/2409.04214v1", "date": "2024-09-06", "relevancy": 2.8697, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diagram%20Formalization%20Enhanced%20Multi-Modal%20Geometry%20Problem%20Solver&body=Title%3A%20Diagram%20Formalization%20Enhanced%20Multi-Modal%20Geometry%20Problem%20Solver%0AAuthor%3A%20Zeren%20Zhang%20and%20Jo-Ku%20Cheng%20and%20Jingyang%20Deng%20and%20Lu%20Tian%20and%20Jinwen%20Ma%20and%20Ziran%20Qin%20and%20Xiaokai%20Zhang%20and%20Na%20Zhu%20and%20Tuo%20Leng%0AAbstract%3A%20%20%20Mathematical%20reasoning%20remains%20an%20ongoing%20challenge%20for%20AI%20models%2C%20especially%0Afor%20geometry%20problems%20that%20require%20both%20linguistic%20and%20visual%20signals.%20As%20the%0Avision%20encoders%20of%20most%20MLLMs%20are%20trained%20on%20natural%20scenes%2C%20they%20often%0Astruggle%20to%20understand%20geometric%20diagrams%2C%20performing%20no%20better%20in%20geometry%0Aproblem%20solving%20than%20LLMs%20that%20only%20process%20text.%20This%20limitation%20is%20amplified%0Aby%20the%20lack%20of%20effective%20methods%20for%20representing%20geometric%20relationships.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20the%20Diagram%20Formalization%20Enhanced%20Geometry%0AProblem%20Solver%20%28DFE-GPS%29%2C%20a%20new%20framework%20that%20integrates%20visual%20features%2C%0Ageometric%20formal%20language%2C%20and%20natural%20language%20representations.%20We%20propose%20a%0Anovel%20synthetic%20data%20approach%20and%20create%20a%20large-scale%20geometric%20dataset%2C%0ASynthGeo228K%2C%20annotated%20with%20both%20formal%20and%20natural%20language%20captions%2C%0Adesigned%20to%20enhance%20the%20vision%20encoder%20for%20a%20better%20understanding%20of%20geometric%0Astructures.%20Our%20framework%20improves%20MLLMs%27%20ability%20to%20process%20geometric%20diagrams%0Aand%20extends%20their%20application%20to%20open-ended%20tasks%20on%20the%20formalgeo7k%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagram%2520Formalization%2520Enhanced%2520Multi-Modal%2520Geometry%2520Problem%2520Solver%26entry.906535625%3DZeren%2520Zhang%2520and%2520Jo-Ku%2520Cheng%2520and%2520Jingyang%2520Deng%2520and%2520Lu%2520Tian%2520and%2520Jinwen%2520Ma%2520and%2520Ziran%2520Qin%2520and%2520Xiaokai%2520Zhang%2520and%2520Na%2520Zhu%2520and%2520Tuo%2520Leng%26entry.1292438233%3D%2520%2520Mathematical%2520reasoning%2520remains%2520an%2520ongoing%2520challenge%2520for%2520AI%2520models%252C%2520especially%250Afor%2520geometry%2520problems%2520that%2520require%2520both%2520linguistic%2520and%2520visual%2520signals.%2520As%2520the%250Avision%2520encoders%2520of%2520most%2520MLLMs%2520are%2520trained%2520on%2520natural%2520scenes%252C%2520they%2520often%250Astruggle%2520to%2520understand%2520geometric%2520diagrams%252C%2520performing%2520no%2520better%2520in%2520geometry%250Aproblem%2520solving%2520than%2520LLMs%2520that%2520only%2520process%2520text.%2520This%2520limitation%2520is%2520amplified%250Aby%2520the%2520lack%2520of%2520effective%2520methods%2520for%2520representing%2520geometric%2520relationships.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520introduce%2520the%2520Diagram%2520Formalization%2520Enhanced%2520Geometry%250AProblem%2520Solver%2520%2528DFE-GPS%2529%252C%2520a%2520new%2520framework%2520that%2520integrates%2520visual%2520features%252C%250Ageometric%2520formal%2520language%252C%2520and%2520natural%2520language%2520representations.%2520We%2520propose%2520a%250Anovel%2520synthetic%2520data%2520approach%2520and%2520create%2520a%2520large-scale%2520geometric%2520dataset%252C%250ASynthGeo228K%252C%2520annotated%2520with%2520both%2520formal%2520and%2520natural%2520language%2520captions%252C%250Adesigned%2520to%2520enhance%2520the%2520vision%2520encoder%2520for%2520a%2520better%2520understanding%2520of%2520geometric%250Astructures.%2520Our%2520framework%2520improves%2520MLLMs%2527%2520ability%2520to%2520process%2520geometric%2520diagrams%250Aand%2520extends%2520their%2520application%2520to%2520open-ended%2520tasks%2520on%2520the%2520formalgeo7k%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diagram%20Formalization%20Enhanced%20Multi-Modal%20Geometry%20Problem%20Solver&entry.906535625=Zeren%20Zhang%20and%20Jo-Ku%20Cheng%20and%20Jingyang%20Deng%20and%20Lu%20Tian%20and%20Jinwen%20Ma%20and%20Ziran%20Qin%20and%20Xiaokai%20Zhang%20and%20Na%20Zhu%20and%20Tuo%20Leng&entry.1292438233=%20%20Mathematical%20reasoning%20remains%20an%20ongoing%20challenge%20for%20AI%20models%2C%20especially%0Afor%20geometry%20problems%20that%20require%20both%20linguistic%20and%20visual%20signals.%20As%20the%0Avision%20encoders%20of%20most%20MLLMs%20are%20trained%20on%20natural%20scenes%2C%20they%20often%0Astruggle%20to%20understand%20geometric%20diagrams%2C%20performing%20no%20better%20in%20geometry%0Aproblem%20solving%20than%20LLMs%20that%20only%20process%20text.%20This%20limitation%20is%20amplified%0Aby%20the%20lack%20of%20effective%20methods%20for%20representing%20geometric%20relationships.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20the%20Diagram%20Formalization%20Enhanced%20Geometry%0AProblem%20Solver%20%28DFE-GPS%29%2C%20a%20new%20framework%20that%20integrates%20visual%20features%2C%0Ageometric%20formal%20language%2C%20and%20natural%20language%20representations.%20We%20propose%20a%0Anovel%20synthetic%20data%20approach%20and%20create%20a%20large-scale%20geometric%20dataset%2C%0ASynthGeo228K%2C%20annotated%20with%20both%20formal%20and%20natural%20language%20captions%2C%0Adesigned%20to%20enhance%20the%20vision%20encoder%20for%20a%20better%20understanding%20of%20geometric%0Astructures.%20Our%20framework%20improves%20MLLMs%27%20ability%20to%20process%20geometric%20diagrams%0Aand%20extends%20their%20application%20to%20open-ended%20tasks%20on%20the%20formalgeo7k%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04214v1&entry.124074799=Read"},
{"title": "RCNet: Deep Recurrent Collaborative Network for Multi-View Low-Light\n  Image Enhancement", "author": "Hao Luo and Baoliang Chen and Lingyu Zhu and Peilin Chen and Shiqi Wang", "abstract": "  Scene observation from multiple perspectives would bring a more comprehensive\nvisual experience. However, in the context of acquiring multiple views in the\ndark, the highly correlated views are seriously alienated, making it\nchallenging to improve scene understanding with auxiliary views. Recent single\nimage-based enhancement methods may not be able to provide consistently\ndesirable restoration performance for all views due to the ignorance of\npotential feature correspondence among different views. To alleviate this\nissue, we make the first attempt to investigate multi-view low-light image\nenhancement. First, we construct a new dataset called Multi-View Low-light\nTriplets (MVLT), including 1,860 pairs of triple images with large illumination\nranges and wide noise distribution. Each triplet is equipped with three\ndifferent viewpoints towards the same scene. Second, we propose a deep\nmulti-view enhancement framework based on the Recurrent Collaborative Network\n(RCNet). Specifically, in order to benefit from similar texture correspondence\nacross different views, we design the recurrent feature enhancement, alignment\nand fusion (ReEAF) module, in which intra-view feature enhancement (Intra-view\nEN) followed by inter-view feature alignment and fusion (Inter-view AF) is\nperformed to model the intra-view and inter-view feature propagation\nsequentially via multi-view collaboration. In addition, two different modules\nfrom enhancement to alignment (E2A) and from alignment to enhancement (A2E) are\ndeveloped to enable the interactions between Intra-view EN and Inter-view AF,\nwhich explicitly utilize attentive feature weighting and sampling for\nenhancement and alignment, respectively. Experimental results demonstrate that\nour RCNet significantly outperforms other state-of-the-art methods. All of our\ndataset, code, and model will be available at https://github.com/hluo29/RCNet.\n", "link": "http://arxiv.org/abs/2409.04363v1", "date": "2024-09-06", "relevancy": 2.8483, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5693}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RCNet%3A%20Deep%20Recurrent%20Collaborative%20Network%20for%20Multi-View%20Low-Light%0A%20%20Image%20Enhancement&body=Title%3A%20RCNet%3A%20Deep%20Recurrent%20Collaborative%20Network%20for%20Multi-View%20Low-Light%0A%20%20Image%20Enhancement%0AAuthor%3A%20Hao%20Luo%20and%20Baoliang%20Chen%20and%20Lingyu%20Zhu%20and%20Peilin%20Chen%20and%20Shiqi%20Wang%0AAbstract%3A%20%20%20Scene%20observation%20from%20multiple%20perspectives%20would%20bring%20a%20more%20comprehensive%0Avisual%20experience.%20However%2C%20in%20the%20context%20of%20acquiring%20multiple%20views%20in%20the%0Adark%2C%20the%20highly%20correlated%20views%20are%20seriously%20alienated%2C%20making%20it%0Achallenging%20to%20improve%20scene%20understanding%20with%20auxiliary%20views.%20Recent%20single%0Aimage-based%20enhancement%20methods%20may%20not%20be%20able%20to%20provide%20consistently%0Adesirable%20restoration%20performance%20for%20all%20views%20due%20to%20the%20ignorance%20of%0Apotential%20feature%20correspondence%20among%20different%20views.%20To%20alleviate%20this%0Aissue%2C%20we%20make%20the%20first%20attempt%20to%20investigate%20multi-view%20low-light%20image%0Aenhancement.%20First%2C%20we%20construct%20a%20new%20dataset%20called%20Multi-View%20Low-light%0ATriplets%20%28MVLT%29%2C%20including%201%2C860%20pairs%20of%20triple%20images%20with%20large%20illumination%0Aranges%20and%20wide%20noise%20distribution.%20Each%20triplet%20is%20equipped%20with%20three%0Adifferent%20viewpoints%20towards%20the%20same%20scene.%20Second%2C%20we%20propose%20a%20deep%0Amulti-view%20enhancement%20framework%20based%20on%20the%20Recurrent%20Collaborative%20Network%0A%28RCNet%29.%20Specifically%2C%20in%20order%20to%20benefit%20from%20similar%20texture%20correspondence%0Aacross%20different%20views%2C%20we%20design%20the%20recurrent%20feature%20enhancement%2C%20alignment%0Aand%20fusion%20%28ReEAF%29%20module%2C%20in%20which%20intra-view%20feature%20enhancement%20%28Intra-view%0AEN%29%20followed%20by%20inter-view%20feature%20alignment%20and%20fusion%20%28Inter-view%20AF%29%20is%0Aperformed%20to%20model%20the%20intra-view%20and%20inter-view%20feature%20propagation%0Asequentially%20via%20multi-view%20collaboration.%20In%20addition%2C%20two%20different%20modules%0Afrom%20enhancement%20to%20alignment%20%28E2A%29%20and%20from%20alignment%20to%20enhancement%20%28A2E%29%20are%0Adeveloped%20to%20enable%20the%20interactions%20between%20Intra-view%20EN%20and%20Inter-view%20AF%2C%0Awhich%20explicitly%20utilize%20attentive%20feature%20weighting%20and%20sampling%20for%0Aenhancement%20and%20alignment%2C%20respectively.%20Experimental%20results%20demonstrate%20that%0Aour%20RCNet%20significantly%20outperforms%20other%20state-of-the-art%20methods.%20All%20of%20our%0Adataset%2C%20code%2C%20and%20model%20will%20be%20available%20at%20https%3A//github.com/hluo29/RCNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRCNet%253A%2520Deep%2520Recurrent%2520Collaborative%2520Network%2520for%2520Multi-View%2520Low-Light%250A%2520%2520Image%2520Enhancement%26entry.906535625%3DHao%2520Luo%2520and%2520Baoliang%2520Chen%2520and%2520Lingyu%2520Zhu%2520and%2520Peilin%2520Chen%2520and%2520Shiqi%2520Wang%26entry.1292438233%3D%2520%2520Scene%2520observation%2520from%2520multiple%2520perspectives%2520would%2520bring%2520a%2520more%2520comprehensive%250Avisual%2520experience.%2520However%252C%2520in%2520the%2520context%2520of%2520acquiring%2520multiple%2520views%2520in%2520the%250Adark%252C%2520the%2520highly%2520correlated%2520views%2520are%2520seriously%2520alienated%252C%2520making%2520it%250Achallenging%2520to%2520improve%2520scene%2520understanding%2520with%2520auxiliary%2520views.%2520Recent%2520single%250Aimage-based%2520enhancement%2520methods%2520may%2520not%2520be%2520able%2520to%2520provide%2520consistently%250Adesirable%2520restoration%2520performance%2520for%2520all%2520views%2520due%2520to%2520the%2520ignorance%2520of%250Apotential%2520feature%2520correspondence%2520among%2520different%2520views.%2520To%2520alleviate%2520this%250Aissue%252C%2520we%2520make%2520the%2520first%2520attempt%2520to%2520investigate%2520multi-view%2520low-light%2520image%250Aenhancement.%2520First%252C%2520we%2520construct%2520a%2520new%2520dataset%2520called%2520Multi-View%2520Low-light%250ATriplets%2520%2528MVLT%2529%252C%2520including%25201%252C860%2520pairs%2520of%2520triple%2520images%2520with%2520large%2520illumination%250Aranges%2520and%2520wide%2520noise%2520distribution.%2520Each%2520triplet%2520is%2520equipped%2520with%2520three%250Adifferent%2520viewpoints%2520towards%2520the%2520same%2520scene.%2520Second%252C%2520we%2520propose%2520a%2520deep%250Amulti-view%2520enhancement%2520framework%2520based%2520on%2520the%2520Recurrent%2520Collaborative%2520Network%250A%2528RCNet%2529.%2520Specifically%252C%2520in%2520order%2520to%2520benefit%2520from%2520similar%2520texture%2520correspondence%250Aacross%2520different%2520views%252C%2520we%2520design%2520the%2520recurrent%2520feature%2520enhancement%252C%2520alignment%250Aand%2520fusion%2520%2528ReEAF%2529%2520module%252C%2520in%2520which%2520intra-view%2520feature%2520enhancement%2520%2528Intra-view%250AEN%2529%2520followed%2520by%2520inter-view%2520feature%2520alignment%2520and%2520fusion%2520%2528Inter-view%2520AF%2529%2520is%250Aperformed%2520to%2520model%2520the%2520intra-view%2520and%2520inter-view%2520feature%2520propagation%250Asequentially%2520via%2520multi-view%2520collaboration.%2520In%2520addition%252C%2520two%2520different%2520modules%250Afrom%2520enhancement%2520to%2520alignment%2520%2528E2A%2529%2520and%2520from%2520alignment%2520to%2520enhancement%2520%2528A2E%2529%2520are%250Adeveloped%2520to%2520enable%2520the%2520interactions%2520between%2520Intra-view%2520EN%2520and%2520Inter-view%2520AF%252C%250Awhich%2520explicitly%2520utilize%2520attentive%2520feature%2520weighting%2520and%2520sampling%2520for%250Aenhancement%2520and%2520alignment%252C%2520respectively.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520RCNet%2520significantly%2520outperforms%2520other%2520state-of-the-art%2520methods.%2520All%2520of%2520our%250Adataset%252C%2520code%252C%2520and%2520model%2520will%2520be%2520available%2520at%2520https%253A//github.com/hluo29/RCNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RCNet%3A%20Deep%20Recurrent%20Collaborative%20Network%20for%20Multi-View%20Low-Light%0A%20%20Image%20Enhancement&entry.906535625=Hao%20Luo%20and%20Baoliang%20Chen%20and%20Lingyu%20Zhu%20and%20Peilin%20Chen%20and%20Shiqi%20Wang&entry.1292438233=%20%20Scene%20observation%20from%20multiple%20perspectives%20would%20bring%20a%20more%20comprehensive%0Avisual%20experience.%20However%2C%20in%20the%20context%20of%20acquiring%20multiple%20views%20in%20the%0Adark%2C%20the%20highly%20correlated%20views%20are%20seriously%20alienated%2C%20making%20it%0Achallenging%20to%20improve%20scene%20understanding%20with%20auxiliary%20views.%20Recent%20single%0Aimage-based%20enhancement%20methods%20may%20not%20be%20able%20to%20provide%20consistently%0Adesirable%20restoration%20performance%20for%20all%20views%20due%20to%20the%20ignorance%20of%0Apotential%20feature%20correspondence%20among%20different%20views.%20To%20alleviate%20this%0Aissue%2C%20we%20make%20the%20first%20attempt%20to%20investigate%20multi-view%20low-light%20image%0Aenhancement.%20First%2C%20we%20construct%20a%20new%20dataset%20called%20Multi-View%20Low-light%0ATriplets%20%28MVLT%29%2C%20including%201%2C860%20pairs%20of%20triple%20images%20with%20large%20illumination%0Aranges%20and%20wide%20noise%20distribution.%20Each%20triplet%20is%20equipped%20with%20three%0Adifferent%20viewpoints%20towards%20the%20same%20scene.%20Second%2C%20we%20propose%20a%20deep%0Amulti-view%20enhancement%20framework%20based%20on%20the%20Recurrent%20Collaborative%20Network%0A%28RCNet%29.%20Specifically%2C%20in%20order%20to%20benefit%20from%20similar%20texture%20correspondence%0Aacross%20different%20views%2C%20we%20design%20the%20recurrent%20feature%20enhancement%2C%20alignment%0Aand%20fusion%20%28ReEAF%29%20module%2C%20in%20which%20intra-view%20feature%20enhancement%20%28Intra-view%0AEN%29%20followed%20by%20inter-view%20feature%20alignment%20and%20fusion%20%28Inter-view%20AF%29%20is%0Aperformed%20to%20model%20the%20intra-view%20and%20inter-view%20feature%20propagation%0Asequentially%20via%20multi-view%20collaboration.%20In%20addition%2C%20two%20different%20modules%0Afrom%20enhancement%20to%20alignment%20%28E2A%29%20and%20from%20alignment%20to%20enhancement%20%28A2E%29%20are%0Adeveloped%20to%20enable%20the%20interactions%20between%20Intra-view%20EN%20and%20Inter-view%20AF%2C%0Awhich%20explicitly%20utilize%20attentive%20feature%20weighting%20and%20sampling%20for%0Aenhancement%20and%20alignment%2C%20respectively.%20Experimental%20results%20demonstrate%20that%0Aour%20RCNet%20significantly%20outperforms%20other%20state-of-the-art%20methods.%20All%20of%20our%0Adataset%2C%20code%2C%20and%20model%20will%20be%20available%20at%20https%3A//github.com/hluo29/RCNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04363v1&entry.124074799=Read"},
{"title": "Connectivity-Inspired Network for Context-Aware Recognition", "author": "Gianluca Carloni and Sara Colantonio", "abstract": "  The aim of this paper is threefold. We inform the AI practitioner about the\nhuman visual system with an extensive literature review; we propose a novel\nbiologically motivated neural network for image classification; and, finally,\nwe present a new plug-and-play module to model context awareness. We focus on\nthe effect of incorporating circuit motifs found in biological brains to\naddress visual recognition. Our convolutional architecture is inspired by the\nconnectivity of human cortical and subcortical streams, and we implement\nbottom-up and top-down modulations that mimic the extensive afferent and\nefferent connections between visual and cognitive areas. Our Contextual\nAttention Block is simple and effective and can be integrated with any\nfeed-forward neural network. It infers weights that multiply the feature maps\naccording to their causal influence on the scene, modeling the co-occurrence of\ndifferent objects in the image. We place our module at different bottlenecks to\ninfuse a hierarchical context awareness into the model. We validated our\nproposals through image classification experiments on benchmark data and found\na consistent improvement in performance and the robustness of the produced\nexplanations via class activation. Our code is available at\nhttps://github.com/gianlucarloni/CoCoReco.\n", "link": "http://arxiv.org/abs/2409.04360v1", "date": "2024-09-06", "relevancy": 2.7161, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5456}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Connectivity-Inspired%20Network%20for%20Context-Aware%20Recognition&body=Title%3A%20Connectivity-Inspired%20Network%20for%20Context-Aware%20Recognition%0AAuthor%3A%20Gianluca%20Carloni%20and%20Sara%20Colantonio%0AAbstract%3A%20%20%20The%20aim%20of%20this%20paper%20is%20threefold.%20We%20inform%20the%20AI%20practitioner%20about%20the%0Ahuman%20visual%20system%20with%20an%20extensive%20literature%20review%3B%20we%20propose%20a%20novel%0Abiologically%20motivated%20neural%20network%20for%20image%20classification%3B%20and%2C%20finally%2C%0Awe%20present%20a%20new%20plug-and-play%20module%20to%20model%20context%20awareness.%20We%20focus%20on%0Athe%20effect%20of%20incorporating%20circuit%20motifs%20found%20in%20biological%20brains%20to%0Aaddress%20visual%20recognition.%20Our%20convolutional%20architecture%20is%20inspired%20by%20the%0Aconnectivity%20of%20human%20cortical%20and%20subcortical%20streams%2C%20and%20we%20implement%0Abottom-up%20and%20top-down%20modulations%20that%20mimic%20the%20extensive%20afferent%20and%0Aefferent%20connections%20between%20visual%20and%20cognitive%20areas.%20Our%20Contextual%0AAttention%20Block%20is%20simple%20and%20effective%20and%20can%20be%20integrated%20with%20any%0Afeed-forward%20neural%20network.%20It%20infers%20weights%20that%20multiply%20the%20feature%20maps%0Aaccording%20to%20their%20causal%20influence%20on%20the%20scene%2C%20modeling%20the%20co-occurrence%20of%0Adifferent%20objects%20in%20the%20image.%20We%20place%20our%20module%20at%20different%20bottlenecks%20to%0Ainfuse%20a%20hierarchical%20context%20awareness%20into%20the%20model.%20We%20validated%20our%0Aproposals%20through%20image%20classification%20experiments%20on%20benchmark%20data%20and%20found%0Aa%20consistent%20improvement%20in%20performance%20and%20the%20robustness%20of%20the%20produced%0Aexplanations%20via%20class%20activation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/gianlucarloni/CoCoReco.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConnectivity-Inspired%2520Network%2520for%2520Context-Aware%2520Recognition%26entry.906535625%3DGianluca%2520Carloni%2520and%2520Sara%2520Colantonio%26entry.1292438233%3D%2520%2520The%2520aim%2520of%2520this%2520paper%2520is%2520threefold.%2520We%2520inform%2520the%2520AI%2520practitioner%2520about%2520the%250Ahuman%2520visual%2520system%2520with%2520an%2520extensive%2520literature%2520review%253B%2520we%2520propose%2520a%2520novel%250Abiologically%2520motivated%2520neural%2520network%2520for%2520image%2520classification%253B%2520and%252C%2520finally%252C%250Awe%2520present%2520a%2520new%2520plug-and-play%2520module%2520to%2520model%2520context%2520awareness.%2520We%2520focus%2520on%250Athe%2520effect%2520of%2520incorporating%2520circuit%2520motifs%2520found%2520in%2520biological%2520brains%2520to%250Aaddress%2520visual%2520recognition.%2520Our%2520convolutional%2520architecture%2520is%2520inspired%2520by%2520the%250Aconnectivity%2520of%2520human%2520cortical%2520and%2520subcortical%2520streams%252C%2520and%2520we%2520implement%250Abottom-up%2520and%2520top-down%2520modulations%2520that%2520mimic%2520the%2520extensive%2520afferent%2520and%250Aefferent%2520connections%2520between%2520visual%2520and%2520cognitive%2520areas.%2520Our%2520Contextual%250AAttention%2520Block%2520is%2520simple%2520and%2520effective%2520and%2520can%2520be%2520integrated%2520with%2520any%250Afeed-forward%2520neural%2520network.%2520It%2520infers%2520weights%2520that%2520multiply%2520the%2520feature%2520maps%250Aaccording%2520to%2520their%2520causal%2520influence%2520on%2520the%2520scene%252C%2520modeling%2520the%2520co-occurrence%2520of%250Adifferent%2520objects%2520in%2520the%2520image.%2520We%2520place%2520our%2520module%2520at%2520different%2520bottlenecks%2520to%250Ainfuse%2520a%2520hierarchical%2520context%2520awareness%2520into%2520the%2520model.%2520We%2520validated%2520our%250Aproposals%2520through%2520image%2520classification%2520experiments%2520on%2520benchmark%2520data%2520and%2520found%250Aa%2520consistent%2520improvement%2520in%2520performance%2520and%2520the%2520robustness%2520of%2520the%2520produced%250Aexplanations%2520via%2520class%2520activation.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/gianlucarloni/CoCoReco.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Connectivity-Inspired%20Network%20for%20Context-Aware%20Recognition&entry.906535625=Gianluca%20Carloni%20and%20Sara%20Colantonio&entry.1292438233=%20%20The%20aim%20of%20this%20paper%20is%20threefold.%20We%20inform%20the%20AI%20practitioner%20about%20the%0Ahuman%20visual%20system%20with%20an%20extensive%20literature%20review%3B%20we%20propose%20a%20novel%0Abiologically%20motivated%20neural%20network%20for%20image%20classification%3B%20and%2C%20finally%2C%0Awe%20present%20a%20new%20plug-and-play%20module%20to%20model%20context%20awareness.%20We%20focus%20on%0Athe%20effect%20of%20incorporating%20circuit%20motifs%20found%20in%20biological%20brains%20to%0Aaddress%20visual%20recognition.%20Our%20convolutional%20architecture%20is%20inspired%20by%20the%0Aconnectivity%20of%20human%20cortical%20and%20subcortical%20streams%2C%20and%20we%20implement%0Abottom-up%20and%20top-down%20modulations%20that%20mimic%20the%20extensive%20afferent%20and%0Aefferent%20connections%20between%20visual%20and%20cognitive%20areas.%20Our%20Contextual%0AAttention%20Block%20is%20simple%20and%20effective%20and%20can%20be%20integrated%20with%20any%0Afeed-forward%20neural%20network.%20It%20infers%20weights%20that%20multiply%20the%20feature%20maps%0Aaccording%20to%20their%20causal%20influence%20on%20the%20scene%2C%20modeling%20the%20co-occurrence%20of%0Adifferent%20objects%20in%20the%20image.%20We%20place%20our%20module%20at%20different%20bottlenecks%20to%0Ainfuse%20a%20hierarchical%20context%20awareness%20into%20the%20model.%20We%20validated%20our%0Aproposals%20through%20image%20classification%20experiments%20on%20benchmark%20data%20and%20found%0Aa%20consistent%20improvement%20in%20performance%20and%20the%20robustness%20of%20the%20produced%0Aexplanations%20via%20class%20activation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/gianlucarloni/CoCoReco.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04360v1&entry.124074799=Read"},
{"title": "Open-Vocabulary Object Detectors: Robustness Challenges under\n  Distribution Shifts", "author": "Prakash Chandra Chhipa and Kanjar De and Meenakshi Subhash Chippa and Rajkumar Saini and Marcus Liwicki", "abstract": "  The challenge of Out-Of-Distribution (OOD) robustness remains a critical\nhurdle towards deploying deep vision models. Vision-Language Models (VLMs) have\nrecently achieved groundbreaking results. VLM-based open-vocabulary object\ndetection extends the capabilities of traditional object detection frameworks,\nenabling the recognition and classification of objects beyond predefined\ncategories. Investigating OOD robustness in recent open-vocabulary object\ndetection is essential to increase the trustworthiness of these models. This\nstudy presents a comprehensive robustness evaluation of the zero-shot\ncapabilities of three recent open-vocabulary (OV) foundation object detection\nmodels: OWL-ViT, YOLO World, and Grounding DINO. Experiments carried out on the\nrobustness benchmarks COCO-O, COCO-DC, and COCO-C encompassing distribution\nshifts due to information loss, corruption, adversarial attacks, and\ngeometrical deformation, highlighting the challenges of the model's robustness\nto foster the research for achieving robustness. Project page:\nhttps://prakashchhipa.github.io/projects/ovod_robustness\n", "link": "http://arxiv.org/abs/2405.14874v4", "date": "2024-09-06", "relevancy": 2.6765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Vocabulary%20Object%20Detectors%3A%20Robustness%20Challenges%20under%0A%20%20Distribution%20Shifts&body=Title%3A%20Open-Vocabulary%20Object%20Detectors%3A%20Robustness%20Challenges%20under%0A%20%20Distribution%20Shifts%0AAuthor%3A%20Prakash%20Chandra%20Chhipa%20and%20Kanjar%20De%20and%20Meenakshi%20Subhash%20Chippa%20and%20Rajkumar%20Saini%20and%20Marcus%20Liwicki%0AAbstract%3A%20%20%20The%20challenge%20of%20Out-Of-Distribution%20%28OOD%29%20robustness%20remains%20a%20critical%0Ahurdle%20towards%20deploying%20deep%20vision%20models.%20Vision-Language%20Models%20%28VLMs%29%20have%0Arecently%20achieved%20groundbreaking%20results.%20VLM-based%20open-vocabulary%20object%0Adetection%20extends%20the%20capabilities%20of%20traditional%20object%20detection%20frameworks%2C%0Aenabling%20the%20recognition%20and%20classification%20of%20objects%20beyond%20predefined%0Acategories.%20Investigating%20OOD%20robustness%20in%20recent%20open-vocabulary%20object%0Adetection%20is%20essential%20to%20increase%20the%20trustworthiness%20of%20these%20models.%20This%0Astudy%20presents%20a%20comprehensive%20robustness%20evaluation%20of%20the%20zero-shot%0Acapabilities%20of%20three%20recent%20open-vocabulary%20%28OV%29%20foundation%20object%20detection%0Amodels%3A%20OWL-ViT%2C%20YOLO%20World%2C%20and%20Grounding%20DINO.%20Experiments%20carried%20out%20on%20the%0Arobustness%20benchmarks%20COCO-O%2C%20COCO-DC%2C%20and%20COCO-C%20encompassing%20distribution%0Ashifts%20due%20to%20information%20loss%2C%20corruption%2C%20adversarial%20attacks%2C%20and%0Ageometrical%20deformation%2C%20highlighting%20the%20challenges%20of%20the%20model%27s%20robustness%0Ato%20foster%20the%20research%20for%20achieving%20robustness.%20Project%20page%3A%0Ahttps%3A//prakashchhipa.github.io/projects/ovod_robustness%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14874v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Vocabulary%2520Object%2520Detectors%253A%2520Robustness%2520Challenges%2520under%250A%2520%2520Distribution%2520Shifts%26entry.906535625%3DPrakash%2520Chandra%2520Chhipa%2520and%2520Kanjar%2520De%2520and%2520Meenakshi%2520Subhash%2520Chippa%2520and%2520Rajkumar%2520Saini%2520and%2520Marcus%2520Liwicki%26entry.1292438233%3D%2520%2520The%2520challenge%2520of%2520Out-Of-Distribution%2520%2528OOD%2529%2520robustness%2520remains%2520a%2520critical%250Ahurdle%2520towards%2520deploying%2520deep%2520vision%2520models.%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%250Arecently%2520achieved%2520groundbreaking%2520results.%2520VLM-based%2520open-vocabulary%2520object%250Adetection%2520extends%2520the%2520capabilities%2520of%2520traditional%2520object%2520detection%2520frameworks%252C%250Aenabling%2520the%2520recognition%2520and%2520classification%2520of%2520objects%2520beyond%2520predefined%250Acategories.%2520Investigating%2520OOD%2520robustness%2520in%2520recent%2520open-vocabulary%2520object%250Adetection%2520is%2520essential%2520to%2520increase%2520the%2520trustworthiness%2520of%2520these%2520models.%2520This%250Astudy%2520presents%2520a%2520comprehensive%2520robustness%2520evaluation%2520of%2520the%2520zero-shot%250Acapabilities%2520of%2520three%2520recent%2520open-vocabulary%2520%2528OV%2529%2520foundation%2520object%2520detection%250Amodels%253A%2520OWL-ViT%252C%2520YOLO%2520World%252C%2520and%2520Grounding%2520DINO.%2520Experiments%2520carried%2520out%2520on%2520the%250Arobustness%2520benchmarks%2520COCO-O%252C%2520COCO-DC%252C%2520and%2520COCO-C%2520encompassing%2520distribution%250Ashifts%2520due%2520to%2520information%2520loss%252C%2520corruption%252C%2520adversarial%2520attacks%252C%2520and%250Ageometrical%2520deformation%252C%2520highlighting%2520the%2520challenges%2520of%2520the%2520model%2527s%2520robustness%250Ato%2520foster%2520the%2520research%2520for%2520achieving%2520robustness.%2520Project%2520page%253A%250Ahttps%253A//prakashchhipa.github.io/projects/ovod_robustness%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14874v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Vocabulary%20Object%20Detectors%3A%20Robustness%20Challenges%20under%0A%20%20Distribution%20Shifts&entry.906535625=Prakash%20Chandra%20Chhipa%20and%20Kanjar%20De%20and%20Meenakshi%20Subhash%20Chippa%20and%20Rajkumar%20Saini%20and%20Marcus%20Liwicki&entry.1292438233=%20%20The%20challenge%20of%20Out-Of-Distribution%20%28OOD%29%20robustness%20remains%20a%20critical%0Ahurdle%20towards%20deploying%20deep%20vision%20models.%20Vision-Language%20Models%20%28VLMs%29%20have%0Arecently%20achieved%20groundbreaking%20results.%20VLM-based%20open-vocabulary%20object%0Adetection%20extends%20the%20capabilities%20of%20traditional%20object%20detection%20frameworks%2C%0Aenabling%20the%20recognition%20and%20classification%20of%20objects%20beyond%20predefined%0Acategories.%20Investigating%20OOD%20robustness%20in%20recent%20open-vocabulary%20object%0Adetection%20is%20essential%20to%20increase%20the%20trustworthiness%20of%20these%20models.%20This%0Astudy%20presents%20a%20comprehensive%20robustness%20evaluation%20of%20the%20zero-shot%0Acapabilities%20of%20three%20recent%20open-vocabulary%20%28OV%29%20foundation%20object%20detection%0Amodels%3A%20OWL-ViT%2C%20YOLO%20World%2C%20and%20Grounding%20DINO.%20Experiments%20carried%20out%20on%20the%0Arobustness%20benchmarks%20COCO-O%2C%20COCO-DC%2C%20and%20COCO-C%20encompassing%20distribution%0Ashifts%20due%20to%20information%20loss%2C%20corruption%2C%20adversarial%20attacks%2C%20and%0Ageometrical%20deformation%2C%20highlighting%20the%20challenges%20of%20the%20model%27s%20robustness%0Ato%20foster%20the%20research%20for%20achieving%20robustness.%20Project%20page%3A%0Ahttps%3A//prakashchhipa.github.io/projects/ovod_robustness%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14874v4&entry.124074799=Read"},
{"title": "Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with\n  Selective State-Space Model", "author": "Hongqiu Wang and Yixian Chen and Wu Chen and Huihui Xu and Haoyu Zhao and Bin Sheng and Huazhu Fu and Guang Yang and Lei Zhu", "abstract": "  Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) images capture\nhigh-resolution views of the retina with typically 200 spanning degrees.\nAccurate segmentation of vessels in UWF-SLO images is essential for detecting\nand diagnosing fundus disease. Recent studies have revealed that the selective\nState Space Model (SSM) in Mamba performs well in modeling long-range\ndependencies, which is crucial for capturing the continuity of elongated vessel\nstructures. Inspired by this, we propose the first Serpentine Mamba\n(Serp-Mamba) network to address this challenging task. Specifically, we\nrecognize the intricate, varied, and delicate nature of the tubular structure\nof vessels. Furthermore, the high-resolution of UWF-SLO images exacerbates the\nimbalance between the vessel and background categories. Based on the above\nobservations, we first devise a Serpentine Interwoven Adaptive (SIA) scan\nmechanism, which scans UWF-SLO images along curved vessel structures in a\nsnake-like crawling manner. This approach, consistent with vascular texture\ntransformations, ensures the effective and continuous capture of curved\nvascular structure features. Second, we propose an Ambiguity-Driven Dual\nRecalibration (ADDR) module to address the category imbalance problem\nintensified by high-resolution images. Our ADDR module delineates pixels by two\nlearnable thresholds and refines ambiguous pixels through a dual-driven\nstrategy, thereby accurately distinguishing vessels and background regions.\nExperiment results on three datasets demonstrate the superior performance of\nour Serp-Mamba on high-resolution vessel segmentation. We also conduct a series\nof ablation studies to verify the impact of our designs. Our code shall be\nreleased upon publication of this work.\n", "link": "http://arxiv.org/abs/2409.04356v1", "date": "2024-09-06", "relevancy": 2.6638, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5692}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Serp-Mamba%3A%20Advancing%20High-Resolution%20Retinal%20Vessel%20Segmentation%20with%0A%20%20Selective%20State-Space%20Model&body=Title%3A%20Serp-Mamba%3A%20Advancing%20High-Resolution%20Retinal%20Vessel%20Segmentation%20with%0A%20%20Selective%20State-Space%20Model%0AAuthor%3A%20Hongqiu%20Wang%20and%20Yixian%20Chen%20and%20Wu%20Chen%20and%20Huihui%20Xu%20and%20Haoyu%20Zhao%20and%20Bin%20Sheng%20and%20Huazhu%20Fu%20and%20Guang%20Yang%20and%20Lei%20Zhu%0AAbstract%3A%20%20%20Ultra-Wide-Field%20Scanning%20Laser%20Ophthalmoscopy%20%28UWF-SLO%29%20images%20capture%0Ahigh-resolution%20views%20of%20the%20retina%20with%20typically%20200%20spanning%20degrees.%0AAccurate%20segmentation%20of%20vessels%20in%20UWF-SLO%20images%20is%20essential%20for%20detecting%0Aand%20diagnosing%20fundus%20disease.%20Recent%20studies%20have%20revealed%20that%20the%20selective%0AState%20Space%20Model%20%28SSM%29%20in%20Mamba%20performs%20well%20in%20modeling%20long-range%0Adependencies%2C%20which%20is%20crucial%20for%20capturing%20the%20continuity%20of%20elongated%20vessel%0Astructures.%20Inspired%20by%20this%2C%20we%20propose%20the%20first%20Serpentine%20Mamba%0A%28Serp-Mamba%29%20network%20to%20address%20this%20challenging%20task.%20Specifically%2C%20we%0Arecognize%20the%20intricate%2C%20varied%2C%20and%20delicate%20nature%20of%20the%20tubular%20structure%0Aof%20vessels.%20Furthermore%2C%20the%20high-resolution%20of%20UWF-SLO%20images%20exacerbates%20the%0Aimbalance%20between%20the%20vessel%20and%20background%20categories.%20Based%20on%20the%20above%0Aobservations%2C%20we%20first%20devise%20a%20Serpentine%20Interwoven%20Adaptive%20%28SIA%29%20scan%0Amechanism%2C%20which%20scans%20UWF-SLO%20images%20along%20curved%20vessel%20structures%20in%20a%0Asnake-like%20crawling%20manner.%20This%20approach%2C%20consistent%20with%20vascular%20texture%0Atransformations%2C%20ensures%20the%20effective%20and%20continuous%20capture%20of%20curved%0Avascular%20structure%20features.%20Second%2C%20we%20propose%20an%20Ambiguity-Driven%20Dual%0ARecalibration%20%28ADDR%29%20module%20to%20address%20the%20category%20imbalance%20problem%0Aintensified%20by%20high-resolution%20images.%20Our%20ADDR%20module%20delineates%20pixels%20by%20two%0Alearnable%20thresholds%20and%20refines%20ambiguous%20pixels%20through%20a%20dual-driven%0Astrategy%2C%20thereby%20accurately%20distinguishing%20vessels%20and%20background%20regions.%0AExperiment%20results%20on%20three%20datasets%20demonstrate%20the%20superior%20performance%20of%0Aour%20Serp-Mamba%20on%20high-resolution%20vessel%20segmentation.%20We%20also%20conduct%20a%20series%0Aof%20ablation%20studies%20to%20verify%20the%20impact%20of%20our%20designs.%20Our%20code%20shall%20be%0Areleased%20upon%20publication%20of%20this%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSerp-Mamba%253A%2520Advancing%2520High-Resolution%2520Retinal%2520Vessel%2520Segmentation%2520with%250A%2520%2520Selective%2520State-Space%2520Model%26entry.906535625%3DHongqiu%2520Wang%2520and%2520Yixian%2520Chen%2520and%2520Wu%2520Chen%2520and%2520Huihui%2520Xu%2520and%2520Haoyu%2520Zhao%2520and%2520Bin%2520Sheng%2520and%2520Huazhu%2520Fu%2520and%2520Guang%2520Yang%2520and%2520Lei%2520Zhu%26entry.1292438233%3D%2520%2520Ultra-Wide-Field%2520Scanning%2520Laser%2520Ophthalmoscopy%2520%2528UWF-SLO%2529%2520images%2520capture%250Ahigh-resolution%2520views%2520of%2520the%2520retina%2520with%2520typically%2520200%2520spanning%2520degrees.%250AAccurate%2520segmentation%2520of%2520vessels%2520in%2520UWF-SLO%2520images%2520is%2520essential%2520for%2520detecting%250Aand%2520diagnosing%2520fundus%2520disease.%2520Recent%2520studies%2520have%2520revealed%2520that%2520the%2520selective%250AState%2520Space%2520Model%2520%2528SSM%2529%2520in%2520Mamba%2520performs%2520well%2520in%2520modeling%2520long-range%250Adependencies%252C%2520which%2520is%2520crucial%2520for%2520capturing%2520the%2520continuity%2520of%2520elongated%2520vessel%250Astructures.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520the%2520first%2520Serpentine%2520Mamba%250A%2528Serp-Mamba%2529%2520network%2520to%2520address%2520this%2520challenging%2520task.%2520Specifically%252C%2520we%250Arecognize%2520the%2520intricate%252C%2520varied%252C%2520and%2520delicate%2520nature%2520of%2520the%2520tubular%2520structure%250Aof%2520vessels.%2520Furthermore%252C%2520the%2520high-resolution%2520of%2520UWF-SLO%2520images%2520exacerbates%2520the%250Aimbalance%2520between%2520the%2520vessel%2520and%2520background%2520categories.%2520Based%2520on%2520the%2520above%250Aobservations%252C%2520we%2520first%2520devise%2520a%2520Serpentine%2520Interwoven%2520Adaptive%2520%2528SIA%2529%2520scan%250Amechanism%252C%2520which%2520scans%2520UWF-SLO%2520images%2520along%2520curved%2520vessel%2520structures%2520in%2520a%250Asnake-like%2520crawling%2520manner.%2520This%2520approach%252C%2520consistent%2520with%2520vascular%2520texture%250Atransformations%252C%2520ensures%2520the%2520effective%2520and%2520continuous%2520capture%2520of%2520curved%250Avascular%2520structure%2520features.%2520Second%252C%2520we%2520propose%2520an%2520Ambiguity-Driven%2520Dual%250ARecalibration%2520%2528ADDR%2529%2520module%2520to%2520address%2520the%2520category%2520imbalance%2520problem%250Aintensified%2520by%2520high-resolution%2520images.%2520Our%2520ADDR%2520module%2520delineates%2520pixels%2520by%2520two%250Alearnable%2520thresholds%2520and%2520refines%2520ambiguous%2520pixels%2520through%2520a%2520dual-driven%250Astrategy%252C%2520thereby%2520accurately%2520distinguishing%2520vessels%2520and%2520background%2520regions.%250AExperiment%2520results%2520on%2520three%2520datasets%2520demonstrate%2520the%2520superior%2520performance%2520of%250Aour%2520Serp-Mamba%2520on%2520high-resolution%2520vessel%2520segmentation.%2520We%2520also%2520conduct%2520a%2520series%250Aof%2520ablation%2520studies%2520to%2520verify%2520the%2520impact%2520of%2520our%2520designs.%2520Our%2520code%2520shall%2520be%250Areleased%2520upon%2520publication%2520of%2520this%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Serp-Mamba%3A%20Advancing%20High-Resolution%20Retinal%20Vessel%20Segmentation%20with%0A%20%20Selective%20State-Space%20Model&entry.906535625=Hongqiu%20Wang%20and%20Yixian%20Chen%20and%20Wu%20Chen%20and%20Huihui%20Xu%20and%20Haoyu%20Zhao%20and%20Bin%20Sheng%20and%20Huazhu%20Fu%20and%20Guang%20Yang%20and%20Lei%20Zhu&entry.1292438233=%20%20Ultra-Wide-Field%20Scanning%20Laser%20Ophthalmoscopy%20%28UWF-SLO%29%20images%20capture%0Ahigh-resolution%20views%20of%20the%20retina%20with%20typically%20200%20spanning%20degrees.%0AAccurate%20segmentation%20of%20vessels%20in%20UWF-SLO%20images%20is%20essential%20for%20detecting%0Aand%20diagnosing%20fundus%20disease.%20Recent%20studies%20have%20revealed%20that%20the%20selective%0AState%20Space%20Model%20%28SSM%29%20in%20Mamba%20performs%20well%20in%20modeling%20long-range%0Adependencies%2C%20which%20is%20crucial%20for%20capturing%20the%20continuity%20of%20elongated%20vessel%0Astructures.%20Inspired%20by%20this%2C%20we%20propose%20the%20first%20Serpentine%20Mamba%0A%28Serp-Mamba%29%20network%20to%20address%20this%20challenging%20task.%20Specifically%2C%20we%0Arecognize%20the%20intricate%2C%20varied%2C%20and%20delicate%20nature%20of%20the%20tubular%20structure%0Aof%20vessels.%20Furthermore%2C%20the%20high-resolution%20of%20UWF-SLO%20images%20exacerbates%20the%0Aimbalance%20between%20the%20vessel%20and%20background%20categories.%20Based%20on%20the%20above%0Aobservations%2C%20we%20first%20devise%20a%20Serpentine%20Interwoven%20Adaptive%20%28SIA%29%20scan%0Amechanism%2C%20which%20scans%20UWF-SLO%20images%20along%20curved%20vessel%20structures%20in%20a%0Asnake-like%20crawling%20manner.%20This%20approach%2C%20consistent%20with%20vascular%20texture%0Atransformations%2C%20ensures%20the%20effective%20and%20continuous%20capture%20of%20curved%0Avascular%20structure%20features.%20Second%2C%20we%20propose%20an%20Ambiguity-Driven%20Dual%0ARecalibration%20%28ADDR%29%20module%20to%20address%20the%20category%20imbalance%20problem%0Aintensified%20by%20high-resolution%20images.%20Our%20ADDR%20module%20delineates%20pixels%20by%20two%0Alearnable%20thresholds%20and%20refines%20ambiguous%20pixels%20through%20a%20dual-driven%0Astrategy%2C%20thereby%20accurately%20distinguishing%20vessels%20and%20background%20regions.%0AExperiment%20results%20on%20three%20datasets%20demonstrate%20the%20superior%20performance%20of%0Aour%20Serp-Mamba%20on%20high-resolution%20vessel%20segmentation.%20We%20also%20conduct%20a%20series%0Aof%20ablation%20studies%20to%20verify%20the%20impact%20of%20our%20designs.%20Our%20code%20shall%20be%0Areleased%20upon%20publication%20of%20this%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04356v1&entry.124074799=Read"},
{"title": "Training-free Camera Control for Video Generation", "author": "Chen Hou and Guoqiang Wei and Yan Zeng and Zhibo Chen", "abstract": "  We propose a training-free and robust solution to offer camera movement\ncontrol for off-the-shelf video diffusion models. Unlike previous work, our\nmethod does not require any supervised finetuning on camera-annotated datasets\nor self-supervised training via data augmentation. Instead, it can be plugged\nand played with most pretrained video diffusion models and generate camera\ncontrollable videos with a single image or text prompt as input. The\ninspiration of our work comes from the layout prior that intermediate latents\nhold towards generated results, thus rearranging noisy pixels in them will make\noutput content reallocated as well. As camera move could also be seen as a kind\nof pixel rearrangement caused by perspective change, videos could be\nreorganized following specific camera motion if their noisy latents change\naccordingly. Established on this, we propose our method CamTrol, which enables\nrobust camera control for video diffusion models. It is achieved by a two-stage\nprocess. First, we model image layout rearrangement through explicit camera\nmovement in 3D point cloud space. Second, we generate videos with camera motion\nusing layout prior of noisy latents formed by a series of rearranged images.\nExtensive experiments have demonstrated the robustness our method holds in\ncontrolling camera motion of generated videos. Furthermore, we show that our\nmethod can produce impressive results in generating 3D rotation videos with\ndynamic content. Project page at https://lifedecoder.github.io/CamTrol/.\n", "link": "http://arxiv.org/abs/2406.10126v2", "date": "2024-09-06", "relevancy": 2.6518, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6857}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6651}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-free%20Camera%20Control%20for%20Video%20Generation&body=Title%3A%20Training-free%20Camera%20Control%20for%20Video%20Generation%0AAuthor%3A%20Chen%20Hou%20and%20Guoqiang%20Wei%20and%20Yan%20Zeng%20and%20Zhibo%20Chen%0AAbstract%3A%20%20%20We%20propose%20a%20training-free%20and%20robust%20solution%20to%20offer%20camera%20movement%0Acontrol%20for%20off-the-shelf%20video%20diffusion%20models.%20Unlike%20previous%20work%2C%20our%0Amethod%20does%20not%20require%20any%20supervised%20finetuning%20on%20camera-annotated%20datasets%0Aor%20self-supervised%20training%20via%20data%20augmentation.%20Instead%2C%20it%20can%20be%20plugged%0Aand%20played%20with%20most%20pretrained%20video%20diffusion%20models%20and%20generate%20camera%0Acontrollable%20videos%20with%20a%20single%20image%20or%20text%20prompt%20as%20input.%20The%0Ainspiration%20of%20our%20work%20comes%20from%20the%20layout%20prior%20that%20intermediate%20latents%0Ahold%20towards%20generated%20results%2C%20thus%20rearranging%20noisy%20pixels%20in%20them%20will%20make%0Aoutput%20content%20reallocated%20as%20well.%20As%20camera%20move%20could%20also%20be%20seen%20as%20a%20kind%0Aof%20pixel%20rearrangement%20caused%20by%20perspective%20change%2C%20videos%20could%20be%0Areorganized%20following%20specific%20camera%20motion%20if%20their%20noisy%20latents%20change%0Aaccordingly.%20Established%20on%20this%2C%20we%20propose%20our%20method%20CamTrol%2C%20which%20enables%0Arobust%20camera%20control%20for%20video%20diffusion%20models.%20It%20is%20achieved%20by%20a%20two-stage%0Aprocess.%20First%2C%20we%20model%20image%20layout%20rearrangement%20through%20explicit%20camera%0Amovement%20in%203D%20point%20cloud%20space.%20Second%2C%20we%20generate%20videos%20with%20camera%20motion%0Ausing%20layout%20prior%20of%20noisy%20latents%20formed%20by%20a%20series%20of%20rearranged%20images.%0AExtensive%20experiments%20have%20demonstrated%20the%20robustness%20our%20method%20holds%20in%0Acontrolling%20camera%20motion%20of%20generated%20videos.%20Furthermore%2C%20we%20show%20that%20our%0Amethod%20can%20produce%20impressive%20results%20in%20generating%203D%20rotation%20videos%20with%0Adynamic%20content.%20Project%20page%20at%20https%3A//lifedecoder.github.io/CamTrol/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10126v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-free%2520Camera%2520Control%2520for%2520Video%2520Generation%26entry.906535625%3DChen%2520Hou%2520and%2520Guoqiang%2520Wei%2520and%2520Yan%2520Zeng%2520and%2520Zhibo%2520Chen%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520training-free%2520and%2520robust%2520solution%2520to%2520offer%2520camera%2520movement%250Acontrol%2520for%2520off-the-shelf%2520video%2520diffusion%2520models.%2520Unlike%2520previous%2520work%252C%2520our%250Amethod%2520does%2520not%2520require%2520any%2520supervised%2520finetuning%2520on%2520camera-annotated%2520datasets%250Aor%2520self-supervised%2520training%2520via%2520data%2520augmentation.%2520Instead%252C%2520it%2520can%2520be%2520plugged%250Aand%2520played%2520with%2520most%2520pretrained%2520video%2520diffusion%2520models%2520and%2520generate%2520camera%250Acontrollable%2520videos%2520with%2520a%2520single%2520image%2520or%2520text%2520prompt%2520as%2520input.%2520The%250Ainspiration%2520of%2520our%2520work%2520comes%2520from%2520the%2520layout%2520prior%2520that%2520intermediate%2520latents%250Ahold%2520towards%2520generated%2520results%252C%2520thus%2520rearranging%2520noisy%2520pixels%2520in%2520them%2520will%2520make%250Aoutput%2520content%2520reallocated%2520as%2520well.%2520As%2520camera%2520move%2520could%2520also%2520be%2520seen%2520as%2520a%2520kind%250Aof%2520pixel%2520rearrangement%2520caused%2520by%2520perspective%2520change%252C%2520videos%2520could%2520be%250Areorganized%2520following%2520specific%2520camera%2520motion%2520if%2520their%2520noisy%2520latents%2520change%250Aaccordingly.%2520Established%2520on%2520this%252C%2520we%2520propose%2520our%2520method%2520CamTrol%252C%2520which%2520enables%250Arobust%2520camera%2520control%2520for%2520video%2520diffusion%2520models.%2520It%2520is%2520achieved%2520by%2520a%2520two-stage%250Aprocess.%2520First%252C%2520we%2520model%2520image%2520layout%2520rearrangement%2520through%2520explicit%2520camera%250Amovement%2520in%25203D%2520point%2520cloud%2520space.%2520Second%252C%2520we%2520generate%2520videos%2520with%2520camera%2520motion%250Ausing%2520layout%2520prior%2520of%2520noisy%2520latents%2520formed%2520by%2520a%2520series%2520of%2520rearranged%2520images.%250AExtensive%2520experiments%2520have%2520demonstrated%2520the%2520robustness%2520our%2520method%2520holds%2520in%250Acontrolling%2520camera%2520motion%2520of%2520generated%2520videos.%2520Furthermore%252C%2520we%2520show%2520that%2520our%250Amethod%2520can%2520produce%2520impressive%2520results%2520in%2520generating%25203D%2520rotation%2520videos%2520with%250Adynamic%2520content.%2520Project%2520page%2520at%2520https%253A//lifedecoder.github.io/CamTrol/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10126v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-free%20Camera%20Control%20for%20Video%20Generation&entry.906535625=Chen%20Hou%20and%20Guoqiang%20Wei%20and%20Yan%20Zeng%20and%20Zhibo%20Chen&entry.1292438233=%20%20We%20propose%20a%20training-free%20and%20robust%20solution%20to%20offer%20camera%20movement%0Acontrol%20for%20off-the-shelf%20video%20diffusion%20models.%20Unlike%20previous%20work%2C%20our%0Amethod%20does%20not%20require%20any%20supervised%20finetuning%20on%20camera-annotated%20datasets%0Aor%20self-supervised%20training%20via%20data%20augmentation.%20Instead%2C%20it%20can%20be%20plugged%0Aand%20played%20with%20most%20pretrained%20video%20diffusion%20models%20and%20generate%20camera%0Acontrollable%20videos%20with%20a%20single%20image%20or%20text%20prompt%20as%20input.%20The%0Ainspiration%20of%20our%20work%20comes%20from%20the%20layout%20prior%20that%20intermediate%20latents%0Ahold%20towards%20generated%20results%2C%20thus%20rearranging%20noisy%20pixels%20in%20them%20will%20make%0Aoutput%20content%20reallocated%20as%20well.%20As%20camera%20move%20could%20also%20be%20seen%20as%20a%20kind%0Aof%20pixel%20rearrangement%20caused%20by%20perspective%20change%2C%20videos%20could%20be%0Areorganized%20following%20specific%20camera%20motion%20if%20their%20noisy%20latents%20change%0Aaccordingly.%20Established%20on%20this%2C%20we%20propose%20our%20method%20CamTrol%2C%20which%20enables%0Arobust%20camera%20control%20for%20video%20diffusion%20models.%20It%20is%20achieved%20by%20a%20two-stage%0Aprocess.%20First%2C%20we%20model%20image%20layout%20rearrangement%20through%20explicit%20camera%0Amovement%20in%203D%20point%20cloud%20space.%20Second%2C%20we%20generate%20videos%20with%20camera%20motion%0Ausing%20layout%20prior%20of%20noisy%20latents%20formed%20by%20a%20series%20of%20rearranged%20images.%0AExtensive%20experiments%20have%20demonstrated%20the%20robustness%20our%20method%20holds%20in%0Acontrolling%20camera%20motion%20of%20generated%20videos.%20Furthermore%2C%20we%20show%20that%20our%0Amethod%20can%20produce%20impressive%20results%20in%20generating%203D%20rotation%20videos%20with%0Adynamic%20content.%20Project%20page%20at%20https%3A//lifedecoder.github.io/CamTrol/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10126v2&entry.124074799=Read"},
{"title": "Zero-Shot Video Editing through Adaptive Sliding Score Distillation", "author": "Lianghan Zhu and Yanqi Bao and Jing Huo and Jing Wu and Yu-Kun Lai and Wenbin Li and Yang Gao", "abstract": "  The rapidly evolving field of Text-to-Video generation (T2V) has catalyzed\nrenewed interest in controllable video editing research. While the application\nof editing prompts to guide diffusion model denoising has gained prominence,\nmirroring advancements in image editing, this noise-based inference process\ninherently compromises the original video's integrity, resulting in unintended\nover-editing and temporal discontinuities. To address these challenges, this\nstudy proposes a novel paradigm of video-based score distillation, facilitating\ndirect manipulation of original video content. Specifically, distinguishing it\nfrom image-based score distillation, we propose an Adaptive Sliding Score\nDistillation strategy, which incorporates both global and local video guidance\nto reduce the impact of editing errors. Combined with our proposed Image-based\nJoint Guidance mechanism, it has the ability to mitigate the inherent\ninstability of the T2V model and single-step sampling. Additionally, we design\na Weighted Attention Fusion module to further preserve the key features of the\noriginal video and avoid over-editing. Extensive experiments demonstrate that\nthese strategies effectively address existing challenges, achieving superior\nperformance compared to current state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2406.04888v2", "date": "2024-09-06", "relevancy": 2.629, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7084}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6474}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Video%20Editing%20through%20Adaptive%20Sliding%20Score%20Distillation&body=Title%3A%20Zero-Shot%20Video%20Editing%20through%20Adaptive%20Sliding%20Score%20Distillation%0AAuthor%3A%20Lianghan%20Zhu%20and%20Yanqi%20Bao%20and%20Jing%20Huo%20and%20Jing%20Wu%20and%20Yu-Kun%20Lai%20and%20Wenbin%20Li%20and%20Yang%20Gao%0AAbstract%3A%20%20%20The%20rapidly%20evolving%20field%20of%20Text-to-Video%20generation%20%28T2V%29%20has%20catalyzed%0Arenewed%20interest%20in%20controllable%20video%20editing%20research.%20While%20the%20application%0Aof%20editing%20prompts%20to%20guide%20diffusion%20model%20denoising%20has%20gained%20prominence%2C%0Amirroring%20advancements%20in%20image%20editing%2C%20this%20noise-based%20inference%20process%0Ainherently%20compromises%20the%20original%20video%27s%20integrity%2C%20resulting%20in%20unintended%0Aover-editing%20and%20temporal%20discontinuities.%20To%20address%20these%20challenges%2C%20this%0Astudy%20proposes%20a%20novel%20paradigm%20of%20video-based%20score%20distillation%2C%20facilitating%0Adirect%20manipulation%20of%20original%20video%20content.%20Specifically%2C%20distinguishing%20it%0Afrom%20image-based%20score%20distillation%2C%20we%20propose%20an%20Adaptive%20Sliding%20Score%0ADistillation%20strategy%2C%20which%20incorporates%20both%20global%20and%20local%20video%20guidance%0Ato%20reduce%20the%20impact%20of%20editing%20errors.%20Combined%20with%20our%20proposed%20Image-based%0AJoint%20Guidance%20mechanism%2C%20it%20has%20the%20ability%20to%20mitigate%20the%20inherent%0Ainstability%20of%20the%20T2V%20model%20and%20single-step%20sampling.%20Additionally%2C%20we%20design%0Aa%20Weighted%20Attention%20Fusion%20module%20to%20further%20preserve%20the%20key%20features%20of%20the%0Aoriginal%20video%20and%20avoid%20over-editing.%20Extensive%20experiments%20demonstrate%20that%0Athese%20strategies%20effectively%20address%20existing%20challenges%2C%20achieving%20superior%0Aperformance%20compared%20to%20current%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Video%2520Editing%2520through%2520Adaptive%2520Sliding%2520Score%2520Distillation%26entry.906535625%3DLianghan%2520Zhu%2520and%2520Yanqi%2520Bao%2520and%2520Jing%2520Huo%2520and%2520Jing%2520Wu%2520and%2520Yu-Kun%2520Lai%2520and%2520Wenbin%2520Li%2520and%2520Yang%2520Gao%26entry.1292438233%3D%2520%2520The%2520rapidly%2520evolving%2520field%2520of%2520Text-to-Video%2520generation%2520%2528T2V%2529%2520has%2520catalyzed%250Arenewed%2520interest%2520in%2520controllable%2520video%2520editing%2520research.%2520While%2520the%2520application%250Aof%2520editing%2520prompts%2520to%2520guide%2520diffusion%2520model%2520denoising%2520has%2520gained%2520prominence%252C%250Amirroring%2520advancements%2520in%2520image%2520editing%252C%2520this%2520noise-based%2520inference%2520process%250Ainherently%2520compromises%2520the%2520original%2520video%2527s%2520integrity%252C%2520resulting%2520in%2520unintended%250Aover-editing%2520and%2520temporal%2520discontinuities.%2520To%2520address%2520these%2520challenges%252C%2520this%250Astudy%2520proposes%2520a%2520novel%2520paradigm%2520of%2520video-based%2520score%2520distillation%252C%2520facilitating%250Adirect%2520manipulation%2520of%2520original%2520video%2520content.%2520Specifically%252C%2520distinguishing%2520it%250Afrom%2520image-based%2520score%2520distillation%252C%2520we%2520propose%2520an%2520Adaptive%2520Sliding%2520Score%250ADistillation%2520strategy%252C%2520which%2520incorporates%2520both%2520global%2520and%2520local%2520video%2520guidance%250Ato%2520reduce%2520the%2520impact%2520of%2520editing%2520errors.%2520Combined%2520with%2520our%2520proposed%2520Image-based%250AJoint%2520Guidance%2520mechanism%252C%2520it%2520has%2520the%2520ability%2520to%2520mitigate%2520the%2520inherent%250Ainstability%2520of%2520the%2520T2V%2520model%2520and%2520single-step%2520sampling.%2520Additionally%252C%2520we%2520design%250Aa%2520Weighted%2520Attention%2520Fusion%2520module%2520to%2520further%2520preserve%2520the%2520key%2520features%2520of%2520the%250Aoriginal%2520video%2520and%2520avoid%2520over-editing.%2520Extensive%2520experiments%2520demonstrate%2520that%250Athese%2520strategies%2520effectively%2520address%2520existing%2520challenges%252C%2520achieving%2520superior%250Aperformance%2520compared%2520to%2520current%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Video%20Editing%20through%20Adaptive%20Sliding%20Score%20Distillation&entry.906535625=Lianghan%20Zhu%20and%20Yanqi%20Bao%20and%20Jing%20Huo%20and%20Jing%20Wu%20and%20Yu-Kun%20Lai%20and%20Wenbin%20Li%20and%20Yang%20Gao&entry.1292438233=%20%20The%20rapidly%20evolving%20field%20of%20Text-to-Video%20generation%20%28T2V%29%20has%20catalyzed%0Arenewed%20interest%20in%20controllable%20video%20editing%20research.%20While%20the%20application%0Aof%20editing%20prompts%20to%20guide%20diffusion%20model%20denoising%20has%20gained%20prominence%2C%0Amirroring%20advancements%20in%20image%20editing%2C%20this%20noise-based%20inference%20process%0Ainherently%20compromises%20the%20original%20video%27s%20integrity%2C%20resulting%20in%20unintended%0Aover-editing%20and%20temporal%20discontinuities.%20To%20address%20these%20challenges%2C%20this%0Astudy%20proposes%20a%20novel%20paradigm%20of%20video-based%20score%20distillation%2C%20facilitating%0Adirect%20manipulation%20of%20original%20video%20content.%20Specifically%2C%20distinguishing%20it%0Afrom%20image-based%20score%20distillation%2C%20we%20propose%20an%20Adaptive%20Sliding%20Score%0ADistillation%20strategy%2C%20which%20incorporates%20both%20global%20and%20local%20video%20guidance%0Ato%20reduce%20the%20impact%20of%20editing%20errors.%20Combined%20with%20our%20proposed%20Image-based%0AJoint%20Guidance%20mechanism%2C%20it%20has%20the%20ability%20to%20mitigate%20the%20inherent%0Ainstability%20of%20the%20T2V%20model%20and%20single-step%20sampling.%20Additionally%2C%20we%20design%0Aa%20Weighted%20Attention%20Fusion%20module%20to%20further%20preserve%20the%20key%20features%20of%20the%0Aoriginal%20video%20and%20avoid%20over-editing.%20Extensive%20experiments%20demonstrate%20that%0Athese%20strategies%20effectively%20address%20existing%20challenges%2C%20achieving%20superior%0Aperformance%20compared%20to%20current%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04888v2&entry.124074799=Read"},
{"title": "Learning to Learn Transferable Generative Attack for Person\n  Re-Identification", "author": "Yuan Bian and Min Liu and Xueping Wang and Yunfeng Ma and Yaonan Wang", "abstract": "  Deep learning-based person re-identification (re-id) models are widely\nemployed in surveillance systems and inevitably inherit the vulnerability of\ndeep networks to adversarial attacks. Existing attacks merely consider\ncross-dataset and cross-model transferability, ignoring the cross-test\ncapability to perturb models trained in different domains. To powerfully\nexamine the robustness of real-world re-id models, the Meta Transferable\nGenerative Attack (MTGA) method is proposed, which adopts meta-learning\noptimization to promote the generative attacker producing highly transferable\nadversarial examples by learning comprehensively simulated transfer-based\ncross-model\\&dataset\\&test black-box meta attack tasks. Specifically,\ncross-model\\&dataset black-box attack tasks are first mimicked by selecting\ndifferent re-id models and datasets for meta-train and meta-test attack\nprocesses. As different models may focus on different feature regions, the\nPerturbation Random Erasing module is further devised to prevent the attacker\nfrom learning to only corrupt model-specific features. To boost the attacker\nlearning to possess cross-test transferability, the Normalization Mix strategy\nis introduced to imitate diverse feature embedding spaces by mixing\nmulti-domain statistics of target models. Extensive experiments show the\nsuperiority of MTGA, especially in cross-model\\&dataset and\ncross-model\\&dataset\\&test attacks, our MTGA outperforms the SOTA methods by\n21.5\\% and 11.3\\% on mean mAP drop rate, respectively. The code of MTGA will be\nreleased after the paper is accepted.\n", "link": "http://arxiv.org/abs/2409.04208v1", "date": "2024-09-06", "relevancy": 2.6157, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5502}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5099}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Learn%20Transferable%20Generative%20Attack%20for%20Person%0A%20%20Re-Identification&body=Title%3A%20Learning%20to%20Learn%20Transferable%20Generative%20Attack%20for%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Yuan%20Bian%20and%20Min%20Liu%20and%20Xueping%20Wang%20and%20Yunfeng%20Ma%20and%20Yaonan%20Wang%0AAbstract%3A%20%20%20Deep%20learning-based%20person%20re-identification%20%28re-id%29%20models%20are%20widely%0Aemployed%20in%20surveillance%20systems%20and%20inevitably%20inherit%20the%20vulnerability%20of%0Adeep%20networks%20to%20adversarial%20attacks.%20Existing%20attacks%20merely%20consider%0Across-dataset%20and%20cross-model%20transferability%2C%20ignoring%20the%20cross-test%0Acapability%20to%20perturb%20models%20trained%20in%20different%20domains.%20To%20powerfully%0Aexamine%20the%20robustness%20of%20real-world%20re-id%20models%2C%20the%20Meta%20Transferable%0AGenerative%20Attack%20%28MTGA%29%20method%20is%20proposed%2C%20which%20adopts%20meta-learning%0Aoptimization%20to%20promote%20the%20generative%20attacker%20producing%20highly%20transferable%0Aadversarial%20examples%20by%20learning%20comprehensively%20simulated%20transfer-based%0Across-model%5C%26dataset%5C%26test%20black-box%20meta%20attack%20tasks.%20Specifically%2C%0Across-model%5C%26dataset%20black-box%20attack%20tasks%20are%20first%20mimicked%20by%20selecting%0Adifferent%20re-id%20models%20and%20datasets%20for%20meta-train%20and%20meta-test%20attack%0Aprocesses.%20As%20different%20models%20may%20focus%20on%20different%20feature%20regions%2C%20the%0APerturbation%20Random%20Erasing%20module%20is%20further%20devised%20to%20prevent%20the%20attacker%0Afrom%20learning%20to%20only%20corrupt%20model-specific%20features.%20To%20boost%20the%20attacker%0Alearning%20to%20possess%20cross-test%20transferability%2C%20the%20Normalization%20Mix%20strategy%0Ais%20introduced%20to%20imitate%20diverse%20feature%20embedding%20spaces%20by%20mixing%0Amulti-domain%20statistics%20of%20target%20models.%20Extensive%20experiments%20show%20the%0Asuperiority%20of%20MTGA%2C%20especially%20in%20cross-model%5C%26dataset%20and%0Across-model%5C%26dataset%5C%26test%20attacks%2C%20our%20MTGA%20outperforms%20the%20SOTA%20methods%20by%0A21.5%5C%25%20and%2011.3%5C%25%20on%20mean%20mAP%20drop%20rate%2C%20respectively.%20The%20code%20of%20MTGA%20will%20be%0Areleased%20after%20the%20paper%20is%20accepted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Learn%2520Transferable%2520Generative%2520Attack%2520for%2520Person%250A%2520%2520Re-Identification%26entry.906535625%3DYuan%2520Bian%2520and%2520Min%2520Liu%2520and%2520Xueping%2520Wang%2520and%2520Yunfeng%2520Ma%2520and%2520Yaonan%2520Wang%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520person%2520re-identification%2520%2528re-id%2529%2520models%2520are%2520widely%250Aemployed%2520in%2520surveillance%2520systems%2520and%2520inevitably%2520inherit%2520the%2520vulnerability%2520of%250Adeep%2520networks%2520to%2520adversarial%2520attacks.%2520Existing%2520attacks%2520merely%2520consider%250Across-dataset%2520and%2520cross-model%2520transferability%252C%2520ignoring%2520the%2520cross-test%250Acapability%2520to%2520perturb%2520models%2520trained%2520in%2520different%2520domains.%2520To%2520powerfully%250Aexamine%2520the%2520robustness%2520of%2520real-world%2520re-id%2520models%252C%2520the%2520Meta%2520Transferable%250AGenerative%2520Attack%2520%2528MTGA%2529%2520method%2520is%2520proposed%252C%2520which%2520adopts%2520meta-learning%250Aoptimization%2520to%2520promote%2520the%2520generative%2520attacker%2520producing%2520highly%2520transferable%250Aadversarial%2520examples%2520by%2520learning%2520comprehensively%2520simulated%2520transfer-based%250Across-model%255C%2526dataset%255C%2526test%2520black-box%2520meta%2520attack%2520tasks.%2520Specifically%252C%250Across-model%255C%2526dataset%2520black-box%2520attack%2520tasks%2520are%2520first%2520mimicked%2520by%2520selecting%250Adifferent%2520re-id%2520models%2520and%2520datasets%2520for%2520meta-train%2520and%2520meta-test%2520attack%250Aprocesses.%2520As%2520different%2520models%2520may%2520focus%2520on%2520different%2520feature%2520regions%252C%2520the%250APerturbation%2520Random%2520Erasing%2520module%2520is%2520further%2520devised%2520to%2520prevent%2520the%2520attacker%250Afrom%2520learning%2520to%2520only%2520corrupt%2520model-specific%2520features.%2520To%2520boost%2520the%2520attacker%250Alearning%2520to%2520possess%2520cross-test%2520transferability%252C%2520the%2520Normalization%2520Mix%2520strategy%250Ais%2520introduced%2520to%2520imitate%2520diverse%2520feature%2520embedding%2520spaces%2520by%2520mixing%250Amulti-domain%2520statistics%2520of%2520target%2520models.%2520Extensive%2520experiments%2520show%2520the%250Asuperiority%2520of%2520MTGA%252C%2520especially%2520in%2520cross-model%255C%2526dataset%2520and%250Across-model%255C%2526dataset%255C%2526test%2520attacks%252C%2520our%2520MTGA%2520outperforms%2520the%2520SOTA%2520methods%2520by%250A21.5%255C%2525%2520and%252011.3%255C%2525%2520on%2520mean%2520mAP%2520drop%2520rate%252C%2520respectively.%2520The%2520code%2520of%2520MTGA%2520will%2520be%250Areleased%2520after%2520the%2520paper%2520is%2520accepted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Learn%20Transferable%20Generative%20Attack%20for%20Person%0A%20%20Re-Identification&entry.906535625=Yuan%20Bian%20and%20Min%20Liu%20and%20Xueping%20Wang%20and%20Yunfeng%20Ma%20and%20Yaonan%20Wang&entry.1292438233=%20%20Deep%20learning-based%20person%20re-identification%20%28re-id%29%20models%20are%20widely%0Aemployed%20in%20surveillance%20systems%20and%20inevitably%20inherit%20the%20vulnerability%20of%0Adeep%20networks%20to%20adversarial%20attacks.%20Existing%20attacks%20merely%20consider%0Across-dataset%20and%20cross-model%20transferability%2C%20ignoring%20the%20cross-test%0Acapability%20to%20perturb%20models%20trained%20in%20different%20domains.%20To%20powerfully%0Aexamine%20the%20robustness%20of%20real-world%20re-id%20models%2C%20the%20Meta%20Transferable%0AGenerative%20Attack%20%28MTGA%29%20method%20is%20proposed%2C%20which%20adopts%20meta-learning%0Aoptimization%20to%20promote%20the%20generative%20attacker%20producing%20highly%20transferable%0Aadversarial%20examples%20by%20learning%20comprehensively%20simulated%20transfer-based%0Across-model%5C%26dataset%5C%26test%20black-box%20meta%20attack%20tasks.%20Specifically%2C%0Across-model%5C%26dataset%20black-box%20attack%20tasks%20are%20first%20mimicked%20by%20selecting%0Adifferent%20re-id%20models%20and%20datasets%20for%20meta-train%20and%20meta-test%20attack%0Aprocesses.%20As%20different%20models%20may%20focus%20on%20different%20feature%20regions%2C%20the%0APerturbation%20Random%20Erasing%20module%20is%20further%20devised%20to%20prevent%20the%20attacker%0Afrom%20learning%20to%20only%20corrupt%20model-specific%20features.%20To%20boost%20the%20attacker%0Alearning%20to%20possess%20cross-test%20transferability%2C%20the%20Normalization%20Mix%20strategy%0Ais%20introduced%20to%20imitate%20diverse%20feature%20embedding%20spaces%20by%20mixing%0Amulti-domain%20statistics%20of%20target%20models.%20Extensive%20experiments%20show%20the%0Asuperiority%20of%20MTGA%2C%20especially%20in%20cross-model%5C%26dataset%20and%0Across-model%5C%26dataset%5C%26test%20attacks%2C%20our%20MTGA%20outperforms%20the%20SOTA%20methods%20by%0A21.5%5C%25%20and%2011.3%5C%25%20on%20mean%20mAP%20drop%20rate%2C%20respectively.%20The%20code%20of%20MTGA%20will%20be%0Areleased%20after%20the%20paper%20is%20accepted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04208v1&entry.124074799=Read"},
{"title": "Res-VMamba: Fine-Grained Food Category Visual Classification Using\n  Selective State Space Models with Deep Residual Learning", "author": "Chi-Sheng Chen and Guan-Ying Chen and Dong Zhou and Di Jiang and Dai-Shi Chen", "abstract": "  Food classification is the foundation for developing food vision tasks and\nplays a key role in the burgeoning field of computational nutrition. Due to the\ncomplexity of food requiring fine-grained classification, recent academic\nresearch mainly modifies Convolutional Neural Networks (CNNs) and/or Vision\nTransformers (ViTs) to perform food category classification. However, to learn\nfine-grained features, the CNN backbone needs additional structural design,\nwhereas ViT, containing the self-attention module, has increased computational\ncomplexity. In recent months, a new Sequence State Space (S4) model, through a\nSelection mechanism and computation with a Scan (S6), colloquially termed\nMamba, has demonstrated superior performance and computation efficiency\ncompared to the Transformer architecture. The VMamba model, which incorporates\nthe Mamba mechanism into image tasks (such as classification), currently\nestablishes the state-of-the-art (SOTA) on the ImageNet dataset. In this\nresearch, we introduce an academically underestimated food dataset CNFOOD-241,\nand pioneer the integration of a residual learning framework within the VMamba\nmodel to concurrently harness both global and local state features inherent in\nthe original VMamba architectural design. The research results show that VMamba\nsurpasses current SOTA models in fine-grained and food classification. The\nproposed Res-VMamba further improves the classification accuracy to 79.54\\%\nwithout pretrained weight. Our findings elucidate that our proposed methodology\nestablishes a new benchmark for SOTA performance in food recognition on the\nCNFOOD-241 dataset. The code can be obtained on GitHub:\nhttps://github.com/ChiShengChen/ResVMamba.\n", "link": "http://arxiv.org/abs/2402.15761v3", "date": "2024-09-06", "relevancy": 2.5641, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.54}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5029}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Res-VMamba%3A%20Fine-Grained%20Food%20Category%20Visual%20Classification%20Using%0A%20%20Selective%20State%20Space%20Models%20with%20Deep%20Residual%20Learning&body=Title%3A%20Res-VMamba%3A%20Fine-Grained%20Food%20Category%20Visual%20Classification%20Using%0A%20%20Selective%20State%20Space%20Models%20with%20Deep%20Residual%20Learning%0AAuthor%3A%20Chi-Sheng%20Chen%20and%20Guan-Ying%20Chen%20and%20Dong%20Zhou%20and%20Di%20Jiang%20and%20Dai-Shi%20Chen%0AAbstract%3A%20%20%20Food%20classification%20is%20the%20foundation%20for%20developing%20food%20vision%20tasks%20and%0Aplays%20a%20key%20role%20in%20the%20burgeoning%20field%20of%20computational%20nutrition.%20Due%20to%20the%0Acomplexity%20of%20food%20requiring%20fine-grained%20classification%2C%20recent%20academic%0Aresearch%20mainly%20modifies%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and/or%20Vision%0ATransformers%20%28ViTs%29%20to%20perform%20food%20category%20classification.%20However%2C%20to%20learn%0Afine-grained%20features%2C%20the%20CNN%20backbone%20needs%20additional%20structural%20design%2C%0Awhereas%20ViT%2C%20containing%20the%20self-attention%20module%2C%20has%20increased%20computational%0Acomplexity.%20In%20recent%20months%2C%20a%20new%20Sequence%20State%20Space%20%28S4%29%20model%2C%20through%20a%0ASelection%20mechanism%20and%20computation%20with%20a%20Scan%20%28S6%29%2C%20colloquially%20termed%0AMamba%2C%20has%20demonstrated%20superior%20performance%20and%20computation%20efficiency%0Acompared%20to%20the%20Transformer%20architecture.%20The%20VMamba%20model%2C%20which%20incorporates%0Athe%20Mamba%20mechanism%20into%20image%20tasks%20%28such%20as%20classification%29%2C%20currently%0Aestablishes%20the%20state-of-the-art%20%28SOTA%29%20on%20the%20ImageNet%20dataset.%20In%20this%0Aresearch%2C%20we%20introduce%20an%20academically%20underestimated%20food%20dataset%20CNFOOD-241%2C%0Aand%20pioneer%20the%20integration%20of%20a%20residual%20learning%20framework%20within%20the%20VMamba%0Amodel%20to%20concurrently%20harness%20both%20global%20and%20local%20state%20features%20inherent%20in%0Athe%20original%20VMamba%20architectural%20design.%20The%20research%20results%20show%20that%20VMamba%0Asurpasses%20current%20SOTA%20models%20in%20fine-grained%20and%20food%20classification.%20The%0Aproposed%20Res-VMamba%20further%20improves%20the%20classification%20accuracy%20to%2079.54%5C%25%0Awithout%20pretrained%20weight.%20Our%20findings%20elucidate%20that%20our%20proposed%20methodology%0Aestablishes%20a%20new%20benchmark%20for%20SOTA%20performance%20in%20food%20recognition%20on%20the%0ACNFOOD-241%20dataset.%20The%20code%20can%20be%20obtained%20on%20GitHub%3A%0Ahttps%3A//github.com/ChiShengChen/ResVMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15761v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRes-VMamba%253A%2520Fine-Grained%2520Food%2520Category%2520Visual%2520Classification%2520Using%250A%2520%2520Selective%2520State%2520Space%2520Models%2520with%2520Deep%2520Residual%2520Learning%26entry.906535625%3DChi-Sheng%2520Chen%2520and%2520Guan-Ying%2520Chen%2520and%2520Dong%2520Zhou%2520and%2520Di%2520Jiang%2520and%2520Dai-Shi%2520Chen%26entry.1292438233%3D%2520%2520Food%2520classification%2520is%2520the%2520foundation%2520for%2520developing%2520food%2520vision%2520tasks%2520and%250Aplays%2520a%2520key%2520role%2520in%2520the%2520burgeoning%2520field%2520of%2520computational%2520nutrition.%2520Due%2520to%2520the%250Acomplexity%2520of%2520food%2520requiring%2520fine-grained%2520classification%252C%2520recent%2520academic%250Aresearch%2520mainly%2520modifies%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and/or%2520Vision%250ATransformers%2520%2528ViTs%2529%2520to%2520perform%2520food%2520category%2520classification.%2520However%252C%2520to%2520learn%250Afine-grained%2520features%252C%2520the%2520CNN%2520backbone%2520needs%2520additional%2520structural%2520design%252C%250Awhereas%2520ViT%252C%2520containing%2520the%2520self-attention%2520module%252C%2520has%2520increased%2520computational%250Acomplexity.%2520In%2520recent%2520months%252C%2520a%2520new%2520Sequence%2520State%2520Space%2520%2528S4%2529%2520model%252C%2520through%2520a%250ASelection%2520mechanism%2520and%2520computation%2520with%2520a%2520Scan%2520%2528S6%2529%252C%2520colloquially%2520termed%250AMamba%252C%2520has%2520demonstrated%2520superior%2520performance%2520and%2520computation%2520efficiency%250Acompared%2520to%2520the%2520Transformer%2520architecture.%2520The%2520VMamba%2520model%252C%2520which%2520incorporates%250Athe%2520Mamba%2520mechanism%2520into%2520image%2520tasks%2520%2528such%2520as%2520classification%2529%252C%2520currently%250Aestablishes%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520on%2520the%2520ImageNet%2520dataset.%2520In%2520this%250Aresearch%252C%2520we%2520introduce%2520an%2520academically%2520underestimated%2520food%2520dataset%2520CNFOOD-241%252C%250Aand%2520pioneer%2520the%2520integration%2520of%2520a%2520residual%2520learning%2520framework%2520within%2520the%2520VMamba%250Amodel%2520to%2520concurrently%2520harness%2520both%2520global%2520and%2520local%2520state%2520features%2520inherent%2520in%250Athe%2520original%2520VMamba%2520architectural%2520design.%2520The%2520research%2520results%2520show%2520that%2520VMamba%250Asurpasses%2520current%2520SOTA%2520models%2520in%2520fine-grained%2520and%2520food%2520classification.%2520The%250Aproposed%2520Res-VMamba%2520further%2520improves%2520the%2520classification%2520accuracy%2520to%252079.54%255C%2525%250Awithout%2520pretrained%2520weight.%2520Our%2520findings%2520elucidate%2520that%2520our%2520proposed%2520methodology%250Aestablishes%2520a%2520new%2520benchmark%2520for%2520SOTA%2520performance%2520in%2520food%2520recognition%2520on%2520the%250ACNFOOD-241%2520dataset.%2520The%2520code%2520can%2520be%2520obtained%2520on%2520GitHub%253A%250Ahttps%253A//github.com/ChiShengChen/ResVMamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15761v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Res-VMamba%3A%20Fine-Grained%20Food%20Category%20Visual%20Classification%20Using%0A%20%20Selective%20State%20Space%20Models%20with%20Deep%20Residual%20Learning&entry.906535625=Chi-Sheng%20Chen%20and%20Guan-Ying%20Chen%20and%20Dong%20Zhou%20and%20Di%20Jiang%20and%20Dai-Shi%20Chen&entry.1292438233=%20%20Food%20classification%20is%20the%20foundation%20for%20developing%20food%20vision%20tasks%20and%0Aplays%20a%20key%20role%20in%20the%20burgeoning%20field%20of%20computational%20nutrition.%20Due%20to%20the%0Acomplexity%20of%20food%20requiring%20fine-grained%20classification%2C%20recent%20academic%0Aresearch%20mainly%20modifies%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and/or%20Vision%0ATransformers%20%28ViTs%29%20to%20perform%20food%20category%20classification.%20However%2C%20to%20learn%0Afine-grained%20features%2C%20the%20CNN%20backbone%20needs%20additional%20structural%20design%2C%0Awhereas%20ViT%2C%20containing%20the%20self-attention%20module%2C%20has%20increased%20computational%0Acomplexity.%20In%20recent%20months%2C%20a%20new%20Sequence%20State%20Space%20%28S4%29%20model%2C%20through%20a%0ASelection%20mechanism%20and%20computation%20with%20a%20Scan%20%28S6%29%2C%20colloquially%20termed%0AMamba%2C%20has%20demonstrated%20superior%20performance%20and%20computation%20efficiency%0Acompared%20to%20the%20Transformer%20architecture.%20The%20VMamba%20model%2C%20which%20incorporates%0Athe%20Mamba%20mechanism%20into%20image%20tasks%20%28such%20as%20classification%29%2C%20currently%0Aestablishes%20the%20state-of-the-art%20%28SOTA%29%20on%20the%20ImageNet%20dataset.%20In%20this%0Aresearch%2C%20we%20introduce%20an%20academically%20underestimated%20food%20dataset%20CNFOOD-241%2C%0Aand%20pioneer%20the%20integration%20of%20a%20residual%20learning%20framework%20within%20the%20VMamba%0Amodel%20to%20concurrently%20harness%20both%20global%20and%20local%20state%20features%20inherent%20in%0Athe%20original%20VMamba%20architectural%20design.%20The%20research%20results%20show%20that%20VMamba%0Asurpasses%20current%20SOTA%20models%20in%20fine-grained%20and%20food%20classification.%20The%0Aproposed%20Res-VMamba%20further%20improves%20the%20classification%20accuracy%20to%2079.54%5C%25%0Awithout%20pretrained%20weight.%20Our%20findings%20elucidate%20that%20our%20proposed%20methodology%0Aestablishes%20a%20new%20benchmark%20for%20SOTA%20performance%20in%20food%20recognition%20on%20the%0ACNFOOD-241%20dataset.%20The%20code%20can%20be%20obtained%20on%20GitHub%3A%0Ahttps%3A//github.com/ChiShengChen/ResVMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15761v3&entry.124074799=Read"},
{"title": "Zero-Shot Topic Classification of Column Headers: Leveraging LLMs for\n  Metadata Enrichment", "author": "Margherita Martorana and Tobias Kuhn and Lise Stork and Jacco van Ossenbruggen", "abstract": "  Traditional dataset retrieval systems rely on metadata for indexing, rather\nthan on the underlying data values. However, high-quality metadata creation and\nenrichment often require manual annotations, which is a labour-intensive and\nchallenging process to automate. In this study, we propose a method to support\nmetadata enrichment using topic annotations generated by three Large Language\nModels (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focuses\non classifying column headers based on domain-specific topics from the\nConsortium of European Social Science Data Archives (CESSDA), a Linked Data\ncontrolled vocabulary. Our approach operates in a zero-shot setting,\nintegrating the controlled topic vocabulary directly within the input prompt.\nThis integration serves as a Large Context Windows approach, with the aim of\nimproving the results of the topic classification task.\n  We evaluated the performance of the LLMs in terms of internal consistency,\ninter-machine alignment, and agreement with human classification. Additionally,\nwe investigate the impact of contextual information (i.e., dataset description)\non the classification outcomes. Our findings suggest that ChatGPT and\nGoogleGemini outperform GoogleBard in terms of internal consistency as well as\nLLM-human-agreement. Interestingly, we found that contextual information had no\nsignificant impact on LLM performance.\n  This work proposes a novel approach that leverages LLMs for topic\nclassification of column headers using a controlled vocabulary, presenting a\npractical application of LLMs and Large Context Windows within the Semantic Web\ndomain. This approach has the potential to facilitate automated metadata\nenrichment, thereby enhancing dataset retrieval and the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) of research data on the\nWeb.\n", "link": "http://arxiv.org/abs/2403.00884v3", "date": "2024-09-06", "relevancy": 2.5518, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Topic%20Classification%20of%20Column%20Headers%3A%20Leveraging%20LLMs%20for%0A%20%20Metadata%20Enrichment&body=Title%3A%20Zero-Shot%20Topic%20Classification%20of%20Column%20Headers%3A%20Leveraging%20LLMs%20for%0A%20%20Metadata%20Enrichment%0AAuthor%3A%20Margherita%20Martorana%20and%20Tobias%20Kuhn%20and%20Lise%20Stork%20and%20Jacco%20van%20Ossenbruggen%0AAbstract%3A%20%20%20Traditional%20dataset%20retrieval%20systems%20rely%20on%20metadata%20for%20indexing%2C%20rather%0Athan%20on%20the%20underlying%20data%20values.%20However%2C%20high-quality%20metadata%20creation%20and%0Aenrichment%20often%20require%20manual%20annotations%2C%20which%20is%20a%20labour-intensive%20and%0Achallenging%20process%20to%20automate.%20In%20this%20study%2C%20we%20propose%20a%20method%20to%20support%0Ametadata%20enrichment%20using%20topic%20annotations%20generated%20by%20three%20Large%20Language%0AModels%20%28LLMs%29%3A%20ChatGPT-3.5%2C%20GoogleBard%2C%20and%20GoogleGemini.%20Our%20analysis%20focuses%0Aon%20classifying%20column%20headers%20based%20on%20domain-specific%20topics%20from%20the%0AConsortium%20of%20European%20Social%20Science%20Data%20Archives%20%28CESSDA%29%2C%20a%20Linked%20Data%0Acontrolled%20vocabulary.%20Our%20approach%20operates%20in%20a%20zero-shot%20setting%2C%0Aintegrating%20the%20controlled%20topic%20vocabulary%20directly%20within%20the%20input%20prompt.%0AThis%20integration%20serves%20as%20a%20Large%20Context%20Windows%20approach%2C%20with%20the%20aim%20of%0Aimproving%20the%20results%20of%20the%20topic%20classification%20task.%0A%20%20We%20evaluated%20the%20performance%20of%20the%20LLMs%20in%20terms%20of%20internal%20consistency%2C%0Ainter-machine%20alignment%2C%20and%20agreement%20with%20human%20classification.%20Additionally%2C%0Awe%20investigate%20the%20impact%20of%20contextual%20information%20%28i.e.%2C%20dataset%20description%29%0Aon%20the%20classification%20outcomes.%20Our%20findings%20suggest%20that%20ChatGPT%20and%0AGoogleGemini%20outperform%20GoogleBard%20in%20terms%20of%20internal%20consistency%20as%20well%20as%0ALLM-human-agreement.%20Interestingly%2C%20we%20found%20that%20contextual%20information%20had%20no%0Asignificant%20impact%20on%20LLM%20performance.%0A%20%20This%20work%20proposes%20a%20novel%20approach%20that%20leverages%20LLMs%20for%20topic%0Aclassification%20of%20column%20headers%20using%20a%20controlled%20vocabulary%2C%20presenting%20a%0Apractical%20application%20of%20LLMs%20and%20Large%20Context%20Windows%20within%20the%20Semantic%20Web%0Adomain.%20This%20approach%20has%20the%20potential%20to%20facilitate%20automated%20metadata%0Aenrichment%2C%20thereby%20enhancing%20dataset%20retrieval%20and%20the%20Findability%2C%0AAccessibility%2C%20Interoperability%2C%20and%20Reusability%20%28FAIR%29%20of%20research%20data%20on%20the%0AWeb.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00884v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Topic%2520Classification%2520of%2520Column%2520Headers%253A%2520Leveraging%2520LLMs%2520for%250A%2520%2520Metadata%2520Enrichment%26entry.906535625%3DMargherita%2520Martorana%2520and%2520Tobias%2520Kuhn%2520and%2520Lise%2520Stork%2520and%2520Jacco%2520van%2520Ossenbruggen%26entry.1292438233%3D%2520%2520Traditional%2520dataset%2520retrieval%2520systems%2520rely%2520on%2520metadata%2520for%2520indexing%252C%2520rather%250Athan%2520on%2520the%2520underlying%2520data%2520values.%2520However%252C%2520high-quality%2520metadata%2520creation%2520and%250Aenrichment%2520often%2520require%2520manual%2520annotations%252C%2520which%2520is%2520a%2520labour-intensive%2520and%250Achallenging%2520process%2520to%2520automate.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520method%2520to%2520support%250Ametadata%2520enrichment%2520using%2520topic%2520annotations%2520generated%2520by%2520three%2520Large%2520Language%250AModels%2520%2528LLMs%2529%253A%2520ChatGPT-3.5%252C%2520GoogleBard%252C%2520and%2520GoogleGemini.%2520Our%2520analysis%2520focuses%250Aon%2520classifying%2520column%2520headers%2520based%2520on%2520domain-specific%2520topics%2520from%2520the%250AConsortium%2520of%2520European%2520Social%2520Science%2520Data%2520Archives%2520%2528CESSDA%2529%252C%2520a%2520Linked%2520Data%250Acontrolled%2520vocabulary.%2520Our%2520approach%2520operates%2520in%2520a%2520zero-shot%2520setting%252C%250Aintegrating%2520the%2520controlled%2520topic%2520vocabulary%2520directly%2520within%2520the%2520input%2520prompt.%250AThis%2520integration%2520serves%2520as%2520a%2520Large%2520Context%2520Windows%2520approach%252C%2520with%2520the%2520aim%2520of%250Aimproving%2520the%2520results%2520of%2520the%2520topic%2520classification%2520task.%250A%2520%2520We%2520evaluated%2520the%2520performance%2520of%2520the%2520LLMs%2520in%2520terms%2520of%2520internal%2520consistency%252C%250Ainter-machine%2520alignment%252C%2520and%2520agreement%2520with%2520human%2520classification.%2520Additionally%252C%250Awe%2520investigate%2520the%2520impact%2520of%2520contextual%2520information%2520%2528i.e.%252C%2520dataset%2520description%2529%250Aon%2520the%2520classification%2520outcomes.%2520Our%2520findings%2520suggest%2520that%2520ChatGPT%2520and%250AGoogleGemini%2520outperform%2520GoogleBard%2520in%2520terms%2520of%2520internal%2520consistency%2520as%2520well%2520as%250ALLM-human-agreement.%2520Interestingly%252C%2520we%2520found%2520that%2520contextual%2520information%2520had%2520no%250Asignificant%2520impact%2520on%2520LLM%2520performance.%250A%2520%2520This%2520work%2520proposes%2520a%2520novel%2520approach%2520that%2520leverages%2520LLMs%2520for%2520topic%250Aclassification%2520of%2520column%2520headers%2520using%2520a%2520controlled%2520vocabulary%252C%2520presenting%2520a%250Apractical%2520application%2520of%2520LLMs%2520and%2520Large%2520Context%2520Windows%2520within%2520the%2520Semantic%2520Web%250Adomain.%2520This%2520approach%2520has%2520the%2520potential%2520to%2520facilitate%2520automated%2520metadata%250Aenrichment%252C%2520thereby%2520enhancing%2520dataset%2520retrieval%2520and%2520the%2520Findability%252C%250AAccessibility%252C%2520Interoperability%252C%2520and%2520Reusability%2520%2528FAIR%2529%2520of%2520research%2520data%2520on%2520the%250AWeb.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00884v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Topic%20Classification%20of%20Column%20Headers%3A%20Leveraging%20LLMs%20for%0A%20%20Metadata%20Enrichment&entry.906535625=Margherita%20Martorana%20and%20Tobias%20Kuhn%20and%20Lise%20Stork%20and%20Jacco%20van%20Ossenbruggen&entry.1292438233=%20%20Traditional%20dataset%20retrieval%20systems%20rely%20on%20metadata%20for%20indexing%2C%20rather%0Athan%20on%20the%20underlying%20data%20values.%20However%2C%20high-quality%20metadata%20creation%20and%0Aenrichment%20often%20require%20manual%20annotations%2C%20which%20is%20a%20labour-intensive%20and%0Achallenging%20process%20to%20automate.%20In%20this%20study%2C%20we%20propose%20a%20method%20to%20support%0Ametadata%20enrichment%20using%20topic%20annotations%20generated%20by%20three%20Large%20Language%0AModels%20%28LLMs%29%3A%20ChatGPT-3.5%2C%20GoogleBard%2C%20and%20GoogleGemini.%20Our%20analysis%20focuses%0Aon%20classifying%20column%20headers%20based%20on%20domain-specific%20topics%20from%20the%0AConsortium%20of%20European%20Social%20Science%20Data%20Archives%20%28CESSDA%29%2C%20a%20Linked%20Data%0Acontrolled%20vocabulary.%20Our%20approach%20operates%20in%20a%20zero-shot%20setting%2C%0Aintegrating%20the%20controlled%20topic%20vocabulary%20directly%20within%20the%20input%20prompt.%0AThis%20integration%20serves%20as%20a%20Large%20Context%20Windows%20approach%2C%20with%20the%20aim%20of%0Aimproving%20the%20results%20of%20the%20topic%20classification%20task.%0A%20%20We%20evaluated%20the%20performance%20of%20the%20LLMs%20in%20terms%20of%20internal%20consistency%2C%0Ainter-machine%20alignment%2C%20and%20agreement%20with%20human%20classification.%20Additionally%2C%0Awe%20investigate%20the%20impact%20of%20contextual%20information%20%28i.e.%2C%20dataset%20description%29%0Aon%20the%20classification%20outcomes.%20Our%20findings%20suggest%20that%20ChatGPT%20and%0AGoogleGemini%20outperform%20GoogleBard%20in%20terms%20of%20internal%20consistency%20as%20well%20as%0ALLM-human-agreement.%20Interestingly%2C%20we%20found%20that%20contextual%20information%20had%20no%0Asignificant%20impact%20on%20LLM%20performance.%0A%20%20This%20work%20proposes%20a%20novel%20approach%20that%20leverages%20LLMs%20for%20topic%0Aclassification%20of%20column%20headers%20using%20a%20controlled%20vocabulary%2C%20presenting%20a%0Apractical%20application%20of%20LLMs%20and%20Large%20Context%20Windows%20within%20the%20Semantic%20Web%0Adomain.%20This%20approach%20has%20the%20potential%20to%20facilitate%20automated%20metadata%0Aenrichment%2C%20thereby%20enhancing%20dataset%20retrieval%20and%20the%20Findability%2C%0AAccessibility%2C%20Interoperability%2C%20and%20Reusability%20%28FAIR%29%20of%20research%20data%20on%20the%0AWeb.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00884v3&entry.124074799=Read"},
{"title": "Residual Stream Analysis with Multi-Layer SAEs", "author": "Tim Lawson and Lucy Farnik and Conor Houghton and Laurence Aitchison", "abstract": "  Sparse autoencoders (SAEs) are a promising approach to interpreting the\ninternal representations of transformer language models. However, standard SAEs\nare trained separately on each transformer layer, making it difficult to use\nthem to study how information flows across layers. To solve this problem, we\nintroduce the multi-layer SAE (MLSAE): a single SAE trained on the residual\nstream activation vectors from every transformer layer simultaneously. The\nresidual stream is usually understood as preserving information across layers,\nso we expected to, and did, find individual SAE features that are active at\nmultiple layers. Interestingly, while a single SAE feature is active at\ndifferent layers for different prompts, for a single prompt, we find that a\nsingle feature is far more likely to be active at a single layer. For larger\nunderlying models, we find that the cosine similarities between adjacent layers\nin the residual stream are higher, so we expect more features to be active at\nmultiple layers. These results show that MLSAEs are a promising method to study\ninformation flow in transformers. We release our code to train and analyze\nMLSAEs at https://github.com/tim-lawson/mlsae.\n", "link": "http://arxiv.org/abs/2409.04185v1", "date": "2024-09-06", "relevancy": 2.5501, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Stream%20Analysis%20with%20Multi-Layer%20SAEs&body=Title%3A%20Residual%20Stream%20Analysis%20with%20Multi-Layer%20SAEs%0AAuthor%3A%20Tim%20Lawson%20and%20Lucy%20Farnik%20and%20Conor%20Houghton%20and%20Laurence%20Aitchison%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20approach%20to%20interpreting%20the%0Ainternal%20representations%20of%20transformer%20language%20models.%20However%2C%20standard%20SAEs%0Aare%20trained%20separately%20on%20each%20transformer%20layer%2C%20making%20it%20difficult%20to%20use%0Athem%20to%20study%20how%20information%20flows%20across%20layers.%20To%20solve%20this%20problem%2C%20we%0Aintroduce%20the%20multi-layer%20SAE%20%28MLSAE%29%3A%20a%20single%20SAE%20trained%20on%20the%20residual%0Astream%20activation%20vectors%20from%20every%20transformer%20layer%20simultaneously.%20The%0Aresidual%20stream%20is%20usually%20understood%20as%20preserving%20information%20across%20layers%2C%0Aso%20we%20expected%20to%2C%20and%20did%2C%20find%20individual%20SAE%20features%20that%20are%20active%20at%0Amultiple%20layers.%20Interestingly%2C%20while%20a%20single%20SAE%20feature%20is%20active%20at%0Adifferent%20layers%20for%20different%20prompts%2C%20for%20a%20single%20prompt%2C%20we%20find%20that%20a%0Asingle%20feature%20is%20far%20more%20likely%20to%20be%20active%20at%20a%20single%20layer.%20For%20larger%0Aunderlying%20models%2C%20we%20find%20that%20the%20cosine%20similarities%20between%20adjacent%20layers%0Ain%20the%20residual%20stream%20are%20higher%2C%20so%20we%20expect%20more%20features%20to%20be%20active%20at%0Amultiple%20layers.%20These%20results%20show%20that%20MLSAEs%20are%20a%20promising%20method%20to%20study%0Ainformation%20flow%20in%20transformers.%20We%20release%20our%20code%20to%20train%20and%20analyze%0AMLSAEs%20at%20https%3A//github.com/tim-lawson/mlsae.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Stream%2520Analysis%2520with%2520Multi-Layer%2520SAEs%26entry.906535625%3DTim%2520Lawson%2520and%2520Lucy%2520Farnik%2520and%2520Conor%2520Houghton%2520and%2520Laurence%2520Aitchison%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520promising%2520approach%2520to%2520interpreting%2520the%250Ainternal%2520representations%2520of%2520transformer%2520language%2520models.%2520However%252C%2520standard%2520SAEs%250Aare%2520trained%2520separately%2520on%2520each%2520transformer%2520layer%252C%2520making%2520it%2520difficult%2520to%2520use%250Athem%2520to%2520study%2520how%2520information%2520flows%2520across%2520layers.%2520To%2520solve%2520this%2520problem%252C%2520we%250Aintroduce%2520the%2520multi-layer%2520SAE%2520%2528MLSAE%2529%253A%2520a%2520single%2520SAE%2520trained%2520on%2520the%2520residual%250Astream%2520activation%2520vectors%2520from%2520every%2520transformer%2520layer%2520simultaneously.%2520The%250Aresidual%2520stream%2520is%2520usually%2520understood%2520as%2520preserving%2520information%2520across%2520layers%252C%250Aso%2520we%2520expected%2520to%252C%2520and%2520did%252C%2520find%2520individual%2520SAE%2520features%2520that%2520are%2520active%2520at%250Amultiple%2520layers.%2520Interestingly%252C%2520while%2520a%2520single%2520SAE%2520feature%2520is%2520active%2520at%250Adifferent%2520layers%2520for%2520different%2520prompts%252C%2520for%2520a%2520single%2520prompt%252C%2520we%2520find%2520that%2520a%250Asingle%2520feature%2520is%2520far%2520more%2520likely%2520to%2520be%2520active%2520at%2520a%2520single%2520layer.%2520For%2520larger%250Aunderlying%2520models%252C%2520we%2520find%2520that%2520the%2520cosine%2520similarities%2520between%2520adjacent%2520layers%250Ain%2520the%2520residual%2520stream%2520are%2520higher%252C%2520so%2520we%2520expect%2520more%2520features%2520to%2520be%2520active%2520at%250Amultiple%2520layers.%2520These%2520results%2520show%2520that%2520MLSAEs%2520are%2520a%2520promising%2520method%2520to%2520study%250Ainformation%2520flow%2520in%2520transformers.%2520We%2520release%2520our%2520code%2520to%2520train%2520and%2520analyze%250AMLSAEs%2520at%2520https%253A//github.com/tim-lawson/mlsae.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Stream%20Analysis%20with%20Multi-Layer%20SAEs&entry.906535625=Tim%20Lawson%20and%20Lucy%20Farnik%20and%20Conor%20Houghton%20and%20Laurence%20Aitchison&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20approach%20to%20interpreting%20the%0Ainternal%20representations%20of%20transformer%20language%20models.%20However%2C%20standard%20SAEs%0Aare%20trained%20separately%20on%20each%20transformer%20layer%2C%20making%20it%20difficult%20to%20use%0Athem%20to%20study%20how%20information%20flows%20across%20layers.%20To%20solve%20this%20problem%2C%20we%0Aintroduce%20the%20multi-layer%20SAE%20%28MLSAE%29%3A%20a%20single%20SAE%20trained%20on%20the%20residual%0Astream%20activation%20vectors%20from%20every%20transformer%20layer%20simultaneously.%20The%0Aresidual%20stream%20is%20usually%20understood%20as%20preserving%20information%20across%20layers%2C%0Aso%20we%20expected%20to%2C%20and%20did%2C%20find%20individual%20SAE%20features%20that%20are%20active%20at%0Amultiple%20layers.%20Interestingly%2C%20while%20a%20single%20SAE%20feature%20is%20active%20at%0Adifferent%20layers%20for%20different%20prompts%2C%20for%20a%20single%20prompt%2C%20we%20find%20that%20a%0Asingle%20feature%20is%20far%20more%20likely%20to%20be%20active%20at%20a%20single%20layer.%20For%20larger%0Aunderlying%20models%2C%20we%20find%20that%20the%20cosine%20similarities%20between%20adjacent%20layers%0Ain%20the%20residual%20stream%20are%20higher%2C%20so%20we%20expect%20more%20features%20to%20be%20active%20at%0Amultiple%20layers.%20These%20results%20show%20that%20MLSAEs%20are%20a%20promising%20method%20to%20study%0Ainformation%20flow%20in%20transformers.%20We%20release%20our%20code%20to%20train%20and%20analyze%0AMLSAEs%20at%20https%3A//github.com/tim-lawson/mlsae.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04185v1&entry.124074799=Read"},
{"title": "Learning vs Retrieval: The Role of In-Context Examples in Regression\n  with LLMs", "author": "Aliakbar Nafar and Kristen Brent Venable and Parisa Kordjamshidi", "abstract": "  Generative Large Language Models (LLMs) are capable of being in-context\nlearners. However, the underlying mechanism of in-context learning (ICL) is\nstill a major research question, and experimental research results about how\nmodels exploit ICL are not always consistent. In this work, we propose a\nframework for evaluating in-context learning mechanisms, which we claim are a\ncombination of retrieving internal knowledge and learning from in-context\nexamples by focusing on regression tasks. First, we show that LLMs can perform\nregression on real-world datasets and then design experiments to measure the\nextent to which the LLM retrieves its internal knowledge versus learning from\nin-context examples. We argue that this process lies on a spectrum between\nthese two extremes. We provide an in-depth analysis of the degrees to which\nthese mechanisms are triggered depending on various factors, such as prior\nknowledge about the tasks and the type and richness of the information provided\nby the in-context examples. We employ three LLMs and utilize multiple datasets\nto corroborate the robustness of our findings. Our results shed light on how to\nengineer prompts to leverage meta-learning from in-context examples and foster\nknowledge retrieval depending on the problem being addressed.\n", "link": "http://arxiv.org/abs/2409.04318v1", "date": "2024-09-06", "relevancy": 2.5417, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20vs%20Retrieval%3A%20The%20Role%20of%20In-Context%20Examples%20in%20Regression%0A%20%20with%20LLMs&body=Title%3A%20Learning%20vs%20Retrieval%3A%20The%20Role%20of%20In-Context%20Examples%20in%20Regression%0A%20%20with%20LLMs%0AAuthor%3A%20Aliakbar%20Nafar%20and%20Kristen%20Brent%20Venable%20and%20Parisa%20Kordjamshidi%0AAbstract%3A%20%20%20Generative%20Large%20Language%20Models%20%28LLMs%29%20are%20capable%20of%20being%20in-context%0Alearners.%20However%2C%20the%20underlying%20mechanism%20of%20in-context%20learning%20%28ICL%29%20is%0Astill%20a%20major%20research%20question%2C%20and%20experimental%20research%20results%20about%20how%0Amodels%20exploit%20ICL%20are%20not%20always%20consistent.%20In%20this%20work%2C%20we%20propose%20a%0Aframework%20for%20evaluating%20in-context%20learning%20mechanisms%2C%20which%20we%20claim%20are%20a%0Acombination%20of%20retrieving%20internal%20knowledge%20and%20learning%20from%20in-context%0Aexamples%20by%20focusing%20on%20regression%20tasks.%20First%2C%20we%20show%20that%20LLMs%20can%20perform%0Aregression%20on%20real-world%20datasets%20and%20then%20design%20experiments%20to%20measure%20the%0Aextent%20to%20which%20the%20LLM%20retrieves%20its%20internal%20knowledge%20versus%20learning%20from%0Ain-context%20examples.%20We%20argue%20that%20this%20process%20lies%20on%20a%20spectrum%20between%0Athese%20two%20extremes.%20We%20provide%20an%20in-depth%20analysis%20of%20the%20degrees%20to%20which%0Athese%20mechanisms%20are%20triggered%20depending%20on%20various%20factors%2C%20such%20as%20prior%0Aknowledge%20about%20the%20tasks%20and%20the%20type%20and%20richness%20of%20the%20information%20provided%0Aby%20the%20in-context%20examples.%20We%20employ%20three%20LLMs%20and%20utilize%20multiple%20datasets%0Ato%20corroborate%20the%20robustness%20of%20our%20findings.%20Our%20results%20shed%20light%20on%20how%20to%0Aengineer%20prompts%20to%20leverage%20meta-learning%20from%20in-context%20examples%20and%20foster%0Aknowledge%20retrieval%20depending%20on%20the%20problem%20being%20addressed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520vs%2520Retrieval%253A%2520The%2520Role%2520of%2520In-Context%2520Examples%2520in%2520Regression%250A%2520%2520with%2520LLMs%26entry.906535625%3DAliakbar%2520Nafar%2520and%2520Kristen%2520Brent%2520Venable%2520and%2520Parisa%2520Kordjamshidi%26entry.1292438233%3D%2520%2520Generative%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520capable%2520of%2520being%2520in-context%250Alearners.%2520However%252C%2520the%2520underlying%2520mechanism%2520of%2520in-context%2520learning%2520%2528ICL%2529%2520is%250Astill%2520a%2520major%2520research%2520question%252C%2520and%2520experimental%2520research%2520results%2520about%2520how%250Amodels%2520exploit%2520ICL%2520are%2520not%2520always%2520consistent.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Aframework%2520for%2520evaluating%2520in-context%2520learning%2520mechanisms%252C%2520which%2520we%2520claim%2520are%2520a%250Acombination%2520of%2520retrieving%2520internal%2520knowledge%2520and%2520learning%2520from%2520in-context%250Aexamples%2520by%2520focusing%2520on%2520regression%2520tasks.%2520First%252C%2520we%2520show%2520that%2520LLMs%2520can%2520perform%250Aregression%2520on%2520real-world%2520datasets%2520and%2520then%2520design%2520experiments%2520to%2520measure%2520the%250Aextent%2520to%2520which%2520the%2520LLM%2520retrieves%2520its%2520internal%2520knowledge%2520versus%2520learning%2520from%250Ain-context%2520examples.%2520We%2520argue%2520that%2520this%2520process%2520lies%2520on%2520a%2520spectrum%2520between%250Athese%2520two%2520extremes.%2520We%2520provide%2520an%2520in-depth%2520analysis%2520of%2520the%2520degrees%2520to%2520which%250Athese%2520mechanisms%2520are%2520triggered%2520depending%2520on%2520various%2520factors%252C%2520such%2520as%2520prior%250Aknowledge%2520about%2520the%2520tasks%2520and%2520the%2520type%2520and%2520richness%2520of%2520the%2520information%2520provided%250Aby%2520the%2520in-context%2520examples.%2520We%2520employ%2520three%2520LLMs%2520and%2520utilize%2520multiple%2520datasets%250Ato%2520corroborate%2520the%2520robustness%2520of%2520our%2520findings.%2520Our%2520results%2520shed%2520light%2520on%2520how%2520to%250Aengineer%2520prompts%2520to%2520leverage%2520meta-learning%2520from%2520in-context%2520examples%2520and%2520foster%250Aknowledge%2520retrieval%2520depending%2520on%2520the%2520problem%2520being%2520addressed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20vs%20Retrieval%3A%20The%20Role%20of%20In-Context%20Examples%20in%20Regression%0A%20%20with%20LLMs&entry.906535625=Aliakbar%20Nafar%20and%20Kristen%20Brent%20Venable%20and%20Parisa%20Kordjamshidi&entry.1292438233=%20%20Generative%20Large%20Language%20Models%20%28LLMs%29%20are%20capable%20of%20being%20in-context%0Alearners.%20However%2C%20the%20underlying%20mechanism%20of%20in-context%20learning%20%28ICL%29%20is%0Astill%20a%20major%20research%20question%2C%20and%20experimental%20research%20results%20about%20how%0Amodels%20exploit%20ICL%20are%20not%20always%20consistent.%20In%20this%20work%2C%20we%20propose%20a%0Aframework%20for%20evaluating%20in-context%20learning%20mechanisms%2C%20which%20we%20claim%20are%20a%0Acombination%20of%20retrieving%20internal%20knowledge%20and%20learning%20from%20in-context%0Aexamples%20by%20focusing%20on%20regression%20tasks.%20First%2C%20we%20show%20that%20LLMs%20can%20perform%0Aregression%20on%20real-world%20datasets%20and%20then%20design%20experiments%20to%20measure%20the%0Aextent%20to%20which%20the%20LLM%20retrieves%20its%20internal%20knowledge%20versus%20learning%20from%0Ain-context%20examples.%20We%20argue%20that%20this%20process%20lies%20on%20a%20spectrum%20between%0Athese%20two%20extremes.%20We%20provide%20an%20in-depth%20analysis%20of%20the%20degrees%20to%20which%0Athese%20mechanisms%20are%20triggered%20depending%20on%20various%20factors%2C%20such%20as%20prior%0Aknowledge%20about%20the%20tasks%20and%20the%20type%20and%20richness%20of%20the%20information%20provided%0Aby%20the%20in-context%20examples.%20We%20employ%20three%20LLMs%20and%20utilize%20multiple%20datasets%0Ato%20corroborate%20the%20robustness%20of%20our%20findings.%20Our%20results%20shed%20light%20on%20how%20to%0Aengineer%20prompts%20to%20leverage%20meta-learning%20from%20in-context%20examples%20and%20foster%0Aknowledge%20retrieval%20depending%20on%20the%20problem%20being%20addressed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04318v1&entry.124074799=Read"},
{"title": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding", "author": "Ziyin Zhang and Hang Yu and Shijie Li and Peng Di and Jianguo Li and Rui Wang", "abstract": "  Programming languages possess rich semantic information such as data flow\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with four different baseline LLMs ranging in size from 350M to 8B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3.\n", "link": "http://arxiv.org/abs/2409.04183v1", "date": "2024-09-06", "relevancy": 2.5382, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5082}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5082}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GALLa%3A%20Graph%20Aligned%20Large%20Language%20Models%20for%20Improved%20Source%20Code%0A%20%20Understanding&body=Title%3A%20GALLa%3A%20Graph%20Aligned%20Large%20Language%20Models%20for%20Improved%20Source%20Code%0A%20%20Understanding%0AAuthor%3A%20Ziyin%20Zhang%20and%20Hang%20Yu%20and%20Shijie%20Li%20and%20Peng%20Di%20and%20Jianguo%20Li%20and%20Rui%20Wang%0AAbstract%3A%20%20%20Programming%20languages%20possess%20rich%20semantic%20information%20such%20as%20data%20flow%0Athat%20is%20represented%20by%20graphs%20and%20not%20available%20from%20the%20surface%20form%20of%20source%0Acode.%20Recent%20code%20language%20models%20have%20scaled%20to%20billions%20of%20parameters%2C%20but%0Amodel%20source%20code%20solely%20as%20text%20tokens%20while%20ignoring%20any%20other%20structural%0Ainformation.%20Conversely%2C%20models%20that%20do%20encode%20structural%20information%20of%20code%0Amake%20modifications%20to%20the%20Transformer%20architecture%2C%20limiting%20their%20scale%20and%0Acompatibility%20with%20pretrained%20LLMs.%20In%20this%20work%2C%20we%20take%20the%20best%20of%20both%0Aworlds%20with%20GALLa%20-%20Graph%20Aligned%20Large%20Language%20Model.%20GALLa%20utilizes%20graph%0Aneural%20networks%20and%20cross-modal%20alignment%20technologies%20to%20inject%20the%20structural%0Ainformation%20of%20code%20into%20LLMs%20as%20an%20auxiliary%20task%20during%20finetuning.%20This%0Aframework%20is%20both%20model-agnostic%20and%20task-agnostic%2C%20as%20it%20can%20be%20applied%20to%20any%0Acode%20LLM%20for%20any%20code%20downstream%20task%2C%20and%20requires%20the%20structural%20graph%20data%0Aonly%20at%20training%20time%20from%20a%20corpus%20unrelated%20to%20the%20finetuning%20data%2C%20while%0Aincurring%20no%20cost%20at%20inference%20time%20over%20the%20baseline%20LLM.%20Experiments%20on%20five%0Acode%20tasks%20with%20four%20different%20baseline%20LLMs%20ranging%20in%20size%20from%20350M%20to%208B%0Avalidate%20the%20effectiveness%20of%20GALLa%2C%20demonstrating%20consistent%20improvement%20over%0Athe%20baseline%2C%20even%20for%20powerful%20models%20such%20as%20LLaMA3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGALLa%253A%2520Graph%2520Aligned%2520Large%2520Language%2520Models%2520for%2520Improved%2520Source%2520Code%250A%2520%2520Understanding%26entry.906535625%3DZiyin%2520Zhang%2520and%2520Hang%2520Yu%2520and%2520Shijie%2520Li%2520and%2520Peng%2520Di%2520and%2520Jianguo%2520Li%2520and%2520Rui%2520Wang%26entry.1292438233%3D%2520%2520Programming%2520languages%2520possess%2520rich%2520semantic%2520information%2520such%2520as%2520data%2520flow%250Athat%2520is%2520represented%2520by%2520graphs%2520and%2520not%2520available%2520from%2520the%2520surface%2520form%2520of%2520source%250Acode.%2520Recent%2520code%2520language%2520models%2520have%2520scaled%2520to%2520billions%2520of%2520parameters%252C%2520but%250Amodel%2520source%2520code%2520solely%2520as%2520text%2520tokens%2520while%2520ignoring%2520any%2520other%2520structural%250Ainformation.%2520Conversely%252C%2520models%2520that%2520do%2520encode%2520structural%2520information%2520of%2520code%250Amake%2520modifications%2520to%2520the%2520Transformer%2520architecture%252C%2520limiting%2520their%2520scale%2520and%250Acompatibility%2520with%2520pretrained%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520take%2520the%2520best%2520of%2520both%250Aworlds%2520with%2520GALLa%2520-%2520Graph%2520Aligned%2520Large%2520Language%2520Model.%2520GALLa%2520utilizes%2520graph%250Aneural%2520networks%2520and%2520cross-modal%2520alignment%2520technologies%2520to%2520inject%2520the%2520structural%250Ainformation%2520of%2520code%2520into%2520LLMs%2520as%2520an%2520auxiliary%2520task%2520during%2520finetuning.%2520This%250Aframework%2520is%2520both%2520model-agnostic%2520and%2520task-agnostic%252C%2520as%2520it%2520can%2520be%2520applied%2520to%2520any%250Acode%2520LLM%2520for%2520any%2520code%2520downstream%2520task%252C%2520and%2520requires%2520the%2520structural%2520graph%2520data%250Aonly%2520at%2520training%2520time%2520from%2520a%2520corpus%2520unrelated%2520to%2520the%2520finetuning%2520data%252C%2520while%250Aincurring%2520no%2520cost%2520at%2520inference%2520time%2520over%2520the%2520baseline%2520LLM.%2520Experiments%2520on%2520five%250Acode%2520tasks%2520with%2520four%2520different%2520baseline%2520LLMs%2520ranging%2520in%2520size%2520from%2520350M%2520to%25208B%250Avalidate%2520the%2520effectiveness%2520of%2520GALLa%252C%2520demonstrating%2520consistent%2520improvement%2520over%250Athe%2520baseline%252C%2520even%2520for%2520powerful%2520models%2520such%2520as%2520LLaMA3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GALLa%3A%20Graph%20Aligned%20Large%20Language%20Models%20for%20Improved%20Source%20Code%0A%20%20Understanding&entry.906535625=Ziyin%20Zhang%20and%20Hang%20Yu%20and%20Shijie%20Li%20and%20Peng%20Di%20and%20Jianguo%20Li%20and%20Rui%20Wang&entry.1292438233=%20%20Programming%20languages%20possess%20rich%20semantic%20information%20such%20as%20data%20flow%0Athat%20is%20represented%20by%20graphs%20and%20not%20available%20from%20the%20surface%20form%20of%20source%0Acode.%20Recent%20code%20language%20models%20have%20scaled%20to%20billions%20of%20parameters%2C%20but%0Amodel%20source%20code%20solely%20as%20text%20tokens%20while%20ignoring%20any%20other%20structural%0Ainformation.%20Conversely%2C%20models%20that%20do%20encode%20structural%20information%20of%20code%0Amake%20modifications%20to%20the%20Transformer%20architecture%2C%20limiting%20their%20scale%20and%0Acompatibility%20with%20pretrained%20LLMs.%20In%20this%20work%2C%20we%20take%20the%20best%20of%20both%0Aworlds%20with%20GALLa%20-%20Graph%20Aligned%20Large%20Language%20Model.%20GALLa%20utilizes%20graph%0Aneural%20networks%20and%20cross-modal%20alignment%20technologies%20to%20inject%20the%20structural%0Ainformation%20of%20code%20into%20LLMs%20as%20an%20auxiliary%20task%20during%20finetuning.%20This%0Aframework%20is%20both%20model-agnostic%20and%20task-agnostic%2C%20as%20it%20can%20be%20applied%20to%20any%0Acode%20LLM%20for%20any%20code%20downstream%20task%2C%20and%20requires%20the%20structural%20graph%20data%0Aonly%20at%20training%20time%20from%20a%20corpus%20unrelated%20to%20the%20finetuning%20data%2C%20while%0Aincurring%20no%20cost%20at%20inference%20time%20over%20the%20baseline%20LLM.%20Experiments%20on%20five%0Acode%20tasks%20with%20four%20different%20baseline%20LLMs%20ranging%20in%20size%20from%20350M%20to%208B%0Avalidate%20the%20effectiveness%20of%20GALLa%2C%20demonstrating%20consistent%20improvement%20over%0Athe%20baseline%2C%20even%20for%20powerful%20models%20such%20as%20LLaMA3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04183v1&entry.124074799=Read"},
{"title": "Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language\n  Models for Text-to-Code Generation", "author": "Luis Mayer and Christian Heumann and Matthias A\u00dfenmacher", "abstract": "  In recent years, large language models (LLMs) have emerged as powerful tools\nwith potential applications in various fields, including software engineering.\nWithin the scope of this research, we evaluate five different state-of-the-art\nLLMs - Bard, BingChat, ChatGPT, Llama2, and Code Llama - concerning their\ncapabilities for text-to-code generation. In an empirical study, we feed\nprompts with textual descriptions of coding problems sourced from the\nprogramming website LeetCode to the models with the task of creating solutions\nin Python. Subsequently, the quality of the generated outputs is assessed using\nthe testing functionalities of LeetCode. The results indicate large differences\nin performance between the investigated models. ChatGPT can handle these\ntypical programming challenges by far the most effectively, surpassing even\ncode-specialized models like Code Llama. To gain further insights, we measure\nthe runtime as well as the memory usage of the generated outputs and compared\nthem to the other code submissions on Leetcode. A detailed error analysis,\nencompassing a comparison of the differences concerning correct indentation and\nform of the generated code as well as an assignment of the incorrectly solved\ntasks to certain error categories allows us to obtain a more nuanced picture of\nthe results and potential for improvement. The results also show a clear\npattern of increasingly incorrect produced code when the models are facing a\nlot of context in the form of longer prompts.\n", "link": "http://arxiv.org/abs/2409.04164v1", "date": "2024-09-06", "relevancy": 2.4709, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5129}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20OpenSource%20beat%20ChatGPT%3F%20--%20A%20Comparative%20Study%20of%20Large%20Language%0A%20%20Models%20for%20Text-to-Code%20Generation&body=Title%3A%20Can%20OpenSource%20beat%20ChatGPT%3F%20--%20A%20Comparative%20Study%20of%20Large%20Language%0A%20%20Models%20for%20Text-to-Code%20Generation%0AAuthor%3A%20Luis%20Mayer%20and%20Christian%20Heumann%20and%20Matthias%20A%C3%9Fenmacher%0AAbstract%3A%20%20%20In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%0Awith%20potential%20applications%20in%20various%20fields%2C%20including%20software%20engineering.%0AWithin%20the%20scope%20of%20this%20research%2C%20we%20evaluate%20five%20different%20state-of-the-art%0ALLMs%20-%20Bard%2C%20BingChat%2C%20ChatGPT%2C%20Llama2%2C%20and%20Code%20Llama%20-%20concerning%20their%0Acapabilities%20for%20text-to-code%20generation.%20In%20an%20empirical%20study%2C%20we%20feed%0Aprompts%20with%20textual%20descriptions%20of%20coding%20problems%20sourced%20from%20the%0Aprogramming%20website%20LeetCode%20to%20the%20models%20with%20the%20task%20of%20creating%20solutions%0Ain%20Python.%20Subsequently%2C%20the%20quality%20of%20the%20generated%20outputs%20is%20assessed%20using%0Athe%20testing%20functionalities%20of%20LeetCode.%20The%20results%20indicate%20large%20differences%0Ain%20performance%20between%20the%20investigated%20models.%20ChatGPT%20can%20handle%20these%0Atypical%20programming%20challenges%20by%20far%20the%20most%20effectively%2C%20surpassing%20even%0Acode-specialized%20models%20like%20Code%20Llama.%20To%20gain%20further%20insights%2C%20we%20measure%0Athe%20runtime%20as%20well%20as%20the%20memory%20usage%20of%20the%20generated%20outputs%20and%20compared%0Athem%20to%20the%20other%20code%20submissions%20on%20Leetcode.%20A%20detailed%20error%20analysis%2C%0Aencompassing%20a%20comparison%20of%20the%20differences%20concerning%20correct%20indentation%20and%0Aform%20of%20the%20generated%20code%20as%20well%20as%20an%20assignment%20of%20the%20incorrectly%20solved%0Atasks%20to%20certain%20error%20categories%20allows%20us%20to%20obtain%20a%20more%20nuanced%20picture%20of%0Athe%20results%20and%20potential%20for%20improvement.%20The%20results%20also%20show%20a%20clear%0Apattern%20of%20increasingly%20incorrect%20produced%20code%20when%20the%20models%20are%20facing%20a%0Alot%20of%20context%20in%20the%20form%20of%20longer%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520OpenSource%2520beat%2520ChatGPT%253F%2520--%2520A%2520Comparative%2520Study%2520of%2520Large%2520Language%250A%2520%2520Models%2520for%2520Text-to-Code%2520Generation%26entry.906535625%3DLuis%2520Mayer%2520and%2520Christian%2520Heumann%2520and%2520Matthias%2520A%25C3%259Fenmacher%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%250Awith%2520potential%2520applications%2520in%2520various%2520fields%252C%2520including%2520software%2520engineering.%250AWithin%2520the%2520scope%2520of%2520this%2520research%252C%2520we%2520evaluate%2520five%2520different%2520state-of-the-art%250ALLMs%2520-%2520Bard%252C%2520BingChat%252C%2520ChatGPT%252C%2520Llama2%252C%2520and%2520Code%2520Llama%2520-%2520concerning%2520their%250Acapabilities%2520for%2520text-to-code%2520generation.%2520In%2520an%2520empirical%2520study%252C%2520we%2520feed%250Aprompts%2520with%2520textual%2520descriptions%2520of%2520coding%2520problems%2520sourced%2520from%2520the%250Aprogramming%2520website%2520LeetCode%2520to%2520the%2520models%2520with%2520the%2520task%2520of%2520creating%2520solutions%250Ain%2520Python.%2520Subsequently%252C%2520the%2520quality%2520of%2520the%2520generated%2520outputs%2520is%2520assessed%2520using%250Athe%2520testing%2520functionalities%2520of%2520LeetCode.%2520The%2520results%2520indicate%2520large%2520differences%250Ain%2520performance%2520between%2520the%2520investigated%2520models.%2520ChatGPT%2520can%2520handle%2520these%250Atypical%2520programming%2520challenges%2520by%2520far%2520the%2520most%2520effectively%252C%2520surpassing%2520even%250Acode-specialized%2520models%2520like%2520Code%2520Llama.%2520To%2520gain%2520further%2520insights%252C%2520we%2520measure%250Athe%2520runtime%2520as%2520well%2520as%2520the%2520memory%2520usage%2520of%2520the%2520generated%2520outputs%2520and%2520compared%250Athem%2520to%2520the%2520other%2520code%2520submissions%2520on%2520Leetcode.%2520A%2520detailed%2520error%2520analysis%252C%250Aencompassing%2520a%2520comparison%2520of%2520the%2520differences%2520concerning%2520correct%2520indentation%2520and%250Aform%2520of%2520the%2520generated%2520code%2520as%2520well%2520as%2520an%2520assignment%2520of%2520the%2520incorrectly%2520solved%250Atasks%2520to%2520certain%2520error%2520categories%2520allows%2520us%2520to%2520obtain%2520a%2520more%2520nuanced%2520picture%2520of%250Athe%2520results%2520and%2520potential%2520for%2520improvement.%2520The%2520results%2520also%2520show%2520a%2520clear%250Apattern%2520of%2520increasingly%2520incorrect%2520produced%2520code%2520when%2520the%2520models%2520are%2520facing%2520a%250Alot%2520of%2520context%2520in%2520the%2520form%2520of%2520longer%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20OpenSource%20beat%20ChatGPT%3F%20--%20A%20Comparative%20Study%20of%20Large%20Language%0A%20%20Models%20for%20Text-to-Code%20Generation&entry.906535625=Luis%20Mayer%20and%20Christian%20Heumann%20and%20Matthias%20A%C3%9Fenmacher&entry.1292438233=%20%20In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%0Awith%20potential%20applications%20in%20various%20fields%2C%20including%20software%20engineering.%0AWithin%20the%20scope%20of%20this%20research%2C%20we%20evaluate%20five%20different%20state-of-the-art%0ALLMs%20-%20Bard%2C%20BingChat%2C%20ChatGPT%2C%20Llama2%2C%20and%20Code%20Llama%20-%20concerning%20their%0Acapabilities%20for%20text-to-code%20generation.%20In%20an%20empirical%20study%2C%20we%20feed%0Aprompts%20with%20textual%20descriptions%20of%20coding%20problems%20sourced%20from%20the%0Aprogramming%20website%20LeetCode%20to%20the%20models%20with%20the%20task%20of%20creating%20solutions%0Ain%20Python.%20Subsequently%2C%20the%20quality%20of%20the%20generated%20outputs%20is%20assessed%20using%0Athe%20testing%20functionalities%20of%20LeetCode.%20The%20results%20indicate%20large%20differences%0Ain%20performance%20between%20the%20investigated%20models.%20ChatGPT%20can%20handle%20these%0Atypical%20programming%20challenges%20by%20far%20the%20most%20effectively%2C%20surpassing%20even%0Acode-specialized%20models%20like%20Code%20Llama.%20To%20gain%20further%20insights%2C%20we%20measure%0Athe%20runtime%20as%20well%20as%20the%20memory%20usage%20of%20the%20generated%20outputs%20and%20compared%0Athem%20to%20the%20other%20code%20submissions%20on%20Leetcode.%20A%20detailed%20error%20analysis%2C%0Aencompassing%20a%20comparison%20of%20the%20differences%20concerning%20correct%20indentation%20and%0Aform%20of%20the%20generated%20code%20as%20well%20as%20an%20assignment%20of%20the%20incorrectly%20solved%0Atasks%20to%20certain%20error%20categories%20allows%20us%20to%20obtain%20a%20more%20nuanced%20picture%20of%0Athe%20results%20and%20potential%20for%20improvement.%20The%20results%20also%20show%20a%20clear%0Apattern%20of%20increasingly%20incorrect%20produced%20code%20when%20the%20models%20are%20facing%20a%0Alot%20of%20context%20in%20the%20form%20of%20longer%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04164v1&entry.124074799=Read"},
{"title": "Future Does Matter: Boosting 3D Object Detection with Temporal Motion\n  Estimation in Point Cloud Sequences", "author": "Rui Yu and Runkai Zhao and Cong Nie and Heng Wang and HuaiCheng Yan and Meng Wang", "abstract": "  Accurate and robust LiDAR 3D object detection is essential for comprehensive\nscene understanding in autonomous driving. Despite its importance, LiDAR\ndetection performance is limited by inherent constraints of point cloud data,\nparticularly under conditions of extended distances and occlusions. Recently,\ntemporal aggregation has been proven to significantly enhance detection\naccuracy by fusing multi-frame viewpoint information and enriching the spatial\nrepresentation of objects. In this work, we introduce a novel LiDAR 3D object\ndetection framework, namely LiSTM, to facilitate spatial-temporal feature\nlearning with cross-frame motion forecasting information. We aim to improve the\nspatial-temporal interpretation capabilities of the LiDAR detector by\nincorporating a dynamic prior, generated from a non-learnable motion estimation\nmodel. Specifically, Motion-Guided Feature Aggregation (MGFA) is proposed to\nutilize the object trajectory from previous and future motion states to model\nspatial-temporal correlations into gaussian heatmap over a driving sequence.\nThis motion-based heatmap then guides the temporal feature fusion, enriching\nthe proposed object features. Moreover, we design a Dual Correlation Weighting\nModule (DCWM) that effectively facilitates the interaction between past and\nprospective frames through scene- and channel-wise feature abstraction. In the\nend, a cascade cross-attention-based decoder is employed to refine the 3D\nprediction. We have conducted experiments on the Waymo and nuScenes datasets to\ndemonstrate that the proposed framework achieves superior 3D detection\nperformance with effective spatial-temporal feature learning.\n", "link": "http://arxiv.org/abs/2409.04390v1", "date": "2024-09-06", "relevancy": 2.4365, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6252}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6034}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Future%20Does%20Matter%3A%20Boosting%203D%20Object%20Detection%20with%20Temporal%20Motion%0A%20%20Estimation%20in%20Point%20Cloud%20Sequences&body=Title%3A%20Future%20Does%20Matter%3A%20Boosting%203D%20Object%20Detection%20with%20Temporal%20Motion%0A%20%20Estimation%20in%20Point%20Cloud%20Sequences%0AAuthor%3A%20Rui%20Yu%20and%20Runkai%20Zhao%20and%20Cong%20Nie%20and%20Heng%20Wang%20and%20HuaiCheng%20Yan%20and%20Meng%20Wang%0AAbstract%3A%20%20%20Accurate%20and%20robust%20LiDAR%203D%20object%20detection%20is%20essential%20for%20comprehensive%0Ascene%20understanding%20in%20autonomous%20driving.%20Despite%20its%20importance%2C%20LiDAR%0Adetection%20performance%20is%20limited%20by%20inherent%20constraints%20of%20point%20cloud%20data%2C%0Aparticularly%20under%20conditions%20of%20extended%20distances%20and%20occlusions.%20Recently%2C%0Atemporal%20aggregation%20has%20been%20proven%20to%20significantly%20enhance%20detection%0Aaccuracy%20by%20fusing%20multi-frame%20viewpoint%20information%20and%20enriching%20the%20spatial%0Arepresentation%20of%20objects.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20LiDAR%203D%20object%0Adetection%20framework%2C%20namely%20LiSTM%2C%20to%20facilitate%20spatial-temporal%20feature%0Alearning%20with%20cross-frame%20motion%20forecasting%20information.%20We%20aim%20to%20improve%20the%0Aspatial-temporal%20interpretation%20capabilities%20of%20the%20LiDAR%20detector%20by%0Aincorporating%20a%20dynamic%20prior%2C%20generated%20from%20a%20non-learnable%20motion%20estimation%0Amodel.%20Specifically%2C%20Motion-Guided%20Feature%20Aggregation%20%28MGFA%29%20is%20proposed%20to%0Autilize%20the%20object%20trajectory%20from%20previous%20and%20future%20motion%20states%20to%20model%0Aspatial-temporal%20correlations%20into%20gaussian%20heatmap%20over%20a%20driving%20sequence.%0AThis%20motion-based%20heatmap%20then%20guides%20the%20temporal%20feature%20fusion%2C%20enriching%0Athe%20proposed%20object%20features.%20Moreover%2C%20we%20design%20a%20Dual%20Correlation%20Weighting%0AModule%20%28DCWM%29%20that%20effectively%20facilitates%20the%20interaction%20between%20past%20and%0Aprospective%20frames%20through%20scene-%20and%20channel-wise%20feature%20abstraction.%20In%20the%0Aend%2C%20a%20cascade%20cross-attention-based%20decoder%20is%20employed%20to%20refine%20the%203D%0Aprediction.%20We%20have%20conducted%20experiments%20on%20the%20Waymo%20and%20nuScenes%20datasets%20to%0Ademonstrate%20that%20the%20proposed%20framework%20achieves%20superior%203D%20detection%0Aperformance%20with%20effective%20spatial-temporal%20feature%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuture%2520Does%2520Matter%253A%2520Boosting%25203D%2520Object%2520Detection%2520with%2520Temporal%2520Motion%250A%2520%2520Estimation%2520in%2520Point%2520Cloud%2520Sequences%26entry.906535625%3DRui%2520Yu%2520and%2520Runkai%2520Zhao%2520and%2520Cong%2520Nie%2520and%2520Heng%2520Wang%2520and%2520HuaiCheng%2520Yan%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520Accurate%2520and%2520robust%2520LiDAR%25203D%2520object%2520detection%2520is%2520essential%2520for%2520comprehensive%250Ascene%2520understanding%2520in%2520autonomous%2520driving.%2520Despite%2520its%2520importance%252C%2520LiDAR%250Adetection%2520performance%2520is%2520limited%2520by%2520inherent%2520constraints%2520of%2520point%2520cloud%2520data%252C%250Aparticularly%2520under%2520conditions%2520of%2520extended%2520distances%2520and%2520occlusions.%2520Recently%252C%250Atemporal%2520aggregation%2520has%2520been%2520proven%2520to%2520significantly%2520enhance%2520detection%250Aaccuracy%2520by%2520fusing%2520multi-frame%2520viewpoint%2520information%2520and%2520enriching%2520the%2520spatial%250Arepresentation%2520of%2520objects.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520LiDAR%25203D%2520object%250Adetection%2520framework%252C%2520namely%2520LiSTM%252C%2520to%2520facilitate%2520spatial-temporal%2520feature%250Alearning%2520with%2520cross-frame%2520motion%2520forecasting%2520information.%2520We%2520aim%2520to%2520improve%2520the%250Aspatial-temporal%2520interpretation%2520capabilities%2520of%2520the%2520LiDAR%2520detector%2520by%250Aincorporating%2520a%2520dynamic%2520prior%252C%2520generated%2520from%2520a%2520non-learnable%2520motion%2520estimation%250Amodel.%2520Specifically%252C%2520Motion-Guided%2520Feature%2520Aggregation%2520%2528MGFA%2529%2520is%2520proposed%2520to%250Autilize%2520the%2520object%2520trajectory%2520from%2520previous%2520and%2520future%2520motion%2520states%2520to%2520model%250Aspatial-temporal%2520correlations%2520into%2520gaussian%2520heatmap%2520over%2520a%2520driving%2520sequence.%250AThis%2520motion-based%2520heatmap%2520then%2520guides%2520the%2520temporal%2520feature%2520fusion%252C%2520enriching%250Athe%2520proposed%2520object%2520features.%2520Moreover%252C%2520we%2520design%2520a%2520Dual%2520Correlation%2520Weighting%250AModule%2520%2528DCWM%2529%2520that%2520effectively%2520facilitates%2520the%2520interaction%2520between%2520past%2520and%250Aprospective%2520frames%2520through%2520scene-%2520and%2520channel-wise%2520feature%2520abstraction.%2520In%2520the%250Aend%252C%2520a%2520cascade%2520cross-attention-based%2520decoder%2520is%2520employed%2520to%2520refine%2520the%25203D%250Aprediction.%2520We%2520have%2520conducted%2520experiments%2520on%2520the%2520Waymo%2520and%2520nuScenes%2520datasets%2520to%250Ademonstrate%2520that%2520the%2520proposed%2520framework%2520achieves%2520superior%25203D%2520detection%250Aperformance%2520with%2520effective%2520spatial-temporal%2520feature%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Future%20Does%20Matter%3A%20Boosting%203D%20Object%20Detection%20with%20Temporal%20Motion%0A%20%20Estimation%20in%20Point%20Cloud%20Sequences&entry.906535625=Rui%20Yu%20and%20Runkai%20Zhao%20and%20Cong%20Nie%20and%20Heng%20Wang%20and%20HuaiCheng%20Yan%20and%20Meng%20Wang&entry.1292438233=%20%20Accurate%20and%20robust%20LiDAR%203D%20object%20detection%20is%20essential%20for%20comprehensive%0Ascene%20understanding%20in%20autonomous%20driving.%20Despite%20its%20importance%2C%20LiDAR%0Adetection%20performance%20is%20limited%20by%20inherent%20constraints%20of%20point%20cloud%20data%2C%0Aparticularly%20under%20conditions%20of%20extended%20distances%20and%20occlusions.%20Recently%2C%0Atemporal%20aggregation%20has%20been%20proven%20to%20significantly%20enhance%20detection%0Aaccuracy%20by%20fusing%20multi-frame%20viewpoint%20information%20and%20enriching%20the%20spatial%0Arepresentation%20of%20objects.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20LiDAR%203D%20object%0Adetection%20framework%2C%20namely%20LiSTM%2C%20to%20facilitate%20spatial-temporal%20feature%0Alearning%20with%20cross-frame%20motion%20forecasting%20information.%20We%20aim%20to%20improve%20the%0Aspatial-temporal%20interpretation%20capabilities%20of%20the%20LiDAR%20detector%20by%0Aincorporating%20a%20dynamic%20prior%2C%20generated%20from%20a%20non-learnable%20motion%20estimation%0Amodel.%20Specifically%2C%20Motion-Guided%20Feature%20Aggregation%20%28MGFA%29%20is%20proposed%20to%0Autilize%20the%20object%20trajectory%20from%20previous%20and%20future%20motion%20states%20to%20model%0Aspatial-temporal%20correlations%20into%20gaussian%20heatmap%20over%20a%20driving%20sequence.%0AThis%20motion-based%20heatmap%20then%20guides%20the%20temporal%20feature%20fusion%2C%20enriching%0Athe%20proposed%20object%20features.%20Moreover%2C%20we%20design%20a%20Dual%20Correlation%20Weighting%0AModule%20%28DCWM%29%20that%20effectively%20facilitates%20the%20interaction%20between%20past%20and%0Aprospective%20frames%20through%20scene-%20and%20channel-wise%20feature%20abstraction.%20In%20the%0Aend%2C%20a%20cascade%20cross-attention-based%20decoder%20is%20employed%20to%20refine%20the%203D%0Aprediction.%20We%20have%20conducted%20experiments%20on%20the%20Waymo%20and%20nuScenes%20datasets%20to%0Ademonstrate%20that%20the%20proposed%20framework%20achieves%20superior%203D%20detection%0Aperformance%20with%20effective%20spatial-temporal%20feature%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04390v1&entry.124074799=Read"},
{"title": "CISCA and CytoDArk0: a Cell Instance Segmentation and Classification\n  method for histo(patho)logical image Analyses and a new, open, Nissl-stained\n  dataset for brain cytoarchitecture studies", "author": "Valentina Vadori and Jean-Marie Gra\u00efc and Antonella Peruffo and Giulia Vadori and Livio Finos and Enrico Grisan", "abstract": "  Delineating and classifying individual cells in microscopy tissue images is a\ncomplex task, yet it is a pivotal endeavor in various medical and biological\ninvestigations. We propose a new deep learning framework (CISCA) for automatic\ncell instance segmentation and classification in histological slices to support\ndetailed morphological and structural analysis or straightforward cell counting\nin digital pathology workflows and brain cytoarchitecture studies. At the core\nof CISCA lies a network architecture featuring a lightweight U-Net with three\nheads in the decoder. The first head classifies pixels into boundaries between\nneighboring cells, cell bodies, and background, while the second head regresses\nfour distance maps along four directions. The network outputs from the first\nand second heads are integrated through a tailored post-processing step, which\nultimately yields the segmentation of individual cells. A third head enables\nsimultaneous classification of cells into relevant classes, if required. We\nshowcase the effectiveness of our method using four datasets, including CoNIC,\nPanNuke, and MoNuSeg, which are publicly available H\\&E datasets. Additionally,\nwe introduce CytoDArk0, a novel dataset consisting of Nissl-stained images of\nthe cortex, cerebellum, and hippocampus from mammals belonging to the orders\nCetartiodactyla and Primates. We evaluate CISCA in comparison to other\nstate-of-the-art methods, demonstrating CISCA's robustness and accuracy in\nsegmenting and classifying cells across diverse tissue types, magnifications,\nand staining techniques.\n", "link": "http://arxiv.org/abs/2409.04175v1", "date": "2024-09-06", "relevancy": 2.4203, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4902}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4902}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CISCA%20and%20CytoDArk0%3A%20a%20Cell%20Instance%20Segmentation%20and%20Classification%0A%20%20method%20for%20histo%28patho%29logical%20image%20Analyses%20and%20a%20new%2C%20open%2C%20Nissl-stained%0A%20%20dataset%20for%20brain%20cytoarchitecture%20studies&body=Title%3A%20CISCA%20and%20CytoDArk0%3A%20a%20Cell%20Instance%20Segmentation%20and%20Classification%0A%20%20method%20for%20histo%28patho%29logical%20image%20Analyses%20and%20a%20new%2C%20open%2C%20Nissl-stained%0A%20%20dataset%20for%20brain%20cytoarchitecture%20studies%0AAuthor%3A%20Valentina%20Vadori%20and%20Jean-Marie%20Gra%C3%AFc%20and%20Antonella%20Peruffo%20and%20Giulia%20Vadori%20and%20Livio%20Finos%20and%20Enrico%20Grisan%0AAbstract%3A%20%20%20Delineating%20and%20classifying%20individual%20cells%20in%20microscopy%20tissue%20images%20is%20a%0Acomplex%20task%2C%20yet%20it%20is%20a%20pivotal%20endeavor%20in%20various%20medical%20and%20biological%0Ainvestigations.%20We%20propose%20a%20new%20deep%20learning%20framework%20%28CISCA%29%20for%20automatic%0Acell%20instance%20segmentation%20and%20classification%20in%20histological%20slices%20to%20support%0Adetailed%20morphological%20and%20structural%20analysis%20or%20straightforward%20cell%20counting%0Ain%20digital%20pathology%20workflows%20and%20brain%20cytoarchitecture%20studies.%20At%20the%20core%0Aof%20CISCA%20lies%20a%20network%20architecture%20featuring%20a%20lightweight%20U-Net%20with%20three%0Aheads%20in%20the%20decoder.%20The%20first%20head%20classifies%20pixels%20into%20boundaries%20between%0Aneighboring%20cells%2C%20cell%20bodies%2C%20and%20background%2C%20while%20the%20second%20head%20regresses%0Afour%20distance%20maps%20along%20four%20directions.%20The%20network%20outputs%20from%20the%20first%0Aand%20second%20heads%20are%20integrated%20through%20a%20tailored%20post-processing%20step%2C%20which%0Aultimately%20yields%20the%20segmentation%20of%20individual%20cells.%20A%20third%20head%20enables%0Asimultaneous%20classification%20of%20cells%20into%20relevant%20classes%2C%20if%20required.%20We%0Ashowcase%20the%20effectiveness%20of%20our%20method%20using%20four%20datasets%2C%20including%20CoNIC%2C%0APanNuke%2C%20and%20MoNuSeg%2C%20which%20are%20publicly%20available%20H%5C%26E%20datasets.%20Additionally%2C%0Awe%20introduce%20CytoDArk0%2C%20a%20novel%20dataset%20consisting%20of%20Nissl-stained%20images%20of%0Athe%20cortex%2C%20cerebellum%2C%20and%20hippocampus%20from%20mammals%20belonging%20to%20the%20orders%0ACetartiodactyla%20and%20Primates.%20We%20evaluate%20CISCA%20in%20comparison%20to%20other%0Astate-of-the-art%20methods%2C%20demonstrating%20CISCA%27s%20robustness%20and%20accuracy%20in%0Asegmenting%20and%20classifying%20cells%20across%20diverse%20tissue%20types%2C%20magnifications%2C%0Aand%20staining%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCISCA%2520and%2520CytoDArk0%253A%2520a%2520Cell%2520Instance%2520Segmentation%2520and%2520Classification%250A%2520%2520method%2520for%2520histo%2528patho%2529logical%2520image%2520Analyses%2520and%2520a%2520new%252C%2520open%252C%2520Nissl-stained%250A%2520%2520dataset%2520for%2520brain%2520cytoarchitecture%2520studies%26entry.906535625%3DValentina%2520Vadori%2520and%2520Jean-Marie%2520Gra%25C3%25AFc%2520and%2520Antonella%2520Peruffo%2520and%2520Giulia%2520Vadori%2520and%2520Livio%2520Finos%2520and%2520Enrico%2520Grisan%26entry.1292438233%3D%2520%2520Delineating%2520and%2520classifying%2520individual%2520cells%2520in%2520microscopy%2520tissue%2520images%2520is%2520a%250Acomplex%2520task%252C%2520yet%2520it%2520is%2520a%2520pivotal%2520endeavor%2520in%2520various%2520medical%2520and%2520biological%250Ainvestigations.%2520We%2520propose%2520a%2520new%2520deep%2520learning%2520framework%2520%2528CISCA%2529%2520for%2520automatic%250Acell%2520instance%2520segmentation%2520and%2520classification%2520in%2520histological%2520slices%2520to%2520support%250Adetailed%2520morphological%2520and%2520structural%2520analysis%2520or%2520straightforward%2520cell%2520counting%250Ain%2520digital%2520pathology%2520workflows%2520and%2520brain%2520cytoarchitecture%2520studies.%2520At%2520the%2520core%250Aof%2520CISCA%2520lies%2520a%2520network%2520architecture%2520featuring%2520a%2520lightweight%2520U-Net%2520with%2520three%250Aheads%2520in%2520the%2520decoder.%2520The%2520first%2520head%2520classifies%2520pixels%2520into%2520boundaries%2520between%250Aneighboring%2520cells%252C%2520cell%2520bodies%252C%2520and%2520background%252C%2520while%2520the%2520second%2520head%2520regresses%250Afour%2520distance%2520maps%2520along%2520four%2520directions.%2520The%2520network%2520outputs%2520from%2520the%2520first%250Aand%2520second%2520heads%2520are%2520integrated%2520through%2520a%2520tailored%2520post-processing%2520step%252C%2520which%250Aultimately%2520yields%2520the%2520segmentation%2520of%2520individual%2520cells.%2520A%2520third%2520head%2520enables%250Asimultaneous%2520classification%2520of%2520cells%2520into%2520relevant%2520classes%252C%2520if%2520required.%2520We%250Ashowcase%2520the%2520effectiveness%2520of%2520our%2520method%2520using%2520four%2520datasets%252C%2520including%2520CoNIC%252C%250APanNuke%252C%2520and%2520MoNuSeg%252C%2520which%2520are%2520publicly%2520available%2520H%255C%2526E%2520datasets.%2520Additionally%252C%250Awe%2520introduce%2520CytoDArk0%252C%2520a%2520novel%2520dataset%2520consisting%2520of%2520Nissl-stained%2520images%2520of%250Athe%2520cortex%252C%2520cerebellum%252C%2520and%2520hippocampus%2520from%2520mammals%2520belonging%2520to%2520the%2520orders%250ACetartiodactyla%2520and%2520Primates.%2520We%2520evaluate%2520CISCA%2520in%2520comparison%2520to%2520other%250Astate-of-the-art%2520methods%252C%2520demonstrating%2520CISCA%2527s%2520robustness%2520and%2520accuracy%2520in%250Asegmenting%2520and%2520classifying%2520cells%2520across%2520diverse%2520tissue%2520types%252C%2520magnifications%252C%250Aand%2520staining%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CISCA%20and%20CytoDArk0%3A%20a%20Cell%20Instance%20Segmentation%20and%20Classification%0A%20%20method%20for%20histo%28patho%29logical%20image%20Analyses%20and%20a%20new%2C%20open%2C%20Nissl-stained%0A%20%20dataset%20for%20brain%20cytoarchitecture%20studies&entry.906535625=Valentina%20Vadori%20and%20Jean-Marie%20Gra%C3%AFc%20and%20Antonella%20Peruffo%20and%20Giulia%20Vadori%20and%20Livio%20Finos%20and%20Enrico%20Grisan&entry.1292438233=%20%20Delineating%20and%20classifying%20individual%20cells%20in%20microscopy%20tissue%20images%20is%20a%0Acomplex%20task%2C%20yet%20it%20is%20a%20pivotal%20endeavor%20in%20various%20medical%20and%20biological%0Ainvestigations.%20We%20propose%20a%20new%20deep%20learning%20framework%20%28CISCA%29%20for%20automatic%0Acell%20instance%20segmentation%20and%20classification%20in%20histological%20slices%20to%20support%0Adetailed%20morphological%20and%20structural%20analysis%20or%20straightforward%20cell%20counting%0Ain%20digital%20pathology%20workflows%20and%20brain%20cytoarchitecture%20studies.%20At%20the%20core%0Aof%20CISCA%20lies%20a%20network%20architecture%20featuring%20a%20lightweight%20U-Net%20with%20three%0Aheads%20in%20the%20decoder.%20The%20first%20head%20classifies%20pixels%20into%20boundaries%20between%0Aneighboring%20cells%2C%20cell%20bodies%2C%20and%20background%2C%20while%20the%20second%20head%20regresses%0Afour%20distance%20maps%20along%20four%20directions.%20The%20network%20outputs%20from%20the%20first%0Aand%20second%20heads%20are%20integrated%20through%20a%20tailored%20post-processing%20step%2C%20which%0Aultimately%20yields%20the%20segmentation%20of%20individual%20cells.%20A%20third%20head%20enables%0Asimultaneous%20classification%20of%20cells%20into%20relevant%20classes%2C%20if%20required.%20We%0Ashowcase%20the%20effectiveness%20of%20our%20method%20using%20four%20datasets%2C%20including%20CoNIC%2C%0APanNuke%2C%20and%20MoNuSeg%2C%20which%20are%20publicly%20available%20H%5C%26E%20datasets.%20Additionally%2C%0Awe%20introduce%20CytoDArk0%2C%20a%20novel%20dataset%20consisting%20of%20Nissl-stained%20images%20of%0Athe%20cortex%2C%20cerebellum%2C%20and%20hippocampus%20from%20mammals%20belonging%20to%20the%20orders%0ACetartiodactyla%20and%20Primates.%20We%20evaluate%20CISCA%20in%20comparison%20to%20other%0Astate-of-the-art%20methods%2C%20demonstrating%20CISCA%27s%20robustness%20and%20accuracy%20in%0Asegmenting%20and%20classifying%20cells%20across%20diverse%20tissue%20types%2C%20magnifications%2C%0Aand%20staining%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04175v1&entry.124074799=Read"},
{"title": "Fast Forwarding Low-Rank Training", "author": "Adir Rahamim and Naomi Saphra and Sara Kangaslahti and Yonatan Belinkov", "abstract": "  Parameter efficient finetuning methods like low-rank adaptation (LoRA) aim to\nreduce the computational costs of finetuning pretrained Language Models (LMs).\nEnabled by these low-rank settings, we propose an even more efficient\noptimization strategy: Fast Forward, a simple and effective approach to\naccelerate large segments of training. In a Fast Forward stage, we repeat the\nmost recent optimizer step until the loss stops improving on a tiny validation\nset. By alternating between regular optimization steps and Fast Forward stages,\nFast Forward provides up to an 87\\% reduction in FLOPs and up to an 81\\%\nreduction in train time over standard SGD with Adam. We validate Fast Forward\nby finetuning various models on different tasks and demonstrate that it speeds\nup training without compromising model performance. Additionally, we analyze\nwhen and how to apply Fast Forward.\n", "link": "http://arxiv.org/abs/2409.04206v1", "date": "2024-09-06", "relevancy": 2.4065, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4993}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4751}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Forwarding%20Low-Rank%20Training&body=Title%3A%20Fast%20Forwarding%20Low-Rank%20Training%0AAuthor%3A%20Adir%20Rahamim%20and%20Naomi%20Saphra%20and%20Sara%20Kangaslahti%20and%20Yonatan%20Belinkov%0AAbstract%3A%20%20%20Parameter%20efficient%20finetuning%20methods%20like%20low-rank%20adaptation%20%28LoRA%29%20aim%20to%0Areduce%20the%20computational%20costs%20of%20finetuning%20pretrained%20Language%20Models%20%28LMs%29.%0AEnabled%20by%20these%20low-rank%20settings%2C%20we%20propose%20an%20even%20more%20efficient%0Aoptimization%20strategy%3A%20Fast%20Forward%2C%20a%20simple%20and%20effective%20approach%20to%0Aaccelerate%20large%20segments%20of%20training.%20In%20a%20Fast%20Forward%20stage%2C%20we%20repeat%20the%0Amost%20recent%20optimizer%20step%20until%20the%20loss%20stops%20improving%20on%20a%20tiny%20validation%0Aset.%20By%20alternating%20between%20regular%20optimization%20steps%20and%20Fast%20Forward%20stages%2C%0AFast%20Forward%20provides%20up%20to%20an%2087%5C%25%20reduction%20in%20FLOPs%20and%20up%20to%20an%2081%5C%25%0Areduction%20in%20train%20time%20over%20standard%20SGD%20with%20Adam.%20We%20validate%20Fast%20Forward%0Aby%20finetuning%20various%20models%20on%20different%20tasks%20and%20demonstrate%20that%20it%20speeds%0Aup%20training%20without%20compromising%20model%20performance.%20Additionally%2C%20we%20analyze%0Awhen%20and%20how%20to%20apply%20Fast%20Forward.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Forwarding%2520Low-Rank%2520Training%26entry.906535625%3DAdir%2520Rahamim%2520and%2520Naomi%2520Saphra%2520and%2520Sara%2520Kangaslahti%2520and%2520Yonatan%2520Belinkov%26entry.1292438233%3D%2520%2520Parameter%2520efficient%2520finetuning%2520methods%2520like%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520aim%2520to%250Areduce%2520the%2520computational%2520costs%2520of%2520finetuning%2520pretrained%2520Language%2520Models%2520%2528LMs%2529.%250AEnabled%2520by%2520these%2520low-rank%2520settings%252C%2520we%2520propose%2520an%2520even%2520more%2520efficient%250Aoptimization%2520strategy%253A%2520Fast%2520Forward%252C%2520a%2520simple%2520and%2520effective%2520approach%2520to%250Aaccelerate%2520large%2520segments%2520of%2520training.%2520In%2520a%2520Fast%2520Forward%2520stage%252C%2520we%2520repeat%2520the%250Amost%2520recent%2520optimizer%2520step%2520until%2520the%2520loss%2520stops%2520improving%2520on%2520a%2520tiny%2520validation%250Aset.%2520By%2520alternating%2520between%2520regular%2520optimization%2520steps%2520and%2520Fast%2520Forward%2520stages%252C%250AFast%2520Forward%2520provides%2520up%2520to%2520an%252087%255C%2525%2520reduction%2520in%2520FLOPs%2520and%2520up%2520to%2520an%252081%255C%2525%250Areduction%2520in%2520train%2520time%2520over%2520standard%2520SGD%2520with%2520Adam.%2520We%2520validate%2520Fast%2520Forward%250Aby%2520finetuning%2520various%2520models%2520on%2520different%2520tasks%2520and%2520demonstrate%2520that%2520it%2520speeds%250Aup%2520training%2520without%2520compromising%2520model%2520performance.%2520Additionally%252C%2520we%2520analyze%250Awhen%2520and%2520how%2520to%2520apply%2520Fast%2520Forward.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Forwarding%20Low-Rank%20Training&entry.906535625=Adir%20Rahamim%20and%20Naomi%20Saphra%20and%20Sara%20Kangaslahti%20and%20Yonatan%20Belinkov&entry.1292438233=%20%20Parameter%20efficient%20finetuning%20methods%20like%20low-rank%20adaptation%20%28LoRA%29%20aim%20to%0Areduce%20the%20computational%20costs%20of%20finetuning%20pretrained%20Language%20Models%20%28LMs%29.%0AEnabled%20by%20these%20low-rank%20settings%2C%20we%20propose%20an%20even%20more%20efficient%0Aoptimization%20strategy%3A%20Fast%20Forward%2C%20a%20simple%20and%20effective%20approach%20to%0Aaccelerate%20large%20segments%20of%20training.%20In%20a%20Fast%20Forward%20stage%2C%20we%20repeat%20the%0Amost%20recent%20optimizer%20step%20until%20the%20loss%20stops%20improving%20on%20a%20tiny%20validation%0Aset.%20By%20alternating%20between%20regular%20optimization%20steps%20and%20Fast%20Forward%20stages%2C%0AFast%20Forward%20provides%20up%20to%20an%2087%5C%25%20reduction%20in%20FLOPs%20and%20up%20to%20an%2081%5C%25%0Areduction%20in%20train%20time%20over%20standard%20SGD%20with%20Adam.%20We%20validate%20Fast%20Forward%0Aby%20finetuning%20various%20models%20on%20different%20tasks%20and%20demonstrate%20that%20it%20speeds%0Aup%20training%20without%20compromising%20model%20performance.%20Additionally%2C%20we%20analyze%0Awhen%20and%20how%20to%20apply%20Fast%20Forward.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04206v1&entry.124074799=Read"},
{"title": "Computer-Generated Sand Mixtures and Sand-based Images", "author": "Ryan A. Subong and Alma Jean D. Subong", "abstract": "  This paper aims to verify the effectiveness of the software implementation of\nthe proposed algorithm in creating computer-generated images of sand mixtures\nusing a photograph of sand as an input and its effectiveness in converting\ndigital pictures into sand-based images out of the mixtures it generated. The\nmethod of this paper is to visually compare the photographed image of the\nactual mixtures to its computer-generated counterpart to verify if the mixture\ngeneration produces results as expected and compare the computer-generated\nsand-based images with its source to verify image reproduction maintains same\nimage content. The results of the mixture comparison shows that the actual and\nthe computer-generated ones have similar overall shade and color. Still, the\ngenerated one has a rougher texture and higher contrast due to the method of\ninheriting visual features by pixel, not by individual sand particles. The\ncomparison of the sand-based image and its source has demonstrated the\nsoftware's ability to maintain the essence of its contents during conversion\nwhile replacing its texture with the visual properties of the generated sand\nmixture. The result have shown that the software implementation of the proposed\nalgorithm can effectively use the images of sand to generate images of its\nmixtures and use those mixture images to convert a digital picture into a\ncomputer-generated sand-based image.\n", "link": "http://arxiv.org/abs/2409.04345v1", "date": "2024-09-06", "relevancy": 2.405, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4824}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4824}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computer-Generated%20Sand%20Mixtures%20and%20Sand-based%20Images&body=Title%3A%20Computer-Generated%20Sand%20Mixtures%20and%20Sand-based%20Images%0AAuthor%3A%20Ryan%20A.%20Subong%20and%20Alma%20Jean%20D.%20Subong%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20verify%20the%20effectiveness%20of%20the%20software%20implementation%20of%0Athe%20proposed%20algorithm%20in%20creating%20computer-generated%20images%20of%20sand%20mixtures%0Ausing%20a%20photograph%20of%20sand%20as%20an%20input%20and%20its%20effectiveness%20in%20converting%0Adigital%20pictures%20into%20sand-based%20images%20out%20of%20the%20mixtures%20it%20generated.%20The%0Amethod%20of%20this%20paper%20is%20to%20visually%20compare%20the%20photographed%20image%20of%20the%0Aactual%20mixtures%20to%20its%20computer-generated%20counterpart%20to%20verify%20if%20the%20mixture%0Ageneration%20produces%20results%20as%20expected%20and%20compare%20the%20computer-generated%0Asand-based%20images%20with%20its%20source%20to%20verify%20image%20reproduction%20maintains%20same%0Aimage%20content.%20The%20results%20of%20the%20mixture%20comparison%20shows%20that%20the%20actual%20and%0Athe%20computer-generated%20ones%20have%20similar%20overall%20shade%20and%20color.%20Still%2C%20the%0Agenerated%20one%20has%20a%20rougher%20texture%20and%20higher%20contrast%20due%20to%20the%20method%20of%0Ainheriting%20visual%20features%20by%20pixel%2C%20not%20by%20individual%20sand%20particles.%20The%0Acomparison%20of%20the%20sand-based%20image%20and%20its%20source%20has%20demonstrated%20the%0Asoftware%27s%20ability%20to%20maintain%20the%20essence%20of%20its%20contents%20during%20conversion%0Awhile%20replacing%20its%20texture%20with%20the%20visual%20properties%20of%20the%20generated%20sand%0Amixture.%20The%20result%20have%20shown%20that%20the%20software%20implementation%20of%20the%20proposed%0Aalgorithm%20can%20effectively%20use%20the%20images%20of%20sand%20to%20generate%20images%20of%20its%0Amixtures%20and%20use%20those%20mixture%20images%20to%20convert%20a%20digital%20picture%20into%20a%0Acomputer-generated%20sand-based%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputer-Generated%2520Sand%2520Mixtures%2520and%2520Sand-based%2520Images%26entry.906535625%3DRyan%2520A.%2520Subong%2520and%2520Alma%2520Jean%2520D.%2520Subong%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520verify%2520the%2520effectiveness%2520of%2520the%2520software%2520implementation%2520of%250Athe%2520proposed%2520algorithm%2520in%2520creating%2520computer-generated%2520images%2520of%2520sand%2520mixtures%250Ausing%2520a%2520photograph%2520of%2520sand%2520as%2520an%2520input%2520and%2520its%2520effectiveness%2520in%2520converting%250Adigital%2520pictures%2520into%2520sand-based%2520images%2520out%2520of%2520the%2520mixtures%2520it%2520generated.%2520The%250Amethod%2520of%2520this%2520paper%2520is%2520to%2520visually%2520compare%2520the%2520photographed%2520image%2520of%2520the%250Aactual%2520mixtures%2520to%2520its%2520computer-generated%2520counterpart%2520to%2520verify%2520if%2520the%2520mixture%250Ageneration%2520produces%2520results%2520as%2520expected%2520and%2520compare%2520the%2520computer-generated%250Asand-based%2520images%2520with%2520its%2520source%2520to%2520verify%2520image%2520reproduction%2520maintains%2520same%250Aimage%2520content.%2520The%2520results%2520of%2520the%2520mixture%2520comparison%2520shows%2520that%2520the%2520actual%2520and%250Athe%2520computer-generated%2520ones%2520have%2520similar%2520overall%2520shade%2520and%2520color.%2520Still%252C%2520the%250Agenerated%2520one%2520has%2520a%2520rougher%2520texture%2520and%2520higher%2520contrast%2520due%2520to%2520the%2520method%2520of%250Ainheriting%2520visual%2520features%2520by%2520pixel%252C%2520not%2520by%2520individual%2520sand%2520particles.%2520The%250Acomparison%2520of%2520the%2520sand-based%2520image%2520and%2520its%2520source%2520has%2520demonstrated%2520the%250Asoftware%2527s%2520ability%2520to%2520maintain%2520the%2520essence%2520of%2520its%2520contents%2520during%2520conversion%250Awhile%2520replacing%2520its%2520texture%2520with%2520the%2520visual%2520properties%2520of%2520the%2520generated%2520sand%250Amixture.%2520The%2520result%2520have%2520shown%2520that%2520the%2520software%2520implementation%2520of%2520the%2520proposed%250Aalgorithm%2520can%2520effectively%2520use%2520the%2520images%2520of%2520sand%2520to%2520generate%2520images%2520of%2520its%250Amixtures%2520and%2520use%2520those%2520mixture%2520images%2520to%2520convert%2520a%2520digital%2520picture%2520into%2520a%250Acomputer-generated%2520sand-based%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computer-Generated%20Sand%20Mixtures%20and%20Sand-based%20Images&entry.906535625=Ryan%20A.%20Subong%20and%20Alma%20Jean%20D.%20Subong&entry.1292438233=%20%20This%20paper%20aims%20to%20verify%20the%20effectiveness%20of%20the%20software%20implementation%20of%0Athe%20proposed%20algorithm%20in%20creating%20computer-generated%20images%20of%20sand%20mixtures%0Ausing%20a%20photograph%20of%20sand%20as%20an%20input%20and%20its%20effectiveness%20in%20converting%0Adigital%20pictures%20into%20sand-based%20images%20out%20of%20the%20mixtures%20it%20generated.%20The%0Amethod%20of%20this%20paper%20is%20to%20visually%20compare%20the%20photographed%20image%20of%20the%0Aactual%20mixtures%20to%20its%20computer-generated%20counterpart%20to%20verify%20if%20the%20mixture%0Ageneration%20produces%20results%20as%20expected%20and%20compare%20the%20computer-generated%0Asand-based%20images%20with%20its%20source%20to%20verify%20image%20reproduction%20maintains%20same%0Aimage%20content.%20The%20results%20of%20the%20mixture%20comparison%20shows%20that%20the%20actual%20and%0Athe%20computer-generated%20ones%20have%20similar%20overall%20shade%20and%20color.%20Still%2C%20the%0Agenerated%20one%20has%20a%20rougher%20texture%20and%20higher%20contrast%20due%20to%20the%20method%20of%0Ainheriting%20visual%20features%20by%20pixel%2C%20not%20by%20individual%20sand%20particles.%20The%0Acomparison%20of%20the%20sand-based%20image%20and%20its%20source%20has%20demonstrated%20the%0Asoftware%27s%20ability%20to%20maintain%20the%20essence%20of%20its%20contents%20during%20conversion%0Awhile%20replacing%20its%20texture%20with%20the%20visual%20properties%20of%20the%20generated%20sand%0Amixture.%20The%20result%20have%20shown%20that%20the%20software%20implementation%20of%20the%20proposed%0Aalgorithm%20can%20effectively%20use%20the%20images%20of%20sand%20to%20generate%20images%20of%20its%0Amixtures%20and%20use%20those%20mixture%20images%20to%20convert%20a%20digital%20picture%20into%20a%0Acomputer-generated%20sand-based%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04345v1&entry.124074799=Read"},
{"title": "UniPortrait: A Unified Framework for Identity-Preserving Single- and\n  Multi-Human Image Personalization", "author": "Junjie He and Yifeng Geng and Liefeng Bo", "abstract": "  This paper presents UniPortrait, an innovative human image personalization\nframework that unifies single- and multi-ID customization with high face\nfidelity, extensive facial editability, free-form input description, and\ndiverse layout generation. UniPortrait consists of only two plug-and-play\nmodules: an ID embedding module and an ID routing module. The ID embedding\nmodule extracts versatile editable facial features with a decoupling strategy\nfor each ID and embeds them into the context space of diffusion models. The ID\nrouting module then combines and distributes these embeddings adaptively to\ntheir respective regions within the synthesized image, achieving the\ncustomization of single and multiple IDs. With a carefully designed two-stage\ntraining scheme, UniPortrait achieves superior performance in both single- and\nmulti-ID customization. Quantitative and qualitative experiments demonstrate\nthe advantages of our method over existing approaches as well as its good\nscalability, e.g., the universal compatibility with existing generative control\ntools. The project page is at\nhttps://aigcdesigngroup.github.io/UniPortrait-Page/ .\n", "link": "http://arxiv.org/abs/2408.05939v2", "date": "2024-09-06", "relevancy": 2.3928, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6093}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5981}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniPortrait%3A%20A%20Unified%20Framework%20for%20Identity-Preserving%20Single-%20and%0A%20%20Multi-Human%20Image%20Personalization&body=Title%3A%20UniPortrait%3A%20A%20Unified%20Framework%20for%20Identity-Preserving%20Single-%20and%0A%20%20Multi-Human%20Image%20Personalization%0AAuthor%3A%20Junjie%20He%20and%20Yifeng%20Geng%20and%20Liefeng%20Bo%0AAbstract%3A%20%20%20This%20paper%20presents%20UniPortrait%2C%20an%20innovative%20human%20image%20personalization%0Aframework%20that%20unifies%20single-%20and%20multi-ID%20customization%20with%20high%20face%0Afidelity%2C%20extensive%20facial%20editability%2C%20free-form%20input%20description%2C%20and%0Adiverse%20layout%20generation.%20UniPortrait%20consists%20of%20only%20two%20plug-and-play%0Amodules%3A%20an%20ID%20embedding%20module%20and%20an%20ID%20routing%20module.%20The%20ID%20embedding%0Amodule%20extracts%20versatile%20editable%20facial%20features%20with%20a%20decoupling%20strategy%0Afor%20each%20ID%20and%20embeds%20them%20into%20the%20context%20space%20of%20diffusion%20models.%20The%20ID%0Arouting%20module%20then%20combines%20and%20distributes%20these%20embeddings%20adaptively%20to%0Atheir%20respective%20regions%20within%20the%20synthesized%20image%2C%20achieving%20the%0Acustomization%20of%20single%20and%20multiple%20IDs.%20With%20a%20carefully%20designed%20two-stage%0Atraining%20scheme%2C%20UniPortrait%20achieves%20superior%20performance%20in%20both%20single-%20and%0Amulti-ID%20customization.%20Quantitative%20and%20qualitative%20experiments%20demonstrate%0Athe%20advantages%20of%20our%20method%20over%20existing%20approaches%20as%20well%20as%20its%20good%0Ascalability%2C%20e.g.%2C%20the%20universal%20compatibility%20with%20existing%20generative%20control%0Atools.%20The%20project%20page%20is%20at%0Ahttps%3A//aigcdesigngroup.github.io/UniPortrait-Page/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05939v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniPortrait%253A%2520A%2520Unified%2520Framework%2520for%2520Identity-Preserving%2520Single-%2520and%250A%2520%2520Multi-Human%2520Image%2520Personalization%26entry.906535625%3DJunjie%2520He%2520and%2520Yifeng%2520Geng%2520and%2520Liefeng%2520Bo%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520UniPortrait%252C%2520an%2520innovative%2520human%2520image%2520personalization%250Aframework%2520that%2520unifies%2520single-%2520and%2520multi-ID%2520customization%2520with%2520high%2520face%250Afidelity%252C%2520extensive%2520facial%2520editability%252C%2520free-form%2520input%2520description%252C%2520and%250Adiverse%2520layout%2520generation.%2520UniPortrait%2520consists%2520of%2520only%2520two%2520plug-and-play%250Amodules%253A%2520an%2520ID%2520embedding%2520module%2520and%2520an%2520ID%2520routing%2520module.%2520The%2520ID%2520embedding%250Amodule%2520extracts%2520versatile%2520editable%2520facial%2520features%2520with%2520a%2520decoupling%2520strategy%250Afor%2520each%2520ID%2520and%2520embeds%2520them%2520into%2520the%2520context%2520space%2520of%2520diffusion%2520models.%2520The%2520ID%250Arouting%2520module%2520then%2520combines%2520and%2520distributes%2520these%2520embeddings%2520adaptively%2520to%250Atheir%2520respective%2520regions%2520within%2520the%2520synthesized%2520image%252C%2520achieving%2520the%250Acustomization%2520of%2520single%2520and%2520multiple%2520IDs.%2520With%2520a%2520carefully%2520designed%2520two-stage%250Atraining%2520scheme%252C%2520UniPortrait%2520achieves%2520superior%2520performance%2520in%2520both%2520single-%2520and%250Amulti-ID%2520customization.%2520Quantitative%2520and%2520qualitative%2520experiments%2520demonstrate%250Athe%2520advantages%2520of%2520our%2520method%2520over%2520existing%2520approaches%2520as%2520well%2520as%2520its%2520good%250Ascalability%252C%2520e.g.%252C%2520the%2520universal%2520compatibility%2520with%2520existing%2520generative%2520control%250Atools.%2520The%2520project%2520page%2520is%2520at%250Ahttps%253A//aigcdesigngroup.github.io/UniPortrait-Page/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05939v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniPortrait%3A%20A%20Unified%20Framework%20for%20Identity-Preserving%20Single-%20and%0A%20%20Multi-Human%20Image%20Personalization&entry.906535625=Junjie%20He%20and%20Yifeng%20Geng%20and%20Liefeng%20Bo&entry.1292438233=%20%20This%20paper%20presents%20UniPortrait%2C%20an%20innovative%20human%20image%20personalization%0Aframework%20that%20unifies%20single-%20and%20multi-ID%20customization%20with%20high%20face%0Afidelity%2C%20extensive%20facial%20editability%2C%20free-form%20input%20description%2C%20and%0Adiverse%20layout%20generation.%20UniPortrait%20consists%20of%20only%20two%20plug-and-play%0Amodules%3A%20an%20ID%20embedding%20module%20and%20an%20ID%20routing%20module.%20The%20ID%20embedding%0Amodule%20extracts%20versatile%20editable%20facial%20features%20with%20a%20decoupling%20strategy%0Afor%20each%20ID%20and%20embeds%20them%20into%20the%20context%20space%20of%20diffusion%20models.%20The%20ID%0Arouting%20module%20then%20combines%20and%20distributes%20these%20embeddings%20adaptively%20to%0Atheir%20respective%20regions%20within%20the%20synthesized%20image%2C%20achieving%20the%0Acustomization%20of%20single%20and%20multiple%20IDs.%20With%20a%20carefully%20designed%20two-stage%0Atraining%20scheme%2C%20UniPortrait%20achieves%20superior%20performance%20in%20both%20single-%20and%0Amulti-ID%20customization.%20Quantitative%20and%20qualitative%20experiments%20demonstrate%0Athe%20advantages%20of%20our%20method%20over%20existing%20approaches%20as%20well%20as%20its%20good%0Ascalability%2C%20e.g.%2C%20the%20universal%20compatibility%20with%20existing%20generative%20control%0Atools.%20The%20project%20page%20is%20at%0Ahttps%3A//aigcdesigngroup.github.io/UniPortrait-Page/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05939v2&entry.124074799=Read"},
{"title": "Training-Free Condition Video Diffusion Models for single frame\n  Spatial-Semantic Echocardiogram Synthesis", "author": "Van Phi Nguyen and Tri Nhan Luong Ha and Huy Hieu Pham and Quoc Long Tran", "abstract": "  Conditional video diffusion models (CDM) have shown promising results for\nvideo synthesis, potentially enabling the generation of realistic\nechocardiograms to address the problem of data scarcity. However, current CDMs\nrequire a paired segmentation map and echocardiogram dataset. We present a new\nmethod called Free-Echo for generating realistic echocardiograms from a single\nend-diastolic segmentation map without additional training data. Our method is\nbased on the 3D-Unet with Temporal Attention Layers model and is conditioned on\nthe segmentation map using a training-free conditioning method based on SDEdit.\nWe evaluate our model on two public echocardiogram datasets, CAMUS and\nEchoNet-Dynamic. We show that our model can generate plausible echocardiograms\nthat are spatially aligned with the input segmentation map, achieving\nperformance comparable to training-based CDMs. Our work opens up new\npossibilities for generating echocardiograms from a single segmentation map,\nwhich can be used for data augmentation, domain adaptation, and other\napplications in medical imaging. Our code is available at\n\\url{https://github.com/gungui98/echo-free}\n", "link": "http://arxiv.org/abs/2408.03035v2", "date": "2024-09-06", "relevancy": 2.3885, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6355}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5908}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Condition%20Video%20Diffusion%20Models%20for%20single%20frame%0A%20%20Spatial-Semantic%20Echocardiogram%20Synthesis&body=Title%3A%20Training-Free%20Condition%20Video%20Diffusion%20Models%20for%20single%20frame%0A%20%20Spatial-Semantic%20Echocardiogram%20Synthesis%0AAuthor%3A%20Van%20Phi%20Nguyen%20and%20Tri%20Nhan%20Luong%20Ha%20and%20Huy%20Hieu%20Pham%20and%20Quoc%20Long%20Tran%0AAbstract%3A%20%20%20Conditional%20video%20diffusion%20models%20%28CDM%29%20have%20shown%20promising%20results%20for%0Avideo%20synthesis%2C%20potentially%20enabling%20the%20generation%20of%20realistic%0Aechocardiograms%20to%20address%20the%20problem%20of%20data%20scarcity.%20However%2C%20current%20CDMs%0Arequire%20a%20paired%20segmentation%20map%20and%20echocardiogram%20dataset.%20We%20present%20a%20new%0Amethod%20called%20Free-Echo%20for%20generating%20realistic%20echocardiograms%20from%20a%20single%0Aend-diastolic%20segmentation%20map%20without%20additional%20training%20data.%20Our%20method%20is%0Abased%20on%20the%203D-Unet%20with%20Temporal%20Attention%20Layers%20model%20and%20is%20conditioned%20on%0Athe%20segmentation%20map%20using%20a%20training-free%20conditioning%20method%20based%20on%20SDEdit.%0AWe%20evaluate%20our%20model%20on%20two%20public%20echocardiogram%20datasets%2C%20CAMUS%20and%0AEchoNet-Dynamic.%20We%20show%20that%20our%20model%20can%20generate%20plausible%20echocardiograms%0Athat%20are%20spatially%20aligned%20with%20the%20input%20segmentation%20map%2C%20achieving%0Aperformance%20comparable%20to%20training-based%20CDMs.%20Our%20work%20opens%20up%20new%0Apossibilities%20for%20generating%20echocardiograms%20from%20a%20single%20segmentation%20map%2C%0Awhich%20can%20be%20used%20for%20data%20augmentation%2C%20domain%20adaptation%2C%20and%20other%0Aapplications%20in%20medical%20imaging.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/gungui98/echo-free%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03035v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Condition%2520Video%2520Diffusion%2520Models%2520for%2520single%2520frame%250A%2520%2520Spatial-Semantic%2520Echocardiogram%2520Synthesis%26entry.906535625%3DVan%2520Phi%2520Nguyen%2520and%2520Tri%2520Nhan%2520Luong%2520Ha%2520and%2520Huy%2520Hieu%2520Pham%2520and%2520Quoc%2520Long%2520Tran%26entry.1292438233%3D%2520%2520Conditional%2520video%2520diffusion%2520models%2520%2528CDM%2529%2520have%2520shown%2520promising%2520results%2520for%250Avideo%2520synthesis%252C%2520potentially%2520enabling%2520the%2520generation%2520of%2520realistic%250Aechocardiograms%2520to%2520address%2520the%2520problem%2520of%2520data%2520scarcity.%2520However%252C%2520current%2520CDMs%250Arequire%2520a%2520paired%2520segmentation%2520map%2520and%2520echocardiogram%2520dataset.%2520We%2520present%2520a%2520new%250Amethod%2520called%2520Free-Echo%2520for%2520generating%2520realistic%2520echocardiograms%2520from%2520a%2520single%250Aend-diastolic%2520segmentation%2520map%2520without%2520additional%2520training%2520data.%2520Our%2520method%2520is%250Abased%2520on%2520the%25203D-Unet%2520with%2520Temporal%2520Attention%2520Layers%2520model%2520and%2520is%2520conditioned%2520on%250Athe%2520segmentation%2520map%2520using%2520a%2520training-free%2520conditioning%2520method%2520based%2520on%2520SDEdit.%250AWe%2520evaluate%2520our%2520model%2520on%2520two%2520public%2520echocardiogram%2520datasets%252C%2520CAMUS%2520and%250AEchoNet-Dynamic.%2520We%2520show%2520that%2520our%2520model%2520can%2520generate%2520plausible%2520echocardiograms%250Athat%2520are%2520spatially%2520aligned%2520with%2520the%2520input%2520segmentation%2520map%252C%2520achieving%250Aperformance%2520comparable%2520to%2520training-based%2520CDMs.%2520Our%2520work%2520opens%2520up%2520new%250Apossibilities%2520for%2520generating%2520echocardiograms%2520from%2520a%2520single%2520segmentation%2520map%252C%250Awhich%2520can%2520be%2520used%2520for%2520data%2520augmentation%252C%2520domain%2520adaptation%252C%2520and%2520other%250Aapplications%2520in%2520medical%2520imaging.%2520Our%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/gungui98/echo-free%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03035v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Condition%20Video%20Diffusion%20Models%20for%20single%20frame%0A%20%20Spatial-Semantic%20Echocardiogram%20Synthesis&entry.906535625=Van%20Phi%20Nguyen%20and%20Tri%20Nhan%20Luong%20Ha%20and%20Huy%20Hieu%20Pham%20and%20Quoc%20Long%20Tran&entry.1292438233=%20%20Conditional%20video%20diffusion%20models%20%28CDM%29%20have%20shown%20promising%20results%20for%0Avideo%20synthesis%2C%20potentially%20enabling%20the%20generation%20of%20realistic%0Aechocardiograms%20to%20address%20the%20problem%20of%20data%20scarcity.%20However%2C%20current%20CDMs%0Arequire%20a%20paired%20segmentation%20map%20and%20echocardiogram%20dataset.%20We%20present%20a%20new%0Amethod%20called%20Free-Echo%20for%20generating%20realistic%20echocardiograms%20from%20a%20single%0Aend-diastolic%20segmentation%20map%20without%20additional%20training%20data.%20Our%20method%20is%0Abased%20on%20the%203D-Unet%20with%20Temporal%20Attention%20Layers%20model%20and%20is%20conditioned%20on%0Athe%20segmentation%20map%20using%20a%20training-free%20conditioning%20method%20based%20on%20SDEdit.%0AWe%20evaluate%20our%20model%20on%20two%20public%20echocardiogram%20datasets%2C%20CAMUS%20and%0AEchoNet-Dynamic.%20We%20show%20that%20our%20model%20can%20generate%20plausible%20echocardiograms%0Athat%20are%20spatially%20aligned%20with%20the%20input%20segmentation%20map%2C%20achieving%0Aperformance%20comparable%20to%20training-based%20CDMs.%20Our%20work%20opens%20up%20new%0Apossibilities%20for%20generating%20echocardiograms%20from%20a%20single%20segmentation%20map%2C%0Awhich%20can%20be%20used%20for%20data%20augmentation%2C%20domain%20adaptation%2C%20and%20other%0Aapplications%20in%20medical%20imaging.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/gungui98/echo-free%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03035v2&entry.124074799=Read"},
{"title": "Open-MAGVIT2: An Open-Source Project Toward Democratizing\n  Auto-regressive Visual Generation", "author": "Zhuoyan Luo and Fengyuan Shi and Yixiao Ge and Yujiu Yang and Limin Wang and Ying Shan", "abstract": "  We present Open-MAGVIT2, a family of auto-regressive image generation models\nranging from 300M to 1.5B. The Open-MAGVIT2 project produces an open-source\nreplication of Google's MAGVIT-v2 tokenizer, a tokenizer with a super-large\ncodebook (i.e., $2^{18}$ codes), and achieves the state-of-the-art\nreconstruction performance (1.17 rFID) on ImageNet $256 \\times 256$.\nFurthermore, we explore its application in plain auto-regressive models and\nvalidate scalability properties. To assist auto-regressive models in predicting\nwith a super-large vocabulary, we factorize it into two sub-vocabulary of\ndifferent sizes by asymmetric token factorization, and further introduce \"next\nsub-token prediction\" to enhance sub-token interaction for better generation\nquality. We release all models and codes to foster innovation and creativity in\nthe field of auto-regressive visual generation.\n", "link": "http://arxiv.org/abs/2409.04410v1", "date": "2024-09-06", "relevancy": 2.3775, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6126}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5821}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-MAGVIT2%3A%20An%20Open-Source%20Project%20Toward%20Democratizing%0A%20%20Auto-regressive%20Visual%20Generation&body=Title%3A%20Open-MAGVIT2%3A%20An%20Open-Source%20Project%20Toward%20Democratizing%0A%20%20Auto-regressive%20Visual%20Generation%0AAuthor%3A%20Zhuoyan%20Luo%20and%20Fengyuan%20Shi%20and%20Yixiao%20Ge%20and%20Yujiu%20Yang%20and%20Limin%20Wang%20and%20Ying%20Shan%0AAbstract%3A%20%20%20We%20present%20Open-MAGVIT2%2C%20a%20family%20of%20auto-regressive%20image%20generation%20models%0Aranging%20from%20300M%20to%201.5B.%20The%20Open-MAGVIT2%20project%20produces%20an%20open-source%0Areplication%20of%20Google%27s%20MAGVIT-v2%20tokenizer%2C%20a%20tokenizer%20with%20a%20super-large%0Acodebook%20%28i.e.%2C%20%242%5E%7B18%7D%24%20codes%29%2C%20and%20achieves%20the%20state-of-the-art%0Areconstruction%20performance%20%281.17%20rFID%29%20on%20ImageNet%20%24256%20%5Ctimes%20256%24.%0AFurthermore%2C%20we%20explore%20its%20application%20in%20plain%20auto-regressive%20models%20and%0Avalidate%20scalability%20properties.%20To%20assist%20auto-regressive%20models%20in%20predicting%0Awith%20a%20super-large%20vocabulary%2C%20we%20factorize%20it%20into%20two%20sub-vocabulary%20of%0Adifferent%20sizes%20by%20asymmetric%20token%20factorization%2C%20and%20further%20introduce%20%22next%0Asub-token%20prediction%22%20to%20enhance%20sub-token%20interaction%20for%20better%20generation%0Aquality.%20We%20release%20all%20models%20and%20codes%20to%20foster%20innovation%20and%20creativity%20in%0Athe%20field%20of%20auto-regressive%20visual%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-MAGVIT2%253A%2520An%2520Open-Source%2520Project%2520Toward%2520Democratizing%250A%2520%2520Auto-regressive%2520Visual%2520Generation%26entry.906535625%3DZhuoyan%2520Luo%2520and%2520Fengyuan%2520Shi%2520and%2520Yixiao%2520Ge%2520and%2520Yujiu%2520Yang%2520and%2520Limin%2520Wang%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520We%2520present%2520Open-MAGVIT2%252C%2520a%2520family%2520of%2520auto-regressive%2520image%2520generation%2520models%250Aranging%2520from%2520300M%2520to%25201.5B.%2520The%2520Open-MAGVIT2%2520project%2520produces%2520an%2520open-source%250Areplication%2520of%2520Google%2527s%2520MAGVIT-v2%2520tokenizer%252C%2520a%2520tokenizer%2520with%2520a%2520super-large%250Acodebook%2520%2528i.e.%252C%2520%25242%255E%257B18%257D%2524%2520codes%2529%252C%2520and%2520achieves%2520the%2520state-of-the-art%250Areconstruction%2520performance%2520%25281.17%2520rFID%2529%2520on%2520ImageNet%2520%2524256%2520%255Ctimes%2520256%2524.%250AFurthermore%252C%2520we%2520explore%2520its%2520application%2520in%2520plain%2520auto-regressive%2520models%2520and%250Avalidate%2520scalability%2520properties.%2520To%2520assist%2520auto-regressive%2520models%2520in%2520predicting%250Awith%2520a%2520super-large%2520vocabulary%252C%2520we%2520factorize%2520it%2520into%2520two%2520sub-vocabulary%2520of%250Adifferent%2520sizes%2520by%2520asymmetric%2520token%2520factorization%252C%2520and%2520further%2520introduce%2520%2522next%250Asub-token%2520prediction%2522%2520to%2520enhance%2520sub-token%2520interaction%2520for%2520better%2520generation%250Aquality.%2520We%2520release%2520all%2520models%2520and%2520codes%2520to%2520foster%2520innovation%2520and%2520creativity%2520in%250Athe%2520field%2520of%2520auto-regressive%2520visual%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-MAGVIT2%3A%20An%20Open-Source%20Project%20Toward%20Democratizing%0A%20%20Auto-regressive%20Visual%20Generation&entry.906535625=Zhuoyan%20Luo%20and%20Fengyuan%20Shi%20and%20Yixiao%20Ge%20and%20Yujiu%20Yang%20and%20Limin%20Wang%20and%20Ying%20Shan&entry.1292438233=%20%20We%20present%20Open-MAGVIT2%2C%20a%20family%20of%20auto-regressive%20image%20generation%20models%0Aranging%20from%20300M%20to%201.5B.%20The%20Open-MAGVIT2%20project%20produces%20an%20open-source%0Areplication%20of%20Google%27s%20MAGVIT-v2%20tokenizer%2C%20a%20tokenizer%20with%20a%20super-large%0Acodebook%20%28i.e.%2C%20%242%5E%7B18%7D%24%20codes%29%2C%20and%20achieves%20the%20state-of-the-art%0Areconstruction%20performance%20%281.17%20rFID%29%20on%20ImageNet%20%24256%20%5Ctimes%20256%24.%0AFurthermore%2C%20we%20explore%20its%20application%20in%20plain%20auto-regressive%20models%20and%0Avalidate%20scalability%20properties.%20To%20assist%20auto-regressive%20models%20in%20predicting%0Awith%20a%20super-large%20vocabulary%2C%20we%20factorize%20it%20into%20two%20sub-vocabulary%20of%0Adifferent%20sizes%20by%20asymmetric%20token%20factorization%2C%20and%20further%20introduce%20%22next%0Asub-token%20prediction%22%20to%20enhance%20sub-token%20interaction%20for%20better%20generation%0Aquality.%20We%20release%20all%20models%20and%20codes%20to%20foster%20innovation%20and%20creativity%20in%0Athe%20field%20of%20auto-regressive%20visual%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04410v1&entry.124074799=Read"},
{"title": "UniDet3D: Multi-dataset Indoor 3D Object Detection", "author": "Maksim Kolodiazhnyi and Anna Vorontsova and Matvey Skripkin and Danila Rukhovich and Anton Konushin", "abstract": "  Growing customer demand for smart solutions in robotics and augmented reality\nhas attracted considerable attention to 3D object detection from point clouds.\nYet, existing indoor datasets taken individually are too small and\ninsufficiently diverse to train a powerful and general 3D object detection\nmodel. In the meantime, more general approaches utilizing foundation models are\nstill inferior in quality to those based on supervised training for a specific\ntask. In this work, we propose \\ours{}, a simple yet effective 3D object\ndetection model, which is trained on a mixture of indoor datasets and is\ncapable of working in various indoor environments. By unifying different label\nspaces, \\ours{} enables learning a strong representation across multiple\ndatasets through a supervised joint training scheme. The proposed network\narchitecture is built upon a vanilla transformer encoder, making it easy to\nrun, customize and extend the prediction pipeline for practical use. Extensive\nexperiments demonstrate that \\ours{} obtains significant gains over existing 3D\nobject detection methods in 6 indoor benchmarks: ScanNet (+1.1 mAP50),\nARKitScenes (+19.4 mAP25), S3DIS (+9.1 mAP50), MultiScan (+9.3 mAP50), 3RScan\n(+3.2 mAP50), and ScanNet++ (+2.7 mAP50). Code is available at\nhttps://github.com/filapro/unidet3d .\n", "link": "http://arxiv.org/abs/2409.04234v1", "date": "2024-09-06", "relevancy": 2.3762, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6206}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniDet3D%3A%20Multi-dataset%20Indoor%203D%20Object%20Detection&body=Title%3A%20UniDet3D%3A%20Multi-dataset%20Indoor%203D%20Object%20Detection%0AAuthor%3A%20Maksim%20Kolodiazhnyi%20and%20Anna%20Vorontsova%20and%20Matvey%20Skripkin%20and%20Danila%20Rukhovich%20and%20Anton%20Konushin%0AAbstract%3A%20%20%20Growing%20customer%20demand%20for%20smart%20solutions%20in%20robotics%20and%20augmented%20reality%0Ahas%20attracted%20considerable%20attention%20to%203D%20object%20detection%20from%20point%20clouds.%0AYet%2C%20existing%20indoor%20datasets%20taken%20individually%20are%20too%20small%20and%0Ainsufficiently%20diverse%20to%20train%20a%20powerful%20and%20general%203D%20object%20detection%0Amodel.%20In%20the%20meantime%2C%20more%20general%20approaches%20utilizing%20foundation%20models%20are%0Astill%20inferior%20in%20quality%20to%20those%20based%20on%20supervised%20training%20for%20a%20specific%0Atask.%20In%20this%20work%2C%20we%20propose%20%5Cours%7B%7D%2C%20a%20simple%20yet%20effective%203D%20object%0Adetection%20model%2C%20which%20is%20trained%20on%20a%20mixture%20of%20indoor%20datasets%20and%20is%0Acapable%20of%20working%20in%20various%20indoor%20environments.%20By%20unifying%20different%20label%0Aspaces%2C%20%5Cours%7B%7D%20enables%20learning%20a%20strong%20representation%20across%20multiple%0Adatasets%20through%20a%20supervised%20joint%20training%20scheme.%20The%20proposed%20network%0Aarchitecture%20is%20built%20upon%20a%20vanilla%20transformer%20encoder%2C%20making%20it%20easy%20to%0Arun%2C%20customize%20and%20extend%20the%20prediction%20pipeline%20for%20practical%20use.%20Extensive%0Aexperiments%20demonstrate%20that%20%5Cours%7B%7D%20obtains%20significant%20gains%20over%20existing%203D%0Aobject%20detection%20methods%20in%206%20indoor%20benchmarks%3A%20ScanNet%20%28%2B1.1%20mAP50%29%2C%0AARKitScenes%20%28%2B19.4%20mAP25%29%2C%20S3DIS%20%28%2B9.1%20mAP50%29%2C%20MultiScan%20%28%2B9.3%20mAP50%29%2C%203RScan%0A%28%2B3.2%20mAP50%29%2C%20and%20ScanNet%2B%2B%20%28%2B2.7%20mAP50%29.%20Code%20is%20available%20at%0Ahttps%3A//github.com/filapro/unidet3d%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniDet3D%253A%2520Multi-dataset%2520Indoor%25203D%2520Object%2520Detection%26entry.906535625%3DMaksim%2520Kolodiazhnyi%2520and%2520Anna%2520Vorontsova%2520and%2520Matvey%2520Skripkin%2520and%2520Danila%2520Rukhovich%2520and%2520Anton%2520Konushin%26entry.1292438233%3D%2520%2520Growing%2520customer%2520demand%2520for%2520smart%2520solutions%2520in%2520robotics%2520and%2520augmented%2520reality%250Ahas%2520attracted%2520considerable%2520attention%2520to%25203D%2520object%2520detection%2520from%2520point%2520clouds.%250AYet%252C%2520existing%2520indoor%2520datasets%2520taken%2520individually%2520are%2520too%2520small%2520and%250Ainsufficiently%2520diverse%2520to%2520train%2520a%2520powerful%2520and%2520general%25203D%2520object%2520detection%250Amodel.%2520In%2520the%2520meantime%252C%2520more%2520general%2520approaches%2520utilizing%2520foundation%2520models%2520are%250Astill%2520inferior%2520in%2520quality%2520to%2520those%2520based%2520on%2520supervised%2520training%2520for%2520a%2520specific%250Atask.%2520In%2520this%2520work%252C%2520we%2520propose%2520%255Cours%257B%257D%252C%2520a%2520simple%2520yet%2520effective%25203D%2520object%250Adetection%2520model%252C%2520which%2520is%2520trained%2520on%2520a%2520mixture%2520of%2520indoor%2520datasets%2520and%2520is%250Acapable%2520of%2520working%2520in%2520various%2520indoor%2520environments.%2520By%2520unifying%2520different%2520label%250Aspaces%252C%2520%255Cours%257B%257D%2520enables%2520learning%2520a%2520strong%2520representation%2520across%2520multiple%250Adatasets%2520through%2520a%2520supervised%2520joint%2520training%2520scheme.%2520The%2520proposed%2520network%250Aarchitecture%2520is%2520built%2520upon%2520a%2520vanilla%2520transformer%2520encoder%252C%2520making%2520it%2520easy%2520to%250Arun%252C%2520customize%2520and%2520extend%2520the%2520prediction%2520pipeline%2520for%2520practical%2520use.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520%255Cours%257B%257D%2520obtains%2520significant%2520gains%2520over%2520existing%25203D%250Aobject%2520detection%2520methods%2520in%25206%2520indoor%2520benchmarks%253A%2520ScanNet%2520%2528%252B1.1%2520mAP50%2529%252C%250AARKitScenes%2520%2528%252B19.4%2520mAP25%2529%252C%2520S3DIS%2520%2528%252B9.1%2520mAP50%2529%252C%2520MultiScan%2520%2528%252B9.3%2520mAP50%2529%252C%25203RScan%250A%2528%252B3.2%2520mAP50%2529%252C%2520and%2520ScanNet%252B%252B%2520%2528%252B2.7%2520mAP50%2529.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/filapro/unidet3d%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniDet3D%3A%20Multi-dataset%20Indoor%203D%20Object%20Detection&entry.906535625=Maksim%20Kolodiazhnyi%20and%20Anna%20Vorontsova%20and%20Matvey%20Skripkin%20and%20Danila%20Rukhovich%20and%20Anton%20Konushin&entry.1292438233=%20%20Growing%20customer%20demand%20for%20smart%20solutions%20in%20robotics%20and%20augmented%20reality%0Ahas%20attracted%20considerable%20attention%20to%203D%20object%20detection%20from%20point%20clouds.%0AYet%2C%20existing%20indoor%20datasets%20taken%20individually%20are%20too%20small%20and%0Ainsufficiently%20diverse%20to%20train%20a%20powerful%20and%20general%203D%20object%20detection%0Amodel.%20In%20the%20meantime%2C%20more%20general%20approaches%20utilizing%20foundation%20models%20are%0Astill%20inferior%20in%20quality%20to%20those%20based%20on%20supervised%20training%20for%20a%20specific%0Atask.%20In%20this%20work%2C%20we%20propose%20%5Cours%7B%7D%2C%20a%20simple%20yet%20effective%203D%20object%0Adetection%20model%2C%20which%20is%20trained%20on%20a%20mixture%20of%20indoor%20datasets%20and%20is%0Acapable%20of%20working%20in%20various%20indoor%20environments.%20By%20unifying%20different%20label%0Aspaces%2C%20%5Cours%7B%7D%20enables%20learning%20a%20strong%20representation%20across%20multiple%0Adatasets%20through%20a%20supervised%20joint%20training%20scheme.%20The%20proposed%20network%0Aarchitecture%20is%20built%20upon%20a%20vanilla%20transformer%20encoder%2C%20making%20it%20easy%20to%0Arun%2C%20customize%20and%20extend%20the%20prediction%20pipeline%20for%20practical%20use.%20Extensive%0Aexperiments%20demonstrate%20that%20%5Cours%7B%7D%20obtains%20significant%20gains%20over%20existing%203D%0Aobject%20detection%20methods%20in%206%20indoor%20benchmarks%3A%20ScanNet%20%28%2B1.1%20mAP50%29%2C%0AARKitScenes%20%28%2B19.4%20mAP25%29%2C%20S3DIS%20%28%2B9.1%20mAP50%29%2C%20MultiScan%20%28%2B9.3%20mAP50%29%2C%203RScan%0A%28%2B3.2%20mAP50%29%2C%20and%20ScanNet%2B%2B%20%28%2B2.7%20mAP50%29.%20Code%20is%20available%20at%0Ahttps%3A//github.com/filapro/unidet3d%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04234v1&entry.124074799=Read"},
{"title": "Faster Sampling from Log-Concave Densities over Polytopes via Efficient\n  Linear Solvers", "author": "Oren Mangoubi and Nisheeth K. Vishnoi", "abstract": "  We consider the problem of sampling from a log-concave distribution\n$\\pi(\\theta) \\propto e^{-f(\\theta)}$ constrained to a polytope $K:=\\{\\theta \\in\n\\mathbb{R}^d: A\\theta \\leq b\\}$, where $A\\in \\mathbb{R}^{m\\times d}$ and $b \\in\n\\mathbb{R}^m$.The fastest-known algorithm \\cite{mangoubi2022faster} for the\nsetting when $f$ is $O(1)$-Lipschitz or $O(1)$-smooth runs in roughly $O(md\n\\times md^{\\omega -1})$ arithmetic operations, where the $md^{\\omega -1}$ term\narises because each Markov chain step requires computing a matrix inversion and\ndeterminant (here $\\omega \\approx 2.37$ is the matrix multiplication constant).\nWe present a nearly-optimal implementation of this Markov chain with per-step\ncomplexity which is roughly the number of non-zero entries of $A$ while the\nnumber of Markov chain steps remains the same. The key technical ingredients\nare 1) to show that the matrices that arise in this Dikin walk change slowly,\n2) to deploy efficient linear solvers that can leverage this slow change to\nspeed up matrix inversion by using information computed in previous steps, and\n3) to speed up the computation of the determinantal term in the Metropolis\nfilter step via a randomized Taylor series-based estimator.\n", "link": "http://arxiv.org/abs/2409.04320v1", "date": "2024-09-06", "relevancy": 2.3461, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4903}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.461}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20Sampling%20from%20Log-Concave%20Densities%20over%20Polytopes%20via%20Efficient%0A%20%20Linear%20Solvers&body=Title%3A%20Faster%20Sampling%20from%20Log-Concave%20Densities%20over%20Polytopes%20via%20Efficient%0A%20%20Linear%20Solvers%0AAuthor%3A%20Oren%20Mangoubi%20and%20Nisheeth%20K.%20Vishnoi%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20sampling%20from%20a%20log-concave%20distribution%0A%24%5Cpi%28%5Ctheta%29%20%5Cpropto%20e%5E%7B-f%28%5Ctheta%29%7D%24%20constrained%20to%20a%20polytope%20%24K%3A%3D%5C%7B%5Ctheta%20%5Cin%0A%5Cmathbb%7BR%7D%5Ed%3A%20A%5Ctheta%20%5Cleq%20b%5C%7D%24%2C%20where%20%24A%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes%20d%7D%24%20and%20%24b%20%5Cin%0A%5Cmathbb%7BR%7D%5Em%24.The%20fastest-known%20algorithm%20%5Ccite%7Bmangoubi2022faster%7D%20for%20the%0Asetting%20when%20%24f%24%20is%20%24O%281%29%24-Lipschitz%20or%20%24O%281%29%24-smooth%20runs%20in%20roughly%20%24O%28md%0A%5Ctimes%20md%5E%7B%5Comega%20-1%7D%29%24%20arithmetic%20operations%2C%20where%20the%20%24md%5E%7B%5Comega%20-1%7D%24%20term%0Aarises%20because%20each%20Markov%20chain%20step%20requires%20computing%20a%20matrix%20inversion%20and%0Adeterminant%20%28here%20%24%5Comega%20%5Capprox%202.37%24%20is%20the%20matrix%20multiplication%20constant%29.%0AWe%20present%20a%20nearly-optimal%20implementation%20of%20this%20Markov%20chain%20with%20per-step%0Acomplexity%20which%20is%20roughly%20the%20number%20of%20non-zero%20entries%20of%20%24A%24%20while%20the%0Anumber%20of%20Markov%20chain%20steps%20remains%20the%20same.%20The%20key%20technical%20ingredients%0Aare%201%29%20to%20show%20that%20the%20matrices%20that%20arise%20in%20this%20Dikin%20walk%20change%20slowly%2C%0A2%29%20to%20deploy%20efficient%20linear%20solvers%20that%20can%20leverage%20this%20slow%20change%20to%0Aspeed%20up%20matrix%20inversion%20by%20using%20information%20computed%20in%20previous%20steps%2C%20and%0A3%29%20to%20speed%20up%20the%20computation%20of%20the%20determinantal%20term%20in%20the%20Metropolis%0Afilter%20step%20via%20a%20randomized%20Taylor%20series-based%20estimator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520Sampling%2520from%2520Log-Concave%2520Densities%2520over%2520Polytopes%2520via%2520Efficient%250A%2520%2520Linear%2520Solvers%26entry.906535625%3DOren%2520Mangoubi%2520and%2520Nisheeth%2520K.%2520Vishnoi%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520sampling%2520from%2520a%2520log-concave%2520distribution%250A%2524%255Cpi%2528%255Ctheta%2529%2520%255Cpropto%2520e%255E%257B-f%2528%255Ctheta%2529%257D%2524%2520constrained%2520to%2520a%2520polytope%2520%2524K%253A%253D%255C%257B%255Ctheta%2520%255Cin%250A%255Cmathbb%257BR%257D%255Ed%253A%2520A%255Ctheta%2520%255Cleq%2520b%255C%257D%2524%252C%2520where%2520%2524A%255Cin%2520%255Cmathbb%257BR%257D%255E%257Bm%255Ctimes%2520d%257D%2524%2520and%2520%2524b%2520%255Cin%250A%255Cmathbb%257BR%257D%255Em%2524.The%2520fastest-known%2520algorithm%2520%255Ccite%257Bmangoubi2022faster%257D%2520for%2520the%250Asetting%2520when%2520%2524f%2524%2520is%2520%2524O%25281%2529%2524-Lipschitz%2520or%2520%2524O%25281%2529%2524-smooth%2520runs%2520in%2520roughly%2520%2524O%2528md%250A%255Ctimes%2520md%255E%257B%255Comega%2520-1%257D%2529%2524%2520arithmetic%2520operations%252C%2520where%2520the%2520%2524md%255E%257B%255Comega%2520-1%257D%2524%2520term%250Aarises%2520because%2520each%2520Markov%2520chain%2520step%2520requires%2520computing%2520a%2520matrix%2520inversion%2520and%250Adeterminant%2520%2528here%2520%2524%255Comega%2520%255Capprox%25202.37%2524%2520is%2520the%2520matrix%2520multiplication%2520constant%2529.%250AWe%2520present%2520a%2520nearly-optimal%2520implementation%2520of%2520this%2520Markov%2520chain%2520with%2520per-step%250Acomplexity%2520which%2520is%2520roughly%2520the%2520number%2520of%2520non-zero%2520entries%2520of%2520%2524A%2524%2520while%2520the%250Anumber%2520of%2520Markov%2520chain%2520steps%2520remains%2520the%2520same.%2520The%2520key%2520technical%2520ingredients%250Aare%25201%2529%2520to%2520show%2520that%2520the%2520matrices%2520that%2520arise%2520in%2520this%2520Dikin%2520walk%2520change%2520slowly%252C%250A2%2529%2520to%2520deploy%2520efficient%2520linear%2520solvers%2520that%2520can%2520leverage%2520this%2520slow%2520change%2520to%250Aspeed%2520up%2520matrix%2520inversion%2520by%2520using%2520information%2520computed%2520in%2520previous%2520steps%252C%2520and%250A3%2529%2520to%2520speed%2520up%2520the%2520computation%2520of%2520the%2520determinantal%2520term%2520in%2520the%2520Metropolis%250Afilter%2520step%2520via%2520a%2520randomized%2520Taylor%2520series-based%2520estimator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Sampling%20from%20Log-Concave%20Densities%20over%20Polytopes%20via%20Efficient%0A%20%20Linear%20Solvers&entry.906535625=Oren%20Mangoubi%20and%20Nisheeth%20K.%20Vishnoi&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20sampling%20from%20a%20log-concave%20distribution%0A%24%5Cpi%28%5Ctheta%29%20%5Cpropto%20e%5E%7B-f%28%5Ctheta%29%7D%24%20constrained%20to%20a%20polytope%20%24K%3A%3D%5C%7B%5Ctheta%20%5Cin%0A%5Cmathbb%7BR%7D%5Ed%3A%20A%5Ctheta%20%5Cleq%20b%5C%7D%24%2C%20where%20%24A%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes%20d%7D%24%20and%20%24b%20%5Cin%0A%5Cmathbb%7BR%7D%5Em%24.The%20fastest-known%20algorithm%20%5Ccite%7Bmangoubi2022faster%7D%20for%20the%0Asetting%20when%20%24f%24%20is%20%24O%281%29%24-Lipschitz%20or%20%24O%281%29%24-smooth%20runs%20in%20roughly%20%24O%28md%0A%5Ctimes%20md%5E%7B%5Comega%20-1%7D%29%24%20arithmetic%20operations%2C%20where%20the%20%24md%5E%7B%5Comega%20-1%7D%24%20term%0Aarises%20because%20each%20Markov%20chain%20step%20requires%20computing%20a%20matrix%20inversion%20and%0Adeterminant%20%28here%20%24%5Comega%20%5Capprox%202.37%24%20is%20the%20matrix%20multiplication%20constant%29.%0AWe%20present%20a%20nearly-optimal%20implementation%20of%20this%20Markov%20chain%20with%20per-step%0Acomplexity%20which%20is%20roughly%20the%20number%20of%20non-zero%20entries%20of%20%24A%24%20while%20the%0Anumber%20of%20Markov%20chain%20steps%20remains%20the%20same.%20The%20key%20technical%20ingredients%0Aare%201%29%20to%20show%20that%20the%20matrices%20that%20arise%20in%20this%20Dikin%20walk%20change%20slowly%2C%0A2%29%20to%20deploy%20efficient%20linear%20solvers%20that%20can%20leverage%20this%20slow%20change%20to%0Aspeed%20up%20matrix%20inversion%20by%20using%20information%20computed%20in%20previous%20steps%2C%20and%0A3%29%20to%20speed%20up%20the%20computation%20of%20the%20determinantal%20term%20in%20the%20Metropolis%0Afilter%20step%20via%20a%20randomized%20Taylor%20series-based%20estimator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04320v1&entry.124074799=Read"},
{"title": "Video alignment using unsupervised learning of local and global features", "author": "Niloufar Fakhfour and Mohammad ShahverdiKondori and Sajjad Hashembeiki and Mohammadjavad Norouzi and Hoda Mohammadzade", "abstract": "  In this paper, we tackle the problem of video alignment, the process of\nmatching the frames of a pair of videos containing similar actions. The main\nchallenge in video alignment is that accurate correspondence should be\nestablished despite the differences in the execution processes and appearances\nbetween the two videos. We introduce an unsupervised method for alignment that\nuses global and local features of the frames. In particular, we introduce\neffective features for each video frame by means of three machine vision tools:\nperson detection, pose estimation, and VGG network. Then the features are\nprocessed and combined to construct a multidimensional time series that\nrepresent the video. The resulting time series are used to align videos of the\nsame actions using a novel version of dynamic time warping named Diagonalized\nDynamic Time Warping(DDTW). The main advantage of our approach is that no\ntraining is required, which makes it applicable for any new type of action\nwithout any need to collect training samples for it. Additionally, our approach\ncan be used for framewise labeling of action phases in a dataset with only a\nfew labeled videos. For evaluation, we considered video synchronization and\nphase classification tasks on the Penn action and subset of UCF101 datasets.\nAlso, for an effective evaluation of the video synchronization task, we present\na new metric called Enclosed Area Error(EAE). The results show that our method\noutperforms previous state-of-the-art methods, such as TCC, and other\nself-supervised and weakly supervised methods.\n", "link": "http://arxiv.org/abs/2304.06841v3", "date": "2024-09-06", "relevancy": 2.3396, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6224}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5626}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20alignment%20using%20unsupervised%20learning%20of%20local%20and%20global%20features&body=Title%3A%20Video%20alignment%20using%20unsupervised%20learning%20of%20local%20and%20global%20features%0AAuthor%3A%20Niloufar%20Fakhfour%20and%20Mohammad%20ShahverdiKondori%20and%20Sajjad%20Hashembeiki%20and%20Mohammadjavad%20Norouzi%20and%20Hoda%20Mohammadzade%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20tackle%20the%20problem%20of%20video%20alignment%2C%20the%20process%20of%0Amatching%20the%20frames%20of%20a%20pair%20of%20videos%20containing%20similar%20actions.%20The%20main%0Achallenge%20in%20video%20alignment%20is%20that%20accurate%20correspondence%20should%20be%0Aestablished%20despite%20the%20differences%20in%20the%20execution%20processes%20and%20appearances%0Abetween%20the%20two%20videos.%20We%20introduce%20an%20unsupervised%20method%20for%20alignment%20that%0Auses%20global%20and%20local%20features%20of%20the%20frames.%20In%20particular%2C%20we%20introduce%0Aeffective%20features%20for%20each%20video%20frame%20by%20means%20of%20three%20machine%20vision%20tools%3A%0Aperson%20detection%2C%20pose%20estimation%2C%20and%20VGG%20network.%20Then%20the%20features%20are%0Aprocessed%20and%20combined%20to%20construct%20a%20multidimensional%20time%20series%20that%0Arepresent%20the%20video.%20The%20resulting%20time%20series%20are%20used%20to%20align%20videos%20of%20the%0Asame%20actions%20using%20a%20novel%20version%20of%20dynamic%20time%20warping%20named%20Diagonalized%0ADynamic%20Time%20Warping%28DDTW%29.%20The%20main%20advantage%20of%20our%20approach%20is%20that%20no%0Atraining%20is%20required%2C%20which%20makes%20it%20applicable%20for%20any%20new%20type%20of%20action%0Awithout%20any%20need%20to%20collect%20training%20samples%20for%20it.%20Additionally%2C%20our%20approach%0Acan%20be%20used%20for%20framewise%20labeling%20of%20action%20phases%20in%20a%20dataset%20with%20only%20a%0Afew%20labeled%20videos.%20For%20evaluation%2C%20we%20considered%20video%20synchronization%20and%0Aphase%20classification%20tasks%20on%20the%20Penn%20action%20and%20subset%20of%20UCF101%20datasets.%0AAlso%2C%20for%20an%20effective%20evaluation%20of%20the%20video%20synchronization%20task%2C%20we%20present%0Aa%20new%20metric%20called%20Enclosed%20Area%20Error%28EAE%29.%20The%20results%20show%20that%20our%20method%0Aoutperforms%20previous%20state-of-the-art%20methods%2C%20such%20as%20TCC%2C%20and%20other%0Aself-supervised%20and%20weakly%20supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.06841v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520alignment%2520using%2520unsupervised%2520learning%2520of%2520local%2520and%2520global%2520features%26entry.906535625%3DNiloufar%2520Fakhfour%2520and%2520Mohammad%2520ShahverdiKondori%2520and%2520Sajjad%2520Hashembeiki%2520and%2520Mohammadjavad%2520Norouzi%2520and%2520Hoda%2520Mohammadzade%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520tackle%2520the%2520problem%2520of%2520video%2520alignment%252C%2520the%2520process%2520of%250Amatching%2520the%2520frames%2520of%2520a%2520pair%2520of%2520videos%2520containing%2520similar%2520actions.%2520The%2520main%250Achallenge%2520in%2520video%2520alignment%2520is%2520that%2520accurate%2520correspondence%2520should%2520be%250Aestablished%2520despite%2520the%2520differences%2520in%2520the%2520execution%2520processes%2520and%2520appearances%250Abetween%2520the%2520two%2520videos.%2520We%2520introduce%2520an%2520unsupervised%2520method%2520for%2520alignment%2520that%250Auses%2520global%2520and%2520local%2520features%2520of%2520the%2520frames.%2520In%2520particular%252C%2520we%2520introduce%250Aeffective%2520features%2520for%2520each%2520video%2520frame%2520by%2520means%2520of%2520three%2520machine%2520vision%2520tools%253A%250Aperson%2520detection%252C%2520pose%2520estimation%252C%2520and%2520VGG%2520network.%2520Then%2520the%2520features%2520are%250Aprocessed%2520and%2520combined%2520to%2520construct%2520a%2520multidimensional%2520time%2520series%2520that%250Arepresent%2520the%2520video.%2520The%2520resulting%2520time%2520series%2520are%2520used%2520to%2520align%2520videos%2520of%2520the%250Asame%2520actions%2520using%2520a%2520novel%2520version%2520of%2520dynamic%2520time%2520warping%2520named%2520Diagonalized%250ADynamic%2520Time%2520Warping%2528DDTW%2529.%2520The%2520main%2520advantage%2520of%2520our%2520approach%2520is%2520that%2520no%250Atraining%2520is%2520required%252C%2520which%2520makes%2520it%2520applicable%2520for%2520any%2520new%2520type%2520of%2520action%250Awithout%2520any%2520need%2520to%2520collect%2520training%2520samples%2520for%2520it.%2520Additionally%252C%2520our%2520approach%250Acan%2520be%2520used%2520for%2520framewise%2520labeling%2520of%2520action%2520phases%2520in%2520a%2520dataset%2520with%2520only%2520a%250Afew%2520labeled%2520videos.%2520For%2520evaluation%252C%2520we%2520considered%2520video%2520synchronization%2520and%250Aphase%2520classification%2520tasks%2520on%2520the%2520Penn%2520action%2520and%2520subset%2520of%2520UCF101%2520datasets.%250AAlso%252C%2520for%2520an%2520effective%2520evaluation%2520of%2520the%2520video%2520synchronization%2520task%252C%2520we%2520present%250Aa%2520new%2520metric%2520called%2520Enclosed%2520Area%2520Error%2528EAE%2529.%2520The%2520results%2520show%2520that%2520our%2520method%250Aoutperforms%2520previous%2520state-of-the-art%2520methods%252C%2520such%2520as%2520TCC%252C%2520and%2520other%250Aself-supervised%2520and%2520weakly%2520supervised%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.06841v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20alignment%20using%20unsupervised%20learning%20of%20local%20and%20global%20features&entry.906535625=Niloufar%20Fakhfour%20and%20Mohammad%20ShahverdiKondori%20and%20Sajjad%20Hashembeiki%20and%20Mohammadjavad%20Norouzi%20and%20Hoda%20Mohammadzade&entry.1292438233=%20%20In%20this%20paper%2C%20we%20tackle%20the%20problem%20of%20video%20alignment%2C%20the%20process%20of%0Amatching%20the%20frames%20of%20a%20pair%20of%20videos%20containing%20similar%20actions.%20The%20main%0Achallenge%20in%20video%20alignment%20is%20that%20accurate%20correspondence%20should%20be%0Aestablished%20despite%20the%20differences%20in%20the%20execution%20processes%20and%20appearances%0Abetween%20the%20two%20videos.%20We%20introduce%20an%20unsupervised%20method%20for%20alignment%20that%0Auses%20global%20and%20local%20features%20of%20the%20frames.%20In%20particular%2C%20we%20introduce%0Aeffective%20features%20for%20each%20video%20frame%20by%20means%20of%20three%20machine%20vision%20tools%3A%0Aperson%20detection%2C%20pose%20estimation%2C%20and%20VGG%20network.%20Then%20the%20features%20are%0Aprocessed%20and%20combined%20to%20construct%20a%20multidimensional%20time%20series%20that%0Arepresent%20the%20video.%20The%20resulting%20time%20series%20are%20used%20to%20align%20videos%20of%20the%0Asame%20actions%20using%20a%20novel%20version%20of%20dynamic%20time%20warping%20named%20Diagonalized%0ADynamic%20Time%20Warping%28DDTW%29.%20The%20main%20advantage%20of%20our%20approach%20is%20that%20no%0Atraining%20is%20required%2C%20which%20makes%20it%20applicable%20for%20any%20new%20type%20of%20action%0Awithout%20any%20need%20to%20collect%20training%20samples%20for%20it.%20Additionally%2C%20our%20approach%0Acan%20be%20used%20for%20framewise%20labeling%20of%20action%20phases%20in%20a%20dataset%20with%20only%20a%0Afew%20labeled%20videos.%20For%20evaluation%2C%20we%20considered%20video%20synchronization%20and%0Aphase%20classification%20tasks%20on%20the%20Penn%20action%20and%20subset%20of%20UCF101%20datasets.%0AAlso%2C%20for%20an%20effective%20evaluation%20of%20the%20video%20synchronization%20task%2C%20we%20present%0Aa%20new%20metric%20called%20Enclosed%20Area%20Error%28EAE%29.%20The%20results%20show%20that%20our%20method%0Aoutperforms%20previous%20state-of-the-art%20methods%2C%20such%20as%20TCC%2C%20and%20other%0Aself-supervised%20and%20weakly%20supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.06841v3&entry.124074799=Read"},
{"title": "HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale\n  Space Using Wearable IMUs and LiDAR", "author": "Yudi Dai and Zhiyong Wang and Xiping Lin and Chenglu Wen and Lan Xu and Siqi Shen and Yuexin Ma and Cheng Wang", "abstract": "  We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture\nmethod, aimed at accurately and efficiently creating a dynamic digital world,\ncontaining large-scale indoor-outdoor scenes, diverse human motions, rich\nhuman-human interactions, and human-environment interactions. By utilizing\nbody-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human\nmotions in unconstrained space without the need for external devices and\npre-built maps. This affords great flexibility and accessibility for\nhuman-centered interaction and 4D scene capturing in various environments.\nTaking into account that IMUs can capture human spatially unrestricted poses\nbut are prone to drifting for long-period using, and while LiDAR is stable for\nglobal localization but rough for local positions and orientations, HiSC4D\nemploys a joint optimization method, harmonizing all sensors and utilizing\nenvironment cues, yielding promising results for long-term capture in large\nscenes. To promote research of egocentric human interaction in large scenes and\nfacilitate downstream tasks, we also present a dataset, containing 8 sequences\nin 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4D\nhuman motions with SMPL annotations and dynamic scenes, 31k frames of cropped\nhuman point clouds, and scene mesh of the environment. A variety of scenarios,\nsuch as the basketball gym and commercial street, alongside challenging human\nmotions, such as daily greeting, one-on-one basketball playing, and tour\nguiding, demonstrate the effectiveness and the generalization ability of\nHiSC4D. The dataset and code will be publicated on\nwww.lidarhumanmotion.net/hisc4d available for research purposes.\n", "link": "http://arxiv.org/abs/2409.04398v1", "date": "2024-09-06", "relevancy": 2.3258, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5843}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5843}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiSC4D%3A%20Human-centered%20interaction%20and%204D%20Scene%20Capture%20in%20Large-scale%0A%20%20Space%20Using%20Wearable%20IMUs%20and%20LiDAR&body=Title%3A%20HiSC4D%3A%20Human-centered%20interaction%20and%204D%20Scene%20Capture%20in%20Large-scale%0A%20%20Space%20Using%20Wearable%20IMUs%20and%20LiDAR%0AAuthor%3A%20Yudi%20Dai%20and%20Zhiyong%20Wang%20and%20Xiping%20Lin%20and%20Chenglu%20Wen%20and%20Lan%20Xu%20and%20Siqi%20Shen%20and%20Yuexin%20Ma%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20We%20introduce%20HiSC4D%2C%20a%20novel%20Human-centered%20interaction%20and%204D%20Scene%20Capture%0Amethod%2C%20aimed%20at%20accurately%20and%20efficiently%20creating%20a%20dynamic%20digital%20world%2C%0Acontaining%20large-scale%20indoor-outdoor%20scenes%2C%20diverse%20human%20motions%2C%20rich%0Ahuman-human%20interactions%2C%20and%20human-environment%20interactions.%20By%20utilizing%0Abody-mounted%20IMUs%20and%20a%20head-mounted%20LiDAR%2C%20HiSC4D%20can%20capture%20egocentric%20human%0Amotions%20in%20unconstrained%20space%20without%20the%20need%20for%20external%20devices%20and%0Apre-built%20maps.%20This%20affords%20great%20flexibility%20and%20accessibility%20for%0Ahuman-centered%20interaction%20and%204D%20scene%20capturing%20in%20various%20environments.%0ATaking%20into%20account%20that%20IMUs%20can%20capture%20human%20spatially%20unrestricted%20poses%0Abut%20are%20prone%20to%20drifting%20for%20long-period%20using%2C%20and%20while%20LiDAR%20is%20stable%20for%0Aglobal%20localization%20but%20rough%20for%20local%20positions%20and%20orientations%2C%20HiSC4D%0Aemploys%20a%20joint%20optimization%20method%2C%20harmonizing%20all%20sensors%20and%20utilizing%0Aenvironment%20cues%2C%20yielding%20promising%20results%20for%20long-term%20capture%20in%20large%0Ascenes.%20To%20promote%20research%20of%20egocentric%20human%20interaction%20in%20large%20scenes%20and%0Afacilitate%20downstream%20tasks%2C%20we%20also%20present%20a%20dataset%2C%20containing%208%20sequences%0Ain%204%20large%20scenes%20%28200%20to%205%2C000%20%24m%5E2%24%29%2C%20providing%2036k%20frames%20of%20accurate%204D%0Ahuman%20motions%20with%20SMPL%20annotations%20and%20dynamic%20scenes%2C%2031k%20frames%20of%20cropped%0Ahuman%20point%20clouds%2C%20and%20scene%20mesh%20of%20the%20environment.%20A%20variety%20of%20scenarios%2C%0Asuch%20as%20the%20basketball%20gym%20and%20commercial%20street%2C%20alongside%20challenging%20human%0Amotions%2C%20such%20as%20daily%20greeting%2C%20one-on-one%20basketball%20playing%2C%20and%20tour%0Aguiding%2C%20demonstrate%20the%20effectiveness%20and%20the%20generalization%20ability%20of%0AHiSC4D.%20The%20dataset%20and%20code%20will%20be%20publicated%20on%0Awww.lidarhumanmotion.net/hisc4d%20available%20for%20research%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiSC4D%253A%2520Human-centered%2520interaction%2520and%25204D%2520Scene%2520Capture%2520in%2520Large-scale%250A%2520%2520Space%2520Using%2520Wearable%2520IMUs%2520and%2520LiDAR%26entry.906535625%3DYudi%2520Dai%2520and%2520Zhiyong%2520Wang%2520and%2520Xiping%2520Lin%2520and%2520Chenglu%2520Wen%2520and%2520Lan%2520Xu%2520and%2520Siqi%2520Shen%2520and%2520Yuexin%2520Ma%2520and%2520Cheng%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520HiSC4D%252C%2520a%2520novel%2520Human-centered%2520interaction%2520and%25204D%2520Scene%2520Capture%250Amethod%252C%2520aimed%2520at%2520accurately%2520and%2520efficiently%2520creating%2520a%2520dynamic%2520digital%2520world%252C%250Acontaining%2520large-scale%2520indoor-outdoor%2520scenes%252C%2520diverse%2520human%2520motions%252C%2520rich%250Ahuman-human%2520interactions%252C%2520and%2520human-environment%2520interactions.%2520By%2520utilizing%250Abody-mounted%2520IMUs%2520and%2520a%2520head-mounted%2520LiDAR%252C%2520HiSC4D%2520can%2520capture%2520egocentric%2520human%250Amotions%2520in%2520unconstrained%2520space%2520without%2520the%2520need%2520for%2520external%2520devices%2520and%250Apre-built%2520maps.%2520This%2520affords%2520great%2520flexibility%2520and%2520accessibility%2520for%250Ahuman-centered%2520interaction%2520and%25204D%2520scene%2520capturing%2520in%2520various%2520environments.%250ATaking%2520into%2520account%2520that%2520IMUs%2520can%2520capture%2520human%2520spatially%2520unrestricted%2520poses%250Abut%2520are%2520prone%2520to%2520drifting%2520for%2520long-period%2520using%252C%2520and%2520while%2520LiDAR%2520is%2520stable%2520for%250Aglobal%2520localization%2520but%2520rough%2520for%2520local%2520positions%2520and%2520orientations%252C%2520HiSC4D%250Aemploys%2520a%2520joint%2520optimization%2520method%252C%2520harmonizing%2520all%2520sensors%2520and%2520utilizing%250Aenvironment%2520cues%252C%2520yielding%2520promising%2520results%2520for%2520long-term%2520capture%2520in%2520large%250Ascenes.%2520To%2520promote%2520research%2520of%2520egocentric%2520human%2520interaction%2520in%2520large%2520scenes%2520and%250Afacilitate%2520downstream%2520tasks%252C%2520we%2520also%2520present%2520a%2520dataset%252C%2520containing%25208%2520sequences%250Ain%25204%2520large%2520scenes%2520%2528200%2520to%25205%252C000%2520%2524m%255E2%2524%2529%252C%2520providing%252036k%2520frames%2520of%2520accurate%25204D%250Ahuman%2520motions%2520with%2520SMPL%2520annotations%2520and%2520dynamic%2520scenes%252C%252031k%2520frames%2520of%2520cropped%250Ahuman%2520point%2520clouds%252C%2520and%2520scene%2520mesh%2520of%2520the%2520environment.%2520A%2520variety%2520of%2520scenarios%252C%250Asuch%2520as%2520the%2520basketball%2520gym%2520and%2520commercial%2520street%252C%2520alongside%2520challenging%2520human%250Amotions%252C%2520such%2520as%2520daily%2520greeting%252C%2520one-on-one%2520basketball%2520playing%252C%2520and%2520tour%250Aguiding%252C%2520demonstrate%2520the%2520effectiveness%2520and%2520the%2520generalization%2520ability%2520of%250AHiSC4D.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520publicated%2520on%250Awww.lidarhumanmotion.net/hisc4d%2520available%2520for%2520research%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiSC4D%3A%20Human-centered%20interaction%20and%204D%20Scene%20Capture%20in%20Large-scale%0A%20%20Space%20Using%20Wearable%20IMUs%20and%20LiDAR&entry.906535625=Yudi%20Dai%20and%20Zhiyong%20Wang%20and%20Xiping%20Lin%20and%20Chenglu%20Wen%20and%20Lan%20Xu%20and%20Siqi%20Shen%20and%20Yuexin%20Ma%20and%20Cheng%20Wang&entry.1292438233=%20%20We%20introduce%20HiSC4D%2C%20a%20novel%20Human-centered%20interaction%20and%204D%20Scene%20Capture%0Amethod%2C%20aimed%20at%20accurately%20and%20efficiently%20creating%20a%20dynamic%20digital%20world%2C%0Acontaining%20large-scale%20indoor-outdoor%20scenes%2C%20diverse%20human%20motions%2C%20rich%0Ahuman-human%20interactions%2C%20and%20human-environment%20interactions.%20By%20utilizing%0Abody-mounted%20IMUs%20and%20a%20head-mounted%20LiDAR%2C%20HiSC4D%20can%20capture%20egocentric%20human%0Amotions%20in%20unconstrained%20space%20without%20the%20need%20for%20external%20devices%20and%0Apre-built%20maps.%20This%20affords%20great%20flexibility%20and%20accessibility%20for%0Ahuman-centered%20interaction%20and%204D%20scene%20capturing%20in%20various%20environments.%0ATaking%20into%20account%20that%20IMUs%20can%20capture%20human%20spatially%20unrestricted%20poses%0Abut%20are%20prone%20to%20drifting%20for%20long-period%20using%2C%20and%20while%20LiDAR%20is%20stable%20for%0Aglobal%20localization%20but%20rough%20for%20local%20positions%20and%20orientations%2C%20HiSC4D%0Aemploys%20a%20joint%20optimization%20method%2C%20harmonizing%20all%20sensors%20and%20utilizing%0Aenvironment%20cues%2C%20yielding%20promising%20results%20for%20long-term%20capture%20in%20large%0Ascenes.%20To%20promote%20research%20of%20egocentric%20human%20interaction%20in%20large%20scenes%20and%0Afacilitate%20downstream%20tasks%2C%20we%20also%20present%20a%20dataset%2C%20containing%208%20sequences%0Ain%204%20large%20scenes%20%28200%20to%205%2C000%20%24m%5E2%24%29%2C%20providing%2036k%20frames%20of%20accurate%204D%0Ahuman%20motions%20with%20SMPL%20annotations%20and%20dynamic%20scenes%2C%2031k%20frames%20of%20cropped%0Ahuman%20point%20clouds%2C%20and%20scene%20mesh%20of%20the%20environment.%20A%20variety%20of%20scenarios%2C%0Asuch%20as%20the%20basketball%20gym%20and%20commercial%20street%2C%20alongside%20challenging%20human%0Amotions%2C%20such%20as%20daily%20greeting%2C%20one-on-one%20basketball%20playing%2C%20and%20tour%0Aguiding%2C%20demonstrate%20the%20effectiveness%20and%20the%20generalization%20ability%20of%0AHiSC4D.%20The%20dataset%20and%20code%20will%20be%20publicated%20on%0Awww.lidarhumanmotion.net/hisc4d%20available%20for%20research%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04398v1&entry.124074799=Read"},
{"title": "Introducing Gating and Context into Temporal Action Detection", "author": "Aglind Reka and Diana Laura Borza and Dominick Reilly and Michal Balazia and Francois Bremond", "abstract": "  Temporal Action Detection (TAD), the task of localizing and classifying\nactions in untrimmed video, remains challenging due to action overlaps and\nvariable action durations. Recent findings suggest that TAD performance is\ndependent on the structural design of transformers rather than on the\nself-attention mechanism. Building on this insight, we propose a refined\nfeature extraction process through lightweight, yet effective operations.\nFirst, we employ a local branch that employs parallel convolutions with varying\nwindow sizes to capture both fine-grained and coarse-grained temporal features.\nThis branch incorporates a gating mechanism to select the most relevant\nfeatures. Second, we introduce a context branch that uses boundary frames as\nkey-value pairs to analyze their relationship with the central frame through\ncross-attention. The proposed method captures temporal dependencies and\nimproves contextual understanding. Evaluations of the gating mechanism and\ncontext branch on challenging datasets (THUMOS14 and EPIC-KITCHEN 100) show a\nconsistent improvement over the baseline and existing methods.\n", "link": "http://arxiv.org/abs/2409.04205v1", "date": "2024-09-06", "relevancy": 2.3245, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6475}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20Gating%20and%20Context%20into%20Temporal%20Action%20Detection&body=Title%3A%20Introducing%20Gating%20and%20Context%20into%20Temporal%20Action%20Detection%0AAuthor%3A%20Aglind%20Reka%20and%20Diana%20Laura%20Borza%20and%20Dominick%20Reilly%20and%20Michal%20Balazia%20and%20Francois%20Bremond%0AAbstract%3A%20%20%20Temporal%20Action%20Detection%20%28TAD%29%2C%20the%20task%20of%20localizing%20and%20classifying%0Aactions%20in%20untrimmed%20video%2C%20remains%20challenging%20due%20to%20action%20overlaps%20and%0Avariable%20action%20durations.%20Recent%20findings%20suggest%20that%20TAD%20performance%20is%0Adependent%20on%20the%20structural%20design%20of%20transformers%20rather%20than%20on%20the%0Aself-attention%20mechanism.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20refined%0Afeature%20extraction%20process%20through%20lightweight%2C%20yet%20effective%20operations.%0AFirst%2C%20we%20employ%20a%20local%20branch%20that%20employs%20parallel%20convolutions%20with%20varying%0Awindow%20sizes%20to%20capture%20both%20fine-grained%20and%20coarse-grained%20temporal%20features.%0AThis%20branch%20incorporates%20a%20gating%20mechanism%20to%20select%20the%20most%20relevant%0Afeatures.%20Second%2C%20we%20introduce%20a%20context%20branch%20that%20uses%20boundary%20frames%20as%0Akey-value%20pairs%20to%20analyze%20their%20relationship%20with%20the%20central%20frame%20through%0Across-attention.%20The%20proposed%20method%20captures%20temporal%20dependencies%20and%0Aimproves%20contextual%20understanding.%20Evaluations%20of%20the%20gating%20mechanism%20and%0Acontext%20branch%20on%20challenging%20datasets%20%28THUMOS14%20and%20EPIC-KITCHEN%20100%29%20show%20a%0Aconsistent%20improvement%20over%20the%20baseline%20and%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520Gating%2520and%2520Context%2520into%2520Temporal%2520Action%2520Detection%26entry.906535625%3DAglind%2520Reka%2520and%2520Diana%2520Laura%2520Borza%2520and%2520Dominick%2520Reilly%2520and%2520Michal%2520Balazia%2520and%2520Francois%2520Bremond%26entry.1292438233%3D%2520%2520Temporal%2520Action%2520Detection%2520%2528TAD%2529%252C%2520the%2520task%2520of%2520localizing%2520and%2520classifying%250Aactions%2520in%2520untrimmed%2520video%252C%2520remains%2520challenging%2520due%2520to%2520action%2520overlaps%2520and%250Avariable%2520action%2520durations.%2520Recent%2520findings%2520suggest%2520that%2520TAD%2520performance%2520is%250Adependent%2520on%2520the%2520structural%2520design%2520of%2520transformers%2520rather%2520than%2520on%2520the%250Aself-attention%2520mechanism.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520a%2520refined%250Afeature%2520extraction%2520process%2520through%2520lightweight%252C%2520yet%2520effective%2520operations.%250AFirst%252C%2520we%2520employ%2520a%2520local%2520branch%2520that%2520employs%2520parallel%2520convolutions%2520with%2520varying%250Awindow%2520sizes%2520to%2520capture%2520both%2520fine-grained%2520and%2520coarse-grained%2520temporal%2520features.%250AThis%2520branch%2520incorporates%2520a%2520gating%2520mechanism%2520to%2520select%2520the%2520most%2520relevant%250Afeatures.%2520Second%252C%2520we%2520introduce%2520a%2520context%2520branch%2520that%2520uses%2520boundary%2520frames%2520as%250Akey-value%2520pairs%2520to%2520analyze%2520their%2520relationship%2520with%2520the%2520central%2520frame%2520through%250Across-attention.%2520The%2520proposed%2520method%2520captures%2520temporal%2520dependencies%2520and%250Aimproves%2520contextual%2520understanding.%2520Evaluations%2520of%2520the%2520gating%2520mechanism%2520and%250Acontext%2520branch%2520on%2520challenging%2520datasets%2520%2528THUMOS14%2520and%2520EPIC-KITCHEN%2520100%2529%2520show%2520a%250Aconsistent%2520improvement%2520over%2520the%2520baseline%2520and%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20Gating%20and%20Context%20into%20Temporal%20Action%20Detection&entry.906535625=Aglind%20Reka%20and%20Diana%20Laura%20Borza%20and%20Dominick%20Reilly%20and%20Michal%20Balazia%20and%20Francois%20Bremond&entry.1292438233=%20%20Temporal%20Action%20Detection%20%28TAD%29%2C%20the%20task%20of%20localizing%20and%20classifying%0Aactions%20in%20untrimmed%20video%2C%20remains%20challenging%20due%20to%20action%20overlaps%20and%0Avariable%20action%20durations.%20Recent%20findings%20suggest%20that%20TAD%20performance%20is%0Adependent%20on%20the%20structural%20design%20of%20transformers%20rather%20than%20on%20the%0Aself-attention%20mechanism.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20refined%0Afeature%20extraction%20process%20through%20lightweight%2C%20yet%20effective%20operations.%0AFirst%2C%20we%20employ%20a%20local%20branch%20that%20employs%20parallel%20convolutions%20with%20varying%0Awindow%20sizes%20to%20capture%20both%20fine-grained%20and%20coarse-grained%20temporal%20features.%0AThis%20branch%20incorporates%20a%20gating%20mechanism%20to%20select%20the%20most%20relevant%0Afeatures.%20Second%2C%20we%20introduce%20a%20context%20branch%20that%20uses%20boundary%20frames%20as%0Akey-value%20pairs%20to%20analyze%20their%20relationship%20with%20the%20central%20frame%20through%0Across-attention.%20The%20proposed%20method%20captures%20temporal%20dependencies%20and%0Aimproves%20contextual%20understanding.%20Evaluations%20of%20the%20gating%20mechanism%20and%0Acontext%20branch%20on%20challenging%20datasets%20%28THUMOS14%20and%20EPIC-KITCHEN%20100%29%20show%20a%0Aconsistent%20improvement%20over%20the%20baseline%20and%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04205v1&entry.124074799=Read"},
{"title": "HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical\n  MLLM Prompts", "author": "Xinyu Liu and Yingqing He and Lanqing Guo and Xiang Li and Bu Jin and Peng Li and Yan Li and Chi-Min Chan and Qifeng Chen and Wei Xue and Wenhan Luo and Qingfeng Liu and Yike Guo", "abstract": "  The potential for higher-resolution image generation using pretrained\ndiffusion models is immense, yet these models often struggle with issues of\nobject repetition and structural artifacts especially when scaling to 4K\nresolution and higher. We figure out that the problem is caused by that, a\nsingle prompt for the generation of multiple scales provides insufficient\nefficacy. In response, we propose HiPrompt, a new tuning-free solution that\ntackles the above problems by introducing hierarchical prompts. The\nhierarchical prompts offer both global and local guidance. Specifically, the\nglobal guidance comes from the user input that describes the overall content,\nwhile the local guidance utilizes patch-wise descriptions from MLLMs to\nelaborately guide the regional structure and texture generation. Furthermore,\nduring the inverse denoising process, the generated noise is decomposed into\nlow- and high-frequency spatial components. These components are conditioned on\nmultiple prompt levels, including detailed patch-wise descriptions and broader\nimage-level prompts, facilitating prompt-guided denoising under hierarchical\nsemantic guidance. It further allows the generation to focus more on local\nspatial regions and ensures the generated images maintain coherent local and\nglobal semantics, structures, and textures with high definition. Extensive\nexperiments demonstrate that HiPrompt outperforms state-of-the-art works in\nhigher-resolution image generation, significantly reducing object repetition\nand enhancing structural quality.\n", "link": "http://arxiv.org/abs/2409.02919v2", "date": "2024-09-06", "relevancy": 2.314, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5944}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5747}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiPrompt%3A%20Tuning-free%20Higher-Resolution%20Generation%20with%20Hierarchical%0A%20%20MLLM%20Prompts&body=Title%3A%20HiPrompt%3A%20Tuning-free%20Higher-Resolution%20Generation%20with%20Hierarchical%0A%20%20MLLM%20Prompts%0AAuthor%3A%20Xinyu%20Liu%20and%20Yingqing%20He%20and%20Lanqing%20Guo%20and%20Xiang%20Li%20and%20Bu%20Jin%20and%20Peng%20Li%20and%20Yan%20Li%20and%20Chi-Min%20Chan%20and%20Qifeng%20Chen%20and%20Wei%20Xue%20and%20Wenhan%20Luo%20and%20Qingfeng%20Liu%20and%20Yike%20Guo%0AAbstract%3A%20%20%20The%20potential%20for%20higher-resolution%20image%20generation%20using%20pretrained%0Adiffusion%20models%20is%20immense%2C%20yet%20these%20models%20often%20struggle%20with%20issues%20of%0Aobject%20repetition%20and%20structural%20artifacts%20especially%20when%20scaling%20to%204K%0Aresolution%20and%20higher.%20We%20figure%20out%20that%20the%20problem%20is%20caused%20by%20that%2C%20a%0Asingle%20prompt%20for%20the%20generation%20of%20multiple%20scales%20provides%20insufficient%0Aefficacy.%20In%20response%2C%20we%20propose%20HiPrompt%2C%20a%20new%20tuning-free%20solution%20that%0Atackles%20the%20above%20problems%20by%20introducing%20hierarchical%20prompts.%20The%0Ahierarchical%20prompts%20offer%20both%20global%20and%20local%20guidance.%20Specifically%2C%20the%0Aglobal%20guidance%20comes%20from%20the%20user%20input%20that%20describes%20the%20overall%20content%2C%0Awhile%20the%20local%20guidance%20utilizes%20patch-wise%20descriptions%20from%20MLLMs%20to%0Aelaborately%20guide%20the%20regional%20structure%20and%20texture%20generation.%20Furthermore%2C%0Aduring%20the%20inverse%20denoising%20process%2C%20the%20generated%20noise%20is%20decomposed%20into%0Alow-%20and%20high-frequency%20spatial%20components.%20These%20components%20are%20conditioned%20on%0Amultiple%20prompt%20levels%2C%20including%20detailed%20patch-wise%20descriptions%20and%20broader%0Aimage-level%20prompts%2C%20facilitating%20prompt-guided%20denoising%20under%20hierarchical%0Asemantic%20guidance.%20It%20further%20allows%20the%20generation%20to%20focus%20more%20on%20local%0Aspatial%20regions%20and%20ensures%20the%20generated%20images%20maintain%20coherent%20local%20and%0Aglobal%20semantics%2C%20structures%2C%20and%20textures%20with%20high%20definition.%20Extensive%0Aexperiments%20demonstrate%20that%20HiPrompt%20outperforms%20state-of-the-art%20works%20in%0Ahigher-resolution%20image%20generation%2C%20significantly%20reducing%20object%20repetition%0Aand%20enhancing%20structural%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02919v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiPrompt%253A%2520Tuning-free%2520Higher-Resolution%2520Generation%2520with%2520Hierarchical%250A%2520%2520MLLM%2520Prompts%26entry.906535625%3DXinyu%2520Liu%2520and%2520Yingqing%2520He%2520and%2520Lanqing%2520Guo%2520and%2520Xiang%2520Li%2520and%2520Bu%2520Jin%2520and%2520Peng%2520Li%2520and%2520Yan%2520Li%2520and%2520Chi-Min%2520Chan%2520and%2520Qifeng%2520Chen%2520and%2520Wei%2520Xue%2520and%2520Wenhan%2520Luo%2520and%2520Qingfeng%2520Liu%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520The%2520potential%2520for%2520higher-resolution%2520image%2520generation%2520using%2520pretrained%250Adiffusion%2520models%2520is%2520immense%252C%2520yet%2520these%2520models%2520often%2520struggle%2520with%2520issues%2520of%250Aobject%2520repetition%2520and%2520structural%2520artifacts%2520especially%2520when%2520scaling%2520to%25204K%250Aresolution%2520and%2520higher.%2520We%2520figure%2520out%2520that%2520the%2520problem%2520is%2520caused%2520by%2520that%252C%2520a%250Asingle%2520prompt%2520for%2520the%2520generation%2520of%2520multiple%2520scales%2520provides%2520insufficient%250Aefficacy.%2520In%2520response%252C%2520we%2520propose%2520HiPrompt%252C%2520a%2520new%2520tuning-free%2520solution%2520that%250Atackles%2520the%2520above%2520problems%2520by%2520introducing%2520hierarchical%2520prompts.%2520The%250Ahierarchical%2520prompts%2520offer%2520both%2520global%2520and%2520local%2520guidance.%2520Specifically%252C%2520the%250Aglobal%2520guidance%2520comes%2520from%2520the%2520user%2520input%2520that%2520describes%2520the%2520overall%2520content%252C%250Awhile%2520the%2520local%2520guidance%2520utilizes%2520patch-wise%2520descriptions%2520from%2520MLLMs%2520to%250Aelaborately%2520guide%2520the%2520regional%2520structure%2520and%2520texture%2520generation.%2520Furthermore%252C%250Aduring%2520the%2520inverse%2520denoising%2520process%252C%2520the%2520generated%2520noise%2520is%2520decomposed%2520into%250Alow-%2520and%2520high-frequency%2520spatial%2520components.%2520These%2520components%2520are%2520conditioned%2520on%250Amultiple%2520prompt%2520levels%252C%2520including%2520detailed%2520patch-wise%2520descriptions%2520and%2520broader%250Aimage-level%2520prompts%252C%2520facilitating%2520prompt-guided%2520denoising%2520under%2520hierarchical%250Asemantic%2520guidance.%2520It%2520further%2520allows%2520the%2520generation%2520to%2520focus%2520more%2520on%2520local%250Aspatial%2520regions%2520and%2520ensures%2520the%2520generated%2520images%2520maintain%2520coherent%2520local%2520and%250Aglobal%2520semantics%252C%2520structures%252C%2520and%2520textures%2520with%2520high%2520definition.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520HiPrompt%2520outperforms%2520state-of-the-art%2520works%2520in%250Ahigher-resolution%2520image%2520generation%252C%2520significantly%2520reducing%2520object%2520repetition%250Aand%2520enhancing%2520structural%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02919v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiPrompt%3A%20Tuning-free%20Higher-Resolution%20Generation%20with%20Hierarchical%0A%20%20MLLM%20Prompts&entry.906535625=Xinyu%20Liu%20and%20Yingqing%20He%20and%20Lanqing%20Guo%20and%20Xiang%20Li%20and%20Bu%20Jin%20and%20Peng%20Li%20and%20Yan%20Li%20and%20Chi-Min%20Chan%20and%20Qifeng%20Chen%20and%20Wei%20Xue%20and%20Wenhan%20Luo%20and%20Qingfeng%20Liu%20and%20Yike%20Guo&entry.1292438233=%20%20The%20potential%20for%20higher-resolution%20image%20generation%20using%20pretrained%0Adiffusion%20models%20is%20immense%2C%20yet%20these%20models%20often%20struggle%20with%20issues%20of%0Aobject%20repetition%20and%20structural%20artifacts%20especially%20when%20scaling%20to%204K%0Aresolution%20and%20higher.%20We%20figure%20out%20that%20the%20problem%20is%20caused%20by%20that%2C%20a%0Asingle%20prompt%20for%20the%20generation%20of%20multiple%20scales%20provides%20insufficient%0Aefficacy.%20In%20response%2C%20we%20propose%20HiPrompt%2C%20a%20new%20tuning-free%20solution%20that%0Atackles%20the%20above%20problems%20by%20introducing%20hierarchical%20prompts.%20The%0Ahierarchical%20prompts%20offer%20both%20global%20and%20local%20guidance.%20Specifically%2C%20the%0Aglobal%20guidance%20comes%20from%20the%20user%20input%20that%20describes%20the%20overall%20content%2C%0Awhile%20the%20local%20guidance%20utilizes%20patch-wise%20descriptions%20from%20MLLMs%20to%0Aelaborately%20guide%20the%20regional%20structure%20and%20texture%20generation.%20Furthermore%2C%0Aduring%20the%20inverse%20denoising%20process%2C%20the%20generated%20noise%20is%20decomposed%20into%0Alow-%20and%20high-frequency%20spatial%20components.%20These%20components%20are%20conditioned%20on%0Amultiple%20prompt%20levels%2C%20including%20detailed%20patch-wise%20descriptions%20and%20broader%0Aimage-level%20prompts%2C%20facilitating%20prompt-guided%20denoising%20under%20hierarchical%0Asemantic%20guidance.%20It%20further%20allows%20the%20generation%20to%20focus%20more%20on%20local%0Aspatial%20regions%20and%20ensures%20the%20generated%20images%20maintain%20coherent%20local%20and%0Aglobal%20semantics%2C%20structures%2C%20and%20textures%20with%20high%20definition.%20Extensive%0Aexperiments%20demonstrate%20that%20HiPrompt%20outperforms%20state-of-the-art%20works%20in%0Ahigher-resolution%20image%20generation%2C%20significantly%20reducing%20object%20repetition%0Aand%20enhancing%20structural%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02919v2&entry.124074799=Read"},
{"title": "Hyp2Nav: Hyperbolic Planning and Curiosity for Crowd Navigation", "author": "Guido Maria D'Amely di Melendugno and Alessandro Flaborea and Pascal Mettes and Fabio Galasso", "abstract": "  Autonomous robots are increasingly becoming a strong fixture in social\nenvironments. Effective crowd navigation requires not only safe yet fast\nplanning, but should also enable interpretability and computational efficiency\nfor working in real-time on embedded devices. In this work, we advocate for\nhyperbolic learning to enable crowd navigation and we introduce Hyp2Nav.\nDifferent from conventional reinforcement learning-based crowd navigation\nmethods, Hyp2Nav leverages the intrinsic properties of hyperbolic geometry to\nbetter encode the hierarchical nature of decision-making processes in\nnavigation tasks. We propose a hyperbolic policy model and a hyperbolic\ncuriosity module that results in effective social navigation, best success\nrates, and returns across multiple simulation settings, using up to 6 times\nfewer parameters than competitor state-of-the-art models. With our approach, it\nbecomes even possible to obtain policies that work in 2-dimensional embedding\nspaces, opening up new possibilities for low-resource crowd navigation and\nmodel interpretability. Insightfully, the internal hyperbolic representation of\nHyp2Nav correlates with how much attention the robot pays to the surrounding\ncrowds, e.g. due to multiple people occluding its pathway or to a few of them\nshowing colliding plans, rather than to its own planned route. The code is\navailable at https://github.com/GDam90/hyp2nav.\n", "link": "http://arxiv.org/abs/2407.13567v3", "date": "2024-09-06", "relevancy": 2.3104, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5902}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5789}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyp2Nav%3A%20Hyperbolic%20Planning%20and%20Curiosity%20for%20Crowd%20Navigation&body=Title%3A%20Hyp2Nav%3A%20Hyperbolic%20Planning%20and%20Curiosity%20for%20Crowd%20Navigation%0AAuthor%3A%20Guido%20Maria%20D%27Amely%20di%20Melendugno%20and%20Alessandro%20Flaborea%20and%20Pascal%20Mettes%20and%20Fabio%20Galasso%0AAbstract%3A%20%20%20Autonomous%20robots%20are%20increasingly%20becoming%20a%20strong%20fixture%20in%20social%0Aenvironments.%20Effective%20crowd%20navigation%20requires%20not%20only%20safe%20yet%20fast%0Aplanning%2C%20but%20should%20also%20enable%20interpretability%20and%20computational%20efficiency%0Afor%20working%20in%20real-time%20on%20embedded%20devices.%20In%20this%20work%2C%20we%20advocate%20for%0Ahyperbolic%20learning%20to%20enable%20crowd%20navigation%20and%20we%20introduce%20Hyp2Nav.%0ADifferent%20from%20conventional%20reinforcement%20learning-based%20crowd%20navigation%0Amethods%2C%20Hyp2Nav%20leverages%20the%20intrinsic%20properties%20of%20hyperbolic%20geometry%20to%0Abetter%20encode%20the%20hierarchical%20nature%20of%20decision-making%20processes%20in%0Anavigation%20tasks.%20We%20propose%20a%20hyperbolic%20policy%20model%20and%20a%20hyperbolic%0Acuriosity%20module%20that%20results%20in%20effective%20social%20navigation%2C%20best%20success%0Arates%2C%20and%20returns%20across%20multiple%20simulation%20settings%2C%20using%20up%20to%206%20times%0Afewer%20parameters%20than%20competitor%20state-of-the-art%20models.%20With%20our%20approach%2C%20it%0Abecomes%20even%20possible%20to%20obtain%20policies%20that%20work%20in%202-dimensional%20embedding%0Aspaces%2C%20opening%20up%20new%20possibilities%20for%20low-resource%20crowd%20navigation%20and%0Amodel%20interpretability.%20Insightfully%2C%20the%20internal%20hyperbolic%20representation%20of%0AHyp2Nav%20correlates%20with%20how%20much%20attention%20the%20robot%20pays%20to%20the%20surrounding%0Acrowds%2C%20e.g.%20due%20to%20multiple%20people%20occluding%20its%20pathway%20or%20to%20a%20few%20of%20them%0Ashowing%20colliding%20plans%2C%20rather%20than%20to%20its%20own%20planned%20route.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/GDam90/hyp2nav.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13567v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyp2Nav%253A%2520Hyperbolic%2520Planning%2520and%2520Curiosity%2520for%2520Crowd%2520Navigation%26entry.906535625%3DGuido%2520Maria%2520D%2527Amely%2520di%2520Melendugno%2520and%2520Alessandro%2520Flaborea%2520and%2520Pascal%2520Mettes%2520and%2520Fabio%2520Galasso%26entry.1292438233%3D%2520%2520Autonomous%2520robots%2520are%2520increasingly%2520becoming%2520a%2520strong%2520fixture%2520in%2520social%250Aenvironments.%2520Effective%2520crowd%2520navigation%2520requires%2520not%2520only%2520safe%2520yet%2520fast%250Aplanning%252C%2520but%2520should%2520also%2520enable%2520interpretability%2520and%2520computational%2520efficiency%250Afor%2520working%2520in%2520real-time%2520on%2520embedded%2520devices.%2520In%2520this%2520work%252C%2520we%2520advocate%2520for%250Ahyperbolic%2520learning%2520to%2520enable%2520crowd%2520navigation%2520and%2520we%2520introduce%2520Hyp2Nav.%250ADifferent%2520from%2520conventional%2520reinforcement%2520learning-based%2520crowd%2520navigation%250Amethods%252C%2520Hyp2Nav%2520leverages%2520the%2520intrinsic%2520properties%2520of%2520hyperbolic%2520geometry%2520to%250Abetter%2520encode%2520the%2520hierarchical%2520nature%2520of%2520decision-making%2520processes%2520in%250Anavigation%2520tasks.%2520We%2520propose%2520a%2520hyperbolic%2520policy%2520model%2520and%2520a%2520hyperbolic%250Acuriosity%2520module%2520that%2520results%2520in%2520effective%2520social%2520navigation%252C%2520best%2520success%250Arates%252C%2520and%2520returns%2520across%2520multiple%2520simulation%2520settings%252C%2520using%2520up%2520to%25206%2520times%250Afewer%2520parameters%2520than%2520competitor%2520state-of-the-art%2520models.%2520With%2520our%2520approach%252C%2520it%250Abecomes%2520even%2520possible%2520to%2520obtain%2520policies%2520that%2520work%2520in%25202-dimensional%2520embedding%250Aspaces%252C%2520opening%2520up%2520new%2520possibilities%2520for%2520low-resource%2520crowd%2520navigation%2520and%250Amodel%2520interpretability.%2520Insightfully%252C%2520the%2520internal%2520hyperbolic%2520representation%2520of%250AHyp2Nav%2520correlates%2520with%2520how%2520much%2520attention%2520the%2520robot%2520pays%2520to%2520the%2520surrounding%250Acrowds%252C%2520e.g.%2520due%2520to%2520multiple%2520people%2520occluding%2520its%2520pathway%2520or%2520to%2520a%2520few%2520of%2520them%250Ashowing%2520colliding%2520plans%252C%2520rather%2520than%2520to%2520its%2520own%2520planned%2520route.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/GDam90/hyp2nav.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13567v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyp2Nav%3A%20Hyperbolic%20Planning%20and%20Curiosity%20for%20Crowd%20Navigation&entry.906535625=Guido%20Maria%20D%27Amely%20di%20Melendugno%20and%20Alessandro%20Flaborea%20and%20Pascal%20Mettes%20and%20Fabio%20Galasso&entry.1292438233=%20%20Autonomous%20robots%20are%20increasingly%20becoming%20a%20strong%20fixture%20in%20social%0Aenvironments.%20Effective%20crowd%20navigation%20requires%20not%20only%20safe%20yet%20fast%0Aplanning%2C%20but%20should%20also%20enable%20interpretability%20and%20computational%20efficiency%0Afor%20working%20in%20real-time%20on%20embedded%20devices.%20In%20this%20work%2C%20we%20advocate%20for%0Ahyperbolic%20learning%20to%20enable%20crowd%20navigation%20and%20we%20introduce%20Hyp2Nav.%0ADifferent%20from%20conventional%20reinforcement%20learning-based%20crowd%20navigation%0Amethods%2C%20Hyp2Nav%20leverages%20the%20intrinsic%20properties%20of%20hyperbolic%20geometry%20to%0Abetter%20encode%20the%20hierarchical%20nature%20of%20decision-making%20processes%20in%0Anavigation%20tasks.%20We%20propose%20a%20hyperbolic%20policy%20model%20and%20a%20hyperbolic%0Acuriosity%20module%20that%20results%20in%20effective%20social%20navigation%2C%20best%20success%0Arates%2C%20and%20returns%20across%20multiple%20simulation%20settings%2C%20using%20up%20to%206%20times%0Afewer%20parameters%20than%20competitor%20state-of-the-art%20models.%20With%20our%20approach%2C%20it%0Abecomes%20even%20possible%20to%20obtain%20policies%20that%20work%20in%202-dimensional%20embedding%0Aspaces%2C%20opening%20up%20new%20possibilities%20for%20low-resource%20crowd%20navigation%20and%0Amodel%20interpretability.%20Insightfully%2C%20the%20internal%20hyperbolic%20representation%20of%0AHyp2Nav%20correlates%20with%20how%20much%20attention%20the%20robot%20pays%20to%20the%20surrounding%0Acrowds%2C%20e.g.%20due%20to%20multiple%20people%20occluding%20its%20pathway%20or%20to%20a%20few%20of%20them%0Ashowing%20colliding%20plans%2C%20rather%20than%20to%20its%20own%20planned%20route.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/GDam90/hyp2nav.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13567v3&entry.124074799=Read"},
{"title": "Train Till You Drop: Towards Stable and Robust Source-free Unsupervised\n  3D Domain Adaptation", "author": "Bj\u00f6rn Michele and Alexandre Boulch and Tuan-Hung Vu and Gilles Puy and Renaud Marlet and Nicolas Courty", "abstract": "  We tackle the challenging problem of source-free unsupervised domain\nadaptation (SFUDA) for 3D semantic segmentation. It amounts to performing\ndomain adaptation on an unlabeled target domain without any access to source\ndata; the available information is a model trained to achieve good performance\non the source domain. A common issue with existing SFUDA approaches is that\nperformance degrades after some training time, which is a by product of an\nunder-constrained and ill-posed problem. We discuss two strategies to alleviate\nthis issue. First, we propose a sensible way to regularize the learning\nproblem. Second, we introduce a novel criterion based on agreement with a\nreference model. It is used (1) to stop the training when appropriate and (2)\nas validator to select hyperparameters without any knowledge on the target\ndomain. Our contributions are easy to implement and readily amenable for all\nSFUDA methods, ensuring stable improvements over all baselines. We validate our\nfindings on various 3D lidar settings, achieving state-of-the-art performance.\nThe project repository (with code) is: github.com/valeoai/TTYD.\n", "link": "http://arxiv.org/abs/2409.04409v1", "date": "2024-09-06", "relevancy": 2.3014, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5893}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5806}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Train%20Till%20You%20Drop%3A%20Towards%20Stable%20and%20Robust%20Source-free%20Unsupervised%0A%20%203D%20Domain%20Adaptation&body=Title%3A%20Train%20Till%20You%20Drop%3A%20Towards%20Stable%20and%20Robust%20Source-free%20Unsupervised%0A%20%203D%20Domain%20Adaptation%0AAuthor%3A%20Bj%C3%B6rn%20Michele%20and%20Alexandre%20Boulch%20and%20Tuan-Hung%20Vu%20and%20Gilles%20Puy%20and%20Renaud%20Marlet%20and%20Nicolas%20Courty%0AAbstract%3A%20%20%20We%20tackle%20the%20challenging%20problem%20of%20source-free%20unsupervised%20domain%0Aadaptation%20%28SFUDA%29%20for%203D%20semantic%20segmentation.%20It%20amounts%20to%20performing%0Adomain%20adaptation%20on%20an%20unlabeled%20target%20domain%20without%20any%20access%20to%20source%0Adata%3B%20the%20available%20information%20is%20a%20model%20trained%20to%20achieve%20good%20performance%0Aon%20the%20source%20domain.%20A%20common%20issue%20with%20existing%20SFUDA%20approaches%20is%20that%0Aperformance%20degrades%20after%20some%20training%20time%2C%20which%20is%20a%20by%20product%20of%20an%0Aunder-constrained%20and%20ill-posed%20problem.%20We%20discuss%20two%20strategies%20to%20alleviate%0Athis%20issue.%20First%2C%20we%20propose%20a%20sensible%20way%20to%20regularize%20the%20learning%0Aproblem.%20Second%2C%20we%20introduce%20a%20novel%20criterion%20based%20on%20agreement%20with%20a%0Areference%20model.%20It%20is%20used%20%281%29%20to%20stop%20the%20training%20when%20appropriate%20and%20%282%29%0Aas%20validator%20to%20select%20hyperparameters%20without%20any%20knowledge%20on%20the%20target%0Adomain.%20Our%20contributions%20are%20easy%20to%20implement%20and%20readily%20amenable%20for%20all%0ASFUDA%20methods%2C%20ensuring%20stable%20improvements%20over%20all%20baselines.%20We%20validate%20our%0Afindings%20on%20various%203D%20lidar%20settings%2C%20achieving%20state-of-the-art%20performance.%0AThe%20project%20repository%20%28with%20code%29%20is%3A%20github.com/valeoai/TTYD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrain%2520Till%2520You%2520Drop%253A%2520Towards%2520Stable%2520and%2520Robust%2520Source-free%2520Unsupervised%250A%2520%25203D%2520Domain%2520Adaptation%26entry.906535625%3DBj%25C3%25B6rn%2520Michele%2520and%2520Alexandre%2520Boulch%2520and%2520Tuan-Hung%2520Vu%2520and%2520Gilles%2520Puy%2520and%2520Renaud%2520Marlet%2520and%2520Nicolas%2520Courty%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520challenging%2520problem%2520of%2520source-free%2520unsupervised%2520domain%250Aadaptation%2520%2528SFUDA%2529%2520for%25203D%2520semantic%2520segmentation.%2520It%2520amounts%2520to%2520performing%250Adomain%2520adaptation%2520on%2520an%2520unlabeled%2520target%2520domain%2520without%2520any%2520access%2520to%2520source%250Adata%253B%2520the%2520available%2520information%2520is%2520a%2520model%2520trained%2520to%2520achieve%2520good%2520performance%250Aon%2520the%2520source%2520domain.%2520A%2520common%2520issue%2520with%2520existing%2520SFUDA%2520approaches%2520is%2520that%250Aperformance%2520degrades%2520after%2520some%2520training%2520time%252C%2520which%2520is%2520a%2520by%2520product%2520of%2520an%250Aunder-constrained%2520and%2520ill-posed%2520problem.%2520We%2520discuss%2520two%2520strategies%2520to%2520alleviate%250Athis%2520issue.%2520First%252C%2520we%2520propose%2520a%2520sensible%2520way%2520to%2520regularize%2520the%2520learning%250Aproblem.%2520Second%252C%2520we%2520introduce%2520a%2520novel%2520criterion%2520based%2520on%2520agreement%2520with%2520a%250Areference%2520model.%2520It%2520is%2520used%2520%25281%2529%2520to%2520stop%2520the%2520training%2520when%2520appropriate%2520and%2520%25282%2529%250Aas%2520validator%2520to%2520select%2520hyperparameters%2520without%2520any%2520knowledge%2520on%2520the%2520target%250Adomain.%2520Our%2520contributions%2520are%2520easy%2520to%2520implement%2520and%2520readily%2520amenable%2520for%2520all%250ASFUDA%2520methods%252C%2520ensuring%2520stable%2520improvements%2520over%2520all%2520baselines.%2520We%2520validate%2520our%250Afindings%2520on%2520various%25203D%2520lidar%2520settings%252C%2520achieving%2520state-of-the-art%2520performance.%250AThe%2520project%2520repository%2520%2528with%2520code%2529%2520is%253A%2520github.com/valeoai/TTYD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Train%20Till%20You%20Drop%3A%20Towards%20Stable%20and%20Robust%20Source-free%20Unsupervised%0A%20%203D%20Domain%20Adaptation&entry.906535625=Bj%C3%B6rn%20Michele%20and%20Alexandre%20Boulch%20and%20Tuan-Hung%20Vu%20and%20Gilles%20Puy%20and%20Renaud%20Marlet%20and%20Nicolas%20Courty&entry.1292438233=%20%20We%20tackle%20the%20challenging%20problem%20of%20source-free%20unsupervised%20domain%0Aadaptation%20%28SFUDA%29%20for%203D%20semantic%20segmentation.%20It%20amounts%20to%20performing%0Adomain%20adaptation%20on%20an%20unlabeled%20target%20domain%20without%20any%20access%20to%20source%0Adata%3B%20the%20available%20information%20is%20a%20model%20trained%20to%20achieve%20good%20performance%0Aon%20the%20source%20domain.%20A%20common%20issue%20with%20existing%20SFUDA%20approaches%20is%20that%0Aperformance%20degrades%20after%20some%20training%20time%2C%20which%20is%20a%20by%20product%20of%20an%0Aunder-constrained%20and%20ill-posed%20problem.%20We%20discuss%20two%20strategies%20to%20alleviate%0Athis%20issue.%20First%2C%20we%20propose%20a%20sensible%20way%20to%20regularize%20the%20learning%0Aproblem.%20Second%2C%20we%20introduce%20a%20novel%20criterion%20based%20on%20agreement%20with%20a%0Areference%20model.%20It%20is%20used%20%281%29%20to%20stop%20the%20training%20when%20appropriate%20and%20%282%29%0Aas%20validator%20to%20select%20hyperparameters%20without%20any%20knowledge%20on%20the%20target%0Adomain.%20Our%20contributions%20are%20easy%20to%20implement%20and%20readily%20amenable%20for%20all%0ASFUDA%20methods%2C%20ensuring%20stable%20improvements%20over%20all%20baselines.%20We%20validate%20our%0Afindings%20on%20various%203D%20lidar%20settings%2C%20achieving%20state-of-the-art%20performance.%0AThe%20project%20repository%20%28with%20code%29%20is%3A%20github.com/valeoai/TTYD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04409v1&entry.124074799=Read"},
{"title": "EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and\n  Intermittent Observations Everywhere", "author": "Jiaxi Jiang and Paul Streli and Manuel Meier and Christian Holz", "abstract": "  Full-body egocentric pose estimation from head and hand poses alone has\nbecome an active area of research to power articulate avatar representations on\nheadset-based platforms. However, existing methods over-rely on the indoor\nmotion-capture spaces in which datasets were recorded, while simultaneously\nassuming continuous joint motion capture and uniform body dimensions. We\npropose EgoPoser to overcome these limitations with four main contributions. 1)\nEgoPoser robustly models body pose from intermittent hand position and\norientation tracking only when inside a headset's field of view. 2) We rethink\ninput representations for headset-based ego-pose estimation and introduce a\nnovel global motion decomposition method that predicts full-body pose\nindependent of global positions. 3) We enhance pose estimation by capturing\nlonger motion time series through an efficient SlowFast module design that\nmaintains computational efficiency. 4) EgoPoser generalizes across various body\nshapes for different users. We experimentally evaluate our method and show that\nit outperforms state-of-the-art methods both qualitatively and quantitatively\nwhile maintaining a high inference speed of over 600fps. EgoPoser establishes a\nrobust baseline for future work where full-body pose estimation no longer needs\nto rely on outside-in capture and can scale to large-scale and unseen\nenvironments.\n", "link": "http://arxiv.org/abs/2308.06493v3", "date": "2024-09-06", "relevancy": 2.2531, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5728}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5696}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoPoser%3A%20Robust%20Real-Time%20Egocentric%20Pose%20Estimation%20from%20Sparse%20and%0A%20%20Intermittent%20Observations%20Everywhere&body=Title%3A%20EgoPoser%3A%20Robust%20Real-Time%20Egocentric%20Pose%20Estimation%20from%20Sparse%20and%0A%20%20Intermittent%20Observations%20Everywhere%0AAuthor%3A%20Jiaxi%20Jiang%20and%20Paul%20Streli%20and%20Manuel%20Meier%20and%20Christian%20Holz%0AAbstract%3A%20%20%20Full-body%20egocentric%20pose%20estimation%20from%20head%20and%20hand%20poses%20alone%20has%0Abecome%20an%20active%20area%20of%20research%20to%20power%20articulate%20avatar%20representations%20on%0Aheadset-based%20platforms.%20However%2C%20existing%20methods%20over-rely%20on%20the%20indoor%0Amotion-capture%20spaces%20in%20which%20datasets%20were%20recorded%2C%20while%20simultaneously%0Aassuming%20continuous%20joint%20motion%20capture%20and%20uniform%20body%20dimensions.%20We%0Apropose%20EgoPoser%20to%20overcome%20these%20limitations%20with%20four%20main%20contributions.%201%29%0AEgoPoser%20robustly%20models%20body%20pose%20from%20intermittent%20hand%20position%20and%0Aorientation%20tracking%20only%20when%20inside%20a%20headset%27s%20field%20of%20view.%202%29%20We%20rethink%0Ainput%20representations%20for%20headset-based%20ego-pose%20estimation%20and%20introduce%20a%0Anovel%20global%20motion%20decomposition%20method%20that%20predicts%20full-body%20pose%0Aindependent%20of%20global%20positions.%203%29%20We%20enhance%20pose%20estimation%20by%20capturing%0Alonger%20motion%20time%20series%20through%20an%20efficient%20SlowFast%20module%20design%20that%0Amaintains%20computational%20efficiency.%204%29%20EgoPoser%20generalizes%20across%20various%20body%0Ashapes%20for%20different%20users.%20We%20experimentally%20evaluate%20our%20method%20and%20show%20that%0Ait%20outperforms%20state-of-the-art%20methods%20both%20qualitatively%20and%20quantitatively%0Awhile%20maintaining%20a%20high%20inference%20speed%20of%20over%20600fps.%20EgoPoser%20establishes%20a%0Arobust%20baseline%20for%20future%20work%20where%20full-body%20pose%20estimation%20no%20longer%20needs%0Ato%20rely%20on%20outside-in%20capture%20and%20can%20scale%20to%20large-scale%20and%20unseen%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.06493v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoPoser%253A%2520Robust%2520Real-Time%2520Egocentric%2520Pose%2520Estimation%2520from%2520Sparse%2520and%250A%2520%2520Intermittent%2520Observations%2520Everywhere%26entry.906535625%3DJiaxi%2520Jiang%2520and%2520Paul%2520Streli%2520and%2520Manuel%2520Meier%2520and%2520Christian%2520Holz%26entry.1292438233%3D%2520%2520Full-body%2520egocentric%2520pose%2520estimation%2520from%2520head%2520and%2520hand%2520poses%2520alone%2520has%250Abecome%2520an%2520active%2520area%2520of%2520research%2520to%2520power%2520articulate%2520avatar%2520representations%2520on%250Aheadset-based%2520platforms.%2520However%252C%2520existing%2520methods%2520over-rely%2520on%2520the%2520indoor%250Amotion-capture%2520spaces%2520in%2520which%2520datasets%2520were%2520recorded%252C%2520while%2520simultaneously%250Aassuming%2520continuous%2520joint%2520motion%2520capture%2520and%2520uniform%2520body%2520dimensions.%2520We%250Apropose%2520EgoPoser%2520to%2520overcome%2520these%2520limitations%2520with%2520four%2520main%2520contributions.%25201%2529%250AEgoPoser%2520robustly%2520models%2520body%2520pose%2520from%2520intermittent%2520hand%2520position%2520and%250Aorientation%2520tracking%2520only%2520when%2520inside%2520a%2520headset%2527s%2520field%2520of%2520view.%25202%2529%2520We%2520rethink%250Ainput%2520representations%2520for%2520headset-based%2520ego-pose%2520estimation%2520and%2520introduce%2520a%250Anovel%2520global%2520motion%2520decomposition%2520method%2520that%2520predicts%2520full-body%2520pose%250Aindependent%2520of%2520global%2520positions.%25203%2529%2520We%2520enhance%2520pose%2520estimation%2520by%2520capturing%250Alonger%2520motion%2520time%2520series%2520through%2520an%2520efficient%2520SlowFast%2520module%2520design%2520that%250Amaintains%2520computational%2520efficiency.%25204%2529%2520EgoPoser%2520generalizes%2520across%2520various%2520body%250Ashapes%2520for%2520different%2520users.%2520We%2520experimentally%2520evaluate%2520our%2520method%2520and%2520show%2520that%250Ait%2520outperforms%2520state-of-the-art%2520methods%2520both%2520qualitatively%2520and%2520quantitatively%250Awhile%2520maintaining%2520a%2520high%2520inference%2520speed%2520of%2520over%2520600fps.%2520EgoPoser%2520establishes%2520a%250Arobust%2520baseline%2520for%2520future%2520work%2520where%2520full-body%2520pose%2520estimation%2520no%2520longer%2520needs%250Ato%2520rely%2520on%2520outside-in%2520capture%2520and%2520can%2520scale%2520to%2520large-scale%2520and%2520unseen%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.06493v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoPoser%3A%20Robust%20Real-Time%20Egocentric%20Pose%20Estimation%20from%20Sparse%20and%0A%20%20Intermittent%20Observations%20Everywhere&entry.906535625=Jiaxi%20Jiang%20and%20Paul%20Streli%20and%20Manuel%20Meier%20and%20Christian%20Holz&entry.1292438233=%20%20Full-body%20egocentric%20pose%20estimation%20from%20head%20and%20hand%20poses%20alone%20has%0Abecome%20an%20active%20area%20of%20research%20to%20power%20articulate%20avatar%20representations%20on%0Aheadset-based%20platforms.%20However%2C%20existing%20methods%20over-rely%20on%20the%20indoor%0Amotion-capture%20spaces%20in%20which%20datasets%20were%20recorded%2C%20while%20simultaneously%0Aassuming%20continuous%20joint%20motion%20capture%20and%20uniform%20body%20dimensions.%20We%0Apropose%20EgoPoser%20to%20overcome%20these%20limitations%20with%20four%20main%20contributions.%201%29%0AEgoPoser%20robustly%20models%20body%20pose%20from%20intermittent%20hand%20position%20and%0Aorientation%20tracking%20only%20when%20inside%20a%20headset%27s%20field%20of%20view.%202%29%20We%20rethink%0Ainput%20representations%20for%20headset-based%20ego-pose%20estimation%20and%20introduce%20a%0Anovel%20global%20motion%20decomposition%20method%20that%20predicts%20full-body%20pose%0Aindependent%20of%20global%20positions.%203%29%20We%20enhance%20pose%20estimation%20by%20capturing%0Alonger%20motion%20time%20series%20through%20an%20efficient%20SlowFast%20module%20design%20that%0Amaintains%20computational%20efficiency.%204%29%20EgoPoser%20generalizes%20across%20various%20body%0Ashapes%20for%20different%20users.%20We%20experimentally%20evaluate%20our%20method%20and%20show%20that%0Ait%20outperforms%20state-of-the-art%20methods%20both%20qualitatively%20and%20quantitatively%0Awhile%20maintaining%20a%20high%20inference%20speed%20of%20over%20600fps.%20EgoPoser%20establishes%20a%0Arobust%20baseline%20for%20future%20work%20where%20full-body%20pose%20estimation%20no%20longer%20needs%0Ato%20rely%20on%20outside-in%20capture%20and%20can%20scale%20to%20large-scale%20and%20unseen%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.06493v3&entry.124074799=Read"},
{"title": "A Hypergraph-Based Machine Learning Ensemble Network Intrusion Detection\n  System", "author": "Zong-Zhi Lin and Thomas D. Pike and Mark M. Bailey and Nathaniel D. Bastian", "abstract": "  Network intrusion detection systems (NIDS) to detect malicious attacks\ncontinue to meet challenges. NIDS are often developed offline while they face\nauto-generated port scan infiltration attempts, resulting in a significant time\nlag from adversarial adaption to NIDS response. To address these challenges, we\nuse hypergraphs focused on internet protocol addresses and destination ports to\ncapture evolving patterns of port scan attacks. The derived set of\nhypergraph-based metrics are then used to train an ensemble machine learning\n(ML) based NIDS that allows for real-time adaption in monitoring and detecting\nport scanning activities, other types of attacks, and adversarial intrusions at\nhigh accuracy, precision and recall performances. This ML adapting NIDS was\ndeveloped through the combination of (1) intrusion examples, (2) NIDS update\nrules, (3) attack threshold choices to trigger NIDS retraining requests, and\n(4) a production environment with no prior knowledge of the nature of network\ntraffic. 40 scenarios were auto-generated to evaluate the ML ensemble NIDS\ncomprising three tree-based models. The resulting ML Ensemble NIDS was extended\nand evaluated with the CIC-IDS2017 dataset. Results show that under the model\nsettings of an Update-ALL-NIDS rule (specifically retrain and update all the\nthree models upon the same NIDS retraining request) the proposed ML ensemble\nNIDS evolved intelligently and produced the best results with nearly 100%\ndetection performance throughout the simulation.\n", "link": "http://arxiv.org/abs/2211.03933v3", "date": "2024-09-06", "relevancy": 2.2437, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4677}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4471}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hypergraph-Based%20Machine%20Learning%20Ensemble%20Network%20Intrusion%20Detection%0A%20%20System&body=Title%3A%20A%20Hypergraph-Based%20Machine%20Learning%20Ensemble%20Network%20Intrusion%20Detection%0A%20%20System%0AAuthor%3A%20Zong-Zhi%20Lin%20and%20Thomas%20D.%20Pike%20and%20Mark%20M.%20Bailey%20and%20Nathaniel%20D.%20Bastian%0AAbstract%3A%20%20%20Network%20intrusion%20detection%20systems%20%28NIDS%29%20to%20detect%20malicious%20attacks%0Acontinue%20to%20meet%20challenges.%20NIDS%20are%20often%20developed%20offline%20while%20they%20face%0Aauto-generated%20port%20scan%20infiltration%20attempts%2C%20resulting%20in%20a%20significant%20time%0Alag%20from%20adversarial%20adaption%20to%20NIDS%20response.%20To%20address%20these%20challenges%2C%20we%0Ause%20hypergraphs%20focused%20on%20internet%20protocol%20addresses%20and%20destination%20ports%20to%0Acapture%20evolving%20patterns%20of%20port%20scan%20attacks.%20The%20derived%20set%20of%0Ahypergraph-based%20metrics%20are%20then%20used%20to%20train%20an%20ensemble%20machine%20learning%0A%28ML%29%20based%20NIDS%20that%20allows%20for%20real-time%20adaption%20in%20monitoring%20and%20detecting%0Aport%20scanning%20activities%2C%20other%20types%20of%20attacks%2C%20and%20adversarial%20intrusions%20at%0Ahigh%20accuracy%2C%20precision%20and%20recall%20performances.%20This%20ML%20adapting%20NIDS%20was%0Adeveloped%20through%20the%20combination%20of%20%281%29%20intrusion%20examples%2C%20%282%29%20NIDS%20update%0Arules%2C%20%283%29%20attack%20threshold%20choices%20to%20trigger%20NIDS%20retraining%20requests%2C%20and%0A%284%29%20a%20production%20environment%20with%20no%20prior%20knowledge%20of%20the%20nature%20of%20network%0Atraffic.%2040%20scenarios%20were%20auto-generated%20to%20evaluate%20the%20ML%20ensemble%20NIDS%0Acomprising%20three%20tree-based%20models.%20The%20resulting%20ML%20Ensemble%20NIDS%20was%20extended%0Aand%20evaluated%20with%20the%20CIC-IDS2017%20dataset.%20Results%20show%20that%20under%20the%20model%0Asettings%20of%20an%20Update-ALL-NIDS%20rule%20%28specifically%20retrain%20and%20update%20all%20the%0Athree%20models%20upon%20the%20same%20NIDS%20retraining%20request%29%20the%20proposed%20ML%20ensemble%0ANIDS%20evolved%20intelligently%20and%20produced%20the%20best%20results%20with%20nearly%20100%25%0Adetection%20performance%20throughout%20the%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.03933v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hypergraph-Based%2520Machine%2520Learning%2520Ensemble%2520Network%2520Intrusion%2520Detection%250A%2520%2520System%26entry.906535625%3DZong-Zhi%2520Lin%2520and%2520Thomas%2520D.%2520Pike%2520and%2520Mark%2520M.%2520Bailey%2520and%2520Nathaniel%2520D.%2520Bastian%26entry.1292438233%3D%2520%2520Network%2520intrusion%2520detection%2520systems%2520%2528NIDS%2529%2520to%2520detect%2520malicious%2520attacks%250Acontinue%2520to%2520meet%2520challenges.%2520NIDS%2520are%2520often%2520developed%2520offline%2520while%2520they%2520face%250Aauto-generated%2520port%2520scan%2520infiltration%2520attempts%252C%2520resulting%2520in%2520a%2520significant%2520time%250Alag%2520from%2520adversarial%2520adaption%2520to%2520NIDS%2520response.%2520To%2520address%2520these%2520challenges%252C%2520we%250Ause%2520hypergraphs%2520focused%2520on%2520internet%2520protocol%2520addresses%2520and%2520destination%2520ports%2520to%250Acapture%2520evolving%2520patterns%2520of%2520port%2520scan%2520attacks.%2520The%2520derived%2520set%2520of%250Ahypergraph-based%2520metrics%2520are%2520then%2520used%2520to%2520train%2520an%2520ensemble%2520machine%2520learning%250A%2528ML%2529%2520based%2520NIDS%2520that%2520allows%2520for%2520real-time%2520adaption%2520in%2520monitoring%2520and%2520detecting%250Aport%2520scanning%2520activities%252C%2520other%2520types%2520of%2520attacks%252C%2520and%2520adversarial%2520intrusions%2520at%250Ahigh%2520accuracy%252C%2520precision%2520and%2520recall%2520performances.%2520This%2520ML%2520adapting%2520NIDS%2520was%250Adeveloped%2520through%2520the%2520combination%2520of%2520%25281%2529%2520intrusion%2520examples%252C%2520%25282%2529%2520NIDS%2520update%250Arules%252C%2520%25283%2529%2520attack%2520threshold%2520choices%2520to%2520trigger%2520NIDS%2520retraining%2520requests%252C%2520and%250A%25284%2529%2520a%2520production%2520environment%2520with%2520no%2520prior%2520knowledge%2520of%2520the%2520nature%2520of%2520network%250Atraffic.%252040%2520scenarios%2520were%2520auto-generated%2520to%2520evaluate%2520the%2520ML%2520ensemble%2520NIDS%250Acomprising%2520three%2520tree-based%2520models.%2520The%2520resulting%2520ML%2520Ensemble%2520NIDS%2520was%2520extended%250Aand%2520evaluated%2520with%2520the%2520CIC-IDS2017%2520dataset.%2520Results%2520show%2520that%2520under%2520the%2520model%250Asettings%2520of%2520an%2520Update-ALL-NIDS%2520rule%2520%2528specifically%2520retrain%2520and%2520update%2520all%2520the%250Athree%2520models%2520upon%2520the%2520same%2520NIDS%2520retraining%2520request%2529%2520the%2520proposed%2520ML%2520ensemble%250ANIDS%2520evolved%2520intelligently%2520and%2520produced%2520the%2520best%2520results%2520with%2520nearly%2520100%2525%250Adetection%2520performance%2520throughout%2520the%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.03933v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hypergraph-Based%20Machine%20Learning%20Ensemble%20Network%20Intrusion%20Detection%0A%20%20System&entry.906535625=Zong-Zhi%20Lin%20and%20Thomas%20D.%20Pike%20and%20Mark%20M.%20Bailey%20and%20Nathaniel%20D.%20Bastian&entry.1292438233=%20%20Network%20intrusion%20detection%20systems%20%28NIDS%29%20to%20detect%20malicious%20attacks%0Acontinue%20to%20meet%20challenges.%20NIDS%20are%20often%20developed%20offline%20while%20they%20face%0Aauto-generated%20port%20scan%20infiltration%20attempts%2C%20resulting%20in%20a%20significant%20time%0Alag%20from%20adversarial%20adaption%20to%20NIDS%20response.%20To%20address%20these%20challenges%2C%20we%0Ause%20hypergraphs%20focused%20on%20internet%20protocol%20addresses%20and%20destination%20ports%20to%0Acapture%20evolving%20patterns%20of%20port%20scan%20attacks.%20The%20derived%20set%20of%0Ahypergraph-based%20metrics%20are%20then%20used%20to%20train%20an%20ensemble%20machine%20learning%0A%28ML%29%20based%20NIDS%20that%20allows%20for%20real-time%20adaption%20in%20monitoring%20and%20detecting%0Aport%20scanning%20activities%2C%20other%20types%20of%20attacks%2C%20and%20adversarial%20intrusions%20at%0Ahigh%20accuracy%2C%20precision%20and%20recall%20performances.%20This%20ML%20adapting%20NIDS%20was%0Adeveloped%20through%20the%20combination%20of%20%281%29%20intrusion%20examples%2C%20%282%29%20NIDS%20update%0Arules%2C%20%283%29%20attack%20threshold%20choices%20to%20trigger%20NIDS%20retraining%20requests%2C%20and%0A%284%29%20a%20production%20environment%20with%20no%20prior%20knowledge%20of%20the%20nature%20of%20network%0Atraffic.%2040%20scenarios%20were%20auto-generated%20to%20evaluate%20the%20ML%20ensemble%20NIDS%0Acomprising%20three%20tree-based%20models.%20The%20resulting%20ML%20Ensemble%20NIDS%20was%20extended%0Aand%20evaluated%20with%20the%20CIC-IDS2017%20dataset.%20Results%20show%20that%20under%20the%20model%0Asettings%20of%20an%20Update-ALL-NIDS%20rule%20%28specifically%20retrain%20and%20update%20all%20the%0Athree%20models%20upon%20the%20same%20NIDS%20retraining%20request%29%20the%20proposed%20ML%20ensemble%0ANIDS%20evolved%20intelligently%20and%20produced%20the%20best%20results%20with%20nearly%20100%25%0Adetection%20performance%20throughout%20the%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.03933v3&entry.124074799=Read"},
{"title": "Extension of Recurrent Kernels to different Reservoir Computing\n  topologies", "author": "Giuseppe Alessio D'Inverno and Jonathan Dong", "abstract": "  Reservoir Computing (RC) has become popular in recent years due to its fast\nand efficient computational capabilities. Standard RC has been shown to be\nequivalent in the asymptotic limit to Recurrent Kernels, which helps in\nanalyzing its expressive power. However, many well-established RC paradigms,\nsuch as Leaky RC, Sparse RC, and Deep RC, are yet to be analyzed in such a way.\nThis study aims to fill this gap by providing an empirical analysis of the\nequivalence of specific RC architectures with their corresponding Recurrent\nKernel formulation. We conduct a convergence study by varying the activation\nfunction implemented in each architecture. Our study also sheds light on the\nrole of sparse connections in RC architectures and propose an optimal sparsity\nlevel that depends on the reservoir size. Furthermore, our systematic analysis\nshows that in Deep RC models, convergence is better achieved with successive\nreservoirs of decreasing sizes.\n", "link": "http://arxiv.org/abs/2401.14557v2", "date": "2024-09-06", "relevancy": 2.2408, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4505}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extension%20of%20Recurrent%20Kernels%20to%20different%20Reservoir%20Computing%0A%20%20topologies&body=Title%3A%20Extension%20of%20Recurrent%20Kernels%20to%20different%20Reservoir%20Computing%0A%20%20topologies%0AAuthor%3A%20Giuseppe%20Alessio%20D%27Inverno%20and%20Jonathan%20Dong%0AAbstract%3A%20%20%20Reservoir%20Computing%20%28RC%29%20has%20become%20popular%20in%20recent%20years%20due%20to%20its%20fast%0Aand%20efficient%20computational%20capabilities.%20Standard%20RC%20has%20been%20shown%20to%20be%0Aequivalent%20in%20the%20asymptotic%20limit%20to%20Recurrent%20Kernels%2C%20which%20helps%20in%0Aanalyzing%20its%20expressive%20power.%20However%2C%20many%20well-established%20RC%20paradigms%2C%0Asuch%20as%20Leaky%20RC%2C%20Sparse%20RC%2C%20and%20Deep%20RC%2C%20are%20yet%20to%20be%20analyzed%20in%20such%20a%20way.%0AThis%20study%20aims%20to%20fill%20this%20gap%20by%20providing%20an%20empirical%20analysis%20of%20the%0Aequivalence%20of%20specific%20RC%20architectures%20with%20their%20corresponding%20Recurrent%0AKernel%20formulation.%20We%20conduct%20a%20convergence%20study%20by%20varying%20the%20activation%0Afunction%20implemented%20in%20each%20architecture.%20Our%20study%20also%20sheds%20light%20on%20the%0Arole%20of%20sparse%20connections%20in%20RC%20architectures%20and%20propose%20an%20optimal%20sparsity%0Alevel%20that%20depends%20on%20the%20reservoir%20size.%20Furthermore%2C%20our%20systematic%20analysis%0Ashows%20that%20in%20Deep%20RC%20models%2C%20convergence%20is%20better%20achieved%20with%20successive%0Areservoirs%20of%20decreasing%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14557v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtension%2520of%2520Recurrent%2520Kernels%2520to%2520different%2520Reservoir%2520Computing%250A%2520%2520topologies%26entry.906535625%3DGiuseppe%2520Alessio%2520D%2527Inverno%2520and%2520Jonathan%2520Dong%26entry.1292438233%3D%2520%2520Reservoir%2520Computing%2520%2528RC%2529%2520has%2520become%2520popular%2520in%2520recent%2520years%2520due%2520to%2520its%2520fast%250Aand%2520efficient%2520computational%2520capabilities.%2520Standard%2520RC%2520has%2520been%2520shown%2520to%2520be%250Aequivalent%2520in%2520the%2520asymptotic%2520limit%2520to%2520Recurrent%2520Kernels%252C%2520which%2520helps%2520in%250Aanalyzing%2520its%2520expressive%2520power.%2520However%252C%2520many%2520well-established%2520RC%2520paradigms%252C%250Asuch%2520as%2520Leaky%2520RC%252C%2520Sparse%2520RC%252C%2520and%2520Deep%2520RC%252C%2520are%2520yet%2520to%2520be%2520analyzed%2520in%2520such%2520a%2520way.%250AThis%2520study%2520aims%2520to%2520fill%2520this%2520gap%2520by%2520providing%2520an%2520empirical%2520analysis%2520of%2520the%250Aequivalence%2520of%2520specific%2520RC%2520architectures%2520with%2520their%2520corresponding%2520Recurrent%250AKernel%2520formulation.%2520We%2520conduct%2520a%2520convergence%2520study%2520by%2520varying%2520the%2520activation%250Afunction%2520implemented%2520in%2520each%2520architecture.%2520Our%2520study%2520also%2520sheds%2520light%2520on%2520the%250Arole%2520of%2520sparse%2520connections%2520in%2520RC%2520architectures%2520and%2520propose%2520an%2520optimal%2520sparsity%250Alevel%2520that%2520depends%2520on%2520the%2520reservoir%2520size.%2520Furthermore%252C%2520our%2520systematic%2520analysis%250Ashows%2520that%2520in%2520Deep%2520RC%2520models%252C%2520convergence%2520is%2520better%2520achieved%2520with%2520successive%250Areservoirs%2520of%2520decreasing%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14557v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extension%20of%20Recurrent%20Kernels%20to%20different%20Reservoir%20Computing%0A%20%20topologies&entry.906535625=Giuseppe%20Alessio%20D%27Inverno%20and%20Jonathan%20Dong&entry.1292438233=%20%20Reservoir%20Computing%20%28RC%29%20has%20become%20popular%20in%20recent%20years%20due%20to%20its%20fast%0Aand%20efficient%20computational%20capabilities.%20Standard%20RC%20has%20been%20shown%20to%20be%0Aequivalent%20in%20the%20asymptotic%20limit%20to%20Recurrent%20Kernels%2C%20which%20helps%20in%0Aanalyzing%20its%20expressive%20power.%20However%2C%20many%20well-established%20RC%20paradigms%2C%0Asuch%20as%20Leaky%20RC%2C%20Sparse%20RC%2C%20and%20Deep%20RC%2C%20are%20yet%20to%20be%20analyzed%20in%20such%20a%20way.%0AThis%20study%20aims%20to%20fill%20this%20gap%20by%20providing%20an%20empirical%20analysis%20of%20the%0Aequivalence%20of%20specific%20RC%20architectures%20with%20their%20corresponding%20Recurrent%0AKernel%20formulation.%20We%20conduct%20a%20convergence%20study%20by%20varying%20the%20activation%0Afunction%20implemented%20in%20each%20architecture.%20Our%20study%20also%20sheds%20light%20on%20the%0Arole%20of%20sparse%20connections%20in%20RC%20architectures%20and%20propose%20an%20optimal%20sparsity%0Alevel%20that%20depends%20on%20the%20reservoir%20size.%20Furthermore%2C%20our%20systematic%20analysis%0Ashows%20that%20in%20Deep%20RC%20models%2C%20convergence%20is%20better%20achieved%20with%20successive%0Areservoirs%20of%20decreasing%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14557v2&entry.124074799=Read"},
{"title": "Question-Answering Dense Video Events", "author": "Hangyu Qin and Junbin Xiao and Angela Yao", "abstract": "  Multimodal Large Language Models (MLLMs) have shown excellent performance in\nquestion-answering of single-event videos. In this paper, we present\nquestion-answering dense video events, a novel task that requires answering and\ngrounding the dense-event questions in long videos, thus challenging MLLMs to\nfaithfully comprehend and reason about multiple events occurring over extended\ntime periods. To facilitate the study, we construct DeVE-QA - a dataset\nfeaturing 78K questions about 26K events on 10.6K long videos. We then\nbenchmark and show that existing MLLMs excelling at single-event QA struggle to\nperform well in DeVE-QA. For improvement, we propose DeVi, a novel\ntraining-free MLLM approach that highlights a hierarchical captioning module, a\ntemporal event memory module, and a self-consistency checking module to\nrespectively detect, contextualize and memorize, and ground dense-events in\nlong videos for question answering. Extensive experiments show that DeVi is\nsuperior at answering dense-event questions and grounding relevant video\nmoments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1\npercent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA\nrespectively.\n", "link": "http://arxiv.org/abs/2409.04388v1", "date": "2024-09-06", "relevancy": 2.211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Question-Answering%20Dense%20Video%20Events&body=Title%3A%20Question-Answering%20Dense%20Video%20Events%0AAuthor%3A%20Hangyu%20Qin%20and%20Junbin%20Xiao%20and%20Angela%20Yao%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20excellent%20performance%20in%0Aquestion-answering%20of%20single-event%20videos.%20In%20this%20paper%2C%20we%20present%0Aquestion-answering%20dense%20video%20events%2C%20a%20novel%20task%20that%20requires%20answering%20and%0Agrounding%20the%20dense-event%20questions%20in%20long%20videos%2C%20thus%20challenging%20MLLMs%20to%0Afaithfully%20comprehend%20and%20reason%20about%20multiple%20events%20occurring%20over%20extended%0Atime%20periods.%20To%20facilitate%20the%20study%2C%20we%20construct%20DeVE-QA%20-%20a%20dataset%0Afeaturing%2078K%20questions%20about%2026K%20events%20on%2010.6K%20long%20videos.%20We%20then%0Abenchmark%20and%20show%20that%20existing%20MLLMs%20excelling%20at%20single-event%20QA%20struggle%20to%0Aperform%20well%20in%20DeVE-QA.%20For%20improvement%2C%20we%20propose%20DeVi%2C%20a%20novel%0Atraining-free%20MLLM%20approach%20that%20highlights%20a%20hierarchical%20captioning%20module%2C%20a%0Atemporal%20event%20memory%20module%2C%20and%20a%20self-consistency%20checking%20module%20to%0Arespectively%20detect%2C%20contextualize%20and%20memorize%2C%20and%20ground%20dense-events%20in%0Along%20videos%20for%20question%20answering.%20Extensive%20experiments%20show%20that%20DeVi%20is%0Asuperior%20at%20answering%20dense-event%20questions%20and%20grounding%20relevant%20video%0Amoments.%20Compared%20with%20existing%20MLLMs%2C%20it%20achieves%20a%20remarkable%20increase%20of%204.1%0Apercent%20and%203.7%20percent%20for%20G%28round%29QA%20accuracy%20on%20DeVE-QA%20and%20NExT-GQA%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuestion-Answering%2520Dense%2520Video%2520Events%26entry.906535625%3DHangyu%2520Qin%2520and%2520Junbin%2520Xiao%2520and%2520Angela%2520Yao%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520shown%2520excellent%2520performance%2520in%250Aquestion-answering%2520of%2520single-event%2520videos.%2520In%2520this%2520paper%252C%2520we%2520present%250Aquestion-answering%2520dense%2520video%2520events%252C%2520a%2520novel%2520task%2520that%2520requires%2520answering%2520and%250Agrounding%2520the%2520dense-event%2520questions%2520in%2520long%2520videos%252C%2520thus%2520challenging%2520MLLMs%2520to%250Afaithfully%2520comprehend%2520and%2520reason%2520about%2520multiple%2520events%2520occurring%2520over%2520extended%250Atime%2520periods.%2520To%2520facilitate%2520the%2520study%252C%2520we%2520construct%2520DeVE-QA%2520-%2520a%2520dataset%250Afeaturing%252078K%2520questions%2520about%252026K%2520events%2520on%252010.6K%2520long%2520videos.%2520We%2520then%250Abenchmark%2520and%2520show%2520that%2520existing%2520MLLMs%2520excelling%2520at%2520single-event%2520QA%2520struggle%2520to%250Aperform%2520well%2520in%2520DeVE-QA.%2520For%2520improvement%252C%2520we%2520propose%2520DeVi%252C%2520a%2520novel%250Atraining-free%2520MLLM%2520approach%2520that%2520highlights%2520a%2520hierarchical%2520captioning%2520module%252C%2520a%250Atemporal%2520event%2520memory%2520module%252C%2520and%2520a%2520self-consistency%2520checking%2520module%2520to%250Arespectively%2520detect%252C%2520contextualize%2520and%2520memorize%252C%2520and%2520ground%2520dense-events%2520in%250Along%2520videos%2520for%2520question%2520answering.%2520Extensive%2520experiments%2520show%2520that%2520DeVi%2520is%250Asuperior%2520at%2520answering%2520dense-event%2520questions%2520and%2520grounding%2520relevant%2520video%250Amoments.%2520Compared%2520with%2520existing%2520MLLMs%252C%2520it%2520achieves%2520a%2520remarkable%2520increase%2520of%25204.1%250Apercent%2520and%25203.7%2520percent%2520for%2520G%2528round%2529QA%2520accuracy%2520on%2520DeVE-QA%2520and%2520NExT-GQA%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Question-Answering%20Dense%20Video%20Events&entry.906535625=Hangyu%20Qin%20and%20Junbin%20Xiao%20and%20Angela%20Yao&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20excellent%20performance%20in%0Aquestion-answering%20of%20single-event%20videos.%20In%20this%20paper%2C%20we%20present%0Aquestion-answering%20dense%20video%20events%2C%20a%20novel%20task%20that%20requires%20answering%20and%0Agrounding%20the%20dense-event%20questions%20in%20long%20videos%2C%20thus%20challenging%20MLLMs%20to%0Afaithfully%20comprehend%20and%20reason%20about%20multiple%20events%20occurring%20over%20extended%0Atime%20periods.%20To%20facilitate%20the%20study%2C%20we%20construct%20DeVE-QA%20-%20a%20dataset%0Afeaturing%2078K%20questions%20about%2026K%20events%20on%2010.6K%20long%20videos.%20We%20then%0Abenchmark%20and%20show%20that%20existing%20MLLMs%20excelling%20at%20single-event%20QA%20struggle%20to%0Aperform%20well%20in%20DeVE-QA.%20For%20improvement%2C%20we%20propose%20DeVi%2C%20a%20novel%0Atraining-free%20MLLM%20approach%20that%20highlights%20a%20hierarchical%20captioning%20module%2C%20a%0Atemporal%20event%20memory%20module%2C%20and%20a%20self-consistency%20checking%20module%20to%0Arespectively%20detect%2C%20contextualize%20and%20memorize%2C%20and%20ground%20dense-events%20in%0Along%20videos%20for%20question%20answering.%20Extensive%20experiments%20show%20that%20DeVi%20is%0Asuperior%20at%20answering%20dense-event%20questions%20and%20grounding%20relevant%20video%0Amoments.%20Compared%20with%20existing%20MLLMs%2C%20it%20achieves%20a%20remarkable%20increase%20of%204.1%0Apercent%20and%203.7%20percent%20for%20G%28round%29QA%20accuracy%20on%20DeVE-QA%20and%20NExT-GQA%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04388v1&entry.124074799=Read"},
{"title": "A Survey on Benchmarks of Multimodal Large Language Models", "author": "Jian Li and Weiheng Lu and Hao Fei and Meng Luo and Ming Dai and Min Xia and Yizhang Jin and Zhenye Gan and Ding Qi and Chaoyou Fu and Ying Tai and Wankou Yang and Yabiao Wang and Chengjie Wang", "abstract": "  Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on\n(1)perception and understanding, (2)cognition and reasoning, (3)specific\ndomains, (4)key capabilities, and (5)other modalities. Finally, we discuss the\nlimitations of the current evaluation methods for MLLMs and explore promising\nfuture directions. Our key argument is that evaluation should be regarded as a\ncrucial discipline to support the development of MLLMs better. For more\ndetails, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.\n", "link": "http://arxiv.org/abs/2408.08632v2", "date": "2024-09-06", "relevancy": 2.1977, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Benchmarks%20of%20Multimodal%20Large%20Language%20Models&body=Title%3A%20A%20Survey%20on%20Benchmarks%20of%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Jian%20Li%20and%20Weiheng%20Lu%20and%20Hao%20Fei%20and%20Meng%20Luo%20and%20Ming%20Dai%20and%20Min%20Xia%20and%20Yizhang%20Jin%20and%20Zhenye%20Gan%20and%20Ding%20Qi%20and%20Chaoyou%20Fu%20and%20Ying%20Tai%20and%20Wankou%20Yang%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20gaining%20increasing%20popularity%20in%0Aboth%20academia%20and%20industry%20due%20to%20their%20remarkable%20performance%20in%20various%0Aapplications%20such%20as%20visual%20question%20answering%2C%20visual%20perception%2C%0Aunderstanding%2C%20and%20reasoning.%20Over%20the%20past%20few%20years%2C%20significant%20efforts%20have%0Abeen%20made%20to%20examine%20MLLMs%20from%20multiple%20perspectives.%20This%20paper%20presents%20a%0Acomprehensive%20review%20of%20200%20benchmarks%20and%20evaluations%20for%20MLLMs%2C%20focusing%20on%0A%281%29perception%20and%20understanding%2C%20%282%29cognition%20and%20reasoning%2C%20%283%29specific%0Adomains%2C%20%284%29key%20capabilities%2C%20and%20%285%29other%20modalities.%20Finally%2C%20we%20discuss%20the%0Alimitations%20of%20the%20current%20evaluation%20methods%20for%20MLLMs%20and%20explore%20promising%0Afuture%20directions.%20Our%20key%20argument%20is%20that%20evaluation%20should%20be%20regarded%20as%20a%0Acrucial%20discipline%20to%20support%20the%20development%20of%20MLLMs%20better.%20For%20more%0Adetails%2C%20please%20visit%20our%20GitHub%20repository%3A%0Ahttps%3A//github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08632v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Benchmarks%2520of%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DJian%2520Li%2520and%2520Weiheng%2520Lu%2520and%2520Hao%2520Fei%2520and%2520Meng%2520Luo%2520and%2520Ming%2520Dai%2520and%2520Min%2520Xia%2520and%2520Yizhang%2520Jin%2520and%2520Zhenye%2520Gan%2520and%2520Ding%2520Qi%2520and%2520Chaoyou%2520Fu%2520and%2520Ying%2520Tai%2520and%2520Wankou%2520Yang%2520and%2520Yabiao%2520Wang%2520and%2520Chengjie%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520gaining%2520increasing%2520popularity%2520in%250Aboth%2520academia%2520and%2520industry%2520due%2520to%2520their%2520remarkable%2520performance%2520in%2520various%250Aapplications%2520such%2520as%2520visual%2520question%2520answering%252C%2520visual%2520perception%252C%250Aunderstanding%252C%2520and%2520reasoning.%2520Over%2520the%2520past%2520few%2520years%252C%2520significant%2520efforts%2520have%250Abeen%2520made%2520to%2520examine%2520MLLMs%2520from%2520multiple%2520perspectives.%2520This%2520paper%2520presents%2520a%250Acomprehensive%2520review%2520of%2520200%2520benchmarks%2520and%2520evaluations%2520for%2520MLLMs%252C%2520focusing%2520on%250A%25281%2529perception%2520and%2520understanding%252C%2520%25282%2529cognition%2520and%2520reasoning%252C%2520%25283%2529specific%250Adomains%252C%2520%25284%2529key%2520capabilities%252C%2520and%2520%25285%2529other%2520modalities.%2520Finally%252C%2520we%2520discuss%2520the%250Alimitations%2520of%2520the%2520current%2520evaluation%2520methods%2520for%2520MLLMs%2520and%2520explore%2520promising%250Afuture%2520directions.%2520Our%2520key%2520argument%2520is%2520that%2520evaluation%2520should%2520be%2520regarded%2520as%2520a%250Acrucial%2520discipline%2520to%2520support%2520the%2520development%2520of%2520MLLMs%2520better.%2520For%2520more%250Adetails%252C%2520please%2520visit%2520our%2520GitHub%2520repository%253A%250Ahttps%253A//github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08632v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Benchmarks%20of%20Multimodal%20Large%20Language%20Models&entry.906535625=Jian%20Li%20and%20Weiheng%20Lu%20and%20Hao%20Fei%20and%20Meng%20Luo%20and%20Ming%20Dai%20and%20Min%20Xia%20and%20Yizhang%20Jin%20and%20Zhenye%20Gan%20and%20Ding%20Qi%20and%20Chaoyou%20Fu%20and%20Ying%20Tai%20and%20Wankou%20Yang%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20gaining%20increasing%20popularity%20in%0Aboth%20academia%20and%20industry%20due%20to%20their%20remarkable%20performance%20in%20various%0Aapplications%20such%20as%20visual%20question%20answering%2C%20visual%20perception%2C%0Aunderstanding%2C%20and%20reasoning.%20Over%20the%20past%20few%20years%2C%20significant%20efforts%20have%0Abeen%20made%20to%20examine%20MLLMs%20from%20multiple%20perspectives.%20This%20paper%20presents%20a%0Acomprehensive%20review%20of%20200%20benchmarks%20and%20evaluations%20for%20MLLMs%2C%20focusing%20on%0A%281%29perception%20and%20understanding%2C%20%282%29cognition%20and%20reasoning%2C%20%283%29specific%0Adomains%2C%20%284%29key%20capabilities%2C%20and%20%285%29other%20modalities.%20Finally%2C%20we%20discuss%20the%0Alimitations%20of%20the%20current%20evaluation%20methods%20for%20MLLMs%20and%20explore%20promising%0Afuture%20directions.%20Our%20key%20argument%20is%20that%20evaluation%20should%20be%20regarded%20as%20a%0Acrucial%20discipline%20to%20support%20the%20development%20of%20MLLMs%20better.%20For%20more%0Adetails%2C%20please%20visit%20our%20GitHub%20repository%3A%0Ahttps%3A//github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08632v2&entry.124074799=Read"},
{"title": "HSTR-Net: Reference Based Video Super-resolution with Dual Cameras", "author": "H. Umut Suluhan and Abdullah Enes Doruk and Hasan F. Ates and Bahadir K. Gunturk", "abstract": "  High-spatio-temporal resolution (HSTR) video recording plays a crucial role\nin enhancing various imagery tasks that require fine-detailed information.\nState-of-the-art cameras provide this required high frame-rate and high spatial\nresolution together, albeit at a high cost. To alleviate this issue, this paper\nproposes a dual camera system for the generation of HSTR video using\nreference-based super-resolution (RefSR). One camera captures high spatial\nresolution low frame rate (HSLF) video while the other captures low spatial\nresolution high frame rate (LSHF) video simultaneously for the same scene. A\nnovel deep learning architecture is proposed to fuse HSLF and LSHF video feeds\nand synthesize HSTR video frames. The proposed model combines optical flow\nestimation and (channel-wise and spatial) attention mechanisms to capture the\nfine motion and complex dependencies between frames of the two video feeds.\nSimulations show that the proposed model provides significant improvement over\nexisting reference-based SR techniques in terms of PSNR and SSIM metrics. The\nmethod also exhibits sufficient frames per second (FPS) for aerial monitoring\nwhen deployed on a power-constrained drone equipped with dual cameras.\n", "link": "http://arxiv.org/abs/2310.12092v2", "date": "2024-09-06", "relevancy": 2.1921, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5841}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5239}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HSTR-Net%3A%20Reference%20Based%20Video%20Super-resolution%20with%20Dual%20Cameras&body=Title%3A%20HSTR-Net%3A%20Reference%20Based%20Video%20Super-resolution%20with%20Dual%20Cameras%0AAuthor%3A%20H.%20Umut%20Suluhan%20and%20Abdullah%20Enes%20Doruk%20and%20Hasan%20F.%20Ates%20and%20Bahadir%20K.%20Gunturk%0AAbstract%3A%20%20%20High-spatio-temporal%20resolution%20%28HSTR%29%20video%20recording%20plays%20a%20crucial%20role%0Ain%20enhancing%20various%20imagery%20tasks%20that%20require%20fine-detailed%20information.%0AState-of-the-art%20cameras%20provide%20this%20required%20high%20frame-rate%20and%20high%20spatial%0Aresolution%20together%2C%20albeit%20at%20a%20high%20cost.%20To%20alleviate%20this%20issue%2C%20this%20paper%0Aproposes%20a%20dual%20camera%20system%20for%20the%20generation%20of%20HSTR%20video%20using%0Areference-based%20super-resolution%20%28RefSR%29.%20One%20camera%20captures%20high%20spatial%0Aresolution%20low%20frame%20rate%20%28HSLF%29%20video%20while%20the%20other%20captures%20low%20spatial%0Aresolution%20high%20frame%20rate%20%28LSHF%29%20video%20simultaneously%20for%20the%20same%20scene.%20A%0Anovel%20deep%20learning%20architecture%20is%20proposed%20to%20fuse%20HSLF%20and%20LSHF%20video%20feeds%0Aand%20synthesize%20HSTR%20video%20frames.%20The%20proposed%20model%20combines%20optical%20flow%0Aestimation%20and%20%28channel-wise%20and%20spatial%29%20attention%20mechanisms%20to%20capture%20the%0Afine%20motion%20and%20complex%20dependencies%20between%20frames%20of%20the%20two%20video%20feeds.%0ASimulations%20show%20that%20the%20proposed%20model%20provides%20significant%20improvement%20over%0Aexisting%20reference-based%20SR%20techniques%20in%20terms%20of%20PSNR%20and%20SSIM%20metrics.%20The%0Amethod%20also%20exhibits%20sufficient%20frames%20per%20second%20%28FPS%29%20for%20aerial%20monitoring%0Awhen%20deployed%20on%20a%20power-constrained%20drone%20equipped%20with%20dual%20cameras.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12092v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHSTR-Net%253A%2520Reference%2520Based%2520Video%2520Super-resolution%2520with%2520Dual%2520Cameras%26entry.906535625%3DH.%2520Umut%2520Suluhan%2520and%2520Abdullah%2520Enes%2520Doruk%2520and%2520Hasan%2520F.%2520Ates%2520and%2520Bahadir%2520K.%2520Gunturk%26entry.1292438233%3D%2520%2520High-spatio-temporal%2520resolution%2520%2528HSTR%2529%2520video%2520recording%2520plays%2520a%2520crucial%2520role%250Ain%2520enhancing%2520various%2520imagery%2520tasks%2520that%2520require%2520fine-detailed%2520information.%250AState-of-the-art%2520cameras%2520provide%2520this%2520required%2520high%2520frame-rate%2520and%2520high%2520spatial%250Aresolution%2520together%252C%2520albeit%2520at%2520a%2520high%2520cost.%2520To%2520alleviate%2520this%2520issue%252C%2520this%2520paper%250Aproposes%2520a%2520dual%2520camera%2520system%2520for%2520the%2520generation%2520of%2520HSTR%2520video%2520using%250Areference-based%2520super-resolution%2520%2528RefSR%2529.%2520One%2520camera%2520captures%2520high%2520spatial%250Aresolution%2520low%2520frame%2520rate%2520%2528HSLF%2529%2520video%2520while%2520the%2520other%2520captures%2520low%2520spatial%250Aresolution%2520high%2520frame%2520rate%2520%2528LSHF%2529%2520video%2520simultaneously%2520for%2520the%2520same%2520scene.%2520A%250Anovel%2520deep%2520learning%2520architecture%2520is%2520proposed%2520to%2520fuse%2520HSLF%2520and%2520LSHF%2520video%2520feeds%250Aand%2520synthesize%2520HSTR%2520video%2520frames.%2520The%2520proposed%2520model%2520combines%2520optical%2520flow%250Aestimation%2520and%2520%2528channel-wise%2520and%2520spatial%2529%2520attention%2520mechanisms%2520to%2520capture%2520the%250Afine%2520motion%2520and%2520complex%2520dependencies%2520between%2520frames%2520of%2520the%2520two%2520video%2520feeds.%250ASimulations%2520show%2520that%2520the%2520proposed%2520model%2520provides%2520significant%2520improvement%2520over%250Aexisting%2520reference-based%2520SR%2520techniques%2520in%2520terms%2520of%2520PSNR%2520and%2520SSIM%2520metrics.%2520The%250Amethod%2520also%2520exhibits%2520sufficient%2520frames%2520per%2520second%2520%2528FPS%2529%2520for%2520aerial%2520monitoring%250Awhen%2520deployed%2520on%2520a%2520power-constrained%2520drone%2520equipped%2520with%2520dual%2520cameras.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12092v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HSTR-Net%3A%20Reference%20Based%20Video%20Super-resolution%20with%20Dual%20Cameras&entry.906535625=H.%20Umut%20Suluhan%20and%20Abdullah%20Enes%20Doruk%20and%20Hasan%20F.%20Ates%20and%20Bahadir%20K.%20Gunturk&entry.1292438233=%20%20High-spatio-temporal%20resolution%20%28HSTR%29%20video%20recording%20plays%20a%20crucial%20role%0Ain%20enhancing%20various%20imagery%20tasks%20that%20require%20fine-detailed%20information.%0AState-of-the-art%20cameras%20provide%20this%20required%20high%20frame-rate%20and%20high%20spatial%0Aresolution%20together%2C%20albeit%20at%20a%20high%20cost.%20To%20alleviate%20this%20issue%2C%20this%20paper%0Aproposes%20a%20dual%20camera%20system%20for%20the%20generation%20of%20HSTR%20video%20using%0Areference-based%20super-resolution%20%28RefSR%29.%20One%20camera%20captures%20high%20spatial%0Aresolution%20low%20frame%20rate%20%28HSLF%29%20video%20while%20the%20other%20captures%20low%20spatial%0Aresolution%20high%20frame%20rate%20%28LSHF%29%20video%20simultaneously%20for%20the%20same%20scene.%20A%0Anovel%20deep%20learning%20architecture%20is%20proposed%20to%20fuse%20HSLF%20and%20LSHF%20video%20feeds%0Aand%20synthesize%20HSTR%20video%20frames.%20The%20proposed%20model%20combines%20optical%20flow%0Aestimation%20and%20%28channel-wise%20and%20spatial%29%20attention%20mechanisms%20to%20capture%20the%0Afine%20motion%20and%20complex%20dependencies%20between%20frames%20of%20the%20two%20video%20feeds.%0ASimulations%20show%20that%20the%20proposed%20model%20provides%20significant%20improvement%20over%0Aexisting%20reference-based%20SR%20techniques%20in%20terms%20of%20PSNR%20and%20SSIM%20metrics.%20The%0Amethod%20also%20exhibits%20sufficient%20frames%20per%20second%20%28FPS%29%20for%20aerial%20monitoring%0Awhen%20deployed%20on%20a%20power-constrained%20drone%20equipped%20with%20dual%20cameras.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12092v2&entry.124074799=Read"},
{"title": "iSeg: An Iterative Refinement-based Framework for Training-free\n  Segmentation", "author": "Lin Sun and Jiale Cao and Jin Xie and Fahad Shahbaz Khan and Yanwei Pang", "abstract": "  Stable diffusion has demonstrated strong image synthesis ability to given\ntext descriptions, suggesting it to contain strong semantic clue for grouping\nobjects. Inspired by this, researchers have explored employing stable diffusion\nfor trainingfree segmentation. Most existing approaches either simply employ\ncross-attention map or refine it by self-attention map, to generate\nsegmentation masks. We believe that iterative refinement with self-attention\nmap would lead to better results. However, we mpirically demonstrate that such\na refinement is sub-optimal likely due to the self-attention map containing\nirrelevant global information which hampers accurately refining cross-attention\nmap with multiple iterations. To address this, we propose an iterative\nrefinement framework for training-free segmentation, named iSeg, having an\nentropy-reduced self-attention module which utilizes a gradient descent scheme\nto reduce the entropy of self-attention map, thereby suppressing the weak\nresponses corresponding to irrelevant global information. Leveraging the\nentropy-reduced self-attention module, our iSeg stably improves refined\ncrossattention map with iterative refinement. Further, we design a\ncategory-enhanced cross-attention module to generate accurate cross-attention\nmap, providing a better initial input for iterative refinement. Extensive\nexperiments across different datasets and diverse segmentation tasks reveal the\nmerits of proposed contributions, leading to promising performance on diverse\nsegmentation tasks. For unsupervised semantic segmentation on Cityscapes, our\niSeg achieves an absolute gain of 3.8% in terms of mIoU compared to the best\nexisting training-free approach in literature. Moreover, our proposed iSeg can\nsupport segmentation with different kind of images and interactions.\n", "link": "http://arxiv.org/abs/2409.03209v2", "date": "2024-09-06", "relevancy": 2.1913, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5624}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5394}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iSeg%3A%20An%20Iterative%20Refinement-based%20Framework%20for%20Training-free%0A%20%20Segmentation&body=Title%3A%20iSeg%3A%20An%20Iterative%20Refinement-based%20Framework%20for%20Training-free%0A%20%20Segmentation%0AAuthor%3A%20Lin%20Sun%20and%20Jiale%20Cao%20and%20Jin%20Xie%20and%20Fahad%20Shahbaz%20Khan%20and%20Yanwei%20Pang%0AAbstract%3A%20%20%20Stable%20diffusion%20has%20demonstrated%20strong%20image%20synthesis%20ability%20to%20given%0Atext%20descriptions%2C%20suggesting%20it%20to%20contain%20strong%20semantic%20clue%20for%20grouping%0Aobjects.%20Inspired%20by%20this%2C%20researchers%20have%20explored%20employing%20stable%20diffusion%0Afor%20trainingfree%20segmentation.%20Most%20existing%20approaches%20either%20simply%20employ%0Across-attention%20map%20or%20refine%20it%20by%20self-attention%20map%2C%20to%20generate%0Asegmentation%20masks.%20We%20believe%20that%20iterative%20refinement%20with%20self-attention%0Amap%20would%20lead%20to%20better%20results.%20However%2C%20we%20mpirically%20demonstrate%20that%20such%0Aa%20refinement%20is%20sub-optimal%20likely%20due%20to%20the%20self-attention%20map%20containing%0Airrelevant%20global%20information%20which%20hampers%20accurately%20refining%20cross-attention%0Amap%20with%20multiple%20iterations.%20To%20address%20this%2C%20we%20propose%20an%20iterative%0Arefinement%20framework%20for%20training-free%20segmentation%2C%20named%20iSeg%2C%20having%20an%0Aentropy-reduced%20self-attention%20module%20which%20utilizes%20a%20gradient%20descent%20scheme%0Ato%20reduce%20the%20entropy%20of%20self-attention%20map%2C%20thereby%20suppressing%20the%20weak%0Aresponses%20corresponding%20to%20irrelevant%20global%20information.%20Leveraging%20the%0Aentropy-reduced%20self-attention%20module%2C%20our%20iSeg%20stably%20improves%20refined%0Acrossattention%20map%20with%20iterative%20refinement.%20Further%2C%20we%20design%20a%0Acategory-enhanced%20cross-attention%20module%20to%20generate%20accurate%20cross-attention%0Amap%2C%20providing%20a%20better%20initial%20input%20for%20iterative%20refinement.%20Extensive%0Aexperiments%20across%20different%20datasets%20and%20diverse%20segmentation%20tasks%20reveal%20the%0Amerits%20of%20proposed%20contributions%2C%20leading%20to%20promising%20performance%20on%20diverse%0Asegmentation%20tasks.%20For%20unsupervised%20semantic%20segmentation%20on%20Cityscapes%2C%20our%0AiSeg%20achieves%20an%20absolute%20gain%20of%203.8%25%20in%20terms%20of%20mIoU%20compared%20to%20the%20best%0Aexisting%20training-free%20approach%20in%20literature.%20Moreover%2C%20our%20proposed%20iSeg%20can%0Asupport%20segmentation%20with%20different%20kind%20of%20images%20and%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiSeg%253A%2520An%2520Iterative%2520Refinement-based%2520Framework%2520for%2520Training-free%250A%2520%2520Segmentation%26entry.906535625%3DLin%2520Sun%2520and%2520Jiale%2520Cao%2520and%2520Jin%2520Xie%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Yanwei%2520Pang%26entry.1292438233%3D%2520%2520Stable%2520diffusion%2520has%2520demonstrated%2520strong%2520image%2520synthesis%2520ability%2520to%2520given%250Atext%2520descriptions%252C%2520suggesting%2520it%2520to%2520contain%2520strong%2520semantic%2520clue%2520for%2520grouping%250Aobjects.%2520Inspired%2520by%2520this%252C%2520researchers%2520have%2520explored%2520employing%2520stable%2520diffusion%250Afor%2520trainingfree%2520segmentation.%2520Most%2520existing%2520approaches%2520either%2520simply%2520employ%250Across-attention%2520map%2520or%2520refine%2520it%2520by%2520self-attention%2520map%252C%2520to%2520generate%250Asegmentation%2520masks.%2520We%2520believe%2520that%2520iterative%2520refinement%2520with%2520self-attention%250Amap%2520would%2520lead%2520to%2520better%2520results.%2520However%252C%2520we%2520mpirically%2520demonstrate%2520that%2520such%250Aa%2520refinement%2520is%2520sub-optimal%2520likely%2520due%2520to%2520the%2520self-attention%2520map%2520containing%250Airrelevant%2520global%2520information%2520which%2520hampers%2520accurately%2520refining%2520cross-attention%250Amap%2520with%2520multiple%2520iterations.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520iterative%250Arefinement%2520framework%2520for%2520training-free%2520segmentation%252C%2520named%2520iSeg%252C%2520having%2520an%250Aentropy-reduced%2520self-attention%2520module%2520which%2520utilizes%2520a%2520gradient%2520descent%2520scheme%250Ato%2520reduce%2520the%2520entropy%2520of%2520self-attention%2520map%252C%2520thereby%2520suppressing%2520the%2520weak%250Aresponses%2520corresponding%2520to%2520irrelevant%2520global%2520information.%2520Leveraging%2520the%250Aentropy-reduced%2520self-attention%2520module%252C%2520our%2520iSeg%2520stably%2520improves%2520refined%250Acrossattention%2520map%2520with%2520iterative%2520refinement.%2520Further%252C%2520we%2520design%2520a%250Acategory-enhanced%2520cross-attention%2520module%2520to%2520generate%2520accurate%2520cross-attention%250Amap%252C%2520providing%2520a%2520better%2520initial%2520input%2520for%2520iterative%2520refinement.%2520Extensive%250Aexperiments%2520across%2520different%2520datasets%2520and%2520diverse%2520segmentation%2520tasks%2520reveal%2520the%250Amerits%2520of%2520proposed%2520contributions%252C%2520leading%2520to%2520promising%2520performance%2520on%2520diverse%250Asegmentation%2520tasks.%2520For%2520unsupervised%2520semantic%2520segmentation%2520on%2520Cityscapes%252C%2520our%250AiSeg%2520achieves%2520an%2520absolute%2520gain%2520of%25203.8%2525%2520in%2520terms%2520of%2520mIoU%2520compared%2520to%2520the%2520best%250Aexisting%2520training-free%2520approach%2520in%2520literature.%2520Moreover%252C%2520our%2520proposed%2520iSeg%2520can%250Asupport%2520segmentation%2520with%2520different%2520kind%2520of%2520images%2520and%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iSeg%3A%20An%20Iterative%20Refinement-based%20Framework%20for%20Training-free%0A%20%20Segmentation&entry.906535625=Lin%20Sun%20and%20Jiale%20Cao%20and%20Jin%20Xie%20and%20Fahad%20Shahbaz%20Khan%20and%20Yanwei%20Pang&entry.1292438233=%20%20Stable%20diffusion%20has%20demonstrated%20strong%20image%20synthesis%20ability%20to%20given%0Atext%20descriptions%2C%20suggesting%20it%20to%20contain%20strong%20semantic%20clue%20for%20grouping%0Aobjects.%20Inspired%20by%20this%2C%20researchers%20have%20explored%20employing%20stable%20diffusion%0Afor%20trainingfree%20segmentation.%20Most%20existing%20approaches%20either%20simply%20employ%0Across-attention%20map%20or%20refine%20it%20by%20self-attention%20map%2C%20to%20generate%0Asegmentation%20masks.%20We%20believe%20that%20iterative%20refinement%20with%20self-attention%0Amap%20would%20lead%20to%20better%20results.%20However%2C%20we%20mpirically%20demonstrate%20that%20such%0Aa%20refinement%20is%20sub-optimal%20likely%20due%20to%20the%20self-attention%20map%20containing%0Airrelevant%20global%20information%20which%20hampers%20accurately%20refining%20cross-attention%0Amap%20with%20multiple%20iterations.%20To%20address%20this%2C%20we%20propose%20an%20iterative%0Arefinement%20framework%20for%20training-free%20segmentation%2C%20named%20iSeg%2C%20having%20an%0Aentropy-reduced%20self-attention%20module%20which%20utilizes%20a%20gradient%20descent%20scheme%0Ato%20reduce%20the%20entropy%20of%20self-attention%20map%2C%20thereby%20suppressing%20the%20weak%0Aresponses%20corresponding%20to%20irrelevant%20global%20information.%20Leveraging%20the%0Aentropy-reduced%20self-attention%20module%2C%20our%20iSeg%20stably%20improves%20refined%0Acrossattention%20map%20with%20iterative%20refinement.%20Further%2C%20we%20design%20a%0Acategory-enhanced%20cross-attention%20module%20to%20generate%20accurate%20cross-attention%0Amap%2C%20providing%20a%20better%20initial%20input%20for%20iterative%20refinement.%20Extensive%0Aexperiments%20across%20different%20datasets%20and%20diverse%20segmentation%20tasks%20reveal%20the%0Amerits%20of%20proposed%20contributions%2C%20leading%20to%20promising%20performance%20on%20diverse%0Asegmentation%20tasks.%20For%20unsupervised%20semantic%20segmentation%20on%20Cityscapes%2C%20our%0AiSeg%20achieves%20an%20absolute%20gain%20of%203.8%25%20in%20terms%20of%20mIoU%20compared%20to%20the%20best%0Aexisting%20training-free%20approach%20in%20literature.%20Moreover%2C%20our%20proposed%20iSeg%20can%0Asupport%20segmentation%20with%20different%20kind%20of%20images%20and%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03209v2&entry.124074799=Read"},
{"title": "Exploring Foundation Models for Synthetic Medical Imaging: A Study on\n  Chest X-Rays and Fine-Tuning Techniques", "author": "Davide Clode da Silva and Marina Musse Bernardes and Nathalia Giacomini Ceretta and Gabriel Vaz de Souza and Gabriel Fonseca Silva and Rafael Heitor Bordini and Soraia Raupp Musse", "abstract": "  Machine learning has significantly advanced healthcare by aiding in disease\nprevention and treatment identification. However, accessing patient data can be\nchallenging due to privacy concerns and strict regulations. Generating\nsynthetic, realistic data offers a potential solution for overcoming these\nlimitations, and recent studies suggest that fine-tuning foundation models can\nproduce such data effectively. In this study, we explore the potential of\nfoundation models for generating realistic medical images, particularly chest\nx-rays, and assess how their performance improves with fine-tuning. We propose\nusing a Latent Diffusion Model, starting with a pre-trained foundation model\nand refining it through various configurations. Additionally, we performed\nexperiments with input from a medical professional to assess the realism of the\nimages produced by each trained model.\n", "link": "http://arxiv.org/abs/2409.04424v1", "date": "2024-09-06", "relevancy": 2.1805, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Foundation%20Models%20for%20Synthetic%20Medical%20Imaging%3A%20A%20Study%20on%0A%20%20Chest%20X-Rays%20and%20Fine-Tuning%20Techniques&body=Title%3A%20Exploring%20Foundation%20Models%20for%20Synthetic%20Medical%20Imaging%3A%20A%20Study%20on%0A%20%20Chest%20X-Rays%20and%20Fine-Tuning%20Techniques%0AAuthor%3A%20Davide%20Clode%20da%20Silva%20and%20Marina%20Musse%20Bernardes%20and%20Nathalia%20Giacomini%20Ceretta%20and%20Gabriel%20Vaz%20de%20Souza%20and%20Gabriel%20Fonseca%20Silva%20and%20Rafael%20Heitor%20Bordini%20and%20Soraia%20Raupp%20Musse%0AAbstract%3A%20%20%20Machine%20learning%20has%20significantly%20advanced%20healthcare%20by%20aiding%20in%20disease%0Aprevention%20and%20treatment%20identification.%20However%2C%20accessing%20patient%20data%20can%20be%0Achallenging%20due%20to%20privacy%20concerns%20and%20strict%20regulations.%20Generating%0Asynthetic%2C%20realistic%20data%20offers%20a%20potential%20solution%20for%20overcoming%20these%0Alimitations%2C%20and%20recent%20studies%20suggest%20that%20fine-tuning%20foundation%20models%20can%0Aproduce%20such%20data%20effectively.%20In%20this%20study%2C%20we%20explore%20the%20potential%20of%0Afoundation%20models%20for%20generating%20realistic%20medical%20images%2C%20particularly%20chest%0Ax-rays%2C%20and%20assess%20how%20their%20performance%20improves%20with%20fine-tuning.%20We%20propose%0Ausing%20a%20Latent%20Diffusion%20Model%2C%20starting%20with%20a%20pre-trained%20foundation%20model%0Aand%20refining%20it%20through%20various%20configurations.%20Additionally%2C%20we%20performed%0Aexperiments%20with%20input%20from%20a%20medical%20professional%20to%20assess%20the%20realism%20of%20the%0Aimages%20produced%20by%20each%20trained%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Foundation%2520Models%2520for%2520Synthetic%2520Medical%2520Imaging%253A%2520A%2520Study%2520on%250A%2520%2520Chest%2520X-Rays%2520and%2520Fine-Tuning%2520Techniques%26entry.906535625%3DDavide%2520Clode%2520da%2520Silva%2520and%2520Marina%2520Musse%2520Bernardes%2520and%2520Nathalia%2520Giacomini%2520Ceretta%2520and%2520Gabriel%2520Vaz%2520de%2520Souza%2520and%2520Gabriel%2520Fonseca%2520Silva%2520and%2520Rafael%2520Heitor%2520Bordini%2520and%2520Soraia%2520Raupp%2520Musse%26entry.1292438233%3D%2520%2520Machine%2520learning%2520has%2520significantly%2520advanced%2520healthcare%2520by%2520aiding%2520in%2520disease%250Aprevention%2520and%2520treatment%2520identification.%2520However%252C%2520accessing%2520patient%2520data%2520can%2520be%250Achallenging%2520due%2520to%2520privacy%2520concerns%2520and%2520strict%2520regulations.%2520Generating%250Asynthetic%252C%2520realistic%2520data%2520offers%2520a%2520potential%2520solution%2520for%2520overcoming%2520these%250Alimitations%252C%2520and%2520recent%2520studies%2520suggest%2520that%2520fine-tuning%2520foundation%2520models%2520can%250Aproduce%2520such%2520data%2520effectively.%2520In%2520this%2520study%252C%2520we%2520explore%2520the%2520potential%2520of%250Afoundation%2520models%2520for%2520generating%2520realistic%2520medical%2520images%252C%2520particularly%2520chest%250Ax-rays%252C%2520and%2520assess%2520how%2520their%2520performance%2520improves%2520with%2520fine-tuning.%2520We%2520propose%250Ausing%2520a%2520Latent%2520Diffusion%2520Model%252C%2520starting%2520with%2520a%2520pre-trained%2520foundation%2520model%250Aand%2520refining%2520it%2520through%2520various%2520configurations.%2520Additionally%252C%2520we%2520performed%250Aexperiments%2520with%2520input%2520from%2520a%2520medical%2520professional%2520to%2520assess%2520the%2520realism%2520of%2520the%250Aimages%2520produced%2520by%2520each%2520trained%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Foundation%20Models%20for%20Synthetic%20Medical%20Imaging%3A%20A%20Study%20on%0A%20%20Chest%20X-Rays%20and%20Fine-Tuning%20Techniques&entry.906535625=Davide%20Clode%20da%20Silva%20and%20Marina%20Musse%20Bernardes%20and%20Nathalia%20Giacomini%20Ceretta%20and%20Gabriel%20Vaz%20de%20Souza%20and%20Gabriel%20Fonseca%20Silva%20and%20Rafael%20Heitor%20Bordini%20and%20Soraia%20Raupp%20Musse&entry.1292438233=%20%20Machine%20learning%20has%20significantly%20advanced%20healthcare%20by%20aiding%20in%20disease%0Aprevention%20and%20treatment%20identification.%20However%2C%20accessing%20patient%20data%20can%20be%0Achallenging%20due%20to%20privacy%20concerns%20and%20strict%20regulations.%20Generating%0Asynthetic%2C%20realistic%20data%20offers%20a%20potential%20solution%20for%20overcoming%20these%0Alimitations%2C%20and%20recent%20studies%20suggest%20that%20fine-tuning%20foundation%20models%20can%0Aproduce%20such%20data%20effectively.%20In%20this%20study%2C%20we%20explore%20the%20potential%20of%0Afoundation%20models%20for%20generating%20realistic%20medical%20images%2C%20particularly%20chest%0Ax-rays%2C%20and%20assess%20how%20their%20performance%20improves%20with%20fine-tuning.%20We%20propose%0Ausing%20a%20Latent%20Diffusion%20Model%2C%20starting%20with%20a%20pre-trained%20foundation%20model%0Aand%20refining%20it%20through%20various%20configurations.%20Additionally%2C%20we%20performed%0Aexperiments%20with%20input%20from%20a%20medical%20professional%20to%20assess%20the%20realism%20of%20the%0Aimages%20produced%20by%20each%20trained%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04424v1&entry.124074799=Read"},
{"title": "Accelerating Training with Neuron Interaction and Nowcasting Networks", "author": "Boris Knyazev and Abhinav Moudgil and Guillaume Lajoie and Eugene Belilovsky and Simon Lacoste-Julien", "abstract": "  Neural network training can be accelerated when a learnable update rule is\nused in lieu of classic adaptive optimizers (e.g. Adam). However, learnable\nupdate rules can be costly and unstable to train and use. A simpler recently\nproposed approach to accelerate training is to use Adam for most of the\noptimization steps and periodically, only every few steps, nowcast (predict\nfuture) parameters. We improve this approach by Neuron interaction and\nNowcasting (NiNo) networks. NiNo leverages neuron connectivity and graph neural\nnetworks to more accurately nowcast parameters by learning in a supervised way\nfrom a set of training trajectories over multiple tasks. We show that in some\nnetworks, such as Transformers, neuron connectivity is non-trivial. By\naccurately modeling neuron connectivity, we allow NiNo to accelerate Adam\ntraining by up to 50\\% in vision and language tasks.\n", "link": "http://arxiv.org/abs/2409.04434v1", "date": "2024-09-06", "relevancy": 2.1774, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5857}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5233}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Training%20with%20Neuron%20Interaction%20and%20Nowcasting%20Networks&body=Title%3A%20Accelerating%20Training%20with%20Neuron%20Interaction%20and%20Nowcasting%20Networks%0AAuthor%3A%20Boris%20Knyazev%20and%20Abhinav%20Moudgil%20and%20Guillaume%20Lajoie%20and%20Eugene%20Belilovsky%20and%20Simon%20Lacoste-Julien%0AAbstract%3A%20%20%20Neural%20network%20training%20can%20be%20accelerated%20when%20a%20learnable%20update%20rule%20is%0Aused%20in%20lieu%20of%20classic%20adaptive%20optimizers%20%28e.g.%20Adam%29.%20However%2C%20learnable%0Aupdate%20rules%20can%20be%20costly%20and%20unstable%20to%20train%20and%20use.%20A%20simpler%20recently%0Aproposed%20approach%20to%20accelerate%20training%20is%20to%20use%20Adam%20for%20most%20of%20the%0Aoptimization%20steps%20and%20periodically%2C%20only%20every%20few%20steps%2C%20nowcast%20%28predict%0Afuture%29%20parameters.%20We%20improve%20this%20approach%20by%20Neuron%20interaction%20and%0ANowcasting%20%28NiNo%29%20networks.%20NiNo%20leverages%20neuron%20connectivity%20and%20graph%20neural%0Anetworks%20to%20more%20accurately%20nowcast%20parameters%20by%20learning%20in%20a%20supervised%20way%0Afrom%20a%20set%20of%20training%20trajectories%20over%20multiple%20tasks.%20We%20show%20that%20in%20some%0Anetworks%2C%20such%20as%20Transformers%2C%20neuron%20connectivity%20is%20non-trivial.%20By%0Aaccurately%20modeling%20neuron%20connectivity%2C%20we%20allow%20NiNo%20to%20accelerate%20Adam%0Atraining%20by%20up%20to%2050%5C%25%20in%20vision%20and%20language%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Training%2520with%2520Neuron%2520Interaction%2520and%2520Nowcasting%2520Networks%26entry.906535625%3DBoris%2520Knyazev%2520and%2520Abhinav%2520Moudgil%2520and%2520Guillaume%2520Lajoie%2520and%2520Eugene%2520Belilovsky%2520and%2520Simon%2520Lacoste-Julien%26entry.1292438233%3D%2520%2520Neural%2520network%2520training%2520can%2520be%2520accelerated%2520when%2520a%2520learnable%2520update%2520rule%2520is%250Aused%2520in%2520lieu%2520of%2520classic%2520adaptive%2520optimizers%2520%2528e.g.%2520Adam%2529.%2520However%252C%2520learnable%250Aupdate%2520rules%2520can%2520be%2520costly%2520and%2520unstable%2520to%2520train%2520and%2520use.%2520A%2520simpler%2520recently%250Aproposed%2520approach%2520to%2520accelerate%2520training%2520is%2520to%2520use%2520Adam%2520for%2520most%2520of%2520the%250Aoptimization%2520steps%2520and%2520periodically%252C%2520only%2520every%2520few%2520steps%252C%2520nowcast%2520%2528predict%250Afuture%2529%2520parameters.%2520We%2520improve%2520this%2520approach%2520by%2520Neuron%2520interaction%2520and%250ANowcasting%2520%2528NiNo%2529%2520networks.%2520NiNo%2520leverages%2520neuron%2520connectivity%2520and%2520graph%2520neural%250Anetworks%2520to%2520more%2520accurately%2520nowcast%2520parameters%2520by%2520learning%2520in%2520a%2520supervised%2520way%250Afrom%2520a%2520set%2520of%2520training%2520trajectories%2520over%2520multiple%2520tasks.%2520We%2520show%2520that%2520in%2520some%250Anetworks%252C%2520such%2520as%2520Transformers%252C%2520neuron%2520connectivity%2520is%2520non-trivial.%2520By%250Aaccurately%2520modeling%2520neuron%2520connectivity%252C%2520we%2520allow%2520NiNo%2520to%2520accelerate%2520Adam%250Atraining%2520by%2520up%2520to%252050%255C%2525%2520in%2520vision%2520and%2520language%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Training%20with%20Neuron%20Interaction%20and%20Nowcasting%20Networks&entry.906535625=Boris%20Knyazev%20and%20Abhinav%20Moudgil%20and%20Guillaume%20Lajoie%20and%20Eugene%20Belilovsky%20and%20Simon%20Lacoste-Julien&entry.1292438233=%20%20Neural%20network%20training%20can%20be%20accelerated%20when%20a%20learnable%20update%20rule%20is%0Aused%20in%20lieu%20of%20classic%20adaptive%20optimizers%20%28e.g.%20Adam%29.%20However%2C%20learnable%0Aupdate%20rules%20can%20be%20costly%20and%20unstable%20to%20train%20and%20use.%20A%20simpler%20recently%0Aproposed%20approach%20to%20accelerate%20training%20is%20to%20use%20Adam%20for%20most%20of%20the%0Aoptimization%20steps%20and%20periodically%2C%20only%20every%20few%20steps%2C%20nowcast%20%28predict%0Afuture%29%20parameters.%20We%20improve%20this%20approach%20by%20Neuron%20interaction%20and%0ANowcasting%20%28NiNo%29%20networks.%20NiNo%20leverages%20neuron%20connectivity%20and%20graph%20neural%0Anetworks%20to%20more%20accurately%20nowcast%20parameters%20by%20learning%20in%20a%20supervised%20way%0Afrom%20a%20set%20of%20training%20trajectories%20over%20multiple%20tasks.%20We%20show%20that%20in%20some%0Anetworks%2C%20such%20as%20Transformers%2C%20neuron%20connectivity%20is%20non-trivial.%20By%0Aaccurately%20modeling%20neuron%20connectivity%2C%20we%20allow%20NiNo%20to%20accelerate%20Adam%0Atraining%20by%20up%20to%2050%5C%25%20in%20vision%20and%20language%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04434v1&entry.124074799=Read"},
{"title": "An overview of domain-specific foundation model: key technologies,\n  applications and challenges", "author": "Haolong Chen and Hanzhi Chen and Zijian Zhao and Kaifeng Han and Guangxu Zhu and Yichen Zhao and Ying Du and Wei Xu and Qingjiang Shi", "abstract": "  The impressive performance of ChatGPT and other foundation-model-based\nproducts in human language understanding has prompted both academia and\nindustry to explore how these models can be tailored for specific industries\nand application scenarios. This process, known as the customization of\ndomain-specific foundation models, addresses the limitations of general-purpose\nmodels, which may not fully capture the unique patterns and requirements of\ndomain-specific data. Despite its importance, there is a notable lack of\ncomprehensive overview papers on building domain-specific foundation models,\nwhile numerous resources exist for general-purpose models. To bridge this gap,\nthis article provides a timely and thorough overview of the methodology for\ncustomizing domain-specific foundation models. It introduces basic concepts,\noutlines the general architecture, and surveys key methods for constructing\ndomain-specific models. Furthermore, the article discusses various domains that\ncan benefit from these specialized models and highlights the challenges ahead.\nThrough this overview, we aim to offer valuable guidance and reference for\nresearchers and practitioners from diverse fields to develop their own\ncustomized foundation models.\n", "link": "http://arxiv.org/abs/2409.04267v1", "date": "2024-09-06", "relevancy": 2.1743, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5491}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20overview%20of%20domain-specific%20foundation%20model%3A%20key%20technologies%2C%0A%20%20applications%20and%20challenges&body=Title%3A%20An%20overview%20of%20domain-specific%20foundation%20model%3A%20key%20technologies%2C%0A%20%20applications%20and%20challenges%0AAuthor%3A%20Haolong%20Chen%20and%20Hanzhi%20Chen%20and%20Zijian%20Zhao%20and%20Kaifeng%20Han%20and%20Guangxu%20Zhu%20and%20Yichen%20Zhao%20and%20Ying%20Du%20and%20Wei%20Xu%20and%20Qingjiang%20Shi%0AAbstract%3A%20%20%20The%20impressive%20performance%20of%20ChatGPT%20and%20other%20foundation-model-based%0Aproducts%20in%20human%20language%20understanding%20has%20prompted%20both%20academia%20and%0Aindustry%20to%20explore%20how%20these%20models%20can%20be%20tailored%20for%20specific%20industries%0Aand%20application%20scenarios.%20This%20process%2C%20known%20as%20the%20customization%20of%0Adomain-specific%20foundation%20models%2C%20addresses%20the%20limitations%20of%20general-purpose%0Amodels%2C%20which%20may%20not%20fully%20capture%20the%20unique%20patterns%20and%20requirements%20of%0Adomain-specific%20data.%20Despite%20its%20importance%2C%20there%20is%20a%20notable%20lack%20of%0Acomprehensive%20overview%20papers%20on%20building%20domain-specific%20foundation%20models%2C%0Awhile%20numerous%20resources%20exist%20for%20general-purpose%20models.%20To%20bridge%20this%20gap%2C%0Athis%20article%20provides%20a%20timely%20and%20thorough%20overview%20of%20the%20methodology%20for%0Acustomizing%20domain-specific%20foundation%20models.%20It%20introduces%20basic%20concepts%2C%0Aoutlines%20the%20general%20architecture%2C%20and%20surveys%20key%20methods%20for%20constructing%0Adomain-specific%20models.%20Furthermore%2C%20the%20article%20discusses%20various%20domains%20that%0Acan%20benefit%20from%20these%20specialized%20models%20and%20highlights%20the%20challenges%20ahead.%0AThrough%20this%20overview%2C%20we%20aim%20to%20offer%20valuable%20guidance%20and%20reference%20for%0Aresearchers%20and%20practitioners%20from%20diverse%20fields%20to%20develop%20their%20own%0Acustomized%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520overview%2520of%2520domain-specific%2520foundation%2520model%253A%2520key%2520technologies%252C%250A%2520%2520applications%2520and%2520challenges%26entry.906535625%3DHaolong%2520Chen%2520and%2520Hanzhi%2520Chen%2520and%2520Zijian%2520Zhao%2520and%2520Kaifeng%2520Han%2520and%2520Guangxu%2520Zhu%2520and%2520Yichen%2520Zhao%2520and%2520Ying%2520Du%2520and%2520Wei%2520Xu%2520and%2520Qingjiang%2520Shi%26entry.1292438233%3D%2520%2520The%2520impressive%2520performance%2520of%2520ChatGPT%2520and%2520other%2520foundation-model-based%250Aproducts%2520in%2520human%2520language%2520understanding%2520has%2520prompted%2520both%2520academia%2520and%250Aindustry%2520to%2520explore%2520how%2520these%2520models%2520can%2520be%2520tailored%2520for%2520specific%2520industries%250Aand%2520application%2520scenarios.%2520This%2520process%252C%2520known%2520as%2520the%2520customization%2520of%250Adomain-specific%2520foundation%2520models%252C%2520addresses%2520the%2520limitations%2520of%2520general-purpose%250Amodels%252C%2520which%2520may%2520not%2520fully%2520capture%2520the%2520unique%2520patterns%2520and%2520requirements%2520of%250Adomain-specific%2520data.%2520Despite%2520its%2520importance%252C%2520there%2520is%2520a%2520notable%2520lack%2520of%250Acomprehensive%2520overview%2520papers%2520on%2520building%2520domain-specific%2520foundation%2520models%252C%250Awhile%2520numerous%2520resources%2520exist%2520for%2520general-purpose%2520models.%2520To%2520bridge%2520this%2520gap%252C%250Athis%2520article%2520provides%2520a%2520timely%2520and%2520thorough%2520overview%2520of%2520the%2520methodology%2520for%250Acustomizing%2520domain-specific%2520foundation%2520models.%2520It%2520introduces%2520basic%2520concepts%252C%250Aoutlines%2520the%2520general%2520architecture%252C%2520and%2520surveys%2520key%2520methods%2520for%2520constructing%250Adomain-specific%2520models.%2520Furthermore%252C%2520the%2520article%2520discusses%2520various%2520domains%2520that%250Acan%2520benefit%2520from%2520these%2520specialized%2520models%2520and%2520highlights%2520the%2520challenges%2520ahead.%250AThrough%2520this%2520overview%252C%2520we%2520aim%2520to%2520offer%2520valuable%2520guidance%2520and%2520reference%2520for%250Aresearchers%2520and%2520practitioners%2520from%2520diverse%2520fields%2520to%2520develop%2520their%2520own%250Acustomized%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20overview%20of%20domain-specific%20foundation%20model%3A%20key%20technologies%2C%0A%20%20applications%20and%20challenges&entry.906535625=Haolong%20Chen%20and%20Hanzhi%20Chen%20and%20Zijian%20Zhao%20and%20Kaifeng%20Han%20and%20Guangxu%20Zhu%20and%20Yichen%20Zhao%20and%20Ying%20Du%20and%20Wei%20Xu%20and%20Qingjiang%20Shi&entry.1292438233=%20%20The%20impressive%20performance%20of%20ChatGPT%20and%20other%20foundation-model-based%0Aproducts%20in%20human%20language%20understanding%20has%20prompted%20both%20academia%20and%0Aindustry%20to%20explore%20how%20these%20models%20can%20be%20tailored%20for%20specific%20industries%0Aand%20application%20scenarios.%20This%20process%2C%20known%20as%20the%20customization%20of%0Adomain-specific%20foundation%20models%2C%20addresses%20the%20limitations%20of%20general-purpose%0Amodels%2C%20which%20may%20not%20fully%20capture%20the%20unique%20patterns%20and%20requirements%20of%0Adomain-specific%20data.%20Despite%20its%20importance%2C%20there%20is%20a%20notable%20lack%20of%0Acomprehensive%20overview%20papers%20on%20building%20domain-specific%20foundation%20models%2C%0Awhile%20numerous%20resources%20exist%20for%20general-purpose%20models.%20To%20bridge%20this%20gap%2C%0Athis%20article%20provides%20a%20timely%20and%20thorough%20overview%20of%20the%20methodology%20for%0Acustomizing%20domain-specific%20foundation%20models.%20It%20introduces%20basic%20concepts%2C%0Aoutlines%20the%20general%20architecture%2C%20and%20surveys%20key%20methods%20for%20constructing%0Adomain-specific%20models.%20Furthermore%2C%20the%20article%20discusses%20various%20domains%20that%0Acan%20benefit%20from%20these%20specialized%20models%20and%20highlights%20the%20challenges%20ahead.%0AThrough%20this%20overview%2C%20we%20aim%20to%20offer%20valuable%20guidance%20and%20reference%20for%0Aresearchers%20and%20practitioners%20from%20diverse%20fields%20to%20develop%20their%20own%0Acustomized%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04267v1&entry.124074799=Read"},
{"title": "Cycle Pixel Difference Network for Crisp Edge Detection", "author": "Changsong Liu and Wei Zhang and Yanyan Liu and Mingyang Li and Wenlin Li and Yimeng Fan and Xiangnan Bai and Liang Zhangd", "abstract": "  Edge detection, as a fundamental task in computer vision, has garnered\nincreasing attention. The advent of deep learning has significantly advanced\nthis field. However, recent deep learning-based methods which rely on\nlarge-scale pre-trained weights cannot be trained from scratch, with very\nlimited research addressing this issue. This paper proposes a novel cycle pixel\ndifference convolution (CPDC), which effectively integrates image gradient\ninformation with modern convolution operations. Based on the CPDC, we develop a\nU-shape encoder-decoder model named CPD-Net, which is a purely end-to-end\nnetwork. Additionally, to address the issue of edge thickness produced by most\nexisting methods, we construct a multi-scale information enhancement module\n(MSEM) to enhance the discriminative ability of the model, thereby generating\ncrisp and clean contour maps. Comprehensive experiments conducted on three\nstandard benchmarks demonstrate that our method achieves competitive\nperformance on the BSDS500 dataset (ODS=0.813), NYUD-V2 (ODS=0.760), and BIPED\ndataset (ODS=0.898). Our approach provides a novel perspective for addressing\nthese challenges in edge detection.\n", "link": "http://arxiv.org/abs/2409.04272v1", "date": "2024-09-06", "relevancy": 2.169, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5629}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5483}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cycle%20Pixel%20Difference%20Network%20for%20Crisp%20Edge%20Detection&body=Title%3A%20Cycle%20Pixel%20Difference%20Network%20for%20Crisp%20Edge%20Detection%0AAuthor%3A%20Changsong%20Liu%20and%20Wei%20Zhang%20and%20Yanyan%20Liu%20and%20Mingyang%20Li%20and%20Wenlin%20Li%20and%20Yimeng%20Fan%20and%20Xiangnan%20Bai%20and%20Liang%20Zhangd%0AAbstract%3A%20%20%20Edge%20detection%2C%20as%20a%20fundamental%20task%20in%20computer%20vision%2C%20has%20garnered%0Aincreasing%20attention.%20The%20advent%20of%20deep%20learning%20has%20significantly%20advanced%0Athis%20field.%20However%2C%20recent%20deep%20learning-based%20methods%20which%20rely%20on%0Alarge-scale%20pre-trained%20weights%20cannot%20be%20trained%20from%20scratch%2C%20with%20very%0Alimited%20research%20addressing%20this%20issue.%20This%20paper%20proposes%20a%20novel%20cycle%20pixel%0Adifference%20convolution%20%28CPDC%29%2C%20which%20effectively%20integrates%20image%20gradient%0Ainformation%20with%20modern%20convolution%20operations.%20Based%20on%20the%20CPDC%2C%20we%20develop%20a%0AU-shape%20encoder-decoder%20model%20named%20CPD-Net%2C%20which%20is%20a%20purely%20end-to-end%0Anetwork.%20Additionally%2C%20to%20address%20the%20issue%20of%20edge%20thickness%20produced%20by%20most%0Aexisting%20methods%2C%20we%20construct%20a%20multi-scale%20information%20enhancement%20module%0A%28MSEM%29%20to%20enhance%20the%20discriminative%20ability%20of%20the%20model%2C%20thereby%20generating%0Acrisp%20and%20clean%20contour%20maps.%20Comprehensive%20experiments%20conducted%20on%20three%0Astandard%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20competitive%0Aperformance%20on%20the%20BSDS500%20dataset%20%28ODS%3D0.813%29%2C%20NYUD-V2%20%28ODS%3D0.760%29%2C%20and%20BIPED%0Adataset%20%28ODS%3D0.898%29.%20Our%20approach%20provides%20a%20novel%20perspective%20for%20addressing%0Athese%20challenges%20in%20edge%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCycle%2520Pixel%2520Difference%2520Network%2520for%2520Crisp%2520Edge%2520Detection%26entry.906535625%3DChangsong%2520Liu%2520and%2520Wei%2520Zhang%2520and%2520Yanyan%2520Liu%2520and%2520Mingyang%2520Li%2520and%2520Wenlin%2520Li%2520and%2520Yimeng%2520Fan%2520and%2520Xiangnan%2520Bai%2520and%2520Liang%2520Zhangd%26entry.1292438233%3D%2520%2520Edge%2520detection%252C%2520as%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%252C%2520has%2520garnered%250Aincreasing%2520attention.%2520The%2520advent%2520of%2520deep%2520learning%2520has%2520significantly%2520advanced%250Athis%2520field.%2520However%252C%2520recent%2520deep%2520learning-based%2520methods%2520which%2520rely%2520on%250Alarge-scale%2520pre-trained%2520weights%2520cannot%2520be%2520trained%2520from%2520scratch%252C%2520with%2520very%250Alimited%2520research%2520addressing%2520this%2520issue.%2520This%2520paper%2520proposes%2520a%2520novel%2520cycle%2520pixel%250Adifference%2520convolution%2520%2528CPDC%2529%252C%2520which%2520effectively%2520integrates%2520image%2520gradient%250Ainformation%2520with%2520modern%2520convolution%2520operations.%2520Based%2520on%2520the%2520CPDC%252C%2520we%2520develop%2520a%250AU-shape%2520encoder-decoder%2520model%2520named%2520CPD-Net%252C%2520which%2520is%2520a%2520purely%2520end-to-end%250Anetwork.%2520Additionally%252C%2520to%2520address%2520the%2520issue%2520of%2520edge%2520thickness%2520produced%2520by%2520most%250Aexisting%2520methods%252C%2520we%2520construct%2520a%2520multi-scale%2520information%2520enhancement%2520module%250A%2528MSEM%2529%2520to%2520enhance%2520the%2520discriminative%2520ability%2520of%2520the%2520model%252C%2520thereby%2520generating%250Acrisp%2520and%2520clean%2520contour%2520maps.%2520Comprehensive%2520experiments%2520conducted%2520on%2520three%250Astandard%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520competitive%250Aperformance%2520on%2520the%2520BSDS500%2520dataset%2520%2528ODS%253D0.813%2529%252C%2520NYUD-V2%2520%2528ODS%253D0.760%2529%252C%2520and%2520BIPED%250Adataset%2520%2528ODS%253D0.898%2529.%2520Our%2520approach%2520provides%2520a%2520novel%2520perspective%2520for%2520addressing%250Athese%2520challenges%2520in%2520edge%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cycle%20Pixel%20Difference%20Network%20for%20Crisp%20Edge%20Detection&entry.906535625=Changsong%20Liu%20and%20Wei%20Zhang%20and%20Yanyan%20Liu%20and%20Mingyang%20Li%20and%20Wenlin%20Li%20and%20Yimeng%20Fan%20and%20Xiangnan%20Bai%20and%20Liang%20Zhangd&entry.1292438233=%20%20Edge%20detection%2C%20as%20a%20fundamental%20task%20in%20computer%20vision%2C%20has%20garnered%0Aincreasing%20attention.%20The%20advent%20of%20deep%20learning%20has%20significantly%20advanced%0Athis%20field.%20However%2C%20recent%20deep%20learning-based%20methods%20which%20rely%20on%0Alarge-scale%20pre-trained%20weights%20cannot%20be%20trained%20from%20scratch%2C%20with%20very%0Alimited%20research%20addressing%20this%20issue.%20This%20paper%20proposes%20a%20novel%20cycle%20pixel%0Adifference%20convolution%20%28CPDC%29%2C%20which%20effectively%20integrates%20image%20gradient%0Ainformation%20with%20modern%20convolution%20operations.%20Based%20on%20the%20CPDC%2C%20we%20develop%20a%0AU-shape%20encoder-decoder%20model%20named%20CPD-Net%2C%20which%20is%20a%20purely%20end-to-end%0Anetwork.%20Additionally%2C%20to%20address%20the%20issue%20of%20edge%20thickness%20produced%20by%20most%0Aexisting%20methods%2C%20we%20construct%20a%20multi-scale%20information%20enhancement%20module%0A%28MSEM%29%20to%20enhance%20the%20discriminative%20ability%20of%20the%20model%2C%20thereby%20generating%0Acrisp%20and%20clean%20contour%20maps.%20Comprehensive%20experiments%20conducted%20on%20three%0Astandard%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20competitive%0Aperformance%20on%20the%20BSDS500%20dataset%20%28ODS%3D0.813%29%2C%20NYUD-V2%20%28ODS%3D0.760%29%2C%20and%20BIPED%0Adataset%20%28ODS%3D0.898%29.%20Our%20approach%20provides%20a%20novel%20perspective%20for%20addressing%0Athese%20challenges%20in%20edge%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04272v1&entry.124074799=Read"},
{"title": "Many-Worlds Inverse Rendering", "author": "Ziyi Zhang and Nicolas Roussel and Wenzel Jakob", "abstract": "  Discontinuous visibility changes remain a major bottleneck when optimizing\nsurfaces within a physically-based inverse renderer. Many previous works have\nproposed sophisticated algorithms and data structures to sample visibility\nsilhouettes more efficiently.\n  Our work presents another solution: instead of differentiating a tentative\nsurface locally, we differentiate a volumetric perturbation of a surface. We\nrefer this as a many-worlds representation because it models a non-interacting\nsuperposition of conflicting explanations (worlds) of the input dataset. Each\nworld is optically isolated from others, leading to a new transport law that\ndistinguishes our method from prior work based on exponential random media.\n  The resulting Monte Carlo algorithm is simpler and more efficient than prior\nmethods. We demonstrate that our method promotes rapid convergence, both in\nterms of the total iteration count and the cost per iteration.\n", "link": "http://arxiv.org/abs/2408.16005v3", "date": "2024-09-06", "relevancy": 2.1354, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5408}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5408}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Many-Worlds%20Inverse%20Rendering&body=Title%3A%20Many-Worlds%20Inverse%20Rendering%0AAuthor%3A%20Ziyi%20Zhang%20and%20Nicolas%20Roussel%20and%20Wenzel%20Jakob%0AAbstract%3A%20%20%20Discontinuous%20visibility%20changes%20remain%20a%20major%20bottleneck%20when%20optimizing%0Asurfaces%20within%20a%20physically-based%20inverse%20renderer.%20Many%20previous%20works%20have%0Aproposed%20sophisticated%20algorithms%20and%20data%20structures%20to%20sample%20visibility%0Asilhouettes%20more%20efficiently.%0A%20%20Our%20work%20presents%20another%20solution%3A%20instead%20of%20differentiating%20a%20tentative%0Asurface%20locally%2C%20we%20differentiate%20a%20volumetric%20perturbation%20of%20a%20surface.%20We%0Arefer%20this%20as%20a%20many-worlds%20representation%20because%20it%20models%20a%20non-interacting%0Asuperposition%20of%20conflicting%20explanations%20%28worlds%29%20of%20the%20input%20dataset.%20Each%0Aworld%20is%20optically%20isolated%20from%20others%2C%20leading%20to%20a%20new%20transport%20law%20that%0Adistinguishes%20our%20method%20from%20prior%20work%20based%20on%20exponential%20random%20media.%0A%20%20The%20resulting%20Monte%20Carlo%20algorithm%20is%20simpler%20and%20more%20efficient%20than%20prior%0Amethods.%20We%20demonstrate%20that%20our%20method%20promotes%20rapid%20convergence%2C%20both%20in%0Aterms%20of%20the%20total%20iteration%20count%20and%20the%20cost%20per%20iteration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16005v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMany-Worlds%2520Inverse%2520Rendering%26entry.906535625%3DZiyi%2520Zhang%2520and%2520Nicolas%2520Roussel%2520and%2520Wenzel%2520Jakob%26entry.1292438233%3D%2520%2520Discontinuous%2520visibility%2520changes%2520remain%2520a%2520major%2520bottleneck%2520when%2520optimizing%250Asurfaces%2520within%2520a%2520physically-based%2520inverse%2520renderer.%2520Many%2520previous%2520works%2520have%250Aproposed%2520sophisticated%2520algorithms%2520and%2520data%2520structures%2520to%2520sample%2520visibility%250Asilhouettes%2520more%2520efficiently.%250A%2520%2520Our%2520work%2520presents%2520another%2520solution%253A%2520instead%2520of%2520differentiating%2520a%2520tentative%250Asurface%2520locally%252C%2520we%2520differentiate%2520a%2520volumetric%2520perturbation%2520of%2520a%2520surface.%2520We%250Arefer%2520this%2520as%2520a%2520many-worlds%2520representation%2520because%2520it%2520models%2520a%2520non-interacting%250Asuperposition%2520of%2520conflicting%2520explanations%2520%2528worlds%2529%2520of%2520the%2520input%2520dataset.%2520Each%250Aworld%2520is%2520optically%2520isolated%2520from%2520others%252C%2520leading%2520to%2520a%2520new%2520transport%2520law%2520that%250Adistinguishes%2520our%2520method%2520from%2520prior%2520work%2520based%2520on%2520exponential%2520random%2520media.%250A%2520%2520The%2520resulting%2520Monte%2520Carlo%2520algorithm%2520is%2520simpler%2520and%2520more%2520efficient%2520than%2520prior%250Amethods.%2520We%2520demonstrate%2520that%2520our%2520method%2520promotes%2520rapid%2520convergence%252C%2520both%2520in%250Aterms%2520of%2520the%2520total%2520iteration%2520count%2520and%2520the%2520cost%2520per%2520iteration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16005v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Many-Worlds%20Inverse%20Rendering&entry.906535625=Ziyi%20Zhang%20and%20Nicolas%20Roussel%20and%20Wenzel%20Jakob&entry.1292438233=%20%20Discontinuous%20visibility%20changes%20remain%20a%20major%20bottleneck%20when%20optimizing%0Asurfaces%20within%20a%20physically-based%20inverse%20renderer.%20Many%20previous%20works%20have%0Aproposed%20sophisticated%20algorithms%20and%20data%20structures%20to%20sample%20visibility%0Asilhouettes%20more%20efficiently.%0A%20%20Our%20work%20presents%20another%20solution%3A%20instead%20of%20differentiating%20a%20tentative%0Asurface%20locally%2C%20we%20differentiate%20a%20volumetric%20perturbation%20of%20a%20surface.%20We%0Arefer%20this%20as%20a%20many-worlds%20representation%20because%20it%20models%20a%20non-interacting%0Asuperposition%20of%20conflicting%20explanations%20%28worlds%29%20of%20the%20input%20dataset.%20Each%0Aworld%20is%20optically%20isolated%20from%20others%2C%20leading%20to%20a%20new%20transport%20law%20that%0Adistinguishes%20our%20method%20from%20prior%20work%20based%20on%20exponential%20random%20media.%0A%20%20The%20resulting%20Monte%20Carlo%20algorithm%20is%20simpler%20and%20more%20efficient%20than%20prior%0Amethods.%20We%20demonstrate%20that%20our%20method%20promotes%20rapid%20convergence%2C%20both%20in%0Aterms%20of%20the%20total%20iteration%20count%20and%20the%20cost%20per%20iteration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16005v3&entry.124074799=Read"},
{"title": "AttentionX: Exploiting Consensus Discrepancy In Attention from A\n  Distributed Optimization Perspective", "author": "Guoqiang Zhang and Richard Heusdens", "abstract": "  In this paper, we extend the standard Attention in transformer by exploiting\nthe consensus discrepancy from a distributed optimization perspective, referred\nto as AttentionX. It is noted that %the popular distributed optimization\nalgorithm \\cite{Boyd11ADMM} and the primal-dual method of multipliers (PDMM)\n\\cite{Zhang16PDMM} is designed to iteratively solve a broad class of\ndistributed optimization problems over a pear-to-pear (P2P) network, where\nneighbouring nodes gradually reach consensus as specified by predefined linear\nedge-constraints in the optimization process. In particular, at each iteration\nof PDMM, each node in a network first performs information-gathering from\nneighbours and then performs local information-fusion. From a high-level point\nof view, the $KQ$-softmax-based weighted summation of $V$-representations in\nAttention corresponds information-gathering from neighbours while the\nfeature-processing via the feed-forward network (FFN) in transformer\ncorresponds to local information fusion. PDMM exploits the Lagrangian\nmultipliers to capture the historical consensus discrepancy in the form of\nresidual errors of the linear edge-constraints, which plays a crucial role for\nthe algorithm to converge. Inspired by PDMM, we propose AttentionX to\nincorporate the consensus discrepancy in the output update-expression of the\nstandard Attention. The consensus discrepancy in AttentionX refers to the\ndifference between the weighted summation of $V$-representations and scaled\n$V$-representions themselves. Experiments on ViT and nanoGPT show promising\nperformance.\n", "link": "http://arxiv.org/abs/2409.04275v1", "date": "2024-09-06", "relevancy": 2.1223, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5624}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5239}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttentionX%3A%20Exploiting%20Consensus%20Discrepancy%20In%20Attention%20from%20A%0A%20%20Distributed%20Optimization%20Perspective&body=Title%3A%20AttentionX%3A%20Exploiting%20Consensus%20Discrepancy%20In%20Attention%20from%20A%0A%20%20Distributed%20Optimization%20Perspective%0AAuthor%3A%20Guoqiang%20Zhang%20and%20Richard%20Heusdens%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20extend%20the%20standard%20Attention%20in%20transformer%20by%20exploiting%0Athe%20consensus%20discrepancy%20from%20a%20distributed%20optimization%20perspective%2C%20referred%0Ato%20as%20AttentionX.%20It%20is%20noted%20that%20%25the%20popular%20distributed%20optimization%0Aalgorithm%20%5Ccite%7BBoyd11ADMM%7D%20and%20the%20primal-dual%20method%20of%20multipliers%20%28PDMM%29%0A%5Ccite%7BZhang16PDMM%7D%20is%20designed%20to%20iteratively%20solve%20a%20broad%20class%20of%0Adistributed%20optimization%20problems%20over%20a%20pear-to-pear%20%28P2P%29%20network%2C%20where%0Aneighbouring%20nodes%20gradually%20reach%20consensus%20as%20specified%20by%20predefined%20linear%0Aedge-constraints%20in%20the%20optimization%20process.%20In%20particular%2C%20at%20each%20iteration%0Aof%20PDMM%2C%20each%20node%20in%20a%20network%20first%20performs%20information-gathering%20from%0Aneighbours%20and%20then%20performs%20local%20information-fusion.%20From%20a%20high-level%20point%0Aof%20view%2C%20the%20%24KQ%24-softmax-based%20weighted%20summation%20of%20%24V%24-representations%20in%0AAttention%20corresponds%20information-gathering%20from%20neighbours%20while%20the%0Afeature-processing%20via%20the%20feed-forward%20network%20%28FFN%29%20in%20transformer%0Acorresponds%20to%20local%20information%20fusion.%20PDMM%20exploits%20the%20Lagrangian%0Amultipliers%20to%20capture%20the%20historical%20consensus%20discrepancy%20in%20the%20form%20of%0Aresidual%20errors%20of%20the%20linear%20edge-constraints%2C%20which%20plays%20a%20crucial%20role%20for%0Athe%20algorithm%20to%20converge.%20Inspired%20by%20PDMM%2C%20we%20propose%20AttentionX%20to%0Aincorporate%20the%20consensus%20discrepancy%20in%20the%20output%20update-expression%20of%20the%0Astandard%20Attention.%20The%20consensus%20discrepancy%20in%20AttentionX%20refers%20to%20the%0Adifference%20between%20the%20weighted%20summation%20of%20%24V%24-representations%20and%20scaled%0A%24V%24-representions%20themselves.%20Experiments%20on%20ViT%20and%20nanoGPT%20show%20promising%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttentionX%253A%2520Exploiting%2520Consensus%2520Discrepancy%2520In%2520Attention%2520from%2520A%250A%2520%2520Distributed%2520Optimization%2520Perspective%26entry.906535625%3DGuoqiang%2520Zhang%2520and%2520Richard%2520Heusdens%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520standard%2520Attention%2520in%2520transformer%2520by%2520exploiting%250Athe%2520consensus%2520discrepancy%2520from%2520a%2520distributed%2520optimization%2520perspective%252C%2520referred%250Ato%2520as%2520AttentionX.%2520It%2520is%2520noted%2520that%2520%2525the%2520popular%2520distributed%2520optimization%250Aalgorithm%2520%255Ccite%257BBoyd11ADMM%257D%2520and%2520the%2520primal-dual%2520method%2520of%2520multipliers%2520%2528PDMM%2529%250A%255Ccite%257BZhang16PDMM%257D%2520is%2520designed%2520to%2520iteratively%2520solve%2520a%2520broad%2520class%2520of%250Adistributed%2520optimization%2520problems%2520over%2520a%2520pear-to-pear%2520%2528P2P%2529%2520network%252C%2520where%250Aneighbouring%2520nodes%2520gradually%2520reach%2520consensus%2520as%2520specified%2520by%2520predefined%2520linear%250Aedge-constraints%2520in%2520the%2520optimization%2520process.%2520In%2520particular%252C%2520at%2520each%2520iteration%250Aof%2520PDMM%252C%2520each%2520node%2520in%2520a%2520network%2520first%2520performs%2520information-gathering%2520from%250Aneighbours%2520and%2520then%2520performs%2520local%2520information-fusion.%2520From%2520a%2520high-level%2520point%250Aof%2520view%252C%2520the%2520%2524KQ%2524-softmax-based%2520weighted%2520summation%2520of%2520%2524V%2524-representations%2520in%250AAttention%2520corresponds%2520information-gathering%2520from%2520neighbours%2520while%2520the%250Afeature-processing%2520via%2520the%2520feed-forward%2520network%2520%2528FFN%2529%2520in%2520transformer%250Acorresponds%2520to%2520local%2520information%2520fusion.%2520PDMM%2520exploits%2520the%2520Lagrangian%250Amultipliers%2520to%2520capture%2520the%2520historical%2520consensus%2520discrepancy%2520in%2520the%2520form%2520of%250Aresidual%2520errors%2520of%2520the%2520linear%2520edge-constraints%252C%2520which%2520plays%2520a%2520crucial%2520role%2520for%250Athe%2520algorithm%2520to%2520converge.%2520Inspired%2520by%2520PDMM%252C%2520we%2520propose%2520AttentionX%2520to%250Aincorporate%2520the%2520consensus%2520discrepancy%2520in%2520the%2520output%2520update-expression%2520of%2520the%250Astandard%2520Attention.%2520The%2520consensus%2520discrepancy%2520in%2520AttentionX%2520refers%2520to%2520the%250Adifference%2520between%2520the%2520weighted%2520summation%2520of%2520%2524V%2524-representations%2520and%2520scaled%250A%2524V%2524-representions%2520themselves.%2520Experiments%2520on%2520ViT%2520and%2520nanoGPT%2520show%2520promising%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttentionX%3A%20Exploiting%20Consensus%20Discrepancy%20In%20Attention%20from%20A%0A%20%20Distributed%20Optimization%20Perspective&entry.906535625=Guoqiang%20Zhang%20and%20Richard%20Heusdens&entry.1292438233=%20%20In%20this%20paper%2C%20we%20extend%20the%20standard%20Attention%20in%20transformer%20by%20exploiting%0Athe%20consensus%20discrepancy%20from%20a%20distributed%20optimization%20perspective%2C%20referred%0Ato%20as%20AttentionX.%20It%20is%20noted%20that%20%25the%20popular%20distributed%20optimization%0Aalgorithm%20%5Ccite%7BBoyd11ADMM%7D%20and%20the%20primal-dual%20method%20of%20multipliers%20%28PDMM%29%0A%5Ccite%7BZhang16PDMM%7D%20is%20designed%20to%20iteratively%20solve%20a%20broad%20class%20of%0Adistributed%20optimization%20problems%20over%20a%20pear-to-pear%20%28P2P%29%20network%2C%20where%0Aneighbouring%20nodes%20gradually%20reach%20consensus%20as%20specified%20by%20predefined%20linear%0Aedge-constraints%20in%20the%20optimization%20process.%20In%20particular%2C%20at%20each%20iteration%0Aof%20PDMM%2C%20each%20node%20in%20a%20network%20first%20performs%20information-gathering%20from%0Aneighbours%20and%20then%20performs%20local%20information-fusion.%20From%20a%20high-level%20point%0Aof%20view%2C%20the%20%24KQ%24-softmax-based%20weighted%20summation%20of%20%24V%24-representations%20in%0AAttention%20corresponds%20information-gathering%20from%20neighbours%20while%20the%0Afeature-processing%20via%20the%20feed-forward%20network%20%28FFN%29%20in%20transformer%0Acorresponds%20to%20local%20information%20fusion.%20PDMM%20exploits%20the%20Lagrangian%0Amultipliers%20to%20capture%20the%20historical%20consensus%20discrepancy%20in%20the%20form%20of%0Aresidual%20errors%20of%20the%20linear%20edge-constraints%2C%20which%20plays%20a%20crucial%20role%20for%0Athe%20algorithm%20to%20converge.%20Inspired%20by%20PDMM%2C%20we%20propose%20AttentionX%20to%0Aincorporate%20the%20consensus%20discrepancy%20in%20the%20output%20update-expression%20of%20the%0Astandard%20Attention.%20The%20consensus%20discrepancy%20in%20AttentionX%20refers%20to%20the%0Adifference%20between%20the%20weighted%20summation%20of%20%24V%24-representations%20and%20scaled%0A%24V%24-representions%20themselves.%20Experiments%20on%20ViT%20and%20nanoGPT%20show%20promising%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04275v1&entry.124074799=Read"},
{"title": "A Black-Box Physics-Informed Estimator based on Gaussian Process\n  Regression for Robot Inverse Dynamics Identification", "author": "Giulio Giacomuzzos and Ruggero Carli and Diego Romeres and Alberto Dalla Libera", "abstract": "  Learning the inverse dynamics of robots directly from data, adopting a\nblack-box approach, is interesting for several real-world scenarios where\nlimited knowledge about the system is available. In this paper, we propose a\nblack-box model based on Gaussian Process (GP) Regression for the\nidentification of the inverse dynamics of robotic manipulators. The proposed\nmodel relies on a novel multidimensional kernel, called \\textit{Lagrangian\nInspired Polynomial} (\\kernelInitials{}) kernel. The \\kernelInitials{} kernel\nis based on two main ideas. First, instead of directly modeling the inverse\ndynamics components, we model as GPs the kinetic and potential energy of the\nsystem. The GP prior on the inverse dynamics components is derived from those\non the energies by applying the properties of GPs under linear operators.\nSecond, as regards the energy prior definition, we prove a polynomial structure\nof the kinetic and potential energy, and we derive a polynomial kernel that\nencodes this property. As a consequence, the proposed model allows also to\nestimate the kinetic and potential energy without requiring any label on these\nquantities. Results on simulation and on two real robotic manipulators, namely\na 7 DOF Franka Emika Panda, and a 6 DOF MELFA RV4FL, show that the proposed\nmodel outperforms state-of-the-art black-box estimators based both on Gaussian\nProcesses and Neural Networks in terms of accuracy, generality and data\nefficiency. The experiments on the MELFA robot also demonstrate that our\napproach achieves performance comparable to fine-tuned model-based estimators,\ndespite requiring less prior information.\n", "link": "http://arxiv.org/abs/2310.06585v2", "date": "2024-09-06", "relevancy": 2.0885, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6053}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5077}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Black-Box%20Physics-Informed%20Estimator%20based%20on%20Gaussian%20Process%0A%20%20Regression%20for%20Robot%20Inverse%20Dynamics%20Identification&body=Title%3A%20A%20Black-Box%20Physics-Informed%20Estimator%20based%20on%20Gaussian%20Process%0A%20%20Regression%20for%20Robot%20Inverse%20Dynamics%20Identification%0AAuthor%3A%20Giulio%20Giacomuzzos%20and%20Ruggero%20Carli%20and%20Diego%20Romeres%20and%20Alberto%20Dalla%20Libera%0AAbstract%3A%20%20%20Learning%20the%20inverse%20dynamics%20of%20robots%20directly%20from%20data%2C%20adopting%20a%0Ablack-box%20approach%2C%20is%20interesting%20for%20several%20real-world%20scenarios%20where%0Alimited%20knowledge%20about%20the%20system%20is%20available.%20In%20this%20paper%2C%20we%20propose%20a%0Ablack-box%20model%20based%20on%20Gaussian%20Process%20%28GP%29%20Regression%20for%20the%0Aidentification%20of%20the%20inverse%20dynamics%20of%20robotic%20manipulators.%20The%20proposed%0Amodel%20relies%20on%20a%20novel%20multidimensional%20kernel%2C%20called%20%5Ctextit%7BLagrangian%0AInspired%20Polynomial%7D%20%28%5CkernelInitials%7B%7D%29%20kernel.%20The%20%5CkernelInitials%7B%7D%20kernel%0Ais%20based%20on%20two%20main%20ideas.%20First%2C%20instead%20of%20directly%20modeling%20the%20inverse%0Adynamics%20components%2C%20we%20model%20as%20GPs%20the%20kinetic%20and%20potential%20energy%20of%20the%0Asystem.%20The%20GP%20prior%20on%20the%20inverse%20dynamics%20components%20is%20derived%20from%20those%0Aon%20the%20energies%20by%20applying%20the%20properties%20of%20GPs%20under%20linear%20operators.%0ASecond%2C%20as%20regards%20the%20energy%20prior%20definition%2C%20we%20prove%20a%20polynomial%20structure%0Aof%20the%20kinetic%20and%20potential%20energy%2C%20and%20we%20derive%20a%20polynomial%20kernel%20that%0Aencodes%20this%20property.%20As%20a%20consequence%2C%20the%20proposed%20model%20allows%20also%20to%0Aestimate%20the%20kinetic%20and%20potential%20energy%20without%20requiring%20any%20label%20on%20these%0Aquantities.%20Results%20on%20simulation%20and%20on%20two%20real%20robotic%20manipulators%2C%20namely%0Aa%207%20DOF%20Franka%20Emika%20Panda%2C%20and%20a%206%20DOF%20MELFA%20RV4FL%2C%20show%20that%20the%20proposed%0Amodel%20outperforms%20state-of-the-art%20black-box%20estimators%20based%20both%20on%20Gaussian%0AProcesses%20and%20Neural%20Networks%20in%20terms%20of%20accuracy%2C%20generality%20and%20data%0Aefficiency.%20The%20experiments%20on%20the%20MELFA%20robot%20also%20demonstrate%20that%20our%0Aapproach%20achieves%20performance%20comparable%20to%20fine-tuned%20model-based%20estimators%2C%0Adespite%20requiring%20less%20prior%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Black-Box%2520Physics-Informed%2520Estimator%2520based%2520on%2520Gaussian%2520Process%250A%2520%2520Regression%2520for%2520Robot%2520Inverse%2520Dynamics%2520Identification%26entry.906535625%3DGiulio%2520Giacomuzzos%2520and%2520Ruggero%2520Carli%2520and%2520Diego%2520Romeres%2520and%2520Alberto%2520Dalla%2520Libera%26entry.1292438233%3D%2520%2520Learning%2520the%2520inverse%2520dynamics%2520of%2520robots%2520directly%2520from%2520data%252C%2520adopting%2520a%250Ablack-box%2520approach%252C%2520is%2520interesting%2520for%2520several%2520real-world%2520scenarios%2520where%250Alimited%2520knowledge%2520about%2520the%2520system%2520is%2520available.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Ablack-box%2520model%2520based%2520on%2520Gaussian%2520Process%2520%2528GP%2529%2520Regression%2520for%2520the%250Aidentification%2520of%2520the%2520inverse%2520dynamics%2520of%2520robotic%2520manipulators.%2520The%2520proposed%250Amodel%2520relies%2520on%2520a%2520novel%2520multidimensional%2520kernel%252C%2520called%2520%255Ctextit%257BLagrangian%250AInspired%2520Polynomial%257D%2520%2528%255CkernelInitials%257B%257D%2529%2520kernel.%2520The%2520%255CkernelInitials%257B%257D%2520kernel%250Ais%2520based%2520on%2520two%2520main%2520ideas.%2520First%252C%2520instead%2520of%2520directly%2520modeling%2520the%2520inverse%250Adynamics%2520components%252C%2520we%2520model%2520as%2520GPs%2520the%2520kinetic%2520and%2520potential%2520energy%2520of%2520the%250Asystem.%2520The%2520GP%2520prior%2520on%2520the%2520inverse%2520dynamics%2520components%2520is%2520derived%2520from%2520those%250Aon%2520the%2520energies%2520by%2520applying%2520the%2520properties%2520of%2520GPs%2520under%2520linear%2520operators.%250ASecond%252C%2520as%2520regards%2520the%2520energy%2520prior%2520definition%252C%2520we%2520prove%2520a%2520polynomial%2520structure%250Aof%2520the%2520kinetic%2520and%2520potential%2520energy%252C%2520and%2520we%2520derive%2520a%2520polynomial%2520kernel%2520that%250Aencodes%2520this%2520property.%2520As%2520a%2520consequence%252C%2520the%2520proposed%2520model%2520allows%2520also%2520to%250Aestimate%2520the%2520kinetic%2520and%2520potential%2520energy%2520without%2520requiring%2520any%2520label%2520on%2520these%250Aquantities.%2520Results%2520on%2520simulation%2520and%2520on%2520two%2520real%2520robotic%2520manipulators%252C%2520namely%250Aa%25207%2520DOF%2520Franka%2520Emika%2520Panda%252C%2520and%2520a%25206%2520DOF%2520MELFA%2520RV4FL%252C%2520show%2520that%2520the%2520proposed%250Amodel%2520outperforms%2520state-of-the-art%2520black-box%2520estimators%2520based%2520both%2520on%2520Gaussian%250AProcesses%2520and%2520Neural%2520Networks%2520in%2520terms%2520of%2520accuracy%252C%2520generality%2520and%2520data%250Aefficiency.%2520The%2520experiments%2520on%2520the%2520MELFA%2520robot%2520also%2520demonstrate%2520that%2520our%250Aapproach%2520achieves%2520performance%2520comparable%2520to%2520fine-tuned%2520model-based%2520estimators%252C%250Adespite%2520requiring%2520less%2520prior%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.06585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Black-Box%20Physics-Informed%20Estimator%20based%20on%20Gaussian%20Process%0A%20%20Regression%20for%20Robot%20Inverse%20Dynamics%20Identification&entry.906535625=Giulio%20Giacomuzzos%20and%20Ruggero%20Carli%20and%20Diego%20Romeres%20and%20Alberto%20Dalla%20Libera&entry.1292438233=%20%20Learning%20the%20inverse%20dynamics%20of%20robots%20directly%20from%20data%2C%20adopting%20a%0Ablack-box%20approach%2C%20is%20interesting%20for%20several%20real-world%20scenarios%20where%0Alimited%20knowledge%20about%20the%20system%20is%20available.%20In%20this%20paper%2C%20we%20propose%20a%0Ablack-box%20model%20based%20on%20Gaussian%20Process%20%28GP%29%20Regression%20for%20the%0Aidentification%20of%20the%20inverse%20dynamics%20of%20robotic%20manipulators.%20The%20proposed%0Amodel%20relies%20on%20a%20novel%20multidimensional%20kernel%2C%20called%20%5Ctextit%7BLagrangian%0AInspired%20Polynomial%7D%20%28%5CkernelInitials%7B%7D%29%20kernel.%20The%20%5CkernelInitials%7B%7D%20kernel%0Ais%20based%20on%20two%20main%20ideas.%20First%2C%20instead%20of%20directly%20modeling%20the%20inverse%0Adynamics%20components%2C%20we%20model%20as%20GPs%20the%20kinetic%20and%20potential%20energy%20of%20the%0Asystem.%20The%20GP%20prior%20on%20the%20inverse%20dynamics%20components%20is%20derived%20from%20those%0Aon%20the%20energies%20by%20applying%20the%20properties%20of%20GPs%20under%20linear%20operators.%0ASecond%2C%20as%20regards%20the%20energy%20prior%20definition%2C%20we%20prove%20a%20polynomial%20structure%0Aof%20the%20kinetic%20and%20potential%20energy%2C%20and%20we%20derive%20a%20polynomial%20kernel%20that%0Aencodes%20this%20property.%20As%20a%20consequence%2C%20the%20proposed%20model%20allows%20also%20to%0Aestimate%20the%20kinetic%20and%20potential%20energy%20without%20requiring%20any%20label%20on%20these%0Aquantities.%20Results%20on%20simulation%20and%20on%20two%20real%20robotic%20manipulators%2C%20namely%0Aa%207%20DOF%20Franka%20Emika%20Panda%2C%20and%20a%206%20DOF%20MELFA%20RV4FL%2C%20show%20that%20the%20proposed%0Amodel%20outperforms%20state-of-the-art%20black-box%20estimators%20based%20both%20on%20Gaussian%0AProcesses%20and%20Neural%20Networks%20in%20terms%20of%20accuracy%2C%20generality%20and%20data%0Aefficiency.%20The%20experiments%20on%20the%20MELFA%20robot%20also%20demonstrate%20that%20our%0Aapproach%20achieves%20performance%20comparable%20to%20fine-tuned%20model-based%20estimators%2C%0Adespite%20requiring%20less%20prior%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06585v2&entry.124074799=Read"},
{"title": "LMFLOSS: A Hybrid Loss For Imbalanced Medical Image Classification", "author": "Abu Adnan Sadi and Labib Chowdhury and Nusrat Jahan and Mohammad Newaz Sharif Rafi and Radeya Chowdhury and Faisal Ahamed Khan and Nabeel Mohammed", "abstract": "  With advances in digital technology, the classification of medical images has\nbecome a crucial step for image-based clinical decision support systems.\nAutomatic medical image classification represents a pivotal domain where the\nuse of AI holds the potential to create a significant social impact. However,\nseveral challenges act as obstacles to the development of practical and\neffective solutions. One of these challenges is the prevalent class imbalance\nproblem in most medical imaging datasets. As a result, existing AI techniques,\nparticularly deep-learning-based methodologies, often underperform in such\nscenarios. In this study, we propose a novel framework called Large Margin\naware Focal (LMF) loss to mitigate the class imbalance problem in medical\nimaging. The LMF loss represents a linear combination of two loss functions\noptimized by two hyperparameters. This framework harnesses the distinct\ncharacteristics of both loss functions by enforcing wider margins for minority\nclasses while simultaneously emphasizing challenging samples found in the\ndatasets. We perform rigorous experiments on three neural network architectures\nand with four medical imaging datasets. We provide empirical evidence that our\nproposed framework consistently outperforms other baseline methods, showing an\nimprovement of 2%-9% in macro-f1 scores. Through class-wise analysis of f1\nscores, we also demonstrate how the proposed framework can significantly\nimprove performance for minority classes. The results of our experiments show\nthat our proposed framework can perform consistently well across different\narchitectures and datasets. Overall, our study demonstrates a simple and\neffective approach to addressing the class imbalance problem in medical imaging\ndatasets. We hope our work will inspire new research toward a more generalized\napproach to medical image classification.\n", "link": "http://arxiv.org/abs/2212.12741v2", "date": "2024-09-06", "relevancy": 2.0746, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5275}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.523}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMFLOSS%3A%20A%20Hybrid%20Loss%20For%20Imbalanced%20Medical%20Image%20Classification&body=Title%3A%20LMFLOSS%3A%20A%20Hybrid%20Loss%20For%20Imbalanced%20Medical%20Image%20Classification%0AAuthor%3A%20Abu%20Adnan%20Sadi%20and%20Labib%20Chowdhury%20and%20Nusrat%20Jahan%20and%20Mohammad%20Newaz%20Sharif%20Rafi%20and%20Radeya%20Chowdhury%20and%20Faisal%20Ahamed%20Khan%20and%20Nabeel%20Mohammed%0AAbstract%3A%20%20%20With%20advances%20in%20digital%20technology%2C%20the%20classification%20of%20medical%20images%20has%0Abecome%20a%20crucial%20step%20for%20image-based%20clinical%20decision%20support%20systems.%0AAutomatic%20medical%20image%20classification%20represents%20a%20pivotal%20domain%20where%20the%0Ause%20of%20AI%20holds%20the%20potential%20to%20create%20a%20significant%20social%20impact.%20However%2C%0Aseveral%20challenges%20act%20as%20obstacles%20to%20the%20development%20of%20practical%20and%0Aeffective%20solutions.%20One%20of%20these%20challenges%20is%20the%20prevalent%20class%20imbalance%0Aproblem%20in%20most%20medical%20imaging%20datasets.%20As%20a%20result%2C%20existing%20AI%20techniques%2C%0Aparticularly%20deep-learning-based%20methodologies%2C%20often%20underperform%20in%20such%0Ascenarios.%20In%20this%20study%2C%20we%20propose%20a%20novel%20framework%20called%20Large%20Margin%0Aaware%20Focal%20%28LMF%29%20loss%20to%20mitigate%20the%20class%20imbalance%20problem%20in%20medical%0Aimaging.%20The%20LMF%20loss%20represents%20a%20linear%20combination%20of%20two%20loss%20functions%0Aoptimized%20by%20two%20hyperparameters.%20This%20framework%20harnesses%20the%20distinct%0Acharacteristics%20of%20both%20loss%20functions%20by%20enforcing%20wider%20margins%20for%20minority%0Aclasses%20while%20simultaneously%20emphasizing%20challenging%20samples%20found%20in%20the%0Adatasets.%20We%20perform%20rigorous%20experiments%20on%20three%20neural%20network%20architectures%0Aand%20with%20four%20medical%20imaging%20datasets.%20We%20provide%20empirical%20evidence%20that%20our%0Aproposed%20framework%20consistently%20outperforms%20other%20baseline%20methods%2C%20showing%20an%0Aimprovement%20of%202%25-9%25%20in%20macro-f1%20scores.%20Through%20class-wise%20analysis%20of%20f1%0Ascores%2C%20we%20also%20demonstrate%20how%20the%20proposed%20framework%20can%20significantly%0Aimprove%20performance%20for%20minority%20classes.%20The%20results%20of%20our%20experiments%20show%0Athat%20our%20proposed%20framework%20can%20perform%20consistently%20well%20across%20different%0Aarchitectures%20and%20datasets.%20Overall%2C%20our%20study%20demonstrates%20a%20simple%20and%0Aeffective%20approach%20to%20addressing%20the%20class%20imbalance%20problem%20in%20medical%20imaging%0Adatasets.%20We%20hope%20our%20work%20will%20inspire%20new%20research%20toward%20a%20more%20generalized%0Aapproach%20to%20medical%20image%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.12741v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMFLOSS%253A%2520A%2520Hybrid%2520Loss%2520For%2520Imbalanced%2520Medical%2520Image%2520Classification%26entry.906535625%3DAbu%2520Adnan%2520Sadi%2520and%2520Labib%2520Chowdhury%2520and%2520Nusrat%2520Jahan%2520and%2520Mohammad%2520Newaz%2520Sharif%2520Rafi%2520and%2520Radeya%2520Chowdhury%2520and%2520Faisal%2520Ahamed%2520Khan%2520and%2520Nabeel%2520Mohammed%26entry.1292438233%3D%2520%2520With%2520advances%2520in%2520digital%2520technology%252C%2520the%2520classification%2520of%2520medical%2520images%2520has%250Abecome%2520a%2520crucial%2520step%2520for%2520image-based%2520clinical%2520decision%2520support%2520systems.%250AAutomatic%2520medical%2520image%2520classification%2520represents%2520a%2520pivotal%2520domain%2520where%2520the%250Ause%2520of%2520AI%2520holds%2520the%2520potential%2520to%2520create%2520a%2520significant%2520social%2520impact.%2520However%252C%250Aseveral%2520challenges%2520act%2520as%2520obstacles%2520to%2520the%2520development%2520of%2520practical%2520and%250Aeffective%2520solutions.%2520One%2520of%2520these%2520challenges%2520is%2520the%2520prevalent%2520class%2520imbalance%250Aproblem%2520in%2520most%2520medical%2520imaging%2520datasets.%2520As%2520a%2520result%252C%2520existing%2520AI%2520techniques%252C%250Aparticularly%2520deep-learning-based%2520methodologies%252C%2520often%2520underperform%2520in%2520such%250Ascenarios.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520framework%2520called%2520Large%2520Margin%250Aaware%2520Focal%2520%2528LMF%2529%2520loss%2520to%2520mitigate%2520the%2520class%2520imbalance%2520problem%2520in%2520medical%250Aimaging.%2520The%2520LMF%2520loss%2520represents%2520a%2520linear%2520combination%2520of%2520two%2520loss%2520functions%250Aoptimized%2520by%2520two%2520hyperparameters.%2520This%2520framework%2520harnesses%2520the%2520distinct%250Acharacteristics%2520of%2520both%2520loss%2520functions%2520by%2520enforcing%2520wider%2520margins%2520for%2520minority%250Aclasses%2520while%2520simultaneously%2520emphasizing%2520challenging%2520samples%2520found%2520in%2520the%250Adatasets.%2520We%2520perform%2520rigorous%2520experiments%2520on%2520three%2520neural%2520network%2520architectures%250Aand%2520with%2520four%2520medical%2520imaging%2520datasets.%2520We%2520provide%2520empirical%2520evidence%2520that%2520our%250Aproposed%2520framework%2520consistently%2520outperforms%2520other%2520baseline%2520methods%252C%2520showing%2520an%250Aimprovement%2520of%25202%2525-9%2525%2520in%2520macro-f1%2520scores.%2520Through%2520class-wise%2520analysis%2520of%2520f1%250Ascores%252C%2520we%2520also%2520demonstrate%2520how%2520the%2520proposed%2520framework%2520can%2520significantly%250Aimprove%2520performance%2520for%2520minority%2520classes.%2520The%2520results%2520of%2520our%2520experiments%2520show%250Athat%2520our%2520proposed%2520framework%2520can%2520perform%2520consistently%2520well%2520across%2520different%250Aarchitectures%2520and%2520datasets.%2520Overall%252C%2520our%2520study%2520demonstrates%2520a%2520simple%2520and%250Aeffective%2520approach%2520to%2520addressing%2520the%2520class%2520imbalance%2520problem%2520in%2520medical%2520imaging%250Adatasets.%2520We%2520hope%2520our%2520work%2520will%2520inspire%2520new%2520research%2520toward%2520a%2520more%2520generalized%250Aapproach%2520to%2520medical%2520image%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.12741v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMFLOSS%3A%20A%20Hybrid%20Loss%20For%20Imbalanced%20Medical%20Image%20Classification&entry.906535625=Abu%20Adnan%20Sadi%20and%20Labib%20Chowdhury%20and%20Nusrat%20Jahan%20and%20Mohammad%20Newaz%20Sharif%20Rafi%20and%20Radeya%20Chowdhury%20and%20Faisal%20Ahamed%20Khan%20and%20Nabeel%20Mohammed&entry.1292438233=%20%20With%20advances%20in%20digital%20technology%2C%20the%20classification%20of%20medical%20images%20has%0Abecome%20a%20crucial%20step%20for%20image-based%20clinical%20decision%20support%20systems.%0AAutomatic%20medical%20image%20classification%20represents%20a%20pivotal%20domain%20where%20the%0Ause%20of%20AI%20holds%20the%20potential%20to%20create%20a%20significant%20social%20impact.%20However%2C%0Aseveral%20challenges%20act%20as%20obstacles%20to%20the%20development%20of%20practical%20and%0Aeffective%20solutions.%20One%20of%20these%20challenges%20is%20the%20prevalent%20class%20imbalance%0Aproblem%20in%20most%20medical%20imaging%20datasets.%20As%20a%20result%2C%20existing%20AI%20techniques%2C%0Aparticularly%20deep-learning-based%20methodologies%2C%20often%20underperform%20in%20such%0Ascenarios.%20In%20this%20study%2C%20we%20propose%20a%20novel%20framework%20called%20Large%20Margin%0Aaware%20Focal%20%28LMF%29%20loss%20to%20mitigate%20the%20class%20imbalance%20problem%20in%20medical%0Aimaging.%20The%20LMF%20loss%20represents%20a%20linear%20combination%20of%20two%20loss%20functions%0Aoptimized%20by%20two%20hyperparameters.%20This%20framework%20harnesses%20the%20distinct%0Acharacteristics%20of%20both%20loss%20functions%20by%20enforcing%20wider%20margins%20for%20minority%0Aclasses%20while%20simultaneously%20emphasizing%20challenging%20samples%20found%20in%20the%0Adatasets.%20We%20perform%20rigorous%20experiments%20on%20three%20neural%20network%20architectures%0Aand%20with%20four%20medical%20imaging%20datasets.%20We%20provide%20empirical%20evidence%20that%20our%0Aproposed%20framework%20consistently%20outperforms%20other%20baseline%20methods%2C%20showing%20an%0Aimprovement%20of%202%25-9%25%20in%20macro-f1%20scores.%20Through%20class-wise%20analysis%20of%20f1%0Ascores%2C%20we%20also%20demonstrate%20how%20the%20proposed%20framework%20can%20significantly%0Aimprove%20performance%20for%20minority%20classes.%20The%20results%20of%20our%20experiments%20show%0Athat%20our%20proposed%20framework%20can%20perform%20consistently%20well%20across%20different%0Aarchitectures%20and%20datasets.%20Overall%2C%20our%20study%20demonstrates%20a%20simple%20and%0Aeffective%20approach%20to%20addressing%20the%20class%20imbalance%20problem%20in%20medical%20imaging%0Adatasets.%20We%20hope%20our%20work%20will%20inspire%20new%20research%20toward%20a%20more%20generalized%0Aapproach%20to%20medical%20image%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.12741v2&entry.124074799=Read"},
{"title": "The Faiss library", "author": "Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazar\u00e9 and Maria Lomeli and Lucas Hosseini and Herv\u00e9 J\u00e9gou", "abstract": "  Vector databases typically manage large collections of embedding vectors.\nCurrently, AI applications are growing rapidly, and so is the number of\nembeddings that need to be stored and indexed. The Faiss library is dedicated\nto vector similarity search, a core functionality of vector databases. Faiss is\na toolkit of indexing methods and related primitives used to search, cluster,\ncompress and transform vectors. This paper describes the trade-off space of\nvector search and the design principles of Faiss in terms of structure,\napproach to optimization and interfacing. We benchmark key features of the\nlibrary and discuss a few selected applications to highlight its broad\napplicability.\n", "link": "http://arxiv.org/abs/2401.08281v2", "date": "2024-09-06", "relevancy": 2.0685, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4247}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4247}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.3916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Faiss%20library&body=Title%3A%20The%20Faiss%20library%0AAuthor%3A%20Matthijs%20Douze%20and%20Alexandr%20Guzhva%20and%20Chengqi%20Deng%20and%20Jeff%20Johnson%20and%20Gergely%20Szilvasy%20and%20Pierre-Emmanuel%20Mazar%C3%A9%20and%20Maria%20Lomeli%20and%20Lucas%20Hosseini%20and%20Herv%C3%A9%20J%C3%A9gou%0AAbstract%3A%20%20%20Vector%20databases%20typically%20manage%20large%20collections%20of%20embedding%20vectors.%0ACurrently%2C%20AI%20applications%20are%20growing%20rapidly%2C%20and%20so%20is%20the%20number%20of%0Aembeddings%20that%20need%20to%20be%20stored%20and%20indexed.%20The%20Faiss%20library%20is%20dedicated%0Ato%20vector%20similarity%20search%2C%20a%20core%20functionality%20of%20vector%20databases.%20Faiss%20is%0Aa%20toolkit%20of%20indexing%20methods%20and%20related%20primitives%20used%20to%20search%2C%20cluster%2C%0Acompress%20and%20transform%20vectors.%20This%20paper%20describes%20the%20trade-off%20space%20of%0Avector%20search%20and%20the%20design%20principles%20of%20Faiss%20in%20terms%20of%20structure%2C%0Aapproach%20to%20optimization%20and%20interfacing.%20We%20benchmark%20key%20features%20of%20the%0Alibrary%20and%20discuss%20a%20few%20selected%20applications%20to%20highlight%20its%20broad%0Aapplicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08281v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Faiss%2520library%26entry.906535625%3DMatthijs%2520Douze%2520and%2520Alexandr%2520Guzhva%2520and%2520Chengqi%2520Deng%2520and%2520Jeff%2520Johnson%2520and%2520Gergely%2520Szilvasy%2520and%2520Pierre-Emmanuel%2520Mazar%25C3%25A9%2520and%2520Maria%2520Lomeli%2520and%2520Lucas%2520Hosseini%2520and%2520Herv%25C3%25A9%2520J%25C3%25A9gou%26entry.1292438233%3D%2520%2520Vector%2520databases%2520typically%2520manage%2520large%2520collections%2520of%2520embedding%2520vectors.%250ACurrently%252C%2520AI%2520applications%2520are%2520growing%2520rapidly%252C%2520and%2520so%2520is%2520the%2520number%2520of%250Aembeddings%2520that%2520need%2520to%2520be%2520stored%2520and%2520indexed.%2520The%2520Faiss%2520library%2520is%2520dedicated%250Ato%2520vector%2520similarity%2520search%252C%2520a%2520core%2520functionality%2520of%2520vector%2520databases.%2520Faiss%2520is%250Aa%2520toolkit%2520of%2520indexing%2520methods%2520and%2520related%2520primitives%2520used%2520to%2520search%252C%2520cluster%252C%250Acompress%2520and%2520transform%2520vectors.%2520This%2520paper%2520describes%2520the%2520trade-off%2520space%2520of%250Avector%2520search%2520and%2520the%2520design%2520principles%2520of%2520Faiss%2520in%2520terms%2520of%2520structure%252C%250Aapproach%2520to%2520optimization%2520and%2520interfacing.%2520We%2520benchmark%2520key%2520features%2520of%2520the%250Alibrary%2520and%2520discuss%2520a%2520few%2520selected%2520applications%2520to%2520highlight%2520its%2520broad%250Aapplicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08281v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Faiss%20library&entry.906535625=Matthijs%20Douze%20and%20Alexandr%20Guzhva%20and%20Chengqi%20Deng%20and%20Jeff%20Johnson%20and%20Gergely%20Szilvasy%20and%20Pierre-Emmanuel%20Mazar%C3%A9%20and%20Maria%20Lomeli%20and%20Lucas%20Hosseini%20and%20Herv%C3%A9%20J%C3%A9gou&entry.1292438233=%20%20Vector%20databases%20typically%20manage%20large%20collections%20of%20embedding%20vectors.%0ACurrently%2C%20AI%20applications%20are%20growing%20rapidly%2C%20and%20so%20is%20the%20number%20of%0Aembeddings%20that%20need%20to%20be%20stored%20and%20indexed.%20The%20Faiss%20library%20is%20dedicated%0Ato%20vector%20similarity%20search%2C%20a%20core%20functionality%20of%20vector%20databases.%20Faiss%20is%0Aa%20toolkit%20of%20indexing%20methods%20and%20related%20primitives%20used%20to%20search%2C%20cluster%2C%0Acompress%20and%20transform%20vectors.%20This%20paper%20describes%20the%20trade-off%20space%20of%0Avector%20search%20and%20the%20design%20principles%20of%20Faiss%20in%20terms%20of%20structure%2C%0Aapproach%20to%20optimization%20and%20interfacing.%20We%20benchmark%20key%20features%20of%20the%0Alibrary%20and%20discuss%20a%20few%20selected%20applications%20to%20highlight%20its%20broad%0Aapplicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08281v2&entry.124074799=Read"},
{"title": "Calibration of Network Confidence for Unsupervised Domain Adaptation\n  Using Estimated Accuracy", "author": "Coby Penso and Jacob Goldberger", "abstract": "  This study addresses the problem of calibrating network confidence while\nadapting a model that was originally trained on a source domain to a target\ndomain using unlabeled samples from the target domain. The absence of labels\nfrom the target domain makes it impossible to directly calibrate the adapted\nnetwork on the target domain. To tackle this challenge, we introduce a\ncalibration procedure that relies on estimating the network's accuracy on the\ntarget domain. The network accuracy is first computed on the labeled source\ndata and then is modified to represent the actual accuracy of the model on the\ntarget domain. The proposed algorithm calibrates the prediction confidence\ndirectly in the target domain by minimizing the disparity between the estimated\naccuracy and the computed confidence. The experimental results show that our\nmethod significantly outperforms existing methods, which rely on importance\nweighting, across several standard datasets.\n", "link": "http://arxiv.org/abs/2409.04241v1", "date": "2024-09-06", "relevancy": 2.0097, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5251}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5214}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibration%20of%20Network%20Confidence%20for%20Unsupervised%20Domain%20Adaptation%0A%20%20Using%20Estimated%20Accuracy&body=Title%3A%20Calibration%20of%20Network%20Confidence%20for%20Unsupervised%20Domain%20Adaptation%0A%20%20Using%20Estimated%20Accuracy%0AAuthor%3A%20Coby%20Penso%20and%20Jacob%20Goldberger%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20problem%20of%20calibrating%20network%20confidence%20while%0Aadapting%20a%20model%20that%20was%20originally%20trained%20on%20a%20source%20domain%20to%20a%20target%0Adomain%20using%20unlabeled%20samples%20from%20the%20target%20domain.%20The%20absence%20of%20labels%0Afrom%20the%20target%20domain%20makes%20it%20impossible%20to%20directly%20calibrate%20the%20adapted%0Anetwork%20on%20the%20target%20domain.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20a%0Acalibration%20procedure%20that%20relies%20on%20estimating%20the%20network%27s%20accuracy%20on%20the%0Atarget%20domain.%20The%20network%20accuracy%20is%20first%20computed%20on%20the%20labeled%20source%0Adata%20and%20then%20is%20modified%20to%20represent%20the%20actual%20accuracy%20of%20the%20model%20on%20the%0Atarget%20domain.%20The%20proposed%20algorithm%20calibrates%20the%20prediction%20confidence%0Adirectly%20in%20the%20target%20domain%20by%20minimizing%20the%20disparity%20between%20the%20estimated%0Aaccuracy%20and%20the%20computed%20confidence.%20The%20experimental%20results%20show%20that%20our%0Amethod%20significantly%20outperforms%20existing%20methods%2C%20which%20rely%20on%20importance%0Aweighting%2C%20across%20several%20standard%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibration%2520of%2520Network%2520Confidence%2520for%2520Unsupervised%2520Domain%2520Adaptation%250A%2520%2520Using%2520Estimated%2520Accuracy%26entry.906535625%3DCoby%2520Penso%2520and%2520Jacob%2520Goldberger%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520problem%2520of%2520calibrating%2520network%2520confidence%2520while%250Aadapting%2520a%2520model%2520that%2520was%2520originally%2520trained%2520on%2520a%2520source%2520domain%2520to%2520a%2520target%250Adomain%2520using%2520unlabeled%2520samples%2520from%2520the%2520target%2520domain.%2520The%2520absence%2520of%2520labels%250Afrom%2520the%2520target%2520domain%2520makes%2520it%2520impossible%2520to%2520directly%2520calibrate%2520the%2520adapted%250Anetwork%2520on%2520the%2520target%2520domain.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520introduce%2520a%250Acalibration%2520procedure%2520that%2520relies%2520on%2520estimating%2520the%2520network%2527s%2520accuracy%2520on%2520the%250Atarget%2520domain.%2520The%2520network%2520accuracy%2520is%2520first%2520computed%2520on%2520the%2520labeled%2520source%250Adata%2520and%2520then%2520is%2520modified%2520to%2520represent%2520the%2520actual%2520accuracy%2520of%2520the%2520model%2520on%2520the%250Atarget%2520domain.%2520The%2520proposed%2520algorithm%2520calibrates%2520the%2520prediction%2520confidence%250Adirectly%2520in%2520the%2520target%2520domain%2520by%2520minimizing%2520the%2520disparity%2520between%2520the%2520estimated%250Aaccuracy%2520and%2520the%2520computed%2520confidence.%2520The%2520experimental%2520results%2520show%2520that%2520our%250Amethod%2520significantly%2520outperforms%2520existing%2520methods%252C%2520which%2520rely%2520on%2520importance%250Aweighting%252C%2520across%2520several%2520standard%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibration%20of%20Network%20Confidence%20for%20Unsupervised%20Domain%20Adaptation%0A%20%20Using%20Estimated%20Accuracy&entry.906535625=Coby%20Penso%20and%20Jacob%20Goldberger&entry.1292438233=%20%20This%20study%20addresses%20the%20problem%20of%20calibrating%20network%20confidence%20while%0Aadapting%20a%20model%20that%20was%20originally%20trained%20on%20a%20source%20domain%20to%20a%20target%0Adomain%20using%20unlabeled%20samples%20from%20the%20target%20domain.%20The%20absence%20of%20labels%0Afrom%20the%20target%20domain%20makes%20it%20impossible%20to%20directly%20calibrate%20the%20adapted%0Anetwork%20on%20the%20target%20domain.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20a%0Acalibration%20procedure%20that%20relies%20on%20estimating%20the%20network%27s%20accuracy%20on%20the%0Atarget%20domain.%20The%20network%20accuracy%20is%20first%20computed%20on%20the%20labeled%20source%0Adata%20and%20then%20is%20modified%20to%20represent%20the%20actual%20accuracy%20of%20the%20model%20on%20the%0Atarget%20domain.%20The%20proposed%20algorithm%20calibrates%20the%20prediction%20confidence%0Adirectly%20in%20the%20target%20domain%20by%20minimizing%20the%20disparity%20between%20the%20estimated%0Aaccuracy%20and%20the%20computed%20confidence.%20The%20experimental%20results%20show%20that%20our%0Amethod%20significantly%20outperforms%20existing%20methods%2C%20which%20rely%20on%20importance%0Aweighting%2C%20across%20several%20standard%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04241v1&entry.124074799=Read"},
{"title": "LITE: A Paradigm Shift in Multi-Object Tracking with Efficient ReID\n  Feature Integration", "author": "Jumabek Alikhanov and Dilshod Obidov and Hakil Kim", "abstract": "  The Lightweight Integrated Tracking-Feature Extraction (LITE) paradigm is\nintroduced as a novel multi-object tracking (MOT) approach. It enhances\nReID-based trackers by eliminating inference, pre-processing, post-processing,\nand ReID model training costs. LITE uses real-time appearance features without\ncompromising speed. By integrating appearance feature extraction directly into\nthe tracking pipeline using standard CNN-based detectors such as YOLOv8m, LITE\ndemonstrates significant performance improvements. The simplest implementation\nof LITE on top of classic DeepSORT achieves a HOTA score of 43.03% at 28.3 FPS\non the MOT17 benchmark, making it twice as fast as DeepSORT on MOT17 and four\ntimes faster on the more crowded MOT20 dataset, while maintaining similar\naccuracy. Additionally, a new evaluation framework for tracking-by-detection\napproaches reveals that conventional trackers like DeepSORT remain competitive\nwith modern state-of-the-art trackers when evaluated under fair conditions. The\ncode will be available post-publication at https://github.com/Jumabek/LITE.\n", "link": "http://arxiv.org/abs/2409.04187v1", "date": "2024-09-06", "relevancy": 2.0003, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5124}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5061}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LITE%3A%20A%20Paradigm%20Shift%20in%20Multi-Object%20Tracking%20with%20Efficient%20ReID%0A%20%20Feature%20Integration&body=Title%3A%20LITE%3A%20A%20Paradigm%20Shift%20in%20Multi-Object%20Tracking%20with%20Efficient%20ReID%0A%20%20Feature%20Integration%0AAuthor%3A%20Jumabek%20Alikhanov%20and%20Dilshod%20Obidov%20and%20Hakil%20Kim%0AAbstract%3A%20%20%20The%20Lightweight%20Integrated%20Tracking-Feature%20Extraction%20%28LITE%29%20paradigm%20is%0Aintroduced%20as%20a%20novel%20multi-object%20tracking%20%28MOT%29%20approach.%20It%20enhances%0AReID-based%20trackers%20by%20eliminating%20inference%2C%20pre-processing%2C%20post-processing%2C%0Aand%20ReID%20model%20training%20costs.%20LITE%20uses%20real-time%20appearance%20features%20without%0Acompromising%20speed.%20By%20integrating%20appearance%20feature%20extraction%20directly%20into%0Athe%20tracking%20pipeline%20using%20standard%20CNN-based%20detectors%20such%20as%20YOLOv8m%2C%20LITE%0Ademonstrates%20significant%20performance%20improvements.%20The%20simplest%20implementation%0Aof%20LITE%20on%20top%20of%20classic%20DeepSORT%20achieves%20a%20HOTA%20score%20of%2043.03%25%20at%2028.3%20FPS%0Aon%20the%20MOT17%20benchmark%2C%20making%20it%20twice%20as%20fast%20as%20DeepSORT%20on%20MOT17%20and%20four%0Atimes%20faster%20on%20the%20more%20crowded%20MOT20%20dataset%2C%20while%20maintaining%20similar%0Aaccuracy.%20Additionally%2C%20a%20new%20evaluation%20framework%20for%20tracking-by-detection%0Aapproaches%20reveals%20that%20conventional%20trackers%20like%20DeepSORT%20remain%20competitive%0Awith%20modern%20state-of-the-art%20trackers%20when%20evaluated%20under%20fair%20conditions.%20The%0Acode%20will%20be%20available%20post-publication%20at%20https%3A//github.com/Jumabek/LITE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLITE%253A%2520A%2520Paradigm%2520Shift%2520in%2520Multi-Object%2520Tracking%2520with%2520Efficient%2520ReID%250A%2520%2520Feature%2520Integration%26entry.906535625%3DJumabek%2520Alikhanov%2520and%2520Dilshod%2520Obidov%2520and%2520Hakil%2520Kim%26entry.1292438233%3D%2520%2520The%2520Lightweight%2520Integrated%2520Tracking-Feature%2520Extraction%2520%2528LITE%2529%2520paradigm%2520is%250Aintroduced%2520as%2520a%2520novel%2520multi-object%2520tracking%2520%2528MOT%2529%2520approach.%2520It%2520enhances%250AReID-based%2520trackers%2520by%2520eliminating%2520inference%252C%2520pre-processing%252C%2520post-processing%252C%250Aand%2520ReID%2520model%2520training%2520costs.%2520LITE%2520uses%2520real-time%2520appearance%2520features%2520without%250Acompromising%2520speed.%2520By%2520integrating%2520appearance%2520feature%2520extraction%2520directly%2520into%250Athe%2520tracking%2520pipeline%2520using%2520standard%2520CNN-based%2520detectors%2520such%2520as%2520YOLOv8m%252C%2520LITE%250Ademonstrates%2520significant%2520performance%2520improvements.%2520The%2520simplest%2520implementation%250Aof%2520LITE%2520on%2520top%2520of%2520classic%2520DeepSORT%2520achieves%2520a%2520HOTA%2520score%2520of%252043.03%2525%2520at%252028.3%2520FPS%250Aon%2520the%2520MOT17%2520benchmark%252C%2520making%2520it%2520twice%2520as%2520fast%2520as%2520DeepSORT%2520on%2520MOT17%2520and%2520four%250Atimes%2520faster%2520on%2520the%2520more%2520crowded%2520MOT20%2520dataset%252C%2520while%2520maintaining%2520similar%250Aaccuracy.%2520Additionally%252C%2520a%2520new%2520evaluation%2520framework%2520for%2520tracking-by-detection%250Aapproaches%2520reveals%2520that%2520conventional%2520trackers%2520like%2520DeepSORT%2520remain%2520competitive%250Awith%2520modern%2520state-of-the-art%2520trackers%2520when%2520evaluated%2520under%2520fair%2520conditions.%2520The%250Acode%2520will%2520be%2520available%2520post-publication%2520at%2520https%253A//github.com/Jumabek/LITE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LITE%3A%20A%20Paradigm%20Shift%20in%20Multi-Object%20Tracking%20with%20Efficient%20ReID%0A%20%20Feature%20Integration&entry.906535625=Jumabek%20Alikhanov%20and%20Dilshod%20Obidov%20and%20Hakil%20Kim&entry.1292438233=%20%20The%20Lightweight%20Integrated%20Tracking-Feature%20Extraction%20%28LITE%29%20paradigm%20is%0Aintroduced%20as%20a%20novel%20multi-object%20tracking%20%28MOT%29%20approach.%20It%20enhances%0AReID-based%20trackers%20by%20eliminating%20inference%2C%20pre-processing%2C%20post-processing%2C%0Aand%20ReID%20model%20training%20costs.%20LITE%20uses%20real-time%20appearance%20features%20without%0Acompromising%20speed.%20By%20integrating%20appearance%20feature%20extraction%20directly%20into%0Athe%20tracking%20pipeline%20using%20standard%20CNN-based%20detectors%20such%20as%20YOLOv8m%2C%20LITE%0Ademonstrates%20significant%20performance%20improvements.%20The%20simplest%20implementation%0Aof%20LITE%20on%20top%20of%20classic%20DeepSORT%20achieves%20a%20HOTA%20score%20of%2043.03%25%20at%2028.3%20FPS%0Aon%20the%20MOT17%20benchmark%2C%20making%20it%20twice%20as%20fast%20as%20DeepSORT%20on%20MOT17%20and%20four%0Atimes%20faster%20on%20the%20more%20crowded%20MOT20%20dataset%2C%20while%20maintaining%20similar%0Aaccuracy.%20Additionally%2C%20a%20new%20evaluation%20framework%20for%20tracking-by-detection%0Aapproaches%20reveals%20that%20conventional%20trackers%20like%20DeepSORT%20remain%20competitive%0Awith%20modern%20state-of-the-art%20trackers%20when%20evaluated%20under%20fair%20conditions.%20The%0Acode%20will%20be%20available%20post-publication%20at%20https%3A//github.com/Jumabek/LITE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04187v1&entry.124074799=Read"},
{"title": "Enhancing Biomedical Knowledge Discovery for Diseases: An Open-Source\n  Framework Applied on Rett Syndrome and Alzheimer's Disease", "author": "Christos Theodoropoulos and Andrei Catalin Coman and James Henderson and Marie-Francine Moens", "abstract": "  The ever-growing volume of biomedical publications creates a critical need\nfor efficient knowledge discovery. In this context, we introduce an open-source\nend-to-end framework designed to construct knowledge around specific diseases\ndirectly from raw text. To facilitate research in disease-related knowledge\ndiscovery, we create two annotated datasets focused on Rett syndrome and\nAlzheimer's disease, enabling the identification of semantic relations between\nbiomedical entities. Extensive benchmarking explores various ways to represent\nrelations and entity representations, offering insights into optimal modeling\nstrategies for semantic relation detection and highlighting language models'\ncompetence in knowledge discovery. We also conduct probing experiments using\ndifferent layer representations and attention scores to explore transformers'\nability to capture semantic relations.\n", "link": "http://arxiv.org/abs/2407.13492v2", "date": "2024-09-06", "relevancy": 1.9881, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4979}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Biomedical%20Knowledge%20Discovery%20for%20Diseases%3A%20An%20Open-Source%0A%20%20Framework%20Applied%20on%20Rett%20Syndrome%20and%20Alzheimer%27s%20Disease&body=Title%3A%20Enhancing%20Biomedical%20Knowledge%20Discovery%20for%20Diseases%3A%20An%20Open-Source%0A%20%20Framework%20Applied%20on%20Rett%20Syndrome%20and%20Alzheimer%27s%20Disease%0AAuthor%3A%20Christos%20Theodoropoulos%20and%20Andrei%20Catalin%20Coman%20and%20James%20Henderson%20and%20Marie-Francine%20Moens%0AAbstract%3A%20%20%20The%20ever-growing%20volume%20of%20biomedical%20publications%20creates%20a%20critical%20need%0Afor%20efficient%20knowledge%20discovery.%20In%20this%20context%2C%20we%20introduce%20an%20open-source%0Aend-to-end%20framework%20designed%20to%20construct%20knowledge%20around%20specific%20diseases%0Adirectly%20from%20raw%20text.%20To%20facilitate%20research%20in%20disease-related%20knowledge%0Adiscovery%2C%20we%20create%20two%20annotated%20datasets%20focused%20on%20Rett%20syndrome%20and%0AAlzheimer%27s%20disease%2C%20enabling%20the%20identification%20of%20semantic%20relations%20between%0Abiomedical%20entities.%20Extensive%20benchmarking%20explores%20various%20ways%20to%20represent%0Arelations%20and%20entity%20representations%2C%20offering%20insights%20into%20optimal%20modeling%0Astrategies%20for%20semantic%20relation%20detection%20and%20highlighting%20language%20models%27%0Acompetence%20in%20knowledge%20discovery.%20We%20also%20conduct%20probing%20experiments%20using%0Adifferent%20layer%20representations%20and%20attention%20scores%20to%20explore%20transformers%27%0Aability%20to%20capture%20semantic%20relations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Biomedical%2520Knowledge%2520Discovery%2520for%2520Diseases%253A%2520An%2520Open-Source%250A%2520%2520Framework%2520Applied%2520on%2520Rett%2520Syndrome%2520and%2520Alzheimer%2527s%2520Disease%26entry.906535625%3DChristos%2520Theodoropoulos%2520and%2520Andrei%2520Catalin%2520Coman%2520and%2520James%2520Henderson%2520and%2520Marie-Francine%2520Moens%26entry.1292438233%3D%2520%2520The%2520ever-growing%2520volume%2520of%2520biomedical%2520publications%2520creates%2520a%2520critical%2520need%250Afor%2520efficient%2520knowledge%2520discovery.%2520In%2520this%2520context%252C%2520we%2520introduce%2520an%2520open-source%250Aend-to-end%2520framework%2520designed%2520to%2520construct%2520knowledge%2520around%2520specific%2520diseases%250Adirectly%2520from%2520raw%2520text.%2520To%2520facilitate%2520research%2520in%2520disease-related%2520knowledge%250Adiscovery%252C%2520we%2520create%2520two%2520annotated%2520datasets%2520focused%2520on%2520Rett%2520syndrome%2520and%250AAlzheimer%2527s%2520disease%252C%2520enabling%2520the%2520identification%2520of%2520semantic%2520relations%2520between%250Abiomedical%2520entities.%2520Extensive%2520benchmarking%2520explores%2520various%2520ways%2520to%2520represent%250Arelations%2520and%2520entity%2520representations%252C%2520offering%2520insights%2520into%2520optimal%2520modeling%250Astrategies%2520for%2520semantic%2520relation%2520detection%2520and%2520highlighting%2520language%2520models%2527%250Acompetence%2520in%2520knowledge%2520discovery.%2520We%2520also%2520conduct%2520probing%2520experiments%2520using%250Adifferent%2520layer%2520representations%2520and%2520attention%2520scores%2520to%2520explore%2520transformers%2527%250Aability%2520to%2520capture%2520semantic%2520relations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Biomedical%20Knowledge%20Discovery%20for%20Diseases%3A%20An%20Open-Source%0A%20%20Framework%20Applied%20on%20Rett%20Syndrome%20and%20Alzheimer%27s%20Disease&entry.906535625=Christos%20Theodoropoulos%20and%20Andrei%20Catalin%20Coman%20and%20James%20Henderson%20and%20Marie-Francine%20Moens&entry.1292438233=%20%20The%20ever-growing%20volume%20of%20biomedical%20publications%20creates%20a%20critical%20need%0Afor%20efficient%20knowledge%20discovery.%20In%20this%20context%2C%20we%20introduce%20an%20open-source%0Aend-to-end%20framework%20designed%20to%20construct%20knowledge%20around%20specific%20diseases%0Adirectly%20from%20raw%20text.%20To%20facilitate%20research%20in%20disease-related%20knowledge%0Adiscovery%2C%20we%20create%20two%20annotated%20datasets%20focused%20on%20Rett%20syndrome%20and%0AAlzheimer%27s%20disease%2C%20enabling%20the%20identification%20of%20semantic%20relations%20between%0Abiomedical%20entities.%20Extensive%20benchmarking%20explores%20various%20ways%20to%20represent%0Arelations%20and%20entity%20representations%2C%20offering%20insights%20into%20optimal%20modeling%0Astrategies%20for%20semantic%20relation%20detection%20and%20highlighting%20language%20models%27%0Acompetence%20in%20knowledge%20discovery.%20We%20also%20conduct%20probing%20experiments%20using%0Adifferent%20layer%20representations%20and%20attention%20scores%20to%20explore%20transformers%27%0Aability%20to%20capture%20semantic%20relations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13492v2&entry.124074799=Read"},
{"title": "Uncertainty Modeling in Graph Neural Networks via Stochastic\n  Differential Equations", "author": "Richard Bergna and Sergio Calvo-Ordo\u00f1ez and Felix L. Opolka and Pietro Li\u00f2 and Jose Miguel Hernandez-Lobato", "abstract": "  We address the problem of learning uncertainty-aware representations for\ngraph-structured data. While Graph Neural Ordinary Differential Equations\n(GNODE) are effective in learning node representations, they fail to quantify\nuncertainty. To address this, we introduce Latent Graph Neural Stochastic\nDifferential Equations (LGNSDE), which enhance GNODE by embedding randomness\nthrough Brownian motion to quantify uncertainty. We provide theoretical\nguarantees for LGNSDE and empirically show better performance in uncertainty\nquantification.\n", "link": "http://arxiv.org/abs/2408.16115v3", "date": "2024-09-06", "relevancy": 1.9802, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4978}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4946}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Modeling%20in%20Graph%20Neural%20Networks%20via%20Stochastic%0A%20%20Differential%20Equations&body=Title%3A%20Uncertainty%20Modeling%20in%20Graph%20Neural%20Networks%20via%20Stochastic%0A%20%20Differential%20Equations%0AAuthor%3A%20Richard%20Bergna%20and%20Sergio%20Calvo-Ordo%C3%B1ez%20and%20Felix%20L.%20Opolka%20and%20Pietro%20Li%C3%B2%20and%20Jose%20Miguel%20Hernandez-Lobato%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20learning%20uncertainty-aware%20representations%20for%0Agraph-structured%20data.%20While%20Graph%20Neural%20Ordinary%20Differential%20Equations%0A%28GNODE%29%20are%20effective%20in%20learning%20node%20representations%2C%20they%20fail%20to%20quantify%0Auncertainty.%20To%20address%20this%2C%20we%20introduce%20Latent%20Graph%20Neural%20Stochastic%0ADifferential%20Equations%20%28LGNSDE%29%2C%20which%20enhance%20GNODE%20by%20embedding%20randomness%0Athrough%20Brownian%20motion%20to%20quantify%20uncertainty.%20We%20provide%20theoretical%0Aguarantees%20for%20LGNSDE%20and%20empirically%20show%20better%20performance%20in%20uncertainty%0Aquantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16115v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Modeling%2520in%2520Graph%2520Neural%2520Networks%2520via%2520Stochastic%250A%2520%2520Differential%2520Equations%26entry.906535625%3DRichard%2520Bergna%2520and%2520Sergio%2520Calvo-Ordo%25C3%25B1ez%2520and%2520Felix%2520L.%2520Opolka%2520and%2520Pietro%2520Li%25C3%25B2%2520and%2520Jose%2520Miguel%2520Hernandez-Lobato%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520learning%2520uncertainty-aware%2520representations%2520for%250Agraph-structured%2520data.%2520While%2520Graph%2520Neural%2520Ordinary%2520Differential%2520Equations%250A%2528GNODE%2529%2520are%2520effective%2520in%2520learning%2520node%2520representations%252C%2520they%2520fail%2520to%2520quantify%250Auncertainty.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Latent%2520Graph%2520Neural%2520Stochastic%250ADifferential%2520Equations%2520%2528LGNSDE%2529%252C%2520which%2520enhance%2520GNODE%2520by%2520embedding%2520randomness%250Athrough%2520Brownian%2520motion%2520to%2520quantify%2520uncertainty.%2520We%2520provide%2520theoretical%250Aguarantees%2520for%2520LGNSDE%2520and%2520empirically%2520show%2520better%2520performance%2520in%2520uncertainty%250Aquantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16115v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Modeling%20in%20Graph%20Neural%20Networks%20via%20Stochastic%0A%20%20Differential%20Equations&entry.906535625=Richard%20Bergna%20and%20Sergio%20Calvo-Ordo%C3%B1ez%20and%20Felix%20L.%20Opolka%20and%20Pietro%20Li%C3%B2%20and%20Jose%20Miguel%20Hernandez-Lobato&entry.1292438233=%20%20We%20address%20the%20problem%20of%20learning%20uncertainty-aware%20representations%20for%0Agraph-structured%20data.%20While%20Graph%20Neural%20Ordinary%20Differential%20Equations%0A%28GNODE%29%20are%20effective%20in%20learning%20node%20representations%2C%20they%20fail%20to%20quantify%0Auncertainty.%20To%20address%20this%2C%20we%20introduce%20Latent%20Graph%20Neural%20Stochastic%0ADifferential%20Equations%20%28LGNSDE%29%2C%20which%20enhance%20GNODE%20by%20embedding%20randomness%0Athrough%20Brownian%20motion%20to%20quantify%20uncertainty.%20We%20provide%20theoretical%0Aguarantees%20for%20LGNSDE%20and%20empirically%20show%20better%20performance%20in%20uncertainty%0Aquantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16115v3&entry.124074799=Read"},
{"title": "Hybrid Spiking Neural Networks for Low-Power Intra-Cortical\n  Brain-Machine Interfaces", "author": "Alexandru Vasilache and Jann Krausse and Klaus Knobloch and Juergen Becker", "abstract": "  Intra-cortical brain-machine interfaces (iBMIs) have the potential to\ndramatically improve the lives of people with paraplegia by restoring their\nability to perform daily activities. However, current iBMIs suffer from\nscalability and mobility limitations due to bulky hardware and wiring. Wireless\niBMIs offer a solution but are constrained by a limited data rate. To overcome\nthis challenge, we are investigating hybrid spiking neural networks for\nembedded neural decoding in wireless iBMIs. The networks consist of a temporal\nconvolution-based compression followed by recurrent processing and a final\ninterpolation back to the original sequence length. As recurrent units, we\nexplore gated recurrent units (GRUs), leaky integrate-and-fire (LIF) neurons,\nand a combination of both - spiking GRUs (sGRUs) and analyze their differences\nin terms of accuracy, footprint, and activation sparsity. To that end, we train\ndecoders on the \"Nonhuman Primate Reaching with Multichannel Sensorimotor\nCortex Electrophysiology\" dataset and evaluate it using the NeuroBench\nframework, targeting both tracks of the IEEE BioCAS Grand Challenge on Neural\nDecoding. Our approach achieves high accuracy in predicting velocities of\nprimate reaching movements from multichannel primary motor cortex recordings\nwhile maintaining a low number of synaptic operations, surpassing the current\nbaseline models in the NeuroBench framework. This work highlights the potential\nof hybrid neural networks to facilitate wireless iBMIs with high decoding\nprecision and a substantial increase in the number of monitored neurons, paving\nthe way toward more advanced neuroprosthetic technologies.\n", "link": "http://arxiv.org/abs/2409.04428v1", "date": "2024-09-06", "relevancy": 1.978, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4904}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Spiking%20Neural%20Networks%20for%20Low-Power%20Intra-Cortical%0A%20%20Brain-Machine%20Interfaces&body=Title%3A%20Hybrid%20Spiking%20Neural%20Networks%20for%20Low-Power%20Intra-Cortical%0A%20%20Brain-Machine%20Interfaces%0AAuthor%3A%20Alexandru%20Vasilache%20and%20Jann%20Krausse%20and%20Klaus%20Knobloch%20and%20Juergen%20Becker%0AAbstract%3A%20%20%20Intra-cortical%20brain-machine%20interfaces%20%28iBMIs%29%20have%20the%20potential%20to%0Adramatically%20improve%20the%20lives%20of%20people%20with%20paraplegia%20by%20restoring%20their%0Aability%20to%20perform%20daily%20activities.%20However%2C%20current%20iBMIs%20suffer%20from%0Ascalability%20and%20mobility%20limitations%20due%20to%20bulky%20hardware%20and%20wiring.%20Wireless%0AiBMIs%20offer%20a%20solution%20but%20are%20constrained%20by%20a%20limited%20data%20rate.%20To%20overcome%0Athis%20challenge%2C%20we%20are%20investigating%20hybrid%20spiking%20neural%20networks%20for%0Aembedded%20neural%20decoding%20in%20wireless%20iBMIs.%20The%20networks%20consist%20of%20a%20temporal%0Aconvolution-based%20compression%20followed%20by%20recurrent%20processing%20and%20a%20final%0Ainterpolation%20back%20to%20the%20original%20sequence%20length.%20As%20recurrent%20units%2C%20we%0Aexplore%20gated%20recurrent%20units%20%28GRUs%29%2C%20leaky%20integrate-and-fire%20%28LIF%29%20neurons%2C%0Aand%20a%20combination%20of%20both%20-%20spiking%20GRUs%20%28sGRUs%29%20and%20analyze%20their%20differences%0Ain%20terms%20of%20accuracy%2C%20footprint%2C%20and%20activation%20sparsity.%20To%20that%20end%2C%20we%20train%0Adecoders%20on%20the%20%22Nonhuman%20Primate%20Reaching%20with%20Multichannel%20Sensorimotor%0ACortex%20Electrophysiology%22%20dataset%20and%20evaluate%20it%20using%20the%20NeuroBench%0Aframework%2C%20targeting%20both%20tracks%20of%20the%20IEEE%20BioCAS%20Grand%20Challenge%20on%20Neural%0ADecoding.%20Our%20approach%20achieves%20high%20accuracy%20in%20predicting%20velocities%20of%0Aprimate%20reaching%20movements%20from%20multichannel%20primary%20motor%20cortex%20recordings%0Awhile%20maintaining%20a%20low%20number%20of%20synaptic%20operations%2C%20surpassing%20the%20current%0Abaseline%20models%20in%20the%20NeuroBench%20framework.%20This%20work%20highlights%20the%20potential%0Aof%20hybrid%20neural%20networks%20to%20facilitate%20wireless%20iBMIs%20with%20high%20decoding%0Aprecision%20and%20a%20substantial%20increase%20in%20the%20number%20of%20monitored%20neurons%2C%20paving%0Athe%20way%20toward%20more%20advanced%20neuroprosthetic%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Spiking%2520Neural%2520Networks%2520for%2520Low-Power%2520Intra-Cortical%250A%2520%2520Brain-Machine%2520Interfaces%26entry.906535625%3DAlexandru%2520Vasilache%2520and%2520Jann%2520Krausse%2520and%2520Klaus%2520Knobloch%2520and%2520Juergen%2520Becker%26entry.1292438233%3D%2520%2520Intra-cortical%2520brain-machine%2520interfaces%2520%2528iBMIs%2529%2520have%2520the%2520potential%2520to%250Adramatically%2520improve%2520the%2520lives%2520of%2520people%2520with%2520paraplegia%2520by%2520restoring%2520their%250Aability%2520to%2520perform%2520daily%2520activities.%2520However%252C%2520current%2520iBMIs%2520suffer%2520from%250Ascalability%2520and%2520mobility%2520limitations%2520due%2520to%2520bulky%2520hardware%2520and%2520wiring.%2520Wireless%250AiBMIs%2520offer%2520a%2520solution%2520but%2520are%2520constrained%2520by%2520a%2520limited%2520data%2520rate.%2520To%2520overcome%250Athis%2520challenge%252C%2520we%2520are%2520investigating%2520hybrid%2520spiking%2520neural%2520networks%2520for%250Aembedded%2520neural%2520decoding%2520in%2520wireless%2520iBMIs.%2520The%2520networks%2520consist%2520of%2520a%2520temporal%250Aconvolution-based%2520compression%2520followed%2520by%2520recurrent%2520processing%2520and%2520a%2520final%250Ainterpolation%2520back%2520to%2520the%2520original%2520sequence%2520length.%2520As%2520recurrent%2520units%252C%2520we%250Aexplore%2520gated%2520recurrent%2520units%2520%2528GRUs%2529%252C%2520leaky%2520integrate-and-fire%2520%2528LIF%2529%2520neurons%252C%250Aand%2520a%2520combination%2520of%2520both%2520-%2520spiking%2520GRUs%2520%2528sGRUs%2529%2520and%2520analyze%2520their%2520differences%250Ain%2520terms%2520of%2520accuracy%252C%2520footprint%252C%2520and%2520activation%2520sparsity.%2520To%2520that%2520end%252C%2520we%2520train%250Adecoders%2520on%2520the%2520%2522Nonhuman%2520Primate%2520Reaching%2520with%2520Multichannel%2520Sensorimotor%250ACortex%2520Electrophysiology%2522%2520dataset%2520and%2520evaluate%2520it%2520using%2520the%2520NeuroBench%250Aframework%252C%2520targeting%2520both%2520tracks%2520of%2520the%2520IEEE%2520BioCAS%2520Grand%2520Challenge%2520on%2520Neural%250ADecoding.%2520Our%2520approach%2520achieves%2520high%2520accuracy%2520in%2520predicting%2520velocities%2520of%250Aprimate%2520reaching%2520movements%2520from%2520multichannel%2520primary%2520motor%2520cortex%2520recordings%250Awhile%2520maintaining%2520a%2520low%2520number%2520of%2520synaptic%2520operations%252C%2520surpassing%2520the%2520current%250Abaseline%2520models%2520in%2520the%2520NeuroBench%2520framework.%2520This%2520work%2520highlights%2520the%2520potential%250Aof%2520hybrid%2520neural%2520networks%2520to%2520facilitate%2520wireless%2520iBMIs%2520with%2520high%2520decoding%250Aprecision%2520and%2520a%2520substantial%2520increase%2520in%2520the%2520number%2520of%2520monitored%2520neurons%252C%2520paving%250Athe%2520way%2520toward%2520more%2520advanced%2520neuroprosthetic%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Spiking%20Neural%20Networks%20for%20Low-Power%20Intra-Cortical%0A%20%20Brain-Machine%20Interfaces&entry.906535625=Alexandru%20Vasilache%20and%20Jann%20Krausse%20and%20Klaus%20Knobloch%20and%20Juergen%20Becker&entry.1292438233=%20%20Intra-cortical%20brain-machine%20interfaces%20%28iBMIs%29%20have%20the%20potential%20to%0Adramatically%20improve%20the%20lives%20of%20people%20with%20paraplegia%20by%20restoring%20their%0Aability%20to%20perform%20daily%20activities.%20However%2C%20current%20iBMIs%20suffer%20from%0Ascalability%20and%20mobility%20limitations%20due%20to%20bulky%20hardware%20and%20wiring.%20Wireless%0AiBMIs%20offer%20a%20solution%20but%20are%20constrained%20by%20a%20limited%20data%20rate.%20To%20overcome%0Athis%20challenge%2C%20we%20are%20investigating%20hybrid%20spiking%20neural%20networks%20for%0Aembedded%20neural%20decoding%20in%20wireless%20iBMIs.%20The%20networks%20consist%20of%20a%20temporal%0Aconvolution-based%20compression%20followed%20by%20recurrent%20processing%20and%20a%20final%0Ainterpolation%20back%20to%20the%20original%20sequence%20length.%20As%20recurrent%20units%2C%20we%0Aexplore%20gated%20recurrent%20units%20%28GRUs%29%2C%20leaky%20integrate-and-fire%20%28LIF%29%20neurons%2C%0Aand%20a%20combination%20of%20both%20-%20spiking%20GRUs%20%28sGRUs%29%20and%20analyze%20their%20differences%0Ain%20terms%20of%20accuracy%2C%20footprint%2C%20and%20activation%20sparsity.%20To%20that%20end%2C%20we%20train%0Adecoders%20on%20the%20%22Nonhuman%20Primate%20Reaching%20with%20Multichannel%20Sensorimotor%0ACortex%20Electrophysiology%22%20dataset%20and%20evaluate%20it%20using%20the%20NeuroBench%0Aframework%2C%20targeting%20both%20tracks%20of%20the%20IEEE%20BioCAS%20Grand%20Challenge%20on%20Neural%0ADecoding.%20Our%20approach%20achieves%20high%20accuracy%20in%20predicting%20velocities%20of%0Aprimate%20reaching%20movements%20from%20multichannel%20primary%20motor%20cortex%20recordings%0Awhile%20maintaining%20a%20low%20number%20of%20synaptic%20operations%2C%20surpassing%20the%20current%0Abaseline%20models%20in%20the%20NeuroBench%20framework.%20This%20work%20highlights%20the%20potential%0Aof%20hybrid%20neural%20networks%20to%20facilitate%20wireless%20iBMIs%20with%20high%20decoding%0Aprecision%20and%20a%20substantial%20increase%20in%20the%20number%20of%20monitored%20neurons%2C%20paving%0Athe%20way%20toward%20more%20advanced%20neuroprosthetic%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04428v1&entry.124074799=Read"},
{"title": "From Calculation to Adjudication: Examining LLM judges on Mathematical\n  Reasoning Tasks", "author": "Andreas Stephan and Dawei Zhu and Matthias A\u00dfenmacher and Xiaoyu Shen and Benjamin Roth", "abstract": "  To reduce the need for human annotations, large language models (LLMs) have\nbeen proposed as judges of the quality of other candidate models. LLM judges\nare typically evaluated by measuring the correlation with human judgments on\ngeneration tasks such as summarization or machine translation. In contrast, we\nstudy LLM judges on mathematical reasoning tasks. These tasks require\nmulti-step reasoning, and the correctness of their solutions is verifiable,\nenabling a more objective evaluation. We perform a detailed performance\nanalysis and find that the used judges are mostly unable to improve task\nperformance but are able to pick the better model. Our analysis uncovers a\nstrong correlation between judgment performance and the candidate model task\nperformance. We observe that judges tend to choose the model of higher quality\neven if its answer is incorrect. Further, we show that it is possible to use\nstatistics, such as the task performances of the individual models, to predict\njudgment performance. In an ablation, we either swap or mask the candidate\nanswers and observe that judges often keep the original judgment, providing\nevidence that judges incorporate writing style in their judgments. In summary,\nwe find that regularities in the judgments are quantifiable using statistical\nmeasures and provide various angles on exploiting them.\n", "link": "http://arxiv.org/abs/2409.04168v1", "date": "2024-09-06", "relevancy": 1.9766, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Calculation%20to%20Adjudication%3A%20Examining%20LLM%20judges%20on%20Mathematical%0A%20%20Reasoning%20Tasks&body=Title%3A%20From%20Calculation%20to%20Adjudication%3A%20Examining%20LLM%20judges%20on%20Mathematical%0A%20%20Reasoning%20Tasks%0AAuthor%3A%20Andreas%20Stephan%20and%20Dawei%20Zhu%20and%20Matthias%20A%C3%9Fenmacher%20and%20Xiaoyu%20Shen%20and%20Benjamin%20Roth%0AAbstract%3A%20%20%20To%20reduce%20the%20need%20for%20human%20annotations%2C%20large%20language%20models%20%28LLMs%29%20have%0Abeen%20proposed%20as%20judges%20of%20the%20quality%20of%20other%20candidate%20models.%20LLM%20judges%0Aare%20typically%20evaluated%20by%20measuring%20the%20correlation%20with%20human%20judgments%20on%0Ageneration%20tasks%20such%20as%20summarization%20or%20machine%20translation.%20In%20contrast%2C%20we%0Astudy%20LLM%20judges%20on%20mathematical%20reasoning%20tasks.%20These%20tasks%20require%0Amulti-step%20reasoning%2C%20and%20the%20correctness%20of%20their%20solutions%20is%20verifiable%2C%0Aenabling%20a%20more%20objective%20evaluation.%20We%20perform%20a%20detailed%20performance%0Aanalysis%20and%20find%20that%20the%20used%20judges%20are%20mostly%20unable%20to%20improve%20task%0Aperformance%20but%20are%20able%20to%20pick%20the%20better%20model.%20Our%20analysis%20uncovers%20a%0Astrong%20correlation%20between%20judgment%20performance%20and%20the%20candidate%20model%20task%0Aperformance.%20We%20observe%20that%20judges%20tend%20to%20choose%20the%20model%20of%20higher%20quality%0Aeven%20if%20its%20answer%20is%20incorrect.%20Further%2C%20we%20show%20that%20it%20is%20possible%20to%20use%0Astatistics%2C%20such%20as%20the%20task%20performances%20of%20the%20individual%20models%2C%20to%20predict%0Ajudgment%20performance.%20In%20an%20ablation%2C%20we%20either%20swap%20or%20mask%20the%20candidate%0Aanswers%20and%20observe%20that%20judges%20often%20keep%20the%20original%20judgment%2C%20providing%0Aevidence%20that%20judges%20incorporate%20writing%20style%20in%20their%20judgments.%20In%20summary%2C%0Awe%20find%20that%20regularities%20in%20the%20judgments%20are%20quantifiable%20using%20statistical%0Ameasures%20and%20provide%20various%20angles%20on%20exploiting%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Calculation%2520to%2520Adjudication%253A%2520Examining%2520LLM%2520judges%2520on%2520Mathematical%250A%2520%2520Reasoning%2520Tasks%26entry.906535625%3DAndreas%2520Stephan%2520and%2520Dawei%2520Zhu%2520and%2520Matthias%2520A%25C3%259Fenmacher%2520and%2520Xiaoyu%2520Shen%2520and%2520Benjamin%2520Roth%26entry.1292438233%3D%2520%2520To%2520reduce%2520the%2520need%2520for%2520human%2520annotations%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Abeen%2520proposed%2520as%2520judges%2520of%2520the%2520quality%2520of%2520other%2520candidate%2520models.%2520LLM%2520judges%250Aare%2520typically%2520evaluated%2520by%2520measuring%2520the%2520correlation%2520with%2520human%2520judgments%2520on%250Ageneration%2520tasks%2520such%2520as%2520summarization%2520or%2520machine%2520translation.%2520In%2520contrast%252C%2520we%250Astudy%2520LLM%2520judges%2520on%2520mathematical%2520reasoning%2520tasks.%2520These%2520tasks%2520require%250Amulti-step%2520reasoning%252C%2520and%2520the%2520correctness%2520of%2520their%2520solutions%2520is%2520verifiable%252C%250Aenabling%2520a%2520more%2520objective%2520evaluation.%2520We%2520perform%2520a%2520detailed%2520performance%250Aanalysis%2520and%2520find%2520that%2520the%2520used%2520judges%2520are%2520mostly%2520unable%2520to%2520improve%2520task%250Aperformance%2520but%2520are%2520able%2520to%2520pick%2520the%2520better%2520model.%2520Our%2520analysis%2520uncovers%2520a%250Astrong%2520correlation%2520between%2520judgment%2520performance%2520and%2520the%2520candidate%2520model%2520task%250Aperformance.%2520We%2520observe%2520that%2520judges%2520tend%2520to%2520choose%2520the%2520model%2520of%2520higher%2520quality%250Aeven%2520if%2520its%2520answer%2520is%2520incorrect.%2520Further%252C%2520we%2520show%2520that%2520it%2520is%2520possible%2520to%2520use%250Astatistics%252C%2520such%2520as%2520the%2520task%2520performances%2520of%2520the%2520individual%2520models%252C%2520to%2520predict%250Ajudgment%2520performance.%2520In%2520an%2520ablation%252C%2520we%2520either%2520swap%2520or%2520mask%2520the%2520candidate%250Aanswers%2520and%2520observe%2520that%2520judges%2520often%2520keep%2520the%2520original%2520judgment%252C%2520providing%250Aevidence%2520that%2520judges%2520incorporate%2520writing%2520style%2520in%2520their%2520judgments.%2520In%2520summary%252C%250Awe%2520find%2520that%2520regularities%2520in%2520the%2520judgments%2520are%2520quantifiable%2520using%2520statistical%250Ameasures%2520and%2520provide%2520various%2520angles%2520on%2520exploiting%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Calculation%20to%20Adjudication%3A%20Examining%20LLM%20judges%20on%20Mathematical%0A%20%20Reasoning%20Tasks&entry.906535625=Andreas%20Stephan%20and%20Dawei%20Zhu%20and%20Matthias%20A%C3%9Fenmacher%20and%20Xiaoyu%20Shen%20and%20Benjamin%20Roth&entry.1292438233=%20%20To%20reduce%20the%20need%20for%20human%20annotations%2C%20large%20language%20models%20%28LLMs%29%20have%0Abeen%20proposed%20as%20judges%20of%20the%20quality%20of%20other%20candidate%20models.%20LLM%20judges%0Aare%20typically%20evaluated%20by%20measuring%20the%20correlation%20with%20human%20judgments%20on%0Ageneration%20tasks%20such%20as%20summarization%20or%20machine%20translation.%20In%20contrast%2C%20we%0Astudy%20LLM%20judges%20on%20mathematical%20reasoning%20tasks.%20These%20tasks%20require%0Amulti-step%20reasoning%2C%20and%20the%20correctness%20of%20their%20solutions%20is%20verifiable%2C%0Aenabling%20a%20more%20objective%20evaluation.%20We%20perform%20a%20detailed%20performance%0Aanalysis%20and%20find%20that%20the%20used%20judges%20are%20mostly%20unable%20to%20improve%20task%0Aperformance%20but%20are%20able%20to%20pick%20the%20better%20model.%20Our%20analysis%20uncovers%20a%0Astrong%20correlation%20between%20judgment%20performance%20and%20the%20candidate%20model%20task%0Aperformance.%20We%20observe%20that%20judges%20tend%20to%20choose%20the%20model%20of%20higher%20quality%0Aeven%20if%20its%20answer%20is%20incorrect.%20Further%2C%20we%20show%20that%20it%20is%20possible%20to%20use%0Astatistics%2C%20such%20as%20the%20task%20performances%20of%20the%20individual%20models%2C%20to%20predict%0Ajudgment%20performance.%20In%20an%20ablation%2C%20we%20either%20swap%20or%20mask%20the%20candidate%0Aanswers%20and%20observe%20that%20judges%20often%20keep%20the%20original%20judgment%2C%20providing%0Aevidence%20that%20judges%20incorporate%20writing%20style%20in%20their%20judgments.%20In%20summary%2C%0Awe%20find%20that%20regularities%20in%20the%20judgments%20are%20quantifiable%20using%20statistical%0Ameasures%20and%20provide%20various%20angles%20on%20exploiting%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04168v1&entry.124074799=Read"},
{"title": "WarpAdam: A new Adam optimizer based on Meta-Learning approach", "author": "Chengxi Pan and Junshang Chen and Jingrui Ye", "abstract": "  Optimal selection of optimization algorithms is crucial for training deep\nlearning models. The Adam optimizer has gained significant attention due to its\nefficiency and wide applicability. However, to enhance the adaptability of\noptimizers across diverse datasets, we propose an innovative optimization\nstrategy by integrating the 'warped gradient descend'concept from Meta Learning\ninto the Adam optimizer. In the conventional Adam optimizer, gradients are\nutilized to compute estimates of gradient mean and variance, subsequently\nupdating model parameters. Our approach introduces a learnable distortion\nmatrix, denoted as P, which is employed for linearly transforming gradients.\nThis transformation slightly adjusts gradients during each iteration, enabling\nthe optimizer to better adapt to distinct dataset characteristics. By learning\nan appropriate distortion matrix P, our method aims to adaptively adjust\ngradient information across different data distributions, thereby enhancing\noptimization performance. Our research showcases the potential of this novel\napproach through theoretical insights and empirical evaluations. Experimental\nresults across various tasks and datasets validate the superiority of our\noptimizer that integrates the 'warped gradient descend' concept in terms of\nadaptability. Furthermore, we explore effective strategies for training the\nadaptation matrix P and identify scenarios where this method can yield optimal\nresults. In summary, this study introduces an innovative approach that merges\nthe 'warped gradient descend' concept from Meta Learning with the Adam\noptimizer. By introducing a learnable distortion matrix P within the optimizer,\nwe aim to enhance the model's generalization capability across diverse data\ndistributions, thus opening up new possibilities in the field of deep learning\noptimization.\n", "link": "http://arxiv.org/abs/2409.04244v1", "date": "2024-09-06", "relevancy": 1.9604, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4799}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WarpAdam%3A%20A%20new%20Adam%20optimizer%20based%20on%20Meta-Learning%20approach&body=Title%3A%20WarpAdam%3A%20A%20new%20Adam%20optimizer%20based%20on%20Meta-Learning%20approach%0AAuthor%3A%20Chengxi%20Pan%20and%20Junshang%20Chen%20and%20Jingrui%20Ye%0AAbstract%3A%20%20%20Optimal%20selection%20of%20optimization%20algorithms%20is%20crucial%20for%20training%20deep%0Alearning%20models.%20The%20Adam%20optimizer%20has%20gained%20significant%20attention%20due%20to%20its%0Aefficiency%20and%20wide%20applicability.%20However%2C%20to%20enhance%20the%20adaptability%20of%0Aoptimizers%20across%20diverse%20datasets%2C%20we%20propose%20an%20innovative%20optimization%0Astrategy%20by%20integrating%20the%20%27warped%20gradient%20descend%27concept%20from%20Meta%20Learning%0Ainto%20the%20Adam%20optimizer.%20In%20the%20conventional%20Adam%20optimizer%2C%20gradients%20are%0Autilized%20to%20compute%20estimates%20of%20gradient%20mean%20and%20variance%2C%20subsequently%0Aupdating%20model%20parameters.%20Our%20approach%20introduces%20a%20learnable%20distortion%0Amatrix%2C%20denoted%20as%20P%2C%20which%20is%20employed%20for%20linearly%20transforming%20gradients.%0AThis%20transformation%20slightly%20adjusts%20gradients%20during%20each%20iteration%2C%20enabling%0Athe%20optimizer%20to%20better%20adapt%20to%20distinct%20dataset%20characteristics.%20By%20learning%0Aan%20appropriate%20distortion%20matrix%20P%2C%20our%20method%20aims%20to%20adaptively%20adjust%0Agradient%20information%20across%20different%20data%20distributions%2C%20thereby%20enhancing%0Aoptimization%20performance.%20Our%20research%20showcases%20the%20potential%20of%20this%20novel%0Aapproach%20through%20theoretical%20insights%20and%20empirical%20evaluations.%20Experimental%0Aresults%20across%20various%20tasks%20and%20datasets%20validate%20the%20superiority%20of%20our%0Aoptimizer%20that%20integrates%20the%20%27warped%20gradient%20descend%27%20concept%20in%20terms%20of%0Aadaptability.%20Furthermore%2C%20we%20explore%20effective%20strategies%20for%20training%20the%0Aadaptation%20matrix%20P%20and%20identify%20scenarios%20where%20this%20method%20can%20yield%20optimal%0Aresults.%20In%20summary%2C%20this%20study%20introduces%20an%20innovative%20approach%20that%20merges%0Athe%20%27warped%20gradient%20descend%27%20concept%20from%20Meta%20Learning%20with%20the%20Adam%0Aoptimizer.%20By%20introducing%20a%20learnable%20distortion%20matrix%20P%20within%20the%20optimizer%2C%0Awe%20aim%20to%20enhance%20the%20model%27s%20generalization%20capability%20across%20diverse%20data%0Adistributions%2C%20thus%20opening%20up%20new%20possibilities%20in%20the%20field%20of%20deep%20learning%0Aoptimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWarpAdam%253A%2520A%2520new%2520Adam%2520optimizer%2520based%2520on%2520Meta-Learning%2520approach%26entry.906535625%3DChengxi%2520Pan%2520and%2520Junshang%2520Chen%2520and%2520Jingrui%2520Ye%26entry.1292438233%3D%2520%2520Optimal%2520selection%2520of%2520optimization%2520algorithms%2520is%2520crucial%2520for%2520training%2520deep%250Alearning%2520models.%2520The%2520Adam%2520optimizer%2520has%2520gained%2520significant%2520attention%2520due%2520to%2520its%250Aefficiency%2520and%2520wide%2520applicability.%2520However%252C%2520to%2520enhance%2520the%2520adaptability%2520of%250Aoptimizers%2520across%2520diverse%2520datasets%252C%2520we%2520propose%2520an%2520innovative%2520optimization%250Astrategy%2520by%2520integrating%2520the%2520%2527warped%2520gradient%2520descend%2527concept%2520from%2520Meta%2520Learning%250Ainto%2520the%2520Adam%2520optimizer.%2520In%2520the%2520conventional%2520Adam%2520optimizer%252C%2520gradients%2520are%250Autilized%2520to%2520compute%2520estimates%2520of%2520gradient%2520mean%2520and%2520variance%252C%2520subsequently%250Aupdating%2520model%2520parameters.%2520Our%2520approach%2520introduces%2520a%2520learnable%2520distortion%250Amatrix%252C%2520denoted%2520as%2520P%252C%2520which%2520is%2520employed%2520for%2520linearly%2520transforming%2520gradients.%250AThis%2520transformation%2520slightly%2520adjusts%2520gradients%2520during%2520each%2520iteration%252C%2520enabling%250Athe%2520optimizer%2520to%2520better%2520adapt%2520to%2520distinct%2520dataset%2520characteristics.%2520By%2520learning%250Aan%2520appropriate%2520distortion%2520matrix%2520P%252C%2520our%2520method%2520aims%2520to%2520adaptively%2520adjust%250Agradient%2520information%2520across%2520different%2520data%2520distributions%252C%2520thereby%2520enhancing%250Aoptimization%2520performance.%2520Our%2520research%2520showcases%2520the%2520potential%2520of%2520this%2520novel%250Aapproach%2520through%2520theoretical%2520insights%2520and%2520empirical%2520evaluations.%2520Experimental%250Aresults%2520across%2520various%2520tasks%2520and%2520datasets%2520validate%2520the%2520superiority%2520of%2520our%250Aoptimizer%2520that%2520integrates%2520the%2520%2527warped%2520gradient%2520descend%2527%2520concept%2520in%2520terms%2520of%250Aadaptability.%2520Furthermore%252C%2520we%2520explore%2520effective%2520strategies%2520for%2520training%2520the%250Aadaptation%2520matrix%2520P%2520and%2520identify%2520scenarios%2520where%2520this%2520method%2520can%2520yield%2520optimal%250Aresults.%2520In%2520summary%252C%2520this%2520study%2520introduces%2520an%2520innovative%2520approach%2520that%2520merges%250Athe%2520%2527warped%2520gradient%2520descend%2527%2520concept%2520from%2520Meta%2520Learning%2520with%2520the%2520Adam%250Aoptimizer.%2520By%2520introducing%2520a%2520learnable%2520distortion%2520matrix%2520P%2520within%2520the%2520optimizer%252C%250Awe%2520aim%2520to%2520enhance%2520the%2520model%2527s%2520generalization%2520capability%2520across%2520diverse%2520data%250Adistributions%252C%2520thus%2520opening%2520up%2520new%2520possibilities%2520in%2520the%2520field%2520of%2520deep%2520learning%250Aoptimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WarpAdam%3A%20A%20new%20Adam%20optimizer%20based%20on%20Meta-Learning%20approach&entry.906535625=Chengxi%20Pan%20and%20Junshang%20Chen%20and%20Jingrui%20Ye&entry.1292438233=%20%20Optimal%20selection%20of%20optimization%20algorithms%20is%20crucial%20for%20training%20deep%0Alearning%20models.%20The%20Adam%20optimizer%20has%20gained%20significant%20attention%20due%20to%20its%0Aefficiency%20and%20wide%20applicability.%20However%2C%20to%20enhance%20the%20adaptability%20of%0Aoptimizers%20across%20diverse%20datasets%2C%20we%20propose%20an%20innovative%20optimization%0Astrategy%20by%20integrating%20the%20%27warped%20gradient%20descend%27concept%20from%20Meta%20Learning%0Ainto%20the%20Adam%20optimizer.%20In%20the%20conventional%20Adam%20optimizer%2C%20gradients%20are%0Autilized%20to%20compute%20estimates%20of%20gradient%20mean%20and%20variance%2C%20subsequently%0Aupdating%20model%20parameters.%20Our%20approach%20introduces%20a%20learnable%20distortion%0Amatrix%2C%20denoted%20as%20P%2C%20which%20is%20employed%20for%20linearly%20transforming%20gradients.%0AThis%20transformation%20slightly%20adjusts%20gradients%20during%20each%20iteration%2C%20enabling%0Athe%20optimizer%20to%20better%20adapt%20to%20distinct%20dataset%20characteristics.%20By%20learning%0Aan%20appropriate%20distortion%20matrix%20P%2C%20our%20method%20aims%20to%20adaptively%20adjust%0Agradient%20information%20across%20different%20data%20distributions%2C%20thereby%20enhancing%0Aoptimization%20performance.%20Our%20research%20showcases%20the%20potential%20of%20this%20novel%0Aapproach%20through%20theoretical%20insights%20and%20empirical%20evaluations.%20Experimental%0Aresults%20across%20various%20tasks%20and%20datasets%20validate%20the%20superiority%20of%20our%0Aoptimizer%20that%20integrates%20the%20%27warped%20gradient%20descend%27%20concept%20in%20terms%20of%0Aadaptability.%20Furthermore%2C%20we%20explore%20effective%20strategies%20for%20training%20the%0Aadaptation%20matrix%20P%20and%20identify%20scenarios%20where%20this%20method%20can%20yield%20optimal%0Aresults.%20In%20summary%2C%20this%20study%20introduces%20an%20innovative%20approach%20that%20merges%0Athe%20%27warped%20gradient%20descend%27%20concept%20from%20Meta%20Learning%20with%20the%20Adam%0Aoptimizer.%20By%20introducing%20a%20learnable%20distortion%20matrix%20P%20within%20the%20optimizer%2C%0Awe%20aim%20to%20enhance%20the%20model%27s%20generalization%20capability%20across%20diverse%20data%0Adistributions%2C%20thus%20opening%20up%20new%20possibilities%20in%20the%20field%20of%20deep%20learning%0Aoptimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04244v1&entry.124074799=Read"},
{"title": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models", "author": "Jing Cui and Yishi Xu and Zhewei Huang and Shuchang Zhou and Jianbin Jiao and Junge Zhang", "abstract": "  Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures.\n", "link": "http://arxiv.org/abs/2409.03274v2", "date": "2024-09-06", "relevancy": 1.946, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4928}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recent%20Advances%20in%20Attack%20and%20Defense%20Approaches%20of%20Large%20Language%0A%20%20Models&body=Title%3A%20Recent%20Advances%20in%20Attack%20and%20Defense%20Approaches%20of%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Jing%20Cui%20and%20Yishi%20Xu%20and%20Zhewei%20Huang%20and%20Shuchang%20Zhou%20and%20Jianbin%20Jiao%20and%20Junge%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20artificial%20intelligence%20and%0Amachine%20learning%20through%20their%20advanced%20text%20processing%20and%20generating%0Acapabilities.%20However%2C%20their%20widespread%20deployment%20has%20raised%20significant%0Asafety%20and%20reliability%20concerns.%20Established%20vulnerabilities%20in%20deep%20neural%0Anetworks%2C%20coupled%20with%20emerging%20threat%20models%2C%20may%20compromise%20security%0Aevaluations%20and%20create%20a%20false%20sense%20of%20security.%20Given%20the%20extensive%20research%0Ain%20the%20field%20of%20LLM%20security%2C%20we%20believe%20that%20summarizing%20the%20current%20state%20of%0Aaffairs%20will%20help%20the%20research%20community%20better%20understand%20the%20present%0Alandscape%20and%20inform%20future%20developments.%20This%20paper%20reviews%20current%20research%0Aon%20LLM%20vulnerabilities%20and%20threats%2C%20and%20evaluates%20the%20effectiveness%20of%0Acontemporary%20defense%20mechanisms.%20We%20analyze%20recent%20studies%20on%20attack%20vectors%0Aand%20model%20weaknesses%2C%20providing%20insights%20into%20attack%20mechanisms%20and%20the%0Aevolving%20threat%20landscape.%20We%20also%20examine%20current%20defense%20strategies%2C%0Ahighlighting%20their%20strengths%20and%20limitations.%20By%20contrasting%20advancements%20in%0Aattack%20and%20defense%20methodologies%2C%20we%20identify%20research%20gaps%20and%20propose%20future%0Adirections%20to%20enhance%20LLM%20security.%20Our%20goal%20is%20to%20advance%20the%20understanding%20of%0ALLM%20safety%20challenges%20and%20guide%20the%20development%20of%20more%20robust%20security%0Ameasures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03274v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecent%2520Advances%2520in%2520Attack%2520and%2520Defense%2520Approaches%2520of%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DJing%2520Cui%2520and%2520Yishi%2520Xu%2520and%2520Zhewei%2520Huang%2520and%2520Shuchang%2520Zhou%2520and%2520Jianbin%2520Jiao%2520and%2520Junge%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520artificial%2520intelligence%2520and%250Amachine%2520learning%2520through%2520their%2520advanced%2520text%2520processing%2520and%2520generating%250Acapabilities.%2520However%252C%2520their%2520widespread%2520deployment%2520has%2520raised%2520significant%250Asafety%2520and%2520reliability%2520concerns.%2520Established%2520vulnerabilities%2520in%2520deep%2520neural%250Anetworks%252C%2520coupled%2520with%2520emerging%2520threat%2520models%252C%2520may%2520compromise%2520security%250Aevaluations%2520and%2520create%2520a%2520false%2520sense%2520of%2520security.%2520Given%2520the%2520extensive%2520research%250Ain%2520the%2520field%2520of%2520LLM%2520security%252C%2520we%2520believe%2520that%2520summarizing%2520the%2520current%2520state%2520of%250Aaffairs%2520will%2520help%2520the%2520research%2520community%2520better%2520understand%2520the%2520present%250Alandscape%2520and%2520inform%2520future%2520developments.%2520This%2520paper%2520reviews%2520current%2520research%250Aon%2520LLM%2520vulnerabilities%2520and%2520threats%252C%2520and%2520evaluates%2520the%2520effectiveness%2520of%250Acontemporary%2520defense%2520mechanisms.%2520We%2520analyze%2520recent%2520studies%2520on%2520attack%2520vectors%250Aand%2520model%2520weaknesses%252C%2520providing%2520insights%2520into%2520attack%2520mechanisms%2520and%2520the%250Aevolving%2520threat%2520landscape.%2520We%2520also%2520examine%2520current%2520defense%2520strategies%252C%250Ahighlighting%2520their%2520strengths%2520and%2520limitations.%2520By%2520contrasting%2520advancements%2520in%250Aattack%2520and%2520defense%2520methodologies%252C%2520we%2520identify%2520research%2520gaps%2520and%2520propose%2520future%250Adirections%2520to%2520enhance%2520LLM%2520security.%2520Our%2520goal%2520is%2520to%2520advance%2520the%2520understanding%2520of%250ALLM%2520safety%2520challenges%2520and%2520guide%2520the%2520development%2520of%2520more%2520robust%2520security%250Ameasures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03274v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20Advances%20in%20Attack%20and%20Defense%20Approaches%20of%20Large%20Language%0A%20%20Models&entry.906535625=Jing%20Cui%20and%20Yishi%20Xu%20and%20Zhewei%20Huang%20and%20Shuchang%20Zhou%20and%20Jianbin%20Jiao%20and%20Junge%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20artificial%20intelligence%20and%0Amachine%20learning%20through%20their%20advanced%20text%20processing%20and%20generating%0Acapabilities.%20However%2C%20their%20widespread%20deployment%20has%20raised%20significant%0Asafety%20and%20reliability%20concerns.%20Established%20vulnerabilities%20in%20deep%20neural%0Anetworks%2C%20coupled%20with%20emerging%20threat%20models%2C%20may%20compromise%20security%0Aevaluations%20and%20create%20a%20false%20sense%20of%20security.%20Given%20the%20extensive%20research%0Ain%20the%20field%20of%20LLM%20security%2C%20we%20believe%20that%20summarizing%20the%20current%20state%20of%0Aaffairs%20will%20help%20the%20research%20community%20better%20understand%20the%20present%0Alandscape%20and%20inform%20future%20developments.%20This%20paper%20reviews%20current%20research%0Aon%20LLM%20vulnerabilities%20and%20threats%2C%20and%20evaluates%20the%20effectiveness%20of%0Acontemporary%20defense%20mechanisms.%20We%20analyze%20recent%20studies%20on%20attack%20vectors%0Aand%20model%20weaknesses%2C%20providing%20insights%20into%20attack%20mechanisms%20and%20the%0Aevolving%20threat%20landscape.%20We%20also%20examine%20current%20defense%20strategies%2C%0Ahighlighting%20their%20strengths%20and%20limitations.%20By%20contrasting%20advancements%20in%0Aattack%20and%20defense%20methodologies%2C%20we%20identify%20research%20gaps%20and%20propose%20future%0Adirections%20to%20enhance%20LLM%20security.%20Our%20goal%20is%20to%20advance%20the%20understanding%20of%0ALLM%20safety%20challenges%20and%20guide%20the%20development%20of%20more%20robust%20security%0Ameasures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03274v2&entry.124074799=Read"},
{"title": "Is my Data in your AI Model? Membership Inference Test with Application\n  to Face Images", "author": "Daniel DeAlcala and Aythami Morales and Julian Fierrez and Gonzalo Mancera and Ruben Tolosana and Javier Ortega-Garcia", "abstract": "  This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs).\n", "link": "http://arxiv.org/abs/2402.09225v2", "date": "2024-09-06", "relevancy": 1.9319, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4962}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4867}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20my%20Data%20in%20your%20AI%20Model%3F%20Membership%20Inference%20Test%20with%20Application%0A%20%20to%20Face%20Images&body=Title%3A%20Is%20my%20Data%20in%20your%20AI%20Model%3F%20Membership%20Inference%20Test%20with%20Application%0A%20%20to%20Face%20Images%0AAuthor%3A%20Daniel%20DeAlcala%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Gonzalo%20Mancera%20and%20Ruben%20Tolosana%20and%20Javier%20Ortega-Garcia%0AAbstract%3A%20%20%20This%20article%20introduces%20the%20Membership%20Inference%20Test%20%28MINT%29%2C%20a%20novel%0Aapproach%20that%20aims%20to%20empirically%20assess%20if%20given%20data%20was%20used%20during%20the%0Atraining%20of%20AI/ML%20models.%20Specifically%2C%20we%20propose%20two%20MINT%20architectures%0Adesigned%20to%20learn%20the%20distinct%20activation%20patterns%20that%20emerge%20when%20an%20Audited%0AModel%20is%20exposed%20to%20data%20used%20during%20its%20training%20process.%20These%20architectures%0Aare%20based%20on%20Multilayer%20Perceptrons%20%28MLPs%29%20and%20Convolutional%20Neural%20Networks%0A%28CNNs%29.%20The%20experimental%20framework%20focuses%20on%20the%20challenging%20task%20of%20Face%0ARecognition%2C%20considering%20three%20state-of-the-art%20Face%20Recognition%20systems.%0AExperiments%20are%20carried%20out%20using%20six%20publicly%20available%20databases%2C%20comprising%0Aover%2022%20million%20face%20images%20in%20total.%20Different%20experimental%20scenarios%20are%0Aconsidered%20depending%20on%20the%20context%20of%20the%20AI%20model%20to%20test.%20Our%20proposed%20MINT%0Aapproach%20achieves%20promising%20results%2C%20with%20up%20to%2090%25%20accuracy%2C%20indicating%20the%0Apotential%20to%20recognize%20if%20an%20AI%20model%20has%20been%20trained%20with%20specific%20data.%20The%0Aproposed%20MINT%20approach%20can%20serve%20to%20enforce%20privacy%20and%20fairness%20in%20several%20AI%0Aapplications%2C%20e.g.%2C%20revealing%20if%20sensitive%20or%20private%20data%20was%20used%20for%0Atraining%20or%20tuning%20Large%20Language%20Models%20%28LLMs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520my%2520Data%2520in%2520your%2520AI%2520Model%253F%2520Membership%2520Inference%2520Test%2520with%2520Application%250A%2520%2520to%2520Face%2520Images%26entry.906535625%3DDaniel%2520DeAlcala%2520and%2520Aythami%2520Morales%2520and%2520Julian%2520Fierrez%2520and%2520Gonzalo%2520Mancera%2520and%2520Ruben%2520Tolosana%2520and%2520Javier%2520Ortega-Garcia%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520the%2520Membership%2520Inference%2520Test%2520%2528MINT%2529%252C%2520a%2520novel%250Aapproach%2520that%2520aims%2520to%2520empirically%2520assess%2520if%2520given%2520data%2520was%2520used%2520during%2520the%250Atraining%2520of%2520AI/ML%2520models.%2520Specifically%252C%2520we%2520propose%2520two%2520MINT%2520architectures%250Adesigned%2520to%2520learn%2520the%2520distinct%2520activation%2520patterns%2520that%2520emerge%2520when%2520an%2520Audited%250AModel%2520is%2520exposed%2520to%2520data%2520used%2520during%2520its%2520training%2520process.%2520These%2520architectures%250Aare%2520based%2520on%2520Multilayer%2520Perceptrons%2520%2528MLPs%2529%2520and%2520Convolutional%2520Neural%2520Networks%250A%2528CNNs%2529.%2520The%2520experimental%2520framework%2520focuses%2520on%2520the%2520challenging%2520task%2520of%2520Face%250ARecognition%252C%2520considering%2520three%2520state-of-the-art%2520Face%2520Recognition%2520systems.%250AExperiments%2520are%2520carried%2520out%2520using%2520six%2520publicly%2520available%2520databases%252C%2520comprising%250Aover%252022%2520million%2520face%2520images%2520in%2520total.%2520Different%2520experimental%2520scenarios%2520are%250Aconsidered%2520depending%2520on%2520the%2520context%2520of%2520the%2520AI%2520model%2520to%2520test.%2520Our%2520proposed%2520MINT%250Aapproach%2520achieves%2520promising%2520results%252C%2520with%2520up%2520to%252090%2525%2520accuracy%252C%2520indicating%2520the%250Apotential%2520to%2520recognize%2520if%2520an%2520AI%2520model%2520has%2520been%2520trained%2520with%2520specific%2520data.%2520The%250Aproposed%2520MINT%2520approach%2520can%2520serve%2520to%2520enforce%2520privacy%2520and%2520fairness%2520in%2520several%2520AI%250Aapplications%252C%2520e.g.%252C%2520revealing%2520if%2520sensitive%2520or%2520private%2520data%2520was%2520used%2520for%250Atraining%2520or%2520tuning%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20my%20Data%20in%20your%20AI%20Model%3F%20Membership%20Inference%20Test%20with%20Application%0A%20%20to%20Face%20Images&entry.906535625=Daniel%20DeAlcala%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Gonzalo%20Mancera%20and%20Ruben%20Tolosana%20and%20Javier%20Ortega-Garcia&entry.1292438233=%20%20This%20article%20introduces%20the%20Membership%20Inference%20Test%20%28MINT%29%2C%20a%20novel%0Aapproach%20that%20aims%20to%20empirically%20assess%20if%20given%20data%20was%20used%20during%20the%0Atraining%20of%20AI/ML%20models.%20Specifically%2C%20we%20propose%20two%20MINT%20architectures%0Adesigned%20to%20learn%20the%20distinct%20activation%20patterns%20that%20emerge%20when%20an%20Audited%0AModel%20is%20exposed%20to%20data%20used%20during%20its%20training%20process.%20These%20architectures%0Aare%20based%20on%20Multilayer%20Perceptrons%20%28MLPs%29%20and%20Convolutional%20Neural%20Networks%0A%28CNNs%29.%20The%20experimental%20framework%20focuses%20on%20the%20challenging%20task%20of%20Face%0ARecognition%2C%20considering%20three%20state-of-the-art%20Face%20Recognition%20systems.%0AExperiments%20are%20carried%20out%20using%20six%20publicly%20available%20databases%2C%20comprising%0Aover%2022%20million%20face%20images%20in%20total.%20Different%20experimental%20scenarios%20are%0Aconsidered%20depending%20on%20the%20context%20of%20the%20AI%20model%20to%20test.%20Our%20proposed%20MINT%0Aapproach%20achieves%20promising%20results%2C%20with%20up%20to%2090%25%20accuracy%2C%20indicating%20the%0Apotential%20to%20recognize%20if%20an%20AI%20model%20has%20been%20trained%20with%20specific%20data.%20The%0Aproposed%20MINT%20approach%20can%20serve%20to%20enforce%20privacy%20and%20fairness%20in%20several%20AI%0Aapplications%2C%20e.g.%2C%20revealing%20if%20sensitive%20or%20private%20data%20was%20used%20for%0Atraining%20or%20tuning%20Large%20Language%20Models%20%28LLMs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09225v2&entry.124074799=Read"},
{"title": "Universal randomised signatures for generative time series modelling", "author": "Francesca Biagini and Lukas Gonon and Niklas Walter", "abstract": "  Randomised signature has been proposed as a flexible and easily implementable\nalternative to the well-established path signature. In this article, we employ\nrandomised signature to introduce a generative model for financial time series\ndata in the spirit of reservoir computing. Specifically, we propose a novel\nWasserstein-type distance based on discrete-time randomised signatures. This\nmetric on the space of probability measures captures the distance between\n(conditional) distributions. Its use is justified by our novel universal\napproximation results for randomised signatures on the space of continuous\nfunctions taking the underlying path as an input. We then use our metric as the\nloss function in a non-adversarial generator model for synthetic time series\ndata based on a reservoir neural stochastic differential equation. We compare\nthe results of our model to benchmarks from the existing literature.\n", "link": "http://arxiv.org/abs/2406.10214v2", "date": "2024-09-06", "relevancy": 1.9297, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5149}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4855}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20randomised%20signatures%20for%20generative%20time%20series%20modelling&body=Title%3A%20Universal%20randomised%20signatures%20for%20generative%20time%20series%20modelling%0AAuthor%3A%20Francesca%20Biagini%20and%20Lukas%20Gonon%20and%20Niklas%20Walter%0AAbstract%3A%20%20%20Randomised%20signature%20has%20been%20proposed%20as%20a%20flexible%20and%20easily%20implementable%0Aalternative%20to%20the%20well-established%20path%20signature.%20In%20this%20article%2C%20we%20employ%0Arandomised%20signature%20to%20introduce%20a%20generative%20model%20for%20financial%20time%20series%0Adata%20in%20the%20spirit%20of%20reservoir%20computing.%20Specifically%2C%20we%20propose%20a%20novel%0AWasserstein-type%20distance%20based%20on%20discrete-time%20randomised%20signatures.%20This%0Ametric%20on%20the%20space%20of%20probability%20measures%20captures%20the%20distance%20between%0A%28conditional%29%20distributions.%20Its%20use%20is%20justified%20by%20our%20novel%20universal%0Aapproximation%20results%20for%20randomised%20signatures%20on%20the%20space%20of%20continuous%0Afunctions%20taking%20the%20underlying%20path%20as%20an%20input.%20We%20then%20use%20our%20metric%20as%20the%0Aloss%20function%20in%20a%20non-adversarial%20generator%20model%20for%20synthetic%20time%20series%0Adata%20based%20on%20a%20reservoir%20neural%20stochastic%20differential%20equation.%20We%20compare%0Athe%20results%20of%20our%20model%20to%20benchmarks%20from%20the%20existing%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520randomised%2520signatures%2520for%2520generative%2520time%2520series%2520modelling%26entry.906535625%3DFrancesca%2520Biagini%2520and%2520Lukas%2520Gonon%2520and%2520Niklas%2520Walter%26entry.1292438233%3D%2520%2520Randomised%2520signature%2520has%2520been%2520proposed%2520as%2520a%2520flexible%2520and%2520easily%2520implementable%250Aalternative%2520to%2520the%2520well-established%2520path%2520signature.%2520In%2520this%2520article%252C%2520we%2520employ%250Arandomised%2520signature%2520to%2520introduce%2520a%2520generative%2520model%2520for%2520financial%2520time%2520series%250Adata%2520in%2520the%2520spirit%2520of%2520reservoir%2520computing.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%250AWasserstein-type%2520distance%2520based%2520on%2520discrete-time%2520randomised%2520signatures.%2520This%250Ametric%2520on%2520the%2520space%2520of%2520probability%2520measures%2520captures%2520the%2520distance%2520between%250A%2528conditional%2529%2520distributions.%2520Its%2520use%2520is%2520justified%2520by%2520our%2520novel%2520universal%250Aapproximation%2520results%2520for%2520randomised%2520signatures%2520on%2520the%2520space%2520of%2520continuous%250Afunctions%2520taking%2520the%2520underlying%2520path%2520as%2520an%2520input.%2520We%2520then%2520use%2520our%2520metric%2520as%2520the%250Aloss%2520function%2520in%2520a%2520non-adversarial%2520generator%2520model%2520for%2520synthetic%2520time%2520series%250Adata%2520based%2520on%2520a%2520reservoir%2520neural%2520stochastic%2520differential%2520equation.%2520We%2520compare%250Athe%2520results%2520of%2520our%2520model%2520to%2520benchmarks%2520from%2520the%2520existing%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20randomised%20signatures%20for%20generative%20time%20series%20modelling&entry.906535625=Francesca%20Biagini%20and%20Lukas%20Gonon%20and%20Niklas%20Walter&entry.1292438233=%20%20Randomised%20signature%20has%20been%20proposed%20as%20a%20flexible%20and%20easily%20implementable%0Aalternative%20to%20the%20well-established%20path%20signature.%20In%20this%20article%2C%20we%20employ%0Arandomised%20signature%20to%20introduce%20a%20generative%20model%20for%20financial%20time%20series%0Adata%20in%20the%20spirit%20of%20reservoir%20computing.%20Specifically%2C%20we%20propose%20a%20novel%0AWasserstein-type%20distance%20based%20on%20discrete-time%20randomised%20signatures.%20This%0Ametric%20on%20the%20space%20of%20probability%20measures%20captures%20the%20distance%20between%0A%28conditional%29%20distributions.%20Its%20use%20is%20justified%20by%20our%20novel%20universal%0Aapproximation%20results%20for%20randomised%20signatures%20on%20the%20space%20of%20continuous%0Afunctions%20taking%20the%20underlying%20path%20as%20an%20input.%20We%20then%20use%20our%20metric%20as%20the%0Aloss%20function%20in%20a%20non-adversarial%20generator%20model%20for%20synthetic%20time%20series%0Adata%20based%20on%20a%20reservoir%20neural%20stochastic%20differential%20equation.%20We%20compare%0Athe%20results%20of%20our%20model%20to%20benchmarks%20from%20the%20existing%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10214v2&entry.124074799=Read"},
{"title": "Hybrid Semantic Search: Unveiling User Intent Beyond Keywords", "author": "Aman Ahluwalia and Bishwajit Sutradhar and Karishma Ghosh and Indrapal Yadav and Arpan Sheetal and Prashant Patil", "abstract": "  This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes.\n", "link": "http://arxiv.org/abs/2408.09236v3", "date": "2024-09-06", "relevancy": 1.9284, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4833}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4833}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Semantic%20Search%3A%20Unveiling%20User%20Intent%20Beyond%20Keywords&body=Title%3A%20Hybrid%20Semantic%20Search%3A%20Unveiling%20User%20Intent%20Beyond%20Keywords%0AAuthor%3A%20Aman%20Ahluwalia%20and%20Bishwajit%20Sutradhar%20and%20Karishma%20Ghosh%20and%20Indrapal%20Yadav%20and%20Arpan%20Sheetal%20and%20Prashant%20Patil%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20limitations%20of%20traditional%20keyword-based%20search%20in%0Aunderstanding%20user%20intent%20and%20introduces%20a%20novel%20hybrid%20search%20approach%20that%0Aleverages%20the%20strengths%20of%20non-semantic%20search%20engines%2C%20Large%20Language%20Models%0A%28LLMs%29%2C%20and%20embedding%20models.%20The%20proposed%20system%20integrates%20keyword%20matching%2C%0Asemantic%20vector%20embeddings%2C%20and%20LLM-generated%20structured%20queries%20to%20deliver%0Ahighly%20relevant%20and%20contextually%20appropriate%20search%20results.%20By%20combining%20these%0Acomplementary%20methods%2C%20the%20hybrid%20approach%20effectively%20captures%20both%20explicit%0Aand%20implicit%20user%20intent.The%20paper%20further%20explores%20techniques%20to%20optimize%0Aquery%20execution%20for%20faster%20response%20times%20and%20demonstrates%20the%20effectiveness%20of%0Athis%20hybrid%20search%20model%20in%20producing%20comprehensive%20and%20accurate%20search%0Aoutcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09236v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Semantic%2520Search%253A%2520Unveiling%2520User%2520Intent%2520Beyond%2520Keywords%26entry.906535625%3DAman%2520Ahluwalia%2520and%2520Bishwajit%2520Sutradhar%2520and%2520Karishma%2520Ghosh%2520and%2520Indrapal%2520Yadav%2520and%2520Arpan%2520Sheetal%2520and%2520Prashant%2520Patil%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520limitations%2520of%2520traditional%2520keyword-based%2520search%2520in%250Aunderstanding%2520user%2520intent%2520and%2520introduces%2520a%2520novel%2520hybrid%2520search%2520approach%2520that%250Aleverages%2520the%2520strengths%2520of%2520non-semantic%2520search%2520engines%252C%2520Large%2520Language%2520Models%250A%2528LLMs%2529%252C%2520and%2520embedding%2520models.%2520The%2520proposed%2520system%2520integrates%2520keyword%2520matching%252C%250Asemantic%2520vector%2520embeddings%252C%2520and%2520LLM-generated%2520structured%2520queries%2520to%2520deliver%250Ahighly%2520relevant%2520and%2520contextually%2520appropriate%2520search%2520results.%2520By%2520combining%2520these%250Acomplementary%2520methods%252C%2520the%2520hybrid%2520approach%2520effectively%2520captures%2520both%2520explicit%250Aand%2520implicit%2520user%2520intent.The%2520paper%2520further%2520explores%2520techniques%2520to%2520optimize%250Aquery%2520execution%2520for%2520faster%2520response%2520times%2520and%2520demonstrates%2520the%2520effectiveness%2520of%250Athis%2520hybrid%2520search%2520model%2520in%2520producing%2520comprehensive%2520and%2520accurate%2520search%250Aoutcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09236v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Semantic%20Search%3A%20Unveiling%20User%20Intent%20Beyond%20Keywords&entry.906535625=Aman%20Ahluwalia%20and%20Bishwajit%20Sutradhar%20and%20Karishma%20Ghosh%20and%20Indrapal%20Yadav%20and%20Arpan%20Sheetal%20and%20Prashant%20Patil&entry.1292438233=%20%20This%20paper%20addresses%20the%20limitations%20of%20traditional%20keyword-based%20search%20in%0Aunderstanding%20user%20intent%20and%20introduces%20a%20novel%20hybrid%20search%20approach%20that%0Aleverages%20the%20strengths%20of%20non-semantic%20search%20engines%2C%20Large%20Language%20Models%0A%28LLMs%29%2C%20and%20embedding%20models.%20The%20proposed%20system%20integrates%20keyword%20matching%2C%0Asemantic%20vector%20embeddings%2C%20and%20LLM-generated%20structured%20queries%20to%20deliver%0Ahighly%20relevant%20and%20contextually%20appropriate%20search%20results.%20By%20combining%20these%0Acomplementary%20methods%2C%20the%20hybrid%20approach%20effectively%20captures%20both%20explicit%0Aand%20implicit%20user%20intent.The%20paper%20further%20explores%20techniques%20to%20optimize%0Aquery%20execution%20for%20faster%20response%20times%20and%20demonstrates%20the%20effectiveness%20of%0Athis%20hybrid%20search%20model%20in%20producing%20comprehensive%20and%20accurate%20search%0Aoutcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09236v3&entry.124074799=Read"},
{"title": "TacoGFN: Target-conditioned GFlowNet for Structure-based Drug Design", "author": "Tony Shen and Seonghwan Seo and Grayson Lee and Mohit Pandey and Jason R Smith and Artem Cherkasov and Woo Youn Kim and Martin Ester", "abstract": "  Searching the vast chemical space for drug-like molecules that bind with a\nprotein pocket is a challenging task in drug discovery. Recently,\nstructure-based generative models have been introduced which promise to be more\nefficient by learning to generate molecules for any given protein structure.\nHowever, since they learn the distribution of a limited protein-ligand complex\ndataset, structure-based methods do not yet outperform optimization-based\nmethods that generate binding molecules for just one pocket. To overcome\nlimitations on data while leveraging learning across protein targets, we choose\nto model the reward distribution conditioned on pocket structure, instead of\nthe training data distribution. We design TacoGFN, a novel GFlowNet-based\napproach for structure-based drug design, which can generate molecules\nconditioned on any protein pocket structure with probabilities proportional to\nits affinity and property rewards. In the generative setting for\nCrossDocked2020 benchmark, TacoGFN attains a state-of-the-art success rate of\n$56.0\\%$ and $-8.44$ kcal/mol in median Vina Dock score while improving the\ngeneration time by multiple orders of magnitude. Fine-tuning TacoGFN further\nimproves the median Vina Dock score to $-10.93$ kcal/mol and the success rate\nto $88.8\\%$, outperforming all optimization-based methods.\n", "link": "http://arxiv.org/abs/2310.03223v6", "date": "2024-09-06", "relevancy": 1.9245, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5139}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4802}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TacoGFN%3A%20Target-conditioned%20GFlowNet%20for%20Structure-based%20Drug%20Design&body=Title%3A%20TacoGFN%3A%20Target-conditioned%20GFlowNet%20for%20Structure-based%20Drug%20Design%0AAuthor%3A%20Tony%20Shen%20and%20Seonghwan%20Seo%20and%20Grayson%20Lee%20and%20Mohit%20Pandey%20and%20Jason%20R%20Smith%20and%20Artem%20Cherkasov%20and%20Woo%20Youn%20Kim%20and%20Martin%20Ester%0AAbstract%3A%20%20%20Searching%20the%20vast%20chemical%20space%20for%20drug-like%20molecules%20that%20bind%20with%20a%0Aprotein%20pocket%20is%20a%20challenging%20task%20in%20drug%20discovery.%20Recently%2C%0Astructure-based%20generative%20models%20have%20been%20introduced%20which%20promise%20to%20be%20more%0Aefficient%20by%20learning%20to%20generate%20molecules%20for%20any%20given%20protein%20structure.%0AHowever%2C%20since%20they%20learn%20the%20distribution%20of%20a%20limited%20protein-ligand%20complex%0Adataset%2C%20structure-based%20methods%20do%20not%20yet%20outperform%20optimization-based%0Amethods%20that%20generate%20binding%20molecules%20for%20just%20one%20pocket.%20To%20overcome%0Alimitations%20on%20data%20while%20leveraging%20learning%20across%20protein%20targets%2C%20we%20choose%0Ato%20model%20the%20reward%20distribution%20conditioned%20on%20pocket%20structure%2C%20instead%20of%0Athe%20training%20data%20distribution.%20We%20design%20TacoGFN%2C%20a%20novel%20GFlowNet-based%0Aapproach%20for%20structure-based%20drug%20design%2C%20which%20can%20generate%20molecules%0Aconditioned%20on%20any%20protein%20pocket%20structure%20with%20probabilities%20proportional%20to%0Aits%20affinity%20and%20property%20rewards.%20In%20the%20generative%20setting%20for%0ACrossDocked2020%20benchmark%2C%20TacoGFN%20attains%20a%20state-of-the-art%20success%20rate%20of%0A%2456.0%5C%25%24%20and%20%24-8.44%24%20kcal/mol%20in%20median%20Vina%20Dock%20score%20while%20improving%20the%0Ageneration%20time%20by%20multiple%20orders%20of%20magnitude.%20Fine-tuning%20TacoGFN%20further%0Aimproves%20the%20median%20Vina%20Dock%20score%20to%20%24-10.93%24%20kcal/mol%20and%20the%20success%20rate%0Ato%20%2488.8%5C%25%24%2C%20outperforming%20all%20optimization-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03223v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTacoGFN%253A%2520Target-conditioned%2520GFlowNet%2520for%2520Structure-based%2520Drug%2520Design%26entry.906535625%3DTony%2520Shen%2520and%2520Seonghwan%2520Seo%2520and%2520Grayson%2520Lee%2520and%2520Mohit%2520Pandey%2520and%2520Jason%2520R%2520Smith%2520and%2520Artem%2520Cherkasov%2520and%2520Woo%2520Youn%2520Kim%2520and%2520Martin%2520Ester%26entry.1292438233%3D%2520%2520Searching%2520the%2520vast%2520chemical%2520space%2520for%2520drug-like%2520molecules%2520that%2520bind%2520with%2520a%250Aprotein%2520pocket%2520is%2520a%2520challenging%2520task%2520in%2520drug%2520discovery.%2520Recently%252C%250Astructure-based%2520generative%2520models%2520have%2520been%2520introduced%2520which%2520promise%2520to%2520be%2520more%250Aefficient%2520by%2520learning%2520to%2520generate%2520molecules%2520for%2520any%2520given%2520protein%2520structure.%250AHowever%252C%2520since%2520they%2520learn%2520the%2520distribution%2520of%2520a%2520limited%2520protein-ligand%2520complex%250Adataset%252C%2520structure-based%2520methods%2520do%2520not%2520yet%2520outperform%2520optimization-based%250Amethods%2520that%2520generate%2520binding%2520molecules%2520for%2520just%2520one%2520pocket.%2520To%2520overcome%250Alimitations%2520on%2520data%2520while%2520leveraging%2520learning%2520across%2520protein%2520targets%252C%2520we%2520choose%250Ato%2520model%2520the%2520reward%2520distribution%2520conditioned%2520on%2520pocket%2520structure%252C%2520instead%2520of%250Athe%2520training%2520data%2520distribution.%2520We%2520design%2520TacoGFN%252C%2520a%2520novel%2520GFlowNet-based%250Aapproach%2520for%2520structure-based%2520drug%2520design%252C%2520which%2520can%2520generate%2520molecules%250Aconditioned%2520on%2520any%2520protein%2520pocket%2520structure%2520with%2520probabilities%2520proportional%2520to%250Aits%2520affinity%2520and%2520property%2520rewards.%2520In%2520the%2520generative%2520setting%2520for%250ACrossDocked2020%2520benchmark%252C%2520TacoGFN%2520attains%2520a%2520state-of-the-art%2520success%2520rate%2520of%250A%252456.0%255C%2525%2524%2520and%2520%2524-8.44%2524%2520kcal/mol%2520in%2520median%2520Vina%2520Dock%2520score%2520while%2520improving%2520the%250Ageneration%2520time%2520by%2520multiple%2520orders%2520of%2520magnitude.%2520Fine-tuning%2520TacoGFN%2520further%250Aimproves%2520the%2520median%2520Vina%2520Dock%2520score%2520to%2520%2524-10.93%2524%2520kcal/mol%2520and%2520the%2520success%2520rate%250Ato%2520%252488.8%255C%2525%2524%252C%2520outperforming%2520all%2520optimization-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03223v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TacoGFN%3A%20Target-conditioned%20GFlowNet%20for%20Structure-based%20Drug%20Design&entry.906535625=Tony%20Shen%20and%20Seonghwan%20Seo%20and%20Grayson%20Lee%20and%20Mohit%20Pandey%20and%20Jason%20R%20Smith%20and%20Artem%20Cherkasov%20and%20Woo%20Youn%20Kim%20and%20Martin%20Ester&entry.1292438233=%20%20Searching%20the%20vast%20chemical%20space%20for%20drug-like%20molecules%20that%20bind%20with%20a%0Aprotein%20pocket%20is%20a%20challenging%20task%20in%20drug%20discovery.%20Recently%2C%0Astructure-based%20generative%20models%20have%20been%20introduced%20which%20promise%20to%20be%20more%0Aefficient%20by%20learning%20to%20generate%20molecules%20for%20any%20given%20protein%20structure.%0AHowever%2C%20since%20they%20learn%20the%20distribution%20of%20a%20limited%20protein-ligand%20complex%0Adataset%2C%20structure-based%20methods%20do%20not%20yet%20outperform%20optimization-based%0Amethods%20that%20generate%20binding%20molecules%20for%20just%20one%20pocket.%20To%20overcome%0Alimitations%20on%20data%20while%20leveraging%20learning%20across%20protein%20targets%2C%20we%20choose%0Ato%20model%20the%20reward%20distribution%20conditioned%20on%20pocket%20structure%2C%20instead%20of%0Athe%20training%20data%20distribution.%20We%20design%20TacoGFN%2C%20a%20novel%20GFlowNet-based%0Aapproach%20for%20structure-based%20drug%20design%2C%20which%20can%20generate%20molecules%0Aconditioned%20on%20any%20protein%20pocket%20structure%20with%20probabilities%20proportional%20to%0Aits%20affinity%20and%20property%20rewards.%20In%20the%20generative%20setting%20for%0ACrossDocked2020%20benchmark%2C%20TacoGFN%20attains%20a%20state-of-the-art%20success%20rate%20of%0A%2456.0%5C%25%24%20and%20%24-8.44%24%20kcal/mol%20in%20median%20Vina%20Dock%20score%20while%20improving%20the%0Ageneration%20time%20by%20multiple%20orders%20of%20magnitude.%20Fine-tuning%20TacoGFN%20further%0Aimproves%20the%20median%20Vina%20Dock%20score%20to%20%24-10.93%24%20kcal/mol%20and%20the%20success%20rate%0Ato%20%2488.8%5C%25%24%2C%20outperforming%20all%20optimization-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03223v6&entry.124074799=Read"},
{"title": "Enhancing Skin Lesion Diagnosis with Ensemble Learning", "author": "Xiaoyi Liu and Zhou Yu and Lianghao Tan and Yafeng Yan and Ge Shi", "abstract": "  Skin lesions are an increasingly significant medical concern, varying widely\nin severity from benign to cancerous. Accurate diagnosis is essential for\nensuring timely and appropriate treatment. This study examines the\nimplementation of deep learning methods to assist in the diagnosis of skin\nlesions using the HAM10000 dataset, which contains seven distinct types of\nlesions. First, we evaluated three pre-trained models: MobileNetV2, ResNet18,\nand VGG11, achieving accuracies of 0.798, 0.802, and 0.805, respectively. To\nfurther enhance classification accuracy, we developed ensemble models employing\nmax voting, average voting, and stacking, resulting in accuracies of 0.803,\n0.82, and 0.83. Building on the best-performing ensemble learning model,\nstacking, we developed our proposed model, SkinNet, which incorporates a\ncustomized architecture and fine-tuning, achieving an accuracy of 0.867 and an\nAUC of 0.96. This substantial improvement over individual models demonstrates\nthe effectiveness of ensemble learning in improving skin lesion classification.\n", "link": "http://arxiv.org/abs/2409.04381v1", "date": "2024-09-06", "relevancy": 1.9176, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4804}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4797}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Skin%20Lesion%20Diagnosis%20with%20Ensemble%20Learning&body=Title%3A%20Enhancing%20Skin%20Lesion%20Diagnosis%20with%20Ensemble%20Learning%0AAuthor%3A%20Xiaoyi%20Liu%20and%20Zhou%20Yu%20and%20Lianghao%20Tan%20and%20Yafeng%20Yan%20and%20Ge%20Shi%0AAbstract%3A%20%20%20Skin%20lesions%20are%20an%20increasingly%20significant%20medical%20concern%2C%20varying%20widely%0Ain%20severity%20from%20benign%20to%20cancerous.%20Accurate%20diagnosis%20is%20essential%20for%0Aensuring%20timely%20and%20appropriate%20treatment.%20This%20study%20examines%20the%0Aimplementation%20of%20deep%20learning%20methods%20to%20assist%20in%20the%20diagnosis%20of%20skin%0Alesions%20using%20the%20HAM10000%20dataset%2C%20which%20contains%20seven%20distinct%20types%20of%0Alesions.%20First%2C%20we%20evaluated%20three%20pre-trained%20models%3A%20MobileNetV2%2C%20ResNet18%2C%0Aand%20VGG11%2C%20achieving%20accuracies%20of%200.798%2C%200.802%2C%20and%200.805%2C%20respectively.%20To%0Afurther%20enhance%20classification%20accuracy%2C%20we%20developed%20ensemble%20models%20employing%0Amax%20voting%2C%20average%20voting%2C%20and%20stacking%2C%20resulting%20in%20accuracies%20of%200.803%2C%0A0.82%2C%20and%200.83.%20Building%20on%20the%20best-performing%20ensemble%20learning%20model%2C%0Astacking%2C%20we%20developed%20our%20proposed%20model%2C%20SkinNet%2C%20which%20incorporates%20a%0Acustomized%20architecture%20and%20fine-tuning%2C%20achieving%20an%20accuracy%20of%200.867%20and%20an%0AAUC%20of%200.96.%20This%20substantial%20improvement%20over%20individual%20models%20demonstrates%0Athe%20effectiveness%20of%20ensemble%20learning%20in%20improving%20skin%20lesion%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Skin%2520Lesion%2520Diagnosis%2520with%2520Ensemble%2520Learning%26entry.906535625%3DXiaoyi%2520Liu%2520and%2520Zhou%2520Yu%2520and%2520Lianghao%2520Tan%2520and%2520Yafeng%2520Yan%2520and%2520Ge%2520Shi%26entry.1292438233%3D%2520%2520Skin%2520lesions%2520are%2520an%2520increasingly%2520significant%2520medical%2520concern%252C%2520varying%2520widely%250Ain%2520severity%2520from%2520benign%2520to%2520cancerous.%2520Accurate%2520diagnosis%2520is%2520essential%2520for%250Aensuring%2520timely%2520and%2520appropriate%2520treatment.%2520This%2520study%2520examines%2520the%250Aimplementation%2520of%2520deep%2520learning%2520methods%2520to%2520assist%2520in%2520the%2520diagnosis%2520of%2520skin%250Alesions%2520using%2520the%2520HAM10000%2520dataset%252C%2520which%2520contains%2520seven%2520distinct%2520types%2520of%250Alesions.%2520First%252C%2520we%2520evaluated%2520three%2520pre-trained%2520models%253A%2520MobileNetV2%252C%2520ResNet18%252C%250Aand%2520VGG11%252C%2520achieving%2520accuracies%2520of%25200.798%252C%25200.802%252C%2520and%25200.805%252C%2520respectively.%2520To%250Afurther%2520enhance%2520classification%2520accuracy%252C%2520we%2520developed%2520ensemble%2520models%2520employing%250Amax%2520voting%252C%2520average%2520voting%252C%2520and%2520stacking%252C%2520resulting%2520in%2520accuracies%2520of%25200.803%252C%250A0.82%252C%2520and%25200.83.%2520Building%2520on%2520the%2520best-performing%2520ensemble%2520learning%2520model%252C%250Astacking%252C%2520we%2520developed%2520our%2520proposed%2520model%252C%2520SkinNet%252C%2520which%2520incorporates%2520a%250Acustomized%2520architecture%2520and%2520fine-tuning%252C%2520achieving%2520an%2520accuracy%2520of%25200.867%2520and%2520an%250AAUC%2520of%25200.96.%2520This%2520substantial%2520improvement%2520over%2520individual%2520models%2520demonstrates%250Athe%2520effectiveness%2520of%2520ensemble%2520learning%2520in%2520improving%2520skin%2520lesion%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Skin%20Lesion%20Diagnosis%20with%20Ensemble%20Learning&entry.906535625=Xiaoyi%20Liu%20and%20Zhou%20Yu%20and%20Lianghao%20Tan%20and%20Yafeng%20Yan%20and%20Ge%20Shi&entry.1292438233=%20%20Skin%20lesions%20are%20an%20increasingly%20significant%20medical%20concern%2C%20varying%20widely%0Ain%20severity%20from%20benign%20to%20cancerous.%20Accurate%20diagnosis%20is%20essential%20for%0Aensuring%20timely%20and%20appropriate%20treatment.%20This%20study%20examines%20the%0Aimplementation%20of%20deep%20learning%20methods%20to%20assist%20in%20the%20diagnosis%20of%20skin%0Alesions%20using%20the%20HAM10000%20dataset%2C%20which%20contains%20seven%20distinct%20types%20of%0Alesions.%20First%2C%20we%20evaluated%20three%20pre-trained%20models%3A%20MobileNetV2%2C%20ResNet18%2C%0Aand%20VGG11%2C%20achieving%20accuracies%20of%200.798%2C%200.802%2C%20and%200.805%2C%20respectively.%20To%0Afurther%20enhance%20classification%20accuracy%2C%20we%20developed%20ensemble%20models%20employing%0Amax%20voting%2C%20average%20voting%2C%20and%20stacking%2C%20resulting%20in%20accuracies%20of%200.803%2C%0A0.82%2C%20and%200.83.%20Building%20on%20the%20best-performing%20ensemble%20learning%20model%2C%0Astacking%2C%20we%20developed%20our%20proposed%20model%2C%20SkinNet%2C%20which%20incorporates%20a%0Acustomized%20architecture%20and%20fine-tuning%2C%20achieving%20an%20accuracy%20of%200.867%20and%20an%0AAUC%20of%200.96.%20This%20substantial%20improvement%20over%20individual%20models%20demonstrates%0Athe%20effectiveness%20of%20ensemble%20learning%20in%20improving%20skin%20lesion%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04381v1&entry.124074799=Read"},
{"title": "A Unified Representation Framework for the Evaluation of Optical Music\n  Recognition Systems", "author": "Pau Torras and Sanket Biswas and Alicia Forn\u00e9s", "abstract": "  Modern-day Optical Music Recognition (OMR) is a fairly fragmented field. Most\nOMR approaches use datasets that are independent and incompatible between each\nother, making it difficult to both combine them and compare recognition systems\nbuilt upon them. In this paper we identify the need of a common music\nrepresentation language and propose the Music Tree Notation (MTN) format, with\nthe idea to construct a common endpoint for OMR research that allows\ncoordination, reuse of technology and fair evaluation of community efforts.\nThis format represents music as a set of primitives that group together into\nhigher-abstraction nodes, a compromise between the expression of fully\ngraph-based and sequential notation formats. We have also developed a specific\nset of OMR metrics and a typeset score dataset as a proof of concept of this\nidea.\n", "link": "http://arxiv.org/abs/2312.12908v2", "date": "2024-09-06", "relevancy": 1.9136, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Representation%20Framework%20for%20the%20Evaluation%20of%20Optical%20Music%0A%20%20Recognition%20Systems&body=Title%3A%20A%20Unified%20Representation%20Framework%20for%20the%20Evaluation%20of%20Optical%20Music%0A%20%20Recognition%20Systems%0AAuthor%3A%20Pau%20Torras%20and%20Sanket%20Biswas%20and%20Alicia%20Forn%C3%A9s%0AAbstract%3A%20%20%20Modern-day%20Optical%20Music%20Recognition%20%28OMR%29%20is%20a%20fairly%20fragmented%20field.%20Most%0AOMR%20approaches%20use%20datasets%20that%20are%20independent%20and%20incompatible%20between%20each%0Aother%2C%20making%20it%20difficult%20to%20both%20combine%20them%20and%20compare%20recognition%20systems%0Abuilt%20upon%20them.%20In%20this%20paper%20we%20identify%20the%20need%20of%20a%20common%20music%0Arepresentation%20language%20and%20propose%20the%20Music%20Tree%20Notation%20%28MTN%29%20format%2C%20with%0Athe%20idea%20to%20construct%20a%20common%20endpoint%20for%20OMR%20research%20that%20allows%0Acoordination%2C%20reuse%20of%20technology%20and%20fair%20evaluation%20of%20community%20efforts.%0AThis%20format%20represents%20music%20as%20a%20set%20of%20primitives%20that%20group%20together%20into%0Ahigher-abstraction%20nodes%2C%20a%20compromise%20between%20the%20expression%20of%20fully%0Agraph-based%20and%20sequential%20notation%20formats.%20We%20have%20also%20developed%20a%20specific%0Aset%20of%20OMR%20metrics%20and%20a%20typeset%20score%20dataset%20as%20a%20proof%20of%20concept%20of%20this%0Aidea.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12908v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Representation%2520Framework%2520for%2520the%2520Evaluation%2520of%2520Optical%2520Music%250A%2520%2520Recognition%2520Systems%26entry.906535625%3DPau%2520Torras%2520and%2520Sanket%2520Biswas%2520and%2520Alicia%2520Forn%25C3%25A9s%26entry.1292438233%3D%2520%2520Modern-day%2520Optical%2520Music%2520Recognition%2520%2528OMR%2529%2520is%2520a%2520fairly%2520fragmented%2520field.%2520Most%250AOMR%2520approaches%2520use%2520datasets%2520that%2520are%2520independent%2520and%2520incompatible%2520between%2520each%250Aother%252C%2520making%2520it%2520difficult%2520to%2520both%2520combine%2520them%2520and%2520compare%2520recognition%2520systems%250Abuilt%2520upon%2520them.%2520In%2520this%2520paper%2520we%2520identify%2520the%2520need%2520of%2520a%2520common%2520music%250Arepresentation%2520language%2520and%2520propose%2520the%2520Music%2520Tree%2520Notation%2520%2528MTN%2529%2520format%252C%2520with%250Athe%2520idea%2520to%2520construct%2520a%2520common%2520endpoint%2520for%2520OMR%2520research%2520that%2520allows%250Acoordination%252C%2520reuse%2520of%2520technology%2520and%2520fair%2520evaluation%2520of%2520community%2520efforts.%250AThis%2520format%2520represents%2520music%2520as%2520a%2520set%2520of%2520primitives%2520that%2520group%2520together%2520into%250Ahigher-abstraction%2520nodes%252C%2520a%2520compromise%2520between%2520the%2520expression%2520of%2520fully%250Agraph-based%2520and%2520sequential%2520notation%2520formats.%2520We%2520have%2520also%2520developed%2520a%2520specific%250Aset%2520of%2520OMR%2520metrics%2520and%2520a%2520typeset%2520score%2520dataset%2520as%2520a%2520proof%2520of%2520concept%2520of%2520this%250Aidea.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12908v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Representation%20Framework%20for%20the%20Evaluation%20of%20Optical%20Music%0A%20%20Recognition%20Systems&entry.906535625=Pau%20Torras%20and%20Sanket%20Biswas%20and%20Alicia%20Forn%C3%A9s&entry.1292438233=%20%20Modern-day%20Optical%20Music%20Recognition%20%28OMR%29%20is%20a%20fairly%20fragmented%20field.%20Most%0AOMR%20approaches%20use%20datasets%20that%20are%20independent%20and%20incompatible%20between%20each%0Aother%2C%20making%20it%20difficult%20to%20both%20combine%20them%20and%20compare%20recognition%20systems%0Abuilt%20upon%20them.%20In%20this%20paper%20we%20identify%20the%20need%20of%20a%20common%20music%0Arepresentation%20language%20and%20propose%20the%20Music%20Tree%20Notation%20%28MTN%29%20format%2C%20with%0Athe%20idea%20to%20construct%20a%20common%20endpoint%20for%20OMR%20research%20that%20allows%0Acoordination%2C%20reuse%20of%20technology%20and%20fair%20evaluation%20of%20community%20efforts.%0AThis%20format%20represents%20music%20as%20a%20set%20of%20primitives%20that%20group%20together%20into%0Ahigher-abstraction%20nodes%2C%20a%20compromise%20between%20the%20expression%20of%20fully%0Agraph-based%20and%20sequential%20notation%20formats.%20We%20have%20also%20developed%20a%20specific%0Aset%20of%20OMR%20metrics%20and%20a%20typeset%20score%20dataset%20as%20a%20proof%20of%20concept%20of%20this%0Aidea.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12908v2&entry.124074799=Read"},
{"title": "Gaussian-Mixture-Model Q-Functions for Reinforcement Learning by\n  Riemannian Optimization", "author": "Minh Vu and Konstantinos Slavakis", "abstract": "  This paper establishes a novel role for Gaussian-mixture models (GMMs) as\nfunctional approximators of Q-function losses in reinforcement learning (RL).\nUnlike the existing RL literature, where GMMs play their typical role as\nestimates of probability density functions, GMMs approximate here Q-function\nlosses. The new Q-function approximators, coined GMM-QFs, are incorporated in\nBellman residuals to promote a Riemannian-optimization task as a novel\npolicy-evaluation step in standard policy-iteration schemes. The paper\ndemonstrates how the hyperparameters (means and covariance matrices) of the\nGaussian kernels are learned from the data, opening thus the door of RL to the\npowerful toolbox of Riemannian optimization. Numerical tests show that with no\nuse of training data, the proposed design outperforms state-of-the-art methods,\neven deep Q-networks which use training data, on benchmark RL tasks.\n", "link": "http://arxiv.org/abs/2409.04374v1", "date": "2024-09-06", "relevancy": 1.9048, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4734}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian-Mixture-Model%20Q-Functions%20for%20Reinforcement%20Learning%20by%0A%20%20Riemannian%20Optimization&body=Title%3A%20Gaussian-Mixture-Model%20Q-Functions%20for%20Reinforcement%20Learning%20by%0A%20%20Riemannian%20Optimization%0AAuthor%3A%20Minh%20Vu%20and%20Konstantinos%20Slavakis%0AAbstract%3A%20%20%20This%20paper%20establishes%20a%20novel%20role%20for%20Gaussian-mixture%20models%20%28GMMs%29%20as%0Afunctional%20approximators%20of%20Q-function%20losses%20in%20reinforcement%20learning%20%28RL%29.%0AUnlike%20the%20existing%20RL%20literature%2C%20where%20GMMs%20play%20their%20typical%20role%20as%0Aestimates%20of%20probability%20density%20functions%2C%20GMMs%20approximate%20here%20Q-function%0Alosses.%20The%20new%20Q-function%20approximators%2C%20coined%20GMM-QFs%2C%20are%20incorporated%20in%0ABellman%20residuals%20to%20promote%20a%20Riemannian-optimization%20task%20as%20a%20novel%0Apolicy-evaluation%20step%20in%20standard%20policy-iteration%20schemes.%20The%20paper%0Ademonstrates%20how%20the%20hyperparameters%20%28means%20and%20covariance%20matrices%29%20of%20the%0AGaussian%20kernels%20are%20learned%20from%20the%20data%2C%20opening%20thus%20the%20door%20of%20RL%20to%20the%0Apowerful%20toolbox%20of%20Riemannian%20optimization.%20Numerical%20tests%20show%20that%20with%20no%0Ause%20of%20training%20data%2C%20the%20proposed%20design%20outperforms%20state-of-the-art%20methods%2C%0Aeven%20deep%20Q-networks%20which%20use%20training%20data%2C%20on%20benchmark%20RL%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian-Mixture-Model%2520Q-Functions%2520for%2520Reinforcement%2520Learning%2520by%250A%2520%2520Riemannian%2520Optimization%26entry.906535625%3DMinh%2520Vu%2520and%2520Konstantinos%2520Slavakis%26entry.1292438233%3D%2520%2520This%2520paper%2520establishes%2520a%2520novel%2520role%2520for%2520Gaussian-mixture%2520models%2520%2528GMMs%2529%2520as%250Afunctional%2520approximators%2520of%2520Q-function%2520losses%2520in%2520reinforcement%2520learning%2520%2528RL%2529.%250AUnlike%2520the%2520existing%2520RL%2520literature%252C%2520where%2520GMMs%2520play%2520their%2520typical%2520role%2520as%250Aestimates%2520of%2520probability%2520density%2520functions%252C%2520GMMs%2520approximate%2520here%2520Q-function%250Alosses.%2520The%2520new%2520Q-function%2520approximators%252C%2520coined%2520GMM-QFs%252C%2520are%2520incorporated%2520in%250ABellman%2520residuals%2520to%2520promote%2520a%2520Riemannian-optimization%2520task%2520as%2520a%2520novel%250Apolicy-evaluation%2520step%2520in%2520standard%2520policy-iteration%2520schemes.%2520The%2520paper%250Ademonstrates%2520how%2520the%2520hyperparameters%2520%2528means%2520and%2520covariance%2520matrices%2529%2520of%2520the%250AGaussian%2520kernels%2520are%2520learned%2520from%2520the%2520data%252C%2520opening%2520thus%2520the%2520door%2520of%2520RL%2520to%2520the%250Apowerful%2520toolbox%2520of%2520Riemannian%2520optimization.%2520Numerical%2520tests%2520show%2520that%2520with%2520no%250Ause%2520of%2520training%2520data%252C%2520the%2520proposed%2520design%2520outperforms%2520state-of-the-art%2520methods%252C%250Aeven%2520deep%2520Q-networks%2520which%2520use%2520training%2520data%252C%2520on%2520benchmark%2520RL%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian-Mixture-Model%20Q-Functions%20for%20Reinforcement%20Learning%20by%0A%20%20Riemannian%20Optimization&entry.906535625=Minh%20Vu%20and%20Konstantinos%20Slavakis&entry.1292438233=%20%20This%20paper%20establishes%20a%20novel%20role%20for%20Gaussian-mixture%20models%20%28GMMs%29%20as%0Afunctional%20approximators%20of%20Q-function%20losses%20in%20reinforcement%20learning%20%28RL%29.%0AUnlike%20the%20existing%20RL%20literature%2C%20where%20GMMs%20play%20their%20typical%20role%20as%0Aestimates%20of%20probability%20density%20functions%2C%20GMMs%20approximate%20here%20Q-function%0Alosses.%20The%20new%20Q-function%20approximators%2C%20coined%20GMM-QFs%2C%20are%20incorporated%20in%0ABellman%20residuals%20to%20promote%20a%20Riemannian-optimization%20task%20as%20a%20novel%0Apolicy-evaluation%20step%20in%20standard%20policy-iteration%20schemes.%20The%20paper%0Ademonstrates%20how%20the%20hyperparameters%20%28means%20and%20covariance%20matrices%29%20of%20the%0AGaussian%20kernels%20are%20learned%20from%20the%20data%2C%20opening%20thus%20the%20door%20of%20RL%20to%20the%0Apowerful%20toolbox%20of%20Riemannian%20optimization.%20Numerical%20tests%20show%20that%20with%20no%0Ause%20of%20training%20data%2C%20the%20proposed%20design%20outperforms%20state-of-the-art%20methods%2C%0Aeven%20deep%20Q-networks%20which%20use%20training%20data%2C%20on%20benchmark%20RL%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04374v1&entry.124074799=Read"},
{"title": "FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese\n  Large Language Models", "author": "Yan Liu and Renren Jin and Ling Shi and Zheng Yao and Deyi Xiong", "abstract": "  To thoroughly assess the mathematical reasoning abilities of Large Language\nModels (LLMs), we need to carefully curate evaluation datasets covering diverse\nmathematical concepts and mathematical problems at different difficulty levels.\nIn pursuit of this objective, we propose FineMath in this paper, a fine-grained\nmathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath\nis created to cover the major key mathematical concepts taught in elementary\nschool math, which are further divided into 17 categories of math word\nproblems, enabling in-depth analysis of mathematical reasoning abilities of\nLLMs. All the 17 categories of math word problems are manually annotated with\ntheir difficulty levels according to the number of reasoning steps required to\nsolve these problems. We conduct extensive experiments on a wide range of LLMs\non FineMath and find that there is still considerable room for improvements in\nterms of mathematical reasoning capability of Chinese LLMs. We also carry out\nan in-depth analysis on the evaluation process and methods that have been\noverlooked previously. These two factors significantly influence the model\nresults and our understanding of their mathematical reasoning capabilities. The\ndataset will be publicly available soon.\n", "link": "http://arxiv.org/abs/2403.07747v2", "date": "2024-09-06", "relevancy": 1.9016, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4757}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FineMath%3A%20A%20Fine-Grained%20Mathematical%20Evaluation%20Benchmark%20for%20Chinese%0A%20%20Large%20Language%20Models&body=Title%3A%20FineMath%3A%20A%20Fine-Grained%20Mathematical%20Evaluation%20Benchmark%20for%20Chinese%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yan%20Liu%20and%20Renren%20Jin%20and%20Ling%20Shi%20and%20Zheng%20Yao%20and%20Deyi%20Xiong%0AAbstract%3A%20%20%20To%20thoroughly%20assess%20the%20mathematical%20reasoning%20abilities%20of%20Large%20Language%0AModels%20%28LLMs%29%2C%20we%20need%20to%20carefully%20curate%20evaluation%20datasets%20covering%20diverse%0Amathematical%20concepts%20and%20mathematical%20problems%20at%20different%20difficulty%20levels.%0AIn%20pursuit%20of%20this%20objective%2C%20we%20propose%20FineMath%20in%20this%20paper%2C%20a%20fine-grained%0Amathematical%20evaluation%20benchmark%20dataset%20for%20assessing%20Chinese%20LLMs.%20FineMath%0Ais%20created%20to%20cover%20the%20major%20key%20mathematical%20concepts%20taught%20in%20elementary%0Aschool%20math%2C%20which%20are%20further%20divided%20into%2017%20categories%20of%20math%20word%0Aproblems%2C%20enabling%20in-depth%20analysis%20of%20mathematical%20reasoning%20abilities%20of%0ALLMs.%20All%20the%2017%20categories%20of%20math%20word%20problems%20are%20manually%20annotated%20with%0Atheir%20difficulty%20levels%20according%20to%20the%20number%20of%20reasoning%20steps%20required%20to%0Asolve%20these%20problems.%20We%20conduct%20extensive%20experiments%20on%20a%20wide%20range%20of%20LLMs%0Aon%20FineMath%20and%20find%20that%20there%20is%20still%20considerable%20room%20for%20improvements%20in%0Aterms%20of%20mathematical%20reasoning%20capability%20of%20Chinese%20LLMs.%20We%20also%20carry%20out%0Aan%20in-depth%20analysis%20on%20the%20evaluation%20process%20and%20methods%20that%20have%20been%0Aoverlooked%20previously.%20These%20two%20factors%20significantly%20influence%20the%20model%0Aresults%20and%20our%20understanding%20of%20their%20mathematical%20reasoning%20capabilities.%20The%0Adataset%20will%20be%20publicly%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFineMath%253A%2520A%2520Fine-Grained%2520Mathematical%2520Evaluation%2520Benchmark%2520for%2520Chinese%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYan%2520Liu%2520and%2520Renren%2520Jin%2520and%2520Ling%2520Shi%2520and%2520Zheng%2520Yao%2520and%2520Deyi%2520Xiong%26entry.1292438233%3D%2520%2520To%2520thoroughly%2520assess%2520the%2520mathematical%2520reasoning%2520abilities%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%252C%2520we%2520need%2520to%2520carefully%2520curate%2520evaluation%2520datasets%2520covering%2520diverse%250Amathematical%2520concepts%2520and%2520mathematical%2520problems%2520at%2520different%2520difficulty%2520levels.%250AIn%2520pursuit%2520of%2520this%2520objective%252C%2520we%2520propose%2520FineMath%2520in%2520this%2520paper%252C%2520a%2520fine-grained%250Amathematical%2520evaluation%2520benchmark%2520dataset%2520for%2520assessing%2520Chinese%2520LLMs.%2520FineMath%250Ais%2520created%2520to%2520cover%2520the%2520major%2520key%2520mathematical%2520concepts%2520taught%2520in%2520elementary%250Aschool%2520math%252C%2520which%2520are%2520further%2520divided%2520into%252017%2520categories%2520of%2520math%2520word%250Aproblems%252C%2520enabling%2520in-depth%2520analysis%2520of%2520mathematical%2520reasoning%2520abilities%2520of%250ALLMs.%2520All%2520the%252017%2520categories%2520of%2520math%2520word%2520problems%2520are%2520manually%2520annotated%2520with%250Atheir%2520difficulty%2520levels%2520according%2520to%2520the%2520number%2520of%2520reasoning%2520steps%2520required%2520to%250Asolve%2520these%2520problems.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520a%2520wide%2520range%2520of%2520LLMs%250Aon%2520FineMath%2520and%2520find%2520that%2520there%2520is%2520still%2520considerable%2520room%2520for%2520improvements%2520in%250Aterms%2520of%2520mathematical%2520reasoning%2520capability%2520of%2520Chinese%2520LLMs.%2520We%2520also%2520carry%2520out%250Aan%2520in-depth%2520analysis%2520on%2520the%2520evaluation%2520process%2520and%2520methods%2520that%2520have%2520been%250Aoverlooked%2520previously.%2520These%2520two%2520factors%2520significantly%2520influence%2520the%2520model%250Aresults%2520and%2520our%2520understanding%2520of%2520their%2520mathematical%2520reasoning%2520capabilities.%2520The%250Adataset%2520will%2520be%2520publicly%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FineMath%3A%20A%20Fine-Grained%20Mathematical%20Evaluation%20Benchmark%20for%20Chinese%0A%20%20Large%20Language%20Models&entry.906535625=Yan%20Liu%20and%20Renren%20Jin%20and%20Ling%20Shi%20and%20Zheng%20Yao%20and%20Deyi%20Xiong&entry.1292438233=%20%20To%20thoroughly%20assess%20the%20mathematical%20reasoning%20abilities%20of%20Large%20Language%0AModels%20%28LLMs%29%2C%20we%20need%20to%20carefully%20curate%20evaluation%20datasets%20covering%20diverse%0Amathematical%20concepts%20and%20mathematical%20problems%20at%20different%20difficulty%20levels.%0AIn%20pursuit%20of%20this%20objective%2C%20we%20propose%20FineMath%20in%20this%20paper%2C%20a%20fine-grained%0Amathematical%20evaluation%20benchmark%20dataset%20for%20assessing%20Chinese%20LLMs.%20FineMath%0Ais%20created%20to%20cover%20the%20major%20key%20mathematical%20concepts%20taught%20in%20elementary%0Aschool%20math%2C%20which%20are%20further%20divided%20into%2017%20categories%20of%20math%20word%0Aproblems%2C%20enabling%20in-depth%20analysis%20of%20mathematical%20reasoning%20abilities%20of%0ALLMs.%20All%20the%2017%20categories%20of%20math%20word%20problems%20are%20manually%20annotated%20with%0Atheir%20difficulty%20levels%20according%20to%20the%20number%20of%20reasoning%20steps%20required%20to%0Asolve%20these%20problems.%20We%20conduct%20extensive%20experiments%20on%20a%20wide%20range%20of%20LLMs%0Aon%20FineMath%20and%20find%20that%20there%20is%20still%20considerable%20room%20for%20improvements%20in%0Aterms%20of%20mathematical%20reasoning%20capability%20of%20Chinese%20LLMs.%20We%20also%20carry%20out%0Aan%20in-depth%20analysis%20on%20the%20evaluation%20process%20and%20methods%20that%20have%20been%0Aoverlooked%20previously.%20These%20two%20factors%20significantly%20influence%20the%20model%0Aresults%20and%20our%20understanding%20of%20their%20mathematical%20reasoning%20capabilities.%20The%0Adataset%20will%20be%20publicly%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07747v2&entry.124074799=Read"},
{"title": "Exploiting the Data Gap: Utilizing Non-ignorable Missingness to\n  Manipulate Model Learning", "author": "Deniz Koyuncu and Alex Gittens and B\u00fclent Yener and Moti Yung", "abstract": "  Missing data is commonly encountered in practice, and when the missingness is\nnon-ignorable, effective remediation depends on knowledge of the missingness\nmechanism. Learning the underlying missingness mechanism from the data is not\npossible in general, so adversaries can exploit this fact by maliciously\nengineering non-ignorable missingness mechanisms. Such Adversarial Missingness\n(AM) attacks have only recently been motivated and introduced, and then\nsuccessfully tailored to mislead causal structure learning algorithms into\nhiding specific cause-and-effect relationships. However, existing AM attacks\nassume the modeler (victim) uses full-information maximum likelihood methods to\nhandle the missing data, and are of limited applicability when the modeler uses\ndifferent remediation strategies. In this work we focus on associational\nlearning in the context of AM attacks. We consider (i) complete case analysis,\n(ii) mean imputation, and (iii) regression-based imputation as alternative\nstrategies used by the modeler. Instead of combinatorially searching for\nmissing entries, we propose a novel probabilistic approximation by deriving the\nasymptotic forms of these methods used for handling the missing entries. We\nthen formulate the learning of the adversarial missingness mechanism as a\nbi-level optimization problem. Experiments on generalized linear models show\nthat AM attacks can be used to change the p-values of features from significant\nto insignificant in real datasets, such as the California-housing dataset,\nwhile using relatively moderate amounts of missingness (<20%). Additionally, we\nassess the robustness of our attacks against defense strategies based on data\nvaluation.\n", "link": "http://arxiv.org/abs/2409.04407v1", "date": "2024-09-06", "relevancy": 1.8976, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4959}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4656}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20the%20Data%20Gap%3A%20Utilizing%20Non-ignorable%20Missingness%20to%0A%20%20Manipulate%20Model%20Learning&body=Title%3A%20Exploiting%20the%20Data%20Gap%3A%20Utilizing%20Non-ignorable%20Missingness%20to%0A%20%20Manipulate%20Model%20Learning%0AAuthor%3A%20Deniz%20Koyuncu%20and%20Alex%20Gittens%20and%20B%C3%BClent%20Yener%20and%20Moti%20Yung%0AAbstract%3A%20%20%20Missing%20data%20is%20commonly%20encountered%20in%20practice%2C%20and%20when%20the%20missingness%20is%0Anon-ignorable%2C%20effective%20remediation%20depends%20on%20knowledge%20of%20the%20missingness%0Amechanism.%20Learning%20the%20underlying%20missingness%20mechanism%20from%20the%20data%20is%20not%0Apossible%20in%20general%2C%20so%20adversaries%20can%20exploit%20this%20fact%20by%20maliciously%0Aengineering%20non-ignorable%20missingness%20mechanisms.%20Such%20Adversarial%20Missingness%0A%28AM%29%20attacks%20have%20only%20recently%20been%20motivated%20and%20introduced%2C%20and%20then%0Asuccessfully%20tailored%20to%20mislead%20causal%20structure%20learning%20algorithms%20into%0Ahiding%20specific%20cause-and-effect%20relationships.%20However%2C%20existing%20AM%20attacks%0Aassume%20the%20modeler%20%28victim%29%20uses%20full-information%20maximum%20likelihood%20methods%20to%0Ahandle%20the%20missing%20data%2C%20and%20are%20of%20limited%20applicability%20when%20the%20modeler%20uses%0Adifferent%20remediation%20strategies.%20In%20this%20work%20we%20focus%20on%20associational%0Alearning%20in%20the%20context%20of%20AM%20attacks.%20We%20consider%20%28i%29%20complete%20case%20analysis%2C%0A%28ii%29%20mean%20imputation%2C%20and%20%28iii%29%20regression-based%20imputation%20as%20alternative%0Astrategies%20used%20by%20the%20modeler.%20Instead%20of%20combinatorially%20searching%20for%0Amissing%20entries%2C%20we%20propose%20a%20novel%20probabilistic%20approximation%20by%20deriving%20the%0Aasymptotic%20forms%20of%20these%20methods%20used%20for%20handling%20the%20missing%20entries.%20We%0Athen%20formulate%20the%20learning%20of%20the%20adversarial%20missingness%20mechanism%20as%20a%0Abi-level%20optimization%20problem.%20Experiments%20on%20generalized%20linear%20models%20show%0Athat%20AM%20attacks%20can%20be%20used%20to%20change%20the%20p-values%20of%20features%20from%20significant%0Ato%20insignificant%20in%20real%20datasets%2C%20such%20as%20the%20California-housing%20dataset%2C%0Awhile%20using%20relatively%20moderate%20amounts%20of%20missingness%20%28%3C20%25%29.%20Additionally%2C%20we%0Aassess%20the%20robustness%20of%20our%20attacks%20against%20defense%20strategies%20based%20on%20data%0Avaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520the%2520Data%2520Gap%253A%2520Utilizing%2520Non-ignorable%2520Missingness%2520to%250A%2520%2520Manipulate%2520Model%2520Learning%26entry.906535625%3DDeniz%2520Koyuncu%2520and%2520Alex%2520Gittens%2520and%2520B%25C3%25BClent%2520Yener%2520and%2520Moti%2520Yung%26entry.1292438233%3D%2520%2520Missing%2520data%2520is%2520commonly%2520encountered%2520in%2520practice%252C%2520and%2520when%2520the%2520missingness%2520is%250Anon-ignorable%252C%2520effective%2520remediation%2520depends%2520on%2520knowledge%2520of%2520the%2520missingness%250Amechanism.%2520Learning%2520the%2520underlying%2520missingness%2520mechanism%2520from%2520the%2520data%2520is%2520not%250Apossible%2520in%2520general%252C%2520so%2520adversaries%2520can%2520exploit%2520this%2520fact%2520by%2520maliciously%250Aengineering%2520non-ignorable%2520missingness%2520mechanisms.%2520Such%2520Adversarial%2520Missingness%250A%2528AM%2529%2520attacks%2520have%2520only%2520recently%2520been%2520motivated%2520and%2520introduced%252C%2520and%2520then%250Asuccessfully%2520tailored%2520to%2520mislead%2520causal%2520structure%2520learning%2520algorithms%2520into%250Ahiding%2520specific%2520cause-and-effect%2520relationships.%2520However%252C%2520existing%2520AM%2520attacks%250Aassume%2520the%2520modeler%2520%2528victim%2529%2520uses%2520full-information%2520maximum%2520likelihood%2520methods%2520to%250Ahandle%2520the%2520missing%2520data%252C%2520and%2520are%2520of%2520limited%2520applicability%2520when%2520the%2520modeler%2520uses%250Adifferent%2520remediation%2520strategies.%2520In%2520this%2520work%2520we%2520focus%2520on%2520associational%250Alearning%2520in%2520the%2520context%2520of%2520AM%2520attacks.%2520We%2520consider%2520%2528i%2529%2520complete%2520case%2520analysis%252C%250A%2528ii%2529%2520mean%2520imputation%252C%2520and%2520%2528iii%2529%2520regression-based%2520imputation%2520as%2520alternative%250Astrategies%2520used%2520by%2520the%2520modeler.%2520Instead%2520of%2520combinatorially%2520searching%2520for%250Amissing%2520entries%252C%2520we%2520propose%2520a%2520novel%2520probabilistic%2520approximation%2520by%2520deriving%2520the%250Aasymptotic%2520forms%2520of%2520these%2520methods%2520used%2520for%2520handling%2520the%2520missing%2520entries.%2520We%250Athen%2520formulate%2520the%2520learning%2520of%2520the%2520adversarial%2520missingness%2520mechanism%2520as%2520a%250Abi-level%2520optimization%2520problem.%2520Experiments%2520on%2520generalized%2520linear%2520models%2520show%250Athat%2520AM%2520attacks%2520can%2520be%2520used%2520to%2520change%2520the%2520p-values%2520of%2520features%2520from%2520significant%250Ato%2520insignificant%2520in%2520real%2520datasets%252C%2520such%2520as%2520the%2520California-housing%2520dataset%252C%250Awhile%2520using%2520relatively%2520moderate%2520amounts%2520of%2520missingness%2520%2528%253C20%2525%2529.%2520Additionally%252C%2520we%250Aassess%2520the%2520robustness%2520of%2520our%2520attacks%2520against%2520defense%2520strategies%2520based%2520on%2520data%250Avaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20the%20Data%20Gap%3A%20Utilizing%20Non-ignorable%20Missingness%20to%0A%20%20Manipulate%20Model%20Learning&entry.906535625=Deniz%20Koyuncu%20and%20Alex%20Gittens%20and%20B%C3%BClent%20Yener%20and%20Moti%20Yung&entry.1292438233=%20%20Missing%20data%20is%20commonly%20encountered%20in%20practice%2C%20and%20when%20the%20missingness%20is%0Anon-ignorable%2C%20effective%20remediation%20depends%20on%20knowledge%20of%20the%20missingness%0Amechanism.%20Learning%20the%20underlying%20missingness%20mechanism%20from%20the%20data%20is%20not%0Apossible%20in%20general%2C%20so%20adversaries%20can%20exploit%20this%20fact%20by%20maliciously%0Aengineering%20non-ignorable%20missingness%20mechanisms.%20Such%20Adversarial%20Missingness%0A%28AM%29%20attacks%20have%20only%20recently%20been%20motivated%20and%20introduced%2C%20and%20then%0Asuccessfully%20tailored%20to%20mislead%20causal%20structure%20learning%20algorithms%20into%0Ahiding%20specific%20cause-and-effect%20relationships.%20However%2C%20existing%20AM%20attacks%0Aassume%20the%20modeler%20%28victim%29%20uses%20full-information%20maximum%20likelihood%20methods%20to%0Ahandle%20the%20missing%20data%2C%20and%20are%20of%20limited%20applicability%20when%20the%20modeler%20uses%0Adifferent%20remediation%20strategies.%20In%20this%20work%20we%20focus%20on%20associational%0Alearning%20in%20the%20context%20of%20AM%20attacks.%20We%20consider%20%28i%29%20complete%20case%20analysis%2C%0A%28ii%29%20mean%20imputation%2C%20and%20%28iii%29%20regression-based%20imputation%20as%20alternative%0Astrategies%20used%20by%20the%20modeler.%20Instead%20of%20combinatorially%20searching%20for%0Amissing%20entries%2C%20we%20propose%20a%20novel%20probabilistic%20approximation%20by%20deriving%20the%0Aasymptotic%20forms%20of%20these%20methods%20used%20for%20handling%20the%20missing%20entries.%20We%0Athen%20formulate%20the%20learning%20of%20the%20adversarial%20missingness%20mechanism%20as%20a%0Abi-level%20optimization%20problem.%20Experiments%20on%20generalized%20linear%20models%20show%0Athat%20AM%20attacks%20can%20be%20used%20to%20change%20the%20p-values%20of%20features%20from%20significant%0Ato%20insignificant%20in%20real%20datasets%2C%20such%20as%20the%20California-housing%20dataset%2C%0Awhile%20using%20relatively%20moderate%20amounts%20of%20missingness%20%28%3C20%25%29.%20Additionally%2C%20we%0Aassess%20the%20robustness%20of%20our%20attacks%20against%20defense%20strategies%20based%20on%20data%0Avaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04407v1&entry.124074799=Read"},
{"title": "A high-accuracy multi-model mixing retrosynthetic method", "author": "Shang Xiang and Lin Yao and Zhen Wang and Qifan Yu and Wentan Liu and Wentao Guo and Guolin Ke", "abstract": "  The field of computer-aided synthesis planning (CASP) has seen rapid\nadvancements in recent years, achieving significant progress across various\nalgorithmic benchmarks. However, chemists often encounter numerous infeasible\nreactions when using CASP in practice. This article delves into common errors\nassociated with CASP and introduces a product prediction model aimed at\nenhancing the accuracy of single-step models. While the product prediction\nmodel reduces the number of single-step reactions, it integrates multiple\nsingle-step models to maintain the overall reaction count and increase reaction\ndiversity. Based on manual analysis and large-scale testing, the product\nprediction model, combined with the multi-model ensemble approach, has been\nproven to offer higher feasibility and greater diversity.\n", "link": "http://arxiv.org/abs/2409.04335v1", "date": "2024-09-06", "relevancy": 1.8971, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4936}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4704}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20high-accuracy%20multi-model%20mixing%20retrosynthetic%20method&body=Title%3A%20A%20high-accuracy%20multi-model%20mixing%20retrosynthetic%20method%0AAuthor%3A%20Shang%20Xiang%20and%20Lin%20Yao%20and%20Zhen%20Wang%20and%20Qifan%20Yu%20and%20Wentan%20Liu%20and%20Wentao%20Guo%20and%20Guolin%20Ke%0AAbstract%3A%20%20%20The%20field%20of%20computer-aided%20synthesis%20planning%20%28CASP%29%20has%20seen%20rapid%0Aadvancements%20in%20recent%20years%2C%20achieving%20significant%20progress%20across%20various%0Aalgorithmic%20benchmarks.%20However%2C%20chemists%20often%20encounter%20numerous%20infeasible%0Areactions%20when%20using%20CASP%20in%20practice.%20This%20article%20delves%20into%20common%20errors%0Aassociated%20with%20CASP%20and%20introduces%20a%20product%20prediction%20model%20aimed%20at%0Aenhancing%20the%20accuracy%20of%20single-step%20models.%20While%20the%20product%20prediction%0Amodel%20reduces%20the%20number%20of%20single-step%20reactions%2C%20it%20integrates%20multiple%0Asingle-step%20models%20to%20maintain%20the%20overall%20reaction%20count%20and%20increase%20reaction%0Adiversity.%20Based%20on%20manual%20analysis%20and%20large-scale%20testing%2C%20the%20product%0Aprediction%20model%2C%20combined%20with%20the%20multi-model%20ensemble%20approach%2C%20has%20been%0Aproven%20to%20offer%20higher%20feasibility%20and%20greater%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520high-accuracy%2520multi-model%2520mixing%2520retrosynthetic%2520method%26entry.906535625%3DShang%2520Xiang%2520and%2520Lin%2520Yao%2520and%2520Zhen%2520Wang%2520and%2520Qifan%2520Yu%2520and%2520Wentan%2520Liu%2520and%2520Wentao%2520Guo%2520and%2520Guolin%2520Ke%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520computer-aided%2520synthesis%2520planning%2520%2528CASP%2529%2520has%2520seen%2520rapid%250Aadvancements%2520in%2520recent%2520years%252C%2520achieving%2520significant%2520progress%2520across%2520various%250Aalgorithmic%2520benchmarks.%2520However%252C%2520chemists%2520often%2520encounter%2520numerous%2520infeasible%250Areactions%2520when%2520using%2520CASP%2520in%2520practice.%2520This%2520article%2520delves%2520into%2520common%2520errors%250Aassociated%2520with%2520CASP%2520and%2520introduces%2520a%2520product%2520prediction%2520model%2520aimed%2520at%250Aenhancing%2520the%2520accuracy%2520of%2520single-step%2520models.%2520While%2520the%2520product%2520prediction%250Amodel%2520reduces%2520the%2520number%2520of%2520single-step%2520reactions%252C%2520it%2520integrates%2520multiple%250Asingle-step%2520models%2520to%2520maintain%2520the%2520overall%2520reaction%2520count%2520and%2520increase%2520reaction%250Adiversity.%2520Based%2520on%2520manual%2520analysis%2520and%2520large-scale%2520testing%252C%2520the%2520product%250Aprediction%2520model%252C%2520combined%2520with%2520the%2520multi-model%2520ensemble%2520approach%252C%2520has%2520been%250Aproven%2520to%2520offer%2520higher%2520feasibility%2520and%2520greater%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20high-accuracy%20multi-model%20mixing%20retrosynthetic%20method&entry.906535625=Shang%20Xiang%20and%20Lin%20Yao%20and%20Zhen%20Wang%20and%20Qifan%20Yu%20and%20Wentan%20Liu%20and%20Wentao%20Guo%20and%20Guolin%20Ke&entry.1292438233=%20%20The%20field%20of%20computer-aided%20synthesis%20planning%20%28CASP%29%20has%20seen%20rapid%0Aadvancements%20in%20recent%20years%2C%20achieving%20significant%20progress%20across%20various%0Aalgorithmic%20benchmarks.%20However%2C%20chemists%20often%20encounter%20numerous%20infeasible%0Areactions%20when%20using%20CASP%20in%20practice.%20This%20article%20delves%20into%20common%20errors%0Aassociated%20with%20CASP%20and%20introduces%20a%20product%20prediction%20model%20aimed%20at%0Aenhancing%20the%20accuracy%20of%20single-step%20models.%20While%20the%20product%20prediction%0Amodel%20reduces%20the%20number%20of%20single-step%20reactions%2C%20it%20integrates%20multiple%0Asingle-step%20models%20to%20maintain%20the%20overall%20reaction%20count%20and%20increase%20reaction%0Adiversity.%20Based%20on%20manual%20analysis%20and%20large-scale%20testing%2C%20the%20product%0Aprediction%20model%2C%20combined%20with%20the%20multi-model%20ensemble%20approach%2C%20has%20been%0Aproven%20to%20offer%20higher%20feasibility%20and%20greater%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04335v1&entry.124074799=Read"},
{"title": "Enhancing AI-based Generation of Software Exploits with Contextual\n  Information", "author": "Pietro Liguori and Cristina Improta and Roberto Natella and Bojan Cukic and Domenico Cotroneo", "abstract": "  This practical experience report explores Neural Machine Translation (NMT)\nmodels' capability to generate offensive security code from natural language\n(NL) descriptions, highlighting the significance of contextual understanding\nand its impact on model performance. Our study employs a dataset comprising\nreal shellcodes to evaluate the models across various scenarios, including\nmissing information, necessary context, and unnecessary context. The\nexperiments are designed to assess the models' resilience against incomplete\ndescriptions, their proficiency in leveraging context for enhanced accuracy,\nand their ability to discern irrelevant information. The findings reveal that\nthe introduction of contextual data significantly improves performance.\nHowever, the benefits of additional context diminish beyond a certain point,\nindicating an optimal level of contextual information for model training.\nMoreover, the models demonstrate an ability to filter out unnecessary context,\nmaintaining high levels of accuracy in the generation of offensive security\ncode. This study paves the way for future research on optimizing context use in\nAI-driven code generation, particularly for applications requiring a high\ndegree of technical precision such as the generation of offensive code.\n", "link": "http://arxiv.org/abs/2408.02402v3", "date": "2024-09-06", "relevancy": 1.8854, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20AI-based%20Generation%20of%20Software%20Exploits%20with%20Contextual%0A%20%20Information&body=Title%3A%20Enhancing%20AI-based%20Generation%20of%20Software%20Exploits%20with%20Contextual%0A%20%20Information%0AAuthor%3A%20Pietro%20Liguori%20and%20Cristina%20Improta%20and%20Roberto%20Natella%20and%20Bojan%20Cukic%20and%20Domenico%20Cotroneo%0AAbstract%3A%20%20%20This%20practical%20experience%20report%20explores%20Neural%20Machine%20Translation%20%28NMT%29%0Amodels%27%20capability%20to%20generate%20offensive%20security%20code%20from%20natural%20language%0A%28NL%29%20descriptions%2C%20highlighting%20the%20significance%20of%20contextual%20understanding%0Aand%20its%20impact%20on%20model%20performance.%20Our%20study%20employs%20a%20dataset%20comprising%0Areal%20shellcodes%20to%20evaluate%20the%20models%20across%20various%20scenarios%2C%20including%0Amissing%20information%2C%20necessary%20context%2C%20and%20unnecessary%20context.%20The%0Aexperiments%20are%20designed%20to%20assess%20the%20models%27%20resilience%20against%20incomplete%0Adescriptions%2C%20their%20proficiency%20in%20leveraging%20context%20for%20enhanced%20accuracy%2C%0Aand%20their%20ability%20to%20discern%20irrelevant%20information.%20The%20findings%20reveal%20that%0Athe%20introduction%20of%20contextual%20data%20significantly%20improves%20performance.%0AHowever%2C%20the%20benefits%20of%20additional%20context%20diminish%20beyond%20a%20certain%20point%2C%0Aindicating%20an%20optimal%20level%20of%20contextual%20information%20for%20model%20training.%0AMoreover%2C%20the%20models%20demonstrate%20an%20ability%20to%20filter%20out%20unnecessary%20context%2C%0Amaintaining%20high%20levels%20of%20accuracy%20in%20the%20generation%20of%20offensive%20security%0Acode.%20This%20study%20paves%20the%20way%20for%20future%20research%20on%20optimizing%20context%20use%20in%0AAI-driven%20code%20generation%2C%20particularly%20for%20applications%20requiring%20a%20high%0Adegree%20of%20technical%20precision%20such%20as%20the%20generation%20of%20offensive%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02402v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520AI-based%2520Generation%2520of%2520Software%2520Exploits%2520with%2520Contextual%250A%2520%2520Information%26entry.906535625%3DPietro%2520Liguori%2520and%2520Cristina%2520Improta%2520and%2520Roberto%2520Natella%2520and%2520Bojan%2520Cukic%2520and%2520Domenico%2520Cotroneo%26entry.1292438233%3D%2520%2520This%2520practical%2520experience%2520report%2520explores%2520Neural%2520Machine%2520Translation%2520%2528NMT%2529%250Amodels%2527%2520capability%2520to%2520generate%2520offensive%2520security%2520code%2520from%2520natural%2520language%250A%2528NL%2529%2520descriptions%252C%2520highlighting%2520the%2520significance%2520of%2520contextual%2520understanding%250Aand%2520its%2520impact%2520on%2520model%2520performance.%2520Our%2520study%2520employs%2520a%2520dataset%2520comprising%250Areal%2520shellcodes%2520to%2520evaluate%2520the%2520models%2520across%2520various%2520scenarios%252C%2520including%250Amissing%2520information%252C%2520necessary%2520context%252C%2520and%2520unnecessary%2520context.%2520The%250Aexperiments%2520are%2520designed%2520to%2520assess%2520the%2520models%2527%2520resilience%2520against%2520incomplete%250Adescriptions%252C%2520their%2520proficiency%2520in%2520leveraging%2520context%2520for%2520enhanced%2520accuracy%252C%250Aand%2520their%2520ability%2520to%2520discern%2520irrelevant%2520information.%2520The%2520findings%2520reveal%2520that%250Athe%2520introduction%2520of%2520contextual%2520data%2520significantly%2520improves%2520performance.%250AHowever%252C%2520the%2520benefits%2520of%2520additional%2520context%2520diminish%2520beyond%2520a%2520certain%2520point%252C%250Aindicating%2520an%2520optimal%2520level%2520of%2520contextual%2520information%2520for%2520model%2520training.%250AMoreover%252C%2520the%2520models%2520demonstrate%2520an%2520ability%2520to%2520filter%2520out%2520unnecessary%2520context%252C%250Amaintaining%2520high%2520levels%2520of%2520accuracy%2520in%2520the%2520generation%2520of%2520offensive%2520security%250Acode.%2520This%2520study%2520paves%2520the%2520way%2520for%2520future%2520research%2520on%2520optimizing%2520context%2520use%2520in%250AAI-driven%2520code%2520generation%252C%2520particularly%2520for%2520applications%2520requiring%2520a%2520high%250Adegree%2520of%2520technical%2520precision%2520such%2520as%2520the%2520generation%2520of%2520offensive%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02402v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20AI-based%20Generation%20of%20Software%20Exploits%20with%20Contextual%0A%20%20Information&entry.906535625=Pietro%20Liguori%20and%20Cristina%20Improta%20and%20Roberto%20Natella%20and%20Bojan%20Cukic%20and%20Domenico%20Cotroneo&entry.1292438233=%20%20This%20practical%20experience%20report%20explores%20Neural%20Machine%20Translation%20%28NMT%29%0Amodels%27%20capability%20to%20generate%20offensive%20security%20code%20from%20natural%20language%0A%28NL%29%20descriptions%2C%20highlighting%20the%20significance%20of%20contextual%20understanding%0Aand%20its%20impact%20on%20model%20performance.%20Our%20study%20employs%20a%20dataset%20comprising%0Areal%20shellcodes%20to%20evaluate%20the%20models%20across%20various%20scenarios%2C%20including%0Amissing%20information%2C%20necessary%20context%2C%20and%20unnecessary%20context.%20The%0Aexperiments%20are%20designed%20to%20assess%20the%20models%27%20resilience%20against%20incomplete%0Adescriptions%2C%20their%20proficiency%20in%20leveraging%20context%20for%20enhanced%20accuracy%2C%0Aand%20their%20ability%20to%20discern%20irrelevant%20information.%20The%20findings%20reveal%20that%0Athe%20introduction%20of%20contextual%20data%20significantly%20improves%20performance.%0AHowever%2C%20the%20benefits%20of%20additional%20context%20diminish%20beyond%20a%20certain%20point%2C%0Aindicating%20an%20optimal%20level%20of%20contextual%20information%20for%20model%20training.%0AMoreover%2C%20the%20models%20demonstrate%20an%20ability%20to%20filter%20out%20unnecessary%20context%2C%0Amaintaining%20high%20levels%20of%20accuracy%20in%20the%20generation%20of%20offensive%20security%0Acode.%20This%20study%20paves%20the%20way%20for%20future%20research%20on%20optimizing%20context%20use%20in%0AAI-driven%20code%20generation%2C%20particularly%20for%20applications%20requiring%20a%20high%0Adegree%20of%20technical%20precision%20such%20as%20the%20generation%20of%20offensive%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02402v3&entry.124074799=Read"},
{"title": "Amortized Bayesian Workflow (Extended Abstract)", "author": "Marvin Schmitt and Chengkun Li and Aki Vehtari and Luigi Acerbi and Paul-Christian B\u00fcrkner and Stefan T. Radev", "abstract": "  Bayesian inference often faces a trade-off between computational speed and\nsampling accuracy. We propose an adaptive workflow that integrates rapid\namortized inference with gold-standard MCMC techniques to achieve both speed\nand accuracy when performing inference on many observed datasets. Our approach\nuses principled diagnostics to guide the choice of inference method for each\ndataset, moving along the Pareto front from fast amortized sampling to slower\nbut guaranteed-accurate MCMC when necessary. By reusing computations across\nsteps, our workflow creates synergies between amortized and MCMC-based\ninference. We demonstrate the effectiveness of this integrated approach on a\ngeneralized extreme value task with 1000 observed data sets, showing 90x time\nefficiency gains while maintaining high posterior quality.\n", "link": "http://arxiv.org/abs/2409.04332v1", "date": "2024-09-06", "relevancy": 1.883, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4622}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Amortized%20Bayesian%20Workflow%20%28Extended%20Abstract%29&body=Title%3A%20Amortized%20Bayesian%20Workflow%20%28Extended%20Abstract%29%0AAuthor%3A%20Marvin%20Schmitt%20and%20Chengkun%20Li%20and%20Aki%20Vehtari%20and%20Luigi%20Acerbi%20and%20Paul-Christian%20B%C3%BCrkner%20and%20Stefan%20T.%20Radev%0AAbstract%3A%20%20%20Bayesian%20inference%20often%20faces%20a%20trade-off%20between%20computational%20speed%20and%0Asampling%20accuracy.%20We%20propose%20an%20adaptive%20workflow%20that%20integrates%20rapid%0Aamortized%20inference%20with%20gold-standard%20MCMC%20techniques%20to%20achieve%20both%20speed%0Aand%20accuracy%20when%20performing%20inference%20on%20many%20observed%20datasets.%20Our%20approach%0Auses%20principled%20diagnostics%20to%20guide%20the%20choice%20of%20inference%20method%20for%20each%0Adataset%2C%20moving%20along%20the%20Pareto%20front%20from%20fast%20amortized%20sampling%20to%20slower%0Abut%20guaranteed-accurate%20MCMC%20when%20necessary.%20By%20reusing%20computations%20across%0Asteps%2C%20our%20workflow%20creates%20synergies%20between%20amortized%20and%20MCMC-based%0Ainference.%20We%20demonstrate%20the%20effectiveness%20of%20this%20integrated%20approach%20on%20a%0Ageneralized%20extreme%20value%20task%20with%201000%20observed%20data%20sets%2C%20showing%2090x%20time%0Aefficiency%20gains%20while%20maintaining%20high%20posterior%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAmortized%2520Bayesian%2520Workflow%2520%2528Extended%2520Abstract%2529%26entry.906535625%3DMarvin%2520Schmitt%2520and%2520Chengkun%2520Li%2520and%2520Aki%2520Vehtari%2520and%2520Luigi%2520Acerbi%2520and%2520Paul-Christian%2520B%25C3%25BCrkner%2520and%2520Stefan%2520T.%2520Radev%26entry.1292438233%3D%2520%2520Bayesian%2520inference%2520often%2520faces%2520a%2520trade-off%2520between%2520computational%2520speed%2520and%250Asampling%2520accuracy.%2520We%2520propose%2520an%2520adaptive%2520workflow%2520that%2520integrates%2520rapid%250Aamortized%2520inference%2520with%2520gold-standard%2520MCMC%2520techniques%2520to%2520achieve%2520both%2520speed%250Aand%2520accuracy%2520when%2520performing%2520inference%2520on%2520many%2520observed%2520datasets.%2520Our%2520approach%250Auses%2520principled%2520diagnostics%2520to%2520guide%2520the%2520choice%2520of%2520inference%2520method%2520for%2520each%250Adataset%252C%2520moving%2520along%2520the%2520Pareto%2520front%2520from%2520fast%2520amortized%2520sampling%2520to%2520slower%250Abut%2520guaranteed-accurate%2520MCMC%2520when%2520necessary.%2520By%2520reusing%2520computations%2520across%250Asteps%252C%2520our%2520workflow%2520creates%2520synergies%2520between%2520amortized%2520and%2520MCMC-based%250Ainference.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520integrated%2520approach%2520on%2520a%250Ageneralized%2520extreme%2520value%2520task%2520with%25201000%2520observed%2520data%2520sets%252C%2520showing%252090x%2520time%250Aefficiency%2520gains%2520while%2520maintaining%2520high%2520posterior%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Amortized%20Bayesian%20Workflow%20%28Extended%20Abstract%29&entry.906535625=Marvin%20Schmitt%20and%20Chengkun%20Li%20and%20Aki%20Vehtari%20and%20Luigi%20Acerbi%20and%20Paul-Christian%20B%C3%BCrkner%20and%20Stefan%20T.%20Radev&entry.1292438233=%20%20Bayesian%20inference%20often%20faces%20a%20trade-off%20between%20computational%20speed%20and%0Asampling%20accuracy.%20We%20propose%20an%20adaptive%20workflow%20that%20integrates%20rapid%0Aamortized%20inference%20with%20gold-standard%20MCMC%20techniques%20to%20achieve%20both%20speed%0Aand%20accuracy%20when%20performing%20inference%20on%20many%20observed%20datasets.%20Our%20approach%0Auses%20principled%20diagnostics%20to%20guide%20the%20choice%20of%20inference%20method%20for%20each%0Adataset%2C%20moving%20along%20the%20Pareto%20front%20from%20fast%20amortized%20sampling%20to%20slower%0Abut%20guaranteed-accurate%20MCMC%20when%20necessary.%20By%20reusing%20computations%20across%0Asteps%2C%20our%20workflow%20creates%20synergies%20between%20amortized%20and%20MCMC-based%0Ainference.%20We%20demonstrate%20the%20effectiveness%20of%20this%20integrated%20approach%20on%20a%0Ageneralized%20extreme%20value%20task%20with%201000%20observed%20data%20sets%2C%20showing%2090x%20time%0Aefficiency%20gains%20while%20maintaining%20high%20posterior%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04332v1&entry.124074799=Read"},
{"title": "Quantum Kernel Methods under Scrutiny: A Benchmarking Study", "author": "Jan Schnabel and Marco Roth", "abstract": "  Since the entry of kernel theory in the field of quantum machine learning,\nquantum kernel methods (QKMs) have gained increasing attention with regard to\nboth probing promising applications and delivering intriguing research\ninsights. Two common approaches for computing the underlying Gram matrix have\nemerged: fidelity quantum kernels (FQKs) and projected quantum kernels (PQKs).\nBenchmarking these methods is crucial to gain robust insights and to understand\ntheir practical utility. In this work, we present a comprehensive large-scale\nstudy examining QKMs based on FQKs and PQKs across a manifold of design\nchoices. Our investigation encompasses both classification and regression tasks\nfor five dataset families and 64 datasets, systematically comparing the use of\nFQKs and PQKs quantum support vector machines and kernel ridge regression. This\nresulted in over 20,000 models that were trained and optimized using a\nstate-of-the-art hyperparameter search to ensure robust and comprehensive\ninsights. We delve into the importance of hyperparameters on model performance\nscores and support our findings through rigorous correlation analyses. In this,\nwe also closely inspect two data encoding strategies. Moreover, we provide an\nin-depth analysis addressing the design freedom of PQKs and explore the\nunderlying principles responsible for learning. Our goal is not to identify the\nbest-performing model for a specific task but to uncover the mechanisms that\nlead to effective QKMs and reveal universal patterns.\n", "link": "http://arxiv.org/abs/2409.04406v1", "date": "2024-09-06", "relevancy": 1.8746, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.474}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Kernel%20Methods%20under%20Scrutiny%3A%20A%20Benchmarking%20Study&body=Title%3A%20Quantum%20Kernel%20Methods%20under%20Scrutiny%3A%20A%20Benchmarking%20Study%0AAuthor%3A%20Jan%20Schnabel%20and%20Marco%20Roth%0AAbstract%3A%20%20%20Since%20the%20entry%20of%20kernel%20theory%20in%20the%20field%20of%20quantum%20machine%20learning%2C%0Aquantum%20kernel%20methods%20%28QKMs%29%20have%20gained%20increasing%20attention%20with%20regard%20to%0Aboth%20probing%20promising%20applications%20and%20delivering%20intriguing%20research%0Ainsights.%20Two%20common%20approaches%20for%20computing%20the%20underlying%20Gram%20matrix%20have%0Aemerged%3A%20fidelity%20quantum%20kernels%20%28FQKs%29%20and%20projected%20quantum%20kernels%20%28PQKs%29.%0ABenchmarking%20these%20methods%20is%20crucial%20to%20gain%20robust%20insights%20and%20to%20understand%0Atheir%20practical%20utility.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%20large-scale%0Astudy%20examining%20QKMs%20based%20on%20FQKs%20and%20PQKs%20across%20a%20manifold%20of%20design%0Achoices.%20Our%20investigation%20encompasses%20both%20classification%20and%20regression%20tasks%0Afor%20five%20dataset%20families%20and%2064%20datasets%2C%20systematically%20comparing%20the%20use%20of%0AFQKs%20and%20PQKs%20quantum%20support%20vector%20machines%20and%20kernel%20ridge%20regression.%20This%0Aresulted%20in%20over%2020%2C000%20models%20that%20were%20trained%20and%20optimized%20using%20a%0Astate-of-the-art%20hyperparameter%20search%20to%20ensure%20robust%20and%20comprehensive%0Ainsights.%20We%20delve%20into%20the%20importance%20of%20hyperparameters%20on%20model%20performance%0Ascores%20and%20support%20our%20findings%20through%20rigorous%20correlation%20analyses.%20In%20this%2C%0Awe%20also%20closely%20inspect%20two%20data%20encoding%20strategies.%20Moreover%2C%20we%20provide%20an%0Ain-depth%20analysis%20addressing%20the%20design%20freedom%20of%20PQKs%20and%20explore%20the%0Aunderlying%20principles%20responsible%20for%20learning.%20Our%20goal%20is%20not%20to%20identify%20the%0Abest-performing%20model%20for%20a%20specific%20task%20but%20to%20uncover%20the%20mechanisms%20that%0Alead%20to%20effective%20QKMs%20and%20reveal%20universal%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Kernel%2520Methods%2520under%2520Scrutiny%253A%2520A%2520Benchmarking%2520Study%26entry.906535625%3DJan%2520Schnabel%2520and%2520Marco%2520Roth%26entry.1292438233%3D%2520%2520Since%2520the%2520entry%2520of%2520kernel%2520theory%2520in%2520the%2520field%2520of%2520quantum%2520machine%2520learning%252C%250Aquantum%2520kernel%2520methods%2520%2528QKMs%2529%2520have%2520gained%2520increasing%2520attention%2520with%2520regard%2520to%250Aboth%2520probing%2520promising%2520applications%2520and%2520delivering%2520intriguing%2520research%250Ainsights.%2520Two%2520common%2520approaches%2520for%2520computing%2520the%2520underlying%2520Gram%2520matrix%2520have%250Aemerged%253A%2520fidelity%2520quantum%2520kernels%2520%2528FQKs%2529%2520and%2520projected%2520quantum%2520kernels%2520%2528PQKs%2529.%250ABenchmarking%2520these%2520methods%2520is%2520crucial%2520to%2520gain%2520robust%2520insights%2520and%2520to%2520understand%250Atheir%2520practical%2520utility.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520comprehensive%2520large-scale%250Astudy%2520examining%2520QKMs%2520based%2520on%2520FQKs%2520and%2520PQKs%2520across%2520a%2520manifold%2520of%2520design%250Achoices.%2520Our%2520investigation%2520encompasses%2520both%2520classification%2520and%2520regression%2520tasks%250Afor%2520five%2520dataset%2520families%2520and%252064%2520datasets%252C%2520systematically%2520comparing%2520the%2520use%2520of%250AFQKs%2520and%2520PQKs%2520quantum%2520support%2520vector%2520machines%2520and%2520kernel%2520ridge%2520regression.%2520This%250Aresulted%2520in%2520over%252020%252C000%2520models%2520that%2520were%2520trained%2520and%2520optimized%2520using%2520a%250Astate-of-the-art%2520hyperparameter%2520search%2520to%2520ensure%2520robust%2520and%2520comprehensive%250Ainsights.%2520We%2520delve%2520into%2520the%2520importance%2520of%2520hyperparameters%2520on%2520model%2520performance%250Ascores%2520and%2520support%2520our%2520findings%2520through%2520rigorous%2520correlation%2520analyses.%2520In%2520this%252C%250Awe%2520also%2520closely%2520inspect%2520two%2520data%2520encoding%2520strategies.%2520Moreover%252C%2520we%2520provide%2520an%250Ain-depth%2520analysis%2520addressing%2520the%2520design%2520freedom%2520of%2520PQKs%2520and%2520explore%2520the%250Aunderlying%2520principles%2520responsible%2520for%2520learning.%2520Our%2520goal%2520is%2520not%2520to%2520identify%2520the%250Abest-performing%2520model%2520for%2520a%2520specific%2520task%2520but%2520to%2520uncover%2520the%2520mechanisms%2520that%250Alead%2520to%2520effective%2520QKMs%2520and%2520reveal%2520universal%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Kernel%20Methods%20under%20Scrutiny%3A%20A%20Benchmarking%20Study&entry.906535625=Jan%20Schnabel%20and%20Marco%20Roth&entry.1292438233=%20%20Since%20the%20entry%20of%20kernel%20theory%20in%20the%20field%20of%20quantum%20machine%20learning%2C%0Aquantum%20kernel%20methods%20%28QKMs%29%20have%20gained%20increasing%20attention%20with%20regard%20to%0Aboth%20probing%20promising%20applications%20and%20delivering%20intriguing%20research%0Ainsights.%20Two%20common%20approaches%20for%20computing%20the%20underlying%20Gram%20matrix%20have%0Aemerged%3A%20fidelity%20quantum%20kernels%20%28FQKs%29%20and%20projected%20quantum%20kernels%20%28PQKs%29.%0ABenchmarking%20these%20methods%20is%20crucial%20to%20gain%20robust%20insights%20and%20to%20understand%0Atheir%20practical%20utility.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%20large-scale%0Astudy%20examining%20QKMs%20based%20on%20FQKs%20and%20PQKs%20across%20a%20manifold%20of%20design%0Achoices.%20Our%20investigation%20encompasses%20both%20classification%20and%20regression%20tasks%0Afor%20five%20dataset%20families%20and%2064%20datasets%2C%20systematically%20comparing%20the%20use%20of%0AFQKs%20and%20PQKs%20quantum%20support%20vector%20machines%20and%20kernel%20ridge%20regression.%20This%0Aresulted%20in%20over%2020%2C000%20models%20that%20were%20trained%20and%20optimized%20using%20a%0Astate-of-the-art%20hyperparameter%20search%20to%20ensure%20robust%20and%20comprehensive%0Ainsights.%20We%20delve%20into%20the%20importance%20of%20hyperparameters%20on%20model%20performance%0Ascores%20and%20support%20our%20findings%20through%20rigorous%20correlation%20analyses.%20In%20this%2C%0Awe%20also%20closely%20inspect%20two%20data%20encoding%20strategies.%20Moreover%2C%20we%20provide%20an%0Ain-depth%20analysis%20addressing%20the%20design%20freedom%20of%20PQKs%20and%20explore%20the%0Aunderlying%20principles%20responsible%20for%20learning.%20Our%20goal%20is%20not%20to%20identify%20the%0Abest-performing%20model%20for%20a%20specific%20task%20but%20to%20uncover%20the%20mechanisms%20that%0Alead%20to%20effective%20QKMs%20and%20reveal%20universal%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04406v1&entry.124074799=Read"},
{"title": "How to Identify Good Superpixels for Deforestation Detection on Tropical\n  Rainforests", "author": "Isabela Borlido and Eduardo Bouhid and Victor Sundermann and Hugo Resende and Alvaro Luiz Fazenda and Fabio Faria and Silvio Jamil F. Guimar\u00e3es", "abstract": "  The conservation of tropical forests is a topic of significant social and\necological relevance due to their crucial role in the global ecosystem.\nUnfortunately, deforestation and degradation impact millions of hectares\nannually, requiring government or private initiatives for effective forest\nmonitoring. However, identifying deforested regions in satellite images is\nchallenging due to data imbalance, image resolution, low-contrast regions, and\nocclusion. Superpixel segmentation can overcome these drawbacks, reducing\nworkload and preserving important image boundaries. However, most works for\nremote sensing images do not exploit recent superpixel methods. In this work,\nwe evaluate 16 superpixel methods in satellite images to support a\ndeforestation detection system in tropical forests. We also assess the\nperformance of superpixel methods for the target task, establishing a\nrelationship with segmentation methodological evaluation. According to our\nresults, ERS, GMMSP, and DISF perform best on UE, BR, and SIRS, respectively,\nwhereas ERS has the best trade-off with CO and Reg. In classification, SH,\nDISF, and ISF perform best on RGB, UMDA, and PCA compositions, respectively.\nAccording to our experiments, superpixel methods with better trade-offs between\ndelineation, homogeneity, compactness, and regularity are more suitable for\nidentifying good superpixels for deforestation detection tasks.\n", "link": "http://arxiv.org/abs/2409.04330v1", "date": "2024-09-06", "relevancy": 1.867, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4746}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4703}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Identify%20Good%20Superpixels%20for%20Deforestation%20Detection%20on%20Tropical%0A%20%20Rainforests&body=Title%3A%20How%20to%20Identify%20Good%20Superpixels%20for%20Deforestation%20Detection%20on%20Tropical%0A%20%20Rainforests%0AAuthor%3A%20Isabela%20Borlido%20and%20Eduardo%20Bouhid%20and%20Victor%20Sundermann%20and%20Hugo%20Resende%20and%20Alvaro%20Luiz%20Fazenda%20and%20Fabio%20Faria%20and%20Silvio%20Jamil%20F.%20Guimar%C3%A3es%0AAbstract%3A%20%20%20The%20conservation%20of%20tropical%20forests%20is%20a%20topic%20of%20significant%20social%20and%0Aecological%20relevance%20due%20to%20their%20crucial%20role%20in%20the%20global%20ecosystem.%0AUnfortunately%2C%20deforestation%20and%20degradation%20impact%20millions%20of%20hectares%0Aannually%2C%20requiring%20government%20or%20private%20initiatives%20for%20effective%20forest%0Amonitoring.%20However%2C%20identifying%20deforested%20regions%20in%20satellite%20images%20is%0Achallenging%20due%20to%20data%20imbalance%2C%20image%20resolution%2C%20low-contrast%20regions%2C%20and%0Aocclusion.%20Superpixel%20segmentation%20can%20overcome%20these%20drawbacks%2C%20reducing%0Aworkload%20and%20preserving%20important%20image%20boundaries.%20However%2C%20most%20works%20for%0Aremote%20sensing%20images%20do%20not%20exploit%20recent%20superpixel%20methods.%20In%20this%20work%2C%0Awe%20evaluate%2016%20superpixel%20methods%20in%20satellite%20images%20to%20support%20a%0Adeforestation%20detection%20system%20in%20tropical%20forests.%20We%20also%20assess%20the%0Aperformance%20of%20superpixel%20methods%20for%20the%20target%20task%2C%20establishing%20a%0Arelationship%20with%20segmentation%20methodological%20evaluation.%20According%20to%20our%0Aresults%2C%20ERS%2C%20GMMSP%2C%20and%20DISF%20perform%20best%20on%20UE%2C%20BR%2C%20and%20SIRS%2C%20respectively%2C%0Awhereas%20ERS%20has%20the%20best%20trade-off%20with%20CO%20and%20Reg.%20In%20classification%2C%20SH%2C%0ADISF%2C%20and%20ISF%20perform%20best%20on%20RGB%2C%20UMDA%2C%20and%20PCA%20compositions%2C%20respectively.%0AAccording%20to%20our%20experiments%2C%20superpixel%20methods%20with%20better%20trade-offs%20between%0Adelineation%2C%20homogeneity%2C%20compactness%2C%20and%20regularity%20are%20more%20suitable%20for%0Aidentifying%20good%20superpixels%20for%20deforestation%20detection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Identify%2520Good%2520Superpixels%2520for%2520Deforestation%2520Detection%2520on%2520Tropical%250A%2520%2520Rainforests%26entry.906535625%3DIsabela%2520Borlido%2520and%2520Eduardo%2520Bouhid%2520and%2520Victor%2520Sundermann%2520and%2520Hugo%2520Resende%2520and%2520Alvaro%2520Luiz%2520Fazenda%2520and%2520Fabio%2520Faria%2520and%2520Silvio%2520Jamil%2520F.%2520Guimar%25C3%25A3es%26entry.1292438233%3D%2520%2520The%2520conservation%2520of%2520tropical%2520forests%2520is%2520a%2520topic%2520of%2520significant%2520social%2520and%250Aecological%2520relevance%2520due%2520to%2520their%2520crucial%2520role%2520in%2520the%2520global%2520ecosystem.%250AUnfortunately%252C%2520deforestation%2520and%2520degradation%2520impact%2520millions%2520of%2520hectares%250Aannually%252C%2520requiring%2520government%2520or%2520private%2520initiatives%2520for%2520effective%2520forest%250Amonitoring.%2520However%252C%2520identifying%2520deforested%2520regions%2520in%2520satellite%2520images%2520is%250Achallenging%2520due%2520to%2520data%2520imbalance%252C%2520image%2520resolution%252C%2520low-contrast%2520regions%252C%2520and%250Aocclusion.%2520Superpixel%2520segmentation%2520can%2520overcome%2520these%2520drawbacks%252C%2520reducing%250Aworkload%2520and%2520preserving%2520important%2520image%2520boundaries.%2520However%252C%2520most%2520works%2520for%250Aremote%2520sensing%2520images%2520do%2520not%2520exploit%2520recent%2520superpixel%2520methods.%2520In%2520this%2520work%252C%250Awe%2520evaluate%252016%2520superpixel%2520methods%2520in%2520satellite%2520images%2520to%2520support%2520a%250Adeforestation%2520detection%2520system%2520in%2520tropical%2520forests.%2520We%2520also%2520assess%2520the%250Aperformance%2520of%2520superpixel%2520methods%2520for%2520the%2520target%2520task%252C%2520establishing%2520a%250Arelationship%2520with%2520segmentation%2520methodological%2520evaluation.%2520According%2520to%2520our%250Aresults%252C%2520ERS%252C%2520GMMSP%252C%2520and%2520DISF%2520perform%2520best%2520on%2520UE%252C%2520BR%252C%2520and%2520SIRS%252C%2520respectively%252C%250Awhereas%2520ERS%2520has%2520the%2520best%2520trade-off%2520with%2520CO%2520and%2520Reg.%2520In%2520classification%252C%2520SH%252C%250ADISF%252C%2520and%2520ISF%2520perform%2520best%2520on%2520RGB%252C%2520UMDA%252C%2520and%2520PCA%2520compositions%252C%2520respectively.%250AAccording%2520to%2520our%2520experiments%252C%2520superpixel%2520methods%2520with%2520better%2520trade-offs%2520between%250Adelineation%252C%2520homogeneity%252C%2520compactness%252C%2520and%2520regularity%2520are%2520more%2520suitable%2520for%250Aidentifying%2520good%2520superpixels%2520for%2520deforestation%2520detection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Identify%20Good%20Superpixels%20for%20Deforestation%20Detection%20on%20Tropical%0A%20%20Rainforests&entry.906535625=Isabela%20Borlido%20and%20Eduardo%20Bouhid%20and%20Victor%20Sundermann%20and%20Hugo%20Resende%20and%20Alvaro%20Luiz%20Fazenda%20and%20Fabio%20Faria%20and%20Silvio%20Jamil%20F.%20Guimar%C3%A3es&entry.1292438233=%20%20The%20conservation%20of%20tropical%20forests%20is%20a%20topic%20of%20significant%20social%20and%0Aecological%20relevance%20due%20to%20their%20crucial%20role%20in%20the%20global%20ecosystem.%0AUnfortunately%2C%20deforestation%20and%20degradation%20impact%20millions%20of%20hectares%0Aannually%2C%20requiring%20government%20or%20private%20initiatives%20for%20effective%20forest%0Amonitoring.%20However%2C%20identifying%20deforested%20regions%20in%20satellite%20images%20is%0Achallenging%20due%20to%20data%20imbalance%2C%20image%20resolution%2C%20low-contrast%20regions%2C%20and%0Aocclusion.%20Superpixel%20segmentation%20can%20overcome%20these%20drawbacks%2C%20reducing%0Aworkload%20and%20preserving%20important%20image%20boundaries.%20However%2C%20most%20works%20for%0Aremote%20sensing%20images%20do%20not%20exploit%20recent%20superpixel%20methods.%20In%20this%20work%2C%0Awe%20evaluate%2016%20superpixel%20methods%20in%20satellite%20images%20to%20support%20a%0Adeforestation%20detection%20system%20in%20tropical%20forests.%20We%20also%20assess%20the%0Aperformance%20of%20superpixel%20methods%20for%20the%20target%20task%2C%20establishing%20a%0Arelationship%20with%20segmentation%20methodological%20evaluation.%20According%20to%20our%0Aresults%2C%20ERS%2C%20GMMSP%2C%20and%20DISF%20perform%20best%20on%20UE%2C%20BR%2C%20and%20SIRS%2C%20respectively%2C%0Awhereas%20ERS%20has%20the%20best%20trade-off%20with%20CO%20and%20Reg.%20In%20classification%2C%20SH%2C%0ADISF%2C%20and%20ISF%20perform%20best%20on%20RGB%2C%20UMDA%2C%20and%20PCA%20compositions%2C%20respectively.%0AAccording%20to%20our%20experiments%2C%20superpixel%20methods%20with%20better%20trade-offs%20between%0Adelineation%2C%20homogeneity%2C%20compactness%2C%20and%20regularity%20are%20more%20suitable%20for%0Aidentifying%20good%20superpixels%20for%20deforestation%20detection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04330v1&entry.124074799=Read"},
{"title": "MpoxMamba: A Grouped Mamba-based Lightweight Hybrid Network for Mpox\n  Detection", "author": "Yubiao Yue and Jun Xue and Haihuang Liang and Zhenzhang Li and Yufeng Wang", "abstract": "  Due to the lack of effective mpox detection tools, the mpox virus continues\nto spread worldwide and has once again been declared a public health emergency\nof international concern by the World Health Organization. Deep learning-based\nmpox detection tools are crucial to alleviate mpox outbreak. However, existing\nmethods have difficulty in achieving a good trade-off between detection\nperformance, parameter size, and model complexity, which is crucial for\npractical applications and widespread deployment, especially in\nresource-limited scenarios. Given that the success of Mamba in modeling\nlong-range dependencies and its linear complexity, we proposed a lightweight\nhybrid architecture called MpoxMamba. MpoxMamba utilizes deep separable\nconvolutions to extract local feature representations in mpox skin lesions, and\ngreatly enhances the model's ability to model the global contextual information\nby grouped Mamba modules. Experimental results on two widely recognized mpox\ndatasets demonstrate that MpoxMamba outperforms existing mpox detection methods\nand state-of-the-art lightweight models. We also developed a web-based online\napplication to provide free mpox detection services to the public in the\nepidemic areas (http://5227i971s5.goho.co:30290). The source codes of MpoxMamba\nare available at https://github.com/YubiaoYue/MpoxMamba.\n", "link": "http://arxiv.org/abs/2409.04218v1", "date": "2024-09-06", "relevancy": 1.8629, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4822}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4553}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MpoxMamba%3A%20A%20Grouped%20Mamba-based%20Lightweight%20Hybrid%20Network%20for%20Mpox%0A%20%20Detection&body=Title%3A%20MpoxMamba%3A%20A%20Grouped%20Mamba-based%20Lightweight%20Hybrid%20Network%20for%20Mpox%0A%20%20Detection%0AAuthor%3A%20Yubiao%20Yue%20and%20Jun%20Xue%20and%20Haihuang%20Liang%20and%20Zhenzhang%20Li%20and%20Yufeng%20Wang%0AAbstract%3A%20%20%20Due%20to%20the%20lack%20of%20effective%20mpox%20detection%20tools%2C%20the%20mpox%20virus%20continues%0Ato%20spread%20worldwide%20and%20has%20once%20again%20been%20declared%20a%20public%20health%20emergency%0Aof%20international%20concern%20by%20the%20World%20Health%20Organization.%20Deep%20learning-based%0Ampox%20detection%20tools%20are%20crucial%20to%20alleviate%20mpox%20outbreak.%20However%2C%20existing%0Amethods%20have%20difficulty%20in%20achieving%20a%20good%20trade-off%20between%20detection%0Aperformance%2C%20parameter%20size%2C%20and%20model%20complexity%2C%20which%20is%20crucial%20for%0Apractical%20applications%20and%20widespread%20deployment%2C%20especially%20in%0Aresource-limited%20scenarios.%20Given%20that%20the%20success%20of%20Mamba%20in%20modeling%0Along-range%20dependencies%20and%20its%20linear%20complexity%2C%20we%20proposed%20a%20lightweight%0Ahybrid%20architecture%20called%20MpoxMamba.%20MpoxMamba%20utilizes%20deep%20separable%0Aconvolutions%20to%20extract%20local%20feature%20representations%20in%20mpox%20skin%20lesions%2C%20and%0Agreatly%20enhances%20the%20model%27s%20ability%20to%20model%20the%20global%20contextual%20information%0Aby%20grouped%20Mamba%20modules.%20Experimental%20results%20on%20two%20widely%20recognized%20mpox%0Adatasets%20demonstrate%20that%20MpoxMamba%20outperforms%20existing%20mpox%20detection%20methods%0Aand%20state-of-the-art%20lightweight%20models.%20We%20also%20developed%20a%20web-based%20online%0Aapplication%20to%20provide%20free%20mpox%20detection%20services%20to%20the%20public%20in%20the%0Aepidemic%20areas%20%28http%3A//5227i971s5.goho.co%3A30290%29.%20The%20source%20codes%20of%20MpoxMamba%0Aare%20available%20at%20https%3A//github.com/YubiaoYue/MpoxMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMpoxMamba%253A%2520A%2520Grouped%2520Mamba-based%2520Lightweight%2520Hybrid%2520Network%2520for%2520Mpox%250A%2520%2520Detection%26entry.906535625%3DYubiao%2520Yue%2520and%2520Jun%2520Xue%2520and%2520Haihuang%2520Liang%2520and%2520Zhenzhang%2520Li%2520and%2520Yufeng%2520Wang%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520lack%2520of%2520effective%2520mpox%2520detection%2520tools%252C%2520the%2520mpox%2520virus%2520continues%250Ato%2520spread%2520worldwide%2520and%2520has%2520once%2520again%2520been%2520declared%2520a%2520public%2520health%2520emergency%250Aof%2520international%2520concern%2520by%2520the%2520World%2520Health%2520Organization.%2520Deep%2520learning-based%250Ampox%2520detection%2520tools%2520are%2520crucial%2520to%2520alleviate%2520mpox%2520outbreak.%2520However%252C%2520existing%250Amethods%2520have%2520difficulty%2520in%2520achieving%2520a%2520good%2520trade-off%2520between%2520detection%250Aperformance%252C%2520parameter%2520size%252C%2520and%2520model%2520complexity%252C%2520which%2520is%2520crucial%2520for%250Apractical%2520applications%2520and%2520widespread%2520deployment%252C%2520especially%2520in%250Aresource-limited%2520scenarios.%2520Given%2520that%2520the%2520success%2520of%2520Mamba%2520in%2520modeling%250Along-range%2520dependencies%2520and%2520its%2520linear%2520complexity%252C%2520we%2520proposed%2520a%2520lightweight%250Ahybrid%2520architecture%2520called%2520MpoxMamba.%2520MpoxMamba%2520utilizes%2520deep%2520separable%250Aconvolutions%2520to%2520extract%2520local%2520feature%2520representations%2520in%2520mpox%2520skin%2520lesions%252C%2520and%250Agreatly%2520enhances%2520the%2520model%2527s%2520ability%2520to%2520model%2520the%2520global%2520contextual%2520information%250Aby%2520grouped%2520Mamba%2520modules.%2520Experimental%2520results%2520on%2520two%2520widely%2520recognized%2520mpox%250Adatasets%2520demonstrate%2520that%2520MpoxMamba%2520outperforms%2520existing%2520mpox%2520detection%2520methods%250Aand%2520state-of-the-art%2520lightweight%2520models.%2520We%2520also%2520developed%2520a%2520web-based%2520online%250Aapplication%2520to%2520provide%2520free%2520mpox%2520detection%2520services%2520to%2520the%2520public%2520in%2520the%250Aepidemic%2520areas%2520%2528http%253A//5227i971s5.goho.co%253A30290%2529.%2520The%2520source%2520codes%2520of%2520MpoxMamba%250Aare%2520available%2520at%2520https%253A//github.com/YubiaoYue/MpoxMamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MpoxMamba%3A%20A%20Grouped%20Mamba-based%20Lightweight%20Hybrid%20Network%20for%20Mpox%0A%20%20Detection&entry.906535625=Yubiao%20Yue%20and%20Jun%20Xue%20and%20Haihuang%20Liang%20and%20Zhenzhang%20Li%20and%20Yufeng%20Wang&entry.1292438233=%20%20Due%20to%20the%20lack%20of%20effective%20mpox%20detection%20tools%2C%20the%20mpox%20virus%20continues%0Ato%20spread%20worldwide%20and%20has%20once%20again%20been%20declared%20a%20public%20health%20emergency%0Aof%20international%20concern%20by%20the%20World%20Health%20Organization.%20Deep%20learning-based%0Ampox%20detection%20tools%20are%20crucial%20to%20alleviate%20mpox%20outbreak.%20However%2C%20existing%0Amethods%20have%20difficulty%20in%20achieving%20a%20good%20trade-off%20between%20detection%0Aperformance%2C%20parameter%20size%2C%20and%20model%20complexity%2C%20which%20is%20crucial%20for%0Apractical%20applications%20and%20widespread%20deployment%2C%20especially%20in%0Aresource-limited%20scenarios.%20Given%20that%20the%20success%20of%20Mamba%20in%20modeling%0Along-range%20dependencies%20and%20its%20linear%20complexity%2C%20we%20proposed%20a%20lightweight%0Ahybrid%20architecture%20called%20MpoxMamba.%20MpoxMamba%20utilizes%20deep%20separable%0Aconvolutions%20to%20extract%20local%20feature%20representations%20in%20mpox%20skin%20lesions%2C%20and%0Agreatly%20enhances%20the%20model%27s%20ability%20to%20model%20the%20global%20contextual%20information%0Aby%20grouped%20Mamba%20modules.%20Experimental%20results%20on%20two%20widely%20recognized%20mpox%0Adatasets%20demonstrate%20that%20MpoxMamba%20outperforms%20existing%20mpox%20detection%20methods%0Aand%20state-of-the-art%20lightweight%20models.%20We%20also%20developed%20a%20web-based%20online%0Aapplication%20to%20provide%20free%20mpox%20detection%20services%20to%20the%20public%20in%20the%0Aepidemic%20areas%20%28http%3A//5227i971s5.goho.co%3A30290%29.%20The%20source%20codes%20of%20MpoxMamba%0Aare%20available%20at%20https%3A//github.com/YubiaoYue/MpoxMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04218v1&entry.124074799=Read"},
{"title": "A naive aggregation algorithm for improving generalization in a class of\n  learning problems", "author": "Getachew K Befekadu", "abstract": "  In this brief paper, we present a naive aggregation algorithm for a typical\nlearning problem with expert advice setting, in which the task of improving\ngeneralization, i.e., model validation, is embedded in the learning process as\na sequential decision-making problem. In particular, we consider a class of\nlearning problem of point estimations for modeling high-dimensional nonlinear\nfunctions, where a group of experts update their parameter estimates using the\ndiscrete-time version of gradient systems, with small additive noise term,\nguided by the corresponding subsample datasets obtained from the original\ndataset. Here, our main objective is to provide conditions under which such an\nalgorithm will sequentially determine a set of mixing distribution strategies\nused for aggregating the experts' estimates that ultimately leading to an\noptimal parameter estimate, i.e., as a consensus solution for all experts,\nwhich is better than any individual expert's estimate in terms of improved\ngeneralization or learning performances. Finally, as part of this work, we\npresent some numerical results for a typical case of nonlinear regression\nproblem.\n", "link": "http://arxiv.org/abs/2409.04352v1", "date": "2024-09-06", "relevancy": 1.8564, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4671}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4637}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20naive%20aggregation%20algorithm%20for%20improving%20generalization%20in%20a%20class%20of%0A%20%20learning%20problems&body=Title%3A%20A%20naive%20aggregation%20algorithm%20for%20improving%20generalization%20in%20a%20class%20of%0A%20%20learning%20problems%0AAuthor%3A%20Getachew%20K%20Befekadu%0AAbstract%3A%20%20%20In%20this%20brief%20paper%2C%20we%20present%20a%20naive%20aggregation%20algorithm%20for%20a%20typical%0Alearning%20problem%20with%20expert%20advice%20setting%2C%20in%20which%20the%20task%20of%20improving%0Ageneralization%2C%20i.e.%2C%20model%20validation%2C%20is%20embedded%20in%20the%20learning%20process%20as%0Aa%20sequential%20decision-making%20problem.%20In%20particular%2C%20we%20consider%20a%20class%20of%0Alearning%20problem%20of%20point%20estimations%20for%20modeling%20high-dimensional%20nonlinear%0Afunctions%2C%20where%20a%20group%20of%20experts%20update%20their%20parameter%20estimates%20using%20the%0Adiscrete-time%20version%20of%20gradient%20systems%2C%20with%20small%20additive%20noise%20term%2C%0Aguided%20by%20the%20corresponding%20subsample%20datasets%20obtained%20from%20the%20original%0Adataset.%20Here%2C%20our%20main%20objective%20is%20to%20provide%20conditions%20under%20which%20such%20an%0Aalgorithm%20will%20sequentially%20determine%20a%20set%20of%20mixing%20distribution%20strategies%0Aused%20for%20aggregating%20the%20experts%27%20estimates%20that%20ultimately%20leading%20to%20an%0Aoptimal%20parameter%20estimate%2C%20i.e.%2C%20as%20a%20consensus%20solution%20for%20all%20experts%2C%0Awhich%20is%20better%20than%20any%20individual%20expert%27s%20estimate%20in%20terms%20of%20improved%0Ageneralization%20or%20learning%20performances.%20Finally%2C%20as%20part%20of%20this%20work%2C%20we%0Apresent%20some%20numerical%20results%20for%20a%20typical%20case%20of%20nonlinear%20regression%0Aproblem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520naive%2520aggregation%2520algorithm%2520for%2520improving%2520generalization%2520in%2520a%2520class%2520of%250A%2520%2520learning%2520problems%26entry.906535625%3DGetachew%2520K%2520Befekadu%26entry.1292438233%3D%2520%2520In%2520this%2520brief%2520paper%252C%2520we%2520present%2520a%2520naive%2520aggregation%2520algorithm%2520for%2520a%2520typical%250Alearning%2520problem%2520with%2520expert%2520advice%2520setting%252C%2520in%2520which%2520the%2520task%2520of%2520improving%250Ageneralization%252C%2520i.e.%252C%2520model%2520validation%252C%2520is%2520embedded%2520in%2520the%2520learning%2520process%2520as%250Aa%2520sequential%2520decision-making%2520problem.%2520In%2520particular%252C%2520we%2520consider%2520a%2520class%2520of%250Alearning%2520problem%2520of%2520point%2520estimations%2520for%2520modeling%2520high-dimensional%2520nonlinear%250Afunctions%252C%2520where%2520a%2520group%2520of%2520experts%2520update%2520their%2520parameter%2520estimates%2520using%2520the%250Adiscrete-time%2520version%2520of%2520gradient%2520systems%252C%2520with%2520small%2520additive%2520noise%2520term%252C%250Aguided%2520by%2520the%2520corresponding%2520subsample%2520datasets%2520obtained%2520from%2520the%2520original%250Adataset.%2520Here%252C%2520our%2520main%2520objective%2520is%2520to%2520provide%2520conditions%2520under%2520which%2520such%2520an%250Aalgorithm%2520will%2520sequentially%2520determine%2520a%2520set%2520of%2520mixing%2520distribution%2520strategies%250Aused%2520for%2520aggregating%2520the%2520experts%2527%2520estimates%2520that%2520ultimately%2520leading%2520to%2520an%250Aoptimal%2520parameter%2520estimate%252C%2520i.e.%252C%2520as%2520a%2520consensus%2520solution%2520for%2520all%2520experts%252C%250Awhich%2520is%2520better%2520than%2520any%2520individual%2520expert%2527s%2520estimate%2520in%2520terms%2520of%2520improved%250Ageneralization%2520or%2520learning%2520performances.%2520Finally%252C%2520as%2520part%2520of%2520this%2520work%252C%2520we%250Apresent%2520some%2520numerical%2520results%2520for%2520a%2520typical%2520case%2520of%2520nonlinear%2520regression%250Aproblem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20naive%20aggregation%20algorithm%20for%20improving%20generalization%20in%20a%20class%20of%0A%20%20learning%20problems&entry.906535625=Getachew%20K%20Befekadu&entry.1292438233=%20%20In%20this%20brief%20paper%2C%20we%20present%20a%20naive%20aggregation%20algorithm%20for%20a%20typical%0Alearning%20problem%20with%20expert%20advice%20setting%2C%20in%20which%20the%20task%20of%20improving%0Ageneralization%2C%20i.e.%2C%20model%20validation%2C%20is%20embedded%20in%20the%20learning%20process%20as%0Aa%20sequential%20decision-making%20problem.%20In%20particular%2C%20we%20consider%20a%20class%20of%0Alearning%20problem%20of%20point%20estimations%20for%20modeling%20high-dimensional%20nonlinear%0Afunctions%2C%20where%20a%20group%20of%20experts%20update%20their%20parameter%20estimates%20using%20the%0Adiscrete-time%20version%20of%20gradient%20systems%2C%20with%20small%20additive%20noise%20term%2C%0Aguided%20by%20the%20corresponding%20subsample%20datasets%20obtained%20from%20the%20original%0Adataset.%20Here%2C%20our%20main%20objective%20is%20to%20provide%20conditions%20under%20which%20such%20an%0Aalgorithm%20will%20sequentially%20determine%20a%20set%20of%20mixing%20distribution%20strategies%0Aused%20for%20aggregating%20the%20experts%27%20estimates%20that%20ultimately%20leading%20to%20an%0Aoptimal%20parameter%20estimate%2C%20i.e.%2C%20as%20a%20consensus%20solution%20for%20all%20experts%2C%0Awhich%20is%20better%20than%20any%20individual%20expert%27s%20estimate%20in%20terms%20of%20improved%0Ageneralization%20or%20learning%20performances.%20Finally%2C%20as%20part%20of%20this%20work%2C%20we%0Apresent%20some%20numerical%20results%20for%20a%20typical%20case%20of%20nonlinear%20regression%0Aproblem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04352v1&entry.124074799=Read"},
{"title": "Approximating Metric Magnitude of Point Sets", "author": "Rayna Andreeva and James Ward and Primoz Skraba and Jie Gao and Rik Sarkar", "abstract": "  Metric magnitude is a measure of the \"size\" of point clouds with many\ndesirable geometric properties. It has been adapted to various mathematical\ncontexts and recent work suggests that it can enhance machine learning and\noptimization algorithms. But its usability is limited due to the computational\ncost when the dataset is large or when the computation must be carried out\nrepeatedly (e.g. in model training). In this paper, we study the magnitude\ncomputation problem, and show efficient ways of approximating it. We show that\nit can be cast as a convex optimization problem, but not as a submodular\noptimization. The paper describes two new algorithms - an iterative\napproximation algorithm that converges fast and is accurate, and a subset\nselection method that makes the computation even faster. It has been previously\nproposed that magnitude of model sequences generated during stochastic gradient\ndescent is correlated to generalization gap. Extension of this result using our\nmore scalable algorithms shows that longer sequences in fact bear higher\ncorrelations. We also describe new applications of magnitude in machine\nlearning - as an effective regularizer for neural network training, and as a\nnovel clustering criterion.\n", "link": "http://arxiv.org/abs/2409.04411v1", "date": "2024-09-06", "relevancy": 1.8165, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.464}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4565}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approximating%20Metric%20Magnitude%20of%20Point%20Sets&body=Title%3A%20Approximating%20Metric%20Magnitude%20of%20Point%20Sets%0AAuthor%3A%20Rayna%20Andreeva%20and%20James%20Ward%20and%20Primoz%20Skraba%20and%20Jie%20Gao%20and%20Rik%20Sarkar%0AAbstract%3A%20%20%20Metric%20magnitude%20is%20a%20measure%20of%20the%20%22size%22%20of%20point%20clouds%20with%20many%0Adesirable%20geometric%20properties.%20It%20has%20been%20adapted%20to%20various%20mathematical%0Acontexts%20and%20recent%20work%20suggests%20that%20it%20can%20enhance%20machine%20learning%20and%0Aoptimization%20algorithms.%20But%20its%20usability%20is%20limited%20due%20to%20the%20computational%0Acost%20when%20the%20dataset%20is%20large%20or%20when%20the%20computation%20must%20be%20carried%20out%0Arepeatedly%20%28e.g.%20in%20model%20training%29.%20In%20this%20paper%2C%20we%20study%20the%20magnitude%0Acomputation%20problem%2C%20and%20show%20efficient%20ways%20of%20approximating%20it.%20We%20show%20that%0Ait%20can%20be%20cast%20as%20a%20convex%20optimization%20problem%2C%20but%20not%20as%20a%20submodular%0Aoptimization.%20The%20paper%20describes%20two%20new%20algorithms%20-%20an%20iterative%0Aapproximation%20algorithm%20that%20converges%20fast%20and%20is%20accurate%2C%20and%20a%20subset%0Aselection%20method%20that%20makes%20the%20computation%20even%20faster.%20It%20has%20been%20previously%0Aproposed%20that%20magnitude%20of%20model%20sequences%20generated%20during%20stochastic%20gradient%0Adescent%20is%20correlated%20to%20generalization%20gap.%20Extension%20of%20this%20result%20using%20our%0Amore%20scalable%20algorithms%20shows%20that%20longer%20sequences%20in%20fact%20bear%20higher%0Acorrelations.%20We%20also%20describe%20new%20applications%20of%20magnitude%20in%20machine%0Alearning%20-%20as%20an%20effective%20regularizer%20for%20neural%20network%20training%2C%20and%20as%20a%0Anovel%20clustering%20criterion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproximating%2520Metric%2520Magnitude%2520of%2520Point%2520Sets%26entry.906535625%3DRayna%2520Andreeva%2520and%2520James%2520Ward%2520and%2520Primoz%2520Skraba%2520and%2520Jie%2520Gao%2520and%2520Rik%2520Sarkar%26entry.1292438233%3D%2520%2520Metric%2520magnitude%2520is%2520a%2520measure%2520of%2520the%2520%2522size%2522%2520of%2520point%2520clouds%2520with%2520many%250Adesirable%2520geometric%2520properties.%2520It%2520has%2520been%2520adapted%2520to%2520various%2520mathematical%250Acontexts%2520and%2520recent%2520work%2520suggests%2520that%2520it%2520can%2520enhance%2520machine%2520learning%2520and%250Aoptimization%2520algorithms.%2520But%2520its%2520usability%2520is%2520limited%2520due%2520to%2520the%2520computational%250Acost%2520when%2520the%2520dataset%2520is%2520large%2520or%2520when%2520the%2520computation%2520must%2520be%2520carried%2520out%250Arepeatedly%2520%2528e.g.%2520in%2520model%2520training%2529.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520magnitude%250Acomputation%2520problem%252C%2520and%2520show%2520efficient%2520ways%2520of%2520approximating%2520it.%2520We%2520show%2520that%250Ait%2520can%2520be%2520cast%2520as%2520a%2520convex%2520optimization%2520problem%252C%2520but%2520not%2520as%2520a%2520submodular%250Aoptimization.%2520The%2520paper%2520describes%2520two%2520new%2520algorithms%2520-%2520an%2520iterative%250Aapproximation%2520algorithm%2520that%2520converges%2520fast%2520and%2520is%2520accurate%252C%2520and%2520a%2520subset%250Aselection%2520method%2520that%2520makes%2520the%2520computation%2520even%2520faster.%2520It%2520has%2520been%2520previously%250Aproposed%2520that%2520magnitude%2520of%2520model%2520sequences%2520generated%2520during%2520stochastic%2520gradient%250Adescent%2520is%2520correlated%2520to%2520generalization%2520gap.%2520Extension%2520of%2520this%2520result%2520using%2520our%250Amore%2520scalable%2520algorithms%2520shows%2520that%2520longer%2520sequences%2520in%2520fact%2520bear%2520higher%250Acorrelations.%2520We%2520also%2520describe%2520new%2520applications%2520of%2520magnitude%2520in%2520machine%250Alearning%2520-%2520as%2520an%2520effective%2520regularizer%2520for%2520neural%2520network%2520training%252C%2520and%2520as%2520a%250Anovel%2520clustering%2520criterion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximating%20Metric%20Magnitude%20of%20Point%20Sets&entry.906535625=Rayna%20Andreeva%20and%20James%20Ward%20and%20Primoz%20Skraba%20and%20Jie%20Gao%20and%20Rik%20Sarkar&entry.1292438233=%20%20Metric%20magnitude%20is%20a%20measure%20of%20the%20%22size%22%20of%20point%20clouds%20with%20many%0Adesirable%20geometric%20properties.%20It%20has%20been%20adapted%20to%20various%20mathematical%0Acontexts%20and%20recent%20work%20suggests%20that%20it%20can%20enhance%20machine%20learning%20and%0Aoptimization%20algorithms.%20But%20its%20usability%20is%20limited%20due%20to%20the%20computational%0Acost%20when%20the%20dataset%20is%20large%20or%20when%20the%20computation%20must%20be%20carried%20out%0Arepeatedly%20%28e.g.%20in%20model%20training%29.%20In%20this%20paper%2C%20we%20study%20the%20magnitude%0Acomputation%20problem%2C%20and%20show%20efficient%20ways%20of%20approximating%20it.%20We%20show%20that%0Ait%20can%20be%20cast%20as%20a%20convex%20optimization%20problem%2C%20but%20not%20as%20a%20submodular%0Aoptimization.%20The%20paper%20describes%20two%20new%20algorithms%20-%20an%20iterative%0Aapproximation%20algorithm%20that%20converges%20fast%20and%20is%20accurate%2C%20and%20a%20subset%0Aselection%20method%20that%20makes%20the%20computation%20even%20faster.%20It%20has%20been%20previously%0Aproposed%20that%20magnitude%20of%20model%20sequences%20generated%20during%20stochastic%20gradient%0Adescent%20is%20correlated%20to%20generalization%20gap.%20Extension%20of%20this%20result%20using%20our%0Amore%20scalable%20algorithms%20shows%20that%20longer%20sequences%20in%20fact%20bear%20higher%0Acorrelations.%20We%20also%20describe%20new%20applications%20of%20magnitude%20in%20machine%0Alearning%20-%20as%20an%20effective%20regularizer%20for%20neural%20network%20training%2C%20and%20as%20a%0Anovel%20clustering%20criterion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04411v1&entry.124074799=Read"},
{"title": "The Prevalence of Neural Collapse in Neural Multivariate Regression", "author": "George Andriopoulos and Zixuan Dong and Li Guo and Zifan Zhao and Keith Ross", "abstract": "  Recently it has been observed that neural networks exhibit Neural Collapse\n(NC) during the final stage of training for the classification problem. We\nempirically show that multivariate regression, as employed in imitation\nlearning and other applications, exhibits Neural Regression Collapse (NRC), a\nnew form of neural collapse: (NRC1) The last-layer feature vectors collapse to\nthe subspace spanned by the $n$ principal components of the feature vectors,\nwhere $n$ is the dimension of the targets (for univariate regression, $n=1$);\n(NRC2) The last-layer feature vectors also collapse to the subspace spanned by\nthe last-layer weight vectors; (NRC3) The Gram matrix for the weight vectors\nconverges to a specific functional form that depends on the covariance matrix\nof the targets. After empirically establishing the prevalence of (NRC1)-(NRC3)\nfor a variety of datasets and network architectures, we provide an explanation\nof these phenomena by modeling the regression task in the context of the\nUnconstrained Feature Model (UFM), in which the last layer feature vectors are\ntreated as free variables when minimizing the loss function. We show that when\nthe regularization parameters in the UFM model are strictly positive, then\n(NRC1)-(NRC3) also emerge as solutions in the UFM optimization problem. We also\nshow that if the regularization parameters are equal to zero, then there is no\ncollapse. To our knowledge, this is the first empirical and theoretical study\nof neural collapse in the context of regression. This extension is significant\nnot only because it broadens the applicability of neural collapse to a new\ncategory of problems but also because it suggests that the phenomena of neural\ncollapse could be a universal behavior in deep learning.\n", "link": "http://arxiv.org/abs/2409.04180v1", "date": "2024-09-06", "relevancy": 1.7942, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4577}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4435}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Prevalence%20of%20Neural%20Collapse%20in%20Neural%20Multivariate%20Regression&body=Title%3A%20The%20Prevalence%20of%20Neural%20Collapse%20in%20Neural%20Multivariate%20Regression%0AAuthor%3A%20George%20Andriopoulos%20and%20Zixuan%20Dong%20and%20Li%20Guo%20and%20Zifan%20Zhao%20and%20Keith%20Ross%0AAbstract%3A%20%20%20Recently%20it%20has%20been%20observed%20that%20neural%20networks%20exhibit%20Neural%20Collapse%0A%28NC%29%20during%20the%20final%20stage%20of%20training%20for%20the%20classification%20problem.%20We%0Aempirically%20show%20that%20multivariate%20regression%2C%20as%20employed%20in%20imitation%0Alearning%20and%20other%20applications%2C%20exhibits%20Neural%20Regression%20Collapse%20%28NRC%29%2C%20a%0Anew%20form%20of%20neural%20collapse%3A%20%28NRC1%29%20The%20last-layer%20feature%20vectors%20collapse%20to%0Athe%20subspace%20spanned%20by%20the%20%24n%24%20principal%20components%20of%20the%20feature%20vectors%2C%0Awhere%20%24n%24%20is%20the%20dimension%20of%20the%20targets%20%28for%20univariate%20regression%2C%20%24n%3D1%24%29%3B%0A%28NRC2%29%20The%20last-layer%20feature%20vectors%20also%20collapse%20to%20the%20subspace%20spanned%20by%0Athe%20last-layer%20weight%20vectors%3B%20%28NRC3%29%20The%20Gram%20matrix%20for%20the%20weight%20vectors%0Aconverges%20to%20a%20specific%20functional%20form%20that%20depends%20on%20the%20covariance%20matrix%0Aof%20the%20targets.%20After%20empirically%20establishing%20the%20prevalence%20of%20%28NRC1%29-%28NRC3%29%0Afor%20a%20variety%20of%20datasets%20and%20network%20architectures%2C%20we%20provide%20an%20explanation%0Aof%20these%20phenomena%20by%20modeling%20the%20regression%20task%20in%20the%20context%20of%20the%0AUnconstrained%20Feature%20Model%20%28UFM%29%2C%20in%20which%20the%20last%20layer%20feature%20vectors%20are%0Atreated%20as%20free%20variables%20when%20minimizing%20the%20loss%20function.%20We%20show%20that%20when%0Athe%20regularization%20parameters%20in%20the%20UFM%20model%20are%20strictly%20positive%2C%20then%0A%28NRC1%29-%28NRC3%29%20also%20emerge%20as%20solutions%20in%20the%20UFM%20optimization%20problem.%20We%20also%0Ashow%20that%20if%20the%20regularization%20parameters%20are%20equal%20to%20zero%2C%20then%20there%20is%20no%0Acollapse.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20empirical%20and%20theoretical%20study%0Aof%20neural%20collapse%20in%20the%20context%20of%20regression.%20This%20extension%20is%20significant%0Anot%20only%20because%20it%20broadens%20the%20applicability%20of%20neural%20collapse%20to%20a%20new%0Acategory%20of%20problems%20but%20also%20because%20it%20suggests%20that%20the%20phenomena%20of%20neural%0Acollapse%20could%20be%20a%20universal%20behavior%20in%20deep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Prevalence%2520of%2520Neural%2520Collapse%2520in%2520Neural%2520Multivariate%2520Regression%26entry.906535625%3DGeorge%2520Andriopoulos%2520and%2520Zixuan%2520Dong%2520and%2520Li%2520Guo%2520and%2520Zifan%2520Zhao%2520and%2520Keith%2520Ross%26entry.1292438233%3D%2520%2520Recently%2520it%2520has%2520been%2520observed%2520that%2520neural%2520networks%2520exhibit%2520Neural%2520Collapse%250A%2528NC%2529%2520during%2520the%2520final%2520stage%2520of%2520training%2520for%2520the%2520classification%2520problem.%2520We%250Aempirically%2520show%2520that%2520multivariate%2520regression%252C%2520as%2520employed%2520in%2520imitation%250Alearning%2520and%2520other%2520applications%252C%2520exhibits%2520Neural%2520Regression%2520Collapse%2520%2528NRC%2529%252C%2520a%250Anew%2520form%2520of%2520neural%2520collapse%253A%2520%2528NRC1%2529%2520The%2520last-layer%2520feature%2520vectors%2520collapse%2520to%250Athe%2520subspace%2520spanned%2520by%2520the%2520%2524n%2524%2520principal%2520components%2520of%2520the%2520feature%2520vectors%252C%250Awhere%2520%2524n%2524%2520is%2520the%2520dimension%2520of%2520the%2520targets%2520%2528for%2520univariate%2520regression%252C%2520%2524n%253D1%2524%2529%253B%250A%2528NRC2%2529%2520The%2520last-layer%2520feature%2520vectors%2520also%2520collapse%2520to%2520the%2520subspace%2520spanned%2520by%250Athe%2520last-layer%2520weight%2520vectors%253B%2520%2528NRC3%2529%2520The%2520Gram%2520matrix%2520for%2520the%2520weight%2520vectors%250Aconverges%2520to%2520a%2520specific%2520functional%2520form%2520that%2520depends%2520on%2520the%2520covariance%2520matrix%250Aof%2520the%2520targets.%2520After%2520empirically%2520establishing%2520the%2520prevalence%2520of%2520%2528NRC1%2529-%2528NRC3%2529%250Afor%2520a%2520variety%2520of%2520datasets%2520and%2520network%2520architectures%252C%2520we%2520provide%2520an%2520explanation%250Aof%2520these%2520phenomena%2520by%2520modeling%2520the%2520regression%2520task%2520in%2520the%2520context%2520of%2520the%250AUnconstrained%2520Feature%2520Model%2520%2528UFM%2529%252C%2520in%2520which%2520the%2520last%2520layer%2520feature%2520vectors%2520are%250Atreated%2520as%2520free%2520variables%2520when%2520minimizing%2520the%2520loss%2520function.%2520We%2520show%2520that%2520when%250Athe%2520regularization%2520parameters%2520in%2520the%2520UFM%2520model%2520are%2520strictly%2520positive%252C%2520then%250A%2528NRC1%2529-%2528NRC3%2529%2520also%2520emerge%2520as%2520solutions%2520in%2520the%2520UFM%2520optimization%2520problem.%2520We%2520also%250Ashow%2520that%2520if%2520the%2520regularization%2520parameters%2520are%2520equal%2520to%2520zero%252C%2520then%2520there%2520is%2520no%250Acollapse.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520empirical%2520and%2520theoretical%2520study%250Aof%2520neural%2520collapse%2520in%2520the%2520context%2520of%2520regression.%2520This%2520extension%2520is%2520significant%250Anot%2520only%2520because%2520it%2520broadens%2520the%2520applicability%2520of%2520neural%2520collapse%2520to%2520a%2520new%250Acategory%2520of%2520problems%2520but%2520also%2520because%2520it%2520suggests%2520that%2520the%2520phenomena%2520of%2520neural%250Acollapse%2520could%2520be%2520a%2520universal%2520behavior%2520in%2520deep%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Prevalence%20of%20Neural%20Collapse%20in%20Neural%20Multivariate%20Regression&entry.906535625=George%20Andriopoulos%20and%20Zixuan%20Dong%20and%20Li%20Guo%20and%20Zifan%20Zhao%20and%20Keith%20Ross&entry.1292438233=%20%20Recently%20it%20has%20been%20observed%20that%20neural%20networks%20exhibit%20Neural%20Collapse%0A%28NC%29%20during%20the%20final%20stage%20of%20training%20for%20the%20classification%20problem.%20We%0Aempirically%20show%20that%20multivariate%20regression%2C%20as%20employed%20in%20imitation%0Alearning%20and%20other%20applications%2C%20exhibits%20Neural%20Regression%20Collapse%20%28NRC%29%2C%20a%0Anew%20form%20of%20neural%20collapse%3A%20%28NRC1%29%20The%20last-layer%20feature%20vectors%20collapse%20to%0Athe%20subspace%20spanned%20by%20the%20%24n%24%20principal%20components%20of%20the%20feature%20vectors%2C%0Awhere%20%24n%24%20is%20the%20dimension%20of%20the%20targets%20%28for%20univariate%20regression%2C%20%24n%3D1%24%29%3B%0A%28NRC2%29%20The%20last-layer%20feature%20vectors%20also%20collapse%20to%20the%20subspace%20spanned%20by%0Athe%20last-layer%20weight%20vectors%3B%20%28NRC3%29%20The%20Gram%20matrix%20for%20the%20weight%20vectors%0Aconverges%20to%20a%20specific%20functional%20form%20that%20depends%20on%20the%20covariance%20matrix%0Aof%20the%20targets.%20After%20empirically%20establishing%20the%20prevalence%20of%20%28NRC1%29-%28NRC3%29%0Afor%20a%20variety%20of%20datasets%20and%20network%20architectures%2C%20we%20provide%20an%20explanation%0Aof%20these%20phenomena%20by%20modeling%20the%20regression%20task%20in%20the%20context%20of%20the%0AUnconstrained%20Feature%20Model%20%28UFM%29%2C%20in%20which%20the%20last%20layer%20feature%20vectors%20are%0Atreated%20as%20free%20variables%20when%20minimizing%20the%20loss%20function.%20We%20show%20that%20when%0Athe%20regularization%20parameters%20in%20the%20UFM%20model%20are%20strictly%20positive%2C%20then%0A%28NRC1%29-%28NRC3%29%20also%20emerge%20as%20solutions%20in%20the%20UFM%20optimization%20problem.%20We%20also%0Ashow%20that%20if%20the%20regularization%20parameters%20are%20equal%20to%20zero%2C%20then%20there%20is%20no%0Acollapse.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20empirical%20and%20theoretical%20study%0Aof%20neural%20collapse%20in%20the%20context%20of%20regression.%20This%20extension%20is%20significant%0Anot%20only%20because%20it%20broadens%20the%20applicability%20of%20neural%20collapse%20to%20a%20new%0Acategory%20of%20problems%20but%20also%20because%20it%20suggests%20that%20the%20phenomena%20of%20neural%0Acollapse%20could%20be%20a%20universal%20behavior%20in%20deep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04180v1&entry.124074799=Read"},
{"title": "Improved Parallel Algorithm for Non-Monotone Submodular Maximization\n  under Knapsack Constraint", "author": "Tan D. Tran and Canh V. Pham and Dung T. K. Ha and Phuong N. H. Pham", "abstract": "  This work proposes an efficient parallel algorithm for non-monotone\nsubmodular maximization under a knapsack constraint problem over the ground set\nof size $n$. Our algorithm improves the best approximation factor of the\nexisting parallel one from $8+\\epsilon$ to $7+\\epsilon$ with $O(\\log n)$\nadaptive complexity.\n  The key idea of our approach is to create a new alternate threshold\nalgorithmic framework. This strategy alternately constructs two disjoint\ncandidate solutions within a constant number of sequence rounds. Then, the\nalgorithm boosts solution quality without sacrificing the adaptive complexity.\nExtensive experimental studies on three applications, Revenue Maximization,\nImage Summarization, and Maximum Weighted Cut, show that our algorithm not only\nsignificantly increases solution quality but also requires comparative\nadaptivity to state-of-the-art algorithms.\n", "link": "http://arxiv.org/abs/2409.04415v1", "date": "2024-09-06", "relevancy": 1.7864, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4545}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4439}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Parallel%20Algorithm%20for%20Non-Monotone%20Submodular%20Maximization%0A%20%20under%20Knapsack%20Constraint&body=Title%3A%20Improved%20Parallel%20Algorithm%20for%20Non-Monotone%20Submodular%20Maximization%0A%20%20under%20Knapsack%20Constraint%0AAuthor%3A%20Tan%20D.%20Tran%20and%20Canh%20V.%20Pham%20and%20Dung%20T.%20K.%20Ha%20and%20Phuong%20N.%20H.%20Pham%0AAbstract%3A%20%20%20This%20work%20proposes%20an%20efficient%20parallel%20algorithm%20for%20non-monotone%0Asubmodular%20maximization%20under%20a%20knapsack%20constraint%20problem%20over%20the%20ground%20set%0Aof%20size%20%24n%24.%20Our%20algorithm%20improves%20the%20best%20approximation%20factor%20of%20the%0Aexisting%20parallel%20one%20from%20%248%2B%5Cepsilon%24%20to%20%247%2B%5Cepsilon%24%20with%20%24O%28%5Clog%20n%29%24%0Aadaptive%20complexity.%0A%20%20The%20key%20idea%20of%20our%20approach%20is%20to%20create%20a%20new%20alternate%20threshold%0Aalgorithmic%20framework.%20This%20strategy%20alternately%20constructs%20two%20disjoint%0Acandidate%20solutions%20within%20a%20constant%20number%20of%20sequence%20rounds.%20Then%2C%20the%0Aalgorithm%20boosts%20solution%20quality%20without%20sacrificing%20the%20adaptive%20complexity.%0AExtensive%20experimental%20studies%20on%20three%20applications%2C%20Revenue%20Maximization%2C%0AImage%20Summarization%2C%20and%20Maximum%20Weighted%20Cut%2C%20show%20that%20our%20algorithm%20not%20only%0Asignificantly%20increases%20solution%20quality%20but%20also%20requires%20comparative%0Aadaptivity%20to%20state-of-the-art%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Parallel%2520Algorithm%2520for%2520Non-Monotone%2520Submodular%2520Maximization%250A%2520%2520under%2520Knapsack%2520Constraint%26entry.906535625%3DTan%2520D.%2520Tran%2520and%2520Canh%2520V.%2520Pham%2520and%2520Dung%2520T.%2520K.%2520Ha%2520and%2520Phuong%2520N.%2520H.%2520Pham%26entry.1292438233%3D%2520%2520This%2520work%2520proposes%2520an%2520efficient%2520parallel%2520algorithm%2520for%2520non-monotone%250Asubmodular%2520maximization%2520under%2520a%2520knapsack%2520constraint%2520problem%2520over%2520the%2520ground%2520set%250Aof%2520size%2520%2524n%2524.%2520Our%2520algorithm%2520improves%2520the%2520best%2520approximation%2520factor%2520of%2520the%250Aexisting%2520parallel%2520one%2520from%2520%25248%252B%255Cepsilon%2524%2520to%2520%25247%252B%255Cepsilon%2524%2520with%2520%2524O%2528%255Clog%2520n%2529%2524%250Aadaptive%2520complexity.%250A%2520%2520The%2520key%2520idea%2520of%2520our%2520approach%2520is%2520to%2520create%2520a%2520new%2520alternate%2520threshold%250Aalgorithmic%2520framework.%2520This%2520strategy%2520alternately%2520constructs%2520two%2520disjoint%250Acandidate%2520solutions%2520within%2520a%2520constant%2520number%2520of%2520sequence%2520rounds.%2520Then%252C%2520the%250Aalgorithm%2520boosts%2520solution%2520quality%2520without%2520sacrificing%2520the%2520adaptive%2520complexity.%250AExtensive%2520experimental%2520studies%2520on%2520three%2520applications%252C%2520Revenue%2520Maximization%252C%250AImage%2520Summarization%252C%2520and%2520Maximum%2520Weighted%2520Cut%252C%2520show%2520that%2520our%2520algorithm%2520not%2520only%250Asignificantly%2520increases%2520solution%2520quality%2520but%2520also%2520requires%2520comparative%250Aadaptivity%2520to%2520state-of-the-art%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Parallel%20Algorithm%20for%20Non-Monotone%20Submodular%20Maximization%0A%20%20under%20Knapsack%20Constraint&entry.906535625=Tan%20D.%20Tran%20and%20Canh%20V.%20Pham%20and%20Dung%20T.%20K.%20Ha%20and%20Phuong%20N.%20H.%20Pham&entry.1292438233=%20%20This%20work%20proposes%20an%20efficient%20parallel%20algorithm%20for%20non-monotone%0Asubmodular%20maximization%20under%20a%20knapsack%20constraint%20problem%20over%20the%20ground%20set%0Aof%20size%20%24n%24.%20Our%20algorithm%20improves%20the%20best%20approximation%20factor%20of%20the%0Aexisting%20parallel%20one%20from%20%248%2B%5Cepsilon%24%20to%20%247%2B%5Cepsilon%24%20with%20%24O%28%5Clog%20n%29%24%0Aadaptive%20complexity.%0A%20%20The%20key%20idea%20of%20our%20approach%20is%20to%20create%20a%20new%20alternate%20threshold%0Aalgorithmic%20framework.%20This%20strategy%20alternately%20constructs%20two%20disjoint%0Acandidate%20solutions%20within%20a%20constant%20number%20of%20sequence%20rounds.%20Then%2C%20the%0Aalgorithm%20boosts%20solution%20quality%20without%20sacrificing%20the%20adaptive%20complexity.%0AExtensive%20experimental%20studies%20on%20three%20applications%2C%20Revenue%20Maximization%2C%0AImage%20Summarization%2C%20and%20Maximum%20Weighted%20Cut%2C%20show%20that%20our%20algorithm%20not%20only%0Asignificantly%20increases%20solution%20quality%20but%20also%20requires%20comparative%0Aadaptivity%20to%20state-of-the-art%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04415v1&entry.124074799=Read"},
{"title": "Provable Hyperparameter Tuning for Structured Pfaffian Settings", "author": "Maria-Florina Balcan and Anh Tuan Nguyen and Dravyansh Sharma", "abstract": "  Data-driven algorithm design automatically adapts algorithms to specific\napplication domains, achieving better performance. In the context of\nparameterized algorithms, this approach involves tuning the algorithm\nparameters using problem instances drawn from the problem distribution of the\ntarget application domain. While empirical evidence supports the effectiveness\nof data-driven algorithm design, providing theoretical guarantees for several\nparameterized families remains challenging. This is due to the intricate\nbehaviors of their corresponding utility functions, which typically admit\npiece-wise and discontinuity structures. In this work, we present refined\nframeworks for providing learning guarantees for parameterized data-driven\nalgorithm design problems in both distributional and online learning settings.\nFor the distributional learning setting, we introduce the Pfaffian GJ\nframework, an extension of the classical GJ framework, capable of providing\nlearning guarantees for function classes for which the computation involves\nPfaffian functions. Unlike the GJ framework, which is limited to function\nclasses with computation characterized by rational functions, our proposed\nframework can deal with function classes involving Pfaffian functions, which\nare much more general and widely applicable. We then show that for many\nparameterized algorithms of interest, their utility function possesses a\nrefined piece-wise structure, which automatically translates to learning\nguarantees using our proposed framework. For the online learning setting, we\nprovide a new tool for verifying dispersion property of a sequence of loss\nfunctions. This sufficient condition allows no-regret learning for sequences of\npiece-wise structured loss functions where the piece-wise structure involves\nPfaffian transition boundaries.\n", "link": "http://arxiv.org/abs/2409.04367v1", "date": "2024-09-06", "relevancy": 1.7812, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4593}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4519}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Hyperparameter%20Tuning%20for%20Structured%20Pfaffian%20Settings&body=Title%3A%20Provable%20Hyperparameter%20Tuning%20for%20Structured%20Pfaffian%20Settings%0AAuthor%3A%20Maria-Florina%20Balcan%20and%20Anh%20Tuan%20Nguyen%20and%20Dravyansh%20Sharma%0AAbstract%3A%20%20%20Data-driven%20algorithm%20design%20automatically%20adapts%20algorithms%20to%20specific%0Aapplication%20domains%2C%20achieving%20better%20performance.%20In%20the%20context%20of%0Aparameterized%20algorithms%2C%20this%20approach%20involves%20tuning%20the%20algorithm%0Aparameters%20using%20problem%20instances%20drawn%20from%20the%20problem%20distribution%20of%20the%0Atarget%20application%20domain.%20While%20empirical%20evidence%20supports%20the%20effectiveness%0Aof%20data-driven%20algorithm%20design%2C%20providing%20theoretical%20guarantees%20for%20several%0Aparameterized%20families%20remains%20challenging.%20This%20is%20due%20to%20the%20intricate%0Abehaviors%20of%20their%20corresponding%20utility%20functions%2C%20which%20typically%20admit%0Apiece-wise%20and%20discontinuity%20structures.%20In%20this%20work%2C%20we%20present%20refined%0Aframeworks%20for%20providing%20learning%20guarantees%20for%20parameterized%20data-driven%0Aalgorithm%20design%20problems%20in%20both%20distributional%20and%20online%20learning%20settings.%0AFor%20the%20distributional%20learning%20setting%2C%20we%20introduce%20the%20Pfaffian%20GJ%0Aframework%2C%20an%20extension%20of%20the%20classical%20GJ%20framework%2C%20capable%20of%20providing%0Alearning%20guarantees%20for%20function%20classes%20for%20which%20the%20computation%20involves%0APfaffian%20functions.%20Unlike%20the%20GJ%20framework%2C%20which%20is%20limited%20to%20function%0Aclasses%20with%20computation%20characterized%20by%20rational%20functions%2C%20our%20proposed%0Aframework%20can%20deal%20with%20function%20classes%20involving%20Pfaffian%20functions%2C%20which%0Aare%20much%20more%20general%20and%20widely%20applicable.%20We%20then%20show%20that%20for%20many%0Aparameterized%20algorithms%20of%20interest%2C%20their%20utility%20function%20possesses%20a%0Arefined%20piece-wise%20structure%2C%20which%20automatically%20translates%20to%20learning%0Aguarantees%20using%20our%20proposed%20framework.%20For%20the%20online%20learning%20setting%2C%20we%0Aprovide%20a%20new%20tool%20for%20verifying%20dispersion%20property%20of%20a%20sequence%20of%20loss%0Afunctions.%20This%20sufficient%20condition%20allows%20no-regret%20learning%20for%20sequences%20of%0Apiece-wise%20structured%20loss%20functions%20where%20the%20piece-wise%20structure%20involves%0APfaffian%20transition%20boundaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Hyperparameter%2520Tuning%2520for%2520Structured%2520Pfaffian%2520Settings%26entry.906535625%3DMaria-Florina%2520Balcan%2520and%2520Anh%2520Tuan%2520Nguyen%2520and%2520Dravyansh%2520Sharma%26entry.1292438233%3D%2520%2520Data-driven%2520algorithm%2520design%2520automatically%2520adapts%2520algorithms%2520to%2520specific%250Aapplication%2520domains%252C%2520achieving%2520better%2520performance.%2520In%2520the%2520context%2520of%250Aparameterized%2520algorithms%252C%2520this%2520approach%2520involves%2520tuning%2520the%2520algorithm%250Aparameters%2520using%2520problem%2520instances%2520drawn%2520from%2520the%2520problem%2520distribution%2520of%2520the%250Atarget%2520application%2520domain.%2520While%2520empirical%2520evidence%2520supports%2520the%2520effectiveness%250Aof%2520data-driven%2520algorithm%2520design%252C%2520providing%2520theoretical%2520guarantees%2520for%2520several%250Aparameterized%2520families%2520remains%2520challenging.%2520This%2520is%2520due%2520to%2520the%2520intricate%250Abehaviors%2520of%2520their%2520corresponding%2520utility%2520functions%252C%2520which%2520typically%2520admit%250Apiece-wise%2520and%2520discontinuity%2520structures.%2520In%2520this%2520work%252C%2520we%2520present%2520refined%250Aframeworks%2520for%2520providing%2520learning%2520guarantees%2520for%2520parameterized%2520data-driven%250Aalgorithm%2520design%2520problems%2520in%2520both%2520distributional%2520and%2520online%2520learning%2520settings.%250AFor%2520the%2520distributional%2520learning%2520setting%252C%2520we%2520introduce%2520the%2520Pfaffian%2520GJ%250Aframework%252C%2520an%2520extension%2520of%2520the%2520classical%2520GJ%2520framework%252C%2520capable%2520of%2520providing%250Alearning%2520guarantees%2520for%2520function%2520classes%2520for%2520which%2520the%2520computation%2520involves%250APfaffian%2520functions.%2520Unlike%2520the%2520GJ%2520framework%252C%2520which%2520is%2520limited%2520to%2520function%250Aclasses%2520with%2520computation%2520characterized%2520by%2520rational%2520functions%252C%2520our%2520proposed%250Aframework%2520can%2520deal%2520with%2520function%2520classes%2520involving%2520Pfaffian%2520functions%252C%2520which%250Aare%2520much%2520more%2520general%2520and%2520widely%2520applicable.%2520We%2520then%2520show%2520that%2520for%2520many%250Aparameterized%2520algorithms%2520of%2520interest%252C%2520their%2520utility%2520function%2520possesses%2520a%250Arefined%2520piece-wise%2520structure%252C%2520which%2520automatically%2520translates%2520to%2520learning%250Aguarantees%2520using%2520our%2520proposed%2520framework.%2520For%2520the%2520online%2520learning%2520setting%252C%2520we%250Aprovide%2520a%2520new%2520tool%2520for%2520verifying%2520dispersion%2520property%2520of%2520a%2520sequence%2520of%2520loss%250Afunctions.%2520This%2520sufficient%2520condition%2520allows%2520no-regret%2520learning%2520for%2520sequences%2520of%250Apiece-wise%2520structured%2520loss%2520functions%2520where%2520the%2520piece-wise%2520structure%2520involves%250APfaffian%2520transition%2520boundaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Hyperparameter%20Tuning%20for%20Structured%20Pfaffian%20Settings&entry.906535625=Maria-Florina%20Balcan%20and%20Anh%20Tuan%20Nguyen%20and%20Dravyansh%20Sharma&entry.1292438233=%20%20Data-driven%20algorithm%20design%20automatically%20adapts%20algorithms%20to%20specific%0Aapplication%20domains%2C%20achieving%20better%20performance.%20In%20the%20context%20of%0Aparameterized%20algorithms%2C%20this%20approach%20involves%20tuning%20the%20algorithm%0Aparameters%20using%20problem%20instances%20drawn%20from%20the%20problem%20distribution%20of%20the%0Atarget%20application%20domain.%20While%20empirical%20evidence%20supports%20the%20effectiveness%0Aof%20data-driven%20algorithm%20design%2C%20providing%20theoretical%20guarantees%20for%20several%0Aparameterized%20families%20remains%20challenging.%20This%20is%20due%20to%20the%20intricate%0Abehaviors%20of%20their%20corresponding%20utility%20functions%2C%20which%20typically%20admit%0Apiece-wise%20and%20discontinuity%20structures.%20In%20this%20work%2C%20we%20present%20refined%0Aframeworks%20for%20providing%20learning%20guarantees%20for%20parameterized%20data-driven%0Aalgorithm%20design%20problems%20in%20both%20distributional%20and%20online%20learning%20settings.%0AFor%20the%20distributional%20learning%20setting%2C%20we%20introduce%20the%20Pfaffian%20GJ%0Aframework%2C%20an%20extension%20of%20the%20classical%20GJ%20framework%2C%20capable%20of%20providing%0Alearning%20guarantees%20for%20function%20classes%20for%20which%20the%20computation%20involves%0APfaffian%20functions.%20Unlike%20the%20GJ%20framework%2C%20which%20is%20limited%20to%20function%0Aclasses%20with%20computation%20characterized%20by%20rational%20functions%2C%20our%20proposed%0Aframework%20can%20deal%20with%20function%20classes%20involving%20Pfaffian%20functions%2C%20which%0Aare%20much%20more%20general%20and%20widely%20applicable.%20We%20then%20show%20that%20for%20many%0Aparameterized%20algorithms%20of%20interest%2C%20their%20utility%20function%20possesses%20a%0Arefined%20piece-wise%20structure%2C%20which%20automatically%20translates%20to%20learning%0Aguarantees%20using%20our%20proposed%20framework.%20For%20the%20online%20learning%20setting%2C%20we%0Aprovide%20a%20new%20tool%20for%20verifying%20dispersion%20property%20of%20a%20sequence%20of%20loss%0Afunctions.%20This%20sufficient%20condition%20allows%20no-regret%20learning%20for%20sequences%20of%0Apiece-wise%20structured%20loss%20functions%20where%20the%20piece-wise%20structure%20involves%0APfaffian%20transition%20boundaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04367v1&entry.124074799=Read"},
{"title": "A Survey on Knowledge Organization Systems of Research Fields: Resources\n  and Challenges", "author": "Angelo Salatino and Tanay Aggarwal and Andrea Mannocci and Francesco Osborne and Enrico Motta", "abstract": "  Knowledge Organization Systems (KOSs), such as term lists, thesauri,\ntaxonomies, and ontologies, play a fundamental role in categorising, managing,\nand retrieving information. In the academic domain, KOSs are often adopted for\nrepresenting research areas and their relationships, primarily aiming to\nclassify research articles, academic courses, patents, books, scientific\nvenues, domain experts, grants, software, experiment materials, and several\nother relevant products and agents. These structured representations of\nresearch areas, widely embraced by many academic fields, have proven effective\nin empowering AI-based systems to i) enhance retrievability of relevant\ndocuments, ii) enable advanced analytic solutions to quantify the impact of\nacademic research, and iii) analyse and forecast research dynamics. This paper\naims to present a comprehensive survey of the current KOS for academic\ndisciplines. We analysed and compared 45 KOSs according to five main\ndimensions: scope, structure, curation, usage, and links to other KOSs. Our\nresults reveal a very heterogeneous scenario in terms of scope, scale, quality,\nand usage, highlighting the need for more integrated solutions for representing\nresearch knowledge across academic fields. We conclude by discussing the main\nchallenges and the most promising future directions.\n", "link": "http://arxiv.org/abs/2409.04432v1", "date": "2024-09-06", "relevancy": 1.7703, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Knowledge%20Organization%20Systems%20of%20Research%20Fields%3A%20Resources%0A%20%20and%20Challenges&body=Title%3A%20A%20Survey%20on%20Knowledge%20Organization%20Systems%20of%20Research%20Fields%3A%20Resources%0A%20%20and%20Challenges%0AAuthor%3A%20Angelo%20Salatino%20and%20Tanay%20Aggarwal%20and%20Andrea%20Mannocci%20and%20Francesco%20Osborne%20and%20Enrico%20Motta%0AAbstract%3A%20%20%20Knowledge%20Organization%20Systems%20%28KOSs%29%2C%20such%20as%20term%20lists%2C%20thesauri%2C%0Ataxonomies%2C%20and%20ontologies%2C%20play%20a%20fundamental%20role%20in%20categorising%2C%20managing%2C%0Aand%20retrieving%20information.%20In%20the%20academic%20domain%2C%20KOSs%20are%20often%20adopted%20for%0Arepresenting%20research%20areas%20and%20their%20relationships%2C%20primarily%20aiming%20to%0Aclassify%20research%20articles%2C%20academic%20courses%2C%20patents%2C%20books%2C%20scientific%0Avenues%2C%20domain%20experts%2C%20grants%2C%20software%2C%20experiment%20materials%2C%20and%20several%0Aother%20relevant%20products%20and%20agents.%20These%20structured%20representations%20of%0Aresearch%20areas%2C%20widely%20embraced%20by%20many%20academic%20fields%2C%20have%20proven%20effective%0Ain%20empowering%20AI-based%20systems%20to%20i%29%20enhance%20retrievability%20of%20relevant%0Adocuments%2C%20ii%29%20enable%20advanced%20analytic%20solutions%20to%20quantify%20the%20impact%20of%0Aacademic%20research%2C%20and%20iii%29%20analyse%20and%20forecast%20research%20dynamics.%20This%20paper%0Aaims%20to%20present%20a%20comprehensive%20survey%20of%20the%20current%20KOS%20for%20academic%0Adisciplines.%20We%20analysed%20and%20compared%2045%20KOSs%20according%20to%20five%20main%0Adimensions%3A%20scope%2C%20structure%2C%20curation%2C%20usage%2C%20and%20links%20to%20other%20KOSs.%20Our%0Aresults%20reveal%20a%20very%20heterogeneous%20scenario%20in%20terms%20of%20scope%2C%20scale%2C%20quality%2C%0Aand%20usage%2C%20highlighting%20the%20need%20for%20more%20integrated%20solutions%20for%20representing%0Aresearch%20knowledge%20across%20academic%20fields.%20We%20conclude%20by%20discussing%20the%20main%0Achallenges%20and%20the%20most%20promising%20future%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Knowledge%2520Organization%2520Systems%2520of%2520Research%2520Fields%253A%2520Resources%250A%2520%2520and%2520Challenges%26entry.906535625%3DAngelo%2520Salatino%2520and%2520Tanay%2520Aggarwal%2520and%2520Andrea%2520Mannocci%2520and%2520Francesco%2520Osborne%2520and%2520Enrico%2520Motta%26entry.1292438233%3D%2520%2520Knowledge%2520Organization%2520Systems%2520%2528KOSs%2529%252C%2520such%2520as%2520term%2520lists%252C%2520thesauri%252C%250Ataxonomies%252C%2520and%2520ontologies%252C%2520play%2520a%2520fundamental%2520role%2520in%2520categorising%252C%2520managing%252C%250Aand%2520retrieving%2520information.%2520In%2520the%2520academic%2520domain%252C%2520KOSs%2520are%2520often%2520adopted%2520for%250Arepresenting%2520research%2520areas%2520and%2520their%2520relationships%252C%2520primarily%2520aiming%2520to%250Aclassify%2520research%2520articles%252C%2520academic%2520courses%252C%2520patents%252C%2520books%252C%2520scientific%250Avenues%252C%2520domain%2520experts%252C%2520grants%252C%2520software%252C%2520experiment%2520materials%252C%2520and%2520several%250Aother%2520relevant%2520products%2520and%2520agents.%2520These%2520structured%2520representations%2520of%250Aresearch%2520areas%252C%2520widely%2520embraced%2520by%2520many%2520academic%2520fields%252C%2520have%2520proven%2520effective%250Ain%2520empowering%2520AI-based%2520systems%2520to%2520i%2529%2520enhance%2520retrievability%2520of%2520relevant%250Adocuments%252C%2520ii%2529%2520enable%2520advanced%2520analytic%2520solutions%2520to%2520quantify%2520the%2520impact%2520of%250Aacademic%2520research%252C%2520and%2520iii%2529%2520analyse%2520and%2520forecast%2520research%2520dynamics.%2520This%2520paper%250Aaims%2520to%2520present%2520a%2520comprehensive%2520survey%2520of%2520the%2520current%2520KOS%2520for%2520academic%250Adisciplines.%2520We%2520analysed%2520and%2520compared%252045%2520KOSs%2520according%2520to%2520five%2520main%250Adimensions%253A%2520scope%252C%2520structure%252C%2520curation%252C%2520usage%252C%2520and%2520links%2520to%2520other%2520KOSs.%2520Our%250Aresults%2520reveal%2520a%2520very%2520heterogeneous%2520scenario%2520in%2520terms%2520of%2520scope%252C%2520scale%252C%2520quality%252C%250Aand%2520usage%252C%2520highlighting%2520the%2520need%2520for%2520more%2520integrated%2520solutions%2520for%2520representing%250Aresearch%2520knowledge%2520across%2520academic%2520fields.%2520We%2520conclude%2520by%2520discussing%2520the%2520main%250Achallenges%2520and%2520the%2520most%2520promising%2520future%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Knowledge%20Organization%20Systems%20of%20Research%20Fields%3A%20Resources%0A%20%20and%20Challenges&entry.906535625=Angelo%20Salatino%20and%20Tanay%20Aggarwal%20and%20Andrea%20Mannocci%20and%20Francesco%20Osborne%20and%20Enrico%20Motta&entry.1292438233=%20%20Knowledge%20Organization%20Systems%20%28KOSs%29%2C%20such%20as%20term%20lists%2C%20thesauri%2C%0Ataxonomies%2C%20and%20ontologies%2C%20play%20a%20fundamental%20role%20in%20categorising%2C%20managing%2C%0Aand%20retrieving%20information.%20In%20the%20academic%20domain%2C%20KOSs%20are%20often%20adopted%20for%0Arepresenting%20research%20areas%20and%20their%20relationships%2C%20primarily%20aiming%20to%0Aclassify%20research%20articles%2C%20academic%20courses%2C%20patents%2C%20books%2C%20scientific%0Avenues%2C%20domain%20experts%2C%20grants%2C%20software%2C%20experiment%20materials%2C%20and%20several%0Aother%20relevant%20products%20and%20agents.%20These%20structured%20representations%20of%0Aresearch%20areas%2C%20widely%20embraced%20by%20many%20academic%20fields%2C%20have%20proven%20effective%0Ain%20empowering%20AI-based%20systems%20to%20i%29%20enhance%20retrievability%20of%20relevant%0Adocuments%2C%20ii%29%20enable%20advanced%20analytic%20solutions%20to%20quantify%20the%20impact%20of%0Aacademic%20research%2C%20and%20iii%29%20analyse%20and%20forecast%20research%20dynamics.%20This%20paper%0Aaims%20to%20present%20a%20comprehensive%20survey%20of%20the%20current%20KOS%20for%20academic%0Adisciplines.%20We%20analysed%20and%20compared%2045%20KOSs%20according%20to%20five%20main%0Adimensions%3A%20scope%2C%20structure%2C%20curation%2C%20usage%2C%20and%20links%20to%20other%20KOSs.%20Our%0Aresults%20reveal%20a%20very%20heterogeneous%20scenario%20in%20terms%20of%20scope%2C%20scale%2C%20quality%2C%0Aand%20usage%2C%20highlighting%20the%20need%20for%20more%20integrated%20solutions%20for%20representing%0Aresearch%20knowledge%20across%20academic%20fields.%20We%20conclude%20by%20discussing%20the%20main%0Achallenges%20and%20the%20most%20promising%20future%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04432v1&entry.124074799=Read"},
{"title": "Re-evaluating Retrosynthesis Algorithms with Syntheseus", "author": "Krzysztof Maziarz and Austin Tripp and Guoqing Liu and Megan Stanley and Shufang Xie and Piotr Gai\u0144ski and Philipp Seidl and Marwin Segler", "abstract": "  Automated Synthesis Planning has recently re-emerged as a research area at\nthe intersection of chemistry and machine learning. Despite the appearance of\nsteady progress, we argue that imperfect benchmarks and inconsistent\ncomparisons mask systematic shortcomings of existing techniques, and\nunnecessarily hamper progress. To remedy this, we present a synthesis planning\nlibrary with an extensive benchmarking framework, called syntheseus, which\npromotes best practice by default, enabling consistent meaningful evaluation of\nsingle-step models and multi-step planning algorithms. We demonstrate the\ncapabilities of syntheseus by re-evaluating several previous retrosynthesis\nalgorithms, and find that the ranking of state-of-the-art models changes in\ncontrolled evaluation experiments. We end with guidance for future works in\nthis area, and call the community to engage in the discussion on how to improve\nbenchmarks for synthesis planning.\n", "link": "http://arxiv.org/abs/2310.19796v3", "date": "2024-09-06", "relevancy": 1.74, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4309}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-evaluating%20Retrosynthesis%20Algorithms%20with%20Syntheseus&body=Title%3A%20Re-evaluating%20Retrosynthesis%20Algorithms%20with%20Syntheseus%0AAuthor%3A%20Krzysztof%20Maziarz%20and%20Austin%20Tripp%20and%20Guoqing%20Liu%20and%20Megan%20Stanley%20and%20Shufang%20Xie%20and%20Piotr%20Gai%C5%84ski%20and%20Philipp%20Seidl%20and%20Marwin%20Segler%0AAbstract%3A%20%20%20Automated%20Synthesis%20Planning%20has%20recently%20re-emerged%20as%20a%20research%20area%20at%0Athe%20intersection%20of%20chemistry%20and%20machine%20learning.%20Despite%20the%20appearance%20of%0Asteady%20progress%2C%20we%20argue%20that%20imperfect%20benchmarks%20and%20inconsistent%0Acomparisons%20mask%20systematic%20shortcomings%20of%20existing%20techniques%2C%20and%0Aunnecessarily%20hamper%20progress.%20To%20remedy%20this%2C%20we%20present%20a%20synthesis%20planning%0Alibrary%20with%20an%20extensive%20benchmarking%20framework%2C%20called%20syntheseus%2C%20which%0Apromotes%20best%20practice%20by%20default%2C%20enabling%20consistent%20meaningful%20evaluation%20of%0Asingle-step%20models%20and%20multi-step%20planning%20algorithms.%20We%20demonstrate%20the%0Acapabilities%20of%20syntheseus%20by%20re-evaluating%20several%20previous%20retrosynthesis%0Aalgorithms%2C%20and%20find%20that%20the%20ranking%20of%20state-of-the-art%20models%20changes%20in%0Acontrolled%20evaluation%20experiments.%20We%20end%20with%20guidance%20for%20future%20works%20in%0Athis%20area%2C%20and%20call%20the%20community%20to%20engage%20in%20the%20discussion%20on%20how%20to%20improve%0Abenchmarks%20for%20synthesis%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19796v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-evaluating%2520Retrosynthesis%2520Algorithms%2520with%2520Syntheseus%26entry.906535625%3DKrzysztof%2520Maziarz%2520and%2520Austin%2520Tripp%2520and%2520Guoqing%2520Liu%2520and%2520Megan%2520Stanley%2520and%2520Shufang%2520Xie%2520and%2520Piotr%2520Gai%25C5%2584ski%2520and%2520Philipp%2520Seidl%2520and%2520Marwin%2520Segler%26entry.1292438233%3D%2520%2520Automated%2520Synthesis%2520Planning%2520has%2520recently%2520re-emerged%2520as%2520a%2520research%2520area%2520at%250Athe%2520intersection%2520of%2520chemistry%2520and%2520machine%2520learning.%2520Despite%2520the%2520appearance%2520of%250Asteady%2520progress%252C%2520we%2520argue%2520that%2520imperfect%2520benchmarks%2520and%2520inconsistent%250Acomparisons%2520mask%2520systematic%2520shortcomings%2520of%2520existing%2520techniques%252C%2520and%250Aunnecessarily%2520hamper%2520progress.%2520To%2520remedy%2520this%252C%2520we%2520present%2520a%2520synthesis%2520planning%250Alibrary%2520with%2520an%2520extensive%2520benchmarking%2520framework%252C%2520called%2520syntheseus%252C%2520which%250Apromotes%2520best%2520practice%2520by%2520default%252C%2520enabling%2520consistent%2520meaningful%2520evaluation%2520of%250Asingle-step%2520models%2520and%2520multi-step%2520planning%2520algorithms.%2520We%2520demonstrate%2520the%250Acapabilities%2520of%2520syntheseus%2520by%2520re-evaluating%2520several%2520previous%2520retrosynthesis%250Aalgorithms%252C%2520and%2520find%2520that%2520the%2520ranking%2520of%2520state-of-the-art%2520models%2520changes%2520in%250Acontrolled%2520evaluation%2520experiments.%2520We%2520end%2520with%2520guidance%2520for%2520future%2520works%2520in%250Athis%2520area%252C%2520and%2520call%2520the%2520community%2520to%2520engage%2520in%2520the%2520discussion%2520on%2520how%2520to%2520improve%250Abenchmarks%2520for%2520synthesis%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19796v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-evaluating%20Retrosynthesis%20Algorithms%20with%20Syntheseus&entry.906535625=Krzysztof%20Maziarz%20and%20Austin%20Tripp%20and%20Guoqing%20Liu%20and%20Megan%20Stanley%20and%20Shufang%20Xie%20and%20Piotr%20Gai%C5%84ski%20and%20Philipp%20Seidl%20and%20Marwin%20Segler&entry.1292438233=%20%20Automated%20Synthesis%20Planning%20has%20recently%20re-emerged%20as%20a%20research%20area%20at%0Athe%20intersection%20of%20chemistry%20and%20machine%20learning.%20Despite%20the%20appearance%20of%0Asteady%20progress%2C%20we%20argue%20that%20imperfect%20benchmarks%20and%20inconsistent%0Acomparisons%20mask%20systematic%20shortcomings%20of%20existing%20techniques%2C%20and%0Aunnecessarily%20hamper%20progress.%20To%20remedy%20this%2C%20we%20present%20a%20synthesis%20planning%0Alibrary%20with%20an%20extensive%20benchmarking%20framework%2C%20called%20syntheseus%2C%20which%0Apromotes%20best%20practice%20by%20default%2C%20enabling%20consistent%20meaningful%20evaluation%20of%0Asingle-step%20models%20and%20multi-step%20planning%20algorithms.%20We%20demonstrate%20the%0Acapabilities%20of%20syntheseus%20by%20re-evaluating%20several%20previous%20retrosynthesis%0Aalgorithms%2C%20and%20find%20that%20the%20ranking%20of%20state-of-the-art%20models%20changes%20in%0Acontrolled%20evaluation%20experiments.%20We%20end%20with%20guidance%20for%20future%20works%20in%0Athis%20area%2C%20and%20call%20the%20community%20to%20engage%20in%20the%20discussion%20on%20how%20to%20improve%0Abenchmarks%20for%20synthesis%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19796v3&entry.124074799=Read"},
{"title": "Safe and Efficient Path Planning under Uncertainty via Deep Collision\n  Probability Fields", "author": "Felix Herrmann and Sebastian Zach and Jacopo Banfi and Jan Peters and Georgia Chalvatzaki and Davide Tateo", "abstract": "  Estimating collision probabilities between robots and environmental obstacles\nor other moving agents is crucial to ensure safety during path planning. This\nis an important building block of modern planning algorithms in many\napplication scenarios such as autonomous driving, where noisy sensors perceive\nobstacles. While many approaches exist, they either provide too conservative\nestimates of the collision probabilities or are computationally intensive due\nto their sampling-based nature. To deal with these issues, we introduce Deep\nCollision Probability Fields, a neural-based approach for computing collision\nprobabilities of arbitrary objects with arbitrary unimodal uncertainty\ndistributions. Our approach relegates the computationally intensive estimation\nof collision probabilities via sampling at the training step, allowing for fast\nneural network inference of the constraints during planning. In extensive\nexperiments, we show that Deep Collision Probability Fields can produce\nreasonably accurate collision probabilities (up to 10^{-3}) for planning and\nthat our approach can be easily plugged into standard path planning approaches\nto plan safe paths on 2-D maps containing uncertain static and dynamic\nobstacles. Additional material, code, and videos are available at\nhttps://sites.google.com/view/ral-dcpf.\n", "link": "http://arxiv.org/abs/2409.04306v1", "date": "2024-09-06", "relevancy": 1.7316, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5984}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5949}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20and%20Efficient%20Path%20Planning%20under%20Uncertainty%20via%20Deep%20Collision%0A%20%20Probability%20Fields&body=Title%3A%20Safe%20and%20Efficient%20Path%20Planning%20under%20Uncertainty%20via%20Deep%20Collision%0A%20%20Probability%20Fields%0AAuthor%3A%20Felix%20Herrmann%20and%20Sebastian%20Zach%20and%20Jacopo%20Banfi%20and%20Jan%20Peters%20and%20Georgia%20Chalvatzaki%20and%20Davide%20Tateo%0AAbstract%3A%20%20%20Estimating%20collision%20probabilities%20between%20robots%20and%20environmental%20obstacles%0Aor%20other%20moving%20agents%20is%20crucial%20to%20ensure%20safety%20during%20path%20planning.%20This%0Ais%20an%20important%20building%20block%20of%20modern%20planning%20algorithms%20in%20many%0Aapplication%20scenarios%20such%20as%20autonomous%20driving%2C%20where%20noisy%20sensors%20perceive%0Aobstacles.%20While%20many%20approaches%20exist%2C%20they%20either%20provide%20too%20conservative%0Aestimates%20of%20the%20collision%20probabilities%20or%20are%20computationally%20intensive%20due%0Ato%20their%20sampling-based%20nature.%20To%20deal%20with%20these%20issues%2C%20we%20introduce%20Deep%0ACollision%20Probability%20Fields%2C%20a%20neural-based%20approach%20for%20computing%20collision%0Aprobabilities%20of%20arbitrary%20objects%20with%20arbitrary%20unimodal%20uncertainty%0Adistributions.%20Our%20approach%20relegates%20the%20computationally%20intensive%20estimation%0Aof%20collision%20probabilities%20via%20sampling%20at%20the%20training%20step%2C%20allowing%20for%20fast%0Aneural%20network%20inference%20of%20the%20constraints%20during%20planning.%20In%20extensive%0Aexperiments%2C%20we%20show%20that%20Deep%20Collision%20Probability%20Fields%20can%20produce%0Areasonably%20accurate%20collision%20probabilities%20%28up%20to%2010%5E%7B-3%7D%29%20for%20planning%20and%0Athat%20our%20approach%20can%20be%20easily%20plugged%20into%20standard%20path%20planning%20approaches%0Ato%20plan%20safe%20paths%20on%202-D%20maps%20containing%20uncertain%20static%20and%20dynamic%0Aobstacles.%20Additional%20material%2C%20code%2C%20and%20videos%20are%20available%20at%0Ahttps%3A//sites.google.com/view/ral-dcpf.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520and%2520Efficient%2520Path%2520Planning%2520under%2520Uncertainty%2520via%2520Deep%2520Collision%250A%2520%2520Probability%2520Fields%26entry.906535625%3DFelix%2520Herrmann%2520and%2520Sebastian%2520Zach%2520and%2520Jacopo%2520Banfi%2520and%2520Jan%2520Peters%2520and%2520Georgia%2520Chalvatzaki%2520and%2520Davide%2520Tateo%26entry.1292438233%3D%2520%2520Estimating%2520collision%2520probabilities%2520between%2520robots%2520and%2520environmental%2520obstacles%250Aor%2520other%2520moving%2520agents%2520is%2520crucial%2520to%2520ensure%2520safety%2520during%2520path%2520planning.%2520This%250Ais%2520an%2520important%2520building%2520block%2520of%2520modern%2520planning%2520algorithms%2520in%2520many%250Aapplication%2520scenarios%2520such%2520as%2520autonomous%2520driving%252C%2520where%2520noisy%2520sensors%2520perceive%250Aobstacles.%2520While%2520many%2520approaches%2520exist%252C%2520they%2520either%2520provide%2520too%2520conservative%250Aestimates%2520of%2520the%2520collision%2520probabilities%2520or%2520are%2520computationally%2520intensive%2520due%250Ato%2520their%2520sampling-based%2520nature.%2520To%2520deal%2520with%2520these%2520issues%252C%2520we%2520introduce%2520Deep%250ACollision%2520Probability%2520Fields%252C%2520a%2520neural-based%2520approach%2520for%2520computing%2520collision%250Aprobabilities%2520of%2520arbitrary%2520objects%2520with%2520arbitrary%2520unimodal%2520uncertainty%250Adistributions.%2520Our%2520approach%2520relegates%2520the%2520computationally%2520intensive%2520estimation%250Aof%2520collision%2520probabilities%2520via%2520sampling%2520at%2520the%2520training%2520step%252C%2520allowing%2520for%2520fast%250Aneural%2520network%2520inference%2520of%2520the%2520constraints%2520during%2520planning.%2520In%2520extensive%250Aexperiments%252C%2520we%2520show%2520that%2520Deep%2520Collision%2520Probability%2520Fields%2520can%2520produce%250Areasonably%2520accurate%2520collision%2520probabilities%2520%2528up%2520to%252010%255E%257B-3%257D%2529%2520for%2520planning%2520and%250Athat%2520our%2520approach%2520can%2520be%2520easily%2520plugged%2520into%2520standard%2520path%2520planning%2520approaches%250Ato%2520plan%2520safe%2520paths%2520on%25202-D%2520maps%2520containing%2520uncertain%2520static%2520and%2520dynamic%250Aobstacles.%2520Additional%2520material%252C%2520code%252C%2520and%2520videos%2520are%2520available%2520at%250Ahttps%253A//sites.google.com/view/ral-dcpf.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20and%20Efficient%20Path%20Planning%20under%20Uncertainty%20via%20Deep%20Collision%0A%20%20Probability%20Fields&entry.906535625=Felix%20Herrmann%20and%20Sebastian%20Zach%20and%20Jacopo%20Banfi%20and%20Jan%20Peters%20and%20Georgia%20Chalvatzaki%20and%20Davide%20Tateo&entry.1292438233=%20%20Estimating%20collision%20probabilities%20between%20robots%20and%20environmental%20obstacles%0Aor%20other%20moving%20agents%20is%20crucial%20to%20ensure%20safety%20during%20path%20planning.%20This%0Ais%20an%20important%20building%20block%20of%20modern%20planning%20algorithms%20in%20many%0Aapplication%20scenarios%20such%20as%20autonomous%20driving%2C%20where%20noisy%20sensors%20perceive%0Aobstacles.%20While%20many%20approaches%20exist%2C%20they%20either%20provide%20too%20conservative%0Aestimates%20of%20the%20collision%20probabilities%20or%20are%20computationally%20intensive%20due%0Ato%20their%20sampling-based%20nature.%20To%20deal%20with%20these%20issues%2C%20we%20introduce%20Deep%0ACollision%20Probability%20Fields%2C%20a%20neural-based%20approach%20for%20computing%20collision%0Aprobabilities%20of%20arbitrary%20objects%20with%20arbitrary%20unimodal%20uncertainty%0Adistributions.%20Our%20approach%20relegates%20the%20computationally%20intensive%20estimation%0Aof%20collision%20probabilities%20via%20sampling%20at%20the%20training%20step%2C%20allowing%20for%20fast%0Aneural%20network%20inference%20of%20the%20constraints%20during%20planning.%20In%20extensive%0Aexperiments%2C%20we%20show%20that%20Deep%20Collision%20Probability%20Fields%20can%20produce%0Areasonably%20accurate%20collision%20probabilities%20%28up%20to%2010%5E%7B-3%7D%29%20for%20planning%20and%0Athat%20our%20approach%20can%20be%20easily%20plugged%20into%20standard%20path%20planning%20approaches%0Ato%20plan%20safe%20paths%20on%202-D%20maps%20containing%20uncertain%20static%20and%20dynamic%0Aobstacles.%20Additional%20material%2C%20code%2C%20and%20videos%20are%20available%20at%0Ahttps%3A//sites.google.com/view/ral-dcpf.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04306v1&entry.124074799=Read"},
{"title": "Empirical Bayesian image restoration by Langevin sampling with a\n  denoising diffusion implicit prior", "author": "Charlesquin Kemajou Mbakam and Jean-Francois Giovannelli and Marcelo Pereyra", "abstract": "  Score-based diffusion methods provide a powerful strategy to solve image\nrestoration tasks by flexibly combining a pre-trained foundational prior model\nwith a likelihood function specified during test time. Such methods are\npredominantly derived from two stochastic processes: reversing\nOrnstein-Uhlenbeck, which underpins the celebrated denoising diffusion\nprobabilistic models (DDPM) and denoising diffusion implicit models (DDIM), and\nthe Langevin diffusion process. The solutions delivered by DDPM and DDIM are\noften remarkably realistic, but they are not always consistent with\nmeasurements because of likelihood intractability issues and the associated\nrequired approximations. Alternatively, using a Langevin process circumvents\nthe intractable likelihood issue, but usually leads to restoration results of\ninferior quality and longer computing times. This paper presents a novel and\nhighly computationally efficient image restoration method that carefully embeds\na foundational DDPM denoiser within an empirical Bayesian Langevin algorithm,\nwhich jointly calibrates key model hyper-parameters as it estimates the model's\nposterior mean. Extensive experimental results on three canonical tasks (image\ndeblurring, super-resolution, and inpainting) demonstrate that the proposed\napproach improves on state-of-the-art strategies both in image estimation\naccuracy and computing time.\n", "link": "http://arxiv.org/abs/2409.04384v1", "date": "2024-09-06", "relevancy": 1.7301, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6107}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5769}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empirical%20Bayesian%20image%20restoration%20by%20Langevin%20sampling%20with%20a%0A%20%20denoising%20diffusion%20implicit%20prior&body=Title%3A%20Empirical%20Bayesian%20image%20restoration%20by%20Langevin%20sampling%20with%20a%0A%20%20denoising%20diffusion%20implicit%20prior%0AAuthor%3A%20Charlesquin%20Kemajou%20Mbakam%20and%20Jean-Francois%20Giovannelli%20and%20Marcelo%20Pereyra%0AAbstract%3A%20%20%20Score-based%20diffusion%20methods%20provide%20a%20powerful%20strategy%20to%20solve%20image%0Arestoration%20tasks%20by%20flexibly%20combining%20a%20pre-trained%20foundational%20prior%20model%0Awith%20a%20likelihood%20function%20specified%20during%20test%20time.%20Such%20methods%20are%0Apredominantly%20derived%20from%20two%20stochastic%20processes%3A%20reversing%0AOrnstein-Uhlenbeck%2C%20which%20underpins%20the%20celebrated%20denoising%20diffusion%0Aprobabilistic%20models%20%28DDPM%29%20and%20denoising%20diffusion%20implicit%20models%20%28DDIM%29%2C%20and%0Athe%20Langevin%20diffusion%20process.%20The%20solutions%20delivered%20by%20DDPM%20and%20DDIM%20are%0Aoften%20remarkably%20realistic%2C%20but%20they%20are%20not%20always%20consistent%20with%0Ameasurements%20because%20of%20likelihood%20intractability%20issues%20and%20the%20associated%0Arequired%20approximations.%20Alternatively%2C%20using%20a%20Langevin%20process%20circumvents%0Athe%20intractable%20likelihood%20issue%2C%20but%20usually%20leads%20to%20restoration%20results%20of%0Ainferior%20quality%20and%20longer%20computing%20times.%20This%20paper%20presents%20a%20novel%20and%0Ahighly%20computationally%20efficient%20image%20restoration%20method%20that%20carefully%20embeds%0Aa%20foundational%20DDPM%20denoiser%20within%20an%20empirical%20Bayesian%20Langevin%20algorithm%2C%0Awhich%20jointly%20calibrates%20key%20model%20hyper-parameters%20as%20it%20estimates%20the%20model%27s%0Aposterior%20mean.%20Extensive%20experimental%20results%20on%20three%20canonical%20tasks%20%28image%0Adeblurring%2C%20super-resolution%2C%20and%20inpainting%29%20demonstrate%20that%20the%20proposed%0Aapproach%20improves%20on%20state-of-the-art%20strategies%20both%20in%20image%20estimation%0Aaccuracy%20and%20computing%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpirical%2520Bayesian%2520image%2520restoration%2520by%2520Langevin%2520sampling%2520with%2520a%250A%2520%2520denoising%2520diffusion%2520implicit%2520prior%26entry.906535625%3DCharlesquin%2520Kemajou%2520Mbakam%2520and%2520Jean-Francois%2520Giovannelli%2520and%2520Marcelo%2520Pereyra%26entry.1292438233%3D%2520%2520Score-based%2520diffusion%2520methods%2520provide%2520a%2520powerful%2520strategy%2520to%2520solve%2520image%250Arestoration%2520tasks%2520by%2520flexibly%2520combining%2520a%2520pre-trained%2520foundational%2520prior%2520model%250Awith%2520a%2520likelihood%2520function%2520specified%2520during%2520test%2520time.%2520Such%2520methods%2520are%250Apredominantly%2520derived%2520from%2520two%2520stochastic%2520processes%253A%2520reversing%250AOrnstein-Uhlenbeck%252C%2520which%2520underpins%2520the%2520celebrated%2520denoising%2520diffusion%250Aprobabilistic%2520models%2520%2528DDPM%2529%2520and%2520denoising%2520diffusion%2520implicit%2520models%2520%2528DDIM%2529%252C%2520and%250Athe%2520Langevin%2520diffusion%2520process.%2520The%2520solutions%2520delivered%2520by%2520DDPM%2520and%2520DDIM%2520are%250Aoften%2520remarkably%2520realistic%252C%2520but%2520they%2520are%2520not%2520always%2520consistent%2520with%250Ameasurements%2520because%2520of%2520likelihood%2520intractability%2520issues%2520and%2520the%2520associated%250Arequired%2520approximations.%2520Alternatively%252C%2520using%2520a%2520Langevin%2520process%2520circumvents%250Athe%2520intractable%2520likelihood%2520issue%252C%2520but%2520usually%2520leads%2520to%2520restoration%2520results%2520of%250Ainferior%2520quality%2520and%2520longer%2520computing%2520times.%2520This%2520paper%2520presents%2520a%2520novel%2520and%250Ahighly%2520computationally%2520efficient%2520image%2520restoration%2520method%2520that%2520carefully%2520embeds%250Aa%2520foundational%2520DDPM%2520denoiser%2520within%2520an%2520empirical%2520Bayesian%2520Langevin%2520algorithm%252C%250Awhich%2520jointly%2520calibrates%2520key%2520model%2520hyper-parameters%2520as%2520it%2520estimates%2520the%2520model%2527s%250Aposterior%2520mean.%2520Extensive%2520experimental%2520results%2520on%2520three%2520canonical%2520tasks%2520%2528image%250Adeblurring%252C%2520super-resolution%252C%2520and%2520inpainting%2529%2520demonstrate%2520that%2520the%2520proposed%250Aapproach%2520improves%2520on%2520state-of-the-art%2520strategies%2520both%2520in%2520image%2520estimation%250Aaccuracy%2520and%2520computing%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empirical%20Bayesian%20image%20restoration%20by%20Langevin%20sampling%20with%20a%0A%20%20denoising%20diffusion%20implicit%20prior&entry.906535625=Charlesquin%20Kemajou%20Mbakam%20and%20Jean-Francois%20Giovannelli%20and%20Marcelo%20Pereyra&entry.1292438233=%20%20Score-based%20diffusion%20methods%20provide%20a%20powerful%20strategy%20to%20solve%20image%0Arestoration%20tasks%20by%20flexibly%20combining%20a%20pre-trained%20foundational%20prior%20model%0Awith%20a%20likelihood%20function%20specified%20during%20test%20time.%20Such%20methods%20are%0Apredominantly%20derived%20from%20two%20stochastic%20processes%3A%20reversing%0AOrnstein-Uhlenbeck%2C%20which%20underpins%20the%20celebrated%20denoising%20diffusion%0Aprobabilistic%20models%20%28DDPM%29%20and%20denoising%20diffusion%20implicit%20models%20%28DDIM%29%2C%20and%0Athe%20Langevin%20diffusion%20process.%20The%20solutions%20delivered%20by%20DDPM%20and%20DDIM%20are%0Aoften%20remarkably%20realistic%2C%20but%20they%20are%20not%20always%20consistent%20with%0Ameasurements%20because%20of%20likelihood%20intractability%20issues%20and%20the%20associated%0Arequired%20approximations.%20Alternatively%2C%20using%20a%20Langevin%20process%20circumvents%0Athe%20intractable%20likelihood%20issue%2C%20but%20usually%20leads%20to%20restoration%20results%20of%0Ainferior%20quality%20and%20longer%20computing%20times.%20This%20paper%20presents%20a%20novel%20and%0Ahighly%20computationally%20efficient%20image%20restoration%20method%20that%20carefully%20embeds%0Aa%20foundational%20DDPM%20denoiser%20within%20an%20empirical%20Bayesian%20Langevin%20algorithm%2C%0Awhich%20jointly%20calibrates%20key%20model%20hyper-parameters%20as%20it%20estimates%20the%20model%27s%0Aposterior%20mean.%20Extensive%20experimental%20results%20on%20three%20canonical%20tasks%20%28image%0Adeblurring%2C%20super-resolution%2C%20and%20inpainting%29%20demonstrate%20that%20the%20proposed%0Aapproach%20improves%20on%20state-of-the-art%20strategies%20both%20in%20image%20estimation%0Aaccuracy%20and%20computing%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04384v1&entry.124074799=Read"},
{"title": "Are LLM-based methods good enough for detecting unfair terms of service?", "author": "Mirgita Frasheri and Arian Bakhtiarnia and Lukas Esterle and Alexandros Iosifidis", "abstract": "  Countless terms of service (ToS) are being signed everyday by users all over\nthe world while interacting with all kinds of apps and websites. More often\nthan not, these online contracts spanning double-digit pages are signed blindly\nby users who simply want immediate access to the desired service. What would\nnormally require a consultation with a legal team, has now become a mundane\nactivity consisting of a few clicks where users potentially sign away their\nrights, for instance in terms of their data privacy, to countless online\nentities/companies. Large language models (LLMs) are good at parsing long\ntext-based documents, and could potentially be adopted to help users when\ndealing with dubious clauses in ToS and their underlying privacy policies. To\ninvestigate the utility of existing models for this task, we first build a\ndataset consisting of 12 questions applied individually to a set of privacy\npolicies crawled from popular websites. Thereafter, a series of open-source as\nwell as commercial chatbots such as ChatGPT, are queried over each question,\nwith the answers being compared to a given ground truth. Our results show that\nsome open-source models are able to provide a higher accuracy compared to some\ncommercial models. However, the best performance is recorded from a commercial\nchatbot (ChatGPT4). Overall, all models perform only slightly better than\nrandom at this task. Consequently, their performance needs to be significantly\nimproved before they can be adopted at large for this purpose.\n", "link": "http://arxiv.org/abs/2409.00077v2", "date": "2024-09-06", "relevancy": 1.6712, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20LLM-based%20methods%20good%20enough%20for%20detecting%20unfair%20terms%20of%20service%3F&body=Title%3A%20Are%20LLM-based%20methods%20good%20enough%20for%20detecting%20unfair%20terms%20of%20service%3F%0AAuthor%3A%20Mirgita%20Frasheri%20and%20Arian%20Bakhtiarnia%20and%20Lukas%20Esterle%20and%20Alexandros%20Iosifidis%0AAbstract%3A%20%20%20Countless%20terms%20of%20service%20%28ToS%29%20are%20being%20signed%20everyday%20by%20users%20all%20over%0Athe%20world%20while%20interacting%20with%20all%20kinds%20of%20apps%20and%20websites.%20More%20often%0Athan%20not%2C%20these%20online%20contracts%20spanning%20double-digit%20pages%20are%20signed%20blindly%0Aby%20users%20who%20simply%20want%20immediate%20access%20to%20the%20desired%20service.%20What%20would%0Anormally%20require%20a%20consultation%20with%20a%20legal%20team%2C%20has%20now%20become%20a%20mundane%0Aactivity%20consisting%20of%20a%20few%20clicks%20where%20users%20potentially%20sign%20away%20their%0Arights%2C%20for%20instance%20in%20terms%20of%20their%20data%20privacy%2C%20to%20countless%20online%0Aentities/companies.%20Large%20language%20models%20%28LLMs%29%20are%20good%20at%20parsing%20long%0Atext-based%20documents%2C%20and%20could%20potentially%20be%20adopted%20to%20help%20users%20when%0Adealing%20with%20dubious%20clauses%20in%20ToS%20and%20their%20underlying%20privacy%20policies.%20To%0Ainvestigate%20the%20utility%20of%20existing%20models%20for%20this%20task%2C%20we%20first%20build%20a%0Adataset%20consisting%20of%2012%20questions%20applied%20individually%20to%20a%20set%20of%20privacy%0Apolicies%20crawled%20from%20popular%20websites.%20Thereafter%2C%20a%20series%20of%20open-source%20as%0Awell%20as%20commercial%20chatbots%20such%20as%20ChatGPT%2C%20are%20queried%20over%20each%20question%2C%0Awith%20the%20answers%20being%20compared%20to%20a%20given%20ground%20truth.%20Our%20results%20show%20that%0Asome%20open-source%20models%20are%20able%20to%20provide%20a%20higher%20accuracy%20compared%20to%20some%0Acommercial%20models.%20However%2C%20the%20best%20performance%20is%20recorded%20from%20a%20commercial%0Achatbot%20%28ChatGPT4%29.%20Overall%2C%20all%20models%20perform%20only%20slightly%20better%20than%0Arandom%20at%20this%20task.%20Consequently%2C%20their%20performance%20needs%20to%20be%20significantly%0Aimproved%20before%20they%20can%20be%20adopted%20at%20large%20for%20this%20purpose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00077v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520LLM-based%2520methods%2520good%2520enough%2520for%2520detecting%2520unfair%2520terms%2520of%2520service%253F%26entry.906535625%3DMirgita%2520Frasheri%2520and%2520Arian%2520Bakhtiarnia%2520and%2520Lukas%2520Esterle%2520and%2520Alexandros%2520Iosifidis%26entry.1292438233%3D%2520%2520Countless%2520terms%2520of%2520service%2520%2528ToS%2529%2520are%2520being%2520signed%2520everyday%2520by%2520users%2520all%2520over%250Athe%2520world%2520while%2520interacting%2520with%2520all%2520kinds%2520of%2520apps%2520and%2520websites.%2520More%2520often%250Athan%2520not%252C%2520these%2520online%2520contracts%2520spanning%2520double-digit%2520pages%2520are%2520signed%2520blindly%250Aby%2520users%2520who%2520simply%2520want%2520immediate%2520access%2520to%2520the%2520desired%2520service.%2520What%2520would%250Anormally%2520require%2520a%2520consultation%2520with%2520a%2520legal%2520team%252C%2520has%2520now%2520become%2520a%2520mundane%250Aactivity%2520consisting%2520of%2520a%2520few%2520clicks%2520where%2520users%2520potentially%2520sign%2520away%2520their%250Arights%252C%2520for%2520instance%2520in%2520terms%2520of%2520their%2520data%2520privacy%252C%2520to%2520countless%2520online%250Aentities/companies.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520good%2520at%2520parsing%2520long%250Atext-based%2520documents%252C%2520and%2520could%2520potentially%2520be%2520adopted%2520to%2520help%2520users%2520when%250Adealing%2520with%2520dubious%2520clauses%2520in%2520ToS%2520and%2520their%2520underlying%2520privacy%2520policies.%2520To%250Ainvestigate%2520the%2520utility%2520of%2520existing%2520models%2520for%2520this%2520task%252C%2520we%2520first%2520build%2520a%250Adataset%2520consisting%2520of%252012%2520questions%2520applied%2520individually%2520to%2520a%2520set%2520of%2520privacy%250Apolicies%2520crawled%2520from%2520popular%2520websites.%2520Thereafter%252C%2520a%2520series%2520of%2520open-source%2520as%250Awell%2520as%2520commercial%2520chatbots%2520such%2520as%2520ChatGPT%252C%2520are%2520queried%2520over%2520each%2520question%252C%250Awith%2520the%2520answers%2520being%2520compared%2520to%2520a%2520given%2520ground%2520truth.%2520Our%2520results%2520show%2520that%250Asome%2520open-source%2520models%2520are%2520able%2520to%2520provide%2520a%2520higher%2520accuracy%2520compared%2520to%2520some%250Acommercial%2520models.%2520However%252C%2520the%2520best%2520performance%2520is%2520recorded%2520from%2520a%2520commercial%250Achatbot%2520%2528ChatGPT4%2529.%2520Overall%252C%2520all%2520models%2520perform%2520only%2520slightly%2520better%2520than%250Arandom%2520at%2520this%2520task.%2520Consequently%252C%2520their%2520performance%2520needs%2520to%2520be%2520significantly%250Aimproved%2520before%2520they%2520can%2520be%2520adopted%2520at%2520large%2520for%2520this%2520purpose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00077v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20LLM-based%20methods%20good%20enough%20for%20detecting%20unfair%20terms%20of%20service%3F&entry.906535625=Mirgita%20Frasheri%20and%20Arian%20Bakhtiarnia%20and%20Lukas%20Esterle%20and%20Alexandros%20Iosifidis&entry.1292438233=%20%20Countless%20terms%20of%20service%20%28ToS%29%20are%20being%20signed%20everyday%20by%20users%20all%20over%0Athe%20world%20while%20interacting%20with%20all%20kinds%20of%20apps%20and%20websites.%20More%20often%0Athan%20not%2C%20these%20online%20contracts%20spanning%20double-digit%20pages%20are%20signed%20blindly%0Aby%20users%20who%20simply%20want%20immediate%20access%20to%20the%20desired%20service.%20What%20would%0Anormally%20require%20a%20consultation%20with%20a%20legal%20team%2C%20has%20now%20become%20a%20mundane%0Aactivity%20consisting%20of%20a%20few%20clicks%20where%20users%20potentially%20sign%20away%20their%0Arights%2C%20for%20instance%20in%20terms%20of%20their%20data%20privacy%2C%20to%20countless%20online%0Aentities/companies.%20Large%20language%20models%20%28LLMs%29%20are%20good%20at%20parsing%20long%0Atext-based%20documents%2C%20and%20could%20potentially%20be%20adopted%20to%20help%20users%20when%0Adealing%20with%20dubious%20clauses%20in%20ToS%20and%20their%20underlying%20privacy%20policies.%20To%0Ainvestigate%20the%20utility%20of%20existing%20models%20for%20this%20task%2C%20we%20first%20build%20a%0Adataset%20consisting%20of%2012%20questions%20applied%20individually%20to%20a%20set%20of%20privacy%0Apolicies%20crawled%20from%20popular%20websites.%20Thereafter%2C%20a%20series%20of%20open-source%20as%0Awell%20as%20commercial%20chatbots%20such%20as%20ChatGPT%2C%20are%20queried%20over%20each%20question%2C%0Awith%20the%20answers%20being%20compared%20to%20a%20given%20ground%20truth.%20Our%20results%20show%20that%0Asome%20open-source%20models%20are%20able%20to%20provide%20a%20higher%20accuracy%20compared%20to%20some%0Acommercial%20models.%20However%2C%20the%20best%20performance%20is%20recorded%20from%20a%20commercial%0Achatbot%20%28ChatGPT4%29.%20Overall%2C%20all%20models%20perform%20only%20slightly%20better%20than%0Arandom%20at%20this%20task.%20Consequently%2C%20their%20performance%20needs%20to%20be%20significantly%0Aimproved%20before%20they%20can%20be%20adopted%20at%20large%20for%20this%20purpose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00077v2&entry.124074799=Read"},
{"title": "LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality\n  Assessment Model", "author": "Nasim Jamshidi Avanaki and Abhijay Ghildyal and Nabajeet Barman and Saman Zadtootaghaj", "abstract": "  Recent advancements in the field of No-Reference Image Quality Assessment\n(NR-IQA) using deep learning techniques demonstrate high performance across\nmultiple open-source datasets. However, such models are typically very large\nand complex making them not so suitable for real-world deployment, especially\non resource- and battery-constrained mobile devices. To address this\nlimitation, we propose a compact, lightweight NR-IQA model that achieves\nstate-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation\nand test datasets while being also nearly 5.7 times faster than the fastest\nSOTA model. Our model features a dual-branch architecture, with each branch\nseparately trained on synthetically and authentically distorted images which\nenhances the model's generalizability across different distortion types. To\nimprove robustness under diverse real-world visual conditions, we additionally\nincorporate multiple color spaces during the training process. We also\ndemonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks\n(KANs) for final quality regression as compared to the conventional Multi-Layer\nPerceptrons (MLPs). Our evaluation considering various open-source datasets\nhighlights the practical, high-accuracy, and robust performance of our proposed\nlightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA.\n", "link": "http://arxiv.org/abs/2408.17057v2", "date": "2024-09-06", "relevancy": 1.6661, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5633}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5539}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAR-IQA%3A%20A%20Lightweight%2C%20Accurate%2C%20and%20Robust%20No-Reference%20Image%20Quality%0A%20%20Assessment%20Model&body=Title%3A%20LAR-IQA%3A%20A%20Lightweight%2C%20Accurate%2C%20and%20Robust%20No-Reference%20Image%20Quality%0A%20%20Assessment%20Model%0AAuthor%3A%20Nasim%20Jamshidi%20Avanaki%20and%20Abhijay%20Ghildyal%20and%20Nabajeet%20Barman%20and%20Saman%20Zadtootaghaj%0AAbstract%3A%20%20%20Recent%20advancements%20in%20the%20field%20of%20No-Reference%20Image%20Quality%20Assessment%0A%28NR-IQA%29%20using%20deep%20learning%20techniques%20demonstrate%20high%20performance%20across%0Amultiple%20open-source%20datasets.%20However%2C%20such%20models%20are%20typically%20very%20large%0Aand%20complex%20making%20them%20not%20so%20suitable%20for%20real-world%20deployment%2C%20especially%0Aon%20resource-%20and%20battery-constrained%20mobile%20devices.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20compact%2C%20lightweight%20NR-IQA%20model%20that%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20on%20ECCV%20AIM%20UHD-IQA%20challenge%20validation%0Aand%20test%20datasets%20while%20being%20also%20nearly%205.7%20times%20faster%20than%20the%20fastest%0ASOTA%20model.%20Our%20model%20features%20a%20dual-branch%20architecture%2C%20with%20each%20branch%0Aseparately%20trained%20on%20synthetically%20and%20authentically%20distorted%20images%20which%0Aenhances%20the%20model%27s%20generalizability%20across%20different%20distortion%20types.%20To%0Aimprove%20robustness%20under%20diverse%20real-world%20visual%20conditions%2C%20we%20additionally%0Aincorporate%20multiple%20color%20spaces%20during%20the%20training%20process.%20We%20also%0Ademonstrate%20the%20higher%20accuracy%20of%20recently%20proposed%20Kolmogorov-Arnold%20Networks%0A%28KANs%29%20for%20final%20quality%20regression%20as%20compared%20to%20the%20conventional%20Multi-Layer%0APerceptrons%20%28MLPs%29.%20Our%20evaluation%20considering%20various%20open-source%20datasets%0Ahighlights%20the%20practical%2C%20high-accuracy%2C%20and%20robust%20performance%20of%20our%20proposed%0Alightweight%20model.%20Code%3A%20https%3A//github.com/nasimjamshidi/LAR-IQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17057v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAR-IQA%253A%2520A%2520Lightweight%252C%2520Accurate%252C%2520and%2520Robust%2520No-Reference%2520Image%2520Quality%250A%2520%2520Assessment%2520Model%26entry.906535625%3DNasim%2520Jamshidi%2520Avanaki%2520and%2520Abhijay%2520Ghildyal%2520and%2520Nabajeet%2520Barman%2520and%2520Saman%2520Zadtootaghaj%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520the%2520field%2520of%2520No-Reference%2520Image%2520Quality%2520Assessment%250A%2528NR-IQA%2529%2520using%2520deep%2520learning%2520techniques%2520demonstrate%2520high%2520performance%2520across%250Amultiple%2520open-source%2520datasets.%2520However%252C%2520such%2520models%2520are%2520typically%2520very%2520large%250Aand%2520complex%2520making%2520them%2520not%2520so%2520suitable%2520for%2520real-world%2520deployment%252C%2520especially%250Aon%2520resource-%2520and%2520battery-constrained%2520mobile%2520devices.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520compact%252C%2520lightweight%2520NR-IQA%2520model%2520that%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520on%2520ECCV%2520AIM%2520UHD-IQA%2520challenge%2520validation%250Aand%2520test%2520datasets%2520while%2520being%2520also%2520nearly%25205.7%2520times%2520faster%2520than%2520the%2520fastest%250ASOTA%2520model.%2520Our%2520model%2520features%2520a%2520dual-branch%2520architecture%252C%2520with%2520each%2520branch%250Aseparately%2520trained%2520on%2520synthetically%2520and%2520authentically%2520distorted%2520images%2520which%250Aenhances%2520the%2520model%2527s%2520generalizability%2520across%2520different%2520distortion%2520types.%2520To%250Aimprove%2520robustness%2520under%2520diverse%2520real-world%2520visual%2520conditions%252C%2520we%2520additionally%250Aincorporate%2520multiple%2520color%2520spaces%2520during%2520the%2520training%2520process.%2520We%2520also%250Ademonstrate%2520the%2520higher%2520accuracy%2520of%2520recently%2520proposed%2520Kolmogorov-Arnold%2520Networks%250A%2528KANs%2529%2520for%2520final%2520quality%2520regression%2520as%2520compared%2520to%2520the%2520conventional%2520Multi-Layer%250APerceptrons%2520%2528MLPs%2529.%2520Our%2520evaluation%2520considering%2520various%2520open-source%2520datasets%250Ahighlights%2520the%2520practical%252C%2520high-accuracy%252C%2520and%2520robust%2520performance%2520of%2520our%2520proposed%250Alightweight%2520model.%2520Code%253A%2520https%253A//github.com/nasimjamshidi/LAR-IQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17057v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAR-IQA%3A%20A%20Lightweight%2C%20Accurate%2C%20and%20Robust%20No-Reference%20Image%20Quality%0A%20%20Assessment%20Model&entry.906535625=Nasim%20Jamshidi%20Avanaki%20and%20Abhijay%20Ghildyal%20and%20Nabajeet%20Barman%20and%20Saman%20Zadtootaghaj&entry.1292438233=%20%20Recent%20advancements%20in%20the%20field%20of%20No-Reference%20Image%20Quality%20Assessment%0A%28NR-IQA%29%20using%20deep%20learning%20techniques%20demonstrate%20high%20performance%20across%0Amultiple%20open-source%20datasets.%20However%2C%20such%20models%20are%20typically%20very%20large%0Aand%20complex%20making%20them%20not%20so%20suitable%20for%20real-world%20deployment%2C%20especially%0Aon%20resource-%20and%20battery-constrained%20mobile%20devices.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20compact%2C%20lightweight%20NR-IQA%20model%20that%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20on%20ECCV%20AIM%20UHD-IQA%20challenge%20validation%0Aand%20test%20datasets%20while%20being%20also%20nearly%205.7%20times%20faster%20than%20the%20fastest%0ASOTA%20model.%20Our%20model%20features%20a%20dual-branch%20architecture%2C%20with%20each%20branch%0Aseparately%20trained%20on%20synthetically%20and%20authentically%20distorted%20images%20which%0Aenhances%20the%20model%27s%20generalizability%20across%20different%20distortion%20types.%20To%0Aimprove%20robustness%20under%20diverse%20real-world%20visual%20conditions%2C%20we%20additionally%0Aincorporate%20multiple%20color%20spaces%20during%20the%20training%20process.%20We%20also%0Ademonstrate%20the%20higher%20accuracy%20of%20recently%20proposed%20Kolmogorov-Arnold%20Networks%0A%28KANs%29%20for%20final%20quality%20regression%20as%20compared%20to%20the%20conventional%20Multi-Layer%0APerceptrons%20%28MLPs%29.%20Our%20evaluation%20considering%20various%20open-source%20datasets%0Ahighlights%20the%20practical%2C%20high-accuracy%2C%20and%20robust%20performance%20of%20our%20proposed%0Alightweight%20model.%20Code%3A%20https%3A//github.com/nasimjamshidi/LAR-IQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17057v2&entry.124074799=Read"},
{"title": "Towards Fine-Grained Webpage Fingerprinting at Scale", "author": "Xiyuan Zhao and Xinhao Deng and Qi Li and Yunpeng Liu and Zhuotao Liu and Kun Sun and Ke Xu", "abstract": "  Website Fingerprinting (WF) attacks can effectively identify the websites\nvisited by Tor clients via analyzing encrypted traffic patterns. Existing\nattacks focus on identifying different websites, but their accuracy\ndramatically decreases when applied to identify fine-grained webpages,\nespecially when distinguishing among different subpages of the same website.\nWebPage Fingerprinting (WPF) attacks face the challenges of highly similar\ntraffic patterns and a much larger scale of webpages. Furthermore, clients\noften visit multiple webpages concurrently, increasing the difficulty of\nextracting the traffic patterns of each webpage from the obfuscated traffic. In\nthis paper, we propose Oscar, a WPF attack based on multi-label metric learning\nthat identifies different webpages from obfuscated traffic by transforming the\nfeature space. Oscar can extract the subtle differences among various webpages,\neven those with similar traffic patterns. In particular, Oscar combines\nproxy-based and sample-based metric learning losses to extract webpage features\nfrom obfuscated traffic and identify multiple webpages. We prototype Oscar and\nevaluate its performance using traffic collected from 1,000 monitored webpages\nand over 9,000 unmonitored webpages in the real world. Oscar demonstrates an\n88.6% improvement in the multi-label metric Recall@5 compared to the\nstate-of-the-art attacks.\n", "link": "http://arxiv.org/abs/2409.04341v1", "date": "2024-09-06", "relevancy": 1.6524, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4237}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4104}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Fine-Grained%20Webpage%20Fingerprinting%20at%20Scale&body=Title%3A%20Towards%20Fine-Grained%20Webpage%20Fingerprinting%20at%20Scale%0AAuthor%3A%20Xiyuan%20Zhao%20and%20Xinhao%20Deng%20and%20Qi%20Li%20and%20Yunpeng%20Liu%20and%20Zhuotao%20Liu%20and%20Kun%20Sun%20and%20Ke%20Xu%0AAbstract%3A%20%20%20Website%20Fingerprinting%20%28WF%29%20attacks%20can%20effectively%20identify%20the%20websites%0Avisited%20by%20Tor%20clients%20via%20analyzing%20encrypted%20traffic%20patterns.%20Existing%0Aattacks%20focus%20on%20identifying%20different%20websites%2C%20but%20their%20accuracy%0Adramatically%20decreases%20when%20applied%20to%20identify%20fine-grained%20webpages%2C%0Aespecially%20when%20distinguishing%20among%20different%20subpages%20of%20the%20same%20website.%0AWebPage%20Fingerprinting%20%28WPF%29%20attacks%20face%20the%20challenges%20of%20highly%20similar%0Atraffic%20patterns%20and%20a%20much%20larger%20scale%20of%20webpages.%20Furthermore%2C%20clients%0Aoften%20visit%20multiple%20webpages%20concurrently%2C%20increasing%20the%20difficulty%20of%0Aextracting%20the%20traffic%20patterns%20of%20each%20webpage%20from%20the%20obfuscated%20traffic.%20In%0Athis%20paper%2C%20we%20propose%20Oscar%2C%20a%20WPF%20attack%20based%20on%20multi-label%20metric%20learning%0Athat%20identifies%20different%20webpages%20from%20obfuscated%20traffic%20by%20transforming%20the%0Afeature%20space.%20Oscar%20can%20extract%20the%20subtle%20differences%20among%20various%20webpages%2C%0Aeven%20those%20with%20similar%20traffic%20patterns.%20In%20particular%2C%20Oscar%20combines%0Aproxy-based%20and%20sample-based%20metric%20learning%20losses%20to%20extract%20webpage%20features%0Afrom%20obfuscated%20traffic%20and%20identify%20multiple%20webpages.%20We%20prototype%20Oscar%20and%0Aevaluate%20its%20performance%20using%20traffic%20collected%20from%201%2C000%20monitored%20webpages%0Aand%20over%209%2C000%20unmonitored%20webpages%20in%20the%20real%20world.%20Oscar%20demonstrates%20an%0A88.6%25%20improvement%20in%20the%20multi-label%20metric%20Recall%405%20compared%20to%20the%0Astate-of-the-art%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04341v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Fine-Grained%2520Webpage%2520Fingerprinting%2520at%2520Scale%26entry.906535625%3DXiyuan%2520Zhao%2520and%2520Xinhao%2520Deng%2520and%2520Qi%2520Li%2520and%2520Yunpeng%2520Liu%2520and%2520Zhuotao%2520Liu%2520and%2520Kun%2520Sun%2520and%2520Ke%2520Xu%26entry.1292438233%3D%2520%2520Website%2520Fingerprinting%2520%2528WF%2529%2520attacks%2520can%2520effectively%2520identify%2520the%2520websites%250Avisited%2520by%2520Tor%2520clients%2520via%2520analyzing%2520encrypted%2520traffic%2520patterns.%2520Existing%250Aattacks%2520focus%2520on%2520identifying%2520different%2520websites%252C%2520but%2520their%2520accuracy%250Adramatically%2520decreases%2520when%2520applied%2520to%2520identify%2520fine-grained%2520webpages%252C%250Aespecially%2520when%2520distinguishing%2520among%2520different%2520subpages%2520of%2520the%2520same%2520website.%250AWebPage%2520Fingerprinting%2520%2528WPF%2529%2520attacks%2520face%2520the%2520challenges%2520of%2520highly%2520similar%250Atraffic%2520patterns%2520and%2520a%2520much%2520larger%2520scale%2520of%2520webpages.%2520Furthermore%252C%2520clients%250Aoften%2520visit%2520multiple%2520webpages%2520concurrently%252C%2520increasing%2520the%2520difficulty%2520of%250Aextracting%2520the%2520traffic%2520patterns%2520of%2520each%2520webpage%2520from%2520the%2520obfuscated%2520traffic.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520Oscar%252C%2520a%2520WPF%2520attack%2520based%2520on%2520multi-label%2520metric%2520learning%250Athat%2520identifies%2520different%2520webpages%2520from%2520obfuscated%2520traffic%2520by%2520transforming%2520the%250Afeature%2520space.%2520Oscar%2520can%2520extract%2520the%2520subtle%2520differences%2520among%2520various%2520webpages%252C%250Aeven%2520those%2520with%2520similar%2520traffic%2520patterns.%2520In%2520particular%252C%2520Oscar%2520combines%250Aproxy-based%2520and%2520sample-based%2520metric%2520learning%2520losses%2520to%2520extract%2520webpage%2520features%250Afrom%2520obfuscated%2520traffic%2520and%2520identify%2520multiple%2520webpages.%2520We%2520prototype%2520Oscar%2520and%250Aevaluate%2520its%2520performance%2520using%2520traffic%2520collected%2520from%25201%252C000%2520monitored%2520webpages%250Aand%2520over%25209%252C000%2520unmonitored%2520webpages%2520in%2520the%2520real%2520world.%2520Oscar%2520demonstrates%2520an%250A88.6%2525%2520improvement%2520in%2520the%2520multi-label%2520metric%2520Recall%25405%2520compared%2520to%2520the%250Astate-of-the-art%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04341v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Fine-Grained%20Webpage%20Fingerprinting%20at%20Scale&entry.906535625=Xiyuan%20Zhao%20and%20Xinhao%20Deng%20and%20Qi%20Li%20and%20Yunpeng%20Liu%20and%20Zhuotao%20Liu%20and%20Kun%20Sun%20and%20Ke%20Xu&entry.1292438233=%20%20Website%20Fingerprinting%20%28WF%29%20attacks%20can%20effectively%20identify%20the%20websites%0Avisited%20by%20Tor%20clients%20via%20analyzing%20encrypted%20traffic%20patterns.%20Existing%0Aattacks%20focus%20on%20identifying%20different%20websites%2C%20but%20their%20accuracy%0Adramatically%20decreases%20when%20applied%20to%20identify%20fine-grained%20webpages%2C%0Aespecially%20when%20distinguishing%20among%20different%20subpages%20of%20the%20same%20website.%0AWebPage%20Fingerprinting%20%28WPF%29%20attacks%20face%20the%20challenges%20of%20highly%20similar%0Atraffic%20patterns%20and%20a%20much%20larger%20scale%20of%20webpages.%20Furthermore%2C%20clients%0Aoften%20visit%20multiple%20webpages%20concurrently%2C%20increasing%20the%20difficulty%20of%0Aextracting%20the%20traffic%20patterns%20of%20each%20webpage%20from%20the%20obfuscated%20traffic.%20In%0Athis%20paper%2C%20we%20propose%20Oscar%2C%20a%20WPF%20attack%20based%20on%20multi-label%20metric%20learning%0Athat%20identifies%20different%20webpages%20from%20obfuscated%20traffic%20by%20transforming%20the%0Afeature%20space.%20Oscar%20can%20extract%20the%20subtle%20differences%20among%20various%20webpages%2C%0Aeven%20those%20with%20similar%20traffic%20patterns.%20In%20particular%2C%20Oscar%20combines%0Aproxy-based%20and%20sample-based%20metric%20learning%20losses%20to%20extract%20webpage%20features%0Afrom%20obfuscated%20traffic%20and%20identify%20multiple%20webpages.%20We%20prototype%20Oscar%20and%0Aevaluate%20its%20performance%20using%20traffic%20collected%20from%201%2C000%20monitored%20webpages%0Aand%20over%209%2C000%20unmonitored%20webpages%20in%20the%20real%20world.%20Oscar%20demonstrates%20an%0A88.6%25%20improvement%20in%20the%20multi-label%20metric%20Recall%405%20compared%20to%20the%0Astate-of-the-art%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04341v1&entry.124074799=Read"},
{"title": "Leveraging Machine Learning for Official Statistics: A Statistical\n  Manifesto", "author": "Marco Puts and David Salgado and Piet Daas", "abstract": "  It is important for official statistics production to apply ML with\nstatistical rigor, as it presents both opportunities and challenges. Although\nmachine learning has enjoyed rapid technological advances in recent years, its\napplication does not possess the methodological robustness necessary to produce\nhigh quality statistical results. In order to account for all sources of error\nin machine learning models, the Total Machine Learning Error (TMLE) is\npresented as a framework analogous to the Total Survey Error Model used in\nsurvey methodology. As a means of ensuring that ML models are both internally\nvalid as well as externally valid, the TMLE model addresses issues such as\nrepresentativeness and measurement errors. There are several case studies\npresented, illustrating the importance of applying more rigor to the\napplication of machine learning in official statistics.\n", "link": "http://arxiv.org/abs/2409.04365v1", "date": "2024-09-06", "relevancy": 1.6499, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4562}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4044}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Machine%20Learning%20for%20Official%20Statistics%3A%20A%20Statistical%0A%20%20Manifesto&body=Title%3A%20Leveraging%20Machine%20Learning%20for%20Official%20Statistics%3A%20A%20Statistical%0A%20%20Manifesto%0AAuthor%3A%20Marco%20Puts%20and%20David%20Salgado%20and%20Piet%20Daas%0AAbstract%3A%20%20%20It%20is%20important%20for%20official%20statistics%20production%20to%20apply%20ML%20with%0Astatistical%20rigor%2C%20as%20it%20presents%20both%20opportunities%20and%20challenges.%20Although%0Amachine%20learning%20has%20enjoyed%20rapid%20technological%20advances%20in%20recent%20years%2C%20its%0Aapplication%20does%20not%20possess%20the%20methodological%20robustness%20necessary%20to%20produce%0Ahigh%20quality%20statistical%20results.%20In%20order%20to%20account%20for%20all%20sources%20of%20error%0Ain%20machine%20learning%20models%2C%20the%20Total%20Machine%20Learning%20Error%20%28TMLE%29%20is%0Apresented%20as%20a%20framework%20analogous%20to%20the%20Total%20Survey%20Error%20Model%20used%20in%0Asurvey%20methodology.%20As%20a%20means%20of%20ensuring%20that%20ML%20models%20are%20both%20internally%0Avalid%20as%20well%20as%20externally%20valid%2C%20the%20TMLE%20model%20addresses%20issues%20such%20as%0Arepresentativeness%20and%20measurement%20errors.%20There%20are%20several%20case%20studies%0Apresented%2C%20illustrating%20the%20importance%20of%20applying%20more%20rigor%20to%20the%0Aapplication%20of%20machine%20learning%20in%20official%20statistics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Machine%2520Learning%2520for%2520Official%2520Statistics%253A%2520A%2520Statistical%250A%2520%2520Manifesto%26entry.906535625%3DMarco%2520Puts%2520and%2520David%2520Salgado%2520and%2520Piet%2520Daas%26entry.1292438233%3D%2520%2520It%2520is%2520important%2520for%2520official%2520statistics%2520production%2520to%2520apply%2520ML%2520with%250Astatistical%2520rigor%252C%2520as%2520it%2520presents%2520both%2520opportunities%2520and%2520challenges.%2520Although%250Amachine%2520learning%2520has%2520enjoyed%2520rapid%2520technological%2520advances%2520in%2520recent%2520years%252C%2520its%250Aapplication%2520does%2520not%2520possess%2520the%2520methodological%2520robustness%2520necessary%2520to%2520produce%250Ahigh%2520quality%2520statistical%2520results.%2520In%2520order%2520to%2520account%2520for%2520all%2520sources%2520of%2520error%250Ain%2520machine%2520learning%2520models%252C%2520the%2520Total%2520Machine%2520Learning%2520Error%2520%2528TMLE%2529%2520is%250Apresented%2520as%2520a%2520framework%2520analogous%2520to%2520the%2520Total%2520Survey%2520Error%2520Model%2520used%2520in%250Asurvey%2520methodology.%2520As%2520a%2520means%2520of%2520ensuring%2520that%2520ML%2520models%2520are%2520both%2520internally%250Avalid%2520as%2520well%2520as%2520externally%2520valid%252C%2520the%2520TMLE%2520model%2520addresses%2520issues%2520such%2520as%250Arepresentativeness%2520and%2520measurement%2520errors.%2520There%2520are%2520several%2520case%2520studies%250Apresented%252C%2520illustrating%2520the%2520importance%2520of%2520applying%2520more%2520rigor%2520to%2520the%250Aapplication%2520of%2520machine%2520learning%2520in%2520official%2520statistics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Machine%20Learning%20for%20Official%20Statistics%3A%20A%20Statistical%0A%20%20Manifesto&entry.906535625=Marco%20Puts%20and%20David%20Salgado%20and%20Piet%20Daas&entry.1292438233=%20%20It%20is%20important%20for%20official%20statistics%20production%20to%20apply%20ML%20with%0Astatistical%20rigor%2C%20as%20it%20presents%20both%20opportunities%20and%20challenges.%20Although%0Amachine%20learning%20has%20enjoyed%20rapid%20technological%20advances%20in%20recent%20years%2C%20its%0Aapplication%20does%20not%20possess%20the%20methodological%20robustness%20necessary%20to%20produce%0Ahigh%20quality%20statistical%20results.%20In%20order%20to%20account%20for%20all%20sources%20of%20error%0Ain%20machine%20learning%20models%2C%20the%20Total%20Machine%20Learning%20Error%20%28TMLE%29%20is%0Apresented%20as%20a%20framework%20analogous%20to%20the%20Total%20Survey%20Error%20Model%20used%20in%0Asurvey%20methodology.%20As%20a%20means%20of%20ensuring%20that%20ML%20models%20are%20both%20internally%0Avalid%20as%20well%20as%20externally%20valid%2C%20the%20TMLE%20model%20addresses%20issues%20such%20as%0Arepresentativeness%20and%20measurement%20errors.%20There%20are%20several%20case%20studies%0Apresented%2C%20illustrating%20the%20importance%20of%20applying%20more%20rigor%20to%20the%0Aapplication%20of%20machine%20learning%20in%20official%20statistics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04365v1&entry.124074799=Read"},
{"title": "Reassessing the Validity of Spurious Correlations Benchmarks", "author": "Samuel J. Bell and Diane Bouchacourt and Levent Sagun", "abstract": "  Neural networks can fail when the data contains spurious correlations. To\nunderstand this phenomenon, researchers have proposed numerous spurious\ncorrelations benchmarks upon which to evaluate mitigation methods. However, we\nobserve that these benchmarks exhibit substantial disagreement, with the best\nmethods on one benchmark performing poorly on another. We explore this\ndisagreement, and examine benchmark validity by defining three desiderata that\na benchmark should satisfy in order to meaningfully evaluate methods. Our\nresults have implications for both benchmarks and mitigations: we find that\ncertain benchmarks are not meaningful measures of method performance, and that\nseveral methods are not sufficiently robust for widespread use. We present a\nsimple recipe for practitioners to choose methods using the most similar\nbenchmark to their given problem.\n", "link": "http://arxiv.org/abs/2409.04188v1", "date": "2024-09-06", "relevancy": 1.5883, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4002}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3965}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reassessing%20the%20Validity%20of%20Spurious%20Correlations%20Benchmarks&body=Title%3A%20Reassessing%20the%20Validity%20of%20Spurious%20Correlations%20Benchmarks%0AAuthor%3A%20Samuel%20J.%20Bell%20and%20Diane%20Bouchacourt%20and%20Levent%20Sagun%0AAbstract%3A%20%20%20Neural%20networks%20can%20fail%20when%20the%20data%20contains%20spurious%20correlations.%20To%0Aunderstand%20this%20phenomenon%2C%20researchers%20have%20proposed%20numerous%20spurious%0Acorrelations%20benchmarks%20upon%20which%20to%20evaluate%20mitigation%20methods.%20However%2C%20we%0Aobserve%20that%20these%20benchmarks%20exhibit%20substantial%20disagreement%2C%20with%20the%20best%0Amethods%20on%20one%20benchmark%20performing%20poorly%20on%20another.%20We%20explore%20this%0Adisagreement%2C%20and%20examine%20benchmark%20validity%20by%20defining%20three%20desiderata%20that%0Aa%20benchmark%20should%20satisfy%20in%20order%20to%20meaningfully%20evaluate%20methods.%20Our%0Aresults%20have%20implications%20for%20both%20benchmarks%20and%20mitigations%3A%20we%20find%20that%0Acertain%20benchmarks%20are%20not%20meaningful%20measures%20of%20method%20performance%2C%20and%20that%0Aseveral%20methods%20are%20not%20sufficiently%20robust%20for%20widespread%20use.%20We%20present%20a%0Asimple%20recipe%20for%20practitioners%20to%20choose%20methods%20using%20the%20most%20similar%0Abenchmark%20to%20their%20given%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReassessing%2520the%2520Validity%2520of%2520Spurious%2520Correlations%2520Benchmarks%26entry.906535625%3DSamuel%2520J.%2520Bell%2520and%2520Diane%2520Bouchacourt%2520and%2520Levent%2520Sagun%26entry.1292438233%3D%2520%2520Neural%2520networks%2520can%2520fail%2520when%2520the%2520data%2520contains%2520spurious%2520correlations.%2520To%250Aunderstand%2520this%2520phenomenon%252C%2520researchers%2520have%2520proposed%2520numerous%2520spurious%250Acorrelations%2520benchmarks%2520upon%2520which%2520to%2520evaluate%2520mitigation%2520methods.%2520However%252C%2520we%250Aobserve%2520that%2520these%2520benchmarks%2520exhibit%2520substantial%2520disagreement%252C%2520with%2520the%2520best%250Amethods%2520on%2520one%2520benchmark%2520performing%2520poorly%2520on%2520another.%2520We%2520explore%2520this%250Adisagreement%252C%2520and%2520examine%2520benchmark%2520validity%2520by%2520defining%2520three%2520desiderata%2520that%250Aa%2520benchmark%2520should%2520satisfy%2520in%2520order%2520to%2520meaningfully%2520evaluate%2520methods.%2520Our%250Aresults%2520have%2520implications%2520for%2520both%2520benchmarks%2520and%2520mitigations%253A%2520we%2520find%2520that%250Acertain%2520benchmarks%2520are%2520not%2520meaningful%2520measures%2520of%2520method%2520performance%252C%2520and%2520that%250Aseveral%2520methods%2520are%2520not%2520sufficiently%2520robust%2520for%2520widespread%2520use.%2520We%2520present%2520a%250Asimple%2520recipe%2520for%2520practitioners%2520to%2520choose%2520methods%2520using%2520the%2520most%2520similar%250Abenchmark%2520to%2520their%2520given%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reassessing%20the%20Validity%20of%20Spurious%20Correlations%20Benchmarks&entry.906535625=Samuel%20J.%20Bell%20and%20Diane%20Bouchacourt%20and%20Levent%20Sagun&entry.1292438233=%20%20Neural%20networks%20can%20fail%20when%20the%20data%20contains%20spurious%20correlations.%20To%0Aunderstand%20this%20phenomenon%2C%20researchers%20have%20proposed%20numerous%20spurious%0Acorrelations%20benchmarks%20upon%20which%20to%20evaluate%20mitigation%20methods.%20However%2C%20we%0Aobserve%20that%20these%20benchmarks%20exhibit%20substantial%20disagreement%2C%20with%20the%20best%0Amethods%20on%20one%20benchmark%20performing%20poorly%20on%20another.%20We%20explore%20this%0Adisagreement%2C%20and%20examine%20benchmark%20validity%20by%20defining%20three%20desiderata%20that%0Aa%20benchmark%20should%20satisfy%20in%20order%20to%20meaningfully%20evaluate%20methods.%20Our%0Aresults%20have%20implications%20for%20both%20benchmarks%20and%20mitigations%3A%20we%20find%20that%0Acertain%20benchmarks%20are%20not%20meaningful%20measures%20of%20method%20performance%2C%20and%20that%0Aseveral%20methods%20are%20not%20sufficiently%20robust%20for%20widespread%20use.%20We%20present%20a%0Asimple%20recipe%20for%20practitioners%20to%20choose%20methods%20using%20the%20most%20similar%0Abenchmark%20to%20their%20given%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04188v1&entry.124074799=Read"},
{"title": "When big data actually are low-rank, or entrywise approximation of\n  certain function-generated matrices", "author": "Stanislav Budzinskiy", "abstract": "  The article concerns low-rank approximation of matrices generated by sampling\na smooth function of two $m$-dimensional variables. We refute an argument made\nin the literature to prove that, for a specific class of analytic functions,\nsuch matrices admit accurate entrywise approximation of rank that is\nindependent of $m$ -- a claim known as \"big-data matrices are approximately\nlow-rank\". We provide a theoretical explanation of the numerical results\npresented in support of this claim, describing three narrower classes of\nfunctions for which $n \\times n$ function-generated matrices can be\napproximated within an entrywise error of order $\\varepsilon$ with rank\n$\\mathcal{O}(\\log(n) \\varepsilon^{-2} \\mathrm{polylog}(\\varepsilon^{-1}))$ that\nis independent of the dimension $m$: (i) functions of the inner product of the\ntwo variables, (ii) functions of the Euclidean distance between the variables,\nand (iii) shift-invariant positive-definite kernels. We extend our argument to\ntensor-train approximation of tensors generated with functions of the\nmulti-linear product of their $m$-dimensional variables. We discuss our results\nin the context of low-rank approximation of (a) growing datasets and (b)\nattention in transformer neural networks.\n", "link": "http://arxiv.org/abs/2407.03250v3", "date": "2024-09-06", "relevancy": 1.2836, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4357}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.422}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20big%20data%20actually%20are%20low-rank%2C%20or%20entrywise%20approximation%20of%0A%20%20certain%20function-generated%20matrices&body=Title%3A%20When%20big%20data%20actually%20are%20low-rank%2C%20or%20entrywise%20approximation%20of%0A%20%20certain%20function-generated%20matrices%0AAuthor%3A%20Stanislav%20Budzinskiy%0AAbstract%3A%20%20%20The%20article%20concerns%20low-rank%20approximation%20of%20matrices%20generated%20by%20sampling%0Aa%20smooth%20function%20of%20two%20%24m%24-dimensional%20variables.%20We%20refute%20an%20argument%20made%0Ain%20the%20literature%20to%20prove%20that%2C%20for%20a%20specific%20class%20of%20analytic%20functions%2C%0Asuch%20matrices%20admit%20accurate%20entrywise%20approximation%20of%20rank%20that%20is%0Aindependent%20of%20%24m%24%20--%20a%20claim%20known%20as%20%22big-data%20matrices%20are%20approximately%0Alow-rank%22.%20We%20provide%20a%20theoretical%20explanation%20of%20the%20numerical%20results%0Apresented%20in%20support%20of%20this%20claim%2C%20describing%20three%20narrower%20classes%20of%0Afunctions%20for%20which%20%24n%20%5Ctimes%20n%24%20function-generated%20matrices%20can%20be%0Aapproximated%20within%20an%20entrywise%20error%20of%20order%20%24%5Cvarepsilon%24%20with%20rank%0A%24%5Cmathcal%7BO%7D%28%5Clog%28n%29%20%5Cvarepsilon%5E%7B-2%7D%20%5Cmathrm%7Bpolylog%7D%28%5Cvarepsilon%5E%7B-1%7D%29%29%24%20that%0Ais%20independent%20of%20the%20dimension%20%24m%24%3A%20%28i%29%20functions%20of%20the%20inner%20product%20of%20the%0Atwo%20variables%2C%20%28ii%29%20functions%20of%20the%20Euclidean%20distance%20between%20the%20variables%2C%0Aand%20%28iii%29%20shift-invariant%20positive-definite%20kernels.%20We%20extend%20our%20argument%20to%0Atensor-train%20approximation%20of%20tensors%20generated%20with%20functions%20of%20the%0Amulti-linear%20product%20of%20their%20%24m%24-dimensional%20variables.%20We%20discuss%20our%20results%0Ain%20the%20context%20of%20low-rank%20approximation%20of%20%28a%29%20growing%20datasets%20and%20%28b%29%0Aattention%20in%20transformer%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03250v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520big%2520data%2520actually%2520are%2520low-rank%252C%2520or%2520entrywise%2520approximation%2520of%250A%2520%2520certain%2520function-generated%2520matrices%26entry.906535625%3DStanislav%2520Budzinskiy%26entry.1292438233%3D%2520%2520The%2520article%2520concerns%2520low-rank%2520approximation%2520of%2520matrices%2520generated%2520by%2520sampling%250Aa%2520smooth%2520function%2520of%2520two%2520%2524m%2524-dimensional%2520variables.%2520We%2520refute%2520an%2520argument%2520made%250Ain%2520the%2520literature%2520to%2520prove%2520that%252C%2520for%2520a%2520specific%2520class%2520of%2520analytic%2520functions%252C%250Asuch%2520matrices%2520admit%2520accurate%2520entrywise%2520approximation%2520of%2520rank%2520that%2520is%250Aindependent%2520of%2520%2524m%2524%2520--%2520a%2520claim%2520known%2520as%2520%2522big-data%2520matrices%2520are%2520approximately%250Alow-rank%2522.%2520We%2520provide%2520a%2520theoretical%2520explanation%2520of%2520the%2520numerical%2520results%250Apresented%2520in%2520support%2520of%2520this%2520claim%252C%2520describing%2520three%2520narrower%2520classes%2520of%250Afunctions%2520for%2520which%2520%2524n%2520%255Ctimes%2520n%2524%2520function-generated%2520matrices%2520can%2520be%250Aapproximated%2520within%2520an%2520entrywise%2520error%2520of%2520order%2520%2524%255Cvarepsilon%2524%2520with%2520rank%250A%2524%255Cmathcal%257BO%257D%2528%255Clog%2528n%2529%2520%255Cvarepsilon%255E%257B-2%257D%2520%255Cmathrm%257Bpolylog%257D%2528%255Cvarepsilon%255E%257B-1%257D%2529%2529%2524%2520that%250Ais%2520independent%2520of%2520the%2520dimension%2520%2524m%2524%253A%2520%2528i%2529%2520functions%2520of%2520the%2520inner%2520product%2520of%2520the%250Atwo%2520variables%252C%2520%2528ii%2529%2520functions%2520of%2520the%2520Euclidean%2520distance%2520between%2520the%2520variables%252C%250Aand%2520%2528iii%2529%2520shift-invariant%2520positive-definite%2520kernels.%2520We%2520extend%2520our%2520argument%2520to%250Atensor-train%2520approximation%2520of%2520tensors%2520generated%2520with%2520functions%2520of%2520the%250Amulti-linear%2520product%2520of%2520their%2520%2524m%2524-dimensional%2520variables.%2520We%2520discuss%2520our%2520results%250Ain%2520the%2520context%2520of%2520low-rank%2520approximation%2520of%2520%2528a%2529%2520growing%2520datasets%2520and%2520%2528b%2529%250Aattention%2520in%2520transformer%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03250v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20big%20data%20actually%20are%20low-rank%2C%20or%20entrywise%20approximation%20of%0A%20%20certain%20function-generated%20matrices&entry.906535625=Stanislav%20Budzinskiy&entry.1292438233=%20%20The%20article%20concerns%20low-rank%20approximation%20of%20matrices%20generated%20by%20sampling%0Aa%20smooth%20function%20of%20two%20%24m%24-dimensional%20variables.%20We%20refute%20an%20argument%20made%0Ain%20the%20literature%20to%20prove%20that%2C%20for%20a%20specific%20class%20of%20analytic%20functions%2C%0Asuch%20matrices%20admit%20accurate%20entrywise%20approximation%20of%20rank%20that%20is%0Aindependent%20of%20%24m%24%20--%20a%20claim%20known%20as%20%22big-data%20matrices%20are%20approximately%0Alow-rank%22.%20We%20provide%20a%20theoretical%20explanation%20of%20the%20numerical%20results%0Apresented%20in%20support%20of%20this%20claim%2C%20describing%20three%20narrower%20classes%20of%0Afunctions%20for%20which%20%24n%20%5Ctimes%20n%24%20function-generated%20matrices%20can%20be%0Aapproximated%20within%20an%20entrywise%20error%20of%20order%20%24%5Cvarepsilon%24%20with%20rank%0A%24%5Cmathcal%7BO%7D%28%5Clog%28n%29%20%5Cvarepsilon%5E%7B-2%7D%20%5Cmathrm%7Bpolylog%7D%28%5Cvarepsilon%5E%7B-1%7D%29%29%24%20that%0Ais%20independent%20of%20the%20dimension%20%24m%24%3A%20%28i%29%20functions%20of%20the%20inner%20product%20of%20the%0Atwo%20variables%2C%20%28ii%29%20functions%20of%20the%20Euclidean%20distance%20between%20the%20variables%2C%0Aand%20%28iii%29%20shift-invariant%20positive-definite%20kernels.%20We%20extend%20our%20argument%20to%0Atensor-train%20approximation%20of%20tensors%20generated%20with%20functions%20of%20the%0Amulti-linear%20product%20of%20their%20%24m%24-dimensional%20variables.%20We%20discuss%20our%20results%0Ain%20the%20context%20of%20low-rank%20approximation%20of%20%28a%29%20growing%20datasets%20and%20%28b%29%0Aattention%20in%20transformer%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03250v3&entry.124074799=Read"},
{"title": "Standing on the shoulders of giants", "author": "Lucas Felipe Ferraro Cardoso and Jos\u00e9 de Sousa Ribeiro Filho and Vitor Cirilo Araujo Santos and Regiane Silva Kawasaki Frances and Ronnie Cley de Oliveira Alves", "abstract": "  Although fundamental to the advancement of Machine Learning, the classic\nevaluation metrics extracted from the confusion matrix, such as precision and\nF1, are limited. Such metrics only offer a quantitative view of the models'\nperformance, without considering the complexity of the data or the quality of\nthe hit. To overcome these limitations, recent research has introduced the use\nof psychometric metrics such as Item Response Theory (IRT), which allows an\nassessment at the level of latent characteristics of instances. This work\ninvestigates how IRT concepts can enrich a confusion matrix in order to\nidentify which model is the most appropriate among options with similar\nperformance. In the study carried out, IRT does not replace, but complements\nclassical metrics by offering a new layer of evaluation and observation of the\nfine behavior of models in specific instances. It was also observed that there\nis 97% confidence that the score from the IRT has different contributions from\n66% of the classical metrics analyzed.\n", "link": "http://arxiv.org/abs/2409.03151v2", "date": "2024-09-06", "relevancy": 1.3578, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4626}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4405}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Standing%20on%20the%20shoulders%20of%20giants&body=Title%3A%20Standing%20on%20the%20shoulders%20of%20giants%0AAuthor%3A%20Lucas%20Felipe%20Ferraro%20Cardoso%20and%20Jos%C3%A9%20de%20Sousa%20Ribeiro%20Filho%20and%20Vitor%20Cirilo%20Araujo%20Santos%20and%20Regiane%20Silva%20Kawasaki%20Frances%20and%20Ronnie%20Cley%20de%20Oliveira%20Alves%0AAbstract%3A%20%20%20Although%20fundamental%20to%20the%20advancement%20of%20Machine%20Learning%2C%20the%20classic%0Aevaluation%20metrics%20extracted%20from%20the%20confusion%20matrix%2C%20such%20as%20precision%20and%0AF1%2C%20are%20limited.%20Such%20metrics%20only%20offer%20a%20quantitative%20view%20of%20the%20models%27%0Aperformance%2C%20without%20considering%20the%20complexity%20of%20the%20data%20or%20the%20quality%20of%0Athe%20hit.%20To%20overcome%20these%20limitations%2C%20recent%20research%20has%20introduced%20the%20use%0Aof%20psychometric%20metrics%20such%20as%20Item%20Response%20Theory%20%28IRT%29%2C%20which%20allows%20an%0Aassessment%20at%20the%20level%20of%20latent%20characteristics%20of%20instances.%20This%20work%0Ainvestigates%20how%20IRT%20concepts%20can%20enrich%20a%20confusion%20matrix%20in%20order%20to%0Aidentify%20which%20model%20is%20the%20most%20appropriate%20among%20options%20with%20similar%0Aperformance.%20In%20the%20study%20carried%20out%2C%20IRT%20does%20not%20replace%2C%20but%20complements%0Aclassical%20metrics%20by%20offering%20a%20new%20layer%20of%20evaluation%20and%20observation%20of%20the%0Afine%20behavior%20of%20models%20in%20specific%20instances.%20It%20was%20also%20observed%20that%20there%0Ais%2097%25%20confidence%20that%20the%20score%20from%20the%20IRT%20has%20different%20contributions%20from%0A66%25%20of%20the%20classical%20metrics%20analyzed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStanding%2520on%2520the%2520shoulders%2520of%2520giants%26entry.906535625%3DLucas%2520Felipe%2520Ferraro%2520Cardoso%2520and%2520Jos%25C3%25A9%2520de%2520Sousa%2520Ribeiro%2520Filho%2520and%2520Vitor%2520Cirilo%2520Araujo%2520Santos%2520and%2520Regiane%2520Silva%2520Kawasaki%2520Frances%2520and%2520Ronnie%2520Cley%2520de%2520Oliveira%2520Alves%26entry.1292438233%3D%2520%2520Although%2520fundamental%2520to%2520the%2520advancement%2520of%2520Machine%2520Learning%252C%2520the%2520classic%250Aevaluation%2520metrics%2520extracted%2520from%2520the%2520confusion%2520matrix%252C%2520such%2520as%2520precision%2520and%250AF1%252C%2520are%2520limited.%2520Such%2520metrics%2520only%2520offer%2520a%2520quantitative%2520view%2520of%2520the%2520models%2527%250Aperformance%252C%2520without%2520considering%2520the%2520complexity%2520of%2520the%2520data%2520or%2520the%2520quality%2520of%250Athe%2520hit.%2520To%2520overcome%2520these%2520limitations%252C%2520recent%2520research%2520has%2520introduced%2520the%2520use%250Aof%2520psychometric%2520metrics%2520such%2520as%2520Item%2520Response%2520Theory%2520%2528IRT%2529%252C%2520which%2520allows%2520an%250Aassessment%2520at%2520the%2520level%2520of%2520latent%2520characteristics%2520of%2520instances.%2520This%2520work%250Ainvestigates%2520how%2520IRT%2520concepts%2520can%2520enrich%2520a%2520confusion%2520matrix%2520in%2520order%2520to%250Aidentify%2520which%2520model%2520is%2520the%2520most%2520appropriate%2520among%2520options%2520with%2520similar%250Aperformance.%2520In%2520the%2520study%2520carried%2520out%252C%2520IRT%2520does%2520not%2520replace%252C%2520but%2520complements%250Aclassical%2520metrics%2520by%2520offering%2520a%2520new%2520layer%2520of%2520evaluation%2520and%2520observation%2520of%2520the%250Afine%2520behavior%2520of%2520models%2520in%2520specific%2520instances.%2520It%2520was%2520also%2520observed%2520that%2520there%250Ais%252097%2525%2520confidence%2520that%2520the%2520score%2520from%2520the%2520IRT%2520has%2520different%2520contributions%2520from%250A66%2525%2520of%2520the%2520classical%2520metrics%2520analyzed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Standing%20on%20the%20shoulders%20of%20giants&entry.906535625=Lucas%20Felipe%20Ferraro%20Cardoso%20and%20Jos%C3%A9%20de%20Sousa%20Ribeiro%20Filho%20and%20Vitor%20Cirilo%20Araujo%20Santos%20and%20Regiane%20Silva%20Kawasaki%20Frances%20and%20Ronnie%20Cley%20de%20Oliveira%20Alves&entry.1292438233=%20%20Although%20fundamental%20to%20the%20advancement%20of%20Machine%20Learning%2C%20the%20classic%0Aevaluation%20metrics%20extracted%20from%20the%20confusion%20matrix%2C%20such%20as%20precision%20and%0AF1%2C%20are%20limited.%20Such%20metrics%20only%20offer%20a%20quantitative%20view%20of%20the%20models%27%0Aperformance%2C%20without%20considering%20the%20complexity%20of%20the%20data%20or%20the%20quality%20of%0Athe%20hit.%20To%20overcome%20these%20limitations%2C%20recent%20research%20has%20introduced%20the%20use%0Aof%20psychometric%20metrics%20such%20as%20Item%20Response%20Theory%20%28IRT%29%2C%20which%20allows%20an%0Aassessment%20at%20the%20level%20of%20latent%20characteristics%20of%20instances.%20This%20work%0Ainvestigates%20how%20IRT%20concepts%20can%20enrich%20a%20confusion%20matrix%20in%20order%20to%0Aidentify%20which%20model%20is%20the%20most%20appropriate%20among%20options%20with%20similar%0Aperformance.%20In%20the%20study%20carried%20out%2C%20IRT%20does%20not%20replace%2C%20but%20complements%0Aclassical%20metrics%20by%20offering%20a%20new%20layer%20of%20evaluation%20and%20observation%20of%20the%0Afine%20behavior%20of%20models%20in%20specific%20instances.%20It%20was%20also%20observed%20that%20there%0Ais%2097%25%20confidence%20that%20the%20score%20from%20the%20IRT%20has%20different%20contributions%20from%0A66%25%20of%20the%20classical%20metrics%20analyzed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03151v2&entry.124074799=Read"},
{"title": "Theory, Analysis, and Best Practices for Sigmoid Self-Attention", "author": "Jason Ramapuram and Federico Danieli and Eeshan Dhekane and Floris Weers and Dan Busbridge and Pierre Ablin and Tatiana Likhomanenko and Jagrit Digani and Zijin Gu and Amitis Shidani and Russ Webb", "abstract": "  Attention is a key part of the transformer architecture. It is a\nsequence-to-sequence mapping that transforms each sequence element into a\nweighted sum of values. The weights are typically obtained as the softmax of\ndot products between keys and queries. Recent work has explored alternatives to\nsoftmax attention in transformers, such as ReLU and sigmoid activations. In\nthis work, we revisit sigmoid attention and conduct an in-depth theoretical and\nempirical analysis. Theoretically, we prove that transformers with sigmoid\nattention are universal function approximators and benefit from improved\nregularity compared to softmax attention. Through detailed empirical analysis,\nwe identify stabilization of large initial attention norms during the early\nstages of training as a crucial factor for the successful training of models\nwith sigmoid attention, outperforming prior attempts. We also introduce\nFLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid\nattention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100\nGPUs. Experiments across language, vision, and speech show that properly\nnormalized sigmoid attention matches the strong performance of softmax\nattention on a wide range of domains and scales, which previous attempts at\nsigmoid attention were unable to fully achieve. Our work unifies prior art and\nestablishes best practices for sigmoid attention as a drop-in softmax\nreplacement in transformers.\n", "link": "http://arxiv.org/abs/2409.04431v1", "date": "2024-09-06", "relevancy": 1.5793, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5818}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5289}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theory%2C%20Analysis%2C%20and%20Best%20Practices%20for%20Sigmoid%20Self-Attention&body=Title%3A%20Theory%2C%20Analysis%2C%20and%20Best%20Practices%20for%20Sigmoid%20Self-Attention%0AAuthor%3A%20Jason%20Ramapuram%20and%20Federico%20Danieli%20and%20Eeshan%20Dhekane%20and%20Floris%20Weers%20and%20Dan%20Busbridge%20and%20Pierre%20Ablin%20and%20Tatiana%20Likhomanenko%20and%20Jagrit%20Digani%20and%20Zijin%20Gu%20and%20Amitis%20Shidani%20and%20Russ%20Webb%0AAbstract%3A%20%20%20Attention%20is%20a%20key%20part%20of%20the%20transformer%20architecture.%20It%20is%20a%0Asequence-to-sequence%20mapping%20that%20transforms%20each%20sequence%20element%20into%20a%0Aweighted%20sum%20of%20values.%20The%20weights%20are%20typically%20obtained%20as%20the%20softmax%20of%0Adot%20products%20between%20keys%20and%20queries.%20Recent%20work%20has%20explored%20alternatives%20to%0Asoftmax%20attention%20in%20transformers%2C%20such%20as%20ReLU%20and%20sigmoid%20activations.%20In%0Athis%20work%2C%20we%20revisit%20sigmoid%20attention%20and%20conduct%20an%20in-depth%20theoretical%20and%0Aempirical%20analysis.%20Theoretically%2C%20we%20prove%20that%20transformers%20with%20sigmoid%0Aattention%20are%20universal%20function%20approximators%20and%20benefit%20from%20improved%0Aregularity%20compared%20to%20softmax%20attention.%20Through%20detailed%20empirical%20analysis%2C%0Awe%20identify%20stabilization%20of%20large%20initial%20attention%20norms%20during%20the%20early%0Astages%20of%20training%20as%20a%20crucial%20factor%20for%20the%20successful%20training%20of%20models%0Awith%20sigmoid%20attention%2C%20outperforming%20prior%20attempts.%20We%20also%20introduce%0AFLASHSIGMOID%2C%20a%20hardware-aware%20and%20memory-efficient%20implementation%20of%20sigmoid%0Aattention%20yielding%20a%2017%25%20inference%20kernel%20speed-up%20over%20FLASHATTENTION2%20on%20H100%0AGPUs.%20Experiments%20across%20language%2C%20vision%2C%20and%20speech%20show%20that%20properly%0Anormalized%20sigmoid%20attention%20matches%20the%20strong%20performance%20of%20softmax%0Aattention%20on%20a%20wide%20range%20of%20domains%20and%20scales%2C%20which%20previous%20attempts%20at%0Asigmoid%20attention%20were%20unable%20to%20fully%20achieve.%20Our%20work%20unifies%20prior%20art%20and%0Aestablishes%20best%20practices%20for%20sigmoid%20attention%20as%20a%20drop-in%20softmax%0Areplacement%20in%20transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheory%252C%2520Analysis%252C%2520and%2520Best%2520Practices%2520for%2520Sigmoid%2520Self-Attention%26entry.906535625%3DJason%2520Ramapuram%2520and%2520Federico%2520Danieli%2520and%2520Eeshan%2520Dhekane%2520and%2520Floris%2520Weers%2520and%2520Dan%2520Busbridge%2520and%2520Pierre%2520Ablin%2520and%2520Tatiana%2520Likhomanenko%2520and%2520Jagrit%2520Digani%2520and%2520Zijin%2520Gu%2520and%2520Amitis%2520Shidani%2520and%2520Russ%2520Webb%26entry.1292438233%3D%2520%2520Attention%2520is%2520a%2520key%2520part%2520of%2520the%2520transformer%2520architecture.%2520It%2520is%2520a%250Asequence-to-sequence%2520mapping%2520that%2520transforms%2520each%2520sequence%2520element%2520into%2520a%250Aweighted%2520sum%2520of%2520values.%2520The%2520weights%2520are%2520typically%2520obtained%2520as%2520the%2520softmax%2520of%250Adot%2520products%2520between%2520keys%2520and%2520queries.%2520Recent%2520work%2520has%2520explored%2520alternatives%2520to%250Asoftmax%2520attention%2520in%2520transformers%252C%2520such%2520as%2520ReLU%2520and%2520sigmoid%2520activations.%2520In%250Athis%2520work%252C%2520we%2520revisit%2520sigmoid%2520attention%2520and%2520conduct%2520an%2520in-depth%2520theoretical%2520and%250Aempirical%2520analysis.%2520Theoretically%252C%2520we%2520prove%2520that%2520transformers%2520with%2520sigmoid%250Aattention%2520are%2520universal%2520function%2520approximators%2520and%2520benefit%2520from%2520improved%250Aregularity%2520compared%2520to%2520softmax%2520attention.%2520Through%2520detailed%2520empirical%2520analysis%252C%250Awe%2520identify%2520stabilization%2520of%2520large%2520initial%2520attention%2520norms%2520during%2520the%2520early%250Astages%2520of%2520training%2520as%2520a%2520crucial%2520factor%2520for%2520the%2520successful%2520training%2520of%2520models%250Awith%2520sigmoid%2520attention%252C%2520outperforming%2520prior%2520attempts.%2520We%2520also%2520introduce%250AFLASHSIGMOID%252C%2520a%2520hardware-aware%2520and%2520memory-efficient%2520implementation%2520of%2520sigmoid%250Aattention%2520yielding%2520a%252017%2525%2520inference%2520kernel%2520speed-up%2520over%2520FLASHATTENTION2%2520on%2520H100%250AGPUs.%2520Experiments%2520across%2520language%252C%2520vision%252C%2520and%2520speech%2520show%2520that%2520properly%250Anormalized%2520sigmoid%2520attention%2520matches%2520the%2520strong%2520performance%2520of%2520softmax%250Aattention%2520on%2520a%2520wide%2520range%2520of%2520domains%2520and%2520scales%252C%2520which%2520previous%2520attempts%2520at%250Asigmoid%2520attention%2520were%2520unable%2520to%2520fully%2520achieve.%2520Our%2520work%2520unifies%2520prior%2520art%2520and%250Aestablishes%2520best%2520practices%2520for%2520sigmoid%2520attention%2520as%2520a%2520drop-in%2520softmax%250Areplacement%2520in%2520transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theory%2C%20Analysis%2C%20and%20Best%20Practices%20for%20Sigmoid%20Self-Attention&entry.906535625=Jason%20Ramapuram%20and%20Federico%20Danieli%20and%20Eeshan%20Dhekane%20and%20Floris%20Weers%20and%20Dan%20Busbridge%20and%20Pierre%20Ablin%20and%20Tatiana%20Likhomanenko%20and%20Jagrit%20Digani%20and%20Zijin%20Gu%20and%20Amitis%20Shidani%20and%20Russ%20Webb&entry.1292438233=%20%20Attention%20is%20a%20key%20part%20of%20the%20transformer%20architecture.%20It%20is%20a%0Asequence-to-sequence%20mapping%20that%20transforms%20each%20sequence%20element%20into%20a%0Aweighted%20sum%20of%20values.%20The%20weights%20are%20typically%20obtained%20as%20the%20softmax%20of%0Adot%20products%20between%20keys%20and%20queries.%20Recent%20work%20has%20explored%20alternatives%20to%0Asoftmax%20attention%20in%20transformers%2C%20such%20as%20ReLU%20and%20sigmoid%20activations.%20In%0Athis%20work%2C%20we%20revisit%20sigmoid%20attention%20and%20conduct%20an%20in-depth%20theoretical%20and%0Aempirical%20analysis.%20Theoretically%2C%20we%20prove%20that%20transformers%20with%20sigmoid%0Aattention%20are%20universal%20function%20approximators%20and%20benefit%20from%20improved%0Aregularity%20compared%20to%20softmax%20attention.%20Through%20detailed%20empirical%20analysis%2C%0Awe%20identify%20stabilization%20of%20large%20initial%20attention%20norms%20during%20the%20early%0Astages%20of%20training%20as%20a%20crucial%20factor%20for%20the%20successful%20training%20of%20models%0Awith%20sigmoid%20attention%2C%20outperforming%20prior%20attempts.%20We%20also%20introduce%0AFLASHSIGMOID%2C%20a%20hardware-aware%20and%20memory-efficient%20implementation%20of%20sigmoid%0Aattention%20yielding%20a%2017%25%20inference%20kernel%20speed-up%20over%20FLASHATTENTION2%20on%20H100%0AGPUs.%20Experiments%20across%20language%2C%20vision%2C%20and%20speech%20show%20that%20properly%0Anormalized%20sigmoid%20attention%20matches%20the%20strong%20performance%20of%20softmax%0Aattention%20on%20a%20wide%20range%20of%20domains%20and%20scales%2C%20which%20previous%20attempts%20at%0Asigmoid%20attention%20were%20unable%20to%20fully%20achieve.%20Our%20work%20unifies%20prior%20art%20and%0Aestablishes%20best%20practices%20for%20sigmoid%20attention%20as%20a%20drop-in%20softmax%0Areplacement%20in%20transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04431v1&entry.124074799=Read"},
{"title": "The Transformation Logics", "author": "Alessandro Ronca", "abstract": "  We introduce a new family of temporal logics designed to finely balance the\ntrade-off between expressivity and complexity. Their key feature is the\npossibility of defining operators of a new kind that we call transformation\noperators. Some of them subsume existing temporal operators, while others are\nentirely novel. Of particular interest are transformation operators based on\nsemigroups. They enable logics to harness the richness of semigroup theory, and\nwe show them to yield logics capable of creating hierarchies of increasing\nexpressivity and complexity which are non-trivial to characterise in existing\nlogics. The result is a genuinely novel and yet unexplored landscape of\ntemporal logics, each of them with the potential of matching the trade-off\nbetween expressivity and complexity required by specific applications.\n", "link": "http://arxiv.org/abs/2304.09639v3", "date": "2024-09-06", "relevancy": 1.146, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3825}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3817}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Transformation%20Logics&body=Title%3A%20The%20Transformation%20Logics%0AAuthor%3A%20Alessandro%20Ronca%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20family%20of%20temporal%20logics%20designed%20to%20finely%20balance%20the%0Atrade-off%20between%20expressivity%20and%20complexity.%20Their%20key%20feature%20is%20the%0Apossibility%20of%20defining%20operators%20of%20a%20new%20kind%20that%20we%20call%20transformation%0Aoperators.%20Some%20of%20them%20subsume%20existing%20temporal%20operators%2C%20while%20others%20are%0Aentirely%20novel.%20Of%20particular%20interest%20are%20transformation%20operators%20based%20on%0Asemigroups.%20They%20enable%20logics%20to%20harness%20the%20richness%20of%20semigroup%20theory%2C%20and%0Awe%20show%20them%20to%20yield%20logics%20capable%20of%20creating%20hierarchies%20of%20increasing%0Aexpressivity%20and%20complexity%20which%20are%20non-trivial%20to%20characterise%20in%20existing%0Alogics.%20The%20result%20is%20a%20genuinely%20novel%20and%20yet%20unexplored%20landscape%20of%0Atemporal%20logics%2C%20each%20of%20them%20with%20the%20potential%20of%20matching%20the%20trade-off%0Abetween%20expressivity%20and%20complexity%20required%20by%20specific%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.09639v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Transformation%2520Logics%26entry.906535625%3DAlessandro%2520Ronca%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520family%2520of%2520temporal%2520logics%2520designed%2520to%2520finely%2520balance%2520the%250Atrade-off%2520between%2520expressivity%2520and%2520complexity.%2520Their%2520key%2520feature%2520is%2520the%250Apossibility%2520of%2520defining%2520operators%2520of%2520a%2520new%2520kind%2520that%2520we%2520call%2520transformation%250Aoperators.%2520Some%2520of%2520them%2520subsume%2520existing%2520temporal%2520operators%252C%2520while%2520others%2520are%250Aentirely%2520novel.%2520Of%2520particular%2520interest%2520are%2520transformation%2520operators%2520based%2520on%250Asemigroups.%2520They%2520enable%2520logics%2520to%2520harness%2520the%2520richness%2520of%2520semigroup%2520theory%252C%2520and%250Awe%2520show%2520them%2520to%2520yield%2520logics%2520capable%2520of%2520creating%2520hierarchies%2520of%2520increasing%250Aexpressivity%2520and%2520complexity%2520which%2520are%2520non-trivial%2520to%2520characterise%2520in%2520existing%250Alogics.%2520The%2520result%2520is%2520a%2520genuinely%2520novel%2520and%2520yet%2520unexplored%2520landscape%2520of%250Atemporal%2520logics%252C%2520each%2520of%2520them%2520with%2520the%2520potential%2520of%2520matching%2520the%2520trade-off%250Abetween%2520expressivity%2520and%2520complexity%2520required%2520by%2520specific%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.09639v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Transformation%20Logics&entry.906535625=Alessandro%20Ronca&entry.1292438233=%20%20We%20introduce%20a%20new%20family%20of%20temporal%20logics%20designed%20to%20finely%20balance%20the%0Atrade-off%20between%20expressivity%20and%20complexity.%20Their%20key%20feature%20is%20the%0Apossibility%20of%20defining%20operators%20of%20a%20new%20kind%20that%20we%20call%20transformation%0Aoperators.%20Some%20of%20them%20subsume%20existing%20temporal%20operators%2C%20while%20others%20are%0Aentirely%20novel.%20Of%20particular%20interest%20are%20transformation%20operators%20based%20on%0Asemigroups.%20They%20enable%20logics%20to%20harness%20the%20richness%20of%20semigroup%20theory%2C%20and%0Awe%20show%20them%20to%20yield%20logics%20capable%20of%20creating%20hierarchies%20of%20increasing%0Aexpressivity%20and%20complexity%20which%20are%20non-trivial%20to%20characterise%20in%20existing%0Alogics.%20The%20result%20is%20a%20genuinely%20novel%20and%20yet%20unexplored%20landscape%20of%0Atemporal%20logics%2C%20each%20of%20them%20with%20the%20potential%20of%20matching%20the%20trade-off%0Abetween%20expressivity%20and%20complexity%20required%20by%20specific%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.09639v3&entry.124074799=Read"},
{"title": "Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent\n  Reinforcement Learning Framework", "author": "Daniel J. Tan and Qianyi Xu and Kay Choong See and Dilruk Perera and Mengling Feng", "abstract": "  Multi-organ diseases present significant challenges due to their simultaneous\nimpact on multiple organ systems, necessitating complex and adaptive treatment\nstrategies. Despite recent advancements in AI-powered healthcare decision\nsupport systems, existing solutions are limited to individual organ systems.\nThey often ignore the intricate dependencies between organ system and thereby\nfails to provide holistic treatment recommendations that are useful in\npractice. We propose a novel hierarchical multi-agent reinforcement learning\n(HMARL) framework to address these challenges. This framework uses dedicated\nagents for each organ system, and model dynamic through explicit inter-agent\ncommunication channels, enabling coordinated treatment strategies across\norgans. Furthermore, we introduce a dual-layer state representation technique\nto contextualize patient conditions at various hierarchical levels, enhancing\nthe treatment accuracy and relevance. Through extensive qualitative and\nquantitative evaluations in managing sepsis (a complex multi-organ disease),\nour approach demonstrates its ability to learn effective treatment policies\nthat significantly improve patient survival rates. This framework marks a\nsubstantial advancement in clinical decision support systems, pioneering a\ncomprehensive approach for multi-organ treatment recommendations.\n", "link": "http://arxiv.org/abs/2409.04224v1", "date": "2024-09-06", "relevancy": 1.4901, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5247}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4904}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Multi-Organ%20Disease%20Care%3A%20A%20Hierarchical%20Multi-Agent%0A%20%20Reinforcement%20Learning%20Framework&body=Title%3A%20Advancing%20Multi-Organ%20Disease%20Care%3A%20A%20Hierarchical%20Multi-Agent%0A%20%20Reinforcement%20Learning%20Framework%0AAuthor%3A%20Daniel%20J.%20Tan%20and%20Qianyi%20Xu%20and%20Kay%20Choong%20See%20and%20Dilruk%20Perera%20and%20Mengling%20Feng%0AAbstract%3A%20%20%20Multi-organ%20diseases%20present%20significant%20challenges%20due%20to%20their%20simultaneous%0Aimpact%20on%20multiple%20organ%20systems%2C%20necessitating%20complex%20and%20adaptive%20treatment%0Astrategies.%20Despite%20recent%20advancements%20in%20AI-powered%20healthcare%20decision%0Asupport%20systems%2C%20existing%20solutions%20are%20limited%20to%20individual%20organ%20systems.%0AThey%20often%20ignore%20the%20intricate%20dependencies%20between%20organ%20system%20and%20thereby%0Afails%20to%20provide%20holistic%20treatment%20recommendations%20that%20are%20useful%20in%0Apractice.%20We%20propose%20a%20novel%20hierarchical%20multi-agent%20reinforcement%20learning%0A%28HMARL%29%20framework%20to%20address%20these%20challenges.%20This%20framework%20uses%20dedicated%0Aagents%20for%20each%20organ%20system%2C%20and%20model%20dynamic%20through%20explicit%20inter-agent%0Acommunication%20channels%2C%20enabling%20coordinated%20treatment%20strategies%20across%0Aorgans.%20Furthermore%2C%20we%20introduce%20a%20dual-layer%20state%20representation%20technique%0Ato%20contextualize%20patient%20conditions%20at%20various%20hierarchical%20levels%2C%20enhancing%0Athe%20treatment%20accuracy%20and%20relevance.%20Through%20extensive%20qualitative%20and%0Aquantitative%20evaluations%20in%20managing%20sepsis%20%28a%20complex%20multi-organ%20disease%29%2C%0Aour%20approach%20demonstrates%20its%20ability%20to%20learn%20effective%20treatment%20policies%0Athat%20significantly%20improve%20patient%20survival%20rates.%20This%20framework%20marks%20a%0Asubstantial%20advancement%20in%20clinical%20decision%20support%20systems%2C%20pioneering%20a%0Acomprehensive%20approach%20for%20multi-organ%20treatment%20recommendations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Multi-Organ%2520Disease%2520Care%253A%2520A%2520Hierarchical%2520Multi-Agent%250A%2520%2520Reinforcement%2520Learning%2520Framework%26entry.906535625%3DDaniel%2520J.%2520Tan%2520and%2520Qianyi%2520Xu%2520and%2520Kay%2520Choong%2520See%2520and%2520Dilruk%2520Perera%2520and%2520Mengling%2520Feng%26entry.1292438233%3D%2520%2520Multi-organ%2520diseases%2520present%2520significant%2520challenges%2520due%2520to%2520their%2520simultaneous%250Aimpact%2520on%2520multiple%2520organ%2520systems%252C%2520necessitating%2520complex%2520and%2520adaptive%2520treatment%250Astrategies.%2520Despite%2520recent%2520advancements%2520in%2520AI-powered%2520healthcare%2520decision%250Asupport%2520systems%252C%2520existing%2520solutions%2520are%2520limited%2520to%2520individual%2520organ%2520systems.%250AThey%2520often%2520ignore%2520the%2520intricate%2520dependencies%2520between%2520organ%2520system%2520and%2520thereby%250Afails%2520to%2520provide%2520holistic%2520treatment%2520recommendations%2520that%2520are%2520useful%2520in%250Apractice.%2520We%2520propose%2520a%2520novel%2520hierarchical%2520multi-agent%2520reinforcement%2520learning%250A%2528HMARL%2529%2520framework%2520to%2520address%2520these%2520challenges.%2520This%2520framework%2520uses%2520dedicated%250Aagents%2520for%2520each%2520organ%2520system%252C%2520and%2520model%2520dynamic%2520through%2520explicit%2520inter-agent%250Acommunication%2520channels%252C%2520enabling%2520coordinated%2520treatment%2520strategies%2520across%250Aorgans.%2520Furthermore%252C%2520we%2520introduce%2520a%2520dual-layer%2520state%2520representation%2520technique%250Ato%2520contextualize%2520patient%2520conditions%2520at%2520various%2520hierarchical%2520levels%252C%2520enhancing%250Athe%2520treatment%2520accuracy%2520and%2520relevance.%2520Through%2520extensive%2520qualitative%2520and%250Aquantitative%2520evaluations%2520in%2520managing%2520sepsis%2520%2528a%2520complex%2520multi-organ%2520disease%2529%252C%250Aour%2520approach%2520demonstrates%2520its%2520ability%2520to%2520learn%2520effective%2520treatment%2520policies%250Athat%2520significantly%2520improve%2520patient%2520survival%2520rates.%2520This%2520framework%2520marks%2520a%250Asubstantial%2520advancement%2520in%2520clinical%2520decision%2520support%2520systems%252C%2520pioneering%2520a%250Acomprehensive%2520approach%2520for%2520multi-organ%2520treatment%2520recommendations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Multi-Organ%20Disease%20Care%3A%20A%20Hierarchical%20Multi-Agent%0A%20%20Reinforcement%20Learning%20Framework&entry.906535625=Daniel%20J.%20Tan%20and%20Qianyi%20Xu%20and%20Kay%20Choong%20See%20and%20Dilruk%20Perera%20and%20Mengling%20Feng&entry.1292438233=%20%20Multi-organ%20diseases%20present%20significant%20challenges%20due%20to%20their%20simultaneous%0Aimpact%20on%20multiple%20organ%20systems%2C%20necessitating%20complex%20and%20adaptive%20treatment%0Astrategies.%20Despite%20recent%20advancements%20in%20AI-powered%20healthcare%20decision%0Asupport%20systems%2C%20existing%20solutions%20are%20limited%20to%20individual%20organ%20systems.%0AThey%20often%20ignore%20the%20intricate%20dependencies%20between%20organ%20system%20and%20thereby%0Afails%20to%20provide%20holistic%20treatment%20recommendations%20that%20are%20useful%20in%0Apractice.%20We%20propose%20a%20novel%20hierarchical%20multi-agent%20reinforcement%20learning%0A%28HMARL%29%20framework%20to%20address%20these%20challenges.%20This%20framework%20uses%20dedicated%0Aagents%20for%20each%20organ%20system%2C%20and%20model%20dynamic%20through%20explicit%20inter-agent%0Acommunication%20channels%2C%20enabling%20coordinated%20treatment%20strategies%20across%0Aorgans.%20Furthermore%2C%20we%20introduce%20a%20dual-layer%20state%20representation%20technique%0Ato%20contextualize%20patient%20conditions%20at%20various%20hierarchical%20levels%2C%20enhancing%0Athe%20treatment%20accuracy%20and%20relevance.%20Through%20extensive%20qualitative%20and%0Aquantitative%20evaluations%20in%20managing%20sepsis%20%28a%20complex%20multi-organ%20disease%29%2C%0Aour%20approach%20demonstrates%20its%20ability%20to%20learn%20effective%20treatment%20policies%0Athat%20significantly%20improve%20patient%20survival%20rates.%20This%20framework%20marks%20a%0Asubstantial%20advancement%20in%20clinical%20decision%20support%20systems%2C%20pioneering%20a%0Acomprehensive%20approach%20for%20multi-organ%20treatment%20recommendations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04224v1&entry.124074799=Read"},
{"title": "NUMOSIM: A Synthetic Mobility Dataset with Anomaly Detection Benchmarks", "author": "Chris Stanford and Suman Adari and Xishun Liao and Yueshuai He and Qinhua Jiang and Chenchen Kuai and Jiaqi Ma and Emmanuel Tung and Yinlong Qian and Lingyi Zhao and Zihao Zhou and Zeeshan Rasheed and Khurram Shafique", "abstract": "  Collecting real-world mobility data is challenging. It is often fraught with\nprivacy concerns, logistical difficulties, and inherent biases. Moreover,\naccurately annotating anomalies in large-scale data is nearly impossible, as it\ndemands meticulous effort to distinguish subtle and complex patterns. These\nchallenges significantly impede progress in geospatial anomaly detection\nresearch by restricting access to reliable data and complicating the rigorous\nevaluation, comparison, and benchmarking of methodologies. To address these\nlimitations, we introduce a synthetic mobility dataset, NUMOSIM, that provides\na controlled, ethical, and diverse environment for benchmarking anomaly\ndetection techniques. NUMOSIM simulates a wide array of realistic mobility\nscenarios, encompassing both typical and anomalous behaviours, generated\nthrough advanced deep learning models trained on real mobility data. This\napproach allows NUMOSIM to accurately replicate the complexities of real-world\nmovement patterns while strategically injecting anomalies to challenge and\nevaluate detection algorithms based on how effectively they capture the\ninterplay between demographic, geospatial, and temporal factors. Our goal is to\nadvance geospatial mobility analysis by offering a realistic benchmark for\nimproving anomaly detection and mobility modeling techniques. To support this,\nwe provide open access to the NUMOSIM dataset, along with comprehensive\ndocumentation, evaluation metrics, and benchmark results.\n", "link": "http://arxiv.org/abs/2409.03024v2", "date": "2024-09-06", "relevancy": 1.464, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4989}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4933}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NUMOSIM%3A%20A%20Synthetic%20Mobility%20Dataset%20with%20Anomaly%20Detection%20Benchmarks&body=Title%3A%20NUMOSIM%3A%20A%20Synthetic%20Mobility%20Dataset%20with%20Anomaly%20Detection%20Benchmarks%0AAuthor%3A%20Chris%20Stanford%20and%20Suman%20Adari%20and%20Xishun%20Liao%20and%20Yueshuai%20He%20and%20Qinhua%20Jiang%20and%20Chenchen%20Kuai%20and%20Jiaqi%20Ma%20and%20Emmanuel%20Tung%20and%20Yinlong%20Qian%20and%20Lingyi%20Zhao%20and%20Zihao%20Zhou%20and%20Zeeshan%20Rasheed%20and%20Khurram%20Shafique%0AAbstract%3A%20%20%20Collecting%20real-world%20mobility%20data%20is%20challenging.%20It%20is%20often%20fraught%20with%0Aprivacy%20concerns%2C%20logistical%20difficulties%2C%20and%20inherent%20biases.%20Moreover%2C%0Aaccurately%20annotating%20anomalies%20in%20large-scale%20data%20is%20nearly%20impossible%2C%20as%20it%0Ademands%20meticulous%20effort%20to%20distinguish%20subtle%20and%20complex%20patterns.%20These%0Achallenges%20significantly%20impede%20progress%20in%20geospatial%20anomaly%20detection%0Aresearch%20by%20restricting%20access%20to%20reliable%20data%20and%20complicating%20the%20rigorous%0Aevaluation%2C%20comparison%2C%20and%20benchmarking%20of%20methodologies.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20synthetic%20mobility%20dataset%2C%20NUMOSIM%2C%20that%20provides%0Aa%20controlled%2C%20ethical%2C%20and%20diverse%20environment%20for%20benchmarking%20anomaly%0Adetection%20techniques.%20NUMOSIM%20simulates%20a%20wide%20array%20of%20realistic%20mobility%0Ascenarios%2C%20encompassing%20both%20typical%20and%20anomalous%20behaviours%2C%20generated%0Athrough%20advanced%20deep%20learning%20models%20trained%20on%20real%20mobility%20data.%20This%0Aapproach%20allows%20NUMOSIM%20to%20accurately%20replicate%20the%20complexities%20of%20real-world%0Amovement%20patterns%20while%20strategically%20injecting%20anomalies%20to%20challenge%20and%0Aevaluate%20detection%20algorithms%20based%20on%20how%20effectively%20they%20capture%20the%0Ainterplay%20between%20demographic%2C%20geospatial%2C%20and%20temporal%20factors.%20Our%20goal%20is%20to%0Aadvance%20geospatial%20mobility%20analysis%20by%20offering%20a%20realistic%20benchmark%20for%0Aimproving%20anomaly%20detection%20and%20mobility%20modeling%20techniques.%20To%20support%20this%2C%0Awe%20provide%20open%20access%20to%20the%20NUMOSIM%20dataset%2C%20along%20with%20comprehensive%0Adocumentation%2C%20evaluation%20metrics%2C%20and%20benchmark%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03024v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNUMOSIM%253A%2520A%2520Synthetic%2520Mobility%2520Dataset%2520with%2520Anomaly%2520Detection%2520Benchmarks%26entry.906535625%3DChris%2520Stanford%2520and%2520Suman%2520Adari%2520and%2520Xishun%2520Liao%2520and%2520Yueshuai%2520He%2520and%2520Qinhua%2520Jiang%2520and%2520Chenchen%2520Kuai%2520and%2520Jiaqi%2520Ma%2520and%2520Emmanuel%2520Tung%2520and%2520Yinlong%2520Qian%2520and%2520Lingyi%2520Zhao%2520and%2520Zihao%2520Zhou%2520and%2520Zeeshan%2520Rasheed%2520and%2520Khurram%2520Shafique%26entry.1292438233%3D%2520%2520Collecting%2520real-world%2520mobility%2520data%2520is%2520challenging.%2520It%2520is%2520often%2520fraught%2520with%250Aprivacy%2520concerns%252C%2520logistical%2520difficulties%252C%2520and%2520inherent%2520biases.%2520Moreover%252C%250Aaccurately%2520annotating%2520anomalies%2520in%2520large-scale%2520data%2520is%2520nearly%2520impossible%252C%2520as%2520it%250Ademands%2520meticulous%2520effort%2520to%2520distinguish%2520subtle%2520and%2520complex%2520patterns.%2520These%250Achallenges%2520significantly%2520impede%2520progress%2520in%2520geospatial%2520anomaly%2520detection%250Aresearch%2520by%2520restricting%2520access%2520to%2520reliable%2520data%2520and%2520complicating%2520the%2520rigorous%250Aevaluation%252C%2520comparison%252C%2520and%2520benchmarking%2520of%2520methodologies.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520a%2520synthetic%2520mobility%2520dataset%252C%2520NUMOSIM%252C%2520that%2520provides%250Aa%2520controlled%252C%2520ethical%252C%2520and%2520diverse%2520environment%2520for%2520benchmarking%2520anomaly%250Adetection%2520techniques.%2520NUMOSIM%2520simulates%2520a%2520wide%2520array%2520of%2520realistic%2520mobility%250Ascenarios%252C%2520encompassing%2520both%2520typical%2520and%2520anomalous%2520behaviours%252C%2520generated%250Athrough%2520advanced%2520deep%2520learning%2520models%2520trained%2520on%2520real%2520mobility%2520data.%2520This%250Aapproach%2520allows%2520NUMOSIM%2520to%2520accurately%2520replicate%2520the%2520complexities%2520of%2520real-world%250Amovement%2520patterns%2520while%2520strategically%2520injecting%2520anomalies%2520to%2520challenge%2520and%250Aevaluate%2520detection%2520algorithms%2520based%2520on%2520how%2520effectively%2520they%2520capture%2520the%250Ainterplay%2520between%2520demographic%252C%2520geospatial%252C%2520and%2520temporal%2520factors.%2520Our%2520goal%2520is%2520to%250Aadvance%2520geospatial%2520mobility%2520analysis%2520by%2520offering%2520a%2520realistic%2520benchmark%2520for%250Aimproving%2520anomaly%2520detection%2520and%2520mobility%2520modeling%2520techniques.%2520To%2520support%2520this%252C%250Awe%2520provide%2520open%2520access%2520to%2520the%2520NUMOSIM%2520dataset%252C%2520along%2520with%2520comprehensive%250Adocumentation%252C%2520evaluation%2520metrics%252C%2520and%2520benchmark%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03024v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NUMOSIM%3A%20A%20Synthetic%20Mobility%20Dataset%20with%20Anomaly%20Detection%20Benchmarks&entry.906535625=Chris%20Stanford%20and%20Suman%20Adari%20and%20Xishun%20Liao%20and%20Yueshuai%20He%20and%20Qinhua%20Jiang%20and%20Chenchen%20Kuai%20and%20Jiaqi%20Ma%20and%20Emmanuel%20Tung%20and%20Yinlong%20Qian%20and%20Lingyi%20Zhao%20and%20Zihao%20Zhou%20and%20Zeeshan%20Rasheed%20and%20Khurram%20Shafique&entry.1292438233=%20%20Collecting%20real-world%20mobility%20data%20is%20challenging.%20It%20is%20often%20fraught%20with%0Aprivacy%20concerns%2C%20logistical%20difficulties%2C%20and%20inherent%20biases.%20Moreover%2C%0Aaccurately%20annotating%20anomalies%20in%20large-scale%20data%20is%20nearly%20impossible%2C%20as%20it%0Ademands%20meticulous%20effort%20to%20distinguish%20subtle%20and%20complex%20patterns.%20These%0Achallenges%20significantly%20impede%20progress%20in%20geospatial%20anomaly%20detection%0Aresearch%20by%20restricting%20access%20to%20reliable%20data%20and%20complicating%20the%20rigorous%0Aevaluation%2C%20comparison%2C%20and%20benchmarking%20of%20methodologies.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20synthetic%20mobility%20dataset%2C%20NUMOSIM%2C%20that%20provides%0Aa%20controlled%2C%20ethical%2C%20and%20diverse%20environment%20for%20benchmarking%20anomaly%0Adetection%20techniques.%20NUMOSIM%20simulates%20a%20wide%20array%20of%20realistic%20mobility%0Ascenarios%2C%20encompassing%20both%20typical%20and%20anomalous%20behaviours%2C%20generated%0Athrough%20advanced%20deep%20learning%20models%20trained%20on%20real%20mobility%20data.%20This%0Aapproach%20allows%20NUMOSIM%20to%20accurately%20replicate%20the%20complexities%20of%20real-world%0Amovement%20patterns%20while%20strategically%20injecting%20anomalies%20to%20challenge%20and%0Aevaluate%20detection%20algorithms%20based%20on%20how%20effectively%20they%20capture%20the%0Ainterplay%20between%20demographic%2C%20geospatial%2C%20and%20temporal%20factors.%20Our%20goal%20is%20to%0Aadvance%20geospatial%20mobility%20analysis%20by%20offering%20a%20realistic%20benchmark%20for%0Aimproving%20anomaly%20detection%20and%20mobility%20modeling%20techniques.%20To%20support%20this%2C%0Awe%20provide%20open%20access%20to%20the%20NUMOSIM%20dataset%2C%20along%20with%20comprehensive%0Adocumentation%2C%20evaluation%20metrics%2C%20and%20benchmark%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03024v2&entry.124074799=Read"},
{"title": "Towards Privacy-Preserving Relational Data Synthesis via Probabilistic\n  Relational Models", "author": "Malte Luttermann and Ralf M\u00f6ller and Mattis Hartwig", "abstract": "  Probabilistic relational models provide a well-established formalism to\ncombine first-order logic and probabilistic models, thereby allowing to\nrepresent relationships between objects in a relational domain. At the same\ntime, the field of artificial intelligence requires increasingly large amounts\nof relational training data for various machine learning tasks. Collecting\nreal-world data, however, is often challenging due to privacy concerns, data\nprotection regulations, high costs, and so on. To mitigate these challenges,\nthe generation of synthetic data is a promising approach. In this paper, we\nsolve the problem of generating synthetic relational data via probabilistic\nrelational models. In particular, we propose a fully-fledged pipeline to go\nfrom relational database to probabilistic relational model, which can then be\nused to sample new synthetic relational data points from its underlying\nprobability distribution. As part of our proposed pipeline, we introduce a\nlearning algorithm to construct a probabilistic relational model from a given\nrelational database.\n", "link": "http://arxiv.org/abs/2409.04194v1", "date": "2024-09-06", "relevancy": 0.9154, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.48}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4588}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Privacy-Preserving%20Relational%20Data%20Synthesis%20via%20Probabilistic%0A%20%20Relational%20Models&body=Title%3A%20Towards%20Privacy-Preserving%20Relational%20Data%20Synthesis%20via%20Probabilistic%0A%20%20Relational%20Models%0AAuthor%3A%20Malte%20Luttermann%20and%20Ralf%20M%C3%B6ller%20and%20Mattis%20Hartwig%0AAbstract%3A%20%20%20Probabilistic%20relational%20models%20provide%20a%20well-established%20formalism%20to%0Acombine%20first-order%20logic%20and%20probabilistic%20models%2C%20thereby%20allowing%20to%0Arepresent%20relationships%20between%20objects%20in%20a%20relational%20domain.%20At%20the%20same%0Atime%2C%20the%20field%20of%20artificial%20intelligence%20requires%20increasingly%20large%20amounts%0Aof%20relational%20training%20data%20for%20various%20machine%20learning%20tasks.%20Collecting%0Areal-world%20data%2C%20however%2C%20is%20often%20challenging%20due%20to%20privacy%20concerns%2C%20data%0Aprotection%20regulations%2C%20high%20costs%2C%20and%20so%20on.%20To%20mitigate%20these%20challenges%2C%0Athe%20generation%20of%20synthetic%20data%20is%20a%20promising%20approach.%20In%20this%20paper%2C%20we%0Asolve%20the%20problem%20of%20generating%20synthetic%20relational%20data%20via%20probabilistic%0Arelational%20models.%20In%20particular%2C%20we%20propose%20a%20fully-fledged%20pipeline%20to%20go%0Afrom%20relational%20database%20to%20probabilistic%20relational%20model%2C%20which%20can%20then%20be%0Aused%20to%20sample%20new%20synthetic%20relational%20data%20points%20from%20its%20underlying%0Aprobability%20distribution.%20As%20part%20of%20our%20proposed%20pipeline%2C%20we%20introduce%20a%0Alearning%20algorithm%20to%20construct%20a%20probabilistic%20relational%20model%20from%20a%20given%0Arelational%20database.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Privacy-Preserving%2520Relational%2520Data%2520Synthesis%2520via%2520Probabilistic%250A%2520%2520Relational%2520Models%26entry.906535625%3DMalte%2520Luttermann%2520and%2520Ralf%2520M%25C3%25B6ller%2520and%2520Mattis%2520Hartwig%26entry.1292438233%3D%2520%2520Probabilistic%2520relational%2520models%2520provide%2520a%2520well-established%2520formalism%2520to%250Acombine%2520first-order%2520logic%2520and%2520probabilistic%2520models%252C%2520thereby%2520allowing%2520to%250Arepresent%2520relationships%2520between%2520objects%2520in%2520a%2520relational%2520domain.%2520At%2520the%2520same%250Atime%252C%2520the%2520field%2520of%2520artificial%2520intelligence%2520requires%2520increasingly%2520large%2520amounts%250Aof%2520relational%2520training%2520data%2520for%2520various%2520machine%2520learning%2520tasks.%2520Collecting%250Areal-world%2520data%252C%2520however%252C%2520is%2520often%2520challenging%2520due%2520to%2520privacy%2520concerns%252C%2520data%250Aprotection%2520regulations%252C%2520high%2520costs%252C%2520and%2520so%2520on.%2520To%2520mitigate%2520these%2520challenges%252C%250Athe%2520generation%2520of%2520synthetic%2520data%2520is%2520a%2520promising%2520approach.%2520In%2520this%2520paper%252C%2520we%250Asolve%2520the%2520problem%2520of%2520generating%2520synthetic%2520relational%2520data%2520via%2520probabilistic%250Arelational%2520models.%2520In%2520particular%252C%2520we%2520propose%2520a%2520fully-fledged%2520pipeline%2520to%2520go%250Afrom%2520relational%2520database%2520to%2520probabilistic%2520relational%2520model%252C%2520which%2520can%2520then%2520be%250Aused%2520to%2520sample%2520new%2520synthetic%2520relational%2520data%2520points%2520from%2520its%2520underlying%250Aprobability%2520distribution.%2520As%2520part%2520of%2520our%2520proposed%2520pipeline%252C%2520we%2520introduce%2520a%250Alearning%2520algorithm%2520to%2520construct%2520a%2520probabilistic%2520relational%2520model%2520from%2520a%2520given%250Arelational%2520database.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Privacy-Preserving%20Relational%20Data%20Synthesis%20via%20Probabilistic%0A%20%20Relational%20Models&entry.906535625=Malte%20Luttermann%20and%20Ralf%20M%C3%B6ller%20and%20Mattis%20Hartwig&entry.1292438233=%20%20Probabilistic%20relational%20models%20provide%20a%20well-established%20formalism%20to%0Acombine%20first-order%20logic%20and%20probabilistic%20models%2C%20thereby%20allowing%20to%0Arepresent%20relationships%20between%20objects%20in%20a%20relational%20domain.%20At%20the%20same%0Atime%2C%20the%20field%20of%20artificial%20intelligence%20requires%20increasingly%20large%20amounts%0Aof%20relational%20training%20data%20for%20various%20machine%20learning%20tasks.%20Collecting%0Areal-world%20data%2C%20however%2C%20is%20often%20challenging%20due%20to%20privacy%20concerns%2C%20data%0Aprotection%20regulations%2C%20high%20costs%2C%20and%20so%20on.%20To%20mitigate%20these%20challenges%2C%0Athe%20generation%20of%20synthetic%20data%20is%20a%20promising%20approach.%20In%20this%20paper%2C%20we%0Asolve%20the%20problem%20of%20generating%20synthetic%20relational%20data%20via%20probabilistic%0Arelational%20models.%20In%20particular%2C%20we%20propose%20a%20fully-fledged%20pipeline%20to%20go%0Afrom%20relational%20database%20to%20probabilistic%20relational%20model%2C%20which%20can%20then%20be%0Aused%20to%20sample%20new%20synthetic%20relational%20data%20points%20from%20its%20underlying%0Aprobability%20distribution.%20As%20part%20of%20our%20proposed%20pipeline%2C%20we%20introduce%20a%0Alearning%20algorithm%20to%20construct%20a%20probabilistic%20relational%20model%20from%20a%20given%0Arelational%20database.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04194v1&entry.124074799=Read"},
{"title": "Rico: extended TIAGo robot towards up-to-date social and assistive robot\n  usage scenarios", "author": "Tomasz Winiarski and Wojciech Dudek and Daniel Gie\u0142dowski", "abstract": "  Social and assistive robotics have vastly increased in popularity in recent\nyears. Due to the wide range of usage, robots executing such tasks must be\nhighly reliable and possess enough functions to satisfy multiple scenarios.\nThis article describes a mobile, artificial intelligence-driven, robotic\nplatform Rico. Its prior usage in similar scenarios, the number of its\ncapabilities, and the experiments it presented should qualify it as a proper\narm-less platform for social and assistive circumstances.\n", "link": "http://arxiv.org/abs/2407.21401v3", "date": "2024-09-06", "relevancy": 1.4198, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5077}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4657}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rico%3A%20extended%20TIAGo%20robot%20towards%20up-to-date%20social%20and%20assistive%20robot%0A%20%20usage%20scenarios&body=Title%3A%20Rico%3A%20extended%20TIAGo%20robot%20towards%20up-to-date%20social%20and%20assistive%20robot%0A%20%20usage%20scenarios%0AAuthor%3A%20Tomasz%20Winiarski%20and%20Wojciech%20Dudek%20and%20Daniel%20Gie%C5%82dowski%0AAbstract%3A%20%20%20Social%20and%20assistive%20robotics%20have%20vastly%20increased%20in%20popularity%20in%20recent%0Ayears.%20Due%20to%20the%20wide%20range%20of%20usage%2C%20robots%20executing%20such%20tasks%20must%20be%0Ahighly%20reliable%20and%20possess%20enough%20functions%20to%20satisfy%20multiple%20scenarios.%0AThis%20article%20describes%20a%20mobile%2C%20artificial%20intelligence-driven%2C%20robotic%0Aplatform%20Rico.%20Its%20prior%20usage%20in%20similar%20scenarios%2C%20the%20number%20of%20its%0Acapabilities%2C%20and%20the%20experiments%20it%20presented%20should%20qualify%20it%20as%20a%20proper%0Aarm-less%20platform%20for%20social%20and%20assistive%20circumstances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21401v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRico%253A%2520extended%2520TIAGo%2520robot%2520towards%2520up-to-date%2520social%2520and%2520assistive%2520robot%250A%2520%2520usage%2520scenarios%26entry.906535625%3DTomasz%2520Winiarski%2520and%2520Wojciech%2520Dudek%2520and%2520Daniel%2520Gie%25C5%2582dowski%26entry.1292438233%3D%2520%2520Social%2520and%2520assistive%2520robotics%2520have%2520vastly%2520increased%2520in%2520popularity%2520in%2520recent%250Ayears.%2520Due%2520to%2520the%2520wide%2520range%2520of%2520usage%252C%2520robots%2520executing%2520such%2520tasks%2520must%2520be%250Ahighly%2520reliable%2520and%2520possess%2520enough%2520functions%2520to%2520satisfy%2520multiple%2520scenarios.%250AThis%2520article%2520describes%2520a%2520mobile%252C%2520artificial%2520intelligence-driven%252C%2520robotic%250Aplatform%2520Rico.%2520Its%2520prior%2520usage%2520in%2520similar%2520scenarios%252C%2520the%2520number%2520of%2520its%250Acapabilities%252C%2520and%2520the%2520experiments%2520it%2520presented%2520should%2520qualify%2520it%2520as%2520a%2520proper%250Aarm-less%2520platform%2520for%2520social%2520and%2520assistive%2520circumstances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21401v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rico%3A%20extended%20TIAGo%20robot%20towards%20up-to-date%20social%20and%20assistive%20robot%0A%20%20usage%20scenarios&entry.906535625=Tomasz%20Winiarski%20and%20Wojciech%20Dudek%20and%20Daniel%20Gie%C5%82dowski&entry.1292438233=%20%20Social%20and%20assistive%20robotics%20have%20vastly%20increased%20in%20popularity%20in%20recent%0Ayears.%20Due%20to%20the%20wide%20range%20of%20usage%2C%20robots%20executing%20such%20tasks%20must%20be%0Ahighly%20reliable%20and%20possess%20enough%20functions%20to%20satisfy%20multiple%20scenarios.%0AThis%20article%20describes%20a%20mobile%2C%20artificial%20intelligence-driven%2C%20robotic%0Aplatform%20Rico.%20Its%20prior%20usage%20in%20similar%20scenarios%2C%20the%20number%20of%20its%0Acapabilities%2C%20and%20the%20experiments%20it%20presented%20should%20qualify%20it%20as%20a%20proper%0Aarm-less%20platform%20for%20social%20and%20assistive%20circumstances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21401v3&entry.124074799=Read"},
{"title": "CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance\n  Survival Analysis", "author": "William Knottenbelt and Zeyu Gao and Rebecca Wray and Woody Zhidong Zhang and Jiashuai Liu and Mireia Crispin-Ortuzar", "abstract": "  Survival analysis is a branch of statistics used for modeling the time until\na specific event occurs and is widely used in medicine, engineering, finance,\nand many other fields. When choosing survival models, there is typically a\ntrade-off between performance and interpretability, where the highest\nperformance is achieved by black-box models based on deep learning. This is a\nmajor problem in fields such as medicine where practitioners are reluctant to\nblindly trust black-box models to make important patient decisions.\nKolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable\nand accurate alternative to multi-layer perceptrons (MLPs). We introduce\nCoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable,\nhigh-performance survival analysis. We evaluate the proposed CoxKAN on 4\nsynthetic datasets and 9 real medical datasets. The synthetic experiments\ndemonstrate that CoxKAN accurately recovers interpretable symbolic formulae for\nthe hazard function, and effectively performs automatic feature selection.\nEvaluation on the 9 real datasets show that CoxKAN consistently outperforms the\nCox proportional hazards model and achieves performance that is superior or\ncomparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies\ncomplex interactions between predictor variables that would be extremely\ndifficult to recognise using existing survival methods, and automatically finds\nsymbolic formulae which uncover the precise effect of important biomarkers on\npatient risk.\n", "link": "http://arxiv.org/abs/2409.04290v1", "date": "2024-09-06", "relevancy": 1.284, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4483}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4296}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoxKAN%3A%20Kolmogorov-Arnold%20Networks%20for%20Interpretable%2C%20High-Performance%0A%20%20Survival%20Analysis&body=Title%3A%20CoxKAN%3A%20Kolmogorov-Arnold%20Networks%20for%20Interpretable%2C%20High-Performance%0A%20%20Survival%20Analysis%0AAuthor%3A%20William%20Knottenbelt%20and%20Zeyu%20Gao%20and%20Rebecca%20Wray%20and%20Woody%20Zhidong%20Zhang%20and%20Jiashuai%20Liu%20and%20Mireia%20Crispin-Ortuzar%0AAbstract%3A%20%20%20Survival%20analysis%20is%20a%20branch%20of%20statistics%20used%20for%20modeling%20the%20time%20until%0Aa%20specific%20event%20occurs%20and%20is%20widely%20used%20in%20medicine%2C%20engineering%2C%20finance%2C%0Aand%20many%20other%20fields.%20When%20choosing%20survival%20models%2C%20there%20is%20typically%20a%0Atrade-off%20between%20performance%20and%20interpretability%2C%20where%20the%20highest%0Aperformance%20is%20achieved%20by%20black-box%20models%20based%20on%20deep%20learning.%20This%20is%20a%0Amajor%20problem%20in%20fields%20such%20as%20medicine%20where%20practitioners%20are%20reluctant%20to%0Ablindly%20trust%20black-box%20models%20to%20make%20important%20patient%20decisions.%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20were%20recently%20proposed%20as%20an%20interpretable%0Aand%20accurate%20alternative%20to%20multi-layer%20perceptrons%20%28MLPs%29.%20We%20introduce%0ACoxKAN%2C%20a%20Cox%20proportional%20hazards%20Kolmogorov-Arnold%20Network%20for%20interpretable%2C%0Ahigh-performance%20survival%20analysis.%20We%20evaluate%20the%20proposed%20CoxKAN%20on%204%0Asynthetic%20datasets%20and%209%20real%20medical%20datasets.%20The%20synthetic%20experiments%0Ademonstrate%20that%20CoxKAN%20accurately%20recovers%20interpretable%20symbolic%20formulae%20for%0Athe%20hazard%20function%2C%20and%20effectively%20performs%20automatic%20feature%20selection.%0AEvaluation%20on%20the%209%20real%20datasets%20show%20that%20CoxKAN%20consistently%20outperforms%20the%0ACox%20proportional%20hazards%20model%20and%20achieves%20performance%20that%20is%20superior%20or%0Acomparable%20to%20that%20of%20tuned%20MLPs.%20Furthermore%2C%20we%20find%20that%20CoxKAN%20identifies%0Acomplex%20interactions%20between%20predictor%20variables%20that%20would%20be%20extremely%0Adifficult%20to%20recognise%20using%20existing%20survival%20methods%2C%20and%20automatically%20finds%0Asymbolic%20formulae%20which%20uncover%20the%20precise%20effect%20of%20important%20biomarkers%20on%0Apatient%20risk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoxKAN%253A%2520Kolmogorov-Arnold%2520Networks%2520for%2520Interpretable%252C%2520High-Performance%250A%2520%2520Survival%2520Analysis%26entry.906535625%3DWilliam%2520Knottenbelt%2520and%2520Zeyu%2520Gao%2520and%2520Rebecca%2520Wray%2520and%2520Woody%2520Zhidong%2520Zhang%2520and%2520Jiashuai%2520Liu%2520and%2520Mireia%2520Crispin-Ortuzar%26entry.1292438233%3D%2520%2520Survival%2520analysis%2520is%2520a%2520branch%2520of%2520statistics%2520used%2520for%2520modeling%2520the%2520time%2520until%250Aa%2520specific%2520event%2520occurs%2520and%2520is%2520widely%2520used%2520in%2520medicine%252C%2520engineering%252C%2520finance%252C%250Aand%2520many%2520other%2520fields.%2520When%2520choosing%2520survival%2520models%252C%2520there%2520is%2520typically%2520a%250Atrade-off%2520between%2520performance%2520and%2520interpretability%252C%2520where%2520the%2520highest%250Aperformance%2520is%2520achieved%2520by%2520black-box%2520models%2520based%2520on%2520deep%2520learning.%2520This%2520is%2520a%250Amajor%2520problem%2520in%2520fields%2520such%2520as%2520medicine%2520where%2520practitioners%2520are%2520reluctant%2520to%250Ablindly%2520trust%2520black-box%2520models%2520to%2520make%2520important%2520patient%2520decisions.%250AKolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520were%2520recently%2520proposed%2520as%2520an%2520interpretable%250Aand%2520accurate%2520alternative%2520to%2520multi-layer%2520perceptrons%2520%2528MLPs%2529.%2520We%2520introduce%250ACoxKAN%252C%2520a%2520Cox%2520proportional%2520hazards%2520Kolmogorov-Arnold%2520Network%2520for%2520interpretable%252C%250Ahigh-performance%2520survival%2520analysis.%2520We%2520evaluate%2520the%2520proposed%2520CoxKAN%2520on%25204%250Asynthetic%2520datasets%2520and%25209%2520real%2520medical%2520datasets.%2520The%2520synthetic%2520experiments%250Ademonstrate%2520that%2520CoxKAN%2520accurately%2520recovers%2520interpretable%2520symbolic%2520formulae%2520for%250Athe%2520hazard%2520function%252C%2520and%2520effectively%2520performs%2520automatic%2520feature%2520selection.%250AEvaluation%2520on%2520the%25209%2520real%2520datasets%2520show%2520that%2520CoxKAN%2520consistently%2520outperforms%2520the%250ACox%2520proportional%2520hazards%2520model%2520and%2520achieves%2520performance%2520that%2520is%2520superior%2520or%250Acomparable%2520to%2520that%2520of%2520tuned%2520MLPs.%2520Furthermore%252C%2520we%2520find%2520that%2520CoxKAN%2520identifies%250Acomplex%2520interactions%2520between%2520predictor%2520variables%2520that%2520would%2520be%2520extremely%250Adifficult%2520to%2520recognise%2520using%2520existing%2520survival%2520methods%252C%2520and%2520automatically%2520finds%250Asymbolic%2520formulae%2520which%2520uncover%2520the%2520precise%2520effect%2520of%2520important%2520biomarkers%2520on%250Apatient%2520risk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoxKAN%3A%20Kolmogorov-Arnold%20Networks%20for%20Interpretable%2C%20High-Performance%0A%20%20Survival%20Analysis&entry.906535625=William%20Knottenbelt%20and%20Zeyu%20Gao%20and%20Rebecca%20Wray%20and%20Woody%20Zhidong%20Zhang%20and%20Jiashuai%20Liu%20and%20Mireia%20Crispin-Ortuzar&entry.1292438233=%20%20Survival%20analysis%20is%20a%20branch%20of%20statistics%20used%20for%20modeling%20the%20time%20until%0Aa%20specific%20event%20occurs%20and%20is%20widely%20used%20in%20medicine%2C%20engineering%2C%20finance%2C%0Aand%20many%20other%20fields.%20When%20choosing%20survival%20models%2C%20there%20is%20typically%20a%0Atrade-off%20between%20performance%20and%20interpretability%2C%20where%20the%20highest%0Aperformance%20is%20achieved%20by%20black-box%20models%20based%20on%20deep%20learning.%20This%20is%20a%0Amajor%20problem%20in%20fields%20such%20as%20medicine%20where%20practitioners%20are%20reluctant%20to%0Ablindly%20trust%20black-box%20models%20to%20make%20important%20patient%20decisions.%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20were%20recently%20proposed%20as%20an%20interpretable%0Aand%20accurate%20alternative%20to%20multi-layer%20perceptrons%20%28MLPs%29.%20We%20introduce%0ACoxKAN%2C%20a%20Cox%20proportional%20hazards%20Kolmogorov-Arnold%20Network%20for%20interpretable%2C%0Ahigh-performance%20survival%20analysis.%20We%20evaluate%20the%20proposed%20CoxKAN%20on%204%0Asynthetic%20datasets%20and%209%20real%20medical%20datasets.%20The%20synthetic%20experiments%0Ademonstrate%20that%20CoxKAN%20accurately%20recovers%20interpretable%20symbolic%20formulae%20for%0Athe%20hazard%20function%2C%20and%20effectively%20performs%20automatic%20feature%20selection.%0AEvaluation%20on%20the%209%20real%20datasets%20show%20that%20CoxKAN%20consistently%20outperforms%20the%0ACox%20proportional%20hazards%20model%20and%20achieves%20performance%20that%20is%20superior%20or%0Acomparable%20to%20that%20of%20tuned%20MLPs.%20Furthermore%2C%20we%20find%20that%20CoxKAN%20identifies%0Acomplex%20interactions%20between%20predictor%20variables%20that%20would%20be%20extremely%0Adifficult%20to%20recognise%20using%20existing%20survival%20methods%2C%20and%20automatically%20finds%0Asymbolic%20formulae%20which%20uncover%20the%20precise%20effect%20of%20important%20biomarkers%20on%0Apatient%20risk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04290v1&entry.124074799=Read"},
{"title": "Imitation learning for sim-to-real transfer of robotic cutting policies\n  based on residual Gaussian process disturbance force model", "author": "Jamie Hathaway and Rustam Stolkin and Alireza Rastegarpanah", "abstract": "  Robotic cutting, or milling, plays a significant role in applications such as\ndisassembly, decommissioning, and demolition. Planning and control of cutting\nin real-world scenarios in uncertain environments is a complex task, with the\npotential to benefit from simulated training environments. This letter focuses\non sim-to-real transfer for robotic cutting policies, addressing the need for\neffective policy transfer from simulation to practical implementation. We\nextend our previous domain generalisation approach to learning cutting tasks\nbased on a mechanistic model-based simulation framework, by proposing a hybrid\napproach for sim-to-real transfer based on a milling process force model and\nresidual Gaussian process (GP) force model, learned from either single or\nmultiple real-world cutting force examples. We demonstrate successful\nsim-to-real transfer of a robotic cutting policy without the need for\nfine-tuning on the real robot setup. The proposed approach autonomously adapts\nto materials with differing structural and mechanical properties. Furthermore,\nwe demonstrate the proposed method outperforms fine-tuning or re-training\nalone.\n", "link": "http://arxiv.org/abs/2311.04096v2", "date": "2024-09-06", "relevancy": 1.5781, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5792}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5134}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitation%20learning%20for%20sim-to-real%20transfer%20of%20robotic%20cutting%20policies%0A%20%20based%20on%20residual%20Gaussian%20process%20disturbance%20force%20model&body=Title%3A%20Imitation%20learning%20for%20sim-to-real%20transfer%20of%20robotic%20cutting%20policies%0A%20%20based%20on%20residual%20Gaussian%20process%20disturbance%20force%20model%0AAuthor%3A%20Jamie%20Hathaway%20and%20Rustam%20Stolkin%20and%20Alireza%20Rastegarpanah%0AAbstract%3A%20%20%20Robotic%20cutting%2C%20or%20milling%2C%20plays%20a%20significant%20role%20in%20applications%20such%20as%0Adisassembly%2C%20decommissioning%2C%20and%20demolition.%20Planning%20and%20control%20of%20cutting%0Ain%20real-world%20scenarios%20in%20uncertain%20environments%20is%20a%20complex%20task%2C%20with%20the%0Apotential%20to%20benefit%20from%20simulated%20training%20environments.%20This%20letter%20focuses%0Aon%20sim-to-real%20transfer%20for%20robotic%20cutting%20policies%2C%20addressing%20the%20need%20for%0Aeffective%20policy%20transfer%20from%20simulation%20to%20practical%20implementation.%20We%0Aextend%20our%20previous%20domain%20generalisation%20approach%20to%20learning%20cutting%20tasks%0Abased%20on%20a%20mechanistic%20model-based%20simulation%20framework%2C%20by%20proposing%20a%20hybrid%0Aapproach%20for%20sim-to-real%20transfer%20based%20on%20a%20milling%20process%20force%20model%20and%0Aresidual%20Gaussian%20process%20%28GP%29%20force%20model%2C%20learned%20from%20either%20single%20or%0Amultiple%20real-world%20cutting%20force%20examples.%20We%20demonstrate%20successful%0Asim-to-real%20transfer%20of%20a%20robotic%20cutting%20policy%20without%20the%20need%20for%0Afine-tuning%20on%20the%20real%20robot%20setup.%20The%20proposed%20approach%20autonomously%20adapts%0Ato%20materials%20with%20differing%20structural%20and%20mechanical%20properties.%20Furthermore%2C%0Awe%20demonstrate%20the%20proposed%20method%20outperforms%20fine-tuning%20or%20re-training%0Aalone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04096v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitation%2520learning%2520for%2520sim-to-real%2520transfer%2520of%2520robotic%2520cutting%2520policies%250A%2520%2520based%2520on%2520residual%2520Gaussian%2520process%2520disturbance%2520force%2520model%26entry.906535625%3DJamie%2520Hathaway%2520and%2520Rustam%2520Stolkin%2520and%2520Alireza%2520Rastegarpanah%26entry.1292438233%3D%2520%2520Robotic%2520cutting%252C%2520or%2520milling%252C%2520plays%2520a%2520significant%2520role%2520in%2520applications%2520such%2520as%250Adisassembly%252C%2520decommissioning%252C%2520and%2520demolition.%2520Planning%2520and%2520control%2520of%2520cutting%250Ain%2520real-world%2520scenarios%2520in%2520uncertain%2520environments%2520is%2520a%2520complex%2520task%252C%2520with%2520the%250Apotential%2520to%2520benefit%2520from%2520simulated%2520training%2520environments.%2520This%2520letter%2520focuses%250Aon%2520sim-to-real%2520transfer%2520for%2520robotic%2520cutting%2520policies%252C%2520addressing%2520the%2520need%2520for%250Aeffective%2520policy%2520transfer%2520from%2520simulation%2520to%2520practical%2520implementation.%2520We%250Aextend%2520our%2520previous%2520domain%2520generalisation%2520approach%2520to%2520learning%2520cutting%2520tasks%250Abased%2520on%2520a%2520mechanistic%2520model-based%2520simulation%2520framework%252C%2520by%2520proposing%2520a%2520hybrid%250Aapproach%2520for%2520sim-to-real%2520transfer%2520based%2520on%2520a%2520milling%2520process%2520force%2520model%2520and%250Aresidual%2520Gaussian%2520process%2520%2528GP%2529%2520force%2520model%252C%2520learned%2520from%2520either%2520single%2520or%250Amultiple%2520real-world%2520cutting%2520force%2520examples.%2520We%2520demonstrate%2520successful%250Asim-to-real%2520transfer%2520of%2520a%2520robotic%2520cutting%2520policy%2520without%2520the%2520need%2520for%250Afine-tuning%2520on%2520the%2520real%2520robot%2520setup.%2520The%2520proposed%2520approach%2520autonomously%2520adapts%250Ato%2520materials%2520with%2520differing%2520structural%2520and%2520mechanical%2520properties.%2520Furthermore%252C%250Awe%2520demonstrate%2520the%2520proposed%2520method%2520outperforms%2520fine-tuning%2520or%2520re-training%250Aalone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04096v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitation%20learning%20for%20sim-to-real%20transfer%20of%20robotic%20cutting%20policies%0A%20%20based%20on%20residual%20Gaussian%20process%20disturbance%20force%20model&entry.906535625=Jamie%20Hathaway%20and%20Rustam%20Stolkin%20and%20Alireza%20Rastegarpanah&entry.1292438233=%20%20Robotic%20cutting%2C%20or%20milling%2C%20plays%20a%20significant%20role%20in%20applications%20such%20as%0Adisassembly%2C%20decommissioning%2C%20and%20demolition.%20Planning%20and%20control%20of%20cutting%0Ain%20real-world%20scenarios%20in%20uncertain%20environments%20is%20a%20complex%20task%2C%20with%20the%0Apotential%20to%20benefit%20from%20simulated%20training%20environments.%20This%20letter%20focuses%0Aon%20sim-to-real%20transfer%20for%20robotic%20cutting%20policies%2C%20addressing%20the%20need%20for%0Aeffective%20policy%20transfer%20from%20simulation%20to%20practical%20implementation.%20We%0Aextend%20our%20previous%20domain%20generalisation%20approach%20to%20learning%20cutting%20tasks%0Abased%20on%20a%20mechanistic%20model-based%20simulation%20framework%2C%20by%20proposing%20a%20hybrid%0Aapproach%20for%20sim-to-real%20transfer%20based%20on%20a%20milling%20process%20force%20model%20and%0Aresidual%20Gaussian%20process%20%28GP%29%20force%20model%2C%20learned%20from%20either%20single%20or%0Amultiple%20real-world%20cutting%20force%20examples.%20We%20demonstrate%20successful%0Asim-to-real%20transfer%20of%20a%20robotic%20cutting%20policy%20without%20the%20need%20for%0Afine-tuning%20on%20the%20real%20robot%20setup.%20The%20proposed%20approach%20autonomously%20adapts%0Ato%20materials%20with%20differing%20structural%20and%20mechanical%20properties.%20Furthermore%2C%0Awe%20demonstrate%20the%20proposed%20method%20outperforms%20fine-tuning%20or%20re-training%0Aalone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04096v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


