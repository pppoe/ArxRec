<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250203.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Reflective Gaussian Splatting", "author": "Yuxuan Yao and Zixuan Zeng and Chun Gu and Xiatian Zhu and Li Zhang", "abstract": "  Novel view synthesis has experienced significant advancements owing to\nincreasingly capable NeRF- and 3DGS-based methods. However, reflective object\nreconstruction remains challenging, lacking a proper solution to achieve\nreal-time, high-quality rendering while accommodating inter-reflection. To fill\nthis gap, we introduce a Reflective Gaussian splatting (Ref-Gaussian) framework\ncharacterized with two components: (I) Physically based deferred rendering that\nempowers the rendering equation with pixel-level material properties via\nformulating split-sum approximation; (II) Gaussian-grounded inter-reflection\nthat realizes the desired inter-reflection function within a Gaussian splatting\nparadigm for the first time. To enhance geometry modeling, we further introduce\nmaterial-aware normal propagation and an initial per-Gaussian shading stage,\nalong with 2D Gaussian primitives. Extensive experiments on standard datasets\ndemonstrate that Ref-Gaussian surpasses existing approaches in terms of\nquantitative metrics, visual quality, and compute efficiency. Further, we show\nthat our method serves as a unified solution for both reflective and\nnon-reflective scenes, going beyond the previous alternatives focusing on only\nreflective scenes. Also, we illustrate that Ref-Gaussian supports more\napplications such as relighting and editing.\n", "link": "http://arxiv.org/abs/2412.19282v2", "date": "2025-02-03", "relevancy": 3.4135, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.717}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.7123}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reflective%20Gaussian%20Splatting&body=Title%3A%20Reflective%20Gaussian%20Splatting%0AAuthor%3A%20Yuxuan%20Yao%20and%20Zixuan%20Zeng%20and%20Chun%20Gu%20and%20Xiatian%20Zhu%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20has%20experienced%20significant%20advancements%20owing%20to%0Aincreasingly%20capable%20NeRF-%20and%203DGS-based%20methods.%20However%2C%20reflective%20object%0Areconstruction%20remains%20challenging%2C%20lacking%20a%20proper%20solution%20to%20achieve%0Areal-time%2C%20high-quality%20rendering%20while%20accommodating%20inter-reflection.%20To%20fill%0Athis%20gap%2C%20we%20introduce%20a%20Reflective%20Gaussian%20splatting%20%28Ref-Gaussian%29%20framework%0Acharacterized%20with%20two%20components%3A%20%28I%29%20Physically%20based%20deferred%20rendering%20that%0Aempowers%20the%20rendering%20equation%20with%20pixel-level%20material%20properties%20via%0Aformulating%20split-sum%20approximation%3B%20%28II%29%20Gaussian-grounded%20inter-reflection%0Athat%20realizes%20the%20desired%20inter-reflection%20function%20within%20a%20Gaussian%20splatting%0Aparadigm%20for%20the%20first%20time.%20To%20enhance%20geometry%20modeling%2C%20we%20further%20introduce%0Amaterial-aware%20normal%20propagation%20and%20an%20initial%20per-Gaussian%20shading%20stage%2C%0Aalong%20with%202D%20Gaussian%20primitives.%20Extensive%20experiments%20on%20standard%20datasets%0Ademonstrate%20that%20Ref-Gaussian%20surpasses%20existing%20approaches%20in%20terms%20of%0Aquantitative%20metrics%2C%20visual%20quality%2C%20and%20compute%20efficiency.%20Further%2C%20we%20show%0Athat%20our%20method%20serves%20as%20a%20unified%20solution%20for%20both%20reflective%20and%0Anon-reflective%20scenes%2C%20going%20beyond%20the%20previous%20alternatives%20focusing%20on%20only%0Areflective%20scenes.%20Also%2C%20we%20illustrate%20that%20Ref-Gaussian%20supports%20more%0Aapplications%20such%20as%20relighting%20and%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19282v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReflective%2520Gaussian%2520Splatting%26entry.906535625%3DYuxuan%2520Yao%2520and%2520Zixuan%2520Zeng%2520and%2520Chun%2520Gu%2520and%2520Xiatian%2520Zhu%2520and%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520has%2520experienced%2520significant%2520advancements%2520owing%2520to%250Aincreasingly%2520capable%2520NeRF-%2520and%25203DGS-based%2520methods.%2520However%252C%2520reflective%2520object%250Areconstruction%2520remains%2520challenging%252C%2520lacking%2520a%2520proper%2520solution%2520to%2520achieve%250Areal-time%252C%2520high-quality%2520rendering%2520while%2520accommodating%2520inter-reflection.%2520To%2520fill%250Athis%2520gap%252C%2520we%2520introduce%2520a%2520Reflective%2520Gaussian%2520splatting%2520%2528Ref-Gaussian%2529%2520framework%250Acharacterized%2520with%2520two%2520components%253A%2520%2528I%2529%2520Physically%2520based%2520deferred%2520rendering%2520that%250Aempowers%2520the%2520rendering%2520equation%2520with%2520pixel-level%2520material%2520properties%2520via%250Aformulating%2520split-sum%2520approximation%253B%2520%2528II%2529%2520Gaussian-grounded%2520inter-reflection%250Athat%2520realizes%2520the%2520desired%2520inter-reflection%2520function%2520within%2520a%2520Gaussian%2520splatting%250Aparadigm%2520for%2520the%2520first%2520time.%2520To%2520enhance%2520geometry%2520modeling%252C%2520we%2520further%2520introduce%250Amaterial-aware%2520normal%2520propagation%2520and%2520an%2520initial%2520per-Gaussian%2520shading%2520stage%252C%250Aalong%2520with%25202D%2520Gaussian%2520primitives.%2520Extensive%2520experiments%2520on%2520standard%2520datasets%250Ademonstrate%2520that%2520Ref-Gaussian%2520surpasses%2520existing%2520approaches%2520in%2520terms%2520of%250Aquantitative%2520metrics%252C%2520visual%2520quality%252C%2520and%2520compute%2520efficiency.%2520Further%252C%2520we%2520show%250Athat%2520our%2520method%2520serves%2520as%2520a%2520unified%2520solution%2520for%2520both%2520reflective%2520and%250Anon-reflective%2520scenes%252C%2520going%2520beyond%2520the%2520previous%2520alternatives%2520focusing%2520on%2520only%250Areflective%2520scenes.%2520Also%252C%2520we%2520illustrate%2520that%2520Ref-Gaussian%2520supports%2520more%250Aapplications%2520such%2520as%2520relighting%2520and%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19282v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reflective%20Gaussian%20Splatting&entry.906535625=Yuxuan%20Yao%20and%20Zixuan%20Zeng%20and%20Chun%20Gu%20and%20Xiatian%20Zhu%20and%20Li%20Zhang&entry.1292438233=%20%20Novel%20view%20synthesis%20has%20experienced%20significant%20advancements%20owing%20to%0Aincreasingly%20capable%20NeRF-%20and%203DGS-based%20methods.%20However%2C%20reflective%20object%0Areconstruction%20remains%20challenging%2C%20lacking%20a%20proper%20solution%20to%20achieve%0Areal-time%2C%20high-quality%20rendering%20while%20accommodating%20inter-reflection.%20To%20fill%0Athis%20gap%2C%20we%20introduce%20a%20Reflective%20Gaussian%20splatting%20%28Ref-Gaussian%29%20framework%0Acharacterized%20with%20two%20components%3A%20%28I%29%20Physically%20based%20deferred%20rendering%20that%0Aempowers%20the%20rendering%20equation%20with%20pixel-level%20material%20properties%20via%0Aformulating%20split-sum%20approximation%3B%20%28II%29%20Gaussian-grounded%20inter-reflection%0Athat%20realizes%20the%20desired%20inter-reflection%20function%20within%20a%20Gaussian%20splatting%0Aparadigm%20for%20the%20first%20time.%20To%20enhance%20geometry%20modeling%2C%20we%20further%20introduce%0Amaterial-aware%20normal%20propagation%20and%20an%20initial%20per-Gaussian%20shading%20stage%2C%0Aalong%20with%202D%20Gaussian%20primitives.%20Extensive%20experiments%20on%20standard%20datasets%0Ademonstrate%20that%20Ref-Gaussian%20surpasses%20existing%20approaches%20in%20terms%20of%0Aquantitative%20metrics%2C%20visual%20quality%2C%20and%20compute%20efficiency.%20Further%2C%20we%20show%0Athat%20our%20method%20serves%20as%20a%20unified%20solution%20for%20both%20reflective%20and%0Anon-reflective%20scenes%2C%20going%20beyond%20the%20previous%20alternatives%20focusing%20on%20only%0Areflective%20scenes.%20Also%2C%20we%20illustrate%20that%20Ref-Gaussian%20supports%20more%0Aapplications%20such%20as%20relighting%20and%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19282v2&entry.124074799=Read"},
{"title": "CityLoc: 6DoF Pose Distributional Localization for Text Descriptions in\n  Large-Scale Scenes with Gaussian Representation", "author": "Qi Ma and Runyi Yang and Bin Ren and Nicu Sebe and Ender Konukoglu and Luc Van Gool and Danda Pani Paudel", "abstract": "  Localizing textual descriptions within large-scale 3D scenes presents\ninherent ambiguities, such as identifying all traffic lights in a city.\nAddressing this, we introduce a method to generate distributions of camera\nposes conditioned on textual descriptions, facilitating robust reasoning for\nbroadly defined concepts.\n  Our approach employs a diffusion-based architecture to refine noisy 6DoF\ncamera poses towards plausible locations, with conditional signals derived from\npre-trained text encoders. Integration with the pretrained Vision-Language\nModel, CLIP, establishes a strong linkage between text descriptions and pose\ndistributions. Enhancement of localization accuracy is achieved by rendering\ncandidate poses using 3D Gaussian splatting, which corrects misaligned samples\nthrough visual reasoning.\n  We validate our method's superiority by comparing it against standard\ndistribution estimation methods across five large-scale datasets, demonstrating\nconsistent outperformance. Code, datasets and more information will be publicly\navailable at our project page.\n", "link": "http://arxiv.org/abs/2501.08982v2", "date": "2025-02-03", "relevancy": 2.9561, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.595}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.593}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CityLoc%3A%206DoF%20Pose%20Distributional%20Localization%20for%20Text%20Descriptions%20in%0A%20%20Large-Scale%20Scenes%20with%20Gaussian%20Representation&body=Title%3A%20CityLoc%3A%206DoF%20Pose%20Distributional%20Localization%20for%20Text%20Descriptions%20in%0A%20%20Large-Scale%20Scenes%20with%20Gaussian%20Representation%0AAuthor%3A%20Qi%20Ma%20and%20Runyi%20Yang%20and%20Bin%20Ren%20and%20Nicu%20Sebe%20and%20Ender%20Konukoglu%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel%0AAbstract%3A%20%20%20Localizing%20textual%20descriptions%20within%20large-scale%203D%20scenes%20presents%0Ainherent%20ambiguities%2C%20such%20as%20identifying%20all%20traffic%20lights%20in%20a%20city.%0AAddressing%20this%2C%20we%20introduce%20a%20method%20to%20generate%20distributions%20of%20camera%0Aposes%20conditioned%20on%20textual%20descriptions%2C%20facilitating%20robust%20reasoning%20for%0Abroadly%20defined%20concepts.%0A%20%20Our%20approach%20employs%20a%20diffusion-based%20architecture%20to%20refine%20noisy%206DoF%0Acamera%20poses%20towards%20plausible%20locations%2C%20with%20conditional%20signals%20derived%20from%0Apre-trained%20text%20encoders.%20Integration%20with%20the%20pretrained%20Vision-Language%0AModel%2C%20CLIP%2C%20establishes%20a%20strong%20linkage%20between%20text%20descriptions%20and%20pose%0Adistributions.%20Enhancement%20of%20localization%20accuracy%20is%20achieved%20by%20rendering%0Acandidate%20poses%20using%203D%20Gaussian%20splatting%2C%20which%20corrects%20misaligned%20samples%0Athrough%20visual%20reasoning.%0A%20%20We%20validate%20our%20method%27s%20superiority%20by%20comparing%20it%20against%20standard%0Adistribution%20estimation%20methods%20across%20five%20large-scale%20datasets%2C%20demonstrating%0Aconsistent%20outperformance.%20Code%2C%20datasets%20and%20more%20information%20will%20be%20publicly%0Aavailable%20at%20our%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08982v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCityLoc%253A%25206DoF%2520Pose%2520Distributional%2520Localization%2520for%2520Text%2520Descriptions%2520in%250A%2520%2520Large-Scale%2520Scenes%2520with%2520Gaussian%2520Representation%26entry.906535625%3DQi%2520Ma%2520and%2520Runyi%2520Yang%2520and%2520Bin%2520Ren%2520and%2520Nicu%2520Sebe%2520and%2520Ender%2520Konukoglu%2520and%2520Luc%2520Van%2520Gool%2520and%2520Danda%2520Pani%2520Paudel%26entry.1292438233%3D%2520%2520Localizing%2520textual%2520descriptions%2520within%2520large-scale%25203D%2520scenes%2520presents%250Ainherent%2520ambiguities%252C%2520such%2520as%2520identifying%2520all%2520traffic%2520lights%2520in%2520a%2520city.%250AAddressing%2520this%252C%2520we%2520introduce%2520a%2520method%2520to%2520generate%2520distributions%2520of%2520camera%250Aposes%2520conditioned%2520on%2520textual%2520descriptions%252C%2520facilitating%2520robust%2520reasoning%2520for%250Abroadly%2520defined%2520concepts.%250A%2520%2520Our%2520approach%2520employs%2520a%2520diffusion-based%2520architecture%2520to%2520refine%2520noisy%25206DoF%250Acamera%2520poses%2520towards%2520plausible%2520locations%252C%2520with%2520conditional%2520signals%2520derived%2520from%250Apre-trained%2520text%2520encoders.%2520Integration%2520with%2520the%2520pretrained%2520Vision-Language%250AModel%252C%2520CLIP%252C%2520establishes%2520a%2520strong%2520linkage%2520between%2520text%2520descriptions%2520and%2520pose%250Adistributions.%2520Enhancement%2520of%2520localization%2520accuracy%2520is%2520achieved%2520by%2520rendering%250Acandidate%2520poses%2520using%25203D%2520Gaussian%2520splatting%252C%2520which%2520corrects%2520misaligned%2520samples%250Athrough%2520visual%2520reasoning.%250A%2520%2520We%2520validate%2520our%2520method%2527s%2520superiority%2520by%2520comparing%2520it%2520against%2520standard%250Adistribution%2520estimation%2520methods%2520across%2520five%2520large-scale%2520datasets%252C%2520demonstrating%250Aconsistent%2520outperformance.%2520Code%252C%2520datasets%2520and%2520more%2520information%2520will%2520be%2520publicly%250Aavailable%2520at%2520our%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08982v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CityLoc%3A%206DoF%20Pose%20Distributional%20Localization%20for%20Text%20Descriptions%20in%0A%20%20Large-Scale%20Scenes%20with%20Gaussian%20Representation&entry.906535625=Qi%20Ma%20and%20Runyi%20Yang%20and%20Bin%20Ren%20and%20Nicu%20Sebe%20and%20Ender%20Konukoglu%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel&entry.1292438233=%20%20Localizing%20textual%20descriptions%20within%20large-scale%203D%20scenes%20presents%0Ainherent%20ambiguities%2C%20such%20as%20identifying%20all%20traffic%20lights%20in%20a%20city.%0AAddressing%20this%2C%20we%20introduce%20a%20method%20to%20generate%20distributions%20of%20camera%0Aposes%20conditioned%20on%20textual%20descriptions%2C%20facilitating%20robust%20reasoning%20for%0Abroadly%20defined%20concepts.%0A%20%20Our%20approach%20employs%20a%20diffusion-based%20architecture%20to%20refine%20noisy%206DoF%0Acamera%20poses%20towards%20plausible%20locations%2C%20with%20conditional%20signals%20derived%20from%0Apre-trained%20text%20encoders.%20Integration%20with%20the%20pretrained%20Vision-Language%0AModel%2C%20CLIP%2C%20establishes%20a%20strong%20linkage%20between%20text%20descriptions%20and%20pose%0Adistributions.%20Enhancement%20of%20localization%20accuracy%20is%20achieved%20by%20rendering%0Acandidate%20poses%20using%203D%20Gaussian%20splatting%2C%20which%20corrects%20misaligned%20samples%0Athrough%20visual%20reasoning.%0A%20%20We%20validate%20our%20method%27s%20superiority%20by%20comparing%20it%20against%20standard%0Adistribution%20estimation%20methods%20across%20five%20large-scale%20datasets%2C%20demonstrating%0Aconsistent%20outperformance.%20Code%2C%20datasets%20and%20more%20information%20will%20be%20publicly%0Aavailable%20at%20our%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08982v2&entry.124074799=Read"},
{"title": "Beyond Pixels: Enhancing LIME with Hierarchical Features and\n  Segmentation Foundation Models", "author": "Patrick Knab and Sascha Marton and Christian Bartelt", "abstract": "  LIME (Local Interpretable Model-agnostic Explanations) is a popular XAI\nframework for unraveling decision-making processes in vision machine-learning\nmodels. The technique utilizes image segmentation methods to identify fixed\nregions for calculating feature importance scores as explanations. Therefore,\npoor segmentation can weaken the explanation and reduce the importance of\nsegments, ultimately affecting the overall clarity of interpretation. To\naddress these challenges, we introduce the DSEG-LIME (Data-Driven Segmentation\nLIME) framework, featuring: i) a data-driven segmentation for human-recognized\nfeature generation by foundation model integration, and ii) a user-steered\ngranularity in the hierarchical segmentation procedure through composition. Our\nfindings demonstrate that DSEG outperforms on several XAI metrics on\npre-trained ImageNet models and improves the alignment of explanations with\nhuman-recognized concepts. The code is available under: https://github.\ncom/patrick-knab/DSEG-LIME\n", "link": "http://arxiv.org/abs/2403.07733v4", "date": "2025-02-03", "relevancy": 2.8438, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5857}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Pixels%3A%20Enhancing%20LIME%20with%20Hierarchical%20Features%20and%0A%20%20Segmentation%20Foundation%20Models&body=Title%3A%20Beyond%20Pixels%3A%20Enhancing%20LIME%20with%20Hierarchical%20Features%20and%0A%20%20Segmentation%20Foundation%20Models%0AAuthor%3A%20Patrick%20Knab%20and%20Sascha%20Marton%20and%20Christian%20Bartelt%0AAbstract%3A%20%20%20LIME%20%28Local%20Interpretable%20Model-agnostic%20Explanations%29%20is%20a%20popular%20XAI%0Aframework%20for%20unraveling%20decision-making%20processes%20in%20vision%20machine-learning%0Amodels.%20The%20technique%20utilizes%20image%20segmentation%20methods%20to%20identify%20fixed%0Aregions%20for%20calculating%20feature%20importance%20scores%20as%20explanations.%20Therefore%2C%0Apoor%20segmentation%20can%20weaken%20the%20explanation%20and%20reduce%20the%20importance%20of%0Asegments%2C%20ultimately%20affecting%20the%20overall%20clarity%20of%20interpretation.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20the%20DSEG-LIME%20%28Data-Driven%20Segmentation%0ALIME%29%20framework%2C%20featuring%3A%20i%29%20a%20data-driven%20segmentation%20for%20human-recognized%0Afeature%20generation%20by%20foundation%20model%20integration%2C%20and%20ii%29%20a%20user-steered%0Agranularity%20in%20the%20hierarchical%20segmentation%20procedure%20through%20composition.%20Our%0Afindings%20demonstrate%20that%20DSEG%20outperforms%20on%20several%20XAI%20metrics%20on%0Apre-trained%20ImageNet%20models%20and%20improves%20the%20alignment%20of%20explanations%20with%0Ahuman-recognized%20concepts.%20The%20code%20is%20available%20under%3A%20https%3A//github.%0Acom/patrick-knab/DSEG-LIME%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07733v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Pixels%253A%2520Enhancing%2520LIME%2520with%2520Hierarchical%2520Features%2520and%250A%2520%2520Segmentation%2520Foundation%2520Models%26entry.906535625%3DPatrick%2520Knab%2520and%2520Sascha%2520Marton%2520and%2520Christian%2520Bartelt%26entry.1292438233%3D%2520%2520LIME%2520%2528Local%2520Interpretable%2520Model-agnostic%2520Explanations%2529%2520is%2520a%2520popular%2520XAI%250Aframework%2520for%2520unraveling%2520decision-making%2520processes%2520in%2520vision%2520machine-learning%250Amodels.%2520The%2520technique%2520utilizes%2520image%2520segmentation%2520methods%2520to%2520identify%2520fixed%250Aregions%2520for%2520calculating%2520feature%2520importance%2520scores%2520as%2520explanations.%2520Therefore%252C%250Apoor%2520segmentation%2520can%2520weaken%2520the%2520explanation%2520and%2520reduce%2520the%2520importance%2520of%250Asegments%252C%2520ultimately%2520affecting%2520the%2520overall%2520clarity%2520of%2520interpretation.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520DSEG-LIME%2520%2528Data-Driven%2520Segmentation%250ALIME%2529%2520framework%252C%2520featuring%253A%2520i%2529%2520a%2520data-driven%2520segmentation%2520for%2520human-recognized%250Afeature%2520generation%2520by%2520foundation%2520model%2520integration%252C%2520and%2520ii%2529%2520a%2520user-steered%250Agranularity%2520in%2520the%2520hierarchical%2520segmentation%2520procedure%2520through%2520composition.%2520Our%250Afindings%2520demonstrate%2520that%2520DSEG%2520outperforms%2520on%2520several%2520XAI%2520metrics%2520on%250Apre-trained%2520ImageNet%2520models%2520and%2520improves%2520the%2520alignment%2520of%2520explanations%2520with%250Ahuman-recognized%2520concepts.%2520The%2520code%2520is%2520available%2520under%253A%2520https%253A//github.%250Acom/patrick-knab/DSEG-LIME%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07733v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Pixels%3A%20Enhancing%20LIME%20with%20Hierarchical%20Features%20and%0A%20%20Segmentation%20Foundation%20Models&entry.906535625=Patrick%20Knab%20and%20Sascha%20Marton%20and%20Christian%20Bartelt&entry.1292438233=%20%20LIME%20%28Local%20Interpretable%20Model-agnostic%20Explanations%29%20is%20a%20popular%20XAI%0Aframework%20for%20unraveling%20decision-making%20processes%20in%20vision%20machine-learning%0Amodels.%20The%20technique%20utilizes%20image%20segmentation%20methods%20to%20identify%20fixed%0Aregions%20for%20calculating%20feature%20importance%20scores%20as%20explanations.%20Therefore%2C%0Apoor%20segmentation%20can%20weaken%20the%20explanation%20and%20reduce%20the%20importance%20of%0Asegments%2C%20ultimately%20affecting%20the%20overall%20clarity%20of%20interpretation.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20the%20DSEG-LIME%20%28Data-Driven%20Segmentation%0ALIME%29%20framework%2C%20featuring%3A%20i%29%20a%20data-driven%20segmentation%20for%20human-recognized%0Afeature%20generation%20by%20foundation%20model%20integration%2C%20and%20ii%29%20a%20user-steered%0Agranularity%20in%20the%20hierarchical%20segmentation%20procedure%20through%20composition.%20Our%0Afindings%20demonstrate%20that%20DSEG%20outperforms%20on%20several%20XAI%20metrics%20on%0Apre-trained%20ImageNet%20models%20and%20improves%20the%20alignment%20of%20explanations%20with%0Ahuman-recognized%20concepts.%20The%20code%20is%20available%20under%3A%20https%3A//github.%0Acom/patrick-knab/DSEG-LIME%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07733v4&entry.124074799=Read"},
{"title": "A Benchmark and Evaluation for Real-World Out-of-Distribution Detection\n  Using Vision-Language Models", "author": "Shiho Noda and Atsuyuki Miyai and Qing Yu and Go Irie and Kiyoharu Aizawa", "abstract": "  Out-of-distribution (OOD) detection is a task that detects OOD samples during\ninference to ensure the safety of deployed models. However, conventional\nbenchmarks have reached performance saturation, making it difficult to compare\nrecent OOD detection methods. To address this challenge, we introduce three\nnovel OOD detection benchmarks that enable a deeper understanding of method\ncharacteristics and reflect real-world conditions. First, we present\nImageNet-X, designed to evaluate performance under challenging semantic shifts.\nSecond, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing\nrobustness to covariate shifts (feature distribution shifts). Finally, we\npropose Wilds-FS-X, which extends these evaluations to real-world datasets,\noffering a more comprehensive testbed. Our experiments reveal that recent\nCLIP-based OOD detection methods struggle to varying degrees across the three\nproposed benchmarks, and none of them consistently outperforms the others. We\nhope the community goes beyond specific benchmarks and includes more\nchallenging conditions reflecting real-world scenarios. The code is\nhttps://github.com/hoshi23/OOD-X-Benchmarks.\n", "link": "http://arxiv.org/abs/2501.18463v2", "date": "2025-02-03", "relevancy": 2.8097, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Benchmark%20and%20Evaluation%20for%20Real-World%20Out-of-Distribution%20Detection%0A%20%20Using%20Vision-Language%20Models&body=Title%3A%20A%20Benchmark%20and%20Evaluation%20for%20Real-World%20Out-of-Distribution%20Detection%0A%20%20Using%20Vision-Language%20Models%0AAuthor%3A%20Shiho%20Noda%20and%20Atsuyuki%20Miyai%20and%20Qing%20Yu%20and%20Go%20Irie%20and%20Kiyoharu%20Aizawa%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20a%20task%20that%20detects%20OOD%20samples%20during%0Ainference%20to%20ensure%20the%20safety%20of%20deployed%20models.%20However%2C%20conventional%0Abenchmarks%20have%20reached%20performance%20saturation%2C%20making%20it%20difficult%20to%20compare%0Arecent%20OOD%20detection%20methods.%20To%20address%20this%20challenge%2C%20we%20introduce%20three%0Anovel%20OOD%20detection%20benchmarks%20that%20enable%20a%20deeper%20understanding%20of%20method%0Acharacteristics%20and%20reflect%20real-world%20conditions.%20First%2C%20we%20present%0AImageNet-X%2C%20designed%20to%20evaluate%20performance%20under%20challenging%20semantic%20shifts.%0ASecond%2C%20we%20propose%20ImageNet-FS-X%20for%20full-spectrum%20OOD%20detection%2C%20assessing%0Arobustness%20to%20covariate%20shifts%20%28feature%20distribution%20shifts%29.%20Finally%2C%20we%0Apropose%20Wilds-FS-X%2C%20which%20extends%20these%20evaluations%20to%20real-world%20datasets%2C%0Aoffering%20a%20more%20comprehensive%20testbed.%20Our%20experiments%20reveal%20that%20recent%0ACLIP-based%20OOD%20detection%20methods%20struggle%20to%20varying%20degrees%20across%20the%20three%0Aproposed%20benchmarks%2C%20and%20none%20of%20them%20consistently%20outperforms%20the%20others.%20We%0Ahope%20the%20community%20goes%20beyond%20specific%20benchmarks%20and%20includes%20more%0Achallenging%20conditions%20reflecting%20real-world%20scenarios.%20The%20code%20is%0Ahttps%3A//github.com/hoshi23/OOD-X-Benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Benchmark%2520and%2520Evaluation%2520for%2520Real-World%2520Out-of-Distribution%2520Detection%250A%2520%2520Using%2520Vision-Language%2520Models%26entry.906535625%3DShiho%2520Noda%2520and%2520Atsuyuki%2520Miyai%2520and%2520Qing%2520Yu%2520and%2520Go%2520Irie%2520and%2520Kiyoharu%2520Aizawa%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520is%2520a%2520task%2520that%2520detects%2520OOD%2520samples%2520during%250Ainference%2520to%2520ensure%2520the%2520safety%2520of%2520deployed%2520models.%2520However%252C%2520conventional%250Abenchmarks%2520have%2520reached%2520performance%2520saturation%252C%2520making%2520it%2520difficult%2520to%2520compare%250Arecent%2520OOD%2520detection%2520methods.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520three%250Anovel%2520OOD%2520detection%2520benchmarks%2520that%2520enable%2520a%2520deeper%2520understanding%2520of%2520method%250Acharacteristics%2520and%2520reflect%2520real-world%2520conditions.%2520First%252C%2520we%2520present%250AImageNet-X%252C%2520designed%2520to%2520evaluate%2520performance%2520under%2520challenging%2520semantic%2520shifts.%250ASecond%252C%2520we%2520propose%2520ImageNet-FS-X%2520for%2520full-spectrum%2520OOD%2520detection%252C%2520assessing%250Arobustness%2520to%2520covariate%2520shifts%2520%2528feature%2520distribution%2520shifts%2529.%2520Finally%252C%2520we%250Apropose%2520Wilds-FS-X%252C%2520which%2520extends%2520these%2520evaluations%2520to%2520real-world%2520datasets%252C%250Aoffering%2520a%2520more%2520comprehensive%2520testbed.%2520Our%2520experiments%2520reveal%2520that%2520recent%250ACLIP-based%2520OOD%2520detection%2520methods%2520struggle%2520to%2520varying%2520degrees%2520across%2520the%2520three%250Aproposed%2520benchmarks%252C%2520and%2520none%2520of%2520them%2520consistently%2520outperforms%2520the%2520others.%2520We%250Ahope%2520the%2520community%2520goes%2520beyond%2520specific%2520benchmarks%2520and%2520includes%2520more%250Achallenging%2520conditions%2520reflecting%2520real-world%2520scenarios.%2520The%2520code%2520is%250Ahttps%253A//github.com/hoshi23/OOD-X-Benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Benchmark%20and%20Evaluation%20for%20Real-World%20Out-of-Distribution%20Detection%0A%20%20Using%20Vision-Language%20Models&entry.906535625=Shiho%20Noda%20and%20Atsuyuki%20Miyai%20and%20Qing%20Yu%20and%20Go%20Irie%20and%20Kiyoharu%20Aizawa&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20a%20task%20that%20detects%20OOD%20samples%20during%0Ainference%20to%20ensure%20the%20safety%20of%20deployed%20models.%20However%2C%20conventional%0Abenchmarks%20have%20reached%20performance%20saturation%2C%20making%20it%20difficult%20to%20compare%0Arecent%20OOD%20detection%20methods.%20To%20address%20this%20challenge%2C%20we%20introduce%20three%0Anovel%20OOD%20detection%20benchmarks%20that%20enable%20a%20deeper%20understanding%20of%20method%0Acharacteristics%20and%20reflect%20real-world%20conditions.%20First%2C%20we%20present%0AImageNet-X%2C%20designed%20to%20evaluate%20performance%20under%20challenging%20semantic%20shifts.%0ASecond%2C%20we%20propose%20ImageNet-FS-X%20for%20full-spectrum%20OOD%20detection%2C%20assessing%0Arobustness%20to%20covariate%20shifts%20%28feature%20distribution%20shifts%29.%20Finally%2C%20we%0Apropose%20Wilds-FS-X%2C%20which%20extends%20these%20evaluations%20to%20real-world%20datasets%2C%0Aoffering%20a%20more%20comprehensive%20testbed.%20Our%20experiments%20reveal%20that%20recent%0ACLIP-based%20OOD%20detection%20methods%20struggle%20to%20varying%20degrees%20across%20the%20three%0Aproposed%20benchmarks%2C%20and%20none%20of%20them%20consistently%20outperforms%20the%20others.%20We%0Ahope%20the%20community%20goes%20beyond%20specific%20benchmarks%20and%20includes%20more%0Achallenging%20conditions%20reflecting%20real-world%20scenarios.%20The%20code%20is%0Ahttps%3A//github.com/hoshi23/OOD-X-Benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18463v2&entry.124074799=Read"},
{"title": "DAWN: Domain-Adaptive Weakly Supervised Nuclei Segmentation via\n  Cross-Task Interactions", "author": "Ye Zhang and Yifeng Wang and Zijie Fang and Hao Bian and Linghan Cai and Ziyue Wang and Yongbing Zhang", "abstract": "  Weakly supervised segmentation methods have gained significant attention due\nto their ability to reduce the reliance on costly pixel-level annotations\nduring model training. However, the current weakly supervised nuclei\nsegmentation approaches typically follow a two-stage pseudo-label generation\nand network training process. The performance of the nuclei segmentation\nheavily relies on the quality of the generated pseudo-labels, thereby limiting\nits effectiveness. This paper introduces a novel domain-adaptive weakly\nsupervised nuclei segmentation framework using cross-task interaction\nstrategies to overcome the challenge of pseudo-label generation. Specifically,\nwe utilize weakly annotated data to train an auxiliary detection task, which\nassists the domain adaptation of the segmentation network. To enhance the\nefficiency of domain adaptation, we design a consistent feature constraint\nmodule integrating prior knowledge from the source domain. Furthermore, we\ndevelop pseudo-label optimization and interactive training methods to improve\nthe domain transfer capability. To validate the effectiveness of our proposed\nmethod, we conduct extensive comparative and ablation experiments on six\ndatasets. The results demonstrate the superiority of our approach over existing\nweakly supervised approaches. Remarkably, our method achieves comparable or\neven better performance than fully supervised methods. Our code will be\nreleased in https://github.com/zhangye-zoe/DAWN.\n", "link": "http://arxiv.org/abs/2404.14956v3", "date": "2025-02-03", "relevancy": 2.7695, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5848}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAWN%3A%20Domain-Adaptive%20Weakly%20Supervised%20Nuclei%20Segmentation%20via%0A%20%20Cross-Task%20Interactions&body=Title%3A%20DAWN%3A%20Domain-Adaptive%20Weakly%20Supervised%20Nuclei%20Segmentation%20via%0A%20%20Cross-Task%20Interactions%0AAuthor%3A%20Ye%20Zhang%20and%20Yifeng%20Wang%20and%20Zijie%20Fang%20and%20Hao%20Bian%20and%20Linghan%20Cai%20and%20Ziyue%20Wang%20and%20Yongbing%20Zhang%0AAbstract%3A%20%20%20Weakly%20supervised%20segmentation%20methods%20have%20gained%20significant%20attention%20due%0Ato%20their%20ability%20to%20reduce%20the%20reliance%20on%20costly%20pixel-level%20annotations%0Aduring%20model%20training.%20However%2C%20the%20current%20weakly%20supervised%20nuclei%0Asegmentation%20approaches%20typically%20follow%20a%20two-stage%20pseudo-label%20generation%0Aand%20network%20training%20process.%20The%20performance%20of%20the%20nuclei%20segmentation%0Aheavily%20relies%20on%20the%20quality%20of%20the%20generated%20pseudo-labels%2C%20thereby%20limiting%0Aits%20effectiveness.%20This%20paper%20introduces%20a%20novel%20domain-adaptive%20weakly%0Asupervised%20nuclei%20segmentation%20framework%20using%20cross-task%20interaction%0Astrategies%20to%20overcome%20the%20challenge%20of%20pseudo-label%20generation.%20Specifically%2C%0Awe%20utilize%20weakly%20annotated%20data%20to%20train%20an%20auxiliary%20detection%20task%2C%20which%0Aassists%20the%20domain%20adaptation%20of%20the%20segmentation%20network.%20To%20enhance%20the%0Aefficiency%20of%20domain%20adaptation%2C%20we%20design%20a%20consistent%20feature%20constraint%0Amodule%20integrating%20prior%20knowledge%20from%20the%20source%20domain.%20Furthermore%2C%20we%0Adevelop%20pseudo-label%20optimization%20and%20interactive%20training%20methods%20to%20improve%0Athe%20domain%20transfer%20capability.%20To%20validate%20the%20effectiveness%20of%20our%20proposed%0Amethod%2C%20we%20conduct%20extensive%20comparative%20and%20ablation%20experiments%20on%20six%0Adatasets.%20The%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20existing%0Aweakly%20supervised%20approaches.%20Remarkably%2C%20our%20method%20achieves%20comparable%20or%0Aeven%20better%20performance%20than%20fully%20supervised%20methods.%20Our%20code%20will%20be%0Areleased%20in%20https%3A//github.com/zhangye-zoe/DAWN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14956v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAWN%253A%2520Domain-Adaptive%2520Weakly%2520Supervised%2520Nuclei%2520Segmentation%2520via%250A%2520%2520Cross-Task%2520Interactions%26entry.906535625%3DYe%2520Zhang%2520and%2520Yifeng%2520Wang%2520and%2520Zijie%2520Fang%2520and%2520Hao%2520Bian%2520and%2520Linghan%2520Cai%2520and%2520Ziyue%2520Wang%2520and%2520Yongbing%2520Zhang%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520segmentation%2520methods%2520have%2520gained%2520significant%2520attention%2520due%250Ato%2520their%2520ability%2520to%2520reduce%2520the%2520reliance%2520on%2520costly%2520pixel-level%2520annotations%250Aduring%2520model%2520training.%2520However%252C%2520the%2520current%2520weakly%2520supervised%2520nuclei%250Asegmentation%2520approaches%2520typically%2520follow%2520a%2520two-stage%2520pseudo-label%2520generation%250Aand%2520network%2520training%2520process.%2520The%2520performance%2520of%2520the%2520nuclei%2520segmentation%250Aheavily%2520relies%2520on%2520the%2520quality%2520of%2520the%2520generated%2520pseudo-labels%252C%2520thereby%2520limiting%250Aits%2520effectiveness.%2520This%2520paper%2520introduces%2520a%2520novel%2520domain-adaptive%2520weakly%250Asupervised%2520nuclei%2520segmentation%2520framework%2520using%2520cross-task%2520interaction%250Astrategies%2520to%2520overcome%2520the%2520challenge%2520of%2520pseudo-label%2520generation.%2520Specifically%252C%250Awe%2520utilize%2520weakly%2520annotated%2520data%2520to%2520train%2520an%2520auxiliary%2520detection%2520task%252C%2520which%250Aassists%2520the%2520domain%2520adaptation%2520of%2520the%2520segmentation%2520network.%2520To%2520enhance%2520the%250Aefficiency%2520of%2520domain%2520adaptation%252C%2520we%2520design%2520a%2520consistent%2520feature%2520constraint%250Amodule%2520integrating%2520prior%2520knowledge%2520from%2520the%2520source%2520domain.%2520Furthermore%252C%2520we%250Adevelop%2520pseudo-label%2520optimization%2520and%2520interactive%2520training%2520methods%2520to%2520improve%250Athe%2520domain%2520transfer%2520capability.%2520To%2520validate%2520the%2520effectiveness%2520of%2520our%2520proposed%250Amethod%252C%2520we%2520conduct%2520extensive%2520comparative%2520and%2520ablation%2520experiments%2520on%2520six%250Adatasets.%2520The%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520existing%250Aweakly%2520supervised%2520approaches.%2520Remarkably%252C%2520our%2520method%2520achieves%2520comparable%2520or%250Aeven%2520better%2520performance%2520than%2520fully%2520supervised%2520methods.%2520Our%2520code%2520will%2520be%250Areleased%2520in%2520https%253A//github.com/zhangye-zoe/DAWN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14956v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAWN%3A%20Domain-Adaptive%20Weakly%20Supervised%20Nuclei%20Segmentation%20via%0A%20%20Cross-Task%20Interactions&entry.906535625=Ye%20Zhang%20and%20Yifeng%20Wang%20and%20Zijie%20Fang%20and%20Hao%20Bian%20and%20Linghan%20Cai%20and%20Ziyue%20Wang%20and%20Yongbing%20Zhang&entry.1292438233=%20%20Weakly%20supervised%20segmentation%20methods%20have%20gained%20significant%20attention%20due%0Ato%20their%20ability%20to%20reduce%20the%20reliance%20on%20costly%20pixel-level%20annotations%0Aduring%20model%20training.%20However%2C%20the%20current%20weakly%20supervised%20nuclei%0Asegmentation%20approaches%20typically%20follow%20a%20two-stage%20pseudo-label%20generation%0Aand%20network%20training%20process.%20The%20performance%20of%20the%20nuclei%20segmentation%0Aheavily%20relies%20on%20the%20quality%20of%20the%20generated%20pseudo-labels%2C%20thereby%20limiting%0Aits%20effectiveness.%20This%20paper%20introduces%20a%20novel%20domain-adaptive%20weakly%0Asupervised%20nuclei%20segmentation%20framework%20using%20cross-task%20interaction%0Astrategies%20to%20overcome%20the%20challenge%20of%20pseudo-label%20generation.%20Specifically%2C%0Awe%20utilize%20weakly%20annotated%20data%20to%20train%20an%20auxiliary%20detection%20task%2C%20which%0Aassists%20the%20domain%20adaptation%20of%20the%20segmentation%20network.%20To%20enhance%20the%0Aefficiency%20of%20domain%20adaptation%2C%20we%20design%20a%20consistent%20feature%20constraint%0Amodule%20integrating%20prior%20knowledge%20from%20the%20source%20domain.%20Furthermore%2C%20we%0Adevelop%20pseudo-label%20optimization%20and%20interactive%20training%20methods%20to%20improve%0Athe%20domain%20transfer%20capability.%20To%20validate%20the%20effectiveness%20of%20our%20proposed%0Amethod%2C%20we%20conduct%20extensive%20comparative%20and%20ablation%20experiments%20on%20six%0Adatasets.%20The%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20existing%0Aweakly%20supervised%20approaches.%20Remarkably%2C%20our%20method%20achieves%20comparable%20or%0Aeven%20better%20performance%20than%20fully%20supervised%20methods.%20Our%20code%20will%20be%0Areleased%20in%20https%3A//github.com/zhangye-zoe/DAWN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14956v3&entry.124074799=Read"},
{"title": "Scalable Multi-phase Word Embedding Using Conjunctive Propositional\n  Clauses", "author": "Ahmed K. Kadhim and Lei Jiao and Rishad Shafik and Ole-Christoffer Granmo and Bimal Bhattarai", "abstract": "  The Tsetlin Machine (TM) architecture has recently demonstrated effectiveness\nin Machine Learning (ML), particularly within Natural Language Processing\n(NLP). It has been utilized to construct word embedding using conjunctive\npropositional clauses, thereby significantly enhancing our understanding and\ninterpretation of machine-derived decisions. The previous approach performed\nthe word embedding over a sequence of input words to consolidate the\ninformation into a cohesive and unified representation. However, that approach\nencounters scalability challenges as the input size increases. In this study,\nwe introduce a novel approach incorporating two-phase training to discover\ncontextual embeddings of input sequences. Specifically, this method\nencapsulates the knowledge for each input word within the dataset's vocabulary,\nsubsequently constructing embeddings for a sequence of input words utilizing\nthe extracted knowledge. This technique not only facilitates the design of a\nscalable model but also preserves interpretability. Our experimental findings\nrevealed that the proposed method yields competitive performance compared to\nthe previous approaches, demonstrating promising results in contrast to\nhuman-generated benchmarks. Furthermore, we applied the proposed approach to\nsentiment analysis on the IMDB dataset, where the TM embedding and the TM\nclassifier, along with other interpretable classifiers, offered a transparent\nend-to-end solution with competitive performance.\n", "link": "http://arxiv.org/abs/2501.19018v2", "date": "2025-02-03", "relevancy": 2.7211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5739}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Multi-phase%20Word%20Embedding%20Using%20Conjunctive%20Propositional%0A%20%20Clauses&body=Title%3A%20Scalable%20Multi-phase%20Word%20Embedding%20Using%20Conjunctive%20Propositional%0A%20%20Clauses%0AAuthor%3A%20Ahmed%20K.%20Kadhim%20and%20Lei%20Jiao%20and%20Rishad%20Shafik%20and%20Ole-Christoffer%20Granmo%20and%20Bimal%20Bhattarai%0AAbstract%3A%20%20%20The%20Tsetlin%20Machine%20%28TM%29%20architecture%20has%20recently%20demonstrated%20effectiveness%0Ain%20Machine%20Learning%20%28ML%29%2C%20particularly%20within%20Natural%20Language%20Processing%0A%28NLP%29.%20It%20has%20been%20utilized%20to%20construct%20word%20embedding%20using%20conjunctive%0Apropositional%20clauses%2C%20thereby%20significantly%20enhancing%20our%20understanding%20and%0Ainterpretation%20of%20machine-derived%20decisions.%20The%20previous%20approach%20performed%0Athe%20word%20embedding%20over%20a%20sequence%20of%20input%20words%20to%20consolidate%20the%0Ainformation%20into%20a%20cohesive%20and%20unified%20representation.%20However%2C%20that%20approach%0Aencounters%20scalability%20challenges%20as%20the%20input%20size%20increases.%20In%20this%20study%2C%0Awe%20introduce%20a%20novel%20approach%20incorporating%20two-phase%20training%20to%20discover%0Acontextual%20embeddings%20of%20input%20sequences.%20Specifically%2C%20this%20method%0Aencapsulates%20the%20knowledge%20for%20each%20input%20word%20within%20the%20dataset%27s%20vocabulary%2C%0Asubsequently%20constructing%20embeddings%20for%20a%20sequence%20of%20input%20words%20utilizing%0Athe%20extracted%20knowledge.%20This%20technique%20not%20only%20facilitates%20the%20design%20of%20a%0Ascalable%20model%20but%20also%20preserves%20interpretability.%20Our%20experimental%20findings%0Arevealed%20that%20the%20proposed%20method%20yields%20competitive%20performance%20compared%20to%0Athe%20previous%20approaches%2C%20demonstrating%20promising%20results%20in%20contrast%20to%0Ahuman-generated%20benchmarks.%20Furthermore%2C%20we%20applied%20the%20proposed%20approach%20to%0Asentiment%20analysis%20on%20the%20IMDB%20dataset%2C%20where%20the%20TM%20embedding%20and%20the%20TM%0Aclassifier%2C%20along%20with%20other%20interpretable%20classifiers%2C%20offered%20a%20transparent%0Aend-to-end%20solution%20with%20competitive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19018v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Multi-phase%2520Word%2520Embedding%2520Using%2520Conjunctive%2520Propositional%250A%2520%2520Clauses%26entry.906535625%3DAhmed%2520K.%2520Kadhim%2520and%2520Lei%2520Jiao%2520and%2520Rishad%2520Shafik%2520and%2520Ole-Christoffer%2520Granmo%2520and%2520Bimal%2520Bhattarai%26entry.1292438233%3D%2520%2520The%2520Tsetlin%2520Machine%2520%2528TM%2529%2520architecture%2520has%2520recently%2520demonstrated%2520effectiveness%250Ain%2520Machine%2520Learning%2520%2528ML%2529%252C%2520particularly%2520within%2520Natural%2520Language%2520Processing%250A%2528NLP%2529.%2520It%2520has%2520been%2520utilized%2520to%2520construct%2520word%2520embedding%2520using%2520conjunctive%250Apropositional%2520clauses%252C%2520thereby%2520significantly%2520enhancing%2520our%2520understanding%2520and%250Ainterpretation%2520of%2520machine-derived%2520decisions.%2520The%2520previous%2520approach%2520performed%250Athe%2520word%2520embedding%2520over%2520a%2520sequence%2520of%2520input%2520words%2520to%2520consolidate%2520the%250Ainformation%2520into%2520a%2520cohesive%2520and%2520unified%2520representation.%2520However%252C%2520that%2520approach%250Aencounters%2520scalability%2520challenges%2520as%2520the%2520input%2520size%2520increases.%2520In%2520this%2520study%252C%250Awe%2520introduce%2520a%2520novel%2520approach%2520incorporating%2520two-phase%2520training%2520to%2520discover%250Acontextual%2520embeddings%2520of%2520input%2520sequences.%2520Specifically%252C%2520this%2520method%250Aencapsulates%2520the%2520knowledge%2520for%2520each%2520input%2520word%2520within%2520the%2520dataset%2527s%2520vocabulary%252C%250Asubsequently%2520constructing%2520embeddings%2520for%2520a%2520sequence%2520of%2520input%2520words%2520utilizing%250Athe%2520extracted%2520knowledge.%2520This%2520technique%2520not%2520only%2520facilitates%2520the%2520design%2520of%2520a%250Ascalable%2520model%2520but%2520also%2520preserves%2520interpretability.%2520Our%2520experimental%2520findings%250Arevealed%2520that%2520the%2520proposed%2520method%2520yields%2520competitive%2520performance%2520compared%2520to%250Athe%2520previous%2520approaches%252C%2520demonstrating%2520promising%2520results%2520in%2520contrast%2520to%250Ahuman-generated%2520benchmarks.%2520Furthermore%252C%2520we%2520applied%2520the%2520proposed%2520approach%2520to%250Asentiment%2520analysis%2520on%2520the%2520IMDB%2520dataset%252C%2520where%2520the%2520TM%2520embedding%2520and%2520the%2520TM%250Aclassifier%252C%2520along%2520with%2520other%2520interpretable%2520classifiers%252C%2520offered%2520a%2520transparent%250Aend-to-end%2520solution%2520with%2520competitive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19018v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Multi-phase%20Word%20Embedding%20Using%20Conjunctive%20Propositional%0A%20%20Clauses&entry.906535625=Ahmed%20K.%20Kadhim%20and%20Lei%20Jiao%20and%20Rishad%20Shafik%20and%20Ole-Christoffer%20Granmo%20and%20Bimal%20Bhattarai&entry.1292438233=%20%20The%20Tsetlin%20Machine%20%28TM%29%20architecture%20has%20recently%20demonstrated%20effectiveness%0Ain%20Machine%20Learning%20%28ML%29%2C%20particularly%20within%20Natural%20Language%20Processing%0A%28NLP%29.%20It%20has%20been%20utilized%20to%20construct%20word%20embedding%20using%20conjunctive%0Apropositional%20clauses%2C%20thereby%20significantly%20enhancing%20our%20understanding%20and%0Ainterpretation%20of%20machine-derived%20decisions.%20The%20previous%20approach%20performed%0Athe%20word%20embedding%20over%20a%20sequence%20of%20input%20words%20to%20consolidate%20the%0Ainformation%20into%20a%20cohesive%20and%20unified%20representation.%20However%2C%20that%20approach%0Aencounters%20scalability%20challenges%20as%20the%20input%20size%20increases.%20In%20this%20study%2C%0Awe%20introduce%20a%20novel%20approach%20incorporating%20two-phase%20training%20to%20discover%0Acontextual%20embeddings%20of%20input%20sequences.%20Specifically%2C%20this%20method%0Aencapsulates%20the%20knowledge%20for%20each%20input%20word%20within%20the%20dataset%27s%20vocabulary%2C%0Asubsequently%20constructing%20embeddings%20for%20a%20sequence%20of%20input%20words%20utilizing%0Athe%20extracted%20knowledge.%20This%20technique%20not%20only%20facilitates%20the%20design%20of%20a%0Ascalable%20model%20but%20also%20preserves%20interpretability.%20Our%20experimental%20findings%0Arevealed%20that%20the%20proposed%20method%20yields%20competitive%20performance%20compared%20to%0Athe%20previous%20approaches%2C%20demonstrating%20promising%20results%20in%20contrast%20to%0Ahuman-generated%20benchmarks.%20Furthermore%2C%20we%20applied%20the%20proposed%20approach%20to%0Asentiment%20analysis%20on%20the%20IMDB%20dataset%2C%20where%20the%20TM%20embedding%20and%20the%20TM%0Aclassifier%2C%20along%20with%20other%20interpretable%20classifiers%2C%20offered%20a%20transparent%0Aend-to-end%20solution%20with%20competitive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19018v2&entry.124074799=Read"},
{"title": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text\n  Alignment", "author": "Song-Lin Lv and Yu-Yang Chen and Zhi Zhou and Yu-Feng Li and Lan-Zhe Guo", "abstract": "  Vision-language models (VLMs), such as CLIP, have demonstrated exceptional\ngeneralization capabilities and can quickly adapt to downstream tasks through\nprompt fine-tuning. Unfortunately, in classification tasks involving\nnon-training classes, known as open-vocabulary setting, fine-tuned VLMs often\noverfit to train classes, resulting in a misalignment between confidence scores\nand actual accuracy on unseen classes, which significantly undermines their\nreliability in real-world deployments. Existing confidence calibration methods\ntypically require training parameters or analyzing features from the training\ndataset, restricting their ability to generalize unseen classes without\ncorresponding train data. Moreover, VLM-specific calibration methods rely\nsolely on text features from train classes as calibration indicators, which\ninherently limits their ability to calibrate train classes. To address these\nchallenges, we propose an effective multimodal calibration method\nContrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot\nadaptability and the conclusion from empirical analysis that poor intra-class\nand inter-class discriminative ability on unseen classes is the root cause, we\ncalculate calibration weights based on the contrastive difference between the\noriginal and fine-tuned CLIP. This method not only adapts to calibrating unseen\nclasses but also overcomes the limitations of previous VLM calibration methods\nthat could not calibrate train classes. In experiments involving 11 datasets\nwith 5 fine-tuning methods, CAC consistently achieved the best calibration\neffect on both train and unseen classes without sacrificing accuracy and\ninference speed.\n", "link": "http://arxiv.org/abs/2501.19060v2", "date": "2025-02-03", "relevancy": 2.7026, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.597}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5161}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrast-Aware%20Calibration%20for%20Fine-Tuned%20CLIP%3A%20Leveraging%20Image-Text%0A%20%20Alignment&body=Title%3A%20Contrast-Aware%20Calibration%20for%20Fine-Tuned%20CLIP%3A%20Leveraging%20Image-Text%0A%20%20Alignment%0AAuthor%3A%20Song-Lin%20Lv%20and%20Yu-Yang%20Chen%20and%20Zhi%20Zhou%20and%20Yu-Feng%20Li%20and%20Lan-Zhe%20Guo%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20demonstrated%20exceptional%0Ageneralization%20capabilities%20and%20can%20quickly%20adapt%20to%20downstream%20tasks%20through%0Aprompt%20fine-tuning.%20Unfortunately%2C%20in%20classification%20tasks%20involving%0Anon-training%20classes%2C%20known%20as%20open-vocabulary%20setting%2C%20fine-tuned%20VLMs%20often%0Aoverfit%20to%20train%20classes%2C%20resulting%20in%20a%20misalignment%20between%20confidence%20scores%0Aand%20actual%20accuracy%20on%20unseen%20classes%2C%20which%20significantly%20undermines%20their%0Areliability%20in%20real-world%20deployments.%20Existing%20confidence%20calibration%20methods%0Atypically%20require%20training%20parameters%20or%20analyzing%20features%20from%20the%20training%0Adataset%2C%20restricting%20their%20ability%20to%20generalize%20unseen%20classes%20without%0Acorresponding%20train%20data.%20Moreover%2C%20VLM-specific%20calibration%20methods%20rely%0Asolely%20on%20text%20features%20from%20train%20classes%20as%20calibration%20indicators%2C%20which%0Ainherently%20limits%20their%20ability%20to%20calibrate%20train%20classes.%20To%20address%20these%0Achallenges%2C%20we%20propose%20an%20effective%20multimodal%20calibration%20method%0AContrast-Aware%20Calibration%20%28CAC%29.%20Building%20on%20the%20original%20CLIP%27s%20zero-shot%0Aadaptability%20and%20the%20conclusion%20from%20empirical%20analysis%20that%20poor%20intra-class%0Aand%20inter-class%20discriminative%20ability%20on%20unseen%20classes%20is%20the%20root%20cause%2C%20we%0Acalculate%20calibration%20weights%20based%20on%20the%20contrastive%20difference%20between%20the%0Aoriginal%20and%20fine-tuned%20CLIP.%20This%20method%20not%20only%20adapts%20to%20calibrating%20unseen%0Aclasses%20but%20also%20overcomes%20the%20limitations%20of%20previous%20VLM%20calibration%20methods%0Athat%20could%20not%20calibrate%20train%20classes.%20In%20experiments%20involving%2011%20datasets%0Awith%205%20fine-tuning%20methods%2C%20CAC%20consistently%20achieved%20the%20best%20calibration%0Aeffect%20on%20both%20train%20and%20unseen%20classes%20without%20sacrificing%20accuracy%20and%0Ainference%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrast-Aware%2520Calibration%2520for%2520Fine-Tuned%2520CLIP%253A%2520Leveraging%2520Image-Text%250A%2520%2520Alignment%26entry.906535625%3DSong-Lin%2520Lv%2520and%2520Yu-Yang%2520Chen%2520and%2520Zhi%2520Zhou%2520and%2520Yu-Feng%2520Li%2520and%2520Lan-Zhe%2520Guo%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520have%2520demonstrated%2520exceptional%250Ageneralization%2520capabilities%2520and%2520can%2520quickly%2520adapt%2520to%2520downstream%2520tasks%2520through%250Aprompt%2520fine-tuning.%2520Unfortunately%252C%2520in%2520classification%2520tasks%2520involving%250Anon-training%2520classes%252C%2520known%2520as%2520open-vocabulary%2520setting%252C%2520fine-tuned%2520VLMs%2520often%250Aoverfit%2520to%2520train%2520classes%252C%2520resulting%2520in%2520a%2520misalignment%2520between%2520confidence%2520scores%250Aand%2520actual%2520accuracy%2520on%2520unseen%2520classes%252C%2520which%2520significantly%2520undermines%2520their%250Areliability%2520in%2520real-world%2520deployments.%2520Existing%2520confidence%2520calibration%2520methods%250Atypically%2520require%2520training%2520parameters%2520or%2520analyzing%2520features%2520from%2520the%2520training%250Adataset%252C%2520restricting%2520their%2520ability%2520to%2520generalize%2520unseen%2520classes%2520without%250Acorresponding%2520train%2520data.%2520Moreover%252C%2520VLM-specific%2520calibration%2520methods%2520rely%250Asolely%2520on%2520text%2520features%2520from%2520train%2520classes%2520as%2520calibration%2520indicators%252C%2520which%250Ainherently%2520limits%2520their%2520ability%2520to%2520calibrate%2520train%2520classes.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520an%2520effective%2520multimodal%2520calibration%2520method%250AContrast-Aware%2520Calibration%2520%2528CAC%2529.%2520Building%2520on%2520the%2520original%2520CLIP%2527s%2520zero-shot%250Aadaptability%2520and%2520the%2520conclusion%2520from%2520empirical%2520analysis%2520that%2520poor%2520intra-class%250Aand%2520inter-class%2520discriminative%2520ability%2520on%2520unseen%2520classes%2520is%2520the%2520root%2520cause%252C%2520we%250Acalculate%2520calibration%2520weights%2520based%2520on%2520the%2520contrastive%2520difference%2520between%2520the%250Aoriginal%2520and%2520fine-tuned%2520CLIP.%2520This%2520method%2520not%2520only%2520adapts%2520to%2520calibrating%2520unseen%250Aclasses%2520but%2520also%2520overcomes%2520the%2520limitations%2520of%2520previous%2520VLM%2520calibration%2520methods%250Athat%2520could%2520not%2520calibrate%2520train%2520classes.%2520In%2520experiments%2520involving%252011%2520datasets%250Awith%25205%2520fine-tuning%2520methods%252C%2520CAC%2520consistently%2520achieved%2520the%2520best%2520calibration%250Aeffect%2520on%2520both%2520train%2520and%2520unseen%2520classes%2520without%2520sacrificing%2520accuracy%2520and%250Ainference%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrast-Aware%20Calibration%20for%20Fine-Tuned%20CLIP%3A%20Leveraging%20Image-Text%0A%20%20Alignment&entry.906535625=Song-Lin%20Lv%20and%20Yu-Yang%20Chen%20and%20Zhi%20Zhou%20and%20Yu-Feng%20Li%20and%20Lan-Zhe%20Guo&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20demonstrated%20exceptional%0Ageneralization%20capabilities%20and%20can%20quickly%20adapt%20to%20downstream%20tasks%20through%0Aprompt%20fine-tuning.%20Unfortunately%2C%20in%20classification%20tasks%20involving%0Anon-training%20classes%2C%20known%20as%20open-vocabulary%20setting%2C%20fine-tuned%20VLMs%20often%0Aoverfit%20to%20train%20classes%2C%20resulting%20in%20a%20misalignment%20between%20confidence%20scores%0Aand%20actual%20accuracy%20on%20unseen%20classes%2C%20which%20significantly%20undermines%20their%0Areliability%20in%20real-world%20deployments.%20Existing%20confidence%20calibration%20methods%0Atypically%20require%20training%20parameters%20or%20analyzing%20features%20from%20the%20training%0Adataset%2C%20restricting%20their%20ability%20to%20generalize%20unseen%20classes%20without%0Acorresponding%20train%20data.%20Moreover%2C%20VLM-specific%20calibration%20methods%20rely%0Asolely%20on%20text%20features%20from%20train%20classes%20as%20calibration%20indicators%2C%20which%0Ainherently%20limits%20their%20ability%20to%20calibrate%20train%20classes.%20To%20address%20these%0Achallenges%2C%20we%20propose%20an%20effective%20multimodal%20calibration%20method%0AContrast-Aware%20Calibration%20%28CAC%29.%20Building%20on%20the%20original%20CLIP%27s%20zero-shot%0Aadaptability%20and%20the%20conclusion%20from%20empirical%20analysis%20that%20poor%20intra-class%0Aand%20inter-class%20discriminative%20ability%20on%20unseen%20classes%20is%20the%20root%20cause%2C%20we%0Acalculate%20calibration%20weights%20based%20on%20the%20contrastive%20difference%20between%20the%0Aoriginal%20and%20fine-tuned%20CLIP.%20This%20method%20not%20only%20adapts%20to%20calibrating%20unseen%0Aclasses%20but%20also%20overcomes%20the%20limitations%20of%20previous%20VLM%20calibration%20methods%0Athat%20could%20not%20calibrate%20train%20classes.%20In%20experiments%20involving%2011%20datasets%0Awith%205%20fine-tuning%20methods%2C%20CAC%20consistently%20achieved%20the%20best%20calibration%0Aeffect%20on%20both%20train%20and%20unseen%20classes%20without%20sacrificing%20accuracy%20and%0Ainference%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19060v2&entry.124074799=Read"},
{"title": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models", "author": "Junyao Ge and Xu Zhang and Yang Zheng and Kaitai Guo and Jimin Liang", "abstract": "  Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller.\n", "link": "http://arxiv.org/abs/2408.14744v2", "date": "2025-02-03", "relevancy": 2.6887, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSTeller%3A%20Scaling%20Up%20Visual%20Language%20Modeling%20in%20Remote%20Sensing%20with%0A%20%20Rich%20Linguistic%20Semantics%20from%20Openly%20Available%20Data%20and%20Large%20Language%0A%20%20Models&body=Title%3A%20RSTeller%3A%20Scaling%20Up%20Visual%20Language%20Modeling%20in%20Remote%20Sensing%20with%0A%20%20Rich%20Linguistic%20Semantics%20from%20Openly%20Available%20Data%20and%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Junyao%20Ge%20and%20Xu%20Zhang%20and%20Yang%20Zheng%20and%20Kaitai%20Guo%20and%20Jimin%20Liang%0AAbstract%3A%20%20%20Abundant%2C%20well-annotated%20multimodal%20data%20in%20remote%20sensing%20are%20pivotal%20for%0Aaligning%20complex%20visual%20remote%20sensing%20%28RS%29%20scenes%20with%20human%20language%2C%0Aenabling%20the%20development%20of%20specialized%20vision%20language%20models%20across%20diverse%0ARS%20interpretation%20tasks.%20However%2C%20annotating%20RS%20images%20with%20rich%20linguistic%0Asemantics%20at%20scale%20demands%20expertise%20in%20RS%20and%20substantial%20human%20labor%2C%20making%0Ait%20costly%20and%20often%20impractical.%20In%20this%20study%2C%20we%20propose%20a%20workflow%20that%0Aleverages%20large%20language%20models%20%28LLMs%29%20to%20generate%20multimodal%20datasets%20with%0Asemantically%20rich%20captions%20at%20scale%20from%20plain%20OpenStreetMap%20%28OSM%29%20data%20for%0Aimages%20sourced%20from%20the%20Google%20Earth%20Engine%20%28GEE%29%20platform.%20This%20approach%0Afacilitates%20the%20generation%20of%20paired%20remote%20sensing%20data%20and%20can%20be%20readily%0Ascaled%20up%20using%20openly%20available%20data.%20Within%20this%20framework%2C%20we%20present%0ARSTeller%2C%20a%20multimodal%20dataset%20comprising%20over%201.3%20million%20RS%20images%2C%20each%0Aaccompanied%20by%20two%20descriptive%20captions.%20Extensive%20experiments%20demonstrate%20that%0ARSTeller%20enhances%20the%20performance%20of%20multiple%20existing%20vision%20language%20models%0Afor%20RS%20scene%20understanding%20through%20continual%20pre-training.%20Our%20methodology%0Asignificantly%20reduces%20the%20manual%20effort%20and%20expertise%20needed%20for%20annotating%0Aremote%20sensing%20imagery%20while%20democratizing%20access%20to%20high-quality%20annotated%0Adata.%20This%20advancement%20fosters%20progress%20in%20visual%20language%20modeling%20and%0Aencourages%20broader%20participation%20in%20remote%20sensing%20research%20and%20applications.%0AThe%20RSTeller%20dataset%20is%20available%20at%20https%3A//github.com/SlytherinGe/RSTeller.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14744v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSTeller%253A%2520Scaling%2520Up%2520Visual%2520Language%2520Modeling%2520in%2520Remote%2520Sensing%2520with%250A%2520%2520Rich%2520Linguistic%2520Semantics%2520from%2520Openly%2520Available%2520Data%2520and%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DJunyao%2520Ge%2520and%2520Xu%2520Zhang%2520and%2520Yang%2520Zheng%2520and%2520Kaitai%2520Guo%2520and%2520Jimin%2520Liang%26entry.1292438233%3D%2520%2520Abundant%252C%2520well-annotated%2520multimodal%2520data%2520in%2520remote%2520sensing%2520are%2520pivotal%2520for%250Aaligning%2520complex%2520visual%2520remote%2520sensing%2520%2528RS%2529%2520scenes%2520with%2520human%2520language%252C%250Aenabling%2520the%2520development%2520of%2520specialized%2520vision%2520language%2520models%2520across%2520diverse%250ARS%2520interpretation%2520tasks.%2520However%252C%2520annotating%2520RS%2520images%2520with%2520rich%2520linguistic%250Asemantics%2520at%2520scale%2520demands%2520expertise%2520in%2520RS%2520and%2520substantial%2520human%2520labor%252C%2520making%250Ait%2520costly%2520and%2520often%2520impractical.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520workflow%2520that%250Aleverages%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520generate%2520multimodal%2520datasets%2520with%250Asemantically%2520rich%2520captions%2520at%2520scale%2520from%2520plain%2520OpenStreetMap%2520%2528OSM%2529%2520data%2520for%250Aimages%2520sourced%2520from%2520the%2520Google%2520Earth%2520Engine%2520%2528GEE%2529%2520platform.%2520This%2520approach%250Afacilitates%2520the%2520generation%2520of%2520paired%2520remote%2520sensing%2520data%2520and%2520can%2520be%2520readily%250Ascaled%2520up%2520using%2520openly%2520available%2520data.%2520Within%2520this%2520framework%252C%2520we%2520present%250ARSTeller%252C%2520a%2520multimodal%2520dataset%2520comprising%2520over%25201.3%2520million%2520RS%2520images%252C%2520each%250Aaccompanied%2520by%2520two%2520descriptive%2520captions.%2520Extensive%2520experiments%2520demonstrate%2520that%250ARSTeller%2520enhances%2520the%2520performance%2520of%2520multiple%2520existing%2520vision%2520language%2520models%250Afor%2520RS%2520scene%2520understanding%2520through%2520continual%2520pre-training.%2520Our%2520methodology%250Asignificantly%2520reduces%2520the%2520manual%2520effort%2520and%2520expertise%2520needed%2520for%2520annotating%250Aremote%2520sensing%2520imagery%2520while%2520democratizing%2520access%2520to%2520high-quality%2520annotated%250Adata.%2520This%2520advancement%2520fosters%2520progress%2520in%2520visual%2520language%2520modeling%2520and%250Aencourages%2520broader%2520participation%2520in%2520remote%2520sensing%2520research%2520and%2520applications.%250AThe%2520RSTeller%2520dataset%2520is%2520available%2520at%2520https%253A//github.com/SlytherinGe/RSTeller.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14744v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSTeller%3A%20Scaling%20Up%20Visual%20Language%20Modeling%20in%20Remote%20Sensing%20with%0A%20%20Rich%20Linguistic%20Semantics%20from%20Openly%20Available%20Data%20and%20Large%20Language%0A%20%20Models&entry.906535625=Junyao%20Ge%20and%20Xu%20Zhang%20and%20Yang%20Zheng%20and%20Kaitai%20Guo%20and%20Jimin%20Liang&entry.1292438233=%20%20Abundant%2C%20well-annotated%20multimodal%20data%20in%20remote%20sensing%20are%20pivotal%20for%0Aaligning%20complex%20visual%20remote%20sensing%20%28RS%29%20scenes%20with%20human%20language%2C%0Aenabling%20the%20development%20of%20specialized%20vision%20language%20models%20across%20diverse%0ARS%20interpretation%20tasks.%20However%2C%20annotating%20RS%20images%20with%20rich%20linguistic%0Asemantics%20at%20scale%20demands%20expertise%20in%20RS%20and%20substantial%20human%20labor%2C%20making%0Ait%20costly%20and%20often%20impractical.%20In%20this%20study%2C%20we%20propose%20a%20workflow%20that%0Aleverages%20large%20language%20models%20%28LLMs%29%20to%20generate%20multimodal%20datasets%20with%0Asemantically%20rich%20captions%20at%20scale%20from%20plain%20OpenStreetMap%20%28OSM%29%20data%20for%0Aimages%20sourced%20from%20the%20Google%20Earth%20Engine%20%28GEE%29%20platform.%20This%20approach%0Afacilitates%20the%20generation%20of%20paired%20remote%20sensing%20data%20and%20can%20be%20readily%0Ascaled%20up%20using%20openly%20available%20data.%20Within%20this%20framework%2C%20we%20present%0ARSTeller%2C%20a%20multimodal%20dataset%20comprising%20over%201.3%20million%20RS%20images%2C%20each%0Aaccompanied%20by%20two%20descriptive%20captions.%20Extensive%20experiments%20demonstrate%20that%0ARSTeller%20enhances%20the%20performance%20of%20multiple%20existing%20vision%20language%20models%0Afor%20RS%20scene%20understanding%20through%20continual%20pre-training.%20Our%20methodology%0Asignificantly%20reduces%20the%20manual%20effort%20and%20expertise%20needed%20for%20annotating%0Aremote%20sensing%20imagery%20while%20democratizing%20access%20to%20high-quality%20annotated%0Adata.%20This%20advancement%20fosters%20progress%20in%20visual%20language%20modeling%20and%0Aencourages%20broader%20participation%20in%20remote%20sensing%20research%20and%20applications.%0AThe%20RSTeller%20dataset%20is%20available%20at%20https%3A//github.com/SlytherinGe/RSTeller.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14744v2&entry.124074799=Read"},
{"title": "GIFT: A Framework for Global Interpretable Faithful Textual Explanations\n  of Vision Classifiers", "author": "\u00c9loi Zablocki and Valentin Gerard and Amaia Cardiel and Eric Gaussier and Matthieu Cord and Eduardo Valle", "abstract": "  Understanding deep models is crucial for deploying them in safety-critical\napplications. We introduce GIFT, a framework for deriving post-hoc, global,\ninterpretable, and faithful textual explanations for vision classifiers. GIFT\nstarts from local faithful visual counterfactual explanations and employs\n(vision) language models to translate those into global textual explanations.\nCrucially, GIFT provides a verification stage measuring the causal effect of\nthe proposed explanations on the classifier decision. Through experiments\nacross diverse datasets, including CLEVR, CelebA, and BDD, we demonstrate that\nGIFT effectively reveals meaningful insights, uncovering tasks, concepts, and\nbiases used by deep vision classifiers. The framework is released at\nhttps://github.com/valeoai/GIFT.\n", "link": "http://arxiv.org/abs/2411.15605v2", "date": "2025-02-03", "relevancy": 2.6405, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5337}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5272}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIFT%3A%20A%20Framework%20for%20Global%20Interpretable%20Faithful%20Textual%20Explanations%0A%20%20of%20Vision%20Classifiers&body=Title%3A%20GIFT%3A%20A%20Framework%20for%20Global%20Interpretable%20Faithful%20Textual%20Explanations%0A%20%20of%20Vision%20Classifiers%0AAuthor%3A%20%C3%89loi%20Zablocki%20and%20Valentin%20Gerard%20and%20Amaia%20Cardiel%20and%20Eric%20Gaussier%20and%20Matthieu%20Cord%20and%20Eduardo%20Valle%0AAbstract%3A%20%20%20Understanding%20deep%20models%20is%20crucial%20for%20deploying%20them%20in%20safety-critical%0Aapplications.%20We%20introduce%20GIFT%2C%20a%20framework%20for%20deriving%20post-hoc%2C%20global%2C%0Ainterpretable%2C%20and%20faithful%20textual%20explanations%20for%20vision%20classifiers.%20GIFT%0Astarts%20from%20local%20faithful%20visual%20counterfactual%20explanations%20and%20employs%0A%28vision%29%20language%20models%20to%20translate%20those%20into%20global%20textual%20explanations.%0ACrucially%2C%20GIFT%20provides%20a%20verification%20stage%20measuring%20the%20causal%20effect%20of%0Athe%20proposed%20explanations%20on%20the%20classifier%20decision.%20Through%20experiments%0Aacross%20diverse%20datasets%2C%20including%20CLEVR%2C%20CelebA%2C%20and%20BDD%2C%20we%20demonstrate%20that%0AGIFT%20effectively%20reveals%20meaningful%20insights%2C%20uncovering%20tasks%2C%20concepts%2C%20and%0Abiases%20used%20by%20deep%20vision%20classifiers.%20The%20framework%20is%20released%20at%0Ahttps%3A//github.com/valeoai/GIFT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15605v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIFT%253A%2520A%2520Framework%2520for%2520Global%2520Interpretable%2520Faithful%2520Textual%2520Explanations%250A%2520%2520of%2520Vision%2520Classifiers%26entry.906535625%3D%25C3%2589loi%2520Zablocki%2520and%2520Valentin%2520Gerard%2520and%2520Amaia%2520Cardiel%2520and%2520Eric%2520Gaussier%2520and%2520Matthieu%2520Cord%2520and%2520Eduardo%2520Valle%26entry.1292438233%3D%2520%2520Understanding%2520deep%2520models%2520is%2520crucial%2520for%2520deploying%2520them%2520in%2520safety-critical%250Aapplications.%2520We%2520introduce%2520GIFT%252C%2520a%2520framework%2520for%2520deriving%2520post-hoc%252C%2520global%252C%250Ainterpretable%252C%2520and%2520faithful%2520textual%2520explanations%2520for%2520vision%2520classifiers.%2520GIFT%250Astarts%2520from%2520local%2520faithful%2520visual%2520counterfactual%2520explanations%2520and%2520employs%250A%2528vision%2529%2520language%2520models%2520to%2520translate%2520those%2520into%2520global%2520textual%2520explanations.%250ACrucially%252C%2520GIFT%2520provides%2520a%2520verification%2520stage%2520measuring%2520the%2520causal%2520effect%2520of%250Athe%2520proposed%2520explanations%2520on%2520the%2520classifier%2520decision.%2520Through%2520experiments%250Aacross%2520diverse%2520datasets%252C%2520including%2520CLEVR%252C%2520CelebA%252C%2520and%2520BDD%252C%2520we%2520demonstrate%2520that%250AGIFT%2520effectively%2520reveals%2520meaningful%2520insights%252C%2520uncovering%2520tasks%252C%2520concepts%252C%2520and%250Abiases%2520used%2520by%2520deep%2520vision%2520classifiers.%2520The%2520framework%2520is%2520released%2520at%250Ahttps%253A//github.com/valeoai/GIFT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15605v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIFT%3A%20A%20Framework%20for%20Global%20Interpretable%20Faithful%20Textual%20Explanations%0A%20%20of%20Vision%20Classifiers&entry.906535625=%C3%89loi%20Zablocki%20and%20Valentin%20Gerard%20and%20Amaia%20Cardiel%20and%20Eric%20Gaussier%20and%20Matthieu%20Cord%20and%20Eduardo%20Valle&entry.1292438233=%20%20Understanding%20deep%20models%20is%20crucial%20for%20deploying%20them%20in%20safety-critical%0Aapplications.%20We%20introduce%20GIFT%2C%20a%20framework%20for%20deriving%20post-hoc%2C%20global%2C%0Ainterpretable%2C%20and%20faithful%20textual%20explanations%20for%20vision%20classifiers.%20GIFT%0Astarts%20from%20local%20faithful%20visual%20counterfactual%20explanations%20and%20employs%0A%28vision%29%20language%20models%20to%20translate%20those%20into%20global%20textual%20explanations.%0ACrucially%2C%20GIFT%20provides%20a%20verification%20stage%20measuring%20the%20causal%20effect%20of%0Athe%20proposed%20explanations%20on%20the%20classifier%20decision.%20Through%20experiments%0Aacross%20diverse%20datasets%2C%20including%20CLEVR%2C%20CelebA%2C%20and%20BDD%2C%20we%20demonstrate%20that%0AGIFT%20effectively%20reveals%20meaningful%20insights%2C%20uncovering%20tasks%2C%20concepts%2C%20and%0Abiases%20used%20by%20deep%20vision%20classifiers.%20The%20framework%20is%20released%20at%0Ahttps%3A//github.com/valeoai/GIFT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15605v2&entry.124074799=Read"},
{"title": "How Do the Architecture and Optimizer Affect Representation Learning? On\n  the Training Dynamics of Representations in Deep Neural Networks", "author": "Yuval Sharon and Yehuda Dar", "abstract": "  In this paper, we elucidate how representations in deep neural networks\n(DNNs) evolve during training. Our focus is on overparameterized learning\nsettings where the training continues much after the trained DNN starts to\nperfectly fit its training data. We examine the evolution of learned\nrepresentations along the entire training process. We explore the\nrepresentational similarity of DNN layers, each layer with respect to its own\nrepresentations throughout the training process. For this, we use two\nsimilarity metrics: (1) The centered kernel alignment (CKA) similarity; (2)\nSimilarity of decision regions of linear classifier probes that we train for\nthe DNN layers. We visualize and analyze the decision regions of the DNN output\nand the layer probes during the DNN training to show how they geometrically\nevolve. Our extensive experiments discover training dynamics patterns that can\nemerge in layers depending on the relative layer-depth, architecture and\noptimizer. Among our findings: (i) The training phases, including those related\nto memorization, are more distinguishable in SGD training than in Adam\ntraining, and for Vision Transformer (ViT) than for ResNet; (ii) Unlike ResNet,\nthe ViT layers have synchronized dynamics of representation learning.\n", "link": "http://arxiv.org/abs/2405.17377v2", "date": "2025-02-03", "relevancy": 2.5441, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5231}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5159}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Do%20the%20Architecture%20and%20Optimizer%20Affect%20Representation%20Learning%3F%20On%0A%20%20the%20Training%20Dynamics%20of%20Representations%20in%20Deep%20Neural%20Networks&body=Title%3A%20How%20Do%20the%20Architecture%20and%20Optimizer%20Affect%20Representation%20Learning%3F%20On%0A%20%20the%20Training%20Dynamics%20of%20Representations%20in%20Deep%20Neural%20Networks%0AAuthor%3A%20Yuval%20Sharon%20and%20Yehuda%20Dar%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20elucidate%20how%20representations%20in%20deep%20neural%20networks%0A%28DNNs%29%20evolve%20during%20training.%20Our%20focus%20is%20on%20overparameterized%20learning%0Asettings%20where%20the%20training%20continues%20much%20after%20the%20trained%20DNN%20starts%20to%0Aperfectly%20fit%20its%20training%20data.%20We%20examine%20the%20evolution%20of%20learned%0Arepresentations%20along%20the%20entire%20training%20process.%20We%20explore%20the%0Arepresentational%20similarity%20of%20DNN%20layers%2C%20each%20layer%20with%20respect%20to%20its%20own%0Arepresentations%20throughout%20the%20training%20process.%20For%20this%2C%20we%20use%20two%0Asimilarity%20metrics%3A%20%281%29%20The%20centered%20kernel%20alignment%20%28CKA%29%20similarity%3B%20%282%29%0ASimilarity%20of%20decision%20regions%20of%20linear%20classifier%20probes%20that%20we%20train%20for%0Athe%20DNN%20layers.%20We%20visualize%20and%20analyze%20the%20decision%20regions%20of%20the%20DNN%20output%0Aand%20the%20layer%20probes%20during%20the%20DNN%20training%20to%20show%20how%20they%20geometrically%0Aevolve.%20Our%20extensive%20experiments%20discover%20training%20dynamics%20patterns%20that%20can%0Aemerge%20in%20layers%20depending%20on%20the%20relative%20layer-depth%2C%20architecture%20and%0Aoptimizer.%20Among%20our%20findings%3A%20%28i%29%20The%20training%20phases%2C%20including%20those%20related%0Ato%20memorization%2C%20are%20more%20distinguishable%20in%20SGD%20training%20than%20in%20Adam%0Atraining%2C%20and%20for%20Vision%20Transformer%20%28ViT%29%20than%20for%20ResNet%3B%20%28ii%29%20Unlike%20ResNet%2C%0Athe%20ViT%20layers%20have%20synchronized%20dynamics%20of%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17377v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Do%2520the%2520Architecture%2520and%2520Optimizer%2520Affect%2520Representation%2520Learning%253F%2520On%250A%2520%2520the%2520Training%2520Dynamics%2520of%2520Representations%2520in%2520Deep%2520Neural%2520Networks%26entry.906535625%3DYuval%2520Sharon%2520and%2520Yehuda%2520Dar%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520elucidate%2520how%2520representations%2520in%2520deep%2520neural%2520networks%250A%2528DNNs%2529%2520evolve%2520during%2520training.%2520Our%2520focus%2520is%2520on%2520overparameterized%2520learning%250Asettings%2520where%2520the%2520training%2520continues%2520much%2520after%2520the%2520trained%2520DNN%2520starts%2520to%250Aperfectly%2520fit%2520its%2520training%2520data.%2520We%2520examine%2520the%2520evolution%2520of%2520learned%250Arepresentations%2520along%2520the%2520entire%2520training%2520process.%2520We%2520explore%2520the%250Arepresentational%2520similarity%2520of%2520DNN%2520layers%252C%2520each%2520layer%2520with%2520respect%2520to%2520its%2520own%250Arepresentations%2520throughout%2520the%2520training%2520process.%2520For%2520this%252C%2520we%2520use%2520two%250Asimilarity%2520metrics%253A%2520%25281%2529%2520The%2520centered%2520kernel%2520alignment%2520%2528CKA%2529%2520similarity%253B%2520%25282%2529%250ASimilarity%2520of%2520decision%2520regions%2520of%2520linear%2520classifier%2520probes%2520that%2520we%2520train%2520for%250Athe%2520DNN%2520layers.%2520We%2520visualize%2520and%2520analyze%2520the%2520decision%2520regions%2520of%2520the%2520DNN%2520output%250Aand%2520the%2520layer%2520probes%2520during%2520the%2520DNN%2520training%2520to%2520show%2520how%2520they%2520geometrically%250Aevolve.%2520Our%2520extensive%2520experiments%2520discover%2520training%2520dynamics%2520patterns%2520that%2520can%250Aemerge%2520in%2520layers%2520depending%2520on%2520the%2520relative%2520layer-depth%252C%2520architecture%2520and%250Aoptimizer.%2520Among%2520our%2520findings%253A%2520%2528i%2529%2520The%2520training%2520phases%252C%2520including%2520those%2520related%250Ato%2520memorization%252C%2520are%2520more%2520distinguishable%2520in%2520SGD%2520training%2520than%2520in%2520Adam%250Atraining%252C%2520and%2520for%2520Vision%2520Transformer%2520%2528ViT%2529%2520than%2520for%2520ResNet%253B%2520%2528ii%2529%2520Unlike%2520ResNet%252C%250Athe%2520ViT%2520layers%2520have%2520synchronized%2520dynamics%2520of%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17377v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Do%20the%20Architecture%20and%20Optimizer%20Affect%20Representation%20Learning%3F%20On%0A%20%20the%20Training%20Dynamics%20of%20Representations%20in%20Deep%20Neural%20Networks&entry.906535625=Yuval%20Sharon%20and%20Yehuda%20Dar&entry.1292438233=%20%20In%20this%20paper%2C%20we%20elucidate%20how%20representations%20in%20deep%20neural%20networks%0A%28DNNs%29%20evolve%20during%20training.%20Our%20focus%20is%20on%20overparameterized%20learning%0Asettings%20where%20the%20training%20continues%20much%20after%20the%20trained%20DNN%20starts%20to%0Aperfectly%20fit%20its%20training%20data.%20We%20examine%20the%20evolution%20of%20learned%0Arepresentations%20along%20the%20entire%20training%20process.%20We%20explore%20the%0Arepresentational%20similarity%20of%20DNN%20layers%2C%20each%20layer%20with%20respect%20to%20its%20own%0Arepresentations%20throughout%20the%20training%20process.%20For%20this%2C%20we%20use%20two%0Asimilarity%20metrics%3A%20%281%29%20The%20centered%20kernel%20alignment%20%28CKA%29%20similarity%3B%20%282%29%0ASimilarity%20of%20decision%20regions%20of%20linear%20classifier%20probes%20that%20we%20train%20for%0Athe%20DNN%20layers.%20We%20visualize%20and%20analyze%20the%20decision%20regions%20of%20the%20DNN%20output%0Aand%20the%20layer%20probes%20during%20the%20DNN%20training%20to%20show%20how%20they%20geometrically%0Aevolve.%20Our%20extensive%20experiments%20discover%20training%20dynamics%20patterns%20that%20can%0Aemerge%20in%20layers%20depending%20on%20the%20relative%20layer-depth%2C%20architecture%20and%0Aoptimizer.%20Among%20our%20findings%3A%20%28i%29%20The%20training%20phases%2C%20including%20those%20related%0Ato%20memorization%2C%20are%20more%20distinguishable%20in%20SGD%20training%20than%20in%20Adam%0Atraining%2C%20and%20for%20Vision%20Transformer%20%28ViT%29%20than%20for%20ResNet%3B%20%28ii%29%20Unlike%20ResNet%2C%0Athe%20ViT%20layers%20have%20synchronized%20dynamics%20of%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17377v2&entry.124074799=Read"},
{"title": "SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions", "author": "Dominik Wagner and Alexander Churchill and Siddharth Sigtia and Erik Marchi", "abstract": "  In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline.\n", "link": "http://arxiv.org/abs/2501.19377v2", "date": "2025-02-03", "relevancy": 2.4956, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SELMA%3A%20A%20Speech-Enabled%20Language%20Model%20for%20Virtual%20Assistant%0A%20%20Interactions&body=Title%3A%20SELMA%3A%20A%20Speech-Enabled%20Language%20Model%20for%20Virtual%20Assistant%0A%20%20Interactions%0AAuthor%3A%20Dominik%20Wagner%20and%20Alexander%20Churchill%20and%20Siddharth%20Sigtia%20and%20Erik%20Marchi%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20and%20evaluate%20SELMA%2C%20a%20Speech-Enabled%20Language%20Model%0Afor%20virtual%20Assistant%20interactions%20that%20integrates%20audio%20and%20text%20as%20inputs%20to%0Aa%20Large%20Language%20Model%20%28LLM%29.%20SELMA%20is%20designed%20to%20handle%20three%20primary%20and%20two%0Aauxiliary%20tasks%20related%20to%20interactions%20with%20virtual%20assistants%20simultaneously%0Awithin%20a%20single%20end-to-end%20model.%20We%20employ%20low-rank%20adaptation%20modules%20for%0Aparameter-efficient%20training%20of%20both%20the%20audio%20encoder%20and%20the%20LLM.%0AAdditionally%2C%20we%20implement%20a%20feature%20pooling%20strategy%20enabling%20the%20system%20to%0Arecognize%20global%20patterns%20and%20improve%20accuracy%20on%20tasks%20less%20reliant%20on%0Aindividual%20sequence%20elements.%20Experimental%20results%20on%20Voice%20Trigger%20%28VT%29%0Adetection%2C%20Device-Directed%20Speech%20Detection%20%28DDSD%29%2C%20and%20Automatic%20Speech%0ARecognition%20%28ASR%29%2C%20demonstrate%20that%20our%20approach%20both%20simplifies%20the%20typical%0Ainput%20processing%20pipeline%20of%20virtual%20assistants%20significantly%20and%20also%20improves%0Aperformance%20compared%20to%20dedicated%20models%20for%20each%20individual%20task.%20SELMA%20yields%0Arelative%20Equal-Error%20Rate%20improvements%20of%2064%25%20on%20the%20VT%20detection%20task%2C%20and%2022%25%0Aon%20DDSD%2C%20while%20also%20achieving%20word%20error%20rates%20close%20to%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19377v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSELMA%253A%2520A%2520Speech-Enabled%2520Language%2520Model%2520for%2520Virtual%2520Assistant%250A%2520%2520Interactions%26entry.906535625%3DDominik%2520Wagner%2520and%2520Alexander%2520Churchill%2520and%2520Siddharth%2520Sigtia%2520and%2520Erik%2520Marchi%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520and%2520evaluate%2520SELMA%252C%2520a%2520Speech-Enabled%2520Language%2520Model%250Afor%2520virtual%2520Assistant%2520interactions%2520that%2520integrates%2520audio%2520and%2520text%2520as%2520inputs%2520to%250Aa%2520Large%2520Language%2520Model%2520%2528LLM%2529.%2520SELMA%2520is%2520designed%2520to%2520handle%2520three%2520primary%2520and%2520two%250Aauxiliary%2520tasks%2520related%2520to%2520interactions%2520with%2520virtual%2520assistants%2520simultaneously%250Awithin%2520a%2520single%2520end-to-end%2520model.%2520We%2520employ%2520low-rank%2520adaptation%2520modules%2520for%250Aparameter-efficient%2520training%2520of%2520both%2520the%2520audio%2520encoder%2520and%2520the%2520LLM.%250AAdditionally%252C%2520we%2520implement%2520a%2520feature%2520pooling%2520strategy%2520enabling%2520the%2520system%2520to%250Arecognize%2520global%2520patterns%2520and%2520improve%2520accuracy%2520on%2520tasks%2520less%2520reliant%2520on%250Aindividual%2520sequence%2520elements.%2520Experimental%2520results%2520on%2520Voice%2520Trigger%2520%2528VT%2529%250Adetection%252C%2520Device-Directed%2520Speech%2520Detection%2520%2528DDSD%2529%252C%2520and%2520Automatic%2520Speech%250ARecognition%2520%2528ASR%2529%252C%2520demonstrate%2520that%2520our%2520approach%2520both%2520simplifies%2520the%2520typical%250Ainput%2520processing%2520pipeline%2520of%2520virtual%2520assistants%2520significantly%2520and%2520also%2520improves%250Aperformance%2520compared%2520to%2520dedicated%2520models%2520for%2520each%2520individual%2520task.%2520SELMA%2520yields%250Arelative%2520Equal-Error%2520Rate%2520improvements%2520of%252064%2525%2520on%2520the%2520VT%2520detection%2520task%252C%2520and%252022%2525%250Aon%2520DDSD%252C%2520while%2520also%2520achieving%2520word%2520error%2520rates%2520close%2520to%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19377v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SELMA%3A%20A%20Speech-Enabled%20Language%20Model%20for%20Virtual%20Assistant%0A%20%20Interactions&entry.906535625=Dominik%20Wagner%20and%20Alexander%20Churchill%20and%20Siddharth%20Sigtia%20and%20Erik%20Marchi&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20and%20evaluate%20SELMA%2C%20a%20Speech-Enabled%20Language%20Model%0Afor%20virtual%20Assistant%20interactions%20that%20integrates%20audio%20and%20text%20as%20inputs%20to%0Aa%20Large%20Language%20Model%20%28LLM%29.%20SELMA%20is%20designed%20to%20handle%20three%20primary%20and%20two%0Aauxiliary%20tasks%20related%20to%20interactions%20with%20virtual%20assistants%20simultaneously%0Awithin%20a%20single%20end-to-end%20model.%20We%20employ%20low-rank%20adaptation%20modules%20for%0Aparameter-efficient%20training%20of%20both%20the%20audio%20encoder%20and%20the%20LLM.%0AAdditionally%2C%20we%20implement%20a%20feature%20pooling%20strategy%20enabling%20the%20system%20to%0Arecognize%20global%20patterns%20and%20improve%20accuracy%20on%20tasks%20less%20reliant%20on%0Aindividual%20sequence%20elements.%20Experimental%20results%20on%20Voice%20Trigger%20%28VT%29%0Adetection%2C%20Device-Directed%20Speech%20Detection%20%28DDSD%29%2C%20and%20Automatic%20Speech%0ARecognition%20%28ASR%29%2C%20demonstrate%20that%20our%20approach%20both%20simplifies%20the%20typical%0Ainput%20processing%20pipeline%20of%20virtual%20assistants%20significantly%20and%20also%20improves%0Aperformance%20compared%20to%20dedicated%20models%20for%20each%20individual%20task.%20SELMA%20yields%0Arelative%20Equal-Error%20Rate%20improvements%20of%2064%25%20on%20the%20VT%20detection%20task%2C%20and%2022%25%0Aon%20DDSD%2C%20while%20also%20achieving%20word%20error%20rates%20close%20to%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19377v2&entry.124074799=Read"},
{"title": "STOP! Benchmarking Large Language Models with Sensitivity Testing on\n  Offensive Progressions", "author": "Robert Morabito and Sangmitra Madhusudan and Tyler McDonald and Ali Emami", "abstract": "  Mitigating explicit and implicit biases in Large Language Models (LLMs) has\nbecome a critical focus in the field of natural language processing. However,\nmany current methodologies evaluate scenarios in isolation, without considering\nthe broader context or the spectrum of potential biases within each situation.\nTo address this, we introduce the Sensitivity Testing on Offensive Progressions\n(STOP) dataset, which includes 450 offensive progressions containing 2,700\nunique sentences of varying severity that progressively escalate from less to\nmore explicitly offensive. Covering a broad spectrum of 9 demographics and 46\nsub-demographics, STOP ensures inclusivity and comprehensive coverage. We\nevaluate several leading closed- and open-source models, including GPT-4,\nMixtral, and Llama 3. Our findings reveal that even the best-performing models\ndetect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We\nalso demonstrate how aligning models with human judgments on STOP can improve\nmodel answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs\nby up to 191%, while maintaining or even improving performance. STOP presents a\nnovel framework for assessing the complex nature of biases in LLMs, which will\nenable more effective bias mitigation strategies and facilitates the creation\nof fairer language models.\n", "link": "http://arxiv.org/abs/2409.13843v2", "date": "2025-02-03", "relevancy": 2.4917, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5067}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STOP%21%20Benchmarking%20Large%20Language%20Models%20with%20Sensitivity%20Testing%20on%0A%20%20Offensive%20Progressions&body=Title%3A%20STOP%21%20Benchmarking%20Large%20Language%20Models%20with%20Sensitivity%20Testing%20on%0A%20%20Offensive%20Progressions%0AAuthor%3A%20Robert%20Morabito%20and%20Sangmitra%20Madhusudan%20and%20Tyler%20McDonald%20and%20Ali%20Emami%0AAbstract%3A%20%20%20Mitigating%20explicit%20and%20implicit%20biases%20in%20Large%20Language%20Models%20%28LLMs%29%20has%0Abecome%20a%20critical%20focus%20in%20the%20field%20of%20natural%20language%20processing.%20However%2C%0Amany%20current%20methodologies%20evaluate%20scenarios%20in%20isolation%2C%20without%20considering%0Athe%20broader%20context%20or%20the%20spectrum%20of%20potential%20biases%20within%20each%20situation.%0ATo%20address%20this%2C%20we%20introduce%20the%20Sensitivity%20Testing%20on%20Offensive%20Progressions%0A%28STOP%29%20dataset%2C%20which%20includes%20450%20offensive%20progressions%20containing%202%2C700%0Aunique%20sentences%20of%20varying%20severity%20that%20progressively%20escalate%20from%20less%20to%0Amore%20explicitly%20offensive.%20Covering%20a%20broad%20spectrum%20of%209%20demographics%20and%2046%0Asub-demographics%2C%20STOP%20ensures%20inclusivity%20and%20comprehensive%20coverage.%20We%0Aevaluate%20several%20leading%20closed-%20and%20open-source%20models%2C%20including%20GPT-4%2C%0AMixtral%2C%20and%20Llama%203.%20Our%20findings%20reveal%20that%20even%20the%20best-performing%20models%0Adetect%20bias%20inconsistently%2C%20with%20success%20rates%20ranging%20from%2019.3%25%20to%2069.8%25.%20We%0Aalso%20demonstrate%20how%20aligning%20models%20with%20human%20judgments%20on%20STOP%20can%20improve%0Amodel%20answer%20rates%20on%20sensitive%20tasks%20such%20as%20BBQ%2C%20StereoSet%2C%20and%20CrowS-Pairs%0Aby%20up%20to%20191%25%2C%20while%20maintaining%20or%20even%20improving%20performance.%20STOP%20presents%20a%0Anovel%20framework%20for%20assessing%20the%20complex%20nature%20of%20biases%20in%20LLMs%2C%20which%20will%0Aenable%20more%20effective%20bias%20mitigation%20strategies%20and%20facilitates%20the%20creation%0Aof%20fairer%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13843v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTOP%2521%2520Benchmarking%2520Large%2520Language%2520Models%2520with%2520Sensitivity%2520Testing%2520on%250A%2520%2520Offensive%2520Progressions%26entry.906535625%3DRobert%2520Morabito%2520and%2520Sangmitra%2520Madhusudan%2520and%2520Tyler%2520McDonald%2520and%2520Ali%2520Emami%26entry.1292438233%3D%2520%2520Mitigating%2520explicit%2520and%2520implicit%2520biases%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%250Abecome%2520a%2520critical%2520focus%2520in%2520the%2520field%2520of%2520natural%2520language%2520processing.%2520However%252C%250Amany%2520current%2520methodologies%2520evaluate%2520scenarios%2520in%2520isolation%252C%2520without%2520considering%250Athe%2520broader%2520context%2520or%2520the%2520spectrum%2520of%2520potential%2520biases%2520within%2520each%2520situation.%250ATo%2520address%2520this%252C%2520we%2520introduce%2520the%2520Sensitivity%2520Testing%2520on%2520Offensive%2520Progressions%250A%2528STOP%2529%2520dataset%252C%2520which%2520includes%2520450%2520offensive%2520progressions%2520containing%25202%252C700%250Aunique%2520sentences%2520of%2520varying%2520severity%2520that%2520progressively%2520escalate%2520from%2520less%2520to%250Amore%2520explicitly%2520offensive.%2520Covering%2520a%2520broad%2520spectrum%2520of%25209%2520demographics%2520and%252046%250Asub-demographics%252C%2520STOP%2520ensures%2520inclusivity%2520and%2520comprehensive%2520coverage.%2520We%250Aevaluate%2520several%2520leading%2520closed-%2520and%2520open-source%2520models%252C%2520including%2520GPT-4%252C%250AMixtral%252C%2520and%2520Llama%25203.%2520Our%2520findings%2520reveal%2520that%2520even%2520the%2520best-performing%2520models%250Adetect%2520bias%2520inconsistently%252C%2520with%2520success%2520rates%2520ranging%2520from%252019.3%2525%2520to%252069.8%2525.%2520We%250Aalso%2520demonstrate%2520how%2520aligning%2520models%2520with%2520human%2520judgments%2520on%2520STOP%2520can%2520improve%250Amodel%2520answer%2520rates%2520on%2520sensitive%2520tasks%2520such%2520as%2520BBQ%252C%2520StereoSet%252C%2520and%2520CrowS-Pairs%250Aby%2520up%2520to%2520191%2525%252C%2520while%2520maintaining%2520or%2520even%2520improving%2520performance.%2520STOP%2520presents%2520a%250Anovel%2520framework%2520for%2520assessing%2520the%2520complex%2520nature%2520of%2520biases%2520in%2520LLMs%252C%2520which%2520will%250Aenable%2520more%2520effective%2520bias%2520mitigation%2520strategies%2520and%2520facilitates%2520the%2520creation%250Aof%2520fairer%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13843v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STOP%21%20Benchmarking%20Large%20Language%20Models%20with%20Sensitivity%20Testing%20on%0A%20%20Offensive%20Progressions&entry.906535625=Robert%20Morabito%20and%20Sangmitra%20Madhusudan%20and%20Tyler%20McDonald%20and%20Ali%20Emami&entry.1292438233=%20%20Mitigating%20explicit%20and%20implicit%20biases%20in%20Large%20Language%20Models%20%28LLMs%29%20has%0Abecome%20a%20critical%20focus%20in%20the%20field%20of%20natural%20language%20processing.%20However%2C%0Amany%20current%20methodologies%20evaluate%20scenarios%20in%20isolation%2C%20without%20considering%0Athe%20broader%20context%20or%20the%20spectrum%20of%20potential%20biases%20within%20each%20situation.%0ATo%20address%20this%2C%20we%20introduce%20the%20Sensitivity%20Testing%20on%20Offensive%20Progressions%0A%28STOP%29%20dataset%2C%20which%20includes%20450%20offensive%20progressions%20containing%202%2C700%0Aunique%20sentences%20of%20varying%20severity%20that%20progressively%20escalate%20from%20less%20to%0Amore%20explicitly%20offensive.%20Covering%20a%20broad%20spectrum%20of%209%20demographics%20and%2046%0Asub-demographics%2C%20STOP%20ensures%20inclusivity%20and%20comprehensive%20coverage.%20We%0Aevaluate%20several%20leading%20closed-%20and%20open-source%20models%2C%20including%20GPT-4%2C%0AMixtral%2C%20and%20Llama%203.%20Our%20findings%20reveal%20that%20even%20the%20best-performing%20models%0Adetect%20bias%20inconsistently%2C%20with%20success%20rates%20ranging%20from%2019.3%25%20to%2069.8%25.%20We%0Aalso%20demonstrate%20how%20aligning%20models%20with%20human%20judgments%20on%20STOP%20can%20improve%0Amodel%20answer%20rates%20on%20sensitive%20tasks%20such%20as%20BBQ%2C%20StereoSet%2C%20and%20CrowS-Pairs%0Aby%20up%20to%20191%25%2C%20while%20maintaining%20or%20even%20improving%20performance.%20STOP%20presents%20a%0Anovel%20framework%20for%20assessing%20the%20complex%20nature%20of%20biases%20in%20LLMs%2C%20which%20will%0Aenable%20more%20effective%20bias%20mitigation%20strategies%20and%20facilitates%20the%20creation%0Aof%20fairer%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13843v2&entry.124074799=Read"},
{"title": "Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth\n  Observation Applications", "author": "Daniela Szwarcman and Sujit Roy and Paolo Fraccaro and \u00deorsteinn El\u00ed G\u00edslason and Benedikt Blumenstiel and Rinki Ghosal and Pedro Henrique de Oliveira and Joao Lucas de Sousa Almeida and Rocco Sedona and Yanghui Kang and Srija Chakraborty and Sizhe Wang and Carlos Gomes and Ankur Kumar and Myscon Truong and Denys Godwin and Hyunho Lee and Chia-Yu Hsu and Ata Akbari Asanjan and Besart Mujeci and Disha Shidham and Trevor Keenan and Paulo Arevalo and Wenwen Li and Hamed Alemohammad and Pontus Olofsson and Christopher Hain and Robert Kennedy and Bianca Zadrozny and David Bell and Gabriele Cavallaro and Campbell Watson and Manil Maskey and Rahul Ramachandran and Juan Bernabe Moreno", "abstract": "  This technical report presents Prithvi-EO-2.0, a new geospatial foundation\nmodel that offers significant improvements over its predecessor,\nPrithvi-EO-1.0. Trained on 4.2M global time series samples from NASA's\nHarmonized Landsat and Sentinel-2 data archive at 30m resolution, the new 300M\nand 600M parameter models incorporate temporal and location embeddings for\nenhanced performance across various geospatial tasks. Through extensive\nbenchmarking with GEO-Bench, the 600M version outperforms the previous\nPrithvi-EO model by 8\\% across a range of tasks. It also outperforms six other\ngeospatial foundation models when benchmarked on remote sensing tasks from\ndifferent domains and resolutions (i.e. from 0.1m to 15m). The results\ndemonstrate the versatility of the model in both classical earth observation\nand high-resolution applications. Early involvement of end-users and subject\nmatter experts (SMEs) are among the key factors that contributed to the\nproject's success. In particular, SME involvement allowed for constant feedback\non model and dataset design, as well as successful customization for diverse\nSME-led applications in disaster response, land use and crop mapping, and\necosystem dynamics monitoring. Prithvi-EO-2.0 is available on Hugging Face and\nIBM terratorch, with additional resources on GitHub. The project exemplifies\nthe Trusted Open Science approach embraced by all involved organizations.\n", "link": "http://arxiv.org/abs/2412.02732v2", "date": "2025-02-03", "relevancy": 2.4741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prithvi-EO-2.0%3A%20A%20Versatile%20Multi-Temporal%20Foundation%20Model%20for%20Earth%0A%20%20Observation%20Applications&body=Title%3A%20Prithvi-EO-2.0%3A%20A%20Versatile%20Multi-Temporal%20Foundation%20Model%20for%20Earth%0A%20%20Observation%20Applications%0AAuthor%3A%20Daniela%20Szwarcman%20and%20Sujit%20Roy%20and%20Paolo%20Fraccaro%20and%20%C3%9Eorsteinn%20El%C3%AD%20G%C3%ADslason%20and%20Benedikt%20Blumenstiel%20and%20Rinki%20Ghosal%20and%20Pedro%20Henrique%20de%20Oliveira%20and%20Joao%20Lucas%20de%20Sousa%20Almeida%20and%20Rocco%20Sedona%20and%20Yanghui%20Kang%20and%20Srija%20Chakraborty%20and%20Sizhe%20Wang%20and%20Carlos%20Gomes%20and%20Ankur%20Kumar%20and%20Myscon%20Truong%20and%20Denys%20Godwin%20and%20Hyunho%20Lee%20and%20Chia-Yu%20Hsu%20and%20Ata%20Akbari%20Asanjan%20and%20Besart%20Mujeci%20and%20Disha%20Shidham%20and%20Trevor%20Keenan%20and%20Paulo%20Arevalo%20and%20Wenwen%20Li%20and%20Hamed%20Alemohammad%20and%20Pontus%20Olofsson%20and%20Christopher%20Hain%20and%20Robert%20Kennedy%20and%20Bianca%20Zadrozny%20and%20David%20Bell%20and%20Gabriele%20Cavallaro%20and%20Campbell%20Watson%20and%20Manil%20Maskey%20and%20Rahul%20Ramachandran%20and%20Juan%20Bernabe%20Moreno%0AAbstract%3A%20%20%20This%20technical%20report%20presents%20Prithvi-EO-2.0%2C%20a%20new%20geospatial%20foundation%0Amodel%20that%20offers%20significant%20improvements%20over%20its%20predecessor%2C%0APrithvi-EO-1.0.%20Trained%20on%204.2M%20global%20time%20series%20samples%20from%20NASA%27s%0AHarmonized%20Landsat%20and%20Sentinel-2%20data%20archive%20at%2030m%20resolution%2C%20the%20new%20300M%0Aand%20600M%20parameter%20models%20incorporate%20temporal%20and%20location%20embeddings%20for%0Aenhanced%20performance%20across%20various%20geospatial%20tasks.%20Through%20extensive%0Abenchmarking%20with%20GEO-Bench%2C%20the%20600M%20version%20outperforms%20the%20previous%0APrithvi-EO%20model%20by%208%5C%25%20across%20a%20range%20of%20tasks.%20It%20also%20outperforms%20six%20other%0Ageospatial%20foundation%20models%20when%20benchmarked%20on%20remote%20sensing%20tasks%20from%0Adifferent%20domains%20and%20resolutions%20%28i.e.%20from%200.1m%20to%2015m%29.%20The%20results%0Ademonstrate%20the%20versatility%20of%20the%20model%20in%20both%20classical%20earth%20observation%0Aand%20high-resolution%20applications.%20Early%20involvement%20of%20end-users%20and%20subject%0Amatter%20experts%20%28SMEs%29%20are%20among%20the%20key%20factors%20that%20contributed%20to%20the%0Aproject%27s%20success.%20In%20particular%2C%20SME%20involvement%20allowed%20for%20constant%20feedback%0Aon%20model%20and%20dataset%20design%2C%20as%20well%20as%20successful%20customization%20for%20diverse%0ASME-led%20applications%20in%20disaster%20response%2C%20land%20use%20and%20crop%20mapping%2C%20and%0Aecosystem%20dynamics%20monitoring.%20Prithvi-EO-2.0%20is%20available%20on%20Hugging%20Face%20and%0AIBM%20terratorch%2C%20with%20additional%20resources%20on%20GitHub.%20The%20project%20exemplifies%0Athe%20Trusted%20Open%20Science%20approach%20embraced%20by%20all%20involved%20organizations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02732v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrithvi-EO-2.0%253A%2520A%2520Versatile%2520Multi-Temporal%2520Foundation%2520Model%2520for%2520Earth%250A%2520%2520Observation%2520Applications%26entry.906535625%3DDaniela%2520Szwarcman%2520and%2520Sujit%2520Roy%2520and%2520Paolo%2520Fraccaro%2520and%2520%25C3%259Eorsteinn%2520El%25C3%25AD%2520G%25C3%25ADslason%2520and%2520Benedikt%2520Blumenstiel%2520and%2520Rinki%2520Ghosal%2520and%2520Pedro%2520Henrique%2520de%2520Oliveira%2520and%2520Joao%2520Lucas%2520de%2520Sousa%2520Almeida%2520and%2520Rocco%2520Sedona%2520and%2520Yanghui%2520Kang%2520and%2520Srija%2520Chakraborty%2520and%2520Sizhe%2520Wang%2520and%2520Carlos%2520Gomes%2520and%2520Ankur%2520Kumar%2520and%2520Myscon%2520Truong%2520and%2520Denys%2520Godwin%2520and%2520Hyunho%2520Lee%2520and%2520Chia-Yu%2520Hsu%2520and%2520Ata%2520Akbari%2520Asanjan%2520and%2520Besart%2520Mujeci%2520and%2520Disha%2520Shidham%2520and%2520Trevor%2520Keenan%2520and%2520Paulo%2520Arevalo%2520and%2520Wenwen%2520Li%2520and%2520Hamed%2520Alemohammad%2520and%2520Pontus%2520Olofsson%2520and%2520Christopher%2520Hain%2520and%2520Robert%2520Kennedy%2520and%2520Bianca%2520Zadrozny%2520and%2520David%2520Bell%2520and%2520Gabriele%2520Cavallaro%2520and%2520Campbell%2520Watson%2520and%2520Manil%2520Maskey%2520and%2520Rahul%2520Ramachandran%2520and%2520Juan%2520Bernabe%2520Moreno%26entry.1292438233%3D%2520%2520This%2520technical%2520report%2520presents%2520Prithvi-EO-2.0%252C%2520a%2520new%2520geospatial%2520foundation%250Amodel%2520that%2520offers%2520significant%2520improvements%2520over%2520its%2520predecessor%252C%250APrithvi-EO-1.0.%2520Trained%2520on%25204.2M%2520global%2520time%2520series%2520samples%2520from%2520NASA%2527s%250AHarmonized%2520Landsat%2520and%2520Sentinel-2%2520data%2520archive%2520at%252030m%2520resolution%252C%2520the%2520new%2520300M%250Aand%2520600M%2520parameter%2520models%2520incorporate%2520temporal%2520and%2520location%2520embeddings%2520for%250Aenhanced%2520performance%2520across%2520various%2520geospatial%2520tasks.%2520Through%2520extensive%250Abenchmarking%2520with%2520GEO-Bench%252C%2520the%2520600M%2520version%2520outperforms%2520the%2520previous%250APrithvi-EO%2520model%2520by%25208%255C%2525%2520across%2520a%2520range%2520of%2520tasks.%2520It%2520also%2520outperforms%2520six%2520other%250Ageospatial%2520foundation%2520models%2520when%2520benchmarked%2520on%2520remote%2520sensing%2520tasks%2520from%250Adifferent%2520domains%2520and%2520resolutions%2520%2528i.e.%2520from%25200.1m%2520to%252015m%2529.%2520The%2520results%250Ademonstrate%2520the%2520versatility%2520of%2520the%2520model%2520in%2520both%2520classical%2520earth%2520observation%250Aand%2520high-resolution%2520applications.%2520Early%2520involvement%2520of%2520end-users%2520and%2520subject%250Amatter%2520experts%2520%2528SMEs%2529%2520are%2520among%2520the%2520key%2520factors%2520that%2520contributed%2520to%2520the%250Aproject%2527s%2520success.%2520In%2520particular%252C%2520SME%2520involvement%2520allowed%2520for%2520constant%2520feedback%250Aon%2520model%2520and%2520dataset%2520design%252C%2520as%2520well%2520as%2520successful%2520customization%2520for%2520diverse%250ASME-led%2520applications%2520in%2520disaster%2520response%252C%2520land%2520use%2520and%2520crop%2520mapping%252C%2520and%250Aecosystem%2520dynamics%2520monitoring.%2520Prithvi-EO-2.0%2520is%2520available%2520on%2520Hugging%2520Face%2520and%250AIBM%2520terratorch%252C%2520with%2520additional%2520resources%2520on%2520GitHub.%2520The%2520project%2520exemplifies%250Athe%2520Trusted%2520Open%2520Science%2520approach%2520embraced%2520by%2520all%2520involved%2520organizations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02732v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prithvi-EO-2.0%3A%20A%20Versatile%20Multi-Temporal%20Foundation%20Model%20for%20Earth%0A%20%20Observation%20Applications&entry.906535625=Daniela%20Szwarcman%20and%20Sujit%20Roy%20and%20Paolo%20Fraccaro%20and%20%C3%9Eorsteinn%20El%C3%AD%20G%C3%ADslason%20and%20Benedikt%20Blumenstiel%20and%20Rinki%20Ghosal%20and%20Pedro%20Henrique%20de%20Oliveira%20and%20Joao%20Lucas%20de%20Sousa%20Almeida%20and%20Rocco%20Sedona%20and%20Yanghui%20Kang%20and%20Srija%20Chakraborty%20and%20Sizhe%20Wang%20and%20Carlos%20Gomes%20and%20Ankur%20Kumar%20and%20Myscon%20Truong%20and%20Denys%20Godwin%20and%20Hyunho%20Lee%20and%20Chia-Yu%20Hsu%20and%20Ata%20Akbari%20Asanjan%20and%20Besart%20Mujeci%20and%20Disha%20Shidham%20and%20Trevor%20Keenan%20and%20Paulo%20Arevalo%20and%20Wenwen%20Li%20and%20Hamed%20Alemohammad%20and%20Pontus%20Olofsson%20and%20Christopher%20Hain%20and%20Robert%20Kennedy%20and%20Bianca%20Zadrozny%20and%20David%20Bell%20and%20Gabriele%20Cavallaro%20and%20Campbell%20Watson%20and%20Manil%20Maskey%20and%20Rahul%20Ramachandran%20and%20Juan%20Bernabe%20Moreno&entry.1292438233=%20%20This%20technical%20report%20presents%20Prithvi-EO-2.0%2C%20a%20new%20geospatial%20foundation%0Amodel%20that%20offers%20significant%20improvements%20over%20its%20predecessor%2C%0APrithvi-EO-1.0.%20Trained%20on%204.2M%20global%20time%20series%20samples%20from%20NASA%27s%0AHarmonized%20Landsat%20and%20Sentinel-2%20data%20archive%20at%2030m%20resolution%2C%20the%20new%20300M%0Aand%20600M%20parameter%20models%20incorporate%20temporal%20and%20location%20embeddings%20for%0Aenhanced%20performance%20across%20various%20geospatial%20tasks.%20Through%20extensive%0Abenchmarking%20with%20GEO-Bench%2C%20the%20600M%20version%20outperforms%20the%20previous%0APrithvi-EO%20model%20by%208%5C%25%20across%20a%20range%20of%20tasks.%20It%20also%20outperforms%20six%20other%0Ageospatial%20foundation%20models%20when%20benchmarked%20on%20remote%20sensing%20tasks%20from%0Adifferent%20domains%20and%20resolutions%20%28i.e.%20from%200.1m%20to%2015m%29.%20The%20results%0Ademonstrate%20the%20versatility%20of%20the%20model%20in%20both%20classical%20earth%20observation%0Aand%20high-resolution%20applications.%20Early%20involvement%20of%20end-users%20and%20subject%0Amatter%20experts%20%28SMEs%29%20are%20among%20the%20key%20factors%20that%20contributed%20to%20the%0Aproject%27s%20success.%20In%20particular%2C%20SME%20involvement%20allowed%20for%20constant%20feedback%0Aon%20model%20and%20dataset%20design%2C%20as%20well%20as%20successful%20customization%20for%20diverse%0ASME-led%20applications%20in%20disaster%20response%2C%20land%20use%20and%20crop%20mapping%2C%20and%0Aecosystem%20dynamics%20monitoring.%20Prithvi-EO-2.0%20is%20available%20on%20Hugging%20Face%20and%0AIBM%20terratorch%2C%20with%20additional%20resources%20on%20GitHub.%20The%20project%20exemplifies%0Athe%20Trusted%20Open%20Science%20approach%20embraced%20by%20all%20involved%20organizations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02732v2&entry.124074799=Read"},
{"title": "BLens: Contrastive Captioning of Binary Functions using Ensemble\n  Embedding", "author": "Tristan Benoit and Yunru Wang and Moritz Dannehl and Johannes Kinder", "abstract": "  Function names can greatly aid human reverse engineers, which has spurred the\ndevelopment of machine learning-based approaches to predicting function names\nin stripped binaries. Much current work in this area now uses transformers,\napplying a metaphor of machine translation from code to function names. Still,\nfunction naming models face challenges in generalizing to projects unrelated to\nthe training set. In this paper, we take a completely new approach by\ntransferring advances in automated image captioning to the domain of binary\nreverse engineering, such that different parts of a binary function can be\nassociated with parts of its name. We propose BLens, which combines multiple\nbinary function embeddings into a new ensemble representation, aligns it with\nthe name representation latent space via a contrastive learning approach, and\ngenerates function names with a transformer architecture tailored for function\nnames. Our experiments demonstrate that BLens significantly outperforms the\nstate of the art. In the usual setting of splitting per binary, we achieve an\n$F_1$ score of 0.79 compared to 0.70. In the cross-project setting, which\nemphasizes generalizability, we achieve an $F_1$ score of 0.46 compared to\n0.29. Finally, in an experimental setting reducing shared components across\nprojects, we achieve an $F_1$ score of $0.32$ compared to $0.19$.\n", "link": "http://arxiv.org/abs/2409.07889v2", "date": "2025-02-03", "relevancy": 2.468, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLens%3A%20Contrastive%20Captioning%20of%20Binary%20Functions%20using%20Ensemble%0A%20%20Embedding&body=Title%3A%20BLens%3A%20Contrastive%20Captioning%20of%20Binary%20Functions%20using%20Ensemble%0A%20%20Embedding%0AAuthor%3A%20Tristan%20Benoit%20and%20Yunru%20Wang%20and%20Moritz%20Dannehl%20and%20Johannes%20Kinder%0AAbstract%3A%20%20%20Function%20names%20can%20greatly%20aid%20human%20reverse%20engineers%2C%20which%20has%20spurred%20the%0Adevelopment%20of%20machine%20learning-based%20approaches%20to%20predicting%20function%20names%0Ain%20stripped%20binaries.%20Much%20current%20work%20in%20this%20area%20now%20uses%20transformers%2C%0Aapplying%20a%20metaphor%20of%20machine%20translation%20from%20code%20to%20function%20names.%20Still%2C%0Afunction%20naming%20models%20face%20challenges%20in%20generalizing%20to%20projects%20unrelated%20to%0Athe%20training%20set.%20In%20this%20paper%2C%20we%20take%20a%20completely%20new%20approach%20by%0Atransferring%20advances%20in%20automated%20image%20captioning%20to%20the%20domain%20of%20binary%0Areverse%20engineering%2C%20such%20that%20different%20parts%20of%20a%20binary%20function%20can%20be%0Aassociated%20with%20parts%20of%20its%20name.%20We%20propose%20BLens%2C%20which%20combines%20multiple%0Abinary%20function%20embeddings%20into%20a%20new%20ensemble%20representation%2C%20aligns%20it%20with%0Athe%20name%20representation%20latent%20space%20via%20a%20contrastive%20learning%20approach%2C%20and%0Agenerates%20function%20names%20with%20a%20transformer%20architecture%20tailored%20for%20function%0Anames.%20Our%20experiments%20demonstrate%20that%20BLens%20significantly%20outperforms%20the%0Astate%20of%20the%20art.%20In%20the%20usual%20setting%20of%20splitting%20per%20binary%2C%20we%20achieve%20an%0A%24F_1%24%20score%20of%200.79%20compared%20to%200.70.%20In%20the%20cross-project%20setting%2C%20which%0Aemphasizes%20generalizability%2C%20we%20achieve%20an%20%24F_1%24%20score%20of%200.46%20compared%20to%0A0.29.%20Finally%2C%20in%20an%20experimental%20setting%20reducing%20shared%20components%20across%0Aprojects%2C%20we%20achieve%20an%20%24F_1%24%20score%20of%20%240.32%24%20compared%20to%20%240.19%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07889v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLens%253A%2520Contrastive%2520Captioning%2520of%2520Binary%2520Functions%2520using%2520Ensemble%250A%2520%2520Embedding%26entry.906535625%3DTristan%2520Benoit%2520and%2520Yunru%2520Wang%2520and%2520Moritz%2520Dannehl%2520and%2520Johannes%2520Kinder%26entry.1292438233%3D%2520%2520Function%2520names%2520can%2520greatly%2520aid%2520human%2520reverse%2520engineers%252C%2520which%2520has%2520spurred%2520the%250Adevelopment%2520of%2520machine%2520learning-based%2520approaches%2520to%2520predicting%2520function%2520names%250Ain%2520stripped%2520binaries.%2520Much%2520current%2520work%2520in%2520this%2520area%2520now%2520uses%2520transformers%252C%250Aapplying%2520a%2520metaphor%2520of%2520machine%2520translation%2520from%2520code%2520to%2520function%2520names.%2520Still%252C%250Afunction%2520naming%2520models%2520face%2520challenges%2520in%2520generalizing%2520to%2520projects%2520unrelated%2520to%250Athe%2520training%2520set.%2520In%2520this%2520paper%252C%2520we%2520take%2520a%2520completely%2520new%2520approach%2520by%250Atransferring%2520advances%2520in%2520automated%2520image%2520captioning%2520to%2520the%2520domain%2520of%2520binary%250Areverse%2520engineering%252C%2520such%2520that%2520different%2520parts%2520of%2520a%2520binary%2520function%2520can%2520be%250Aassociated%2520with%2520parts%2520of%2520its%2520name.%2520We%2520propose%2520BLens%252C%2520which%2520combines%2520multiple%250Abinary%2520function%2520embeddings%2520into%2520a%2520new%2520ensemble%2520representation%252C%2520aligns%2520it%2520with%250Athe%2520name%2520representation%2520latent%2520space%2520via%2520a%2520contrastive%2520learning%2520approach%252C%2520and%250Agenerates%2520function%2520names%2520with%2520a%2520transformer%2520architecture%2520tailored%2520for%2520function%250Anames.%2520Our%2520experiments%2520demonstrate%2520that%2520BLens%2520significantly%2520outperforms%2520the%250Astate%2520of%2520the%2520art.%2520In%2520the%2520usual%2520setting%2520of%2520splitting%2520per%2520binary%252C%2520we%2520achieve%2520an%250A%2524F_1%2524%2520score%2520of%25200.79%2520compared%2520to%25200.70.%2520In%2520the%2520cross-project%2520setting%252C%2520which%250Aemphasizes%2520generalizability%252C%2520we%2520achieve%2520an%2520%2524F_1%2524%2520score%2520of%25200.46%2520compared%2520to%250A0.29.%2520Finally%252C%2520in%2520an%2520experimental%2520setting%2520reducing%2520shared%2520components%2520across%250Aprojects%252C%2520we%2520achieve%2520an%2520%2524F_1%2524%2520score%2520of%2520%25240.32%2524%2520compared%2520to%2520%25240.19%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07889v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLens%3A%20Contrastive%20Captioning%20of%20Binary%20Functions%20using%20Ensemble%0A%20%20Embedding&entry.906535625=Tristan%20Benoit%20and%20Yunru%20Wang%20and%20Moritz%20Dannehl%20and%20Johannes%20Kinder&entry.1292438233=%20%20Function%20names%20can%20greatly%20aid%20human%20reverse%20engineers%2C%20which%20has%20spurred%20the%0Adevelopment%20of%20machine%20learning-based%20approaches%20to%20predicting%20function%20names%0Ain%20stripped%20binaries.%20Much%20current%20work%20in%20this%20area%20now%20uses%20transformers%2C%0Aapplying%20a%20metaphor%20of%20machine%20translation%20from%20code%20to%20function%20names.%20Still%2C%0Afunction%20naming%20models%20face%20challenges%20in%20generalizing%20to%20projects%20unrelated%20to%0Athe%20training%20set.%20In%20this%20paper%2C%20we%20take%20a%20completely%20new%20approach%20by%0Atransferring%20advances%20in%20automated%20image%20captioning%20to%20the%20domain%20of%20binary%0Areverse%20engineering%2C%20such%20that%20different%20parts%20of%20a%20binary%20function%20can%20be%0Aassociated%20with%20parts%20of%20its%20name.%20We%20propose%20BLens%2C%20which%20combines%20multiple%0Abinary%20function%20embeddings%20into%20a%20new%20ensemble%20representation%2C%20aligns%20it%20with%0Athe%20name%20representation%20latent%20space%20via%20a%20contrastive%20learning%20approach%2C%20and%0Agenerates%20function%20names%20with%20a%20transformer%20architecture%20tailored%20for%20function%0Anames.%20Our%20experiments%20demonstrate%20that%20BLens%20significantly%20outperforms%20the%0Astate%20of%20the%20art.%20In%20the%20usual%20setting%20of%20splitting%20per%20binary%2C%20we%20achieve%20an%0A%24F_1%24%20score%20of%200.79%20compared%20to%200.70.%20In%20the%20cross-project%20setting%2C%20which%0Aemphasizes%20generalizability%2C%20we%20achieve%20an%20%24F_1%24%20score%20of%200.46%20compared%20to%0A0.29.%20Finally%2C%20in%20an%20experimental%20setting%20reducing%20shared%20components%20across%0Aprojects%2C%20we%20achieve%20an%20%24F_1%24%20score%20of%20%240.32%24%20compared%20to%20%240.19%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07889v2&entry.124074799=Read"},
{"title": "Learning from Linear Algebra: A Graph Neural Network Approach to\n  Preconditioner Design for Conjugate Gradient Solvers", "author": "Vladislav Trifonov and Alexander Rudikov and Oleg Iliev and Yuri M. Laevsky and Ivan Oseledets and Ekaterina Muravleva", "abstract": "  Large linear systems are ubiquitous in modern computational science and\nengineering. The main recipe for solving them is the use of Krylov subspace\niterative methods with well-designed preconditioners. Recently, GNNs have been\nshown to be a promising tool for designing preconditioners to reduce the\noverall computational cost of iterative methods by constructing them more\nefficiently than with classical linear algebra techniques. Preconditioners\ndesigned with these approaches cannot outperform those designed with classical\nmethods in terms of the number of iterations in CG. In our work, we recall\nwell-established preconditioners from linear algebra and use them as a starting\npoint for training the GNN to obtain preconditioners that reduce the condition\nnumber of the system more significantly than classical preconditioners.\nNumerical experiments show that our approach outperforms both classical and\nneural network-based methods for an important class of parametric partial\ndifferential equations. We also provide a heuristic justification for the loss\nfunction used and show that preconditioners obtained by learning with this loss\nfunction reduce the condition number in a more desirable way for CG.\n", "link": "http://arxiv.org/abs/2405.15557v3", "date": "2025-02-03", "relevancy": 2.4545, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5036}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5014}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Linear%20Algebra%3A%20A%20Graph%20Neural%20Network%20Approach%20to%0A%20%20Preconditioner%20Design%20for%20Conjugate%20Gradient%20Solvers&body=Title%3A%20Learning%20from%20Linear%20Algebra%3A%20A%20Graph%20Neural%20Network%20Approach%20to%0A%20%20Preconditioner%20Design%20for%20Conjugate%20Gradient%20Solvers%0AAuthor%3A%20Vladislav%20Trifonov%20and%20Alexander%20Rudikov%20and%20Oleg%20Iliev%20and%20Yuri%20M.%20Laevsky%20and%20Ivan%20Oseledets%20and%20Ekaterina%20Muravleva%0AAbstract%3A%20%20%20Large%20linear%20systems%20are%20ubiquitous%20in%20modern%20computational%20science%20and%0Aengineering.%20The%20main%20recipe%20for%20solving%20them%20is%20the%20use%20of%20Krylov%20subspace%0Aiterative%20methods%20with%20well-designed%20preconditioners.%20Recently%2C%20GNNs%20have%20been%0Ashown%20to%20be%20a%20promising%20tool%20for%20designing%20preconditioners%20to%20reduce%20the%0Aoverall%20computational%20cost%20of%20iterative%20methods%20by%20constructing%20them%20more%0Aefficiently%20than%20with%20classical%20linear%20algebra%20techniques.%20Preconditioners%0Adesigned%20with%20these%20approaches%20cannot%20outperform%20those%20designed%20with%20classical%0Amethods%20in%20terms%20of%20the%20number%20of%20iterations%20in%20CG.%20In%20our%20work%2C%20we%20recall%0Awell-established%20preconditioners%20from%20linear%20algebra%20and%20use%20them%20as%20a%20starting%0Apoint%20for%20training%20the%20GNN%20to%20obtain%20preconditioners%20that%20reduce%20the%20condition%0Anumber%20of%20the%20system%20more%20significantly%20than%20classical%20preconditioners.%0ANumerical%20experiments%20show%20that%20our%20approach%20outperforms%20both%20classical%20and%0Aneural%20network-based%20methods%20for%20an%20important%20class%20of%20parametric%20partial%0Adifferential%20equations.%20We%20also%20provide%20a%20heuristic%20justification%20for%20the%20loss%0Afunction%20used%20and%20show%20that%20preconditioners%20obtained%20by%20learning%20with%20this%20loss%0Afunction%20reduce%20the%20condition%20number%20in%20a%20more%20desirable%20way%20for%20CG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15557v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Linear%2520Algebra%253A%2520A%2520Graph%2520Neural%2520Network%2520Approach%2520to%250A%2520%2520Preconditioner%2520Design%2520for%2520Conjugate%2520Gradient%2520Solvers%26entry.906535625%3DVladislav%2520Trifonov%2520and%2520Alexander%2520Rudikov%2520and%2520Oleg%2520Iliev%2520and%2520Yuri%2520M.%2520Laevsky%2520and%2520Ivan%2520Oseledets%2520and%2520Ekaterina%2520Muravleva%26entry.1292438233%3D%2520%2520Large%2520linear%2520systems%2520are%2520ubiquitous%2520in%2520modern%2520computational%2520science%2520and%250Aengineering.%2520The%2520main%2520recipe%2520for%2520solving%2520them%2520is%2520the%2520use%2520of%2520Krylov%2520subspace%250Aiterative%2520methods%2520with%2520well-designed%2520preconditioners.%2520Recently%252C%2520GNNs%2520have%2520been%250Ashown%2520to%2520be%2520a%2520promising%2520tool%2520for%2520designing%2520preconditioners%2520to%2520reduce%2520the%250Aoverall%2520computational%2520cost%2520of%2520iterative%2520methods%2520by%2520constructing%2520them%2520more%250Aefficiently%2520than%2520with%2520classical%2520linear%2520algebra%2520techniques.%2520Preconditioners%250Adesigned%2520with%2520these%2520approaches%2520cannot%2520outperform%2520those%2520designed%2520with%2520classical%250Amethods%2520in%2520terms%2520of%2520the%2520number%2520of%2520iterations%2520in%2520CG.%2520In%2520our%2520work%252C%2520we%2520recall%250Awell-established%2520preconditioners%2520from%2520linear%2520algebra%2520and%2520use%2520them%2520as%2520a%2520starting%250Apoint%2520for%2520training%2520the%2520GNN%2520to%2520obtain%2520preconditioners%2520that%2520reduce%2520the%2520condition%250Anumber%2520of%2520the%2520system%2520more%2520significantly%2520than%2520classical%2520preconditioners.%250ANumerical%2520experiments%2520show%2520that%2520our%2520approach%2520outperforms%2520both%2520classical%2520and%250Aneural%2520network-based%2520methods%2520for%2520an%2520important%2520class%2520of%2520parametric%2520partial%250Adifferential%2520equations.%2520We%2520also%2520provide%2520a%2520heuristic%2520justification%2520for%2520the%2520loss%250Afunction%2520used%2520and%2520show%2520that%2520preconditioners%2520obtained%2520by%2520learning%2520with%2520this%2520loss%250Afunction%2520reduce%2520the%2520condition%2520number%2520in%2520a%2520more%2520desirable%2520way%2520for%2520CG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15557v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Linear%20Algebra%3A%20A%20Graph%20Neural%20Network%20Approach%20to%0A%20%20Preconditioner%20Design%20for%20Conjugate%20Gradient%20Solvers&entry.906535625=Vladislav%20Trifonov%20and%20Alexander%20Rudikov%20and%20Oleg%20Iliev%20and%20Yuri%20M.%20Laevsky%20and%20Ivan%20Oseledets%20and%20Ekaterina%20Muravleva&entry.1292438233=%20%20Large%20linear%20systems%20are%20ubiquitous%20in%20modern%20computational%20science%20and%0Aengineering.%20The%20main%20recipe%20for%20solving%20them%20is%20the%20use%20of%20Krylov%20subspace%0Aiterative%20methods%20with%20well-designed%20preconditioners.%20Recently%2C%20GNNs%20have%20been%0Ashown%20to%20be%20a%20promising%20tool%20for%20designing%20preconditioners%20to%20reduce%20the%0Aoverall%20computational%20cost%20of%20iterative%20methods%20by%20constructing%20them%20more%0Aefficiently%20than%20with%20classical%20linear%20algebra%20techniques.%20Preconditioners%0Adesigned%20with%20these%20approaches%20cannot%20outperform%20those%20designed%20with%20classical%0Amethods%20in%20terms%20of%20the%20number%20of%20iterations%20in%20CG.%20In%20our%20work%2C%20we%20recall%0Awell-established%20preconditioners%20from%20linear%20algebra%20and%20use%20them%20as%20a%20starting%0Apoint%20for%20training%20the%20GNN%20to%20obtain%20preconditioners%20that%20reduce%20the%20condition%0Anumber%20of%20the%20system%20more%20significantly%20than%20classical%20preconditioners.%0ANumerical%20experiments%20show%20that%20our%20approach%20outperforms%20both%20classical%20and%0Aneural%20network-based%20methods%20for%20an%20important%20class%20of%20parametric%20partial%0Adifferential%20equations.%20We%20also%20provide%20a%20heuristic%20justification%20for%20the%20loss%0Afunction%20used%20and%20show%20that%20preconditioners%20obtained%20by%20learning%20with%20this%20loss%0Afunction%20reduce%20the%20condition%20number%20in%20a%20more%20desirable%20way%20for%20CG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15557v3&entry.124074799=Read"},
{"title": "Can sparse autoencoders make sense of latent representations?", "author": "Viktoria Schuster", "abstract": "  Sparse autoencoders (SAEs) have lately been used to uncover interpretable\nlatent features in large language models. Here, we explore their potential for\ndecomposing latent representations in complex and high-dimensional biological\ndata, where the underlying variables are often unknown. Using simulated data,\nwe find that latent representations can encode observable and directly\nconnected upstream hidden variables in superposition. The degree to which they\nare learned depends on the type of variable and the model architecture,\nfavoring shallow and wide networks. Superpositions, however, are not\nidentifiable if the generative variables are unknown. SAEs can recover these\nvariables and their structure with respect to the observables. Applied to\nsingle-cell multi-omics data, we show that SAEs can uncover key biological\nprocesses. We further present an automated method for linking SAE features to\nbiological concepts to enable large-scale analysis of single-cell expression\nmodels.\n", "link": "http://arxiv.org/abs/2410.11468v2", "date": "2025-02-03", "relevancy": 2.4456, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20sparse%20autoencoders%20make%20sense%20of%20latent%20representations%3F&body=Title%3A%20Can%20sparse%20autoencoders%20make%20sense%20of%20latent%20representations%3F%0AAuthor%3A%20Viktoria%20Schuster%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20lately%20been%20used%20to%20uncover%20interpretable%0Alatent%20features%20in%20large%20language%20models.%20Here%2C%20we%20explore%20their%20potential%20for%0Adecomposing%20latent%20representations%20in%20complex%20and%20high-dimensional%20biological%0Adata%2C%20where%20the%20underlying%20variables%20are%20often%20unknown.%20Using%20simulated%20data%2C%0Awe%20find%20that%20latent%20representations%20can%20encode%20observable%20and%20directly%0Aconnected%20upstream%20hidden%20variables%20in%20superposition.%20The%20degree%20to%20which%20they%0Aare%20learned%20depends%20on%20the%20type%20of%20variable%20and%20the%20model%20architecture%2C%0Afavoring%20shallow%20and%20wide%20networks.%20Superpositions%2C%20however%2C%20are%20not%0Aidentifiable%20if%20the%20generative%20variables%20are%20unknown.%20SAEs%20can%20recover%20these%0Avariables%20and%20their%20structure%20with%20respect%20to%20the%20observables.%20Applied%20to%0Asingle-cell%20multi-omics%20data%2C%20we%20show%20that%20SAEs%20can%20uncover%20key%20biological%0Aprocesses.%20We%20further%20present%20an%20automated%20method%20for%20linking%20SAE%20features%20to%0Abiological%20concepts%20to%20enable%20large-scale%20analysis%20of%20single-cell%20expression%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520sparse%2520autoencoders%2520make%2520sense%2520of%2520latent%2520representations%253F%26entry.906535625%3DViktoria%2520Schuster%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520have%2520lately%2520been%2520used%2520to%2520uncover%2520interpretable%250Alatent%2520features%2520in%2520large%2520language%2520models.%2520Here%252C%2520we%2520explore%2520their%2520potential%2520for%250Adecomposing%2520latent%2520representations%2520in%2520complex%2520and%2520high-dimensional%2520biological%250Adata%252C%2520where%2520the%2520underlying%2520variables%2520are%2520often%2520unknown.%2520Using%2520simulated%2520data%252C%250Awe%2520find%2520that%2520latent%2520representations%2520can%2520encode%2520observable%2520and%2520directly%250Aconnected%2520upstream%2520hidden%2520variables%2520in%2520superposition.%2520The%2520degree%2520to%2520which%2520they%250Aare%2520learned%2520depends%2520on%2520the%2520type%2520of%2520variable%2520and%2520the%2520model%2520architecture%252C%250Afavoring%2520shallow%2520and%2520wide%2520networks.%2520Superpositions%252C%2520however%252C%2520are%2520not%250Aidentifiable%2520if%2520the%2520generative%2520variables%2520are%2520unknown.%2520SAEs%2520can%2520recover%2520these%250Avariables%2520and%2520their%2520structure%2520with%2520respect%2520to%2520the%2520observables.%2520Applied%2520to%250Asingle-cell%2520multi-omics%2520data%252C%2520we%2520show%2520that%2520SAEs%2520can%2520uncover%2520key%2520biological%250Aprocesses.%2520We%2520further%2520present%2520an%2520automated%2520method%2520for%2520linking%2520SAE%2520features%2520to%250Abiological%2520concepts%2520to%2520enable%2520large-scale%2520analysis%2520of%2520single-cell%2520expression%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20sparse%20autoencoders%20make%20sense%20of%20latent%20representations%3F&entry.906535625=Viktoria%20Schuster&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20lately%20been%20used%20to%20uncover%20interpretable%0Alatent%20features%20in%20large%20language%20models.%20Here%2C%20we%20explore%20their%20potential%20for%0Adecomposing%20latent%20representations%20in%20complex%20and%20high-dimensional%20biological%0Adata%2C%20where%20the%20underlying%20variables%20are%20often%20unknown.%20Using%20simulated%20data%2C%0Awe%20find%20that%20latent%20representations%20can%20encode%20observable%20and%20directly%0Aconnected%20upstream%20hidden%20variables%20in%20superposition.%20The%20degree%20to%20which%20they%0Aare%20learned%20depends%20on%20the%20type%20of%20variable%20and%20the%20model%20architecture%2C%0Afavoring%20shallow%20and%20wide%20networks.%20Superpositions%2C%20however%2C%20are%20not%0Aidentifiable%20if%20the%20generative%20variables%20are%20unknown.%20SAEs%20can%20recover%20these%0Avariables%20and%20their%20structure%20with%20respect%20to%20the%20observables.%20Applied%20to%0Asingle-cell%20multi-omics%20data%2C%20we%20show%20that%20SAEs%20can%20uncover%20key%20biological%0Aprocesses.%20We%20further%20present%20an%20automated%20method%20for%20linking%20SAE%20features%20to%0Abiological%20concepts%20to%20enable%20large-scale%20analysis%20of%20single-cell%20expression%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11468v2&entry.124074799=Read"},
{"title": "SEED4D: A Synthetic Ego--Exo Dynamic 4D Data Generator, Driving Dataset\n  and Benchmark", "author": "Marius K\u00e4stingsch\u00e4fer and Th\u00e9o Gieruc and Sebastian Bernhard and Dylan Campbell and Eldar Insafutdinov and Eyvaz Najafli and Thomas Brox", "abstract": "  Models for egocentric 3D and 4D reconstruction, including few-shot\ninterpolation and extrapolation settings, can benefit from having images from\nexocentric viewpoints as supervision signals. No existing dataset provides the\nnecessary mixture of complex, dynamic, and multi-view data. To facilitate the\ndevelopment of 3D and 4D reconstruction methods in the autonomous driving\ncontext, we propose a Synthetic Ego--Exo Dynamic 4D (SEED4D) data generator and\ndataset. We present a customizable, easy-to-use data generator for\nspatio-temporal multi-view data creation. Our open-source data generator allows\nthe creation of synthetic data for camera setups commonly used in the NuScenes,\nKITTI360, and Waymo datasets. Additionally, SEED4D encompasses two large-scale\nmulti-view synthetic urban scene datasets. Our static (3D) dataset encompasses\n212k inward- and outward-facing vehicle images from 2k scenes, while our\ndynamic (4D) dataset contains 16.8M images from 10k trajectories, each sampled\nat 100 points in time with egocentric images, exocentric images, and LiDAR\ndata. The datasets and the data generator can be found at\nhttps://seed4d.github.io/.\n", "link": "http://arxiv.org/abs/2412.00730v2", "date": "2025-02-03", "relevancy": 2.4312, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6111}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6111}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEED4D%3A%20A%20Synthetic%20Ego--Exo%20Dynamic%204D%20Data%20Generator%2C%20Driving%20Dataset%0A%20%20and%20Benchmark&body=Title%3A%20SEED4D%3A%20A%20Synthetic%20Ego--Exo%20Dynamic%204D%20Data%20Generator%2C%20Driving%20Dataset%0A%20%20and%20Benchmark%0AAuthor%3A%20Marius%20K%C3%A4stingsch%C3%A4fer%20and%20Th%C3%A9o%20Gieruc%20and%20Sebastian%20Bernhard%20and%20Dylan%20Campbell%20and%20Eldar%20Insafutdinov%20and%20Eyvaz%20Najafli%20and%20Thomas%20Brox%0AAbstract%3A%20%20%20Models%20for%20egocentric%203D%20and%204D%20reconstruction%2C%20including%20few-shot%0Ainterpolation%20and%20extrapolation%20settings%2C%20can%20benefit%20from%20having%20images%20from%0Aexocentric%20viewpoints%20as%20supervision%20signals.%20No%20existing%20dataset%20provides%20the%0Anecessary%20mixture%20of%20complex%2C%20dynamic%2C%20and%20multi-view%20data.%20To%20facilitate%20the%0Adevelopment%20of%203D%20and%204D%20reconstruction%20methods%20in%20the%20autonomous%20driving%0Acontext%2C%20we%20propose%20a%20Synthetic%20Ego--Exo%20Dynamic%204D%20%28SEED4D%29%20data%20generator%20and%0Adataset.%20We%20present%20a%20customizable%2C%20easy-to-use%20data%20generator%20for%0Aspatio-temporal%20multi-view%20data%20creation.%20Our%20open-source%20data%20generator%20allows%0Athe%20creation%20of%20synthetic%20data%20for%20camera%20setups%20commonly%20used%20in%20the%20NuScenes%2C%0AKITTI360%2C%20and%20Waymo%20datasets.%20Additionally%2C%20SEED4D%20encompasses%20two%20large-scale%0Amulti-view%20synthetic%20urban%20scene%20datasets.%20Our%20static%20%283D%29%20dataset%20encompasses%0A212k%20inward-%20and%20outward-facing%20vehicle%20images%20from%202k%20scenes%2C%20while%20our%0Adynamic%20%284D%29%20dataset%20contains%2016.8M%20images%20from%2010k%20trajectories%2C%20each%20sampled%0Aat%20100%20points%20in%20time%20with%20egocentric%20images%2C%20exocentric%20images%2C%20and%20LiDAR%0Adata.%20The%20datasets%20and%20the%20data%20generator%20can%20be%20found%20at%0Ahttps%3A//seed4d.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00730v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEED4D%253A%2520A%2520Synthetic%2520Ego--Exo%2520Dynamic%25204D%2520Data%2520Generator%252C%2520Driving%2520Dataset%250A%2520%2520and%2520Benchmark%26entry.906535625%3DMarius%2520K%25C3%25A4stingsch%25C3%25A4fer%2520and%2520Th%25C3%25A9o%2520Gieruc%2520and%2520Sebastian%2520Bernhard%2520and%2520Dylan%2520Campbell%2520and%2520Eldar%2520Insafutdinov%2520and%2520Eyvaz%2520Najafli%2520and%2520Thomas%2520Brox%26entry.1292438233%3D%2520%2520Models%2520for%2520egocentric%25203D%2520and%25204D%2520reconstruction%252C%2520including%2520few-shot%250Ainterpolation%2520and%2520extrapolation%2520settings%252C%2520can%2520benefit%2520from%2520having%2520images%2520from%250Aexocentric%2520viewpoints%2520as%2520supervision%2520signals.%2520No%2520existing%2520dataset%2520provides%2520the%250Anecessary%2520mixture%2520of%2520complex%252C%2520dynamic%252C%2520and%2520multi-view%2520data.%2520To%2520facilitate%2520the%250Adevelopment%2520of%25203D%2520and%25204D%2520reconstruction%2520methods%2520in%2520the%2520autonomous%2520driving%250Acontext%252C%2520we%2520propose%2520a%2520Synthetic%2520Ego--Exo%2520Dynamic%25204D%2520%2528SEED4D%2529%2520data%2520generator%2520and%250Adataset.%2520We%2520present%2520a%2520customizable%252C%2520easy-to-use%2520data%2520generator%2520for%250Aspatio-temporal%2520multi-view%2520data%2520creation.%2520Our%2520open-source%2520data%2520generator%2520allows%250Athe%2520creation%2520of%2520synthetic%2520data%2520for%2520camera%2520setups%2520commonly%2520used%2520in%2520the%2520NuScenes%252C%250AKITTI360%252C%2520and%2520Waymo%2520datasets.%2520Additionally%252C%2520SEED4D%2520encompasses%2520two%2520large-scale%250Amulti-view%2520synthetic%2520urban%2520scene%2520datasets.%2520Our%2520static%2520%25283D%2529%2520dataset%2520encompasses%250A212k%2520inward-%2520and%2520outward-facing%2520vehicle%2520images%2520from%25202k%2520scenes%252C%2520while%2520our%250Adynamic%2520%25284D%2529%2520dataset%2520contains%252016.8M%2520images%2520from%252010k%2520trajectories%252C%2520each%2520sampled%250Aat%2520100%2520points%2520in%2520time%2520with%2520egocentric%2520images%252C%2520exocentric%2520images%252C%2520and%2520LiDAR%250Adata.%2520The%2520datasets%2520and%2520the%2520data%2520generator%2520can%2520be%2520found%2520at%250Ahttps%253A//seed4d.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00730v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEED4D%3A%20A%20Synthetic%20Ego--Exo%20Dynamic%204D%20Data%20Generator%2C%20Driving%20Dataset%0A%20%20and%20Benchmark&entry.906535625=Marius%20K%C3%A4stingsch%C3%A4fer%20and%20Th%C3%A9o%20Gieruc%20and%20Sebastian%20Bernhard%20and%20Dylan%20Campbell%20and%20Eldar%20Insafutdinov%20and%20Eyvaz%20Najafli%20and%20Thomas%20Brox&entry.1292438233=%20%20Models%20for%20egocentric%203D%20and%204D%20reconstruction%2C%20including%20few-shot%0Ainterpolation%20and%20extrapolation%20settings%2C%20can%20benefit%20from%20having%20images%20from%0Aexocentric%20viewpoints%20as%20supervision%20signals.%20No%20existing%20dataset%20provides%20the%0Anecessary%20mixture%20of%20complex%2C%20dynamic%2C%20and%20multi-view%20data.%20To%20facilitate%20the%0Adevelopment%20of%203D%20and%204D%20reconstruction%20methods%20in%20the%20autonomous%20driving%0Acontext%2C%20we%20propose%20a%20Synthetic%20Ego--Exo%20Dynamic%204D%20%28SEED4D%29%20data%20generator%20and%0Adataset.%20We%20present%20a%20customizable%2C%20easy-to-use%20data%20generator%20for%0Aspatio-temporal%20multi-view%20data%20creation.%20Our%20open-source%20data%20generator%20allows%0Athe%20creation%20of%20synthetic%20data%20for%20camera%20setups%20commonly%20used%20in%20the%20NuScenes%2C%0AKITTI360%2C%20and%20Waymo%20datasets.%20Additionally%2C%20SEED4D%20encompasses%20two%20large-scale%0Amulti-view%20synthetic%20urban%20scene%20datasets.%20Our%20static%20%283D%29%20dataset%20encompasses%0A212k%20inward-%20and%20outward-facing%20vehicle%20images%20from%202k%20scenes%2C%20while%20our%0Adynamic%20%284D%29%20dataset%20contains%2016.8M%20images%20from%2010k%20trajectories%2C%20each%20sampled%0Aat%20100%20points%20in%20time%20with%20egocentric%20images%2C%20exocentric%20images%2C%20and%20LiDAR%0Adata.%20The%20datasets%20and%20the%20data%20generator%20can%20be%20found%20at%0Ahttps%3A//seed4d.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00730v2&entry.124074799=Read"},
{"title": "Clarify Confused Nodes via Separated Learning", "author": "Jiajun Zhou and Shengbo Gong and Xuanze Chen and Chenxuan Xie and Shanqing Yu and Qi Xuan and Xiaoniu Yang", "abstract": "  Graph neural networks (GNNs) have achieved remarkable advances in\ngraph-oriented tasks. However, real-world graphs invariably contain a certain\nproportion of heterophilous nodes, challenging the homophily assumption of\ntraditional GNNs and hindering their performance. Most existing studies\ncontinue to design generic models with shared weights between heterophilous and\nhomophilous nodes. Despite the incorporation of high-order messages or\nmulti-channel architectures, these efforts often fall short. A minority of\nstudies attempt to train different node groups separately but suffer from\ninappropriate separation metrics and low efficiency. In this paper, we first\npropose a new metric, termed Neighborhood Confusion (NC), to facilitate a more\nreliable separation of nodes. We observe that node groups with different levels\nof NC values exhibit certain differences in intra-group accuracy and visualized\nembeddings. These pave the way for Neighborhood Confusion-guided Graph\nConvolutional Network (NCGCN), in which nodes are grouped by their NC values\nand accept intra-group weight sharing and message passing. Extensive\nexperiments on both homophilous and heterophilous benchmarks demonstrate that\nour framework can effectively separate nodes and yield significant performance\nimprovement compared to the latest methods. The source code will be available\nin https://github.com/GISec-Team/NCGNN.\n", "link": "http://arxiv.org/abs/2306.02285v6", "date": "2025-02-03", "relevancy": 2.4241, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.51}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4782}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clarify%20Confused%20Nodes%20via%20Separated%20Learning&body=Title%3A%20Clarify%20Confused%20Nodes%20via%20Separated%20Learning%0AAuthor%3A%20Jiajun%20Zhou%20and%20Shengbo%20Gong%20and%20Xuanze%20Chen%20and%20Chenxuan%20Xie%20and%20Shanqing%20Yu%20and%20Qi%20Xuan%20and%20Xiaoniu%20Yang%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20achieved%20remarkable%20advances%20in%0Agraph-oriented%20tasks.%20However%2C%20real-world%20graphs%20invariably%20contain%20a%20certain%0Aproportion%20of%20heterophilous%20nodes%2C%20challenging%20the%20homophily%20assumption%20of%0Atraditional%20GNNs%20and%20hindering%20their%20performance.%20Most%20existing%20studies%0Acontinue%20to%20design%20generic%20models%20with%20shared%20weights%20between%20heterophilous%20and%0Ahomophilous%20nodes.%20Despite%20the%20incorporation%20of%20high-order%20messages%20or%0Amulti-channel%20architectures%2C%20these%20efforts%20often%20fall%20short.%20A%20minority%20of%0Astudies%20attempt%20to%20train%20different%20node%20groups%20separately%20but%20suffer%20from%0Ainappropriate%20separation%20metrics%20and%20low%20efficiency.%20In%20this%20paper%2C%20we%20first%0Apropose%20a%20new%20metric%2C%20termed%20Neighborhood%20Confusion%20%28NC%29%2C%20to%20facilitate%20a%20more%0Areliable%20separation%20of%20nodes.%20We%20observe%20that%20node%20groups%20with%20different%20levels%0Aof%20NC%20values%20exhibit%20certain%20differences%20in%20intra-group%20accuracy%20and%20visualized%0Aembeddings.%20These%20pave%20the%20way%20for%20Neighborhood%20Confusion-guided%20Graph%0AConvolutional%20Network%20%28NCGCN%29%2C%20in%20which%20nodes%20are%20grouped%20by%20their%20NC%20values%0Aand%20accept%20intra-group%20weight%20sharing%20and%20message%20passing.%20Extensive%0Aexperiments%20on%20both%20homophilous%20and%20heterophilous%20benchmarks%20demonstrate%20that%0Aour%20framework%20can%20effectively%20separate%20nodes%20and%20yield%20significant%20performance%0Aimprovement%20compared%20to%20the%20latest%20methods.%20The%20source%20code%20will%20be%20available%0Ain%20https%3A//github.com/GISec-Team/NCGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02285v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClarify%2520Confused%2520Nodes%2520via%2520Separated%2520Learning%26entry.906535625%3DJiajun%2520Zhou%2520and%2520Shengbo%2520Gong%2520and%2520Xuanze%2520Chen%2520and%2520Chenxuan%2520Xie%2520and%2520Shanqing%2520Yu%2520and%2520Qi%2520Xuan%2520and%2520Xiaoniu%2520Yang%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520achieved%2520remarkable%2520advances%2520in%250Agraph-oriented%2520tasks.%2520However%252C%2520real-world%2520graphs%2520invariably%2520contain%2520a%2520certain%250Aproportion%2520of%2520heterophilous%2520nodes%252C%2520challenging%2520the%2520homophily%2520assumption%2520of%250Atraditional%2520GNNs%2520and%2520hindering%2520their%2520performance.%2520Most%2520existing%2520studies%250Acontinue%2520to%2520design%2520generic%2520models%2520with%2520shared%2520weights%2520between%2520heterophilous%2520and%250Ahomophilous%2520nodes.%2520Despite%2520the%2520incorporation%2520of%2520high-order%2520messages%2520or%250Amulti-channel%2520architectures%252C%2520these%2520efforts%2520often%2520fall%2520short.%2520A%2520minority%2520of%250Astudies%2520attempt%2520to%2520train%2520different%2520node%2520groups%2520separately%2520but%2520suffer%2520from%250Ainappropriate%2520separation%2520metrics%2520and%2520low%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520first%250Apropose%2520a%2520new%2520metric%252C%2520termed%2520Neighborhood%2520Confusion%2520%2528NC%2529%252C%2520to%2520facilitate%2520a%2520more%250Areliable%2520separation%2520of%2520nodes.%2520We%2520observe%2520that%2520node%2520groups%2520with%2520different%2520levels%250Aof%2520NC%2520values%2520exhibit%2520certain%2520differences%2520in%2520intra-group%2520accuracy%2520and%2520visualized%250Aembeddings.%2520These%2520pave%2520the%2520way%2520for%2520Neighborhood%2520Confusion-guided%2520Graph%250AConvolutional%2520Network%2520%2528NCGCN%2529%252C%2520in%2520which%2520nodes%2520are%2520grouped%2520by%2520their%2520NC%2520values%250Aand%2520accept%2520intra-group%2520weight%2520sharing%2520and%2520message%2520passing.%2520Extensive%250Aexperiments%2520on%2520both%2520homophilous%2520and%2520heterophilous%2520benchmarks%2520demonstrate%2520that%250Aour%2520framework%2520can%2520effectively%2520separate%2520nodes%2520and%2520yield%2520significant%2520performance%250Aimprovement%2520compared%2520to%2520the%2520latest%2520methods.%2520The%2520source%2520code%2520will%2520be%2520available%250Ain%2520https%253A//github.com/GISec-Team/NCGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.02285v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clarify%20Confused%20Nodes%20via%20Separated%20Learning&entry.906535625=Jiajun%20Zhou%20and%20Shengbo%20Gong%20and%20Xuanze%20Chen%20and%20Chenxuan%20Xie%20and%20Shanqing%20Yu%20and%20Qi%20Xuan%20and%20Xiaoniu%20Yang&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20achieved%20remarkable%20advances%20in%0Agraph-oriented%20tasks.%20However%2C%20real-world%20graphs%20invariably%20contain%20a%20certain%0Aproportion%20of%20heterophilous%20nodes%2C%20challenging%20the%20homophily%20assumption%20of%0Atraditional%20GNNs%20and%20hindering%20their%20performance.%20Most%20existing%20studies%0Acontinue%20to%20design%20generic%20models%20with%20shared%20weights%20between%20heterophilous%20and%0Ahomophilous%20nodes.%20Despite%20the%20incorporation%20of%20high-order%20messages%20or%0Amulti-channel%20architectures%2C%20these%20efforts%20often%20fall%20short.%20A%20minority%20of%0Astudies%20attempt%20to%20train%20different%20node%20groups%20separately%20but%20suffer%20from%0Ainappropriate%20separation%20metrics%20and%20low%20efficiency.%20In%20this%20paper%2C%20we%20first%0Apropose%20a%20new%20metric%2C%20termed%20Neighborhood%20Confusion%20%28NC%29%2C%20to%20facilitate%20a%20more%0Areliable%20separation%20of%20nodes.%20We%20observe%20that%20node%20groups%20with%20different%20levels%0Aof%20NC%20values%20exhibit%20certain%20differences%20in%20intra-group%20accuracy%20and%20visualized%0Aembeddings.%20These%20pave%20the%20way%20for%20Neighborhood%20Confusion-guided%20Graph%0AConvolutional%20Network%20%28NCGCN%29%2C%20in%20which%20nodes%20are%20grouped%20by%20their%20NC%20values%0Aand%20accept%20intra-group%20weight%20sharing%20and%20message%20passing.%20Extensive%0Aexperiments%20on%20both%20homophilous%20and%20heterophilous%20benchmarks%20demonstrate%20that%0Aour%20framework%20can%20effectively%20separate%20nodes%20and%20yield%20significant%20performance%0Aimprovement%20compared%20to%20the%20latest%20methods.%20The%20source%20code%20will%20be%20available%0Ain%20https%3A//github.com/GISec-Team/NCGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02285v6&entry.124074799=Read"},
{"title": "Learning Fairer Representations with FairVIC", "author": "Charmaine Barker and Daniel Bethell and Dimitar Kazakov", "abstract": "  Mitigating bias in automated decision-making systems, particularly in deep\nlearning models, is a critical challenge due to nuanced definitions of\nfairness, dataset-specific biases, and the inherent trade-off between fairness\nand accuracy. To address these issues, we introduce FairVIC, an innovative\napproach that enhances fairness in neural networks by integrating variance,\ninvariance, and covariance terms into the loss function during training. Unlike\nmethods that rely on predefined fairness criteria, FairVIC abstracts fairness\nconcepts to minimise dependency on protected characteristics. We evaluate\nFairVIC against comparable bias mitigation techniques on benchmark datasets,\nconsidering both group and individual fairness, and conduct an ablation study\non the accuracy-fairness trade-off. FairVIC demonstrates significant\nimprovements ($\\approx70\\%$) in fairness across all tested metrics without\ncompromising accuracy, thus offering a robust, generalisable solution for fair\ndeep learning across diverse tasks and datasets.\n", "link": "http://arxiv.org/abs/2404.18134v2", "date": "2025-02-03", "relevancy": 2.4046, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.494}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4754}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Fairer%20Representations%20with%20FairVIC&body=Title%3A%20Learning%20Fairer%20Representations%20with%20FairVIC%0AAuthor%3A%20Charmaine%20Barker%20and%20Daniel%20Bethell%20and%20Dimitar%20Kazakov%0AAbstract%3A%20%20%20Mitigating%20bias%20in%20automated%20decision-making%20systems%2C%20particularly%20in%20deep%0Alearning%20models%2C%20is%20a%20critical%20challenge%20due%20to%20nuanced%20definitions%20of%0Afairness%2C%20dataset-specific%20biases%2C%20and%20the%20inherent%20trade-off%20between%20fairness%0Aand%20accuracy.%20To%20address%20these%20issues%2C%20we%20introduce%20FairVIC%2C%20an%20innovative%0Aapproach%20that%20enhances%20fairness%20in%20neural%20networks%20by%20integrating%20variance%2C%0Ainvariance%2C%20and%20covariance%20terms%20into%20the%20loss%20function%20during%20training.%20Unlike%0Amethods%20that%20rely%20on%20predefined%20fairness%20criteria%2C%20FairVIC%20abstracts%20fairness%0Aconcepts%20to%20minimise%20dependency%20on%20protected%20characteristics.%20We%20evaluate%0AFairVIC%20against%20comparable%20bias%20mitigation%20techniques%20on%20benchmark%20datasets%2C%0Aconsidering%20both%20group%20and%20individual%20fairness%2C%20and%20conduct%20an%20ablation%20study%0Aon%20the%20accuracy-fairness%20trade-off.%20FairVIC%20demonstrates%20significant%0Aimprovements%20%28%24%5Capprox70%5C%25%24%29%20in%20fairness%20across%20all%20tested%20metrics%20without%0Acompromising%20accuracy%2C%20thus%20offering%20a%20robust%2C%20generalisable%20solution%20for%20fair%0Adeep%20learning%20across%20diverse%20tasks%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18134v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Fairer%2520Representations%2520with%2520FairVIC%26entry.906535625%3DCharmaine%2520Barker%2520and%2520Daniel%2520Bethell%2520and%2520Dimitar%2520Kazakov%26entry.1292438233%3D%2520%2520Mitigating%2520bias%2520in%2520automated%2520decision-making%2520systems%252C%2520particularly%2520in%2520deep%250Alearning%2520models%252C%2520is%2520a%2520critical%2520challenge%2520due%2520to%2520nuanced%2520definitions%2520of%250Afairness%252C%2520dataset-specific%2520biases%252C%2520and%2520the%2520inherent%2520trade-off%2520between%2520fairness%250Aand%2520accuracy.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520FairVIC%252C%2520an%2520innovative%250Aapproach%2520that%2520enhances%2520fairness%2520in%2520neural%2520networks%2520by%2520integrating%2520variance%252C%250Ainvariance%252C%2520and%2520covariance%2520terms%2520into%2520the%2520loss%2520function%2520during%2520training.%2520Unlike%250Amethods%2520that%2520rely%2520on%2520predefined%2520fairness%2520criteria%252C%2520FairVIC%2520abstracts%2520fairness%250Aconcepts%2520to%2520minimise%2520dependency%2520on%2520protected%2520characteristics.%2520We%2520evaluate%250AFairVIC%2520against%2520comparable%2520bias%2520mitigation%2520techniques%2520on%2520benchmark%2520datasets%252C%250Aconsidering%2520both%2520group%2520and%2520individual%2520fairness%252C%2520and%2520conduct%2520an%2520ablation%2520study%250Aon%2520the%2520accuracy-fairness%2520trade-off.%2520FairVIC%2520demonstrates%2520significant%250Aimprovements%2520%2528%2524%255Capprox70%255C%2525%2524%2529%2520in%2520fairness%2520across%2520all%2520tested%2520metrics%2520without%250Acompromising%2520accuracy%252C%2520thus%2520offering%2520a%2520robust%252C%2520generalisable%2520solution%2520for%2520fair%250Adeep%2520learning%2520across%2520diverse%2520tasks%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18134v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Fairer%20Representations%20with%20FairVIC&entry.906535625=Charmaine%20Barker%20and%20Daniel%20Bethell%20and%20Dimitar%20Kazakov&entry.1292438233=%20%20Mitigating%20bias%20in%20automated%20decision-making%20systems%2C%20particularly%20in%20deep%0Alearning%20models%2C%20is%20a%20critical%20challenge%20due%20to%20nuanced%20definitions%20of%0Afairness%2C%20dataset-specific%20biases%2C%20and%20the%20inherent%20trade-off%20between%20fairness%0Aand%20accuracy.%20To%20address%20these%20issues%2C%20we%20introduce%20FairVIC%2C%20an%20innovative%0Aapproach%20that%20enhances%20fairness%20in%20neural%20networks%20by%20integrating%20variance%2C%0Ainvariance%2C%20and%20covariance%20terms%20into%20the%20loss%20function%20during%20training.%20Unlike%0Amethods%20that%20rely%20on%20predefined%20fairness%20criteria%2C%20FairVIC%20abstracts%20fairness%0Aconcepts%20to%20minimise%20dependency%20on%20protected%20characteristics.%20We%20evaluate%0AFairVIC%20against%20comparable%20bias%20mitigation%20techniques%20on%20benchmark%20datasets%2C%0Aconsidering%20both%20group%20and%20individual%20fairness%2C%20and%20conduct%20an%20ablation%20study%0Aon%20the%20accuracy-fairness%20trade-off.%20FairVIC%20demonstrates%20significant%0Aimprovements%20%28%24%5Capprox70%5C%25%24%29%20in%20fairness%20across%20all%20tested%20metrics%20without%0Acompromising%20accuracy%2C%20thus%20offering%20a%20robust%2C%20generalisable%20solution%20for%20fair%0Adeep%20learning%20across%20diverse%20tasks%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18134v2&entry.124074799=Read"},
{"title": "Advances in Multimodal Adaptation and Generalization: From Traditional\n  Approaches to Foundation Models", "author": "Hao Dong and Moru Liu and Kaiyang Zhou and Eleni Chatzi and Juho Kannala and Cyrill Stachniss and Olga Fink", "abstract": "  In real-world scenarios, achieving domain adaptation and generalization poses\nsignificant challenges, as models must adapt to or generalize across unknown\ntarget distributions. Extending these capabilities to unseen multimodal\ndistributions, i.e., multimodal domain adaptation and generalization, is even\nmore challenging due to the distinct characteristics of different modalities.\nSignificant progress has been made over the years, with applications ranging\nfrom action recognition to semantic segmentation. Besides, the recent advent of\nlarge-scale pre-trained multimodal foundation models, such as CLIP, has\ninspired works leveraging these models to enhance adaptation and generalization\nperformances or adapting them to downstream tasks. This survey provides the\nfirst comprehensive review of recent advances from traditional approaches to\nfoundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal\ntest-time adaptation; (3) Multimodal domain generalization; (4) Domain\nadaptation and generalization with the help of multimodal foundation models;\nand (5) Adaptation of multimodal foundation models. For each topic, we formally\ndefine the problem and thoroughly review existing methods. Additionally, we\nanalyze relevant datasets and applications, highlighting open challenges and\npotential future research directions. We maintain an active repository that\ncontains up-to-date literature at\nhttps://github.com/donghao51/Awesome-Multimodal-Adaptation.\n", "link": "http://arxiv.org/abs/2501.18592v2", "date": "2025-02-03", "relevancy": 2.3892, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6271}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%20Multimodal%20Adaptation%20and%20Generalization%3A%20From%20Traditional%0A%20%20Approaches%20to%20Foundation%20Models&body=Title%3A%20Advances%20in%20Multimodal%20Adaptation%20and%20Generalization%3A%20From%20Traditional%0A%20%20Approaches%20to%20Foundation%20Models%0AAuthor%3A%20Hao%20Dong%20and%20Moru%20Liu%20and%20Kaiyang%20Zhou%20and%20Eleni%20Chatzi%20and%20Juho%20Kannala%20and%20Cyrill%20Stachniss%20and%20Olga%20Fink%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20achieving%20domain%20adaptation%20and%20generalization%20poses%0Asignificant%20challenges%2C%20as%20models%20must%20adapt%20to%20or%20generalize%20across%20unknown%0Atarget%20distributions.%20Extending%20these%20capabilities%20to%20unseen%20multimodal%0Adistributions%2C%20i.e.%2C%20multimodal%20domain%20adaptation%20and%20generalization%2C%20is%20even%0Amore%20challenging%20due%20to%20the%20distinct%20characteristics%20of%20different%20modalities.%0ASignificant%20progress%20has%20been%20made%20over%20the%20years%2C%20with%20applications%20ranging%0Afrom%20action%20recognition%20to%20semantic%20segmentation.%20Besides%2C%20the%20recent%20advent%20of%0Alarge-scale%20pre-trained%20multimodal%20foundation%20models%2C%20such%20as%20CLIP%2C%20has%0Ainspired%20works%20leveraging%20these%20models%20to%20enhance%20adaptation%20and%20generalization%0Aperformances%20or%20adapting%20them%20to%20downstream%20tasks.%20This%20survey%20provides%20the%0Afirst%20comprehensive%20review%20of%20recent%20advances%20from%20traditional%20approaches%20to%0Afoundation%20models%2C%20covering%3A%20%281%29%20Multimodal%20domain%20adaptation%3B%20%282%29%20Multimodal%0Atest-time%20adaptation%3B%20%283%29%20Multimodal%20domain%20generalization%3B%20%284%29%20Domain%0Aadaptation%20and%20generalization%20with%20the%20help%20of%20multimodal%20foundation%20models%3B%0Aand%20%285%29%20Adaptation%20of%20multimodal%20foundation%20models.%20For%20each%20topic%2C%20we%20formally%0Adefine%20the%20problem%20and%20thoroughly%20review%20existing%20methods.%20Additionally%2C%20we%0Aanalyze%20relevant%20datasets%20and%20applications%2C%20highlighting%20open%20challenges%20and%0Apotential%20future%20research%20directions.%20We%20maintain%20an%20active%20repository%20that%0Acontains%20up-to-date%20literature%20at%0Ahttps%3A//github.com/donghao51/Awesome-Multimodal-Adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18592v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%2520Multimodal%2520Adaptation%2520and%2520Generalization%253A%2520From%2520Traditional%250A%2520%2520Approaches%2520to%2520Foundation%2520Models%26entry.906535625%3DHao%2520Dong%2520and%2520Moru%2520Liu%2520and%2520Kaiyang%2520Zhou%2520and%2520Eleni%2520Chatzi%2520and%2520Juho%2520Kannala%2520and%2520Cyrill%2520Stachniss%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520achieving%2520domain%2520adaptation%2520and%2520generalization%2520poses%250Asignificant%2520challenges%252C%2520as%2520models%2520must%2520adapt%2520to%2520or%2520generalize%2520across%2520unknown%250Atarget%2520distributions.%2520Extending%2520these%2520capabilities%2520to%2520unseen%2520multimodal%250Adistributions%252C%2520i.e.%252C%2520multimodal%2520domain%2520adaptation%2520and%2520generalization%252C%2520is%2520even%250Amore%2520challenging%2520due%2520to%2520the%2520distinct%2520characteristics%2520of%2520different%2520modalities.%250ASignificant%2520progress%2520has%2520been%2520made%2520over%2520the%2520years%252C%2520with%2520applications%2520ranging%250Afrom%2520action%2520recognition%2520to%2520semantic%2520segmentation.%2520Besides%252C%2520the%2520recent%2520advent%2520of%250Alarge-scale%2520pre-trained%2520multimodal%2520foundation%2520models%252C%2520such%2520as%2520CLIP%252C%2520has%250Ainspired%2520works%2520leveraging%2520these%2520models%2520to%2520enhance%2520adaptation%2520and%2520generalization%250Aperformances%2520or%2520adapting%2520them%2520to%2520downstream%2520tasks.%2520This%2520survey%2520provides%2520the%250Afirst%2520comprehensive%2520review%2520of%2520recent%2520advances%2520from%2520traditional%2520approaches%2520to%250Afoundation%2520models%252C%2520covering%253A%2520%25281%2529%2520Multimodal%2520domain%2520adaptation%253B%2520%25282%2529%2520Multimodal%250Atest-time%2520adaptation%253B%2520%25283%2529%2520Multimodal%2520domain%2520generalization%253B%2520%25284%2529%2520Domain%250Aadaptation%2520and%2520generalization%2520with%2520the%2520help%2520of%2520multimodal%2520foundation%2520models%253B%250Aand%2520%25285%2529%2520Adaptation%2520of%2520multimodal%2520foundation%2520models.%2520For%2520each%2520topic%252C%2520we%2520formally%250Adefine%2520the%2520problem%2520and%2520thoroughly%2520review%2520existing%2520methods.%2520Additionally%252C%2520we%250Aanalyze%2520relevant%2520datasets%2520and%2520applications%252C%2520highlighting%2520open%2520challenges%2520and%250Apotential%2520future%2520research%2520directions.%2520We%2520maintain%2520an%2520active%2520repository%2520that%250Acontains%2520up-to-date%2520literature%2520at%250Ahttps%253A//github.com/donghao51/Awesome-Multimodal-Adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18592v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%20Multimodal%20Adaptation%20and%20Generalization%3A%20From%20Traditional%0A%20%20Approaches%20to%20Foundation%20Models&entry.906535625=Hao%20Dong%20and%20Moru%20Liu%20and%20Kaiyang%20Zhou%20and%20Eleni%20Chatzi%20and%20Juho%20Kannala%20and%20Cyrill%20Stachniss%20and%20Olga%20Fink&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20achieving%20domain%20adaptation%20and%20generalization%20poses%0Asignificant%20challenges%2C%20as%20models%20must%20adapt%20to%20or%20generalize%20across%20unknown%0Atarget%20distributions.%20Extending%20these%20capabilities%20to%20unseen%20multimodal%0Adistributions%2C%20i.e.%2C%20multimodal%20domain%20adaptation%20and%20generalization%2C%20is%20even%0Amore%20challenging%20due%20to%20the%20distinct%20characteristics%20of%20different%20modalities.%0ASignificant%20progress%20has%20been%20made%20over%20the%20years%2C%20with%20applications%20ranging%0Afrom%20action%20recognition%20to%20semantic%20segmentation.%20Besides%2C%20the%20recent%20advent%20of%0Alarge-scale%20pre-trained%20multimodal%20foundation%20models%2C%20such%20as%20CLIP%2C%20has%0Ainspired%20works%20leveraging%20these%20models%20to%20enhance%20adaptation%20and%20generalization%0Aperformances%20or%20adapting%20them%20to%20downstream%20tasks.%20This%20survey%20provides%20the%0Afirst%20comprehensive%20review%20of%20recent%20advances%20from%20traditional%20approaches%20to%0Afoundation%20models%2C%20covering%3A%20%281%29%20Multimodal%20domain%20adaptation%3B%20%282%29%20Multimodal%0Atest-time%20adaptation%3B%20%283%29%20Multimodal%20domain%20generalization%3B%20%284%29%20Domain%0Aadaptation%20and%20generalization%20with%20the%20help%20of%20multimodal%20foundation%20models%3B%0Aand%20%285%29%20Adaptation%20of%20multimodal%20foundation%20models.%20For%20each%20topic%2C%20we%20formally%0Adefine%20the%20problem%20and%20thoroughly%20review%20existing%20methods.%20Additionally%2C%20we%0Aanalyze%20relevant%20datasets%20and%20applications%2C%20highlighting%20open%20challenges%20and%0Apotential%20future%20research%20directions.%20We%20maintain%20an%20active%20repository%20that%0Acontains%20up-to-date%20literature%20at%0Ahttps%3A//github.com/donghao51/Awesome-Multimodal-Adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18592v2&entry.124074799=Read"},
{"title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba", "author": "Assaf Ben-Kish and Itamar Zimerman and Shady Abu-Hussein and Nadav Cohen and Amir Globerson and Lior Wolf and Raja Giryes", "abstract": "  Long-range sequence processing poses a significant challenge for Transformers\ndue to their quadratic complexity in input length. A promising alternative is\nMamba, which demonstrates high performance and achieves Transformer-level\ncapabilities while requiring substantially fewer computational resources. In\nthis paper we explore the length-generalization capabilities of Mamba, which we\nfind to be relatively limited. Through a series of visualizations and analyses\nwe identify that the limitations arise from a restricted effective receptive\nfield, dictated by the sequence length used during training. To address this\nconstraint, we introduce DeciMamba, a context-extension method specifically\ndesigned for Mamba. This mechanism, built on top of a hidden filtering\nmechanism embedded within the S6 layer, enables the trained model to\nextrapolate well even without additional training. Empirical experiments over\nreal-world long-range NLP tasks show that DeciMamba can extrapolate to context\nlengths that are significantly longer than the ones seen during training, while\nenjoying faster inference.\n", "link": "http://arxiv.org/abs/2406.14528v2", "date": "2025-02-03", "relevancy": 2.3759, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4757}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeciMamba%3A%20Exploring%20the%20Length%20Extrapolation%20Potential%20of%20Mamba&body=Title%3A%20DeciMamba%3A%20Exploring%20the%20Length%20Extrapolation%20Potential%20of%20Mamba%0AAuthor%3A%20Assaf%20Ben-Kish%20and%20Itamar%20Zimerman%20and%20Shady%20Abu-Hussein%20and%20Nadav%20Cohen%20and%20Amir%20Globerson%20and%20Lior%20Wolf%20and%20Raja%20Giryes%0AAbstract%3A%20%20%20Long-range%20sequence%20processing%20poses%20a%20significant%20challenge%20for%20Transformers%0Adue%20to%20their%20quadratic%20complexity%20in%20input%20length.%20A%20promising%20alternative%20is%0AMamba%2C%20which%20demonstrates%20high%20performance%20and%20achieves%20Transformer-level%0Acapabilities%20while%20requiring%20substantially%20fewer%20computational%20resources.%20In%0Athis%20paper%20we%20explore%20the%20length-generalization%20capabilities%20of%20Mamba%2C%20which%20we%0Afind%20to%20be%20relatively%20limited.%20Through%20a%20series%20of%20visualizations%20and%20analyses%0Awe%20identify%20that%20the%20limitations%20arise%20from%20a%20restricted%20effective%20receptive%0Afield%2C%20dictated%20by%20the%20sequence%20length%20used%20during%20training.%20To%20address%20this%0Aconstraint%2C%20we%20introduce%20DeciMamba%2C%20a%20context-extension%20method%20specifically%0Adesigned%20for%20Mamba.%20This%20mechanism%2C%20built%20on%20top%20of%20a%20hidden%20filtering%0Amechanism%20embedded%20within%20the%20S6%20layer%2C%20enables%20the%20trained%20model%20to%0Aextrapolate%20well%20even%20without%20additional%20training.%20Empirical%20experiments%20over%0Areal-world%20long-range%20NLP%20tasks%20show%20that%20DeciMamba%20can%20extrapolate%20to%20context%0Alengths%20that%20are%20significantly%20longer%20than%20the%20ones%20seen%20during%20training%2C%20while%0Aenjoying%20faster%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14528v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeciMamba%253A%2520Exploring%2520the%2520Length%2520Extrapolation%2520Potential%2520of%2520Mamba%26entry.906535625%3DAssaf%2520Ben-Kish%2520and%2520Itamar%2520Zimerman%2520and%2520Shady%2520Abu-Hussein%2520and%2520Nadav%2520Cohen%2520and%2520Amir%2520Globerson%2520and%2520Lior%2520Wolf%2520and%2520Raja%2520Giryes%26entry.1292438233%3D%2520%2520Long-range%2520sequence%2520processing%2520poses%2520a%2520significant%2520challenge%2520for%2520Transformers%250Adue%2520to%2520their%2520quadratic%2520complexity%2520in%2520input%2520length.%2520A%2520promising%2520alternative%2520is%250AMamba%252C%2520which%2520demonstrates%2520high%2520performance%2520and%2520achieves%2520Transformer-level%250Acapabilities%2520while%2520requiring%2520substantially%2520fewer%2520computational%2520resources.%2520In%250Athis%2520paper%2520we%2520explore%2520the%2520length-generalization%2520capabilities%2520of%2520Mamba%252C%2520which%2520we%250Afind%2520to%2520be%2520relatively%2520limited.%2520Through%2520a%2520series%2520of%2520visualizations%2520and%2520analyses%250Awe%2520identify%2520that%2520the%2520limitations%2520arise%2520from%2520a%2520restricted%2520effective%2520receptive%250Afield%252C%2520dictated%2520by%2520the%2520sequence%2520length%2520used%2520during%2520training.%2520To%2520address%2520this%250Aconstraint%252C%2520we%2520introduce%2520DeciMamba%252C%2520a%2520context-extension%2520method%2520specifically%250Adesigned%2520for%2520Mamba.%2520This%2520mechanism%252C%2520built%2520on%2520top%2520of%2520a%2520hidden%2520filtering%250Amechanism%2520embedded%2520within%2520the%2520S6%2520layer%252C%2520enables%2520the%2520trained%2520model%2520to%250Aextrapolate%2520well%2520even%2520without%2520additional%2520training.%2520Empirical%2520experiments%2520over%250Areal-world%2520long-range%2520NLP%2520tasks%2520show%2520that%2520DeciMamba%2520can%2520extrapolate%2520to%2520context%250Alengths%2520that%2520are%2520significantly%2520longer%2520than%2520the%2520ones%2520seen%2520during%2520training%252C%2520while%250Aenjoying%2520faster%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14528v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeciMamba%3A%20Exploring%20the%20Length%20Extrapolation%20Potential%20of%20Mamba&entry.906535625=Assaf%20Ben-Kish%20and%20Itamar%20Zimerman%20and%20Shady%20Abu-Hussein%20and%20Nadav%20Cohen%20and%20Amir%20Globerson%20and%20Lior%20Wolf%20and%20Raja%20Giryes&entry.1292438233=%20%20Long-range%20sequence%20processing%20poses%20a%20significant%20challenge%20for%20Transformers%0Adue%20to%20their%20quadratic%20complexity%20in%20input%20length.%20A%20promising%20alternative%20is%0AMamba%2C%20which%20demonstrates%20high%20performance%20and%20achieves%20Transformer-level%0Acapabilities%20while%20requiring%20substantially%20fewer%20computational%20resources.%20In%0Athis%20paper%20we%20explore%20the%20length-generalization%20capabilities%20of%20Mamba%2C%20which%20we%0Afind%20to%20be%20relatively%20limited.%20Through%20a%20series%20of%20visualizations%20and%20analyses%0Awe%20identify%20that%20the%20limitations%20arise%20from%20a%20restricted%20effective%20receptive%0Afield%2C%20dictated%20by%20the%20sequence%20length%20used%20during%20training.%20To%20address%20this%0Aconstraint%2C%20we%20introduce%20DeciMamba%2C%20a%20context-extension%20method%20specifically%0Adesigned%20for%20Mamba.%20This%20mechanism%2C%20built%20on%20top%20of%20a%20hidden%20filtering%0Amechanism%20embedded%20within%20the%20S6%20layer%2C%20enables%20the%20trained%20model%20to%0Aextrapolate%20well%20even%20without%20additional%20training.%20Empirical%20experiments%20over%0Areal-world%20long-range%20NLP%20tasks%20show%20that%20DeciMamba%20can%20extrapolate%20to%20context%0Alengths%20that%20are%20significantly%20longer%20than%20the%20ones%20seen%20during%20training%2C%20while%0Aenjoying%20faster%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14528v2&entry.124074799=Read"},
{"title": "JoVALE: Detecting Human Actions in Video Using Audiovisual and Language\n  Contexts", "author": "Taein Son and Soo Won Seo and Jisong Kim and Seok Hwan Lee and Jun Won Choi", "abstract": "  Video Action Detection (VAD) entails localizing and categorizing action\ninstances within videos, which inherently consist of diverse information\nsources such as audio, visual cues, and surrounding scene contexts. Leveraging\nthis multi-modal information effectively for VAD poses a significant challenge,\nas the model must identify action-relevant cues with precision. In this study,\nwe introduce a novel multi-modal VAD architecture, referred to as the Joint\nActor-centric Visual, Audio, Language Encoder (JoVALE). JoVALE is the first VAD\nmethod to integrate audio and visual features with scene descriptive context\nsourced from large-capacity image captioning models. At the heart of JoVALE is\nthe actor-centric aggregation of audio, visual, and scene descriptive\ninformation, enabling adaptive integration of crucial features for recognizing\neach actor's actions. We have developed a Transformer-based architecture, the\nActor-centric Multi-modal Fusion Network, specifically designed to capture the\ndynamic interactions among actors and their multi-modal contexts. Our\nevaluation on three prominent VAD benchmarks, including AVA, UCF101-24, and\nJHMDB51-21, demonstrates that incorporating multi-modal information\nsignificantly enhances performance, setting new state-of-the-art performances\nin the field.\n", "link": "http://arxiv.org/abs/2412.13708v2", "date": "2025-02-03", "relevancy": 2.3364, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5918}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5918}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JoVALE%3A%20Detecting%20Human%20Actions%20in%20Video%20Using%20Audiovisual%20and%20Language%0A%20%20Contexts&body=Title%3A%20JoVALE%3A%20Detecting%20Human%20Actions%20in%20Video%20Using%20Audiovisual%20and%20Language%0A%20%20Contexts%0AAuthor%3A%20Taein%20Son%20and%20Soo%20Won%20Seo%20and%20Jisong%20Kim%20and%20Seok%20Hwan%20Lee%20and%20Jun%20Won%20Choi%0AAbstract%3A%20%20%20Video%20Action%20Detection%20%28VAD%29%20entails%20localizing%20and%20categorizing%20action%0Ainstances%20within%20videos%2C%20which%20inherently%20consist%20of%20diverse%20information%0Asources%20such%20as%20audio%2C%20visual%20cues%2C%20and%20surrounding%20scene%20contexts.%20Leveraging%0Athis%20multi-modal%20information%20effectively%20for%20VAD%20poses%20a%20significant%20challenge%2C%0Aas%20the%20model%20must%20identify%20action-relevant%20cues%20with%20precision.%20In%20this%20study%2C%0Awe%20introduce%20a%20novel%20multi-modal%20VAD%20architecture%2C%20referred%20to%20as%20the%20Joint%0AActor-centric%20Visual%2C%20Audio%2C%20Language%20Encoder%20%28JoVALE%29.%20JoVALE%20is%20the%20first%20VAD%0Amethod%20to%20integrate%20audio%20and%20visual%20features%20with%20scene%20descriptive%20context%0Asourced%20from%20large-capacity%20image%20captioning%20models.%20At%20the%20heart%20of%20JoVALE%20is%0Athe%20actor-centric%20aggregation%20of%20audio%2C%20visual%2C%20and%20scene%20descriptive%0Ainformation%2C%20enabling%20adaptive%20integration%20of%20crucial%20features%20for%20recognizing%0Aeach%20actor%27s%20actions.%20We%20have%20developed%20a%20Transformer-based%20architecture%2C%20the%0AActor-centric%20Multi-modal%20Fusion%20Network%2C%20specifically%20designed%20to%20capture%20the%0Adynamic%20interactions%20among%20actors%20and%20their%20multi-modal%20contexts.%20Our%0Aevaluation%20on%20three%20prominent%20VAD%20benchmarks%2C%20including%20AVA%2C%20UCF101-24%2C%20and%0AJHMDB51-21%2C%20demonstrates%20that%20incorporating%20multi-modal%20information%0Asignificantly%20enhances%20performance%2C%20setting%20new%20state-of-the-art%20performances%0Ain%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoVALE%253A%2520Detecting%2520Human%2520Actions%2520in%2520Video%2520Using%2520Audiovisual%2520and%2520Language%250A%2520%2520Contexts%26entry.906535625%3DTaein%2520Son%2520and%2520Soo%2520Won%2520Seo%2520and%2520Jisong%2520Kim%2520and%2520Seok%2520Hwan%2520Lee%2520and%2520Jun%2520Won%2520Choi%26entry.1292438233%3D%2520%2520Video%2520Action%2520Detection%2520%2528VAD%2529%2520entails%2520localizing%2520and%2520categorizing%2520action%250Ainstances%2520within%2520videos%252C%2520which%2520inherently%2520consist%2520of%2520diverse%2520information%250Asources%2520such%2520as%2520audio%252C%2520visual%2520cues%252C%2520and%2520surrounding%2520scene%2520contexts.%2520Leveraging%250Athis%2520multi-modal%2520information%2520effectively%2520for%2520VAD%2520poses%2520a%2520significant%2520challenge%252C%250Aas%2520the%2520model%2520must%2520identify%2520action-relevant%2520cues%2520with%2520precision.%2520In%2520this%2520study%252C%250Awe%2520introduce%2520a%2520novel%2520multi-modal%2520VAD%2520architecture%252C%2520referred%2520to%2520as%2520the%2520Joint%250AActor-centric%2520Visual%252C%2520Audio%252C%2520Language%2520Encoder%2520%2528JoVALE%2529.%2520JoVALE%2520is%2520the%2520first%2520VAD%250Amethod%2520to%2520integrate%2520audio%2520and%2520visual%2520features%2520with%2520scene%2520descriptive%2520context%250Asourced%2520from%2520large-capacity%2520image%2520captioning%2520models.%2520At%2520the%2520heart%2520of%2520JoVALE%2520is%250Athe%2520actor-centric%2520aggregation%2520of%2520audio%252C%2520visual%252C%2520and%2520scene%2520descriptive%250Ainformation%252C%2520enabling%2520adaptive%2520integration%2520of%2520crucial%2520features%2520for%2520recognizing%250Aeach%2520actor%2527s%2520actions.%2520We%2520have%2520developed%2520a%2520Transformer-based%2520architecture%252C%2520the%250AActor-centric%2520Multi-modal%2520Fusion%2520Network%252C%2520specifically%2520designed%2520to%2520capture%2520the%250Adynamic%2520interactions%2520among%2520actors%2520and%2520their%2520multi-modal%2520contexts.%2520Our%250Aevaluation%2520on%2520three%2520prominent%2520VAD%2520benchmarks%252C%2520including%2520AVA%252C%2520UCF101-24%252C%2520and%250AJHMDB51-21%252C%2520demonstrates%2520that%2520incorporating%2520multi-modal%2520information%250Asignificantly%2520enhances%2520performance%252C%2520setting%2520new%2520state-of-the-art%2520performances%250Ain%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JoVALE%3A%20Detecting%20Human%20Actions%20in%20Video%20Using%20Audiovisual%20and%20Language%0A%20%20Contexts&entry.906535625=Taein%20Son%20and%20Soo%20Won%20Seo%20and%20Jisong%20Kim%20and%20Seok%20Hwan%20Lee%20and%20Jun%20Won%20Choi&entry.1292438233=%20%20Video%20Action%20Detection%20%28VAD%29%20entails%20localizing%20and%20categorizing%20action%0Ainstances%20within%20videos%2C%20which%20inherently%20consist%20of%20diverse%20information%0Asources%20such%20as%20audio%2C%20visual%20cues%2C%20and%20surrounding%20scene%20contexts.%20Leveraging%0Athis%20multi-modal%20information%20effectively%20for%20VAD%20poses%20a%20significant%20challenge%2C%0Aas%20the%20model%20must%20identify%20action-relevant%20cues%20with%20precision.%20In%20this%20study%2C%0Awe%20introduce%20a%20novel%20multi-modal%20VAD%20architecture%2C%20referred%20to%20as%20the%20Joint%0AActor-centric%20Visual%2C%20Audio%2C%20Language%20Encoder%20%28JoVALE%29.%20JoVALE%20is%20the%20first%20VAD%0Amethod%20to%20integrate%20audio%20and%20visual%20features%20with%20scene%20descriptive%20context%0Asourced%20from%20large-capacity%20image%20captioning%20models.%20At%20the%20heart%20of%20JoVALE%20is%0Athe%20actor-centric%20aggregation%20of%20audio%2C%20visual%2C%20and%20scene%20descriptive%0Ainformation%2C%20enabling%20adaptive%20integration%20of%20crucial%20features%20for%20recognizing%0Aeach%20actor%27s%20actions.%20We%20have%20developed%20a%20Transformer-based%20architecture%2C%20the%0AActor-centric%20Multi-modal%20Fusion%20Network%2C%20specifically%20designed%20to%20capture%20the%0Adynamic%20interactions%20among%20actors%20and%20their%20multi-modal%20contexts.%20Our%0Aevaluation%20on%20three%20prominent%20VAD%20benchmarks%2C%20including%20AVA%2C%20UCF101-24%2C%20and%0AJHMDB51-21%2C%20demonstrates%20that%20incorporating%20multi-modal%20information%0Asignificantly%20enhances%20performance%2C%20setting%20new%20state-of-the-art%20performances%0Ain%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13708v2&entry.124074799=Read"},
{"title": "BirdSet: A Large-Scale Dataset for Audio Classification in Avian\n  Bioacoustics", "author": "Lukas Rauch and Raphael Schwinger and Moritz Wirth and Ren\u00e9 Heinrich and Denis Huseljic and Marek Herde and Jonas Lange and Stefan Kahl and Bernhard Sick and Sven Tomforde and Christoph Scholz", "abstract": "  Deep learning (DL) has greatly advanced audio classification, yet the field\nis limited by the scarcity of large-scale benchmark datasets that have\npropelled progress in other domains. While AudioSet is a pivotal step to bridge\nthis gap as a universal-domain dataset, its restricted accessibility and\nlimited range of evaluation use cases challenge its role as the sole resource.\nTherefore, we introduce \\texttt{BirdSet}, a large-scale benchmark dataset for\naudio classification focusing on avian bioacoustics. \\texttt{BirdSet} surpasses\nAudioSet with over 6,800 recording hours~($\\uparrow\\!17\\%$) from nearly 10,000\nclasses~($\\uparrow\\!18\\times$) for training and more than 400\nhours~($\\uparrow\\!7\\times$) across eight strongly labeled evaluation datasets.\nIt serves as a versatile resource for use cases such as multi-label\nclassification, covariate shift or self-supervised learning. We benchmark six\nwell-known DL models in multi-label classification across three distinct\ntraining scenarios and outline further evaluation use cases in audio\nclassification. We host our dataset on Hugging Face for easy accessibility and\noffer an extensive codebase to reproduce our results.\n", "link": "http://arxiv.org/abs/2403.10380v5", "date": "2025-02-03", "relevancy": 2.3198, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4535}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BirdSet%3A%20A%20Large-Scale%20Dataset%20for%20Audio%20Classification%20in%20Avian%0A%20%20Bioacoustics&body=Title%3A%20BirdSet%3A%20A%20Large-Scale%20Dataset%20for%20Audio%20Classification%20in%20Avian%0A%20%20Bioacoustics%0AAuthor%3A%20Lukas%20Rauch%20and%20Raphael%20Schwinger%20and%20Moritz%20Wirth%20and%20Ren%C3%A9%20Heinrich%20and%20Denis%20Huseljic%20and%20Marek%20Herde%20and%20Jonas%20Lange%20and%20Stefan%20Kahl%20and%20Bernhard%20Sick%20and%20Sven%20Tomforde%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20has%20greatly%20advanced%20audio%20classification%2C%20yet%20the%20field%0Ais%20limited%20by%20the%20scarcity%20of%20large-scale%20benchmark%20datasets%20that%20have%0Apropelled%20progress%20in%20other%20domains.%20While%20AudioSet%20is%20a%20pivotal%20step%20to%20bridge%0Athis%20gap%20as%20a%20universal-domain%20dataset%2C%20its%20restricted%20accessibility%20and%0Alimited%20range%20of%20evaluation%20use%20cases%20challenge%20its%20role%20as%20the%20sole%20resource.%0ATherefore%2C%20we%20introduce%20%5Ctexttt%7BBirdSet%7D%2C%20a%20large-scale%20benchmark%20dataset%20for%0Aaudio%20classification%20focusing%20on%20avian%20bioacoustics.%20%5Ctexttt%7BBirdSet%7D%20surpasses%0AAudioSet%20with%20over%206%2C800%20recording%20hours~%28%24%5Cuparrow%5C%2117%5C%25%24%29%20from%20nearly%2010%2C000%0Aclasses~%28%24%5Cuparrow%5C%2118%5Ctimes%24%29%20for%20training%20and%20more%20than%20400%0Ahours~%28%24%5Cuparrow%5C%217%5Ctimes%24%29%20across%20eight%20strongly%20labeled%20evaluation%20datasets.%0AIt%20serves%20as%20a%20versatile%20resource%20for%20use%20cases%20such%20as%20multi-label%0Aclassification%2C%20covariate%20shift%20or%20self-supervised%20learning.%20We%20benchmark%20six%0Awell-known%20DL%20models%20in%20multi-label%20classification%20across%20three%20distinct%0Atraining%20scenarios%20and%20outline%20further%20evaluation%20use%20cases%20in%20audio%0Aclassification.%20We%20host%20our%20dataset%20on%20Hugging%20Face%20for%20easy%20accessibility%20and%0Aoffer%20an%20extensive%20codebase%20to%20reproduce%20our%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10380v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBirdSet%253A%2520A%2520Large-Scale%2520Dataset%2520for%2520Audio%2520Classification%2520in%2520Avian%250A%2520%2520Bioacoustics%26entry.906535625%3DLukas%2520Rauch%2520and%2520Raphael%2520Schwinger%2520and%2520Moritz%2520Wirth%2520and%2520Ren%25C3%25A9%2520Heinrich%2520and%2520Denis%2520Huseljic%2520and%2520Marek%2520Herde%2520and%2520Jonas%2520Lange%2520and%2520Stefan%2520Kahl%2520and%2520Bernhard%2520Sick%2520and%2520Sven%2520Tomforde%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520has%2520greatly%2520advanced%2520audio%2520classification%252C%2520yet%2520the%2520field%250Ais%2520limited%2520by%2520the%2520scarcity%2520of%2520large-scale%2520benchmark%2520datasets%2520that%2520have%250Apropelled%2520progress%2520in%2520other%2520domains.%2520While%2520AudioSet%2520is%2520a%2520pivotal%2520step%2520to%2520bridge%250Athis%2520gap%2520as%2520a%2520universal-domain%2520dataset%252C%2520its%2520restricted%2520accessibility%2520and%250Alimited%2520range%2520of%2520evaluation%2520use%2520cases%2520challenge%2520its%2520role%2520as%2520the%2520sole%2520resource.%250ATherefore%252C%2520we%2520introduce%2520%255Ctexttt%257BBirdSet%257D%252C%2520a%2520large-scale%2520benchmark%2520dataset%2520for%250Aaudio%2520classification%2520focusing%2520on%2520avian%2520bioacoustics.%2520%255Ctexttt%257BBirdSet%257D%2520surpasses%250AAudioSet%2520with%2520over%25206%252C800%2520recording%2520hours~%2528%2524%255Cuparrow%255C%252117%255C%2525%2524%2529%2520from%2520nearly%252010%252C000%250Aclasses~%2528%2524%255Cuparrow%255C%252118%255Ctimes%2524%2529%2520for%2520training%2520and%2520more%2520than%2520400%250Ahours~%2528%2524%255Cuparrow%255C%25217%255Ctimes%2524%2529%2520across%2520eight%2520strongly%2520labeled%2520evaluation%2520datasets.%250AIt%2520serves%2520as%2520a%2520versatile%2520resource%2520for%2520use%2520cases%2520such%2520as%2520multi-label%250Aclassification%252C%2520covariate%2520shift%2520or%2520self-supervised%2520learning.%2520We%2520benchmark%2520six%250Awell-known%2520DL%2520models%2520in%2520multi-label%2520classification%2520across%2520three%2520distinct%250Atraining%2520scenarios%2520and%2520outline%2520further%2520evaluation%2520use%2520cases%2520in%2520audio%250Aclassification.%2520We%2520host%2520our%2520dataset%2520on%2520Hugging%2520Face%2520for%2520easy%2520accessibility%2520and%250Aoffer%2520an%2520extensive%2520codebase%2520to%2520reproduce%2520our%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10380v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BirdSet%3A%20A%20Large-Scale%20Dataset%20for%20Audio%20Classification%20in%20Avian%0A%20%20Bioacoustics&entry.906535625=Lukas%20Rauch%20and%20Raphael%20Schwinger%20and%20Moritz%20Wirth%20and%20Ren%C3%A9%20Heinrich%20and%20Denis%20Huseljic%20and%20Marek%20Herde%20and%20Jonas%20Lange%20and%20Stefan%20Kahl%20and%20Bernhard%20Sick%20and%20Sven%20Tomforde%20and%20Christoph%20Scholz&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20has%20greatly%20advanced%20audio%20classification%2C%20yet%20the%20field%0Ais%20limited%20by%20the%20scarcity%20of%20large-scale%20benchmark%20datasets%20that%20have%0Apropelled%20progress%20in%20other%20domains.%20While%20AudioSet%20is%20a%20pivotal%20step%20to%20bridge%0Athis%20gap%20as%20a%20universal-domain%20dataset%2C%20its%20restricted%20accessibility%20and%0Alimited%20range%20of%20evaluation%20use%20cases%20challenge%20its%20role%20as%20the%20sole%20resource.%0ATherefore%2C%20we%20introduce%20%5Ctexttt%7BBirdSet%7D%2C%20a%20large-scale%20benchmark%20dataset%20for%0Aaudio%20classification%20focusing%20on%20avian%20bioacoustics.%20%5Ctexttt%7BBirdSet%7D%20surpasses%0AAudioSet%20with%20over%206%2C800%20recording%20hours~%28%24%5Cuparrow%5C%2117%5C%25%24%29%20from%20nearly%2010%2C000%0Aclasses~%28%24%5Cuparrow%5C%2118%5Ctimes%24%29%20for%20training%20and%20more%20than%20400%0Ahours~%28%24%5Cuparrow%5C%217%5Ctimes%24%29%20across%20eight%20strongly%20labeled%20evaluation%20datasets.%0AIt%20serves%20as%20a%20versatile%20resource%20for%20use%20cases%20such%20as%20multi-label%0Aclassification%2C%20covariate%20shift%20or%20self-supervised%20learning.%20We%20benchmark%20six%0Awell-known%20DL%20models%20in%20multi-label%20classification%20across%20three%20distinct%0Atraining%20scenarios%20and%20outline%20further%20evaluation%20use%20cases%20in%20audio%0Aclassification.%20We%20host%20our%20dataset%20on%20Hugging%20Face%20for%20easy%20accessibility%20and%0Aoffer%20an%20extensive%20codebase%20to%20reproduce%20our%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10380v5&entry.124074799=Read"},
{"title": "Disentangling Exploration of Large Language Models by Optimal\n  Exploitation", "author": "Tim Grams and Patrick Betz and Christian Bartelt", "abstract": "  Exploration is a crucial skill for self-improvement and open-ended\nproblem-solving. However, it remains unclear if large language models can\neffectively explore the state-space within an unknown environment. This work\nisolates exploration as the sole objective, tasking the agent with delivering\ninformation that enhances future returns. Within this framework, we argue that\nmeasuring agent returns is not sufficient for a fair evaluation and decompose\nmissing rewards into exploration and exploitation components based on the\noptimal achievable return. Comprehensive experiments with various models reveal\nthat most struggle to sufficiently explore the state-space and weak exploration\nis insufficient. We observe a positive correlation between parameter count and\nexploration performance, with larger models demonstrating superior\ncapabilities. Furthermore, we show that our decomposition provides insights\ninto differences in behaviors driven by prompt engineering, offering a valuable\ntool for refining performance in exploratory tasks.\n", "link": "http://arxiv.org/abs/2501.08925v2", "date": "2025-02-03", "relevancy": 2.3068, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5742}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20Exploration%20of%20Large%20Language%20Models%20by%20Optimal%0A%20%20Exploitation&body=Title%3A%20Disentangling%20Exploration%20of%20Large%20Language%20Models%20by%20Optimal%0A%20%20Exploitation%0AAuthor%3A%20Tim%20Grams%20and%20Patrick%20Betz%20and%20Christian%20Bartelt%0AAbstract%3A%20%20%20Exploration%20is%20a%20crucial%20skill%20for%20self-improvement%20and%20open-ended%0Aproblem-solving.%20However%2C%20it%20remains%20unclear%20if%20large%20language%20models%20can%0Aeffectively%20explore%20the%20state-space%20within%20an%20unknown%20environment.%20This%20work%0Aisolates%20exploration%20as%20the%20sole%20objective%2C%20tasking%20the%20agent%20with%20delivering%0Ainformation%20that%20enhances%20future%20returns.%20Within%20this%20framework%2C%20we%20argue%20that%0Ameasuring%20agent%20returns%20is%20not%20sufficient%20for%20a%20fair%20evaluation%20and%20decompose%0Amissing%20rewards%20into%20exploration%20and%20exploitation%20components%20based%20on%20the%0Aoptimal%20achievable%20return.%20Comprehensive%20experiments%20with%20various%20models%20reveal%0Athat%20most%20struggle%20to%20sufficiently%20explore%20the%20state-space%20and%20weak%20exploration%0Ais%20insufficient.%20We%20observe%20a%20positive%20correlation%20between%20parameter%20count%20and%0Aexploration%20performance%2C%20with%20larger%20models%20demonstrating%20superior%0Acapabilities.%20Furthermore%2C%20we%20show%20that%20our%20decomposition%20provides%20insights%0Ainto%20differences%20in%20behaviors%20driven%20by%20prompt%20engineering%2C%20offering%20a%20valuable%0Atool%20for%20refining%20performance%20in%20exploratory%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520Exploration%2520of%2520Large%2520Language%2520Models%2520by%2520Optimal%250A%2520%2520Exploitation%26entry.906535625%3DTim%2520Grams%2520and%2520Patrick%2520Betz%2520and%2520Christian%2520Bartelt%26entry.1292438233%3D%2520%2520Exploration%2520is%2520a%2520crucial%2520skill%2520for%2520self-improvement%2520and%2520open-ended%250Aproblem-solving.%2520However%252C%2520it%2520remains%2520unclear%2520if%2520large%2520language%2520models%2520can%250Aeffectively%2520explore%2520the%2520state-space%2520within%2520an%2520unknown%2520environment.%2520This%2520work%250Aisolates%2520exploration%2520as%2520the%2520sole%2520objective%252C%2520tasking%2520the%2520agent%2520with%2520delivering%250Ainformation%2520that%2520enhances%2520future%2520returns.%2520Within%2520this%2520framework%252C%2520we%2520argue%2520that%250Ameasuring%2520agent%2520returns%2520is%2520not%2520sufficient%2520for%2520a%2520fair%2520evaluation%2520and%2520decompose%250Amissing%2520rewards%2520into%2520exploration%2520and%2520exploitation%2520components%2520based%2520on%2520the%250Aoptimal%2520achievable%2520return.%2520Comprehensive%2520experiments%2520with%2520various%2520models%2520reveal%250Athat%2520most%2520struggle%2520to%2520sufficiently%2520explore%2520the%2520state-space%2520and%2520weak%2520exploration%250Ais%2520insufficient.%2520We%2520observe%2520a%2520positive%2520correlation%2520between%2520parameter%2520count%2520and%250Aexploration%2520performance%252C%2520with%2520larger%2520models%2520demonstrating%2520superior%250Acapabilities.%2520Furthermore%252C%2520we%2520show%2520that%2520our%2520decomposition%2520provides%2520insights%250Ainto%2520differences%2520in%2520behaviors%2520driven%2520by%2520prompt%2520engineering%252C%2520offering%2520a%2520valuable%250Atool%2520for%2520refining%2520performance%2520in%2520exploratory%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20Exploration%20of%20Large%20Language%20Models%20by%20Optimal%0A%20%20Exploitation&entry.906535625=Tim%20Grams%20and%20Patrick%20Betz%20and%20Christian%20Bartelt&entry.1292438233=%20%20Exploration%20is%20a%20crucial%20skill%20for%20self-improvement%20and%20open-ended%0Aproblem-solving.%20However%2C%20it%20remains%20unclear%20if%20large%20language%20models%20can%0Aeffectively%20explore%20the%20state-space%20within%20an%20unknown%20environment.%20This%20work%0Aisolates%20exploration%20as%20the%20sole%20objective%2C%20tasking%20the%20agent%20with%20delivering%0Ainformation%20that%20enhances%20future%20returns.%20Within%20this%20framework%2C%20we%20argue%20that%0Ameasuring%20agent%20returns%20is%20not%20sufficient%20for%20a%20fair%20evaluation%20and%20decompose%0Amissing%20rewards%20into%20exploration%20and%20exploitation%20components%20based%20on%20the%0Aoptimal%20achievable%20return.%20Comprehensive%20experiments%20with%20various%20models%20reveal%0Athat%20most%20struggle%20to%20sufficiently%20explore%20the%20state-space%20and%20weak%20exploration%0Ais%20insufficient.%20We%20observe%20a%20positive%20correlation%20between%20parameter%20count%20and%0Aexploration%20performance%2C%20with%20larger%20models%20demonstrating%20superior%0Acapabilities.%20Furthermore%2C%20we%20show%20that%20our%20decomposition%20provides%20insights%0Ainto%20differences%20in%20behaviors%20driven%20by%20prompt%20engineering%2C%20offering%20a%20valuable%0Atool%20for%20refining%20performance%20in%20exploratory%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08925v2&entry.124074799=Read"},
{"title": "Machine-Learning-Enhanced Optimization of Noise-Resilient Variational\n  Quantum Eigensolvers", "author": "Kim A. Nicoli and Luca J. Wagner and Lena Funcke", "abstract": "  Variational Quantum Eigensolvers (VQEs) are a powerful class of hybrid\nquantum-classical algorithms designed to approximate the ground state of a\nquantum system described by its Hamiltonian. VQEs hold promise for various\napplications, including lattice field theory. However, the inherent noise of\nNoisy Intermediate-Scale Quantum (NISQ) devices poses a significant challenge\nfor running VQEs as these algorithms are particularly susceptible to noise,\ne.g., measurement shot noise and hardware noise.\n  In a recent work, it was proposed to enhance the classical optimization of\nVQEs with Gaussian Processes (GPs) and Bayesian Optimization, as these\nmachine-learning techniques are well-suited for handling noisy data. In these\nproceedings, we provide additional insights into this new algorithm and present\nfurther numerical experiments. In particular, we examine the impact of hardware\nnoise and error mitigation on the algorithm's performance. We validate the\nalgorithm using classical simulations of quantum hardware, including hardware\nnoise benchmarks, which have not been considered in previous works. Our\nnumerical experiments demonstrate that GP-enhanced algorithms can outperform\nstate-of-the-art baselines, laying the foundation for future research on\ndeploying these techniques to real quantum hardware and lattice field theory\nsetups.\n", "link": "http://arxiv.org/abs/2501.17689v2", "date": "2025-02-03", "relevancy": 2.3063, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4966}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4456}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine-Learning-Enhanced%20Optimization%20of%20Noise-Resilient%20Variational%0A%20%20Quantum%20Eigensolvers&body=Title%3A%20Machine-Learning-Enhanced%20Optimization%20of%20Noise-Resilient%20Variational%0A%20%20Quantum%20Eigensolvers%0AAuthor%3A%20Kim%20A.%20Nicoli%20and%20Luca%20J.%20Wagner%20and%20Lena%20Funcke%0AAbstract%3A%20%20%20Variational%20Quantum%20Eigensolvers%20%28VQEs%29%20are%20a%20powerful%20class%20of%20hybrid%0Aquantum-classical%20algorithms%20designed%20to%20approximate%20the%20ground%20state%20of%20a%0Aquantum%20system%20described%20by%20its%20Hamiltonian.%20VQEs%20hold%20promise%20for%20various%0Aapplications%2C%20including%20lattice%20field%20theory.%20However%2C%20the%20inherent%20noise%20of%0ANoisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%20devices%20poses%20a%20significant%20challenge%0Afor%20running%20VQEs%20as%20these%20algorithms%20are%20particularly%20susceptible%20to%20noise%2C%0Ae.g.%2C%20measurement%20shot%20noise%20and%20hardware%20noise.%0A%20%20In%20a%20recent%20work%2C%20it%20was%20proposed%20to%20enhance%20the%20classical%20optimization%20of%0AVQEs%20with%20Gaussian%20Processes%20%28GPs%29%20and%20Bayesian%20Optimization%2C%20as%20these%0Amachine-learning%20techniques%20are%20well-suited%20for%20handling%20noisy%20data.%20In%20these%0Aproceedings%2C%20we%20provide%20additional%20insights%20into%20this%20new%20algorithm%20and%20present%0Afurther%20numerical%20experiments.%20In%20particular%2C%20we%20examine%20the%20impact%20of%20hardware%0Anoise%20and%20error%20mitigation%20on%20the%20algorithm%27s%20performance.%20We%20validate%20the%0Aalgorithm%20using%20classical%20simulations%20of%20quantum%20hardware%2C%20including%20hardware%0Anoise%20benchmarks%2C%20which%20have%20not%20been%20considered%20in%20previous%20works.%20Our%0Anumerical%20experiments%20demonstrate%20that%20GP-enhanced%20algorithms%20can%20outperform%0Astate-of-the-art%20baselines%2C%20laying%20the%20foundation%20for%20future%20research%20on%0Adeploying%20these%20techniques%20to%20real%20quantum%20hardware%20and%20lattice%20field%20theory%0Asetups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17689v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine-Learning-Enhanced%2520Optimization%2520of%2520Noise-Resilient%2520Variational%250A%2520%2520Quantum%2520Eigensolvers%26entry.906535625%3DKim%2520A.%2520Nicoli%2520and%2520Luca%2520J.%2520Wagner%2520and%2520Lena%2520Funcke%26entry.1292438233%3D%2520%2520Variational%2520Quantum%2520Eigensolvers%2520%2528VQEs%2529%2520are%2520a%2520powerful%2520class%2520of%2520hybrid%250Aquantum-classical%2520algorithms%2520designed%2520to%2520approximate%2520the%2520ground%2520state%2520of%2520a%250Aquantum%2520system%2520described%2520by%2520its%2520Hamiltonian.%2520VQEs%2520hold%2520promise%2520for%2520various%250Aapplications%252C%2520including%2520lattice%2520field%2520theory.%2520However%252C%2520the%2520inherent%2520noise%2520of%250ANoisy%2520Intermediate-Scale%2520Quantum%2520%2528NISQ%2529%2520devices%2520poses%2520a%2520significant%2520challenge%250Afor%2520running%2520VQEs%2520as%2520these%2520algorithms%2520are%2520particularly%2520susceptible%2520to%2520noise%252C%250Ae.g.%252C%2520measurement%2520shot%2520noise%2520and%2520hardware%2520noise.%250A%2520%2520In%2520a%2520recent%2520work%252C%2520it%2520was%2520proposed%2520to%2520enhance%2520the%2520classical%2520optimization%2520of%250AVQEs%2520with%2520Gaussian%2520Processes%2520%2528GPs%2529%2520and%2520Bayesian%2520Optimization%252C%2520as%2520these%250Amachine-learning%2520techniques%2520are%2520well-suited%2520for%2520handling%2520noisy%2520data.%2520In%2520these%250Aproceedings%252C%2520we%2520provide%2520additional%2520insights%2520into%2520this%2520new%2520algorithm%2520and%2520present%250Afurther%2520numerical%2520experiments.%2520In%2520particular%252C%2520we%2520examine%2520the%2520impact%2520of%2520hardware%250Anoise%2520and%2520error%2520mitigation%2520on%2520the%2520algorithm%2527s%2520performance.%2520We%2520validate%2520the%250Aalgorithm%2520using%2520classical%2520simulations%2520of%2520quantum%2520hardware%252C%2520including%2520hardware%250Anoise%2520benchmarks%252C%2520which%2520have%2520not%2520been%2520considered%2520in%2520previous%2520works.%2520Our%250Anumerical%2520experiments%2520demonstrate%2520that%2520GP-enhanced%2520algorithms%2520can%2520outperform%250Astate-of-the-art%2520baselines%252C%2520laying%2520the%2520foundation%2520for%2520future%2520research%2520on%250Adeploying%2520these%2520techniques%2520to%2520real%2520quantum%2520hardware%2520and%2520lattice%2520field%2520theory%250Asetups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17689v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine-Learning-Enhanced%20Optimization%20of%20Noise-Resilient%20Variational%0A%20%20Quantum%20Eigensolvers&entry.906535625=Kim%20A.%20Nicoli%20and%20Luca%20J.%20Wagner%20and%20Lena%20Funcke&entry.1292438233=%20%20Variational%20Quantum%20Eigensolvers%20%28VQEs%29%20are%20a%20powerful%20class%20of%20hybrid%0Aquantum-classical%20algorithms%20designed%20to%20approximate%20the%20ground%20state%20of%20a%0Aquantum%20system%20described%20by%20its%20Hamiltonian.%20VQEs%20hold%20promise%20for%20various%0Aapplications%2C%20including%20lattice%20field%20theory.%20However%2C%20the%20inherent%20noise%20of%0ANoisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%20devices%20poses%20a%20significant%20challenge%0Afor%20running%20VQEs%20as%20these%20algorithms%20are%20particularly%20susceptible%20to%20noise%2C%0Ae.g.%2C%20measurement%20shot%20noise%20and%20hardware%20noise.%0A%20%20In%20a%20recent%20work%2C%20it%20was%20proposed%20to%20enhance%20the%20classical%20optimization%20of%0AVQEs%20with%20Gaussian%20Processes%20%28GPs%29%20and%20Bayesian%20Optimization%2C%20as%20these%0Amachine-learning%20techniques%20are%20well-suited%20for%20handling%20noisy%20data.%20In%20these%0Aproceedings%2C%20we%20provide%20additional%20insights%20into%20this%20new%20algorithm%20and%20present%0Afurther%20numerical%20experiments.%20In%20particular%2C%20we%20examine%20the%20impact%20of%20hardware%0Anoise%20and%20error%20mitigation%20on%20the%20algorithm%27s%20performance.%20We%20validate%20the%0Aalgorithm%20using%20classical%20simulations%20of%20quantum%20hardware%2C%20including%20hardware%0Anoise%20benchmarks%2C%20which%20have%20not%20been%20considered%20in%20previous%20works.%20Our%0Anumerical%20experiments%20demonstrate%20that%20GP-enhanced%20algorithms%20can%20outperform%0Astate-of-the-art%20baselines%2C%20laying%20the%20foundation%20for%20future%20research%20on%0Adeploying%20these%20techniques%20to%20real%20quantum%20hardware%20and%20lattice%20field%20theory%0Asetups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17689v2&entry.124074799=Read"},
{"title": "Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin\n  Case Study", "author": "Yuchen Lei and Yuexin Xiang and Qin Wang and Rafael Dowsley and Tsz Hon Yuen and Jiangshan Yu", "abstract": "  Cryptocurrencies are widely used, yet current methods for analyzing\ntransactions heavily rely on opaque, black-box models. These lack\ninterpretability and adaptability, failing to effectively capture behavioral\npatterns. Many researchers, including us, believe that Large Language Models\n(LLMs) could bridge this gap due to their robust reasoning abilities for\ncomplex tasks. In this paper, we test this hypothesis by applying LLMs to\nreal-world cryptocurrency transaction graphs, specifically within the Bitcoin\nnetwork. We introduce a three-tiered framework to assess LLM capabilities:\nfoundational metrics, characteristic overview, and contextual interpretation.\nThis includes a new, human-readable graph representation format, LLM4TG, and a\nconnectivity-enhanced sampling algorithm, CETraS, which simplifies larger\ntransaction graphs. Experimental results show that LLMs excel at foundational\nmetrics and offer detailed characteristic overviews. Their effectiveness in\ncontextual interpretation suggests they can provide useful explanations of\ntransaction behaviors, even with limited labeled data.\n", "link": "http://arxiv.org/abs/2501.18158v2", "date": "2025-02-03", "relevancy": 2.2925, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4658}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20for%20Cryptocurrency%20Transaction%20Analysis%3A%20A%20Bitcoin%0A%20%20Case%20Study&body=Title%3A%20Large%20Language%20Models%20for%20Cryptocurrency%20Transaction%20Analysis%3A%20A%20Bitcoin%0A%20%20Case%20Study%0AAuthor%3A%20Yuchen%20Lei%20and%20Yuexin%20Xiang%20and%20Qin%20Wang%20and%20Rafael%20Dowsley%20and%20Tsz%20Hon%20Yuen%20and%20Jiangshan%20Yu%0AAbstract%3A%20%20%20Cryptocurrencies%20are%20widely%20used%2C%20yet%20current%20methods%20for%20analyzing%0Atransactions%20heavily%20rely%20on%20opaque%2C%20black-box%20models.%20These%20lack%0Ainterpretability%20and%20adaptability%2C%20failing%20to%20effectively%20capture%20behavioral%0Apatterns.%20Many%20researchers%2C%20including%20us%2C%20believe%20that%20Large%20Language%20Models%0A%28LLMs%29%20could%20bridge%20this%20gap%20due%20to%20their%20robust%20reasoning%20abilities%20for%0Acomplex%20tasks.%20In%20this%20paper%2C%20we%20test%20this%20hypothesis%20by%20applying%20LLMs%20to%0Areal-world%20cryptocurrency%20transaction%20graphs%2C%20specifically%20within%20the%20Bitcoin%0Anetwork.%20We%20introduce%20a%20three-tiered%20framework%20to%20assess%20LLM%20capabilities%3A%0Afoundational%20metrics%2C%20characteristic%20overview%2C%20and%20contextual%20interpretation.%0AThis%20includes%20a%20new%2C%20human-readable%20graph%20representation%20format%2C%20LLM4TG%2C%20and%20a%0Aconnectivity-enhanced%20sampling%20algorithm%2C%20CETraS%2C%20which%20simplifies%20larger%0Atransaction%20graphs.%20Experimental%20results%20show%20that%20LLMs%20excel%20at%20foundational%0Ametrics%20and%20offer%20detailed%20characteristic%20overviews.%20Their%20effectiveness%20in%0Acontextual%20interpretation%20suggests%20they%20can%20provide%20useful%20explanations%20of%0Atransaction%20behaviors%2C%20even%20with%20limited%20labeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18158v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520for%2520Cryptocurrency%2520Transaction%2520Analysis%253A%2520A%2520Bitcoin%250A%2520%2520Case%2520Study%26entry.906535625%3DYuchen%2520Lei%2520and%2520Yuexin%2520Xiang%2520and%2520Qin%2520Wang%2520and%2520Rafael%2520Dowsley%2520and%2520Tsz%2520Hon%2520Yuen%2520and%2520Jiangshan%2520Yu%26entry.1292438233%3D%2520%2520Cryptocurrencies%2520are%2520widely%2520used%252C%2520yet%2520current%2520methods%2520for%2520analyzing%250Atransactions%2520heavily%2520rely%2520on%2520opaque%252C%2520black-box%2520models.%2520These%2520lack%250Ainterpretability%2520and%2520adaptability%252C%2520failing%2520to%2520effectively%2520capture%2520behavioral%250Apatterns.%2520Many%2520researchers%252C%2520including%2520us%252C%2520believe%2520that%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520could%2520bridge%2520this%2520gap%2520due%2520to%2520their%2520robust%2520reasoning%2520abilities%2520for%250Acomplex%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520test%2520this%2520hypothesis%2520by%2520applying%2520LLMs%2520to%250Areal-world%2520cryptocurrency%2520transaction%2520graphs%252C%2520specifically%2520within%2520the%2520Bitcoin%250Anetwork.%2520We%2520introduce%2520a%2520three-tiered%2520framework%2520to%2520assess%2520LLM%2520capabilities%253A%250Afoundational%2520metrics%252C%2520characteristic%2520overview%252C%2520and%2520contextual%2520interpretation.%250AThis%2520includes%2520a%2520new%252C%2520human-readable%2520graph%2520representation%2520format%252C%2520LLM4TG%252C%2520and%2520a%250Aconnectivity-enhanced%2520sampling%2520algorithm%252C%2520CETraS%252C%2520which%2520simplifies%2520larger%250Atransaction%2520graphs.%2520Experimental%2520results%2520show%2520that%2520LLMs%2520excel%2520at%2520foundational%250Ametrics%2520and%2520offer%2520detailed%2520characteristic%2520overviews.%2520Their%2520effectiveness%2520in%250Acontextual%2520interpretation%2520suggests%2520they%2520can%2520provide%2520useful%2520explanations%2520of%250Atransaction%2520behaviors%252C%2520even%2520with%2520limited%2520labeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18158v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20for%20Cryptocurrency%20Transaction%20Analysis%3A%20A%20Bitcoin%0A%20%20Case%20Study&entry.906535625=Yuchen%20Lei%20and%20Yuexin%20Xiang%20and%20Qin%20Wang%20and%20Rafael%20Dowsley%20and%20Tsz%20Hon%20Yuen%20and%20Jiangshan%20Yu&entry.1292438233=%20%20Cryptocurrencies%20are%20widely%20used%2C%20yet%20current%20methods%20for%20analyzing%0Atransactions%20heavily%20rely%20on%20opaque%2C%20black-box%20models.%20These%20lack%0Ainterpretability%20and%20adaptability%2C%20failing%20to%20effectively%20capture%20behavioral%0Apatterns.%20Many%20researchers%2C%20including%20us%2C%20believe%20that%20Large%20Language%20Models%0A%28LLMs%29%20could%20bridge%20this%20gap%20due%20to%20their%20robust%20reasoning%20abilities%20for%0Acomplex%20tasks.%20In%20this%20paper%2C%20we%20test%20this%20hypothesis%20by%20applying%20LLMs%20to%0Areal-world%20cryptocurrency%20transaction%20graphs%2C%20specifically%20within%20the%20Bitcoin%0Anetwork.%20We%20introduce%20a%20three-tiered%20framework%20to%20assess%20LLM%20capabilities%3A%0Afoundational%20metrics%2C%20characteristic%20overview%2C%20and%20contextual%20interpretation.%0AThis%20includes%20a%20new%2C%20human-readable%20graph%20representation%20format%2C%20LLM4TG%2C%20and%20a%0Aconnectivity-enhanced%20sampling%20algorithm%2C%20CETraS%2C%20which%20simplifies%20larger%0Atransaction%20graphs.%20Experimental%20results%20show%20that%20LLMs%20excel%20at%20foundational%0Ametrics%20and%20offer%20detailed%20characteristic%20overviews.%20Their%20effectiveness%20in%0Acontextual%20interpretation%20suggests%20they%20can%20provide%20useful%20explanations%20of%0Atransaction%20behaviors%2C%20even%20with%20limited%20labeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18158v2&entry.124074799=Read"},
{"title": "Category-Theoretical and Topos-Theoretical Frameworks in Machine\n  Learning: A Survey", "author": "Yiyang Jia and Guohong Peng and Zheng Yang and Tianhao Chen", "abstract": "  In this survey, we provide an overview of category theory-derived machine\nlearning from four mainstream perspectives: gradient-based learning,\nprobability-based learning, invariance and equivalence-based learning, and\ntopos-based learning. For the first three topics, we primarily review research\nin the past five years, updating and expanding on the previous survey by\nShiebler et al.. The fourth topic, which delves into higher category theory,\nparticularly topos theory, is surveyed for the first time in this paper. In\ncertain machine learning methods, the compositionality of functors plays a\nvital role, prompting the development of specific categorical frameworks.\nHowever, when considering how the global properties of a network reflect in\nlocal structures and how geometric properties are expressed with logic, the\ntopos structure becomes particularly significant and profound.\n", "link": "http://arxiv.org/abs/2408.14014v3", "date": "2025-02-03", "relevancy": 2.2807, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4619}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Category-Theoretical%20and%20Topos-Theoretical%20Frameworks%20in%20Machine%0A%20%20Learning%3A%20A%20Survey&body=Title%3A%20Category-Theoretical%20and%20Topos-Theoretical%20Frameworks%20in%20Machine%0A%20%20Learning%3A%20A%20Survey%0AAuthor%3A%20Yiyang%20Jia%20and%20Guohong%20Peng%20and%20Zheng%20Yang%20and%20Tianhao%20Chen%0AAbstract%3A%20%20%20In%20this%20survey%2C%20we%20provide%20an%20overview%20of%20category%20theory-derived%20machine%0Alearning%20from%20four%20mainstream%20perspectives%3A%20gradient-based%20learning%2C%0Aprobability-based%20learning%2C%20invariance%20and%20equivalence-based%20learning%2C%20and%0Atopos-based%20learning.%20For%20the%20first%20three%20topics%2C%20we%20primarily%20review%20research%0Ain%20the%20past%20five%20years%2C%20updating%20and%20expanding%20on%20the%20previous%20survey%20by%0AShiebler%20et%20al..%20The%20fourth%20topic%2C%20which%20delves%20into%20higher%20category%20theory%2C%0Aparticularly%20topos%20theory%2C%20is%20surveyed%20for%20the%20first%20time%20in%20this%20paper.%20In%0Acertain%20machine%20learning%20methods%2C%20the%20compositionality%20of%20functors%20plays%20a%0Avital%20role%2C%20prompting%20the%20development%20of%20specific%20categorical%20frameworks.%0AHowever%2C%20when%20considering%20how%20the%20global%20properties%20of%20a%20network%20reflect%20in%0Alocal%20structures%20and%20how%20geometric%20properties%20are%20expressed%20with%20logic%2C%20the%0Atopos%20structure%20becomes%20particularly%20significant%20and%20profound.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14014v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCategory-Theoretical%2520and%2520Topos-Theoretical%2520Frameworks%2520in%2520Machine%250A%2520%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DYiyang%2520Jia%2520and%2520Guohong%2520Peng%2520and%2520Zheng%2520Yang%2520and%2520Tianhao%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520survey%252C%2520we%2520provide%2520an%2520overview%2520of%2520category%2520theory-derived%2520machine%250Alearning%2520from%2520four%2520mainstream%2520perspectives%253A%2520gradient-based%2520learning%252C%250Aprobability-based%2520learning%252C%2520invariance%2520and%2520equivalence-based%2520learning%252C%2520and%250Atopos-based%2520learning.%2520For%2520the%2520first%2520three%2520topics%252C%2520we%2520primarily%2520review%2520research%250Ain%2520the%2520past%2520five%2520years%252C%2520updating%2520and%2520expanding%2520on%2520the%2520previous%2520survey%2520by%250AShiebler%2520et%2520al..%2520The%2520fourth%2520topic%252C%2520which%2520delves%2520into%2520higher%2520category%2520theory%252C%250Aparticularly%2520topos%2520theory%252C%2520is%2520surveyed%2520for%2520the%2520first%2520time%2520in%2520this%2520paper.%2520In%250Acertain%2520machine%2520learning%2520methods%252C%2520the%2520compositionality%2520of%2520functors%2520plays%2520a%250Avital%2520role%252C%2520prompting%2520the%2520development%2520of%2520specific%2520categorical%2520frameworks.%250AHowever%252C%2520when%2520considering%2520how%2520the%2520global%2520properties%2520of%2520a%2520network%2520reflect%2520in%250Alocal%2520structures%2520and%2520how%2520geometric%2520properties%2520are%2520expressed%2520with%2520logic%252C%2520the%250Atopos%2520structure%2520becomes%2520particularly%2520significant%2520and%2520profound.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14014v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Category-Theoretical%20and%20Topos-Theoretical%20Frameworks%20in%20Machine%0A%20%20Learning%3A%20A%20Survey&entry.906535625=Yiyang%20Jia%20and%20Guohong%20Peng%20and%20Zheng%20Yang%20and%20Tianhao%20Chen&entry.1292438233=%20%20In%20this%20survey%2C%20we%20provide%20an%20overview%20of%20category%20theory-derived%20machine%0Alearning%20from%20four%20mainstream%20perspectives%3A%20gradient-based%20learning%2C%0Aprobability-based%20learning%2C%20invariance%20and%20equivalence-based%20learning%2C%20and%0Atopos-based%20learning.%20For%20the%20first%20three%20topics%2C%20we%20primarily%20review%20research%0Ain%20the%20past%20five%20years%2C%20updating%20and%20expanding%20on%20the%20previous%20survey%20by%0AShiebler%20et%20al..%20The%20fourth%20topic%2C%20which%20delves%20into%20higher%20category%20theory%2C%0Aparticularly%20topos%20theory%2C%20is%20surveyed%20for%20the%20first%20time%20in%20this%20paper.%20In%0Acertain%20machine%20learning%20methods%2C%20the%20compositionality%20of%20functors%20plays%20a%0Avital%20role%2C%20prompting%20the%20development%20of%20specific%20categorical%20frameworks.%0AHowever%2C%20when%20considering%20how%20the%20global%20properties%20of%20a%20network%20reflect%20in%0Alocal%20structures%20and%20how%20geometric%20properties%20are%20expressed%20with%20logic%2C%20the%0Atopos%20structure%20becomes%20particularly%20significant%20and%20profound.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14014v3&entry.124074799=Read"},
{"title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt\n  Adversarial Attacks on LLMs", "author": "Sergey Berezin and Reza Farahbakhsh and Noel Crespi", "abstract": "  We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes.\n", "link": "http://arxiv.org/abs/2501.18626v2", "date": "2025-02-03", "relevancy": 2.2518, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4669}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20TIP%20of%20the%20Iceberg%3A%20Revealing%20a%20Hidden%20Class%20of%20Task-In-Prompt%0A%20%20Adversarial%20Attacks%20on%20LLMs&body=Title%3A%20The%20TIP%20of%20the%20Iceberg%3A%20Revealing%20a%20Hidden%20Class%20of%20Task-In-Prompt%0A%20%20Adversarial%20Attacks%20on%20LLMs%0AAuthor%3A%20Sergey%20Berezin%20and%20Reza%20Farahbakhsh%20and%20Noel%20Crespi%0AAbstract%3A%20%20%20We%20present%20a%20novel%20class%20of%20jailbreak%20adversarial%20attacks%20on%20LLMs%2C%20termed%0ATask-in-Prompt%20%28TIP%29%20attacks.%20Our%20approach%20embeds%20sequence-to-sequence%20tasks%0A%28e.g.%2C%20cipher%20decoding%2C%20riddles%2C%20code%20execution%29%20into%20the%20model%27s%20prompt%20to%0Aindirectly%20generate%20prohibited%20inputs.%20To%20systematically%20assess%20the%0Aeffectiveness%20of%20these%20attacks%2C%20we%20introduce%20the%20PHRYGE%20benchmark.%20We%0Ademonstrate%20that%20our%20techniques%20successfully%20circumvent%20safeguards%20in%20six%0Astate-of-the-art%20language%20models%2C%20including%20GPT-4o%20and%20LLaMA%203.2.%20Our%20findings%0Ahighlight%20critical%20weaknesses%20in%20current%20LLM%20safety%20alignments%20and%20underscore%0Athe%20urgent%20need%20for%20more%20sophisticated%20defence%20strategies.%0A%20%20Warning%3A%20this%20paper%20contains%20examples%20of%20unethical%20inquiries%20used%20solely%20for%0Aresearch%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18626v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520TIP%2520of%2520the%2520Iceberg%253A%2520Revealing%2520a%2520Hidden%2520Class%2520of%2520Task-In-Prompt%250A%2520%2520Adversarial%2520Attacks%2520on%2520LLMs%26entry.906535625%3DSergey%2520Berezin%2520and%2520Reza%2520Farahbakhsh%2520and%2520Noel%2520Crespi%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520class%2520of%2520jailbreak%2520adversarial%2520attacks%2520on%2520LLMs%252C%2520termed%250ATask-in-Prompt%2520%2528TIP%2529%2520attacks.%2520Our%2520approach%2520embeds%2520sequence-to-sequence%2520tasks%250A%2528e.g.%252C%2520cipher%2520decoding%252C%2520riddles%252C%2520code%2520execution%2529%2520into%2520the%2520model%2527s%2520prompt%2520to%250Aindirectly%2520generate%2520prohibited%2520inputs.%2520To%2520systematically%2520assess%2520the%250Aeffectiveness%2520of%2520these%2520attacks%252C%2520we%2520introduce%2520the%2520PHRYGE%2520benchmark.%2520We%250Ademonstrate%2520that%2520our%2520techniques%2520successfully%2520circumvent%2520safeguards%2520in%2520six%250Astate-of-the-art%2520language%2520models%252C%2520including%2520GPT-4o%2520and%2520LLaMA%25203.2.%2520Our%2520findings%250Ahighlight%2520critical%2520weaknesses%2520in%2520current%2520LLM%2520safety%2520alignments%2520and%2520underscore%250Athe%2520urgent%2520need%2520for%2520more%2520sophisticated%2520defence%2520strategies.%250A%2520%2520Warning%253A%2520this%2520paper%2520contains%2520examples%2520of%2520unethical%2520inquiries%2520used%2520solely%2520for%250Aresearch%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18626v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20TIP%20of%20the%20Iceberg%3A%20Revealing%20a%20Hidden%20Class%20of%20Task-In-Prompt%0A%20%20Adversarial%20Attacks%20on%20LLMs&entry.906535625=Sergey%20Berezin%20and%20Reza%20Farahbakhsh%20and%20Noel%20Crespi&entry.1292438233=%20%20We%20present%20a%20novel%20class%20of%20jailbreak%20adversarial%20attacks%20on%20LLMs%2C%20termed%0ATask-in-Prompt%20%28TIP%29%20attacks.%20Our%20approach%20embeds%20sequence-to-sequence%20tasks%0A%28e.g.%2C%20cipher%20decoding%2C%20riddles%2C%20code%20execution%29%20into%20the%20model%27s%20prompt%20to%0Aindirectly%20generate%20prohibited%20inputs.%20To%20systematically%20assess%20the%0Aeffectiveness%20of%20these%20attacks%2C%20we%20introduce%20the%20PHRYGE%20benchmark.%20We%0Ademonstrate%20that%20our%20techniques%20successfully%20circumvent%20safeguards%20in%20six%0Astate-of-the-art%20language%20models%2C%20including%20GPT-4o%20and%20LLaMA%203.2.%20Our%20findings%0Ahighlight%20critical%20weaknesses%20in%20current%20LLM%20safety%20alignments%20and%20underscore%0Athe%20urgent%20need%20for%20more%20sophisticated%20defence%20strategies.%0A%20%20Warning%3A%20this%20paper%20contains%20examples%20of%20unethical%20inquiries%20used%20solely%20for%0Aresearch%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18626v2&entry.124074799=Read"},
{"title": "Mutual Information Preserving Neural Network Pruning", "author": "Charles Westphal and Stephen Hailes and Mirco Musolesi", "abstract": "  Pruning has emerged as the primary approach used to limit the resource\nrequirements of large neural networks (NNs). Since the proposal of the lottery\nticket hypothesis, researchers have focused either on pruning at initialization\nor after training. However, recent theoretical findings have shown that the\nsample efficiency of robust pruned models is proportional to the mutual\ninformation (MI) between the pruning masks and the model's training datasets,\n\\textit{whether at initialization or after training}. In this paper, starting\nfrom these results, we introduce Mutual Information Preserving Pruning (MIPP),\na structured activation-based pruning technique applicable before or after\ntraining. The core principle of MIPP is to select nodes in a way that conserves\nMI shared between the activations of adjacent layers, and consequently between\nthe data and masks. Approaching the pruning problem in this manner means we can\nprove that there exists a function that can map the pruned upstream layer's\nactivations to the downstream layer's, implying re-trainability. We demonstrate\nthat MIPP consistently outperforms state-of-the-art methods, regardless of\nwhether pruning is performed before or after training.\n", "link": "http://arxiv.org/abs/2411.00147v2", "date": "2025-02-03", "relevancy": 2.2407, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4694}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4556}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mutual%20Information%20Preserving%20Neural%20Network%20Pruning&body=Title%3A%20Mutual%20Information%20Preserving%20Neural%20Network%20Pruning%0AAuthor%3A%20Charles%20Westphal%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi%0AAbstract%3A%20%20%20Pruning%20has%20emerged%20as%20the%20primary%20approach%20used%20to%20limit%20the%20resource%0Arequirements%20of%20large%20neural%20networks%20%28NNs%29.%20Since%20the%20proposal%20of%20the%20lottery%0Aticket%20hypothesis%2C%20researchers%20have%20focused%20either%20on%20pruning%20at%20initialization%0Aor%20after%20training.%20However%2C%20recent%20theoretical%20findings%20have%20shown%20that%20the%0Asample%20efficiency%20of%20robust%20pruned%20models%20is%20proportional%20to%20the%20mutual%0Ainformation%20%28MI%29%20between%20the%20pruning%20masks%20and%20the%20model%27s%20training%20datasets%2C%0A%5Ctextit%7Bwhether%20at%20initialization%20or%20after%20training%7D.%20In%20this%20paper%2C%20starting%0Afrom%20these%20results%2C%20we%20introduce%20Mutual%20Information%20Preserving%20Pruning%20%28MIPP%29%2C%0Aa%20structured%20activation-based%20pruning%20technique%20applicable%20before%20or%20after%0Atraining.%20The%20core%20principle%20of%20MIPP%20is%20to%20select%20nodes%20in%20a%20way%20that%20conserves%0AMI%20shared%20between%20the%20activations%20of%20adjacent%20layers%2C%20and%20consequently%20between%0Athe%20data%20and%20masks.%20Approaching%20the%20pruning%20problem%20in%20this%20manner%20means%20we%20can%0Aprove%20that%20there%20exists%20a%20function%20that%20can%20map%20the%20pruned%20upstream%20layer%27s%0Aactivations%20to%20the%20downstream%20layer%27s%2C%20implying%20re-trainability.%20We%20demonstrate%0Athat%20MIPP%20consistently%20outperforms%20state-of-the-art%20methods%2C%20regardless%20of%0Awhether%20pruning%20is%20performed%20before%20or%20after%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00147v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMutual%2520Information%2520Preserving%2520Neural%2520Network%2520Pruning%26entry.906535625%3DCharles%2520Westphal%2520and%2520Stephen%2520Hailes%2520and%2520Mirco%2520Musolesi%26entry.1292438233%3D%2520%2520Pruning%2520has%2520emerged%2520as%2520the%2520primary%2520approach%2520used%2520to%2520limit%2520the%2520resource%250Arequirements%2520of%2520large%2520neural%2520networks%2520%2528NNs%2529.%2520Since%2520the%2520proposal%2520of%2520the%2520lottery%250Aticket%2520hypothesis%252C%2520researchers%2520have%2520focused%2520either%2520on%2520pruning%2520at%2520initialization%250Aor%2520after%2520training.%2520However%252C%2520recent%2520theoretical%2520findings%2520have%2520shown%2520that%2520the%250Asample%2520efficiency%2520of%2520robust%2520pruned%2520models%2520is%2520proportional%2520to%2520the%2520mutual%250Ainformation%2520%2528MI%2529%2520between%2520the%2520pruning%2520masks%2520and%2520the%2520model%2527s%2520training%2520datasets%252C%250A%255Ctextit%257Bwhether%2520at%2520initialization%2520or%2520after%2520training%257D.%2520In%2520this%2520paper%252C%2520starting%250Afrom%2520these%2520results%252C%2520we%2520introduce%2520Mutual%2520Information%2520Preserving%2520Pruning%2520%2528MIPP%2529%252C%250Aa%2520structured%2520activation-based%2520pruning%2520technique%2520applicable%2520before%2520or%2520after%250Atraining.%2520The%2520core%2520principle%2520of%2520MIPP%2520is%2520to%2520select%2520nodes%2520in%2520a%2520way%2520that%2520conserves%250AMI%2520shared%2520between%2520the%2520activations%2520of%2520adjacent%2520layers%252C%2520and%2520consequently%2520between%250Athe%2520data%2520and%2520masks.%2520Approaching%2520the%2520pruning%2520problem%2520in%2520this%2520manner%2520means%2520we%2520can%250Aprove%2520that%2520there%2520exists%2520a%2520function%2520that%2520can%2520map%2520the%2520pruned%2520upstream%2520layer%2527s%250Aactivations%2520to%2520the%2520downstream%2520layer%2527s%252C%2520implying%2520re-trainability.%2520We%2520demonstrate%250Athat%2520MIPP%2520consistently%2520outperforms%2520state-of-the-art%2520methods%252C%2520regardless%2520of%250Awhether%2520pruning%2520is%2520performed%2520before%2520or%2520after%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00147v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mutual%20Information%20Preserving%20Neural%20Network%20Pruning&entry.906535625=Charles%20Westphal%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi&entry.1292438233=%20%20Pruning%20has%20emerged%20as%20the%20primary%20approach%20used%20to%20limit%20the%20resource%0Arequirements%20of%20large%20neural%20networks%20%28NNs%29.%20Since%20the%20proposal%20of%20the%20lottery%0Aticket%20hypothesis%2C%20researchers%20have%20focused%20either%20on%20pruning%20at%20initialization%0Aor%20after%20training.%20However%2C%20recent%20theoretical%20findings%20have%20shown%20that%20the%0Asample%20efficiency%20of%20robust%20pruned%20models%20is%20proportional%20to%20the%20mutual%0Ainformation%20%28MI%29%20between%20the%20pruning%20masks%20and%20the%20model%27s%20training%20datasets%2C%0A%5Ctextit%7Bwhether%20at%20initialization%20or%20after%20training%7D.%20In%20this%20paper%2C%20starting%0Afrom%20these%20results%2C%20we%20introduce%20Mutual%20Information%20Preserving%20Pruning%20%28MIPP%29%2C%0Aa%20structured%20activation-based%20pruning%20technique%20applicable%20before%20or%20after%0Atraining.%20The%20core%20principle%20of%20MIPP%20is%20to%20select%20nodes%20in%20a%20way%20that%20conserves%0AMI%20shared%20between%20the%20activations%20of%20adjacent%20layers%2C%20and%20consequently%20between%0Athe%20data%20and%20masks.%20Approaching%20the%20pruning%20problem%20in%20this%20manner%20means%20we%20can%0Aprove%20that%20there%20exists%20a%20function%20that%20can%20map%20the%20pruned%20upstream%20layer%27s%0Aactivations%20to%20the%20downstream%20layer%27s%2C%20implying%20re-trainability.%20We%20demonstrate%0Athat%20MIPP%20consistently%20outperforms%20state-of-the-art%20methods%2C%20regardless%20of%0Awhether%20pruning%20is%20performed%20before%20or%20after%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00147v2&entry.124074799=Read"},
{"title": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool\n  Usage", "author": "Zhi Gao and Bofei Zhang and Pengxiang Li and Xiaojian Ma and Tao Yuan and Yue Fan and Yuwei Wu and Yunde Jia and Song-Chun Zhu and Qing Li", "abstract": "  The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities.\n", "link": "http://arxiv.org/abs/2412.15606v2", "date": "2025-02-03", "relevancy": 2.2107, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5834}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5558}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Agent%20Tuning%3A%20Building%20a%20VLM-Driven%20Agent%20for%20Efficient%20Tool%0A%20%20Usage&body=Title%3A%20Multi-modal%20Agent%20Tuning%3A%20Building%20a%20VLM-Driven%20Agent%20for%20Efficient%20Tool%0A%20%20Usage%0AAuthor%3A%20Zhi%20Gao%20and%20Bofei%20Zhang%20and%20Pengxiang%20Li%20and%20Xiaojian%20Ma%20and%20Tao%20Yuan%20and%20Yue%20Fan%20and%20Yuwei%20Wu%20and%20Yunde%20Jia%20and%20Song-Chun%20Zhu%20and%20Qing%20Li%0AAbstract%3A%20%20%20The%20advancement%20of%20large%20language%20models%20%28LLMs%29%20prompts%20the%20development%20of%0Amulti-modal%20agents%2C%20which%20are%20used%20as%20a%20controller%20to%20call%20external%20tools%2C%0Aproviding%20a%20feasible%20way%20to%20solve%20practical%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%0Amulti-modal%20agent%20tuning%20method%20that%20automatically%20generates%20multi-modal%0Atool-usage%20data%20and%20tunes%20a%20vision-language%20model%20%28VLM%29%20as%20the%20controller%20for%0Apowerful%20tool-usage%20reasoning.%20To%20preserve%20the%20data%20quality%2C%20we%20prompt%20the%0AGPT-4o%20mini%20model%20to%20generate%20queries%2C%20files%2C%20and%20trajectories%2C%20followed%20by%0Aquery-file%20and%20trajectory%20verifiers.%20Based%20on%20the%20data%20synthesis%20pipeline%2C%20we%0Acollect%20the%20MM-Traj%20dataset%20that%20contains%2020K%20tasks%20with%20trajectories%20of%20tool%0Ausage.%20Then%2C%20we%20develop%20the%20T3-Agent%20via%20%5Cunderline%7BT%7Drajectory%0A%5Cunderline%7BT%7Duning%20on%20VLMs%20for%20%5Cunderline%7BT%7Dool%20usage%20using%20MM-Traj.%0AEvaluations%20on%20the%20GTA%20and%20GAIA%20benchmarks%20show%20that%20the%20T3-Agent%20consistently%0Aachieves%20improvements%20on%20two%20popular%20VLMs%3A%20MiniCPM-V-8.5B%20and%20%7BQwen2-VL-7B%7D%2C%0Awhich%20outperforms%20untrained%20VLMs%20by%20%2420%5C%25%24%2C%20showing%20the%20effectiveness%20of%20the%0Aproposed%20data%20synthesis%20pipeline%2C%20leading%20to%20high-quality%20data%20for%20tool-usage%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15606v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Agent%2520Tuning%253A%2520Building%2520a%2520VLM-Driven%2520Agent%2520for%2520Efficient%2520Tool%250A%2520%2520Usage%26entry.906535625%3DZhi%2520Gao%2520and%2520Bofei%2520Zhang%2520and%2520Pengxiang%2520Li%2520and%2520Xiaojian%2520Ma%2520and%2520Tao%2520Yuan%2520and%2520Yue%2520Fan%2520and%2520Yuwei%2520Wu%2520and%2520Yunde%2520Jia%2520and%2520Song-Chun%2520Zhu%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520prompts%2520the%2520development%2520of%250Amulti-modal%2520agents%252C%2520which%2520are%2520used%2520as%2520a%2520controller%2520to%2520call%2520external%2520tools%252C%250Aproviding%2520a%2520feasible%2520way%2520to%2520solve%2520practical%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Amulti-modal%2520agent%2520tuning%2520method%2520that%2520automatically%2520generates%2520multi-modal%250Atool-usage%2520data%2520and%2520tunes%2520a%2520vision-language%2520model%2520%2528VLM%2529%2520as%2520the%2520controller%2520for%250Apowerful%2520tool-usage%2520reasoning.%2520To%2520preserve%2520the%2520data%2520quality%252C%2520we%2520prompt%2520the%250AGPT-4o%2520mini%2520model%2520to%2520generate%2520queries%252C%2520files%252C%2520and%2520trajectories%252C%2520followed%2520by%250Aquery-file%2520and%2520trajectory%2520verifiers.%2520Based%2520on%2520the%2520data%2520synthesis%2520pipeline%252C%2520we%250Acollect%2520the%2520MM-Traj%2520dataset%2520that%2520contains%252020K%2520tasks%2520with%2520trajectories%2520of%2520tool%250Ausage.%2520Then%252C%2520we%2520develop%2520the%2520T3-Agent%2520via%2520%255Cunderline%257BT%257Drajectory%250A%255Cunderline%257BT%257Duning%2520on%2520VLMs%2520for%2520%255Cunderline%257BT%257Dool%2520usage%2520using%2520MM-Traj.%250AEvaluations%2520on%2520the%2520GTA%2520and%2520GAIA%2520benchmarks%2520show%2520that%2520the%2520T3-Agent%2520consistently%250Aachieves%2520improvements%2520on%2520two%2520popular%2520VLMs%253A%2520MiniCPM-V-8.5B%2520and%2520%257BQwen2-VL-7B%257D%252C%250Awhich%2520outperforms%2520untrained%2520VLMs%2520by%2520%252420%255C%2525%2524%252C%2520showing%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520data%2520synthesis%2520pipeline%252C%2520leading%2520to%2520high-quality%2520data%2520for%2520tool-usage%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15606v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Agent%20Tuning%3A%20Building%20a%20VLM-Driven%20Agent%20for%20Efficient%20Tool%0A%20%20Usage&entry.906535625=Zhi%20Gao%20and%20Bofei%20Zhang%20and%20Pengxiang%20Li%20and%20Xiaojian%20Ma%20and%20Tao%20Yuan%20and%20Yue%20Fan%20and%20Yuwei%20Wu%20and%20Yunde%20Jia%20and%20Song-Chun%20Zhu%20and%20Qing%20Li&entry.1292438233=%20%20The%20advancement%20of%20large%20language%20models%20%28LLMs%29%20prompts%20the%20development%20of%0Amulti-modal%20agents%2C%20which%20are%20used%20as%20a%20controller%20to%20call%20external%20tools%2C%0Aproviding%20a%20feasible%20way%20to%20solve%20practical%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%0Amulti-modal%20agent%20tuning%20method%20that%20automatically%20generates%20multi-modal%0Atool-usage%20data%20and%20tunes%20a%20vision-language%20model%20%28VLM%29%20as%20the%20controller%20for%0Apowerful%20tool-usage%20reasoning.%20To%20preserve%20the%20data%20quality%2C%20we%20prompt%20the%0AGPT-4o%20mini%20model%20to%20generate%20queries%2C%20files%2C%20and%20trajectories%2C%20followed%20by%0Aquery-file%20and%20trajectory%20verifiers.%20Based%20on%20the%20data%20synthesis%20pipeline%2C%20we%0Acollect%20the%20MM-Traj%20dataset%20that%20contains%2020K%20tasks%20with%20trajectories%20of%20tool%0Ausage.%20Then%2C%20we%20develop%20the%20T3-Agent%20via%20%5Cunderline%7BT%7Drajectory%0A%5Cunderline%7BT%7Duning%20on%20VLMs%20for%20%5Cunderline%7BT%7Dool%20usage%20using%20MM-Traj.%0AEvaluations%20on%20the%20GTA%20and%20GAIA%20benchmarks%20show%20that%20the%20T3-Agent%20consistently%0Aachieves%20improvements%20on%20two%20popular%20VLMs%3A%20MiniCPM-V-8.5B%20and%20%7BQwen2-VL-7B%7D%2C%0Awhich%20outperforms%20untrained%20VLMs%20by%20%2420%5C%25%24%2C%20showing%20the%20effectiveness%20of%20the%0Aproposed%20data%20synthesis%20pipeline%2C%20leading%20to%20high-quality%20data%20for%20tool-usage%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15606v2&entry.124074799=Read"},
{"title": "Multibranch Generative Models for Multichannel Imaging with an\n  Application to PET/CT Synergistic Reconstruction", "author": "Noel Jeffrey Pinton and Alexandre Bousse and Catherine Cheze-Le-Rest and Dimitris Visvikis", "abstract": "  This paper presents a novel approach for learned synergistic reconstruction\nof medical images using multibranch generative models. Leveraging variational\nautoencoders (VAEs), our model learns from pairs of images simultaneously,\nenabling effective denoising and reconstruction. Synergistic image\nreconstruction is achieved by incorporating the trained models in a regularizer\nthat evaluates the distance between the images and the model. We demonstrate\nthe efficacy of our approach on both Modified National Institute of Standards\nand Technology (MNIST) and positron emission tomography (PET)/computed\ntomography (CT) datasets, showcasing improved image quality for low-dose\nimaging. Despite challenges such as patch decomposition and model limitations,\nour results underscore the potential of generative models for enhancing medical\nimaging reconstruction.\n", "link": "http://arxiv.org/abs/2404.08748v5", "date": "2025-02-03", "relevancy": 2.1826, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5684}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5411}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multibranch%20Generative%20Models%20for%20Multichannel%20Imaging%20with%20an%0A%20%20Application%20to%20PET/CT%20Synergistic%20Reconstruction&body=Title%3A%20Multibranch%20Generative%20Models%20for%20Multichannel%20Imaging%20with%20an%0A%20%20Application%20to%20PET/CT%20Synergistic%20Reconstruction%0AAuthor%3A%20Noel%20Jeffrey%20Pinton%20and%20Alexandre%20Bousse%20and%20Catherine%20Cheze-Le-Rest%20and%20Dimitris%20Visvikis%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20for%20learned%20synergistic%20reconstruction%0Aof%20medical%20images%20using%20multibranch%20generative%20models.%20Leveraging%20variational%0Aautoencoders%20%28VAEs%29%2C%20our%20model%20learns%20from%20pairs%20of%20images%20simultaneously%2C%0Aenabling%20effective%20denoising%20and%20reconstruction.%20Synergistic%20image%0Areconstruction%20is%20achieved%20by%20incorporating%20the%20trained%20models%20in%20a%20regularizer%0Athat%20evaluates%20the%20distance%20between%20the%20images%20and%20the%20model.%20We%20demonstrate%0Athe%20efficacy%20of%20our%20approach%20on%20both%20Modified%20National%20Institute%20of%20Standards%0Aand%20Technology%20%28MNIST%29%20and%20positron%20emission%20tomography%20%28PET%29/computed%0Atomography%20%28CT%29%20datasets%2C%20showcasing%20improved%20image%20quality%20for%20low-dose%0Aimaging.%20Despite%20challenges%20such%20as%20patch%20decomposition%20and%20model%20limitations%2C%0Aour%20results%20underscore%20the%20potential%20of%20generative%20models%20for%20enhancing%20medical%0Aimaging%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08748v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultibranch%2520Generative%2520Models%2520for%2520Multichannel%2520Imaging%2520with%2520an%250A%2520%2520Application%2520to%2520PET/CT%2520Synergistic%2520Reconstruction%26entry.906535625%3DNoel%2520Jeffrey%2520Pinton%2520and%2520Alexandre%2520Bousse%2520and%2520Catherine%2520Cheze-Le-Rest%2520and%2520Dimitris%2520Visvikis%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520learned%2520synergistic%2520reconstruction%250Aof%2520medical%2520images%2520using%2520multibranch%2520generative%2520models.%2520Leveraging%2520variational%250Aautoencoders%2520%2528VAEs%2529%252C%2520our%2520model%2520learns%2520from%2520pairs%2520of%2520images%2520simultaneously%252C%250Aenabling%2520effective%2520denoising%2520and%2520reconstruction.%2520Synergistic%2520image%250Areconstruction%2520is%2520achieved%2520by%2520incorporating%2520the%2520trained%2520models%2520in%2520a%2520regularizer%250Athat%2520evaluates%2520the%2520distance%2520between%2520the%2520images%2520and%2520the%2520model.%2520We%2520demonstrate%250Athe%2520efficacy%2520of%2520our%2520approach%2520on%2520both%2520Modified%2520National%2520Institute%2520of%2520Standards%250Aand%2520Technology%2520%2528MNIST%2529%2520and%2520positron%2520emission%2520tomography%2520%2528PET%2529/computed%250Atomography%2520%2528CT%2529%2520datasets%252C%2520showcasing%2520improved%2520image%2520quality%2520for%2520low-dose%250Aimaging.%2520Despite%2520challenges%2520such%2520as%2520patch%2520decomposition%2520and%2520model%2520limitations%252C%250Aour%2520results%2520underscore%2520the%2520potential%2520of%2520generative%2520models%2520for%2520enhancing%2520medical%250Aimaging%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08748v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multibranch%20Generative%20Models%20for%20Multichannel%20Imaging%20with%20an%0A%20%20Application%20to%20PET/CT%20Synergistic%20Reconstruction&entry.906535625=Noel%20Jeffrey%20Pinton%20and%20Alexandre%20Bousse%20and%20Catherine%20Cheze-Le-Rest%20and%20Dimitris%20Visvikis&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20for%20learned%20synergistic%20reconstruction%0Aof%20medical%20images%20using%20multibranch%20generative%20models.%20Leveraging%20variational%0Aautoencoders%20%28VAEs%29%2C%20our%20model%20learns%20from%20pairs%20of%20images%20simultaneously%2C%0Aenabling%20effective%20denoising%20and%20reconstruction.%20Synergistic%20image%0Areconstruction%20is%20achieved%20by%20incorporating%20the%20trained%20models%20in%20a%20regularizer%0Athat%20evaluates%20the%20distance%20between%20the%20images%20and%20the%20model.%20We%20demonstrate%0Athe%20efficacy%20of%20our%20approach%20on%20both%20Modified%20National%20Institute%20of%20Standards%0Aand%20Technology%20%28MNIST%29%20and%20positron%20emission%20tomography%20%28PET%29/computed%0Atomography%20%28CT%29%20datasets%2C%20showcasing%20improved%20image%20quality%20for%20low-dose%0Aimaging.%20Despite%20challenges%20such%20as%20patch%20decomposition%20and%20model%20limitations%2C%0Aour%20results%20underscore%20the%20potential%20of%20generative%20models%20for%20enhancing%20medical%0Aimaging%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08748v5&entry.124074799=Read"},
{"title": "DecTrain: Deciding When to Train a Monocular Depth DNN Online", "author": "Zih-Sing Fu and Soumya Sudhakar and Sertac Karaman and Vivienne Sze", "abstract": "  Deep neural networks (DNNs) can deteriorate in accuracy when deployment data\ndiffers from training data. While performing online training at all timesteps\ncan improve accuracy, it is computationally expensive. We propose DecTrain, a\nnew algorithm that decides when to train a monocular depth DNN online using\nself-supervision with low overhead. To make the decision at each timestep,\nDecTrain compares the cost of training with the predicted accuracy gain. We\nevaluate DecTrain on out-of-distribution data, and find DecTrain maintains\naccuracy compared to online training at all timesteps, while training only 44%\nof the time on average. We also compare the recovery of a low inference cost\nDNN using DecTrain and a more generalizable high inference cost DNN on various\nsequences. DecTrain recovers the majority (97%) of the accuracy gain of online\ntraining at all timesteps while reducing computation compared to the high\ninference cost DNN which recovers only 66%. With an even smaller DNN, we\nachieve 89% recovery while reducing computation by 56%. DecTrain enables\nlow-cost online training for a smaller DNN to have competitive accuracy with a\nlarger, more generalizable DNN at a lower overall computational cost.\n", "link": "http://arxiv.org/abs/2410.02980v2", "date": "2025-02-03", "relevancy": 2.1776, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5614}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5387}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DecTrain%3A%20Deciding%20When%20to%20Train%20a%20Monocular%20Depth%20DNN%20Online&body=Title%3A%20DecTrain%3A%20Deciding%20When%20to%20Train%20a%20Monocular%20Depth%20DNN%20Online%0AAuthor%3A%20Zih-Sing%20Fu%20and%20Soumya%20Sudhakar%20and%20Sertac%20Karaman%20and%20Vivienne%20Sze%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20can%20deteriorate%20in%20accuracy%20when%20deployment%20data%0Adiffers%20from%20training%20data.%20While%20performing%20online%20training%20at%20all%20timesteps%0Acan%20improve%20accuracy%2C%20it%20is%20computationally%20expensive.%20We%20propose%20DecTrain%2C%20a%0Anew%20algorithm%20that%20decides%20when%20to%20train%20a%20monocular%20depth%20DNN%20online%20using%0Aself-supervision%20with%20low%20overhead.%20To%20make%20the%20decision%20at%20each%20timestep%2C%0ADecTrain%20compares%20the%20cost%20of%20training%20with%20the%20predicted%20accuracy%20gain.%20We%0Aevaluate%20DecTrain%20on%20out-of-distribution%20data%2C%20and%20find%20DecTrain%20maintains%0Aaccuracy%20compared%20to%20online%20training%20at%20all%20timesteps%2C%20while%20training%20only%2044%25%0Aof%20the%20time%20on%20average.%20We%20also%20compare%20the%20recovery%20of%20a%20low%20inference%20cost%0ADNN%20using%20DecTrain%20and%20a%20more%20generalizable%20high%20inference%20cost%20DNN%20on%20various%0Asequences.%20DecTrain%20recovers%20the%20majority%20%2897%25%29%20of%20the%20accuracy%20gain%20of%20online%0Atraining%20at%20all%20timesteps%20while%20reducing%20computation%20compared%20to%20the%20high%0Ainference%20cost%20DNN%20which%20recovers%20only%2066%25.%20With%20an%20even%20smaller%20DNN%2C%20we%0Aachieve%2089%25%20recovery%20while%20reducing%20computation%20by%2056%25.%20DecTrain%20enables%0Alow-cost%20online%20training%20for%20a%20smaller%20DNN%20to%20have%20competitive%20accuracy%20with%20a%0Alarger%2C%20more%20generalizable%20DNN%20at%20a%20lower%20overall%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecTrain%253A%2520Deciding%2520When%2520to%2520Train%2520a%2520Monocular%2520Depth%2520DNN%2520Online%26entry.906535625%3DZih-Sing%2520Fu%2520and%2520Soumya%2520Sudhakar%2520and%2520Sertac%2520Karaman%2520and%2520Vivienne%2520Sze%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520can%2520deteriorate%2520in%2520accuracy%2520when%2520deployment%2520data%250Adiffers%2520from%2520training%2520data.%2520While%2520performing%2520online%2520training%2520at%2520all%2520timesteps%250Acan%2520improve%2520accuracy%252C%2520it%2520is%2520computationally%2520expensive.%2520We%2520propose%2520DecTrain%252C%2520a%250Anew%2520algorithm%2520that%2520decides%2520when%2520to%2520train%2520a%2520monocular%2520depth%2520DNN%2520online%2520using%250Aself-supervision%2520with%2520low%2520overhead.%2520To%2520make%2520the%2520decision%2520at%2520each%2520timestep%252C%250ADecTrain%2520compares%2520the%2520cost%2520of%2520training%2520with%2520the%2520predicted%2520accuracy%2520gain.%2520We%250Aevaluate%2520DecTrain%2520on%2520out-of-distribution%2520data%252C%2520and%2520find%2520DecTrain%2520maintains%250Aaccuracy%2520compared%2520to%2520online%2520training%2520at%2520all%2520timesteps%252C%2520while%2520training%2520only%252044%2525%250Aof%2520the%2520time%2520on%2520average.%2520We%2520also%2520compare%2520the%2520recovery%2520of%2520a%2520low%2520inference%2520cost%250ADNN%2520using%2520DecTrain%2520and%2520a%2520more%2520generalizable%2520high%2520inference%2520cost%2520DNN%2520on%2520various%250Asequences.%2520DecTrain%2520recovers%2520the%2520majority%2520%252897%2525%2529%2520of%2520the%2520accuracy%2520gain%2520of%2520online%250Atraining%2520at%2520all%2520timesteps%2520while%2520reducing%2520computation%2520compared%2520to%2520the%2520high%250Ainference%2520cost%2520DNN%2520which%2520recovers%2520only%252066%2525.%2520With%2520an%2520even%2520smaller%2520DNN%252C%2520we%250Aachieve%252089%2525%2520recovery%2520while%2520reducing%2520computation%2520by%252056%2525.%2520DecTrain%2520enables%250Alow-cost%2520online%2520training%2520for%2520a%2520smaller%2520DNN%2520to%2520have%2520competitive%2520accuracy%2520with%2520a%250Alarger%252C%2520more%2520generalizable%2520DNN%2520at%2520a%2520lower%2520overall%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DecTrain%3A%20Deciding%20When%20to%20Train%20a%20Monocular%20Depth%20DNN%20Online&entry.906535625=Zih-Sing%20Fu%20and%20Soumya%20Sudhakar%20and%20Sertac%20Karaman%20and%20Vivienne%20Sze&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20can%20deteriorate%20in%20accuracy%20when%20deployment%20data%0Adiffers%20from%20training%20data.%20While%20performing%20online%20training%20at%20all%20timesteps%0Acan%20improve%20accuracy%2C%20it%20is%20computationally%20expensive.%20We%20propose%20DecTrain%2C%20a%0Anew%20algorithm%20that%20decides%20when%20to%20train%20a%20monocular%20depth%20DNN%20online%20using%0Aself-supervision%20with%20low%20overhead.%20To%20make%20the%20decision%20at%20each%20timestep%2C%0ADecTrain%20compares%20the%20cost%20of%20training%20with%20the%20predicted%20accuracy%20gain.%20We%0Aevaluate%20DecTrain%20on%20out-of-distribution%20data%2C%20and%20find%20DecTrain%20maintains%0Aaccuracy%20compared%20to%20online%20training%20at%20all%20timesteps%2C%20while%20training%20only%2044%25%0Aof%20the%20time%20on%20average.%20We%20also%20compare%20the%20recovery%20of%20a%20low%20inference%20cost%0ADNN%20using%20DecTrain%20and%20a%20more%20generalizable%20high%20inference%20cost%20DNN%20on%20various%0Asequences.%20DecTrain%20recovers%20the%20majority%20%2897%25%29%20of%20the%20accuracy%20gain%20of%20online%0Atraining%20at%20all%20timesteps%20while%20reducing%20computation%20compared%20to%20the%20high%0Ainference%20cost%20DNN%20which%20recovers%20only%2066%25.%20With%20an%20even%20smaller%20DNN%2C%20we%0Aachieve%2089%25%20recovery%20while%20reducing%20computation%20by%2056%25.%20DecTrain%20enables%0Alow-cost%20online%20training%20for%20a%20smaller%20DNN%20to%20have%20competitive%20accuracy%20with%20a%0Alarger%2C%20more%20generalizable%20DNN%20at%20a%20lower%20overall%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02980v2&entry.124074799=Read"},
{"title": "Single-neuron deep generative model uncovers underlying physics of\n  neuronal activity in Ca imaging data", "author": "Jordi Abante and Angelo Piga and Berta Ros and Clara F L\u00f3pez-Le\u00f3n and Josep M Canals and Jordi Soriano", "abstract": "  Calcium imaging has become a powerful alternative to electrophysiology for\nstudying neuronal activity, offering spatial resolution and the ability to\nmeasure large populations of neurons in a minimally invasive manner. This\ntechnique has broad applications in neuroscience, neuroengineering, and\nmedicine, enabling researchers to explore the relationship between neuron\nlocation and activity. Recent advancements in deep generative models (DGMs)\nhave facilitated the modeling of neuronal population dynamics, uncovering\nlatent representations that provide insights into behavior prediction and\nneuronal variance. However, these models often rely on spike inference\nalgorithms and primarily focus on population-level dynamics, limiting their\napplicability for single-neuron analyses. To address this gap, we propose a\nnovel framework for single-neuron representation learning using autoregressive\nvariational autoencoders (AVAEs). Our approach embeds individual neurons'\nspatiotemporal signals into a reduced-dimensional space without the need for\nspike inference algorithms. The AVAE excels over traditional linear methods by\ngenerating more informative and discriminative latent representations,\nimproving tasks such as visualization, clustering, and the understanding of\nneuronal activity. Additionally, the reconstruction performance of the AVAE\noutperforms the state of the art, demonstrating its ability to accurately\nrecover the original fluorescence signal from the learned representation. Using\nrealistic simulations, we show that our model captures underlying physical\nproperties and connectivity patterns, enabling it to distinguish between\ndifferent firing and connectivity types. These findings position the AVAE as a\nversatile and powerful tool for advancing single-neuron analysis and lays the\ngroundwork for future integration of multimodal single-cell datasets in\nneuroscience.\n", "link": "http://arxiv.org/abs/2501.14615v2", "date": "2025-02-03", "relevancy": 2.0933, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5383}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5261}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-neuron%20deep%20generative%20model%20uncovers%20underlying%20physics%20of%0A%20%20neuronal%20activity%20in%20Ca%20imaging%20data&body=Title%3A%20Single-neuron%20deep%20generative%20model%20uncovers%20underlying%20physics%20of%0A%20%20neuronal%20activity%20in%20Ca%20imaging%20data%0AAuthor%3A%20Jordi%20Abante%20and%20Angelo%20Piga%20and%20Berta%20Ros%20and%20Clara%20F%20L%C3%B3pez-Le%C3%B3n%20and%20Josep%20M%20Canals%20and%20Jordi%20Soriano%0AAbstract%3A%20%20%20Calcium%20imaging%20has%20become%20a%20powerful%20alternative%20to%20electrophysiology%20for%0Astudying%20neuronal%20activity%2C%20offering%20spatial%20resolution%20and%20the%20ability%20to%0Ameasure%20large%20populations%20of%20neurons%20in%20a%20minimally%20invasive%20manner.%20This%0Atechnique%20has%20broad%20applications%20in%20neuroscience%2C%20neuroengineering%2C%20and%0Amedicine%2C%20enabling%20researchers%20to%20explore%20the%20relationship%20between%20neuron%0Alocation%20and%20activity.%20Recent%20advancements%20in%20deep%20generative%20models%20%28DGMs%29%0Ahave%20facilitated%20the%20modeling%20of%20neuronal%20population%20dynamics%2C%20uncovering%0Alatent%20representations%20that%20provide%20insights%20into%20behavior%20prediction%20and%0Aneuronal%20variance.%20However%2C%20these%20models%20often%20rely%20on%20spike%20inference%0Aalgorithms%20and%20primarily%20focus%20on%20population-level%20dynamics%2C%20limiting%20their%0Aapplicability%20for%20single-neuron%20analyses.%20To%20address%20this%20gap%2C%20we%20propose%20a%0Anovel%20framework%20for%20single-neuron%20representation%20learning%20using%20autoregressive%0Avariational%20autoencoders%20%28AVAEs%29.%20Our%20approach%20embeds%20individual%20neurons%27%0Aspatiotemporal%20signals%20into%20a%20reduced-dimensional%20space%20without%20the%20need%20for%0Aspike%20inference%20algorithms.%20The%20AVAE%20excels%20over%20traditional%20linear%20methods%20by%0Agenerating%20more%20informative%20and%20discriminative%20latent%20representations%2C%0Aimproving%20tasks%20such%20as%20visualization%2C%20clustering%2C%20and%20the%20understanding%20of%0Aneuronal%20activity.%20Additionally%2C%20the%20reconstruction%20performance%20of%20the%20AVAE%0Aoutperforms%20the%20state%20of%20the%20art%2C%20demonstrating%20its%20ability%20to%20accurately%0Arecover%20the%20original%20fluorescence%20signal%20from%20the%20learned%20representation.%20Using%0Arealistic%20simulations%2C%20we%20show%20that%20our%20model%20captures%20underlying%20physical%0Aproperties%20and%20connectivity%20patterns%2C%20enabling%20it%20to%20distinguish%20between%0Adifferent%20firing%20and%20connectivity%20types.%20These%20findings%20position%20the%20AVAE%20as%20a%0Aversatile%20and%20powerful%20tool%20for%20advancing%20single-neuron%20analysis%20and%20lays%20the%0Agroundwork%20for%20future%20integration%20of%20multimodal%20single-cell%20datasets%20in%0Aneuroscience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14615v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-neuron%2520deep%2520generative%2520model%2520uncovers%2520underlying%2520physics%2520of%250A%2520%2520neuronal%2520activity%2520in%2520Ca%2520imaging%2520data%26entry.906535625%3DJordi%2520Abante%2520and%2520Angelo%2520Piga%2520and%2520Berta%2520Ros%2520and%2520Clara%2520F%2520L%25C3%25B3pez-Le%25C3%25B3n%2520and%2520Josep%2520M%2520Canals%2520and%2520Jordi%2520Soriano%26entry.1292438233%3D%2520%2520Calcium%2520imaging%2520has%2520become%2520a%2520powerful%2520alternative%2520to%2520electrophysiology%2520for%250Astudying%2520neuronal%2520activity%252C%2520offering%2520spatial%2520resolution%2520and%2520the%2520ability%2520to%250Ameasure%2520large%2520populations%2520of%2520neurons%2520in%2520a%2520minimally%2520invasive%2520manner.%2520This%250Atechnique%2520has%2520broad%2520applications%2520in%2520neuroscience%252C%2520neuroengineering%252C%2520and%250Amedicine%252C%2520enabling%2520researchers%2520to%2520explore%2520the%2520relationship%2520between%2520neuron%250Alocation%2520and%2520activity.%2520Recent%2520advancements%2520in%2520deep%2520generative%2520models%2520%2528DGMs%2529%250Ahave%2520facilitated%2520the%2520modeling%2520of%2520neuronal%2520population%2520dynamics%252C%2520uncovering%250Alatent%2520representations%2520that%2520provide%2520insights%2520into%2520behavior%2520prediction%2520and%250Aneuronal%2520variance.%2520However%252C%2520these%2520models%2520often%2520rely%2520on%2520spike%2520inference%250Aalgorithms%2520and%2520primarily%2520focus%2520on%2520population-level%2520dynamics%252C%2520limiting%2520their%250Aapplicability%2520for%2520single-neuron%2520analyses.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%250Anovel%2520framework%2520for%2520single-neuron%2520representation%2520learning%2520using%2520autoregressive%250Avariational%2520autoencoders%2520%2528AVAEs%2529.%2520Our%2520approach%2520embeds%2520individual%2520neurons%2527%250Aspatiotemporal%2520signals%2520into%2520a%2520reduced-dimensional%2520space%2520without%2520the%2520need%2520for%250Aspike%2520inference%2520algorithms.%2520The%2520AVAE%2520excels%2520over%2520traditional%2520linear%2520methods%2520by%250Agenerating%2520more%2520informative%2520and%2520discriminative%2520latent%2520representations%252C%250Aimproving%2520tasks%2520such%2520as%2520visualization%252C%2520clustering%252C%2520and%2520the%2520understanding%2520of%250Aneuronal%2520activity.%2520Additionally%252C%2520the%2520reconstruction%2520performance%2520of%2520the%2520AVAE%250Aoutperforms%2520the%2520state%2520of%2520the%2520art%252C%2520demonstrating%2520its%2520ability%2520to%2520accurately%250Arecover%2520the%2520original%2520fluorescence%2520signal%2520from%2520the%2520learned%2520representation.%2520Using%250Arealistic%2520simulations%252C%2520we%2520show%2520that%2520our%2520model%2520captures%2520underlying%2520physical%250Aproperties%2520and%2520connectivity%2520patterns%252C%2520enabling%2520it%2520to%2520distinguish%2520between%250Adifferent%2520firing%2520and%2520connectivity%2520types.%2520These%2520findings%2520position%2520the%2520AVAE%2520as%2520a%250Aversatile%2520and%2520powerful%2520tool%2520for%2520advancing%2520single-neuron%2520analysis%2520and%2520lays%2520the%250Agroundwork%2520for%2520future%2520integration%2520of%2520multimodal%2520single-cell%2520datasets%2520in%250Aneuroscience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14615v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-neuron%20deep%20generative%20model%20uncovers%20underlying%20physics%20of%0A%20%20neuronal%20activity%20in%20Ca%20imaging%20data&entry.906535625=Jordi%20Abante%20and%20Angelo%20Piga%20and%20Berta%20Ros%20and%20Clara%20F%20L%C3%B3pez-Le%C3%B3n%20and%20Josep%20M%20Canals%20and%20Jordi%20Soriano&entry.1292438233=%20%20Calcium%20imaging%20has%20become%20a%20powerful%20alternative%20to%20electrophysiology%20for%0Astudying%20neuronal%20activity%2C%20offering%20spatial%20resolution%20and%20the%20ability%20to%0Ameasure%20large%20populations%20of%20neurons%20in%20a%20minimally%20invasive%20manner.%20This%0Atechnique%20has%20broad%20applications%20in%20neuroscience%2C%20neuroengineering%2C%20and%0Amedicine%2C%20enabling%20researchers%20to%20explore%20the%20relationship%20between%20neuron%0Alocation%20and%20activity.%20Recent%20advancements%20in%20deep%20generative%20models%20%28DGMs%29%0Ahave%20facilitated%20the%20modeling%20of%20neuronal%20population%20dynamics%2C%20uncovering%0Alatent%20representations%20that%20provide%20insights%20into%20behavior%20prediction%20and%0Aneuronal%20variance.%20However%2C%20these%20models%20often%20rely%20on%20spike%20inference%0Aalgorithms%20and%20primarily%20focus%20on%20population-level%20dynamics%2C%20limiting%20their%0Aapplicability%20for%20single-neuron%20analyses.%20To%20address%20this%20gap%2C%20we%20propose%20a%0Anovel%20framework%20for%20single-neuron%20representation%20learning%20using%20autoregressive%0Avariational%20autoencoders%20%28AVAEs%29.%20Our%20approach%20embeds%20individual%20neurons%27%0Aspatiotemporal%20signals%20into%20a%20reduced-dimensional%20space%20without%20the%20need%20for%0Aspike%20inference%20algorithms.%20The%20AVAE%20excels%20over%20traditional%20linear%20methods%20by%0Agenerating%20more%20informative%20and%20discriminative%20latent%20representations%2C%0Aimproving%20tasks%20such%20as%20visualization%2C%20clustering%2C%20and%20the%20understanding%20of%0Aneuronal%20activity.%20Additionally%2C%20the%20reconstruction%20performance%20of%20the%20AVAE%0Aoutperforms%20the%20state%20of%20the%20art%2C%20demonstrating%20its%20ability%20to%20accurately%0Arecover%20the%20original%20fluorescence%20signal%20from%20the%20learned%20representation.%20Using%0Arealistic%20simulations%2C%20we%20show%20that%20our%20model%20captures%20underlying%20physical%0Aproperties%20and%20connectivity%20patterns%2C%20enabling%20it%20to%20distinguish%20between%0Adifferent%20firing%20and%20connectivity%20types.%20These%20findings%20position%20the%20AVAE%20as%20a%0Aversatile%20and%20powerful%20tool%20for%20advancing%20single-neuron%20analysis%20and%20lays%20the%0Agroundwork%20for%20future%20integration%20of%20multimodal%20single-cell%20datasets%20in%0Aneuroscience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14615v2&entry.124074799=Read"},
{"title": "DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on\n  Continuous-Time Dynamic Graphs with State Space Models", "author": "Zifeng Ding and Yifeng Li and Yuan He and Antonio Norelli and Jingcheng Wu and Volker Tresp and Yunpu Ma and Michael Bronstein", "abstract": "  Learning useful representations for continuous-time dynamic graphs (CTDGs) is\nchallenging, due to the concurrent need to span long node interaction histories\nand grasp nuanced temporal details. In particular, two problems emerge: (1)\nEncoding longer histories requires more computational resources, making it\ncrucial for CTDG models to maintain low computational complexity to ensure\nefficiency; (2) Meanwhile, more powerful models are needed to identify and\nselect the most critical temporal information within the extended context\nprovided by longer histories. To address these problems, we propose a CTDG\nrepresentation learning model named DyGMamba, originating from the popular\nMamba state space model (SSM). DyGMamba first leverages a node-level SSM to\nencode the sequence of historical node interactions. Another time-level SSM is\nthen employed to exploit the temporal patterns hidden in the historical graph,\nwhere its output is used to dynamically select the critical information from\nthe interaction history. We validate DyGMamba experimentally on the dynamic\nlink prediction task. The results show that our model achieves state-of-the-art\nin most cases. DyGMamba also maintains high efficiency in terms of\ncomputational resources, making it possible to capture long temporal\ndependencies with a limited computation budget.\n", "link": "http://arxiv.org/abs/2408.04713v3", "date": "2025-02-03", "relevancy": 2.0813, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5386}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5172}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyGMamba%3A%20Efficiently%20Modeling%20Long-Term%20Temporal%20Dependency%20on%0A%20%20Continuous-Time%20Dynamic%20Graphs%20with%20State%20Space%20Models&body=Title%3A%20DyGMamba%3A%20Efficiently%20Modeling%20Long-Term%20Temporal%20Dependency%20on%0A%20%20Continuous-Time%20Dynamic%20Graphs%20with%20State%20Space%20Models%0AAuthor%3A%20Zifeng%20Ding%20and%20Yifeng%20Li%20and%20Yuan%20He%20and%20Antonio%20Norelli%20and%20Jingcheng%20Wu%20and%20Volker%20Tresp%20and%20Yunpu%20Ma%20and%20Michael%20Bronstein%0AAbstract%3A%20%20%20Learning%20useful%20representations%20for%20continuous-time%20dynamic%20graphs%20%28CTDGs%29%20is%0Achallenging%2C%20due%20to%20the%20concurrent%20need%20to%20span%20long%20node%20interaction%20histories%0Aand%20grasp%20nuanced%20temporal%20details.%20In%20particular%2C%20two%20problems%20emerge%3A%20%281%29%0AEncoding%20longer%20histories%20requires%20more%20computational%20resources%2C%20making%20it%0Acrucial%20for%20CTDG%20models%20to%20maintain%20low%20computational%20complexity%20to%20ensure%0Aefficiency%3B%20%282%29%20Meanwhile%2C%20more%20powerful%20models%20are%20needed%20to%20identify%20and%0Aselect%20the%20most%20critical%20temporal%20information%20within%20the%20extended%20context%0Aprovided%20by%20longer%20histories.%20To%20address%20these%20problems%2C%20we%20propose%20a%20CTDG%0Arepresentation%20learning%20model%20named%20DyGMamba%2C%20originating%20from%20the%20popular%0AMamba%20state%20space%20model%20%28SSM%29.%20DyGMamba%20first%20leverages%20a%20node-level%20SSM%20to%0Aencode%20the%20sequence%20of%20historical%20node%20interactions.%20Another%20time-level%20SSM%20is%0Athen%20employed%20to%20exploit%20the%20temporal%20patterns%20hidden%20in%20the%20historical%20graph%2C%0Awhere%20its%20output%20is%20used%20to%20dynamically%20select%20the%20critical%20information%20from%0Athe%20interaction%20history.%20We%20validate%20DyGMamba%20experimentally%20on%20the%20dynamic%0Alink%20prediction%20task.%20The%20results%20show%20that%20our%20model%20achieves%20state-of-the-art%0Ain%20most%20cases.%20DyGMamba%20also%20maintains%20high%20efficiency%20in%20terms%20of%0Acomputational%20resources%2C%20making%20it%20possible%20to%20capture%20long%20temporal%0Adependencies%20with%20a%20limited%20computation%20budget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04713v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyGMamba%253A%2520Efficiently%2520Modeling%2520Long-Term%2520Temporal%2520Dependency%2520on%250A%2520%2520Continuous-Time%2520Dynamic%2520Graphs%2520with%2520State%2520Space%2520Models%26entry.906535625%3DZifeng%2520Ding%2520and%2520Yifeng%2520Li%2520and%2520Yuan%2520He%2520and%2520Antonio%2520Norelli%2520and%2520Jingcheng%2520Wu%2520and%2520Volker%2520Tresp%2520and%2520Yunpu%2520Ma%2520and%2520Michael%2520Bronstein%26entry.1292438233%3D%2520%2520Learning%2520useful%2520representations%2520for%2520continuous-time%2520dynamic%2520graphs%2520%2528CTDGs%2529%2520is%250Achallenging%252C%2520due%2520to%2520the%2520concurrent%2520need%2520to%2520span%2520long%2520node%2520interaction%2520histories%250Aand%2520grasp%2520nuanced%2520temporal%2520details.%2520In%2520particular%252C%2520two%2520problems%2520emerge%253A%2520%25281%2529%250AEncoding%2520longer%2520histories%2520requires%2520more%2520computational%2520resources%252C%2520making%2520it%250Acrucial%2520for%2520CTDG%2520models%2520to%2520maintain%2520low%2520computational%2520complexity%2520to%2520ensure%250Aefficiency%253B%2520%25282%2529%2520Meanwhile%252C%2520more%2520powerful%2520models%2520are%2520needed%2520to%2520identify%2520and%250Aselect%2520the%2520most%2520critical%2520temporal%2520information%2520within%2520the%2520extended%2520context%250Aprovided%2520by%2520longer%2520histories.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520CTDG%250Arepresentation%2520learning%2520model%2520named%2520DyGMamba%252C%2520originating%2520from%2520the%2520popular%250AMamba%2520state%2520space%2520model%2520%2528SSM%2529.%2520DyGMamba%2520first%2520leverages%2520a%2520node-level%2520SSM%2520to%250Aencode%2520the%2520sequence%2520of%2520historical%2520node%2520interactions.%2520Another%2520time-level%2520SSM%2520is%250Athen%2520employed%2520to%2520exploit%2520the%2520temporal%2520patterns%2520hidden%2520in%2520the%2520historical%2520graph%252C%250Awhere%2520its%2520output%2520is%2520used%2520to%2520dynamically%2520select%2520the%2520critical%2520information%2520from%250Athe%2520interaction%2520history.%2520We%2520validate%2520DyGMamba%2520experimentally%2520on%2520the%2520dynamic%250Alink%2520prediction%2520task.%2520The%2520results%2520show%2520that%2520our%2520model%2520achieves%2520state-of-the-art%250Ain%2520most%2520cases.%2520DyGMamba%2520also%2520maintains%2520high%2520efficiency%2520in%2520terms%2520of%250Acomputational%2520resources%252C%2520making%2520it%2520possible%2520to%2520capture%2520long%2520temporal%250Adependencies%2520with%2520a%2520limited%2520computation%2520budget.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04713v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyGMamba%3A%20Efficiently%20Modeling%20Long-Term%20Temporal%20Dependency%20on%0A%20%20Continuous-Time%20Dynamic%20Graphs%20with%20State%20Space%20Models&entry.906535625=Zifeng%20Ding%20and%20Yifeng%20Li%20and%20Yuan%20He%20and%20Antonio%20Norelli%20and%20Jingcheng%20Wu%20and%20Volker%20Tresp%20and%20Yunpu%20Ma%20and%20Michael%20Bronstein&entry.1292438233=%20%20Learning%20useful%20representations%20for%20continuous-time%20dynamic%20graphs%20%28CTDGs%29%20is%0Achallenging%2C%20due%20to%20the%20concurrent%20need%20to%20span%20long%20node%20interaction%20histories%0Aand%20grasp%20nuanced%20temporal%20details.%20In%20particular%2C%20two%20problems%20emerge%3A%20%281%29%0AEncoding%20longer%20histories%20requires%20more%20computational%20resources%2C%20making%20it%0Acrucial%20for%20CTDG%20models%20to%20maintain%20low%20computational%20complexity%20to%20ensure%0Aefficiency%3B%20%282%29%20Meanwhile%2C%20more%20powerful%20models%20are%20needed%20to%20identify%20and%0Aselect%20the%20most%20critical%20temporal%20information%20within%20the%20extended%20context%0Aprovided%20by%20longer%20histories.%20To%20address%20these%20problems%2C%20we%20propose%20a%20CTDG%0Arepresentation%20learning%20model%20named%20DyGMamba%2C%20originating%20from%20the%20popular%0AMamba%20state%20space%20model%20%28SSM%29.%20DyGMamba%20first%20leverages%20a%20node-level%20SSM%20to%0Aencode%20the%20sequence%20of%20historical%20node%20interactions.%20Another%20time-level%20SSM%20is%0Athen%20employed%20to%20exploit%20the%20temporal%20patterns%20hidden%20in%20the%20historical%20graph%2C%0Awhere%20its%20output%20is%20used%20to%20dynamically%20select%20the%20critical%20information%20from%0Athe%20interaction%20history.%20We%20validate%20DyGMamba%20experimentally%20on%20the%20dynamic%0Alink%20prediction%20task.%20The%20results%20show%20that%20our%20model%20achieves%20state-of-the-art%0Ain%20most%20cases.%20DyGMamba%20also%20maintains%20high%20efficiency%20in%20terms%20of%0Acomputational%20resources%2C%20making%20it%20possible%20to%20capture%20long%20temporal%0Adependencies%20with%20a%20limited%20computation%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04713v3&entry.124074799=Read"},
{"title": "Learning Transactions Representations for Information Management in\n  Banks: Mastering Local, Global, and External Knowledge", "author": "Alexandra Bazarova and Maria Kovaleva and Ilya Kuleshov and Evgenia Romanenkova and Alexander Stepikin and Alexandr Yugay and Dzhambulat Mollaev and Ivan Kireev and Andrey Savchenko and Alexey Zaytsev", "abstract": "  In today's world, banks use artificial intelligence to optimize diverse\nbusiness processes, aiming to improve customer experience. Most of the\ncustomer-related tasks can be categorized into two groups: 1) local ones, which\nfocus on a client's current state, such as transaction forecasting, and 2)\nglobal ones, which consider the general customer behaviour, e.g., predicting\nsuccessful loan repayment. Unfortunately, maintaining separate models for each\ntask is costly. Therefore, to better facilitate information management, we\ncompared eight state-of-the-art unsupervised methods on 11 tasks in search for\na one-size-fits-all solution. Contrastive self-supervised learning methods were\ndemonstrated to excel at global problems, while generative techniques were\nsuperior at local tasks. We also introduced a novel approach, which enriches\nthe client's representation by incorporating external information gathered from\nother clients. Our method outperforms classical models, boosting accuracy by up\nto 20\\%.\n", "link": "http://arxiv.org/abs/2404.02047v3", "date": "2025-02-03", "relevancy": 2.0536, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5378}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4963}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Transactions%20Representations%20for%20Information%20Management%20in%0A%20%20Banks%3A%20Mastering%20Local%2C%20Global%2C%20and%20External%20Knowledge&body=Title%3A%20Learning%20Transactions%20Representations%20for%20Information%20Management%20in%0A%20%20Banks%3A%20Mastering%20Local%2C%20Global%2C%20and%20External%20Knowledge%0AAuthor%3A%20Alexandra%20Bazarova%20and%20Maria%20Kovaleva%20and%20Ilya%20Kuleshov%20and%20Evgenia%20Romanenkova%20and%20Alexander%20Stepikin%20and%20Alexandr%20Yugay%20and%20Dzhambulat%20Mollaev%20and%20Ivan%20Kireev%20and%20Andrey%20Savchenko%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20In%20today%27s%20world%2C%20banks%20use%20artificial%20intelligence%20to%20optimize%20diverse%0Abusiness%20processes%2C%20aiming%20to%20improve%20customer%20experience.%20Most%20of%20the%0Acustomer-related%20tasks%20can%20be%20categorized%20into%20two%20groups%3A%201%29%20local%20ones%2C%20which%0Afocus%20on%20a%20client%27s%20current%20state%2C%20such%20as%20transaction%20forecasting%2C%20and%202%29%0Aglobal%20ones%2C%20which%20consider%20the%20general%20customer%20behaviour%2C%20e.g.%2C%20predicting%0Asuccessful%20loan%20repayment.%20Unfortunately%2C%20maintaining%20separate%20models%20for%20each%0Atask%20is%20costly.%20Therefore%2C%20to%20better%20facilitate%20information%20management%2C%20we%0Acompared%20eight%20state-of-the-art%20unsupervised%20methods%20on%2011%20tasks%20in%20search%20for%0Aa%20one-size-fits-all%20solution.%20Contrastive%20self-supervised%20learning%20methods%20were%0Ademonstrated%20to%20excel%20at%20global%20problems%2C%20while%20generative%20techniques%20were%0Asuperior%20at%20local%20tasks.%20We%20also%20introduced%20a%20novel%20approach%2C%20which%20enriches%0Athe%20client%27s%20representation%20by%20incorporating%20external%20information%20gathered%20from%0Aother%20clients.%20Our%20method%20outperforms%20classical%20models%2C%20boosting%20accuracy%20by%20up%0Ato%2020%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02047v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Transactions%2520Representations%2520for%2520Information%2520Management%2520in%250A%2520%2520Banks%253A%2520Mastering%2520Local%252C%2520Global%252C%2520and%2520External%2520Knowledge%26entry.906535625%3DAlexandra%2520Bazarova%2520and%2520Maria%2520Kovaleva%2520and%2520Ilya%2520Kuleshov%2520and%2520Evgenia%2520Romanenkova%2520and%2520Alexander%2520Stepikin%2520and%2520Alexandr%2520Yugay%2520and%2520Dzhambulat%2520Mollaev%2520and%2520Ivan%2520Kireev%2520and%2520Andrey%2520Savchenko%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520In%2520today%2527s%2520world%252C%2520banks%2520use%2520artificial%2520intelligence%2520to%2520optimize%2520diverse%250Abusiness%2520processes%252C%2520aiming%2520to%2520improve%2520customer%2520experience.%2520Most%2520of%2520the%250Acustomer-related%2520tasks%2520can%2520be%2520categorized%2520into%2520two%2520groups%253A%25201%2529%2520local%2520ones%252C%2520which%250Afocus%2520on%2520a%2520client%2527s%2520current%2520state%252C%2520such%2520as%2520transaction%2520forecasting%252C%2520and%25202%2529%250Aglobal%2520ones%252C%2520which%2520consider%2520the%2520general%2520customer%2520behaviour%252C%2520e.g.%252C%2520predicting%250Asuccessful%2520loan%2520repayment.%2520Unfortunately%252C%2520maintaining%2520separate%2520models%2520for%2520each%250Atask%2520is%2520costly.%2520Therefore%252C%2520to%2520better%2520facilitate%2520information%2520management%252C%2520we%250Acompared%2520eight%2520state-of-the-art%2520unsupervised%2520methods%2520on%252011%2520tasks%2520in%2520search%2520for%250Aa%2520one-size-fits-all%2520solution.%2520Contrastive%2520self-supervised%2520learning%2520methods%2520were%250Ademonstrated%2520to%2520excel%2520at%2520global%2520problems%252C%2520while%2520generative%2520techniques%2520were%250Asuperior%2520at%2520local%2520tasks.%2520We%2520also%2520introduced%2520a%2520novel%2520approach%252C%2520which%2520enriches%250Athe%2520client%2527s%2520representation%2520by%2520incorporating%2520external%2520information%2520gathered%2520from%250Aother%2520clients.%2520Our%2520method%2520outperforms%2520classical%2520models%252C%2520boosting%2520accuracy%2520by%2520up%250Ato%252020%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02047v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Transactions%20Representations%20for%20Information%20Management%20in%0A%20%20Banks%3A%20Mastering%20Local%2C%20Global%2C%20and%20External%20Knowledge&entry.906535625=Alexandra%20Bazarova%20and%20Maria%20Kovaleva%20and%20Ilya%20Kuleshov%20and%20Evgenia%20Romanenkova%20and%20Alexander%20Stepikin%20and%20Alexandr%20Yugay%20and%20Dzhambulat%20Mollaev%20and%20Ivan%20Kireev%20and%20Andrey%20Savchenko%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20In%20today%27s%20world%2C%20banks%20use%20artificial%20intelligence%20to%20optimize%20diverse%0Abusiness%20processes%2C%20aiming%20to%20improve%20customer%20experience.%20Most%20of%20the%0Acustomer-related%20tasks%20can%20be%20categorized%20into%20two%20groups%3A%201%29%20local%20ones%2C%20which%0Afocus%20on%20a%20client%27s%20current%20state%2C%20such%20as%20transaction%20forecasting%2C%20and%202%29%0Aglobal%20ones%2C%20which%20consider%20the%20general%20customer%20behaviour%2C%20e.g.%2C%20predicting%0Asuccessful%20loan%20repayment.%20Unfortunately%2C%20maintaining%20separate%20models%20for%20each%0Atask%20is%20costly.%20Therefore%2C%20to%20better%20facilitate%20information%20management%2C%20we%0Acompared%20eight%20state-of-the-art%20unsupervised%20methods%20on%2011%20tasks%20in%20search%20for%0Aa%20one-size-fits-all%20solution.%20Contrastive%20self-supervised%20learning%20methods%20were%0Ademonstrated%20to%20excel%20at%20global%20problems%2C%20while%20generative%20techniques%20were%0Asuperior%20at%20local%20tasks.%20We%20also%20introduced%20a%20novel%20approach%2C%20which%20enriches%0Athe%20client%27s%20representation%20by%20incorporating%20external%20information%20gathered%20from%0Aother%20clients.%20Our%20method%20outperforms%20classical%20models%2C%20boosting%20accuracy%20by%20up%0Ato%2020%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02047v3&entry.124074799=Read"},
{"title": "RILe: Reinforced Imitation Learning", "author": "Mert Albaba and Sammy Christen and Thomas Langarek and Christoph Gebhardt and Otmar Hilliges and Michael J. Black", "abstract": "  Acquiring complex behaviors is essential for artificially intelligent agents,\nyet learning these behaviors in high-dimensional settings poses a significant\nchallenge due to the vast search space. Traditional reinforcement learning (RL)\nrequires extensive manual effort for reward function engineering. Inverse\nreinforcement learning (IRL) uncovers reward functions from expert\ndemonstrations but relies on an iterative process that is often computationally\nexpensive. Imitation learning (IL) provides a more efficient alternative by\ndirectly comparing an agent's actions to expert demonstrations; however, in\nhigh-dimensional environments, such direct comparisons offer insufficient\nfeedback for effective learning. We introduce RILe (Reinforced Imitation\nLearning), a framework that combines the strengths of imitation learning and\ninverse reinforcement learning to learn a dense reward function efficiently and\nachieve strong performance in high-dimensional tasks. RILe employs a novel\ntrainer-student framework: the trainer learns an adaptive reward function, and\nthe student uses this reward signal to imitate expert behaviors. By dynamically\nadjusting its guidance as the student evolves, the trainer provides nuanced\nfeedback across different phases of learning. Our framework produces\nhigh-performing policies in high-dimensional tasks where direct imitation fails\nto replicate complex behaviors. We validate RILe in challenging robotic\nlocomotion tasks, demonstrating that it significantly outperforms existing\nmethods and achieves near-expert performance across multiple settings.\n", "link": "http://arxiv.org/abs/2406.08472v3", "date": "2025-02-03", "relevancy": 2.0453, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5169}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5092}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RILe%3A%20Reinforced%20Imitation%20Learning&body=Title%3A%20RILe%3A%20Reinforced%20Imitation%20Learning%0AAuthor%3A%20Mert%20Albaba%20and%20Sammy%20Christen%20and%20Thomas%20Langarek%20and%20Christoph%20Gebhardt%20and%20Otmar%20Hilliges%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20Acquiring%20complex%20behaviors%20is%20essential%20for%20artificially%20intelligent%20agents%2C%0Ayet%20learning%20these%20behaviors%20in%20high-dimensional%20settings%20poses%20a%20significant%0Achallenge%20due%20to%20the%20vast%20search%20space.%20Traditional%20reinforcement%20learning%20%28RL%29%0Arequires%20extensive%20manual%20effort%20for%20reward%20function%20engineering.%20Inverse%0Areinforcement%20learning%20%28IRL%29%20uncovers%20reward%20functions%20from%20expert%0Ademonstrations%20but%20relies%20on%20an%20iterative%20process%20that%20is%20often%20computationally%0Aexpensive.%20Imitation%20learning%20%28IL%29%20provides%20a%20more%20efficient%20alternative%20by%0Adirectly%20comparing%20an%20agent%27s%20actions%20to%20expert%20demonstrations%3B%20however%2C%20in%0Ahigh-dimensional%20environments%2C%20such%20direct%20comparisons%20offer%20insufficient%0Afeedback%20for%20effective%20learning.%20We%20introduce%20RILe%20%28Reinforced%20Imitation%0ALearning%29%2C%20a%20framework%20that%20combines%20the%20strengths%20of%20imitation%20learning%20and%0Ainverse%20reinforcement%20learning%20to%20learn%20a%20dense%20reward%20function%20efficiently%20and%0Aachieve%20strong%20performance%20in%20high-dimensional%20tasks.%20RILe%20employs%20a%20novel%0Atrainer-student%20framework%3A%20the%20trainer%20learns%20an%20adaptive%20reward%20function%2C%20and%0Athe%20student%20uses%20this%20reward%20signal%20to%20imitate%20expert%20behaviors.%20By%20dynamically%0Aadjusting%20its%20guidance%20as%20the%20student%20evolves%2C%20the%20trainer%20provides%20nuanced%0Afeedback%20across%20different%20phases%20of%20learning.%20Our%20framework%20produces%0Ahigh-performing%20policies%20in%20high-dimensional%20tasks%20where%20direct%20imitation%20fails%0Ato%20replicate%20complex%20behaviors.%20We%20validate%20RILe%20in%20challenging%20robotic%0Alocomotion%20tasks%2C%20demonstrating%20that%20it%20significantly%20outperforms%20existing%0Amethods%20and%20achieves%20near-expert%20performance%20across%20multiple%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08472v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRILe%253A%2520Reinforced%2520Imitation%2520Learning%26entry.906535625%3DMert%2520Albaba%2520and%2520Sammy%2520Christen%2520and%2520Thomas%2520Langarek%2520and%2520Christoph%2520Gebhardt%2520and%2520Otmar%2520Hilliges%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520Acquiring%2520complex%2520behaviors%2520is%2520essential%2520for%2520artificially%2520intelligent%2520agents%252C%250Ayet%2520learning%2520these%2520behaviors%2520in%2520high-dimensional%2520settings%2520poses%2520a%2520significant%250Achallenge%2520due%2520to%2520the%2520vast%2520search%2520space.%2520Traditional%2520reinforcement%2520learning%2520%2528RL%2529%250Arequires%2520extensive%2520manual%2520effort%2520for%2520reward%2520function%2520engineering.%2520Inverse%250Areinforcement%2520learning%2520%2528IRL%2529%2520uncovers%2520reward%2520functions%2520from%2520expert%250Ademonstrations%2520but%2520relies%2520on%2520an%2520iterative%2520process%2520that%2520is%2520often%2520computationally%250Aexpensive.%2520Imitation%2520learning%2520%2528IL%2529%2520provides%2520a%2520more%2520efficient%2520alternative%2520by%250Adirectly%2520comparing%2520an%2520agent%2527s%2520actions%2520to%2520expert%2520demonstrations%253B%2520however%252C%2520in%250Ahigh-dimensional%2520environments%252C%2520such%2520direct%2520comparisons%2520offer%2520insufficient%250Afeedback%2520for%2520effective%2520learning.%2520We%2520introduce%2520RILe%2520%2528Reinforced%2520Imitation%250ALearning%2529%252C%2520a%2520framework%2520that%2520combines%2520the%2520strengths%2520of%2520imitation%2520learning%2520and%250Ainverse%2520reinforcement%2520learning%2520to%2520learn%2520a%2520dense%2520reward%2520function%2520efficiently%2520and%250Aachieve%2520strong%2520performance%2520in%2520high-dimensional%2520tasks.%2520RILe%2520employs%2520a%2520novel%250Atrainer-student%2520framework%253A%2520the%2520trainer%2520learns%2520an%2520adaptive%2520reward%2520function%252C%2520and%250Athe%2520student%2520uses%2520this%2520reward%2520signal%2520to%2520imitate%2520expert%2520behaviors.%2520By%2520dynamically%250Aadjusting%2520its%2520guidance%2520as%2520the%2520student%2520evolves%252C%2520the%2520trainer%2520provides%2520nuanced%250Afeedback%2520across%2520different%2520phases%2520of%2520learning.%2520Our%2520framework%2520produces%250Ahigh-performing%2520policies%2520in%2520high-dimensional%2520tasks%2520where%2520direct%2520imitation%2520fails%250Ato%2520replicate%2520complex%2520behaviors.%2520We%2520validate%2520RILe%2520in%2520challenging%2520robotic%250Alocomotion%2520tasks%252C%2520demonstrating%2520that%2520it%2520significantly%2520outperforms%2520existing%250Amethods%2520and%2520achieves%2520near-expert%2520performance%2520across%2520multiple%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08472v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RILe%3A%20Reinforced%20Imitation%20Learning&entry.906535625=Mert%20Albaba%20and%20Sammy%20Christen%20and%20Thomas%20Langarek%20and%20Christoph%20Gebhardt%20and%20Otmar%20Hilliges%20and%20Michael%20J.%20Black&entry.1292438233=%20%20Acquiring%20complex%20behaviors%20is%20essential%20for%20artificially%20intelligent%20agents%2C%0Ayet%20learning%20these%20behaviors%20in%20high-dimensional%20settings%20poses%20a%20significant%0Achallenge%20due%20to%20the%20vast%20search%20space.%20Traditional%20reinforcement%20learning%20%28RL%29%0Arequires%20extensive%20manual%20effort%20for%20reward%20function%20engineering.%20Inverse%0Areinforcement%20learning%20%28IRL%29%20uncovers%20reward%20functions%20from%20expert%0Ademonstrations%20but%20relies%20on%20an%20iterative%20process%20that%20is%20often%20computationally%0Aexpensive.%20Imitation%20learning%20%28IL%29%20provides%20a%20more%20efficient%20alternative%20by%0Adirectly%20comparing%20an%20agent%27s%20actions%20to%20expert%20demonstrations%3B%20however%2C%20in%0Ahigh-dimensional%20environments%2C%20such%20direct%20comparisons%20offer%20insufficient%0Afeedback%20for%20effective%20learning.%20We%20introduce%20RILe%20%28Reinforced%20Imitation%0ALearning%29%2C%20a%20framework%20that%20combines%20the%20strengths%20of%20imitation%20learning%20and%0Ainverse%20reinforcement%20learning%20to%20learn%20a%20dense%20reward%20function%20efficiently%20and%0Aachieve%20strong%20performance%20in%20high-dimensional%20tasks.%20RILe%20employs%20a%20novel%0Atrainer-student%20framework%3A%20the%20trainer%20learns%20an%20adaptive%20reward%20function%2C%20and%0Athe%20student%20uses%20this%20reward%20signal%20to%20imitate%20expert%20behaviors.%20By%20dynamically%0Aadjusting%20its%20guidance%20as%20the%20student%20evolves%2C%20the%20trainer%20provides%20nuanced%0Afeedback%20across%20different%20phases%20of%20learning.%20Our%20framework%20produces%0Ahigh-performing%20policies%20in%20high-dimensional%20tasks%20where%20direct%20imitation%20fails%0Ato%20replicate%20complex%20behaviors.%20We%20validate%20RILe%20in%20challenging%20robotic%0Alocomotion%20tasks%2C%20demonstrating%20that%20it%20significantly%20outperforms%20existing%0Amethods%20and%20achieves%20near-expert%20performance%20across%20multiple%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08472v3&entry.124074799=Read"},
{"title": "Generative AI Models: Opportunities and Risks for Industry and\n  Authorities", "author": "Tobias Alt and Andrea Ibisch and Clemens Meiser and Anna Wilhelm and Raphael Zimmer and Jonas Ditz and Dominique Dresen and Christoph Droste and Jens Karschau and Friederike Laus and Oliver M\u00fcller and Matthias Neu and Rainer Plaga and Carola Plesch and Britta Sennewald and Thomas Thaeren and Kristina Unverricht and Steffen Waurick", "abstract": "  Generative AI models are capable of performing a wide variety of tasks that\nhave traditionally required creativity and human understanding. During\ntraining, they learn patterns from existing data and can subsequently generate\nnew content such as texts, images, audio, and videos that align with these\npatterns. Due to their versatility and generally high-quality results, they\nrepresent, on the one hand, an opportunity for digitalisation. On the other\nhand, the use of generative AI models introduces novel IT security risks that\nmust be considered as part of a comprehensive analysis of the IT security\nthreat landscape. In response to this risk potential, companies or authorities\nintending to use generative AI should conduct an individual risk analysis\nbefore integrating it into their workflows. The same applies to developers and\noperators, as many risks associated with generative AI must be addressed during\ndevelopment or can only be influenced by the operating organisation. Based on\nthis, existing security measures can be adapted, and additional measures\nimplemented.\n", "link": "http://arxiv.org/abs/2406.04734v2", "date": "2025-02-03", "relevancy": 2.0304, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5451}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5203}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20Models%3A%20Opportunities%20and%20Risks%20for%20Industry%20and%0A%20%20Authorities&body=Title%3A%20Generative%20AI%20Models%3A%20Opportunities%20and%20Risks%20for%20Industry%20and%0A%20%20Authorities%0AAuthor%3A%20Tobias%20Alt%20and%20Andrea%20Ibisch%20and%20Clemens%20Meiser%20and%20Anna%20Wilhelm%20and%20Raphael%20Zimmer%20and%20Jonas%20Ditz%20and%20Dominique%20Dresen%20and%20Christoph%20Droste%20and%20Jens%20Karschau%20and%20Friederike%20Laus%20and%20Oliver%20M%C3%BCller%20and%20Matthias%20Neu%20and%20Rainer%20Plaga%20and%20Carola%20Plesch%20and%20Britta%20Sennewald%20and%20Thomas%20Thaeren%20and%20Kristina%20Unverricht%20and%20Steffen%20Waurick%0AAbstract%3A%20%20%20Generative%20AI%20models%20are%20capable%20of%20performing%20a%20wide%20variety%20of%20tasks%20that%0Ahave%20traditionally%20required%20creativity%20and%20human%20understanding.%20During%0Atraining%2C%20they%20learn%20patterns%20from%20existing%20data%20and%20can%20subsequently%20generate%0Anew%20content%20such%20as%20texts%2C%20images%2C%20audio%2C%20and%20videos%20that%20align%20with%20these%0Apatterns.%20Due%20to%20their%20versatility%20and%20generally%20high-quality%20results%2C%20they%0Arepresent%2C%20on%20the%20one%20hand%2C%20an%20opportunity%20for%20digitalisation.%20On%20the%20other%0Ahand%2C%20the%20use%20of%20generative%20AI%20models%20introduces%20novel%20IT%20security%20risks%20that%0Amust%20be%20considered%20as%20part%20of%20a%20comprehensive%20analysis%20of%20the%20IT%20security%0Athreat%20landscape.%20In%20response%20to%20this%20risk%20potential%2C%20companies%20or%20authorities%0Aintending%20to%20use%20generative%20AI%20should%20conduct%20an%20individual%20risk%20analysis%0Abefore%20integrating%20it%20into%20their%20workflows.%20The%20same%20applies%20to%20developers%20and%0Aoperators%2C%20as%20many%20risks%20associated%20with%20generative%20AI%20must%20be%20addressed%20during%0Adevelopment%20or%20can%20only%20be%20influenced%20by%20the%20operating%20organisation.%20Based%20on%0Athis%2C%20existing%20security%20measures%20can%20be%20adapted%2C%20and%20additional%20measures%0Aimplemented.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520Models%253A%2520Opportunities%2520and%2520Risks%2520for%2520Industry%2520and%250A%2520%2520Authorities%26entry.906535625%3DTobias%2520Alt%2520and%2520Andrea%2520Ibisch%2520and%2520Clemens%2520Meiser%2520and%2520Anna%2520Wilhelm%2520and%2520Raphael%2520Zimmer%2520and%2520Jonas%2520Ditz%2520and%2520Dominique%2520Dresen%2520and%2520Christoph%2520Droste%2520and%2520Jens%2520Karschau%2520and%2520Friederike%2520Laus%2520and%2520Oliver%2520M%25C3%25BCller%2520and%2520Matthias%2520Neu%2520and%2520Rainer%2520Plaga%2520and%2520Carola%2520Plesch%2520and%2520Britta%2520Sennewald%2520and%2520Thomas%2520Thaeren%2520and%2520Kristina%2520Unverricht%2520and%2520Steffen%2520Waurick%26entry.1292438233%3D%2520%2520Generative%2520AI%2520models%2520are%2520capable%2520of%2520performing%2520a%2520wide%2520variety%2520of%2520tasks%2520that%250Ahave%2520traditionally%2520required%2520creativity%2520and%2520human%2520understanding.%2520During%250Atraining%252C%2520they%2520learn%2520patterns%2520from%2520existing%2520data%2520and%2520can%2520subsequently%2520generate%250Anew%2520content%2520such%2520as%2520texts%252C%2520images%252C%2520audio%252C%2520and%2520videos%2520that%2520align%2520with%2520these%250Apatterns.%2520Due%2520to%2520their%2520versatility%2520and%2520generally%2520high-quality%2520results%252C%2520they%250Arepresent%252C%2520on%2520the%2520one%2520hand%252C%2520an%2520opportunity%2520for%2520digitalisation.%2520On%2520the%2520other%250Ahand%252C%2520the%2520use%2520of%2520generative%2520AI%2520models%2520introduces%2520novel%2520IT%2520security%2520risks%2520that%250Amust%2520be%2520considered%2520as%2520part%2520of%2520a%2520comprehensive%2520analysis%2520of%2520the%2520IT%2520security%250Athreat%2520landscape.%2520In%2520response%2520to%2520this%2520risk%2520potential%252C%2520companies%2520or%2520authorities%250Aintending%2520to%2520use%2520generative%2520AI%2520should%2520conduct%2520an%2520individual%2520risk%2520analysis%250Abefore%2520integrating%2520it%2520into%2520their%2520workflows.%2520The%2520same%2520applies%2520to%2520developers%2520and%250Aoperators%252C%2520as%2520many%2520risks%2520associated%2520with%2520generative%2520AI%2520must%2520be%2520addressed%2520during%250Adevelopment%2520or%2520can%2520only%2520be%2520influenced%2520by%2520the%2520operating%2520organisation.%2520Based%2520on%250Athis%252C%2520existing%2520security%2520measures%2520can%2520be%2520adapted%252C%2520and%2520additional%2520measures%250Aimplemented.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20Models%3A%20Opportunities%20and%20Risks%20for%20Industry%20and%0A%20%20Authorities&entry.906535625=Tobias%20Alt%20and%20Andrea%20Ibisch%20and%20Clemens%20Meiser%20and%20Anna%20Wilhelm%20and%20Raphael%20Zimmer%20and%20Jonas%20Ditz%20and%20Dominique%20Dresen%20and%20Christoph%20Droste%20and%20Jens%20Karschau%20and%20Friederike%20Laus%20and%20Oliver%20M%C3%BCller%20and%20Matthias%20Neu%20and%20Rainer%20Plaga%20and%20Carola%20Plesch%20and%20Britta%20Sennewald%20and%20Thomas%20Thaeren%20and%20Kristina%20Unverricht%20and%20Steffen%20Waurick&entry.1292438233=%20%20Generative%20AI%20models%20are%20capable%20of%20performing%20a%20wide%20variety%20of%20tasks%20that%0Ahave%20traditionally%20required%20creativity%20and%20human%20understanding.%20During%0Atraining%2C%20they%20learn%20patterns%20from%20existing%20data%20and%20can%20subsequently%20generate%0Anew%20content%20such%20as%20texts%2C%20images%2C%20audio%2C%20and%20videos%20that%20align%20with%20these%0Apatterns.%20Due%20to%20their%20versatility%20and%20generally%20high-quality%20results%2C%20they%0Arepresent%2C%20on%20the%20one%20hand%2C%20an%20opportunity%20for%20digitalisation.%20On%20the%20other%0Ahand%2C%20the%20use%20of%20generative%20AI%20models%20introduces%20novel%20IT%20security%20risks%20that%0Amust%20be%20considered%20as%20part%20of%20a%20comprehensive%20analysis%20of%20the%20IT%20security%0Athreat%20landscape.%20In%20response%20to%20this%20risk%20potential%2C%20companies%20or%20authorities%0Aintending%20to%20use%20generative%20AI%20should%20conduct%20an%20individual%20risk%20analysis%0Abefore%20integrating%20it%20into%20their%20workflows.%20The%20same%20applies%20to%20developers%20and%0Aoperators%2C%20as%20many%20risks%20associated%20with%20generative%20AI%20must%20be%20addressed%20during%0Adevelopment%20or%20can%20only%20be%20influenced%20by%20the%20operating%20organisation.%20Based%20on%0Athis%2C%20existing%20security%20measures%20can%20be%20adapted%2C%20and%20additional%20measures%0Aimplemented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04734v2&entry.124074799=Read"},
{"title": "Learning Time-Varying Multi-Region Communications via Scalable Markovian\n  Gaussian Processes", "author": "Weihan Li and Yule Wang and Chengrui Li and Anqi Wu", "abstract": "  Understanding and constructing brain communications that capture dynamic\ncommunications across multiple regions is fundamental to modern system\nneuroscience, yet current methods struggle to find time-varying region-level\ncommunications or scale to large neural datasets with long recording durations.\nWe present a novel framework using Markovian Gaussian Processes to learn brain\ncommunications with time-varying temporal delays from multi-region neural\nrecordings, named Adaptive Delay Model (ADM). Our method combines Gaussian\nProcesses with State Space Models and employs parallel scan inference\nalgorithms, enabling efficient scaling to large datasets while identifying\nconcurrent communication patterns that evolve over time. This time-varying\napproach captures how brain region interactions shift dynamically during\ncognitive processes. Validated on synthetic and multi-region neural recordings\ndatasets, our approach discovers both the directionality and temporal dynamics\nof neural communication. This work advances our understanding of distributed\nneural computation and provides a scalable tool for analyzing dynamic brain\nnetworks.\n", "link": "http://arxiv.org/abs/2407.00397v2", "date": "2025-02-03", "relevancy": 2.0218, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5051}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Time-Varying%20Multi-Region%20Communications%20via%20Scalable%20Markovian%0A%20%20Gaussian%20Processes&body=Title%3A%20Learning%20Time-Varying%20Multi-Region%20Communications%20via%20Scalable%20Markovian%0A%20%20Gaussian%20Processes%0AAuthor%3A%20Weihan%20Li%20and%20Yule%20Wang%20and%20Chengrui%20Li%20and%20Anqi%20Wu%0AAbstract%3A%20%20%20Understanding%20and%20constructing%20brain%20communications%20that%20capture%20dynamic%0Acommunications%20across%20multiple%20regions%20is%20fundamental%20to%20modern%20system%0Aneuroscience%2C%20yet%20current%20methods%20struggle%20to%20find%20time-varying%20region-level%0Acommunications%20or%20scale%20to%20large%20neural%20datasets%20with%20long%20recording%20durations.%0AWe%20present%20a%20novel%20framework%20using%20Markovian%20Gaussian%20Processes%20to%20learn%20brain%0Acommunications%20with%20time-varying%20temporal%20delays%20from%20multi-region%20neural%0Arecordings%2C%20named%20Adaptive%20Delay%20Model%20%28ADM%29.%20Our%20method%20combines%20Gaussian%0AProcesses%20with%20State%20Space%20Models%20and%20employs%20parallel%20scan%20inference%0Aalgorithms%2C%20enabling%20efficient%20scaling%20to%20large%20datasets%20while%20identifying%0Aconcurrent%20communication%20patterns%20that%20evolve%20over%20time.%20This%20time-varying%0Aapproach%20captures%20how%20brain%20region%20interactions%20shift%20dynamically%20during%0Acognitive%20processes.%20Validated%20on%20synthetic%20and%20multi-region%20neural%20recordings%0Adatasets%2C%20our%20approach%20discovers%20both%20the%20directionality%20and%20temporal%20dynamics%0Aof%20neural%20communication.%20This%20work%20advances%20our%20understanding%20of%20distributed%0Aneural%20computation%20and%20provides%20a%20scalable%20tool%20for%20analyzing%20dynamic%20brain%0Anetworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Time-Varying%2520Multi-Region%2520Communications%2520via%2520Scalable%2520Markovian%250A%2520%2520Gaussian%2520Processes%26entry.906535625%3DWeihan%2520Li%2520and%2520Yule%2520Wang%2520and%2520Chengrui%2520Li%2520and%2520Anqi%2520Wu%26entry.1292438233%3D%2520%2520Understanding%2520and%2520constructing%2520brain%2520communications%2520that%2520capture%2520dynamic%250Acommunications%2520across%2520multiple%2520regions%2520is%2520fundamental%2520to%2520modern%2520system%250Aneuroscience%252C%2520yet%2520current%2520methods%2520struggle%2520to%2520find%2520time-varying%2520region-level%250Acommunications%2520or%2520scale%2520to%2520large%2520neural%2520datasets%2520with%2520long%2520recording%2520durations.%250AWe%2520present%2520a%2520novel%2520framework%2520using%2520Markovian%2520Gaussian%2520Processes%2520to%2520learn%2520brain%250Acommunications%2520with%2520time-varying%2520temporal%2520delays%2520from%2520multi-region%2520neural%250Arecordings%252C%2520named%2520Adaptive%2520Delay%2520Model%2520%2528ADM%2529.%2520Our%2520method%2520combines%2520Gaussian%250AProcesses%2520with%2520State%2520Space%2520Models%2520and%2520employs%2520parallel%2520scan%2520inference%250Aalgorithms%252C%2520enabling%2520efficient%2520scaling%2520to%2520large%2520datasets%2520while%2520identifying%250Aconcurrent%2520communication%2520patterns%2520that%2520evolve%2520over%2520time.%2520This%2520time-varying%250Aapproach%2520captures%2520how%2520brain%2520region%2520interactions%2520shift%2520dynamically%2520during%250Acognitive%2520processes.%2520Validated%2520on%2520synthetic%2520and%2520multi-region%2520neural%2520recordings%250Adatasets%252C%2520our%2520approach%2520discovers%2520both%2520the%2520directionality%2520and%2520temporal%2520dynamics%250Aof%2520neural%2520communication.%2520This%2520work%2520advances%2520our%2520understanding%2520of%2520distributed%250Aneural%2520computation%2520and%2520provides%2520a%2520scalable%2520tool%2520for%2520analyzing%2520dynamic%2520brain%250Anetworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Time-Varying%20Multi-Region%20Communications%20via%20Scalable%20Markovian%0A%20%20Gaussian%20Processes&entry.906535625=Weihan%20Li%20and%20Yule%20Wang%20and%20Chengrui%20Li%20and%20Anqi%20Wu&entry.1292438233=%20%20Understanding%20and%20constructing%20brain%20communications%20that%20capture%20dynamic%0Acommunications%20across%20multiple%20regions%20is%20fundamental%20to%20modern%20system%0Aneuroscience%2C%20yet%20current%20methods%20struggle%20to%20find%20time-varying%20region-level%0Acommunications%20or%20scale%20to%20large%20neural%20datasets%20with%20long%20recording%20durations.%0AWe%20present%20a%20novel%20framework%20using%20Markovian%20Gaussian%20Processes%20to%20learn%20brain%0Acommunications%20with%20time-varying%20temporal%20delays%20from%20multi-region%20neural%0Arecordings%2C%20named%20Adaptive%20Delay%20Model%20%28ADM%29.%20Our%20method%20combines%20Gaussian%0AProcesses%20with%20State%20Space%20Models%20and%20employs%20parallel%20scan%20inference%0Aalgorithms%2C%20enabling%20efficient%20scaling%20to%20large%20datasets%20while%20identifying%0Aconcurrent%20communication%20patterns%20that%20evolve%20over%20time.%20This%20time-varying%0Aapproach%20captures%20how%20brain%20region%20interactions%20shift%20dynamically%20during%0Acognitive%20processes.%20Validated%20on%20synthetic%20and%20multi-region%20neural%20recordings%0Adatasets%2C%20our%20approach%20discovers%20both%20the%20directionality%20and%20temporal%20dynamics%0Aof%20neural%20communication.%20This%20work%20advances%20our%20understanding%20of%20distributed%0Aneural%20computation%20and%20provides%20a%20scalable%20tool%20for%20analyzing%20dynamic%20brain%0Anetworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00397v2&entry.124074799=Read"},
{"title": "E2Former: A Linear-time Efficient and Equivariant Transformer for\n  Scalable Molecular Modeling", "author": "Yunyang Li and Lin Huang and Zhihao Ding and Chu Wang and Xinran Wei and Han Yang and Zun Wang and Chang Liu and Yu Shi and Peiran Jin and Jia Zhang and Mark Gerstein and Tao Qin", "abstract": "  Equivariant Graph Neural Networks (EGNNs) have demonstrated significant\nsuccess in modeling microscale systems, including those in chemistry, biology\nand materials science. However, EGNNs face substantial computational challenges\ndue to the high cost of constructing edge features via spherical tensor\nproducts, making them impractical for large-scale systems. To address this\nlimitation, we introduce E2Former, an equivariant and efficient transformer\narchitecture that incorporates the Wigner $6j$ convolution (Wigner $6j$ Conv).\nBy shifting the computational burden from edges to nodes, the Wigner $6j$ Conv\nreduces the complexity from $O(|\\mathcal{E}|)$ to $ O(| \\mathcal{V}|)$ while\npreserving both the model's expressive power and rotational equivariance. We\nshow that this approach achieves a 7x-30x speedup compared to conventional\n$\\mathrm{SO}(3)$ convolutions. Furthermore, our empirical results demonstrate\nthat the derived E2Former mitigates the computational challenges of existing\napproaches without compromising the ability to capture detailed geometric\ninformation. This development could suggest a promising direction for scalable\nand efficient molecular modeling.\n", "link": "http://arxiv.org/abs/2501.19216v2", "date": "2025-02-03", "relevancy": 2.0119, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5531}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4987}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E2Former%3A%20A%20Linear-time%20Efficient%20and%20Equivariant%20Transformer%20for%0A%20%20Scalable%20Molecular%20Modeling&body=Title%3A%20E2Former%3A%20A%20Linear-time%20Efficient%20and%20Equivariant%20Transformer%20for%0A%20%20Scalable%20Molecular%20Modeling%0AAuthor%3A%20Yunyang%20Li%20and%20Lin%20Huang%20and%20Zhihao%20Ding%20and%20Chu%20Wang%20and%20Xinran%20Wei%20and%20Han%20Yang%20and%20Zun%20Wang%20and%20Chang%20Liu%20and%20Yu%20Shi%20and%20Peiran%20Jin%20and%20Jia%20Zhang%20and%20Mark%20Gerstein%20and%20Tao%20Qin%0AAbstract%3A%20%20%20Equivariant%20Graph%20Neural%20Networks%20%28EGNNs%29%20have%20demonstrated%20significant%0Asuccess%20in%20modeling%20microscale%20systems%2C%20including%20those%20in%20chemistry%2C%20biology%0Aand%20materials%20science.%20However%2C%20EGNNs%20face%20substantial%20computational%20challenges%0Adue%20to%20the%20high%20cost%20of%20constructing%20edge%20features%20via%20spherical%20tensor%0Aproducts%2C%20making%20them%20impractical%20for%20large-scale%20systems.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20E2Former%2C%20an%20equivariant%20and%20efficient%20transformer%0Aarchitecture%20that%20incorporates%20the%20Wigner%20%246j%24%20convolution%20%28Wigner%20%246j%24%20Conv%29.%0ABy%20shifting%20the%20computational%20burden%20from%20edges%20to%20nodes%2C%20the%20Wigner%20%246j%24%20Conv%0Areduces%20the%20complexity%20from%20%24O%28%7C%5Cmathcal%7BE%7D%7C%29%24%20to%20%24%20O%28%7C%20%5Cmathcal%7BV%7D%7C%29%24%20while%0Apreserving%20both%20the%20model%27s%20expressive%20power%20and%20rotational%20equivariance.%20We%0Ashow%20that%20this%20approach%20achieves%20a%207x-30x%20speedup%20compared%20to%20conventional%0A%24%5Cmathrm%7BSO%7D%283%29%24%20convolutions.%20Furthermore%2C%20our%20empirical%20results%20demonstrate%0Athat%20the%20derived%20E2Former%20mitigates%20the%20computational%20challenges%20of%20existing%0Aapproaches%20without%20compromising%20the%20ability%20to%20capture%20detailed%20geometric%0Ainformation.%20This%20development%20could%20suggest%20a%20promising%20direction%20for%20scalable%0Aand%20efficient%20molecular%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE2Former%253A%2520A%2520Linear-time%2520Efficient%2520and%2520Equivariant%2520Transformer%2520for%250A%2520%2520Scalable%2520Molecular%2520Modeling%26entry.906535625%3DYunyang%2520Li%2520and%2520Lin%2520Huang%2520and%2520Zhihao%2520Ding%2520and%2520Chu%2520Wang%2520and%2520Xinran%2520Wei%2520and%2520Han%2520Yang%2520and%2520Zun%2520Wang%2520and%2520Chang%2520Liu%2520and%2520Yu%2520Shi%2520and%2520Peiran%2520Jin%2520and%2520Jia%2520Zhang%2520and%2520Mark%2520Gerstein%2520and%2520Tao%2520Qin%26entry.1292438233%3D%2520%2520Equivariant%2520Graph%2520Neural%2520Networks%2520%2528EGNNs%2529%2520have%2520demonstrated%2520significant%250Asuccess%2520in%2520modeling%2520microscale%2520systems%252C%2520including%2520those%2520in%2520chemistry%252C%2520biology%250Aand%2520materials%2520science.%2520However%252C%2520EGNNs%2520face%2520substantial%2520computational%2520challenges%250Adue%2520to%2520the%2520high%2520cost%2520of%2520constructing%2520edge%2520features%2520via%2520spherical%2520tensor%250Aproducts%252C%2520making%2520them%2520impractical%2520for%2520large-scale%2520systems.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520introduce%2520E2Former%252C%2520an%2520equivariant%2520and%2520efficient%2520transformer%250Aarchitecture%2520that%2520incorporates%2520the%2520Wigner%2520%25246j%2524%2520convolution%2520%2528Wigner%2520%25246j%2524%2520Conv%2529.%250ABy%2520shifting%2520the%2520computational%2520burden%2520from%2520edges%2520to%2520nodes%252C%2520the%2520Wigner%2520%25246j%2524%2520Conv%250Areduces%2520the%2520complexity%2520from%2520%2524O%2528%257C%255Cmathcal%257BE%257D%257C%2529%2524%2520to%2520%2524%2520O%2528%257C%2520%255Cmathcal%257BV%257D%257C%2529%2524%2520while%250Apreserving%2520both%2520the%2520model%2527s%2520expressive%2520power%2520and%2520rotational%2520equivariance.%2520We%250Ashow%2520that%2520this%2520approach%2520achieves%2520a%25207x-30x%2520speedup%2520compared%2520to%2520conventional%250A%2524%255Cmathrm%257BSO%257D%25283%2529%2524%2520convolutions.%2520Furthermore%252C%2520our%2520empirical%2520results%2520demonstrate%250Athat%2520the%2520derived%2520E2Former%2520mitigates%2520the%2520computational%2520challenges%2520of%2520existing%250Aapproaches%2520without%2520compromising%2520the%2520ability%2520to%2520capture%2520detailed%2520geometric%250Ainformation.%2520This%2520development%2520could%2520suggest%2520a%2520promising%2520direction%2520for%2520scalable%250Aand%2520efficient%2520molecular%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E2Former%3A%20A%20Linear-time%20Efficient%20and%20Equivariant%20Transformer%20for%0A%20%20Scalable%20Molecular%20Modeling&entry.906535625=Yunyang%20Li%20and%20Lin%20Huang%20and%20Zhihao%20Ding%20and%20Chu%20Wang%20and%20Xinran%20Wei%20and%20Han%20Yang%20and%20Zun%20Wang%20and%20Chang%20Liu%20and%20Yu%20Shi%20and%20Peiran%20Jin%20and%20Jia%20Zhang%20and%20Mark%20Gerstein%20and%20Tao%20Qin&entry.1292438233=%20%20Equivariant%20Graph%20Neural%20Networks%20%28EGNNs%29%20have%20demonstrated%20significant%0Asuccess%20in%20modeling%20microscale%20systems%2C%20including%20those%20in%20chemistry%2C%20biology%0Aand%20materials%20science.%20However%2C%20EGNNs%20face%20substantial%20computational%20challenges%0Adue%20to%20the%20high%20cost%20of%20constructing%20edge%20features%20via%20spherical%20tensor%0Aproducts%2C%20making%20them%20impractical%20for%20large-scale%20systems.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20E2Former%2C%20an%20equivariant%20and%20efficient%20transformer%0Aarchitecture%20that%20incorporates%20the%20Wigner%20%246j%24%20convolution%20%28Wigner%20%246j%24%20Conv%29.%0ABy%20shifting%20the%20computational%20burden%20from%20edges%20to%20nodes%2C%20the%20Wigner%20%246j%24%20Conv%0Areduces%20the%20complexity%20from%20%24O%28%7C%5Cmathcal%7BE%7D%7C%29%24%20to%20%24%20O%28%7C%20%5Cmathcal%7BV%7D%7C%29%24%20while%0Apreserving%20both%20the%20model%27s%20expressive%20power%20and%20rotational%20equivariance.%20We%0Ashow%20that%20this%20approach%20achieves%20a%207x-30x%20speedup%20compared%20to%20conventional%0A%24%5Cmathrm%7BSO%7D%283%29%24%20convolutions.%20Furthermore%2C%20our%20empirical%20results%20demonstrate%0Athat%20the%20derived%20E2Former%20mitigates%20the%20computational%20challenges%20of%20existing%0Aapproaches%20without%20compromising%20the%20ability%20to%20capture%20detailed%20geometric%0Ainformation.%20This%20development%20could%20suggest%20a%20promising%20direction%20for%20scalable%0Aand%20efficient%20molecular%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19216v2&entry.124074799=Read"},
{"title": "Inverse Entropic Optimal Transport Solves Semi-supervised Learning via\n  Data Likelihood Maximization", "author": "Mikhail Persiianov and Arip Asadulaev and Nikita Andreev and Nikita Starodubcev and Dmitry Baranchuk and Anastasis Kratsios and Evgeny Burnaev and Alexander Korotin", "abstract": "  Learning conditional distributions $\\pi^*(\\cdot|x)$ is a central problem in\nmachine learning, which is typically approached via supervised methods with\npaired data $(x,y) \\sim \\pi^*$. However, acquiring paired data samples is often\nchallenging, especially in problems such as domain translation. This\nnecessitates the development of $\\textit{semi-supervised}$ models that utilize\nboth limited paired data and additional unpaired i.i.d. samples $x \\sim\n\\pi^*_x$ and $y \\sim \\pi^*_y$ from the marginal distributions. The usage of\nsuch combined data is complex and often relies on heuristic approaches. To\ntackle this issue, we propose a new learning paradigm that integrates both\npaired and unpaired data $\\textbf{seamlessly}$ through the data likelihood\nmaximization techniques. We demonstrate that our approach also connects\nintriguingly with inverse entropic optimal transport (OT). This finding allows\nus to apply recent advances in computational OT to establish a $\\textbf{light}$\nlearning algorithm to get $\\pi^*(\\cdot|x)$. Furthermore, we demonstrate through\nempirical tests that our method effectively learns conditional distributions\nusing paired and unpaired data simultaneously.\n", "link": "http://arxiv.org/abs/2410.02628v2", "date": "2025-02-03", "relevancy": 2.0099, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5261}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5104}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Entropic%20Optimal%20Transport%20Solves%20Semi-supervised%20Learning%20via%0A%20%20Data%20Likelihood%20Maximization&body=Title%3A%20Inverse%20Entropic%20Optimal%20Transport%20Solves%20Semi-supervised%20Learning%20via%0A%20%20Data%20Likelihood%20Maximization%0AAuthor%3A%20Mikhail%20Persiianov%20and%20Arip%20Asadulaev%20and%20Nikita%20Andreev%20and%20Nikita%20Starodubcev%20and%20Dmitry%20Baranchuk%20and%20Anastasis%20Kratsios%20and%20Evgeny%20Burnaev%20and%20Alexander%20Korotin%0AAbstract%3A%20%20%20Learning%20conditional%20distributions%20%24%5Cpi%5E%2A%28%5Ccdot%7Cx%29%24%20is%20a%20central%20problem%20in%0Amachine%20learning%2C%20which%20is%20typically%20approached%20via%20supervised%20methods%20with%0Apaired%20data%20%24%28x%2Cy%29%20%5Csim%20%5Cpi%5E%2A%24.%20However%2C%20acquiring%20paired%20data%20samples%20is%20often%0Achallenging%2C%20especially%20in%20problems%20such%20as%20domain%20translation.%20This%0Anecessitates%20the%20development%20of%20%24%5Ctextit%7Bsemi-supervised%7D%24%20models%20that%20utilize%0Aboth%20limited%20paired%20data%20and%20additional%20unpaired%20i.i.d.%20samples%20%24x%20%5Csim%0A%5Cpi%5E%2A_x%24%20and%20%24y%20%5Csim%20%5Cpi%5E%2A_y%24%20from%20the%20marginal%20distributions.%20The%20usage%20of%0Asuch%20combined%20data%20is%20complex%20and%20often%20relies%20on%20heuristic%20approaches.%20To%0Atackle%20this%20issue%2C%20we%20propose%20a%20new%20learning%20paradigm%20that%20integrates%20both%0Apaired%20and%20unpaired%20data%20%24%5Ctextbf%7Bseamlessly%7D%24%20through%20the%20data%20likelihood%0Amaximization%20techniques.%20We%20demonstrate%20that%20our%20approach%20also%20connects%0Aintriguingly%20with%20inverse%20entropic%20optimal%20transport%20%28OT%29.%20This%20finding%20allows%0Aus%20to%20apply%20recent%20advances%20in%20computational%20OT%20to%20establish%20a%20%24%5Ctextbf%7Blight%7D%24%0Alearning%20algorithm%20to%20get%20%24%5Cpi%5E%2A%28%5Ccdot%7Cx%29%24.%20Furthermore%2C%20we%20demonstrate%20through%0Aempirical%20tests%20that%20our%20method%20effectively%20learns%20conditional%20distributions%0Ausing%20paired%20and%20unpaired%20data%20simultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02628v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Entropic%2520Optimal%2520Transport%2520Solves%2520Semi-supervised%2520Learning%2520via%250A%2520%2520Data%2520Likelihood%2520Maximization%26entry.906535625%3DMikhail%2520Persiianov%2520and%2520Arip%2520Asadulaev%2520and%2520Nikita%2520Andreev%2520and%2520Nikita%2520Starodubcev%2520and%2520Dmitry%2520Baranchuk%2520and%2520Anastasis%2520Kratsios%2520and%2520Evgeny%2520Burnaev%2520and%2520Alexander%2520Korotin%26entry.1292438233%3D%2520%2520Learning%2520conditional%2520distributions%2520%2524%255Cpi%255E%252A%2528%255Ccdot%257Cx%2529%2524%2520is%2520a%2520central%2520problem%2520in%250Amachine%2520learning%252C%2520which%2520is%2520typically%2520approached%2520via%2520supervised%2520methods%2520with%250Apaired%2520data%2520%2524%2528x%252Cy%2529%2520%255Csim%2520%255Cpi%255E%252A%2524.%2520However%252C%2520acquiring%2520paired%2520data%2520samples%2520is%2520often%250Achallenging%252C%2520especially%2520in%2520problems%2520such%2520as%2520domain%2520translation.%2520This%250Anecessitates%2520the%2520development%2520of%2520%2524%255Ctextit%257Bsemi-supervised%257D%2524%2520models%2520that%2520utilize%250Aboth%2520limited%2520paired%2520data%2520and%2520additional%2520unpaired%2520i.i.d.%2520samples%2520%2524x%2520%255Csim%250A%255Cpi%255E%252A_x%2524%2520and%2520%2524y%2520%255Csim%2520%255Cpi%255E%252A_y%2524%2520from%2520the%2520marginal%2520distributions.%2520The%2520usage%2520of%250Asuch%2520combined%2520data%2520is%2520complex%2520and%2520often%2520relies%2520on%2520heuristic%2520approaches.%2520To%250Atackle%2520this%2520issue%252C%2520we%2520propose%2520a%2520new%2520learning%2520paradigm%2520that%2520integrates%2520both%250Apaired%2520and%2520unpaired%2520data%2520%2524%255Ctextbf%257Bseamlessly%257D%2524%2520through%2520the%2520data%2520likelihood%250Amaximization%2520techniques.%2520We%2520demonstrate%2520that%2520our%2520approach%2520also%2520connects%250Aintriguingly%2520with%2520inverse%2520entropic%2520optimal%2520transport%2520%2528OT%2529.%2520This%2520finding%2520allows%250Aus%2520to%2520apply%2520recent%2520advances%2520in%2520computational%2520OT%2520to%2520establish%2520a%2520%2524%255Ctextbf%257Blight%257D%2524%250Alearning%2520algorithm%2520to%2520get%2520%2524%255Cpi%255E%252A%2528%255Ccdot%257Cx%2529%2524.%2520Furthermore%252C%2520we%2520demonstrate%2520through%250Aempirical%2520tests%2520that%2520our%2520method%2520effectively%2520learns%2520conditional%2520distributions%250Ausing%2520paired%2520and%2520unpaired%2520data%2520simultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02628v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Entropic%20Optimal%20Transport%20Solves%20Semi-supervised%20Learning%20via%0A%20%20Data%20Likelihood%20Maximization&entry.906535625=Mikhail%20Persiianov%20and%20Arip%20Asadulaev%20and%20Nikita%20Andreev%20and%20Nikita%20Starodubcev%20and%20Dmitry%20Baranchuk%20and%20Anastasis%20Kratsios%20and%20Evgeny%20Burnaev%20and%20Alexander%20Korotin&entry.1292438233=%20%20Learning%20conditional%20distributions%20%24%5Cpi%5E%2A%28%5Ccdot%7Cx%29%24%20is%20a%20central%20problem%20in%0Amachine%20learning%2C%20which%20is%20typically%20approached%20via%20supervised%20methods%20with%0Apaired%20data%20%24%28x%2Cy%29%20%5Csim%20%5Cpi%5E%2A%24.%20However%2C%20acquiring%20paired%20data%20samples%20is%20often%0Achallenging%2C%20especially%20in%20problems%20such%20as%20domain%20translation.%20This%0Anecessitates%20the%20development%20of%20%24%5Ctextit%7Bsemi-supervised%7D%24%20models%20that%20utilize%0Aboth%20limited%20paired%20data%20and%20additional%20unpaired%20i.i.d.%20samples%20%24x%20%5Csim%0A%5Cpi%5E%2A_x%24%20and%20%24y%20%5Csim%20%5Cpi%5E%2A_y%24%20from%20the%20marginal%20distributions.%20The%20usage%20of%0Asuch%20combined%20data%20is%20complex%20and%20often%20relies%20on%20heuristic%20approaches.%20To%0Atackle%20this%20issue%2C%20we%20propose%20a%20new%20learning%20paradigm%20that%20integrates%20both%0Apaired%20and%20unpaired%20data%20%24%5Ctextbf%7Bseamlessly%7D%24%20through%20the%20data%20likelihood%0Amaximization%20techniques.%20We%20demonstrate%20that%20our%20approach%20also%20connects%0Aintriguingly%20with%20inverse%20entropic%20optimal%20transport%20%28OT%29.%20This%20finding%20allows%0Aus%20to%20apply%20recent%20advances%20in%20computational%20OT%20to%20establish%20a%20%24%5Ctextbf%7Blight%7D%24%0Alearning%20algorithm%20to%20get%20%24%5Cpi%5E%2A%28%5Ccdot%7Cx%29%24.%20Furthermore%2C%20we%20demonstrate%20through%0Aempirical%20tests%20that%20our%20method%20effectively%20learns%20conditional%20distributions%0Ausing%20paired%20and%20unpaired%20data%20simultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02628v2&entry.124074799=Read"},
{"title": "On Probabilistic Pullback Metrics on Latent Hyperbolic Manifolds", "author": "Luis Augenstein and No\u00e9mie Jaquier and Tamim Asfour and Leonel Rozo", "abstract": "  Probabilistic Latent Variable Models (LVMs) have proven effective in\ncapturing complex, high-dimensional data through lower-dimensional\nrepresentations. Recent advances show that using Riemannian manifolds as latent\nspaces provides more flexibility to learn higher quality embeddings. This paper\nfocuses on the hyperbolic manifold, a particularly suitable choice for modeling\nhierarchical relationships. Previous approaches relying on hyperbolic geodesics\nfor interpolating the latent space often generate paths crossing low-data\nregions, leading to highly uncertain predictions. Instead, we propose\naugmenting the hyperbolic metric with a pullback metric to account for\ndistortions introduced by the LVM's nonlinear mapping and provide a complete\ndevelopment for pullback metrics of Gaussian Process LVMs (GPLVMs). Our\nexperiments demonstrate that geodesics on the pullback metric not only respect\nthe geometry of the hyperbolic latent space but also align with the underlying\ndata distribution, significantly reducing uncertainty in predictions.\n", "link": "http://arxiv.org/abs/2410.20850v2", "date": "2025-02-03", "relevancy": 1.9959, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5051}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4971}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Probabilistic%20Pullback%20Metrics%20on%20Latent%20Hyperbolic%20Manifolds&body=Title%3A%20On%20Probabilistic%20Pullback%20Metrics%20on%20Latent%20Hyperbolic%20Manifolds%0AAuthor%3A%20Luis%20Augenstein%20and%20No%C3%A9mie%20Jaquier%20and%20Tamim%20Asfour%20and%20Leonel%20Rozo%0AAbstract%3A%20%20%20Probabilistic%20Latent%20Variable%20Models%20%28LVMs%29%20have%20proven%20effective%20in%0Acapturing%20complex%2C%20high-dimensional%20data%20through%20lower-dimensional%0Arepresentations.%20Recent%20advances%20show%20that%20using%20Riemannian%20manifolds%20as%20latent%0Aspaces%20provides%20more%20flexibility%20to%20learn%20higher%20quality%20embeddings.%20This%20paper%0Afocuses%20on%20the%20hyperbolic%20manifold%2C%20a%20particularly%20suitable%20choice%20for%20modeling%0Ahierarchical%20relationships.%20Previous%20approaches%20relying%20on%20hyperbolic%20geodesics%0Afor%20interpolating%20the%20latent%20space%20often%20generate%20paths%20crossing%20low-data%0Aregions%2C%20leading%20to%20highly%20uncertain%20predictions.%20Instead%2C%20we%20propose%0Aaugmenting%20the%20hyperbolic%20metric%20with%20a%20pullback%20metric%20to%20account%20for%0Adistortions%20introduced%20by%20the%20LVM%27s%20nonlinear%20mapping%20and%20provide%20a%20complete%0Adevelopment%20for%20pullback%20metrics%20of%20Gaussian%20Process%20LVMs%20%28GPLVMs%29.%20Our%0Aexperiments%20demonstrate%20that%20geodesics%20on%20the%20pullback%20metric%20not%20only%20respect%0Athe%20geometry%20of%20the%20hyperbolic%20latent%20space%20but%20also%20align%20with%20the%20underlying%0Adata%20distribution%2C%20significantly%20reducing%20uncertainty%20in%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20850v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Probabilistic%2520Pullback%2520Metrics%2520on%2520Latent%2520Hyperbolic%2520Manifolds%26entry.906535625%3DLuis%2520Augenstein%2520and%2520No%25C3%25A9mie%2520Jaquier%2520and%2520Tamim%2520Asfour%2520and%2520Leonel%2520Rozo%26entry.1292438233%3D%2520%2520Probabilistic%2520Latent%2520Variable%2520Models%2520%2528LVMs%2529%2520have%2520proven%2520effective%2520in%250Acapturing%2520complex%252C%2520high-dimensional%2520data%2520through%2520lower-dimensional%250Arepresentations.%2520Recent%2520advances%2520show%2520that%2520using%2520Riemannian%2520manifolds%2520as%2520latent%250Aspaces%2520provides%2520more%2520flexibility%2520to%2520learn%2520higher%2520quality%2520embeddings.%2520This%2520paper%250Afocuses%2520on%2520the%2520hyperbolic%2520manifold%252C%2520a%2520particularly%2520suitable%2520choice%2520for%2520modeling%250Ahierarchical%2520relationships.%2520Previous%2520approaches%2520relying%2520on%2520hyperbolic%2520geodesics%250Afor%2520interpolating%2520the%2520latent%2520space%2520often%2520generate%2520paths%2520crossing%2520low-data%250Aregions%252C%2520leading%2520to%2520highly%2520uncertain%2520predictions.%2520Instead%252C%2520we%2520propose%250Aaugmenting%2520the%2520hyperbolic%2520metric%2520with%2520a%2520pullback%2520metric%2520to%2520account%2520for%250Adistortions%2520introduced%2520by%2520the%2520LVM%2527s%2520nonlinear%2520mapping%2520and%2520provide%2520a%2520complete%250Adevelopment%2520for%2520pullback%2520metrics%2520of%2520Gaussian%2520Process%2520LVMs%2520%2528GPLVMs%2529.%2520Our%250Aexperiments%2520demonstrate%2520that%2520geodesics%2520on%2520the%2520pullback%2520metric%2520not%2520only%2520respect%250Athe%2520geometry%2520of%2520the%2520hyperbolic%2520latent%2520space%2520but%2520also%2520align%2520with%2520the%2520underlying%250Adata%2520distribution%252C%2520significantly%2520reducing%2520uncertainty%2520in%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20850v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Probabilistic%20Pullback%20Metrics%20on%20Latent%20Hyperbolic%20Manifolds&entry.906535625=Luis%20Augenstein%20and%20No%C3%A9mie%20Jaquier%20and%20Tamim%20Asfour%20and%20Leonel%20Rozo&entry.1292438233=%20%20Probabilistic%20Latent%20Variable%20Models%20%28LVMs%29%20have%20proven%20effective%20in%0Acapturing%20complex%2C%20high-dimensional%20data%20through%20lower-dimensional%0Arepresentations.%20Recent%20advances%20show%20that%20using%20Riemannian%20manifolds%20as%20latent%0Aspaces%20provides%20more%20flexibility%20to%20learn%20higher%20quality%20embeddings.%20This%20paper%0Afocuses%20on%20the%20hyperbolic%20manifold%2C%20a%20particularly%20suitable%20choice%20for%20modeling%0Ahierarchical%20relationships.%20Previous%20approaches%20relying%20on%20hyperbolic%20geodesics%0Afor%20interpolating%20the%20latent%20space%20often%20generate%20paths%20crossing%20low-data%0Aregions%2C%20leading%20to%20highly%20uncertain%20predictions.%20Instead%2C%20we%20propose%0Aaugmenting%20the%20hyperbolic%20metric%20with%20a%20pullback%20metric%20to%20account%20for%0Adistortions%20introduced%20by%20the%20LVM%27s%20nonlinear%20mapping%20and%20provide%20a%20complete%0Adevelopment%20for%20pullback%20metrics%20of%20Gaussian%20Process%20LVMs%20%28GPLVMs%29.%20Our%0Aexperiments%20demonstrate%20that%20geodesics%20on%20the%20pullback%20metric%20not%20only%20respect%0Athe%20geometry%20of%20the%20hyperbolic%20latent%20space%20but%20also%20align%20with%20the%20underlying%0Adata%20distribution%2C%20significantly%20reducing%20uncertainty%20in%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20850v2&entry.124074799=Read"},
{"title": "Leveraging Multi-facet Paths for Heterogeneous Graph Representation\n  Learning", "author": "JongWoo Kim and SeongYeub Chu and HyeongMin Park and Bryan Wong and MunYong Yi", "abstract": "  Recent advancements in graph neural networks (GNNs) and heterogeneous GNNs\n(HGNNs) have advanced node embeddings and relationship learning for various\ntasks. However, existing methods often rely on domain-specific predefined\nmeta-paths, which are coarse-grained and focus solely on aspects like node\ntype, limiting their ability to capture complex interactions. We introduce\nMF2Vec, a model that uses multi-faceted (fine-grained) paths instead of\npredefined meta-paths. MF2Vec extracts paths via random walks and generates\nmulti-faceted vectors, ignoring predefined schemas. This method learns diverse\naspects of nodes and their relationships, constructs a homogeneous network, and\ncreates node embeddings for classification, link prediction, and clustering.\nExtensive experiments show that MF2Vec outperforms existing methods, offering a\nmore flexible and comprehensive framework for analyzing complex networks. The\ncode is available at https://anonymous.4open.science/r/MF2Vec-6ABC.\n", "link": "http://arxiv.org/abs/2407.20648v2", "date": "2025-02-03", "relevancy": 1.9948, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5076}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5028}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Multi-facet%20Paths%20for%20Heterogeneous%20Graph%20Representation%0A%20%20Learning&body=Title%3A%20Leveraging%20Multi-facet%20Paths%20for%20Heterogeneous%20Graph%20Representation%0A%20%20Learning%0AAuthor%3A%20JongWoo%20Kim%20and%20SeongYeub%20Chu%20and%20HyeongMin%20Park%20and%20Bryan%20Wong%20and%20MunYong%20Yi%0AAbstract%3A%20%20%20Recent%20advancements%20in%20graph%20neural%20networks%20%28GNNs%29%20and%20heterogeneous%20GNNs%0A%28HGNNs%29%20have%20advanced%20node%20embeddings%20and%20relationship%20learning%20for%20various%0Atasks.%20However%2C%20existing%20methods%20often%20rely%20on%20domain-specific%20predefined%0Ameta-paths%2C%20which%20are%20coarse-grained%20and%20focus%20solely%20on%20aspects%20like%20node%0Atype%2C%20limiting%20their%20ability%20to%20capture%20complex%20interactions.%20We%20introduce%0AMF2Vec%2C%20a%20model%20that%20uses%20multi-faceted%20%28fine-grained%29%20paths%20instead%20of%0Apredefined%20meta-paths.%20MF2Vec%20extracts%20paths%20via%20random%20walks%20and%20generates%0Amulti-faceted%20vectors%2C%20ignoring%20predefined%20schemas.%20This%20method%20learns%20diverse%0Aaspects%20of%20nodes%20and%20their%20relationships%2C%20constructs%20a%20homogeneous%20network%2C%20and%0Acreates%20node%20embeddings%20for%20classification%2C%20link%20prediction%2C%20and%20clustering.%0AExtensive%20experiments%20show%20that%20MF2Vec%20outperforms%20existing%20methods%2C%20offering%20a%0Amore%20flexible%20and%20comprehensive%20framework%20for%20analyzing%20complex%20networks.%20The%0Acode%20is%20available%20at%20https%3A//anonymous.4open.science/r/MF2Vec-6ABC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20648v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Multi-facet%2520Paths%2520for%2520Heterogeneous%2520Graph%2520Representation%250A%2520%2520Learning%26entry.906535625%3DJongWoo%2520Kim%2520and%2520SeongYeub%2520Chu%2520and%2520HyeongMin%2520Park%2520and%2520Bryan%2520Wong%2520and%2520MunYong%2520Yi%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520and%2520heterogeneous%2520GNNs%250A%2528HGNNs%2529%2520have%2520advanced%2520node%2520embeddings%2520and%2520relationship%2520learning%2520for%2520various%250Atasks.%2520However%252C%2520existing%2520methods%2520often%2520rely%2520on%2520domain-specific%2520predefined%250Ameta-paths%252C%2520which%2520are%2520coarse-grained%2520and%2520focus%2520solely%2520on%2520aspects%2520like%2520node%250Atype%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520complex%2520interactions.%2520We%2520introduce%250AMF2Vec%252C%2520a%2520model%2520that%2520uses%2520multi-faceted%2520%2528fine-grained%2529%2520paths%2520instead%2520of%250Apredefined%2520meta-paths.%2520MF2Vec%2520extracts%2520paths%2520via%2520random%2520walks%2520and%2520generates%250Amulti-faceted%2520vectors%252C%2520ignoring%2520predefined%2520schemas.%2520This%2520method%2520learns%2520diverse%250Aaspects%2520of%2520nodes%2520and%2520their%2520relationships%252C%2520constructs%2520a%2520homogeneous%2520network%252C%2520and%250Acreates%2520node%2520embeddings%2520for%2520classification%252C%2520link%2520prediction%252C%2520and%2520clustering.%250AExtensive%2520experiments%2520show%2520that%2520MF2Vec%2520outperforms%2520existing%2520methods%252C%2520offering%2520a%250Amore%2520flexible%2520and%2520comprehensive%2520framework%2520for%2520analyzing%2520complex%2520networks.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//anonymous.4open.science/r/MF2Vec-6ABC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20648v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Multi-facet%20Paths%20for%20Heterogeneous%20Graph%20Representation%0A%20%20Learning&entry.906535625=JongWoo%20Kim%20and%20SeongYeub%20Chu%20and%20HyeongMin%20Park%20and%20Bryan%20Wong%20and%20MunYong%20Yi&entry.1292438233=%20%20Recent%20advancements%20in%20graph%20neural%20networks%20%28GNNs%29%20and%20heterogeneous%20GNNs%0A%28HGNNs%29%20have%20advanced%20node%20embeddings%20and%20relationship%20learning%20for%20various%0Atasks.%20However%2C%20existing%20methods%20often%20rely%20on%20domain-specific%20predefined%0Ameta-paths%2C%20which%20are%20coarse-grained%20and%20focus%20solely%20on%20aspects%20like%20node%0Atype%2C%20limiting%20their%20ability%20to%20capture%20complex%20interactions.%20We%20introduce%0AMF2Vec%2C%20a%20model%20that%20uses%20multi-faceted%20%28fine-grained%29%20paths%20instead%20of%0Apredefined%20meta-paths.%20MF2Vec%20extracts%20paths%20via%20random%20walks%20and%20generates%0Amulti-faceted%20vectors%2C%20ignoring%20predefined%20schemas.%20This%20method%20learns%20diverse%0Aaspects%20of%20nodes%20and%20their%20relationships%2C%20constructs%20a%20homogeneous%20network%2C%20and%0Acreates%20node%20embeddings%20for%20classification%2C%20link%20prediction%2C%20and%20clustering.%0AExtensive%20experiments%20show%20that%20MF2Vec%20outperforms%20existing%20methods%2C%20offering%20a%0Amore%20flexible%20and%20comprehensive%20framework%20for%20analyzing%20complex%20networks.%20The%0Acode%20is%20available%20at%20https%3A//anonymous.4open.science/r/MF2Vec-6ABC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20648v2&entry.124074799=Read"},
{"title": "Robust Hyperbolic Learning with Curvature-Aware Optimization", "author": "Ahmad Bdeir and Johannes Burchert and Lars Schmidt-Thieme and Niels Landwehr", "abstract": "  Hyperbolic deep learning has become a growing research direction in computer\nvision due to the unique properties afforded by the alternate embedding space.\nThe negative curvature and exponentially growing distance metric provide a\nnatural framework for capturing hierarchical relationships between datapoints\nand allowing for finer separability between their embeddings. However, current\nhyperbolic learning approaches are still prone to overfitting, computationally\nexpensive, and prone to instability, especially when attempting to learn the\nmanifold curvature to adapt to tasks and different datasets. To address these\nissues, our paper presents a derivation for Riemannian AdamW that helps\nincrease hyperbolic generalization ability. For improved stability, we\nintroduce a novel fine-tunable hyperbolic scaling approach to constrain\nhyperbolic embeddings and reduce approximation errors. Using this along with\nour curvature-aware learning schema for Lorentzian Optimizers enables the\ncombination of curvature and non-trivialized hyperbolic parameter learning. Our\napproach demonstrates consistent performance improvements across Computer\nVision, EEG classification, and hierarchical metric learning tasks achieving\nstate-of-the-art results in two domains and drastically reducing runtime.\n", "link": "http://arxiv.org/abs/2405.13979v3", "date": "2025-02-03", "relevancy": 1.9948, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5138}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5037}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Hyperbolic%20Learning%20with%20Curvature-Aware%20Optimization&body=Title%3A%20Robust%20Hyperbolic%20Learning%20with%20Curvature-Aware%20Optimization%0AAuthor%3A%20Ahmad%20Bdeir%20and%20Johannes%20Burchert%20and%20Lars%20Schmidt-Thieme%20and%20Niels%20Landwehr%0AAbstract%3A%20%20%20Hyperbolic%20deep%20learning%20has%20become%20a%20growing%20research%20direction%20in%20computer%0Avision%20due%20to%20the%20unique%20properties%20afforded%20by%20the%20alternate%20embedding%20space.%0AThe%20negative%20curvature%20and%20exponentially%20growing%20distance%20metric%20provide%20a%0Anatural%20framework%20for%20capturing%20hierarchical%20relationships%20between%20datapoints%0Aand%20allowing%20for%20finer%20separability%20between%20their%20embeddings.%20However%2C%20current%0Ahyperbolic%20learning%20approaches%20are%20still%20prone%20to%20overfitting%2C%20computationally%0Aexpensive%2C%20and%20prone%20to%20instability%2C%20especially%20when%20attempting%20to%20learn%20the%0Amanifold%20curvature%20to%20adapt%20to%20tasks%20and%20different%20datasets.%20To%20address%20these%0Aissues%2C%20our%20paper%20presents%20a%20derivation%20for%20Riemannian%20AdamW%20that%20helps%0Aincrease%20hyperbolic%20generalization%20ability.%20For%20improved%20stability%2C%20we%0Aintroduce%20a%20novel%20fine-tunable%20hyperbolic%20scaling%20approach%20to%20constrain%0Ahyperbolic%20embeddings%20and%20reduce%20approximation%20errors.%20Using%20this%20along%20with%0Aour%20curvature-aware%20learning%20schema%20for%20Lorentzian%20Optimizers%20enables%20the%0Acombination%20of%20curvature%20and%20non-trivialized%20hyperbolic%20parameter%20learning.%20Our%0Aapproach%20demonstrates%20consistent%20performance%20improvements%20across%20Computer%0AVision%2C%20EEG%20classification%2C%20and%20hierarchical%20metric%20learning%20tasks%20achieving%0Astate-of-the-art%20results%20in%20two%20domains%20and%20drastically%20reducing%20runtime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13979v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Hyperbolic%2520Learning%2520with%2520Curvature-Aware%2520Optimization%26entry.906535625%3DAhmad%2520Bdeir%2520and%2520Johannes%2520Burchert%2520and%2520Lars%2520Schmidt-Thieme%2520and%2520Niels%2520Landwehr%26entry.1292438233%3D%2520%2520Hyperbolic%2520deep%2520learning%2520has%2520become%2520a%2520growing%2520research%2520direction%2520in%2520computer%250Avision%2520due%2520to%2520the%2520unique%2520properties%2520afforded%2520by%2520the%2520alternate%2520embedding%2520space.%250AThe%2520negative%2520curvature%2520and%2520exponentially%2520growing%2520distance%2520metric%2520provide%2520a%250Anatural%2520framework%2520for%2520capturing%2520hierarchical%2520relationships%2520between%2520datapoints%250Aand%2520allowing%2520for%2520finer%2520separability%2520between%2520their%2520embeddings.%2520However%252C%2520current%250Ahyperbolic%2520learning%2520approaches%2520are%2520still%2520prone%2520to%2520overfitting%252C%2520computationally%250Aexpensive%252C%2520and%2520prone%2520to%2520instability%252C%2520especially%2520when%2520attempting%2520to%2520learn%2520the%250Amanifold%2520curvature%2520to%2520adapt%2520to%2520tasks%2520and%2520different%2520datasets.%2520To%2520address%2520these%250Aissues%252C%2520our%2520paper%2520presents%2520a%2520derivation%2520for%2520Riemannian%2520AdamW%2520that%2520helps%250Aincrease%2520hyperbolic%2520generalization%2520ability.%2520For%2520improved%2520stability%252C%2520we%250Aintroduce%2520a%2520novel%2520fine-tunable%2520hyperbolic%2520scaling%2520approach%2520to%2520constrain%250Ahyperbolic%2520embeddings%2520and%2520reduce%2520approximation%2520errors.%2520Using%2520this%2520along%2520with%250Aour%2520curvature-aware%2520learning%2520schema%2520for%2520Lorentzian%2520Optimizers%2520enables%2520the%250Acombination%2520of%2520curvature%2520and%2520non-trivialized%2520hyperbolic%2520parameter%2520learning.%2520Our%250Aapproach%2520demonstrates%2520consistent%2520performance%2520improvements%2520across%2520Computer%250AVision%252C%2520EEG%2520classification%252C%2520and%2520hierarchical%2520metric%2520learning%2520tasks%2520achieving%250Astate-of-the-art%2520results%2520in%2520two%2520domains%2520and%2520drastically%2520reducing%2520runtime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13979v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Hyperbolic%20Learning%20with%20Curvature-Aware%20Optimization&entry.906535625=Ahmad%20Bdeir%20and%20Johannes%20Burchert%20and%20Lars%20Schmidt-Thieme%20and%20Niels%20Landwehr&entry.1292438233=%20%20Hyperbolic%20deep%20learning%20has%20become%20a%20growing%20research%20direction%20in%20computer%0Avision%20due%20to%20the%20unique%20properties%20afforded%20by%20the%20alternate%20embedding%20space.%0AThe%20negative%20curvature%20and%20exponentially%20growing%20distance%20metric%20provide%20a%0Anatural%20framework%20for%20capturing%20hierarchical%20relationships%20between%20datapoints%0Aand%20allowing%20for%20finer%20separability%20between%20their%20embeddings.%20However%2C%20current%0Ahyperbolic%20learning%20approaches%20are%20still%20prone%20to%20overfitting%2C%20computationally%0Aexpensive%2C%20and%20prone%20to%20instability%2C%20especially%20when%20attempting%20to%20learn%20the%0Amanifold%20curvature%20to%20adapt%20to%20tasks%20and%20different%20datasets.%20To%20address%20these%0Aissues%2C%20our%20paper%20presents%20a%20derivation%20for%20Riemannian%20AdamW%20that%20helps%0Aincrease%20hyperbolic%20generalization%20ability.%20For%20improved%20stability%2C%20we%0Aintroduce%20a%20novel%20fine-tunable%20hyperbolic%20scaling%20approach%20to%20constrain%0Ahyperbolic%20embeddings%20and%20reduce%20approximation%20errors.%20Using%20this%20along%20with%0Aour%20curvature-aware%20learning%20schema%20for%20Lorentzian%20Optimizers%20enables%20the%0Acombination%20of%20curvature%20and%20non-trivialized%20hyperbolic%20parameter%20learning.%20Our%0Aapproach%20demonstrates%20consistent%20performance%20improvements%20across%20Computer%0AVision%2C%20EEG%20classification%2C%20and%20hierarchical%20metric%20learning%20tasks%20achieving%0Astate-of-the-art%20results%20in%20two%20domains%20and%20drastically%20reducing%20runtime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13979v3&entry.124074799=Read"},
{"title": "DeTrigger: A Gradient-Centric Approach to Backdoor Attack Mitigation in\n  Federated Learning", "author": "Kichang Lee and Yujin Shin and Jonghyuk Yun and Songkuk Kim and Jun Han and JeongGil Ko", "abstract": "  Federated Learning (FL) enables collaborative model training across\ndistributed devices while preserving local data privacy, making it ideal for\nmobile and embedded systems. However, the decentralized nature of FL also opens\nvulnerabilities to model poisoning attacks, particularly backdoor attacks,\nwhere adversaries implant trigger patterns to manipulate model predictions. In\nthis paper, we propose DeTrigger, a scalable and efficient backdoor-robust\nfederated learning framework that leverages insights from adversarial attack\nmethodologies. By employing gradient analysis with temperature scaling,\nDeTrigger detects and isolates backdoor triggers, allowing for precise model\nweight pruning of backdoor activations without sacrificing benign model\nknowledge. Extensive evaluations across four widely used datasets demonstrate\nthat DeTrigger achieves up to 251x faster detection than traditional methods\nand mitigates backdoor attacks by up to 98.9%, with minimal impact on global\nmodel accuracy. Our findings establish DeTrigger as a robust and scalable\nsolution to protect federated learning environments against sophisticated\nbackdoor threats.\n", "link": "http://arxiv.org/abs/2411.12220v2", "date": "2025-02-03", "relevancy": 1.9774, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5055}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4891}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeTrigger%3A%20A%20Gradient-Centric%20Approach%20to%20Backdoor%20Attack%20Mitigation%20in%0A%20%20Federated%20Learning&body=Title%3A%20DeTrigger%3A%20A%20Gradient-Centric%20Approach%20to%20Backdoor%20Attack%20Mitigation%20in%0A%20%20Federated%20Learning%0AAuthor%3A%20Kichang%20Lee%20and%20Yujin%20Shin%20and%20Jonghyuk%20Yun%20and%20Songkuk%20Kim%20and%20Jun%20Han%20and%20JeongGil%20Ko%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20devices%20while%20preserving%20local%20data%20privacy%2C%20making%20it%20ideal%20for%0Amobile%20and%20embedded%20systems.%20However%2C%20the%20decentralized%20nature%20of%20FL%20also%20opens%0Avulnerabilities%20to%20model%20poisoning%20attacks%2C%20particularly%20backdoor%20attacks%2C%0Awhere%20adversaries%20implant%20trigger%20patterns%20to%20manipulate%20model%20predictions.%20In%0Athis%20paper%2C%20we%20propose%20DeTrigger%2C%20a%20scalable%20and%20efficient%20backdoor-robust%0Afederated%20learning%20framework%20that%20leverages%20insights%20from%20adversarial%20attack%0Amethodologies.%20By%20employing%20gradient%20analysis%20with%20temperature%20scaling%2C%0ADeTrigger%20detects%20and%20isolates%20backdoor%20triggers%2C%20allowing%20for%20precise%20model%0Aweight%20pruning%20of%20backdoor%20activations%20without%20sacrificing%20benign%20model%0Aknowledge.%20Extensive%20evaluations%20across%20four%20widely%20used%20datasets%20demonstrate%0Athat%20DeTrigger%20achieves%20up%20to%20251x%20faster%20detection%20than%20traditional%20methods%0Aand%20mitigates%20backdoor%20attacks%20by%20up%20to%2098.9%25%2C%20with%20minimal%20impact%20on%20global%0Amodel%20accuracy.%20Our%20findings%20establish%20DeTrigger%20as%20a%20robust%20and%20scalable%0Asolution%20to%20protect%20federated%20learning%20environments%20against%20sophisticated%0Abackdoor%20threats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12220v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeTrigger%253A%2520A%2520Gradient-Centric%2520Approach%2520to%2520Backdoor%2520Attack%2520Mitigation%2520in%250A%2520%2520Federated%2520Learning%26entry.906535625%3DKichang%2520Lee%2520and%2520Yujin%2520Shin%2520and%2520Jonghyuk%2520Yun%2520and%2520Songkuk%2520Kim%2520and%2520Jun%2520Han%2520and%2520JeongGil%2520Ko%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%250Adistributed%2520devices%2520while%2520preserving%2520local%2520data%2520privacy%252C%2520making%2520it%2520ideal%2520for%250Amobile%2520and%2520embedded%2520systems.%2520However%252C%2520the%2520decentralized%2520nature%2520of%2520FL%2520also%2520opens%250Avulnerabilities%2520to%2520model%2520poisoning%2520attacks%252C%2520particularly%2520backdoor%2520attacks%252C%250Awhere%2520adversaries%2520implant%2520trigger%2520patterns%2520to%2520manipulate%2520model%2520predictions.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520DeTrigger%252C%2520a%2520scalable%2520and%2520efficient%2520backdoor-robust%250Afederated%2520learning%2520framework%2520that%2520leverages%2520insights%2520from%2520adversarial%2520attack%250Amethodologies.%2520By%2520employing%2520gradient%2520analysis%2520with%2520temperature%2520scaling%252C%250ADeTrigger%2520detects%2520and%2520isolates%2520backdoor%2520triggers%252C%2520allowing%2520for%2520precise%2520model%250Aweight%2520pruning%2520of%2520backdoor%2520activations%2520without%2520sacrificing%2520benign%2520model%250Aknowledge.%2520Extensive%2520evaluations%2520across%2520four%2520widely%2520used%2520datasets%2520demonstrate%250Athat%2520DeTrigger%2520achieves%2520up%2520to%2520251x%2520faster%2520detection%2520than%2520traditional%2520methods%250Aand%2520mitigates%2520backdoor%2520attacks%2520by%2520up%2520to%252098.9%2525%252C%2520with%2520minimal%2520impact%2520on%2520global%250Amodel%2520accuracy.%2520Our%2520findings%2520establish%2520DeTrigger%2520as%2520a%2520robust%2520and%2520scalable%250Asolution%2520to%2520protect%2520federated%2520learning%2520environments%2520against%2520sophisticated%250Abackdoor%2520threats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12220v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeTrigger%3A%20A%20Gradient-Centric%20Approach%20to%20Backdoor%20Attack%20Mitigation%20in%0A%20%20Federated%20Learning&entry.906535625=Kichang%20Lee%20and%20Yujin%20Shin%20and%20Jonghyuk%20Yun%20and%20Songkuk%20Kim%20and%20Jun%20Han%20and%20JeongGil%20Ko&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20devices%20while%20preserving%20local%20data%20privacy%2C%20making%20it%20ideal%20for%0Amobile%20and%20embedded%20systems.%20However%2C%20the%20decentralized%20nature%20of%20FL%20also%20opens%0Avulnerabilities%20to%20model%20poisoning%20attacks%2C%20particularly%20backdoor%20attacks%2C%0Awhere%20adversaries%20implant%20trigger%20patterns%20to%20manipulate%20model%20predictions.%20In%0Athis%20paper%2C%20we%20propose%20DeTrigger%2C%20a%20scalable%20and%20efficient%20backdoor-robust%0Afederated%20learning%20framework%20that%20leverages%20insights%20from%20adversarial%20attack%0Amethodologies.%20By%20employing%20gradient%20analysis%20with%20temperature%20scaling%2C%0ADeTrigger%20detects%20and%20isolates%20backdoor%20triggers%2C%20allowing%20for%20precise%20model%0Aweight%20pruning%20of%20backdoor%20activations%20without%20sacrificing%20benign%20model%0Aknowledge.%20Extensive%20evaluations%20across%20four%20widely%20used%20datasets%20demonstrate%0Athat%20DeTrigger%20achieves%20up%20to%20251x%20faster%20detection%20than%20traditional%20methods%0Aand%20mitigates%20backdoor%20attacks%20by%20up%20to%2098.9%25%2C%20with%20minimal%20impact%20on%20global%0Amodel%20accuracy.%20Our%20findings%20establish%20DeTrigger%20as%20a%20robust%20and%20scalable%0Asolution%20to%20protect%20federated%20learning%20environments%20against%20sophisticated%0Abackdoor%20threats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12220v2&entry.124074799=Read"},
{"title": "CTBENCH: A Library and Benchmark for Certified Training", "author": "Yuhao Mao and Stefan Balauca and Martin Vechev", "abstract": "  Training certifiably robust neural networks is an important but challenging\ntask. While many algorithms for (deterministic) certified training have been\nproposed, they are often evaluated on different training schedules,\ncertification methods, and systematically under-tuned hyperparameters, making\nit difficult to compare their performance. To address this challenge, we\nintroduce CTBench, a unified library and a high-quality benchmark for certified\ntraining that evaluates all algorithms under fair settings and systematically\ntuned hyperparameters. We show that (1) almost all algorithms in CTBench\nsurpass the corresponding reported performance in literature in the magnitude\nof algorithmic improvements, thus establishing new state-of-the-art, and (2)\nthe claimed advantage of recent algorithms drops significantly when we enhance\nthe outdated baselines with a fair training schedule, a fair certification\nmethod and well-tuned hyperparameters. Based on CTBench, we provide new\ninsights into the current state of certified training, including (1) certified\nmodels have less fragmented loss surface, (2) certified models share many\nmistakes, (3) certified models have more sparse activations, (4) reducing\nregularization cleverly is crucial for certified training especially for large\nradii and (5) certified training has the potential to improve\nout-of-distribution generalization. We are confident that CTBench will serve as\na benchmark and testbed for future research in certified training.\n", "link": "http://arxiv.org/abs/2406.04848v3", "date": "2025-02-03", "relevancy": 1.9723, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.536}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4634}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTBENCH%3A%20A%20Library%20and%20Benchmark%20for%20Certified%20Training&body=Title%3A%20CTBENCH%3A%20A%20Library%20and%20Benchmark%20for%20Certified%20Training%0AAuthor%3A%20Yuhao%20Mao%20and%20Stefan%20Balauca%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20Training%20certifiably%20robust%20neural%20networks%20is%20an%20important%20but%20challenging%0Atask.%20While%20many%20algorithms%20for%20%28deterministic%29%20certified%20training%20have%20been%0Aproposed%2C%20they%20are%20often%20evaluated%20on%20different%20training%20schedules%2C%0Acertification%20methods%2C%20and%20systematically%20under-tuned%20hyperparameters%2C%20making%0Ait%20difficult%20to%20compare%20their%20performance.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20CTBench%2C%20a%20unified%20library%20and%20a%20high-quality%20benchmark%20for%20certified%0Atraining%20that%20evaluates%20all%20algorithms%20under%20fair%20settings%20and%20systematically%0Atuned%20hyperparameters.%20We%20show%20that%20%281%29%20almost%20all%20algorithms%20in%20CTBench%0Asurpass%20the%20corresponding%20reported%20performance%20in%20literature%20in%20the%20magnitude%0Aof%20algorithmic%20improvements%2C%20thus%20establishing%20new%20state-of-the-art%2C%20and%20%282%29%0Athe%20claimed%20advantage%20of%20recent%20algorithms%20drops%20significantly%20when%20we%20enhance%0Athe%20outdated%20baselines%20with%20a%20fair%20training%20schedule%2C%20a%20fair%20certification%0Amethod%20and%20well-tuned%20hyperparameters.%20Based%20on%20CTBench%2C%20we%20provide%20new%0Ainsights%20into%20the%20current%20state%20of%20certified%20training%2C%20including%20%281%29%20certified%0Amodels%20have%20less%20fragmented%20loss%20surface%2C%20%282%29%20certified%20models%20share%20many%0Amistakes%2C%20%283%29%20certified%20models%20have%20more%20sparse%20activations%2C%20%284%29%20reducing%0Aregularization%20cleverly%20is%20crucial%20for%20certified%20training%20especially%20for%20large%0Aradii%20and%20%285%29%20certified%20training%20has%20the%20potential%20to%20improve%0Aout-of-distribution%20generalization.%20We%20are%20confident%20that%20CTBench%20will%20serve%20as%0Aa%20benchmark%20and%20testbed%20for%20future%20research%20in%20certified%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04848v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTBENCH%253A%2520A%2520Library%2520and%2520Benchmark%2520for%2520Certified%2520Training%26entry.906535625%3DYuhao%2520Mao%2520and%2520Stefan%2520Balauca%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520Training%2520certifiably%2520robust%2520neural%2520networks%2520is%2520an%2520important%2520but%2520challenging%250Atask.%2520While%2520many%2520algorithms%2520for%2520%2528deterministic%2529%2520certified%2520training%2520have%2520been%250Aproposed%252C%2520they%2520are%2520often%2520evaluated%2520on%2520different%2520training%2520schedules%252C%250Acertification%2520methods%252C%2520and%2520systematically%2520under-tuned%2520hyperparameters%252C%2520making%250Ait%2520difficult%2520to%2520compare%2520their%2520performance.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aintroduce%2520CTBench%252C%2520a%2520unified%2520library%2520and%2520a%2520high-quality%2520benchmark%2520for%2520certified%250Atraining%2520that%2520evaluates%2520all%2520algorithms%2520under%2520fair%2520settings%2520and%2520systematically%250Atuned%2520hyperparameters.%2520We%2520show%2520that%2520%25281%2529%2520almost%2520all%2520algorithms%2520in%2520CTBench%250Asurpass%2520the%2520corresponding%2520reported%2520performance%2520in%2520literature%2520in%2520the%2520magnitude%250Aof%2520algorithmic%2520improvements%252C%2520thus%2520establishing%2520new%2520state-of-the-art%252C%2520and%2520%25282%2529%250Athe%2520claimed%2520advantage%2520of%2520recent%2520algorithms%2520drops%2520significantly%2520when%2520we%2520enhance%250Athe%2520outdated%2520baselines%2520with%2520a%2520fair%2520training%2520schedule%252C%2520a%2520fair%2520certification%250Amethod%2520and%2520well-tuned%2520hyperparameters.%2520Based%2520on%2520CTBench%252C%2520we%2520provide%2520new%250Ainsights%2520into%2520the%2520current%2520state%2520of%2520certified%2520training%252C%2520including%2520%25281%2529%2520certified%250Amodels%2520have%2520less%2520fragmented%2520loss%2520surface%252C%2520%25282%2529%2520certified%2520models%2520share%2520many%250Amistakes%252C%2520%25283%2529%2520certified%2520models%2520have%2520more%2520sparse%2520activations%252C%2520%25284%2529%2520reducing%250Aregularization%2520cleverly%2520is%2520crucial%2520for%2520certified%2520training%2520especially%2520for%2520large%250Aradii%2520and%2520%25285%2529%2520certified%2520training%2520has%2520the%2520potential%2520to%2520improve%250Aout-of-distribution%2520generalization.%2520We%2520are%2520confident%2520that%2520CTBench%2520will%2520serve%2520as%250Aa%2520benchmark%2520and%2520testbed%2520for%2520future%2520research%2520in%2520certified%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04848v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTBENCH%3A%20A%20Library%20and%20Benchmark%20for%20Certified%20Training&entry.906535625=Yuhao%20Mao%20and%20Stefan%20Balauca%20and%20Martin%20Vechev&entry.1292438233=%20%20Training%20certifiably%20robust%20neural%20networks%20is%20an%20important%20but%20challenging%0Atask.%20While%20many%20algorithms%20for%20%28deterministic%29%20certified%20training%20have%20been%0Aproposed%2C%20they%20are%20often%20evaluated%20on%20different%20training%20schedules%2C%0Acertification%20methods%2C%20and%20systematically%20under-tuned%20hyperparameters%2C%20making%0Ait%20difficult%20to%20compare%20their%20performance.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20CTBench%2C%20a%20unified%20library%20and%20a%20high-quality%20benchmark%20for%20certified%0Atraining%20that%20evaluates%20all%20algorithms%20under%20fair%20settings%20and%20systematically%0Atuned%20hyperparameters.%20We%20show%20that%20%281%29%20almost%20all%20algorithms%20in%20CTBench%0Asurpass%20the%20corresponding%20reported%20performance%20in%20literature%20in%20the%20magnitude%0Aof%20algorithmic%20improvements%2C%20thus%20establishing%20new%20state-of-the-art%2C%20and%20%282%29%0Athe%20claimed%20advantage%20of%20recent%20algorithms%20drops%20significantly%20when%20we%20enhance%0Athe%20outdated%20baselines%20with%20a%20fair%20training%20schedule%2C%20a%20fair%20certification%0Amethod%20and%20well-tuned%20hyperparameters.%20Based%20on%20CTBench%2C%20we%20provide%20new%0Ainsights%20into%20the%20current%20state%20of%20certified%20training%2C%20including%20%281%29%20certified%0Amodels%20have%20less%20fragmented%20loss%20surface%2C%20%282%29%20certified%20models%20share%20many%0Amistakes%2C%20%283%29%20certified%20models%20have%20more%20sparse%20activations%2C%20%284%29%20reducing%0Aregularization%20cleverly%20is%20crucial%20for%20certified%20training%20especially%20for%20large%0Aradii%20and%20%285%29%20certified%20training%20has%20the%20potential%20to%20improve%0Aout-of-distribution%20generalization.%20We%20are%20confident%20that%20CTBench%20will%20serve%20as%0Aa%20benchmark%20and%20testbed%20for%20future%20research%20in%20certified%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04848v3&entry.124074799=Read"},
{"title": "Stream-level flow matching with Gaussian processes", "author": "Ganchao Wei and Li Ma", "abstract": "  Flow matching (FM) is a family of training algorithms for fitting continuous\nnormalizing flows (CNFs). Conditional flow matching (CFM) exploits the fact\nthat the marginal vector field of a CNF can be learned by fitting least-squares\nregression to the conditional vector field specified given one or both ends of\nthe flow path. In this paper, we extend the CFM algorithm by defining\nconditional probability paths along ``streams'', instances of latent stochastic\npaths that connect data pairs of source and target, which are modeled with\nGaussian process (GP) distributions. The unique distributional properties of\nGPs help preserve the ``simulation-free\" nature of CFM training. We show that\nthis generalization of the CFM can effectively reduce the variance in the\nestimated marginal vector field at a moderate computational cost, thereby\nimproving the quality of the generated samples under common metrics.\nAdditionally, adopting the GP on the streams allows for flexibly linking\nmultiple correlated training data points (e.g., time series). We empirically\nvalidate our claim through both simulations and applications to image and\nneural time series data.\n", "link": "http://arxiv.org/abs/2409.20423v5", "date": "2025-02-03", "relevancy": 1.9647, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5055}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5049}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stream-level%20flow%20matching%20with%20Gaussian%20processes&body=Title%3A%20Stream-level%20flow%20matching%20with%20Gaussian%20processes%0AAuthor%3A%20Ganchao%20Wei%20and%20Li%20Ma%0AAbstract%3A%20%20%20Flow%20matching%20%28FM%29%20is%20a%20family%20of%20training%20algorithms%20for%20fitting%20continuous%0Anormalizing%20flows%20%28CNFs%29.%20Conditional%20flow%20matching%20%28CFM%29%20exploits%20the%20fact%0Athat%20the%20marginal%20vector%20field%20of%20a%20CNF%20can%20be%20learned%20by%20fitting%20least-squares%0Aregression%20to%20the%20conditional%20vector%20field%20specified%20given%20one%20or%20both%20ends%20of%0Athe%20flow%20path.%20In%20this%20paper%2C%20we%20extend%20the%20CFM%20algorithm%20by%20defining%0Aconditional%20probability%20paths%20along%20%60%60streams%27%27%2C%20instances%20of%20latent%20stochastic%0Apaths%20that%20connect%20data%20pairs%20of%20source%20and%20target%2C%20which%20are%20modeled%20with%0AGaussian%20process%20%28GP%29%20distributions.%20The%20unique%20distributional%20properties%20of%0AGPs%20help%20preserve%20the%20%60%60simulation-free%22%20nature%20of%20CFM%20training.%20We%20show%20that%0Athis%20generalization%20of%20the%20CFM%20can%20effectively%20reduce%20the%20variance%20in%20the%0Aestimated%20marginal%20vector%20field%20at%20a%20moderate%20computational%20cost%2C%20thereby%0Aimproving%20the%20quality%20of%20the%20generated%20samples%20under%20common%20metrics.%0AAdditionally%2C%20adopting%20the%20GP%20on%20the%20streams%20allows%20for%20flexibly%20linking%0Amultiple%20correlated%20training%20data%20points%20%28e.g.%2C%20time%20series%29.%20We%20empirically%0Avalidate%20our%20claim%20through%20both%20simulations%20and%20applications%20to%20image%20and%0Aneural%20time%20series%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.20423v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStream-level%2520flow%2520matching%2520with%2520Gaussian%2520processes%26entry.906535625%3DGanchao%2520Wei%2520and%2520Li%2520Ma%26entry.1292438233%3D%2520%2520Flow%2520matching%2520%2528FM%2529%2520is%2520a%2520family%2520of%2520training%2520algorithms%2520for%2520fitting%2520continuous%250Anormalizing%2520flows%2520%2528CNFs%2529.%2520Conditional%2520flow%2520matching%2520%2528CFM%2529%2520exploits%2520the%2520fact%250Athat%2520the%2520marginal%2520vector%2520field%2520of%2520a%2520CNF%2520can%2520be%2520learned%2520by%2520fitting%2520least-squares%250Aregression%2520to%2520the%2520conditional%2520vector%2520field%2520specified%2520given%2520one%2520or%2520both%2520ends%2520of%250Athe%2520flow%2520path.%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520CFM%2520algorithm%2520by%2520defining%250Aconditional%2520probability%2520paths%2520along%2520%2560%2560streams%2527%2527%252C%2520instances%2520of%2520latent%2520stochastic%250Apaths%2520that%2520connect%2520data%2520pairs%2520of%2520source%2520and%2520target%252C%2520which%2520are%2520modeled%2520with%250AGaussian%2520process%2520%2528GP%2529%2520distributions.%2520The%2520unique%2520distributional%2520properties%2520of%250AGPs%2520help%2520preserve%2520the%2520%2560%2560simulation-free%2522%2520nature%2520of%2520CFM%2520training.%2520We%2520show%2520that%250Athis%2520generalization%2520of%2520the%2520CFM%2520can%2520effectively%2520reduce%2520the%2520variance%2520in%2520the%250Aestimated%2520marginal%2520vector%2520field%2520at%2520a%2520moderate%2520computational%2520cost%252C%2520thereby%250Aimproving%2520the%2520quality%2520of%2520the%2520generated%2520samples%2520under%2520common%2520metrics.%250AAdditionally%252C%2520adopting%2520the%2520GP%2520on%2520the%2520streams%2520allows%2520for%2520flexibly%2520linking%250Amultiple%2520correlated%2520training%2520data%2520points%2520%2528e.g.%252C%2520time%2520series%2529.%2520We%2520empirically%250Avalidate%2520our%2520claim%2520through%2520both%2520simulations%2520and%2520applications%2520to%2520image%2520and%250Aneural%2520time%2520series%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20423v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stream-level%20flow%20matching%20with%20Gaussian%20processes&entry.906535625=Ganchao%20Wei%20and%20Li%20Ma&entry.1292438233=%20%20Flow%20matching%20%28FM%29%20is%20a%20family%20of%20training%20algorithms%20for%20fitting%20continuous%0Anormalizing%20flows%20%28CNFs%29.%20Conditional%20flow%20matching%20%28CFM%29%20exploits%20the%20fact%0Athat%20the%20marginal%20vector%20field%20of%20a%20CNF%20can%20be%20learned%20by%20fitting%20least-squares%0Aregression%20to%20the%20conditional%20vector%20field%20specified%20given%20one%20or%20both%20ends%20of%0Athe%20flow%20path.%20In%20this%20paper%2C%20we%20extend%20the%20CFM%20algorithm%20by%20defining%0Aconditional%20probability%20paths%20along%20%60%60streams%27%27%2C%20instances%20of%20latent%20stochastic%0Apaths%20that%20connect%20data%20pairs%20of%20source%20and%20target%2C%20which%20are%20modeled%20with%0AGaussian%20process%20%28GP%29%20distributions.%20The%20unique%20distributional%20properties%20of%0AGPs%20help%20preserve%20the%20%60%60simulation-free%22%20nature%20of%20CFM%20training.%20We%20show%20that%0Athis%20generalization%20of%20the%20CFM%20can%20effectively%20reduce%20the%20variance%20in%20the%0Aestimated%20marginal%20vector%20field%20at%20a%20moderate%20computational%20cost%2C%20thereby%0Aimproving%20the%20quality%20of%20the%20generated%20samples%20under%20common%20metrics.%0AAdditionally%2C%20adopting%20the%20GP%20on%20the%20streams%20allows%20for%20flexibly%20linking%0Amultiple%20correlated%20training%20data%20points%20%28e.g.%2C%20time%20series%29.%20We%20empirically%0Avalidate%20our%20claim%20through%20both%20simulations%20and%20applications%20to%20image%20and%0Aneural%20time%20series%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.20423v5&entry.124074799=Read"},
{"title": "An Atomic Skill Library Construction Method for Data-Efficient Embodied\n  Manipulation", "author": "Dongjiang Li and Bo Peng and Chang Li and Ning Qiao and Qi Zheng and Lei Sun and Yusen Qin and Bangguo Li and Yifeng Luan and Yibing Zhan and Mingang Sun and Tong Xu and Lusong Li and Hui Shen and Xiaodong He", "abstract": "  Embodied manipulation is a fundamental ability in the realm of embodied\nartificial intelligence. Although current embodied manipulation models show\ncertain generalizations in specific settings, they struggle in new environments\nand tasks due to the complexity and diversity of real-world scenarios. The\ntraditional end-to-end data collection and training manner leads to significant\ndata demands. Decomposing end-to-end tasks into reusable atomic skills helps\nreduce data requirements and improve task execution success rate. However,\nexisting methods are limited by predefined skill sets that cannot be\ndynamically updated. To address the issue, we introduce a three-wheeled\ndata-driven method to build an atomic skill library, which contains general\nskills enabling the execution of complex tasks. We divide tasks into subtasks\nusing the Vision-Language Planning (VLP). Then, atomic skill definitions are\nformed by abstracting the subtasks. Finally, an atomic skill library is\nconstructed via data collection and Vision-Language-Action (VLA) fine-tuning.\nAs the atomic skill library expands dynamically with the three-wheel update\nstrategy, the range of tasks it can cover grows naturally. In this way, our\nmethod shifts focus from end-to-end tasks to atomic skills, significantly\nreducing data costs while maintaining high performance and enabling efficient\nadaptation to new tasks. Extensive experiments in real-world settings\ndemonstrate the effectiveness and efficiency of our approach.\n", "link": "http://arxiv.org/abs/2501.15068v2", "date": "2025-02-03", "relevancy": 1.9534, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4742}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Atomic%20Skill%20Library%20Construction%20Method%20for%20Data-Efficient%20Embodied%0A%20%20Manipulation&body=Title%3A%20An%20Atomic%20Skill%20Library%20Construction%20Method%20for%20Data-Efficient%20Embodied%0A%20%20Manipulation%0AAuthor%3A%20Dongjiang%20Li%20and%20Bo%20Peng%20and%20Chang%20Li%20and%20Ning%20Qiao%20and%20Qi%20Zheng%20and%20Lei%20Sun%20and%20Yusen%20Qin%20and%20Bangguo%20Li%20and%20Yifeng%20Luan%20and%20Yibing%20Zhan%20and%20Mingang%20Sun%20and%20Tong%20Xu%20and%20Lusong%20Li%20and%20Hui%20Shen%20and%20Xiaodong%20He%0AAbstract%3A%20%20%20Embodied%20manipulation%20is%20a%20fundamental%20ability%20in%20the%20realm%20of%20embodied%0Aartificial%20intelligence.%20Although%20current%20embodied%20manipulation%20models%20show%0Acertain%20generalizations%20in%20specific%20settings%2C%20they%20struggle%20in%20new%20environments%0Aand%20tasks%20due%20to%20the%20complexity%20and%20diversity%20of%20real-world%20scenarios.%20The%0Atraditional%20end-to-end%20data%20collection%20and%20training%20manner%20leads%20to%20significant%0Adata%20demands.%20Decomposing%20end-to-end%20tasks%20into%20reusable%20atomic%20skills%20helps%0Areduce%20data%20requirements%20and%20improve%20task%20execution%20success%20rate.%20However%2C%0Aexisting%20methods%20are%20limited%20by%20predefined%20skill%20sets%20that%20cannot%20be%0Adynamically%20updated.%20To%20address%20the%20issue%2C%20we%20introduce%20a%20three-wheeled%0Adata-driven%20method%20to%20build%20an%20atomic%20skill%20library%2C%20which%20contains%20general%0Askills%20enabling%20the%20execution%20of%20complex%20tasks.%20We%20divide%20tasks%20into%20subtasks%0Ausing%20the%20Vision-Language%20Planning%20%28VLP%29.%20Then%2C%20atomic%20skill%20definitions%20are%0Aformed%20by%20abstracting%20the%20subtasks.%20Finally%2C%20an%20atomic%20skill%20library%20is%0Aconstructed%20via%20data%20collection%20and%20Vision-Language-Action%20%28VLA%29%20fine-tuning.%0AAs%20the%20atomic%20skill%20library%20expands%20dynamically%20with%20the%20three-wheel%20update%0Astrategy%2C%20the%20range%20of%20tasks%20it%20can%20cover%20grows%20naturally.%20In%20this%20way%2C%20our%0Amethod%20shifts%20focus%20from%20end-to-end%20tasks%20to%20atomic%20skills%2C%20significantly%0Areducing%20data%20costs%20while%20maintaining%20high%20performance%20and%20enabling%20efficient%0Aadaptation%20to%20new%20tasks.%20Extensive%20experiments%20in%20real-world%20settings%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15068v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Atomic%2520Skill%2520Library%2520Construction%2520Method%2520for%2520Data-Efficient%2520Embodied%250A%2520%2520Manipulation%26entry.906535625%3DDongjiang%2520Li%2520and%2520Bo%2520Peng%2520and%2520Chang%2520Li%2520and%2520Ning%2520Qiao%2520and%2520Qi%2520Zheng%2520and%2520Lei%2520Sun%2520and%2520Yusen%2520Qin%2520and%2520Bangguo%2520Li%2520and%2520Yifeng%2520Luan%2520and%2520Yibing%2520Zhan%2520and%2520Mingang%2520Sun%2520and%2520Tong%2520Xu%2520and%2520Lusong%2520Li%2520and%2520Hui%2520Shen%2520and%2520Xiaodong%2520He%26entry.1292438233%3D%2520%2520Embodied%2520manipulation%2520is%2520a%2520fundamental%2520ability%2520in%2520the%2520realm%2520of%2520embodied%250Aartificial%2520intelligence.%2520Although%2520current%2520embodied%2520manipulation%2520models%2520show%250Acertain%2520generalizations%2520in%2520specific%2520settings%252C%2520they%2520struggle%2520in%2520new%2520environments%250Aand%2520tasks%2520due%2520to%2520the%2520complexity%2520and%2520diversity%2520of%2520real-world%2520scenarios.%2520The%250Atraditional%2520end-to-end%2520data%2520collection%2520and%2520training%2520manner%2520leads%2520to%2520significant%250Adata%2520demands.%2520Decomposing%2520end-to-end%2520tasks%2520into%2520reusable%2520atomic%2520skills%2520helps%250Areduce%2520data%2520requirements%2520and%2520improve%2520task%2520execution%2520success%2520rate.%2520However%252C%250Aexisting%2520methods%2520are%2520limited%2520by%2520predefined%2520skill%2520sets%2520that%2520cannot%2520be%250Adynamically%2520updated.%2520To%2520address%2520the%2520issue%252C%2520we%2520introduce%2520a%2520three-wheeled%250Adata-driven%2520method%2520to%2520build%2520an%2520atomic%2520skill%2520library%252C%2520which%2520contains%2520general%250Askills%2520enabling%2520the%2520execution%2520of%2520complex%2520tasks.%2520We%2520divide%2520tasks%2520into%2520subtasks%250Ausing%2520the%2520Vision-Language%2520Planning%2520%2528VLP%2529.%2520Then%252C%2520atomic%2520skill%2520definitions%2520are%250Aformed%2520by%2520abstracting%2520the%2520subtasks.%2520Finally%252C%2520an%2520atomic%2520skill%2520library%2520is%250Aconstructed%2520via%2520data%2520collection%2520and%2520Vision-Language-Action%2520%2528VLA%2529%2520fine-tuning.%250AAs%2520the%2520atomic%2520skill%2520library%2520expands%2520dynamically%2520with%2520the%2520three-wheel%2520update%250Astrategy%252C%2520the%2520range%2520of%2520tasks%2520it%2520can%2520cover%2520grows%2520naturally.%2520In%2520this%2520way%252C%2520our%250Amethod%2520shifts%2520focus%2520from%2520end-to-end%2520tasks%2520to%2520atomic%2520skills%252C%2520significantly%250Areducing%2520data%2520costs%2520while%2520maintaining%2520high%2520performance%2520and%2520enabling%2520efficient%250Aadaptation%2520to%2520new%2520tasks.%2520Extensive%2520experiments%2520in%2520real-world%2520settings%250Ademonstrate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15068v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Atomic%20Skill%20Library%20Construction%20Method%20for%20Data-Efficient%20Embodied%0A%20%20Manipulation&entry.906535625=Dongjiang%20Li%20and%20Bo%20Peng%20and%20Chang%20Li%20and%20Ning%20Qiao%20and%20Qi%20Zheng%20and%20Lei%20Sun%20and%20Yusen%20Qin%20and%20Bangguo%20Li%20and%20Yifeng%20Luan%20and%20Yibing%20Zhan%20and%20Mingang%20Sun%20and%20Tong%20Xu%20and%20Lusong%20Li%20and%20Hui%20Shen%20and%20Xiaodong%20He&entry.1292438233=%20%20Embodied%20manipulation%20is%20a%20fundamental%20ability%20in%20the%20realm%20of%20embodied%0Aartificial%20intelligence.%20Although%20current%20embodied%20manipulation%20models%20show%0Acertain%20generalizations%20in%20specific%20settings%2C%20they%20struggle%20in%20new%20environments%0Aand%20tasks%20due%20to%20the%20complexity%20and%20diversity%20of%20real-world%20scenarios.%20The%0Atraditional%20end-to-end%20data%20collection%20and%20training%20manner%20leads%20to%20significant%0Adata%20demands.%20Decomposing%20end-to-end%20tasks%20into%20reusable%20atomic%20skills%20helps%0Areduce%20data%20requirements%20and%20improve%20task%20execution%20success%20rate.%20However%2C%0Aexisting%20methods%20are%20limited%20by%20predefined%20skill%20sets%20that%20cannot%20be%0Adynamically%20updated.%20To%20address%20the%20issue%2C%20we%20introduce%20a%20three-wheeled%0Adata-driven%20method%20to%20build%20an%20atomic%20skill%20library%2C%20which%20contains%20general%0Askills%20enabling%20the%20execution%20of%20complex%20tasks.%20We%20divide%20tasks%20into%20subtasks%0Ausing%20the%20Vision-Language%20Planning%20%28VLP%29.%20Then%2C%20atomic%20skill%20definitions%20are%0Aformed%20by%20abstracting%20the%20subtasks.%20Finally%2C%20an%20atomic%20skill%20library%20is%0Aconstructed%20via%20data%20collection%20and%20Vision-Language-Action%20%28VLA%29%20fine-tuning.%0AAs%20the%20atomic%20skill%20library%20expands%20dynamically%20with%20the%20three-wheel%20update%0Astrategy%2C%20the%20range%20of%20tasks%20it%20can%20cover%20grows%20naturally.%20In%20this%20way%2C%20our%0Amethod%20shifts%20focus%20from%20end-to-end%20tasks%20to%20atomic%20skills%2C%20significantly%0Areducing%20data%20costs%20while%20maintaining%20high%20performance%20and%20enabling%20efficient%0Aadaptation%20to%20new%20tasks.%20Extensive%20experiments%20in%20real-world%20settings%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15068v2&entry.124074799=Read"},
{"title": "Applying the maximum entropy principle to neural networks enhances\n  multi-species distribution models", "author": "Maxime Ryckewaert and Diego Marcos and Christophe Botella and Maximilien Servajean and Pierre Bonnet and Alexis Joly", "abstract": "  The rapid expansion of citizen science initiatives has led to a significant\ngrowth of biodiversity databases, and particularly presence-only (PO)\nobservations. PO data are invaluable for understanding species distributions\nand their dynamics, but their use in a Species Distribution Model (SDM) is\ncurtailed by sampling biases and the lack of information on absences. Poisson\npoint processes are widely used for SDMs, with Maxent being one of the most\npopular methods. Maxent maximises the entropy of a probability distribution\nacross sites as a function of predefined transformations of variables, called\nfeatures. In contrast, neural networks and deep learning have emerged as a\npromising technique for automatic feature extraction from complex input\nvariables. Arbitrarily complex transformations of input variables can be\nlearned from the data efficiently through backpropagation and stochastic\ngradient descent (SGD). In this paper, we propose DeepMaxent, which harnesses\nneural networks to automatically learn shared features among species, using the\nmaximum entropy principle. To do so, it employs a normalised Poisson loss where\nfor each species, presence probabilities across sites are modelled by a neural\nnetwork. We evaluate DeepMaxent on a benchmark dataset known for its spatial\nsampling biases, using PO data for calibration and presence-absence (PA) data\nfor validation across six regions with different biological groups and\ncovariates. Our results indicate that DeepMaxent performs better than Maxent\nand other leading SDMs across all regions and taxonomic groups. The method\nperforms particularly well in regions of uneven sampling, demonstrating\nsubstantial potential to increase SDM performances. In particular, our approach\nyields more accurate predictions than traditional single-species models, which\nopens up new possibilities for methodological enhancement.\n", "link": "http://arxiv.org/abs/2412.19217v3", "date": "2025-02-03", "relevancy": 1.9496, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4959}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4818}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Applying%20the%20maximum%20entropy%20principle%20to%20neural%20networks%20enhances%0A%20%20multi-species%20distribution%20models&body=Title%3A%20Applying%20the%20maximum%20entropy%20principle%20to%20neural%20networks%20enhances%0A%20%20multi-species%20distribution%20models%0AAuthor%3A%20Maxime%20Ryckewaert%20and%20Diego%20Marcos%20and%20Christophe%20Botella%20and%20Maximilien%20Servajean%20and%20Pierre%20Bonnet%20and%20Alexis%20Joly%0AAbstract%3A%20%20%20The%20rapid%20expansion%20of%20citizen%20science%20initiatives%20has%20led%20to%20a%20significant%0Agrowth%20of%20biodiversity%20databases%2C%20and%20particularly%20presence-only%20%28PO%29%0Aobservations.%20PO%20data%20are%20invaluable%20for%20understanding%20species%20distributions%0Aand%20their%20dynamics%2C%20but%20their%20use%20in%20a%20Species%20Distribution%20Model%20%28SDM%29%20is%0Acurtailed%20by%20sampling%20biases%20and%20the%20lack%20of%20information%20on%20absences.%20Poisson%0Apoint%20processes%20are%20widely%20used%20for%20SDMs%2C%20with%20Maxent%20being%20one%20of%20the%20most%0Apopular%20methods.%20Maxent%20maximises%20the%20entropy%20of%20a%20probability%20distribution%0Aacross%20sites%20as%20a%20function%20of%20predefined%20transformations%20of%20variables%2C%20called%0Afeatures.%20In%20contrast%2C%20neural%20networks%20and%20deep%20learning%20have%20emerged%20as%20a%0Apromising%20technique%20for%20automatic%20feature%20extraction%20from%20complex%20input%0Avariables.%20Arbitrarily%20complex%20transformations%20of%20input%20variables%20can%20be%0Alearned%20from%20the%20data%20efficiently%20through%20backpropagation%20and%20stochastic%0Agradient%20descent%20%28SGD%29.%20In%20this%20paper%2C%20we%20propose%20DeepMaxent%2C%20which%20harnesses%0Aneural%20networks%20to%20automatically%20learn%20shared%20features%20among%20species%2C%20using%20the%0Amaximum%20entropy%20principle.%20To%20do%20so%2C%20it%20employs%20a%20normalised%20Poisson%20loss%20where%0Afor%20each%20species%2C%20presence%20probabilities%20across%20sites%20are%20modelled%20by%20a%20neural%0Anetwork.%20We%20evaluate%20DeepMaxent%20on%20a%20benchmark%20dataset%20known%20for%20its%20spatial%0Asampling%20biases%2C%20using%20PO%20data%20for%20calibration%20and%20presence-absence%20%28PA%29%20data%0Afor%20validation%20across%20six%20regions%20with%20different%20biological%20groups%20and%0Acovariates.%20Our%20results%20indicate%20that%20DeepMaxent%20performs%20better%20than%20Maxent%0Aand%20other%20leading%20SDMs%20across%20all%20regions%20and%20taxonomic%20groups.%20The%20method%0Aperforms%20particularly%20well%20in%20regions%20of%20uneven%20sampling%2C%20demonstrating%0Asubstantial%20potential%20to%20increase%20SDM%20performances.%20In%20particular%2C%20our%20approach%0Ayields%20more%20accurate%20predictions%20than%20traditional%20single-species%20models%2C%20which%0Aopens%20up%20new%20possibilities%20for%20methodological%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19217v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplying%2520the%2520maximum%2520entropy%2520principle%2520to%2520neural%2520networks%2520enhances%250A%2520%2520multi-species%2520distribution%2520models%26entry.906535625%3DMaxime%2520Ryckewaert%2520and%2520Diego%2520Marcos%2520and%2520Christophe%2520Botella%2520and%2520Maximilien%2520Servajean%2520and%2520Pierre%2520Bonnet%2520and%2520Alexis%2520Joly%26entry.1292438233%3D%2520%2520The%2520rapid%2520expansion%2520of%2520citizen%2520science%2520initiatives%2520has%2520led%2520to%2520a%2520significant%250Agrowth%2520of%2520biodiversity%2520databases%252C%2520and%2520particularly%2520presence-only%2520%2528PO%2529%250Aobservations.%2520PO%2520data%2520are%2520invaluable%2520for%2520understanding%2520species%2520distributions%250Aand%2520their%2520dynamics%252C%2520but%2520their%2520use%2520in%2520a%2520Species%2520Distribution%2520Model%2520%2528SDM%2529%2520is%250Acurtailed%2520by%2520sampling%2520biases%2520and%2520the%2520lack%2520of%2520information%2520on%2520absences.%2520Poisson%250Apoint%2520processes%2520are%2520widely%2520used%2520for%2520SDMs%252C%2520with%2520Maxent%2520being%2520one%2520of%2520the%2520most%250Apopular%2520methods.%2520Maxent%2520maximises%2520the%2520entropy%2520of%2520a%2520probability%2520distribution%250Aacross%2520sites%2520as%2520a%2520function%2520of%2520predefined%2520transformations%2520of%2520variables%252C%2520called%250Afeatures.%2520In%2520contrast%252C%2520neural%2520networks%2520and%2520deep%2520learning%2520have%2520emerged%2520as%2520a%250Apromising%2520technique%2520for%2520automatic%2520feature%2520extraction%2520from%2520complex%2520input%250Avariables.%2520Arbitrarily%2520complex%2520transformations%2520of%2520input%2520variables%2520can%2520be%250Alearned%2520from%2520the%2520data%2520efficiently%2520through%2520backpropagation%2520and%2520stochastic%250Agradient%2520descent%2520%2528SGD%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DeepMaxent%252C%2520which%2520harnesses%250Aneural%2520networks%2520to%2520automatically%2520learn%2520shared%2520features%2520among%2520species%252C%2520using%2520the%250Amaximum%2520entropy%2520principle.%2520To%2520do%2520so%252C%2520it%2520employs%2520a%2520normalised%2520Poisson%2520loss%2520where%250Afor%2520each%2520species%252C%2520presence%2520probabilities%2520across%2520sites%2520are%2520modelled%2520by%2520a%2520neural%250Anetwork.%2520We%2520evaluate%2520DeepMaxent%2520on%2520a%2520benchmark%2520dataset%2520known%2520for%2520its%2520spatial%250Asampling%2520biases%252C%2520using%2520PO%2520data%2520for%2520calibration%2520and%2520presence-absence%2520%2528PA%2529%2520data%250Afor%2520validation%2520across%2520six%2520regions%2520with%2520different%2520biological%2520groups%2520and%250Acovariates.%2520Our%2520results%2520indicate%2520that%2520DeepMaxent%2520performs%2520better%2520than%2520Maxent%250Aand%2520other%2520leading%2520SDMs%2520across%2520all%2520regions%2520and%2520taxonomic%2520groups.%2520The%2520method%250Aperforms%2520particularly%2520well%2520in%2520regions%2520of%2520uneven%2520sampling%252C%2520demonstrating%250Asubstantial%2520potential%2520to%2520increase%2520SDM%2520performances.%2520In%2520particular%252C%2520our%2520approach%250Ayields%2520more%2520accurate%2520predictions%2520than%2520traditional%2520single-species%2520models%252C%2520which%250Aopens%2520up%2520new%2520possibilities%2520for%2520methodological%2520enhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19217v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Applying%20the%20maximum%20entropy%20principle%20to%20neural%20networks%20enhances%0A%20%20multi-species%20distribution%20models&entry.906535625=Maxime%20Ryckewaert%20and%20Diego%20Marcos%20and%20Christophe%20Botella%20and%20Maximilien%20Servajean%20and%20Pierre%20Bonnet%20and%20Alexis%20Joly&entry.1292438233=%20%20The%20rapid%20expansion%20of%20citizen%20science%20initiatives%20has%20led%20to%20a%20significant%0Agrowth%20of%20biodiversity%20databases%2C%20and%20particularly%20presence-only%20%28PO%29%0Aobservations.%20PO%20data%20are%20invaluable%20for%20understanding%20species%20distributions%0Aand%20their%20dynamics%2C%20but%20their%20use%20in%20a%20Species%20Distribution%20Model%20%28SDM%29%20is%0Acurtailed%20by%20sampling%20biases%20and%20the%20lack%20of%20information%20on%20absences.%20Poisson%0Apoint%20processes%20are%20widely%20used%20for%20SDMs%2C%20with%20Maxent%20being%20one%20of%20the%20most%0Apopular%20methods.%20Maxent%20maximises%20the%20entropy%20of%20a%20probability%20distribution%0Aacross%20sites%20as%20a%20function%20of%20predefined%20transformations%20of%20variables%2C%20called%0Afeatures.%20In%20contrast%2C%20neural%20networks%20and%20deep%20learning%20have%20emerged%20as%20a%0Apromising%20technique%20for%20automatic%20feature%20extraction%20from%20complex%20input%0Avariables.%20Arbitrarily%20complex%20transformations%20of%20input%20variables%20can%20be%0Alearned%20from%20the%20data%20efficiently%20through%20backpropagation%20and%20stochastic%0Agradient%20descent%20%28SGD%29.%20In%20this%20paper%2C%20we%20propose%20DeepMaxent%2C%20which%20harnesses%0Aneural%20networks%20to%20automatically%20learn%20shared%20features%20among%20species%2C%20using%20the%0Amaximum%20entropy%20principle.%20To%20do%20so%2C%20it%20employs%20a%20normalised%20Poisson%20loss%20where%0Afor%20each%20species%2C%20presence%20probabilities%20across%20sites%20are%20modelled%20by%20a%20neural%0Anetwork.%20We%20evaluate%20DeepMaxent%20on%20a%20benchmark%20dataset%20known%20for%20its%20spatial%0Asampling%20biases%2C%20using%20PO%20data%20for%20calibration%20and%20presence-absence%20%28PA%29%20data%0Afor%20validation%20across%20six%20regions%20with%20different%20biological%20groups%20and%0Acovariates.%20Our%20results%20indicate%20that%20DeepMaxent%20performs%20better%20than%20Maxent%0Aand%20other%20leading%20SDMs%20across%20all%20regions%20and%20taxonomic%20groups.%20The%20method%0Aperforms%20particularly%20well%20in%20regions%20of%20uneven%20sampling%2C%20demonstrating%0Asubstantial%20potential%20to%20increase%20SDM%20performances.%20In%20particular%2C%20our%20approach%0Ayields%20more%20accurate%20predictions%20than%20traditional%20single-species%20models%2C%20which%0Aopens%20up%20new%20possibilities%20for%20methodological%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19217v3&entry.124074799=Read"},
{"title": "Interpreting Outliers in Time Series Data through Decoding Autoencoder", "author": "Patrick Knab and Sascha Marton and Christian Bartelt and Robert Fuder", "abstract": "  Outlier detection is a crucial analytical tool in various fields. In critical\nsystems like manufacturing, malfunctioning outlier detection can be costly and\nsafety-critical. Therefore, there is a significant need for explainable\nartificial intelligence (XAI) when deploying opaque models in such\nenvironments. This study focuses on manufacturing time series data from a\nGerman automotive supply industry. We utilize autoencoders to compress the\nentire time series and then apply anomaly detection techniques to its latent\nfeatures. For outlier interpretation, we (i) adopt widely used XAI techniques\nto the autoencoder's encoder. Additionally, (ii) we propose AEE, Aggregated\nExplanatory Ensemble, a novel approach that fuses explanations of multiple XAI\ntechniques into a single, more expressive interpretation. For evaluation of\nexplanations, (iii) we propose a technique to measure the quality of encoder\nexplanations quantitatively. Furthermore, we qualitatively assess the\neffectiveness of outlier explanations with domain expertise.\n", "link": "http://arxiv.org/abs/2409.01713v2", "date": "2025-02-03", "relevancy": 1.9489, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5262}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Outliers%20in%20Time%20Series%20Data%20through%20Decoding%20Autoencoder&body=Title%3A%20Interpreting%20Outliers%20in%20Time%20Series%20Data%20through%20Decoding%20Autoencoder%0AAuthor%3A%20Patrick%20Knab%20and%20Sascha%20Marton%20and%20Christian%20Bartelt%20and%20Robert%20Fuder%0AAbstract%3A%20%20%20Outlier%20detection%20is%20a%20crucial%20analytical%20tool%20in%20various%20fields.%20In%20critical%0Asystems%20like%20manufacturing%2C%20malfunctioning%20outlier%20detection%20can%20be%20costly%20and%0Asafety-critical.%20Therefore%2C%20there%20is%20a%20significant%20need%20for%20explainable%0Aartificial%20intelligence%20%28XAI%29%20when%20deploying%20opaque%20models%20in%20such%0Aenvironments.%20This%20study%20focuses%20on%20manufacturing%20time%20series%20data%20from%20a%0AGerman%20automotive%20supply%20industry.%20We%20utilize%20autoencoders%20to%20compress%20the%0Aentire%20time%20series%20and%20then%20apply%20anomaly%20detection%20techniques%20to%20its%20latent%0Afeatures.%20For%20outlier%20interpretation%2C%20we%20%28i%29%20adopt%20widely%20used%20XAI%20techniques%0Ato%20the%20autoencoder%27s%20encoder.%20Additionally%2C%20%28ii%29%20we%20propose%20AEE%2C%20Aggregated%0AExplanatory%20Ensemble%2C%20a%20novel%20approach%20that%20fuses%20explanations%20of%20multiple%20XAI%0Atechniques%20into%20a%20single%2C%20more%20expressive%20interpretation.%20For%20evaluation%20of%0Aexplanations%2C%20%28iii%29%20we%20propose%20a%20technique%20to%20measure%20the%20quality%20of%20encoder%0Aexplanations%20quantitatively.%20Furthermore%2C%20we%20qualitatively%20assess%20the%0Aeffectiveness%20of%20outlier%20explanations%20with%20domain%20expertise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01713v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Outliers%2520in%2520Time%2520Series%2520Data%2520through%2520Decoding%2520Autoencoder%26entry.906535625%3DPatrick%2520Knab%2520and%2520Sascha%2520Marton%2520and%2520Christian%2520Bartelt%2520and%2520Robert%2520Fuder%26entry.1292438233%3D%2520%2520Outlier%2520detection%2520is%2520a%2520crucial%2520analytical%2520tool%2520in%2520various%2520fields.%2520In%2520critical%250Asystems%2520like%2520manufacturing%252C%2520malfunctioning%2520outlier%2520detection%2520can%2520be%2520costly%2520and%250Asafety-critical.%2520Therefore%252C%2520there%2520is%2520a%2520significant%2520need%2520for%2520explainable%250Aartificial%2520intelligence%2520%2528XAI%2529%2520when%2520deploying%2520opaque%2520models%2520in%2520such%250Aenvironments.%2520This%2520study%2520focuses%2520on%2520manufacturing%2520time%2520series%2520data%2520from%2520a%250AGerman%2520automotive%2520supply%2520industry.%2520We%2520utilize%2520autoencoders%2520to%2520compress%2520the%250Aentire%2520time%2520series%2520and%2520then%2520apply%2520anomaly%2520detection%2520techniques%2520to%2520its%2520latent%250Afeatures.%2520For%2520outlier%2520interpretation%252C%2520we%2520%2528i%2529%2520adopt%2520widely%2520used%2520XAI%2520techniques%250Ato%2520the%2520autoencoder%2527s%2520encoder.%2520Additionally%252C%2520%2528ii%2529%2520we%2520propose%2520AEE%252C%2520Aggregated%250AExplanatory%2520Ensemble%252C%2520a%2520novel%2520approach%2520that%2520fuses%2520explanations%2520of%2520multiple%2520XAI%250Atechniques%2520into%2520a%2520single%252C%2520more%2520expressive%2520interpretation.%2520For%2520evaluation%2520of%250Aexplanations%252C%2520%2528iii%2529%2520we%2520propose%2520a%2520technique%2520to%2520measure%2520the%2520quality%2520of%2520encoder%250Aexplanations%2520quantitatively.%2520Furthermore%252C%2520we%2520qualitatively%2520assess%2520the%250Aeffectiveness%2520of%2520outlier%2520explanations%2520with%2520domain%2520expertise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01713v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Outliers%20in%20Time%20Series%20Data%20through%20Decoding%20Autoencoder&entry.906535625=Patrick%20Knab%20and%20Sascha%20Marton%20and%20Christian%20Bartelt%20and%20Robert%20Fuder&entry.1292438233=%20%20Outlier%20detection%20is%20a%20crucial%20analytical%20tool%20in%20various%20fields.%20In%20critical%0Asystems%20like%20manufacturing%2C%20malfunctioning%20outlier%20detection%20can%20be%20costly%20and%0Asafety-critical.%20Therefore%2C%20there%20is%20a%20significant%20need%20for%20explainable%0Aartificial%20intelligence%20%28XAI%29%20when%20deploying%20opaque%20models%20in%20such%0Aenvironments.%20This%20study%20focuses%20on%20manufacturing%20time%20series%20data%20from%20a%0AGerman%20automotive%20supply%20industry.%20We%20utilize%20autoencoders%20to%20compress%20the%0Aentire%20time%20series%20and%20then%20apply%20anomaly%20detection%20techniques%20to%20its%20latent%0Afeatures.%20For%20outlier%20interpretation%2C%20we%20%28i%29%20adopt%20widely%20used%20XAI%20techniques%0Ato%20the%20autoencoder%27s%20encoder.%20Additionally%2C%20%28ii%29%20we%20propose%20AEE%2C%20Aggregated%0AExplanatory%20Ensemble%2C%20a%20novel%20approach%20that%20fuses%20explanations%20of%20multiple%20XAI%0Atechniques%20into%20a%20single%2C%20more%20expressive%20interpretation.%20For%20evaluation%20of%0Aexplanations%2C%20%28iii%29%20we%20propose%20a%20technique%20to%20measure%20the%20quality%20of%20encoder%0Aexplanations%20quantitatively.%20Furthermore%2C%20we%20qualitatively%20assess%20the%0Aeffectiveness%20of%20outlier%20explanations%20with%20domain%20expertise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01713v2&entry.124074799=Read"},
{"title": "Raising the Bar: Investigating the Values of Large Language Models via\n  Generative Evolving Testing", "author": "Han Jiang and Xiaoyuan Yi and Zhihua Wei and Ziang Xiao and Shu Wang and Xing Xie", "abstract": "  Warning: Contains harmful model outputs.\n  Despite significant advancements, the propensity of Large Language Models\n(LLMs) to generate harmful and unethical content poses critical challenges.\nMeasuring value alignment of LLMs becomes crucial for their regulation and\nresponsible deployment. Although numerous benchmarks have been constructed to\nassess social bias, toxicity, and ethical issues in LLMs, those static\nbenchmarks suffer from evaluation chronoeffect, in which, as models rapidly\nevolve, existing benchmarks may leak into training data or become saturated,\noverestimating ever-developing LLMs. To tackle this problem, we propose GETA, a\nnovel generative evolving testing approach based on adaptive testing methods in\nmeasurement theory. Unlike traditional adaptive testing methods that rely on a\nstatic test item pool, GETA probes the underlying moral boundaries of LLMs by\ndynamically generating test items tailored to model capability. GETA co-evolves\nwith LLMs by learning a joint distribution of item difficulty and model value\nconformity, thus effectively addressing evaluation chronoeffect. We evaluated\nvarious popular LLMs with GETA and demonstrated that 1) GETA can dynamically\ncreate difficulty-tailored test items and 2) GETA's evaluation results are more\nconsistent with models' performance on unseen OOD and i.i.d. items, laying the\ngroundwork for future evaluation paradigms.\n", "link": "http://arxiv.org/abs/2406.14230v3", "date": "2025-02-03", "relevancy": 1.9487, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.507}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4882}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Raising%20the%20Bar%3A%20Investigating%20the%20Values%20of%20Large%20Language%20Models%20via%0A%20%20Generative%20Evolving%20Testing&body=Title%3A%20Raising%20the%20Bar%3A%20Investigating%20the%20Values%20of%20Large%20Language%20Models%20via%0A%20%20Generative%20Evolving%20Testing%0AAuthor%3A%20Han%20Jiang%20and%20Xiaoyuan%20Yi%20and%20Zhihua%20Wei%20and%20Ziang%20Xiao%20and%20Shu%20Wang%20and%20Xing%20Xie%0AAbstract%3A%20%20%20Warning%3A%20Contains%20harmful%20model%20outputs.%0A%20%20Despite%20significant%20advancements%2C%20the%20propensity%20of%20Large%20Language%20Models%0A%28LLMs%29%20to%20generate%20harmful%20and%20unethical%20content%20poses%20critical%20challenges.%0AMeasuring%20value%20alignment%20of%20LLMs%20becomes%20crucial%20for%20their%20regulation%20and%0Aresponsible%20deployment.%20Although%20numerous%20benchmarks%20have%20been%20constructed%20to%0Aassess%20social%20bias%2C%20toxicity%2C%20and%20ethical%20issues%20in%20LLMs%2C%20those%20static%0Abenchmarks%20suffer%20from%20evaluation%20chronoeffect%2C%20in%20which%2C%20as%20models%20rapidly%0Aevolve%2C%20existing%20benchmarks%20may%20leak%20into%20training%20data%20or%20become%20saturated%2C%0Aoverestimating%20ever-developing%20LLMs.%20To%20tackle%20this%20problem%2C%20we%20propose%20GETA%2C%20a%0Anovel%20generative%20evolving%20testing%20approach%20based%20on%20adaptive%20testing%20methods%20in%0Ameasurement%20theory.%20Unlike%20traditional%20adaptive%20testing%20methods%20that%20rely%20on%20a%0Astatic%20test%20item%20pool%2C%20GETA%20probes%20the%20underlying%20moral%20boundaries%20of%20LLMs%20by%0Adynamically%20generating%20test%20items%20tailored%20to%20model%20capability.%20GETA%20co-evolves%0Awith%20LLMs%20by%20learning%20a%20joint%20distribution%20of%20item%20difficulty%20and%20model%20value%0Aconformity%2C%20thus%20effectively%20addressing%20evaluation%20chronoeffect.%20We%20evaluated%0Avarious%20popular%20LLMs%20with%20GETA%20and%20demonstrated%20that%201%29%20GETA%20can%20dynamically%0Acreate%20difficulty-tailored%20test%20items%20and%202%29%20GETA%27s%20evaluation%20results%20are%20more%0Aconsistent%20with%20models%27%20performance%20on%20unseen%20OOD%20and%20i.i.d.%20items%2C%20laying%20the%0Agroundwork%20for%20future%20evaluation%20paradigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14230v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaising%2520the%2520Bar%253A%2520Investigating%2520the%2520Values%2520of%2520Large%2520Language%2520Models%2520via%250A%2520%2520Generative%2520Evolving%2520Testing%26entry.906535625%3DHan%2520Jiang%2520and%2520Xiaoyuan%2520Yi%2520and%2520Zhihua%2520Wei%2520and%2520Ziang%2520Xiao%2520and%2520Shu%2520Wang%2520and%2520Xing%2520Xie%26entry.1292438233%3D%2520%2520Warning%253A%2520Contains%2520harmful%2520model%2520outputs.%250A%2520%2520Despite%2520significant%2520advancements%252C%2520the%2520propensity%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520to%2520generate%2520harmful%2520and%2520unethical%2520content%2520poses%2520critical%2520challenges.%250AMeasuring%2520value%2520alignment%2520of%2520LLMs%2520becomes%2520crucial%2520for%2520their%2520regulation%2520and%250Aresponsible%2520deployment.%2520Although%2520numerous%2520benchmarks%2520have%2520been%2520constructed%2520to%250Aassess%2520social%2520bias%252C%2520toxicity%252C%2520and%2520ethical%2520issues%2520in%2520LLMs%252C%2520those%2520static%250Abenchmarks%2520suffer%2520from%2520evaluation%2520chronoeffect%252C%2520in%2520which%252C%2520as%2520models%2520rapidly%250Aevolve%252C%2520existing%2520benchmarks%2520may%2520leak%2520into%2520training%2520data%2520or%2520become%2520saturated%252C%250Aoverestimating%2520ever-developing%2520LLMs.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520GETA%252C%2520a%250Anovel%2520generative%2520evolving%2520testing%2520approach%2520based%2520on%2520adaptive%2520testing%2520methods%2520in%250Ameasurement%2520theory.%2520Unlike%2520traditional%2520adaptive%2520testing%2520methods%2520that%2520rely%2520on%2520a%250Astatic%2520test%2520item%2520pool%252C%2520GETA%2520probes%2520the%2520underlying%2520moral%2520boundaries%2520of%2520LLMs%2520by%250Adynamically%2520generating%2520test%2520items%2520tailored%2520to%2520model%2520capability.%2520GETA%2520co-evolves%250Awith%2520LLMs%2520by%2520learning%2520a%2520joint%2520distribution%2520of%2520item%2520difficulty%2520and%2520model%2520value%250Aconformity%252C%2520thus%2520effectively%2520addressing%2520evaluation%2520chronoeffect.%2520We%2520evaluated%250Avarious%2520popular%2520LLMs%2520with%2520GETA%2520and%2520demonstrated%2520that%25201%2529%2520GETA%2520can%2520dynamically%250Acreate%2520difficulty-tailored%2520test%2520items%2520and%25202%2529%2520GETA%2527s%2520evaluation%2520results%2520are%2520more%250Aconsistent%2520with%2520models%2527%2520performance%2520on%2520unseen%2520OOD%2520and%2520i.i.d.%2520items%252C%2520laying%2520the%250Agroundwork%2520for%2520future%2520evaluation%2520paradigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14230v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Raising%20the%20Bar%3A%20Investigating%20the%20Values%20of%20Large%20Language%20Models%20via%0A%20%20Generative%20Evolving%20Testing&entry.906535625=Han%20Jiang%20and%20Xiaoyuan%20Yi%20and%20Zhihua%20Wei%20and%20Ziang%20Xiao%20and%20Shu%20Wang%20and%20Xing%20Xie&entry.1292438233=%20%20Warning%3A%20Contains%20harmful%20model%20outputs.%0A%20%20Despite%20significant%20advancements%2C%20the%20propensity%20of%20Large%20Language%20Models%0A%28LLMs%29%20to%20generate%20harmful%20and%20unethical%20content%20poses%20critical%20challenges.%0AMeasuring%20value%20alignment%20of%20LLMs%20becomes%20crucial%20for%20their%20regulation%20and%0Aresponsible%20deployment.%20Although%20numerous%20benchmarks%20have%20been%20constructed%20to%0Aassess%20social%20bias%2C%20toxicity%2C%20and%20ethical%20issues%20in%20LLMs%2C%20those%20static%0Abenchmarks%20suffer%20from%20evaluation%20chronoeffect%2C%20in%20which%2C%20as%20models%20rapidly%0Aevolve%2C%20existing%20benchmarks%20may%20leak%20into%20training%20data%20or%20become%20saturated%2C%0Aoverestimating%20ever-developing%20LLMs.%20To%20tackle%20this%20problem%2C%20we%20propose%20GETA%2C%20a%0Anovel%20generative%20evolving%20testing%20approach%20based%20on%20adaptive%20testing%20methods%20in%0Ameasurement%20theory.%20Unlike%20traditional%20adaptive%20testing%20methods%20that%20rely%20on%20a%0Astatic%20test%20item%20pool%2C%20GETA%20probes%20the%20underlying%20moral%20boundaries%20of%20LLMs%20by%0Adynamically%20generating%20test%20items%20tailored%20to%20model%20capability.%20GETA%20co-evolves%0Awith%20LLMs%20by%20learning%20a%20joint%20distribution%20of%20item%20difficulty%20and%20model%20value%0Aconformity%2C%20thus%20effectively%20addressing%20evaluation%20chronoeffect.%20We%20evaluated%0Avarious%20popular%20LLMs%20with%20GETA%20and%20demonstrated%20that%201%29%20GETA%20can%20dynamically%0Acreate%20difficulty-tailored%20test%20items%20and%202%29%20GETA%27s%20evaluation%20results%20are%20more%0Aconsistent%20with%20models%27%20performance%20on%20unseen%20OOD%20and%20i.i.d.%20items%2C%20laying%20the%0Agroundwork%20for%20future%20evaluation%20paradigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14230v3&entry.124074799=Read"},
{"title": "Large Language Models are Advanced Anonymizers", "author": "Robin Staab and Mark Vero and Mislav Balunovi\u0107 and Martin Vechev", "abstract": "  Recent privacy research on large language models (LLMs) has shown that they\nachieve near-human-level performance at inferring personal data from online\ntexts. With ever-increasing model capabilities, existing text anonymization\nmethods are currently lacking behind regulatory requirements and adversarial\nthreats. In this work, we take two steps to bridge this gap: First, we present\na new setting for evaluating anonymization in the face of adversarial LLM\ninferences, allowing for a natural measurement of anonymization performance\nwhile remedying some of the shortcomings of previous metrics. Then, within this\nsetting, we develop a novel LLM-based adversarial anonymization framework\nleveraging the strong inferential capabilities of LLMs to inform our\nanonymization procedure. We conduct a comprehensive experimental evaluation of\nadversarial anonymization across 13 LLMs on real-world and synthetic online\ntexts, comparing it against multiple baselines and industry-grade anonymizers.\nOur evaluation shows that adversarial anonymization outperforms current\ncommercial anonymizers both in terms of the resulting utility and privacy. We\nsupport our findings with a human study (n=50) highlighting a strong and\nconsistent human preference for LLM-anonymized texts.\n", "link": "http://arxiv.org/abs/2402.13846v2", "date": "2025-02-03", "relevancy": 1.9308, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20are%20Advanced%20Anonymizers&body=Title%3A%20Large%20Language%20Models%20are%20Advanced%20Anonymizers%0AAuthor%3A%20Robin%20Staab%20and%20Mark%20Vero%20and%20Mislav%20Balunovi%C4%87%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20Recent%20privacy%20research%20on%20large%20language%20models%20%28LLMs%29%20has%20shown%20that%20they%0Aachieve%20near-human-level%20performance%20at%20inferring%20personal%20data%20from%20online%0Atexts.%20With%20ever-increasing%20model%20capabilities%2C%20existing%20text%20anonymization%0Amethods%20are%20currently%20lacking%20behind%20regulatory%20requirements%20and%20adversarial%0Athreats.%20In%20this%20work%2C%20we%20take%20two%20steps%20to%20bridge%20this%20gap%3A%20First%2C%20we%20present%0Aa%20new%20setting%20for%20evaluating%20anonymization%20in%20the%20face%20of%20adversarial%20LLM%0Ainferences%2C%20allowing%20for%20a%20natural%20measurement%20of%20anonymization%20performance%0Awhile%20remedying%20some%20of%20the%20shortcomings%20of%20previous%20metrics.%20Then%2C%20within%20this%0Asetting%2C%20we%20develop%20a%20novel%20LLM-based%20adversarial%20anonymization%20framework%0Aleveraging%20the%20strong%20inferential%20capabilities%20of%20LLMs%20to%20inform%20our%0Aanonymization%20procedure.%20We%20conduct%20a%20comprehensive%20experimental%20evaluation%20of%0Aadversarial%20anonymization%20across%2013%20LLMs%20on%20real-world%20and%20synthetic%20online%0Atexts%2C%20comparing%20it%20against%20multiple%20baselines%20and%20industry-grade%20anonymizers.%0AOur%20evaluation%20shows%20that%20adversarial%20anonymization%20outperforms%20current%0Acommercial%20anonymizers%20both%20in%20terms%20of%20the%20resulting%20utility%20and%20privacy.%20We%0Asupport%20our%20findings%20with%20a%20human%20study%20%28n%3D50%29%20highlighting%20a%20strong%20and%0Aconsistent%20human%20preference%20for%20LLM-anonymized%20texts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520are%2520Advanced%2520Anonymizers%26entry.906535625%3DRobin%2520Staab%2520and%2520Mark%2520Vero%2520and%2520Mislav%2520Balunovi%25C4%2587%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520Recent%2520privacy%2520research%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520shown%2520that%2520they%250Aachieve%2520near-human-level%2520performance%2520at%2520inferring%2520personal%2520data%2520from%2520online%250Atexts.%2520With%2520ever-increasing%2520model%2520capabilities%252C%2520existing%2520text%2520anonymization%250Amethods%2520are%2520currently%2520lacking%2520behind%2520regulatory%2520requirements%2520and%2520adversarial%250Athreats.%2520In%2520this%2520work%252C%2520we%2520take%2520two%2520steps%2520to%2520bridge%2520this%2520gap%253A%2520First%252C%2520we%2520present%250Aa%2520new%2520setting%2520for%2520evaluating%2520anonymization%2520in%2520the%2520face%2520of%2520adversarial%2520LLM%250Ainferences%252C%2520allowing%2520for%2520a%2520natural%2520measurement%2520of%2520anonymization%2520performance%250Awhile%2520remedying%2520some%2520of%2520the%2520shortcomings%2520of%2520previous%2520metrics.%2520Then%252C%2520within%2520this%250Asetting%252C%2520we%2520develop%2520a%2520novel%2520LLM-based%2520adversarial%2520anonymization%2520framework%250Aleveraging%2520the%2520strong%2520inferential%2520capabilities%2520of%2520LLMs%2520to%2520inform%2520our%250Aanonymization%2520procedure.%2520We%2520conduct%2520a%2520comprehensive%2520experimental%2520evaluation%2520of%250Aadversarial%2520anonymization%2520across%252013%2520LLMs%2520on%2520real-world%2520and%2520synthetic%2520online%250Atexts%252C%2520comparing%2520it%2520against%2520multiple%2520baselines%2520and%2520industry-grade%2520anonymizers.%250AOur%2520evaluation%2520shows%2520that%2520adversarial%2520anonymization%2520outperforms%2520current%250Acommercial%2520anonymizers%2520both%2520in%2520terms%2520of%2520the%2520resulting%2520utility%2520and%2520privacy.%2520We%250Asupport%2520our%2520findings%2520with%2520a%2520human%2520study%2520%2528n%253D50%2529%2520highlighting%2520a%2520strong%2520and%250Aconsistent%2520human%2520preference%2520for%2520LLM-anonymized%2520texts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20are%20Advanced%20Anonymizers&entry.906535625=Robin%20Staab%20and%20Mark%20Vero%20and%20Mislav%20Balunovi%C4%87%20and%20Martin%20Vechev&entry.1292438233=%20%20Recent%20privacy%20research%20on%20large%20language%20models%20%28LLMs%29%20has%20shown%20that%20they%0Aachieve%20near-human-level%20performance%20at%20inferring%20personal%20data%20from%20online%0Atexts.%20With%20ever-increasing%20model%20capabilities%2C%20existing%20text%20anonymization%0Amethods%20are%20currently%20lacking%20behind%20regulatory%20requirements%20and%20adversarial%0Athreats.%20In%20this%20work%2C%20we%20take%20two%20steps%20to%20bridge%20this%20gap%3A%20First%2C%20we%20present%0Aa%20new%20setting%20for%20evaluating%20anonymization%20in%20the%20face%20of%20adversarial%20LLM%0Ainferences%2C%20allowing%20for%20a%20natural%20measurement%20of%20anonymization%20performance%0Awhile%20remedying%20some%20of%20the%20shortcomings%20of%20previous%20metrics.%20Then%2C%20within%20this%0Asetting%2C%20we%20develop%20a%20novel%20LLM-based%20adversarial%20anonymization%20framework%0Aleveraging%20the%20strong%20inferential%20capabilities%20of%20LLMs%20to%20inform%20our%0Aanonymization%20procedure.%20We%20conduct%20a%20comprehensive%20experimental%20evaluation%20of%0Aadversarial%20anonymization%20across%2013%20LLMs%20on%20real-world%20and%20synthetic%20online%0Atexts%2C%20comparing%20it%20against%20multiple%20baselines%20and%20industry-grade%20anonymizers.%0AOur%20evaluation%20shows%20that%20adversarial%20anonymization%20outperforms%20current%0Acommercial%20anonymizers%20both%20in%20terms%20of%20the%20resulting%20utility%20and%20privacy.%20We%0Asupport%20our%20findings%20with%20a%20human%20study%20%28n%3D50%29%20highlighting%20a%20strong%20and%0Aconsistent%20human%20preference%20for%20LLM-anonymized%20texts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13846v2&entry.124074799=Read"},
{"title": "De-singularity Subgradient for the $q$-th-Powered $\\ell_p$-Norm Weber\n  Location Problem", "author": "Zhao-Rong Lai and Xiaotian Wu and Liangda Fang and Ziliang Chen and Cheng Li", "abstract": "  The Weber location problem is widely used in several artificial intelligence\nscenarios. However, the gradient of the objective does not exist at a\nconsiderable set of singular points. Recently, a de-singularity subgradient\nmethod has been proposed to fix this problem, but it can only handle the\n$q$-th-powered $\\ell_2$-norm case ($1\\leqslant q<2$), which has only finite\nsingular points. In this paper, we further establish the de-singularity\nsubgradient for the $q$-th-powered $\\ell_p$-norm case with $1\\leqslant\nq\\leqslant p$ and $1\\leqslant p<2$, which includes all the rest unsolved\nsituations in this problem. This is a challenging task because the singular set\nis a continuum. The geometry of the objective function is also complicated so\nthat the characterizations of the subgradients, minimum and descent direction\nare very difficult. We develop a $q$-th-powered $\\ell_p$-norm Weiszfeld\nAlgorithm without Singularity ($q$P$p$NWAWS) for this problem, which ensures\nconvergence and the descent property of the objective function. Extensive\nexperiments on six real-world data sets demonstrate that $q$P$p$NWAWS\nsuccessfully solves the singularity problem and achieves a linear computational\nconvergence rate in practical scenarios.\n", "link": "http://arxiv.org/abs/2412.15546v2", "date": "2025-02-03", "relevancy": 1.9281, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4005}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.3784}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20De-singularity%20Subgradient%20for%20the%20%24q%24-th-Powered%20%24%5Cell_p%24-Norm%20Weber%0A%20%20Location%20Problem&body=Title%3A%20De-singularity%20Subgradient%20for%20the%20%24q%24-th-Powered%20%24%5Cell_p%24-Norm%20Weber%0A%20%20Location%20Problem%0AAuthor%3A%20Zhao-Rong%20Lai%20and%20Xiaotian%20Wu%20and%20Liangda%20Fang%20and%20Ziliang%20Chen%20and%20Cheng%20Li%0AAbstract%3A%20%20%20The%20Weber%20location%20problem%20is%20widely%20used%20in%20several%20artificial%20intelligence%0Ascenarios.%20However%2C%20the%20gradient%20of%20the%20objective%20does%20not%20exist%20at%20a%0Aconsiderable%20set%20of%20singular%20points.%20Recently%2C%20a%20de-singularity%20subgradient%0Amethod%20has%20been%20proposed%20to%20fix%20this%20problem%2C%20but%20it%20can%20only%20handle%20the%0A%24q%24-th-powered%20%24%5Cell_2%24-norm%20case%20%28%241%5Cleqslant%20q%3C2%24%29%2C%20which%20has%20only%20finite%0Asingular%20points.%20In%20this%20paper%2C%20we%20further%20establish%20the%20de-singularity%0Asubgradient%20for%20the%20%24q%24-th-powered%20%24%5Cell_p%24-norm%20case%20with%20%241%5Cleqslant%0Aq%5Cleqslant%20p%24%20and%20%241%5Cleqslant%20p%3C2%24%2C%20which%20includes%20all%20the%20rest%20unsolved%0Asituations%20in%20this%20problem.%20This%20is%20a%20challenging%20task%20because%20the%20singular%20set%0Ais%20a%20continuum.%20The%20geometry%20of%20the%20objective%20function%20is%20also%20complicated%20so%0Athat%20the%20characterizations%20of%20the%20subgradients%2C%20minimum%20and%20descent%20direction%0Aare%20very%20difficult.%20We%20develop%20a%20%24q%24-th-powered%20%24%5Cell_p%24-norm%20Weiszfeld%0AAlgorithm%20without%20Singularity%20%28%24q%24P%24p%24NWAWS%29%20for%20this%20problem%2C%20which%20ensures%0Aconvergence%20and%20the%20descent%20property%20of%20the%20objective%20function.%20Extensive%0Aexperiments%20on%20six%20real-world%20data%20sets%20demonstrate%20that%20%24q%24P%24p%24NWAWS%0Asuccessfully%20solves%20the%20singularity%20problem%20and%20achieves%20a%20linear%20computational%0Aconvergence%20rate%20in%20practical%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15546v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDe-singularity%2520Subgradient%2520for%2520the%2520%2524q%2524-th-Powered%2520%2524%255Cell_p%2524-Norm%2520Weber%250A%2520%2520Location%2520Problem%26entry.906535625%3DZhao-Rong%2520Lai%2520and%2520Xiaotian%2520Wu%2520and%2520Liangda%2520Fang%2520and%2520Ziliang%2520Chen%2520and%2520Cheng%2520Li%26entry.1292438233%3D%2520%2520The%2520Weber%2520location%2520problem%2520is%2520widely%2520used%2520in%2520several%2520artificial%2520intelligence%250Ascenarios.%2520However%252C%2520the%2520gradient%2520of%2520the%2520objective%2520does%2520not%2520exist%2520at%2520a%250Aconsiderable%2520set%2520of%2520singular%2520points.%2520Recently%252C%2520a%2520de-singularity%2520subgradient%250Amethod%2520has%2520been%2520proposed%2520to%2520fix%2520this%2520problem%252C%2520but%2520it%2520can%2520only%2520handle%2520the%250A%2524q%2524-th-powered%2520%2524%255Cell_2%2524-norm%2520case%2520%2528%25241%255Cleqslant%2520q%253C2%2524%2529%252C%2520which%2520has%2520only%2520finite%250Asingular%2520points.%2520In%2520this%2520paper%252C%2520we%2520further%2520establish%2520the%2520de-singularity%250Asubgradient%2520for%2520the%2520%2524q%2524-th-powered%2520%2524%255Cell_p%2524-norm%2520case%2520with%2520%25241%255Cleqslant%250Aq%255Cleqslant%2520p%2524%2520and%2520%25241%255Cleqslant%2520p%253C2%2524%252C%2520which%2520includes%2520all%2520the%2520rest%2520unsolved%250Asituations%2520in%2520this%2520problem.%2520This%2520is%2520a%2520challenging%2520task%2520because%2520the%2520singular%2520set%250Ais%2520a%2520continuum.%2520The%2520geometry%2520of%2520the%2520objective%2520function%2520is%2520also%2520complicated%2520so%250Athat%2520the%2520characterizations%2520of%2520the%2520subgradients%252C%2520minimum%2520and%2520descent%2520direction%250Aare%2520very%2520difficult.%2520We%2520develop%2520a%2520%2524q%2524-th-powered%2520%2524%255Cell_p%2524-norm%2520Weiszfeld%250AAlgorithm%2520without%2520Singularity%2520%2528%2524q%2524P%2524p%2524NWAWS%2529%2520for%2520this%2520problem%252C%2520which%2520ensures%250Aconvergence%2520and%2520the%2520descent%2520property%2520of%2520the%2520objective%2520function.%2520Extensive%250Aexperiments%2520on%2520six%2520real-world%2520data%2520sets%2520demonstrate%2520that%2520%2524q%2524P%2524p%2524NWAWS%250Asuccessfully%2520solves%2520the%2520singularity%2520problem%2520and%2520achieves%2520a%2520linear%2520computational%250Aconvergence%2520rate%2520in%2520practical%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15546v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=De-singularity%20Subgradient%20for%20the%20%24q%24-th-Powered%20%24%5Cell_p%24-Norm%20Weber%0A%20%20Location%20Problem&entry.906535625=Zhao-Rong%20Lai%20and%20Xiaotian%20Wu%20and%20Liangda%20Fang%20and%20Ziliang%20Chen%20and%20Cheng%20Li&entry.1292438233=%20%20The%20Weber%20location%20problem%20is%20widely%20used%20in%20several%20artificial%20intelligence%0Ascenarios.%20However%2C%20the%20gradient%20of%20the%20objective%20does%20not%20exist%20at%20a%0Aconsiderable%20set%20of%20singular%20points.%20Recently%2C%20a%20de-singularity%20subgradient%0Amethod%20has%20been%20proposed%20to%20fix%20this%20problem%2C%20but%20it%20can%20only%20handle%20the%0A%24q%24-th-powered%20%24%5Cell_2%24-norm%20case%20%28%241%5Cleqslant%20q%3C2%24%29%2C%20which%20has%20only%20finite%0Asingular%20points.%20In%20this%20paper%2C%20we%20further%20establish%20the%20de-singularity%0Asubgradient%20for%20the%20%24q%24-th-powered%20%24%5Cell_p%24-norm%20case%20with%20%241%5Cleqslant%0Aq%5Cleqslant%20p%24%20and%20%241%5Cleqslant%20p%3C2%24%2C%20which%20includes%20all%20the%20rest%20unsolved%0Asituations%20in%20this%20problem.%20This%20is%20a%20challenging%20task%20because%20the%20singular%20set%0Ais%20a%20continuum.%20The%20geometry%20of%20the%20objective%20function%20is%20also%20complicated%20so%0Athat%20the%20characterizations%20of%20the%20subgradients%2C%20minimum%20and%20descent%20direction%0Aare%20very%20difficult.%20We%20develop%20a%20%24q%24-th-powered%20%24%5Cell_p%24-norm%20Weiszfeld%0AAlgorithm%20without%20Singularity%20%28%24q%24P%24p%24NWAWS%29%20for%20this%20problem%2C%20which%20ensures%0Aconvergence%20and%20the%20descent%20property%20of%20the%20objective%20function.%20Extensive%0Aexperiments%20on%20six%20real-world%20data%20sets%20demonstrate%20that%20%24q%24P%24p%24NWAWS%0Asuccessfully%20solves%20the%20singularity%20problem%20and%20achieves%20a%20linear%20computational%0Aconvergence%20rate%20in%20practical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15546v2&entry.124074799=Read"},
{"title": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking\n  Suite for the EU Artificial Intelligence Act", "author": "Philipp Guldimann and Alexander Spiridonov and Robin Staab and Nikola Jovanovi\u0107 and Mark Vero and Velko Vechev and Anna-Maria Gueorguieva and Mislav Balunovi\u0107 and Nikola Konstantinov and Pavol Bielik and Petar Tsankov and Martin Vechev", "abstract": "  The EU's Artificial Intelligence Act (AI Act) is a significant step towards\nresponsible AI development, but lacks clear technical interpretation, making it\ndifficult to assess models' compliance. This work presents COMPL-AI, a\ncomprehensive framework consisting of (i) the first technical interpretation of\nthe EU AI Act, translating its broad regulatory requirements into measurable\ntechnical requirements, with the focus on large language models (LLMs), and\n(ii) an open-source Act-centered benchmarking suite, based on thorough\nsurveying and implementation of state-of-the-art LLM benchmarks. By evaluating\n12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in\nexisting models and benchmarks, particularly in areas like robustness, safety,\ndiversity, and fairness. This work highlights the need for a shift in focus\ntowards these aspects, encouraging balanced development of LLMs and more\ncomprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the\nfirst time demonstrates the possibilities and difficulties of bringing the\nAct's obligations to a more concrete, technical level. As such, our work can\nserve as a useful first step towards having actionable recommendations for\nmodel providers, and contributes to ongoing efforts of the EU to enable\napplication of the Act, such as the drafting of the GPAI Code of Practice.\n", "link": "http://arxiv.org/abs/2410.07959v2", "date": "2025-02-03", "relevancy": 1.9246, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMPL-AI%20Framework%3A%20A%20Technical%20Interpretation%20and%20LLM%20Benchmarking%0A%20%20Suite%20for%20the%20EU%20Artificial%20Intelligence%20Act&body=Title%3A%20COMPL-AI%20Framework%3A%20A%20Technical%20Interpretation%20and%20LLM%20Benchmarking%0A%20%20Suite%20for%20the%20EU%20Artificial%20Intelligence%20Act%0AAuthor%3A%20Philipp%20Guldimann%20and%20Alexander%20Spiridonov%20and%20Robin%20Staab%20and%20Nikola%20Jovanovi%C4%87%20and%20Mark%20Vero%20and%20Velko%20Vechev%20and%20Anna-Maria%20Gueorguieva%20and%20Mislav%20Balunovi%C4%87%20and%20Nikola%20Konstantinov%20and%20Pavol%20Bielik%20and%20Petar%20Tsankov%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20The%20EU%27s%20Artificial%20Intelligence%20Act%20%28AI%20Act%29%20is%20a%20significant%20step%20towards%0Aresponsible%20AI%20development%2C%20but%20lacks%20clear%20technical%20interpretation%2C%20making%20it%0Adifficult%20to%20assess%20models%27%20compliance.%20This%20work%20presents%20COMPL-AI%2C%20a%0Acomprehensive%20framework%20consisting%20of%20%28i%29%20the%20first%20technical%20interpretation%20of%0Athe%20EU%20AI%20Act%2C%20translating%20its%20broad%20regulatory%20requirements%20into%20measurable%0Atechnical%20requirements%2C%20with%20the%20focus%20on%20large%20language%20models%20%28LLMs%29%2C%20and%0A%28ii%29%20an%20open-source%20Act-centered%20benchmarking%20suite%2C%20based%20on%20thorough%0Asurveying%20and%20implementation%20of%20state-of-the-art%20LLM%20benchmarks.%20By%20evaluating%0A12%20prominent%20LLMs%20in%20the%20context%20of%20COMPL-AI%2C%20we%20reveal%20shortcomings%20in%0Aexisting%20models%20and%20benchmarks%2C%20particularly%20in%20areas%20like%20robustness%2C%20safety%2C%0Adiversity%2C%20and%20fairness.%20This%20work%20highlights%20the%20need%20for%20a%20shift%20in%20focus%0Atowards%20these%20aspects%2C%20encouraging%20balanced%20development%20of%20LLMs%20and%20more%0Acomprehensive%20regulation-aligned%20benchmarks.%20Simultaneously%2C%20COMPL-AI%20for%20the%0Afirst%20time%20demonstrates%20the%20possibilities%20and%20difficulties%20of%20bringing%20the%0AAct%27s%20obligations%20to%20a%20more%20concrete%2C%20technical%20level.%20As%20such%2C%20our%20work%20can%0Aserve%20as%20a%20useful%20first%20step%20towards%20having%20actionable%20recommendations%20for%0Amodel%20providers%2C%20and%20contributes%20to%20ongoing%20efforts%20of%20the%20EU%20to%20enable%0Aapplication%20of%20the%20Act%2C%20such%20as%20the%20drafting%20of%20the%20GPAI%20Code%20of%20Practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMPL-AI%2520Framework%253A%2520A%2520Technical%2520Interpretation%2520and%2520LLM%2520Benchmarking%250A%2520%2520Suite%2520for%2520the%2520EU%2520Artificial%2520Intelligence%2520Act%26entry.906535625%3DPhilipp%2520Guldimann%2520and%2520Alexander%2520Spiridonov%2520and%2520Robin%2520Staab%2520and%2520Nikola%2520Jovanovi%25C4%2587%2520and%2520Mark%2520Vero%2520and%2520Velko%2520Vechev%2520and%2520Anna-Maria%2520Gueorguieva%2520and%2520Mislav%2520Balunovi%25C4%2587%2520and%2520Nikola%2520Konstantinov%2520and%2520Pavol%2520Bielik%2520and%2520Petar%2520Tsankov%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520The%2520EU%2527s%2520Artificial%2520Intelligence%2520Act%2520%2528AI%2520Act%2529%2520is%2520a%2520significant%2520step%2520towards%250Aresponsible%2520AI%2520development%252C%2520but%2520lacks%2520clear%2520technical%2520interpretation%252C%2520making%2520it%250Adifficult%2520to%2520assess%2520models%2527%2520compliance.%2520This%2520work%2520presents%2520COMPL-AI%252C%2520a%250Acomprehensive%2520framework%2520consisting%2520of%2520%2528i%2529%2520the%2520first%2520technical%2520interpretation%2520of%250Athe%2520EU%2520AI%2520Act%252C%2520translating%2520its%2520broad%2520regulatory%2520requirements%2520into%2520measurable%250Atechnical%2520requirements%252C%2520with%2520the%2520focus%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520and%250A%2528ii%2529%2520an%2520open-source%2520Act-centered%2520benchmarking%2520suite%252C%2520based%2520on%2520thorough%250Asurveying%2520and%2520implementation%2520of%2520state-of-the-art%2520LLM%2520benchmarks.%2520By%2520evaluating%250A12%2520prominent%2520LLMs%2520in%2520the%2520context%2520of%2520COMPL-AI%252C%2520we%2520reveal%2520shortcomings%2520in%250Aexisting%2520models%2520and%2520benchmarks%252C%2520particularly%2520in%2520areas%2520like%2520robustness%252C%2520safety%252C%250Adiversity%252C%2520and%2520fairness.%2520This%2520work%2520highlights%2520the%2520need%2520for%2520a%2520shift%2520in%2520focus%250Atowards%2520these%2520aspects%252C%2520encouraging%2520balanced%2520development%2520of%2520LLMs%2520and%2520more%250Acomprehensive%2520regulation-aligned%2520benchmarks.%2520Simultaneously%252C%2520COMPL-AI%2520for%2520the%250Afirst%2520time%2520demonstrates%2520the%2520possibilities%2520and%2520difficulties%2520of%2520bringing%2520the%250AAct%2527s%2520obligations%2520to%2520a%2520more%2520concrete%252C%2520technical%2520level.%2520As%2520such%252C%2520our%2520work%2520can%250Aserve%2520as%2520a%2520useful%2520first%2520step%2520towards%2520having%2520actionable%2520recommendations%2520for%250Amodel%2520providers%252C%2520and%2520contributes%2520to%2520ongoing%2520efforts%2520of%2520the%2520EU%2520to%2520enable%250Aapplication%2520of%2520the%2520Act%252C%2520such%2520as%2520the%2520drafting%2520of%2520the%2520GPAI%2520Code%2520of%2520Practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMPL-AI%20Framework%3A%20A%20Technical%20Interpretation%20and%20LLM%20Benchmarking%0A%20%20Suite%20for%20the%20EU%20Artificial%20Intelligence%20Act&entry.906535625=Philipp%20Guldimann%20and%20Alexander%20Spiridonov%20and%20Robin%20Staab%20and%20Nikola%20Jovanovi%C4%87%20and%20Mark%20Vero%20and%20Velko%20Vechev%20and%20Anna-Maria%20Gueorguieva%20and%20Mislav%20Balunovi%C4%87%20and%20Nikola%20Konstantinov%20and%20Pavol%20Bielik%20and%20Petar%20Tsankov%20and%20Martin%20Vechev&entry.1292438233=%20%20The%20EU%27s%20Artificial%20Intelligence%20Act%20%28AI%20Act%29%20is%20a%20significant%20step%20towards%0Aresponsible%20AI%20development%2C%20but%20lacks%20clear%20technical%20interpretation%2C%20making%20it%0Adifficult%20to%20assess%20models%27%20compliance.%20This%20work%20presents%20COMPL-AI%2C%20a%0Acomprehensive%20framework%20consisting%20of%20%28i%29%20the%20first%20technical%20interpretation%20of%0Athe%20EU%20AI%20Act%2C%20translating%20its%20broad%20regulatory%20requirements%20into%20measurable%0Atechnical%20requirements%2C%20with%20the%20focus%20on%20large%20language%20models%20%28LLMs%29%2C%20and%0A%28ii%29%20an%20open-source%20Act-centered%20benchmarking%20suite%2C%20based%20on%20thorough%0Asurveying%20and%20implementation%20of%20state-of-the-art%20LLM%20benchmarks.%20By%20evaluating%0A12%20prominent%20LLMs%20in%20the%20context%20of%20COMPL-AI%2C%20we%20reveal%20shortcomings%20in%0Aexisting%20models%20and%20benchmarks%2C%20particularly%20in%20areas%20like%20robustness%2C%20safety%2C%0Adiversity%2C%20and%20fairness.%20This%20work%20highlights%20the%20need%20for%20a%20shift%20in%20focus%0Atowards%20these%20aspects%2C%20encouraging%20balanced%20development%20of%20LLMs%20and%20more%0Acomprehensive%20regulation-aligned%20benchmarks.%20Simultaneously%2C%20COMPL-AI%20for%20the%0Afirst%20time%20demonstrates%20the%20possibilities%20and%20difficulties%20of%20bringing%20the%0AAct%27s%20obligations%20to%20a%20more%20concrete%2C%20technical%20level.%20As%20such%2C%20our%20work%20can%0Aserve%20as%20a%20useful%20first%20step%20towards%20having%20actionable%20recommendations%20for%0Amodel%20providers%2C%20and%20contributes%20to%20ongoing%20efforts%20of%20the%20EU%20to%20enable%0Aapplication%20of%20the%20Act%2C%20such%20as%20the%20drafting%20of%20the%20GPAI%20Code%20of%20Practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07959v2&entry.124074799=Read"},
{"title": "Tazza: Shuffling Neural Network Parameters for Secure and Private\n  Federated Learning", "author": "Kichang Lee and Jaeho Jin and JaeYeon Park and Songkuk Kim and JeongGil Ko", "abstract": "  Federated learning enables decentralized model training without sharing raw\ndata, preserving data privacy. However, its vulnerability towards critical\nsecurity threats, such as gradient inversion and model poisoning by malicious\nclients, remain unresolved. Existing solutions often address these issues\nseparately, sacrificing either system robustness or model accuracy. This work\nintroduces Tazza, a secure and efficient federated learning framework that\nsimultaneously addresses both challenges. By leveraging the permutation\nequivariance and invariance properties of neural networks via weight shuffling\nand shuffled model validation, Tazza enhances resilience against diverse\npoisoning attacks, while ensuring data confidentiality and high model accuracy.\nComprehensive evaluations on various datasets and embedded platforms show that\nTazza achieves robust defense with up to 6.7x improved computational efficiency\ncompared to alternative schemes, without compromising performance.\n", "link": "http://arxiv.org/abs/2412.07454v2", "date": "2025-02-03", "relevancy": 1.9108, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4905}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4692}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tazza%3A%20Shuffling%20Neural%20Network%20Parameters%20for%20Secure%20and%20Private%0A%20%20Federated%20Learning&body=Title%3A%20Tazza%3A%20Shuffling%20Neural%20Network%20Parameters%20for%20Secure%20and%20Private%0A%20%20Federated%20Learning%0AAuthor%3A%20Kichang%20Lee%20and%20Jaeho%20Jin%20and%20JaeYeon%20Park%20and%20Songkuk%20Kim%20and%20JeongGil%20Ko%0AAbstract%3A%20%20%20Federated%20learning%20enables%20decentralized%20model%20training%20without%20sharing%20raw%0Adata%2C%20preserving%20data%20privacy.%20However%2C%20its%20vulnerability%20towards%20critical%0Asecurity%20threats%2C%20such%20as%20gradient%20inversion%20and%20model%20poisoning%20by%20malicious%0Aclients%2C%20remain%20unresolved.%20Existing%20solutions%20often%20address%20these%20issues%0Aseparately%2C%20sacrificing%20either%20system%20robustness%20or%20model%20accuracy.%20This%20work%0Aintroduces%20Tazza%2C%20a%20secure%20and%20efficient%20federated%20learning%20framework%20that%0Asimultaneously%20addresses%20both%20challenges.%20By%20leveraging%20the%20permutation%0Aequivariance%20and%20invariance%20properties%20of%20neural%20networks%20via%20weight%20shuffling%0Aand%20shuffled%20model%20validation%2C%20Tazza%20enhances%20resilience%20against%20diverse%0Apoisoning%20attacks%2C%20while%20ensuring%20data%20confidentiality%20and%20high%20model%20accuracy.%0AComprehensive%20evaluations%20on%20various%20datasets%20and%20embedded%20platforms%20show%20that%0ATazza%20achieves%20robust%20defense%20with%20up%20to%206.7x%20improved%20computational%20efficiency%0Acompared%20to%20alternative%20schemes%2C%20without%20compromising%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07454v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTazza%253A%2520Shuffling%2520Neural%2520Network%2520Parameters%2520for%2520Secure%2520and%2520Private%250A%2520%2520Federated%2520Learning%26entry.906535625%3DKichang%2520Lee%2520and%2520Jaeho%2520Jin%2520and%2520JaeYeon%2520Park%2520and%2520Songkuk%2520Kim%2520and%2520JeongGil%2520Ko%26entry.1292438233%3D%2520%2520Federated%2520learning%2520enables%2520decentralized%2520model%2520training%2520without%2520sharing%2520raw%250Adata%252C%2520preserving%2520data%2520privacy.%2520However%252C%2520its%2520vulnerability%2520towards%2520critical%250Asecurity%2520threats%252C%2520such%2520as%2520gradient%2520inversion%2520and%2520model%2520poisoning%2520by%2520malicious%250Aclients%252C%2520remain%2520unresolved.%2520Existing%2520solutions%2520often%2520address%2520these%2520issues%250Aseparately%252C%2520sacrificing%2520either%2520system%2520robustness%2520or%2520model%2520accuracy.%2520This%2520work%250Aintroduces%2520Tazza%252C%2520a%2520secure%2520and%2520efficient%2520federated%2520learning%2520framework%2520that%250Asimultaneously%2520addresses%2520both%2520challenges.%2520By%2520leveraging%2520the%2520permutation%250Aequivariance%2520and%2520invariance%2520properties%2520of%2520neural%2520networks%2520via%2520weight%2520shuffling%250Aand%2520shuffled%2520model%2520validation%252C%2520Tazza%2520enhances%2520resilience%2520against%2520diverse%250Apoisoning%2520attacks%252C%2520while%2520ensuring%2520data%2520confidentiality%2520and%2520high%2520model%2520accuracy.%250AComprehensive%2520evaluations%2520on%2520various%2520datasets%2520and%2520embedded%2520platforms%2520show%2520that%250ATazza%2520achieves%2520robust%2520defense%2520with%2520up%2520to%25206.7x%2520improved%2520computational%2520efficiency%250Acompared%2520to%2520alternative%2520schemes%252C%2520without%2520compromising%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07454v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tazza%3A%20Shuffling%20Neural%20Network%20Parameters%20for%20Secure%20and%20Private%0A%20%20Federated%20Learning&entry.906535625=Kichang%20Lee%20and%20Jaeho%20Jin%20and%20JaeYeon%20Park%20and%20Songkuk%20Kim%20and%20JeongGil%20Ko&entry.1292438233=%20%20Federated%20learning%20enables%20decentralized%20model%20training%20without%20sharing%20raw%0Adata%2C%20preserving%20data%20privacy.%20However%2C%20its%20vulnerability%20towards%20critical%0Asecurity%20threats%2C%20such%20as%20gradient%20inversion%20and%20model%20poisoning%20by%20malicious%0Aclients%2C%20remain%20unresolved.%20Existing%20solutions%20often%20address%20these%20issues%0Aseparately%2C%20sacrificing%20either%20system%20robustness%20or%20model%20accuracy.%20This%20work%0Aintroduces%20Tazza%2C%20a%20secure%20and%20efficient%20federated%20learning%20framework%20that%0Asimultaneously%20addresses%20both%20challenges.%20By%20leveraging%20the%20permutation%0Aequivariance%20and%20invariance%20properties%20of%20neural%20networks%20via%20weight%20shuffling%0Aand%20shuffled%20model%20validation%2C%20Tazza%20enhances%20resilience%20against%20diverse%0Apoisoning%20attacks%2C%20while%20ensuring%20data%20confidentiality%20and%20high%20model%20accuracy.%0AComprehensive%20evaluations%20on%20various%20datasets%20and%20embedded%20platforms%20show%20that%0ATazza%20achieves%20robust%20defense%20with%20up%20to%206.7x%20improved%20computational%20efficiency%0Acompared%20to%20alternative%20schemes%2C%20without%20compromising%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07454v2&entry.124074799=Read"},
{"title": "Harnessing Generative AI for Economic Insights", "author": "Manish Jha and Jialin Qian and Michael Weber and Baozhong Yang", "abstract": "  We use generative AI to extract managerial expectations about their economic\noutlook from over 120,000 corporate conference call transcripts. The overall\nmeasure, AI Economy Score, robustly predicts future economic indicators such as\nGDP growth, production, and employment, both in the short term and to 10\nquarters. This predictive power is incremental to that of existing measures,\nincluding survey forecasts. Moreover, industry and firm-level measures provide\nvaluable information about sector-specific and individual firm activities. Our\nfindings suggest that managerial expectations carry unique insights about\neconomic activities, with implications for both macroeconomic and microeconomic\ndecision-making.\n", "link": "http://arxiv.org/abs/2410.03897v3", "date": "2025-02-03", "relevancy": 1.9101, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5067}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4584}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Generative%20AI%20for%20Economic%20Insights&body=Title%3A%20Harnessing%20Generative%20AI%20for%20Economic%20Insights%0AAuthor%3A%20Manish%20Jha%20and%20Jialin%20Qian%20and%20Michael%20Weber%20and%20Baozhong%20Yang%0AAbstract%3A%20%20%20We%20use%20generative%20AI%20to%20extract%20managerial%20expectations%20about%20their%20economic%0Aoutlook%20from%20over%20120%2C000%20corporate%20conference%20call%20transcripts.%20The%20overall%0Ameasure%2C%20AI%20Economy%20Score%2C%20robustly%20predicts%20future%20economic%20indicators%20such%20as%0AGDP%20growth%2C%20production%2C%20and%20employment%2C%20both%20in%20the%20short%20term%20and%20to%2010%0Aquarters.%20This%20predictive%20power%20is%20incremental%20to%20that%20of%20existing%20measures%2C%0Aincluding%20survey%20forecasts.%20Moreover%2C%20industry%20and%20firm-level%20measures%20provide%0Avaluable%20information%20about%20sector-specific%20and%20individual%20firm%20activities.%20Our%0Afindings%20suggest%20that%20managerial%20expectations%20carry%20unique%20insights%20about%0Aeconomic%20activities%2C%20with%20implications%20for%20both%20macroeconomic%20and%20microeconomic%0Adecision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03897v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Generative%2520AI%2520for%2520Economic%2520Insights%26entry.906535625%3DManish%2520Jha%2520and%2520Jialin%2520Qian%2520and%2520Michael%2520Weber%2520and%2520Baozhong%2520Yang%26entry.1292438233%3D%2520%2520We%2520use%2520generative%2520AI%2520to%2520extract%2520managerial%2520expectations%2520about%2520their%2520economic%250Aoutlook%2520from%2520over%2520120%252C000%2520corporate%2520conference%2520call%2520transcripts.%2520The%2520overall%250Ameasure%252C%2520AI%2520Economy%2520Score%252C%2520robustly%2520predicts%2520future%2520economic%2520indicators%2520such%2520as%250AGDP%2520growth%252C%2520production%252C%2520and%2520employment%252C%2520both%2520in%2520the%2520short%2520term%2520and%2520to%252010%250Aquarters.%2520This%2520predictive%2520power%2520is%2520incremental%2520to%2520that%2520of%2520existing%2520measures%252C%250Aincluding%2520survey%2520forecasts.%2520Moreover%252C%2520industry%2520and%2520firm-level%2520measures%2520provide%250Avaluable%2520information%2520about%2520sector-specific%2520and%2520individual%2520firm%2520activities.%2520Our%250Afindings%2520suggest%2520that%2520managerial%2520expectations%2520carry%2520unique%2520insights%2520about%250Aeconomic%2520activities%252C%2520with%2520implications%2520for%2520both%2520macroeconomic%2520and%2520microeconomic%250Adecision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03897v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Generative%20AI%20for%20Economic%20Insights&entry.906535625=Manish%20Jha%20and%20Jialin%20Qian%20and%20Michael%20Weber%20and%20Baozhong%20Yang&entry.1292438233=%20%20We%20use%20generative%20AI%20to%20extract%20managerial%20expectations%20about%20their%20economic%0Aoutlook%20from%20over%20120%2C000%20corporate%20conference%20call%20transcripts.%20The%20overall%0Ameasure%2C%20AI%20Economy%20Score%2C%20robustly%20predicts%20future%20economic%20indicators%20such%20as%0AGDP%20growth%2C%20production%2C%20and%20employment%2C%20both%20in%20the%20short%20term%20and%20to%2010%0Aquarters.%20This%20predictive%20power%20is%20incremental%20to%20that%20of%20existing%20measures%2C%0Aincluding%20survey%20forecasts.%20Moreover%2C%20industry%20and%20firm-level%20measures%20provide%0Avaluable%20information%20about%20sector-specific%20and%20individual%20firm%20activities.%20Our%0Afindings%20suggest%20that%20managerial%20expectations%20carry%20unique%20insights%20about%0Aeconomic%20activities%2C%20with%20implications%20for%20both%20macroeconomic%20and%20microeconomic%0Adecision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03897v3&entry.124074799=Read"},
{"title": "SetPINNs: Set-based Physics-informed Neural Networks", "author": "Mayank Nagda and Phil Ostheimer and Thomas Specht and Frank Rhein and Fabian Jirasek and Stephan Mandt and Marius Kloft and Sophie Fellenz", "abstract": "  Physics-Informed Neural Networks (PINNs) solve partial differential equations\nusing deep learning. However, conventional PINNs perform pointwise predictions\nthat neglect dependencies within a domain, which may result in suboptimal\nsolutions. We introduce SetPINNs, a framework that effectively captures local\ndependencies. With a finite element-inspired sampling scheme, we partition a\ndomain into sets to model local dependencies while simultaneously enforcing\nphysical laws. We provide rigorous theoretical analysis and bounds to show that\nSetPINNs provide improved domain coverage over pointwise prediction methods.\nExtensive experiments across a range of synthetic and real-world tasks show\nimproved accuracy, efficiency, and robustness.\n", "link": "http://arxiv.org/abs/2409.20206v2", "date": "2025-02-03", "relevancy": 1.9074, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4844}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4826}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SetPINNs%3A%20Set-based%20Physics-informed%20Neural%20Networks&body=Title%3A%20SetPINNs%3A%20Set-based%20Physics-informed%20Neural%20Networks%0AAuthor%3A%20Mayank%20Nagda%20and%20Phil%20Ostheimer%20and%20Thomas%20Specht%20and%20Frank%20Rhein%20and%20Fabian%20Jirasek%20and%20Stephan%20Mandt%20and%20Marius%20Kloft%20and%20Sophie%20Fellenz%0AAbstract%3A%20%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20solve%20partial%20differential%20equations%0Ausing%20deep%20learning.%20However%2C%20conventional%20PINNs%20perform%20pointwise%20predictions%0Athat%20neglect%20dependencies%20within%20a%20domain%2C%20which%20may%20result%20in%20suboptimal%0Asolutions.%20We%20introduce%20SetPINNs%2C%20a%20framework%20that%20effectively%20captures%20local%0Adependencies.%20With%20a%20finite%20element-inspired%20sampling%20scheme%2C%20we%20partition%20a%0Adomain%20into%20sets%20to%20model%20local%20dependencies%20while%20simultaneously%20enforcing%0Aphysical%20laws.%20We%20provide%20rigorous%20theoretical%20analysis%20and%20bounds%20to%20show%20that%0ASetPINNs%20provide%20improved%20domain%20coverage%20over%20pointwise%20prediction%20methods.%0AExtensive%20experiments%20across%20a%20range%20of%20synthetic%20and%20real-world%20tasks%20show%0Aimproved%20accuracy%2C%20efficiency%2C%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.20206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSetPINNs%253A%2520Set-based%2520Physics-informed%2520Neural%2520Networks%26entry.906535625%3DMayank%2520Nagda%2520and%2520Phil%2520Ostheimer%2520and%2520Thomas%2520Specht%2520and%2520Frank%2520Rhein%2520and%2520Fabian%2520Jirasek%2520and%2520Stephan%2520Mandt%2520and%2520Marius%2520Kloft%2520and%2520Sophie%2520Fellenz%26entry.1292438233%3D%2520%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520solve%2520partial%2520differential%2520equations%250Ausing%2520deep%2520learning.%2520However%252C%2520conventional%2520PINNs%2520perform%2520pointwise%2520predictions%250Athat%2520neglect%2520dependencies%2520within%2520a%2520domain%252C%2520which%2520may%2520result%2520in%2520suboptimal%250Asolutions.%2520We%2520introduce%2520SetPINNs%252C%2520a%2520framework%2520that%2520effectively%2520captures%2520local%250Adependencies.%2520With%2520a%2520finite%2520element-inspired%2520sampling%2520scheme%252C%2520we%2520partition%2520a%250Adomain%2520into%2520sets%2520to%2520model%2520local%2520dependencies%2520while%2520simultaneously%2520enforcing%250Aphysical%2520laws.%2520We%2520provide%2520rigorous%2520theoretical%2520analysis%2520and%2520bounds%2520to%2520show%2520that%250ASetPINNs%2520provide%2520improved%2520domain%2520coverage%2520over%2520pointwise%2520prediction%2520methods.%250AExtensive%2520experiments%2520across%2520a%2520range%2520of%2520synthetic%2520and%2520real-world%2520tasks%2520show%250Aimproved%2520accuracy%252C%2520efficiency%252C%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SetPINNs%3A%20Set-based%20Physics-informed%20Neural%20Networks&entry.906535625=Mayank%20Nagda%20and%20Phil%20Ostheimer%20and%20Thomas%20Specht%20and%20Frank%20Rhein%20and%20Fabian%20Jirasek%20and%20Stephan%20Mandt%20and%20Marius%20Kloft%20and%20Sophie%20Fellenz&entry.1292438233=%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20solve%20partial%20differential%20equations%0Ausing%20deep%20learning.%20However%2C%20conventional%20PINNs%20perform%20pointwise%20predictions%0Athat%20neglect%20dependencies%20within%20a%20domain%2C%20which%20may%20result%20in%20suboptimal%0Asolutions.%20We%20introduce%20SetPINNs%2C%20a%20framework%20that%20effectively%20captures%20local%0Adependencies.%20With%20a%20finite%20element-inspired%20sampling%20scheme%2C%20we%20partition%20a%0Adomain%20into%20sets%20to%20model%20local%20dependencies%20while%20simultaneously%20enforcing%0Aphysical%20laws.%20We%20provide%20rigorous%20theoretical%20analysis%20and%20bounds%20to%20show%20that%0ASetPINNs%20provide%20improved%20domain%20coverage%20over%20pointwise%20prediction%20methods.%0AExtensive%20experiments%20across%20a%20range%20of%20synthetic%20and%20real-world%20tasks%20show%0Aimproved%20accuracy%2C%20efficiency%2C%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.20206v2&entry.124074799=Read"},
{"title": "Hybrid Quantum Neural Networks with Amplitude Encoding: Advancing\n  Recovery Rate Predictions", "author": "Ying Chen and Paul Griffin and Paolo Recchia and Zhou Lei and Hongrui Zhang", "abstract": "  Recovery rate prediction plays a pivotal role in bond investment strategies,\nenhancing risk assessment, optimizing portfolio allocation, improving pricing\naccuracy, and supporting effective credit risk management. However, forecasting\nfaces challenges like high-dimensional features, small sample sizes, and\noverfitting. We propose a hybrid Quantum Machine Learning model incorporating\nParameterized Quantum Circuits (PQC) within a neural network framework. PQCs\ninherently preserve unitarity, avoiding computationally costly orthogonality\nconstraints, while amplitude encoding enables exponential data compression,\nreducing qubit requirements logarithmically. Applied to a global dataset of\n1,725 observations (1996-2023), our method achieved superior accuracy (RMSE\n0.228) compared to classical neural networks (0.246) and quantum models with\nangle encoding (0.242), with efficient computation times. This work highlights\nthe potential of hybrid quantum-classical architectures in advancing recovery\nrate forecasting.\n", "link": "http://arxiv.org/abs/2501.15828v2", "date": "2025-02-03", "relevancy": 1.9064, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5072}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4853}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Quantum%20Neural%20Networks%20with%20Amplitude%20Encoding%3A%20Advancing%0A%20%20Recovery%20Rate%20Predictions&body=Title%3A%20Hybrid%20Quantum%20Neural%20Networks%20with%20Amplitude%20Encoding%3A%20Advancing%0A%20%20Recovery%20Rate%20Predictions%0AAuthor%3A%20Ying%20Chen%20and%20Paul%20Griffin%20and%20Paolo%20Recchia%20and%20Zhou%20Lei%20and%20Hongrui%20Zhang%0AAbstract%3A%20%20%20Recovery%20rate%20prediction%20plays%20a%20pivotal%20role%20in%20bond%20investment%20strategies%2C%0Aenhancing%20risk%20assessment%2C%20optimizing%20portfolio%20allocation%2C%20improving%20pricing%0Aaccuracy%2C%20and%20supporting%20effective%20credit%20risk%20management.%20However%2C%20forecasting%0Afaces%20challenges%20like%20high-dimensional%20features%2C%20small%20sample%20sizes%2C%20and%0Aoverfitting.%20We%20propose%20a%20hybrid%20Quantum%20Machine%20Learning%20model%20incorporating%0AParameterized%20Quantum%20Circuits%20%28PQC%29%20within%20a%20neural%20network%20framework.%20PQCs%0Ainherently%20preserve%20unitarity%2C%20avoiding%20computationally%20costly%20orthogonality%0Aconstraints%2C%20while%20amplitude%20encoding%20enables%20exponential%20data%20compression%2C%0Areducing%20qubit%20requirements%20logarithmically.%20Applied%20to%20a%20global%20dataset%20of%0A1%2C725%20observations%20%281996-2023%29%2C%20our%20method%20achieved%20superior%20accuracy%20%28RMSE%0A0.228%29%20compared%20to%20classical%20neural%20networks%20%280.246%29%20and%20quantum%20models%20with%0Aangle%20encoding%20%280.242%29%2C%20with%20efficient%20computation%20times.%20This%20work%20highlights%0Athe%20potential%20of%20hybrid%20quantum-classical%20architectures%20in%20advancing%20recovery%0Arate%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Quantum%2520Neural%2520Networks%2520with%2520Amplitude%2520Encoding%253A%2520Advancing%250A%2520%2520Recovery%2520Rate%2520Predictions%26entry.906535625%3DYing%2520Chen%2520and%2520Paul%2520Griffin%2520and%2520Paolo%2520Recchia%2520and%2520Zhou%2520Lei%2520and%2520Hongrui%2520Zhang%26entry.1292438233%3D%2520%2520Recovery%2520rate%2520prediction%2520plays%2520a%2520pivotal%2520role%2520in%2520bond%2520investment%2520strategies%252C%250Aenhancing%2520risk%2520assessment%252C%2520optimizing%2520portfolio%2520allocation%252C%2520improving%2520pricing%250Aaccuracy%252C%2520and%2520supporting%2520effective%2520credit%2520risk%2520management.%2520However%252C%2520forecasting%250Afaces%2520challenges%2520like%2520high-dimensional%2520features%252C%2520small%2520sample%2520sizes%252C%2520and%250Aoverfitting.%2520We%2520propose%2520a%2520hybrid%2520Quantum%2520Machine%2520Learning%2520model%2520incorporating%250AParameterized%2520Quantum%2520Circuits%2520%2528PQC%2529%2520within%2520a%2520neural%2520network%2520framework.%2520PQCs%250Ainherently%2520preserve%2520unitarity%252C%2520avoiding%2520computationally%2520costly%2520orthogonality%250Aconstraints%252C%2520while%2520amplitude%2520encoding%2520enables%2520exponential%2520data%2520compression%252C%250Areducing%2520qubit%2520requirements%2520logarithmically.%2520Applied%2520to%2520a%2520global%2520dataset%2520of%250A1%252C725%2520observations%2520%25281996-2023%2529%252C%2520our%2520method%2520achieved%2520superior%2520accuracy%2520%2528RMSE%250A0.228%2529%2520compared%2520to%2520classical%2520neural%2520networks%2520%25280.246%2529%2520and%2520quantum%2520models%2520with%250Aangle%2520encoding%2520%25280.242%2529%252C%2520with%2520efficient%2520computation%2520times.%2520This%2520work%2520highlights%250Athe%2520potential%2520of%2520hybrid%2520quantum-classical%2520architectures%2520in%2520advancing%2520recovery%250Arate%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Quantum%20Neural%20Networks%20with%20Amplitude%20Encoding%3A%20Advancing%0A%20%20Recovery%20Rate%20Predictions&entry.906535625=Ying%20Chen%20and%20Paul%20Griffin%20and%20Paolo%20Recchia%20and%20Zhou%20Lei%20and%20Hongrui%20Zhang&entry.1292438233=%20%20Recovery%20rate%20prediction%20plays%20a%20pivotal%20role%20in%20bond%20investment%20strategies%2C%0Aenhancing%20risk%20assessment%2C%20optimizing%20portfolio%20allocation%2C%20improving%20pricing%0Aaccuracy%2C%20and%20supporting%20effective%20credit%20risk%20management.%20However%2C%20forecasting%0Afaces%20challenges%20like%20high-dimensional%20features%2C%20small%20sample%20sizes%2C%20and%0Aoverfitting.%20We%20propose%20a%20hybrid%20Quantum%20Machine%20Learning%20model%20incorporating%0AParameterized%20Quantum%20Circuits%20%28PQC%29%20within%20a%20neural%20network%20framework.%20PQCs%0Ainherently%20preserve%20unitarity%2C%20avoiding%20computationally%20costly%20orthogonality%0Aconstraints%2C%20while%20amplitude%20encoding%20enables%20exponential%20data%20compression%2C%0Areducing%20qubit%20requirements%20logarithmically.%20Applied%20to%20a%20global%20dataset%20of%0A1%2C725%20observations%20%281996-2023%29%2C%20our%20method%20achieved%20superior%20accuracy%20%28RMSE%0A0.228%29%20compared%20to%20classical%20neural%20networks%20%280.246%29%20and%20quantum%20models%20with%0Aangle%20encoding%20%280.242%29%2C%20with%20efficient%20computation%20times.%20This%20work%20highlights%0Athe%20potential%20of%20hybrid%20quantum-classical%20architectures%20in%20advancing%20recovery%0Arate%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15828v2&entry.124074799=Read"},
{"title": "The Russian-focused embedders' exploration: ruMTEB benchmark and Russian\n  embedding model design", "author": "Artem Snegirev and Maria Tikhonova and Anna Maksimova and Alena Fenogenova and Alexander Abramov", "abstract": "  Embedding models play a crucial role in Natural Language Processing (NLP) by\ncreating text embeddings used in various tasks such as information retrieval\nand assessing semantic text similarity. This paper focuses on research related\nto embedding models in the Russian language. It introduces a new\nRussian-focused embedding model called ru-en-RoSBERTa and the ruMTEB benchmark,\nthe Russian version extending the Massive Text Embedding Benchmark (MTEB). Our\nbenchmark includes seven categories of tasks, such as semantic textual\nsimilarity, text classification, reranking, and retrieval.The research also\nassesses a representative set of Russian and multilingual models on the\nproposed benchmark. The findings indicate that the new model achieves results\nthat are on par with state-of-the-art models in Russian. We release the model\nru-en-RoSBERTa, and the ruMTEB framework comes with open-source code,\nintegration into the original framework and a public leaderboard.\n", "link": "http://arxiv.org/abs/2408.12503v2", "date": "2025-02-03", "relevancy": 1.9013, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Russian-focused%20embedders%27%20exploration%3A%20ruMTEB%20benchmark%20and%20Russian%0A%20%20embedding%20model%20design&body=Title%3A%20The%20Russian-focused%20embedders%27%20exploration%3A%20ruMTEB%20benchmark%20and%20Russian%0A%20%20embedding%20model%20design%0AAuthor%3A%20Artem%20Snegirev%20and%20Maria%20Tikhonova%20and%20Anna%20Maksimova%20and%20Alena%20Fenogenova%20and%20Alexander%20Abramov%0AAbstract%3A%20%20%20Embedding%20models%20play%20a%20crucial%20role%20in%20Natural%20Language%20Processing%20%28NLP%29%20by%0Acreating%20text%20embeddings%20used%20in%20various%20tasks%20such%20as%20information%20retrieval%0Aand%20assessing%20semantic%20text%20similarity.%20This%20paper%20focuses%20on%20research%20related%0Ato%20embedding%20models%20in%20the%20Russian%20language.%20It%20introduces%20a%20new%0ARussian-focused%20embedding%20model%20called%20ru-en-RoSBERTa%20and%20the%20ruMTEB%20benchmark%2C%0Athe%20Russian%20version%20extending%20the%20Massive%20Text%20Embedding%20Benchmark%20%28MTEB%29.%20Our%0Abenchmark%20includes%20seven%20categories%20of%20tasks%2C%20such%20as%20semantic%20textual%0Asimilarity%2C%20text%20classification%2C%20reranking%2C%20and%20retrieval.The%20research%20also%0Aassesses%20a%20representative%20set%20of%20Russian%20and%20multilingual%20models%20on%20the%0Aproposed%20benchmark.%20The%20findings%20indicate%20that%20the%20new%20model%20achieves%20results%0Athat%20are%20on%20par%20with%20state-of-the-art%20models%20in%20Russian.%20We%20release%20the%20model%0Aru-en-RoSBERTa%2C%20and%20the%20ruMTEB%20framework%20comes%20with%20open-source%20code%2C%0Aintegration%20into%20the%20original%20framework%20and%20a%20public%20leaderboard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12503v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Russian-focused%2520embedders%2527%2520exploration%253A%2520ruMTEB%2520benchmark%2520and%2520Russian%250A%2520%2520embedding%2520model%2520design%26entry.906535625%3DArtem%2520Snegirev%2520and%2520Maria%2520Tikhonova%2520and%2520Anna%2520Maksimova%2520and%2520Alena%2520Fenogenova%2520and%2520Alexander%2520Abramov%26entry.1292438233%3D%2520%2520Embedding%2520models%2520play%2520a%2520crucial%2520role%2520in%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520by%250Acreating%2520text%2520embeddings%2520used%2520in%2520various%2520tasks%2520such%2520as%2520information%2520retrieval%250Aand%2520assessing%2520semantic%2520text%2520similarity.%2520This%2520paper%2520focuses%2520on%2520research%2520related%250Ato%2520embedding%2520models%2520in%2520the%2520Russian%2520language.%2520It%2520introduces%2520a%2520new%250ARussian-focused%2520embedding%2520model%2520called%2520ru-en-RoSBERTa%2520and%2520the%2520ruMTEB%2520benchmark%252C%250Athe%2520Russian%2520version%2520extending%2520the%2520Massive%2520Text%2520Embedding%2520Benchmark%2520%2528MTEB%2529.%2520Our%250Abenchmark%2520includes%2520seven%2520categories%2520of%2520tasks%252C%2520such%2520as%2520semantic%2520textual%250Asimilarity%252C%2520text%2520classification%252C%2520reranking%252C%2520and%2520retrieval.The%2520research%2520also%250Aassesses%2520a%2520representative%2520set%2520of%2520Russian%2520and%2520multilingual%2520models%2520on%2520the%250Aproposed%2520benchmark.%2520The%2520findings%2520indicate%2520that%2520the%2520new%2520model%2520achieves%2520results%250Athat%2520are%2520on%2520par%2520with%2520state-of-the-art%2520models%2520in%2520Russian.%2520We%2520release%2520the%2520model%250Aru-en-RoSBERTa%252C%2520and%2520the%2520ruMTEB%2520framework%2520comes%2520with%2520open-source%2520code%252C%250Aintegration%2520into%2520the%2520original%2520framework%2520and%2520a%2520public%2520leaderboard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12503v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Russian-focused%20embedders%27%20exploration%3A%20ruMTEB%20benchmark%20and%20Russian%0A%20%20embedding%20model%20design&entry.906535625=Artem%20Snegirev%20and%20Maria%20Tikhonova%20and%20Anna%20Maksimova%20and%20Alena%20Fenogenova%20and%20Alexander%20Abramov&entry.1292438233=%20%20Embedding%20models%20play%20a%20crucial%20role%20in%20Natural%20Language%20Processing%20%28NLP%29%20by%0Acreating%20text%20embeddings%20used%20in%20various%20tasks%20such%20as%20information%20retrieval%0Aand%20assessing%20semantic%20text%20similarity.%20This%20paper%20focuses%20on%20research%20related%0Ato%20embedding%20models%20in%20the%20Russian%20language.%20It%20introduces%20a%20new%0ARussian-focused%20embedding%20model%20called%20ru-en-RoSBERTa%20and%20the%20ruMTEB%20benchmark%2C%0Athe%20Russian%20version%20extending%20the%20Massive%20Text%20Embedding%20Benchmark%20%28MTEB%29.%20Our%0Abenchmark%20includes%20seven%20categories%20of%20tasks%2C%20such%20as%20semantic%20textual%0Asimilarity%2C%20text%20classification%2C%20reranking%2C%20and%20retrieval.The%20research%20also%0Aassesses%20a%20representative%20set%20of%20Russian%20and%20multilingual%20models%20on%20the%0Aproposed%20benchmark.%20The%20findings%20indicate%20that%20the%20new%20model%20achieves%20results%0Athat%20are%20on%20par%20with%20state-of-the-art%20models%20in%20Russian.%20We%20release%20the%20model%0Aru-en-RoSBERTa%2C%20and%20the%20ruMTEB%20framework%20comes%20with%20open-source%20code%2C%0Aintegration%20into%20the%20original%20framework%20and%20a%20public%20leaderboard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12503v2&entry.124074799=Read"},
{"title": "Dynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach", "author": "Henrique Don\u00e2ncio and Antoine Barrier and Leah F. South and Florence Forbes", "abstract": "  In deep Reinforcement Learning (RL) models trained using gradient-based\ntechniques, the choice of optimizer and its learning rate are crucial to\nachieving good performance: higher learning rates can prevent the model from\nlearning effectively, while lower ones might slow convergence. Additionally,\ndue to the non-stationarity of the objective function, the best-performing\nlearning rate can change over the training steps. To adapt the learning rate, a\nstandard technique consists of using decay schedulers. However, these\nschedulers assume that the model is progressively approaching convergence,\nwhich may not always be true, leading to delayed or premature adjustments. In\nthis work, we propose dynamic Learning Rate for deep Reinforcement Learning\n(LRRL), a meta-learning approach that selects the learning rate based on the\nagent's performance during training. LRRL is based on a multi-armed bandit\nalgorithm, where each arm represents a different learning rate, and the bandit\nfeedback is provided by the cumulative returns of the RL policy to update the\narms' probability distribution. Our empirical results demonstrate that LRRL can\nsubstantially improve the performance of deep RL algorithms for some tasks.\n", "link": "http://arxiv.org/abs/2410.12598v2", "date": "2025-02-03", "relevancy": 1.8981, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4904}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4876}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Learning%20Rate%20for%20Deep%20Reinforcement%20Learning%3A%20A%20Bandit%20Approach&body=Title%3A%20Dynamic%20Learning%20Rate%20for%20Deep%20Reinforcement%20Learning%3A%20A%20Bandit%20Approach%0AAuthor%3A%20Henrique%20Don%C3%A2ncio%20and%20Antoine%20Barrier%20and%20Leah%20F.%20South%20and%20Florence%20Forbes%0AAbstract%3A%20%20%20In%20deep%20Reinforcement%20Learning%20%28RL%29%20models%20trained%20using%20gradient-based%0Atechniques%2C%20the%20choice%20of%20optimizer%20and%20its%20learning%20rate%20are%20crucial%20to%0Aachieving%20good%20performance%3A%20higher%20learning%20rates%20can%20prevent%20the%20model%20from%0Alearning%20effectively%2C%20while%20lower%20ones%20might%20slow%20convergence.%20Additionally%2C%0Adue%20to%20the%20non-stationarity%20of%20the%20objective%20function%2C%20the%20best-performing%0Alearning%20rate%20can%20change%20over%20the%20training%20steps.%20To%20adapt%20the%20learning%20rate%2C%20a%0Astandard%20technique%20consists%20of%20using%20decay%20schedulers.%20However%2C%20these%0Aschedulers%20assume%20that%20the%20model%20is%20progressively%20approaching%20convergence%2C%0Awhich%20may%20not%20always%20be%20true%2C%20leading%20to%20delayed%20or%20premature%20adjustments.%20In%0Athis%20work%2C%20we%20propose%20dynamic%20Learning%20Rate%20for%20deep%20Reinforcement%20Learning%0A%28LRRL%29%2C%20a%20meta-learning%20approach%20that%20selects%20the%20learning%20rate%20based%20on%20the%0Aagent%27s%20performance%20during%20training.%20LRRL%20is%20based%20on%20a%20multi-armed%20bandit%0Aalgorithm%2C%20where%20each%20arm%20represents%20a%20different%20learning%20rate%2C%20and%20the%20bandit%0Afeedback%20is%20provided%20by%20the%20cumulative%20returns%20of%20the%20RL%20policy%20to%20update%20the%0Aarms%27%20probability%20distribution.%20Our%20empirical%20results%20demonstrate%20that%20LRRL%20can%0Asubstantially%20improve%20the%20performance%20of%20deep%20RL%20algorithms%20for%20some%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12598v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Learning%2520Rate%2520for%2520Deep%2520Reinforcement%2520Learning%253A%2520A%2520Bandit%2520Approach%26entry.906535625%3DHenrique%2520Don%25C3%25A2ncio%2520and%2520Antoine%2520Barrier%2520and%2520Leah%2520F.%2520South%2520and%2520Florence%2520Forbes%26entry.1292438233%3D%2520%2520In%2520deep%2520Reinforcement%2520Learning%2520%2528RL%2529%2520models%2520trained%2520using%2520gradient-based%250Atechniques%252C%2520the%2520choice%2520of%2520optimizer%2520and%2520its%2520learning%2520rate%2520are%2520crucial%2520to%250Aachieving%2520good%2520performance%253A%2520higher%2520learning%2520rates%2520can%2520prevent%2520the%2520model%2520from%250Alearning%2520effectively%252C%2520while%2520lower%2520ones%2520might%2520slow%2520convergence.%2520Additionally%252C%250Adue%2520to%2520the%2520non-stationarity%2520of%2520the%2520objective%2520function%252C%2520the%2520best-performing%250Alearning%2520rate%2520can%2520change%2520over%2520the%2520training%2520steps.%2520To%2520adapt%2520the%2520learning%2520rate%252C%2520a%250Astandard%2520technique%2520consists%2520of%2520using%2520decay%2520schedulers.%2520However%252C%2520these%250Aschedulers%2520assume%2520that%2520the%2520model%2520is%2520progressively%2520approaching%2520convergence%252C%250Awhich%2520may%2520not%2520always%2520be%2520true%252C%2520leading%2520to%2520delayed%2520or%2520premature%2520adjustments.%2520In%250Athis%2520work%252C%2520we%2520propose%2520dynamic%2520Learning%2520Rate%2520for%2520deep%2520Reinforcement%2520Learning%250A%2528LRRL%2529%252C%2520a%2520meta-learning%2520approach%2520that%2520selects%2520the%2520learning%2520rate%2520based%2520on%2520the%250Aagent%2527s%2520performance%2520during%2520training.%2520LRRL%2520is%2520based%2520on%2520a%2520multi-armed%2520bandit%250Aalgorithm%252C%2520where%2520each%2520arm%2520represents%2520a%2520different%2520learning%2520rate%252C%2520and%2520the%2520bandit%250Afeedback%2520is%2520provided%2520by%2520the%2520cumulative%2520returns%2520of%2520the%2520RL%2520policy%2520to%2520update%2520the%250Aarms%2527%2520probability%2520distribution.%2520Our%2520empirical%2520results%2520demonstrate%2520that%2520LRRL%2520can%250Asubstantially%2520improve%2520the%2520performance%2520of%2520deep%2520RL%2520algorithms%2520for%2520some%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12598v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Learning%20Rate%20for%20Deep%20Reinforcement%20Learning%3A%20A%20Bandit%20Approach&entry.906535625=Henrique%20Don%C3%A2ncio%20and%20Antoine%20Barrier%20and%20Leah%20F.%20South%20and%20Florence%20Forbes&entry.1292438233=%20%20In%20deep%20Reinforcement%20Learning%20%28RL%29%20models%20trained%20using%20gradient-based%0Atechniques%2C%20the%20choice%20of%20optimizer%20and%20its%20learning%20rate%20are%20crucial%20to%0Aachieving%20good%20performance%3A%20higher%20learning%20rates%20can%20prevent%20the%20model%20from%0Alearning%20effectively%2C%20while%20lower%20ones%20might%20slow%20convergence.%20Additionally%2C%0Adue%20to%20the%20non-stationarity%20of%20the%20objective%20function%2C%20the%20best-performing%0Alearning%20rate%20can%20change%20over%20the%20training%20steps.%20To%20adapt%20the%20learning%20rate%2C%20a%0Astandard%20technique%20consists%20of%20using%20decay%20schedulers.%20However%2C%20these%0Aschedulers%20assume%20that%20the%20model%20is%20progressively%20approaching%20convergence%2C%0Awhich%20may%20not%20always%20be%20true%2C%20leading%20to%20delayed%20or%20premature%20adjustments.%20In%0Athis%20work%2C%20we%20propose%20dynamic%20Learning%20Rate%20for%20deep%20Reinforcement%20Learning%0A%28LRRL%29%2C%20a%20meta-learning%20approach%20that%20selects%20the%20learning%20rate%20based%20on%20the%0Aagent%27s%20performance%20during%20training.%20LRRL%20is%20based%20on%20a%20multi-armed%20bandit%0Aalgorithm%2C%20where%20each%20arm%20represents%20a%20different%20learning%20rate%2C%20and%20the%20bandit%0Afeedback%20is%20provided%20by%20the%20cumulative%20returns%20of%20the%20RL%20policy%20to%20update%20the%0Aarms%27%20probability%20distribution.%20Our%20empirical%20results%20demonstrate%20that%20LRRL%20can%0Asubstantially%20improve%20the%20performance%20of%20deep%20RL%20algorithms%20for%20some%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12598v2&entry.124074799=Read"},
{"title": "Real-Time Anomaly Detection with Synthetic Anomaly Monitoring (SAM)", "author": "Emanuele Luzio and Moacir Antonelli Ponti", "abstract": "  Anomaly detection is essential for identifying rare and significant events\nacross diverse domains such as finance, cybersecurity, and network monitoring.\nThis paper presents Synthetic Anomaly Monitoring (SAM), an innovative approach\nthat applies synthetic control methods from causal inference to improve both\nthe accuracy and interpretability of anomaly detection processes. By modeling\nnormal behavior through the treatment of each feature as a control unit, SAM\nidentifies anomalies as deviations within this causal framework. We conducted\nextensive experiments comparing SAM with established benchmark models,\nincluding Isolation Forest, Local Outlier Factor (LOF), k-Nearest Neighbors\n(kNN), and One-Class Support Vector Machine (SVM), across five diverse\ndatasets, including Credit Card Fraud, HTTP Dataset CSIC 2010, and KDD Cup\n1999, among others. Our results demonstrate that SAM consistently delivers\nrobust performance, highlighting its potential as a powerful tool for real-time\nanomaly detection in dynamic and complex environments.\n", "link": "http://arxiv.org/abs/2501.18417v2", "date": "2025-02-03", "relevancy": 1.8549, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4776}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4562}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Anomaly%20Detection%20with%20Synthetic%20Anomaly%20Monitoring%20%28SAM%29&body=Title%3A%20Real-Time%20Anomaly%20Detection%20with%20Synthetic%20Anomaly%20Monitoring%20%28SAM%29%0AAuthor%3A%20Emanuele%20Luzio%20and%20Moacir%20Antonelli%20Ponti%0AAbstract%3A%20%20%20Anomaly%20detection%20is%20essential%20for%20identifying%20rare%20and%20significant%20events%0Aacross%20diverse%20domains%20such%20as%20finance%2C%20cybersecurity%2C%20and%20network%20monitoring.%0AThis%20paper%20presents%20Synthetic%20Anomaly%20Monitoring%20%28SAM%29%2C%20an%20innovative%20approach%0Athat%20applies%20synthetic%20control%20methods%20from%20causal%20inference%20to%20improve%20both%0Athe%20accuracy%20and%20interpretability%20of%20anomaly%20detection%20processes.%20By%20modeling%0Anormal%20behavior%20through%20the%20treatment%20of%20each%20feature%20as%20a%20control%20unit%2C%20SAM%0Aidentifies%20anomalies%20as%20deviations%20within%20this%20causal%20framework.%20We%20conducted%0Aextensive%20experiments%20comparing%20SAM%20with%20established%20benchmark%20models%2C%0Aincluding%20Isolation%20Forest%2C%20Local%20Outlier%20Factor%20%28LOF%29%2C%20k-Nearest%20Neighbors%0A%28kNN%29%2C%20and%20One-Class%20Support%20Vector%20Machine%20%28SVM%29%2C%20across%20five%20diverse%0Adatasets%2C%20including%20Credit%20Card%20Fraud%2C%20HTTP%20Dataset%20CSIC%202010%2C%20and%20KDD%20Cup%0A1999%2C%20among%20others.%20Our%20results%20demonstrate%20that%20SAM%20consistently%20delivers%0Arobust%20performance%2C%20highlighting%20its%20potential%20as%20a%20powerful%20tool%20for%20real-time%0Aanomaly%20detection%20in%20dynamic%20and%20complex%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Anomaly%2520Detection%2520with%2520Synthetic%2520Anomaly%2520Monitoring%2520%2528SAM%2529%26entry.906535625%3DEmanuele%2520Luzio%2520and%2520Moacir%2520Antonelli%2520Ponti%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520is%2520essential%2520for%2520identifying%2520rare%2520and%2520significant%2520events%250Aacross%2520diverse%2520domains%2520such%2520as%2520finance%252C%2520cybersecurity%252C%2520and%2520network%2520monitoring.%250AThis%2520paper%2520presents%2520Synthetic%2520Anomaly%2520Monitoring%2520%2528SAM%2529%252C%2520an%2520innovative%2520approach%250Athat%2520applies%2520synthetic%2520control%2520methods%2520from%2520causal%2520inference%2520to%2520improve%2520both%250Athe%2520accuracy%2520and%2520interpretability%2520of%2520anomaly%2520detection%2520processes.%2520By%2520modeling%250Anormal%2520behavior%2520through%2520the%2520treatment%2520of%2520each%2520feature%2520as%2520a%2520control%2520unit%252C%2520SAM%250Aidentifies%2520anomalies%2520as%2520deviations%2520within%2520this%2520causal%2520framework.%2520We%2520conducted%250Aextensive%2520experiments%2520comparing%2520SAM%2520with%2520established%2520benchmark%2520models%252C%250Aincluding%2520Isolation%2520Forest%252C%2520Local%2520Outlier%2520Factor%2520%2528LOF%2529%252C%2520k-Nearest%2520Neighbors%250A%2528kNN%2529%252C%2520and%2520One-Class%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%252C%2520across%2520five%2520diverse%250Adatasets%252C%2520including%2520Credit%2520Card%2520Fraud%252C%2520HTTP%2520Dataset%2520CSIC%25202010%252C%2520and%2520KDD%2520Cup%250A1999%252C%2520among%2520others.%2520Our%2520results%2520demonstrate%2520that%2520SAM%2520consistently%2520delivers%250Arobust%2520performance%252C%2520highlighting%2520its%2520potential%2520as%2520a%2520powerful%2520tool%2520for%2520real-time%250Aanomaly%2520detection%2520in%2520dynamic%2520and%2520complex%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Anomaly%20Detection%20with%20Synthetic%20Anomaly%20Monitoring%20%28SAM%29&entry.906535625=Emanuele%20Luzio%20and%20Moacir%20Antonelli%20Ponti&entry.1292438233=%20%20Anomaly%20detection%20is%20essential%20for%20identifying%20rare%20and%20significant%20events%0Aacross%20diverse%20domains%20such%20as%20finance%2C%20cybersecurity%2C%20and%20network%20monitoring.%0AThis%20paper%20presents%20Synthetic%20Anomaly%20Monitoring%20%28SAM%29%2C%20an%20innovative%20approach%0Athat%20applies%20synthetic%20control%20methods%20from%20causal%20inference%20to%20improve%20both%0Athe%20accuracy%20and%20interpretability%20of%20anomaly%20detection%20processes.%20By%20modeling%0Anormal%20behavior%20through%20the%20treatment%20of%20each%20feature%20as%20a%20control%20unit%2C%20SAM%0Aidentifies%20anomalies%20as%20deviations%20within%20this%20causal%20framework.%20We%20conducted%0Aextensive%20experiments%20comparing%20SAM%20with%20established%20benchmark%20models%2C%0Aincluding%20Isolation%20Forest%2C%20Local%20Outlier%20Factor%20%28LOF%29%2C%20k-Nearest%20Neighbors%0A%28kNN%29%2C%20and%20One-Class%20Support%20Vector%20Machine%20%28SVM%29%2C%20across%20five%20diverse%0Adatasets%2C%20including%20Credit%20Card%20Fraud%2C%20HTTP%20Dataset%20CSIC%202010%2C%20and%20KDD%20Cup%0A1999%2C%20among%20others.%20Our%20results%20demonstrate%20that%20SAM%20consistently%20delivers%0Arobust%20performance%2C%20highlighting%20its%20potential%20as%20a%20powerful%20tool%20for%20real-time%0Aanomaly%20detection%20in%20dynamic%20and%20complex%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18417v2&entry.124074799=Read"},
{"title": "Scaling Up Membership Inference: When and How Attacks Succeed on Large\n  Language Models", "author": "Haritz Puerto and Martin Gubri and Sangdoo Yun and Seong Joon Oh", "abstract": "  Membership inference attacks (MIA) attempt to verify the membership of a\ngiven data sample in the training set for a model. MIA has become relevant in\nrecent years, following the rapid development of large language models (LLM).\nMany are concerned about the usage of copyrighted materials for training them\nand call for methods for detecting such usage. However, recent research has\nlargely concluded that current MIA methods do not work on LLMs. Even when they\nseem to work, it is usually because of the ill-designed experimental setup\nwhere other shortcut features enable \"cheating.\" In this work, we argue that\nMIA still works on LLMs, but only when multiple documents are presented for\ntesting. We construct new benchmarks that measure the MIA performances at a\ncontinuous scale of data samples, from sentences (n-grams) to a collection of\ndocuments (multiple chunks of tokens). To validate the efficacy of current MIA\napproaches at greater scales, we adapt a recent work on Dataset Inference (DI)\nfor the task of binary membership detection that aggregates paragraph-level MIA\nfeatures to enable MIA at document and collection of documents level. This\nbaseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.\n", "link": "http://arxiv.org/abs/2411.00154v2", "date": "2025-02-03", "relevancy": 1.8393, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4628}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4605}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Up%20Membership%20Inference%3A%20When%20and%20How%20Attacks%20Succeed%20on%20Large%0A%20%20Language%20Models&body=Title%3A%20Scaling%20Up%20Membership%20Inference%3A%20When%20and%20How%20Attacks%20Succeed%20on%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Haritz%20Puerto%20and%20Martin%20Gubri%20and%20Sangdoo%20Yun%20and%20Seong%20Joon%20Oh%0AAbstract%3A%20%20%20Membership%20inference%20attacks%20%28MIA%29%20attempt%20to%20verify%20the%20membership%20of%20a%0Agiven%20data%20sample%20in%20the%20training%20set%20for%20a%20model.%20MIA%20has%20become%20relevant%20in%0Arecent%20years%2C%20following%20the%20rapid%20development%20of%20large%20language%20models%20%28LLM%29.%0AMany%20are%20concerned%20about%20the%20usage%20of%20copyrighted%20materials%20for%20training%20them%0Aand%20call%20for%20methods%20for%20detecting%20such%20usage.%20However%2C%20recent%20research%20has%0Alargely%20concluded%20that%20current%20MIA%20methods%20do%20not%20work%20on%20LLMs.%20Even%20when%20they%0Aseem%20to%20work%2C%20it%20is%20usually%20because%20of%20the%20ill-designed%20experimental%20setup%0Awhere%20other%20shortcut%20features%20enable%20%22cheating.%22%20In%20this%20work%2C%20we%20argue%20that%0AMIA%20still%20works%20on%20LLMs%2C%20but%20only%20when%20multiple%20documents%20are%20presented%20for%0Atesting.%20We%20construct%20new%20benchmarks%20that%20measure%20the%20MIA%20performances%20at%20a%0Acontinuous%20scale%20of%20data%20samples%2C%20from%20sentences%20%28n-grams%29%20to%20a%20collection%20of%0Adocuments%20%28multiple%20chunks%20of%20tokens%29.%20To%20validate%20the%20efficacy%20of%20current%20MIA%0Aapproaches%20at%20greater%20scales%2C%20we%20adapt%20a%20recent%20work%20on%20Dataset%20Inference%20%28DI%29%0Afor%20the%20task%20of%20binary%20membership%20detection%20that%20aggregates%20paragraph-level%20MIA%0Afeatures%20to%20enable%20MIA%20at%20document%20and%20collection%20of%20documents%20level.%20This%0Abaseline%20achieves%20the%20first%20successful%20MIA%20on%20pre-trained%20and%20fine-tuned%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00154v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Up%2520Membership%2520Inference%253A%2520When%2520and%2520How%2520Attacks%2520Succeed%2520on%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DHaritz%2520Puerto%2520and%2520Martin%2520Gubri%2520and%2520Sangdoo%2520Yun%2520and%2520Seong%2520Joon%2520Oh%26entry.1292438233%3D%2520%2520Membership%2520inference%2520attacks%2520%2528MIA%2529%2520attempt%2520to%2520verify%2520the%2520membership%2520of%2520a%250Agiven%2520data%2520sample%2520in%2520the%2520training%2520set%2520for%2520a%2520model.%2520MIA%2520has%2520become%2520relevant%2520in%250Arecent%2520years%252C%2520following%2520the%2520rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLM%2529.%250AMany%2520are%2520concerned%2520about%2520the%2520usage%2520of%2520copyrighted%2520materials%2520for%2520training%2520them%250Aand%2520call%2520for%2520methods%2520for%2520detecting%2520such%2520usage.%2520However%252C%2520recent%2520research%2520has%250Alargely%2520concluded%2520that%2520current%2520MIA%2520methods%2520do%2520not%2520work%2520on%2520LLMs.%2520Even%2520when%2520they%250Aseem%2520to%2520work%252C%2520it%2520is%2520usually%2520because%2520of%2520the%2520ill-designed%2520experimental%2520setup%250Awhere%2520other%2520shortcut%2520features%2520enable%2520%2522cheating.%2522%2520In%2520this%2520work%252C%2520we%2520argue%2520that%250AMIA%2520still%2520works%2520on%2520LLMs%252C%2520but%2520only%2520when%2520multiple%2520documents%2520are%2520presented%2520for%250Atesting.%2520We%2520construct%2520new%2520benchmarks%2520that%2520measure%2520the%2520MIA%2520performances%2520at%2520a%250Acontinuous%2520scale%2520of%2520data%2520samples%252C%2520from%2520sentences%2520%2528n-grams%2529%2520to%2520a%2520collection%2520of%250Adocuments%2520%2528multiple%2520chunks%2520of%2520tokens%2529.%2520To%2520validate%2520the%2520efficacy%2520of%2520current%2520MIA%250Aapproaches%2520at%2520greater%2520scales%252C%2520we%2520adapt%2520a%2520recent%2520work%2520on%2520Dataset%2520Inference%2520%2528DI%2529%250Afor%2520the%2520task%2520of%2520binary%2520membership%2520detection%2520that%2520aggregates%2520paragraph-level%2520MIA%250Afeatures%2520to%2520enable%2520MIA%2520at%2520document%2520and%2520collection%2520of%2520documents%2520level.%2520This%250Abaseline%2520achieves%2520the%2520first%2520successful%2520MIA%2520on%2520pre-trained%2520and%2520fine-tuned%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00154v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Up%20Membership%20Inference%3A%20When%20and%20How%20Attacks%20Succeed%20on%20Large%0A%20%20Language%20Models&entry.906535625=Haritz%20Puerto%20and%20Martin%20Gubri%20and%20Sangdoo%20Yun%20and%20Seong%20Joon%20Oh&entry.1292438233=%20%20Membership%20inference%20attacks%20%28MIA%29%20attempt%20to%20verify%20the%20membership%20of%20a%0Agiven%20data%20sample%20in%20the%20training%20set%20for%20a%20model.%20MIA%20has%20become%20relevant%20in%0Arecent%20years%2C%20following%20the%20rapid%20development%20of%20large%20language%20models%20%28LLM%29.%0AMany%20are%20concerned%20about%20the%20usage%20of%20copyrighted%20materials%20for%20training%20them%0Aand%20call%20for%20methods%20for%20detecting%20such%20usage.%20However%2C%20recent%20research%20has%0Alargely%20concluded%20that%20current%20MIA%20methods%20do%20not%20work%20on%20LLMs.%20Even%20when%20they%0Aseem%20to%20work%2C%20it%20is%20usually%20because%20of%20the%20ill-designed%20experimental%20setup%0Awhere%20other%20shortcut%20features%20enable%20%22cheating.%22%20In%20this%20work%2C%20we%20argue%20that%0AMIA%20still%20works%20on%20LLMs%2C%20but%20only%20when%20multiple%20documents%20are%20presented%20for%0Atesting.%20We%20construct%20new%20benchmarks%20that%20measure%20the%20MIA%20performances%20at%20a%0Acontinuous%20scale%20of%20data%20samples%2C%20from%20sentences%20%28n-grams%29%20to%20a%20collection%20of%0Adocuments%20%28multiple%20chunks%20of%20tokens%29.%20To%20validate%20the%20efficacy%20of%20current%20MIA%0Aapproaches%20at%20greater%20scales%2C%20we%20adapt%20a%20recent%20work%20on%20Dataset%20Inference%20%28DI%29%0Afor%20the%20task%20of%20binary%20membership%20detection%20that%20aggregates%20paragraph-level%20MIA%0Afeatures%20to%20enable%20MIA%20at%20document%20and%20collection%20of%20documents%20level.%20This%0Abaseline%20achieves%20the%20first%20successful%20MIA%20on%20pre-trained%20and%20fine-tuned%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00154v2&entry.124074799=Read"},
{"title": "kNN Retrieval for Simple and Effective Zero-Shot Multi-speaker\n  Text-to-Speech", "author": "Karl El Hajal and Ajinkya Kulkarni and Enno Hermann and Mathew Magimai. -Doss", "abstract": "  While recent zero-shot multi-speaker text-to-speech (TTS) models achieve\nimpressive results, they typically rely on extensive transcribed speech\ndatasets from numerous speakers and intricate training pipelines. Meanwhile,\nself-supervised learning (SSL) speech features have emerged as effective\nintermediate representations for TTS. Further, SSL features from different\nspeakers that are linearly close share phonetic information while maintaining\nindividual speaker identity. In this study, we introduce kNN-TTS, a simple and\neffective framework for zero-shot multi-speaker TTS using retrieval methods\nwhich leverage the linear relationships between SSL features. Objective and\nsubjective evaluations show that our models, trained on transcribed speech from\na single speaker only, achieve performance comparable to state-of-the-art\nmodels that are trained on significantly larger training datasets. The low\ntraining data requirements mean that kNN-TTS is well suited for the development\nof multi-speaker TTS systems for low-resource domains and languages. We also\nintroduce an interpolation parameter which enables fine-grained voice morphing.\nDemo samples are available at https://idiap.github.io/knn-tts\n", "link": "http://arxiv.org/abs/2408.10771v3", "date": "2025-02-03", "relevancy": 1.8192, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20kNN%20Retrieval%20for%20Simple%20and%20Effective%20Zero-Shot%20Multi-speaker%0A%20%20Text-to-Speech&body=Title%3A%20kNN%20Retrieval%20for%20Simple%20and%20Effective%20Zero-Shot%20Multi-speaker%0A%20%20Text-to-Speech%0AAuthor%3A%20Karl%20El%20Hajal%20and%20Ajinkya%20Kulkarni%20and%20Enno%20Hermann%20and%20Mathew%20Magimai.%20-Doss%0AAbstract%3A%20%20%20While%20recent%20zero-shot%20multi-speaker%20text-to-speech%20%28TTS%29%20models%20achieve%0Aimpressive%20results%2C%20they%20typically%20rely%20on%20extensive%20transcribed%20speech%0Adatasets%20from%20numerous%20speakers%20and%20intricate%20training%20pipelines.%20Meanwhile%2C%0Aself-supervised%20learning%20%28SSL%29%20speech%20features%20have%20emerged%20as%20effective%0Aintermediate%20representations%20for%20TTS.%20Further%2C%20SSL%20features%20from%20different%0Aspeakers%20that%20are%20linearly%20close%20share%20phonetic%20information%20while%20maintaining%0Aindividual%20speaker%20identity.%20In%20this%20study%2C%20we%20introduce%20kNN-TTS%2C%20a%20simple%20and%0Aeffective%20framework%20for%20zero-shot%20multi-speaker%20TTS%20using%20retrieval%20methods%0Awhich%20leverage%20the%20linear%20relationships%20between%20SSL%20features.%20Objective%20and%0Asubjective%20evaluations%20show%20that%20our%20models%2C%20trained%20on%20transcribed%20speech%20from%0Aa%20single%20speaker%20only%2C%20achieve%20performance%20comparable%20to%20state-of-the-art%0Amodels%20that%20are%20trained%20on%20significantly%20larger%20training%20datasets.%20The%20low%0Atraining%20data%20requirements%20mean%20that%20kNN-TTS%20is%20well%20suited%20for%20the%20development%0Aof%20multi-speaker%20TTS%20systems%20for%20low-resource%20domains%20and%20languages.%20We%20also%0Aintroduce%20an%20interpolation%20parameter%20which%20enables%20fine-grained%20voice%20morphing.%0ADemo%20samples%20are%20available%20at%20https%3A//idiap.github.io/knn-tts%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10771v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DkNN%2520Retrieval%2520for%2520Simple%2520and%2520Effective%2520Zero-Shot%2520Multi-speaker%250A%2520%2520Text-to-Speech%26entry.906535625%3DKarl%2520El%2520Hajal%2520and%2520Ajinkya%2520Kulkarni%2520and%2520Enno%2520Hermann%2520and%2520Mathew%2520Magimai.%2520-Doss%26entry.1292438233%3D%2520%2520While%2520recent%2520zero-shot%2520multi-speaker%2520text-to-speech%2520%2528TTS%2529%2520models%2520achieve%250Aimpressive%2520results%252C%2520they%2520typically%2520rely%2520on%2520extensive%2520transcribed%2520speech%250Adatasets%2520from%2520numerous%2520speakers%2520and%2520intricate%2520training%2520pipelines.%2520Meanwhile%252C%250Aself-supervised%2520learning%2520%2528SSL%2529%2520speech%2520features%2520have%2520emerged%2520as%2520effective%250Aintermediate%2520representations%2520for%2520TTS.%2520Further%252C%2520SSL%2520features%2520from%2520different%250Aspeakers%2520that%2520are%2520linearly%2520close%2520share%2520phonetic%2520information%2520while%2520maintaining%250Aindividual%2520speaker%2520identity.%2520In%2520this%2520study%252C%2520we%2520introduce%2520kNN-TTS%252C%2520a%2520simple%2520and%250Aeffective%2520framework%2520for%2520zero-shot%2520multi-speaker%2520TTS%2520using%2520retrieval%2520methods%250Awhich%2520leverage%2520the%2520linear%2520relationships%2520between%2520SSL%2520features.%2520Objective%2520and%250Asubjective%2520evaluations%2520show%2520that%2520our%2520models%252C%2520trained%2520on%2520transcribed%2520speech%2520from%250Aa%2520single%2520speaker%2520only%252C%2520achieve%2520performance%2520comparable%2520to%2520state-of-the-art%250Amodels%2520that%2520are%2520trained%2520on%2520significantly%2520larger%2520training%2520datasets.%2520The%2520low%250Atraining%2520data%2520requirements%2520mean%2520that%2520kNN-TTS%2520is%2520well%2520suited%2520for%2520the%2520development%250Aof%2520multi-speaker%2520TTS%2520systems%2520for%2520low-resource%2520domains%2520and%2520languages.%2520We%2520also%250Aintroduce%2520an%2520interpolation%2520parameter%2520which%2520enables%2520fine-grained%2520voice%2520morphing.%250ADemo%2520samples%2520are%2520available%2520at%2520https%253A//idiap.github.io/knn-tts%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10771v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=kNN%20Retrieval%20for%20Simple%20and%20Effective%20Zero-Shot%20Multi-speaker%0A%20%20Text-to-Speech&entry.906535625=Karl%20El%20Hajal%20and%20Ajinkya%20Kulkarni%20and%20Enno%20Hermann%20and%20Mathew%20Magimai.%20-Doss&entry.1292438233=%20%20While%20recent%20zero-shot%20multi-speaker%20text-to-speech%20%28TTS%29%20models%20achieve%0Aimpressive%20results%2C%20they%20typically%20rely%20on%20extensive%20transcribed%20speech%0Adatasets%20from%20numerous%20speakers%20and%20intricate%20training%20pipelines.%20Meanwhile%2C%0Aself-supervised%20learning%20%28SSL%29%20speech%20features%20have%20emerged%20as%20effective%0Aintermediate%20representations%20for%20TTS.%20Further%2C%20SSL%20features%20from%20different%0Aspeakers%20that%20are%20linearly%20close%20share%20phonetic%20information%20while%20maintaining%0Aindividual%20speaker%20identity.%20In%20this%20study%2C%20we%20introduce%20kNN-TTS%2C%20a%20simple%20and%0Aeffective%20framework%20for%20zero-shot%20multi-speaker%20TTS%20using%20retrieval%20methods%0Awhich%20leverage%20the%20linear%20relationships%20between%20SSL%20features.%20Objective%20and%0Asubjective%20evaluations%20show%20that%20our%20models%2C%20trained%20on%20transcribed%20speech%20from%0Aa%20single%20speaker%20only%2C%20achieve%20performance%20comparable%20to%20state-of-the-art%0Amodels%20that%20are%20trained%20on%20significantly%20larger%20training%20datasets.%20The%20low%0Atraining%20data%20requirements%20mean%20that%20kNN-TTS%20is%20well%20suited%20for%20the%20development%0Aof%20multi-speaker%20TTS%20systems%20for%20low-resource%20domains%20and%20languages.%20We%20also%0Aintroduce%20an%20interpolation%20parameter%20which%20enables%20fine-grained%20voice%20morphing.%0ADemo%20samples%20are%20available%20at%20https%3A//idiap.github.io/knn-tts%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10771v3&entry.124074799=Read"},
{"title": "Understanding Model Calibration -- A gentle introduction and visual\n  exploration of calibration and the expected calibration error (ECE)", "author": "Maja Pavlovic", "abstract": "  To be considered reliable, a model must be calibrated so that its confidence\nin each decision closely reflects its true outcome. In this blogpost we'll take\na look at the most commonly used definition for calibration and then dive into\na frequently used evaluation measure for model calibration. We'll then cover\nsome of the drawbacks of this measure and how these surfaced the need for\nadditional notions of calibration, which require their own new evaluation\nmeasures. This post is not intended to be an in-depth dissection of all works\non calibration, nor does it focus on how to calibrate models. Instead, it is\nmeant to provide a gentle introduction to the different notions and their\nevaluation measures as well as to re-highlight some issues with a measure that\nis still widely used to evaluate calibration.\n", "link": "http://arxiv.org/abs/2501.19047v2", "date": "2025-02-03", "relevancy": 1.8012, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4504}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Model%20Calibration%20--%20A%20gentle%20introduction%20and%20visual%0A%20%20exploration%20of%20calibration%20and%20the%20expected%20calibration%20error%20%28ECE%29&body=Title%3A%20Understanding%20Model%20Calibration%20--%20A%20gentle%20introduction%20and%20visual%0A%20%20exploration%20of%20calibration%20and%20the%20expected%20calibration%20error%20%28ECE%29%0AAuthor%3A%20Maja%20Pavlovic%0AAbstract%3A%20%20%20To%20be%20considered%20reliable%2C%20a%20model%20must%20be%20calibrated%20so%20that%20its%20confidence%0Ain%20each%20decision%20closely%20reflects%20its%20true%20outcome.%20In%20this%20blogpost%20we%27ll%20take%0Aa%20look%20at%20the%20most%20commonly%20used%20definition%20for%20calibration%20and%20then%20dive%20into%0Aa%20frequently%20used%20evaluation%20measure%20for%20model%20calibration.%20We%27ll%20then%20cover%0Asome%20of%20the%20drawbacks%20of%20this%20measure%20and%20how%20these%20surfaced%20the%20need%20for%0Aadditional%20notions%20of%20calibration%2C%20which%20require%20their%20own%20new%20evaluation%0Ameasures.%20This%20post%20is%20not%20intended%20to%20be%20an%20in-depth%20dissection%20of%20all%20works%0Aon%20calibration%2C%20nor%20does%20it%20focus%20on%20how%20to%20calibrate%20models.%20Instead%2C%20it%20is%0Ameant%20to%20provide%20a%20gentle%20introduction%20to%20the%20different%20notions%20and%20their%0Aevaluation%20measures%20as%20well%20as%20to%20re-highlight%20some%20issues%20with%20a%20measure%20that%0Ais%20still%20widely%20used%20to%20evaluate%20calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Model%2520Calibration%2520--%2520A%2520gentle%2520introduction%2520and%2520visual%250A%2520%2520exploration%2520of%2520calibration%2520and%2520the%2520expected%2520calibration%2520error%2520%2528ECE%2529%26entry.906535625%3DMaja%2520Pavlovic%26entry.1292438233%3D%2520%2520To%2520be%2520considered%2520reliable%252C%2520a%2520model%2520must%2520be%2520calibrated%2520so%2520that%2520its%2520confidence%250Ain%2520each%2520decision%2520closely%2520reflects%2520its%2520true%2520outcome.%2520In%2520this%2520blogpost%2520we%2527ll%2520take%250Aa%2520look%2520at%2520the%2520most%2520commonly%2520used%2520definition%2520for%2520calibration%2520and%2520then%2520dive%2520into%250Aa%2520frequently%2520used%2520evaluation%2520measure%2520for%2520model%2520calibration.%2520We%2527ll%2520then%2520cover%250Asome%2520of%2520the%2520drawbacks%2520of%2520this%2520measure%2520and%2520how%2520these%2520surfaced%2520the%2520need%2520for%250Aadditional%2520notions%2520of%2520calibration%252C%2520which%2520require%2520their%2520own%2520new%2520evaluation%250Ameasures.%2520This%2520post%2520is%2520not%2520intended%2520to%2520be%2520an%2520in-depth%2520dissection%2520of%2520all%2520works%250Aon%2520calibration%252C%2520nor%2520does%2520it%2520focus%2520on%2520how%2520to%2520calibrate%2520models.%2520Instead%252C%2520it%2520is%250Ameant%2520to%2520provide%2520a%2520gentle%2520introduction%2520to%2520the%2520different%2520notions%2520and%2520their%250Aevaluation%2520measures%2520as%2520well%2520as%2520to%2520re-highlight%2520some%2520issues%2520with%2520a%2520measure%2520that%250Ais%2520still%2520widely%2520used%2520to%2520evaluate%2520calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Model%20Calibration%20--%20A%20gentle%20introduction%20and%20visual%0A%20%20exploration%20of%20calibration%20and%20the%20expected%20calibration%20error%20%28ECE%29&entry.906535625=Maja%20Pavlovic&entry.1292438233=%20%20To%20be%20considered%20reliable%2C%20a%20model%20must%20be%20calibrated%20so%20that%20its%20confidence%0Ain%20each%20decision%20closely%20reflects%20its%20true%20outcome.%20In%20this%20blogpost%20we%27ll%20take%0Aa%20look%20at%20the%20most%20commonly%20used%20definition%20for%20calibration%20and%20then%20dive%20into%0Aa%20frequently%20used%20evaluation%20measure%20for%20model%20calibration.%20We%27ll%20then%20cover%0Asome%20of%20the%20drawbacks%20of%20this%20measure%20and%20how%20these%20surfaced%20the%20need%20for%0Aadditional%20notions%20of%20calibration%2C%20which%20require%20their%20own%20new%20evaluation%0Ameasures.%20This%20post%20is%20not%20intended%20to%20be%20an%20in-depth%20dissection%20of%20all%20works%0Aon%20calibration%2C%20nor%20does%20it%20focus%20on%20how%20to%20calibrate%20models.%20Instead%2C%20it%20is%0Ameant%20to%20provide%20a%20gentle%20introduction%20to%20the%20different%20notions%20and%20their%0Aevaluation%20measures%20as%20well%20as%20to%20re-highlight%20some%20issues%20with%20a%20measure%20that%0Ais%20still%20widely%20used%20to%20evaluate%20calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19047v2&entry.124074799=Read"},
{"title": "A New Framework for Nonlinear Kalman Filters", "author": "Shida Jiang and Junzhe Shi and Scott Moura", "abstract": "  The Kalman filter (KF) is a state estimation algorithm that optimally\ncombines system knowledge and measurements to minimize the mean squared error\nof the estimated states. While KF was initially designed for linear systems,\nnumerous extensions of it, such as extended Kalman filter (EKF), unscented\nKalman filter (UKF), cubature Kalman filter (CKF), etc., have been proposed for\nnonlinear systems. Although different types of nonlinear KFs have different\npros and cons, they all use the same framework of linear KF. Yet, according to\nwhat we found in this paper, the framework tends to give overconfident and less\naccurate state estimations when the measurement functions are nonlinear.\nTherefore, in this study, we designed a new framework that can be combined with\nany existing type of nonlinear KFs and showed theoretically and empirically\nthat the new framework estimates the states and covariance more accurately than\nthe old one. The new framework was tested on four different nonlinear KFs and\nfive different tasks, showcasing its ability to reduce estimation errors by\nseveral orders of magnitude in low-measurement-noise conditions.\n", "link": "http://arxiv.org/abs/2407.05717v8", "date": "2025-02-03", "relevancy": 1.7707, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4672}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4485}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Framework%20for%20Nonlinear%20Kalman%20Filters&body=Title%3A%20A%20New%20Framework%20for%20Nonlinear%20Kalman%20Filters%0AAuthor%3A%20Shida%20Jiang%20and%20Junzhe%20Shi%20and%20Scott%20Moura%0AAbstract%3A%20%20%20The%20Kalman%20filter%20%28KF%29%20is%20a%20state%20estimation%20algorithm%20that%20optimally%0Acombines%20system%20knowledge%20and%20measurements%20to%20minimize%20the%20mean%20squared%20error%0Aof%20the%20estimated%20states.%20While%20KF%20was%20initially%20designed%20for%20linear%20systems%2C%0Anumerous%20extensions%20of%20it%2C%20such%20as%20extended%20Kalman%20filter%20%28EKF%29%2C%20unscented%0AKalman%20filter%20%28UKF%29%2C%20cubature%20Kalman%20filter%20%28CKF%29%2C%20etc.%2C%20have%20been%20proposed%20for%0Anonlinear%20systems.%20Although%20different%20types%20of%20nonlinear%20KFs%20have%20different%0Apros%20and%20cons%2C%20they%20all%20use%20the%20same%20framework%20of%20linear%20KF.%20Yet%2C%20according%20to%0Awhat%20we%20found%20in%20this%20paper%2C%20the%20framework%20tends%20to%20give%20overconfident%20and%20less%0Aaccurate%20state%20estimations%20when%20the%20measurement%20functions%20are%20nonlinear.%0ATherefore%2C%20in%20this%20study%2C%20we%20designed%20a%20new%20framework%20that%20can%20be%20combined%20with%0Aany%20existing%20type%20of%20nonlinear%20KFs%20and%20showed%20theoretically%20and%20empirically%0Athat%20the%20new%20framework%20estimates%20the%20states%20and%20covariance%20more%20accurately%20than%0Athe%20old%20one.%20The%20new%20framework%20was%20tested%20on%20four%20different%20nonlinear%20KFs%20and%0Afive%20different%20tasks%2C%20showcasing%20its%20ability%20to%20reduce%20estimation%20errors%20by%0Aseveral%20orders%20of%20magnitude%20in%20low-measurement-noise%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05717v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Framework%2520for%2520Nonlinear%2520Kalman%2520Filters%26entry.906535625%3DShida%2520Jiang%2520and%2520Junzhe%2520Shi%2520and%2520Scott%2520Moura%26entry.1292438233%3D%2520%2520The%2520Kalman%2520filter%2520%2528KF%2529%2520is%2520a%2520state%2520estimation%2520algorithm%2520that%2520optimally%250Acombines%2520system%2520knowledge%2520and%2520measurements%2520to%2520minimize%2520the%2520mean%2520squared%2520error%250Aof%2520the%2520estimated%2520states.%2520While%2520KF%2520was%2520initially%2520designed%2520for%2520linear%2520systems%252C%250Anumerous%2520extensions%2520of%2520it%252C%2520such%2520as%2520extended%2520Kalman%2520filter%2520%2528EKF%2529%252C%2520unscented%250AKalman%2520filter%2520%2528UKF%2529%252C%2520cubature%2520Kalman%2520filter%2520%2528CKF%2529%252C%2520etc.%252C%2520have%2520been%2520proposed%2520for%250Anonlinear%2520systems.%2520Although%2520different%2520types%2520of%2520nonlinear%2520KFs%2520have%2520different%250Apros%2520and%2520cons%252C%2520they%2520all%2520use%2520the%2520same%2520framework%2520of%2520linear%2520KF.%2520Yet%252C%2520according%2520to%250Awhat%2520we%2520found%2520in%2520this%2520paper%252C%2520the%2520framework%2520tends%2520to%2520give%2520overconfident%2520and%2520less%250Aaccurate%2520state%2520estimations%2520when%2520the%2520measurement%2520functions%2520are%2520nonlinear.%250ATherefore%252C%2520in%2520this%2520study%252C%2520we%2520designed%2520a%2520new%2520framework%2520that%2520can%2520be%2520combined%2520with%250Aany%2520existing%2520type%2520of%2520nonlinear%2520KFs%2520and%2520showed%2520theoretically%2520and%2520empirically%250Athat%2520the%2520new%2520framework%2520estimates%2520the%2520states%2520and%2520covariance%2520more%2520accurately%2520than%250Athe%2520old%2520one.%2520The%2520new%2520framework%2520was%2520tested%2520on%2520four%2520different%2520nonlinear%2520KFs%2520and%250Afive%2520different%2520tasks%252C%2520showcasing%2520its%2520ability%2520to%2520reduce%2520estimation%2520errors%2520by%250Aseveral%2520orders%2520of%2520magnitude%2520in%2520low-measurement-noise%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05717v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Framework%20for%20Nonlinear%20Kalman%20Filters&entry.906535625=Shida%20Jiang%20and%20Junzhe%20Shi%20and%20Scott%20Moura&entry.1292438233=%20%20The%20Kalman%20filter%20%28KF%29%20is%20a%20state%20estimation%20algorithm%20that%20optimally%0Acombines%20system%20knowledge%20and%20measurements%20to%20minimize%20the%20mean%20squared%20error%0Aof%20the%20estimated%20states.%20While%20KF%20was%20initially%20designed%20for%20linear%20systems%2C%0Anumerous%20extensions%20of%20it%2C%20such%20as%20extended%20Kalman%20filter%20%28EKF%29%2C%20unscented%0AKalman%20filter%20%28UKF%29%2C%20cubature%20Kalman%20filter%20%28CKF%29%2C%20etc.%2C%20have%20been%20proposed%20for%0Anonlinear%20systems.%20Although%20different%20types%20of%20nonlinear%20KFs%20have%20different%0Apros%20and%20cons%2C%20they%20all%20use%20the%20same%20framework%20of%20linear%20KF.%20Yet%2C%20according%20to%0Awhat%20we%20found%20in%20this%20paper%2C%20the%20framework%20tends%20to%20give%20overconfident%20and%20less%0Aaccurate%20state%20estimations%20when%20the%20measurement%20functions%20are%20nonlinear.%0ATherefore%2C%20in%20this%20study%2C%20we%20designed%20a%20new%20framework%20that%20can%20be%20combined%20with%0Aany%20existing%20type%20of%20nonlinear%20KFs%20and%20showed%20theoretically%20and%20empirically%0Athat%20the%20new%20framework%20estimates%20the%20states%20and%20covariance%20more%20accurately%20than%0Athe%20old%20one.%20The%20new%20framework%20was%20tested%20on%20four%20different%20nonlinear%20KFs%20and%0Afive%20different%20tasks%2C%20showcasing%20its%20ability%20to%20reduce%20estimation%20errors%20by%0Aseveral%20orders%20of%20magnitude%20in%20low-measurement-noise%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05717v8&entry.124074799=Read"},
{"title": "Musical ethnocentrism in Large Language Models", "author": "Anna Kruspe", "abstract": "  Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments.\n", "link": "http://arxiv.org/abs/2501.13720v2", "date": "2025-02-03", "relevancy": 1.743, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4376}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4376}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Musical%20ethnocentrism%20in%20Large%20Language%20Models&body=Title%3A%20Musical%20ethnocentrism%20in%20Large%20Language%20Models%0AAuthor%3A%20Anna%20Kruspe%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20reflect%20the%20biases%20in%20their%20training%20data%20and%2C%0Aby%20extension%2C%20those%20of%20the%20people%20who%20created%20this%20training%20data.%20Detecting%2C%0Aanalyzing%2C%20and%20mitigating%20such%20biases%20is%20becoming%20a%20focus%20of%20research.%20One%20type%0Aof%20bias%20that%20has%20been%20understudied%20so%20far%20are%20geocultural%20biases.%20Those%20can%20be%0Acaused%20by%20an%20imbalance%20in%20the%20representation%20of%20different%20geographic%20regions%0Aand%20cultures%20in%20the%20training%20data%2C%20but%20also%20by%20value%20judgments%20contained%0Atherein.%20In%20this%20paper%2C%20we%20make%20a%20first%20step%20towards%20analyzing%20musical%20biases%0Ain%20LLMs%2C%20particularly%20ChatGPT%20and%20Mixtral.%20We%20conduct%20two%20experiments.%20In%20the%0Afirst%2C%20we%20prompt%20LLMs%20to%20provide%20lists%20of%20the%20%22Top%20100%22%20musical%20contributors%20of%0Avarious%20categories%20and%20analyze%20their%20countries%20of%20origin.%20In%20the%20second%0Aexperiment%2C%20we%20ask%20the%20LLMs%20to%20numerically%20rate%20various%20aspects%20of%20the%20musical%0Acultures%20of%20different%20countries.%20Our%20results%20indicate%20a%20strong%20preference%20of%0Athe%20LLMs%20for%20Western%20music%20cultures%20in%20both%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13720v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMusical%2520ethnocentrism%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DAnna%2520Kruspe%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520reflect%2520the%2520biases%2520in%2520their%2520training%2520data%2520and%252C%250Aby%2520extension%252C%2520those%2520of%2520the%2520people%2520who%2520created%2520this%2520training%2520data.%2520Detecting%252C%250Aanalyzing%252C%2520and%2520mitigating%2520such%2520biases%2520is%2520becoming%2520a%2520focus%2520of%2520research.%2520One%2520type%250Aof%2520bias%2520that%2520has%2520been%2520understudied%2520so%2520far%2520are%2520geocultural%2520biases.%2520Those%2520can%2520be%250Acaused%2520by%2520an%2520imbalance%2520in%2520the%2520representation%2520of%2520different%2520geographic%2520regions%250Aand%2520cultures%2520in%2520the%2520training%2520data%252C%2520but%2520also%2520by%2520value%2520judgments%2520contained%250Atherein.%2520In%2520this%2520paper%252C%2520we%2520make%2520a%2520first%2520step%2520towards%2520analyzing%2520musical%2520biases%250Ain%2520LLMs%252C%2520particularly%2520ChatGPT%2520and%2520Mixtral.%2520We%2520conduct%2520two%2520experiments.%2520In%2520the%250Afirst%252C%2520we%2520prompt%2520LLMs%2520to%2520provide%2520lists%2520of%2520the%2520%2522Top%2520100%2522%2520musical%2520contributors%2520of%250Avarious%2520categories%2520and%2520analyze%2520their%2520countries%2520of%2520origin.%2520In%2520the%2520second%250Aexperiment%252C%2520we%2520ask%2520the%2520LLMs%2520to%2520numerically%2520rate%2520various%2520aspects%2520of%2520the%2520musical%250Acultures%2520of%2520different%2520countries.%2520Our%2520results%2520indicate%2520a%2520strong%2520preference%2520of%250Athe%2520LLMs%2520for%2520Western%2520music%2520cultures%2520in%2520both%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13720v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Musical%20ethnocentrism%20in%20Large%20Language%20Models&entry.906535625=Anna%20Kruspe&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20reflect%20the%20biases%20in%20their%20training%20data%20and%2C%0Aby%20extension%2C%20those%20of%20the%20people%20who%20created%20this%20training%20data.%20Detecting%2C%0Aanalyzing%2C%20and%20mitigating%20such%20biases%20is%20becoming%20a%20focus%20of%20research.%20One%20type%0Aof%20bias%20that%20has%20been%20understudied%20so%20far%20are%20geocultural%20biases.%20Those%20can%20be%0Acaused%20by%20an%20imbalance%20in%20the%20representation%20of%20different%20geographic%20regions%0Aand%20cultures%20in%20the%20training%20data%2C%20but%20also%20by%20value%20judgments%20contained%0Atherein.%20In%20this%20paper%2C%20we%20make%20a%20first%20step%20towards%20analyzing%20musical%20biases%0Ain%20LLMs%2C%20particularly%20ChatGPT%20and%20Mixtral.%20We%20conduct%20two%20experiments.%20In%20the%0Afirst%2C%20we%20prompt%20LLMs%20to%20provide%20lists%20of%20the%20%22Top%20100%22%20musical%20contributors%20of%0Avarious%20categories%20and%20analyze%20their%20countries%20of%20origin.%20In%20the%20second%0Aexperiment%2C%20we%20ask%20the%20LLMs%20to%20numerically%20rate%20various%20aspects%20of%20the%20musical%0Acultures%20of%20different%20countries.%20Our%20results%20indicate%20a%20strong%20preference%20of%0Athe%20LLMs%20for%20Western%20music%20cultures%20in%20both%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13720v2&entry.124074799=Read"},
{"title": "Predict. Optimize. Revise. On Forecast and Policy Stability in Energy\n  Management Systems", "author": "Evgenii Genov and Julian Ruddick and Christoph Bergmeir and Majid Vafaeipour and Thierry Coosemans and Salvador Garcia and Maarten Messagie", "abstract": "  This research addresses the challenge of integrating forecasting and\noptimization in energy management systems, focusing on the impacts of switching\ncosts, forecast accuracy, and stability. It proposes a novel framework for\nanalyzing online optimization problems with switching costs and enabled by\ndeterministic and probabilistic forecasts. Through empirical evaluation and\ntheoretical analysis, the research reveals the balance between forecast\naccuracy, stability, and switching costs in shaping policy performance.\nConducted in the context of battery scheduling within energy management\napplications, it introduces a metric for evaluating probabilistic forecast\nstability and examines the effects of forecast accuracy and stability on\noptimization outcomes using the real-world case of the Citylearn 2022\ncompetition. Findings indicate that switching costs significantly influence the\ntrade-off between forecast accuracy and stability, highlighting the importance\nof integrated systems that enable collaboration between forecasting and\noperational units for improved decision-making. The study shows that committing\nto a policy for longer periods can be advantageous over frequent updates.\nResults also show a correlation between forecast stability and policy\nperformance, suggesting that stable forecasts can mitigate switching costs. The\nproposed framework provides valuable insights for energy sector decision-makers\nand forecast practitioners when designing the operation of an energy management\nsystem.\n", "link": "http://arxiv.org/abs/2407.03368v4", "date": "2025-02-03", "relevancy": 1.7282, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4769}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4329}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predict.%20Optimize.%20Revise.%20On%20Forecast%20and%20Policy%20Stability%20in%20Energy%0A%20%20Management%20Systems&body=Title%3A%20Predict.%20Optimize.%20Revise.%20On%20Forecast%20and%20Policy%20Stability%20in%20Energy%0A%20%20Management%20Systems%0AAuthor%3A%20Evgenii%20Genov%20and%20Julian%20Ruddick%20and%20Christoph%20Bergmeir%20and%20Majid%20Vafaeipour%20and%20Thierry%20Coosemans%20and%20Salvador%20Garcia%20and%20Maarten%20Messagie%0AAbstract%3A%20%20%20This%20research%20addresses%20the%20challenge%20of%20integrating%20forecasting%20and%0Aoptimization%20in%20energy%20management%20systems%2C%20focusing%20on%20the%20impacts%20of%20switching%0Acosts%2C%20forecast%20accuracy%2C%20and%20stability.%20It%20proposes%20a%20novel%20framework%20for%0Aanalyzing%20online%20optimization%20problems%20with%20switching%20costs%20and%20enabled%20by%0Adeterministic%20and%20probabilistic%20forecasts.%20Through%20empirical%20evaluation%20and%0Atheoretical%20analysis%2C%20the%20research%20reveals%20the%20balance%20between%20forecast%0Aaccuracy%2C%20stability%2C%20and%20switching%20costs%20in%20shaping%20policy%20performance.%0AConducted%20in%20the%20context%20of%20battery%20scheduling%20within%20energy%20management%0Aapplications%2C%20it%20introduces%20a%20metric%20for%20evaluating%20probabilistic%20forecast%0Astability%20and%20examines%20the%20effects%20of%20forecast%20accuracy%20and%20stability%20on%0Aoptimization%20outcomes%20using%20the%20real-world%20case%20of%20the%20Citylearn%202022%0Acompetition.%20Findings%20indicate%20that%20switching%20costs%20significantly%20influence%20the%0Atrade-off%20between%20forecast%20accuracy%20and%20stability%2C%20highlighting%20the%20importance%0Aof%20integrated%20systems%20that%20enable%20collaboration%20between%20forecasting%20and%0Aoperational%20units%20for%20improved%20decision-making.%20The%20study%20shows%20that%20committing%0Ato%20a%20policy%20for%20longer%20periods%20can%20be%20advantageous%20over%20frequent%20updates.%0AResults%20also%20show%20a%20correlation%20between%20forecast%20stability%20and%20policy%0Aperformance%2C%20suggesting%20that%20stable%20forecasts%20can%20mitigate%20switching%20costs.%20The%0Aproposed%20framework%20provides%20valuable%20insights%20for%20energy%20sector%20decision-makers%0Aand%20forecast%20practitioners%20when%20designing%20the%20operation%20of%20an%20energy%20management%0Asystem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03368v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredict.%2520Optimize.%2520Revise.%2520On%2520Forecast%2520and%2520Policy%2520Stability%2520in%2520Energy%250A%2520%2520Management%2520Systems%26entry.906535625%3DEvgenii%2520Genov%2520and%2520Julian%2520Ruddick%2520and%2520Christoph%2520Bergmeir%2520and%2520Majid%2520Vafaeipour%2520and%2520Thierry%2520Coosemans%2520and%2520Salvador%2520Garcia%2520and%2520Maarten%2520Messagie%26entry.1292438233%3D%2520%2520This%2520research%2520addresses%2520the%2520challenge%2520of%2520integrating%2520forecasting%2520and%250Aoptimization%2520in%2520energy%2520management%2520systems%252C%2520focusing%2520on%2520the%2520impacts%2520of%2520switching%250Acosts%252C%2520forecast%2520accuracy%252C%2520and%2520stability.%2520It%2520proposes%2520a%2520novel%2520framework%2520for%250Aanalyzing%2520online%2520optimization%2520problems%2520with%2520switching%2520costs%2520and%2520enabled%2520by%250Adeterministic%2520and%2520probabilistic%2520forecasts.%2520Through%2520empirical%2520evaluation%2520and%250Atheoretical%2520analysis%252C%2520the%2520research%2520reveals%2520the%2520balance%2520between%2520forecast%250Aaccuracy%252C%2520stability%252C%2520and%2520switching%2520costs%2520in%2520shaping%2520policy%2520performance.%250AConducted%2520in%2520the%2520context%2520of%2520battery%2520scheduling%2520within%2520energy%2520management%250Aapplications%252C%2520it%2520introduces%2520a%2520metric%2520for%2520evaluating%2520probabilistic%2520forecast%250Astability%2520and%2520examines%2520the%2520effects%2520of%2520forecast%2520accuracy%2520and%2520stability%2520on%250Aoptimization%2520outcomes%2520using%2520the%2520real-world%2520case%2520of%2520the%2520Citylearn%25202022%250Acompetition.%2520Findings%2520indicate%2520that%2520switching%2520costs%2520significantly%2520influence%2520the%250Atrade-off%2520between%2520forecast%2520accuracy%2520and%2520stability%252C%2520highlighting%2520the%2520importance%250Aof%2520integrated%2520systems%2520that%2520enable%2520collaboration%2520between%2520forecasting%2520and%250Aoperational%2520units%2520for%2520improved%2520decision-making.%2520The%2520study%2520shows%2520that%2520committing%250Ato%2520a%2520policy%2520for%2520longer%2520periods%2520can%2520be%2520advantageous%2520over%2520frequent%2520updates.%250AResults%2520also%2520show%2520a%2520correlation%2520between%2520forecast%2520stability%2520and%2520policy%250Aperformance%252C%2520suggesting%2520that%2520stable%2520forecasts%2520can%2520mitigate%2520switching%2520costs.%2520The%250Aproposed%2520framework%2520provides%2520valuable%2520insights%2520for%2520energy%2520sector%2520decision-makers%250Aand%2520forecast%2520practitioners%2520when%2520designing%2520the%2520operation%2520of%2520an%2520energy%2520management%250Asystem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03368v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predict.%20Optimize.%20Revise.%20On%20Forecast%20and%20Policy%20Stability%20in%20Energy%0A%20%20Management%20Systems&entry.906535625=Evgenii%20Genov%20and%20Julian%20Ruddick%20and%20Christoph%20Bergmeir%20and%20Majid%20Vafaeipour%20and%20Thierry%20Coosemans%20and%20Salvador%20Garcia%20and%20Maarten%20Messagie&entry.1292438233=%20%20This%20research%20addresses%20the%20challenge%20of%20integrating%20forecasting%20and%0Aoptimization%20in%20energy%20management%20systems%2C%20focusing%20on%20the%20impacts%20of%20switching%0Acosts%2C%20forecast%20accuracy%2C%20and%20stability.%20It%20proposes%20a%20novel%20framework%20for%0Aanalyzing%20online%20optimization%20problems%20with%20switching%20costs%20and%20enabled%20by%0Adeterministic%20and%20probabilistic%20forecasts.%20Through%20empirical%20evaluation%20and%0Atheoretical%20analysis%2C%20the%20research%20reveals%20the%20balance%20between%20forecast%0Aaccuracy%2C%20stability%2C%20and%20switching%20costs%20in%20shaping%20policy%20performance.%0AConducted%20in%20the%20context%20of%20battery%20scheduling%20within%20energy%20management%0Aapplications%2C%20it%20introduces%20a%20metric%20for%20evaluating%20probabilistic%20forecast%0Astability%20and%20examines%20the%20effects%20of%20forecast%20accuracy%20and%20stability%20on%0Aoptimization%20outcomes%20using%20the%20real-world%20case%20of%20the%20Citylearn%202022%0Acompetition.%20Findings%20indicate%20that%20switching%20costs%20significantly%20influence%20the%0Atrade-off%20between%20forecast%20accuracy%20and%20stability%2C%20highlighting%20the%20importance%0Aof%20integrated%20systems%20that%20enable%20collaboration%20between%20forecasting%20and%0Aoperational%20units%20for%20improved%20decision-making.%20The%20study%20shows%20that%20committing%0Ato%20a%20policy%20for%20longer%20periods%20can%20be%20advantageous%20over%20frequent%20updates.%0AResults%20also%20show%20a%20correlation%20between%20forecast%20stability%20and%20policy%0Aperformance%2C%20suggesting%20that%20stable%20forecasts%20can%20mitigate%20switching%20costs.%20The%0Aproposed%20framework%20provides%20valuable%20insights%20for%20energy%20sector%20decision-makers%0Aand%20forecast%20practitioners%20when%20designing%20the%20operation%20of%20an%20energy%20management%0Asystem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03368v4&entry.124074799=Read"},
{"title": "Reproducible Machine Learning-based Voice Pathology Detection:\n  Introducing the Pitch Difference Feature", "author": "Jan Vrba and Jakub Steinbach and Tom\u00e1\u0161 Jirsa and Laura Verde and Roberta De Fazio and Yuwen Zeng and Kei Ichiji and Luk\u00e1\u0161 H\u00e1jek and Zuzana Sedl\u00e1kov\u00e1 and Zuzana Urb\u00e1niov\u00e1 and Martin Chovanec and Jan Mare\u0161 and Noriyasu Homma", "abstract": "  This study introduces a novel methodology for voice pathology detection using\nthe publicly available Saarbr\\\"ucken Voice Database (SVD) database and a robust\nfeature set combining commonly used acoustic handcrafted features with two\nnovel ones: pitch difference (relative variation in fundamental frequency) and\na NaN feature (failed fundamental frequency estimation).\n  We evaluate six machine learning (ML) classifiers - support vector machine,\nk-nearest neighbors, naive Bayes, decision tree, random forest, and AdaBoost -\nusing grid search for feasible hyperparameters of selected classifiers and\n20480 different feature subsets. Top 1000 classifier-feature subset\ncombinations for each classifier type are validated with repeated stratified\ncross-validation. To address class imbalance, we apply K-Means SMOTE to augment\nthe training data.\n  Our approach achieves outstanding performance, reaching 85.61%, 84.69% and\n85.22% unweighted average recall (UAR) for females, males and combined results\nrespectivelly. We intentionally omit accuracy as it is a highly biased metric\nfor imbalanced data. This advancement demonstrates significant potential for\nclinical deployment of ML methods, offering a valuable supportive tool for an\nobjective examination of voice pathologies. To enable an easier use of our\nmethodology and to support our claims, we provide a publicly available GitHub\nrepository with DOI 10.5281/zenodo.13771573. Finally, we provide a REFORMS\nchecklist to enhance readability, reproducibility and justification of our\napproach.\n", "link": "http://arxiv.org/abs/2410.10537v2", "date": "2025-02-03", "relevancy": 1.7172, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4371}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4259}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reproducible%20Machine%20Learning-based%20Voice%20Pathology%20Detection%3A%0A%20%20Introducing%20the%20Pitch%20Difference%20Feature&body=Title%3A%20Reproducible%20Machine%20Learning-based%20Voice%20Pathology%20Detection%3A%0A%20%20Introducing%20the%20Pitch%20Difference%20Feature%0AAuthor%3A%20Jan%20Vrba%20and%20Jakub%20Steinbach%20and%20Tom%C3%A1%C5%A1%20Jirsa%20and%20Laura%20Verde%20and%20Roberta%20De%20Fazio%20and%20Yuwen%20Zeng%20and%20Kei%20Ichiji%20and%20Luk%C3%A1%C5%A1%20H%C3%A1jek%20and%20Zuzana%20Sedl%C3%A1kov%C3%A1%20and%20Zuzana%20Urb%C3%A1niov%C3%A1%20and%20Martin%20Chovanec%20and%20Jan%20Mare%C5%A1%20and%20Noriyasu%20Homma%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20novel%20methodology%20for%20voice%20pathology%20detection%20using%0Athe%20publicly%20available%20Saarbr%5C%22ucken%20Voice%20Database%20%28SVD%29%20database%20and%20a%20robust%0Afeature%20set%20combining%20commonly%20used%20acoustic%20handcrafted%20features%20with%20two%0Anovel%20ones%3A%20pitch%20difference%20%28relative%20variation%20in%20fundamental%20frequency%29%20and%0Aa%20NaN%20feature%20%28failed%20fundamental%20frequency%20estimation%29.%0A%20%20We%20evaluate%20six%20machine%20learning%20%28ML%29%20classifiers%20-%20support%20vector%20machine%2C%0Ak-nearest%20neighbors%2C%20naive%20Bayes%2C%20decision%20tree%2C%20random%20forest%2C%20and%20AdaBoost%20-%0Ausing%20grid%20search%20for%20feasible%20hyperparameters%20of%20selected%20classifiers%20and%0A20480%20different%20feature%20subsets.%20Top%201000%20classifier-feature%20subset%0Acombinations%20for%20each%20classifier%20type%20are%20validated%20with%20repeated%20stratified%0Across-validation.%20To%20address%20class%20imbalance%2C%20we%20apply%20K-Means%20SMOTE%20to%20augment%0Athe%20training%20data.%0A%20%20Our%20approach%20achieves%20outstanding%20performance%2C%20reaching%2085.61%25%2C%2084.69%25%20and%0A85.22%25%20unweighted%20average%20recall%20%28UAR%29%20for%20females%2C%20males%20and%20combined%20results%0Arespectivelly.%20We%20intentionally%20omit%20accuracy%20as%20it%20is%20a%20highly%20biased%20metric%0Afor%20imbalanced%20data.%20This%20advancement%20demonstrates%20significant%20potential%20for%0Aclinical%20deployment%20of%20ML%20methods%2C%20offering%20a%20valuable%20supportive%20tool%20for%20an%0Aobjective%20examination%20of%20voice%20pathologies.%20To%20enable%20an%20easier%20use%20of%20our%0Amethodology%20and%20to%20support%20our%20claims%2C%20we%20provide%20a%20publicly%20available%20GitHub%0Arepository%20with%20DOI%2010.5281/zenodo.13771573.%20Finally%2C%20we%20provide%20a%20REFORMS%0Achecklist%20to%20enhance%20readability%2C%20reproducibility%20and%20justification%20of%20our%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10537v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReproducible%2520Machine%2520Learning-based%2520Voice%2520Pathology%2520Detection%253A%250A%2520%2520Introducing%2520the%2520Pitch%2520Difference%2520Feature%26entry.906535625%3DJan%2520Vrba%2520and%2520Jakub%2520Steinbach%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Jirsa%2520and%2520Laura%2520Verde%2520and%2520Roberta%2520De%2520Fazio%2520and%2520Yuwen%2520Zeng%2520and%2520Kei%2520Ichiji%2520and%2520Luk%25C3%25A1%25C5%25A1%2520H%25C3%25A1jek%2520and%2520Zuzana%2520Sedl%25C3%25A1kov%25C3%25A1%2520and%2520Zuzana%2520Urb%25C3%25A1niov%25C3%25A1%2520and%2520Martin%2520Chovanec%2520and%2520Jan%2520Mare%25C5%25A1%2520and%2520Noriyasu%2520Homma%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520a%2520novel%2520methodology%2520for%2520voice%2520pathology%2520detection%2520using%250Athe%2520publicly%2520available%2520Saarbr%255C%2522ucken%2520Voice%2520Database%2520%2528SVD%2529%2520database%2520and%2520a%2520robust%250Afeature%2520set%2520combining%2520commonly%2520used%2520acoustic%2520handcrafted%2520features%2520with%2520two%250Anovel%2520ones%253A%2520pitch%2520difference%2520%2528relative%2520variation%2520in%2520fundamental%2520frequency%2529%2520and%250Aa%2520NaN%2520feature%2520%2528failed%2520fundamental%2520frequency%2520estimation%2529.%250A%2520%2520We%2520evaluate%2520six%2520machine%2520learning%2520%2528ML%2529%2520classifiers%2520-%2520support%2520vector%2520machine%252C%250Ak-nearest%2520neighbors%252C%2520naive%2520Bayes%252C%2520decision%2520tree%252C%2520random%2520forest%252C%2520and%2520AdaBoost%2520-%250Ausing%2520grid%2520search%2520for%2520feasible%2520hyperparameters%2520of%2520selected%2520classifiers%2520and%250A20480%2520different%2520feature%2520subsets.%2520Top%25201000%2520classifier-feature%2520subset%250Acombinations%2520for%2520each%2520classifier%2520type%2520are%2520validated%2520with%2520repeated%2520stratified%250Across-validation.%2520To%2520address%2520class%2520imbalance%252C%2520we%2520apply%2520K-Means%2520SMOTE%2520to%2520augment%250Athe%2520training%2520data.%250A%2520%2520Our%2520approach%2520achieves%2520outstanding%2520performance%252C%2520reaching%252085.61%2525%252C%252084.69%2525%2520and%250A85.22%2525%2520unweighted%2520average%2520recall%2520%2528UAR%2529%2520for%2520females%252C%2520males%2520and%2520combined%2520results%250Arespectivelly.%2520We%2520intentionally%2520omit%2520accuracy%2520as%2520it%2520is%2520a%2520highly%2520biased%2520metric%250Afor%2520imbalanced%2520data.%2520This%2520advancement%2520demonstrates%2520significant%2520potential%2520for%250Aclinical%2520deployment%2520of%2520ML%2520methods%252C%2520offering%2520a%2520valuable%2520supportive%2520tool%2520for%2520an%250Aobjective%2520examination%2520of%2520voice%2520pathologies.%2520To%2520enable%2520an%2520easier%2520use%2520of%2520our%250Amethodology%2520and%2520to%2520support%2520our%2520claims%252C%2520we%2520provide%2520a%2520publicly%2520available%2520GitHub%250Arepository%2520with%2520DOI%252010.5281/zenodo.13771573.%2520Finally%252C%2520we%2520provide%2520a%2520REFORMS%250Achecklist%2520to%2520enhance%2520readability%252C%2520reproducibility%2520and%2520justification%2520of%2520our%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10537v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reproducible%20Machine%20Learning-based%20Voice%20Pathology%20Detection%3A%0A%20%20Introducing%20the%20Pitch%20Difference%20Feature&entry.906535625=Jan%20Vrba%20and%20Jakub%20Steinbach%20and%20Tom%C3%A1%C5%A1%20Jirsa%20and%20Laura%20Verde%20and%20Roberta%20De%20Fazio%20and%20Yuwen%20Zeng%20and%20Kei%20Ichiji%20and%20Luk%C3%A1%C5%A1%20H%C3%A1jek%20and%20Zuzana%20Sedl%C3%A1kov%C3%A1%20and%20Zuzana%20Urb%C3%A1niov%C3%A1%20and%20Martin%20Chovanec%20and%20Jan%20Mare%C5%A1%20and%20Noriyasu%20Homma&entry.1292438233=%20%20This%20study%20introduces%20a%20novel%20methodology%20for%20voice%20pathology%20detection%20using%0Athe%20publicly%20available%20Saarbr%5C%22ucken%20Voice%20Database%20%28SVD%29%20database%20and%20a%20robust%0Afeature%20set%20combining%20commonly%20used%20acoustic%20handcrafted%20features%20with%20two%0Anovel%20ones%3A%20pitch%20difference%20%28relative%20variation%20in%20fundamental%20frequency%29%20and%0Aa%20NaN%20feature%20%28failed%20fundamental%20frequency%20estimation%29.%0A%20%20We%20evaluate%20six%20machine%20learning%20%28ML%29%20classifiers%20-%20support%20vector%20machine%2C%0Ak-nearest%20neighbors%2C%20naive%20Bayes%2C%20decision%20tree%2C%20random%20forest%2C%20and%20AdaBoost%20-%0Ausing%20grid%20search%20for%20feasible%20hyperparameters%20of%20selected%20classifiers%20and%0A20480%20different%20feature%20subsets.%20Top%201000%20classifier-feature%20subset%0Acombinations%20for%20each%20classifier%20type%20are%20validated%20with%20repeated%20stratified%0Across-validation.%20To%20address%20class%20imbalance%2C%20we%20apply%20K-Means%20SMOTE%20to%20augment%0Athe%20training%20data.%0A%20%20Our%20approach%20achieves%20outstanding%20performance%2C%20reaching%2085.61%25%2C%2084.69%25%20and%0A85.22%25%20unweighted%20average%20recall%20%28UAR%29%20for%20females%2C%20males%20and%20combined%20results%0Arespectivelly.%20We%20intentionally%20omit%20accuracy%20as%20it%20is%20a%20highly%20biased%20metric%0Afor%20imbalanced%20data.%20This%20advancement%20demonstrates%20significant%20potential%20for%0Aclinical%20deployment%20of%20ML%20methods%2C%20offering%20a%20valuable%20supportive%20tool%20for%20an%0Aobjective%20examination%20of%20voice%20pathologies.%20To%20enable%20an%20easier%20use%20of%20our%0Amethodology%20and%20to%20support%20our%20claims%2C%20we%20provide%20a%20publicly%20available%20GitHub%0Arepository%20with%20DOI%2010.5281/zenodo.13771573.%20Finally%2C%20we%20provide%20a%20REFORMS%0Achecklist%20to%20enhance%20readability%2C%20reproducibility%20and%20justification%20of%20our%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10537v2&entry.124074799=Read"},
{"title": "PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing", "author": "Feng Tian and Yixuan Li and Yichao Yan and Shanyan Guan and Yanhao Ge and Xiaokang Yang", "abstract": "  In the field of image editing, three core challenges persist:\ncontrollability, background preservation, and efficiency. Inversion-based\nmethods rely on time-consuming optimization to preserve the features of the\ninitial images, which results in low efficiency due to the requirement for\nextensive network inference. Conversely, inversion-free methods lack\ntheoretical support for background similarity, as they circumvent the issue of\nmaintaining initial features to achieve efficiency. As a consequence, none of\nthese methods can achieve both high efficiency and background consistency. To\ntackle the challenges and the aforementioned disadvantages, we introduce\nPostEdit, a method that incorporates a posterior scheme to govern the diffusion\nsampling process. Specifically, a corresponding measurement term related to\nboth the initial features and Langevin dynamics is introduced to optimize the\nestimated image generated by the given target prompt. Extensive experimental\nresults indicate that the proposed PostEdit achieves state-of-the-art editing\nperformance while accurately preserving unedited regions. Furthermore, the\nmethod is both inversion- and training-free, necessitating approximately 1.5\nseconds and 18 GB of GPU memory to generate high-quality results.\n", "link": "http://arxiv.org/abs/2410.04844v2", "date": "2025-02-03", "relevancy": 1.7055, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5786}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5683}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PostEdit%3A%20Posterior%20Sampling%20for%20Efficient%20Zero-Shot%20Image%20Editing&body=Title%3A%20PostEdit%3A%20Posterior%20Sampling%20for%20Efficient%20Zero-Shot%20Image%20Editing%0AAuthor%3A%20Feng%20Tian%20and%20Yixuan%20Li%20and%20Yichao%20Yan%20and%20Shanyan%20Guan%20and%20Yanhao%20Ge%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20In%20the%20field%20of%20image%20editing%2C%20three%20core%20challenges%20persist%3A%0Acontrollability%2C%20background%20preservation%2C%20and%20efficiency.%20Inversion-based%0Amethods%20rely%20on%20time-consuming%20optimization%20to%20preserve%20the%20features%20of%20the%0Ainitial%20images%2C%20which%20results%20in%20low%20efficiency%20due%20to%20the%20requirement%20for%0Aextensive%20network%20inference.%20Conversely%2C%20inversion-free%20methods%20lack%0Atheoretical%20support%20for%20background%20similarity%2C%20as%20they%20circumvent%20the%20issue%20of%0Amaintaining%20initial%20features%20to%20achieve%20efficiency.%20As%20a%20consequence%2C%20none%20of%0Athese%20methods%20can%20achieve%20both%20high%20efficiency%20and%20background%20consistency.%20To%0Atackle%20the%20challenges%20and%20the%20aforementioned%20disadvantages%2C%20we%20introduce%0APostEdit%2C%20a%20method%20that%20incorporates%20a%20posterior%20scheme%20to%20govern%20the%20diffusion%0Asampling%20process.%20Specifically%2C%20a%20corresponding%20measurement%20term%20related%20to%0Aboth%20the%20initial%20features%20and%20Langevin%20dynamics%20is%20introduced%20to%20optimize%20the%0Aestimated%20image%20generated%20by%20the%20given%20target%20prompt.%20Extensive%20experimental%0Aresults%20indicate%20that%20the%20proposed%20PostEdit%20achieves%20state-of-the-art%20editing%0Aperformance%20while%20accurately%20preserving%20unedited%20regions.%20Furthermore%2C%20the%0Amethod%20is%20both%20inversion-%20and%20training-free%2C%20necessitating%20approximately%201.5%0Aseconds%20and%2018%20GB%20of%20GPU%20memory%20to%20generate%20high-quality%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04844v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPostEdit%253A%2520Posterior%2520Sampling%2520for%2520Efficient%2520Zero-Shot%2520Image%2520Editing%26entry.906535625%3DFeng%2520Tian%2520and%2520Yixuan%2520Li%2520and%2520Yichao%2520Yan%2520and%2520Shanyan%2520Guan%2520and%2520Yanhao%2520Ge%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520image%2520editing%252C%2520three%2520core%2520challenges%2520persist%253A%250Acontrollability%252C%2520background%2520preservation%252C%2520and%2520efficiency.%2520Inversion-based%250Amethods%2520rely%2520on%2520time-consuming%2520optimization%2520to%2520preserve%2520the%2520features%2520of%2520the%250Ainitial%2520images%252C%2520which%2520results%2520in%2520low%2520efficiency%2520due%2520to%2520the%2520requirement%2520for%250Aextensive%2520network%2520inference.%2520Conversely%252C%2520inversion-free%2520methods%2520lack%250Atheoretical%2520support%2520for%2520background%2520similarity%252C%2520as%2520they%2520circumvent%2520the%2520issue%2520of%250Amaintaining%2520initial%2520features%2520to%2520achieve%2520efficiency.%2520As%2520a%2520consequence%252C%2520none%2520of%250Athese%2520methods%2520can%2520achieve%2520both%2520high%2520efficiency%2520and%2520background%2520consistency.%2520To%250Atackle%2520the%2520challenges%2520and%2520the%2520aforementioned%2520disadvantages%252C%2520we%2520introduce%250APostEdit%252C%2520a%2520method%2520that%2520incorporates%2520a%2520posterior%2520scheme%2520to%2520govern%2520the%2520diffusion%250Asampling%2520process.%2520Specifically%252C%2520a%2520corresponding%2520measurement%2520term%2520related%2520to%250Aboth%2520the%2520initial%2520features%2520and%2520Langevin%2520dynamics%2520is%2520introduced%2520to%2520optimize%2520the%250Aestimated%2520image%2520generated%2520by%2520the%2520given%2520target%2520prompt.%2520Extensive%2520experimental%250Aresults%2520indicate%2520that%2520the%2520proposed%2520PostEdit%2520achieves%2520state-of-the-art%2520editing%250Aperformance%2520while%2520accurately%2520preserving%2520unedited%2520regions.%2520Furthermore%252C%2520the%250Amethod%2520is%2520both%2520inversion-%2520and%2520training-free%252C%2520necessitating%2520approximately%25201.5%250Aseconds%2520and%252018%2520GB%2520of%2520GPU%2520memory%2520to%2520generate%2520high-quality%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04844v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PostEdit%3A%20Posterior%20Sampling%20for%20Efficient%20Zero-Shot%20Image%20Editing&entry.906535625=Feng%20Tian%20and%20Yixuan%20Li%20and%20Yichao%20Yan%20and%20Shanyan%20Guan%20and%20Yanhao%20Ge%20and%20Xiaokang%20Yang&entry.1292438233=%20%20In%20the%20field%20of%20image%20editing%2C%20three%20core%20challenges%20persist%3A%0Acontrollability%2C%20background%20preservation%2C%20and%20efficiency.%20Inversion-based%0Amethods%20rely%20on%20time-consuming%20optimization%20to%20preserve%20the%20features%20of%20the%0Ainitial%20images%2C%20which%20results%20in%20low%20efficiency%20due%20to%20the%20requirement%20for%0Aextensive%20network%20inference.%20Conversely%2C%20inversion-free%20methods%20lack%0Atheoretical%20support%20for%20background%20similarity%2C%20as%20they%20circumvent%20the%20issue%20of%0Amaintaining%20initial%20features%20to%20achieve%20efficiency.%20As%20a%20consequence%2C%20none%20of%0Athese%20methods%20can%20achieve%20both%20high%20efficiency%20and%20background%20consistency.%20To%0Atackle%20the%20challenges%20and%20the%20aforementioned%20disadvantages%2C%20we%20introduce%0APostEdit%2C%20a%20method%20that%20incorporates%20a%20posterior%20scheme%20to%20govern%20the%20diffusion%0Asampling%20process.%20Specifically%2C%20a%20corresponding%20measurement%20term%20related%20to%0Aboth%20the%20initial%20features%20and%20Langevin%20dynamics%20is%20introduced%20to%20optimize%20the%0Aestimated%20image%20generated%20by%20the%20given%20target%20prompt.%20Extensive%20experimental%0Aresults%20indicate%20that%20the%20proposed%20PostEdit%20achieves%20state-of-the-art%20editing%0Aperformance%20while%20accurately%20preserving%20unedited%20regions.%20Furthermore%2C%20the%0Amethod%20is%20both%20inversion-%20and%20training-free%2C%20necessitating%20approximately%201.5%0Aseconds%20and%2018%20GB%20of%20GPU%20memory%20to%20generate%20high-quality%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04844v2&entry.124074799=Read"},
{"title": "Deep Exploration with PAC-Bayes", "author": "Bahareh Tasdighi and Manuel Haussmann and Nicklas Werge and Yi-Shan Wu and Melih Kandemir", "abstract": "  Reinforcement learning for continuous control under delayed rewards is an\nunder-explored problem despite its significance in real life. Many complex\nskills build on intermediate ones as prerequisites. For instance, a humanoid\nlocomotor has to learn how to stand before it can learn to walk. To cope with\ndelayed reward, a reinforcement learning agent has to perform deep exploration.\nHowever, existing deep exploration methods are designed for small discrete\naction spaces, and their successful generalization to state-of-the-art\ncontinuous control remains unproven. We address the deep exploration problem\nfor the first time from a PAC-Bayesian perspective in the context of\nactor-critic learning. To do this, we quantify the error of the Bellman\noperator through a PAC-Bayes bound, where a bootstrapped ensemble of critic\nnetworks represents the posterior distribution, and their targets serve as a\ndata-informed function-space prior. We derive an objective function from this\nbound and use it to train the critic ensemble. Each critic trains an individual\nsoft actor network, implemented as a shared trunk and critic-specific heads.\nThe agent performs deep exploration by acting epsilon-greedily on a randomly\nchosen actor head. Our proposed algorithm, named PAC-Bayesian Actor-Critic\n(PBAC), is the only algorithm to consistently discover delayed rewards on a\ndiverse set of continuous control tasks with varying difficulty.\n", "link": "http://arxiv.org/abs/2402.03055v3", "date": "2025-02-03", "relevancy": 1.6861, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.57}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Exploration%20with%20PAC-Bayes&body=Title%3A%20Deep%20Exploration%20with%20PAC-Bayes%0AAuthor%3A%20Bahareh%20Tasdighi%20and%20Manuel%20Haussmann%20and%20Nicklas%20Werge%20and%20Yi-Shan%20Wu%20and%20Melih%20Kandemir%0AAbstract%3A%20%20%20Reinforcement%20learning%20for%20continuous%20control%20under%20delayed%20rewards%20is%20an%0Aunder-explored%20problem%20despite%20its%20significance%20in%20real%20life.%20Many%20complex%0Askills%20build%20on%20intermediate%20ones%20as%20prerequisites.%20For%20instance%2C%20a%20humanoid%0Alocomotor%20has%20to%20learn%20how%20to%20stand%20before%20it%20can%20learn%20to%20walk.%20To%20cope%20with%0Adelayed%20reward%2C%20a%20reinforcement%20learning%20agent%20has%20to%20perform%20deep%20exploration.%0AHowever%2C%20existing%20deep%20exploration%20methods%20are%20designed%20for%20small%20discrete%0Aaction%20spaces%2C%20and%20their%20successful%20generalization%20to%20state-of-the-art%0Acontinuous%20control%20remains%20unproven.%20We%20address%20the%20deep%20exploration%20problem%0Afor%20the%20first%20time%20from%20a%20PAC-Bayesian%20perspective%20in%20the%20context%20of%0Aactor-critic%20learning.%20To%20do%20this%2C%20we%20quantify%20the%20error%20of%20the%20Bellman%0Aoperator%20through%20a%20PAC-Bayes%20bound%2C%20where%20a%20bootstrapped%20ensemble%20of%20critic%0Anetworks%20represents%20the%20posterior%20distribution%2C%20and%20their%20targets%20serve%20as%20a%0Adata-informed%20function-space%20prior.%20We%20derive%20an%20objective%20function%20from%20this%0Abound%20and%20use%20it%20to%20train%20the%20critic%20ensemble.%20Each%20critic%20trains%20an%20individual%0Asoft%20actor%20network%2C%20implemented%20as%20a%20shared%20trunk%20and%20critic-specific%20heads.%0AThe%20agent%20performs%20deep%20exploration%20by%20acting%20epsilon-greedily%20on%20a%20randomly%0Achosen%20actor%20head.%20Our%20proposed%20algorithm%2C%20named%20PAC-Bayesian%20Actor-Critic%0A%28PBAC%29%2C%20is%20the%20only%20algorithm%20to%20consistently%20discover%20delayed%20rewards%20on%20a%0Adiverse%20set%20of%20continuous%20control%20tasks%20with%20varying%20difficulty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03055v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Exploration%2520with%2520PAC-Bayes%26entry.906535625%3DBahareh%2520Tasdighi%2520and%2520Manuel%2520Haussmann%2520and%2520Nicklas%2520Werge%2520and%2520Yi-Shan%2520Wu%2520and%2520Melih%2520Kandemir%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520for%2520continuous%2520control%2520under%2520delayed%2520rewards%2520is%2520an%250Aunder-explored%2520problem%2520despite%2520its%2520significance%2520in%2520real%2520life.%2520Many%2520complex%250Askills%2520build%2520on%2520intermediate%2520ones%2520as%2520prerequisites.%2520For%2520instance%252C%2520a%2520humanoid%250Alocomotor%2520has%2520to%2520learn%2520how%2520to%2520stand%2520before%2520it%2520can%2520learn%2520to%2520walk.%2520To%2520cope%2520with%250Adelayed%2520reward%252C%2520a%2520reinforcement%2520learning%2520agent%2520has%2520to%2520perform%2520deep%2520exploration.%250AHowever%252C%2520existing%2520deep%2520exploration%2520methods%2520are%2520designed%2520for%2520small%2520discrete%250Aaction%2520spaces%252C%2520and%2520their%2520successful%2520generalization%2520to%2520state-of-the-art%250Acontinuous%2520control%2520remains%2520unproven.%2520We%2520address%2520the%2520deep%2520exploration%2520problem%250Afor%2520the%2520first%2520time%2520from%2520a%2520PAC-Bayesian%2520perspective%2520in%2520the%2520context%2520of%250Aactor-critic%2520learning.%2520To%2520do%2520this%252C%2520we%2520quantify%2520the%2520error%2520of%2520the%2520Bellman%250Aoperator%2520through%2520a%2520PAC-Bayes%2520bound%252C%2520where%2520a%2520bootstrapped%2520ensemble%2520of%2520critic%250Anetworks%2520represents%2520the%2520posterior%2520distribution%252C%2520and%2520their%2520targets%2520serve%2520as%2520a%250Adata-informed%2520function-space%2520prior.%2520We%2520derive%2520an%2520objective%2520function%2520from%2520this%250Abound%2520and%2520use%2520it%2520to%2520train%2520the%2520critic%2520ensemble.%2520Each%2520critic%2520trains%2520an%2520individual%250Asoft%2520actor%2520network%252C%2520implemented%2520as%2520a%2520shared%2520trunk%2520and%2520critic-specific%2520heads.%250AThe%2520agent%2520performs%2520deep%2520exploration%2520by%2520acting%2520epsilon-greedily%2520on%2520a%2520randomly%250Achosen%2520actor%2520head.%2520Our%2520proposed%2520algorithm%252C%2520named%2520PAC-Bayesian%2520Actor-Critic%250A%2528PBAC%2529%252C%2520is%2520the%2520only%2520algorithm%2520to%2520consistently%2520discover%2520delayed%2520rewards%2520on%2520a%250Adiverse%2520set%2520of%2520continuous%2520control%2520tasks%2520with%2520varying%2520difficulty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03055v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Exploration%20with%20PAC-Bayes&entry.906535625=Bahareh%20Tasdighi%20and%20Manuel%20Haussmann%20and%20Nicklas%20Werge%20and%20Yi-Shan%20Wu%20and%20Melih%20Kandemir&entry.1292438233=%20%20Reinforcement%20learning%20for%20continuous%20control%20under%20delayed%20rewards%20is%20an%0Aunder-explored%20problem%20despite%20its%20significance%20in%20real%20life.%20Many%20complex%0Askills%20build%20on%20intermediate%20ones%20as%20prerequisites.%20For%20instance%2C%20a%20humanoid%0Alocomotor%20has%20to%20learn%20how%20to%20stand%20before%20it%20can%20learn%20to%20walk.%20To%20cope%20with%0Adelayed%20reward%2C%20a%20reinforcement%20learning%20agent%20has%20to%20perform%20deep%20exploration.%0AHowever%2C%20existing%20deep%20exploration%20methods%20are%20designed%20for%20small%20discrete%0Aaction%20spaces%2C%20and%20their%20successful%20generalization%20to%20state-of-the-art%0Acontinuous%20control%20remains%20unproven.%20We%20address%20the%20deep%20exploration%20problem%0Afor%20the%20first%20time%20from%20a%20PAC-Bayesian%20perspective%20in%20the%20context%20of%0Aactor-critic%20learning.%20To%20do%20this%2C%20we%20quantify%20the%20error%20of%20the%20Bellman%0Aoperator%20through%20a%20PAC-Bayes%20bound%2C%20where%20a%20bootstrapped%20ensemble%20of%20critic%0Anetworks%20represents%20the%20posterior%20distribution%2C%20and%20their%20targets%20serve%20as%20a%0Adata-informed%20function-space%20prior.%20We%20derive%20an%20objective%20function%20from%20this%0Abound%20and%20use%20it%20to%20train%20the%20critic%20ensemble.%20Each%20critic%20trains%20an%20individual%0Asoft%20actor%20network%2C%20implemented%20as%20a%20shared%20trunk%20and%20critic-specific%20heads.%0AThe%20agent%20performs%20deep%20exploration%20by%20acting%20epsilon-greedily%20on%20a%20randomly%0Achosen%20actor%20head.%20Our%20proposed%20algorithm%2C%20named%20PAC-Bayesian%20Actor-Critic%0A%28PBAC%29%2C%20is%20the%20only%20algorithm%20to%20consistently%20discover%20delayed%20rewards%20on%20a%0Adiverse%20set%20of%20continuous%20control%20tasks%20with%20varying%20difficulty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03055v3&entry.124074799=Read"},
{"title": "ViewpointDepth: A New Dataset for Monocular Depth Estimation Under\n  Viewpoint Shifts", "author": "Aurel Pjetri and Stefano Caprasecca and Leonardo Taccari and Matteo Simoncini and Henrique Pi\u00f1eiro Monteagudo and Wallace Walter and Douglas Coimbra de Andrade and Francesco Sambo and Andrew David Bagdanov", "abstract": "  Monocular depth estimation is a critical task for autonomous driving and many\nother computer vision applications. While significant progress has been made in\nthis field, the effects of viewpoint shifts on depth estimation models remain\nlargely underexplored. This paper introduces a novel dataset and evaluation\nmethodology to quantify the impact of different camera positions and\norientations on monocular depth estimation performance. We propose a ground\ntruth strategy based on homography estimation and object detection, eliminating\nthe need for expensive LIDAR sensors. We collect a diverse dataset of road\nscenes from multiple viewpoints and use it to assess the robustness of a modern\ndepth estimation model to geometric shifts. After assessing the validity of our\nstrategy on a public dataset, we provide valuable insights into the limitations\nof current models and highlight the importance of considering viewpoint\nvariations in real-world applications.\n", "link": "http://arxiv.org/abs/2409.17851v3", "date": "2025-02-03", "relevancy": 1.6721, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5664}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5535}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViewpointDepth%3A%20A%20New%20Dataset%20for%20Monocular%20Depth%20Estimation%20Under%0A%20%20Viewpoint%20Shifts&body=Title%3A%20ViewpointDepth%3A%20A%20New%20Dataset%20for%20Monocular%20Depth%20Estimation%20Under%0A%20%20Viewpoint%20Shifts%0AAuthor%3A%20Aurel%20Pjetri%20and%20Stefano%20Caprasecca%20and%20Leonardo%20Taccari%20and%20Matteo%20Simoncini%20and%20Henrique%20Pi%C3%B1eiro%20Monteagudo%20and%20Wallace%20Walter%20and%20Douglas%20Coimbra%20de%20Andrade%20and%20Francesco%20Sambo%20and%20Andrew%20David%20Bagdanov%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20is%20a%20critical%20task%20for%20autonomous%20driving%20and%20many%0Aother%20computer%20vision%20applications.%20While%20significant%20progress%20has%20been%20made%20in%0Athis%20field%2C%20the%20effects%20of%20viewpoint%20shifts%20on%20depth%20estimation%20models%20remain%0Alargely%20underexplored.%20This%20paper%20introduces%20a%20novel%20dataset%20and%20evaluation%0Amethodology%20to%20quantify%20the%20impact%20of%20different%20camera%20positions%20and%0Aorientations%20on%20monocular%20depth%20estimation%20performance.%20We%20propose%20a%20ground%0Atruth%20strategy%20based%20on%20homography%20estimation%20and%20object%20detection%2C%20eliminating%0Athe%20need%20for%20expensive%20LIDAR%20sensors.%20We%20collect%20a%20diverse%20dataset%20of%20road%0Ascenes%20from%20multiple%20viewpoints%20and%20use%20it%20to%20assess%20the%20robustness%20of%20a%20modern%0Adepth%20estimation%20model%20to%20geometric%20shifts.%20After%20assessing%20the%20validity%20of%20our%0Astrategy%20on%20a%20public%20dataset%2C%20we%20provide%20valuable%20insights%20into%20the%20limitations%0Aof%20current%20models%20and%20highlight%20the%20importance%20of%20considering%20viewpoint%0Avariations%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17851v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewpointDepth%253A%2520A%2520New%2520Dataset%2520for%2520Monocular%2520Depth%2520Estimation%2520Under%250A%2520%2520Viewpoint%2520Shifts%26entry.906535625%3DAurel%2520Pjetri%2520and%2520Stefano%2520Caprasecca%2520and%2520Leonardo%2520Taccari%2520and%2520Matteo%2520Simoncini%2520and%2520Henrique%2520Pi%25C3%25B1eiro%2520Monteagudo%2520and%2520Wallace%2520Walter%2520and%2520Douglas%2520Coimbra%2520de%2520Andrade%2520and%2520Francesco%2520Sambo%2520and%2520Andrew%2520David%2520Bagdanov%26entry.1292438233%3D%2520%2520Monocular%2520depth%2520estimation%2520is%2520a%2520critical%2520task%2520for%2520autonomous%2520driving%2520and%2520many%250Aother%2520computer%2520vision%2520applications.%2520While%2520significant%2520progress%2520has%2520been%2520made%2520in%250Athis%2520field%252C%2520the%2520effects%2520of%2520viewpoint%2520shifts%2520on%2520depth%2520estimation%2520models%2520remain%250Alargely%2520underexplored.%2520This%2520paper%2520introduces%2520a%2520novel%2520dataset%2520and%2520evaluation%250Amethodology%2520to%2520quantify%2520the%2520impact%2520of%2520different%2520camera%2520positions%2520and%250Aorientations%2520on%2520monocular%2520depth%2520estimation%2520performance.%2520We%2520propose%2520a%2520ground%250Atruth%2520strategy%2520based%2520on%2520homography%2520estimation%2520and%2520object%2520detection%252C%2520eliminating%250Athe%2520need%2520for%2520expensive%2520LIDAR%2520sensors.%2520We%2520collect%2520a%2520diverse%2520dataset%2520of%2520road%250Ascenes%2520from%2520multiple%2520viewpoints%2520and%2520use%2520it%2520to%2520assess%2520the%2520robustness%2520of%2520a%2520modern%250Adepth%2520estimation%2520model%2520to%2520geometric%2520shifts.%2520After%2520assessing%2520the%2520validity%2520of%2520our%250Astrategy%2520on%2520a%2520public%2520dataset%252C%2520we%2520provide%2520valuable%2520insights%2520into%2520the%2520limitations%250Aof%2520current%2520models%2520and%2520highlight%2520the%2520importance%2520of%2520considering%2520viewpoint%250Avariations%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17851v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViewpointDepth%3A%20A%20New%20Dataset%20for%20Monocular%20Depth%20Estimation%20Under%0A%20%20Viewpoint%20Shifts&entry.906535625=Aurel%20Pjetri%20and%20Stefano%20Caprasecca%20and%20Leonardo%20Taccari%20and%20Matteo%20Simoncini%20and%20Henrique%20Pi%C3%B1eiro%20Monteagudo%20and%20Wallace%20Walter%20and%20Douglas%20Coimbra%20de%20Andrade%20and%20Francesco%20Sambo%20and%20Andrew%20David%20Bagdanov&entry.1292438233=%20%20Monocular%20depth%20estimation%20is%20a%20critical%20task%20for%20autonomous%20driving%20and%20many%0Aother%20computer%20vision%20applications.%20While%20significant%20progress%20has%20been%20made%20in%0Athis%20field%2C%20the%20effects%20of%20viewpoint%20shifts%20on%20depth%20estimation%20models%20remain%0Alargely%20underexplored.%20This%20paper%20introduces%20a%20novel%20dataset%20and%20evaluation%0Amethodology%20to%20quantify%20the%20impact%20of%20different%20camera%20positions%20and%0Aorientations%20on%20monocular%20depth%20estimation%20performance.%20We%20propose%20a%20ground%0Atruth%20strategy%20based%20on%20homography%20estimation%20and%20object%20detection%2C%20eliminating%0Athe%20need%20for%20expensive%20LIDAR%20sensors.%20We%20collect%20a%20diverse%20dataset%20of%20road%0Ascenes%20from%20multiple%20viewpoints%20and%20use%20it%20to%20assess%20the%20robustness%20of%20a%20modern%0Adepth%20estimation%20model%20to%20geometric%20shifts.%20After%20assessing%20the%20validity%20of%20our%0Astrategy%20on%20a%20public%20dataset%2C%20we%20provide%20valuable%20insights%20into%20the%20limitations%0Aof%20current%20models%20and%20highlight%20the%20importance%20of%20considering%20viewpoint%0Avariations%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17851v3&entry.124074799=Read"},
{"title": "Symmetry-Aware Generative Modeling through Learned Canonicalization", "author": "Kusha Sareen and Daniel Levy and Arnab Kumar Mondal and S\u00e9kou-Oumar Kaba and Tara Akhound-Sadegh and Siamak Ravanbakhsh", "abstract": "  Generative modeling of symmetric densities has a range of applications in AI\nfor science, from drug discovery to physics simulations. The existing\ngenerative modeling paradigm for invariant densities combines an invariant\nprior with an equivariant generative process. However, we observe that this\ntechnique is not necessary and has several drawbacks resulting from the\nlimitations of equivariant networks. Instead, we propose to model a learned\nslice of the density so that only one representative element per orbit is\nlearned. To accomplish this, we learn a group-equivariant canonicalization\nnetwork that maps training samples to a canonical pose and train a\nnon-equivariant generative model over these canonicalized samples. We implement\nthis idea in the context of diffusion models. Our preliminary experimental\nresults on molecular modeling are promising, demonstrating improved sample\nquality and faster inference time.\n", "link": "http://arxiv.org/abs/2501.07773v2", "date": "2025-02-03", "relevancy": 1.6298, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5784}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5466}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symmetry-Aware%20Generative%20Modeling%20through%20Learned%20Canonicalization&body=Title%3A%20Symmetry-Aware%20Generative%20Modeling%20through%20Learned%20Canonicalization%0AAuthor%3A%20Kusha%20Sareen%20and%20Daniel%20Levy%20and%20Arnab%20Kumar%20Mondal%20and%20S%C3%A9kou-Oumar%20Kaba%20and%20Tara%20Akhound-Sadegh%20and%20Siamak%20Ravanbakhsh%0AAbstract%3A%20%20%20Generative%20modeling%20of%20symmetric%20densities%20has%20a%20range%20of%20applications%20in%20AI%0Afor%20science%2C%20from%20drug%20discovery%20to%20physics%20simulations.%20The%20existing%0Agenerative%20modeling%20paradigm%20for%20invariant%20densities%20combines%20an%20invariant%0Aprior%20with%20an%20equivariant%20generative%20process.%20However%2C%20we%20observe%20that%20this%0Atechnique%20is%20not%20necessary%20and%20has%20several%20drawbacks%20resulting%20from%20the%0Alimitations%20of%20equivariant%20networks.%20Instead%2C%20we%20propose%20to%20model%20a%20learned%0Aslice%20of%20the%20density%20so%20that%20only%20one%20representative%20element%20per%20orbit%20is%0Alearned.%20To%20accomplish%20this%2C%20we%20learn%20a%20group-equivariant%20canonicalization%0Anetwork%20that%20maps%20training%20samples%20to%20a%20canonical%20pose%20and%20train%20a%0Anon-equivariant%20generative%20model%20over%20these%20canonicalized%20samples.%20We%20implement%0Athis%20idea%20in%20the%20context%20of%20diffusion%20models.%20Our%20preliminary%20experimental%0Aresults%20on%20molecular%20modeling%20are%20promising%2C%20demonstrating%20improved%20sample%0Aquality%20and%20faster%20inference%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07773v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymmetry-Aware%2520Generative%2520Modeling%2520through%2520Learned%2520Canonicalization%26entry.906535625%3DKusha%2520Sareen%2520and%2520Daniel%2520Levy%2520and%2520Arnab%2520Kumar%2520Mondal%2520and%2520S%25C3%25A9kou-Oumar%2520Kaba%2520and%2520Tara%2520Akhound-Sadegh%2520and%2520Siamak%2520Ravanbakhsh%26entry.1292438233%3D%2520%2520Generative%2520modeling%2520of%2520symmetric%2520densities%2520has%2520a%2520range%2520of%2520applications%2520in%2520AI%250Afor%2520science%252C%2520from%2520drug%2520discovery%2520to%2520physics%2520simulations.%2520The%2520existing%250Agenerative%2520modeling%2520paradigm%2520for%2520invariant%2520densities%2520combines%2520an%2520invariant%250Aprior%2520with%2520an%2520equivariant%2520generative%2520process.%2520However%252C%2520we%2520observe%2520that%2520this%250Atechnique%2520is%2520not%2520necessary%2520and%2520has%2520several%2520drawbacks%2520resulting%2520from%2520the%250Alimitations%2520of%2520equivariant%2520networks.%2520Instead%252C%2520we%2520propose%2520to%2520model%2520a%2520learned%250Aslice%2520of%2520the%2520density%2520so%2520that%2520only%2520one%2520representative%2520element%2520per%2520orbit%2520is%250Alearned.%2520To%2520accomplish%2520this%252C%2520we%2520learn%2520a%2520group-equivariant%2520canonicalization%250Anetwork%2520that%2520maps%2520training%2520samples%2520to%2520a%2520canonical%2520pose%2520and%2520train%2520a%250Anon-equivariant%2520generative%2520model%2520over%2520these%2520canonicalized%2520samples.%2520We%2520implement%250Athis%2520idea%2520in%2520the%2520context%2520of%2520diffusion%2520models.%2520Our%2520preliminary%2520experimental%250Aresults%2520on%2520molecular%2520modeling%2520are%2520promising%252C%2520demonstrating%2520improved%2520sample%250Aquality%2520and%2520faster%2520inference%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07773v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symmetry-Aware%20Generative%20Modeling%20through%20Learned%20Canonicalization&entry.906535625=Kusha%20Sareen%20and%20Daniel%20Levy%20and%20Arnab%20Kumar%20Mondal%20and%20S%C3%A9kou-Oumar%20Kaba%20and%20Tara%20Akhound-Sadegh%20and%20Siamak%20Ravanbakhsh&entry.1292438233=%20%20Generative%20modeling%20of%20symmetric%20densities%20has%20a%20range%20of%20applications%20in%20AI%0Afor%20science%2C%20from%20drug%20discovery%20to%20physics%20simulations.%20The%20existing%0Agenerative%20modeling%20paradigm%20for%20invariant%20densities%20combines%20an%20invariant%0Aprior%20with%20an%20equivariant%20generative%20process.%20However%2C%20we%20observe%20that%20this%0Atechnique%20is%20not%20necessary%20and%20has%20several%20drawbacks%20resulting%20from%20the%0Alimitations%20of%20equivariant%20networks.%20Instead%2C%20we%20propose%20to%20model%20a%20learned%0Aslice%20of%20the%20density%20so%20that%20only%20one%20representative%20element%20per%20orbit%20is%0Alearned.%20To%20accomplish%20this%2C%20we%20learn%20a%20group-equivariant%20canonicalization%0Anetwork%20that%20maps%20training%20samples%20to%20a%20canonical%20pose%20and%20train%20a%0Anon-equivariant%20generative%20model%20over%20these%20canonicalized%20samples.%20We%20implement%0Athis%20idea%20in%20the%20context%20of%20diffusion%20models.%20Our%20preliminary%20experimental%0Aresults%20on%20molecular%20modeling%20are%20promising%2C%20demonstrating%20improved%20sample%0Aquality%20and%20faster%20inference%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07773v2&entry.124074799=Read"},
{"title": "Whom do Explanations Serve? A Systematic Literature Survey of User\n  Characteristics in Explainable Recommender Systems Evaluation", "author": "Kathrin Wardatzky and Oana Inel and Luca Rossetto and Abraham Bernstein", "abstract": "  Adding explanations to recommender systems is said to have multiple benefits,\nsuch as increasing user trust or system transparency. Previous work from other\napplication areas suggests that specific user characteristics impact the users'\nperception of the explanation. However, we rarely find this type of evaluation\nfor recommender systems explanations. This paper addresses this gap by\nsurveying 124 papers in which recommender systems explanations were evaluated\nin user studies. We analyzed their participant descriptions and study results\nwhere the impact of user characteristics on the explanation effects was\nmeasured. Our findings suggest that the results from the surveyed studies\npredominantly cover specific users who do not necessarily represent the users\nof recommender systems in the evaluation domain. This may seriously hamper the\ngeneralizability of any insights we may gain from current studies on\nexplanations in recommender systems. We further find inconsistencies in the\ndata reporting, which impacts the reproducibility of the reported results.\nHence, we recommend actions to move toward a more inclusive and reproducible\nevaluation.\n", "link": "http://arxiv.org/abs/2412.14193v2", "date": "2025-02-03", "relevancy": 1.6281, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4081}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whom%20do%20Explanations%20Serve%3F%20A%20Systematic%20Literature%20Survey%20of%20User%0A%20%20Characteristics%20in%20Explainable%20Recommender%20Systems%20Evaluation&body=Title%3A%20Whom%20do%20Explanations%20Serve%3F%20A%20Systematic%20Literature%20Survey%20of%20User%0A%20%20Characteristics%20in%20Explainable%20Recommender%20Systems%20Evaluation%0AAuthor%3A%20Kathrin%20Wardatzky%20and%20Oana%20Inel%20and%20Luca%20Rossetto%20and%20Abraham%20Bernstein%0AAbstract%3A%20%20%20Adding%20explanations%20to%20recommender%20systems%20is%20said%20to%20have%20multiple%20benefits%2C%0Asuch%20as%20increasing%20user%20trust%20or%20system%20transparency.%20Previous%20work%20from%20other%0Aapplication%20areas%20suggests%20that%20specific%20user%20characteristics%20impact%20the%20users%27%0Aperception%20of%20the%20explanation.%20However%2C%20we%20rarely%20find%20this%20type%20of%20evaluation%0Afor%20recommender%20systems%20explanations.%20This%20paper%20addresses%20this%20gap%20by%0Asurveying%20124%20papers%20in%20which%20recommender%20systems%20explanations%20were%20evaluated%0Ain%20user%20studies.%20We%20analyzed%20their%20participant%20descriptions%20and%20study%20results%0Awhere%20the%20impact%20of%20user%20characteristics%20on%20the%20explanation%20effects%20was%0Ameasured.%20Our%20findings%20suggest%20that%20the%20results%20from%20the%20surveyed%20studies%0Apredominantly%20cover%20specific%20users%20who%20do%20not%20necessarily%20represent%20the%20users%0Aof%20recommender%20systems%20in%20the%20evaluation%20domain.%20This%20may%20seriously%20hamper%20the%0Ageneralizability%20of%20any%20insights%20we%20may%20gain%20from%20current%20studies%20on%0Aexplanations%20in%20recommender%20systems.%20We%20further%20find%20inconsistencies%20in%20the%0Adata%20reporting%2C%20which%20impacts%20the%20reproducibility%20of%20the%20reported%20results.%0AHence%2C%20we%20recommend%20actions%20to%20move%20toward%20a%20more%20inclusive%20and%20reproducible%0Aevaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14193v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhom%2520do%2520Explanations%2520Serve%253F%2520A%2520Systematic%2520Literature%2520Survey%2520of%2520User%250A%2520%2520Characteristics%2520in%2520Explainable%2520Recommender%2520Systems%2520Evaluation%26entry.906535625%3DKathrin%2520Wardatzky%2520and%2520Oana%2520Inel%2520and%2520Luca%2520Rossetto%2520and%2520Abraham%2520Bernstein%26entry.1292438233%3D%2520%2520Adding%2520explanations%2520to%2520recommender%2520systems%2520is%2520said%2520to%2520have%2520multiple%2520benefits%252C%250Asuch%2520as%2520increasing%2520user%2520trust%2520or%2520system%2520transparency.%2520Previous%2520work%2520from%2520other%250Aapplication%2520areas%2520suggests%2520that%2520specific%2520user%2520characteristics%2520impact%2520the%2520users%2527%250Aperception%2520of%2520the%2520explanation.%2520However%252C%2520we%2520rarely%2520find%2520this%2520type%2520of%2520evaluation%250Afor%2520recommender%2520systems%2520explanations.%2520This%2520paper%2520addresses%2520this%2520gap%2520by%250Asurveying%2520124%2520papers%2520in%2520which%2520recommender%2520systems%2520explanations%2520were%2520evaluated%250Ain%2520user%2520studies.%2520We%2520analyzed%2520their%2520participant%2520descriptions%2520and%2520study%2520results%250Awhere%2520the%2520impact%2520of%2520user%2520characteristics%2520on%2520the%2520explanation%2520effects%2520was%250Ameasured.%2520Our%2520findings%2520suggest%2520that%2520the%2520results%2520from%2520the%2520surveyed%2520studies%250Apredominantly%2520cover%2520specific%2520users%2520who%2520do%2520not%2520necessarily%2520represent%2520the%2520users%250Aof%2520recommender%2520systems%2520in%2520the%2520evaluation%2520domain.%2520This%2520may%2520seriously%2520hamper%2520the%250Ageneralizability%2520of%2520any%2520insights%2520we%2520may%2520gain%2520from%2520current%2520studies%2520on%250Aexplanations%2520in%2520recommender%2520systems.%2520We%2520further%2520find%2520inconsistencies%2520in%2520the%250Adata%2520reporting%252C%2520which%2520impacts%2520the%2520reproducibility%2520of%2520the%2520reported%2520results.%250AHence%252C%2520we%2520recommend%2520actions%2520to%2520move%2520toward%2520a%2520more%2520inclusive%2520and%2520reproducible%250Aevaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14193v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whom%20do%20Explanations%20Serve%3F%20A%20Systematic%20Literature%20Survey%20of%20User%0A%20%20Characteristics%20in%20Explainable%20Recommender%20Systems%20Evaluation&entry.906535625=Kathrin%20Wardatzky%20and%20Oana%20Inel%20and%20Luca%20Rossetto%20and%20Abraham%20Bernstein&entry.1292438233=%20%20Adding%20explanations%20to%20recommender%20systems%20is%20said%20to%20have%20multiple%20benefits%2C%0Asuch%20as%20increasing%20user%20trust%20or%20system%20transparency.%20Previous%20work%20from%20other%0Aapplication%20areas%20suggests%20that%20specific%20user%20characteristics%20impact%20the%20users%27%0Aperception%20of%20the%20explanation.%20However%2C%20we%20rarely%20find%20this%20type%20of%20evaluation%0Afor%20recommender%20systems%20explanations.%20This%20paper%20addresses%20this%20gap%20by%0Asurveying%20124%20papers%20in%20which%20recommender%20systems%20explanations%20were%20evaluated%0Ain%20user%20studies.%20We%20analyzed%20their%20participant%20descriptions%20and%20study%20results%0Awhere%20the%20impact%20of%20user%20characteristics%20on%20the%20explanation%20effects%20was%0Ameasured.%20Our%20findings%20suggest%20that%20the%20results%20from%20the%20surveyed%20studies%0Apredominantly%20cover%20specific%20users%20who%20do%20not%20necessarily%20represent%20the%20users%0Aof%20recommender%20systems%20in%20the%20evaluation%20domain.%20This%20may%20seriously%20hamper%20the%0Ageneralizability%20of%20any%20insights%20we%20may%20gain%20from%20current%20studies%20on%0Aexplanations%20in%20recommender%20systems.%20We%20further%20find%20inconsistencies%20in%20the%0Adata%20reporting%2C%20which%20impacts%20the%20reproducibility%20of%20the%20reported%20results.%0AHence%2C%20we%20recommend%20actions%20to%20move%20toward%20a%20more%20inclusive%20and%20reproducible%0Aevaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14193v2&entry.124074799=Read"},
{"title": "\u03bb: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile\n  Manipulation Robotics", "author": "Ahmed Jaafar and Shreyas Sundara Raman and Yichen Wei and Sudarshan Harithas and Sofia Juliani and Anneke Wernerfelt and Benedict Quartey and Ifrah Idrees and Jason Xinyu Liu and Stefanie Tellex", "abstract": "  Efficiently learning and executing long-horizon mobile manipulation (MoMa)\ntasks is crucial for advancing robotics in household and workplace settings.\nHowever, current MoMa models are data-inefficient, underscoring the need for\nimproved models that require realistic-sized benchmarks to evaluate their\nefficiency, which do not exist. To address this, we introduce the LAMBDA\n({\\lambda}) benchmark (Long-horizon Actions for Mobile-manipulation\nBenchmarking of Directed Activities), which evaluates the data efficiency of\nmodels on language-conditioned, long-horizon, multi-room, multi-floor,\npick-and-place tasks using a dataset of manageable size, more feasible for\ncollection. The benchmark includes 571 human-collected demonstrations that\nprovide realism and diversity in simulated and real-world settings. Unlike\nplanner-generated data, these trajectories offer natural variability and\nreplay-verifiability, ensuring robust learning and evaluation. We benchmark\nseveral models, including learning-based models and a neuro-symbolic modular\napproach combining foundation models with task and motion planning.\nLearning-based models show suboptimal success rates, even when leveraging\npretrained weights, underscoring significant data inefficiencies. However, the\nneuro-symbolic approach performs significantly better while being more data\nefficient. Findings highlight the need for more data-efficient learning-based\nMoMa approaches. {\\lambda} addresses this gap by serving as a key benchmark for\nevaluating the data efficiency of those future models in handling household\nrobotics tasks.\n", "link": "http://arxiv.org/abs/2412.05313v5", "date": "2025-02-03", "relevancy": 1.6135, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5936}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%CE%BB%3A%20A%20Benchmark%20for%20Data-Efficiency%20in%20Long-Horizon%20Indoor%20Mobile%0A%20%20Manipulation%20Robotics&body=Title%3A%20%CE%BB%3A%20A%20Benchmark%20for%20Data-Efficiency%20in%20Long-Horizon%20Indoor%20Mobile%0A%20%20Manipulation%20Robotics%0AAuthor%3A%20Ahmed%20Jaafar%20and%20Shreyas%20Sundara%20Raman%20and%20Yichen%20Wei%20and%20Sudarshan%20Harithas%20and%20Sofia%20Juliani%20and%20Anneke%20Wernerfelt%20and%20Benedict%20Quartey%20and%20Ifrah%20Idrees%20and%20Jason%20Xinyu%20Liu%20and%20Stefanie%20Tellex%0AAbstract%3A%20%20%20Efficiently%20learning%20and%20executing%20long-horizon%20mobile%20manipulation%20%28MoMa%29%0Atasks%20is%20crucial%20for%20advancing%20robotics%20in%20household%20and%20workplace%20settings.%0AHowever%2C%20current%20MoMa%20models%20are%20data-inefficient%2C%20underscoring%20the%20need%20for%0Aimproved%20models%20that%20require%20realistic-sized%20benchmarks%20to%20evaluate%20their%0Aefficiency%2C%20which%20do%20not%20exist.%20To%20address%20this%2C%20we%20introduce%20the%20LAMBDA%0A%28%7B%5Clambda%7D%29%20benchmark%20%28Long-horizon%20Actions%20for%20Mobile-manipulation%0ABenchmarking%20of%20Directed%20Activities%29%2C%20which%20evaluates%20the%20data%20efficiency%20of%0Amodels%20on%20language-conditioned%2C%20long-horizon%2C%20multi-room%2C%20multi-floor%2C%0Apick-and-place%20tasks%20using%20a%20dataset%20of%20manageable%20size%2C%20more%20feasible%20for%0Acollection.%20The%20benchmark%20includes%20571%20human-collected%20demonstrations%20that%0Aprovide%20realism%20and%20diversity%20in%20simulated%20and%20real-world%20settings.%20Unlike%0Aplanner-generated%20data%2C%20these%20trajectories%20offer%20natural%20variability%20and%0Areplay-verifiability%2C%20ensuring%20robust%20learning%20and%20evaluation.%20We%20benchmark%0Aseveral%20models%2C%20including%20learning-based%20models%20and%20a%20neuro-symbolic%20modular%0Aapproach%20combining%20foundation%20models%20with%20task%20and%20motion%20planning.%0ALearning-based%20models%20show%20suboptimal%20success%20rates%2C%20even%20when%20leveraging%0Apretrained%20weights%2C%20underscoring%20significant%20data%20inefficiencies.%20However%2C%20the%0Aneuro-symbolic%20approach%20performs%20significantly%20better%20while%20being%20more%20data%0Aefficient.%20Findings%20highlight%20the%20need%20for%20more%20data-efficient%20learning-based%0AMoMa%20approaches.%20%7B%5Clambda%7D%20addresses%20this%20gap%20by%20serving%20as%20a%20key%20benchmark%20for%0Aevaluating%20the%20data%20efficiency%20of%20those%20future%20models%20in%20handling%20household%0Arobotics%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05313v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%25CE%25BB%253A%2520A%2520Benchmark%2520for%2520Data-Efficiency%2520in%2520Long-Horizon%2520Indoor%2520Mobile%250A%2520%2520Manipulation%2520Robotics%26entry.906535625%3DAhmed%2520Jaafar%2520and%2520Shreyas%2520Sundara%2520Raman%2520and%2520Yichen%2520Wei%2520and%2520Sudarshan%2520Harithas%2520and%2520Sofia%2520Juliani%2520and%2520Anneke%2520Wernerfelt%2520and%2520Benedict%2520Quartey%2520and%2520Ifrah%2520Idrees%2520and%2520Jason%2520Xinyu%2520Liu%2520and%2520Stefanie%2520Tellex%26entry.1292438233%3D%2520%2520Efficiently%2520learning%2520and%2520executing%2520long-horizon%2520mobile%2520manipulation%2520%2528MoMa%2529%250Atasks%2520is%2520crucial%2520for%2520advancing%2520robotics%2520in%2520household%2520and%2520workplace%2520settings.%250AHowever%252C%2520current%2520MoMa%2520models%2520are%2520data-inefficient%252C%2520underscoring%2520the%2520need%2520for%250Aimproved%2520models%2520that%2520require%2520realistic-sized%2520benchmarks%2520to%2520evaluate%2520their%250Aefficiency%252C%2520which%2520do%2520not%2520exist.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520LAMBDA%250A%2528%257B%255Clambda%257D%2529%2520benchmark%2520%2528Long-horizon%2520Actions%2520for%2520Mobile-manipulation%250ABenchmarking%2520of%2520Directed%2520Activities%2529%252C%2520which%2520evaluates%2520the%2520data%2520efficiency%2520of%250Amodels%2520on%2520language-conditioned%252C%2520long-horizon%252C%2520multi-room%252C%2520multi-floor%252C%250Apick-and-place%2520tasks%2520using%2520a%2520dataset%2520of%2520manageable%2520size%252C%2520more%2520feasible%2520for%250Acollection.%2520The%2520benchmark%2520includes%2520571%2520human-collected%2520demonstrations%2520that%250Aprovide%2520realism%2520and%2520diversity%2520in%2520simulated%2520and%2520real-world%2520settings.%2520Unlike%250Aplanner-generated%2520data%252C%2520these%2520trajectories%2520offer%2520natural%2520variability%2520and%250Areplay-verifiability%252C%2520ensuring%2520robust%2520learning%2520and%2520evaluation.%2520We%2520benchmark%250Aseveral%2520models%252C%2520including%2520learning-based%2520models%2520and%2520a%2520neuro-symbolic%2520modular%250Aapproach%2520combining%2520foundation%2520models%2520with%2520task%2520and%2520motion%2520planning.%250ALearning-based%2520models%2520show%2520suboptimal%2520success%2520rates%252C%2520even%2520when%2520leveraging%250Apretrained%2520weights%252C%2520underscoring%2520significant%2520data%2520inefficiencies.%2520However%252C%2520the%250Aneuro-symbolic%2520approach%2520performs%2520significantly%2520better%2520while%2520being%2520more%2520data%250Aefficient.%2520Findings%2520highlight%2520the%2520need%2520for%2520more%2520data-efficient%2520learning-based%250AMoMa%2520approaches.%2520%257B%255Clambda%257D%2520addresses%2520this%2520gap%2520by%2520serving%2520as%2520a%2520key%2520benchmark%2520for%250Aevaluating%2520the%2520data%2520efficiency%2520of%2520those%2520future%2520models%2520in%2520handling%2520household%250Arobotics%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05313v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%CE%BB%3A%20A%20Benchmark%20for%20Data-Efficiency%20in%20Long-Horizon%20Indoor%20Mobile%0A%20%20Manipulation%20Robotics&entry.906535625=Ahmed%20Jaafar%20and%20Shreyas%20Sundara%20Raman%20and%20Yichen%20Wei%20and%20Sudarshan%20Harithas%20and%20Sofia%20Juliani%20and%20Anneke%20Wernerfelt%20and%20Benedict%20Quartey%20and%20Ifrah%20Idrees%20and%20Jason%20Xinyu%20Liu%20and%20Stefanie%20Tellex&entry.1292438233=%20%20Efficiently%20learning%20and%20executing%20long-horizon%20mobile%20manipulation%20%28MoMa%29%0Atasks%20is%20crucial%20for%20advancing%20robotics%20in%20household%20and%20workplace%20settings.%0AHowever%2C%20current%20MoMa%20models%20are%20data-inefficient%2C%20underscoring%20the%20need%20for%0Aimproved%20models%20that%20require%20realistic-sized%20benchmarks%20to%20evaluate%20their%0Aefficiency%2C%20which%20do%20not%20exist.%20To%20address%20this%2C%20we%20introduce%20the%20LAMBDA%0A%28%7B%5Clambda%7D%29%20benchmark%20%28Long-horizon%20Actions%20for%20Mobile-manipulation%0ABenchmarking%20of%20Directed%20Activities%29%2C%20which%20evaluates%20the%20data%20efficiency%20of%0Amodels%20on%20language-conditioned%2C%20long-horizon%2C%20multi-room%2C%20multi-floor%2C%0Apick-and-place%20tasks%20using%20a%20dataset%20of%20manageable%20size%2C%20more%20feasible%20for%0Acollection.%20The%20benchmark%20includes%20571%20human-collected%20demonstrations%20that%0Aprovide%20realism%20and%20diversity%20in%20simulated%20and%20real-world%20settings.%20Unlike%0Aplanner-generated%20data%2C%20these%20trajectories%20offer%20natural%20variability%20and%0Areplay-verifiability%2C%20ensuring%20robust%20learning%20and%20evaluation.%20We%20benchmark%0Aseveral%20models%2C%20including%20learning-based%20models%20and%20a%20neuro-symbolic%20modular%0Aapproach%20combining%20foundation%20models%20with%20task%20and%20motion%20planning.%0ALearning-based%20models%20show%20suboptimal%20success%20rates%2C%20even%20when%20leveraging%0Apretrained%20weights%2C%20underscoring%20significant%20data%20inefficiencies.%20However%2C%20the%0Aneuro-symbolic%20approach%20performs%20significantly%20better%20while%20being%20more%20data%0Aefficient.%20Findings%20highlight%20the%20need%20for%20more%20data-efficient%20learning-based%0AMoMa%20approaches.%20%7B%5Clambda%7D%20addresses%20this%20gap%20by%20serving%20as%20a%20key%20benchmark%20for%0Aevaluating%20the%20data%20efficiency%20of%20those%20future%20models%20in%20handling%20household%0Arobotics%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05313v5&entry.124074799=Read"},
{"title": "Task-free Lifelong Robot Learning with Retrieval-based Weighted Local\n  Adaptation", "author": "Pengzhi Yang and Xinyu Wang and Ruipeng Zhang and Cong Wang and Frans A. Oliehoek and Jens Kober", "abstract": "  A fundamental objective in intelligent robotics is to move towards lifelong\nlearning robot that can learn and adapt to unseen scenarios over time. However,\ncontinually learning new tasks would introduce catastrophic forgetting problems\ndue to data distribution shifts. To mitigate this, we store a subset of data\nfrom previous tasks and utilize it in two manners: leveraging experience replay\nto retain learned skills and applying a novel Retrieval-based Local Adaptation\ntechnique to restore relevant knowledge. Since a lifelong learning robot must\noperate in task-free scenarios, where task IDs and even boundaries are not\navailable, our method performs effectively without relying on such information.\nWe also incorporate a selective weighting mechanism to focus on the most\n\"forgotten\" skill segment, ensuring effective knowledge restoration.\nExperimental results across diverse manipulation tasks demonstrate that our\nframework provides a scalable paradigm for lifelong learning, enhancing robot\nperformance in open-ended, task-free scenarios.\n", "link": "http://arxiv.org/abs/2410.02995v3", "date": "2025-02-03", "relevancy": 1.6125, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5678}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5556}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-free%20Lifelong%20Robot%20Learning%20with%20Retrieval-based%20Weighted%20Local%0A%20%20Adaptation&body=Title%3A%20Task-free%20Lifelong%20Robot%20Learning%20with%20Retrieval-based%20Weighted%20Local%0A%20%20Adaptation%0AAuthor%3A%20Pengzhi%20Yang%20and%20Xinyu%20Wang%20and%20Ruipeng%20Zhang%20and%20Cong%20Wang%20and%20Frans%20A.%20Oliehoek%20and%20Jens%20Kober%0AAbstract%3A%20%20%20A%20fundamental%20objective%20in%20intelligent%20robotics%20is%20to%20move%20towards%20lifelong%0Alearning%20robot%20that%20can%20learn%20and%20adapt%20to%20unseen%20scenarios%20over%20time.%20However%2C%0Acontinually%20learning%20new%20tasks%20would%20introduce%20catastrophic%20forgetting%20problems%0Adue%20to%20data%20distribution%20shifts.%20To%20mitigate%20this%2C%20we%20store%20a%20subset%20of%20data%0Afrom%20previous%20tasks%20and%20utilize%20it%20in%20two%20manners%3A%20leveraging%20experience%20replay%0Ato%20retain%20learned%20skills%20and%20applying%20a%20novel%20Retrieval-based%20Local%20Adaptation%0Atechnique%20to%20restore%20relevant%20knowledge.%20Since%20a%20lifelong%20learning%20robot%20must%0Aoperate%20in%20task-free%20scenarios%2C%20where%20task%20IDs%20and%20even%20boundaries%20are%20not%0Aavailable%2C%20our%20method%20performs%20effectively%20without%20relying%20on%20such%20information.%0AWe%20also%20incorporate%20a%20selective%20weighting%20mechanism%20to%20focus%20on%20the%20most%0A%22forgotten%22%20skill%20segment%2C%20ensuring%20effective%20knowledge%20restoration.%0AExperimental%20results%20across%20diverse%20manipulation%20tasks%20demonstrate%20that%20our%0Aframework%20provides%20a%20scalable%20paradigm%20for%20lifelong%20learning%2C%20enhancing%20robot%0Aperformance%20in%20open-ended%2C%20task-free%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02995v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-free%2520Lifelong%2520Robot%2520Learning%2520with%2520Retrieval-based%2520Weighted%2520Local%250A%2520%2520Adaptation%26entry.906535625%3DPengzhi%2520Yang%2520and%2520Xinyu%2520Wang%2520and%2520Ruipeng%2520Zhang%2520and%2520Cong%2520Wang%2520and%2520Frans%2520A.%2520Oliehoek%2520and%2520Jens%2520Kober%26entry.1292438233%3D%2520%2520A%2520fundamental%2520objective%2520in%2520intelligent%2520robotics%2520is%2520to%2520move%2520towards%2520lifelong%250Alearning%2520robot%2520that%2520can%2520learn%2520and%2520adapt%2520to%2520unseen%2520scenarios%2520over%2520time.%2520However%252C%250Acontinually%2520learning%2520new%2520tasks%2520would%2520introduce%2520catastrophic%2520forgetting%2520problems%250Adue%2520to%2520data%2520distribution%2520shifts.%2520To%2520mitigate%2520this%252C%2520we%2520store%2520a%2520subset%2520of%2520data%250Afrom%2520previous%2520tasks%2520and%2520utilize%2520it%2520in%2520two%2520manners%253A%2520leveraging%2520experience%2520replay%250Ato%2520retain%2520learned%2520skills%2520and%2520applying%2520a%2520novel%2520Retrieval-based%2520Local%2520Adaptation%250Atechnique%2520to%2520restore%2520relevant%2520knowledge.%2520Since%2520a%2520lifelong%2520learning%2520robot%2520must%250Aoperate%2520in%2520task-free%2520scenarios%252C%2520where%2520task%2520IDs%2520and%2520even%2520boundaries%2520are%2520not%250Aavailable%252C%2520our%2520method%2520performs%2520effectively%2520without%2520relying%2520on%2520such%2520information.%250AWe%2520also%2520incorporate%2520a%2520selective%2520weighting%2520mechanism%2520to%2520focus%2520on%2520the%2520most%250A%2522forgotten%2522%2520skill%2520segment%252C%2520ensuring%2520effective%2520knowledge%2520restoration.%250AExperimental%2520results%2520across%2520diverse%2520manipulation%2520tasks%2520demonstrate%2520that%2520our%250Aframework%2520provides%2520a%2520scalable%2520paradigm%2520for%2520lifelong%2520learning%252C%2520enhancing%2520robot%250Aperformance%2520in%2520open-ended%252C%2520task-free%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02995v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-free%20Lifelong%20Robot%20Learning%20with%20Retrieval-based%20Weighted%20Local%0A%20%20Adaptation&entry.906535625=Pengzhi%20Yang%20and%20Xinyu%20Wang%20and%20Ruipeng%20Zhang%20and%20Cong%20Wang%20and%20Frans%20A.%20Oliehoek%20and%20Jens%20Kober&entry.1292438233=%20%20A%20fundamental%20objective%20in%20intelligent%20robotics%20is%20to%20move%20towards%20lifelong%0Alearning%20robot%20that%20can%20learn%20and%20adapt%20to%20unseen%20scenarios%20over%20time.%20However%2C%0Acontinually%20learning%20new%20tasks%20would%20introduce%20catastrophic%20forgetting%20problems%0Adue%20to%20data%20distribution%20shifts.%20To%20mitigate%20this%2C%20we%20store%20a%20subset%20of%20data%0Afrom%20previous%20tasks%20and%20utilize%20it%20in%20two%20manners%3A%20leveraging%20experience%20replay%0Ato%20retain%20learned%20skills%20and%20applying%20a%20novel%20Retrieval-based%20Local%20Adaptation%0Atechnique%20to%20restore%20relevant%20knowledge.%20Since%20a%20lifelong%20learning%20robot%20must%0Aoperate%20in%20task-free%20scenarios%2C%20where%20task%20IDs%20and%20even%20boundaries%20are%20not%0Aavailable%2C%20our%20method%20performs%20effectively%20without%20relying%20on%20such%20information.%0AWe%20also%20incorporate%20a%20selective%20weighting%20mechanism%20to%20focus%20on%20the%20most%0A%22forgotten%22%20skill%20segment%2C%20ensuring%20effective%20knowledge%20restoration.%0AExperimental%20results%20across%20diverse%20manipulation%20tasks%20demonstrate%20that%20our%0Aframework%20provides%20a%20scalable%20paradigm%20for%20lifelong%20learning%2C%20enhancing%20robot%0Aperformance%20in%20open-ended%2C%20task-free%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02995v3&entry.124074799=Read"},
{"title": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining", "author": "Zhiqi Ge and Juncheng Li and Xinglei Pang and Minghe Gao and Kaihang Pan and Wang Lin and Hao Fei and Wenqiao Zhang and Siliang Tang and Yueting Zhuang", "abstract": "  Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks.\n", "link": "http://arxiv.org/abs/2412.10342v2", "date": "2025-02-03", "relevancy": 1.5915, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5232}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iris%3A%20Breaking%20GUI%20Complexity%20with%20Adaptive%20Focus%20and%20Self-Refining&body=Title%3A%20Iris%3A%20Breaking%20GUI%20Complexity%20with%20Adaptive%20Focus%20and%20Self-Refining%0AAuthor%3A%20Zhiqi%20Ge%20and%20Juncheng%20Li%20and%20Xinglei%20Pang%20and%20Minghe%20Gao%20and%20Kaihang%20Pan%20and%20Wang%20Lin%20and%20Hao%20Fei%20and%20Wenqiao%20Zhang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Digital%20agents%20are%20increasingly%20employed%20to%20automate%20tasks%20in%20interactive%0Adigital%20environments%20such%20as%20web%20pages%2C%20software%20applications%2C%20and%20operating%0Asystems.%20While%20text-based%20agents%20built%20on%20Large%20Language%20Models%20%28LLMs%29%20often%0Arequire%20frequent%20updates%20due%20to%20platform-specific%20APIs%2C%20visual%20agents%0Aleveraging%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20enhanced%20adaptability%0Aby%20interacting%20directly%20with%20Graphical%20User%20Interfaces%20%28GUIs%29.%20However%2C%20these%0Aagents%20face%20significant%20challenges%20in%20visual%20perception%2C%20particularly%20when%0Ahandling%20high-resolution%2C%20visually%20complex%20digital%20environments.%20This%20paper%0Aintroduces%20Iris%2C%20a%20foundational%20visual%20agent%20that%20addresses%20these%20challenges%0Athrough%20two%20key%20innovations%3A%20Information-Sensitive%20Cropping%20%28ISC%29%20and%0ASelf-Refining%20Dual%20Learning%20%28SRDL%29.%20ISC%20dynamically%20identifies%20and%20prioritizes%0Avisually%20dense%20regions%20using%20a%20edge%20detection%20algorithm%2C%20enabling%20efficient%0Aprocessing%20by%20allocating%20more%20computational%20resources%20to%20areas%20with%20higher%0Ainformation%20density.%20SRDL%20enhances%20the%20agent%27s%20ability%20to%20handle%20complex%20tasks%0Aby%20leveraging%20a%20dual-learning%20loop%2C%20where%20improvements%20in%20referring%20%28describing%0AUI%20elements%29%20reinforce%20grounding%20%28locating%20elements%29%20and%20vice%20versa%2C%20all%0Awithout%20requiring%20additional%20annotated%20data.%20Empirical%20evaluations%20demonstrate%0Athat%20Iris%20achieves%20state-of-the-art%20performance%20across%20multiple%20benchmarks%20with%0Aonly%20850K%20GUI%20annotations%2C%20outperforming%20methods%20using%2010x%20more%20training%20data.%0AThese%20improvements%20further%20translate%20to%20significant%20gains%20in%20both%20web%20and%20OS%0Aagent%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIris%253A%2520Breaking%2520GUI%2520Complexity%2520with%2520Adaptive%2520Focus%2520and%2520Self-Refining%26entry.906535625%3DZhiqi%2520Ge%2520and%2520Juncheng%2520Li%2520and%2520Xinglei%2520Pang%2520and%2520Minghe%2520Gao%2520and%2520Kaihang%2520Pan%2520and%2520Wang%2520Lin%2520and%2520Hao%2520Fei%2520and%2520Wenqiao%2520Zhang%2520and%2520Siliang%2520Tang%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Digital%2520agents%2520are%2520increasingly%2520employed%2520to%2520automate%2520tasks%2520in%2520interactive%250Adigital%2520environments%2520such%2520as%2520web%2520pages%252C%2520software%2520applications%252C%2520and%2520operating%250Asystems.%2520While%2520text-based%2520agents%2520built%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%250Arequire%2520frequent%2520updates%2520due%2520to%2520platform-specific%2520APIs%252C%2520visual%2520agents%250Aleveraging%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520offer%2520enhanced%2520adaptability%250Aby%2520interacting%2520directly%2520with%2520Graphical%2520User%2520Interfaces%2520%2528GUIs%2529.%2520However%252C%2520these%250Aagents%2520face%2520significant%2520challenges%2520in%2520visual%2520perception%252C%2520particularly%2520when%250Ahandling%2520high-resolution%252C%2520visually%2520complex%2520digital%2520environments.%2520This%2520paper%250Aintroduces%2520Iris%252C%2520a%2520foundational%2520visual%2520agent%2520that%2520addresses%2520these%2520challenges%250Athrough%2520two%2520key%2520innovations%253A%2520Information-Sensitive%2520Cropping%2520%2528ISC%2529%2520and%250ASelf-Refining%2520Dual%2520Learning%2520%2528SRDL%2529.%2520ISC%2520dynamically%2520identifies%2520and%2520prioritizes%250Avisually%2520dense%2520regions%2520using%2520a%2520edge%2520detection%2520algorithm%252C%2520enabling%2520efficient%250Aprocessing%2520by%2520allocating%2520more%2520computational%2520resources%2520to%2520areas%2520with%2520higher%250Ainformation%2520density.%2520SRDL%2520enhances%2520the%2520agent%2527s%2520ability%2520to%2520handle%2520complex%2520tasks%250Aby%2520leveraging%2520a%2520dual-learning%2520loop%252C%2520where%2520improvements%2520in%2520referring%2520%2528describing%250AUI%2520elements%2529%2520reinforce%2520grounding%2520%2528locating%2520elements%2529%2520and%2520vice%2520versa%252C%2520all%250Awithout%2520requiring%2520additional%2520annotated%2520data.%2520Empirical%2520evaluations%2520demonstrate%250Athat%2520Iris%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%2520benchmarks%2520with%250Aonly%2520850K%2520GUI%2520annotations%252C%2520outperforming%2520methods%2520using%252010x%2520more%2520training%2520data.%250AThese%2520improvements%2520further%2520translate%2520to%2520significant%2520gains%2520in%2520both%2520web%2520and%2520OS%250Aagent%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iris%3A%20Breaking%20GUI%20Complexity%20with%20Adaptive%20Focus%20and%20Self-Refining&entry.906535625=Zhiqi%20Ge%20and%20Juncheng%20Li%20and%20Xinglei%20Pang%20and%20Minghe%20Gao%20and%20Kaihang%20Pan%20and%20Wang%20Lin%20and%20Hao%20Fei%20and%20Wenqiao%20Zhang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Digital%20agents%20are%20increasingly%20employed%20to%20automate%20tasks%20in%20interactive%0Adigital%20environments%20such%20as%20web%20pages%2C%20software%20applications%2C%20and%20operating%0Asystems.%20While%20text-based%20agents%20built%20on%20Large%20Language%20Models%20%28LLMs%29%20often%0Arequire%20frequent%20updates%20due%20to%20platform-specific%20APIs%2C%20visual%20agents%0Aleveraging%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20enhanced%20adaptability%0Aby%20interacting%20directly%20with%20Graphical%20User%20Interfaces%20%28GUIs%29.%20However%2C%20these%0Aagents%20face%20significant%20challenges%20in%20visual%20perception%2C%20particularly%20when%0Ahandling%20high-resolution%2C%20visually%20complex%20digital%20environments.%20This%20paper%0Aintroduces%20Iris%2C%20a%20foundational%20visual%20agent%20that%20addresses%20these%20challenges%0Athrough%20two%20key%20innovations%3A%20Information-Sensitive%20Cropping%20%28ISC%29%20and%0ASelf-Refining%20Dual%20Learning%20%28SRDL%29.%20ISC%20dynamically%20identifies%20and%20prioritizes%0Avisually%20dense%20regions%20using%20a%20edge%20detection%20algorithm%2C%20enabling%20efficient%0Aprocessing%20by%20allocating%20more%20computational%20resources%20to%20areas%20with%20higher%0Ainformation%20density.%20SRDL%20enhances%20the%20agent%27s%20ability%20to%20handle%20complex%20tasks%0Aby%20leveraging%20a%20dual-learning%20loop%2C%20where%20improvements%20in%20referring%20%28describing%0AUI%20elements%29%20reinforce%20grounding%20%28locating%20elements%29%20and%20vice%20versa%2C%20all%0Awithout%20requiring%20additional%20annotated%20data.%20Empirical%20evaluations%20demonstrate%0Athat%20Iris%20achieves%20state-of-the-art%20performance%20across%20multiple%20benchmarks%20with%0Aonly%20850K%20GUI%20annotations%2C%20outperforming%20methods%20using%2010x%20more%20training%20data.%0AThese%20improvements%20further%20translate%20to%20significant%20gains%20in%20both%20web%20and%20OS%0Aagent%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10342v2&entry.124074799=Read"},
{"title": "Mind the Gap: a Spectral Analysis of Rank Collapse and Signal\n  Propagation in Attention Layers", "author": "Alireza Naderi and Thiziri Nait Saada and Jared Tanner", "abstract": "  Attention layers are the core component of transformers, the current\nstate-of-the-art neural network architecture. Alternatives to softmax-based\nattention are being explored due to its tendency to hinder effective\ninformation flow. Even at initialisation, it remains poorly understood why the\npropagation of signals and gradients through these random networks can be\npathological, resulting in issues known as (i) vanishing/exploding gradients\nand (ii) rank collapse $\\textit{in depth}$, i.e. when all tokens converge to a\nsingle representation along layers. While rank collapse in depth naturally\narises from repeated matrix multiplications$\\unicode{x2013}$a common pattern\nacross various architectures$\\unicode{x2013}$we identify an additional and\npreviously unknown challenge unique to softmax attention layers: (iii) rank\ncollapse $\\textit{in width}$, which occurs as the context length increases.\nUsing Random Matrix Theory, we conduct a rigorous analysis that uncovers a\nspectral gap between the two largest singular values of the attention matrix as\nthe cause of (iii), which in turn exacerbates (i) and (ii). Building on this\ninsight, we propose a novel yet simple practical solution to mitigate rank\ncollapse in width by removing the outlier eigenvalue(s). Our theoretical\nframework offers a fresh perspective on recent practical studies, such as (Ye\net al., 2024; Ali et al., 2023), whose ad hoc solutions can now be interpreted\nas implicit efforts to address the spectral gap issue. This work provides\nvaluable theoretical support for ongoing large-scale empirical research,\nbringing theory and practice one step closer in the understanding of\ntransformers.\n", "link": "http://arxiv.org/abs/2410.07799v2", "date": "2025-02-03", "relevancy": 1.5899, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5703}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.539}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Gap%3A%20a%20Spectral%20Analysis%20of%20Rank%20Collapse%20and%20Signal%0A%20%20Propagation%20in%20Attention%20Layers&body=Title%3A%20Mind%20the%20Gap%3A%20a%20Spectral%20Analysis%20of%20Rank%20Collapse%20and%20Signal%0A%20%20Propagation%20in%20Attention%20Layers%0AAuthor%3A%20Alireza%20Naderi%20and%20Thiziri%20Nait%20Saada%20and%20Jared%20Tanner%0AAbstract%3A%20%20%20Attention%20layers%20are%20the%20core%20component%20of%20transformers%2C%20the%20current%0Astate-of-the-art%20neural%20network%20architecture.%20Alternatives%20to%20softmax-based%0Aattention%20are%20being%20explored%20due%20to%20its%20tendency%20to%20hinder%20effective%0Ainformation%20flow.%20Even%20at%20initialisation%2C%20it%20remains%20poorly%20understood%20why%20the%0Apropagation%20of%20signals%20and%20gradients%20through%20these%20random%20networks%20can%20be%0Apathological%2C%20resulting%20in%20issues%20known%20as%20%28i%29%20vanishing/exploding%20gradients%0Aand%20%28ii%29%20rank%20collapse%20%24%5Ctextit%7Bin%20depth%7D%24%2C%20i.e.%20when%20all%20tokens%20converge%20to%20a%0Asingle%20representation%20along%20layers.%20While%20rank%20collapse%20in%20depth%20naturally%0Aarises%20from%20repeated%20matrix%20multiplications%24%5Cunicode%7Bx2013%7D%24a%20common%20pattern%0Aacross%20various%20architectures%24%5Cunicode%7Bx2013%7D%24we%20identify%20an%20additional%20and%0Apreviously%20unknown%20challenge%20unique%20to%20softmax%20attention%20layers%3A%20%28iii%29%20rank%0Acollapse%20%24%5Ctextit%7Bin%20width%7D%24%2C%20which%20occurs%20as%20the%20context%20length%20increases.%0AUsing%20Random%20Matrix%20Theory%2C%20we%20conduct%20a%20rigorous%20analysis%20that%20uncovers%20a%0Aspectral%20gap%20between%20the%20two%20largest%20singular%20values%20of%20the%20attention%20matrix%20as%0Athe%20cause%20of%20%28iii%29%2C%20which%20in%20turn%20exacerbates%20%28i%29%20and%20%28ii%29.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20a%20novel%20yet%20simple%20practical%20solution%20to%20mitigate%20rank%0Acollapse%20in%20width%20by%20removing%20the%20outlier%20eigenvalue%28s%29.%20Our%20theoretical%0Aframework%20offers%20a%20fresh%20perspective%20on%20recent%20practical%20studies%2C%20such%20as%20%28Ye%0Aet%20al.%2C%202024%3B%20Ali%20et%20al.%2C%202023%29%2C%20whose%20ad%20hoc%20solutions%20can%20now%20be%20interpreted%0Aas%20implicit%20efforts%20to%20address%20the%20spectral%20gap%20issue.%20This%20work%20provides%0Avaluable%20theoretical%20support%20for%20ongoing%20large-scale%20empirical%20research%2C%0Abringing%20theory%20and%20practice%20one%20step%20closer%20in%20the%20understanding%20of%0Atransformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07799v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Gap%253A%2520a%2520Spectral%2520Analysis%2520of%2520Rank%2520Collapse%2520and%2520Signal%250A%2520%2520Propagation%2520in%2520Attention%2520Layers%26entry.906535625%3DAlireza%2520Naderi%2520and%2520Thiziri%2520Nait%2520Saada%2520and%2520Jared%2520Tanner%26entry.1292438233%3D%2520%2520Attention%2520layers%2520are%2520the%2520core%2520component%2520of%2520transformers%252C%2520the%2520current%250Astate-of-the-art%2520neural%2520network%2520architecture.%2520Alternatives%2520to%2520softmax-based%250Aattention%2520are%2520being%2520explored%2520due%2520to%2520its%2520tendency%2520to%2520hinder%2520effective%250Ainformation%2520flow.%2520Even%2520at%2520initialisation%252C%2520it%2520remains%2520poorly%2520understood%2520why%2520the%250Apropagation%2520of%2520signals%2520and%2520gradients%2520through%2520these%2520random%2520networks%2520can%2520be%250Apathological%252C%2520resulting%2520in%2520issues%2520known%2520as%2520%2528i%2529%2520vanishing/exploding%2520gradients%250Aand%2520%2528ii%2529%2520rank%2520collapse%2520%2524%255Ctextit%257Bin%2520depth%257D%2524%252C%2520i.e.%2520when%2520all%2520tokens%2520converge%2520to%2520a%250Asingle%2520representation%2520along%2520layers.%2520While%2520rank%2520collapse%2520in%2520depth%2520naturally%250Aarises%2520from%2520repeated%2520matrix%2520multiplications%2524%255Cunicode%257Bx2013%257D%2524a%2520common%2520pattern%250Aacross%2520various%2520architectures%2524%255Cunicode%257Bx2013%257D%2524we%2520identify%2520an%2520additional%2520and%250Apreviously%2520unknown%2520challenge%2520unique%2520to%2520softmax%2520attention%2520layers%253A%2520%2528iii%2529%2520rank%250Acollapse%2520%2524%255Ctextit%257Bin%2520width%257D%2524%252C%2520which%2520occurs%2520as%2520the%2520context%2520length%2520increases.%250AUsing%2520Random%2520Matrix%2520Theory%252C%2520we%2520conduct%2520a%2520rigorous%2520analysis%2520that%2520uncovers%2520a%250Aspectral%2520gap%2520between%2520the%2520two%2520largest%2520singular%2520values%2520of%2520the%2520attention%2520matrix%2520as%250Athe%2520cause%2520of%2520%2528iii%2529%252C%2520which%2520in%2520turn%2520exacerbates%2520%2528i%2529%2520and%2520%2528ii%2529.%2520Building%2520on%2520this%250Ainsight%252C%2520we%2520propose%2520a%2520novel%2520yet%2520simple%2520practical%2520solution%2520to%2520mitigate%2520rank%250Acollapse%2520in%2520width%2520by%2520removing%2520the%2520outlier%2520eigenvalue%2528s%2529.%2520Our%2520theoretical%250Aframework%2520offers%2520a%2520fresh%2520perspective%2520on%2520recent%2520practical%2520studies%252C%2520such%2520as%2520%2528Ye%250Aet%2520al.%252C%25202024%253B%2520Ali%2520et%2520al.%252C%25202023%2529%252C%2520whose%2520ad%2520hoc%2520solutions%2520can%2520now%2520be%2520interpreted%250Aas%2520implicit%2520efforts%2520to%2520address%2520the%2520spectral%2520gap%2520issue.%2520This%2520work%2520provides%250Avaluable%2520theoretical%2520support%2520for%2520ongoing%2520large-scale%2520empirical%2520research%252C%250Abringing%2520theory%2520and%2520practice%2520one%2520step%2520closer%2520in%2520the%2520understanding%2520of%250Atransformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07799v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Gap%3A%20a%20Spectral%20Analysis%20of%20Rank%20Collapse%20and%20Signal%0A%20%20Propagation%20in%20Attention%20Layers&entry.906535625=Alireza%20Naderi%20and%20Thiziri%20Nait%20Saada%20and%20Jared%20Tanner&entry.1292438233=%20%20Attention%20layers%20are%20the%20core%20component%20of%20transformers%2C%20the%20current%0Astate-of-the-art%20neural%20network%20architecture.%20Alternatives%20to%20softmax-based%0Aattention%20are%20being%20explored%20due%20to%20its%20tendency%20to%20hinder%20effective%0Ainformation%20flow.%20Even%20at%20initialisation%2C%20it%20remains%20poorly%20understood%20why%20the%0Apropagation%20of%20signals%20and%20gradients%20through%20these%20random%20networks%20can%20be%0Apathological%2C%20resulting%20in%20issues%20known%20as%20%28i%29%20vanishing/exploding%20gradients%0Aand%20%28ii%29%20rank%20collapse%20%24%5Ctextit%7Bin%20depth%7D%24%2C%20i.e.%20when%20all%20tokens%20converge%20to%20a%0Asingle%20representation%20along%20layers.%20While%20rank%20collapse%20in%20depth%20naturally%0Aarises%20from%20repeated%20matrix%20multiplications%24%5Cunicode%7Bx2013%7D%24a%20common%20pattern%0Aacross%20various%20architectures%24%5Cunicode%7Bx2013%7D%24we%20identify%20an%20additional%20and%0Apreviously%20unknown%20challenge%20unique%20to%20softmax%20attention%20layers%3A%20%28iii%29%20rank%0Acollapse%20%24%5Ctextit%7Bin%20width%7D%24%2C%20which%20occurs%20as%20the%20context%20length%20increases.%0AUsing%20Random%20Matrix%20Theory%2C%20we%20conduct%20a%20rigorous%20analysis%20that%20uncovers%20a%0Aspectral%20gap%20between%20the%20two%20largest%20singular%20values%20of%20the%20attention%20matrix%20as%0Athe%20cause%20of%20%28iii%29%2C%20which%20in%20turn%20exacerbates%20%28i%29%20and%20%28ii%29.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20a%20novel%20yet%20simple%20practical%20solution%20to%20mitigate%20rank%0Acollapse%20in%20width%20by%20removing%20the%20outlier%20eigenvalue%28s%29.%20Our%20theoretical%0Aframework%20offers%20a%20fresh%20perspective%20on%20recent%20practical%20studies%2C%20such%20as%20%28Ye%0Aet%20al.%2C%202024%3B%20Ali%20et%20al.%2C%202023%29%2C%20whose%20ad%20hoc%20solutions%20can%20now%20be%20interpreted%0Aas%20implicit%20efforts%20to%20address%20the%20spectral%20gap%20issue.%20This%20work%20provides%0Avaluable%20theoretical%20support%20for%20ongoing%20large-scale%20empirical%20research%2C%0Abringing%20theory%20and%20practice%20one%20step%20closer%20in%20the%20understanding%20of%0Atransformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07799v2&entry.124074799=Read"},
{"title": "Boosting Asynchronous Decentralized Learning with Model Fragmentation", "author": "Sayan Biswas and Anne-Marie Kermarrec and Alexis Marouani and Rafael Pires and Rishi Sharma and Martijn de Vos", "abstract": "  Decentralized learning (DL) is an emerging technique that allows nodes on the\nweb to collaboratively train machine learning models without sharing raw data.\nDealing with stragglers, i.e., nodes with slower compute or communication than\nothers, is a key challenge in DL. We present DivShare, a novel asynchronous DL\nalgorithm that achieves fast model convergence in the presence of communication\nstragglers. DivShare achieves this by having nodes fragment their models into\nparameter subsets and send, in parallel to computation, each subset to a random\nsample of other nodes instead of sequentially exchanging full models. The\ntransfer of smaller fragments allows more efficient usage of the collective\nbandwidth and enables nodes with slow network links to quickly contribute with\nat least some of their model parameters. By theoretically proving the\nconvergence of DivShare, we provide, to the best of our knowledge, the first\nformal proof of convergence for a DL algorithm that accounts for the effects of\nasynchronous communication with delays. We experimentally evaluate DivShare\nagainst two state-of-the-art DL baselines, AD-PSGD and Swift, and with two\nstandard datasets, CIFAR-10 and MovieLens. We find that DivShare with\ncommunication stragglers lowers time-to-accuracy by up to 3.9x compared to\nAD-PSGD on the CIFAR-10 dataset. Compared to baselines, DivShare also achieves\nup to 19.4% better accuracy and 9.5% lower test loss on the CIFAR-10 and\nMovieLens datasets, respectively.\n", "link": "http://arxiv.org/abs/2410.12918v2", "date": "2025-02-03", "relevancy": 1.5749, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5712}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5132}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Asynchronous%20Decentralized%20Learning%20with%20Model%20Fragmentation&body=Title%3A%20Boosting%20Asynchronous%20Decentralized%20Learning%20with%20Model%20Fragmentation%0AAuthor%3A%20Sayan%20Biswas%20and%20Anne-Marie%20Kermarrec%20and%20Alexis%20Marouani%20and%20Rafael%20Pires%20and%20Rishi%20Sharma%20and%20Martijn%20de%20Vos%0AAbstract%3A%20%20%20Decentralized%20learning%20%28DL%29%20is%20an%20emerging%20technique%20that%20allows%20nodes%20on%20the%0Aweb%20to%20collaboratively%20train%20machine%20learning%20models%20without%20sharing%20raw%20data.%0ADealing%20with%20stragglers%2C%20i.e.%2C%20nodes%20with%20slower%20compute%20or%20communication%20than%0Aothers%2C%20is%20a%20key%20challenge%20in%20DL.%20We%20present%20DivShare%2C%20a%20novel%20asynchronous%20DL%0Aalgorithm%20that%20achieves%20fast%20model%20convergence%20in%20the%20presence%20of%20communication%0Astragglers.%20DivShare%20achieves%20this%20by%20having%20nodes%20fragment%20their%20models%20into%0Aparameter%20subsets%20and%20send%2C%20in%20parallel%20to%20computation%2C%20each%20subset%20to%20a%20random%0Asample%20of%20other%20nodes%20instead%20of%20sequentially%20exchanging%20full%20models.%20The%0Atransfer%20of%20smaller%20fragments%20allows%20more%20efficient%20usage%20of%20the%20collective%0Abandwidth%20and%20enables%20nodes%20with%20slow%20network%20links%20to%20quickly%20contribute%20with%0Aat%20least%20some%20of%20their%20model%20parameters.%20By%20theoretically%20proving%20the%0Aconvergence%20of%20DivShare%2C%20we%20provide%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%0Aformal%20proof%20of%20convergence%20for%20a%20DL%20algorithm%20that%20accounts%20for%20the%20effects%20of%0Aasynchronous%20communication%20with%20delays.%20We%20experimentally%20evaluate%20DivShare%0Aagainst%20two%20state-of-the-art%20DL%20baselines%2C%20AD-PSGD%20and%20Swift%2C%20and%20with%20two%0Astandard%20datasets%2C%20CIFAR-10%20and%20MovieLens.%20We%20find%20that%20DivShare%20with%0Acommunication%20stragglers%20lowers%20time-to-accuracy%20by%20up%20to%203.9x%20compared%20to%0AAD-PSGD%20on%20the%20CIFAR-10%20dataset.%20Compared%20to%20baselines%2C%20DivShare%20also%20achieves%0Aup%20to%2019.4%25%20better%20accuracy%20and%209.5%25%20lower%20test%20loss%20on%20the%20CIFAR-10%20and%0AMovieLens%20datasets%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Asynchronous%2520Decentralized%2520Learning%2520with%2520Model%2520Fragmentation%26entry.906535625%3DSayan%2520Biswas%2520and%2520Anne-Marie%2520Kermarrec%2520and%2520Alexis%2520Marouani%2520and%2520Rafael%2520Pires%2520and%2520Rishi%2520Sharma%2520and%2520Martijn%2520de%2520Vos%26entry.1292438233%3D%2520%2520Decentralized%2520learning%2520%2528DL%2529%2520is%2520an%2520emerging%2520technique%2520that%2520allows%2520nodes%2520on%2520the%250Aweb%2520to%2520collaboratively%2520train%2520machine%2520learning%2520models%2520without%2520sharing%2520raw%2520data.%250ADealing%2520with%2520stragglers%252C%2520i.e.%252C%2520nodes%2520with%2520slower%2520compute%2520or%2520communication%2520than%250Aothers%252C%2520is%2520a%2520key%2520challenge%2520in%2520DL.%2520We%2520present%2520DivShare%252C%2520a%2520novel%2520asynchronous%2520DL%250Aalgorithm%2520that%2520achieves%2520fast%2520model%2520convergence%2520in%2520the%2520presence%2520of%2520communication%250Astragglers.%2520DivShare%2520achieves%2520this%2520by%2520having%2520nodes%2520fragment%2520their%2520models%2520into%250Aparameter%2520subsets%2520and%2520send%252C%2520in%2520parallel%2520to%2520computation%252C%2520each%2520subset%2520to%2520a%2520random%250Asample%2520of%2520other%2520nodes%2520instead%2520of%2520sequentially%2520exchanging%2520full%2520models.%2520The%250Atransfer%2520of%2520smaller%2520fragments%2520allows%2520more%2520efficient%2520usage%2520of%2520the%2520collective%250Abandwidth%2520and%2520enables%2520nodes%2520with%2520slow%2520network%2520links%2520to%2520quickly%2520contribute%2520with%250Aat%2520least%2520some%2520of%2520their%2520model%2520parameters.%2520By%2520theoretically%2520proving%2520the%250Aconvergence%2520of%2520DivShare%252C%2520we%2520provide%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%250Aformal%2520proof%2520of%2520convergence%2520for%2520a%2520DL%2520algorithm%2520that%2520accounts%2520for%2520the%2520effects%2520of%250Aasynchronous%2520communication%2520with%2520delays.%2520We%2520experimentally%2520evaluate%2520DivShare%250Aagainst%2520two%2520state-of-the-art%2520DL%2520baselines%252C%2520AD-PSGD%2520and%2520Swift%252C%2520and%2520with%2520two%250Astandard%2520datasets%252C%2520CIFAR-10%2520and%2520MovieLens.%2520We%2520find%2520that%2520DivShare%2520with%250Acommunication%2520stragglers%2520lowers%2520time-to-accuracy%2520by%2520up%2520to%25203.9x%2520compared%2520to%250AAD-PSGD%2520on%2520the%2520CIFAR-10%2520dataset.%2520Compared%2520to%2520baselines%252C%2520DivShare%2520also%2520achieves%250Aup%2520to%252019.4%2525%2520better%2520accuracy%2520and%25209.5%2525%2520lower%2520test%2520loss%2520on%2520the%2520CIFAR-10%2520and%250AMovieLens%2520datasets%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Asynchronous%20Decentralized%20Learning%20with%20Model%20Fragmentation&entry.906535625=Sayan%20Biswas%20and%20Anne-Marie%20Kermarrec%20and%20Alexis%20Marouani%20and%20Rafael%20Pires%20and%20Rishi%20Sharma%20and%20Martijn%20de%20Vos&entry.1292438233=%20%20Decentralized%20learning%20%28DL%29%20is%20an%20emerging%20technique%20that%20allows%20nodes%20on%20the%0Aweb%20to%20collaboratively%20train%20machine%20learning%20models%20without%20sharing%20raw%20data.%0ADealing%20with%20stragglers%2C%20i.e.%2C%20nodes%20with%20slower%20compute%20or%20communication%20than%0Aothers%2C%20is%20a%20key%20challenge%20in%20DL.%20We%20present%20DivShare%2C%20a%20novel%20asynchronous%20DL%0Aalgorithm%20that%20achieves%20fast%20model%20convergence%20in%20the%20presence%20of%20communication%0Astragglers.%20DivShare%20achieves%20this%20by%20having%20nodes%20fragment%20their%20models%20into%0Aparameter%20subsets%20and%20send%2C%20in%20parallel%20to%20computation%2C%20each%20subset%20to%20a%20random%0Asample%20of%20other%20nodes%20instead%20of%20sequentially%20exchanging%20full%20models.%20The%0Atransfer%20of%20smaller%20fragments%20allows%20more%20efficient%20usage%20of%20the%20collective%0Abandwidth%20and%20enables%20nodes%20with%20slow%20network%20links%20to%20quickly%20contribute%20with%0Aat%20least%20some%20of%20their%20model%20parameters.%20By%20theoretically%20proving%20the%0Aconvergence%20of%20DivShare%2C%20we%20provide%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%0Aformal%20proof%20of%20convergence%20for%20a%20DL%20algorithm%20that%20accounts%20for%20the%20effects%20of%0Aasynchronous%20communication%20with%20delays.%20We%20experimentally%20evaluate%20DivShare%0Aagainst%20two%20state-of-the-art%20DL%20baselines%2C%20AD-PSGD%20and%20Swift%2C%20and%20with%20two%0Astandard%20datasets%2C%20CIFAR-10%20and%20MovieLens.%20We%20find%20that%20DivShare%20with%0Acommunication%20stragglers%20lowers%20time-to-accuracy%20by%20up%20to%203.9x%20compared%20to%0AAD-PSGD%20on%20the%20CIFAR-10%20dataset.%20Compared%20to%20baselines%2C%20DivShare%20also%20achieves%0Aup%20to%2019.4%25%20better%20accuracy%20and%209.5%25%20lower%20test%20loss%20on%20the%20CIFAR-10%20and%0AMovieLens%20datasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12918v2&entry.124074799=Read"},
{"title": "Flow Matching: Markov Kernels, Stochastic Processes and Transport Plans", "author": "Christian Wald and Gabriele Steidl", "abstract": "  Among generative neural models, flow matching techniques stand out for their\nsimple applicability and good scaling properties. Here, velocity fields of\ncurves connecting a simple latent and a target distribution are learned. Then\nthe corresponding ordinary differential equation can be used to sample from a\ntarget distribution, starting in samples from the latent one. This paper\nreviews from a mathematical point of view different techniques to learn the\nvelocity fields of absolutely continuous curves in the Wasserstein geometry. We\nshow how the velocity fields can be characterized and learned via i) transport\nplans (couplings) between latent and target distributions, ii) Markov kernels\nand iii) stochastic processes, where the latter two include the coupling\napproach, but are in general broader. Besides this main goal, we show how flow\nmatching can be used for solving Bayesian inverse problems, where the\ndefinition of conditional Wasserstein distances plays a central role. Finally,\nwe briefly address continuous normalizing flows and score matching techniques,\nwhich approach the learning of velocity fields of curves from other directions.\n", "link": "http://arxiv.org/abs/2501.16839v2", "date": "2025-02-03", "relevancy": 1.5429, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5615}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5008}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow%20Matching%3A%20Markov%20Kernels%2C%20Stochastic%20Processes%20and%20Transport%20Plans&body=Title%3A%20Flow%20Matching%3A%20Markov%20Kernels%2C%20Stochastic%20Processes%20and%20Transport%20Plans%0AAuthor%3A%20Christian%20Wald%20and%20Gabriele%20Steidl%0AAbstract%3A%20%20%20Among%20generative%20neural%20models%2C%20flow%20matching%20techniques%20stand%20out%20for%20their%0Asimple%20applicability%20and%20good%20scaling%20properties.%20Here%2C%20velocity%20fields%20of%0Acurves%20connecting%20a%20simple%20latent%20and%20a%20target%20distribution%20are%20learned.%20Then%0Athe%20corresponding%20ordinary%20differential%20equation%20can%20be%20used%20to%20sample%20from%20a%0Atarget%20distribution%2C%20starting%20in%20samples%20from%20the%20latent%20one.%20This%20paper%0Areviews%20from%20a%20mathematical%20point%20of%20view%20different%20techniques%20to%20learn%20the%0Avelocity%20fields%20of%20absolutely%20continuous%20curves%20in%20the%20Wasserstein%20geometry.%20We%0Ashow%20how%20the%20velocity%20fields%20can%20be%20characterized%20and%20learned%20via%20i%29%20transport%0Aplans%20%28couplings%29%20between%20latent%20and%20target%20distributions%2C%20ii%29%20Markov%20kernels%0Aand%20iii%29%20stochastic%20processes%2C%20where%20the%20latter%20two%20include%20the%20coupling%0Aapproach%2C%20but%20are%20in%20general%20broader.%20Besides%20this%20main%20goal%2C%20we%20show%20how%20flow%0Amatching%20can%20be%20used%20for%20solving%20Bayesian%20inverse%20problems%2C%20where%20the%0Adefinition%20of%20conditional%20Wasserstein%20distances%20plays%20a%20central%20role.%20Finally%2C%0Awe%20briefly%20address%20continuous%20normalizing%20flows%20and%20score%20matching%20techniques%2C%0Awhich%20approach%20the%20learning%20of%20velocity%20fields%20of%20curves%20from%20other%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow%2520Matching%253A%2520Markov%2520Kernels%252C%2520Stochastic%2520Processes%2520and%2520Transport%2520Plans%26entry.906535625%3DChristian%2520Wald%2520and%2520Gabriele%2520Steidl%26entry.1292438233%3D%2520%2520Among%2520generative%2520neural%2520models%252C%2520flow%2520matching%2520techniques%2520stand%2520out%2520for%2520their%250Asimple%2520applicability%2520and%2520good%2520scaling%2520properties.%2520Here%252C%2520velocity%2520fields%2520of%250Acurves%2520connecting%2520a%2520simple%2520latent%2520and%2520a%2520target%2520distribution%2520are%2520learned.%2520Then%250Athe%2520corresponding%2520ordinary%2520differential%2520equation%2520can%2520be%2520used%2520to%2520sample%2520from%2520a%250Atarget%2520distribution%252C%2520starting%2520in%2520samples%2520from%2520the%2520latent%2520one.%2520This%2520paper%250Areviews%2520from%2520a%2520mathematical%2520point%2520of%2520view%2520different%2520techniques%2520to%2520learn%2520the%250Avelocity%2520fields%2520of%2520absolutely%2520continuous%2520curves%2520in%2520the%2520Wasserstein%2520geometry.%2520We%250Ashow%2520how%2520the%2520velocity%2520fields%2520can%2520be%2520characterized%2520and%2520learned%2520via%2520i%2529%2520transport%250Aplans%2520%2528couplings%2529%2520between%2520latent%2520and%2520target%2520distributions%252C%2520ii%2529%2520Markov%2520kernels%250Aand%2520iii%2529%2520stochastic%2520processes%252C%2520where%2520the%2520latter%2520two%2520include%2520the%2520coupling%250Aapproach%252C%2520but%2520are%2520in%2520general%2520broader.%2520Besides%2520this%2520main%2520goal%252C%2520we%2520show%2520how%2520flow%250Amatching%2520can%2520be%2520used%2520for%2520solving%2520Bayesian%2520inverse%2520problems%252C%2520where%2520the%250Adefinition%2520of%2520conditional%2520Wasserstein%2520distances%2520plays%2520a%2520central%2520role.%2520Finally%252C%250Awe%2520briefly%2520address%2520continuous%2520normalizing%2520flows%2520and%2520score%2520matching%2520techniques%252C%250Awhich%2520approach%2520the%2520learning%2520of%2520velocity%2520fields%2520of%2520curves%2520from%2520other%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow%20Matching%3A%20Markov%20Kernels%2C%20Stochastic%20Processes%20and%20Transport%20Plans&entry.906535625=Christian%20Wald%20and%20Gabriele%20Steidl&entry.1292438233=%20%20Among%20generative%20neural%20models%2C%20flow%20matching%20techniques%20stand%20out%20for%20their%0Asimple%20applicability%20and%20good%20scaling%20properties.%20Here%2C%20velocity%20fields%20of%0Acurves%20connecting%20a%20simple%20latent%20and%20a%20target%20distribution%20are%20learned.%20Then%0Athe%20corresponding%20ordinary%20differential%20equation%20can%20be%20used%20to%20sample%20from%20a%0Atarget%20distribution%2C%20starting%20in%20samples%20from%20the%20latent%20one.%20This%20paper%0Areviews%20from%20a%20mathematical%20point%20of%20view%20different%20techniques%20to%20learn%20the%0Avelocity%20fields%20of%20absolutely%20continuous%20curves%20in%20the%20Wasserstein%20geometry.%20We%0Ashow%20how%20the%20velocity%20fields%20can%20be%20characterized%20and%20learned%20via%20i%29%20transport%0Aplans%20%28couplings%29%20between%20latent%20and%20target%20distributions%2C%20ii%29%20Markov%20kernels%0Aand%20iii%29%20stochastic%20processes%2C%20where%20the%20latter%20two%20include%20the%20coupling%0Aapproach%2C%20but%20are%20in%20general%20broader.%20Besides%20this%20main%20goal%2C%20we%20show%20how%20flow%0Amatching%20can%20be%20used%20for%20solving%20Bayesian%20inverse%20problems%2C%20where%20the%0Adefinition%20of%20conditional%20Wasserstein%20distances%20plays%20a%20central%20role.%20Finally%2C%0Awe%20briefly%20address%20continuous%20normalizing%20flows%20and%20score%20matching%20techniques%2C%0Awhich%20approach%20the%20learning%20of%20velocity%20fields%20of%20curves%20from%20other%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16839v2&entry.124074799=Read"},
{"title": "Metareasoning in uncertain environments: a meta-BAMDP framework", "author": "Prakhar Godara and Tilman Diego Al\u00e9man and Angela J. Yu", "abstract": "  \\textit{Reasoning} may be viewed as an algorithm $P$ that makes a choice of\nan action $a^* \\in \\mathcal{A}$, aiming to optimize some outcome. However,\nexecuting $P$ itself bears costs (time, energy, limited capacity, etc.) and\nneeds to be considered alongside explicit utility obtained by making the choice\nin the underlying decision problem. Finding the right $P$ can itself be framed\nas an optimization problem over the space of reasoning processes $P$, generally\nreferred to as \\textit{metareasoning}. Conventionally, human metareasoning\nmodels assume that the agent knows the transition and reward distributions of\nthe underlying MDP. This paper generalizes such models by proposing a meta\nBayes-Adaptive MDP (meta-BAMDP) framework to handle metareasoning in\nenvironments with unknown reward/transition distributions, which encompasses a\nfar larger and more realistic set of planning problems that humans and AI\nsystems face. As a first step, we apply the framework to Bernoulli bandit\ntasks. Owing to the meta problem's complexity, our solutions are necessarily\napproximate. However, we introduce two novel theorems that significantly\nenhance the tractability of the problem, enabling stronger approximations that\nare robust within a range of assumptions grounded in realistic human\ndecision-making scenarios. These results offer a resource-rational perspective\nand a normative framework for understanding human exploration under cognitive\nconstraints, as well as providing experimentally testable predictions about\nhuman behavior in Bernoulli Bandit tasks.\n", "link": "http://arxiv.org/abs/2408.01253v2", "date": "2025-02-03", "relevancy": 1.5169, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5708}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4973}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metareasoning%20in%20uncertain%20environments%3A%20a%20meta-BAMDP%20framework&body=Title%3A%20Metareasoning%20in%20uncertain%20environments%3A%20a%20meta-BAMDP%20framework%0AAuthor%3A%20Prakhar%20Godara%20and%20Tilman%20Diego%20Al%C3%A9man%20and%20Angela%20J.%20Yu%0AAbstract%3A%20%20%20%5Ctextit%7BReasoning%7D%20may%20be%20viewed%20as%20an%20algorithm%20%24P%24%20that%20makes%20a%20choice%20of%0Aan%20action%20%24a%5E%2A%20%5Cin%20%5Cmathcal%7BA%7D%24%2C%20aiming%20to%20optimize%20some%20outcome.%20However%2C%0Aexecuting%20%24P%24%20itself%20bears%20costs%20%28time%2C%20energy%2C%20limited%20capacity%2C%20etc.%29%20and%0Aneeds%20to%20be%20considered%20alongside%20explicit%20utility%20obtained%20by%20making%20the%20choice%0Ain%20the%20underlying%20decision%20problem.%20Finding%20the%20right%20%24P%24%20can%20itself%20be%20framed%0Aas%20an%20optimization%20problem%20over%20the%20space%20of%20reasoning%20processes%20%24P%24%2C%20generally%0Areferred%20to%20as%20%5Ctextit%7Bmetareasoning%7D.%20Conventionally%2C%20human%20metareasoning%0Amodels%20assume%20that%20the%20agent%20knows%20the%20transition%20and%20reward%20distributions%20of%0Athe%20underlying%20MDP.%20This%20paper%20generalizes%20such%20models%20by%20proposing%20a%20meta%0ABayes-Adaptive%20MDP%20%28meta-BAMDP%29%20framework%20to%20handle%20metareasoning%20in%0Aenvironments%20with%20unknown%20reward/transition%20distributions%2C%20which%20encompasses%20a%0Afar%20larger%20and%20more%20realistic%20set%20of%20planning%20problems%20that%20humans%20and%20AI%0Asystems%20face.%20As%20a%20first%20step%2C%20we%20apply%20the%20framework%20to%20Bernoulli%20bandit%0Atasks.%20Owing%20to%20the%20meta%20problem%27s%20complexity%2C%20our%20solutions%20are%20necessarily%0Aapproximate.%20However%2C%20we%20introduce%20two%20novel%20theorems%20that%20significantly%0Aenhance%20the%20tractability%20of%20the%20problem%2C%20enabling%20stronger%20approximations%20that%0Aare%20robust%20within%20a%20range%20of%20assumptions%20grounded%20in%20realistic%20human%0Adecision-making%20scenarios.%20These%20results%20offer%20a%20resource-rational%20perspective%0Aand%20a%20normative%20framework%20for%20understanding%20human%20exploration%20under%20cognitive%0Aconstraints%2C%20as%20well%20as%20providing%20experimentally%20testable%20predictions%20about%0Ahuman%20behavior%20in%20Bernoulli%20Bandit%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01253v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetareasoning%2520in%2520uncertain%2520environments%253A%2520a%2520meta-BAMDP%2520framework%26entry.906535625%3DPrakhar%2520Godara%2520and%2520Tilman%2520Diego%2520Al%25C3%25A9man%2520and%2520Angela%2520J.%2520Yu%26entry.1292438233%3D%2520%2520%255Ctextit%257BReasoning%257D%2520may%2520be%2520viewed%2520as%2520an%2520algorithm%2520%2524P%2524%2520that%2520makes%2520a%2520choice%2520of%250Aan%2520action%2520%2524a%255E%252A%2520%255Cin%2520%255Cmathcal%257BA%257D%2524%252C%2520aiming%2520to%2520optimize%2520some%2520outcome.%2520However%252C%250Aexecuting%2520%2524P%2524%2520itself%2520bears%2520costs%2520%2528time%252C%2520energy%252C%2520limited%2520capacity%252C%2520etc.%2529%2520and%250Aneeds%2520to%2520be%2520considered%2520alongside%2520explicit%2520utility%2520obtained%2520by%2520making%2520the%2520choice%250Ain%2520the%2520underlying%2520decision%2520problem.%2520Finding%2520the%2520right%2520%2524P%2524%2520can%2520itself%2520be%2520framed%250Aas%2520an%2520optimization%2520problem%2520over%2520the%2520space%2520of%2520reasoning%2520processes%2520%2524P%2524%252C%2520generally%250Areferred%2520to%2520as%2520%255Ctextit%257Bmetareasoning%257D.%2520Conventionally%252C%2520human%2520metareasoning%250Amodels%2520assume%2520that%2520the%2520agent%2520knows%2520the%2520transition%2520and%2520reward%2520distributions%2520of%250Athe%2520underlying%2520MDP.%2520This%2520paper%2520generalizes%2520such%2520models%2520by%2520proposing%2520a%2520meta%250ABayes-Adaptive%2520MDP%2520%2528meta-BAMDP%2529%2520framework%2520to%2520handle%2520metareasoning%2520in%250Aenvironments%2520with%2520unknown%2520reward/transition%2520distributions%252C%2520which%2520encompasses%2520a%250Afar%2520larger%2520and%2520more%2520realistic%2520set%2520of%2520planning%2520problems%2520that%2520humans%2520and%2520AI%250Asystems%2520face.%2520As%2520a%2520first%2520step%252C%2520we%2520apply%2520the%2520framework%2520to%2520Bernoulli%2520bandit%250Atasks.%2520Owing%2520to%2520the%2520meta%2520problem%2527s%2520complexity%252C%2520our%2520solutions%2520are%2520necessarily%250Aapproximate.%2520However%252C%2520we%2520introduce%2520two%2520novel%2520theorems%2520that%2520significantly%250Aenhance%2520the%2520tractability%2520of%2520the%2520problem%252C%2520enabling%2520stronger%2520approximations%2520that%250Aare%2520robust%2520within%2520a%2520range%2520of%2520assumptions%2520grounded%2520in%2520realistic%2520human%250Adecision-making%2520scenarios.%2520These%2520results%2520offer%2520a%2520resource-rational%2520perspective%250Aand%2520a%2520normative%2520framework%2520for%2520understanding%2520human%2520exploration%2520under%2520cognitive%250Aconstraints%252C%2520as%2520well%2520as%2520providing%2520experimentally%2520testable%2520predictions%2520about%250Ahuman%2520behavior%2520in%2520Bernoulli%2520Bandit%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01253v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metareasoning%20in%20uncertain%20environments%3A%20a%20meta-BAMDP%20framework&entry.906535625=Prakhar%20Godara%20and%20Tilman%20Diego%20Al%C3%A9man%20and%20Angela%20J.%20Yu&entry.1292438233=%20%20%5Ctextit%7BReasoning%7D%20may%20be%20viewed%20as%20an%20algorithm%20%24P%24%20that%20makes%20a%20choice%20of%0Aan%20action%20%24a%5E%2A%20%5Cin%20%5Cmathcal%7BA%7D%24%2C%20aiming%20to%20optimize%20some%20outcome.%20However%2C%0Aexecuting%20%24P%24%20itself%20bears%20costs%20%28time%2C%20energy%2C%20limited%20capacity%2C%20etc.%29%20and%0Aneeds%20to%20be%20considered%20alongside%20explicit%20utility%20obtained%20by%20making%20the%20choice%0Ain%20the%20underlying%20decision%20problem.%20Finding%20the%20right%20%24P%24%20can%20itself%20be%20framed%0Aas%20an%20optimization%20problem%20over%20the%20space%20of%20reasoning%20processes%20%24P%24%2C%20generally%0Areferred%20to%20as%20%5Ctextit%7Bmetareasoning%7D.%20Conventionally%2C%20human%20metareasoning%0Amodels%20assume%20that%20the%20agent%20knows%20the%20transition%20and%20reward%20distributions%20of%0Athe%20underlying%20MDP.%20This%20paper%20generalizes%20such%20models%20by%20proposing%20a%20meta%0ABayes-Adaptive%20MDP%20%28meta-BAMDP%29%20framework%20to%20handle%20metareasoning%20in%0Aenvironments%20with%20unknown%20reward/transition%20distributions%2C%20which%20encompasses%20a%0Afar%20larger%20and%20more%20realistic%20set%20of%20planning%20problems%20that%20humans%20and%20AI%0Asystems%20face.%20As%20a%20first%20step%2C%20we%20apply%20the%20framework%20to%20Bernoulli%20bandit%0Atasks.%20Owing%20to%20the%20meta%20problem%27s%20complexity%2C%20our%20solutions%20are%20necessarily%0Aapproximate.%20However%2C%20we%20introduce%20two%20novel%20theorems%20that%20significantly%0Aenhance%20the%20tractability%20of%20the%20problem%2C%20enabling%20stronger%20approximations%20that%0Aare%20robust%20within%20a%20range%20of%20assumptions%20grounded%20in%20realistic%20human%0Adecision-making%20scenarios.%20These%20results%20offer%20a%20resource-rational%20perspective%0Aand%20a%20normative%20framework%20for%20understanding%20human%20exploration%20under%20cognitive%0Aconstraints%2C%20as%20well%20as%20providing%20experimentally%20testable%20predictions%20about%0Ahuman%20behavior%20in%20Bernoulli%20Bandit%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01253v2&entry.124074799=Read"},
{"title": "Wave-U-Mamba: An End-To-End Framework For High-Quality And Efficient\n  Speech Super Resolution", "author": "Yongjoon Lee and Chanwoo Kim", "abstract": "  Speech Super-Resolution (SSR) is a task of enhancing low-resolution speech\nsignals by restoring missing high-frequency components. Conventional approaches\ntypically reconstruct log-mel features, followed by a vocoder that generates\nhigh-resolution speech in the waveform domain. However, as mel features lack\nphase information, this can result in performance degradation during the\nreconstruction phase. Motivated by recent advances with Selective State Spaces\nModels (SSMs), we propose a method, referred to as Wave-U-Mamba that directly\nperforms SSR in time domain. In our comparative study, including models such as\nWSRGlow, NU-Wave 2, and AudioSR, Wave-U-Mamba demonstrates superior\nperformance, achieving the lowest Log-Spectral Distance (LSD) across various\nlow-resolution sampling rates, ranging from 8 to 24 kHz. Additionally,\nsubjective human evaluations, scored using Mean Opinion Score (MOS) reveal that\nour method produces SSR with natural and human-like quality. Furthermore,\nWave-U-Mamba achieves these results while generating high-resolution speech\nover nine times faster than baseline models on a single A100 GPU, with\nparameter sizes less than 2\\% of those in the baseline models.\n", "link": "http://arxiv.org/abs/2409.09337v3", "date": "2025-02-03", "relevancy": 1.5124, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5226}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5036}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wave-U-Mamba%3A%20An%20End-To-End%20Framework%20For%20High-Quality%20And%20Efficient%0A%20%20Speech%20Super%20Resolution&body=Title%3A%20Wave-U-Mamba%3A%20An%20End-To-End%20Framework%20For%20High-Quality%20And%20Efficient%0A%20%20Speech%20Super%20Resolution%0AAuthor%3A%20Yongjoon%20Lee%20and%20Chanwoo%20Kim%0AAbstract%3A%20%20%20Speech%20Super-Resolution%20%28SSR%29%20is%20a%20task%20of%20enhancing%20low-resolution%20speech%0Asignals%20by%20restoring%20missing%20high-frequency%20components.%20Conventional%20approaches%0Atypically%20reconstruct%20log-mel%20features%2C%20followed%20by%20a%20vocoder%20that%20generates%0Ahigh-resolution%20speech%20in%20the%20waveform%20domain.%20However%2C%20as%20mel%20features%20lack%0Aphase%20information%2C%20this%20can%20result%20in%20performance%20degradation%20during%20the%0Areconstruction%20phase.%20Motivated%20by%20recent%20advances%20with%20Selective%20State%20Spaces%0AModels%20%28SSMs%29%2C%20we%20propose%20a%20method%2C%20referred%20to%20as%20Wave-U-Mamba%20that%20directly%0Aperforms%20SSR%20in%20time%20domain.%20In%20our%20comparative%20study%2C%20including%20models%20such%20as%0AWSRGlow%2C%20NU-Wave%202%2C%20and%20AudioSR%2C%20Wave-U-Mamba%20demonstrates%20superior%0Aperformance%2C%20achieving%20the%20lowest%20Log-Spectral%20Distance%20%28LSD%29%20across%20various%0Alow-resolution%20sampling%20rates%2C%20ranging%20from%208%20to%2024%20kHz.%20Additionally%2C%0Asubjective%20human%20evaluations%2C%20scored%20using%20Mean%20Opinion%20Score%20%28MOS%29%20reveal%20that%0Aour%20method%20produces%20SSR%20with%20natural%20and%20human-like%20quality.%20Furthermore%2C%0AWave-U-Mamba%20achieves%20these%20results%20while%20generating%20high-resolution%20speech%0Aover%20nine%20times%20faster%20than%20baseline%20models%20on%20a%20single%20A100%20GPU%2C%20with%0Aparameter%20sizes%20less%20than%202%5C%25%20of%20those%20in%20the%20baseline%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09337v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWave-U-Mamba%253A%2520An%2520End-To-End%2520Framework%2520For%2520High-Quality%2520And%2520Efficient%250A%2520%2520Speech%2520Super%2520Resolution%26entry.906535625%3DYongjoon%2520Lee%2520and%2520Chanwoo%2520Kim%26entry.1292438233%3D%2520%2520Speech%2520Super-Resolution%2520%2528SSR%2529%2520is%2520a%2520task%2520of%2520enhancing%2520low-resolution%2520speech%250Asignals%2520by%2520restoring%2520missing%2520high-frequency%2520components.%2520Conventional%2520approaches%250Atypically%2520reconstruct%2520log-mel%2520features%252C%2520followed%2520by%2520a%2520vocoder%2520that%2520generates%250Ahigh-resolution%2520speech%2520in%2520the%2520waveform%2520domain.%2520However%252C%2520as%2520mel%2520features%2520lack%250Aphase%2520information%252C%2520this%2520can%2520result%2520in%2520performance%2520degradation%2520during%2520the%250Areconstruction%2520phase.%2520Motivated%2520by%2520recent%2520advances%2520with%2520Selective%2520State%2520Spaces%250AModels%2520%2528SSMs%2529%252C%2520we%2520propose%2520a%2520method%252C%2520referred%2520to%2520as%2520Wave-U-Mamba%2520that%2520directly%250Aperforms%2520SSR%2520in%2520time%2520domain.%2520In%2520our%2520comparative%2520study%252C%2520including%2520models%2520such%2520as%250AWSRGlow%252C%2520NU-Wave%25202%252C%2520and%2520AudioSR%252C%2520Wave-U-Mamba%2520demonstrates%2520superior%250Aperformance%252C%2520achieving%2520the%2520lowest%2520Log-Spectral%2520Distance%2520%2528LSD%2529%2520across%2520various%250Alow-resolution%2520sampling%2520rates%252C%2520ranging%2520from%25208%2520to%252024%2520kHz.%2520Additionally%252C%250Asubjective%2520human%2520evaluations%252C%2520scored%2520using%2520Mean%2520Opinion%2520Score%2520%2528MOS%2529%2520reveal%2520that%250Aour%2520method%2520produces%2520SSR%2520with%2520natural%2520and%2520human-like%2520quality.%2520Furthermore%252C%250AWave-U-Mamba%2520achieves%2520these%2520results%2520while%2520generating%2520high-resolution%2520speech%250Aover%2520nine%2520times%2520faster%2520than%2520baseline%2520models%2520on%2520a%2520single%2520A100%2520GPU%252C%2520with%250Aparameter%2520sizes%2520less%2520than%25202%255C%2525%2520of%2520those%2520in%2520the%2520baseline%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09337v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wave-U-Mamba%3A%20An%20End-To-End%20Framework%20For%20High-Quality%20And%20Efficient%0A%20%20Speech%20Super%20Resolution&entry.906535625=Yongjoon%20Lee%20and%20Chanwoo%20Kim&entry.1292438233=%20%20Speech%20Super-Resolution%20%28SSR%29%20is%20a%20task%20of%20enhancing%20low-resolution%20speech%0Asignals%20by%20restoring%20missing%20high-frequency%20components.%20Conventional%20approaches%0Atypically%20reconstruct%20log-mel%20features%2C%20followed%20by%20a%20vocoder%20that%20generates%0Ahigh-resolution%20speech%20in%20the%20waveform%20domain.%20However%2C%20as%20mel%20features%20lack%0Aphase%20information%2C%20this%20can%20result%20in%20performance%20degradation%20during%20the%0Areconstruction%20phase.%20Motivated%20by%20recent%20advances%20with%20Selective%20State%20Spaces%0AModels%20%28SSMs%29%2C%20we%20propose%20a%20method%2C%20referred%20to%20as%20Wave-U-Mamba%20that%20directly%0Aperforms%20SSR%20in%20time%20domain.%20In%20our%20comparative%20study%2C%20including%20models%20such%20as%0AWSRGlow%2C%20NU-Wave%202%2C%20and%20AudioSR%2C%20Wave-U-Mamba%20demonstrates%20superior%0Aperformance%2C%20achieving%20the%20lowest%20Log-Spectral%20Distance%20%28LSD%29%20across%20various%0Alow-resolution%20sampling%20rates%2C%20ranging%20from%208%20to%2024%20kHz.%20Additionally%2C%0Asubjective%20human%20evaluations%2C%20scored%20using%20Mean%20Opinion%20Score%20%28MOS%29%20reveal%20that%0Aour%20method%20produces%20SSR%20with%20natural%20and%20human-like%20quality.%20Furthermore%2C%0AWave-U-Mamba%20achieves%20these%20results%20while%20generating%20high-resolution%20speech%0Aover%20nine%20times%20faster%20than%20baseline%20models%20on%20a%20single%20A100%20GPU%2C%20with%0Aparameter%20sizes%20less%20than%202%5C%25%20of%20those%20in%20the%20baseline%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09337v3&entry.124074799=Read"},
{"title": "Brain-Inspired AI with Hyperbolic Geometry", "author": "Alexander Joseph and Nathan Francis and Meijke Balay", "abstract": "  Artificial neural networks (ANNs) were inspired by the architecture and\nfunctions of the human brain and have revolutionised the field of artificial\nintelligence (AI). Inspired by studies on the latent geometry of the brain, in\nthis perspective paper we posit that an increase in the research and\napplication of hyperbolic geometry in ANNs and machine learning will lead to\nincreased accuracy, improved feature space representations and more efficient\nmodels across a range of tasks. We examine the structure and functions of the\nhuman brain, emphasising the correspondence between its scale-free hierarchical\norganization and hyperbolic geometry, and reflecting on the central role\nhyperbolic geometry plays in facilitating human intelligence. Empirical\nevidence indicates that hyperbolic neural networks outperform Euclidean models\nfor tasks including natural language processing, computer vision and complex\nnetwork analysis, requiring fewer parameters and exhibiting better\ngeneralisation. Despite its nascent adoption, hyperbolic geometry holds promise\nfor improving machine learning models through brain-inspired geometric\nrepresentations.\n", "link": "http://arxiv.org/abs/2409.12990v3", "date": "2025-02-03", "relevancy": 1.5106, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5226}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4823}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-Inspired%20AI%20with%20Hyperbolic%20Geometry&body=Title%3A%20Brain-Inspired%20AI%20with%20Hyperbolic%20Geometry%0AAuthor%3A%20Alexander%20Joseph%20and%20Nathan%20Francis%20and%20Meijke%20Balay%0AAbstract%3A%20%20%20Artificial%20neural%20networks%20%28ANNs%29%20were%20inspired%20by%20the%20architecture%20and%0Afunctions%20of%20the%20human%20brain%20and%20have%20revolutionised%20the%20field%20of%20artificial%0Aintelligence%20%28AI%29.%20Inspired%20by%20studies%20on%20the%20latent%20geometry%20of%20the%20brain%2C%20in%0Athis%20perspective%20paper%20we%20posit%20that%20an%20increase%20in%20the%20research%20and%0Aapplication%20of%20hyperbolic%20geometry%20in%20ANNs%20and%20machine%20learning%20will%20lead%20to%0Aincreased%20accuracy%2C%20improved%20feature%20space%20representations%20and%20more%20efficient%0Amodels%20across%20a%20range%20of%20tasks.%20We%20examine%20the%20structure%20and%20functions%20of%20the%0Ahuman%20brain%2C%20emphasising%20the%20correspondence%20between%20its%20scale-free%20hierarchical%0Aorganization%20and%20hyperbolic%20geometry%2C%20and%20reflecting%20on%20the%20central%20role%0Ahyperbolic%20geometry%20plays%20in%20facilitating%20human%20intelligence.%20Empirical%0Aevidence%20indicates%20that%20hyperbolic%20neural%20networks%20outperform%20Euclidean%20models%0Afor%20tasks%20including%20natural%20language%20processing%2C%20computer%20vision%20and%20complex%0Anetwork%20analysis%2C%20requiring%20fewer%20parameters%20and%20exhibiting%20better%0Ageneralisation.%20Despite%20its%20nascent%20adoption%2C%20hyperbolic%20geometry%20holds%20promise%0Afor%20improving%20machine%20learning%20models%20through%20brain-inspired%20geometric%0Arepresentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12990v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-Inspired%2520AI%2520with%2520Hyperbolic%2520Geometry%26entry.906535625%3DAlexander%2520Joseph%2520and%2520Nathan%2520Francis%2520and%2520Meijke%2520Balay%26entry.1292438233%3D%2520%2520Artificial%2520neural%2520networks%2520%2528ANNs%2529%2520were%2520inspired%2520by%2520the%2520architecture%2520and%250Afunctions%2520of%2520the%2520human%2520brain%2520and%2520have%2520revolutionised%2520the%2520field%2520of%2520artificial%250Aintelligence%2520%2528AI%2529.%2520Inspired%2520by%2520studies%2520on%2520the%2520latent%2520geometry%2520of%2520the%2520brain%252C%2520in%250Athis%2520perspective%2520paper%2520we%2520posit%2520that%2520an%2520increase%2520in%2520the%2520research%2520and%250Aapplication%2520of%2520hyperbolic%2520geometry%2520in%2520ANNs%2520and%2520machine%2520learning%2520will%2520lead%2520to%250Aincreased%2520accuracy%252C%2520improved%2520feature%2520space%2520representations%2520and%2520more%2520efficient%250Amodels%2520across%2520a%2520range%2520of%2520tasks.%2520We%2520examine%2520the%2520structure%2520and%2520functions%2520of%2520the%250Ahuman%2520brain%252C%2520emphasising%2520the%2520correspondence%2520between%2520its%2520scale-free%2520hierarchical%250Aorganization%2520and%2520hyperbolic%2520geometry%252C%2520and%2520reflecting%2520on%2520the%2520central%2520role%250Ahyperbolic%2520geometry%2520plays%2520in%2520facilitating%2520human%2520intelligence.%2520Empirical%250Aevidence%2520indicates%2520that%2520hyperbolic%2520neural%2520networks%2520outperform%2520Euclidean%2520models%250Afor%2520tasks%2520including%2520natural%2520language%2520processing%252C%2520computer%2520vision%2520and%2520complex%250Anetwork%2520analysis%252C%2520requiring%2520fewer%2520parameters%2520and%2520exhibiting%2520better%250Ageneralisation.%2520Despite%2520its%2520nascent%2520adoption%252C%2520hyperbolic%2520geometry%2520holds%2520promise%250Afor%2520improving%2520machine%2520learning%2520models%2520through%2520brain-inspired%2520geometric%250Arepresentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12990v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-Inspired%20AI%20with%20Hyperbolic%20Geometry&entry.906535625=Alexander%20Joseph%20and%20Nathan%20Francis%20and%20Meijke%20Balay&entry.1292438233=%20%20Artificial%20neural%20networks%20%28ANNs%29%20were%20inspired%20by%20the%20architecture%20and%0Afunctions%20of%20the%20human%20brain%20and%20have%20revolutionised%20the%20field%20of%20artificial%0Aintelligence%20%28AI%29.%20Inspired%20by%20studies%20on%20the%20latent%20geometry%20of%20the%20brain%2C%20in%0Athis%20perspective%20paper%20we%20posit%20that%20an%20increase%20in%20the%20research%20and%0Aapplication%20of%20hyperbolic%20geometry%20in%20ANNs%20and%20machine%20learning%20will%20lead%20to%0Aincreased%20accuracy%2C%20improved%20feature%20space%20representations%20and%20more%20efficient%0Amodels%20across%20a%20range%20of%20tasks.%20We%20examine%20the%20structure%20and%20functions%20of%20the%0Ahuman%20brain%2C%20emphasising%20the%20correspondence%20between%20its%20scale-free%20hierarchical%0Aorganization%20and%20hyperbolic%20geometry%2C%20and%20reflecting%20on%20the%20central%20role%0Ahyperbolic%20geometry%20plays%20in%20facilitating%20human%20intelligence.%20Empirical%0Aevidence%20indicates%20that%20hyperbolic%20neural%20networks%20outperform%20Euclidean%20models%0Afor%20tasks%20including%20natural%20language%20processing%2C%20computer%20vision%20and%20complex%0Anetwork%20analysis%2C%20requiring%20fewer%20parameters%20and%20exhibiting%20better%0Ageneralisation.%20Despite%20its%20nascent%20adoption%2C%20hyperbolic%20geometry%20holds%20promise%0Afor%20improving%20machine%20learning%20models%20through%20brain-inspired%20geometric%0Arepresentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12990v3&entry.124074799=Read"},
{"title": "Efficient Annotator Reliability Assessment and Sample Weighting for\n  Knowledge-Based Misinformation Detection on Social Media", "author": "Owen Cook and Charlie Grimshaw and Ben Wu and Sophie Dillon and Jack Hicks and Luke Jones and Thomas Smith and Matyas Szert and Xingyi Song", "abstract": "  Misinformation spreads rapidly on social media, confusing the truth and\ntargeting potentially vulnerable people. To effectively mitigate the negative\nimpact of misinformation, it must first be accurately detected before applying\na mitigation strategy, such as X's community notes, which is currently a manual\nprocess. This study takes a knowledge-based approach to misinformation\ndetection, modelling the problem similarly to one of natural language\ninference. The EffiARA annotation framework is introduced, aiming to utilise\ninter- and intra-annotator agreement to understand the reliability of each\nannotator and influence the training of large language models for\nclassification based on annotator reliability. In assessing the EffiARA\nannotation framework, the Russo-Ukrainian Conflict Knowledge-Based\nMisinformation Classification Dataset (RUC-MCD) was developed and made publicly\navailable. This study finds that sample weighting using annotator reliability\nperforms the best, utilising both inter- and intra-annotator agreement and\nsoft-label training. The highest classification performance achieved using\nLlama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.\n", "link": "http://arxiv.org/abs/2410.14515v2", "date": "2025-02-03", "relevancy": 1.5, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5621}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4879}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Annotator%20Reliability%20Assessment%20and%20Sample%20Weighting%20for%0A%20%20Knowledge-Based%20Misinformation%20Detection%20on%20Social%20Media&body=Title%3A%20Efficient%20Annotator%20Reliability%20Assessment%20and%20Sample%20Weighting%20for%0A%20%20Knowledge-Based%20Misinformation%20Detection%20on%20Social%20Media%0AAuthor%3A%20Owen%20Cook%20and%20Charlie%20Grimshaw%20and%20Ben%20Wu%20and%20Sophie%20Dillon%20and%20Jack%20Hicks%20and%20Luke%20Jones%20and%20Thomas%20Smith%20and%20Matyas%20Szert%20and%20Xingyi%20Song%0AAbstract%3A%20%20%20Misinformation%20spreads%20rapidly%20on%20social%20media%2C%20confusing%20the%20truth%20and%0Atargeting%20potentially%20vulnerable%20people.%20To%20effectively%20mitigate%20the%20negative%0Aimpact%20of%20misinformation%2C%20it%20must%20first%20be%20accurately%20detected%20before%20applying%0Aa%20mitigation%20strategy%2C%20such%20as%20X%27s%20community%20notes%2C%20which%20is%20currently%20a%20manual%0Aprocess.%20This%20study%20takes%20a%20knowledge-based%20approach%20to%20misinformation%0Adetection%2C%20modelling%20the%20problem%20similarly%20to%20one%20of%20natural%20language%0Ainference.%20The%20EffiARA%20annotation%20framework%20is%20introduced%2C%20aiming%20to%20utilise%0Ainter-%20and%20intra-annotator%20agreement%20to%20understand%20the%20reliability%20of%20each%0Aannotator%20and%20influence%20the%20training%20of%20large%20language%20models%20for%0Aclassification%20based%20on%20annotator%20reliability.%20In%20assessing%20the%20EffiARA%0Aannotation%20framework%2C%20the%20Russo-Ukrainian%20Conflict%20Knowledge-Based%0AMisinformation%20Classification%20Dataset%20%28RUC-MCD%29%20was%20developed%20and%20made%20publicly%0Aavailable.%20This%20study%20finds%20that%20sample%20weighting%20using%20annotator%20reliability%0Aperforms%20the%20best%2C%20utilising%20both%20inter-%20and%20intra-annotator%20agreement%20and%0Asoft-label%20training.%20The%20highest%20classification%20performance%20achieved%20using%0ALlama-3.2-1B%20was%20a%20macro-F1%20of%200.757%20and%200.740%20using%20TwHIN-BERT-large.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14515v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Annotator%2520Reliability%2520Assessment%2520and%2520Sample%2520Weighting%2520for%250A%2520%2520Knowledge-Based%2520Misinformation%2520Detection%2520on%2520Social%2520Media%26entry.906535625%3DOwen%2520Cook%2520and%2520Charlie%2520Grimshaw%2520and%2520Ben%2520Wu%2520and%2520Sophie%2520Dillon%2520and%2520Jack%2520Hicks%2520and%2520Luke%2520Jones%2520and%2520Thomas%2520Smith%2520and%2520Matyas%2520Szert%2520and%2520Xingyi%2520Song%26entry.1292438233%3D%2520%2520Misinformation%2520spreads%2520rapidly%2520on%2520social%2520media%252C%2520confusing%2520the%2520truth%2520and%250Atargeting%2520potentially%2520vulnerable%2520people.%2520To%2520effectively%2520mitigate%2520the%2520negative%250Aimpact%2520of%2520misinformation%252C%2520it%2520must%2520first%2520be%2520accurately%2520detected%2520before%2520applying%250Aa%2520mitigation%2520strategy%252C%2520such%2520as%2520X%2527s%2520community%2520notes%252C%2520which%2520is%2520currently%2520a%2520manual%250Aprocess.%2520This%2520study%2520takes%2520a%2520knowledge-based%2520approach%2520to%2520misinformation%250Adetection%252C%2520modelling%2520the%2520problem%2520similarly%2520to%2520one%2520of%2520natural%2520language%250Ainference.%2520The%2520EffiARA%2520annotation%2520framework%2520is%2520introduced%252C%2520aiming%2520to%2520utilise%250Ainter-%2520and%2520intra-annotator%2520agreement%2520to%2520understand%2520the%2520reliability%2520of%2520each%250Aannotator%2520and%2520influence%2520the%2520training%2520of%2520large%2520language%2520models%2520for%250Aclassification%2520based%2520on%2520annotator%2520reliability.%2520In%2520assessing%2520the%2520EffiARA%250Aannotation%2520framework%252C%2520the%2520Russo-Ukrainian%2520Conflict%2520Knowledge-Based%250AMisinformation%2520Classification%2520Dataset%2520%2528RUC-MCD%2529%2520was%2520developed%2520and%2520made%2520publicly%250Aavailable.%2520This%2520study%2520finds%2520that%2520sample%2520weighting%2520using%2520annotator%2520reliability%250Aperforms%2520the%2520best%252C%2520utilising%2520both%2520inter-%2520and%2520intra-annotator%2520agreement%2520and%250Asoft-label%2520training.%2520The%2520highest%2520classification%2520performance%2520achieved%2520using%250ALlama-3.2-1B%2520was%2520a%2520macro-F1%2520of%25200.757%2520and%25200.740%2520using%2520TwHIN-BERT-large.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14515v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Annotator%20Reliability%20Assessment%20and%20Sample%20Weighting%20for%0A%20%20Knowledge-Based%20Misinformation%20Detection%20on%20Social%20Media&entry.906535625=Owen%20Cook%20and%20Charlie%20Grimshaw%20and%20Ben%20Wu%20and%20Sophie%20Dillon%20and%20Jack%20Hicks%20and%20Luke%20Jones%20and%20Thomas%20Smith%20and%20Matyas%20Szert%20and%20Xingyi%20Song&entry.1292438233=%20%20Misinformation%20spreads%20rapidly%20on%20social%20media%2C%20confusing%20the%20truth%20and%0Atargeting%20potentially%20vulnerable%20people.%20To%20effectively%20mitigate%20the%20negative%0Aimpact%20of%20misinformation%2C%20it%20must%20first%20be%20accurately%20detected%20before%20applying%0Aa%20mitigation%20strategy%2C%20such%20as%20X%27s%20community%20notes%2C%20which%20is%20currently%20a%20manual%0Aprocess.%20This%20study%20takes%20a%20knowledge-based%20approach%20to%20misinformation%0Adetection%2C%20modelling%20the%20problem%20similarly%20to%20one%20of%20natural%20language%0Ainference.%20The%20EffiARA%20annotation%20framework%20is%20introduced%2C%20aiming%20to%20utilise%0Ainter-%20and%20intra-annotator%20agreement%20to%20understand%20the%20reliability%20of%20each%0Aannotator%20and%20influence%20the%20training%20of%20large%20language%20models%20for%0Aclassification%20based%20on%20annotator%20reliability.%20In%20assessing%20the%20EffiARA%0Aannotation%20framework%2C%20the%20Russo-Ukrainian%20Conflict%20Knowledge-Based%0AMisinformation%20Classification%20Dataset%20%28RUC-MCD%29%20was%20developed%20and%20made%20publicly%0Aavailable.%20This%20study%20finds%20that%20sample%20weighting%20using%20annotator%20reliability%0Aperforms%20the%20best%2C%20utilising%20both%20inter-%20and%20intra-annotator%20agreement%20and%0Asoft-label%20training.%20The%20highest%20classification%20performance%20achieved%20using%0ALlama-3.2-1B%20was%20a%20macro-F1%20of%200.757%20and%200.740%20using%20TwHIN-BERT-large.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14515v2&entry.124074799=Read"},
{"title": "ConDiff: A Challenging Dataset for Neural Solvers of Partial\n  Differential Equations", "author": "Vladislav Trifonov and Alexander Rudikov and Oleg Iliev and Yuri M. Laevsky and Ivan Oseledets and Ekaterina Muravleva", "abstract": "  We present ConDiff, a novel dataset for scientific machine learning. ConDiff\nfocuses on the parametric diffusion equation with space dependent coefficients,\na fundamental problem in many applications of partial differential equations\n(PDEs). The main novelty of the proposed dataset is that we consider\ndiscontinuous coefficients with high contrast. These coefficient functions are\nsampled from a selected set of distributions. This class of problems is not\nonly of great academic interest, but is also the basis for describing various\nenvironmental and industrial problems. In this way, ConDiff shortens the gap\nwith real-world problems while remaining fully synthetic and easy to use.\nConDiff consists of a diverse set of diffusion equations with coefficients\ncovering a wide range of contrast levels and heterogeneity with a measurable\ncomplexity metric for clearer comparison between different coefficient\nfunctions. We baseline ConDiff on standard deep learning models in the field of\nscientific machine learning. By providing a large number of problem instances,\neach with its own coefficient function and right-hand side, we hope to\nencourage the development of novel physics-based deep learning approaches, such\nas neural operators, ultimately driving progress towards more accurate and\nefficient solutions of complex PDE problems.\n", "link": "http://arxiv.org/abs/2406.04709v2", "date": "2025-02-03", "relevancy": 1.4985, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5442}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4913}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConDiff%3A%20A%20Challenging%20Dataset%20for%20Neural%20Solvers%20of%20Partial%0A%20%20Differential%20Equations&body=Title%3A%20ConDiff%3A%20A%20Challenging%20Dataset%20for%20Neural%20Solvers%20of%20Partial%0A%20%20Differential%20Equations%0AAuthor%3A%20Vladislav%20Trifonov%20and%20Alexander%20Rudikov%20and%20Oleg%20Iliev%20and%20Yuri%20M.%20Laevsky%20and%20Ivan%20Oseledets%20and%20Ekaterina%20Muravleva%0AAbstract%3A%20%20%20We%20present%20ConDiff%2C%20a%20novel%20dataset%20for%20scientific%20machine%20learning.%20ConDiff%0Afocuses%20on%20the%20parametric%20diffusion%20equation%20with%20space%20dependent%20coefficients%2C%0Aa%20fundamental%20problem%20in%20many%20applications%20of%20partial%20differential%20equations%0A%28PDEs%29.%20The%20main%20novelty%20of%20the%20proposed%20dataset%20is%20that%20we%20consider%0Adiscontinuous%20coefficients%20with%20high%20contrast.%20These%20coefficient%20functions%20are%0Asampled%20from%20a%20selected%20set%20of%20distributions.%20This%20class%20of%20problems%20is%20not%0Aonly%20of%20great%20academic%20interest%2C%20but%20is%20also%20the%20basis%20for%20describing%20various%0Aenvironmental%20and%20industrial%20problems.%20In%20this%20way%2C%20ConDiff%20shortens%20the%20gap%0Awith%20real-world%20problems%20while%20remaining%20fully%20synthetic%20and%20easy%20to%20use.%0AConDiff%20consists%20of%20a%20diverse%20set%20of%20diffusion%20equations%20with%20coefficients%0Acovering%20a%20wide%20range%20of%20contrast%20levels%20and%20heterogeneity%20with%20a%20measurable%0Acomplexity%20metric%20for%20clearer%20comparison%20between%20different%20coefficient%0Afunctions.%20We%20baseline%20ConDiff%20on%20standard%20deep%20learning%20models%20in%20the%20field%20of%0Ascientific%20machine%20learning.%20By%20providing%20a%20large%20number%20of%20problem%20instances%2C%0Aeach%20with%20its%20own%20coefficient%20function%20and%20right-hand%20side%2C%20we%20hope%20to%0Aencourage%20the%20development%20of%20novel%20physics-based%20deep%20learning%20approaches%2C%20such%0Aas%20neural%20operators%2C%20ultimately%20driving%20progress%20towards%20more%20accurate%20and%0Aefficient%20solutions%20of%20complex%20PDE%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04709v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConDiff%253A%2520A%2520Challenging%2520Dataset%2520for%2520Neural%2520Solvers%2520of%2520Partial%250A%2520%2520Differential%2520Equations%26entry.906535625%3DVladislav%2520Trifonov%2520and%2520Alexander%2520Rudikov%2520and%2520Oleg%2520Iliev%2520and%2520Yuri%2520M.%2520Laevsky%2520and%2520Ivan%2520Oseledets%2520and%2520Ekaterina%2520Muravleva%26entry.1292438233%3D%2520%2520We%2520present%2520ConDiff%252C%2520a%2520novel%2520dataset%2520for%2520scientific%2520machine%2520learning.%2520ConDiff%250Afocuses%2520on%2520the%2520parametric%2520diffusion%2520equation%2520with%2520space%2520dependent%2520coefficients%252C%250Aa%2520fundamental%2520problem%2520in%2520many%2520applications%2520of%2520partial%2520differential%2520equations%250A%2528PDEs%2529.%2520The%2520main%2520novelty%2520of%2520the%2520proposed%2520dataset%2520is%2520that%2520we%2520consider%250Adiscontinuous%2520coefficients%2520with%2520high%2520contrast.%2520These%2520coefficient%2520functions%2520are%250Asampled%2520from%2520a%2520selected%2520set%2520of%2520distributions.%2520This%2520class%2520of%2520problems%2520is%2520not%250Aonly%2520of%2520great%2520academic%2520interest%252C%2520but%2520is%2520also%2520the%2520basis%2520for%2520describing%2520various%250Aenvironmental%2520and%2520industrial%2520problems.%2520In%2520this%2520way%252C%2520ConDiff%2520shortens%2520the%2520gap%250Awith%2520real-world%2520problems%2520while%2520remaining%2520fully%2520synthetic%2520and%2520easy%2520to%2520use.%250AConDiff%2520consists%2520of%2520a%2520diverse%2520set%2520of%2520diffusion%2520equations%2520with%2520coefficients%250Acovering%2520a%2520wide%2520range%2520of%2520contrast%2520levels%2520and%2520heterogeneity%2520with%2520a%2520measurable%250Acomplexity%2520metric%2520for%2520clearer%2520comparison%2520between%2520different%2520coefficient%250Afunctions.%2520We%2520baseline%2520ConDiff%2520on%2520standard%2520deep%2520learning%2520models%2520in%2520the%2520field%2520of%250Ascientific%2520machine%2520learning.%2520By%2520providing%2520a%2520large%2520number%2520of%2520problem%2520instances%252C%250Aeach%2520with%2520its%2520own%2520coefficient%2520function%2520and%2520right-hand%2520side%252C%2520we%2520hope%2520to%250Aencourage%2520the%2520development%2520of%2520novel%2520physics-based%2520deep%2520learning%2520approaches%252C%2520such%250Aas%2520neural%2520operators%252C%2520ultimately%2520driving%2520progress%2520towards%2520more%2520accurate%2520and%250Aefficient%2520solutions%2520of%2520complex%2520PDE%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04709v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConDiff%3A%20A%20Challenging%20Dataset%20for%20Neural%20Solvers%20of%20Partial%0A%20%20Differential%20Equations&entry.906535625=Vladislav%20Trifonov%20and%20Alexander%20Rudikov%20and%20Oleg%20Iliev%20and%20Yuri%20M.%20Laevsky%20and%20Ivan%20Oseledets%20and%20Ekaterina%20Muravleva&entry.1292438233=%20%20We%20present%20ConDiff%2C%20a%20novel%20dataset%20for%20scientific%20machine%20learning.%20ConDiff%0Afocuses%20on%20the%20parametric%20diffusion%20equation%20with%20space%20dependent%20coefficients%2C%0Aa%20fundamental%20problem%20in%20many%20applications%20of%20partial%20differential%20equations%0A%28PDEs%29.%20The%20main%20novelty%20of%20the%20proposed%20dataset%20is%20that%20we%20consider%0Adiscontinuous%20coefficients%20with%20high%20contrast.%20These%20coefficient%20functions%20are%0Asampled%20from%20a%20selected%20set%20of%20distributions.%20This%20class%20of%20problems%20is%20not%0Aonly%20of%20great%20academic%20interest%2C%20but%20is%20also%20the%20basis%20for%20describing%20various%0Aenvironmental%20and%20industrial%20problems.%20In%20this%20way%2C%20ConDiff%20shortens%20the%20gap%0Awith%20real-world%20problems%20while%20remaining%20fully%20synthetic%20and%20easy%20to%20use.%0AConDiff%20consists%20of%20a%20diverse%20set%20of%20diffusion%20equations%20with%20coefficients%0Acovering%20a%20wide%20range%20of%20contrast%20levels%20and%20heterogeneity%20with%20a%20measurable%0Acomplexity%20metric%20for%20clearer%20comparison%20between%20different%20coefficient%0Afunctions.%20We%20baseline%20ConDiff%20on%20standard%20deep%20learning%20models%20in%20the%20field%20of%0Ascientific%20machine%20learning.%20By%20providing%20a%20large%20number%20of%20problem%20instances%2C%0Aeach%20with%20its%20own%20coefficient%20function%20and%20right-hand%20side%2C%20we%20hope%20to%0Aencourage%20the%20development%20of%20novel%20physics-based%20deep%20learning%20approaches%2C%20such%0Aas%20neural%20operators%2C%20ultimately%20driving%20progress%20towards%20more%20accurate%20and%0Aefficient%20solutions%20of%20complex%20PDE%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04709v2&entry.124074799=Read"},
{"title": "Exoskeleton-Assisted Balance and Task Evaluation During Quiet Stance and\n  Kneeling in Construction", "author": "Gayatri Sreenivasan and Chunchu Zhu and Jingang Yi", "abstract": "  Construction workers exert intense physical effort and experience serious\nsafety and health risks in hazardous working environments. Quiet stance and\nkneeling are among the most common postures performed by construction workers\nduring their daily work. This paper analyzes lower-limb joint influence on\nneural balance control strategies using the frequency behavior of the\nintersection point of ground reaction forces. To evaluate the impact of\nelevation and wearable knee exoskeletons on postural balance and welding task\nperformance, we design and integrate virtual- and mixed-reality (VR/MR) to\nsimulate elevated environments and welding tasks. A linear quadratic\nregulator-controlled triple- and double-link inverted pendulum model is used\nfor balance strategy quantification in quiet stance and kneeling, respectively.\nExtensive multi-subject experiments are conducted to evaluate the usability of\noccupational exoskeletons in destabilizing construction environments. The\nquantified balance strategies capture the significance of knee joint during\nbalance control of quiet stance and kneeling gaits. Results show that center of\npressure sway area reduced up to 62% in quiet stance and 39% in kneeling for\nsubjects tested in high-elevation VR/MR worksites when provided knee\nexoskeleton assistance. The comprehensive balance and multitask evaluation\nmethodology developed aims to reveal exoskeleton design considerations to\nmitigate the fall risk in construction.\n", "link": "http://arxiv.org/abs/2408.07795v2", "date": "2025-02-03", "relevancy": 1.4774, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5124}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4974}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exoskeleton-Assisted%20Balance%20and%20Task%20Evaluation%20During%20Quiet%20Stance%20and%0A%20%20Kneeling%20in%20Construction&body=Title%3A%20Exoskeleton-Assisted%20Balance%20and%20Task%20Evaluation%20During%20Quiet%20Stance%20and%0A%20%20Kneeling%20in%20Construction%0AAuthor%3A%20Gayatri%20Sreenivasan%20and%20Chunchu%20Zhu%20and%20Jingang%20Yi%0AAbstract%3A%20%20%20Construction%20workers%20exert%20intense%20physical%20effort%20and%20experience%20serious%0Asafety%20and%20health%20risks%20in%20hazardous%20working%20environments.%20Quiet%20stance%20and%0Akneeling%20are%20among%20the%20most%20common%20postures%20performed%20by%20construction%20workers%0Aduring%20their%20daily%20work.%20This%20paper%20analyzes%20lower-limb%20joint%20influence%20on%0Aneural%20balance%20control%20strategies%20using%20the%20frequency%20behavior%20of%20the%0Aintersection%20point%20of%20ground%20reaction%20forces.%20To%20evaluate%20the%20impact%20of%0Aelevation%20and%20wearable%20knee%20exoskeletons%20on%20postural%20balance%20and%20welding%20task%0Aperformance%2C%20we%20design%20and%20integrate%20virtual-%20and%20mixed-reality%20%28VR/MR%29%20to%0Asimulate%20elevated%20environments%20and%20welding%20tasks.%20A%20linear%20quadratic%0Aregulator-controlled%20triple-%20and%20double-link%20inverted%20pendulum%20model%20is%20used%0Afor%20balance%20strategy%20quantification%20in%20quiet%20stance%20and%20kneeling%2C%20respectively.%0AExtensive%20multi-subject%20experiments%20are%20conducted%20to%20evaluate%20the%20usability%20of%0Aoccupational%20exoskeletons%20in%20destabilizing%20construction%20environments.%20The%0Aquantified%20balance%20strategies%20capture%20the%20significance%20of%20knee%20joint%20during%0Abalance%20control%20of%20quiet%20stance%20and%20kneeling%20gaits.%20Results%20show%20that%20center%20of%0Apressure%20sway%20area%20reduced%20up%20to%2062%25%20in%20quiet%20stance%20and%2039%25%20in%20kneeling%20for%0Asubjects%20tested%20in%20high-elevation%20VR/MR%20worksites%20when%20provided%20knee%0Aexoskeleton%20assistance.%20The%20comprehensive%20balance%20and%20multitask%20evaluation%0Amethodology%20developed%20aims%20to%20reveal%20exoskeleton%20design%20considerations%20to%0Amitigate%20the%20fall%20risk%20in%20construction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07795v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExoskeleton-Assisted%2520Balance%2520and%2520Task%2520Evaluation%2520During%2520Quiet%2520Stance%2520and%250A%2520%2520Kneeling%2520in%2520Construction%26entry.906535625%3DGayatri%2520Sreenivasan%2520and%2520Chunchu%2520Zhu%2520and%2520Jingang%2520Yi%26entry.1292438233%3D%2520%2520Construction%2520workers%2520exert%2520intense%2520physical%2520effort%2520and%2520experience%2520serious%250Asafety%2520and%2520health%2520risks%2520in%2520hazardous%2520working%2520environments.%2520Quiet%2520stance%2520and%250Akneeling%2520are%2520among%2520the%2520most%2520common%2520postures%2520performed%2520by%2520construction%2520workers%250Aduring%2520their%2520daily%2520work.%2520This%2520paper%2520analyzes%2520lower-limb%2520joint%2520influence%2520on%250Aneural%2520balance%2520control%2520strategies%2520using%2520the%2520frequency%2520behavior%2520of%2520the%250Aintersection%2520point%2520of%2520ground%2520reaction%2520forces.%2520To%2520evaluate%2520the%2520impact%2520of%250Aelevation%2520and%2520wearable%2520knee%2520exoskeletons%2520on%2520postural%2520balance%2520and%2520welding%2520task%250Aperformance%252C%2520we%2520design%2520and%2520integrate%2520virtual-%2520and%2520mixed-reality%2520%2528VR/MR%2529%2520to%250Asimulate%2520elevated%2520environments%2520and%2520welding%2520tasks.%2520A%2520linear%2520quadratic%250Aregulator-controlled%2520triple-%2520and%2520double-link%2520inverted%2520pendulum%2520model%2520is%2520used%250Afor%2520balance%2520strategy%2520quantification%2520in%2520quiet%2520stance%2520and%2520kneeling%252C%2520respectively.%250AExtensive%2520multi-subject%2520experiments%2520are%2520conducted%2520to%2520evaluate%2520the%2520usability%2520of%250Aoccupational%2520exoskeletons%2520in%2520destabilizing%2520construction%2520environments.%2520The%250Aquantified%2520balance%2520strategies%2520capture%2520the%2520significance%2520of%2520knee%2520joint%2520during%250Abalance%2520control%2520of%2520quiet%2520stance%2520and%2520kneeling%2520gaits.%2520Results%2520show%2520that%2520center%2520of%250Apressure%2520sway%2520area%2520reduced%2520up%2520to%252062%2525%2520in%2520quiet%2520stance%2520and%252039%2525%2520in%2520kneeling%2520for%250Asubjects%2520tested%2520in%2520high-elevation%2520VR/MR%2520worksites%2520when%2520provided%2520knee%250Aexoskeleton%2520assistance.%2520The%2520comprehensive%2520balance%2520and%2520multitask%2520evaluation%250Amethodology%2520developed%2520aims%2520to%2520reveal%2520exoskeleton%2520design%2520considerations%2520to%250Amitigate%2520the%2520fall%2520risk%2520in%2520construction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07795v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exoskeleton-Assisted%20Balance%20and%20Task%20Evaluation%20During%20Quiet%20Stance%20and%0A%20%20Kneeling%20in%20Construction&entry.906535625=Gayatri%20Sreenivasan%20and%20Chunchu%20Zhu%20and%20Jingang%20Yi&entry.1292438233=%20%20Construction%20workers%20exert%20intense%20physical%20effort%20and%20experience%20serious%0Asafety%20and%20health%20risks%20in%20hazardous%20working%20environments.%20Quiet%20stance%20and%0Akneeling%20are%20among%20the%20most%20common%20postures%20performed%20by%20construction%20workers%0Aduring%20their%20daily%20work.%20This%20paper%20analyzes%20lower-limb%20joint%20influence%20on%0Aneural%20balance%20control%20strategies%20using%20the%20frequency%20behavior%20of%20the%0Aintersection%20point%20of%20ground%20reaction%20forces.%20To%20evaluate%20the%20impact%20of%0Aelevation%20and%20wearable%20knee%20exoskeletons%20on%20postural%20balance%20and%20welding%20task%0Aperformance%2C%20we%20design%20and%20integrate%20virtual-%20and%20mixed-reality%20%28VR/MR%29%20to%0Asimulate%20elevated%20environments%20and%20welding%20tasks.%20A%20linear%20quadratic%0Aregulator-controlled%20triple-%20and%20double-link%20inverted%20pendulum%20model%20is%20used%0Afor%20balance%20strategy%20quantification%20in%20quiet%20stance%20and%20kneeling%2C%20respectively.%0AExtensive%20multi-subject%20experiments%20are%20conducted%20to%20evaluate%20the%20usability%20of%0Aoccupational%20exoskeletons%20in%20destabilizing%20construction%20environments.%20The%0Aquantified%20balance%20strategies%20capture%20the%20significance%20of%20knee%20joint%20during%0Abalance%20control%20of%20quiet%20stance%20and%20kneeling%20gaits.%20Results%20show%20that%20center%20of%0Apressure%20sway%20area%20reduced%20up%20to%2062%25%20in%20quiet%20stance%20and%2039%25%20in%20kneeling%20for%0Asubjects%20tested%20in%20high-elevation%20VR/MR%20worksites%20when%20provided%20knee%0Aexoskeleton%20assistance.%20The%20comprehensive%20balance%20and%20multitask%20evaluation%0Amethodology%20developed%20aims%20to%20reveal%20exoskeleton%20design%20considerations%20to%0Amitigate%20the%20fall%20risk%20in%20construction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07795v2&entry.124074799=Read"},
{"title": "Improving Pareto Set Learning for Expensive Multi-objective Optimization\n  via Stein Variational Hypernetworks", "author": "Minh-Duc Nguyen and Phuong Mai Dinh and Quang-Huy Nguyen and Long P. Hoang and Dung D. Le", "abstract": "  Expensive multi-objective optimization problems (EMOPs) are common in\nreal-world scenarios where evaluating objective functions is costly and\ninvolves extensive computations or physical experiments. Current Pareto set\nlearning methods for such problems often rely on surrogate models like Gaussian\nprocesses to approximate the objective functions. These surrogate models can\nbecome fragmented, resulting in numerous small uncertain regions between\nexplored solutions. When using acquisition functions such as the Lower\nConfidence Bound (LCB), these uncertain regions can turn into pseudo-local\noptima, complicating the search for globally optimal solutions. To address\nthese challenges, we propose a novel approach called SVH-PSL, which integrates\nStein Variational Gradient Descent (SVGD) with Hypernetworks for efficient\nPareto set learning. Our method addresses the issues of fragmented surrogate\nmodels and pseudo-local optima by collectively moving particles in a manner\nthat smooths out the solution space. The particles interact with each other\nthrough a kernel function, which helps maintain diversity and encourages the\nexploration of underexplored regions. This kernel-based interaction prevents\nparticles from clustering around pseudo-local optima and promotes convergence\ntowards globally optimal solutions. Our approach aims to establish robust\nrelationships between trade-off reference vectors and their corresponding true\nPareto solutions, overcoming the limitations of existing methods. Through\nextensive experiments across both synthetic and real-world MOO benchmarks, we\ndemonstrate that SVH-PSL significantly improves the quality of the learned\nPareto set, offering a promising solution for expensive multi-objective\noptimization problems.\n", "link": "http://arxiv.org/abs/2412.17312v2", "date": "2025-02-03", "relevancy": 1.4638, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5033}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4853}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Pareto%20Set%20Learning%20for%20Expensive%20Multi-objective%20Optimization%0A%20%20via%20Stein%20Variational%20Hypernetworks&body=Title%3A%20Improving%20Pareto%20Set%20Learning%20for%20Expensive%20Multi-objective%20Optimization%0A%20%20via%20Stein%20Variational%20Hypernetworks%0AAuthor%3A%20Minh-Duc%20Nguyen%20and%20Phuong%20Mai%20Dinh%20and%20Quang-Huy%20Nguyen%20and%20Long%20P.%20Hoang%20and%20Dung%20D.%20Le%0AAbstract%3A%20%20%20Expensive%20multi-objective%20optimization%20problems%20%28EMOPs%29%20are%20common%20in%0Areal-world%20scenarios%20where%20evaluating%20objective%20functions%20is%20costly%20and%0Ainvolves%20extensive%20computations%20or%20physical%20experiments.%20Current%20Pareto%20set%0Alearning%20methods%20for%20such%20problems%20often%20rely%20on%20surrogate%20models%20like%20Gaussian%0Aprocesses%20to%20approximate%20the%20objective%20functions.%20These%20surrogate%20models%20can%0Abecome%20fragmented%2C%20resulting%20in%20numerous%20small%20uncertain%20regions%20between%0Aexplored%20solutions.%20When%20using%20acquisition%20functions%20such%20as%20the%20Lower%0AConfidence%20Bound%20%28LCB%29%2C%20these%20uncertain%20regions%20can%20turn%20into%20pseudo-local%0Aoptima%2C%20complicating%20the%20search%20for%20globally%20optimal%20solutions.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20novel%20approach%20called%20SVH-PSL%2C%20which%20integrates%0AStein%20Variational%20Gradient%20Descent%20%28SVGD%29%20with%20Hypernetworks%20for%20efficient%0APareto%20set%20learning.%20Our%20method%20addresses%20the%20issues%20of%20fragmented%20surrogate%0Amodels%20and%20pseudo-local%20optima%20by%20collectively%20moving%20particles%20in%20a%20manner%0Athat%20smooths%20out%20the%20solution%20space.%20The%20particles%20interact%20with%20each%20other%0Athrough%20a%20kernel%20function%2C%20which%20helps%20maintain%20diversity%20and%20encourages%20the%0Aexploration%20of%20underexplored%20regions.%20This%20kernel-based%20interaction%20prevents%0Aparticles%20from%20clustering%20around%20pseudo-local%20optima%20and%20promotes%20convergence%0Atowards%20globally%20optimal%20solutions.%20Our%20approach%20aims%20to%20establish%20robust%0Arelationships%20between%20trade-off%20reference%20vectors%20and%20their%20corresponding%20true%0APareto%20solutions%2C%20overcoming%20the%20limitations%20of%20existing%20methods.%20Through%0Aextensive%20experiments%20across%20both%20synthetic%20and%20real-world%20MOO%20benchmarks%2C%20we%0Ademonstrate%20that%20SVH-PSL%20significantly%20improves%20the%20quality%20of%20the%20learned%0APareto%20set%2C%20offering%20a%20promising%20solution%20for%20expensive%20multi-objective%0Aoptimization%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17312v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Pareto%2520Set%2520Learning%2520for%2520Expensive%2520Multi-objective%2520Optimization%250A%2520%2520via%2520Stein%2520Variational%2520Hypernetworks%26entry.906535625%3DMinh-Duc%2520Nguyen%2520and%2520Phuong%2520Mai%2520Dinh%2520and%2520Quang-Huy%2520Nguyen%2520and%2520Long%2520P.%2520Hoang%2520and%2520Dung%2520D.%2520Le%26entry.1292438233%3D%2520%2520Expensive%2520multi-objective%2520optimization%2520problems%2520%2528EMOPs%2529%2520are%2520common%2520in%250Areal-world%2520scenarios%2520where%2520evaluating%2520objective%2520functions%2520is%2520costly%2520and%250Ainvolves%2520extensive%2520computations%2520or%2520physical%2520experiments.%2520Current%2520Pareto%2520set%250Alearning%2520methods%2520for%2520such%2520problems%2520often%2520rely%2520on%2520surrogate%2520models%2520like%2520Gaussian%250Aprocesses%2520to%2520approximate%2520the%2520objective%2520functions.%2520These%2520surrogate%2520models%2520can%250Abecome%2520fragmented%252C%2520resulting%2520in%2520numerous%2520small%2520uncertain%2520regions%2520between%250Aexplored%2520solutions.%2520When%2520using%2520acquisition%2520functions%2520such%2520as%2520the%2520Lower%250AConfidence%2520Bound%2520%2528LCB%2529%252C%2520these%2520uncertain%2520regions%2520can%2520turn%2520into%2520pseudo-local%250Aoptima%252C%2520complicating%2520the%2520search%2520for%2520globally%2520optimal%2520solutions.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520approach%2520called%2520SVH-PSL%252C%2520which%2520integrates%250AStein%2520Variational%2520Gradient%2520Descent%2520%2528SVGD%2529%2520with%2520Hypernetworks%2520for%2520efficient%250APareto%2520set%2520learning.%2520Our%2520method%2520addresses%2520the%2520issues%2520of%2520fragmented%2520surrogate%250Amodels%2520and%2520pseudo-local%2520optima%2520by%2520collectively%2520moving%2520particles%2520in%2520a%2520manner%250Athat%2520smooths%2520out%2520the%2520solution%2520space.%2520The%2520particles%2520interact%2520with%2520each%2520other%250Athrough%2520a%2520kernel%2520function%252C%2520which%2520helps%2520maintain%2520diversity%2520and%2520encourages%2520the%250Aexploration%2520of%2520underexplored%2520regions.%2520This%2520kernel-based%2520interaction%2520prevents%250Aparticles%2520from%2520clustering%2520around%2520pseudo-local%2520optima%2520and%2520promotes%2520convergence%250Atowards%2520globally%2520optimal%2520solutions.%2520Our%2520approach%2520aims%2520to%2520establish%2520robust%250Arelationships%2520between%2520trade-off%2520reference%2520vectors%2520and%2520their%2520corresponding%2520true%250APareto%2520solutions%252C%2520overcoming%2520the%2520limitations%2520of%2520existing%2520methods.%2520Through%250Aextensive%2520experiments%2520across%2520both%2520synthetic%2520and%2520real-world%2520MOO%2520benchmarks%252C%2520we%250Ademonstrate%2520that%2520SVH-PSL%2520significantly%2520improves%2520the%2520quality%2520of%2520the%2520learned%250APareto%2520set%252C%2520offering%2520a%2520promising%2520solution%2520for%2520expensive%2520multi-objective%250Aoptimization%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17312v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Pareto%20Set%20Learning%20for%20Expensive%20Multi-objective%20Optimization%0A%20%20via%20Stein%20Variational%20Hypernetworks&entry.906535625=Minh-Duc%20Nguyen%20and%20Phuong%20Mai%20Dinh%20and%20Quang-Huy%20Nguyen%20and%20Long%20P.%20Hoang%20and%20Dung%20D.%20Le&entry.1292438233=%20%20Expensive%20multi-objective%20optimization%20problems%20%28EMOPs%29%20are%20common%20in%0Areal-world%20scenarios%20where%20evaluating%20objective%20functions%20is%20costly%20and%0Ainvolves%20extensive%20computations%20or%20physical%20experiments.%20Current%20Pareto%20set%0Alearning%20methods%20for%20such%20problems%20often%20rely%20on%20surrogate%20models%20like%20Gaussian%0Aprocesses%20to%20approximate%20the%20objective%20functions.%20These%20surrogate%20models%20can%0Abecome%20fragmented%2C%20resulting%20in%20numerous%20small%20uncertain%20regions%20between%0Aexplored%20solutions.%20When%20using%20acquisition%20functions%20such%20as%20the%20Lower%0AConfidence%20Bound%20%28LCB%29%2C%20these%20uncertain%20regions%20can%20turn%20into%20pseudo-local%0Aoptima%2C%20complicating%20the%20search%20for%20globally%20optimal%20solutions.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20novel%20approach%20called%20SVH-PSL%2C%20which%20integrates%0AStein%20Variational%20Gradient%20Descent%20%28SVGD%29%20with%20Hypernetworks%20for%20efficient%0APareto%20set%20learning.%20Our%20method%20addresses%20the%20issues%20of%20fragmented%20surrogate%0Amodels%20and%20pseudo-local%20optima%20by%20collectively%20moving%20particles%20in%20a%20manner%0Athat%20smooths%20out%20the%20solution%20space.%20The%20particles%20interact%20with%20each%20other%0Athrough%20a%20kernel%20function%2C%20which%20helps%20maintain%20diversity%20and%20encourages%20the%0Aexploration%20of%20underexplored%20regions.%20This%20kernel-based%20interaction%20prevents%0Aparticles%20from%20clustering%20around%20pseudo-local%20optima%20and%20promotes%20convergence%0Atowards%20globally%20optimal%20solutions.%20Our%20approach%20aims%20to%20establish%20robust%0Arelationships%20between%20trade-off%20reference%20vectors%20and%20their%20corresponding%20true%0APareto%20solutions%2C%20overcoming%20the%20limitations%20of%20existing%20methods.%20Through%0Aextensive%20experiments%20across%20both%20synthetic%20and%20real-world%20MOO%20benchmarks%2C%20we%0Ademonstrate%20that%20SVH-PSL%20significantly%20improves%20the%20quality%20of%20the%20learned%0APareto%20set%2C%20offering%20a%20promising%20solution%20for%20expensive%20multi-objective%0Aoptimization%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17312v2&entry.124074799=Read"},
{"title": "The Master Key Filters Hypothesis: Deep Filters Are General", "author": "Zahra Babaiee and Peyman M. Kiasari and Daniela Rus and Radu Grosu", "abstract": "  This paper challenges the prevailing view that convolutional neural network\n(CNN) filters become increasingly specialized in deeper layers. Motivated by\nrecent observations of clusterable repeating patterns in depthwise separable\nCNNs (DS-CNNs) trained on ImageNet, we extend this investigation across various\ndomains and datasets. Our analysis of DS-CNNs reveals that deep filters\nmaintain generality, contradicting the expected transition to class-specific\nfilters. We demonstrate the generalizability of these filters through transfer\nlearning experiments, showing that frozen filters from models trained on\ndifferent datasets perform well and can be further improved when sourced from\nlarger datasets. Our findings indicate that spatial features learned by\ndepthwise separable convolutions remain generic across all layers, domains, and\narchitectures. This research provides new insights into the nature of\ngeneralization in neural networks, particularly in DS-CNNs, and has significant\nimplications for transfer learning and model design.\n", "link": "http://arxiv.org/abs/2412.16751v2", "date": "2025-02-03", "relevancy": 1.4436, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4907}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4781}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Master%20Key%20Filters%20Hypothesis%3A%20Deep%20Filters%20Are%20General&body=Title%3A%20The%20Master%20Key%20Filters%20Hypothesis%3A%20Deep%20Filters%20Are%20General%0AAuthor%3A%20Zahra%20Babaiee%20and%20Peyman%20M.%20Kiasari%20and%20Daniela%20Rus%20and%20Radu%20Grosu%0AAbstract%3A%20%20%20This%20paper%20challenges%20the%20prevailing%20view%20that%20convolutional%20neural%20network%0A%28CNN%29%20filters%20become%20increasingly%20specialized%20in%20deeper%20layers.%20Motivated%20by%0Arecent%20observations%20of%20clusterable%20repeating%20patterns%20in%20depthwise%20separable%0ACNNs%20%28DS-CNNs%29%20trained%20on%20ImageNet%2C%20we%20extend%20this%20investigation%20across%20various%0Adomains%20and%20datasets.%20Our%20analysis%20of%20DS-CNNs%20reveals%20that%20deep%20filters%0Amaintain%20generality%2C%20contradicting%20the%20expected%20transition%20to%20class-specific%0Afilters.%20We%20demonstrate%20the%20generalizability%20of%20these%20filters%20through%20transfer%0Alearning%20experiments%2C%20showing%20that%20frozen%20filters%20from%20models%20trained%20on%0Adifferent%20datasets%20perform%20well%20and%20can%20be%20further%20improved%20when%20sourced%20from%0Alarger%20datasets.%20Our%20findings%20indicate%20that%20spatial%20features%20learned%20by%0Adepthwise%20separable%20convolutions%20remain%20generic%20across%20all%20layers%2C%20domains%2C%20and%0Aarchitectures.%20This%20research%20provides%20new%20insights%20into%20the%20nature%20of%0Ageneralization%20in%20neural%20networks%2C%20particularly%20in%20DS-CNNs%2C%20and%20has%20significant%0Aimplications%20for%20transfer%20learning%20and%20model%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Master%2520Key%2520Filters%2520Hypothesis%253A%2520Deep%2520Filters%2520Are%2520General%26entry.906535625%3DZahra%2520Babaiee%2520and%2520Peyman%2520M.%2520Kiasari%2520and%2520Daniela%2520Rus%2520and%2520Radu%2520Grosu%26entry.1292438233%3D%2520%2520This%2520paper%2520challenges%2520the%2520prevailing%2520view%2520that%2520convolutional%2520neural%2520network%250A%2528CNN%2529%2520filters%2520become%2520increasingly%2520specialized%2520in%2520deeper%2520layers.%2520Motivated%2520by%250Arecent%2520observations%2520of%2520clusterable%2520repeating%2520patterns%2520in%2520depthwise%2520separable%250ACNNs%2520%2528DS-CNNs%2529%2520trained%2520on%2520ImageNet%252C%2520we%2520extend%2520this%2520investigation%2520across%2520various%250Adomains%2520and%2520datasets.%2520Our%2520analysis%2520of%2520DS-CNNs%2520reveals%2520that%2520deep%2520filters%250Amaintain%2520generality%252C%2520contradicting%2520the%2520expected%2520transition%2520to%2520class-specific%250Afilters.%2520We%2520demonstrate%2520the%2520generalizability%2520of%2520these%2520filters%2520through%2520transfer%250Alearning%2520experiments%252C%2520showing%2520that%2520frozen%2520filters%2520from%2520models%2520trained%2520on%250Adifferent%2520datasets%2520perform%2520well%2520and%2520can%2520be%2520further%2520improved%2520when%2520sourced%2520from%250Alarger%2520datasets.%2520Our%2520findings%2520indicate%2520that%2520spatial%2520features%2520learned%2520by%250Adepthwise%2520separable%2520convolutions%2520remain%2520generic%2520across%2520all%2520layers%252C%2520domains%252C%2520and%250Aarchitectures.%2520This%2520research%2520provides%2520new%2520insights%2520into%2520the%2520nature%2520of%250Ageneralization%2520in%2520neural%2520networks%252C%2520particularly%2520in%2520DS-CNNs%252C%2520and%2520has%2520significant%250Aimplications%2520for%2520transfer%2520learning%2520and%2520model%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Master%20Key%20Filters%20Hypothesis%3A%20Deep%20Filters%20Are%20General&entry.906535625=Zahra%20Babaiee%20and%20Peyman%20M.%20Kiasari%20and%20Daniela%20Rus%20and%20Radu%20Grosu&entry.1292438233=%20%20This%20paper%20challenges%20the%20prevailing%20view%20that%20convolutional%20neural%20network%0A%28CNN%29%20filters%20become%20increasingly%20specialized%20in%20deeper%20layers.%20Motivated%20by%0Arecent%20observations%20of%20clusterable%20repeating%20patterns%20in%20depthwise%20separable%0ACNNs%20%28DS-CNNs%29%20trained%20on%20ImageNet%2C%20we%20extend%20this%20investigation%20across%20various%0Adomains%20and%20datasets.%20Our%20analysis%20of%20DS-CNNs%20reveals%20that%20deep%20filters%0Amaintain%20generality%2C%20contradicting%20the%20expected%20transition%20to%20class-specific%0Afilters.%20We%20demonstrate%20the%20generalizability%20of%20these%20filters%20through%20transfer%0Alearning%20experiments%2C%20showing%20that%20frozen%20filters%20from%20models%20trained%20on%0Adifferent%20datasets%20perform%20well%20and%20can%20be%20further%20improved%20when%20sourced%20from%0Alarger%20datasets.%20Our%20findings%20indicate%20that%20spatial%20features%20learned%20by%0Adepthwise%20separable%20convolutions%20remain%20generic%20across%20all%20layers%2C%20domains%2C%20and%0Aarchitectures.%20This%20research%20provides%20new%20insights%20into%20the%20nature%20of%0Ageneralization%20in%20neural%20networks%2C%20particularly%20in%20DS-CNNs%2C%20and%20has%20significant%0Aimplications%20for%20transfer%20learning%20and%20model%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16751v2&entry.124074799=Read"},
{"title": "Energy-Guided Continuous Entropic Barycenter Estimation for General\n  Costs", "author": "Alexander Kolesov and Petr Mokrov and Igor Udovichenko and Milena Gazdieva and Gudmund Pammer and Anastasis Kratsios and Evgeny Burnaev and Alexander Korotin", "abstract": "  Optimal transport (OT) barycenters are a mathematically grounded way of\naveraging probability distributions while capturing their geometric properties.\nIn short, the barycenter task is to take the average of a collection of\nprobability distributions w.r.t. given OT discrepancies. We propose a novel\nalgorithm for approximating the continuous Entropic OT (EOT) barycenter for\narbitrary OT cost functions. Our approach is built upon the dual reformulation\nof the EOT problem based on weak OT, which has recently gained the attention of\nthe ML community. Beyond its novelty, our method enjoys several advantageous\nproperties: (i) we establish quality bounds for the recovered solution; (ii)\nthis approach seamlessly interconnects with the Energy-Based Models (EBMs)\nlearning procedure enabling the use of well-tuned algorithms for the problem of\ninterest; (iii) it provides an intuitive optimization scheme avoiding min-max,\nreinforce and other intricate technical tricks. For validation, we consider\nseveral low-dimensional scenarios and image-space setups, including\nnon-Euclidean cost functions. Furthermore, we investigate the practical task of\nlearning the barycenter on an image manifold generated by a pretrained\ngenerative model, opening up new directions for real-world applications. Our\ncode is available at https://github.com/justkolesov/EnergyGuidedBarycenters.\n", "link": "http://arxiv.org/abs/2310.01105v4", "date": "2025-02-03", "relevancy": 1.439, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5239}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4784}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Guided%20Continuous%20Entropic%20Barycenter%20Estimation%20for%20General%0A%20%20Costs&body=Title%3A%20Energy-Guided%20Continuous%20Entropic%20Barycenter%20Estimation%20for%20General%0A%20%20Costs%0AAuthor%3A%20Alexander%20Kolesov%20and%20Petr%20Mokrov%20and%20Igor%20Udovichenko%20and%20Milena%20Gazdieva%20and%20Gudmund%20Pammer%20and%20Anastasis%20Kratsios%20and%20Evgeny%20Burnaev%20and%20Alexander%20Korotin%0AAbstract%3A%20%20%20Optimal%20transport%20%28OT%29%20barycenters%20are%20a%20mathematically%20grounded%20way%20of%0Aaveraging%20probability%20distributions%20while%20capturing%20their%20geometric%20properties.%0AIn%20short%2C%20the%20barycenter%20task%20is%20to%20take%20the%20average%20of%20a%20collection%20of%0Aprobability%20distributions%20w.r.t.%20given%20OT%20discrepancies.%20We%20propose%20a%20novel%0Aalgorithm%20for%20approximating%20the%20continuous%20Entropic%20OT%20%28EOT%29%20barycenter%20for%0Aarbitrary%20OT%20cost%20functions.%20Our%20approach%20is%20built%20upon%20the%20dual%20reformulation%0Aof%20the%20EOT%20problem%20based%20on%20weak%20OT%2C%20which%20has%20recently%20gained%20the%20attention%20of%0Athe%20ML%20community.%20Beyond%20its%20novelty%2C%20our%20method%20enjoys%20several%20advantageous%0Aproperties%3A%20%28i%29%20we%20establish%20quality%20bounds%20for%20the%20recovered%20solution%3B%20%28ii%29%0Athis%20approach%20seamlessly%20interconnects%20with%20the%20Energy-Based%20Models%20%28EBMs%29%0Alearning%20procedure%20enabling%20the%20use%20of%20well-tuned%20algorithms%20for%20the%20problem%20of%0Ainterest%3B%20%28iii%29%20it%20provides%20an%20intuitive%20optimization%20scheme%20avoiding%20min-max%2C%0Areinforce%20and%20other%20intricate%20technical%20tricks.%20For%20validation%2C%20we%20consider%0Aseveral%20low-dimensional%20scenarios%20and%20image-space%20setups%2C%20including%0Anon-Euclidean%20cost%20functions.%20Furthermore%2C%20we%20investigate%20the%20practical%20task%20of%0Alearning%20the%20barycenter%20on%20an%20image%20manifold%20generated%20by%20a%20pretrained%0Agenerative%20model%2C%20opening%20up%20new%20directions%20for%20real-world%20applications.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/justkolesov/EnergyGuidedBarycenters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01105v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Guided%2520Continuous%2520Entropic%2520Barycenter%2520Estimation%2520for%2520General%250A%2520%2520Costs%26entry.906535625%3DAlexander%2520Kolesov%2520and%2520Petr%2520Mokrov%2520and%2520Igor%2520Udovichenko%2520and%2520Milena%2520Gazdieva%2520and%2520Gudmund%2520Pammer%2520and%2520Anastasis%2520Kratsios%2520and%2520Evgeny%2520Burnaev%2520and%2520Alexander%2520Korotin%26entry.1292438233%3D%2520%2520Optimal%2520transport%2520%2528OT%2529%2520barycenters%2520are%2520a%2520mathematically%2520grounded%2520way%2520of%250Aaveraging%2520probability%2520distributions%2520while%2520capturing%2520their%2520geometric%2520properties.%250AIn%2520short%252C%2520the%2520barycenter%2520task%2520is%2520to%2520take%2520the%2520average%2520of%2520a%2520collection%2520of%250Aprobability%2520distributions%2520w.r.t.%2520given%2520OT%2520discrepancies.%2520We%2520propose%2520a%2520novel%250Aalgorithm%2520for%2520approximating%2520the%2520continuous%2520Entropic%2520OT%2520%2528EOT%2529%2520barycenter%2520for%250Aarbitrary%2520OT%2520cost%2520functions.%2520Our%2520approach%2520is%2520built%2520upon%2520the%2520dual%2520reformulation%250Aof%2520the%2520EOT%2520problem%2520based%2520on%2520weak%2520OT%252C%2520which%2520has%2520recently%2520gained%2520the%2520attention%2520of%250Athe%2520ML%2520community.%2520Beyond%2520its%2520novelty%252C%2520our%2520method%2520enjoys%2520several%2520advantageous%250Aproperties%253A%2520%2528i%2529%2520we%2520establish%2520quality%2520bounds%2520for%2520the%2520recovered%2520solution%253B%2520%2528ii%2529%250Athis%2520approach%2520seamlessly%2520interconnects%2520with%2520the%2520Energy-Based%2520Models%2520%2528EBMs%2529%250Alearning%2520procedure%2520enabling%2520the%2520use%2520of%2520well-tuned%2520algorithms%2520for%2520the%2520problem%2520of%250Ainterest%253B%2520%2528iii%2529%2520it%2520provides%2520an%2520intuitive%2520optimization%2520scheme%2520avoiding%2520min-max%252C%250Areinforce%2520and%2520other%2520intricate%2520technical%2520tricks.%2520For%2520validation%252C%2520we%2520consider%250Aseveral%2520low-dimensional%2520scenarios%2520and%2520image-space%2520setups%252C%2520including%250Anon-Euclidean%2520cost%2520functions.%2520Furthermore%252C%2520we%2520investigate%2520the%2520practical%2520task%2520of%250Alearning%2520the%2520barycenter%2520on%2520an%2520image%2520manifold%2520generated%2520by%2520a%2520pretrained%250Agenerative%2520model%252C%2520opening%2520up%2520new%2520directions%2520for%2520real-world%2520applications.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/justkolesov/EnergyGuidedBarycenters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.01105v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Guided%20Continuous%20Entropic%20Barycenter%20Estimation%20for%20General%0A%20%20Costs&entry.906535625=Alexander%20Kolesov%20and%20Petr%20Mokrov%20and%20Igor%20Udovichenko%20and%20Milena%20Gazdieva%20and%20Gudmund%20Pammer%20and%20Anastasis%20Kratsios%20and%20Evgeny%20Burnaev%20and%20Alexander%20Korotin&entry.1292438233=%20%20Optimal%20transport%20%28OT%29%20barycenters%20are%20a%20mathematically%20grounded%20way%20of%0Aaveraging%20probability%20distributions%20while%20capturing%20their%20geometric%20properties.%0AIn%20short%2C%20the%20barycenter%20task%20is%20to%20take%20the%20average%20of%20a%20collection%20of%0Aprobability%20distributions%20w.r.t.%20given%20OT%20discrepancies.%20We%20propose%20a%20novel%0Aalgorithm%20for%20approximating%20the%20continuous%20Entropic%20OT%20%28EOT%29%20barycenter%20for%0Aarbitrary%20OT%20cost%20functions.%20Our%20approach%20is%20built%20upon%20the%20dual%20reformulation%0Aof%20the%20EOT%20problem%20based%20on%20weak%20OT%2C%20which%20has%20recently%20gained%20the%20attention%20of%0Athe%20ML%20community.%20Beyond%20its%20novelty%2C%20our%20method%20enjoys%20several%20advantageous%0Aproperties%3A%20%28i%29%20we%20establish%20quality%20bounds%20for%20the%20recovered%20solution%3B%20%28ii%29%0Athis%20approach%20seamlessly%20interconnects%20with%20the%20Energy-Based%20Models%20%28EBMs%29%0Alearning%20procedure%20enabling%20the%20use%20of%20well-tuned%20algorithms%20for%20the%20problem%20of%0Ainterest%3B%20%28iii%29%20it%20provides%20an%20intuitive%20optimization%20scheme%20avoiding%20min-max%2C%0Areinforce%20and%20other%20intricate%20technical%20tricks.%20For%20validation%2C%20we%20consider%0Aseveral%20low-dimensional%20scenarios%20and%20image-space%20setups%2C%20including%0Anon-Euclidean%20cost%20functions.%20Furthermore%2C%20we%20investigate%20the%20practical%20task%20of%0Alearning%20the%20barycenter%20on%20an%20image%20manifold%20generated%20by%20a%20pretrained%0Agenerative%20model%2C%20opening%20up%20new%20directions%20for%20real-world%20applications.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/justkolesov/EnergyGuidedBarycenters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01105v4&entry.124074799=Read"},
{"title": "Jacobian Descent for Multi-Objective Optimization", "author": "Pierre Quinton and Val\u00e9rian Rey", "abstract": "  Many optimization problems require balancing multiple conflicting objectives.\nAs gradient descent is limited to single-objective optimization, we introduce\nits direct generalization: Jacobian descent (JD). This algorithm iteratively\nupdates parameters using the Jacobian matrix of a vector-valued objective\nfunction, in which each row is the gradient of an individual objective. While\nseveral methods to combine gradients already exist in the literature, they are\ngenerally hindered when the objectives conflict. In contrast, we propose\nprojecting gradients to fully resolve conflict while ensuring that they\npreserve an influence proportional to their norm. We prove significantly\nstronger convergence guarantees with this approach, supported by our empirical\nresults. Our method also enables instance-wise risk minimization (IWRM), a\nnovel learning paradigm in which the loss of each training example is\nconsidered a separate objective. Applied to simple image classification tasks,\nIWRM exhibits promising results compared to the direct minimization of the\naverage loss. Additionally, we outline an efficient implementation of JD using\nthe Gramian of the Jacobian matrix to reduce time and memory requirements.\n", "link": "http://arxiv.org/abs/2406.16232v3", "date": "2025-02-03", "relevancy": 1.4237, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4793}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4786}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jacobian%20Descent%20for%20Multi-Objective%20Optimization&body=Title%3A%20Jacobian%20Descent%20for%20Multi-Objective%20Optimization%0AAuthor%3A%20Pierre%20Quinton%20and%20Val%C3%A9rian%20Rey%0AAbstract%3A%20%20%20Many%20optimization%20problems%20require%20balancing%20multiple%20conflicting%20objectives.%0AAs%20gradient%20descent%20is%20limited%20to%20single-objective%20optimization%2C%20we%20introduce%0Aits%20direct%20generalization%3A%20Jacobian%20descent%20%28JD%29.%20This%20algorithm%20iteratively%0Aupdates%20parameters%20using%20the%20Jacobian%20matrix%20of%20a%20vector-valued%20objective%0Afunction%2C%20in%20which%20each%20row%20is%20the%20gradient%20of%20an%20individual%20objective.%20While%0Aseveral%20methods%20to%20combine%20gradients%20already%20exist%20in%20the%20literature%2C%20they%20are%0Agenerally%20hindered%20when%20the%20objectives%20conflict.%20In%20contrast%2C%20we%20propose%0Aprojecting%20gradients%20to%20fully%20resolve%20conflict%20while%20ensuring%20that%20they%0Apreserve%20an%20influence%20proportional%20to%20their%20norm.%20We%20prove%20significantly%0Astronger%20convergence%20guarantees%20with%20this%20approach%2C%20supported%20by%20our%20empirical%0Aresults.%20Our%20method%20also%20enables%20instance-wise%20risk%20minimization%20%28IWRM%29%2C%20a%0Anovel%20learning%20paradigm%20in%20which%20the%20loss%20of%20each%20training%20example%20is%0Aconsidered%20a%20separate%20objective.%20Applied%20to%20simple%20image%20classification%20tasks%2C%0AIWRM%20exhibits%20promising%20results%20compared%20to%20the%20direct%20minimization%20of%20the%0Aaverage%20loss.%20Additionally%2C%20we%20outline%20an%20efficient%20implementation%20of%20JD%20using%0Athe%20Gramian%20of%20the%20Jacobian%20matrix%20to%20reduce%20time%20and%20memory%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16232v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJacobian%2520Descent%2520for%2520Multi-Objective%2520Optimization%26entry.906535625%3DPierre%2520Quinton%2520and%2520Val%25C3%25A9rian%2520Rey%26entry.1292438233%3D%2520%2520Many%2520optimization%2520problems%2520require%2520balancing%2520multiple%2520conflicting%2520objectives.%250AAs%2520gradient%2520descent%2520is%2520limited%2520to%2520single-objective%2520optimization%252C%2520we%2520introduce%250Aits%2520direct%2520generalization%253A%2520Jacobian%2520descent%2520%2528JD%2529.%2520This%2520algorithm%2520iteratively%250Aupdates%2520parameters%2520using%2520the%2520Jacobian%2520matrix%2520of%2520a%2520vector-valued%2520objective%250Afunction%252C%2520in%2520which%2520each%2520row%2520is%2520the%2520gradient%2520of%2520an%2520individual%2520objective.%2520While%250Aseveral%2520methods%2520to%2520combine%2520gradients%2520already%2520exist%2520in%2520the%2520literature%252C%2520they%2520are%250Agenerally%2520hindered%2520when%2520the%2520objectives%2520conflict.%2520In%2520contrast%252C%2520we%2520propose%250Aprojecting%2520gradients%2520to%2520fully%2520resolve%2520conflict%2520while%2520ensuring%2520that%2520they%250Apreserve%2520an%2520influence%2520proportional%2520to%2520their%2520norm.%2520We%2520prove%2520significantly%250Astronger%2520convergence%2520guarantees%2520with%2520this%2520approach%252C%2520supported%2520by%2520our%2520empirical%250Aresults.%2520Our%2520method%2520also%2520enables%2520instance-wise%2520risk%2520minimization%2520%2528IWRM%2529%252C%2520a%250Anovel%2520learning%2520paradigm%2520in%2520which%2520the%2520loss%2520of%2520each%2520training%2520example%2520is%250Aconsidered%2520a%2520separate%2520objective.%2520Applied%2520to%2520simple%2520image%2520classification%2520tasks%252C%250AIWRM%2520exhibits%2520promising%2520results%2520compared%2520to%2520the%2520direct%2520minimization%2520of%2520the%250Aaverage%2520loss.%2520Additionally%252C%2520we%2520outline%2520an%2520efficient%2520implementation%2520of%2520JD%2520using%250Athe%2520Gramian%2520of%2520the%2520Jacobian%2520matrix%2520to%2520reduce%2520time%2520and%2520memory%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16232v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jacobian%20Descent%20for%20Multi-Objective%20Optimization&entry.906535625=Pierre%20Quinton%20and%20Val%C3%A9rian%20Rey&entry.1292438233=%20%20Many%20optimization%20problems%20require%20balancing%20multiple%20conflicting%20objectives.%0AAs%20gradient%20descent%20is%20limited%20to%20single-objective%20optimization%2C%20we%20introduce%0Aits%20direct%20generalization%3A%20Jacobian%20descent%20%28JD%29.%20This%20algorithm%20iteratively%0Aupdates%20parameters%20using%20the%20Jacobian%20matrix%20of%20a%20vector-valued%20objective%0Afunction%2C%20in%20which%20each%20row%20is%20the%20gradient%20of%20an%20individual%20objective.%20While%0Aseveral%20methods%20to%20combine%20gradients%20already%20exist%20in%20the%20literature%2C%20they%20are%0Agenerally%20hindered%20when%20the%20objectives%20conflict.%20In%20contrast%2C%20we%20propose%0Aprojecting%20gradients%20to%20fully%20resolve%20conflict%20while%20ensuring%20that%20they%0Apreserve%20an%20influence%20proportional%20to%20their%20norm.%20We%20prove%20significantly%0Astronger%20convergence%20guarantees%20with%20this%20approach%2C%20supported%20by%20our%20empirical%0Aresults.%20Our%20method%20also%20enables%20instance-wise%20risk%20minimization%20%28IWRM%29%2C%20a%0Anovel%20learning%20paradigm%20in%20which%20the%20loss%20of%20each%20training%20example%20is%0Aconsidered%20a%20separate%20objective.%20Applied%20to%20simple%20image%20classification%20tasks%2C%0AIWRM%20exhibits%20promising%20results%20compared%20to%20the%20direct%20minimization%20of%20the%0Aaverage%20loss.%20Additionally%2C%20we%20outline%20an%20efficient%20implementation%20of%20JD%20using%0Athe%20Gramian%20of%20the%20Jacobian%20matrix%20to%20reduce%20time%20and%20memory%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16232v3&entry.124074799=Read"},
{"title": "AI-Assisted Generation of Difficult Math Questions", "author": "Vedant Shah and Dingli Yu and Kaifeng Lyu and Simon Park and Jiatong Yu and Yinghui He and Nan Rosemary Ke and Michael Mozer and Yoshua Bengio and Sanjeev Arora and Anirudh Goyal", "abstract": "  Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills.\n", "link": "http://arxiv.org/abs/2407.21009v4", "date": "2025-02-03", "relevancy": 1.4215, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4944}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4838}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Assisted%20Generation%20of%20Difficult%20Math%20Questions&body=Title%3A%20AI-Assisted%20Generation%20of%20Difficult%20Math%20Questions%0AAuthor%3A%20Vedant%20Shah%20and%20Dingli%20Yu%20and%20Kaifeng%20Lyu%20and%20Simon%20Park%20and%20Jiatong%20Yu%20and%20Yinghui%20He%20and%20Nan%20Rosemary%20Ke%20and%20Michael%20Mozer%20and%20Yoshua%20Bengio%20and%20Sanjeev%20Arora%20and%20Anirudh%20Goyal%0AAbstract%3A%20%20%20Current%20LLM%20training%20positions%20mathematical%20reasoning%20as%20a%20core%20capability.%0AWith%20publicly%20available%20sources%20fully%20tapped%2C%20there%20is%20unmet%20demand%20for%20diverse%0Aand%20challenging%20math%20questions.%20Relying%20solely%20on%20human%20experts%20is%20both%0Atime-consuming%20and%20costly%2C%20while%20LLM-generated%20questions%20often%20lack%20the%0Arequisite%20diversity%20and%20difficulty.%20We%20present%20a%20design%20framework%20that%20combines%0Athe%20strengths%20of%20LLMs%20with%20a%20human-in-the-loop%20approach%20to%20generate%20a%20diverse%0Aarray%20of%20challenging%20math%20questions.%20We%20leverage%20LLM%20metacognition%20skills%0A%5BDidolkar%20et%20al.%2C%202024%5D%20of%20a%20strong%20LLM%20to%20extract%20core%20%22skills%22%20from%20existing%0Amath%20datasets.%20These%20skills%20serve%20as%20the%20basis%20for%20generating%20novel%20and%0Adifficult%20questions%20by%20prompting%20the%20LLM%20with%20random%20pairs%20of%20core%20skills.%20The%0Ause%20of%20two%20different%20skills%20within%20each%20question%20makes%20finding%20such%20questions%0Aan%20%22out%20of%20distribution%22%20task%20for%20both%20LLMs%20and%20humans.%20Our%20pipeline%20employs%0ALLMs%20to%20iteratively%20generate%20and%20refine%20questions%20and%20solutions%20through%0Amultiturn%20prompting.%20Human%20annotators%20then%20verify%20and%20further%20refine%20the%0Aquestions%2C%20with%20their%20efficiency%20enhanced%20via%20further%20LLM%20interactions.%0AApplying%20this%20pipeline%20on%20skills%20extracted%20from%20the%20MATH%20dataset%20%5BHendrycks%20et%0Aal.%2C%202021%5D%20resulted%20in%20MATH%24%5E2%24%20-%20a%20dataset%20of%20higher-quality%20math%20questions%2C%0Aas%20evidenced%20by%3A%20%28a%29%20Lower%20performance%20of%20all%20models%20on%20MATH%24%5E2%24%20than%20on%20MATH%0A%28b%29%20Higher%20performance%20on%20MATH%20when%20using%20MATH%24%5E2%24%20questions%20as%20in-context%0Aexamples.%20Although%20focused%20on%20mathematics%2C%20our%20methodology%20seems%20applicable%20to%0Aother%20domains%20requiring%20structured%20reasoning%2C%20and%20potentially%20as%20a%20component%20of%0Ascalable%20oversight.%20Also%20of%20interest%20is%20a%20striking%20relationship%20observed%0Abetween%20models%27%20performance%20on%20the%20new%20dataset%3A%20the%20success%20rate%20on%20MATH%24%5E2%24%20is%0Athe%20square%20on%20MATH%2C%20suggesting%20that%20successfully%20solving%20the%20question%20in%0AMATH%24%5E2%24%20requires%20a%20nontrivial%20combination%20of%20two%20distinct%20math%20skills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21009v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Assisted%2520Generation%2520of%2520Difficult%2520Math%2520Questions%26entry.906535625%3DVedant%2520Shah%2520and%2520Dingli%2520Yu%2520and%2520Kaifeng%2520Lyu%2520and%2520Simon%2520Park%2520and%2520Jiatong%2520Yu%2520and%2520Yinghui%2520He%2520and%2520Nan%2520Rosemary%2520Ke%2520and%2520Michael%2520Mozer%2520and%2520Yoshua%2520Bengio%2520and%2520Sanjeev%2520Arora%2520and%2520Anirudh%2520Goyal%26entry.1292438233%3D%2520%2520Current%2520LLM%2520training%2520positions%2520mathematical%2520reasoning%2520as%2520a%2520core%2520capability.%250AWith%2520publicly%2520available%2520sources%2520fully%2520tapped%252C%2520there%2520is%2520unmet%2520demand%2520for%2520diverse%250Aand%2520challenging%2520math%2520questions.%2520Relying%2520solely%2520on%2520human%2520experts%2520is%2520both%250Atime-consuming%2520and%2520costly%252C%2520while%2520LLM-generated%2520questions%2520often%2520lack%2520the%250Arequisite%2520diversity%2520and%2520difficulty.%2520We%2520present%2520a%2520design%2520framework%2520that%2520combines%250Athe%2520strengths%2520of%2520LLMs%2520with%2520a%2520human-in-the-loop%2520approach%2520to%2520generate%2520a%2520diverse%250Aarray%2520of%2520challenging%2520math%2520questions.%2520We%2520leverage%2520LLM%2520metacognition%2520skills%250A%255BDidolkar%2520et%2520al.%252C%25202024%255D%2520of%2520a%2520strong%2520LLM%2520to%2520extract%2520core%2520%2522skills%2522%2520from%2520existing%250Amath%2520datasets.%2520These%2520skills%2520serve%2520as%2520the%2520basis%2520for%2520generating%2520novel%2520and%250Adifficult%2520questions%2520by%2520prompting%2520the%2520LLM%2520with%2520random%2520pairs%2520of%2520core%2520skills.%2520The%250Ause%2520of%2520two%2520different%2520skills%2520within%2520each%2520question%2520makes%2520finding%2520such%2520questions%250Aan%2520%2522out%2520of%2520distribution%2522%2520task%2520for%2520both%2520LLMs%2520and%2520humans.%2520Our%2520pipeline%2520employs%250ALLMs%2520to%2520iteratively%2520generate%2520and%2520refine%2520questions%2520and%2520solutions%2520through%250Amultiturn%2520prompting.%2520Human%2520annotators%2520then%2520verify%2520and%2520further%2520refine%2520the%250Aquestions%252C%2520with%2520their%2520efficiency%2520enhanced%2520via%2520further%2520LLM%2520interactions.%250AApplying%2520this%2520pipeline%2520on%2520skills%2520extracted%2520from%2520the%2520MATH%2520dataset%2520%255BHendrycks%2520et%250Aal.%252C%25202021%255D%2520resulted%2520in%2520MATH%2524%255E2%2524%2520-%2520a%2520dataset%2520of%2520higher-quality%2520math%2520questions%252C%250Aas%2520evidenced%2520by%253A%2520%2528a%2529%2520Lower%2520performance%2520of%2520all%2520models%2520on%2520MATH%2524%255E2%2524%2520than%2520on%2520MATH%250A%2528b%2529%2520Higher%2520performance%2520on%2520MATH%2520when%2520using%2520MATH%2524%255E2%2524%2520questions%2520as%2520in-context%250Aexamples.%2520Although%2520focused%2520on%2520mathematics%252C%2520our%2520methodology%2520seems%2520applicable%2520to%250Aother%2520domains%2520requiring%2520structured%2520reasoning%252C%2520and%2520potentially%2520as%2520a%2520component%2520of%250Ascalable%2520oversight.%2520Also%2520of%2520interest%2520is%2520a%2520striking%2520relationship%2520observed%250Abetween%2520models%2527%2520performance%2520on%2520the%2520new%2520dataset%253A%2520the%2520success%2520rate%2520on%2520MATH%2524%255E2%2524%2520is%250Athe%2520square%2520on%2520MATH%252C%2520suggesting%2520that%2520successfully%2520solving%2520the%2520question%2520in%250AMATH%2524%255E2%2524%2520requires%2520a%2520nontrivial%2520combination%2520of%2520two%2520distinct%2520math%2520skills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21009v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Assisted%20Generation%20of%20Difficult%20Math%20Questions&entry.906535625=Vedant%20Shah%20and%20Dingli%20Yu%20and%20Kaifeng%20Lyu%20and%20Simon%20Park%20and%20Jiatong%20Yu%20and%20Yinghui%20He%20and%20Nan%20Rosemary%20Ke%20and%20Michael%20Mozer%20and%20Yoshua%20Bengio%20and%20Sanjeev%20Arora%20and%20Anirudh%20Goyal&entry.1292438233=%20%20Current%20LLM%20training%20positions%20mathematical%20reasoning%20as%20a%20core%20capability.%0AWith%20publicly%20available%20sources%20fully%20tapped%2C%20there%20is%20unmet%20demand%20for%20diverse%0Aand%20challenging%20math%20questions.%20Relying%20solely%20on%20human%20experts%20is%20both%0Atime-consuming%20and%20costly%2C%20while%20LLM-generated%20questions%20often%20lack%20the%0Arequisite%20diversity%20and%20difficulty.%20We%20present%20a%20design%20framework%20that%20combines%0Athe%20strengths%20of%20LLMs%20with%20a%20human-in-the-loop%20approach%20to%20generate%20a%20diverse%0Aarray%20of%20challenging%20math%20questions.%20We%20leverage%20LLM%20metacognition%20skills%0A%5BDidolkar%20et%20al.%2C%202024%5D%20of%20a%20strong%20LLM%20to%20extract%20core%20%22skills%22%20from%20existing%0Amath%20datasets.%20These%20skills%20serve%20as%20the%20basis%20for%20generating%20novel%20and%0Adifficult%20questions%20by%20prompting%20the%20LLM%20with%20random%20pairs%20of%20core%20skills.%20The%0Ause%20of%20two%20different%20skills%20within%20each%20question%20makes%20finding%20such%20questions%0Aan%20%22out%20of%20distribution%22%20task%20for%20both%20LLMs%20and%20humans.%20Our%20pipeline%20employs%0ALLMs%20to%20iteratively%20generate%20and%20refine%20questions%20and%20solutions%20through%0Amultiturn%20prompting.%20Human%20annotators%20then%20verify%20and%20further%20refine%20the%0Aquestions%2C%20with%20their%20efficiency%20enhanced%20via%20further%20LLM%20interactions.%0AApplying%20this%20pipeline%20on%20skills%20extracted%20from%20the%20MATH%20dataset%20%5BHendrycks%20et%0Aal.%2C%202021%5D%20resulted%20in%20MATH%24%5E2%24%20-%20a%20dataset%20of%20higher-quality%20math%20questions%2C%0Aas%20evidenced%20by%3A%20%28a%29%20Lower%20performance%20of%20all%20models%20on%20MATH%24%5E2%24%20than%20on%20MATH%0A%28b%29%20Higher%20performance%20on%20MATH%20when%20using%20MATH%24%5E2%24%20questions%20as%20in-context%0Aexamples.%20Although%20focused%20on%20mathematics%2C%20our%20methodology%20seems%20applicable%20to%0Aother%20domains%20requiring%20structured%20reasoning%2C%20and%20potentially%20as%20a%20component%20of%0Ascalable%20oversight.%20Also%20of%20interest%20is%20a%20striking%20relationship%20observed%0Abetween%20models%27%20performance%20on%20the%20new%20dataset%3A%20the%20success%20rate%20on%20MATH%24%5E2%24%20is%0Athe%20square%20on%20MATH%2C%20suggesting%20that%20successfully%20solving%20the%20question%20in%0AMATH%24%5E2%24%20requires%20a%20nontrivial%20combination%20of%20two%20distinct%20math%20skills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21009v4&entry.124074799=Read"},
{"title": "The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators", "author": "Tzu-Heng Huang and Catherine Cao and Vaishnavi Bhargava and Frederic Sala", "abstract": "  Large pretrained models can be used as annotators, helping replace or augment\ncrowdworkers and enabling distilling generalist models into smaller specialist\nmodels. Unfortunately, this comes at a cost: employing top-of-the-line models\noften requires paying thousands of dollars for API calls, while the resulting\ndatasets are static and challenging to audit. To address these challenges, we\npropose a simple alternative: rather than directly querying labels from\npretrained models, we task models to generate programs that can produce labels.\nThese programs can be stored and applied locally, re-used and extended, and\ncost orders of magnitude less. Our system, Alchemist, obtains comparable to or\nbetter performance than large language model-based annotation in a range of\ntasks for a fraction of the cost: on average, improvements amount to a 12.9%\nenhancement while the total labeling costs across all datasets are reduced by a\nfactor of approximately 500x.\n", "link": "http://arxiv.org/abs/2407.11004v2", "date": "2025-02-03", "relevancy": 1.4212, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5227}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4612}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20ALCHEmist%3A%20Automated%20Labeling%20500x%20CHEaper%20Than%20LLM%20Data%20Annotators&body=Title%3A%20The%20ALCHEmist%3A%20Automated%20Labeling%20500x%20CHEaper%20Than%20LLM%20Data%20Annotators%0AAuthor%3A%20Tzu-Heng%20Huang%20and%20Catherine%20Cao%20and%20Vaishnavi%20Bhargava%20and%20Frederic%20Sala%0AAbstract%3A%20%20%20Large%20pretrained%20models%20can%20be%20used%20as%20annotators%2C%20helping%20replace%20or%20augment%0Acrowdworkers%20and%20enabling%20distilling%20generalist%20models%20into%20smaller%20specialist%0Amodels.%20Unfortunately%2C%20this%20comes%20at%20a%20cost%3A%20employing%20top-of-the-line%20models%0Aoften%20requires%20paying%20thousands%20of%20dollars%20for%20API%20calls%2C%20while%20the%20resulting%0Adatasets%20are%20static%20and%20challenging%20to%20audit.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20simple%20alternative%3A%20rather%20than%20directly%20querying%20labels%20from%0Apretrained%20models%2C%20we%20task%20models%20to%20generate%20programs%20that%20can%20produce%20labels.%0AThese%20programs%20can%20be%20stored%20and%20applied%20locally%2C%20re-used%20and%20extended%2C%20and%0Acost%20orders%20of%20magnitude%20less.%20Our%20system%2C%20Alchemist%2C%20obtains%20comparable%20to%20or%0Abetter%20performance%20than%20large%20language%20model-based%20annotation%20in%20a%20range%20of%0Atasks%20for%20a%20fraction%20of%20the%20cost%3A%20on%20average%2C%20improvements%20amount%20to%20a%2012.9%25%0Aenhancement%20while%20the%20total%20labeling%20costs%20across%20all%20datasets%20are%20reduced%20by%20a%0Afactor%20of%20approximately%20500x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11004v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520ALCHEmist%253A%2520Automated%2520Labeling%2520500x%2520CHEaper%2520Than%2520LLM%2520Data%2520Annotators%26entry.906535625%3DTzu-Heng%2520Huang%2520and%2520Catherine%2520Cao%2520and%2520Vaishnavi%2520Bhargava%2520and%2520Frederic%2520Sala%26entry.1292438233%3D%2520%2520Large%2520pretrained%2520models%2520can%2520be%2520used%2520as%2520annotators%252C%2520helping%2520replace%2520or%2520augment%250Acrowdworkers%2520and%2520enabling%2520distilling%2520generalist%2520models%2520into%2520smaller%2520specialist%250Amodels.%2520Unfortunately%252C%2520this%2520comes%2520at%2520a%2520cost%253A%2520employing%2520top-of-the-line%2520models%250Aoften%2520requires%2520paying%2520thousands%2520of%2520dollars%2520for%2520API%2520calls%252C%2520while%2520the%2520resulting%250Adatasets%2520are%2520static%2520and%2520challenging%2520to%2520audit.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520simple%2520alternative%253A%2520rather%2520than%2520directly%2520querying%2520labels%2520from%250Apretrained%2520models%252C%2520we%2520task%2520models%2520to%2520generate%2520programs%2520that%2520can%2520produce%2520labels.%250AThese%2520programs%2520can%2520be%2520stored%2520and%2520applied%2520locally%252C%2520re-used%2520and%2520extended%252C%2520and%250Acost%2520orders%2520of%2520magnitude%2520less.%2520Our%2520system%252C%2520Alchemist%252C%2520obtains%2520comparable%2520to%2520or%250Abetter%2520performance%2520than%2520large%2520language%2520model-based%2520annotation%2520in%2520a%2520range%2520of%250Atasks%2520for%2520a%2520fraction%2520of%2520the%2520cost%253A%2520on%2520average%252C%2520improvements%2520amount%2520to%2520a%252012.9%2525%250Aenhancement%2520while%2520the%2520total%2520labeling%2520costs%2520across%2520all%2520datasets%2520are%2520reduced%2520by%2520a%250Afactor%2520of%2520approximately%2520500x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11004v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20ALCHEmist%3A%20Automated%20Labeling%20500x%20CHEaper%20Than%20LLM%20Data%20Annotators&entry.906535625=Tzu-Heng%20Huang%20and%20Catherine%20Cao%20and%20Vaishnavi%20Bhargava%20and%20Frederic%20Sala&entry.1292438233=%20%20Large%20pretrained%20models%20can%20be%20used%20as%20annotators%2C%20helping%20replace%20or%20augment%0Acrowdworkers%20and%20enabling%20distilling%20generalist%20models%20into%20smaller%20specialist%0Amodels.%20Unfortunately%2C%20this%20comes%20at%20a%20cost%3A%20employing%20top-of-the-line%20models%0Aoften%20requires%20paying%20thousands%20of%20dollars%20for%20API%20calls%2C%20while%20the%20resulting%0Adatasets%20are%20static%20and%20challenging%20to%20audit.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20simple%20alternative%3A%20rather%20than%20directly%20querying%20labels%20from%0Apretrained%20models%2C%20we%20task%20models%20to%20generate%20programs%20that%20can%20produce%20labels.%0AThese%20programs%20can%20be%20stored%20and%20applied%20locally%2C%20re-used%20and%20extended%2C%20and%0Acost%20orders%20of%20magnitude%20less.%20Our%20system%2C%20Alchemist%2C%20obtains%20comparable%20to%20or%0Abetter%20performance%20than%20large%20language%20model-based%20annotation%20in%20a%20range%20of%0Atasks%20for%20a%20fraction%20of%20the%20cost%3A%20on%20average%2C%20improvements%20amount%20to%20a%2012.9%25%0Aenhancement%20while%20the%20total%20labeling%20costs%20across%20all%20datasets%20are%20reduced%20by%20a%0Afactor%20of%20approximately%20500x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11004v2&entry.124074799=Read"},
{"title": "s1: Simple test-time scaling", "author": "Niklas Muennighoff and Zitong Yang and Weijia Shi and Xiang Lisa Li and Li Fei-Fei and Hannaneh Hajishirzi and Luke Zettlemoyer and Percy Liang and Emmanuel Cand\u00e8s and Tatsunori Hashimoto", "abstract": "  Test-time scaling is a promising new approach to language modeling that uses\nextra test-time compute to improve performance. Recently, OpenAI's o1 model\nshowed this capability but did not publicly share its methodology, leading to\nmany replication efforts. We seek the simplest approach to achieve test-time\nscaling and strong reasoning performance. First, we curate a small dataset s1K\nof 1,000 questions paired with reasoning traces relying on three criteria we\nvalidate through ablations: difficulty, diversity, and quality. Second, we\ndevelop budget forcing to control test-time compute by forcefully terminating\nthe model's thinking process or lengthening it by appending \"Wait\" multiple\ntimes to the model's generation when it tries to end. This can lead the model\nto double-check its answer, often fixing incorrect reasoning steps. After\nsupervised finetuning the Qwen2.5-32B-Instruct language model on s1K and\nequipping it with budget forcing, our model s1-32B exceeds o1-preview on\ncompetition math questions by up to 27% (MATH and AIME24). Further, scaling\ns1-32B with budget forcing allows extrapolating beyond its performance without\ntest-time intervention: from 50% to 57% on AIME24. Our model, data, and code\nare open-source at https://github.com/simplescaling/s1\n", "link": "http://arxiv.org/abs/2501.19393v2", "date": "2025-02-03", "relevancy": 1.4039, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4954}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20s1%3A%20Simple%20test-time%20scaling&body=Title%3A%20s1%3A%20Simple%20test-time%20scaling%0AAuthor%3A%20Niklas%20Muennighoff%20and%20Zitong%20Yang%20and%20Weijia%20Shi%20and%20Xiang%20Lisa%20Li%20and%20Li%20Fei-Fei%20and%20Hannaneh%20Hajishirzi%20and%20Luke%20Zettlemoyer%20and%20Percy%20Liang%20and%20Emmanuel%20Cand%C3%A8s%20and%20Tatsunori%20Hashimoto%0AAbstract%3A%20%20%20Test-time%20scaling%20is%20a%20promising%20new%20approach%20to%20language%20modeling%20that%20uses%0Aextra%20test-time%20compute%20to%20improve%20performance.%20Recently%2C%20OpenAI%27s%20o1%20model%0Ashowed%20this%20capability%20but%20did%20not%20publicly%20share%20its%20methodology%2C%20leading%20to%0Amany%20replication%20efforts.%20We%20seek%20the%20simplest%20approach%20to%20achieve%20test-time%0Ascaling%20and%20strong%20reasoning%20performance.%20First%2C%20we%20curate%20a%20small%20dataset%20s1K%0Aof%201%2C000%20questions%20paired%20with%20reasoning%20traces%20relying%20on%20three%20criteria%20we%0Avalidate%20through%20ablations%3A%20difficulty%2C%20diversity%2C%20and%20quality.%20Second%2C%20we%0Adevelop%20budget%20forcing%20to%20control%20test-time%20compute%20by%20forcefully%20terminating%0Athe%20model%27s%20thinking%20process%20or%20lengthening%20it%20by%20appending%20%22Wait%22%20multiple%0Atimes%20to%20the%20model%27s%20generation%20when%20it%20tries%20to%20end.%20This%20can%20lead%20the%20model%0Ato%20double-check%20its%20answer%2C%20often%20fixing%20incorrect%20reasoning%20steps.%20After%0Asupervised%20finetuning%20the%20Qwen2.5-32B-Instruct%20language%20model%20on%20s1K%20and%0Aequipping%20it%20with%20budget%20forcing%2C%20our%20model%20s1-32B%20exceeds%20o1-preview%20on%0Acompetition%20math%20questions%20by%20up%20to%2027%25%20%28MATH%20and%20AIME24%29.%20Further%2C%20scaling%0As1-32B%20with%20budget%20forcing%20allows%20extrapolating%20beyond%20its%20performance%20without%0Atest-time%20intervention%3A%20from%2050%25%20to%2057%25%20on%20AIME24.%20Our%20model%2C%20data%2C%20and%20code%0Aare%20open-source%20at%20https%3A//github.com/simplescaling/s1%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19393v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Ds1%253A%2520Simple%2520test-time%2520scaling%26entry.906535625%3DNiklas%2520Muennighoff%2520and%2520Zitong%2520Yang%2520and%2520Weijia%2520Shi%2520and%2520Xiang%2520Lisa%2520Li%2520and%2520Li%2520Fei-Fei%2520and%2520Hannaneh%2520Hajishirzi%2520and%2520Luke%2520Zettlemoyer%2520and%2520Percy%2520Liang%2520and%2520Emmanuel%2520Cand%25C3%25A8s%2520and%2520Tatsunori%2520Hashimoto%26entry.1292438233%3D%2520%2520Test-time%2520scaling%2520is%2520a%2520promising%2520new%2520approach%2520to%2520language%2520modeling%2520that%2520uses%250Aextra%2520test-time%2520compute%2520to%2520improve%2520performance.%2520Recently%252C%2520OpenAI%2527s%2520o1%2520model%250Ashowed%2520this%2520capability%2520but%2520did%2520not%2520publicly%2520share%2520its%2520methodology%252C%2520leading%2520to%250Amany%2520replication%2520efforts.%2520We%2520seek%2520the%2520simplest%2520approach%2520to%2520achieve%2520test-time%250Ascaling%2520and%2520strong%2520reasoning%2520performance.%2520First%252C%2520we%2520curate%2520a%2520small%2520dataset%2520s1K%250Aof%25201%252C000%2520questions%2520paired%2520with%2520reasoning%2520traces%2520relying%2520on%2520three%2520criteria%2520we%250Avalidate%2520through%2520ablations%253A%2520difficulty%252C%2520diversity%252C%2520and%2520quality.%2520Second%252C%2520we%250Adevelop%2520budget%2520forcing%2520to%2520control%2520test-time%2520compute%2520by%2520forcefully%2520terminating%250Athe%2520model%2527s%2520thinking%2520process%2520or%2520lengthening%2520it%2520by%2520appending%2520%2522Wait%2522%2520multiple%250Atimes%2520to%2520the%2520model%2527s%2520generation%2520when%2520it%2520tries%2520to%2520end.%2520This%2520can%2520lead%2520the%2520model%250Ato%2520double-check%2520its%2520answer%252C%2520often%2520fixing%2520incorrect%2520reasoning%2520steps.%2520After%250Asupervised%2520finetuning%2520the%2520Qwen2.5-32B-Instruct%2520language%2520model%2520on%2520s1K%2520and%250Aequipping%2520it%2520with%2520budget%2520forcing%252C%2520our%2520model%2520s1-32B%2520exceeds%2520o1-preview%2520on%250Acompetition%2520math%2520questions%2520by%2520up%2520to%252027%2525%2520%2528MATH%2520and%2520AIME24%2529.%2520Further%252C%2520scaling%250As1-32B%2520with%2520budget%2520forcing%2520allows%2520extrapolating%2520beyond%2520its%2520performance%2520without%250Atest-time%2520intervention%253A%2520from%252050%2525%2520to%252057%2525%2520on%2520AIME24.%2520Our%2520model%252C%2520data%252C%2520and%2520code%250Aare%2520open-source%2520at%2520https%253A//github.com/simplescaling/s1%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19393v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=s1%3A%20Simple%20test-time%20scaling&entry.906535625=Niklas%20Muennighoff%20and%20Zitong%20Yang%20and%20Weijia%20Shi%20and%20Xiang%20Lisa%20Li%20and%20Li%20Fei-Fei%20and%20Hannaneh%20Hajishirzi%20and%20Luke%20Zettlemoyer%20and%20Percy%20Liang%20and%20Emmanuel%20Cand%C3%A8s%20and%20Tatsunori%20Hashimoto&entry.1292438233=%20%20Test-time%20scaling%20is%20a%20promising%20new%20approach%20to%20language%20modeling%20that%20uses%0Aextra%20test-time%20compute%20to%20improve%20performance.%20Recently%2C%20OpenAI%27s%20o1%20model%0Ashowed%20this%20capability%20but%20did%20not%20publicly%20share%20its%20methodology%2C%20leading%20to%0Amany%20replication%20efforts.%20We%20seek%20the%20simplest%20approach%20to%20achieve%20test-time%0Ascaling%20and%20strong%20reasoning%20performance.%20First%2C%20we%20curate%20a%20small%20dataset%20s1K%0Aof%201%2C000%20questions%20paired%20with%20reasoning%20traces%20relying%20on%20three%20criteria%20we%0Avalidate%20through%20ablations%3A%20difficulty%2C%20diversity%2C%20and%20quality.%20Second%2C%20we%0Adevelop%20budget%20forcing%20to%20control%20test-time%20compute%20by%20forcefully%20terminating%0Athe%20model%27s%20thinking%20process%20or%20lengthening%20it%20by%20appending%20%22Wait%22%20multiple%0Atimes%20to%20the%20model%27s%20generation%20when%20it%20tries%20to%20end.%20This%20can%20lead%20the%20model%0Ato%20double-check%20its%20answer%2C%20often%20fixing%20incorrect%20reasoning%20steps.%20After%0Asupervised%20finetuning%20the%20Qwen2.5-32B-Instruct%20language%20model%20on%20s1K%20and%0Aequipping%20it%20with%20budget%20forcing%2C%20our%20model%20s1-32B%20exceeds%20o1-preview%20on%0Acompetition%20math%20questions%20by%20up%20to%2027%25%20%28MATH%20and%20AIME24%29.%20Further%2C%20scaling%0As1-32B%20with%20budget%20forcing%20allows%20extrapolating%20beyond%20its%20performance%20without%0Atest-time%20intervention%3A%20from%2050%25%20to%2057%25%20on%20AIME24.%20Our%20model%2C%20data%2C%20and%20code%0Aare%20open-source%20at%20https%3A//github.com/simplescaling/s1%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19393v2&entry.124074799=Read"},
{"title": "Rethinking Explainable Machine Learning as Applied Statistics", "author": "Sebastian Bordt and Eric Raidl and Ulrike von Luxburg", "abstract": "  In the rapidly growing literature on explanation algorithms, it often remains\nunclear what precisely these algorithms are for and how they should be used. In\nthis position paper, we argue for a novel and pragmatic perspective:\nExplainable machine learning needs to recognize its parallels with applied\nstatistics. Concretely, explanations are statistics of high-dimensional\nfunctions, and we should think about them analogously to traditional\nstatistical quantities. Among others, this implies that we must think carefully\nabout the matter of interpretation, or how the explanations relate to intuitive\nquestions that humans have about the world. The fact that this is scarcely\nbeing discussed in research papers is one of the main drawbacks of the current\nliterature. Luckily, the analogy between explainable machine learning and\napplied statistics suggests fruitful ways for how research practices can be\nimproved.\n", "link": "http://arxiv.org/abs/2402.02870v3", "date": "2025-02-03", "relevancy": 1.2834, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4494}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4232}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Explainable%20Machine%20Learning%20as%20Applied%20Statistics&body=Title%3A%20Rethinking%20Explainable%20Machine%20Learning%20as%20Applied%20Statistics%0AAuthor%3A%20Sebastian%20Bordt%20and%20Eric%20Raidl%20and%20Ulrike%20von%20Luxburg%0AAbstract%3A%20%20%20In%20the%20rapidly%20growing%20literature%20on%20explanation%20algorithms%2C%20it%20often%20remains%0Aunclear%20what%20precisely%20these%20algorithms%20are%20for%20and%20how%20they%20should%20be%20used.%20In%0Athis%20position%20paper%2C%20we%20argue%20for%20a%20novel%20and%20pragmatic%20perspective%3A%0AExplainable%20machine%20learning%20needs%20to%20recognize%20its%20parallels%20with%20applied%0Astatistics.%20Concretely%2C%20explanations%20are%20statistics%20of%20high-dimensional%0Afunctions%2C%20and%20we%20should%20think%20about%20them%20analogously%20to%20traditional%0Astatistical%20quantities.%20Among%20others%2C%20this%20implies%20that%20we%20must%20think%20carefully%0Aabout%20the%20matter%20of%20interpretation%2C%20or%20how%20the%20explanations%20relate%20to%20intuitive%0Aquestions%20that%20humans%20have%20about%20the%20world.%20The%20fact%20that%20this%20is%20scarcely%0Abeing%20discussed%20in%20research%20papers%20is%20one%20of%20the%20main%20drawbacks%20of%20the%20current%0Aliterature.%20Luckily%2C%20the%20analogy%20between%20explainable%20machine%20learning%20and%0Aapplied%20statistics%20suggests%20fruitful%20ways%20for%20how%20research%20practices%20can%20be%0Aimproved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02870v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Explainable%2520Machine%2520Learning%2520as%2520Applied%2520Statistics%26entry.906535625%3DSebastian%2520Bordt%2520and%2520Eric%2520Raidl%2520and%2520Ulrike%2520von%2520Luxburg%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520growing%2520literature%2520on%2520explanation%2520algorithms%252C%2520it%2520often%2520remains%250Aunclear%2520what%2520precisely%2520these%2520algorithms%2520are%2520for%2520and%2520how%2520they%2520should%2520be%2520used.%2520In%250Athis%2520position%2520paper%252C%2520we%2520argue%2520for%2520a%2520novel%2520and%2520pragmatic%2520perspective%253A%250AExplainable%2520machine%2520learning%2520needs%2520to%2520recognize%2520its%2520parallels%2520with%2520applied%250Astatistics.%2520Concretely%252C%2520explanations%2520are%2520statistics%2520of%2520high-dimensional%250Afunctions%252C%2520and%2520we%2520should%2520think%2520about%2520them%2520analogously%2520to%2520traditional%250Astatistical%2520quantities.%2520Among%2520others%252C%2520this%2520implies%2520that%2520we%2520must%2520think%2520carefully%250Aabout%2520the%2520matter%2520of%2520interpretation%252C%2520or%2520how%2520the%2520explanations%2520relate%2520to%2520intuitive%250Aquestions%2520that%2520humans%2520have%2520about%2520the%2520world.%2520The%2520fact%2520that%2520this%2520is%2520scarcely%250Abeing%2520discussed%2520in%2520research%2520papers%2520is%2520one%2520of%2520the%2520main%2520drawbacks%2520of%2520the%2520current%250Aliterature.%2520Luckily%252C%2520the%2520analogy%2520between%2520explainable%2520machine%2520learning%2520and%250Aapplied%2520statistics%2520suggests%2520fruitful%2520ways%2520for%2520how%2520research%2520practices%2520can%2520be%250Aimproved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02870v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Explainable%20Machine%20Learning%20as%20Applied%20Statistics&entry.906535625=Sebastian%20Bordt%20and%20Eric%20Raidl%20and%20Ulrike%20von%20Luxburg&entry.1292438233=%20%20In%20the%20rapidly%20growing%20literature%20on%20explanation%20algorithms%2C%20it%20often%20remains%0Aunclear%20what%20precisely%20these%20algorithms%20are%20for%20and%20how%20they%20should%20be%20used.%20In%0Athis%20position%20paper%2C%20we%20argue%20for%20a%20novel%20and%20pragmatic%20perspective%3A%0AExplainable%20machine%20learning%20needs%20to%20recognize%20its%20parallels%20with%20applied%0Astatistics.%20Concretely%2C%20explanations%20are%20statistics%20of%20high-dimensional%0Afunctions%2C%20and%20we%20should%20think%20about%20them%20analogously%20to%20traditional%0Astatistical%20quantities.%20Among%20others%2C%20this%20implies%20that%20we%20must%20think%20carefully%0Aabout%20the%20matter%20of%20interpretation%2C%20or%20how%20the%20explanations%20relate%20to%20intuitive%0Aquestions%20that%20humans%20have%20about%20the%20world.%20The%20fact%20that%20this%20is%20scarcely%0Abeing%20discussed%20in%20research%20papers%20is%20one%20of%20the%20main%20drawbacks%20of%20the%20current%0Aliterature.%20Luckily%2C%20the%20analogy%20between%20explainable%20machine%20learning%20and%0Aapplied%20statistics%20suggests%20fruitful%20ways%20for%20how%20research%20practices%20can%20be%0Aimproved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02870v3&entry.124074799=Read"},
{"title": "Neuromorphic Wireless Split Computing with Multi-Level Spikes", "author": "Dengyu Wu and Jiechen Chen and Bipin Rajendran and H. Vincent Poor and Osvaldo Simeone", "abstract": "  Inspired by biological processes, neuromorphic computing leverages spiking\nneural networks (SNNs) to perform inference tasks, offering significant\nefficiency gains for workloads involving sequential data. Recent advances in\nhardware and software have shown that embedding a small payload within each\nspike exchanged between spiking neurons can enhance inference accuracy without\nincreasing energy consumption. To scale neuromorphic computing to larger\nworkloads, split computing - where an SNN is partitioned across two devices -\nis a promising solution. In such architectures, the device hosting the initial\nlayers must transmit information about the spikes generated by its output\nneurons to the second device. This establishes a trade-off between the benefits\nof multi-level spikes, which carry additional payload information, and the\ncommunication resources required for transmitting extra bits between devices.\nThis paper presents the first comprehensive study of a neuromorphic wireless\nsplit computing architecture that employs multi-level SNNs. We propose digital\nand analog modulation schemes for an orthogonal frequency division multiplexing\n(OFDM) radio interface to enable efficient communication. Simulation and\nexperimental results using software-defined radios reveal performance\nimprovements achieved by multi-level SNN models and provide insights into the\noptimal payload size as a function of the connection quality between the\ntransmitter and receiver.\n", "link": "http://arxiv.org/abs/2411.04728v2", "date": "2025-02-03", "relevancy": 1.2984, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4455}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4423}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuromorphic%20Wireless%20Split%20Computing%20with%20Multi-Level%20Spikes&body=Title%3A%20Neuromorphic%20Wireless%20Split%20Computing%20with%20Multi-Level%20Spikes%0AAuthor%3A%20Dengyu%20Wu%20and%20Jiechen%20Chen%20and%20Bipin%20Rajendran%20and%20H.%20Vincent%20Poor%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20Inspired%20by%20biological%20processes%2C%20neuromorphic%20computing%20leverages%20spiking%0Aneural%20networks%20%28SNNs%29%20to%20perform%20inference%20tasks%2C%20offering%20significant%0Aefficiency%20gains%20for%20workloads%20involving%20sequential%20data.%20Recent%20advances%20in%0Ahardware%20and%20software%20have%20shown%20that%20embedding%20a%20small%20payload%20within%20each%0Aspike%20exchanged%20between%20spiking%20neurons%20can%20enhance%20inference%20accuracy%20without%0Aincreasing%20energy%20consumption.%20To%20scale%20neuromorphic%20computing%20to%20larger%0Aworkloads%2C%20split%20computing%20-%20where%20an%20SNN%20is%20partitioned%20across%20two%20devices%20-%0Ais%20a%20promising%20solution.%20In%20such%20architectures%2C%20the%20device%20hosting%20the%20initial%0Alayers%20must%20transmit%20information%20about%20the%20spikes%20generated%20by%20its%20output%0Aneurons%20to%20the%20second%20device.%20This%20establishes%20a%20trade-off%20between%20the%20benefits%0Aof%20multi-level%20spikes%2C%20which%20carry%20additional%20payload%20information%2C%20and%20the%0Acommunication%20resources%20required%20for%20transmitting%20extra%20bits%20between%20devices.%0AThis%20paper%20presents%20the%20first%20comprehensive%20study%20of%20a%20neuromorphic%20wireless%0Asplit%20computing%20architecture%20that%20employs%20multi-level%20SNNs.%20We%20propose%20digital%0Aand%20analog%20modulation%20schemes%20for%20an%20orthogonal%20frequency%20division%20multiplexing%0A%28OFDM%29%20radio%20interface%20to%20enable%20efficient%20communication.%20Simulation%20and%0Aexperimental%20results%20using%20software-defined%20radios%20reveal%20performance%0Aimprovements%20achieved%20by%20multi-level%20SNN%20models%20and%20provide%20insights%20into%20the%0Aoptimal%20payload%20size%20as%20a%20function%20of%20the%20connection%20quality%20between%20the%0Atransmitter%20and%20receiver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04728v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuromorphic%2520Wireless%2520Split%2520Computing%2520with%2520Multi-Level%2520Spikes%26entry.906535625%3DDengyu%2520Wu%2520and%2520Jiechen%2520Chen%2520and%2520Bipin%2520Rajendran%2520and%2520H.%2520Vincent%2520Poor%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3D%2520%2520Inspired%2520by%2520biological%2520processes%252C%2520neuromorphic%2520computing%2520leverages%2520spiking%250Aneural%2520networks%2520%2528SNNs%2529%2520to%2520perform%2520inference%2520tasks%252C%2520offering%2520significant%250Aefficiency%2520gains%2520for%2520workloads%2520involving%2520sequential%2520data.%2520Recent%2520advances%2520in%250Ahardware%2520and%2520software%2520have%2520shown%2520that%2520embedding%2520a%2520small%2520payload%2520within%2520each%250Aspike%2520exchanged%2520between%2520spiking%2520neurons%2520can%2520enhance%2520inference%2520accuracy%2520without%250Aincreasing%2520energy%2520consumption.%2520To%2520scale%2520neuromorphic%2520computing%2520to%2520larger%250Aworkloads%252C%2520split%2520computing%2520-%2520where%2520an%2520SNN%2520is%2520partitioned%2520across%2520two%2520devices%2520-%250Ais%2520a%2520promising%2520solution.%2520In%2520such%2520architectures%252C%2520the%2520device%2520hosting%2520the%2520initial%250Alayers%2520must%2520transmit%2520information%2520about%2520the%2520spikes%2520generated%2520by%2520its%2520output%250Aneurons%2520to%2520the%2520second%2520device.%2520This%2520establishes%2520a%2520trade-off%2520between%2520the%2520benefits%250Aof%2520multi-level%2520spikes%252C%2520which%2520carry%2520additional%2520payload%2520information%252C%2520and%2520the%250Acommunication%2520resources%2520required%2520for%2520transmitting%2520extra%2520bits%2520between%2520devices.%250AThis%2520paper%2520presents%2520the%2520first%2520comprehensive%2520study%2520of%2520a%2520neuromorphic%2520wireless%250Asplit%2520computing%2520architecture%2520that%2520employs%2520multi-level%2520SNNs.%2520We%2520propose%2520digital%250Aand%2520analog%2520modulation%2520schemes%2520for%2520an%2520orthogonal%2520frequency%2520division%2520multiplexing%250A%2528OFDM%2529%2520radio%2520interface%2520to%2520enable%2520efficient%2520communication.%2520Simulation%2520and%250Aexperimental%2520results%2520using%2520software-defined%2520radios%2520reveal%2520performance%250Aimprovements%2520achieved%2520by%2520multi-level%2520SNN%2520models%2520and%2520provide%2520insights%2520into%2520the%250Aoptimal%2520payload%2520size%2520as%2520a%2520function%2520of%2520the%2520connection%2520quality%2520between%2520the%250Atransmitter%2520and%2520receiver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04728v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuromorphic%20Wireless%20Split%20Computing%20with%20Multi-Level%20Spikes&entry.906535625=Dengyu%20Wu%20and%20Jiechen%20Chen%20and%20Bipin%20Rajendran%20and%20H.%20Vincent%20Poor%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20Inspired%20by%20biological%20processes%2C%20neuromorphic%20computing%20leverages%20spiking%0Aneural%20networks%20%28SNNs%29%20to%20perform%20inference%20tasks%2C%20offering%20significant%0Aefficiency%20gains%20for%20workloads%20involving%20sequential%20data.%20Recent%20advances%20in%0Ahardware%20and%20software%20have%20shown%20that%20embedding%20a%20small%20payload%20within%20each%0Aspike%20exchanged%20between%20spiking%20neurons%20can%20enhance%20inference%20accuracy%20without%0Aincreasing%20energy%20consumption.%20To%20scale%20neuromorphic%20computing%20to%20larger%0Aworkloads%2C%20split%20computing%20-%20where%20an%20SNN%20is%20partitioned%20across%20two%20devices%20-%0Ais%20a%20promising%20solution.%20In%20such%20architectures%2C%20the%20device%20hosting%20the%20initial%0Alayers%20must%20transmit%20information%20about%20the%20spikes%20generated%20by%20its%20output%0Aneurons%20to%20the%20second%20device.%20This%20establishes%20a%20trade-off%20between%20the%20benefits%0Aof%20multi-level%20spikes%2C%20which%20carry%20additional%20payload%20information%2C%20and%20the%0Acommunication%20resources%20required%20for%20transmitting%20extra%20bits%20between%20devices.%0AThis%20paper%20presents%20the%20first%20comprehensive%20study%20of%20a%20neuromorphic%20wireless%0Asplit%20computing%20architecture%20that%20employs%20multi-level%20SNNs.%20We%20propose%20digital%0Aand%20analog%20modulation%20schemes%20for%20an%20orthogonal%20frequency%20division%20multiplexing%0A%28OFDM%29%20radio%20interface%20to%20enable%20efficient%20communication.%20Simulation%20and%0Aexperimental%20results%20using%20software-defined%20radios%20reveal%20performance%0Aimprovements%20achieved%20by%20multi-level%20SNN%20models%20and%20provide%20insights%20into%20the%0Aoptimal%20payload%20size%20as%20a%20function%20of%20the%20connection%20quality%20between%20the%0Atransmitter%20and%20receiver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04728v2&entry.124074799=Read"},
{"title": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering", "author": "Ryan Ehrlich and Bradley Brown and Jordan Juravsky and Ronald Clark and Christopher R\u00e9 and Azalia Mirhoseini", "abstract": "  Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys.\n", "link": "http://arxiv.org/abs/2501.14723v2", "date": "2025-02-03", "relevancy": 1.3503, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4561}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4518}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodeMonkeys%3A%20Scaling%20Test-Time%20Compute%20for%20Software%20Engineering&body=Title%3A%20CodeMonkeys%3A%20Scaling%20Test-Time%20Compute%20for%20Software%20Engineering%0AAuthor%3A%20Ryan%20Ehrlich%20and%20Bradley%20Brown%20and%20Jordan%20Juravsky%20and%20Ronald%20Clark%20and%20Christopher%20R%C3%A9%20and%20Azalia%20Mirhoseini%0AAbstract%3A%20%20%20Scaling%20test-time%20compute%20is%20a%20promising%20axis%20for%20improving%20LLM%20capabilities.%0AHowever%2C%20test-time%20compute%20can%20be%20scaled%20in%20a%20variety%20of%20ways%2C%20and%20effectively%0Acombining%20different%20approaches%20remains%20an%20active%20area%20of%20research.%20Here%2C%20we%0Aexplore%20this%20problem%20in%20the%20context%20of%20solving%20real-world%20GitHub%20issues%20from%0Athe%20SWE-bench%20dataset.%20Our%20system%2C%20named%20CodeMonkeys%2C%20allows%20models%20to%0Aiteratively%20edit%20a%20codebase%20by%20jointly%20generating%20and%20running%20a%20testing%20script%0Aalongside%20their%20draft%20edit.%20We%20sample%20many%20of%20these%20multi-turn%20trajectories%20for%0Aevery%20issue%20to%20generate%20a%20collection%20of%20candidate%20edits.%20This%20approach%20lets%20us%0Ascale%20%22serial%22%20test-time%20compute%20by%20increasing%20the%20number%20of%20iterations%20per%0Atrajectory%20and%20%22parallel%22%20test-time%20compute%20by%20increasing%20the%20number%20of%0Atrajectories%20per%20problem.%20With%20parallel%20scaling%2C%20we%20can%20amortize%20up-front%20costs%0Aacross%20multiple%20downstream%20samples%2C%20allowing%20us%20to%20identify%20relevant%20codebase%0Acontext%20using%20the%20simple%20method%20of%20letting%20an%20LLM%20read%20every%20file.%20In%20order%20to%0Aselect%20between%20candidate%20edits%2C%20we%20combine%20voting%20using%20model-generated%20tests%0Awith%20a%20final%20multi-turn%20trajectory%20dedicated%20to%20selection.%20Overall%2C%20CodeMonkeys%0Aresolves%2057.4%25%20of%20issues%20from%20SWE-bench%20Verified%20using%20a%20budget%20of%0Aapproximately%202300%20USD.%20Our%20selection%20method%20can%20also%20be%20used%20to%20combine%0Acandidates%20from%20different%20sources.%20Selecting%20over%20an%20ensemble%20of%20edits%20from%0Aexisting%20top%20SWE-bench%20Verified%20submissions%20obtains%20a%20score%20of%2066.2%25%20and%0Aoutperforms%20the%20best%20member%20of%20the%20ensemble%20on%20its%20own.%20We%20fully%20release%20our%0Acode%20and%20data%20at%20https%3A//scalingintelligence.stanford.edu/pubs/codemonkeys.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14723v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodeMonkeys%253A%2520Scaling%2520Test-Time%2520Compute%2520for%2520Software%2520Engineering%26entry.906535625%3DRyan%2520Ehrlich%2520and%2520Bradley%2520Brown%2520and%2520Jordan%2520Juravsky%2520and%2520Ronald%2520Clark%2520and%2520Christopher%2520R%25C3%25A9%2520and%2520Azalia%2520Mirhoseini%26entry.1292438233%3D%2520%2520Scaling%2520test-time%2520compute%2520is%2520a%2520promising%2520axis%2520for%2520improving%2520LLM%2520capabilities.%250AHowever%252C%2520test-time%2520compute%2520can%2520be%2520scaled%2520in%2520a%2520variety%2520of%2520ways%252C%2520and%2520effectively%250Acombining%2520different%2520approaches%2520remains%2520an%2520active%2520area%2520of%2520research.%2520Here%252C%2520we%250Aexplore%2520this%2520problem%2520in%2520the%2520context%2520of%2520solving%2520real-world%2520GitHub%2520issues%2520from%250Athe%2520SWE-bench%2520dataset.%2520Our%2520system%252C%2520named%2520CodeMonkeys%252C%2520allows%2520models%2520to%250Aiteratively%2520edit%2520a%2520codebase%2520by%2520jointly%2520generating%2520and%2520running%2520a%2520testing%2520script%250Aalongside%2520their%2520draft%2520edit.%2520We%2520sample%2520many%2520of%2520these%2520multi-turn%2520trajectories%2520for%250Aevery%2520issue%2520to%2520generate%2520a%2520collection%2520of%2520candidate%2520edits.%2520This%2520approach%2520lets%2520us%250Ascale%2520%2522serial%2522%2520test-time%2520compute%2520by%2520increasing%2520the%2520number%2520of%2520iterations%2520per%250Atrajectory%2520and%2520%2522parallel%2522%2520test-time%2520compute%2520by%2520increasing%2520the%2520number%2520of%250Atrajectories%2520per%2520problem.%2520With%2520parallel%2520scaling%252C%2520we%2520can%2520amortize%2520up-front%2520costs%250Aacross%2520multiple%2520downstream%2520samples%252C%2520allowing%2520us%2520to%2520identify%2520relevant%2520codebase%250Acontext%2520using%2520the%2520simple%2520method%2520of%2520letting%2520an%2520LLM%2520read%2520every%2520file.%2520In%2520order%2520to%250Aselect%2520between%2520candidate%2520edits%252C%2520we%2520combine%2520voting%2520using%2520model-generated%2520tests%250Awith%2520a%2520final%2520multi-turn%2520trajectory%2520dedicated%2520to%2520selection.%2520Overall%252C%2520CodeMonkeys%250Aresolves%252057.4%2525%2520of%2520issues%2520from%2520SWE-bench%2520Verified%2520using%2520a%2520budget%2520of%250Aapproximately%25202300%2520USD.%2520Our%2520selection%2520method%2520can%2520also%2520be%2520used%2520to%2520combine%250Acandidates%2520from%2520different%2520sources.%2520Selecting%2520over%2520an%2520ensemble%2520of%2520edits%2520from%250Aexisting%2520top%2520SWE-bench%2520Verified%2520submissions%2520obtains%2520a%2520score%2520of%252066.2%2525%2520and%250Aoutperforms%2520the%2520best%2520member%2520of%2520the%2520ensemble%2520on%2520its%2520own.%2520We%2520fully%2520release%2520our%250Acode%2520and%2520data%2520at%2520https%253A//scalingintelligence.stanford.edu/pubs/codemonkeys.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14723v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodeMonkeys%3A%20Scaling%20Test-Time%20Compute%20for%20Software%20Engineering&entry.906535625=Ryan%20Ehrlich%20and%20Bradley%20Brown%20and%20Jordan%20Juravsky%20and%20Ronald%20Clark%20and%20Christopher%20R%C3%A9%20and%20Azalia%20Mirhoseini&entry.1292438233=%20%20Scaling%20test-time%20compute%20is%20a%20promising%20axis%20for%20improving%20LLM%20capabilities.%0AHowever%2C%20test-time%20compute%20can%20be%20scaled%20in%20a%20variety%20of%20ways%2C%20and%20effectively%0Acombining%20different%20approaches%20remains%20an%20active%20area%20of%20research.%20Here%2C%20we%0Aexplore%20this%20problem%20in%20the%20context%20of%20solving%20real-world%20GitHub%20issues%20from%0Athe%20SWE-bench%20dataset.%20Our%20system%2C%20named%20CodeMonkeys%2C%20allows%20models%20to%0Aiteratively%20edit%20a%20codebase%20by%20jointly%20generating%20and%20running%20a%20testing%20script%0Aalongside%20their%20draft%20edit.%20We%20sample%20many%20of%20these%20multi-turn%20trajectories%20for%0Aevery%20issue%20to%20generate%20a%20collection%20of%20candidate%20edits.%20This%20approach%20lets%20us%0Ascale%20%22serial%22%20test-time%20compute%20by%20increasing%20the%20number%20of%20iterations%20per%0Atrajectory%20and%20%22parallel%22%20test-time%20compute%20by%20increasing%20the%20number%20of%0Atrajectories%20per%20problem.%20With%20parallel%20scaling%2C%20we%20can%20amortize%20up-front%20costs%0Aacross%20multiple%20downstream%20samples%2C%20allowing%20us%20to%20identify%20relevant%20codebase%0Acontext%20using%20the%20simple%20method%20of%20letting%20an%20LLM%20read%20every%20file.%20In%20order%20to%0Aselect%20between%20candidate%20edits%2C%20we%20combine%20voting%20using%20model-generated%20tests%0Awith%20a%20final%20multi-turn%20trajectory%20dedicated%20to%20selection.%20Overall%2C%20CodeMonkeys%0Aresolves%2057.4%25%20of%20issues%20from%20SWE-bench%20Verified%20using%20a%20budget%20of%0Aapproximately%202300%20USD.%20Our%20selection%20method%20can%20also%20be%20used%20to%20combine%0Acandidates%20from%20different%20sources.%20Selecting%20over%20an%20ensemble%20of%20edits%20from%0Aexisting%20top%20SWE-bench%20Verified%20submissions%20obtains%20a%20score%20of%2066.2%25%20and%0Aoutperforms%20the%20best%20member%20of%20the%20ensemble%20on%20its%20own.%20We%20fully%20release%20our%0Acode%20and%20data%20at%20https%3A//scalingintelligence.stanford.edu/pubs/codemonkeys.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14723v2&entry.124074799=Read"},
{"title": "Divide and Conquer: Provably Unveiling the Pareto Front with\n  Multi-Objective Reinforcement Learning", "author": "Willem R\u00f6pke and Mathieu Reymond and Patrick Mannion and Diederik M. Roijers and Ann Now\u00e9 and Roxana R\u0103dulescu", "abstract": "  An important challenge in multi-objective reinforcement learning is obtaining\na Pareto front of policies to attain optimal performance under different\npreferences. We introduce Iterated Pareto Referent Optimisation (IPRO), which\ndecomposes finding the Pareto front into a sequence of constrained\nsingle-objective problems. This enables us to guarantee convergence while\nproviding an upper bound on the distance to undiscovered Pareto optimal\nsolutions at each step. We evaluate IPRO using utility-based metrics and its\nhypervolume and find that it matches or outperforms methods that require\nadditional assumptions. By leveraging problem-specific single-objective\nsolvers, our approach also holds promise for applications beyond\nmulti-objective reinforcement learning, such as planning and pathfinding.\n", "link": "http://arxiv.org/abs/2402.07182v2", "date": "2025-02-03", "relevancy": 1.4166, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5012}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4669}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divide%20and%20Conquer%3A%20Provably%20Unveiling%20the%20Pareto%20Front%20with%0A%20%20Multi-Objective%20Reinforcement%20Learning&body=Title%3A%20Divide%20and%20Conquer%3A%20Provably%20Unveiling%20the%20Pareto%20Front%20with%0A%20%20Multi-Objective%20Reinforcement%20Learning%0AAuthor%3A%20Willem%20R%C3%B6pke%20and%20Mathieu%20Reymond%20and%20Patrick%20Mannion%20and%20Diederik%20M.%20Roijers%20and%20Ann%20Now%C3%A9%20and%20Roxana%20R%C4%83dulescu%0AAbstract%3A%20%20%20An%20important%20challenge%20in%20multi-objective%20reinforcement%20learning%20is%20obtaining%0Aa%20Pareto%20front%20of%20policies%20to%20attain%20optimal%20performance%20under%20different%0Apreferences.%20We%20introduce%20Iterated%20Pareto%20Referent%20Optimisation%20%28IPRO%29%2C%20which%0Adecomposes%20finding%20the%20Pareto%20front%20into%20a%20sequence%20of%20constrained%0Asingle-objective%20problems.%20This%20enables%20us%20to%20guarantee%20convergence%20while%0Aproviding%20an%20upper%20bound%20on%20the%20distance%20to%20undiscovered%20Pareto%20optimal%0Asolutions%20at%20each%20step.%20We%20evaluate%20IPRO%20using%20utility-based%20metrics%20and%20its%0Ahypervolume%20and%20find%20that%20it%20matches%20or%20outperforms%20methods%20that%20require%0Aadditional%20assumptions.%20By%20leveraging%20problem-specific%20single-objective%0Asolvers%2C%20our%20approach%20also%20holds%20promise%20for%20applications%20beyond%0Amulti-objective%20reinforcement%20learning%2C%20such%20as%20planning%20and%20pathfinding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07182v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivide%2520and%2520Conquer%253A%2520Provably%2520Unveiling%2520the%2520Pareto%2520Front%2520with%250A%2520%2520Multi-Objective%2520Reinforcement%2520Learning%26entry.906535625%3DWillem%2520R%25C3%25B6pke%2520and%2520Mathieu%2520Reymond%2520and%2520Patrick%2520Mannion%2520and%2520Diederik%2520M.%2520Roijers%2520and%2520Ann%2520Now%25C3%25A9%2520and%2520Roxana%2520R%25C4%2583dulescu%26entry.1292438233%3D%2520%2520An%2520important%2520challenge%2520in%2520multi-objective%2520reinforcement%2520learning%2520is%2520obtaining%250Aa%2520Pareto%2520front%2520of%2520policies%2520to%2520attain%2520optimal%2520performance%2520under%2520different%250Apreferences.%2520We%2520introduce%2520Iterated%2520Pareto%2520Referent%2520Optimisation%2520%2528IPRO%2529%252C%2520which%250Adecomposes%2520finding%2520the%2520Pareto%2520front%2520into%2520a%2520sequence%2520of%2520constrained%250Asingle-objective%2520problems.%2520This%2520enables%2520us%2520to%2520guarantee%2520convergence%2520while%250Aproviding%2520an%2520upper%2520bound%2520on%2520the%2520distance%2520to%2520undiscovered%2520Pareto%2520optimal%250Asolutions%2520at%2520each%2520step.%2520We%2520evaluate%2520IPRO%2520using%2520utility-based%2520metrics%2520and%2520its%250Ahypervolume%2520and%2520find%2520that%2520it%2520matches%2520or%2520outperforms%2520methods%2520that%2520require%250Aadditional%2520assumptions.%2520By%2520leveraging%2520problem-specific%2520single-objective%250Asolvers%252C%2520our%2520approach%2520also%2520holds%2520promise%2520for%2520applications%2520beyond%250Amulti-objective%2520reinforcement%2520learning%252C%2520such%2520as%2520planning%2520and%2520pathfinding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07182v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide%20and%20Conquer%3A%20Provably%20Unveiling%20the%20Pareto%20Front%20with%0A%20%20Multi-Objective%20Reinforcement%20Learning&entry.906535625=Willem%20R%C3%B6pke%20and%20Mathieu%20Reymond%20and%20Patrick%20Mannion%20and%20Diederik%20M.%20Roijers%20and%20Ann%20Now%C3%A9%20and%20Roxana%20R%C4%83dulescu&entry.1292438233=%20%20An%20important%20challenge%20in%20multi-objective%20reinforcement%20learning%20is%20obtaining%0Aa%20Pareto%20front%20of%20policies%20to%20attain%20optimal%20performance%20under%20different%0Apreferences.%20We%20introduce%20Iterated%20Pareto%20Referent%20Optimisation%20%28IPRO%29%2C%20which%0Adecomposes%20finding%20the%20Pareto%20front%20into%20a%20sequence%20of%20constrained%0Asingle-objective%20problems.%20This%20enables%20us%20to%20guarantee%20convergence%20while%0Aproviding%20an%20upper%20bound%20on%20the%20distance%20to%20undiscovered%20Pareto%20optimal%0Asolutions%20at%20each%20step.%20We%20evaluate%20IPRO%20using%20utility-based%20metrics%20and%20its%0Ahypervolume%20and%20find%20that%20it%20matches%20or%20outperforms%20methods%20that%20require%0Aadditional%20assumptions.%20By%20leveraging%20problem-specific%20single-objective%0Asolvers%2C%20our%20approach%20also%20holds%20promise%20for%20applications%20beyond%0Amulti-objective%20reinforcement%20learning%2C%20such%20as%20planning%20and%20pathfinding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07182v2&entry.124074799=Read"},
{"title": "Conformal Prediction for Hierarchical Data", "author": "Guillaume Principato and Gilles Stoltz and Yvenn Amara-Ouali and Yannig Goude and Bachir Hamrouche and Jean-Michel Poggi", "abstract": "  We consider conformal prediction of multivariate data series, which consists\nof outputting prediction regions based on empirical quantiles of point-estimate\nerrors. We actually consider hierarchical multivariate data series, for which\nsome components are linear combinations of others. The intuition is that the\nhierarchical structure may be leveraged to improve the prediction regions in\nterms of their sizes for given coverage levels. We implement this intuition by\nincluding a projection step (also called reconciliation step) in the split\nconformal prediction [SCP] procedure and prove that the resulting prediction\nregions are indeed globally smaller than without the projection step. The\nassociated strategies and their analyses rely on the literatures of both SCP\nand forecast reconciliation. We also illustrate the theoretical findings, both\non artificial and on real data.\n", "link": "http://arxiv.org/abs/2411.13479v2", "date": "2025-02-03", "relevancy": 1.2907, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4667}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4256}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20for%20Hierarchical%20Data&body=Title%3A%20Conformal%20Prediction%20for%20Hierarchical%20Data%0AAuthor%3A%20Guillaume%20Principato%20and%20Gilles%20Stoltz%20and%20Yvenn%20Amara-Ouali%20and%20Yannig%20Goude%20and%20Bachir%20Hamrouche%20and%20Jean-Michel%20Poggi%0AAbstract%3A%20%20%20We%20consider%20conformal%20prediction%20of%20multivariate%20data%20series%2C%20which%20consists%0Aof%20outputting%20prediction%20regions%20based%20on%20empirical%20quantiles%20of%20point-estimate%0Aerrors.%20We%20actually%20consider%20hierarchical%20multivariate%20data%20series%2C%20for%20which%0Asome%20components%20are%20linear%20combinations%20of%20others.%20The%20intuition%20is%20that%20the%0Ahierarchical%20structure%20may%20be%20leveraged%20to%20improve%20the%20prediction%20regions%20in%0Aterms%20of%20their%20sizes%20for%20given%20coverage%20levels.%20We%20implement%20this%20intuition%20by%0Aincluding%20a%20projection%20step%20%28also%20called%20reconciliation%20step%29%20in%20the%20split%0Aconformal%20prediction%20%5BSCP%5D%20procedure%20and%20prove%20that%20the%20resulting%20prediction%0Aregions%20are%20indeed%20globally%20smaller%20than%20without%20the%20projection%20step.%20The%0Aassociated%20strategies%20and%20their%20analyses%20rely%20on%20the%20literatures%20of%20both%20SCP%0Aand%20forecast%20reconciliation.%20We%20also%20illustrate%20the%20theoretical%20findings%2C%20both%0Aon%20artificial%20and%20on%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13479v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Prediction%2520for%2520Hierarchical%2520Data%26entry.906535625%3DGuillaume%2520Principato%2520and%2520Gilles%2520Stoltz%2520and%2520Yvenn%2520Amara-Ouali%2520and%2520Yannig%2520Goude%2520and%2520Bachir%2520Hamrouche%2520and%2520Jean-Michel%2520Poggi%26entry.1292438233%3D%2520%2520We%2520consider%2520conformal%2520prediction%2520of%2520multivariate%2520data%2520series%252C%2520which%2520consists%250Aof%2520outputting%2520prediction%2520regions%2520based%2520on%2520empirical%2520quantiles%2520of%2520point-estimate%250Aerrors.%2520We%2520actually%2520consider%2520hierarchical%2520multivariate%2520data%2520series%252C%2520for%2520which%250Asome%2520components%2520are%2520linear%2520combinations%2520of%2520others.%2520The%2520intuition%2520is%2520that%2520the%250Ahierarchical%2520structure%2520may%2520be%2520leveraged%2520to%2520improve%2520the%2520prediction%2520regions%2520in%250Aterms%2520of%2520their%2520sizes%2520for%2520given%2520coverage%2520levels.%2520We%2520implement%2520this%2520intuition%2520by%250Aincluding%2520a%2520projection%2520step%2520%2528also%2520called%2520reconciliation%2520step%2529%2520in%2520the%2520split%250Aconformal%2520prediction%2520%255BSCP%255D%2520procedure%2520and%2520prove%2520that%2520the%2520resulting%2520prediction%250Aregions%2520are%2520indeed%2520globally%2520smaller%2520than%2520without%2520the%2520projection%2520step.%2520The%250Aassociated%2520strategies%2520and%2520their%2520analyses%2520rely%2520on%2520the%2520literatures%2520of%2520both%2520SCP%250Aand%2520forecast%2520reconciliation.%2520We%2520also%2520illustrate%2520the%2520theoretical%2520findings%252C%2520both%250Aon%2520artificial%2520and%2520on%2520real%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13479v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20for%20Hierarchical%20Data&entry.906535625=Guillaume%20Principato%20and%20Gilles%20Stoltz%20and%20Yvenn%20Amara-Ouali%20and%20Yannig%20Goude%20and%20Bachir%20Hamrouche%20and%20Jean-Michel%20Poggi&entry.1292438233=%20%20We%20consider%20conformal%20prediction%20of%20multivariate%20data%20series%2C%20which%20consists%0Aof%20outputting%20prediction%20regions%20based%20on%20empirical%20quantiles%20of%20point-estimate%0Aerrors.%20We%20actually%20consider%20hierarchical%20multivariate%20data%20series%2C%20for%20which%0Asome%20components%20are%20linear%20combinations%20of%20others.%20The%20intuition%20is%20that%20the%0Ahierarchical%20structure%20may%20be%20leveraged%20to%20improve%20the%20prediction%20regions%20in%0Aterms%20of%20their%20sizes%20for%20given%20coverage%20levels.%20We%20implement%20this%20intuition%20by%0Aincluding%20a%20projection%20step%20%28also%20called%20reconciliation%20step%29%20in%20the%20split%0Aconformal%20prediction%20%5BSCP%5D%20procedure%20and%20prove%20that%20the%20resulting%20prediction%0Aregions%20are%20indeed%20globally%20smaller%20than%20without%20the%20projection%20step.%20The%0Aassociated%20strategies%20and%20their%20analyses%20rely%20on%20the%20literatures%20of%20both%20SCP%0Aand%20forecast%20reconciliation.%20We%20also%20illustrate%20the%20theoretical%20findings%2C%20both%0Aon%20artificial%20and%20on%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13479v2&entry.124074799=Read"},
{"title": "A Unified Comparative Study with Generalized Conformity Scores for\n  Multi-Output Conformal Regression", "author": "Victor Dheur and Matteo Fontana and Yorick Estievenart and Naomi Desobry and Souhaib Ben Taieb", "abstract": "  Conformal prediction provides a powerful framework for constructing\ndistribution-free prediction regions with finite-sample coverage guarantees.\nWhile extensively studied in univariate settings, its extension to multi-output\nproblems presents additional challenges, including complex output dependencies\nand high computational costs, and remains relatively underexplored. In this\nwork, we present a unified comparative study of nine conformal methods with\ndifferent multivariate base models for constructing multivariate prediction\nregions within the same framework. This study highlights their key properties\nwhile also exploring the connections between them. Additionally, we introduce\ntwo novel classes of conformity scores for multi-output regression that\ngeneralize their univariate counterparts. These scores ensure asymptotic\nconditional coverage while maintaining exact finite-sample marginal coverage.\nOne class is compatible with any generative model, offering broad\napplicability, while the other is computationally efficient, leveraging the\nproperties of invertible generative models. Finally, we conduct a comprehensive\nempirical evaluation across 13 tabular datasets, comparing all the multi-output\nconformal methods explored in this work. To ensure a fair and consistent\ncomparison, all methods are implemented within a unified code base.\n", "link": "http://arxiv.org/abs/2501.10533v2", "date": "2025-02-03", "relevancy": 0.9894, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5315}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4898}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Comparative%20Study%20with%20Generalized%20Conformity%20Scores%20for%0A%20%20Multi-Output%20Conformal%20Regression&body=Title%3A%20A%20Unified%20Comparative%20Study%20with%20Generalized%20Conformity%20Scores%20for%0A%20%20Multi-Output%20Conformal%20Regression%0AAuthor%3A%20Victor%20Dheur%20and%20Matteo%20Fontana%20and%20Yorick%20Estievenart%20and%20Naomi%20Desobry%20and%20Souhaib%20Ben%20Taieb%0AAbstract%3A%20%20%20Conformal%20prediction%20provides%20a%20powerful%20framework%20for%20constructing%0Adistribution-free%20prediction%20regions%20with%20finite-sample%20coverage%20guarantees.%0AWhile%20extensively%20studied%20in%20univariate%20settings%2C%20its%20extension%20to%20multi-output%0Aproblems%20presents%20additional%20challenges%2C%20including%20complex%20output%20dependencies%0Aand%20high%20computational%20costs%2C%20and%20remains%20relatively%20underexplored.%20In%20this%0Awork%2C%20we%20present%20a%20unified%20comparative%20study%20of%20nine%20conformal%20methods%20with%0Adifferent%20multivariate%20base%20models%20for%20constructing%20multivariate%20prediction%0Aregions%20within%20the%20same%20framework.%20This%20study%20highlights%20their%20key%20properties%0Awhile%20also%20exploring%20the%20connections%20between%20them.%20Additionally%2C%20we%20introduce%0Atwo%20novel%20classes%20of%20conformity%20scores%20for%20multi-output%20regression%20that%0Ageneralize%20their%20univariate%20counterparts.%20These%20scores%20ensure%20asymptotic%0Aconditional%20coverage%20while%20maintaining%20exact%20finite-sample%20marginal%20coverage.%0AOne%20class%20is%20compatible%20with%20any%20generative%20model%2C%20offering%20broad%0Aapplicability%2C%20while%20the%20other%20is%20computationally%20efficient%2C%20leveraging%20the%0Aproperties%20of%20invertible%20generative%20models.%20Finally%2C%20we%20conduct%20a%20comprehensive%0Aempirical%20evaluation%20across%2013%20tabular%20datasets%2C%20comparing%20all%20the%20multi-output%0Aconformal%20methods%20explored%20in%20this%20work.%20To%20ensure%20a%20fair%20and%20consistent%0Acomparison%2C%20all%20methods%20are%20implemented%20within%20a%20unified%20code%20base.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10533v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Comparative%2520Study%2520with%2520Generalized%2520Conformity%2520Scores%2520for%250A%2520%2520Multi-Output%2520Conformal%2520Regression%26entry.906535625%3DVictor%2520Dheur%2520and%2520Matteo%2520Fontana%2520and%2520Yorick%2520Estievenart%2520and%2520Naomi%2520Desobry%2520and%2520Souhaib%2520Ben%2520Taieb%26entry.1292438233%3D%2520%2520Conformal%2520prediction%2520provides%2520a%2520powerful%2520framework%2520for%2520constructing%250Adistribution-free%2520prediction%2520regions%2520with%2520finite-sample%2520coverage%2520guarantees.%250AWhile%2520extensively%2520studied%2520in%2520univariate%2520settings%252C%2520its%2520extension%2520to%2520multi-output%250Aproblems%2520presents%2520additional%2520challenges%252C%2520including%2520complex%2520output%2520dependencies%250Aand%2520high%2520computational%2520costs%252C%2520and%2520remains%2520relatively%2520underexplored.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520unified%2520comparative%2520study%2520of%2520nine%2520conformal%2520methods%2520with%250Adifferent%2520multivariate%2520base%2520models%2520for%2520constructing%2520multivariate%2520prediction%250Aregions%2520within%2520the%2520same%2520framework.%2520This%2520study%2520highlights%2520their%2520key%2520properties%250Awhile%2520also%2520exploring%2520the%2520connections%2520between%2520them.%2520Additionally%252C%2520we%2520introduce%250Atwo%2520novel%2520classes%2520of%2520conformity%2520scores%2520for%2520multi-output%2520regression%2520that%250Ageneralize%2520their%2520univariate%2520counterparts.%2520These%2520scores%2520ensure%2520asymptotic%250Aconditional%2520coverage%2520while%2520maintaining%2520exact%2520finite-sample%2520marginal%2520coverage.%250AOne%2520class%2520is%2520compatible%2520with%2520any%2520generative%2520model%252C%2520offering%2520broad%250Aapplicability%252C%2520while%2520the%2520other%2520is%2520computationally%2520efficient%252C%2520leveraging%2520the%250Aproperties%2520of%2520invertible%2520generative%2520models.%2520Finally%252C%2520we%2520conduct%2520a%2520comprehensive%250Aempirical%2520evaluation%2520across%252013%2520tabular%2520datasets%252C%2520comparing%2520all%2520the%2520multi-output%250Aconformal%2520methods%2520explored%2520in%2520this%2520work.%2520To%2520ensure%2520a%2520fair%2520and%2520consistent%250Acomparison%252C%2520all%2520methods%2520are%2520implemented%2520within%2520a%2520unified%2520code%2520base.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10533v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Comparative%20Study%20with%20Generalized%20Conformity%20Scores%20for%0A%20%20Multi-Output%20Conformal%20Regression&entry.906535625=Victor%20Dheur%20and%20Matteo%20Fontana%20and%20Yorick%20Estievenart%20and%20Naomi%20Desobry%20and%20Souhaib%20Ben%20Taieb&entry.1292438233=%20%20Conformal%20prediction%20provides%20a%20powerful%20framework%20for%20constructing%0Adistribution-free%20prediction%20regions%20with%20finite-sample%20coverage%20guarantees.%0AWhile%20extensively%20studied%20in%20univariate%20settings%2C%20its%20extension%20to%20multi-output%0Aproblems%20presents%20additional%20challenges%2C%20including%20complex%20output%20dependencies%0Aand%20high%20computational%20costs%2C%20and%20remains%20relatively%20underexplored.%20In%20this%0Awork%2C%20we%20present%20a%20unified%20comparative%20study%20of%20nine%20conformal%20methods%20with%0Adifferent%20multivariate%20base%20models%20for%20constructing%20multivariate%20prediction%0Aregions%20within%20the%20same%20framework.%20This%20study%20highlights%20their%20key%20properties%0Awhile%20also%20exploring%20the%20connections%20between%20them.%20Additionally%2C%20we%20introduce%0Atwo%20novel%20classes%20of%20conformity%20scores%20for%20multi-output%20regression%20that%0Ageneralize%20their%20univariate%20counterparts.%20These%20scores%20ensure%20asymptotic%0Aconditional%20coverage%20while%20maintaining%20exact%20finite-sample%20marginal%20coverage.%0AOne%20class%20is%20compatible%20with%20any%20generative%20model%2C%20offering%20broad%0Aapplicability%2C%20while%20the%20other%20is%20computationally%20efficient%2C%20leveraging%20the%0Aproperties%20of%20invertible%20generative%20models.%20Finally%2C%20we%20conduct%20a%20comprehensive%0Aempirical%20evaluation%20across%2013%20tabular%20datasets%2C%20comparing%20all%20the%20multi-output%0Aconformal%20methods%20explored%20in%20this%20work.%20To%20ensure%20a%20fair%20and%20consistent%0Acomparison%2C%20all%20methods%20are%20implemented%20within%20a%20unified%20code%20base.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10533v2&entry.124074799=Read"},
{"title": "Mitigating Information Loss in Tree-Based Reinforcement Learning via\n  Direct Optimization", "author": "Sascha Marton and Tim Grams and Florian Vogt and Stefan L\u00fcdtke and Christian Bartelt and Heiner Stuckenschmidt", "abstract": "  Reinforcement learning (RL) has seen significant success across various\ndomains, but its adoption is often limited by the black-box nature of neural\nnetwork policies, making them difficult to interpret. In contrast, symbolic\npolicies allow representing decision-making strategies in a compact and\ninterpretable way. However, learning symbolic policies directly within\non-policy methods remains challenging. In this paper, we introduce SYMPOL, a\nnovel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-based\nmodel integrated with a policy gradient method, enabling the agent to learn and\nadapt its actions while maintaining a high level of interpretability. We\nevaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiority\nover alternative tree-based RL approaches in terms of performance and\ninterpretability. Unlike existing methods, it enables gradient-based,\nend-to-end learning of interpretable, axis-aligned decision trees within\nstandard on-policy RL algorithms. Therefore, SYMPOL can become the foundation\nfor a new class of interpretable RL based on decision trees.\n", "link": "http://arxiv.org/abs/2408.08761v3", "date": "2025-02-03", "relevancy": 1.3697, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4566}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Information%20Loss%20in%20Tree-Based%20Reinforcement%20Learning%20via%0A%20%20Direct%20Optimization&body=Title%3A%20Mitigating%20Information%20Loss%20in%20Tree-Based%20Reinforcement%20Learning%20via%0A%20%20Direct%20Optimization%0AAuthor%3A%20Sascha%20Marton%20and%20Tim%20Grams%20and%20Florian%20Vogt%20and%20Stefan%20L%C3%BCdtke%20and%20Christian%20Bartelt%20and%20Heiner%20Stuckenschmidt%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20seen%20significant%20success%20across%20various%0Adomains%2C%20but%20its%20adoption%20is%20often%20limited%20by%20the%20black-box%20nature%20of%20neural%0Anetwork%20policies%2C%20making%20them%20difficult%20to%20interpret.%20In%20contrast%2C%20symbolic%0Apolicies%20allow%20representing%20decision-making%20strategies%20in%20a%20compact%20and%0Ainterpretable%20way.%20However%2C%20learning%20symbolic%20policies%20directly%20within%0Aon-policy%20methods%20remains%20challenging.%20In%20this%20paper%2C%20we%20introduce%20SYMPOL%2C%20a%0Anovel%20method%20for%20SYMbolic%20tree-based%20on-POLicy%20RL.%20SYMPOL%20employs%20a%20tree-based%0Amodel%20integrated%20with%20a%20policy%20gradient%20method%2C%20enabling%20the%20agent%20to%20learn%20and%0Aadapt%20its%20actions%20while%20maintaining%20a%20high%20level%20of%20interpretability.%20We%0Aevaluate%20SYMPOL%20on%20a%20set%20of%20benchmark%20RL%20tasks%2C%20demonstrating%20its%20superiority%0Aover%20alternative%20tree-based%20RL%20approaches%20in%20terms%20of%20performance%20and%0Ainterpretability.%20Unlike%20existing%20methods%2C%20it%20enables%20gradient-based%2C%0Aend-to-end%20learning%20of%20interpretable%2C%20axis-aligned%20decision%20trees%20within%0Astandard%20on-policy%20RL%20algorithms.%20Therefore%2C%20SYMPOL%20can%20become%20the%20foundation%0Afor%20a%20new%20class%20of%20interpretable%20RL%20based%20on%20decision%20trees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08761v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Information%2520Loss%2520in%2520Tree-Based%2520Reinforcement%2520Learning%2520via%250A%2520%2520Direct%2520Optimization%26entry.906535625%3DSascha%2520Marton%2520and%2520Tim%2520Grams%2520and%2520Florian%2520Vogt%2520and%2520Stefan%2520L%25C3%25BCdtke%2520and%2520Christian%2520Bartelt%2520and%2520Heiner%2520Stuckenschmidt%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520seen%2520significant%2520success%2520across%2520various%250Adomains%252C%2520but%2520its%2520adoption%2520is%2520often%2520limited%2520by%2520the%2520black-box%2520nature%2520of%2520neural%250Anetwork%2520policies%252C%2520making%2520them%2520difficult%2520to%2520interpret.%2520In%2520contrast%252C%2520symbolic%250Apolicies%2520allow%2520representing%2520decision-making%2520strategies%2520in%2520a%2520compact%2520and%250Ainterpretable%2520way.%2520However%252C%2520learning%2520symbolic%2520policies%2520directly%2520within%250Aon-policy%2520methods%2520remains%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SYMPOL%252C%2520a%250Anovel%2520method%2520for%2520SYMbolic%2520tree-based%2520on-POLicy%2520RL.%2520SYMPOL%2520employs%2520a%2520tree-based%250Amodel%2520integrated%2520with%2520a%2520policy%2520gradient%2520method%252C%2520enabling%2520the%2520agent%2520to%2520learn%2520and%250Aadapt%2520its%2520actions%2520while%2520maintaining%2520a%2520high%2520level%2520of%2520interpretability.%2520We%250Aevaluate%2520SYMPOL%2520on%2520a%2520set%2520of%2520benchmark%2520RL%2520tasks%252C%2520demonstrating%2520its%2520superiority%250Aover%2520alternative%2520tree-based%2520RL%2520approaches%2520in%2520terms%2520of%2520performance%2520and%250Ainterpretability.%2520Unlike%2520existing%2520methods%252C%2520it%2520enables%2520gradient-based%252C%250Aend-to-end%2520learning%2520of%2520interpretable%252C%2520axis-aligned%2520decision%2520trees%2520within%250Astandard%2520on-policy%2520RL%2520algorithms.%2520Therefore%252C%2520SYMPOL%2520can%2520become%2520the%2520foundation%250Afor%2520a%2520new%2520class%2520of%2520interpretable%2520RL%2520based%2520on%2520decision%2520trees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08761v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Information%20Loss%20in%20Tree-Based%20Reinforcement%20Learning%20via%0A%20%20Direct%20Optimization&entry.906535625=Sascha%20Marton%20and%20Tim%20Grams%20and%20Florian%20Vogt%20and%20Stefan%20L%C3%BCdtke%20and%20Christian%20Bartelt%20and%20Heiner%20Stuckenschmidt&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20seen%20significant%20success%20across%20various%0Adomains%2C%20but%20its%20adoption%20is%20often%20limited%20by%20the%20black-box%20nature%20of%20neural%0Anetwork%20policies%2C%20making%20them%20difficult%20to%20interpret.%20In%20contrast%2C%20symbolic%0Apolicies%20allow%20representing%20decision-making%20strategies%20in%20a%20compact%20and%0Ainterpretable%20way.%20However%2C%20learning%20symbolic%20policies%20directly%20within%0Aon-policy%20methods%20remains%20challenging.%20In%20this%20paper%2C%20we%20introduce%20SYMPOL%2C%20a%0Anovel%20method%20for%20SYMbolic%20tree-based%20on-POLicy%20RL.%20SYMPOL%20employs%20a%20tree-based%0Amodel%20integrated%20with%20a%20policy%20gradient%20method%2C%20enabling%20the%20agent%20to%20learn%20and%0Aadapt%20its%20actions%20while%20maintaining%20a%20high%20level%20of%20interpretability.%20We%0Aevaluate%20SYMPOL%20on%20a%20set%20of%20benchmark%20RL%20tasks%2C%20demonstrating%20its%20superiority%0Aover%20alternative%20tree-based%20RL%20approaches%20in%20terms%20of%20performance%20and%0Ainterpretability.%20Unlike%20existing%20methods%2C%20it%20enables%20gradient-based%2C%0Aend-to-end%20learning%20of%20interpretable%2C%20axis-aligned%20decision%20trees%20within%0Astandard%20on-policy%20RL%20algorithms.%20Therefore%2C%20SYMPOL%20can%20become%20the%20foundation%0Afor%20a%20new%20class%20of%20interpretable%20RL%20based%20on%20decision%20trees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08761v3&entry.124074799=Read"},
{"title": "What is the Relationship between Tensor Factorizations and Circuits (and\n  How Can We Exploit it)?", "author": "Lorenzo Loconte and Antonio Mari and Gennaro Gala and Robert Peharz and Cassio de Campos and Erik Quaeghebeur and Gennaro Vessio and Antonio Vergari", "abstract": "  This paper establishes a rigorous connection between circuit representations\nand tensor factorizations, two seemingly distinct yet fundamentally related\nareas. By connecting these fields, we highlight a series of opportunities that\ncan benefit both communities. Our work generalizes popular tensor\nfactorizations within the circuit language, and unifies various circuit\nlearning algorithms under a single, generalized hierarchical factorization\nframework. Specifically, we introduce a modular \"Lego block\" approach to build\ntensorized circuit architectures. This, in turn, allows us to systematically\nconstruct and explore various circuit and tensor factorization models while\nmaintaining tractability. This connection not only clarifies similarities and\ndifferences in existing models, but also enables the development of a\ncomprehensive pipeline for building and optimizing new circuit/tensor\nfactorization architectures. We show the effectiveness of our framework through\nextensive empirical evaluations, and highlight new research opportunities for\ntensor factorizations in probabilistic modeling.\n", "link": "http://arxiv.org/abs/2409.07953v2", "date": "2025-02-03", "relevancy": 1.356, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4833}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20is%20the%20Relationship%20between%20Tensor%20Factorizations%20and%20Circuits%20%28and%0A%20%20How%20Can%20We%20Exploit%20it%29%3F&body=Title%3A%20What%20is%20the%20Relationship%20between%20Tensor%20Factorizations%20and%20Circuits%20%28and%0A%20%20How%20Can%20We%20Exploit%20it%29%3F%0AAuthor%3A%20Lorenzo%20Loconte%20and%20Antonio%20Mari%20and%20Gennaro%20Gala%20and%20Robert%20Peharz%20and%20Cassio%20de%20Campos%20and%20Erik%20Quaeghebeur%20and%20Gennaro%20Vessio%20and%20Antonio%20Vergari%0AAbstract%3A%20%20%20This%20paper%20establishes%20a%20rigorous%20connection%20between%20circuit%20representations%0Aand%20tensor%20factorizations%2C%20two%20seemingly%20distinct%20yet%20fundamentally%20related%0Aareas.%20By%20connecting%20these%20fields%2C%20we%20highlight%20a%20series%20of%20opportunities%20that%0Acan%20benefit%20both%20communities.%20Our%20work%20generalizes%20popular%20tensor%0Afactorizations%20within%20the%20circuit%20language%2C%20and%20unifies%20various%20circuit%0Alearning%20algorithms%20under%20a%20single%2C%20generalized%20hierarchical%20factorization%0Aframework.%20Specifically%2C%20we%20introduce%20a%20modular%20%22Lego%20block%22%20approach%20to%20build%0Atensorized%20circuit%20architectures.%20This%2C%20in%20turn%2C%20allows%20us%20to%20systematically%0Aconstruct%20and%20explore%20various%20circuit%20and%20tensor%20factorization%20models%20while%0Amaintaining%20tractability.%20This%20connection%20not%20only%20clarifies%20similarities%20and%0Adifferences%20in%20existing%20models%2C%20but%20also%20enables%20the%20development%20of%20a%0Acomprehensive%20pipeline%20for%20building%20and%20optimizing%20new%20circuit/tensor%0Afactorization%20architectures.%20We%20show%20the%20effectiveness%20of%20our%20framework%20through%0Aextensive%20empirical%20evaluations%2C%20and%20highlight%20new%20research%20opportunities%20for%0Atensor%20factorizations%20in%20probabilistic%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520is%2520the%2520Relationship%2520between%2520Tensor%2520Factorizations%2520and%2520Circuits%2520%2528and%250A%2520%2520How%2520Can%2520We%2520Exploit%2520it%2529%253F%26entry.906535625%3DLorenzo%2520Loconte%2520and%2520Antonio%2520Mari%2520and%2520Gennaro%2520Gala%2520and%2520Robert%2520Peharz%2520and%2520Cassio%2520de%2520Campos%2520and%2520Erik%2520Quaeghebeur%2520and%2520Gennaro%2520Vessio%2520and%2520Antonio%2520Vergari%26entry.1292438233%3D%2520%2520This%2520paper%2520establishes%2520a%2520rigorous%2520connection%2520between%2520circuit%2520representations%250Aand%2520tensor%2520factorizations%252C%2520two%2520seemingly%2520distinct%2520yet%2520fundamentally%2520related%250Aareas.%2520By%2520connecting%2520these%2520fields%252C%2520we%2520highlight%2520a%2520series%2520of%2520opportunities%2520that%250Acan%2520benefit%2520both%2520communities.%2520Our%2520work%2520generalizes%2520popular%2520tensor%250Afactorizations%2520within%2520the%2520circuit%2520language%252C%2520and%2520unifies%2520various%2520circuit%250Alearning%2520algorithms%2520under%2520a%2520single%252C%2520generalized%2520hierarchical%2520factorization%250Aframework.%2520Specifically%252C%2520we%2520introduce%2520a%2520modular%2520%2522Lego%2520block%2522%2520approach%2520to%2520build%250Atensorized%2520circuit%2520architectures.%2520This%252C%2520in%2520turn%252C%2520allows%2520us%2520to%2520systematically%250Aconstruct%2520and%2520explore%2520various%2520circuit%2520and%2520tensor%2520factorization%2520models%2520while%250Amaintaining%2520tractability.%2520This%2520connection%2520not%2520only%2520clarifies%2520similarities%2520and%250Adifferences%2520in%2520existing%2520models%252C%2520but%2520also%2520enables%2520the%2520development%2520of%2520a%250Acomprehensive%2520pipeline%2520for%2520building%2520and%2520optimizing%2520new%2520circuit/tensor%250Afactorization%2520architectures.%2520We%2520show%2520the%2520effectiveness%2520of%2520our%2520framework%2520through%250Aextensive%2520empirical%2520evaluations%252C%2520and%2520highlight%2520new%2520research%2520opportunities%2520for%250Atensor%2520factorizations%2520in%2520probabilistic%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20is%20the%20Relationship%20between%20Tensor%20Factorizations%20and%20Circuits%20%28and%0A%20%20How%20Can%20We%20Exploit%20it%29%3F&entry.906535625=Lorenzo%20Loconte%20and%20Antonio%20Mari%20and%20Gennaro%20Gala%20and%20Robert%20Peharz%20and%20Cassio%20de%20Campos%20and%20Erik%20Quaeghebeur%20and%20Gennaro%20Vessio%20and%20Antonio%20Vergari&entry.1292438233=%20%20This%20paper%20establishes%20a%20rigorous%20connection%20between%20circuit%20representations%0Aand%20tensor%20factorizations%2C%20two%20seemingly%20distinct%20yet%20fundamentally%20related%0Aareas.%20By%20connecting%20these%20fields%2C%20we%20highlight%20a%20series%20of%20opportunities%20that%0Acan%20benefit%20both%20communities.%20Our%20work%20generalizes%20popular%20tensor%0Afactorizations%20within%20the%20circuit%20language%2C%20and%20unifies%20various%20circuit%0Alearning%20algorithms%20under%20a%20single%2C%20generalized%20hierarchical%20factorization%0Aframework.%20Specifically%2C%20we%20introduce%20a%20modular%20%22Lego%20block%22%20approach%20to%20build%0Atensorized%20circuit%20architectures.%20This%2C%20in%20turn%2C%20allows%20us%20to%20systematically%0Aconstruct%20and%20explore%20various%20circuit%20and%20tensor%20factorization%20models%20while%0Amaintaining%20tractability.%20This%20connection%20not%20only%20clarifies%20similarities%20and%0Adifferences%20in%20existing%20models%2C%20but%20also%20enables%20the%20development%20of%20a%0Acomprehensive%20pipeline%20for%20building%20and%20optimizing%20new%20circuit/tensor%0Afactorization%20architectures.%20We%20show%20the%20effectiveness%20of%20our%20framework%20through%0Aextensive%20empirical%20evaluations%2C%20and%20highlight%20new%20research%20opportunities%20for%0Atensor%20factorizations%20in%20probabilistic%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07953v2&entry.124074799=Read"},
{"title": "PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack\n  for Toxicity Maximization", "author": "Ruoxi Cheng and Yizhong Ding and Shuirong Cao and Ranjie Duan and Xiaoshuang Jia and Shaowei Yuan and Zhiqiang Wang and Xiaojun Jia", "abstract": "  Understanding the vulnerabilities of Large Vision Language Models (LVLMs) to\njailbreak attacks is essential for their responsible real-world deployment.\nMost previous work requires access to model gradients, or is based on human\nknowledge (prompt engineering) to complete jailbreak, and they hardly consider\nthe interaction of images and text, resulting in inability to jailbreak in\nblack box scenarios or poor performance. To overcome these limitations, we\npropose a Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for\ntoxicity maximization, referred to as PBI-Attack. Our method begins by\nextracting malicious features from a harmful corpus using an alternative LVLM\nand embedding these features into a benign image as prior information.\nSubsequently, we enhance these features through bidirectional cross-modal\ninteraction optimization, which iteratively optimizes the bimodal perturbations\nin an alternating manner through greedy search, aiming to maximize the toxicity\nof the generated response. The toxicity level is quantified using a\nwell-trained evaluation model. Experiments demonstrate that PBI-Attack\noutperforms previous state-of-the-art jailbreak methods, achieving an average\nattack success rate of 92.5% across three open-source LVLMs and around 67.3% on\nthree closed-source LVLMs. Disclaimer: This paper contains potentially\ndisturbing and offensive content.\n", "link": "http://arxiv.org/abs/2412.05892v3", "date": "2025-02-03", "relevancy": 1.3818, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4635}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4597}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PBI-Attack%3A%20Prior-Guided%20Bimodal%20Interactive%20Black-Box%20Jailbreak%20Attack%0A%20%20for%20Toxicity%20Maximization&body=Title%3A%20PBI-Attack%3A%20Prior-Guided%20Bimodal%20Interactive%20Black-Box%20Jailbreak%20Attack%0A%20%20for%20Toxicity%20Maximization%0AAuthor%3A%20Ruoxi%20Cheng%20and%20Yizhong%20Ding%20and%20Shuirong%20Cao%20and%20Ranjie%20Duan%20and%20Xiaoshuang%20Jia%20and%20Shaowei%20Yuan%20and%20Zhiqiang%20Wang%20and%20Xiaojun%20Jia%0AAbstract%3A%20%20%20Understanding%20the%20vulnerabilities%20of%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20to%0Ajailbreak%20attacks%20is%20essential%20for%20their%20responsible%20real-world%20deployment.%0AMost%20previous%20work%20requires%20access%20to%20model%20gradients%2C%20or%20is%20based%20on%20human%0Aknowledge%20%28prompt%20engineering%29%20to%20complete%20jailbreak%2C%20and%20they%20hardly%20consider%0Athe%20interaction%20of%20images%20and%20text%2C%20resulting%20in%20inability%20to%20jailbreak%20in%0Ablack%20box%20scenarios%20or%20poor%20performance.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20Prior-Guided%20Bimodal%20Interactive%20Black-Box%20Jailbreak%20Attack%20for%0Atoxicity%20maximization%2C%20referred%20to%20as%20PBI-Attack.%20Our%20method%20begins%20by%0Aextracting%20malicious%20features%20from%20a%20harmful%20corpus%20using%20an%20alternative%20LVLM%0Aand%20embedding%20these%20features%20into%20a%20benign%20image%20as%20prior%20information.%0ASubsequently%2C%20we%20enhance%20these%20features%20through%20bidirectional%20cross-modal%0Ainteraction%20optimization%2C%20which%20iteratively%20optimizes%20the%20bimodal%20perturbations%0Ain%20an%20alternating%20manner%20through%20greedy%20search%2C%20aiming%20to%20maximize%20the%20toxicity%0Aof%20the%20generated%20response.%20The%20toxicity%20level%20is%20quantified%20using%20a%0Awell-trained%20evaluation%20model.%20Experiments%20demonstrate%20that%20PBI-Attack%0Aoutperforms%20previous%20state-of-the-art%20jailbreak%20methods%2C%20achieving%20an%20average%0Aattack%20success%20rate%20of%2092.5%25%20across%20three%20open-source%20LVLMs%20and%20around%2067.3%25%20on%0Athree%20closed-source%20LVLMs.%20Disclaimer%3A%20This%20paper%20contains%20potentially%0Adisturbing%20and%20offensive%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05892v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPBI-Attack%253A%2520Prior-Guided%2520Bimodal%2520Interactive%2520Black-Box%2520Jailbreak%2520Attack%250A%2520%2520for%2520Toxicity%2520Maximization%26entry.906535625%3DRuoxi%2520Cheng%2520and%2520Yizhong%2520Ding%2520and%2520Shuirong%2520Cao%2520and%2520Ranjie%2520Duan%2520and%2520Xiaoshuang%2520Jia%2520and%2520Shaowei%2520Yuan%2520and%2520Zhiqiang%2520Wang%2520and%2520Xiaojun%2520Jia%26entry.1292438233%3D%2520%2520Understanding%2520the%2520vulnerabilities%2520of%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520to%250Ajailbreak%2520attacks%2520is%2520essential%2520for%2520their%2520responsible%2520real-world%2520deployment.%250AMost%2520previous%2520work%2520requires%2520access%2520to%2520model%2520gradients%252C%2520or%2520is%2520based%2520on%2520human%250Aknowledge%2520%2528prompt%2520engineering%2529%2520to%2520complete%2520jailbreak%252C%2520and%2520they%2520hardly%2520consider%250Athe%2520interaction%2520of%2520images%2520and%2520text%252C%2520resulting%2520in%2520inability%2520to%2520jailbreak%2520in%250Ablack%2520box%2520scenarios%2520or%2520poor%2520performance.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520Prior-Guided%2520Bimodal%2520Interactive%2520Black-Box%2520Jailbreak%2520Attack%2520for%250Atoxicity%2520maximization%252C%2520referred%2520to%2520as%2520PBI-Attack.%2520Our%2520method%2520begins%2520by%250Aextracting%2520malicious%2520features%2520from%2520a%2520harmful%2520corpus%2520using%2520an%2520alternative%2520LVLM%250Aand%2520embedding%2520these%2520features%2520into%2520a%2520benign%2520image%2520as%2520prior%2520information.%250ASubsequently%252C%2520we%2520enhance%2520these%2520features%2520through%2520bidirectional%2520cross-modal%250Ainteraction%2520optimization%252C%2520which%2520iteratively%2520optimizes%2520the%2520bimodal%2520perturbations%250Ain%2520an%2520alternating%2520manner%2520through%2520greedy%2520search%252C%2520aiming%2520to%2520maximize%2520the%2520toxicity%250Aof%2520the%2520generated%2520response.%2520The%2520toxicity%2520level%2520is%2520quantified%2520using%2520a%250Awell-trained%2520evaluation%2520model.%2520Experiments%2520demonstrate%2520that%2520PBI-Attack%250Aoutperforms%2520previous%2520state-of-the-art%2520jailbreak%2520methods%252C%2520achieving%2520an%2520average%250Aattack%2520success%2520rate%2520of%252092.5%2525%2520across%2520three%2520open-source%2520LVLMs%2520and%2520around%252067.3%2525%2520on%250Athree%2520closed-source%2520LVLMs.%2520Disclaimer%253A%2520This%2520paper%2520contains%2520potentially%250Adisturbing%2520and%2520offensive%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05892v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PBI-Attack%3A%20Prior-Guided%20Bimodal%20Interactive%20Black-Box%20Jailbreak%20Attack%0A%20%20for%20Toxicity%20Maximization&entry.906535625=Ruoxi%20Cheng%20and%20Yizhong%20Ding%20and%20Shuirong%20Cao%20and%20Ranjie%20Duan%20and%20Xiaoshuang%20Jia%20and%20Shaowei%20Yuan%20and%20Zhiqiang%20Wang%20and%20Xiaojun%20Jia&entry.1292438233=%20%20Understanding%20the%20vulnerabilities%20of%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20to%0Ajailbreak%20attacks%20is%20essential%20for%20their%20responsible%20real-world%20deployment.%0AMost%20previous%20work%20requires%20access%20to%20model%20gradients%2C%20or%20is%20based%20on%20human%0Aknowledge%20%28prompt%20engineering%29%20to%20complete%20jailbreak%2C%20and%20they%20hardly%20consider%0Athe%20interaction%20of%20images%20and%20text%2C%20resulting%20in%20inability%20to%20jailbreak%20in%0Ablack%20box%20scenarios%20or%20poor%20performance.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20Prior-Guided%20Bimodal%20Interactive%20Black-Box%20Jailbreak%20Attack%20for%0Atoxicity%20maximization%2C%20referred%20to%20as%20PBI-Attack.%20Our%20method%20begins%20by%0Aextracting%20malicious%20features%20from%20a%20harmful%20corpus%20using%20an%20alternative%20LVLM%0Aand%20embedding%20these%20features%20into%20a%20benign%20image%20as%20prior%20information.%0ASubsequently%2C%20we%20enhance%20these%20features%20through%20bidirectional%20cross-modal%0Ainteraction%20optimization%2C%20which%20iteratively%20optimizes%20the%20bimodal%20perturbations%0Ain%20an%20alternating%20manner%20through%20greedy%20search%2C%20aiming%20to%20maximize%20the%20toxicity%0Aof%20the%20generated%20response.%20The%20toxicity%20level%20is%20quantified%20using%20a%0Awell-trained%20evaluation%20model.%20Experiments%20demonstrate%20that%20PBI-Attack%0Aoutperforms%20previous%20state-of-the-art%20jailbreak%20methods%2C%20achieving%20an%20average%0Aattack%20success%20rate%20of%2092.5%25%20across%20three%20open-source%20LVLMs%20and%20around%2067.3%25%20on%0Athree%20closed-source%20LVLMs.%20Disclaimer%3A%20This%20paper%20contains%20potentially%0Adisturbing%20and%20offensive%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05892v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


