<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240528.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SA-GS: Semantic-Aware Gaussian Splatting for Large Scene Reconstruction\n  with Geometry Constrain", "author": "Butian Xiong and Xiaoyu Ye and Tze Ho Elden Tse and Kai Han and Shuguang Cui and Zhen Li", "abstract": "  With the emergence of Gaussian Splats, recent efforts have focused on\nlarge-scale scene geometric reconstruction. However, most of these efforts\neither concentrate on memory reduction or spatial space division, neglecting\ninformation in the semantic space. In this paper, we propose a novel method,\nnamed SA-GS, for fine-grained 3D geometry reconstruction using semantic-aware\n3D Gaussian Splats. Specifically, we leverage prior information stored in large\nvision models such as SAM and DINO to generate semantic masks. We then\nintroduce a geometric complexity measurement function to serve as soft\nregularization, guiding the shape of each Gaussian Splat within specific\nsemantic areas. Additionally, we present a method that estimates the expected\nnumber of Gaussian Splats in different semantic areas, effectively providing a\nlower bound for Gaussian Splats in these areas. Subsequently, we extract the\npoint cloud using a novel probability density-based extraction method,\ntransforming Gaussian Splats into a point cloud crucial for downstream tasks.\nOur method also offers the potential for detailed semantic inquiries while\nmaintaining high image-based reconstruction results. We provide extensive\nexperiments on publicly available large-scale scene reconstruction datasets\nwith highly accurate point clouds as ground truth and our novel dataset. Our\nresults demonstrate the superiority of our method over current state-of-the-art\nGaussian Splats reconstruction methods by a significant margin in terms of\ngeometric-based measurement metrics. Code and additional results will soon be\navailable on our project page.\n", "link": "http://arxiv.org/abs/2405.16923v2", "date": "2024-05-28", "relevancy": 3.2411, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7533}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6539}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SA-GS%3A%20Semantic-Aware%20Gaussian%20Splatting%20for%20Large%20Scene%20Reconstruction%0A%20%20with%20Geometry%20Constrain&body=Title%3A%20SA-GS%3A%20Semantic-Aware%20Gaussian%20Splatting%20for%20Large%20Scene%20Reconstruction%0A%20%20with%20Geometry%20Constrain%0AAuthor%3A%20Butian%20Xiong%20and%20Xiaoyu%20Ye%20and%20Tze%20Ho%20Elden%20Tse%20and%20Kai%20Han%20and%20Shuguang%20Cui%20and%20Zhen%20Li%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20Gaussian%20Splats%2C%20recent%20efforts%20have%20focused%20on%0Alarge-scale%20scene%20geometric%20reconstruction.%20However%2C%20most%20of%20these%20efforts%0Aeither%20concentrate%20on%20memory%20reduction%20or%20spatial%20space%20division%2C%20neglecting%0Ainformation%20in%20the%20semantic%20space.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%0Anamed%20SA-GS%2C%20for%20fine-grained%203D%20geometry%20reconstruction%20using%20semantic-aware%0A3D%20Gaussian%20Splats.%20Specifically%2C%20we%20leverage%20prior%20information%20stored%20in%20large%0Avision%20models%20such%20as%20SAM%20and%20DINO%20to%20generate%20semantic%20masks.%20We%20then%0Aintroduce%20a%20geometric%20complexity%20measurement%20function%20to%20serve%20as%20soft%0Aregularization%2C%20guiding%20the%20shape%20of%20each%20Gaussian%20Splat%20within%20specific%0Asemantic%20areas.%20Additionally%2C%20we%20present%20a%20method%20that%20estimates%20the%20expected%0Anumber%20of%20Gaussian%20Splats%20in%20different%20semantic%20areas%2C%20effectively%20providing%20a%0Alower%20bound%20for%20Gaussian%20Splats%20in%20these%20areas.%20Subsequently%2C%20we%20extract%20the%0Apoint%20cloud%20using%20a%20novel%20probability%20density-based%20extraction%20method%2C%0Atransforming%20Gaussian%20Splats%20into%20a%20point%20cloud%20crucial%20for%20downstream%20tasks.%0AOur%20method%20also%20offers%20the%20potential%20for%20detailed%20semantic%20inquiries%20while%0Amaintaining%20high%20image-based%20reconstruction%20results.%20We%20provide%20extensive%0Aexperiments%20on%20publicly%20available%20large-scale%20scene%20reconstruction%20datasets%0Awith%20highly%20accurate%20point%20clouds%20as%20ground%20truth%20and%20our%20novel%20dataset.%20Our%0Aresults%20demonstrate%20the%20superiority%20of%20our%20method%20over%20current%20state-of-the-art%0AGaussian%20Splats%20reconstruction%20methods%20by%20a%20significant%20margin%20in%20terms%20of%0Ageometric-based%20measurement%20metrics.%20Code%20and%20additional%20results%20will%20soon%20be%0Aavailable%20on%20our%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16923v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSA-GS%253A%2520Semantic-Aware%2520Gaussian%2520Splatting%2520for%2520Large%2520Scene%2520Reconstruction%250A%2520%2520with%2520Geometry%2520Constrain%26entry.906535625%3DButian%2520Xiong%2520and%2520Xiaoyu%2520Ye%2520and%2520Tze%2520Ho%2520Elden%2520Tse%2520and%2520Kai%2520Han%2520and%2520Shuguang%2520Cui%2520and%2520Zhen%2520Li%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520Gaussian%2520Splats%252C%2520recent%2520efforts%2520have%2520focused%2520on%250Alarge-scale%2520scene%2520geometric%2520reconstruction.%2520However%252C%2520most%2520of%2520these%2520efforts%250Aeither%2520concentrate%2520on%2520memory%2520reduction%2520or%2520spatial%2520space%2520division%252C%2520neglecting%250Ainformation%2520in%2520the%2520semantic%2520space.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%252C%250Anamed%2520SA-GS%252C%2520for%2520fine-grained%25203D%2520geometry%2520reconstruction%2520using%2520semantic-aware%250A3D%2520Gaussian%2520Splats.%2520Specifically%252C%2520we%2520leverage%2520prior%2520information%2520stored%2520in%2520large%250Avision%2520models%2520such%2520as%2520SAM%2520and%2520DINO%2520to%2520generate%2520semantic%2520masks.%2520We%2520then%250Aintroduce%2520a%2520geometric%2520complexity%2520measurement%2520function%2520to%2520serve%2520as%2520soft%250Aregularization%252C%2520guiding%2520the%2520shape%2520of%2520each%2520Gaussian%2520Splat%2520within%2520specific%250Asemantic%2520areas.%2520Additionally%252C%2520we%2520present%2520a%2520method%2520that%2520estimates%2520the%2520expected%250Anumber%2520of%2520Gaussian%2520Splats%2520in%2520different%2520semantic%2520areas%252C%2520effectively%2520providing%2520a%250Alower%2520bound%2520for%2520Gaussian%2520Splats%2520in%2520these%2520areas.%2520Subsequently%252C%2520we%2520extract%2520the%250Apoint%2520cloud%2520using%2520a%2520novel%2520probability%2520density-based%2520extraction%2520method%252C%250Atransforming%2520Gaussian%2520Splats%2520into%2520a%2520point%2520cloud%2520crucial%2520for%2520downstream%2520tasks.%250AOur%2520method%2520also%2520offers%2520the%2520potential%2520for%2520detailed%2520semantic%2520inquiries%2520while%250Amaintaining%2520high%2520image-based%2520reconstruction%2520results.%2520We%2520provide%2520extensive%250Aexperiments%2520on%2520publicly%2520available%2520large-scale%2520scene%2520reconstruction%2520datasets%250Awith%2520highly%2520accurate%2520point%2520clouds%2520as%2520ground%2520truth%2520and%2520our%2520novel%2520dataset.%2520Our%250Aresults%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520current%2520state-of-the-art%250AGaussian%2520Splats%2520reconstruction%2520methods%2520by%2520a%2520significant%2520margin%2520in%2520terms%2520of%250Ageometric-based%2520measurement%2520metrics.%2520Code%2520and%2520additional%2520results%2520will%2520soon%2520be%250Aavailable%2520on%2520our%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16923v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SA-GS%3A%20Semantic-Aware%20Gaussian%20Splatting%20for%20Large%20Scene%20Reconstruction%0A%20%20with%20Geometry%20Constrain&entry.906535625=Butian%20Xiong%20and%20Xiaoyu%20Ye%20and%20Tze%20Ho%20Elden%20Tse%20and%20Kai%20Han%20and%20Shuguang%20Cui%20and%20Zhen%20Li&entry.1292438233=%20%20With%20the%20emergence%20of%20Gaussian%20Splats%2C%20recent%20efforts%20have%20focused%20on%0Alarge-scale%20scene%20geometric%20reconstruction.%20However%2C%20most%20of%20these%20efforts%0Aeither%20concentrate%20on%20memory%20reduction%20or%20spatial%20space%20division%2C%20neglecting%0Ainformation%20in%20the%20semantic%20space.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%0Anamed%20SA-GS%2C%20for%20fine-grained%203D%20geometry%20reconstruction%20using%20semantic-aware%0A3D%20Gaussian%20Splats.%20Specifically%2C%20we%20leverage%20prior%20information%20stored%20in%20large%0Avision%20models%20such%20as%20SAM%20and%20DINO%20to%20generate%20semantic%20masks.%20We%20then%0Aintroduce%20a%20geometric%20complexity%20measurement%20function%20to%20serve%20as%20soft%0Aregularization%2C%20guiding%20the%20shape%20of%20each%20Gaussian%20Splat%20within%20specific%0Asemantic%20areas.%20Additionally%2C%20we%20present%20a%20method%20that%20estimates%20the%20expected%0Anumber%20of%20Gaussian%20Splats%20in%20different%20semantic%20areas%2C%20effectively%20providing%20a%0Alower%20bound%20for%20Gaussian%20Splats%20in%20these%20areas.%20Subsequently%2C%20we%20extract%20the%0Apoint%20cloud%20using%20a%20novel%20probability%20density-based%20extraction%20method%2C%0Atransforming%20Gaussian%20Splats%20into%20a%20point%20cloud%20crucial%20for%20downstream%20tasks.%0AOur%20method%20also%20offers%20the%20potential%20for%20detailed%20semantic%20inquiries%20while%0Amaintaining%20high%20image-based%20reconstruction%20results.%20We%20provide%20extensive%0Aexperiments%20on%20publicly%20available%20large-scale%20scene%20reconstruction%20datasets%0Awith%20highly%20accurate%20point%20clouds%20as%20ground%20truth%20and%20our%20novel%20dataset.%20Our%0Aresults%20demonstrate%20the%20superiority%20of%20our%20method%20over%20current%20state-of-the-art%0AGaussian%20Splats%20reconstruction%20methods%20by%20a%20significant%20margin%20in%20terms%20of%0Ageometric-based%20measurement%20metrics.%20Code%20and%20additional%20results%20will%20soon%20be%0Aavailable%20on%20our%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16923v2&entry.124074799=Read"},
{"title": "GMTalker: Gaussian Mixture-based Audio-Driven Emotional talking video\n  Portraits", "author": "Yibo Xia and Lizhen Wang and Xiang Deng and Xiaoyan Luo and Yebin Liu", "abstract": "  Synthesizing high-fidelity and emotion-controllable talking video portraits,\nwith audio-lip sync, vivid expressions, realistic head poses, and eye blinks,\nhas been an important and challenging task in recent years. Most existing\nmethods suffer in achieving personalized and precise emotion control, smooth\ntransitions between different emotion states, and the generation of diverse\nmotions. To tackle these challenges, we present GMTalker, a Gaussian\nmixture-based emotional talking portraits generation framework. Specifically,\nwe propose a Gaussian mixture-based expression generator that can construct a\ncontinuous and disentangled latent space, achieving more flexible emotion\nmanipulation. Furthermore, we introduce a normalizing flow-based motion\ngenerator pretrained on a large dataset with a wide-range motion to generate\ndiverse head poses, blinks, and eyeball movements. Finally, we propose a\npersonalized emotion-guided head generator with an emotion mapping network that\ncan synthesize high-fidelity and faithful emotional video portraits. Both\nquantitative and qualitative experiments demonstrate our method outperforms\nprevious methods in image quality, photo-realism, emotion accuracy, and motion\ndiversity.\n", "link": "http://arxiv.org/abs/2312.07669v2", "date": "2024-05-28", "relevancy": 3.2359, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6666}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6666}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMTalker%3A%20Gaussian%20Mixture-based%20Audio-Driven%20Emotional%20talking%20video%0A%20%20Portraits&body=Title%3A%20GMTalker%3A%20Gaussian%20Mixture-based%20Audio-Driven%20Emotional%20talking%20video%0A%20%20Portraits%0AAuthor%3A%20Yibo%20Xia%20and%20Lizhen%20Wang%20and%20Xiang%20Deng%20and%20Xiaoyan%20Luo%20and%20Yebin%20Liu%0AAbstract%3A%20%20%20Synthesizing%20high-fidelity%20and%20emotion-controllable%20talking%20video%20portraits%2C%0Awith%20audio-lip%20sync%2C%20vivid%20expressions%2C%20realistic%20head%20poses%2C%20and%20eye%20blinks%2C%0Ahas%20been%20an%20important%20and%20challenging%20task%20in%20recent%20years.%20Most%20existing%0Amethods%20suffer%20in%20achieving%20personalized%20and%20precise%20emotion%20control%2C%20smooth%0Atransitions%20between%20different%20emotion%20states%2C%20and%20the%20generation%20of%20diverse%0Amotions.%20To%20tackle%20these%20challenges%2C%20we%20present%20GMTalker%2C%20a%20Gaussian%0Amixture-based%20emotional%20talking%20portraits%20generation%20framework.%20Specifically%2C%0Awe%20propose%20a%20Gaussian%20mixture-based%20expression%20generator%20that%20can%20construct%20a%0Acontinuous%20and%20disentangled%20latent%20space%2C%20achieving%20more%20flexible%20emotion%0Amanipulation.%20Furthermore%2C%20we%20introduce%20a%20normalizing%20flow-based%20motion%0Agenerator%20pretrained%20on%20a%20large%20dataset%20with%20a%20wide-range%20motion%20to%20generate%0Adiverse%20head%20poses%2C%20blinks%2C%20and%20eyeball%20movements.%20Finally%2C%20we%20propose%20a%0Apersonalized%20emotion-guided%20head%20generator%20with%20an%20emotion%20mapping%20network%20that%0Acan%20synthesize%20high-fidelity%20and%20faithful%20emotional%20video%20portraits.%20Both%0Aquantitative%20and%20qualitative%20experiments%20demonstrate%20our%20method%20outperforms%0Aprevious%20methods%20in%20image%20quality%2C%20photo-realism%2C%20emotion%20accuracy%2C%20and%20motion%0Adiversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07669v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMTalker%253A%2520Gaussian%2520Mixture-based%2520Audio-Driven%2520Emotional%2520talking%2520video%250A%2520%2520Portraits%26entry.906535625%3DYibo%2520Xia%2520and%2520Lizhen%2520Wang%2520and%2520Xiang%2520Deng%2520and%2520Xiaoyan%2520Luo%2520and%2520Yebin%2520Liu%26entry.1292438233%3D%2520%2520Synthesizing%2520high-fidelity%2520and%2520emotion-controllable%2520talking%2520video%2520portraits%252C%250Awith%2520audio-lip%2520sync%252C%2520vivid%2520expressions%252C%2520realistic%2520head%2520poses%252C%2520and%2520eye%2520blinks%252C%250Ahas%2520been%2520an%2520important%2520and%2520challenging%2520task%2520in%2520recent%2520years.%2520Most%2520existing%250Amethods%2520suffer%2520in%2520achieving%2520personalized%2520and%2520precise%2520emotion%2520control%252C%2520smooth%250Atransitions%2520between%2520different%2520emotion%2520states%252C%2520and%2520the%2520generation%2520of%2520diverse%250Amotions.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520present%2520GMTalker%252C%2520a%2520Gaussian%250Amixture-based%2520emotional%2520talking%2520portraits%2520generation%2520framework.%2520Specifically%252C%250Awe%2520propose%2520a%2520Gaussian%2520mixture-based%2520expression%2520generator%2520that%2520can%2520construct%2520a%250Acontinuous%2520and%2520disentangled%2520latent%2520space%252C%2520achieving%2520more%2520flexible%2520emotion%250Amanipulation.%2520Furthermore%252C%2520we%2520introduce%2520a%2520normalizing%2520flow-based%2520motion%250Agenerator%2520pretrained%2520on%2520a%2520large%2520dataset%2520with%2520a%2520wide-range%2520motion%2520to%2520generate%250Adiverse%2520head%2520poses%252C%2520blinks%252C%2520and%2520eyeball%2520movements.%2520Finally%252C%2520we%2520propose%2520a%250Apersonalized%2520emotion-guided%2520head%2520generator%2520with%2520an%2520emotion%2520mapping%2520network%2520that%250Acan%2520synthesize%2520high-fidelity%2520and%2520faithful%2520emotional%2520video%2520portraits.%2520Both%250Aquantitative%2520and%2520qualitative%2520experiments%2520demonstrate%2520our%2520method%2520outperforms%250Aprevious%2520methods%2520in%2520image%2520quality%252C%2520photo-realism%252C%2520emotion%2520accuracy%252C%2520and%2520motion%250Adiversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07669v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMTalker%3A%20Gaussian%20Mixture-based%20Audio-Driven%20Emotional%20talking%20video%0A%20%20Portraits&entry.906535625=Yibo%20Xia%20and%20Lizhen%20Wang%20and%20Xiang%20Deng%20and%20Xiaoyan%20Luo%20and%20Yebin%20Liu&entry.1292438233=%20%20Synthesizing%20high-fidelity%20and%20emotion-controllable%20talking%20video%20portraits%2C%0Awith%20audio-lip%20sync%2C%20vivid%20expressions%2C%20realistic%20head%20poses%2C%20and%20eye%20blinks%2C%0Ahas%20been%20an%20important%20and%20challenging%20task%20in%20recent%20years.%20Most%20existing%0Amethods%20suffer%20in%20achieving%20personalized%20and%20precise%20emotion%20control%2C%20smooth%0Atransitions%20between%20different%20emotion%20states%2C%20and%20the%20generation%20of%20diverse%0Amotions.%20To%20tackle%20these%20challenges%2C%20we%20present%20GMTalker%2C%20a%20Gaussian%0Amixture-based%20emotional%20talking%20portraits%20generation%20framework.%20Specifically%2C%0Awe%20propose%20a%20Gaussian%20mixture-based%20expression%20generator%20that%20can%20construct%20a%0Acontinuous%20and%20disentangled%20latent%20space%2C%20achieving%20more%20flexible%20emotion%0Amanipulation.%20Furthermore%2C%20we%20introduce%20a%20normalizing%20flow-based%20motion%0Agenerator%20pretrained%20on%20a%20large%20dataset%20with%20a%20wide-range%20motion%20to%20generate%0Adiverse%20head%20poses%2C%20blinks%2C%20and%20eyeball%20movements.%20Finally%2C%20we%20propose%20a%0Apersonalized%20emotion-guided%20head%20generator%20with%20an%20emotion%20mapping%20network%20that%0Acan%20synthesize%20high-fidelity%20and%20faithful%20emotional%20video%20portraits.%20Both%0Aquantitative%20and%20qualitative%20experiments%20demonstrate%20our%20method%20outperforms%0Aprevious%20methods%20in%20image%20quality%2C%20photo-realism%2C%20emotion%20accuracy%2C%20and%20motion%0Adiversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07669v2&entry.124074799=Read"},
{"title": "RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian\n  Representations of Radiance Fields", "author": "Mihnea-Bogdan Jurca and Remco Royen and Ion Giosan and Adrian Munteanu", "abstract": "  Gaussian Splatting has revolutionized the world of novel view synthesis by\nachieving high rendering performance in real-time. Recently, studies have\nfocused on enriching these 3D representations with semantic information for\ndownstream tasks. In this paper, we introduce RT-GS2, the first generalizable\nsemantic segmentation method employing Gaussian Splatting. While existing\nGaussian Splatting-based approaches rely on scene-specific training, RT-GS2\ndemonstrates the ability to generalize to unseen scenes. Our method adopts a\nnew approach by first extracting view-independent 3D Gaussian features in a\nself-supervised manner, followed by a novel View-Dependent / View-Independent\n(VDVI) feature fusion to enhance semantic consistency over different views.\nExtensive experimentation on three different datasets showcases RT-GS2's\nsuperiority over the state-of-the-art methods in semantic segmentation quality,\nexemplified by a 8.01% increase in mIoU on the Replica dataset. Moreover, our\nmethod achieves real-time performance of 27.03 FPS, marking an astonishing 901\ntimes speedup compared to existing approaches. This work represents a\nsignificant advancement in the field by introducing, to the best of our\nknowledge, the first real-time generalizable semantic segmentation method for\n3D Gaussian representations of radiance fields.\n", "link": "http://arxiv.org/abs/2405.18033v1", "date": "2024-05-28", "relevancy": 3.2022, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7846}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5912}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RT-GS2%3A%20Real-Time%20Generalizable%20Semantic%20Segmentation%20for%203D%20Gaussian%0A%20%20Representations%20of%20Radiance%20Fields&body=Title%3A%20RT-GS2%3A%20Real-Time%20Generalizable%20Semantic%20Segmentation%20for%203D%20Gaussian%0A%20%20Representations%20of%20Radiance%20Fields%0AAuthor%3A%20Mihnea-Bogdan%20Jurca%20and%20Remco%20Royen%20and%20Ion%20Giosan%20and%20Adrian%20Munteanu%0AAbstract%3A%20%20%20Gaussian%20Splatting%20has%20revolutionized%20the%20world%20of%20novel%20view%20synthesis%20by%0Aachieving%20high%20rendering%20performance%20in%20real-time.%20Recently%2C%20studies%20have%0Afocused%20on%20enriching%20these%203D%20representations%20with%20semantic%20information%20for%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20introduce%20RT-GS2%2C%20the%20first%20generalizable%0Asemantic%20segmentation%20method%20employing%20Gaussian%20Splatting.%20While%20existing%0AGaussian%20Splatting-based%20approaches%20rely%20on%20scene-specific%20training%2C%20RT-GS2%0Ademonstrates%20the%20ability%20to%20generalize%20to%20unseen%20scenes.%20Our%20method%20adopts%20a%0Anew%20approach%20by%20first%20extracting%20view-independent%203D%20Gaussian%20features%20in%20a%0Aself-supervised%20manner%2C%20followed%20by%20a%20novel%20View-Dependent%20/%20View-Independent%0A%28VDVI%29%20feature%20fusion%20to%20enhance%20semantic%20consistency%20over%20different%20views.%0AExtensive%20experimentation%20on%20three%20different%20datasets%20showcases%20RT-GS2%27s%0Asuperiority%20over%20the%20state-of-the-art%20methods%20in%20semantic%20segmentation%20quality%2C%0Aexemplified%20by%20a%208.01%25%20increase%20in%20mIoU%20on%20the%20Replica%20dataset.%20Moreover%2C%20our%0Amethod%20achieves%20real-time%20performance%20of%2027.03%20FPS%2C%20marking%20an%20astonishing%20901%0Atimes%20speedup%20compared%20to%20existing%20approaches.%20This%20work%20represents%20a%0Asignificant%20advancement%20in%20the%20field%20by%20introducing%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20real-time%20generalizable%20semantic%20segmentation%20method%20for%0A3D%20Gaussian%20representations%20of%20radiance%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRT-GS2%253A%2520Real-Time%2520Generalizable%2520Semantic%2520Segmentation%2520for%25203D%2520Gaussian%250A%2520%2520Representations%2520of%2520Radiance%2520Fields%26entry.906535625%3DMihnea-Bogdan%2520Jurca%2520and%2520Remco%2520Royen%2520and%2520Ion%2520Giosan%2520and%2520Adrian%2520Munteanu%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520has%2520revolutionized%2520the%2520world%2520of%2520novel%2520view%2520synthesis%2520by%250Aachieving%2520high%2520rendering%2520performance%2520in%2520real-time.%2520Recently%252C%2520studies%2520have%250Afocused%2520on%2520enriching%2520these%25203D%2520representations%2520with%2520semantic%2520information%2520for%250Adownstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520RT-GS2%252C%2520the%2520first%2520generalizable%250Asemantic%2520segmentation%2520method%2520employing%2520Gaussian%2520Splatting.%2520While%2520existing%250AGaussian%2520Splatting-based%2520approaches%2520rely%2520on%2520scene-specific%2520training%252C%2520RT-GS2%250Ademonstrates%2520the%2520ability%2520to%2520generalize%2520to%2520unseen%2520scenes.%2520Our%2520method%2520adopts%2520a%250Anew%2520approach%2520by%2520first%2520extracting%2520view-independent%25203D%2520Gaussian%2520features%2520in%2520a%250Aself-supervised%2520manner%252C%2520followed%2520by%2520a%2520novel%2520View-Dependent%2520/%2520View-Independent%250A%2528VDVI%2529%2520feature%2520fusion%2520to%2520enhance%2520semantic%2520consistency%2520over%2520different%2520views.%250AExtensive%2520experimentation%2520on%2520three%2520different%2520datasets%2520showcases%2520RT-GS2%2527s%250Asuperiority%2520over%2520the%2520state-of-the-art%2520methods%2520in%2520semantic%2520segmentation%2520quality%252C%250Aexemplified%2520by%2520a%25208.01%2525%2520increase%2520in%2520mIoU%2520on%2520the%2520Replica%2520dataset.%2520Moreover%252C%2520our%250Amethod%2520achieves%2520real-time%2520performance%2520of%252027.03%2520FPS%252C%2520marking%2520an%2520astonishing%2520901%250Atimes%2520speedup%2520compared%2520to%2520existing%2520approaches.%2520This%2520work%2520represents%2520a%250Asignificant%2520advancement%2520in%2520the%2520field%2520by%2520introducing%252C%2520to%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520the%2520first%2520real-time%2520generalizable%2520semantic%2520segmentation%2520method%2520for%250A3D%2520Gaussian%2520representations%2520of%2520radiance%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RT-GS2%3A%20Real-Time%20Generalizable%20Semantic%20Segmentation%20for%203D%20Gaussian%0A%20%20Representations%20of%20Radiance%20Fields&entry.906535625=Mihnea-Bogdan%20Jurca%20and%20Remco%20Royen%20and%20Ion%20Giosan%20and%20Adrian%20Munteanu&entry.1292438233=%20%20Gaussian%20Splatting%20has%20revolutionized%20the%20world%20of%20novel%20view%20synthesis%20by%0Aachieving%20high%20rendering%20performance%20in%20real-time.%20Recently%2C%20studies%20have%0Afocused%20on%20enriching%20these%203D%20representations%20with%20semantic%20information%20for%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20introduce%20RT-GS2%2C%20the%20first%20generalizable%0Asemantic%20segmentation%20method%20employing%20Gaussian%20Splatting.%20While%20existing%0AGaussian%20Splatting-based%20approaches%20rely%20on%20scene-specific%20training%2C%20RT-GS2%0Ademonstrates%20the%20ability%20to%20generalize%20to%20unseen%20scenes.%20Our%20method%20adopts%20a%0Anew%20approach%20by%20first%20extracting%20view-independent%203D%20Gaussian%20features%20in%20a%0Aself-supervised%20manner%2C%20followed%20by%20a%20novel%20View-Dependent%20/%20View-Independent%0A%28VDVI%29%20feature%20fusion%20to%20enhance%20semantic%20consistency%20over%20different%20views.%0AExtensive%20experimentation%20on%20three%20different%20datasets%20showcases%20RT-GS2%27s%0Asuperiority%20over%20the%20state-of-the-art%20methods%20in%20semantic%20segmentation%20quality%2C%0Aexemplified%20by%20a%208.01%25%20increase%20in%20mIoU%20on%20the%20Replica%20dataset.%20Moreover%2C%20our%0Amethod%20achieves%20real-time%20performance%20of%2027.03%20FPS%2C%20marking%20an%20astonishing%20901%0Atimes%20speedup%20compared%20to%20existing%20approaches.%20This%20work%20represents%20a%0Asignificant%20advancement%20in%20the%20field%20by%20introducing%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20real-time%20generalizable%20semantic%20segmentation%20method%20for%0A3D%20Gaussian%20representations%20of%20radiance%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18033v1&entry.124074799=Read"},
{"title": "EG4D: Explicit Generation of 4D Object without Score Distillation", "author": "Qi Sun and Zhiyang Guo and Ziyu Wan and Jing Nathan Yan and Shengming Yin and Wengang Zhou and Jing Liao and Houqiang Li", "abstract": "  In recent years, the increasing demand for dynamic 3D assets in design and\ngaming applications has given rise to powerful generative pipelines capable of\nsynthesizing high-quality 4D objects. Previous methods generally rely on score\ndistillation sampling (SDS) algorithm to infer the unseen views and motion of\n4D objects, thus leading to unsatisfactory results with defects like\nover-saturation and Janus problem. Therefore, inspired by recent progress of\nvideo diffusion models, we propose to optimize a 4D representation by\nexplicitly generating multi-view videos from one input image. However, it is\nfar from trivial to handle practical challenges faced by such a pipeline,\nincluding dramatic temporal inconsistency, inter-frame geometry and texture\ndiversity, and semantic defects brought by video generation results. To address\nthese issues, we propose DG4D, a novel multi-stage framework that generates\nhigh-quality and consistent 4D assets without score distillation. Specifically,\ncollaborative techniques and solutions are developed, including an attention\ninjection strategy to synthesize temporal-consistent multi-view videos, a\nrobust and efficient dynamic reconstruction method based on Gaussian Splatting,\nand a refinement stage with diffusion prior for semantic restoration. The\nqualitative results and user preference study demonstrate that our framework\noutperforms the baselines in generation quality by a considerable margin. Code\nwill be released at \\url{https://github.com/jasongzy/EG4D}.\n", "link": "http://arxiv.org/abs/2405.18132v1", "date": "2024-05-28", "relevancy": 3.1858, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6537}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.637}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EG4D%3A%20Explicit%20Generation%20of%204D%20Object%20without%20Score%20Distillation&body=Title%3A%20EG4D%3A%20Explicit%20Generation%20of%204D%20Object%20without%20Score%20Distillation%0AAuthor%3A%20Qi%20Sun%20and%20Zhiyang%20Guo%20and%20Ziyu%20Wan%20and%20Jing%20Nathan%20Yan%20and%20Shengming%20Yin%20and%20Wengang%20Zhou%20and%20Jing%20Liao%20and%20Houqiang%20Li%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20increasing%20demand%20for%20dynamic%203D%20assets%20in%20design%20and%0Agaming%20applications%20has%20given%20rise%20to%20powerful%20generative%20pipelines%20capable%20of%0Asynthesizing%20high-quality%204D%20objects.%20Previous%20methods%20generally%20rely%20on%20score%0Adistillation%20sampling%20%28SDS%29%20algorithm%20to%20infer%20the%20unseen%20views%20and%20motion%20of%0A4D%20objects%2C%20thus%20leading%20to%20unsatisfactory%20results%20with%20defects%20like%0Aover-saturation%20and%20Janus%20problem.%20Therefore%2C%20inspired%20by%20recent%20progress%20of%0Avideo%20diffusion%20models%2C%20we%20propose%20to%20optimize%20a%204D%20representation%20by%0Aexplicitly%20generating%20multi-view%20videos%20from%20one%20input%20image.%20However%2C%20it%20is%0Afar%20from%20trivial%20to%20handle%20practical%20challenges%20faced%20by%20such%20a%20pipeline%2C%0Aincluding%20dramatic%20temporal%20inconsistency%2C%20inter-frame%20geometry%20and%20texture%0Adiversity%2C%20and%20semantic%20defects%20brought%20by%20video%20generation%20results.%20To%20address%0Athese%20issues%2C%20we%20propose%20DG4D%2C%20a%20novel%20multi-stage%20framework%20that%20generates%0Ahigh-quality%20and%20consistent%204D%20assets%20without%20score%20distillation.%20Specifically%2C%0Acollaborative%20techniques%20and%20solutions%20are%20developed%2C%20including%20an%20attention%0Ainjection%20strategy%20to%20synthesize%20temporal-consistent%20multi-view%20videos%2C%20a%0Arobust%20and%20efficient%20dynamic%20reconstruction%20method%20based%20on%20Gaussian%20Splatting%2C%0Aand%20a%20refinement%20stage%20with%20diffusion%20prior%20for%20semantic%20restoration.%20The%0Aqualitative%20results%20and%20user%20preference%20study%20demonstrate%20that%20our%20framework%0Aoutperforms%20the%20baselines%20in%20generation%20quality%20by%20a%20considerable%20margin.%20Code%0Awill%20be%20released%20at%20%5Curl%7Bhttps%3A//github.com/jasongzy/EG4D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEG4D%253A%2520Explicit%2520Generation%2520of%25204D%2520Object%2520without%2520Score%2520Distillation%26entry.906535625%3DQi%2520Sun%2520and%2520Zhiyang%2520Guo%2520and%2520Ziyu%2520Wan%2520and%2520Jing%2520Nathan%2520Yan%2520and%2520Shengming%2520Yin%2520and%2520Wengang%2520Zhou%2520and%2520Jing%2520Liao%2520and%2520Houqiang%2520Li%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520increasing%2520demand%2520for%2520dynamic%25203D%2520assets%2520in%2520design%2520and%250Agaming%2520applications%2520has%2520given%2520rise%2520to%2520powerful%2520generative%2520pipelines%2520capable%2520of%250Asynthesizing%2520high-quality%25204D%2520objects.%2520Previous%2520methods%2520generally%2520rely%2520on%2520score%250Adistillation%2520sampling%2520%2528SDS%2529%2520algorithm%2520to%2520infer%2520the%2520unseen%2520views%2520and%2520motion%2520of%250A4D%2520objects%252C%2520thus%2520leading%2520to%2520unsatisfactory%2520results%2520with%2520defects%2520like%250Aover-saturation%2520and%2520Janus%2520problem.%2520Therefore%252C%2520inspired%2520by%2520recent%2520progress%2520of%250Avideo%2520diffusion%2520models%252C%2520we%2520propose%2520to%2520optimize%2520a%25204D%2520representation%2520by%250Aexplicitly%2520generating%2520multi-view%2520videos%2520from%2520one%2520input%2520image.%2520However%252C%2520it%2520is%250Afar%2520from%2520trivial%2520to%2520handle%2520practical%2520challenges%2520faced%2520by%2520such%2520a%2520pipeline%252C%250Aincluding%2520dramatic%2520temporal%2520inconsistency%252C%2520inter-frame%2520geometry%2520and%2520texture%250Adiversity%252C%2520and%2520semantic%2520defects%2520brought%2520by%2520video%2520generation%2520results.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520DG4D%252C%2520a%2520novel%2520multi-stage%2520framework%2520that%2520generates%250Ahigh-quality%2520and%2520consistent%25204D%2520assets%2520without%2520score%2520distillation.%2520Specifically%252C%250Acollaborative%2520techniques%2520and%2520solutions%2520are%2520developed%252C%2520including%2520an%2520attention%250Ainjection%2520strategy%2520to%2520synthesize%2520temporal-consistent%2520multi-view%2520videos%252C%2520a%250Arobust%2520and%2520efficient%2520dynamic%2520reconstruction%2520method%2520based%2520on%2520Gaussian%2520Splatting%252C%250Aand%2520a%2520refinement%2520stage%2520with%2520diffusion%2520prior%2520for%2520semantic%2520restoration.%2520The%250Aqualitative%2520results%2520and%2520user%2520preference%2520study%2520demonstrate%2520that%2520our%2520framework%250Aoutperforms%2520the%2520baselines%2520in%2520generation%2520quality%2520by%2520a%2520considerable%2520margin.%2520Code%250Awill%2520be%2520released%2520at%2520%255Curl%257Bhttps%253A//github.com/jasongzy/EG4D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EG4D%3A%20Explicit%20Generation%20of%204D%20Object%20without%20Score%20Distillation&entry.906535625=Qi%20Sun%20and%20Zhiyang%20Guo%20and%20Ziyu%20Wan%20and%20Jing%20Nathan%20Yan%20and%20Shengming%20Yin%20and%20Wengang%20Zhou%20and%20Jing%20Liao%20and%20Houqiang%20Li&entry.1292438233=%20%20In%20recent%20years%2C%20the%20increasing%20demand%20for%20dynamic%203D%20assets%20in%20design%20and%0Agaming%20applications%20has%20given%20rise%20to%20powerful%20generative%20pipelines%20capable%20of%0Asynthesizing%20high-quality%204D%20objects.%20Previous%20methods%20generally%20rely%20on%20score%0Adistillation%20sampling%20%28SDS%29%20algorithm%20to%20infer%20the%20unseen%20views%20and%20motion%20of%0A4D%20objects%2C%20thus%20leading%20to%20unsatisfactory%20results%20with%20defects%20like%0Aover-saturation%20and%20Janus%20problem.%20Therefore%2C%20inspired%20by%20recent%20progress%20of%0Avideo%20diffusion%20models%2C%20we%20propose%20to%20optimize%20a%204D%20representation%20by%0Aexplicitly%20generating%20multi-view%20videos%20from%20one%20input%20image.%20However%2C%20it%20is%0Afar%20from%20trivial%20to%20handle%20practical%20challenges%20faced%20by%20such%20a%20pipeline%2C%0Aincluding%20dramatic%20temporal%20inconsistency%2C%20inter-frame%20geometry%20and%20texture%0Adiversity%2C%20and%20semantic%20defects%20brought%20by%20video%20generation%20results.%20To%20address%0Athese%20issues%2C%20we%20propose%20DG4D%2C%20a%20novel%20multi-stage%20framework%20that%20generates%0Ahigh-quality%20and%20consistent%204D%20assets%20without%20score%20distillation.%20Specifically%2C%0Acollaborative%20techniques%20and%20solutions%20are%20developed%2C%20including%20an%20attention%0Ainjection%20strategy%20to%20synthesize%20temporal-consistent%20multi-view%20videos%2C%20a%0Arobust%20and%20efficient%20dynamic%20reconstruction%20method%20based%20on%20Gaussian%20Splatting%2C%0Aand%20a%20refinement%20stage%20with%20diffusion%20prior%20for%20semantic%20restoration.%20The%0Aqualitative%20results%20and%20user%20preference%20study%20demonstrate%20that%20our%20framework%0Aoutperforms%20the%20baselines%20in%20generation%20quality%20by%20a%20considerable%20margin.%20Code%0Awill%20be%20released%20at%20%5Curl%7Bhttps%3A//github.com/jasongzy/EG4D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18132v1&entry.124074799=Read"},
{"title": "F-3DGS: Factorized Coordinates and Representations for 3D Gaussian\n  Splatting", "author": "Xiangyu Sun and Joo Chan Lee and Daniel Rho and Jong Hwan Ko and Usman Ali and Eunbyung Park", "abstract": "  The neural radiance field (NeRF) has made significant strides in representing\n3D scenes and synthesizing novel views. Despite its advancements, the high\ncomputational costs of NeRF have posed challenges for its deployment in\nresource-constrained environments and real-time applications. As an alternative\nto NeRF-like neural rendering methods, 3D Gaussian Splatting (3DGS) offers\nrapid rendering speeds while maintaining excellent image quality. However, as\nit represents objects and scenes using a myriad of Gaussians, it requires\nsubstantial storage to achieve high-quality representation. To mitigate the\nstorage overhead, we propose Factorized 3D Gaussian Splatting (F-3DGS), a novel\napproach that drastically reduces storage requirements while preserving image\nquality. Inspired by classical matrix and tensor factorization techniques, our\nmethod represents and approximates dense clusters of Gaussians with\nsignificantly fewer Gaussians through efficient factorization. We aim to\nefficiently represent dense 3D Gaussians by approximating them with a limited\namount of information for each axis and their combinations. This method allows\nus to encode a substantially large number of Gaussians along with their\nessential attributes -- such as color, scale, and rotation -- necessary for\nrendering using a relatively small number of elements. Extensive experimental\nresults demonstrate that F-3DGS achieves a significant reduction in storage\ncosts while maintaining comparable quality in rendered images.\n", "link": "http://arxiv.org/abs/2405.17083v2", "date": "2024-05-28", "relevancy": 3.1854, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7073}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6228}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F-3DGS%3A%20Factorized%20Coordinates%20and%20Representations%20for%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20F-3DGS%3A%20Factorized%20Coordinates%20and%20Representations%20for%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Xiangyu%20Sun%20and%20Joo%20Chan%20Lee%20and%20Daniel%20Rho%20and%20Jong%20Hwan%20Ko%20and%20Usman%20Ali%20and%20Eunbyung%20Park%0AAbstract%3A%20%20%20The%20neural%20radiance%20field%20%28NeRF%29%20has%20made%20significant%20strides%20in%20representing%0A3D%20scenes%20and%20synthesizing%20novel%20views.%20Despite%20its%20advancements%2C%20the%20high%0Acomputational%20costs%20of%20NeRF%20have%20posed%20challenges%20for%20its%20deployment%20in%0Aresource-constrained%20environments%20and%20real-time%20applications.%20As%20an%20alternative%0Ato%20NeRF-like%20neural%20rendering%20methods%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20offers%0Arapid%20rendering%20speeds%20while%20maintaining%20excellent%20image%20quality.%20However%2C%20as%0Ait%20represents%20objects%20and%20scenes%20using%20a%20myriad%20of%20Gaussians%2C%20it%20requires%0Asubstantial%20storage%20to%20achieve%20high-quality%20representation.%20To%20mitigate%20the%0Astorage%20overhead%2C%20we%20propose%20Factorized%203D%20Gaussian%20Splatting%20%28F-3DGS%29%2C%20a%20novel%0Aapproach%20that%20drastically%20reduces%20storage%20requirements%20while%20preserving%20image%0Aquality.%20Inspired%20by%20classical%20matrix%20and%20tensor%20factorization%20techniques%2C%20our%0Amethod%20represents%20and%20approximates%20dense%20clusters%20of%20Gaussians%20with%0Asignificantly%20fewer%20Gaussians%20through%20efficient%20factorization.%20We%20aim%20to%0Aefficiently%20represent%20dense%203D%20Gaussians%20by%20approximating%20them%20with%20a%20limited%0Aamount%20of%20information%20for%20each%20axis%20and%20their%20combinations.%20This%20method%20allows%0Aus%20to%20encode%20a%20substantially%20large%20number%20of%20Gaussians%20along%20with%20their%0Aessential%20attributes%20--%20such%20as%20color%2C%20scale%2C%20and%20rotation%20--%20necessary%20for%0Arendering%20using%20a%20relatively%20small%20number%20of%20elements.%20Extensive%20experimental%0Aresults%20demonstrate%20that%20F-3DGS%20achieves%20a%20significant%20reduction%20in%20storage%0Acosts%20while%20maintaining%20comparable%20quality%20in%20rendered%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF-3DGS%253A%2520Factorized%2520Coordinates%2520and%2520Representations%2520for%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DXiangyu%2520Sun%2520and%2520Joo%2520Chan%2520Lee%2520and%2520Daniel%2520Rho%2520and%2520Jong%2520Hwan%2520Ko%2520and%2520Usman%2520Ali%2520and%2520Eunbyung%2520Park%26entry.1292438233%3D%2520%2520The%2520neural%2520radiance%2520field%2520%2528NeRF%2529%2520has%2520made%2520significant%2520strides%2520in%2520representing%250A3D%2520scenes%2520and%2520synthesizing%2520novel%2520views.%2520Despite%2520its%2520advancements%252C%2520the%2520high%250Acomputational%2520costs%2520of%2520NeRF%2520have%2520posed%2520challenges%2520for%2520its%2520deployment%2520in%250Aresource-constrained%2520environments%2520and%2520real-time%2520applications.%2520As%2520an%2520alternative%250Ato%2520NeRF-like%2520neural%2520rendering%2520methods%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520offers%250Arapid%2520rendering%2520speeds%2520while%2520maintaining%2520excellent%2520image%2520quality.%2520However%252C%2520as%250Ait%2520represents%2520objects%2520and%2520scenes%2520using%2520a%2520myriad%2520of%2520Gaussians%252C%2520it%2520requires%250Asubstantial%2520storage%2520to%2520achieve%2520high-quality%2520representation.%2520To%2520mitigate%2520the%250Astorage%2520overhead%252C%2520we%2520propose%2520Factorized%25203D%2520Gaussian%2520Splatting%2520%2528F-3DGS%2529%252C%2520a%2520novel%250Aapproach%2520that%2520drastically%2520reduces%2520storage%2520requirements%2520while%2520preserving%2520image%250Aquality.%2520Inspired%2520by%2520classical%2520matrix%2520and%2520tensor%2520factorization%2520techniques%252C%2520our%250Amethod%2520represents%2520and%2520approximates%2520dense%2520clusters%2520of%2520Gaussians%2520with%250Asignificantly%2520fewer%2520Gaussians%2520through%2520efficient%2520factorization.%2520We%2520aim%2520to%250Aefficiently%2520represent%2520dense%25203D%2520Gaussians%2520by%2520approximating%2520them%2520with%2520a%2520limited%250Aamount%2520of%2520information%2520for%2520each%2520axis%2520and%2520their%2520combinations.%2520This%2520method%2520allows%250Aus%2520to%2520encode%2520a%2520substantially%2520large%2520number%2520of%2520Gaussians%2520along%2520with%2520their%250Aessential%2520attributes%2520--%2520such%2520as%2520color%252C%2520scale%252C%2520and%2520rotation%2520--%2520necessary%2520for%250Arendering%2520using%2520a%2520relatively%2520small%2520number%2520of%2520elements.%2520Extensive%2520experimental%250Aresults%2520demonstrate%2520that%2520F-3DGS%2520achieves%2520a%2520significant%2520reduction%2520in%2520storage%250Acosts%2520while%2520maintaining%2520comparable%2520quality%2520in%2520rendered%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F-3DGS%3A%20Factorized%20Coordinates%20and%20Representations%20for%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Xiangyu%20Sun%20and%20Joo%20Chan%20Lee%20and%20Daniel%20Rho%20and%20Jong%20Hwan%20Ko%20and%20Usman%20Ali%20and%20Eunbyung%20Park&entry.1292438233=%20%20The%20neural%20radiance%20field%20%28NeRF%29%20has%20made%20significant%20strides%20in%20representing%0A3D%20scenes%20and%20synthesizing%20novel%20views.%20Despite%20its%20advancements%2C%20the%20high%0Acomputational%20costs%20of%20NeRF%20have%20posed%20challenges%20for%20its%20deployment%20in%0Aresource-constrained%20environments%20and%20real-time%20applications.%20As%20an%20alternative%0Ato%20NeRF-like%20neural%20rendering%20methods%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20offers%0Arapid%20rendering%20speeds%20while%20maintaining%20excellent%20image%20quality.%20However%2C%20as%0Ait%20represents%20objects%20and%20scenes%20using%20a%20myriad%20of%20Gaussians%2C%20it%20requires%0Asubstantial%20storage%20to%20achieve%20high-quality%20representation.%20To%20mitigate%20the%0Astorage%20overhead%2C%20we%20propose%20Factorized%203D%20Gaussian%20Splatting%20%28F-3DGS%29%2C%20a%20novel%0Aapproach%20that%20drastically%20reduces%20storage%20requirements%20while%20preserving%20image%0Aquality.%20Inspired%20by%20classical%20matrix%20and%20tensor%20factorization%20techniques%2C%20our%0Amethod%20represents%20and%20approximates%20dense%20clusters%20of%20Gaussians%20with%0Asignificantly%20fewer%20Gaussians%20through%20efficient%20factorization.%20We%20aim%20to%0Aefficiently%20represent%20dense%203D%20Gaussians%20by%20approximating%20them%20with%20a%20limited%0Aamount%20of%20information%20for%20each%20axis%20and%20their%20combinations.%20This%20method%20allows%0Aus%20to%20encode%20a%20substantially%20large%20number%20of%20Gaussians%20along%20with%20their%0Aessential%20attributes%20--%20such%20as%20color%2C%20scale%2C%20and%20rotation%20--%20necessary%20for%0Arendering%20using%20a%20relatively%20small%20number%20of%20elements.%20Extensive%20experimental%0Aresults%20demonstrate%20that%20F-3DGS%20achieves%20a%20significant%20reduction%20in%20storage%0Acosts%20while%20maintaining%20comparable%20quality%20in%20rendered%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17083v2&entry.124074799=Read"},
{"title": "GFlow: Recovering 4D World from Monocular Video", "author": "Shizun Wang and Xingyi Yang and Qiuhong Shen and Zhenxiang Jiang and Xinchao Wang", "abstract": "  Reconstructing 4D scenes from video inputs is a crucial yet challenging task.\nConventional methods usually rely on the assumptions of multi-view video\ninputs, known camera parameters, or static scenes, all of which are typically\nabsent under in-the-wild scenarios. In this paper, we relax all these\nconstraints and tackle a highly ambitious but practical task, which we termed\nas AnyV4D: we assume only one monocular video is available without any camera\nparameters as input, and we aim to recover the dynamic 4D world alongside the\ncamera poses. To this end, we introduce GFlow, a new framework that utilizes\nonly 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit\nrepresentation, entailing a flow of Gaussian splatting through space and time.\nGFlow first clusters the scene into still and moving parts, then applies a\nsequential optimization process that optimizes camera poses and the dynamics of\n3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity\namong neighboring points and smooth movement across frames. Since dynamic\nscenes always introduce new content, we also propose a new pixel-wise\ndensification strategy for Gaussian points to integrate new visual content.\nMoreover, GFlow transcends the boundaries of mere 4D reconstruction; it also\nenables tracking of any points across frames without the need for prior\ntraining and segments moving objects from the scene in an unsupervised way.\nAdditionally, the camera poses of each frame can be derived from GFlow,\nallowing for rendering novel views of a video scene through changing camera\npose. By employing the explicit representation, we may readily conduct\nscene-level or object-level editing as desired, underscoring its versatility\nand power. Visit our project website at: https://littlepure2333.github.io/GFlow\n", "link": "http://arxiv.org/abs/2405.18426v1", "date": "2024-05-28", "relevancy": 3.1001, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6538}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6128}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GFlow%3A%20Recovering%204D%20World%20from%20Monocular%20Video&body=Title%3A%20GFlow%3A%20Recovering%204D%20World%20from%20Monocular%20Video%0AAuthor%3A%20Shizun%20Wang%20and%20Xingyi%20Yang%20and%20Qiuhong%20Shen%20and%20Zhenxiang%20Jiang%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Reconstructing%204D%20scenes%20from%20video%20inputs%20is%20a%20crucial%20yet%20challenging%20task.%0AConventional%20methods%20usually%20rely%20on%20the%20assumptions%20of%20multi-view%20video%0Ainputs%2C%20known%20camera%20parameters%2C%20or%20static%20scenes%2C%20all%20of%20which%20are%20typically%0Aabsent%20under%20in-the-wild%20scenarios.%20In%20this%20paper%2C%20we%20relax%20all%20these%0Aconstraints%20and%20tackle%20a%20highly%20ambitious%20but%20practical%20task%2C%20which%20we%20termed%0Aas%20AnyV4D%3A%20we%20assume%20only%20one%20monocular%20video%20is%20available%20without%20any%20camera%0Aparameters%20as%20input%2C%20and%20we%20aim%20to%20recover%20the%20dynamic%204D%20world%20alongside%20the%0Acamera%20poses.%20To%20this%20end%2C%20we%20introduce%20GFlow%2C%20a%20new%20framework%20that%20utilizes%0Aonly%202D%20priors%20%28depth%20and%20optical%20flow%29%20to%20lift%20a%20video%20%283D%29%20to%20a%204D%20explicit%0Arepresentation%2C%20entailing%20a%20flow%20of%20Gaussian%20splatting%20through%20space%20and%20time.%0AGFlow%20first%20clusters%20the%20scene%20into%20still%20and%20moving%20parts%2C%20then%20applies%20a%0Asequential%20optimization%20process%20that%20optimizes%20camera%20poses%20and%20the%20dynamics%20of%0A3D%20Gaussian%20points%20based%20on%202D%20priors%20and%20scene%20clustering%2C%20ensuring%20fidelity%0Aamong%20neighboring%20points%20and%20smooth%20movement%20across%20frames.%20Since%20dynamic%0Ascenes%20always%20introduce%20new%20content%2C%20we%20also%20propose%20a%20new%20pixel-wise%0Adensification%20strategy%20for%20Gaussian%20points%20to%20integrate%20new%20visual%20content.%0AMoreover%2C%20GFlow%20transcends%20the%20boundaries%20of%20mere%204D%20reconstruction%3B%20it%20also%0Aenables%20tracking%20of%20any%20points%20across%20frames%20without%20the%20need%20for%20prior%0Atraining%20and%20segments%20moving%20objects%20from%20the%20scene%20in%20an%20unsupervised%20way.%0AAdditionally%2C%20the%20camera%20poses%20of%20each%20frame%20can%20be%20derived%20from%20GFlow%2C%0Aallowing%20for%20rendering%20novel%20views%20of%20a%20video%20scene%20through%20changing%20camera%0Apose.%20By%20employing%20the%20explicit%20representation%2C%20we%20may%20readily%20conduct%0Ascene-level%20or%20object-level%20editing%20as%20desired%2C%20underscoring%20its%20versatility%0Aand%20power.%20Visit%20our%20project%20website%20at%3A%20https%3A//littlepure2333.github.io/GFlow%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGFlow%253A%2520Recovering%25204D%2520World%2520from%2520Monocular%2520Video%26entry.906535625%3DShizun%2520Wang%2520and%2520Xingyi%2520Yang%2520and%2520Qiuhong%2520Shen%2520and%2520Zhenxiang%2520Jiang%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Reconstructing%25204D%2520scenes%2520from%2520video%2520inputs%2520is%2520a%2520crucial%2520yet%2520challenging%2520task.%250AConventional%2520methods%2520usually%2520rely%2520on%2520the%2520assumptions%2520of%2520multi-view%2520video%250Ainputs%252C%2520known%2520camera%2520parameters%252C%2520or%2520static%2520scenes%252C%2520all%2520of%2520which%2520are%2520typically%250Aabsent%2520under%2520in-the-wild%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520relax%2520all%2520these%250Aconstraints%2520and%2520tackle%2520a%2520highly%2520ambitious%2520but%2520practical%2520task%252C%2520which%2520we%2520termed%250Aas%2520AnyV4D%253A%2520we%2520assume%2520only%2520one%2520monocular%2520video%2520is%2520available%2520without%2520any%2520camera%250Aparameters%2520as%2520input%252C%2520and%2520we%2520aim%2520to%2520recover%2520the%2520dynamic%25204D%2520world%2520alongside%2520the%250Acamera%2520poses.%2520To%2520this%2520end%252C%2520we%2520introduce%2520GFlow%252C%2520a%2520new%2520framework%2520that%2520utilizes%250Aonly%25202D%2520priors%2520%2528depth%2520and%2520optical%2520flow%2529%2520to%2520lift%2520a%2520video%2520%25283D%2529%2520to%2520a%25204D%2520explicit%250Arepresentation%252C%2520entailing%2520a%2520flow%2520of%2520Gaussian%2520splatting%2520through%2520space%2520and%2520time.%250AGFlow%2520first%2520clusters%2520the%2520scene%2520into%2520still%2520and%2520moving%2520parts%252C%2520then%2520applies%2520a%250Asequential%2520optimization%2520process%2520that%2520optimizes%2520camera%2520poses%2520and%2520the%2520dynamics%2520of%250A3D%2520Gaussian%2520points%2520based%2520on%25202D%2520priors%2520and%2520scene%2520clustering%252C%2520ensuring%2520fidelity%250Aamong%2520neighboring%2520points%2520and%2520smooth%2520movement%2520across%2520frames.%2520Since%2520dynamic%250Ascenes%2520always%2520introduce%2520new%2520content%252C%2520we%2520also%2520propose%2520a%2520new%2520pixel-wise%250Adensification%2520strategy%2520for%2520Gaussian%2520points%2520to%2520integrate%2520new%2520visual%2520content.%250AMoreover%252C%2520GFlow%2520transcends%2520the%2520boundaries%2520of%2520mere%25204D%2520reconstruction%253B%2520it%2520also%250Aenables%2520tracking%2520of%2520any%2520points%2520across%2520frames%2520without%2520the%2520need%2520for%2520prior%250Atraining%2520and%2520segments%2520moving%2520objects%2520from%2520the%2520scene%2520in%2520an%2520unsupervised%2520way.%250AAdditionally%252C%2520the%2520camera%2520poses%2520of%2520each%2520frame%2520can%2520be%2520derived%2520from%2520GFlow%252C%250Aallowing%2520for%2520rendering%2520novel%2520views%2520of%2520a%2520video%2520scene%2520through%2520changing%2520camera%250Apose.%2520By%2520employing%2520the%2520explicit%2520representation%252C%2520we%2520may%2520readily%2520conduct%250Ascene-level%2520or%2520object-level%2520editing%2520as%2520desired%252C%2520underscoring%2520its%2520versatility%250Aand%2520power.%2520Visit%2520our%2520project%2520website%2520at%253A%2520https%253A//littlepure2333.github.io/GFlow%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GFlow%3A%20Recovering%204D%20World%20from%20Monocular%20Video&entry.906535625=Shizun%20Wang%20and%20Xingyi%20Yang%20and%20Qiuhong%20Shen%20and%20Zhenxiang%20Jiang%20and%20Xinchao%20Wang&entry.1292438233=%20%20Reconstructing%204D%20scenes%20from%20video%20inputs%20is%20a%20crucial%20yet%20challenging%20task.%0AConventional%20methods%20usually%20rely%20on%20the%20assumptions%20of%20multi-view%20video%0Ainputs%2C%20known%20camera%20parameters%2C%20or%20static%20scenes%2C%20all%20of%20which%20are%20typically%0Aabsent%20under%20in-the-wild%20scenarios.%20In%20this%20paper%2C%20we%20relax%20all%20these%0Aconstraints%20and%20tackle%20a%20highly%20ambitious%20but%20practical%20task%2C%20which%20we%20termed%0Aas%20AnyV4D%3A%20we%20assume%20only%20one%20monocular%20video%20is%20available%20without%20any%20camera%0Aparameters%20as%20input%2C%20and%20we%20aim%20to%20recover%20the%20dynamic%204D%20world%20alongside%20the%0Acamera%20poses.%20To%20this%20end%2C%20we%20introduce%20GFlow%2C%20a%20new%20framework%20that%20utilizes%0Aonly%202D%20priors%20%28depth%20and%20optical%20flow%29%20to%20lift%20a%20video%20%283D%29%20to%20a%204D%20explicit%0Arepresentation%2C%20entailing%20a%20flow%20of%20Gaussian%20splatting%20through%20space%20and%20time.%0AGFlow%20first%20clusters%20the%20scene%20into%20still%20and%20moving%20parts%2C%20then%20applies%20a%0Asequential%20optimization%20process%20that%20optimizes%20camera%20poses%20and%20the%20dynamics%20of%0A3D%20Gaussian%20points%20based%20on%202D%20priors%20and%20scene%20clustering%2C%20ensuring%20fidelity%0Aamong%20neighboring%20points%20and%20smooth%20movement%20across%20frames.%20Since%20dynamic%0Ascenes%20always%20introduce%20new%20content%2C%20we%20also%20propose%20a%20new%20pixel-wise%0Adensification%20strategy%20for%20Gaussian%20points%20to%20integrate%20new%20visual%20content.%0AMoreover%2C%20GFlow%20transcends%20the%20boundaries%20of%20mere%204D%20reconstruction%3B%20it%20also%0Aenables%20tracking%20of%20any%20points%20across%20frames%20without%20the%20need%20for%20prior%0Atraining%20and%20segments%20moving%20objects%20from%20the%20scene%20in%20an%20unsupervised%20way.%0AAdditionally%2C%20the%20camera%20poses%20of%20each%20frame%20can%20be%20derived%20from%20GFlow%2C%0Aallowing%20for%20rendering%20novel%20views%20of%20a%20video%20scene%20through%20changing%20camera%0Apose.%20By%20employing%20the%20explicit%20representation%2C%20we%20may%20readily%20conduct%0Ascene-level%20or%20object-level%20editing%20as%20desired%2C%20underscoring%20its%20versatility%0Aand%20power.%20Visit%20our%20project%20website%20at%3A%20https%3A//littlepure2333.github.io/GFlow%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18426v1&entry.124074799=Read"},
{"title": "Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting", "author": "Jaewoo Jung and Jisang Han and Honggyu An and Jiwon Kang and Seonghoon Park and Seungryong Kim", "abstract": "  3D Gaussian splatting (3DGS) has recently demonstrated impressive\ncapabilities in real-time novel view synthesis and 3D reconstruction. However,\n3DGS heavily depends on the accurate initialization derived from\nStructure-from-Motion (SfM) methods. When the quality of the initial point\ncloud deteriorates, such as in the presence of noise or when using randomly\ninitialized point cloud, 3DGS often undergoes large performance drops. To\naddress this limitation, we propose a novel optimization strategy dubbed\nRAIN-GS (Relaing Accurate Initialization Constraint for 3D Gaussian Splatting).\nOur approach is based on an in-depth analysis of the original 3DGS optimization\nscheme and the analysis of the SfM initialization in the frequency domain.\nLeveraging simple modifications based on our analyses, RAIN-GS successfully\ntrains 3D Gaussians from sub-optimal point cloud (e.g., randomly initialized\npoint cloud), effectively relaxing the need for accurate initialization. We\ndemonstrate the efficacy of our strategy through quantitative and qualitative\ncomparisons on multiple datasets, where RAIN-GS trained with random point cloud\nachieves performance on-par with or even better than 3DGS trained with accurate\nSfM point cloud. Our project page and code can be found at\nhttps://ku-cvlab.github.io/RAIN-GS.\n", "link": "http://arxiv.org/abs/2403.09413v2", "date": "2024-05-28", "relevancy": 3.0868, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6931}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.634}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%20Gaussian%20Splatting&body=Title%3A%20Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Jaewoo%20Jung%20and%20Jisang%20Han%20and%20Honggyu%20An%20and%20Jiwon%20Kang%20and%20Seonghoon%20Park%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20recently%20demonstrated%20impressive%0Acapabilities%20in%20real-time%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%0A3DGS%20heavily%20depends%20on%20the%20accurate%20initialization%20derived%20from%0AStructure-from-Motion%20%28SfM%29%20methods.%20When%20the%20quality%20of%20the%20initial%20point%0Acloud%20deteriorates%2C%20such%20as%20in%20the%20presence%20of%20noise%20or%20when%20using%20randomly%0Ainitialized%20point%20cloud%2C%203DGS%20often%20undergoes%20large%20performance%20drops.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20novel%20optimization%20strategy%20dubbed%0ARAIN-GS%20%28Relaing%20Accurate%20Initialization%20Constraint%20for%203D%20Gaussian%20Splatting%29.%0AOur%20approach%20is%20based%20on%20an%20in-depth%20analysis%20of%20the%20original%203DGS%20optimization%0Ascheme%20and%20the%20analysis%20of%20the%20SfM%20initialization%20in%20the%20frequency%20domain.%0ALeveraging%20simple%20modifications%20based%20on%20our%20analyses%2C%20RAIN-GS%20successfully%0Atrains%203D%20Gaussians%20from%20sub-optimal%20point%20cloud%20%28e.g.%2C%20randomly%20initialized%0Apoint%20cloud%29%2C%20effectively%20relaxing%20the%20need%20for%20accurate%20initialization.%20We%0Ademonstrate%20the%20efficacy%20of%20our%20strategy%20through%20quantitative%20and%20qualitative%0Acomparisons%20on%20multiple%20datasets%2C%20where%20RAIN-GS%20trained%20with%20random%20point%20cloud%0Aachieves%20performance%20on-par%20with%20or%20even%20better%20than%203DGS%20trained%20with%20accurate%0ASfM%20point%20cloud.%20Our%20project%20page%20and%20code%20can%20be%20found%20at%0Ahttps%3A//ku-cvlab.github.io/RAIN-GS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09413v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelaxing%2520Accurate%2520Initialization%2520Constraint%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DJaewoo%2520Jung%2520and%2520Jisang%2520Han%2520and%2520Honggyu%2520An%2520and%2520Jiwon%2520Kang%2520and%2520Seonghoon%2520Park%2520and%2520Seungryong%2520Kim%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520has%2520recently%2520demonstrated%2520impressive%250Acapabilities%2520in%2520real-time%2520novel%2520view%2520synthesis%2520and%25203D%2520reconstruction.%2520However%252C%250A3DGS%2520heavily%2520depends%2520on%2520the%2520accurate%2520initialization%2520derived%2520from%250AStructure-from-Motion%2520%2528SfM%2529%2520methods.%2520When%2520the%2520quality%2520of%2520the%2520initial%2520point%250Acloud%2520deteriorates%252C%2520such%2520as%2520in%2520the%2520presence%2520of%2520noise%2520or%2520when%2520using%2520randomly%250Ainitialized%2520point%2520cloud%252C%25203DGS%2520often%2520undergoes%2520large%2520performance%2520drops.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520optimization%2520strategy%2520dubbed%250ARAIN-GS%2520%2528Relaing%2520Accurate%2520Initialization%2520Constraint%2520for%25203D%2520Gaussian%2520Splatting%2529.%250AOur%2520approach%2520is%2520based%2520on%2520an%2520in-depth%2520analysis%2520of%2520the%2520original%25203DGS%2520optimization%250Ascheme%2520and%2520the%2520analysis%2520of%2520the%2520SfM%2520initialization%2520in%2520the%2520frequency%2520domain.%250ALeveraging%2520simple%2520modifications%2520based%2520on%2520our%2520analyses%252C%2520RAIN-GS%2520successfully%250Atrains%25203D%2520Gaussians%2520from%2520sub-optimal%2520point%2520cloud%2520%2528e.g.%252C%2520randomly%2520initialized%250Apoint%2520cloud%2529%252C%2520effectively%2520relaxing%2520the%2520need%2520for%2520accurate%2520initialization.%2520We%250Ademonstrate%2520the%2520efficacy%2520of%2520our%2520strategy%2520through%2520quantitative%2520and%2520qualitative%250Acomparisons%2520on%2520multiple%2520datasets%252C%2520where%2520RAIN-GS%2520trained%2520with%2520random%2520point%2520cloud%250Aachieves%2520performance%2520on-par%2520with%2520or%2520even%2520better%2520than%25203DGS%2520trained%2520with%2520accurate%250ASfM%2520point%2520cloud.%2520Our%2520project%2520page%2520and%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//ku-cvlab.github.io/RAIN-GS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09413v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%20Gaussian%20Splatting&entry.906535625=Jaewoo%20Jung%20and%20Jisang%20Han%20and%20Honggyu%20An%20and%20Jiwon%20Kang%20and%20Seonghoon%20Park%20and%20Seungryong%20Kim&entry.1292438233=%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20recently%20demonstrated%20impressive%0Acapabilities%20in%20real-time%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%0A3DGS%20heavily%20depends%20on%20the%20accurate%20initialization%20derived%20from%0AStructure-from-Motion%20%28SfM%29%20methods.%20When%20the%20quality%20of%20the%20initial%20point%0Acloud%20deteriorates%2C%20such%20as%20in%20the%20presence%20of%20noise%20or%20when%20using%20randomly%0Ainitialized%20point%20cloud%2C%203DGS%20often%20undergoes%20large%20performance%20drops.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20novel%20optimization%20strategy%20dubbed%0ARAIN-GS%20%28Relaing%20Accurate%20Initialization%20Constraint%20for%203D%20Gaussian%20Splatting%29.%0AOur%20approach%20is%20based%20on%20an%20in-depth%20analysis%20of%20the%20original%203DGS%20optimization%0Ascheme%20and%20the%20analysis%20of%20the%20SfM%20initialization%20in%20the%20frequency%20domain.%0ALeveraging%20simple%20modifications%20based%20on%20our%20analyses%2C%20RAIN-GS%20successfully%0Atrains%203D%20Gaussians%20from%20sub-optimal%20point%20cloud%20%28e.g.%2C%20randomly%20initialized%0Apoint%20cloud%29%2C%20effectively%20relaxing%20the%20need%20for%20accurate%20initialization.%20We%0Ademonstrate%20the%20efficacy%20of%20our%20strategy%20through%20quantitative%20and%20qualitative%0Acomparisons%20on%20multiple%20datasets%2C%20where%20RAIN-GS%20trained%20with%20random%20point%20cloud%0Aachieves%20performance%20on-par%20with%20or%20even%20better%20than%203DGS%20trained%20with%20accurate%0ASfM%20point%20cloud.%20Our%20project%20page%20and%20code%20can%20be%20found%20at%0Ahttps%3A//ku-cvlab.github.io/RAIN-GS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09413v2&entry.124074799=Read"},
{"title": "3D StreetUnveiler with Semantic-Aware 2DGS", "author": "Jingwei Xu and Yikai Wang and Yiqun Zhao and Yanwei Fu and Shenghua Gao", "abstract": "  Unveiling an empty street from crowded observations captured by in-car\ncameras is crucial for autonomous driving. However, removing all temporary\nstatic objects, such as stopped vehicles and standing pedestrians, presents a\nsignificant challenge. Unlike object-centric 3D inpainting, which relies on\nthorough observation in a small scene, street scenes involve long trajectories\nthat differ from previous 3D inpainting tasks. The camera-centric moving\nenvironment of captured videos further complicates the task due to the limited\ndegree and time duration of object observation. To address these obstacles, we\nintroduce StreetUnveiler to reconstruct an empty street. StreetUnveiler learns\na 3D representation of the empty street from crowded observations. Our\nrepresentation is based on the hard-label semantic 2D Gaussian Splatting (2DGS)\nfor its scalability and ability to identify Gaussians to be removed. We inpaint\nrendered image after removing unwanted Gaussians to provide pseudo-labels and\nsubsequently re-optimize the 2DGS. Given its temporal continuous movement, we\ndivide the empty street scene into observed, partial-observed, and unobserved\nregions, which we propose to locate through a rendered alpha map. This\ndecomposition helps us to minimize the regions that need to be inpainted. To\nenhance the temporal consistency of the inpainting, we introduce a novel\ntime-reversal framework to inpaint frames in reverse order and use later frames\nas references for earlier frames to fully utilize the long-trajectory\nobservations. Our experiments conducted on the street scene dataset\nsuccessfully reconstructed a 3D representation of the empty street. The mesh\nrepresentation of the empty street can be extracted for further applications.\nProject page and more visualizations can be found at:\nhttps://streetunveiler.github.io\n", "link": "http://arxiv.org/abs/2405.18416v1", "date": "2024-05-28", "relevancy": 3.048, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6326}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6095}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20StreetUnveiler%20with%20Semantic-Aware%202DGS&body=Title%3A%203D%20StreetUnveiler%20with%20Semantic-Aware%202DGS%0AAuthor%3A%20Jingwei%20Xu%20and%20Yikai%20Wang%20and%20Yiqun%20Zhao%20and%20Yanwei%20Fu%20and%20Shenghua%20Gao%0AAbstract%3A%20%20%20Unveiling%20an%20empty%20street%20from%20crowded%20observations%20captured%20by%20in-car%0Acameras%20is%20crucial%20for%20autonomous%20driving.%20However%2C%20removing%20all%20temporary%0Astatic%20objects%2C%20such%20as%20stopped%20vehicles%20and%20standing%20pedestrians%2C%20presents%20a%0Asignificant%20challenge.%20Unlike%20object-centric%203D%20inpainting%2C%20which%20relies%20on%0Athorough%20observation%20in%20a%20small%20scene%2C%20street%20scenes%20involve%20long%20trajectories%0Athat%20differ%20from%20previous%203D%20inpainting%20tasks.%20The%20camera-centric%20moving%0Aenvironment%20of%20captured%20videos%20further%20complicates%20the%20task%20due%20to%20the%20limited%0Adegree%20and%20time%20duration%20of%20object%20observation.%20To%20address%20these%20obstacles%2C%20we%0Aintroduce%20StreetUnveiler%20to%20reconstruct%20an%20empty%20street.%20StreetUnveiler%20learns%0Aa%203D%20representation%20of%20the%20empty%20street%20from%20crowded%20observations.%20Our%0Arepresentation%20is%20based%20on%20the%20hard-label%20semantic%202D%20Gaussian%20Splatting%20%282DGS%29%0Afor%20its%20scalability%20and%20ability%20to%20identify%20Gaussians%20to%20be%20removed.%20We%20inpaint%0Arendered%20image%20after%20removing%20unwanted%20Gaussians%20to%20provide%20pseudo-labels%20and%0Asubsequently%20re-optimize%20the%202DGS.%20Given%20its%20temporal%20continuous%20movement%2C%20we%0Adivide%20the%20empty%20street%20scene%20into%20observed%2C%20partial-observed%2C%20and%20unobserved%0Aregions%2C%20which%20we%20propose%20to%20locate%20through%20a%20rendered%20alpha%20map.%20This%0Adecomposition%20helps%20us%20to%20minimize%20the%20regions%20that%20need%20to%20be%20inpainted.%20To%0Aenhance%20the%20temporal%20consistency%20of%20the%20inpainting%2C%20we%20introduce%20a%20novel%0Atime-reversal%20framework%20to%20inpaint%20frames%20in%20reverse%20order%20and%20use%20later%20frames%0Aas%20references%20for%20earlier%20frames%20to%20fully%20utilize%20the%20long-trajectory%0Aobservations.%20Our%20experiments%20conducted%20on%20the%20street%20scene%20dataset%0Asuccessfully%20reconstructed%20a%203D%20representation%20of%20the%20empty%20street.%20The%20mesh%0Arepresentation%20of%20the%20empty%20street%20can%20be%20extracted%20for%20further%20applications.%0AProject%20page%20and%20more%20visualizations%20can%20be%20found%20at%3A%0Ahttps%3A//streetunveiler.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520StreetUnveiler%2520with%2520Semantic-Aware%25202DGS%26entry.906535625%3DJingwei%2520Xu%2520and%2520Yikai%2520Wang%2520and%2520Yiqun%2520Zhao%2520and%2520Yanwei%2520Fu%2520and%2520Shenghua%2520Gao%26entry.1292438233%3D%2520%2520Unveiling%2520an%2520empty%2520street%2520from%2520crowded%2520observations%2520captured%2520by%2520in-car%250Acameras%2520is%2520crucial%2520for%2520autonomous%2520driving.%2520However%252C%2520removing%2520all%2520temporary%250Astatic%2520objects%252C%2520such%2520as%2520stopped%2520vehicles%2520and%2520standing%2520pedestrians%252C%2520presents%2520a%250Asignificant%2520challenge.%2520Unlike%2520object-centric%25203D%2520inpainting%252C%2520which%2520relies%2520on%250Athorough%2520observation%2520in%2520a%2520small%2520scene%252C%2520street%2520scenes%2520involve%2520long%2520trajectories%250Athat%2520differ%2520from%2520previous%25203D%2520inpainting%2520tasks.%2520The%2520camera-centric%2520moving%250Aenvironment%2520of%2520captured%2520videos%2520further%2520complicates%2520the%2520task%2520due%2520to%2520the%2520limited%250Adegree%2520and%2520time%2520duration%2520of%2520object%2520observation.%2520To%2520address%2520these%2520obstacles%252C%2520we%250Aintroduce%2520StreetUnveiler%2520to%2520reconstruct%2520an%2520empty%2520street.%2520StreetUnveiler%2520learns%250Aa%25203D%2520representation%2520of%2520the%2520empty%2520street%2520from%2520crowded%2520observations.%2520Our%250Arepresentation%2520is%2520based%2520on%2520the%2520hard-label%2520semantic%25202D%2520Gaussian%2520Splatting%2520%25282DGS%2529%250Afor%2520its%2520scalability%2520and%2520ability%2520to%2520identify%2520Gaussians%2520to%2520be%2520removed.%2520We%2520inpaint%250Arendered%2520image%2520after%2520removing%2520unwanted%2520Gaussians%2520to%2520provide%2520pseudo-labels%2520and%250Asubsequently%2520re-optimize%2520the%25202DGS.%2520Given%2520its%2520temporal%2520continuous%2520movement%252C%2520we%250Adivide%2520the%2520empty%2520street%2520scene%2520into%2520observed%252C%2520partial-observed%252C%2520and%2520unobserved%250Aregions%252C%2520which%2520we%2520propose%2520to%2520locate%2520through%2520a%2520rendered%2520alpha%2520map.%2520This%250Adecomposition%2520helps%2520us%2520to%2520minimize%2520the%2520regions%2520that%2520need%2520to%2520be%2520inpainted.%2520To%250Aenhance%2520the%2520temporal%2520consistency%2520of%2520the%2520inpainting%252C%2520we%2520introduce%2520a%2520novel%250Atime-reversal%2520framework%2520to%2520inpaint%2520frames%2520in%2520reverse%2520order%2520and%2520use%2520later%2520frames%250Aas%2520references%2520for%2520earlier%2520frames%2520to%2520fully%2520utilize%2520the%2520long-trajectory%250Aobservations.%2520Our%2520experiments%2520conducted%2520on%2520the%2520street%2520scene%2520dataset%250Asuccessfully%2520reconstructed%2520a%25203D%2520representation%2520of%2520the%2520empty%2520street.%2520The%2520mesh%250Arepresentation%2520of%2520the%2520empty%2520street%2520can%2520be%2520extracted%2520for%2520further%2520applications.%250AProject%2520page%2520and%2520more%2520visualizations%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//streetunveiler.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20StreetUnveiler%20with%20Semantic-Aware%202DGS&entry.906535625=Jingwei%20Xu%20and%20Yikai%20Wang%20and%20Yiqun%20Zhao%20and%20Yanwei%20Fu%20and%20Shenghua%20Gao&entry.1292438233=%20%20Unveiling%20an%20empty%20street%20from%20crowded%20observations%20captured%20by%20in-car%0Acameras%20is%20crucial%20for%20autonomous%20driving.%20However%2C%20removing%20all%20temporary%0Astatic%20objects%2C%20such%20as%20stopped%20vehicles%20and%20standing%20pedestrians%2C%20presents%20a%0Asignificant%20challenge.%20Unlike%20object-centric%203D%20inpainting%2C%20which%20relies%20on%0Athorough%20observation%20in%20a%20small%20scene%2C%20street%20scenes%20involve%20long%20trajectories%0Athat%20differ%20from%20previous%203D%20inpainting%20tasks.%20The%20camera-centric%20moving%0Aenvironment%20of%20captured%20videos%20further%20complicates%20the%20task%20due%20to%20the%20limited%0Adegree%20and%20time%20duration%20of%20object%20observation.%20To%20address%20these%20obstacles%2C%20we%0Aintroduce%20StreetUnveiler%20to%20reconstruct%20an%20empty%20street.%20StreetUnveiler%20learns%0Aa%203D%20representation%20of%20the%20empty%20street%20from%20crowded%20observations.%20Our%0Arepresentation%20is%20based%20on%20the%20hard-label%20semantic%202D%20Gaussian%20Splatting%20%282DGS%29%0Afor%20its%20scalability%20and%20ability%20to%20identify%20Gaussians%20to%20be%20removed.%20We%20inpaint%0Arendered%20image%20after%20removing%20unwanted%20Gaussians%20to%20provide%20pseudo-labels%20and%0Asubsequently%20re-optimize%20the%202DGS.%20Given%20its%20temporal%20continuous%20movement%2C%20we%0Adivide%20the%20empty%20street%20scene%20into%20observed%2C%20partial-observed%2C%20and%20unobserved%0Aregions%2C%20which%20we%20propose%20to%20locate%20through%20a%20rendered%20alpha%20map.%20This%0Adecomposition%20helps%20us%20to%20minimize%20the%20regions%20that%20need%20to%20be%20inpainted.%20To%0Aenhance%20the%20temporal%20consistency%20of%20the%20inpainting%2C%20we%20introduce%20a%20novel%0Atime-reversal%20framework%20to%20inpaint%20frames%20in%20reverse%20order%20and%20use%20later%20frames%0Aas%20references%20for%20earlier%20frames%20to%20fully%20utilize%20the%20long-trajectory%0Aobservations.%20Our%20experiments%20conducted%20on%20the%20street%20scene%20dataset%0Asuccessfully%20reconstructed%20a%203D%20representation%20of%20the%20empty%20street.%20The%20mesh%0Arepresentation%20of%20the%20empty%20street%20can%20be%20extracted%20for%20further%20applications.%0AProject%20page%20and%20more%20visualizations%20can%20be%20found%20at%3A%0Ahttps%3A//streetunveiler.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18416v1&entry.124074799=Read"},
{"title": "3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian\n  Splatting", "author": "Qihang Zhang and Yinghao Xu and Chaoyang Wang and Hsin-Ying Lee and Gordon Wetzstein and Bolei Zhou and Ceyuan Yang", "abstract": "  Scene image editing is crucial for entertainment, photography, and\nadvertising design. Existing methods solely focus on either 2D individual\nobject or 3D global scene editing. This results in a lack of a unified approach\nto effectively control and manipulate scenes at the 3D level with different\nlevels of granularity. In this work, we propose 3DitScene, a novel and unified\nscene editing framework leveraging language-guided disentangled Gaussian\nSplatting that enables seamless editing from 2D to 3D, allowing precise control\nover scene composition and individual objects. We first incorporate 3D\nGaussians that are refined through generative priors and optimization\ntechniques. Language features from CLIP then introduce semantics into 3D\ngeometry for object disentanglement. With the disentangled Gaussians, 3DitScene\nallows for manipulation at both the global and individual levels,\nrevolutionizing creative expression and empowering control over scenes and\nobjects. Experimental results demonstrate the effectiveness and versatility of\n3DitScene in scene image editing. Code and online demo can be found at our\nproject homepage: https://zqh0253.github.io/3DitScene/.\n", "link": "http://arxiv.org/abs/2405.18424v1", "date": "2024-05-28", "relevancy": 3.0468, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6236}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6023}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DitScene%3A%20Editing%20Any%20Scene%20via%20Language-guided%20Disentangled%20Gaussian%0A%20%20Splatting&body=Title%3A%203DitScene%3A%20Editing%20Any%20Scene%20via%20Language-guided%20Disentangled%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Qihang%20Zhang%20and%20Yinghao%20Xu%20and%20Chaoyang%20Wang%20and%20Hsin-Ying%20Lee%20and%20Gordon%20Wetzstein%20and%20Bolei%20Zhou%20and%20Ceyuan%20Yang%0AAbstract%3A%20%20%20Scene%20image%20editing%20is%20crucial%20for%20entertainment%2C%20photography%2C%20and%0Aadvertising%20design.%20Existing%20methods%20solely%20focus%20on%20either%202D%20individual%0Aobject%20or%203D%20global%20scene%20editing.%20This%20results%20in%20a%20lack%20of%20a%20unified%20approach%0Ato%20effectively%20control%20and%20manipulate%20scenes%20at%20the%203D%20level%20with%20different%0Alevels%20of%20granularity.%20In%20this%20work%2C%20we%20propose%203DitScene%2C%20a%20novel%20and%20unified%0Ascene%20editing%20framework%20leveraging%20language-guided%20disentangled%20Gaussian%0ASplatting%20that%20enables%20seamless%20editing%20from%202D%20to%203D%2C%20allowing%20precise%20control%0Aover%20scene%20composition%20and%20individual%20objects.%20We%20first%20incorporate%203D%0AGaussians%20that%20are%20refined%20through%20generative%20priors%20and%20optimization%0Atechniques.%20Language%20features%20from%20CLIP%20then%20introduce%20semantics%20into%203D%0Ageometry%20for%20object%20disentanglement.%20With%20the%20disentangled%20Gaussians%2C%203DitScene%0Aallows%20for%20manipulation%20at%20both%20the%20global%20and%20individual%20levels%2C%0Arevolutionizing%20creative%20expression%20and%20empowering%20control%20over%20scenes%20and%0Aobjects.%20Experimental%20results%20demonstrate%20the%20effectiveness%20and%20versatility%20of%0A3DitScene%20in%20scene%20image%20editing.%20Code%20and%20online%20demo%20can%20be%20found%20at%20our%0Aproject%20homepage%3A%20https%3A//zqh0253.github.io/3DitScene/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DitScene%253A%2520Editing%2520Any%2520Scene%2520via%2520Language-guided%2520Disentangled%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DQihang%2520Zhang%2520and%2520Yinghao%2520Xu%2520and%2520Chaoyang%2520Wang%2520and%2520Hsin-Ying%2520Lee%2520and%2520Gordon%2520Wetzstein%2520and%2520Bolei%2520Zhou%2520and%2520Ceyuan%2520Yang%26entry.1292438233%3D%2520%2520Scene%2520image%2520editing%2520is%2520crucial%2520for%2520entertainment%252C%2520photography%252C%2520and%250Aadvertising%2520design.%2520Existing%2520methods%2520solely%2520focus%2520on%2520either%25202D%2520individual%250Aobject%2520or%25203D%2520global%2520scene%2520editing.%2520This%2520results%2520in%2520a%2520lack%2520of%2520a%2520unified%2520approach%250Ato%2520effectively%2520control%2520and%2520manipulate%2520scenes%2520at%2520the%25203D%2520level%2520with%2520different%250Alevels%2520of%2520granularity.%2520In%2520this%2520work%252C%2520we%2520propose%25203DitScene%252C%2520a%2520novel%2520and%2520unified%250Ascene%2520editing%2520framework%2520leveraging%2520language-guided%2520disentangled%2520Gaussian%250ASplatting%2520that%2520enables%2520seamless%2520editing%2520from%25202D%2520to%25203D%252C%2520allowing%2520precise%2520control%250Aover%2520scene%2520composition%2520and%2520individual%2520objects.%2520We%2520first%2520incorporate%25203D%250AGaussians%2520that%2520are%2520refined%2520through%2520generative%2520priors%2520and%2520optimization%250Atechniques.%2520Language%2520features%2520from%2520CLIP%2520then%2520introduce%2520semantics%2520into%25203D%250Ageometry%2520for%2520object%2520disentanglement.%2520With%2520the%2520disentangled%2520Gaussians%252C%25203DitScene%250Aallows%2520for%2520manipulation%2520at%2520both%2520the%2520global%2520and%2520individual%2520levels%252C%250Arevolutionizing%2520creative%2520expression%2520and%2520empowering%2520control%2520over%2520scenes%2520and%250Aobjects.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520and%2520versatility%2520of%250A3DitScene%2520in%2520scene%2520image%2520editing.%2520Code%2520and%2520online%2520demo%2520can%2520be%2520found%2520at%2520our%250Aproject%2520homepage%253A%2520https%253A//zqh0253.github.io/3DitScene/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DitScene%3A%20Editing%20Any%20Scene%20via%20Language-guided%20Disentangled%20Gaussian%0A%20%20Splatting&entry.906535625=Qihang%20Zhang%20and%20Yinghao%20Xu%20and%20Chaoyang%20Wang%20and%20Hsin-Ying%20Lee%20and%20Gordon%20Wetzstein%20and%20Bolei%20Zhou%20and%20Ceyuan%20Yang&entry.1292438233=%20%20Scene%20image%20editing%20is%20crucial%20for%20entertainment%2C%20photography%2C%20and%0Aadvertising%20design.%20Existing%20methods%20solely%20focus%20on%20either%202D%20individual%0Aobject%20or%203D%20global%20scene%20editing.%20This%20results%20in%20a%20lack%20of%20a%20unified%20approach%0Ato%20effectively%20control%20and%20manipulate%20scenes%20at%20the%203D%20level%20with%20different%0Alevels%20of%20granularity.%20In%20this%20work%2C%20we%20propose%203DitScene%2C%20a%20novel%20and%20unified%0Ascene%20editing%20framework%20leveraging%20language-guided%20disentangled%20Gaussian%0ASplatting%20that%20enables%20seamless%20editing%20from%202D%20to%203D%2C%20allowing%20precise%20control%0Aover%20scene%20composition%20and%20individual%20objects.%20We%20first%20incorporate%203D%0AGaussians%20that%20are%20refined%20through%20generative%20priors%20and%20optimization%0Atechniques.%20Language%20features%20from%20CLIP%20then%20introduce%20semantics%20into%203D%0Ageometry%20for%20object%20disentanglement.%20With%20the%20disentangled%20Gaussians%2C%203DitScene%0Aallows%20for%20manipulation%20at%20both%20the%20global%20and%20individual%20levels%2C%0Arevolutionizing%20creative%20expression%20and%20empowering%20control%20over%20scenes%20and%0Aobjects.%20Experimental%20results%20demonstrate%20the%20effectiveness%20and%20versatility%20of%0A3DitScene%20in%20scene%20image%20editing.%20Code%20and%20online%20demo%20can%20be%20found%20at%20our%0Aproject%20homepage%3A%20https%3A//zqh0253.github.io/3DitScene/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18424v1&entry.124074799=Read"},
{"title": "Refractive COLMAP: Refractive Structure-from-Motion Revisited", "author": "Mengkun She and Felix Seegr\u00e4ber and David Nakath and Kevin K\u00f6ser", "abstract": "  In this paper, we present a complete refractive Structure-from-Motion (RSfM)\nframework for underwater 3D reconstruction using refractive camera setups (for\nboth, flat- and dome-port underwater housings). Despite notable achievements in\nrefractive multi-view geometry over the past decade, a robust, complete and\npublicly available solution for such tasks is not available at present, and\noften practical applications have to resort to approximating refraction effects\nby the intrinsic (distortion) parameters of a pinhole camera model. To fill\nthis gap, we have integrated refraction considerations throughout the entire\nSfM process within the state-of-the-art, open-source SfM framework COLMAP.\nNumerical simulations and reconstruction results on synthetically generated but\nphoto-realistic images with ground truth validate that enabling refraction does\nnot compromise accuracy or robustness as compared to in-air reconstructions.\nFinally, we demonstrate the capability of our approach for large-scale\nrefractive scenarios using a dataset consisting of nearly 6000 images. The\nimplementation is released as open-source at:\nhttps://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.\n", "link": "http://arxiv.org/abs/2403.08640v2", "date": "2024-05-28", "relevancy": 2.8998, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5833}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5783}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited&body=Title%3A%20Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited%0AAuthor%3A%20Mengkun%20She%20and%20Felix%20Seegr%C3%A4ber%20and%20David%20Nakath%20and%20Kevin%20K%C3%B6ser%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20complete%20refractive%20Structure-from-Motion%20%28RSfM%29%0Aframework%20for%20underwater%203D%20reconstruction%20using%20refractive%20camera%20setups%20%28for%0Aboth%2C%20flat-%20and%20dome-port%20underwater%20housings%29.%20Despite%20notable%20achievements%20in%0Arefractive%20multi-view%20geometry%20over%20the%20past%20decade%2C%20a%20robust%2C%20complete%20and%0Apublicly%20available%20solution%20for%20such%20tasks%20is%20not%20available%20at%20present%2C%20and%0Aoften%20practical%20applications%20have%20to%20resort%20to%20approximating%20refraction%20effects%0Aby%20the%20intrinsic%20%28distortion%29%20parameters%20of%20a%20pinhole%20camera%20model.%20To%20fill%0Athis%20gap%2C%20we%20have%20integrated%20refraction%20considerations%20throughout%20the%20entire%0ASfM%20process%20within%20the%20state-of-the-art%2C%20open-source%20SfM%20framework%20COLMAP.%0ANumerical%20simulations%20and%20reconstruction%20results%20on%20synthetically%20generated%20but%0Aphoto-realistic%20images%20with%20ground%20truth%20validate%20that%20enabling%20refraction%20does%0Anot%20compromise%20accuracy%20or%20robustness%20as%20compared%20to%20in-air%20reconstructions.%0AFinally%2C%20we%20demonstrate%20the%20capability%20of%20our%20approach%20for%20large-scale%0Arefractive%20scenarios%20using%20a%20dataset%20consisting%20of%20nearly%206000%20images.%20The%0Aimplementation%20is%20released%20as%20open-source%20at%3A%0Ahttps%3A//cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08640v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefractive%2520COLMAP%253A%2520Refractive%2520Structure-from-Motion%2520Revisited%26entry.906535625%3DMengkun%2520She%2520and%2520Felix%2520Seegr%25C3%25A4ber%2520and%2520David%2520Nakath%2520and%2520Kevin%2520K%25C3%25B6ser%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520complete%2520refractive%2520Structure-from-Motion%2520%2528RSfM%2529%250Aframework%2520for%2520underwater%25203D%2520reconstruction%2520using%2520refractive%2520camera%2520setups%2520%2528for%250Aboth%252C%2520flat-%2520and%2520dome-port%2520underwater%2520housings%2529.%2520Despite%2520notable%2520achievements%2520in%250Arefractive%2520multi-view%2520geometry%2520over%2520the%2520past%2520decade%252C%2520a%2520robust%252C%2520complete%2520and%250Apublicly%2520available%2520solution%2520for%2520such%2520tasks%2520is%2520not%2520available%2520at%2520present%252C%2520and%250Aoften%2520practical%2520applications%2520have%2520to%2520resort%2520to%2520approximating%2520refraction%2520effects%250Aby%2520the%2520intrinsic%2520%2528distortion%2529%2520parameters%2520of%2520a%2520pinhole%2520camera%2520model.%2520To%2520fill%250Athis%2520gap%252C%2520we%2520have%2520integrated%2520refraction%2520considerations%2520throughout%2520the%2520entire%250ASfM%2520process%2520within%2520the%2520state-of-the-art%252C%2520open-source%2520SfM%2520framework%2520COLMAP.%250ANumerical%2520simulations%2520and%2520reconstruction%2520results%2520on%2520synthetically%2520generated%2520but%250Aphoto-realistic%2520images%2520with%2520ground%2520truth%2520validate%2520that%2520enabling%2520refraction%2520does%250Anot%2520compromise%2520accuracy%2520or%2520robustness%2520as%2520compared%2520to%2520in-air%2520reconstructions.%250AFinally%252C%2520we%2520demonstrate%2520the%2520capability%2520of%2520our%2520approach%2520for%2520large-scale%250Arefractive%2520scenarios%2520using%2520a%2520dataset%2520consisting%2520of%2520nearly%25206000%2520images.%2520The%250Aimplementation%2520is%2520released%2520as%2520open-source%2520at%253A%250Ahttps%253A//cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08640v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited&entry.906535625=Mengkun%20She%20and%20Felix%20Seegr%C3%A4ber%20and%20David%20Nakath%20and%20Kevin%20K%C3%B6ser&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20complete%20refractive%20Structure-from-Motion%20%28RSfM%29%0Aframework%20for%20underwater%203D%20reconstruction%20using%20refractive%20camera%20setups%20%28for%0Aboth%2C%20flat-%20and%20dome-port%20underwater%20housings%29.%20Despite%20notable%20achievements%20in%0Arefractive%20multi-view%20geometry%20over%20the%20past%20decade%2C%20a%20robust%2C%20complete%20and%0Apublicly%20available%20solution%20for%20such%20tasks%20is%20not%20available%20at%20present%2C%20and%0Aoften%20practical%20applications%20have%20to%20resort%20to%20approximating%20refraction%20effects%0Aby%20the%20intrinsic%20%28distortion%29%20parameters%20of%20a%20pinhole%20camera%20model.%20To%20fill%0Athis%20gap%2C%20we%20have%20integrated%20refraction%20considerations%20throughout%20the%20entire%0ASfM%20process%20within%20the%20state-of-the-art%2C%20open-source%20SfM%20framework%20COLMAP.%0ANumerical%20simulations%20and%20reconstruction%20results%20on%20synthetically%20generated%20but%0Aphoto-realistic%20images%20with%20ground%20truth%20validate%20that%20enabling%20refraction%20does%0Anot%20compromise%20accuracy%20or%20robustness%20as%20compared%20to%20in-air%20reconstructions.%0AFinally%2C%20we%20demonstrate%20the%20capability%20of%20our%20approach%20for%20large-scale%0Arefractive%20scenarios%20using%20a%20dataset%20consisting%20of%20nearly%206000%20images.%20The%0Aimplementation%20is%20released%20as%20open-source%20at%3A%0Ahttps%3A//cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08640v2&entry.124074799=Read"},
{"title": "Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2", "author": "Yiwei Chen and Chao Tang and Amir Aghabiglou and Chung San Chu and Yves Wiaux", "abstract": "  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n", "link": "http://arxiv.org/abs/2403.17905v3", "date": "2024-05-28", "relevancy": 2.8404, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6049}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5538}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Non-Cartesian%20Magnetic%20Resonance%20Imaging%20with%20R2D2&body=Title%3A%20Scalable%20Non-Cartesian%20Magnetic%20Resonance%20Imaging%20with%20R2D2%0AAuthor%3A%20Yiwei%20Chen%20and%20Chao%20Tang%20and%20Amir%20Aghabiglou%20and%20Chung%20San%20Chu%20and%20Yves%20Wiaux%0AAbstract%3A%20%20%20We%20propose%20a%20new%20approach%20for%20non-Cartesian%20magnetic%20resonance%20image%0Areconstruction.%20While%20unrolled%20architectures%20provide%20robustness%20via%0Adata-consistency%20layers%2C%20embedding%20measurement%20operators%20in%20Deep%20Neural%20Network%0A%28DNN%29%20can%20become%20impractical%20at%20large%20scale.%20Alternative%20Plug-and-Play%20%28PnP%29%0Aapproaches%2C%20where%20the%20denoising%20DNNs%20are%20blind%20to%20the%20measurement%20setting%2C%20are%0Anot%20affected%20by%20this%20limitation%20and%20have%20also%20proven%20effective%2C%20but%20their%0Ahighly%20iterative%20nature%20also%20affects%20scalability.%20To%20address%20this%20scalability%0Achallenge%2C%20we%20leverage%20the%20%22Residual-to-Residual%20DNN%20series%20for%20high-Dynamic%0Arange%20imaging%20%28R2D2%29%22%20approach%20recently%20introduced%20in%20astronomical%20imaging.%0AR2D2%27s%20reconstruction%20is%20formed%20as%20a%20series%20of%20residual%20images%2C%20iteratively%0Aestimated%20as%20outputs%20of%20DNNs%20taking%20the%20previous%20iteration%27s%20image%20estimate%20and%0Aassociated%20data%20residual%20as%20inputs.%20The%20method%20can%20be%20interpreted%20as%20a%20learned%0Aversion%20of%20the%20Matching%20Pursuit%20algorithm.%20We%20demonstrate%20R2D2%20in%20simulation%2C%0Aconsidering%20radial%20k-space%20sampling%20acquisition%20sequences.%20Our%20preliminary%0Aresults%20suggest%20that%20R2D2%20achieves%3A%20%28i%29%20suboptimal%20performance%20compared%20to%20its%0Aunrolled%20incarnation%20R2D2-Net%2C%20which%20is%20however%20non-scalable%20due%20to%20the%0Anecessary%20embedding%20of%20NUFFT-based%20data-consistency%20layers%3B%20%28ii%29%20superior%0Areconstruction%20quality%20to%20a%20scalable%20version%20of%20R2D2-Net%20embedding%20an%20FFT-based%0Aapproximation%20for%20data%20consistency%3B%20%28iii%29%20superior%20reconstruction%20quality%20to%0APnP%2C%20while%20only%20requiring%20few%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17905v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Non-Cartesian%2520Magnetic%2520Resonance%2520Imaging%2520with%2520R2D2%26entry.906535625%3DYiwei%2520Chen%2520and%2520Chao%2520Tang%2520and%2520Amir%2520Aghabiglou%2520and%2520Chung%2520San%2520Chu%2520and%2520Yves%2520Wiaux%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520approach%2520for%2520non-Cartesian%2520magnetic%2520resonance%2520image%250Areconstruction.%2520While%2520unrolled%2520architectures%2520provide%2520robustness%2520via%250Adata-consistency%2520layers%252C%2520embedding%2520measurement%2520operators%2520in%2520Deep%2520Neural%2520Network%250A%2528DNN%2529%2520can%2520become%2520impractical%2520at%2520large%2520scale.%2520Alternative%2520Plug-and-Play%2520%2528PnP%2529%250Aapproaches%252C%2520where%2520the%2520denoising%2520DNNs%2520are%2520blind%2520to%2520the%2520measurement%2520setting%252C%2520are%250Anot%2520affected%2520by%2520this%2520limitation%2520and%2520have%2520also%2520proven%2520effective%252C%2520but%2520their%250Ahighly%2520iterative%2520nature%2520also%2520affects%2520scalability.%2520To%2520address%2520this%2520scalability%250Achallenge%252C%2520we%2520leverage%2520the%2520%2522Residual-to-Residual%2520DNN%2520series%2520for%2520high-Dynamic%250Arange%2520imaging%2520%2528R2D2%2529%2522%2520approach%2520recently%2520introduced%2520in%2520astronomical%2520imaging.%250AR2D2%2527s%2520reconstruction%2520is%2520formed%2520as%2520a%2520series%2520of%2520residual%2520images%252C%2520iteratively%250Aestimated%2520as%2520outputs%2520of%2520DNNs%2520taking%2520the%2520previous%2520iteration%2527s%2520image%2520estimate%2520and%250Aassociated%2520data%2520residual%2520as%2520inputs.%2520The%2520method%2520can%2520be%2520interpreted%2520as%2520a%2520learned%250Aversion%2520of%2520the%2520Matching%2520Pursuit%2520algorithm.%2520We%2520demonstrate%2520R2D2%2520in%2520simulation%252C%250Aconsidering%2520radial%2520k-space%2520sampling%2520acquisition%2520sequences.%2520Our%2520preliminary%250Aresults%2520suggest%2520that%2520R2D2%2520achieves%253A%2520%2528i%2529%2520suboptimal%2520performance%2520compared%2520to%2520its%250Aunrolled%2520incarnation%2520R2D2-Net%252C%2520which%2520is%2520however%2520non-scalable%2520due%2520to%2520the%250Anecessary%2520embedding%2520of%2520NUFFT-based%2520data-consistency%2520layers%253B%2520%2528ii%2529%2520superior%250Areconstruction%2520quality%2520to%2520a%2520scalable%2520version%2520of%2520R2D2-Net%2520embedding%2520an%2520FFT-based%250Aapproximation%2520for%2520data%2520consistency%253B%2520%2528iii%2529%2520superior%2520reconstruction%2520quality%2520to%250APnP%252C%2520while%2520only%2520requiring%2520few%2520iterations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17905v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Non-Cartesian%20Magnetic%20Resonance%20Imaging%20with%20R2D2&entry.906535625=Yiwei%20Chen%20and%20Chao%20Tang%20and%20Amir%20Aghabiglou%20and%20Chung%20San%20Chu%20and%20Yves%20Wiaux&entry.1292438233=%20%20We%20propose%20a%20new%20approach%20for%20non-Cartesian%20magnetic%20resonance%20image%0Areconstruction.%20While%20unrolled%20architectures%20provide%20robustness%20via%0Adata-consistency%20layers%2C%20embedding%20measurement%20operators%20in%20Deep%20Neural%20Network%0A%28DNN%29%20can%20become%20impractical%20at%20large%20scale.%20Alternative%20Plug-and-Play%20%28PnP%29%0Aapproaches%2C%20where%20the%20denoising%20DNNs%20are%20blind%20to%20the%20measurement%20setting%2C%20are%0Anot%20affected%20by%20this%20limitation%20and%20have%20also%20proven%20effective%2C%20but%20their%0Ahighly%20iterative%20nature%20also%20affects%20scalability.%20To%20address%20this%20scalability%0Achallenge%2C%20we%20leverage%20the%20%22Residual-to-Residual%20DNN%20series%20for%20high-Dynamic%0Arange%20imaging%20%28R2D2%29%22%20approach%20recently%20introduced%20in%20astronomical%20imaging.%0AR2D2%27s%20reconstruction%20is%20formed%20as%20a%20series%20of%20residual%20images%2C%20iteratively%0Aestimated%20as%20outputs%20of%20DNNs%20taking%20the%20previous%20iteration%27s%20image%20estimate%20and%0Aassociated%20data%20residual%20as%20inputs.%20The%20method%20can%20be%20interpreted%20as%20a%20learned%0Aversion%20of%20the%20Matching%20Pursuit%20algorithm.%20We%20demonstrate%20R2D2%20in%20simulation%2C%0Aconsidering%20radial%20k-space%20sampling%20acquisition%20sequences.%20Our%20preliminary%0Aresults%20suggest%20that%20R2D2%20achieves%3A%20%28i%29%20suboptimal%20performance%20compared%20to%20its%0Aunrolled%20incarnation%20R2D2-Net%2C%20which%20is%20however%20non-scalable%20due%20to%20the%0Anecessary%20embedding%20of%20NUFFT-based%20data-consistency%20layers%3B%20%28ii%29%20superior%0Areconstruction%20quality%20to%20a%20scalable%20version%20of%20R2D2-Net%20embedding%20an%20FFT-based%0Aapproximation%20for%20data%20consistency%3B%20%28iii%29%20superior%20reconstruction%20quality%20to%0APnP%2C%20while%20only%20requiring%20few%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17905v3&entry.124074799=Read"},
{"title": "Edge-guided and Class-balanced Active Learning for Semantic Segmentation\n  of Aerial Images", "author": "Lianlei Shan and Weiqiang Wang and Ke Lv and Bin Luo", "abstract": "  Semantic segmentation requires pixel-level annotation, which is\ntime-consuming. Active Learning (AL) is a promising method for reducing data\nannotation costs. Due to the gap between aerial and natural images, the\nprevious AL methods are not ideal, mainly caused by unreasonable labeling units\nand the neglect of class imbalance. Previous labeling units are based on images\nor regions, which does not consider the characteristics of segmentation tasks\nand aerial images, i.e., the segmentation network often makes mistakes in the\nedge region, and the edge of aerial images is often interlaced and irregular.\nTherefore, an edge-guided labeling unit is proposed and supplemented as the new\nunit. On the other hand, the class imbalance is severe, manifested in two\naspects: the aerial image is seriously imbalanced, and the AL strategy does not\nfully consider the class balance. Both seriously affect the performance of AL\nin aerial images. We comprehensively ensure class balance from all steps that\nmay occur imbalance, including initial labeled data, subsequent labeled data,\nand pseudo-labels. Through the two improvements, our method achieves more than\n11.2\\% gains compared to state-of-the-art methods on three benchmark datasets,\nDeepglobe, Potsdam, and Vaihingen, and more than 18.6\\% gains compared to the\nbaseline. Sufficient ablation studies show that every module is indispensable.\nFurthermore, we establish a fair and strong benchmark for future research on AL\nfor aerial image segmentation.\n", "link": "http://arxiv.org/abs/2405.18078v1", "date": "2024-05-28", "relevancy": 2.7977, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.579}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.55}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-guided%20and%20Class-balanced%20Active%20Learning%20for%20Semantic%20Segmentation%0A%20%20of%20Aerial%20Images&body=Title%3A%20Edge-guided%20and%20Class-balanced%20Active%20Learning%20for%20Semantic%20Segmentation%0A%20%20of%20Aerial%20Images%0AAuthor%3A%20Lianlei%20Shan%20and%20Weiqiang%20Wang%20and%20Ke%20Lv%20and%20Bin%20Luo%0AAbstract%3A%20%20%20Semantic%20segmentation%20requires%20pixel-level%20annotation%2C%20which%20is%0Atime-consuming.%20Active%20Learning%20%28AL%29%20is%20a%20promising%20method%20for%20reducing%20data%0Aannotation%20costs.%20Due%20to%20the%20gap%20between%20aerial%20and%20natural%20images%2C%20the%0Aprevious%20AL%20methods%20are%20not%20ideal%2C%20mainly%20caused%20by%20unreasonable%20labeling%20units%0Aand%20the%20neglect%20of%20class%20imbalance.%20Previous%20labeling%20units%20are%20based%20on%20images%0Aor%20regions%2C%20which%20does%20not%20consider%20the%20characteristics%20of%20segmentation%20tasks%0Aand%20aerial%20images%2C%20i.e.%2C%20the%20segmentation%20network%20often%20makes%20mistakes%20in%20the%0Aedge%20region%2C%20and%20the%20edge%20of%20aerial%20images%20is%20often%20interlaced%20and%20irregular.%0ATherefore%2C%20an%20edge-guided%20labeling%20unit%20is%20proposed%20and%20supplemented%20as%20the%20new%0Aunit.%20On%20the%20other%20hand%2C%20the%20class%20imbalance%20is%20severe%2C%20manifested%20in%20two%0Aaspects%3A%20the%20aerial%20image%20is%20seriously%20imbalanced%2C%20and%20the%20AL%20strategy%20does%20not%0Afully%20consider%20the%20class%20balance.%20Both%20seriously%20affect%20the%20performance%20of%20AL%0Ain%20aerial%20images.%20We%20comprehensively%20ensure%20class%20balance%20from%20all%20steps%20that%0Amay%20occur%20imbalance%2C%20including%20initial%20labeled%20data%2C%20subsequent%20labeled%20data%2C%0Aand%20pseudo-labels.%20Through%20the%20two%20improvements%2C%20our%20method%20achieves%20more%20than%0A11.2%5C%25%20gains%20compared%20to%20state-of-the-art%20methods%20on%20three%20benchmark%20datasets%2C%0ADeepglobe%2C%20Potsdam%2C%20and%20Vaihingen%2C%20and%20more%20than%2018.6%5C%25%20gains%20compared%20to%20the%0Abaseline.%20Sufficient%20ablation%20studies%20show%20that%20every%20module%20is%20indispensable.%0AFurthermore%2C%20we%20establish%20a%20fair%20and%20strong%20benchmark%20for%20future%20research%20on%20AL%0Afor%20aerial%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-guided%2520and%2520Class-balanced%2520Active%2520Learning%2520for%2520Semantic%2520Segmentation%250A%2520%2520of%2520Aerial%2520Images%26entry.906535625%3DLianlei%2520Shan%2520and%2520Weiqiang%2520Wang%2520and%2520Ke%2520Lv%2520and%2520Bin%2520Luo%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520requires%2520pixel-level%2520annotation%252C%2520which%2520is%250Atime-consuming.%2520Active%2520Learning%2520%2528AL%2529%2520is%2520a%2520promising%2520method%2520for%2520reducing%2520data%250Aannotation%2520costs.%2520Due%2520to%2520the%2520gap%2520between%2520aerial%2520and%2520natural%2520images%252C%2520the%250Aprevious%2520AL%2520methods%2520are%2520not%2520ideal%252C%2520mainly%2520caused%2520by%2520unreasonable%2520labeling%2520units%250Aand%2520the%2520neglect%2520of%2520class%2520imbalance.%2520Previous%2520labeling%2520units%2520are%2520based%2520on%2520images%250Aor%2520regions%252C%2520which%2520does%2520not%2520consider%2520the%2520characteristics%2520of%2520segmentation%2520tasks%250Aand%2520aerial%2520images%252C%2520i.e.%252C%2520the%2520segmentation%2520network%2520often%2520makes%2520mistakes%2520in%2520the%250Aedge%2520region%252C%2520and%2520the%2520edge%2520of%2520aerial%2520images%2520is%2520often%2520interlaced%2520and%2520irregular.%250ATherefore%252C%2520an%2520edge-guided%2520labeling%2520unit%2520is%2520proposed%2520and%2520supplemented%2520as%2520the%2520new%250Aunit.%2520On%2520the%2520other%2520hand%252C%2520the%2520class%2520imbalance%2520is%2520severe%252C%2520manifested%2520in%2520two%250Aaspects%253A%2520the%2520aerial%2520image%2520is%2520seriously%2520imbalanced%252C%2520and%2520the%2520AL%2520strategy%2520does%2520not%250Afully%2520consider%2520the%2520class%2520balance.%2520Both%2520seriously%2520affect%2520the%2520performance%2520of%2520AL%250Ain%2520aerial%2520images.%2520We%2520comprehensively%2520ensure%2520class%2520balance%2520from%2520all%2520steps%2520that%250Amay%2520occur%2520imbalance%252C%2520including%2520initial%2520labeled%2520data%252C%2520subsequent%2520labeled%2520data%252C%250Aand%2520pseudo-labels.%2520Through%2520the%2520two%2520improvements%252C%2520our%2520method%2520achieves%2520more%2520than%250A11.2%255C%2525%2520gains%2520compared%2520to%2520state-of-the-art%2520methods%2520on%2520three%2520benchmark%2520datasets%252C%250ADeepglobe%252C%2520Potsdam%252C%2520and%2520Vaihingen%252C%2520and%2520more%2520than%252018.6%255C%2525%2520gains%2520compared%2520to%2520the%250Abaseline.%2520Sufficient%2520ablation%2520studies%2520show%2520that%2520every%2520module%2520is%2520indispensable.%250AFurthermore%252C%2520we%2520establish%2520a%2520fair%2520and%2520strong%2520benchmark%2520for%2520future%2520research%2520on%2520AL%250Afor%2520aerial%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-guided%20and%20Class-balanced%20Active%20Learning%20for%20Semantic%20Segmentation%0A%20%20of%20Aerial%20Images&entry.906535625=Lianlei%20Shan%20and%20Weiqiang%20Wang%20and%20Ke%20Lv%20and%20Bin%20Luo&entry.1292438233=%20%20Semantic%20segmentation%20requires%20pixel-level%20annotation%2C%20which%20is%0Atime-consuming.%20Active%20Learning%20%28AL%29%20is%20a%20promising%20method%20for%20reducing%20data%0Aannotation%20costs.%20Due%20to%20the%20gap%20between%20aerial%20and%20natural%20images%2C%20the%0Aprevious%20AL%20methods%20are%20not%20ideal%2C%20mainly%20caused%20by%20unreasonable%20labeling%20units%0Aand%20the%20neglect%20of%20class%20imbalance.%20Previous%20labeling%20units%20are%20based%20on%20images%0Aor%20regions%2C%20which%20does%20not%20consider%20the%20characteristics%20of%20segmentation%20tasks%0Aand%20aerial%20images%2C%20i.e.%2C%20the%20segmentation%20network%20often%20makes%20mistakes%20in%20the%0Aedge%20region%2C%20and%20the%20edge%20of%20aerial%20images%20is%20often%20interlaced%20and%20irregular.%0ATherefore%2C%20an%20edge-guided%20labeling%20unit%20is%20proposed%20and%20supplemented%20as%20the%20new%0Aunit.%20On%20the%20other%20hand%2C%20the%20class%20imbalance%20is%20severe%2C%20manifested%20in%20two%0Aaspects%3A%20the%20aerial%20image%20is%20seriously%20imbalanced%2C%20and%20the%20AL%20strategy%20does%20not%0Afully%20consider%20the%20class%20balance.%20Both%20seriously%20affect%20the%20performance%20of%20AL%0Ain%20aerial%20images.%20We%20comprehensively%20ensure%20class%20balance%20from%20all%20steps%20that%0Amay%20occur%20imbalance%2C%20including%20initial%20labeled%20data%2C%20subsequent%20labeled%20data%2C%0Aand%20pseudo-labels.%20Through%20the%20two%20improvements%2C%20our%20method%20achieves%20more%20than%0A11.2%5C%25%20gains%20compared%20to%20state-of-the-art%20methods%20on%20three%20benchmark%20datasets%2C%0ADeepglobe%2C%20Potsdam%2C%20and%20Vaihingen%2C%20and%20more%20than%2018.6%5C%25%20gains%20compared%20to%20the%0Abaseline.%20Sufficient%20ablation%20studies%20show%20that%20every%20module%20is%20indispensable.%0AFurthermore%2C%20we%20establish%20a%20fair%20and%20strong%20benchmark%20for%20future%20research%20on%20AL%0Afor%20aerial%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18078v1&entry.124074799=Read"},
{"title": "Mind-to-Image: Projecting Visual Mental Imagination of the Brain from\n  fMRI", "author": "Hugo Caselles-Dupr\u00e9 and Charles Mellerio and Paul H\u00e9rent and Aliz\u00e9e Lopez-Persem and Benoit B\u00e9ranger and Mathieu Soularue and Pierre Fautrel and Gauthier Vernier and Matthieu Cord", "abstract": "  The reconstruction of images observed by subjects from fMRI data collected\nduring visual stimuli has made strong progress in the past decade, thanks to\nthe availability of extensive fMRI datasets and advancements in generative\nmodels for image generation. However, the application of visual reconstruction\nhas remained limited. Reconstructing visual imagination presents a greater\nchallenge, with potentially revolutionary applications ranging from aiding\nindividuals with disabilities to verifying witness accounts in court. The\nprimary hurdles in this field are the absence of data collection protocols for\nvisual imagery and the lack of datasets on the subject. Traditionally,\nfMRI-to-image relies on data collected from subjects exposed to visual stimuli,\nwhich poses issues for generating visual imagery based on the difference of\nbrain activity between visual stimulation and visual imagery. For the first\ntime, we have compiled a substantial dataset (around 6h of scans) on visual\nimagery along with a proposed data collection protocol. We then train a\nmodified version of an fMRI-to-image model and demonstrate the feasibility of\nreconstructing images from two modes of imagination: from memory and from pure\nimagination. The resulting pipeline we call Mind-to-Image marks a step towards\ncreating a technology that allow direct reconstruction of visual imagery.\n", "link": "http://arxiv.org/abs/2404.05468v5", "date": "2024-05-28", "relevancy": 2.7976, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5752}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5752}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind-to-Image%3A%20Projecting%20Visual%20Mental%20Imagination%20of%20the%20Brain%20from%0A%20%20fMRI&body=Title%3A%20Mind-to-Image%3A%20Projecting%20Visual%20Mental%20Imagination%20of%20the%20Brain%20from%0A%20%20fMRI%0AAuthor%3A%20Hugo%20Caselles-Dupr%C3%A9%20and%20Charles%20Mellerio%20and%20Paul%20H%C3%A9rent%20and%20Aliz%C3%A9e%20Lopez-Persem%20and%20Benoit%20B%C3%A9ranger%20and%20Mathieu%20Soularue%20and%20Pierre%20Fautrel%20and%20Gauthier%20Vernier%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20The%20reconstruction%20of%20images%20observed%20by%20subjects%20from%20fMRI%20data%20collected%0Aduring%20visual%20stimuli%20has%20made%20strong%20progress%20in%20the%20past%20decade%2C%20thanks%20to%0Athe%20availability%20of%20extensive%20fMRI%20datasets%20and%20advancements%20in%20generative%0Amodels%20for%20image%20generation.%20However%2C%20the%20application%20of%20visual%20reconstruction%0Ahas%20remained%20limited.%20Reconstructing%20visual%20imagination%20presents%20a%20greater%0Achallenge%2C%20with%20potentially%20revolutionary%20applications%20ranging%20from%20aiding%0Aindividuals%20with%20disabilities%20to%20verifying%20witness%20accounts%20in%20court.%20The%0Aprimary%20hurdles%20in%20this%20field%20are%20the%20absence%20of%20data%20collection%20protocols%20for%0Avisual%20imagery%20and%20the%20lack%20of%20datasets%20on%20the%20subject.%20Traditionally%2C%0AfMRI-to-image%20relies%20on%20data%20collected%20from%20subjects%20exposed%20to%20visual%20stimuli%2C%0Awhich%20poses%20issues%20for%20generating%20visual%20imagery%20based%20on%20the%20difference%20of%0Abrain%20activity%20between%20visual%20stimulation%20and%20visual%20imagery.%20For%20the%20first%0Atime%2C%20we%20have%20compiled%20a%20substantial%20dataset%20%28around%206h%20of%20scans%29%20on%20visual%0Aimagery%20along%20with%20a%20proposed%20data%20collection%20protocol.%20We%20then%20train%20a%0Amodified%20version%20of%20an%20fMRI-to-image%20model%20and%20demonstrate%20the%20feasibility%20of%0Areconstructing%20images%20from%20two%20modes%20of%20imagination%3A%20from%20memory%20and%20from%20pure%0Aimagination.%20The%20resulting%20pipeline%20we%20call%20Mind-to-Image%20marks%20a%20step%20towards%0Acreating%20a%20technology%20that%20allow%20direct%20reconstruction%20of%20visual%20imagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05468v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind-to-Image%253A%2520Projecting%2520Visual%2520Mental%2520Imagination%2520of%2520the%2520Brain%2520from%250A%2520%2520fMRI%26entry.906535625%3DHugo%2520Caselles-Dupr%25C3%25A9%2520and%2520Charles%2520Mellerio%2520and%2520Paul%2520H%25C3%25A9rent%2520and%2520Aliz%25C3%25A9e%2520Lopez-Persem%2520and%2520Benoit%2520B%25C3%25A9ranger%2520and%2520Mathieu%2520Soularue%2520and%2520Pierre%2520Fautrel%2520and%2520Gauthier%2520Vernier%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520The%2520reconstruction%2520of%2520images%2520observed%2520by%2520subjects%2520from%2520fMRI%2520data%2520collected%250Aduring%2520visual%2520stimuli%2520has%2520made%2520strong%2520progress%2520in%2520the%2520past%2520decade%252C%2520thanks%2520to%250Athe%2520availability%2520of%2520extensive%2520fMRI%2520datasets%2520and%2520advancements%2520in%2520generative%250Amodels%2520for%2520image%2520generation.%2520However%252C%2520the%2520application%2520of%2520visual%2520reconstruction%250Ahas%2520remained%2520limited.%2520Reconstructing%2520visual%2520imagination%2520presents%2520a%2520greater%250Achallenge%252C%2520with%2520potentially%2520revolutionary%2520applications%2520ranging%2520from%2520aiding%250Aindividuals%2520with%2520disabilities%2520to%2520verifying%2520witness%2520accounts%2520in%2520court.%2520The%250Aprimary%2520hurdles%2520in%2520this%2520field%2520are%2520the%2520absence%2520of%2520data%2520collection%2520protocols%2520for%250Avisual%2520imagery%2520and%2520the%2520lack%2520of%2520datasets%2520on%2520the%2520subject.%2520Traditionally%252C%250AfMRI-to-image%2520relies%2520on%2520data%2520collected%2520from%2520subjects%2520exposed%2520to%2520visual%2520stimuli%252C%250Awhich%2520poses%2520issues%2520for%2520generating%2520visual%2520imagery%2520based%2520on%2520the%2520difference%2520of%250Abrain%2520activity%2520between%2520visual%2520stimulation%2520and%2520visual%2520imagery.%2520For%2520the%2520first%250Atime%252C%2520we%2520have%2520compiled%2520a%2520substantial%2520dataset%2520%2528around%25206h%2520of%2520scans%2529%2520on%2520visual%250Aimagery%2520along%2520with%2520a%2520proposed%2520data%2520collection%2520protocol.%2520We%2520then%2520train%2520a%250Amodified%2520version%2520of%2520an%2520fMRI-to-image%2520model%2520and%2520demonstrate%2520the%2520feasibility%2520of%250Areconstructing%2520images%2520from%2520two%2520modes%2520of%2520imagination%253A%2520from%2520memory%2520and%2520from%2520pure%250Aimagination.%2520The%2520resulting%2520pipeline%2520we%2520call%2520Mind-to-Image%2520marks%2520a%2520step%2520towards%250Acreating%2520a%2520technology%2520that%2520allow%2520direct%2520reconstruction%2520of%2520visual%2520imagery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05468v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind-to-Image%3A%20Projecting%20Visual%20Mental%20Imagination%20of%20the%20Brain%20from%0A%20%20fMRI&entry.906535625=Hugo%20Caselles-Dupr%C3%A9%20and%20Charles%20Mellerio%20and%20Paul%20H%C3%A9rent%20and%20Aliz%C3%A9e%20Lopez-Persem%20and%20Benoit%20B%C3%A9ranger%20and%20Mathieu%20Soularue%20and%20Pierre%20Fautrel%20and%20Gauthier%20Vernier%20and%20Matthieu%20Cord&entry.1292438233=%20%20The%20reconstruction%20of%20images%20observed%20by%20subjects%20from%20fMRI%20data%20collected%0Aduring%20visual%20stimuli%20has%20made%20strong%20progress%20in%20the%20past%20decade%2C%20thanks%20to%0Athe%20availability%20of%20extensive%20fMRI%20datasets%20and%20advancements%20in%20generative%0Amodels%20for%20image%20generation.%20However%2C%20the%20application%20of%20visual%20reconstruction%0Ahas%20remained%20limited.%20Reconstructing%20visual%20imagination%20presents%20a%20greater%0Achallenge%2C%20with%20potentially%20revolutionary%20applications%20ranging%20from%20aiding%0Aindividuals%20with%20disabilities%20to%20verifying%20witness%20accounts%20in%20court.%20The%0Aprimary%20hurdles%20in%20this%20field%20are%20the%20absence%20of%20data%20collection%20protocols%20for%0Avisual%20imagery%20and%20the%20lack%20of%20datasets%20on%20the%20subject.%20Traditionally%2C%0AfMRI-to-image%20relies%20on%20data%20collected%20from%20subjects%20exposed%20to%20visual%20stimuli%2C%0Awhich%20poses%20issues%20for%20generating%20visual%20imagery%20based%20on%20the%20difference%20of%0Abrain%20activity%20between%20visual%20stimulation%20and%20visual%20imagery.%20For%20the%20first%0Atime%2C%20we%20have%20compiled%20a%20substantial%20dataset%20%28around%206h%20of%20scans%29%20on%20visual%0Aimagery%20along%20with%20a%20proposed%20data%20collection%20protocol.%20We%20then%20train%20a%0Amodified%20version%20of%20an%20fMRI-to-image%20model%20and%20demonstrate%20the%20feasibility%20of%0Areconstructing%20images%20from%20two%20modes%20of%20imagination%3A%20from%20memory%20and%20from%20pure%0Aimagination.%20The%20resulting%20pipeline%20we%20call%20Mind-to-Image%20marks%20a%20step%20towards%0Acreating%20a%20technology%20that%20allow%20direct%20reconstruction%20of%20visual%20imagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05468v5&entry.124074799=Read"},
{"title": "Self-Supervised Dual Contouring", "author": "Ramana Sundararaman and Roman Klokov and Maks Ovsjanikov", "abstract": "  Learning-based isosurface extraction methods have recently emerged as a\nrobust and efficient alternative to axiomatic techniques. However, the vast\nmajority of such approaches rely on supervised training with axiomatically\ncomputed ground truths, thus potentially inheriting biases and data artifacts\nof the corresponding axiomatic methods. Steering away from such dependencies,\nwe propose a self-supervised training scheme for the Neural Dual Contouring\nmeshing framework, resulting in our method: Self-Supervised Dual Contouring\n(SDC). Instead of optimizing predicted mesh vertices with supervised training,\nwe use two novel self-supervised loss functions that encourage the consistency\nbetween distances to the generated mesh up to the first order. Meshes\nreconstructed by SDC surpass existing data-driven methods in capturing\nintricate details while being more robust to possible irregularities in the\ninput. Furthermore, we use the same self-supervised training objective linking\ninferred mesh and input SDF, to regularize the training process of Deep\nImplicit Networks (DINs). We demonstrate that the resulting DINs produce\nhigher-quality implicit functions, ultimately leading to more accurate and\ndetail-preserving surfaces compared to prior baselines for different input\nmodalities. Finally, we demonstrate that our self-supervised losses improve\nmeshing performance in the single-view reconstruction task by enabling joint\ntraining of predicted SDF and resulting output mesh. We open-source our code at\nhttps://github.com/Sentient07/SDC\n", "link": "http://arxiv.org/abs/2405.18131v1", "date": "2024-05-28", "relevancy": 2.7781, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5751}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5481}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Dual%20Contouring&body=Title%3A%20Self-Supervised%20Dual%20Contouring%0AAuthor%3A%20Ramana%20Sundararaman%20and%20Roman%20Klokov%20and%20Maks%20Ovsjanikov%0AAbstract%3A%20%20%20Learning-based%20isosurface%20extraction%20methods%20have%20recently%20emerged%20as%20a%0Arobust%20and%20efficient%20alternative%20to%20axiomatic%20techniques.%20However%2C%20the%20vast%0Amajority%20of%20such%20approaches%20rely%20on%20supervised%20training%20with%20axiomatically%0Acomputed%20ground%20truths%2C%20thus%20potentially%20inheriting%20biases%20and%20data%20artifacts%0Aof%20the%20corresponding%20axiomatic%20methods.%20Steering%20away%20from%20such%20dependencies%2C%0Awe%20propose%20a%20self-supervised%20training%20scheme%20for%20the%20Neural%20Dual%20Contouring%0Ameshing%20framework%2C%20resulting%20in%20our%20method%3A%20Self-Supervised%20Dual%20Contouring%0A%28SDC%29.%20Instead%20of%20optimizing%20predicted%20mesh%20vertices%20with%20supervised%20training%2C%0Awe%20use%20two%20novel%20self-supervised%20loss%20functions%20that%20encourage%20the%20consistency%0Abetween%20distances%20to%20the%20generated%20mesh%20up%20to%20the%20first%20order.%20Meshes%0Areconstructed%20by%20SDC%20surpass%20existing%20data-driven%20methods%20in%20capturing%0Aintricate%20details%20while%20being%20more%20robust%20to%20possible%20irregularities%20in%20the%0Ainput.%20Furthermore%2C%20we%20use%20the%20same%20self-supervised%20training%20objective%20linking%0Ainferred%20mesh%20and%20input%20SDF%2C%20to%20regularize%20the%20training%20process%20of%20Deep%0AImplicit%20Networks%20%28DINs%29.%20We%20demonstrate%20that%20the%20resulting%20DINs%20produce%0Ahigher-quality%20implicit%20functions%2C%20ultimately%20leading%20to%20more%20accurate%20and%0Adetail-preserving%20surfaces%20compared%20to%20prior%20baselines%20for%20different%20input%0Amodalities.%20Finally%2C%20we%20demonstrate%20that%20our%20self-supervised%20losses%20improve%0Ameshing%20performance%20in%20the%20single-view%20reconstruction%20task%20by%20enabling%20joint%0Atraining%20of%20predicted%20SDF%20and%20resulting%20output%20mesh.%20We%20open-source%20our%20code%20at%0Ahttps%3A//github.com/Sentient07/SDC%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Dual%2520Contouring%26entry.906535625%3DRamana%2520Sundararaman%2520and%2520Roman%2520Klokov%2520and%2520Maks%2520Ovsjanikov%26entry.1292438233%3D%2520%2520Learning-based%2520isosurface%2520extraction%2520methods%2520have%2520recently%2520emerged%2520as%2520a%250Arobust%2520and%2520efficient%2520alternative%2520to%2520axiomatic%2520techniques.%2520However%252C%2520the%2520vast%250Amajority%2520of%2520such%2520approaches%2520rely%2520on%2520supervised%2520training%2520with%2520axiomatically%250Acomputed%2520ground%2520truths%252C%2520thus%2520potentially%2520inheriting%2520biases%2520and%2520data%2520artifacts%250Aof%2520the%2520corresponding%2520axiomatic%2520methods.%2520Steering%2520away%2520from%2520such%2520dependencies%252C%250Awe%2520propose%2520a%2520self-supervised%2520training%2520scheme%2520for%2520the%2520Neural%2520Dual%2520Contouring%250Ameshing%2520framework%252C%2520resulting%2520in%2520our%2520method%253A%2520Self-Supervised%2520Dual%2520Contouring%250A%2528SDC%2529.%2520Instead%2520of%2520optimizing%2520predicted%2520mesh%2520vertices%2520with%2520supervised%2520training%252C%250Awe%2520use%2520two%2520novel%2520self-supervised%2520loss%2520functions%2520that%2520encourage%2520the%2520consistency%250Abetween%2520distances%2520to%2520the%2520generated%2520mesh%2520up%2520to%2520the%2520first%2520order.%2520Meshes%250Areconstructed%2520by%2520SDC%2520surpass%2520existing%2520data-driven%2520methods%2520in%2520capturing%250Aintricate%2520details%2520while%2520being%2520more%2520robust%2520to%2520possible%2520irregularities%2520in%2520the%250Ainput.%2520Furthermore%252C%2520we%2520use%2520the%2520same%2520self-supervised%2520training%2520objective%2520linking%250Ainferred%2520mesh%2520and%2520input%2520SDF%252C%2520to%2520regularize%2520the%2520training%2520process%2520of%2520Deep%250AImplicit%2520Networks%2520%2528DINs%2529.%2520We%2520demonstrate%2520that%2520the%2520resulting%2520DINs%2520produce%250Ahigher-quality%2520implicit%2520functions%252C%2520ultimately%2520leading%2520to%2520more%2520accurate%2520and%250Adetail-preserving%2520surfaces%2520compared%2520to%2520prior%2520baselines%2520for%2520different%2520input%250Amodalities.%2520Finally%252C%2520we%2520demonstrate%2520that%2520our%2520self-supervised%2520losses%2520improve%250Ameshing%2520performance%2520in%2520the%2520single-view%2520reconstruction%2520task%2520by%2520enabling%2520joint%250Atraining%2520of%2520predicted%2520SDF%2520and%2520resulting%2520output%2520mesh.%2520We%2520open-source%2520our%2520code%2520at%250Ahttps%253A//github.com/Sentient07/SDC%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Dual%20Contouring&entry.906535625=Ramana%20Sundararaman%20and%20Roman%20Klokov%20and%20Maks%20Ovsjanikov&entry.1292438233=%20%20Learning-based%20isosurface%20extraction%20methods%20have%20recently%20emerged%20as%20a%0Arobust%20and%20efficient%20alternative%20to%20axiomatic%20techniques.%20However%2C%20the%20vast%0Amajority%20of%20such%20approaches%20rely%20on%20supervised%20training%20with%20axiomatically%0Acomputed%20ground%20truths%2C%20thus%20potentially%20inheriting%20biases%20and%20data%20artifacts%0Aof%20the%20corresponding%20axiomatic%20methods.%20Steering%20away%20from%20such%20dependencies%2C%0Awe%20propose%20a%20self-supervised%20training%20scheme%20for%20the%20Neural%20Dual%20Contouring%0Ameshing%20framework%2C%20resulting%20in%20our%20method%3A%20Self-Supervised%20Dual%20Contouring%0A%28SDC%29.%20Instead%20of%20optimizing%20predicted%20mesh%20vertices%20with%20supervised%20training%2C%0Awe%20use%20two%20novel%20self-supervised%20loss%20functions%20that%20encourage%20the%20consistency%0Abetween%20distances%20to%20the%20generated%20mesh%20up%20to%20the%20first%20order.%20Meshes%0Areconstructed%20by%20SDC%20surpass%20existing%20data-driven%20methods%20in%20capturing%0Aintricate%20details%20while%20being%20more%20robust%20to%20possible%20irregularities%20in%20the%0Ainput.%20Furthermore%2C%20we%20use%20the%20same%20self-supervised%20training%20objective%20linking%0Ainferred%20mesh%20and%20input%20SDF%2C%20to%20regularize%20the%20training%20process%20of%20Deep%0AImplicit%20Networks%20%28DINs%29.%20We%20demonstrate%20that%20the%20resulting%20DINs%20produce%0Ahigher-quality%20implicit%20functions%2C%20ultimately%20leading%20to%20more%20accurate%20and%0Adetail-preserving%20surfaces%20compared%20to%20prior%20baselines%20for%20different%20input%0Amodalities.%20Finally%2C%20we%20demonstrate%20that%20our%20self-supervised%20losses%20improve%0Ameshing%20performance%20in%20the%20single-view%20reconstruction%20task%20by%20enabling%20joint%0Atraining%20of%20predicted%20SDF%20and%20resulting%20output%20mesh.%20We%20open-source%20our%20code%20at%0Ahttps%3A//github.com/Sentient07/SDC%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18131v1&entry.124074799=Read"},
{"title": "Efficient Remote Sensing with Harmonized Transfer Learning and Modality\n  Alignment", "author": "Tengjun Huang", "abstract": "  With the rise of Visual and Language Pretraining (VLP), an increasing number\nof downstream tasks are adopting the paradigm of pretraining followed by\nfine-tuning. Although this paradigm has demonstrated potential in various\nmultimodal downstream tasks, its implementation in the remote sensing domain\nencounters some obstacles. Specifically, the tendency for same-modality\nembeddings to cluster together impedes efficient transfer learning. To tackle\nthis issue, we review the aim of multimodal transfer learning for downstream\ntasks from a unified perspective, and rethink the optimization process based on\nthree distinct objectives. We propose \"Harmonized Transfer Learning and\nModality Alignment (HarMA)\", a method that simultaneously satisfies task\nconstraints, modality alignment, and single-modality uniform alignment, while\nminimizing training overhead through parameter-efficient fine-tuning.\nRemarkably, without the need for external data for training, HarMA achieves\nstate-of-the-art performance in two popular multimodal retrieval tasks in the\nfield of remote sensing. Our experiments reveal that HarMA achieves competitive\nand even superior performance to fully fine-tuned models with only minimal\nadjustable parameters. Due to its simplicity, HarMA can be integrated into\nalmost all existing multimodal pretraining models. We hope this method can\nfacilitate the efficient application of large models to a wide range of\ndownstream tasks while significantly reducing the resource consumption. Code is\navailable at https://github.com/seekerhuang/HarMA.\n", "link": "http://arxiv.org/abs/2404.18253v5", "date": "2024-05-28", "relevancy": 2.7576, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6055}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5264}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Remote%20Sensing%20with%20Harmonized%20Transfer%20Learning%20and%20Modality%0A%20%20Alignment&body=Title%3A%20Efficient%20Remote%20Sensing%20with%20Harmonized%20Transfer%20Learning%20and%20Modality%0A%20%20Alignment%0AAuthor%3A%20Tengjun%20Huang%0AAbstract%3A%20%20%20With%20the%20rise%20of%20Visual%20and%20Language%20Pretraining%20%28VLP%29%2C%20an%20increasing%20number%0Aof%20downstream%20tasks%20are%20adopting%20the%20paradigm%20of%20pretraining%20followed%20by%0Afine-tuning.%20Although%20this%20paradigm%20has%20demonstrated%20potential%20in%20various%0Amultimodal%20downstream%20tasks%2C%20its%20implementation%20in%20the%20remote%20sensing%20domain%0Aencounters%20some%20obstacles.%20Specifically%2C%20the%20tendency%20for%20same-modality%0Aembeddings%20to%20cluster%20together%20impedes%20efficient%20transfer%20learning.%20To%20tackle%0Athis%20issue%2C%20we%20review%20the%20aim%20of%20multimodal%20transfer%20learning%20for%20downstream%0Atasks%20from%20a%20unified%20perspective%2C%20and%20rethink%20the%20optimization%20process%20based%20on%0Athree%20distinct%20objectives.%20We%20propose%20%22Harmonized%20Transfer%20Learning%20and%0AModality%20Alignment%20%28HarMA%29%22%2C%20a%20method%20that%20simultaneously%20satisfies%20task%0Aconstraints%2C%20modality%20alignment%2C%20and%20single-modality%20uniform%20alignment%2C%20while%0Aminimizing%20training%20overhead%20through%20parameter-efficient%20fine-tuning.%0ARemarkably%2C%20without%20the%20need%20for%20external%20data%20for%20training%2C%20HarMA%20achieves%0Astate-of-the-art%20performance%20in%20two%20popular%20multimodal%20retrieval%20tasks%20in%20the%0Afield%20of%20remote%20sensing.%20Our%20experiments%20reveal%20that%20HarMA%20achieves%20competitive%0Aand%20even%20superior%20performance%20to%20fully%20fine-tuned%20models%20with%20only%20minimal%0Aadjustable%20parameters.%20Due%20to%20its%20simplicity%2C%20HarMA%20can%20be%20integrated%20into%0Aalmost%20all%20existing%20multimodal%20pretraining%20models.%20We%20hope%20this%20method%20can%0Afacilitate%20the%20efficient%20application%20of%20large%20models%20to%20a%20wide%20range%20of%0Adownstream%20tasks%20while%20significantly%20reducing%20the%20resource%20consumption.%20Code%20is%0Aavailable%20at%20https%3A//github.com/seekerhuang/HarMA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18253v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Remote%2520Sensing%2520with%2520Harmonized%2520Transfer%2520Learning%2520and%2520Modality%250A%2520%2520Alignment%26entry.906535625%3DTengjun%2520Huang%26entry.1292438233%3D%2520%2520With%2520the%2520rise%2520of%2520Visual%2520and%2520Language%2520Pretraining%2520%2528VLP%2529%252C%2520an%2520increasing%2520number%250Aof%2520downstream%2520tasks%2520are%2520adopting%2520the%2520paradigm%2520of%2520pretraining%2520followed%2520by%250Afine-tuning.%2520Although%2520this%2520paradigm%2520has%2520demonstrated%2520potential%2520in%2520various%250Amultimodal%2520downstream%2520tasks%252C%2520its%2520implementation%2520in%2520the%2520remote%2520sensing%2520domain%250Aencounters%2520some%2520obstacles.%2520Specifically%252C%2520the%2520tendency%2520for%2520same-modality%250Aembeddings%2520to%2520cluster%2520together%2520impedes%2520efficient%2520transfer%2520learning.%2520To%2520tackle%250Athis%2520issue%252C%2520we%2520review%2520the%2520aim%2520of%2520multimodal%2520transfer%2520learning%2520for%2520downstream%250Atasks%2520from%2520a%2520unified%2520perspective%252C%2520and%2520rethink%2520the%2520optimization%2520process%2520based%2520on%250Athree%2520distinct%2520objectives.%2520We%2520propose%2520%2522Harmonized%2520Transfer%2520Learning%2520and%250AModality%2520Alignment%2520%2528HarMA%2529%2522%252C%2520a%2520method%2520that%2520simultaneously%2520satisfies%2520task%250Aconstraints%252C%2520modality%2520alignment%252C%2520and%2520single-modality%2520uniform%2520alignment%252C%2520while%250Aminimizing%2520training%2520overhead%2520through%2520parameter-efficient%2520fine-tuning.%250ARemarkably%252C%2520without%2520the%2520need%2520for%2520external%2520data%2520for%2520training%252C%2520HarMA%2520achieves%250Astate-of-the-art%2520performance%2520in%2520two%2520popular%2520multimodal%2520retrieval%2520tasks%2520in%2520the%250Afield%2520of%2520remote%2520sensing.%2520Our%2520experiments%2520reveal%2520that%2520HarMA%2520achieves%2520competitive%250Aand%2520even%2520superior%2520performance%2520to%2520fully%2520fine-tuned%2520models%2520with%2520only%2520minimal%250Aadjustable%2520parameters.%2520Due%2520to%2520its%2520simplicity%252C%2520HarMA%2520can%2520be%2520integrated%2520into%250Aalmost%2520all%2520existing%2520multimodal%2520pretraining%2520models.%2520We%2520hope%2520this%2520method%2520can%250Afacilitate%2520the%2520efficient%2520application%2520of%2520large%2520models%2520to%2520a%2520wide%2520range%2520of%250Adownstream%2520tasks%2520while%2520significantly%2520reducing%2520the%2520resource%2520consumption.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/seekerhuang/HarMA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18253v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Remote%20Sensing%20with%20Harmonized%20Transfer%20Learning%20and%20Modality%0A%20%20Alignment&entry.906535625=Tengjun%20Huang&entry.1292438233=%20%20With%20the%20rise%20of%20Visual%20and%20Language%20Pretraining%20%28VLP%29%2C%20an%20increasing%20number%0Aof%20downstream%20tasks%20are%20adopting%20the%20paradigm%20of%20pretraining%20followed%20by%0Afine-tuning.%20Although%20this%20paradigm%20has%20demonstrated%20potential%20in%20various%0Amultimodal%20downstream%20tasks%2C%20its%20implementation%20in%20the%20remote%20sensing%20domain%0Aencounters%20some%20obstacles.%20Specifically%2C%20the%20tendency%20for%20same-modality%0Aembeddings%20to%20cluster%20together%20impedes%20efficient%20transfer%20learning.%20To%20tackle%0Athis%20issue%2C%20we%20review%20the%20aim%20of%20multimodal%20transfer%20learning%20for%20downstream%0Atasks%20from%20a%20unified%20perspective%2C%20and%20rethink%20the%20optimization%20process%20based%20on%0Athree%20distinct%20objectives.%20We%20propose%20%22Harmonized%20Transfer%20Learning%20and%0AModality%20Alignment%20%28HarMA%29%22%2C%20a%20method%20that%20simultaneously%20satisfies%20task%0Aconstraints%2C%20modality%20alignment%2C%20and%20single-modality%20uniform%20alignment%2C%20while%0Aminimizing%20training%20overhead%20through%20parameter-efficient%20fine-tuning.%0ARemarkably%2C%20without%20the%20need%20for%20external%20data%20for%20training%2C%20HarMA%20achieves%0Astate-of-the-art%20performance%20in%20two%20popular%20multimodal%20retrieval%20tasks%20in%20the%0Afield%20of%20remote%20sensing.%20Our%20experiments%20reveal%20that%20HarMA%20achieves%20competitive%0Aand%20even%20superior%20performance%20to%20fully%20fine-tuned%20models%20with%20only%20minimal%0Aadjustable%20parameters.%20Due%20to%20its%20simplicity%2C%20HarMA%20can%20be%20integrated%20into%0Aalmost%20all%20existing%20multimodal%20pretraining%20models.%20We%20hope%20this%20method%20can%0Afacilitate%20the%20efficient%20application%20of%20large%20models%20to%20a%20wide%20range%20of%0Adownstream%20tasks%20while%20significantly%20reducing%20the%20resource%20consumption.%20Code%20is%0Aavailable%20at%20https%3A//github.com/seekerhuang/HarMA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18253v5&entry.124074799=Read"},
{"title": "DynaHull: Density-centric Dynamic Point Filtering in Point Clouds", "author": "Pejman Habibiroudkenar and Risto Ojala and Kari Tammi", "abstract": "  In the field of indoor robotics, accurately localizing and mapping in dynamic\nenvironments using point clouds can be a challenging task due to the presence\nof dynamic points. These dynamic points are often represented by people in\nindoor environments, but in industrial settings with moving machinery, there\ncan be various types of dynamic points. This study introduces DynaHull, a novel\ntechnique designed to enhance indoor mapping accuracy by effectively removing\ndynamic points from point clouds. DynaHull works by leveraging the observation\nthat, over multiple scans, stationary points have a higher density compared to\ndynamic ones. Furthermore, DynaHull addresses mapping challenges related to\nunevenly distributed points by clustering the map into smaller sections. In\neach section, the density factor of each point is determined by dividing the\nnumber of neighbors by the volume these neighboring points occupy using a\nconvex hull method. The algorithm removes the dynamic points using an adaptive\nthreshold based on the point count of each cluster, thus reducing the false\npositives. The performance of DynaHull was compared to state-of-the-art\ntechniques, such as ERASOR, Removert, OctoMap plus SOR, and Dynablox, by\ncomparing each method to the ground truth map created during a low activity\nperiod in which only a few dynamic points were present. The results indicated\nthat DynaHull outperformed these techniques in various metrics, noticeably in\nthe Earth Mover's Distance, false negatives, and false positives.\n", "link": "http://arxiv.org/abs/2401.07541v2", "date": "2024-05-28", "relevancy": 2.7558, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5661}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5503}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaHull%3A%20Density-centric%20Dynamic%20Point%20Filtering%20in%20Point%20Clouds&body=Title%3A%20DynaHull%3A%20Density-centric%20Dynamic%20Point%20Filtering%20in%20Point%20Clouds%0AAuthor%3A%20Pejman%20Habibiroudkenar%20and%20Risto%20Ojala%20and%20Kari%20Tammi%0AAbstract%3A%20%20%20In%20the%20field%20of%20indoor%20robotics%2C%20accurately%20localizing%20and%20mapping%20in%20dynamic%0Aenvironments%20using%20point%20clouds%20can%20be%20a%20challenging%20task%20due%20to%20the%20presence%0Aof%20dynamic%20points.%20These%20dynamic%20points%20are%20often%20represented%20by%20people%20in%0Aindoor%20environments%2C%20but%20in%20industrial%20settings%20with%20moving%20machinery%2C%20there%0Acan%20be%20various%20types%20of%20dynamic%20points.%20This%20study%20introduces%20DynaHull%2C%20a%20novel%0Atechnique%20designed%20to%20enhance%20indoor%20mapping%20accuracy%20by%20effectively%20removing%0Adynamic%20points%20from%20point%20clouds.%20DynaHull%20works%20by%20leveraging%20the%20observation%0Athat%2C%20over%20multiple%20scans%2C%20stationary%20points%20have%20a%20higher%20density%20compared%20to%0Adynamic%20ones.%20Furthermore%2C%20DynaHull%20addresses%20mapping%20challenges%20related%20to%0Aunevenly%20distributed%20points%20by%20clustering%20the%20map%20into%20smaller%20sections.%20In%0Aeach%20section%2C%20the%20density%20factor%20of%20each%20point%20is%20determined%20by%20dividing%20the%0Anumber%20of%20neighbors%20by%20the%20volume%20these%20neighboring%20points%20occupy%20using%20a%0Aconvex%20hull%20method.%20The%20algorithm%20removes%20the%20dynamic%20points%20using%20an%20adaptive%0Athreshold%20based%20on%20the%20point%20count%20of%20each%20cluster%2C%20thus%20reducing%20the%20false%0Apositives.%20The%20performance%20of%20DynaHull%20was%20compared%20to%20state-of-the-art%0Atechniques%2C%20such%20as%20ERASOR%2C%20Removert%2C%20OctoMap%20plus%20SOR%2C%20and%20Dynablox%2C%20by%0Acomparing%20each%20method%20to%20the%20ground%20truth%20map%20created%20during%20a%20low%20activity%0Aperiod%20in%20which%20only%20a%20few%20dynamic%20points%20were%20present.%20The%20results%20indicated%0Athat%20DynaHull%20outperformed%20these%20techniques%20in%20various%20metrics%2C%20noticeably%20in%0Athe%20Earth%20Mover%27s%20Distance%2C%20false%20negatives%2C%20and%20false%20positives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07541v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaHull%253A%2520Density-centric%2520Dynamic%2520Point%2520Filtering%2520in%2520Point%2520Clouds%26entry.906535625%3DPejman%2520Habibiroudkenar%2520and%2520Risto%2520Ojala%2520and%2520Kari%2520Tammi%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520indoor%2520robotics%252C%2520accurately%2520localizing%2520and%2520mapping%2520in%2520dynamic%250Aenvironments%2520using%2520point%2520clouds%2520can%2520be%2520a%2520challenging%2520task%2520due%2520to%2520the%2520presence%250Aof%2520dynamic%2520points.%2520These%2520dynamic%2520points%2520are%2520often%2520represented%2520by%2520people%2520in%250Aindoor%2520environments%252C%2520but%2520in%2520industrial%2520settings%2520with%2520moving%2520machinery%252C%2520there%250Acan%2520be%2520various%2520types%2520of%2520dynamic%2520points.%2520This%2520study%2520introduces%2520DynaHull%252C%2520a%2520novel%250Atechnique%2520designed%2520to%2520enhance%2520indoor%2520mapping%2520accuracy%2520by%2520effectively%2520removing%250Adynamic%2520points%2520from%2520point%2520clouds.%2520DynaHull%2520works%2520by%2520leveraging%2520the%2520observation%250Athat%252C%2520over%2520multiple%2520scans%252C%2520stationary%2520points%2520have%2520a%2520higher%2520density%2520compared%2520to%250Adynamic%2520ones.%2520Furthermore%252C%2520DynaHull%2520addresses%2520mapping%2520challenges%2520related%2520to%250Aunevenly%2520distributed%2520points%2520by%2520clustering%2520the%2520map%2520into%2520smaller%2520sections.%2520In%250Aeach%2520section%252C%2520the%2520density%2520factor%2520of%2520each%2520point%2520is%2520determined%2520by%2520dividing%2520the%250Anumber%2520of%2520neighbors%2520by%2520the%2520volume%2520these%2520neighboring%2520points%2520occupy%2520using%2520a%250Aconvex%2520hull%2520method.%2520The%2520algorithm%2520removes%2520the%2520dynamic%2520points%2520using%2520an%2520adaptive%250Athreshold%2520based%2520on%2520the%2520point%2520count%2520of%2520each%2520cluster%252C%2520thus%2520reducing%2520the%2520false%250Apositives.%2520The%2520performance%2520of%2520DynaHull%2520was%2520compared%2520to%2520state-of-the-art%250Atechniques%252C%2520such%2520as%2520ERASOR%252C%2520Removert%252C%2520OctoMap%2520plus%2520SOR%252C%2520and%2520Dynablox%252C%2520by%250Acomparing%2520each%2520method%2520to%2520the%2520ground%2520truth%2520map%2520created%2520during%2520a%2520low%2520activity%250Aperiod%2520in%2520which%2520only%2520a%2520few%2520dynamic%2520points%2520were%2520present.%2520The%2520results%2520indicated%250Athat%2520DynaHull%2520outperformed%2520these%2520techniques%2520in%2520various%2520metrics%252C%2520noticeably%2520in%250Athe%2520Earth%2520Mover%2527s%2520Distance%252C%2520false%2520negatives%252C%2520and%2520false%2520positives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07541v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaHull%3A%20Density-centric%20Dynamic%20Point%20Filtering%20in%20Point%20Clouds&entry.906535625=Pejman%20Habibiroudkenar%20and%20Risto%20Ojala%20and%20Kari%20Tammi&entry.1292438233=%20%20In%20the%20field%20of%20indoor%20robotics%2C%20accurately%20localizing%20and%20mapping%20in%20dynamic%0Aenvironments%20using%20point%20clouds%20can%20be%20a%20challenging%20task%20due%20to%20the%20presence%0Aof%20dynamic%20points.%20These%20dynamic%20points%20are%20often%20represented%20by%20people%20in%0Aindoor%20environments%2C%20but%20in%20industrial%20settings%20with%20moving%20machinery%2C%20there%0Acan%20be%20various%20types%20of%20dynamic%20points.%20This%20study%20introduces%20DynaHull%2C%20a%20novel%0Atechnique%20designed%20to%20enhance%20indoor%20mapping%20accuracy%20by%20effectively%20removing%0Adynamic%20points%20from%20point%20clouds.%20DynaHull%20works%20by%20leveraging%20the%20observation%0Athat%2C%20over%20multiple%20scans%2C%20stationary%20points%20have%20a%20higher%20density%20compared%20to%0Adynamic%20ones.%20Furthermore%2C%20DynaHull%20addresses%20mapping%20challenges%20related%20to%0Aunevenly%20distributed%20points%20by%20clustering%20the%20map%20into%20smaller%20sections.%20In%0Aeach%20section%2C%20the%20density%20factor%20of%20each%20point%20is%20determined%20by%20dividing%20the%0Anumber%20of%20neighbors%20by%20the%20volume%20these%20neighboring%20points%20occupy%20using%20a%0Aconvex%20hull%20method.%20The%20algorithm%20removes%20the%20dynamic%20points%20using%20an%20adaptive%0Athreshold%20based%20on%20the%20point%20count%20of%20each%20cluster%2C%20thus%20reducing%20the%20false%0Apositives.%20The%20performance%20of%20DynaHull%20was%20compared%20to%20state-of-the-art%0Atechniques%2C%20such%20as%20ERASOR%2C%20Removert%2C%20OctoMap%20plus%20SOR%2C%20and%20Dynablox%2C%20by%0Acomparing%20each%20method%20to%20the%20ground%20truth%20map%20created%20during%20a%20low%20activity%0Aperiod%20in%20which%20only%20a%20few%20dynamic%20points%20were%20present.%20The%20results%20indicated%0Athat%20DynaHull%20outperformed%20these%20techniques%20in%20various%20metrics%2C%20noticeably%20in%0Athe%20Earth%20Mover%27s%20Distance%2C%20false%20negatives%2C%20and%20false%20positives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07541v2&entry.124074799=Read"},
{"title": "VividPose: Advancing Stable Video Diffusion for Realistic Human Image\n  Animation", "author": "Qilin Wang and Zhengkai Jiang and Chengming Xu and Jiangning Zhang and Yabiao Wang and Xinyi Zhang and Yun Cao and Weijian Cao and Chengjie Wang and Yanwei Fu", "abstract": "  Human image animation involves generating a video from a static image by\nfollowing a specified pose sequence. Current approaches typically adopt a\nmulti-stage pipeline that separately learns appearance and motion, which often\nleads to appearance degradation and temporal inconsistencies. To address these\nissues, we propose VividPose, an innovative end-to-end pipeline based on Stable\nVideo Diffusion (SVD) that ensures superior temporal stability. To enhance the\nretention of human identity, we propose an identity-aware appearance controller\nthat integrates additional facial information without compromising other\nappearance details such as clothing texture and background. This approach\nensures that the generated videos maintain high fidelity to the identity of\nhuman subject, preserving key facial features across various poses. To\naccommodate diverse human body shapes and hand movements, we introduce a\ngeometry-aware pose controller that utilizes both dense rendering maps from\nSMPL-X and sparse skeleton maps. This enables accurate alignment of pose and\nshape in the generated videos, providing a robust framework capable of handling\na wide range of body shapes and dynamic hand movements. Extensive qualitative\nand quantitative experiments on the UBCFashion and TikTok benchmarks\ndemonstrate that our method achieves state-of-the-art performance. Furthermore,\nVividPose exhibits superior generalization capabilities on our proposed\nin-the-wild dataset. Codes and models will be available.\n", "link": "http://arxiv.org/abs/2405.18156v1", "date": "2024-05-28", "relevancy": 2.7184, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.7075}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6631}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VividPose%3A%20Advancing%20Stable%20Video%20Diffusion%20for%20Realistic%20Human%20Image%0A%20%20Animation&body=Title%3A%20VividPose%3A%20Advancing%20Stable%20Video%20Diffusion%20for%20Realistic%20Human%20Image%0A%20%20Animation%0AAuthor%3A%20Qilin%20Wang%20and%20Zhengkai%20Jiang%20and%20Chengming%20Xu%20and%20Jiangning%20Zhang%20and%20Yabiao%20Wang%20and%20Xinyi%20Zhang%20and%20Yun%20Cao%20and%20Weijian%20Cao%20and%20Chengjie%20Wang%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20Human%20image%20animation%20involves%20generating%20a%20video%20from%20a%20static%20image%20by%0Afollowing%20a%20specified%20pose%20sequence.%20Current%20approaches%20typically%20adopt%20a%0Amulti-stage%20pipeline%20that%20separately%20learns%20appearance%20and%20motion%2C%20which%20often%0Aleads%20to%20appearance%20degradation%20and%20temporal%20inconsistencies.%20To%20address%20these%0Aissues%2C%20we%20propose%20VividPose%2C%20an%20innovative%20end-to-end%20pipeline%20based%20on%20Stable%0AVideo%20Diffusion%20%28SVD%29%20that%20ensures%20superior%20temporal%20stability.%20To%20enhance%20the%0Aretention%20of%20human%20identity%2C%20we%20propose%20an%20identity-aware%20appearance%20controller%0Athat%20integrates%20additional%20facial%20information%20without%20compromising%20other%0Aappearance%20details%20such%20as%20clothing%20texture%20and%20background.%20This%20approach%0Aensures%20that%20the%20generated%20videos%20maintain%20high%20fidelity%20to%20the%20identity%20of%0Ahuman%20subject%2C%20preserving%20key%20facial%20features%20across%20various%20poses.%20To%0Aaccommodate%20diverse%20human%20body%20shapes%20and%20hand%20movements%2C%20we%20introduce%20a%0Ageometry-aware%20pose%20controller%20that%20utilizes%20both%20dense%20rendering%20maps%20from%0ASMPL-X%20and%20sparse%20skeleton%20maps.%20This%20enables%20accurate%20alignment%20of%20pose%20and%0Ashape%20in%20the%20generated%20videos%2C%20providing%20a%20robust%20framework%20capable%20of%20handling%0Aa%20wide%20range%20of%20body%20shapes%20and%20dynamic%20hand%20movements.%20Extensive%20qualitative%0Aand%20quantitative%20experiments%20on%20the%20UBCFashion%20and%20TikTok%20benchmarks%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.%20Furthermore%2C%0AVividPose%20exhibits%20superior%20generalization%20capabilities%20on%20our%20proposed%0Ain-the-wild%20dataset.%20Codes%20and%20models%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVividPose%253A%2520Advancing%2520Stable%2520Video%2520Diffusion%2520for%2520Realistic%2520Human%2520Image%250A%2520%2520Animation%26entry.906535625%3DQilin%2520Wang%2520and%2520Zhengkai%2520Jiang%2520and%2520Chengming%2520Xu%2520and%2520Jiangning%2520Zhang%2520and%2520Yabiao%2520Wang%2520and%2520Xinyi%2520Zhang%2520and%2520Yun%2520Cao%2520and%2520Weijian%2520Cao%2520and%2520Chengjie%2520Wang%2520and%2520Yanwei%2520Fu%26entry.1292438233%3D%2520%2520Human%2520image%2520animation%2520involves%2520generating%2520a%2520video%2520from%2520a%2520static%2520image%2520by%250Afollowing%2520a%2520specified%2520pose%2520sequence.%2520Current%2520approaches%2520typically%2520adopt%2520a%250Amulti-stage%2520pipeline%2520that%2520separately%2520learns%2520appearance%2520and%2520motion%252C%2520which%2520often%250Aleads%2520to%2520appearance%2520degradation%2520and%2520temporal%2520inconsistencies.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520VividPose%252C%2520an%2520innovative%2520end-to-end%2520pipeline%2520based%2520on%2520Stable%250AVideo%2520Diffusion%2520%2528SVD%2529%2520that%2520ensures%2520superior%2520temporal%2520stability.%2520To%2520enhance%2520the%250Aretention%2520of%2520human%2520identity%252C%2520we%2520propose%2520an%2520identity-aware%2520appearance%2520controller%250Athat%2520integrates%2520additional%2520facial%2520information%2520without%2520compromising%2520other%250Aappearance%2520details%2520such%2520as%2520clothing%2520texture%2520and%2520background.%2520This%2520approach%250Aensures%2520that%2520the%2520generated%2520videos%2520maintain%2520high%2520fidelity%2520to%2520the%2520identity%2520of%250Ahuman%2520subject%252C%2520preserving%2520key%2520facial%2520features%2520across%2520various%2520poses.%2520To%250Aaccommodate%2520diverse%2520human%2520body%2520shapes%2520and%2520hand%2520movements%252C%2520we%2520introduce%2520a%250Ageometry-aware%2520pose%2520controller%2520that%2520utilizes%2520both%2520dense%2520rendering%2520maps%2520from%250ASMPL-X%2520and%2520sparse%2520skeleton%2520maps.%2520This%2520enables%2520accurate%2520alignment%2520of%2520pose%2520and%250Ashape%2520in%2520the%2520generated%2520videos%252C%2520providing%2520a%2520robust%2520framework%2520capable%2520of%2520handling%250Aa%2520wide%2520range%2520of%2520body%2520shapes%2520and%2520dynamic%2520hand%2520movements.%2520Extensive%2520qualitative%250Aand%2520quantitative%2520experiments%2520on%2520the%2520UBCFashion%2520and%2520TikTok%2520benchmarks%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance.%2520Furthermore%252C%250AVividPose%2520exhibits%2520superior%2520generalization%2520capabilities%2520on%2520our%2520proposed%250Ain-the-wild%2520dataset.%2520Codes%2520and%2520models%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VividPose%3A%20Advancing%20Stable%20Video%20Diffusion%20for%20Realistic%20Human%20Image%0A%20%20Animation&entry.906535625=Qilin%20Wang%20and%20Zhengkai%20Jiang%20and%20Chengming%20Xu%20and%20Jiangning%20Zhang%20and%20Yabiao%20Wang%20and%20Xinyi%20Zhang%20and%20Yun%20Cao%20and%20Weijian%20Cao%20and%20Chengjie%20Wang%20and%20Yanwei%20Fu&entry.1292438233=%20%20Human%20image%20animation%20involves%20generating%20a%20video%20from%20a%20static%20image%20by%0Afollowing%20a%20specified%20pose%20sequence.%20Current%20approaches%20typically%20adopt%20a%0Amulti-stage%20pipeline%20that%20separately%20learns%20appearance%20and%20motion%2C%20which%20often%0Aleads%20to%20appearance%20degradation%20and%20temporal%20inconsistencies.%20To%20address%20these%0Aissues%2C%20we%20propose%20VividPose%2C%20an%20innovative%20end-to-end%20pipeline%20based%20on%20Stable%0AVideo%20Diffusion%20%28SVD%29%20that%20ensures%20superior%20temporal%20stability.%20To%20enhance%20the%0Aretention%20of%20human%20identity%2C%20we%20propose%20an%20identity-aware%20appearance%20controller%0Athat%20integrates%20additional%20facial%20information%20without%20compromising%20other%0Aappearance%20details%20such%20as%20clothing%20texture%20and%20background.%20This%20approach%0Aensures%20that%20the%20generated%20videos%20maintain%20high%20fidelity%20to%20the%20identity%20of%0Ahuman%20subject%2C%20preserving%20key%20facial%20features%20across%20various%20poses.%20To%0Aaccommodate%20diverse%20human%20body%20shapes%20and%20hand%20movements%2C%20we%20introduce%20a%0Ageometry-aware%20pose%20controller%20that%20utilizes%20both%20dense%20rendering%20maps%20from%0ASMPL-X%20and%20sparse%20skeleton%20maps.%20This%20enables%20accurate%20alignment%20of%20pose%20and%0Ashape%20in%20the%20generated%20videos%2C%20providing%20a%20robust%20framework%20capable%20of%20handling%0Aa%20wide%20range%20of%20body%20shapes%20and%20dynamic%20hand%20movements.%20Extensive%20qualitative%0Aand%20quantitative%20experiments%20on%20the%20UBCFashion%20and%20TikTok%20benchmarks%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.%20Furthermore%2C%0AVividPose%20exhibits%20superior%20generalization%20capabilities%20on%20our%20proposed%0Ain-the-wild%20dataset.%20Codes%20and%20models%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18156v1&entry.124074799=Read"},
{"title": "SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder\n  for Self-Supervised Landmark Estimation", "author": "Kejia Yin and Varshanth R. Rao and Ruowei Jiang and Xudong Liu and Parham Aarabi and David B. Lindell", "abstract": "  Self-supervised landmark estimation is a challenging task that demands the\nformation of locally distinct feature representations to identify sparse facial\nlandmarks in the absence of annotated data. To tackle this task, existing\nstate-of-the-art (SOTA) methods (1) extract coarse features from backbones that\nare trained with instance-level self-supervised learning (SSL) paradigms, which\nneglect the dense prediction nature of the task, (2) aggregate them into\nmemory-intensive hypercolumn formations, and (3) supervise lightweight\nprojector networks to naively establish full local correspondences among all\npairs of spatial features. In this paper, we introduce SCE-MAE, a framework\nthat (1) leverages the MAE, a region-level SSL method that naturally better\nsuits the landmark prediction task, (2) operates on the vanilla feature map\ninstead of on expensive hypercolumns, and (3) employs a Correspondence\nApproximation and Refinement Block (CARB) that utilizes a simple density peak\nclustering algorithm and our proposed Locality-Constrained Repellence Loss to\ndirectly hone only select local correspondences. We demonstrate through\nextensive experiments that SCE-MAE is highly effective and robust,\noutperforming existing SOTA methods by large margins of approximately 20%-44%\non the landmark matching and approximately 9%-15% on the landmark detection\ntasks.\n", "link": "http://arxiv.org/abs/2405.18322v1", "date": "2024-05-28", "relevancy": 2.6999, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5515}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5399}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCE-MAE%3A%20Selective%20Correspondence%20Enhancement%20with%20Masked%20Autoencoder%0A%20%20for%20Self-Supervised%20Landmark%20Estimation&body=Title%3A%20SCE-MAE%3A%20Selective%20Correspondence%20Enhancement%20with%20Masked%20Autoencoder%0A%20%20for%20Self-Supervised%20Landmark%20Estimation%0AAuthor%3A%20Kejia%20Yin%20and%20Varshanth%20R.%20Rao%20and%20Ruowei%20Jiang%20and%20Xudong%20Liu%20and%20Parham%20Aarabi%20and%20David%20B.%20Lindell%0AAbstract%3A%20%20%20Self-supervised%20landmark%20estimation%20is%20a%20challenging%20task%20that%20demands%20the%0Aformation%20of%20locally%20distinct%20feature%20representations%20to%20identify%20sparse%20facial%0Alandmarks%20in%20the%20absence%20of%20annotated%20data.%20To%20tackle%20this%20task%2C%20existing%0Astate-of-the-art%20%28SOTA%29%20methods%20%281%29%20extract%20coarse%20features%20from%20backbones%20that%0Aare%20trained%20with%20instance-level%20self-supervised%20learning%20%28SSL%29%20paradigms%2C%20which%0Aneglect%20the%20dense%20prediction%20nature%20of%20the%20task%2C%20%282%29%20aggregate%20them%20into%0Amemory-intensive%20hypercolumn%20formations%2C%20and%20%283%29%20supervise%20lightweight%0Aprojector%20networks%20to%20naively%20establish%20full%20local%20correspondences%20among%20all%0Apairs%20of%20spatial%20features.%20In%20this%20paper%2C%20we%20introduce%20SCE-MAE%2C%20a%20framework%0Athat%20%281%29%20leverages%20the%20MAE%2C%20a%20region-level%20SSL%20method%20that%20naturally%20better%0Asuits%20the%20landmark%20prediction%20task%2C%20%282%29%20operates%20on%20the%20vanilla%20feature%20map%0Ainstead%20of%20on%20expensive%20hypercolumns%2C%20and%20%283%29%20employs%20a%20Correspondence%0AApproximation%20and%20Refinement%20Block%20%28CARB%29%20that%20utilizes%20a%20simple%20density%20peak%0Aclustering%20algorithm%20and%20our%20proposed%20Locality-Constrained%20Repellence%20Loss%20to%0Adirectly%20hone%20only%20select%20local%20correspondences.%20We%20demonstrate%20through%0Aextensive%20experiments%20that%20SCE-MAE%20is%20highly%20effective%20and%20robust%2C%0Aoutperforming%20existing%20SOTA%20methods%20by%20large%20margins%20of%20approximately%2020%25-44%25%0Aon%20the%20landmark%20matching%20and%20approximately%209%25-15%25%20on%20the%20landmark%20detection%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCE-MAE%253A%2520Selective%2520Correspondence%2520Enhancement%2520with%2520Masked%2520Autoencoder%250A%2520%2520for%2520Self-Supervised%2520Landmark%2520Estimation%26entry.906535625%3DKejia%2520Yin%2520and%2520Varshanth%2520R.%2520Rao%2520and%2520Ruowei%2520Jiang%2520and%2520Xudong%2520Liu%2520and%2520Parham%2520Aarabi%2520and%2520David%2520B.%2520Lindell%26entry.1292438233%3D%2520%2520Self-supervised%2520landmark%2520estimation%2520is%2520a%2520challenging%2520task%2520that%2520demands%2520the%250Aformation%2520of%2520locally%2520distinct%2520feature%2520representations%2520to%2520identify%2520sparse%2520facial%250Alandmarks%2520in%2520the%2520absence%2520of%2520annotated%2520data.%2520To%2520tackle%2520this%2520task%252C%2520existing%250Astate-of-the-art%2520%2528SOTA%2529%2520methods%2520%25281%2529%2520extract%2520coarse%2520features%2520from%2520backbones%2520that%250Aare%2520trained%2520with%2520instance-level%2520self-supervised%2520learning%2520%2528SSL%2529%2520paradigms%252C%2520which%250Aneglect%2520the%2520dense%2520prediction%2520nature%2520of%2520the%2520task%252C%2520%25282%2529%2520aggregate%2520them%2520into%250Amemory-intensive%2520hypercolumn%2520formations%252C%2520and%2520%25283%2529%2520supervise%2520lightweight%250Aprojector%2520networks%2520to%2520naively%2520establish%2520full%2520local%2520correspondences%2520among%2520all%250Apairs%2520of%2520spatial%2520features.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SCE-MAE%252C%2520a%2520framework%250Athat%2520%25281%2529%2520leverages%2520the%2520MAE%252C%2520a%2520region-level%2520SSL%2520method%2520that%2520naturally%2520better%250Asuits%2520the%2520landmark%2520prediction%2520task%252C%2520%25282%2529%2520operates%2520on%2520the%2520vanilla%2520feature%2520map%250Ainstead%2520of%2520on%2520expensive%2520hypercolumns%252C%2520and%2520%25283%2529%2520employs%2520a%2520Correspondence%250AApproximation%2520and%2520Refinement%2520Block%2520%2528CARB%2529%2520that%2520utilizes%2520a%2520simple%2520density%2520peak%250Aclustering%2520algorithm%2520and%2520our%2520proposed%2520Locality-Constrained%2520Repellence%2520Loss%2520to%250Adirectly%2520hone%2520only%2520select%2520local%2520correspondences.%2520We%2520demonstrate%2520through%250Aextensive%2520experiments%2520that%2520SCE-MAE%2520is%2520highly%2520effective%2520and%2520robust%252C%250Aoutperforming%2520existing%2520SOTA%2520methods%2520by%2520large%2520margins%2520of%2520approximately%252020%2525-44%2525%250Aon%2520the%2520landmark%2520matching%2520and%2520approximately%25209%2525-15%2525%2520on%2520the%2520landmark%2520detection%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCE-MAE%3A%20Selective%20Correspondence%20Enhancement%20with%20Masked%20Autoencoder%0A%20%20for%20Self-Supervised%20Landmark%20Estimation&entry.906535625=Kejia%20Yin%20and%20Varshanth%20R.%20Rao%20and%20Ruowei%20Jiang%20and%20Xudong%20Liu%20and%20Parham%20Aarabi%20and%20David%20B.%20Lindell&entry.1292438233=%20%20Self-supervised%20landmark%20estimation%20is%20a%20challenging%20task%20that%20demands%20the%0Aformation%20of%20locally%20distinct%20feature%20representations%20to%20identify%20sparse%20facial%0Alandmarks%20in%20the%20absence%20of%20annotated%20data.%20To%20tackle%20this%20task%2C%20existing%0Astate-of-the-art%20%28SOTA%29%20methods%20%281%29%20extract%20coarse%20features%20from%20backbones%20that%0Aare%20trained%20with%20instance-level%20self-supervised%20learning%20%28SSL%29%20paradigms%2C%20which%0Aneglect%20the%20dense%20prediction%20nature%20of%20the%20task%2C%20%282%29%20aggregate%20them%20into%0Amemory-intensive%20hypercolumn%20formations%2C%20and%20%283%29%20supervise%20lightweight%0Aprojector%20networks%20to%20naively%20establish%20full%20local%20correspondences%20among%20all%0Apairs%20of%20spatial%20features.%20In%20this%20paper%2C%20we%20introduce%20SCE-MAE%2C%20a%20framework%0Athat%20%281%29%20leverages%20the%20MAE%2C%20a%20region-level%20SSL%20method%20that%20naturally%20better%0Asuits%20the%20landmark%20prediction%20task%2C%20%282%29%20operates%20on%20the%20vanilla%20feature%20map%0Ainstead%20of%20on%20expensive%20hypercolumns%2C%20and%20%283%29%20employs%20a%20Correspondence%0AApproximation%20and%20Refinement%20Block%20%28CARB%29%20that%20utilizes%20a%20simple%20density%20peak%0Aclustering%20algorithm%20and%20our%20proposed%20Locality-Constrained%20Repellence%20Loss%20to%0Adirectly%20hone%20only%20select%20local%20correspondences.%20We%20demonstrate%20through%0Aextensive%20experiments%20that%20SCE-MAE%20is%20highly%20effective%20and%20robust%2C%0Aoutperforming%20existing%20SOTA%20methods%20by%20large%20margins%20of%20approximately%2020%25-44%25%0Aon%20the%20landmark%20matching%20and%20approximately%209%25-15%25%20on%20the%20landmark%20detection%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18322v1&entry.124074799=Read"},
{"title": "CHGNN: A Semi-Supervised Contrastive Hypergraph Learning Network", "author": "Yumeng Song and Yu Gu and Tianyi Li and Jianzhong Qi and Zhenghao Liu and Christian S. Jensen and Ge Yu", "abstract": "  Hypergraphs can model higher-order relationships among data objects that are\nfound in applications such as social networks and bioinformatics. However,\nrecent studies on hypergraph learning that extend graph convolutional networks\nto hypergraphs cannot learn effectively from features of unlabeled data. To\nsuch learning, we propose a contrastive hypergraph neural network, CHGNN, that\nexploits self-supervised contrastive learning techniques to learn from labeled\nand unlabeled data. First, CHGNN includes an adaptive hypergraph view generator\nthat adopts an auto-augmentation strategy and learns a perturbed probability\ndistribution of minimal sufficient views. Second, CHGNN encompasses an improved\nhypergraph encoder that considers hyperedge homogeneity to fuse information\neffectively. Third, CHGNN is equipped with a joint loss function that combines\na similarity loss for the view generator, a node classification loss, and a\nhyperedge homogeneity loss to inject supervision signals. It also includes\nbasic and cross-validation contrastive losses, associated with an enhanced\ncontrastive loss training process. Experimental results on nine real datasets\noffer insight into the effectiveness of CHGNN, showing that it outperforms 13\ncompetitors in terms of classification accuracy consistently.\n", "link": "http://arxiv.org/abs/2303.06213v2", "date": "2024-05-28", "relevancy": 2.678, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5542}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5511}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHGNN%3A%20A%20Semi-Supervised%20Contrastive%20Hypergraph%20Learning%20Network&body=Title%3A%20CHGNN%3A%20A%20Semi-Supervised%20Contrastive%20Hypergraph%20Learning%20Network%0AAuthor%3A%20Yumeng%20Song%20and%20Yu%20Gu%20and%20Tianyi%20Li%20and%20Jianzhong%20Qi%20and%20Zhenghao%20Liu%20and%20Christian%20S.%20Jensen%20and%20Ge%20Yu%0AAbstract%3A%20%20%20Hypergraphs%20can%20model%20higher-order%20relationships%20among%20data%20objects%20that%20are%0Afound%20in%20applications%20such%20as%20social%20networks%20and%20bioinformatics.%20However%2C%0Arecent%20studies%20on%20hypergraph%20learning%20that%20extend%20graph%20convolutional%20networks%0Ato%20hypergraphs%20cannot%20learn%20effectively%20from%20features%20of%20unlabeled%20data.%20To%0Asuch%20learning%2C%20we%20propose%20a%20contrastive%20hypergraph%20neural%20network%2C%20CHGNN%2C%20that%0Aexploits%20self-supervised%20contrastive%20learning%20techniques%20to%20learn%20from%20labeled%0Aand%20unlabeled%20data.%20First%2C%20CHGNN%20includes%20an%20adaptive%20hypergraph%20view%20generator%0Athat%20adopts%20an%20auto-augmentation%20strategy%20and%20learns%20a%20perturbed%20probability%0Adistribution%20of%20minimal%20sufficient%20views.%20Second%2C%20CHGNN%20encompasses%20an%20improved%0Ahypergraph%20encoder%20that%20considers%20hyperedge%20homogeneity%20to%20fuse%20information%0Aeffectively.%20Third%2C%20CHGNN%20is%20equipped%20with%20a%20joint%20loss%20function%20that%20combines%0Aa%20similarity%20loss%20for%20the%20view%20generator%2C%20a%20node%20classification%20loss%2C%20and%20a%0Ahyperedge%20homogeneity%20loss%20to%20inject%20supervision%20signals.%20It%20also%20includes%0Abasic%20and%20cross-validation%20contrastive%20losses%2C%20associated%20with%20an%20enhanced%0Acontrastive%20loss%20training%20process.%20Experimental%20results%20on%20nine%20real%20datasets%0Aoffer%20insight%20into%20the%20effectiveness%20of%20CHGNN%2C%20showing%20that%20it%20outperforms%2013%0Acompetitors%20in%20terms%20of%20classification%20accuracy%20consistently.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06213v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHGNN%253A%2520A%2520Semi-Supervised%2520Contrastive%2520Hypergraph%2520Learning%2520Network%26entry.906535625%3DYumeng%2520Song%2520and%2520Yu%2520Gu%2520and%2520Tianyi%2520Li%2520and%2520Jianzhong%2520Qi%2520and%2520Zhenghao%2520Liu%2520and%2520Christian%2520S.%2520Jensen%2520and%2520Ge%2520Yu%26entry.1292438233%3D%2520%2520Hypergraphs%2520can%2520model%2520higher-order%2520relationships%2520among%2520data%2520objects%2520that%2520are%250Afound%2520in%2520applications%2520such%2520as%2520social%2520networks%2520and%2520bioinformatics.%2520However%252C%250Arecent%2520studies%2520on%2520hypergraph%2520learning%2520that%2520extend%2520graph%2520convolutional%2520networks%250Ato%2520hypergraphs%2520cannot%2520learn%2520effectively%2520from%2520features%2520of%2520unlabeled%2520data.%2520To%250Asuch%2520learning%252C%2520we%2520propose%2520a%2520contrastive%2520hypergraph%2520neural%2520network%252C%2520CHGNN%252C%2520that%250Aexploits%2520self-supervised%2520contrastive%2520learning%2520techniques%2520to%2520learn%2520from%2520labeled%250Aand%2520unlabeled%2520data.%2520First%252C%2520CHGNN%2520includes%2520an%2520adaptive%2520hypergraph%2520view%2520generator%250Athat%2520adopts%2520an%2520auto-augmentation%2520strategy%2520and%2520learns%2520a%2520perturbed%2520probability%250Adistribution%2520of%2520minimal%2520sufficient%2520views.%2520Second%252C%2520CHGNN%2520encompasses%2520an%2520improved%250Ahypergraph%2520encoder%2520that%2520considers%2520hyperedge%2520homogeneity%2520to%2520fuse%2520information%250Aeffectively.%2520Third%252C%2520CHGNN%2520is%2520equipped%2520with%2520a%2520joint%2520loss%2520function%2520that%2520combines%250Aa%2520similarity%2520loss%2520for%2520the%2520view%2520generator%252C%2520a%2520node%2520classification%2520loss%252C%2520and%2520a%250Ahyperedge%2520homogeneity%2520loss%2520to%2520inject%2520supervision%2520signals.%2520It%2520also%2520includes%250Abasic%2520and%2520cross-validation%2520contrastive%2520losses%252C%2520associated%2520with%2520an%2520enhanced%250Acontrastive%2520loss%2520training%2520process.%2520Experimental%2520results%2520on%2520nine%2520real%2520datasets%250Aoffer%2520insight%2520into%2520the%2520effectiveness%2520of%2520CHGNN%252C%2520showing%2520that%2520it%2520outperforms%252013%250Acompetitors%2520in%2520terms%2520of%2520classification%2520accuracy%2520consistently.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.06213v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHGNN%3A%20A%20Semi-Supervised%20Contrastive%20Hypergraph%20Learning%20Network&entry.906535625=Yumeng%20Song%20and%20Yu%20Gu%20and%20Tianyi%20Li%20and%20Jianzhong%20Qi%20and%20Zhenghao%20Liu%20and%20Christian%20S.%20Jensen%20and%20Ge%20Yu&entry.1292438233=%20%20Hypergraphs%20can%20model%20higher-order%20relationships%20among%20data%20objects%20that%20are%0Afound%20in%20applications%20such%20as%20social%20networks%20and%20bioinformatics.%20However%2C%0Arecent%20studies%20on%20hypergraph%20learning%20that%20extend%20graph%20convolutional%20networks%0Ato%20hypergraphs%20cannot%20learn%20effectively%20from%20features%20of%20unlabeled%20data.%20To%0Asuch%20learning%2C%20we%20propose%20a%20contrastive%20hypergraph%20neural%20network%2C%20CHGNN%2C%20that%0Aexploits%20self-supervised%20contrastive%20learning%20techniques%20to%20learn%20from%20labeled%0Aand%20unlabeled%20data.%20First%2C%20CHGNN%20includes%20an%20adaptive%20hypergraph%20view%20generator%0Athat%20adopts%20an%20auto-augmentation%20strategy%20and%20learns%20a%20perturbed%20probability%0Adistribution%20of%20minimal%20sufficient%20views.%20Second%2C%20CHGNN%20encompasses%20an%20improved%0Ahypergraph%20encoder%20that%20considers%20hyperedge%20homogeneity%20to%20fuse%20information%0Aeffectively.%20Third%2C%20CHGNN%20is%20equipped%20with%20a%20joint%20loss%20function%20that%20combines%0Aa%20similarity%20loss%20for%20the%20view%20generator%2C%20a%20node%20classification%20loss%2C%20and%20a%0Ahyperedge%20homogeneity%20loss%20to%20inject%20supervision%20signals.%20It%20also%20includes%0Abasic%20and%20cross-validation%20contrastive%20losses%2C%20associated%20with%20an%20enhanced%0Acontrastive%20loss%20training%20process.%20Experimental%20results%20on%20nine%20real%20datasets%0Aoffer%20insight%20into%20the%20effectiveness%20of%20CHGNN%2C%20showing%20that%20it%20outperforms%2013%0Acompetitors%20in%20terms%20of%20classification%20accuracy%20consistently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06213v2&entry.124074799=Read"},
{"title": "SSLChange: A Self-supervised Change Detection Framework Based on Domain\n  Adaptation", "author": "Yitao Zhao and Turgay Celik and Nanqing Liu and Feng Gao and Heng-Chao Li", "abstract": "  In conventional remote sensing change detection (RS CD) procedures, extensive\nmanual labeling for bi-temporal images is first required to maintain the\nperformance of subsequent fully supervised training. However, pixel-level\nlabeling for CD tasks is very complex and time-consuming. In this paper, we\nexplore a novel self-supervised contrastive framework applicable to the RS CD\ntask, which promotes the model to accurately capture spatial, structural, and\nsemantic information through domain adapter and hierarchical contrastive head.\nThe proposed SSLChange framework accomplishes self-learning only by taking a\nsingle-temporal sample and can be flexibly transferred to main-stream CD\nbaselines. With self-supervised contrastive learning, feature representation\npre-training can be performed directly based on the original data even without\nlabeling. After a certain amount of labels are subsequently obtained, the\npre-trained features will be aligned with the labels for fully supervised\nfine-tuning. Without introducing any additional data or labels, the performance\nof downstream baselines will experience a significant enhancement. Experimental\nresults on 2 entire datasets and 6 diluted datasets show that our proposed\nSSLChange improves the performance and stability of CD baseline in data-limited\nsituations. The code of SSLChange will be released at\n\\url{https://github.com/MarsZhaoYT/SSLChange}\n", "link": "http://arxiv.org/abs/2405.18224v1", "date": "2024-05-28", "relevancy": 2.6654, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5397}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5372}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSLChange%3A%20A%20Self-supervised%20Change%20Detection%20Framework%20Based%20on%20Domain%0A%20%20Adaptation&body=Title%3A%20SSLChange%3A%20A%20Self-supervised%20Change%20Detection%20Framework%20Based%20on%20Domain%0A%20%20Adaptation%0AAuthor%3A%20Yitao%20Zhao%20and%20Turgay%20Celik%20and%20Nanqing%20Liu%20and%20Feng%20Gao%20and%20Heng-Chao%20Li%0AAbstract%3A%20%20%20In%20conventional%20remote%20sensing%20change%20detection%20%28RS%20CD%29%20procedures%2C%20extensive%0Amanual%20labeling%20for%20bi-temporal%20images%20is%20first%20required%20to%20maintain%20the%0Aperformance%20of%20subsequent%20fully%20supervised%20training.%20However%2C%20pixel-level%0Alabeling%20for%20CD%20tasks%20is%20very%20complex%20and%20time-consuming.%20In%20this%20paper%2C%20we%0Aexplore%20a%20novel%20self-supervised%20contrastive%20framework%20applicable%20to%20the%20RS%20CD%0Atask%2C%20which%20promotes%20the%20model%20to%20accurately%20capture%20spatial%2C%20structural%2C%20and%0Asemantic%20information%20through%20domain%20adapter%20and%20hierarchical%20contrastive%20head.%0AThe%20proposed%20SSLChange%20framework%20accomplishes%20self-learning%20only%20by%20taking%20a%0Asingle-temporal%20sample%20and%20can%20be%20flexibly%20transferred%20to%20main-stream%20CD%0Abaselines.%20With%20self-supervised%20contrastive%20learning%2C%20feature%20representation%0Apre-training%20can%20be%20performed%20directly%20based%20on%20the%20original%20data%20even%20without%0Alabeling.%20After%20a%20certain%20amount%20of%20labels%20are%20subsequently%20obtained%2C%20the%0Apre-trained%20features%20will%20be%20aligned%20with%20the%20labels%20for%20fully%20supervised%0Afine-tuning.%20Without%20introducing%20any%20additional%20data%20or%20labels%2C%20the%20performance%0Aof%20downstream%20baselines%20will%20experience%20a%20significant%20enhancement.%20Experimental%0Aresults%20on%202%20entire%20datasets%20and%206%20diluted%20datasets%20show%20that%20our%20proposed%0ASSLChange%20improves%20the%20performance%20and%20stability%20of%20CD%20baseline%20in%20data-limited%0Asituations.%20The%20code%20of%20SSLChange%20will%20be%20released%20at%0A%5Curl%7Bhttps%3A//github.com/MarsZhaoYT/SSLChange%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSLChange%253A%2520A%2520Self-supervised%2520Change%2520Detection%2520Framework%2520Based%2520on%2520Domain%250A%2520%2520Adaptation%26entry.906535625%3DYitao%2520Zhao%2520and%2520Turgay%2520Celik%2520and%2520Nanqing%2520Liu%2520and%2520Feng%2520Gao%2520and%2520Heng-Chao%2520Li%26entry.1292438233%3D%2520%2520In%2520conventional%2520remote%2520sensing%2520change%2520detection%2520%2528RS%2520CD%2529%2520procedures%252C%2520extensive%250Amanual%2520labeling%2520for%2520bi-temporal%2520images%2520is%2520first%2520required%2520to%2520maintain%2520the%250Aperformance%2520of%2520subsequent%2520fully%2520supervised%2520training.%2520However%252C%2520pixel-level%250Alabeling%2520for%2520CD%2520tasks%2520is%2520very%2520complex%2520and%2520time-consuming.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520a%2520novel%2520self-supervised%2520contrastive%2520framework%2520applicable%2520to%2520the%2520RS%2520CD%250Atask%252C%2520which%2520promotes%2520the%2520model%2520to%2520accurately%2520capture%2520spatial%252C%2520structural%252C%2520and%250Asemantic%2520information%2520through%2520domain%2520adapter%2520and%2520hierarchical%2520contrastive%2520head.%250AThe%2520proposed%2520SSLChange%2520framework%2520accomplishes%2520self-learning%2520only%2520by%2520taking%2520a%250Asingle-temporal%2520sample%2520and%2520can%2520be%2520flexibly%2520transferred%2520to%2520main-stream%2520CD%250Abaselines.%2520With%2520self-supervised%2520contrastive%2520learning%252C%2520feature%2520representation%250Apre-training%2520can%2520be%2520performed%2520directly%2520based%2520on%2520the%2520original%2520data%2520even%2520without%250Alabeling.%2520After%2520a%2520certain%2520amount%2520of%2520labels%2520are%2520subsequently%2520obtained%252C%2520the%250Apre-trained%2520features%2520will%2520be%2520aligned%2520with%2520the%2520labels%2520for%2520fully%2520supervised%250Afine-tuning.%2520Without%2520introducing%2520any%2520additional%2520data%2520or%2520labels%252C%2520the%2520performance%250Aof%2520downstream%2520baselines%2520will%2520experience%2520a%2520significant%2520enhancement.%2520Experimental%250Aresults%2520on%25202%2520entire%2520datasets%2520and%25206%2520diluted%2520datasets%2520show%2520that%2520our%2520proposed%250ASSLChange%2520improves%2520the%2520performance%2520and%2520stability%2520of%2520CD%2520baseline%2520in%2520data-limited%250Asituations.%2520The%2520code%2520of%2520SSLChange%2520will%2520be%2520released%2520at%250A%255Curl%257Bhttps%253A//github.com/MarsZhaoYT/SSLChange%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSLChange%3A%20A%20Self-supervised%20Change%20Detection%20Framework%20Based%20on%20Domain%0A%20%20Adaptation&entry.906535625=Yitao%20Zhao%20and%20Turgay%20Celik%20and%20Nanqing%20Liu%20and%20Feng%20Gao%20and%20Heng-Chao%20Li&entry.1292438233=%20%20In%20conventional%20remote%20sensing%20change%20detection%20%28RS%20CD%29%20procedures%2C%20extensive%0Amanual%20labeling%20for%20bi-temporal%20images%20is%20first%20required%20to%20maintain%20the%0Aperformance%20of%20subsequent%20fully%20supervised%20training.%20However%2C%20pixel-level%0Alabeling%20for%20CD%20tasks%20is%20very%20complex%20and%20time-consuming.%20In%20this%20paper%2C%20we%0Aexplore%20a%20novel%20self-supervised%20contrastive%20framework%20applicable%20to%20the%20RS%20CD%0Atask%2C%20which%20promotes%20the%20model%20to%20accurately%20capture%20spatial%2C%20structural%2C%20and%0Asemantic%20information%20through%20domain%20adapter%20and%20hierarchical%20contrastive%20head.%0AThe%20proposed%20SSLChange%20framework%20accomplishes%20self-learning%20only%20by%20taking%20a%0Asingle-temporal%20sample%20and%20can%20be%20flexibly%20transferred%20to%20main-stream%20CD%0Abaselines.%20With%20self-supervised%20contrastive%20learning%2C%20feature%20representation%0Apre-training%20can%20be%20performed%20directly%20based%20on%20the%20original%20data%20even%20without%0Alabeling.%20After%20a%20certain%20amount%20of%20labels%20are%20subsequently%20obtained%2C%20the%0Apre-trained%20features%20will%20be%20aligned%20with%20the%20labels%20for%20fully%20supervised%0Afine-tuning.%20Without%20introducing%20any%20additional%20data%20or%20labels%2C%20the%20performance%0Aof%20downstream%20baselines%20will%20experience%20a%20significant%20enhancement.%20Experimental%0Aresults%20on%202%20entire%20datasets%20and%206%20diluted%20datasets%20show%20that%20our%20proposed%0ASSLChange%20improves%20the%20performance%20and%20stability%20of%20CD%20baseline%20in%20data-limited%0Asituations.%20The%20code%20of%20SSLChange%20will%20be%20released%20at%0A%5Curl%7Bhttps%3A//github.com/MarsZhaoYT/SSLChange%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18224v1&entry.124074799=Read"},
{"title": "Learning to Detour: Shortcut Mitigating Augmentation for Weakly\n  Supervised Semantic Segmentation", "author": "JuneHyoung Kwon and Eunju Lee and Yunsung Cho and YoungBin Kim", "abstract": "  Weakly supervised semantic segmentation (WSSS) employing weak forms of labels\nhas been actively studied to alleviate the annotation cost of acquiring\npixel-level labels. However, classifiers trained on biased datasets tend to\nexploit shortcut features and make predictions based on spurious correlations\nbetween certain backgrounds and objects, leading to a poor generalization\nperformance. In this paper, we propose shortcut mitigating augmentation (SMA)\nfor WSSS, which generates synthetic representations of object-background\ncombinations not seen in the training data to reduce the use of shortcut\nfeatures. Our approach disentangles the object-relevant and background\nfeatures. We then shuffle and combine the disentangled representations to\ncreate synthetic features of diverse object-background combinations.\nSMA-trained classifier depends less on contexts and focuses more on the target\nobject when making predictions. In addition, we analyzed the behavior of the\nclassifier on shortcut usage after applying our augmentation using an\nattribution method-based metric. The proposed method achieved the improved\nperformance of semantic segmentation result on PASCAL VOC 2012 and MS COCO 2014\ndatasets.\n", "link": "http://arxiv.org/abs/2405.18148v1", "date": "2024-05-28", "relevancy": 2.6546, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5397}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5274}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Detour%3A%20Shortcut%20Mitigating%20Augmentation%20for%20Weakly%0A%20%20Supervised%20Semantic%20Segmentation&body=Title%3A%20Learning%20to%20Detour%3A%20Shortcut%20Mitigating%20Augmentation%20for%20Weakly%0A%20%20Supervised%20Semantic%20Segmentation%0AAuthor%3A%20JuneHyoung%20Kwon%20and%20Eunju%20Lee%20and%20Yunsung%20Cho%20and%20YoungBin%20Kim%0AAbstract%3A%20%20%20Weakly%20supervised%20semantic%20segmentation%20%28WSSS%29%20employing%20weak%20forms%20of%20labels%0Ahas%20been%20actively%20studied%20to%20alleviate%20the%20annotation%20cost%20of%20acquiring%0Apixel-level%20labels.%20However%2C%20classifiers%20trained%20on%20biased%20datasets%20tend%20to%0Aexploit%20shortcut%20features%20and%20make%20predictions%20based%20on%20spurious%20correlations%0Abetween%20certain%20backgrounds%20and%20objects%2C%20leading%20to%20a%20poor%20generalization%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20shortcut%20mitigating%20augmentation%20%28SMA%29%0Afor%20WSSS%2C%20which%20generates%20synthetic%20representations%20of%20object-background%0Acombinations%20not%20seen%20in%20the%20training%20data%20to%20reduce%20the%20use%20of%20shortcut%0Afeatures.%20Our%20approach%20disentangles%20the%20object-relevant%20and%20background%0Afeatures.%20We%20then%20shuffle%20and%20combine%20the%20disentangled%20representations%20to%0Acreate%20synthetic%20features%20of%20diverse%20object-background%20combinations.%0ASMA-trained%20classifier%20depends%20less%20on%20contexts%20and%20focuses%20more%20on%20the%20target%0Aobject%20when%20making%20predictions.%20In%20addition%2C%20we%20analyzed%20the%20behavior%20of%20the%0Aclassifier%20on%20shortcut%20usage%20after%20applying%20our%20augmentation%20using%20an%0Aattribution%20method-based%20metric.%20The%20proposed%20method%20achieved%20the%20improved%0Aperformance%20of%20semantic%20segmentation%20result%20on%20PASCAL%20VOC%202012%20and%20MS%20COCO%202014%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Detour%253A%2520Shortcut%2520Mitigating%2520Augmentation%2520for%2520Weakly%250A%2520%2520Supervised%2520Semantic%2520Segmentation%26entry.906535625%3DJuneHyoung%2520Kwon%2520and%2520Eunju%2520Lee%2520and%2520Yunsung%2520Cho%2520and%2520YoungBin%2520Kim%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520semantic%2520segmentation%2520%2528WSSS%2529%2520employing%2520weak%2520forms%2520of%2520labels%250Ahas%2520been%2520actively%2520studied%2520to%2520alleviate%2520the%2520annotation%2520cost%2520of%2520acquiring%250Apixel-level%2520labels.%2520However%252C%2520classifiers%2520trained%2520on%2520biased%2520datasets%2520tend%2520to%250Aexploit%2520shortcut%2520features%2520and%2520make%2520predictions%2520based%2520on%2520spurious%2520correlations%250Abetween%2520certain%2520backgrounds%2520and%2520objects%252C%2520leading%2520to%2520a%2520poor%2520generalization%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520shortcut%2520mitigating%2520augmentation%2520%2528SMA%2529%250Afor%2520WSSS%252C%2520which%2520generates%2520synthetic%2520representations%2520of%2520object-background%250Acombinations%2520not%2520seen%2520in%2520the%2520training%2520data%2520to%2520reduce%2520the%2520use%2520of%2520shortcut%250Afeatures.%2520Our%2520approach%2520disentangles%2520the%2520object-relevant%2520and%2520background%250Afeatures.%2520We%2520then%2520shuffle%2520and%2520combine%2520the%2520disentangled%2520representations%2520to%250Acreate%2520synthetic%2520features%2520of%2520diverse%2520object-background%2520combinations.%250ASMA-trained%2520classifier%2520depends%2520less%2520on%2520contexts%2520and%2520focuses%2520more%2520on%2520the%2520target%250Aobject%2520when%2520making%2520predictions.%2520In%2520addition%252C%2520we%2520analyzed%2520the%2520behavior%2520of%2520the%250Aclassifier%2520on%2520shortcut%2520usage%2520after%2520applying%2520our%2520augmentation%2520using%2520an%250Aattribution%2520method-based%2520metric.%2520The%2520proposed%2520method%2520achieved%2520the%2520improved%250Aperformance%2520of%2520semantic%2520segmentation%2520result%2520on%2520PASCAL%2520VOC%25202012%2520and%2520MS%2520COCO%25202014%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Detour%3A%20Shortcut%20Mitigating%20Augmentation%20for%20Weakly%0A%20%20Supervised%20Semantic%20Segmentation&entry.906535625=JuneHyoung%20Kwon%20and%20Eunju%20Lee%20and%20Yunsung%20Cho%20and%20YoungBin%20Kim&entry.1292438233=%20%20Weakly%20supervised%20semantic%20segmentation%20%28WSSS%29%20employing%20weak%20forms%20of%20labels%0Ahas%20been%20actively%20studied%20to%20alleviate%20the%20annotation%20cost%20of%20acquiring%0Apixel-level%20labels.%20However%2C%20classifiers%20trained%20on%20biased%20datasets%20tend%20to%0Aexploit%20shortcut%20features%20and%20make%20predictions%20based%20on%20spurious%20correlations%0Abetween%20certain%20backgrounds%20and%20objects%2C%20leading%20to%20a%20poor%20generalization%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20shortcut%20mitigating%20augmentation%20%28SMA%29%0Afor%20WSSS%2C%20which%20generates%20synthetic%20representations%20of%20object-background%0Acombinations%20not%20seen%20in%20the%20training%20data%20to%20reduce%20the%20use%20of%20shortcut%0Afeatures.%20Our%20approach%20disentangles%20the%20object-relevant%20and%20background%0Afeatures.%20We%20then%20shuffle%20and%20combine%20the%20disentangled%20representations%20to%0Acreate%20synthetic%20features%20of%20diverse%20object-background%20combinations.%0ASMA-trained%20classifier%20depends%20less%20on%20contexts%20and%20focuses%20more%20on%20the%20target%0Aobject%20when%20making%20predictions.%20In%20addition%2C%20we%20analyzed%20the%20behavior%20of%20the%0Aclassifier%20on%20shortcut%20usage%20after%20applying%20our%20augmentation%20using%20an%0Aattribution%20method-based%20metric.%20The%20proposed%20method%20achieved%20the%20improved%0Aperformance%20of%20semantic%20segmentation%20result%20on%20PASCAL%20VOC%202012%20and%20MS%20COCO%202014%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18148v1&entry.124074799=Read"},
{"title": "WIDIn: Wording Image for Domain-Invariant Representation in\n  Single-Source Domain Generalization", "author": "Jiawei Ma and Yulei Niu and Shiyuan Huang and Guangxing Han and Shih-Fu Chang", "abstract": "  Language has been useful in extending the vision encoder to data from diverse\ndistributions without empirical discovery in training domains. However, as the\nimage description is mostly at coarse-grained level and ignores visual details,\nthe resulted embeddings are still ineffective in overcoming complexity of\ndomains at inference time. We present a self-supervision framework WIDIn,\nWording Images for Domain-Invariant representation, to disentangle\ndiscriminative visual representation, by only leveraging data in a single\ndomain and without any test prior. Specifically, for each image, we first\nestimate the language embedding with fine-grained alignment, which can be\nconsequently used to adaptively identify and then remove domain-specific\ncounterpart from the raw visual embedding. WIDIn can be applied to both\npretrained vision-language models like CLIP, and separately trained uni-modal\nmodels like MoCo and BERT. Experimental studies on three domain generalization\ndatasets demonstrate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2405.18405v1", "date": "2024-05-28", "relevancy": 2.6517, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5356}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5336}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WIDIn%3A%20Wording%20Image%20for%20Domain-Invariant%20Representation%20in%0A%20%20Single-Source%20Domain%20Generalization&body=Title%3A%20WIDIn%3A%20Wording%20Image%20for%20Domain-Invariant%20Representation%20in%0A%20%20Single-Source%20Domain%20Generalization%0AAuthor%3A%20Jiawei%20Ma%20and%20Yulei%20Niu%20and%20Shiyuan%20Huang%20and%20Guangxing%20Han%20and%20Shih-Fu%20Chang%0AAbstract%3A%20%20%20Language%20has%20been%20useful%20in%20extending%20the%20vision%20encoder%20to%20data%20from%20diverse%0Adistributions%20without%20empirical%20discovery%20in%20training%20domains.%20However%2C%20as%20the%0Aimage%20description%20is%20mostly%20at%20coarse-grained%20level%20and%20ignores%20visual%20details%2C%0Athe%20resulted%20embeddings%20are%20still%20ineffective%20in%20overcoming%20complexity%20of%0Adomains%20at%20inference%20time.%20We%20present%20a%20self-supervision%20framework%20WIDIn%2C%0AWording%20Images%20for%20Domain-Invariant%20representation%2C%20to%20disentangle%0Adiscriminative%20visual%20representation%2C%20by%20only%20leveraging%20data%20in%20a%20single%0Adomain%20and%20without%20any%20test%20prior.%20Specifically%2C%20for%20each%20image%2C%20we%20first%0Aestimate%20the%20language%20embedding%20with%20fine-grained%20alignment%2C%20which%20can%20be%0Aconsequently%20used%20to%20adaptively%20identify%20and%20then%20remove%20domain-specific%0Acounterpart%20from%20the%20raw%20visual%20embedding.%20WIDIn%20can%20be%20applied%20to%20both%0Apretrained%20vision-language%20models%20like%20CLIP%2C%20and%20separately%20trained%20uni-modal%0Amodels%20like%20MoCo%20and%20BERT.%20Experimental%20studies%20on%20three%20domain%20generalization%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWIDIn%253A%2520Wording%2520Image%2520for%2520Domain-Invariant%2520Representation%2520in%250A%2520%2520Single-Source%2520Domain%2520Generalization%26entry.906535625%3DJiawei%2520Ma%2520and%2520Yulei%2520Niu%2520and%2520Shiyuan%2520Huang%2520and%2520Guangxing%2520Han%2520and%2520Shih-Fu%2520Chang%26entry.1292438233%3D%2520%2520Language%2520has%2520been%2520useful%2520in%2520extending%2520the%2520vision%2520encoder%2520to%2520data%2520from%2520diverse%250Adistributions%2520without%2520empirical%2520discovery%2520in%2520training%2520domains.%2520However%252C%2520as%2520the%250Aimage%2520description%2520is%2520mostly%2520at%2520coarse-grained%2520level%2520and%2520ignores%2520visual%2520details%252C%250Athe%2520resulted%2520embeddings%2520are%2520still%2520ineffective%2520in%2520overcoming%2520complexity%2520of%250Adomains%2520at%2520inference%2520time.%2520We%2520present%2520a%2520self-supervision%2520framework%2520WIDIn%252C%250AWording%2520Images%2520for%2520Domain-Invariant%2520representation%252C%2520to%2520disentangle%250Adiscriminative%2520visual%2520representation%252C%2520by%2520only%2520leveraging%2520data%2520in%2520a%2520single%250Adomain%2520and%2520without%2520any%2520test%2520prior.%2520Specifically%252C%2520for%2520each%2520image%252C%2520we%2520first%250Aestimate%2520the%2520language%2520embedding%2520with%2520fine-grained%2520alignment%252C%2520which%2520can%2520be%250Aconsequently%2520used%2520to%2520adaptively%2520identify%2520and%2520then%2520remove%2520domain-specific%250Acounterpart%2520from%2520the%2520raw%2520visual%2520embedding.%2520WIDIn%2520can%2520be%2520applied%2520to%2520both%250Apretrained%2520vision-language%2520models%2520like%2520CLIP%252C%2520and%2520separately%2520trained%2520uni-modal%250Amodels%2520like%2520MoCo%2520and%2520BERT.%2520Experimental%2520studies%2520on%2520three%2520domain%2520generalization%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WIDIn%3A%20Wording%20Image%20for%20Domain-Invariant%20Representation%20in%0A%20%20Single-Source%20Domain%20Generalization&entry.906535625=Jiawei%20Ma%20and%20Yulei%20Niu%20and%20Shiyuan%20Huang%20and%20Guangxing%20Han%20and%20Shih-Fu%20Chang&entry.1292438233=%20%20Language%20has%20been%20useful%20in%20extending%20the%20vision%20encoder%20to%20data%20from%20diverse%0Adistributions%20without%20empirical%20discovery%20in%20training%20domains.%20However%2C%20as%20the%0Aimage%20description%20is%20mostly%20at%20coarse-grained%20level%20and%20ignores%20visual%20details%2C%0Athe%20resulted%20embeddings%20are%20still%20ineffective%20in%20overcoming%20complexity%20of%0Adomains%20at%20inference%20time.%20We%20present%20a%20self-supervision%20framework%20WIDIn%2C%0AWording%20Images%20for%20Domain-Invariant%20representation%2C%20to%20disentangle%0Adiscriminative%20visual%20representation%2C%20by%20only%20leveraging%20data%20in%20a%20single%0Adomain%20and%20without%20any%20test%20prior.%20Specifically%2C%20for%20each%20image%2C%20we%20first%0Aestimate%20the%20language%20embedding%20with%20fine-grained%20alignment%2C%20which%20can%20be%0Aconsequently%20used%20to%20adaptively%20identify%20and%20then%20remove%20domain-specific%0Acounterpart%20from%20the%20raw%20visual%20embedding.%20WIDIn%20can%20be%20applied%20to%20both%0Apretrained%20vision-language%20models%20like%20CLIP%2C%20and%20separately%20trained%20uni-modal%0Amodels%20like%20MoCo%20and%20BERT.%20Experimental%20studies%20on%20three%20domain%20generalization%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18405v1&entry.124074799=Read"},
{"title": "In-Context Symmetries: Self-Supervised Learning through Contextual World\n  Models", "author": "Sharut Gupta and Chenyu Wang and Yifei Wang and Tommi Jaakkola and Stefanie Jegelka", "abstract": "  At the core of self-supervised learning for vision is the idea of learning\ninvariant or equivariant representations with respect to a set of data\ntransformations. This approach, however, introduces strong inductive biases,\nwhich can render the representations fragile in downstream tasks that do not\nconform to these symmetries. In this work, drawing insights from world models,\nwe propose to instead learn a general representation that can adapt to be\ninvariant or equivariant to different transformations by paying attention to\ncontext -- a memory module that tracks task-specific states, actions, and\nfuture states. Here, the action is the transformation, while the current and\nfuture states respectively represent the input's representation before and\nafter the transformation. Our proposed algorithm, Contextual Self-Supervised\nLearning (ContextSSL), learns equivariance to all transformations (as opposed\nto invariance). In this way, the model can learn to encode all relevant\nfeatures as general representations while having the versatility to tail down\nto task-wise symmetries when given a few examples as the context. Empirically,\nwe demonstrate significant performance gains over existing methods on\nequivariance-related tasks, supported by both qualitative and quantitative\nevaluations.\n", "link": "http://arxiv.org/abs/2405.18193v1", "date": "2024-05-28", "relevancy": 2.6193, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5652}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5052}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Symmetries%3A%20Self-Supervised%20Learning%20through%20Contextual%20World%0A%20%20Models&body=Title%3A%20In-Context%20Symmetries%3A%20Self-Supervised%20Learning%20through%20Contextual%20World%0A%20%20Models%0AAuthor%3A%20Sharut%20Gupta%20and%20Chenyu%20Wang%20and%20Yifei%20Wang%20and%20Tommi%20Jaakkola%20and%20Stefanie%20Jegelka%0AAbstract%3A%20%20%20At%20the%20core%20of%20self-supervised%20learning%20for%20vision%20is%20the%20idea%20of%20learning%0Ainvariant%20or%20equivariant%20representations%20with%20respect%20to%20a%20set%20of%20data%0Atransformations.%20This%20approach%2C%20however%2C%20introduces%20strong%20inductive%20biases%2C%0Awhich%20can%20render%20the%20representations%20fragile%20in%20downstream%20tasks%20that%20do%20not%0Aconform%20to%20these%20symmetries.%20In%20this%20work%2C%20drawing%20insights%20from%20world%20models%2C%0Awe%20propose%20to%20instead%20learn%20a%20general%20representation%20that%20can%20adapt%20to%20be%0Ainvariant%20or%20equivariant%20to%20different%20transformations%20by%20paying%20attention%20to%0Acontext%20--%20a%20memory%20module%20that%20tracks%20task-specific%20states%2C%20actions%2C%20and%0Afuture%20states.%20Here%2C%20the%20action%20is%20the%20transformation%2C%20while%20the%20current%20and%0Afuture%20states%20respectively%20represent%20the%20input%27s%20representation%20before%20and%0Aafter%20the%20transformation.%20Our%20proposed%20algorithm%2C%20Contextual%20Self-Supervised%0ALearning%20%28ContextSSL%29%2C%20learns%20equivariance%20to%20all%20transformations%20%28as%20opposed%0Ato%20invariance%29.%20In%20this%20way%2C%20the%20model%20can%20learn%20to%20encode%20all%20relevant%0Afeatures%20as%20general%20representations%20while%20having%20the%20versatility%20to%20tail%20down%0Ato%20task-wise%20symmetries%20when%20given%20a%20few%20examples%20as%20the%20context.%20Empirically%2C%0Awe%20demonstrate%20significant%20performance%20gains%20over%20existing%20methods%20on%0Aequivariance-related%20tasks%2C%20supported%20by%20both%20qualitative%20and%20quantitative%0Aevaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Symmetries%253A%2520Self-Supervised%2520Learning%2520through%2520Contextual%2520World%250A%2520%2520Models%26entry.906535625%3DSharut%2520Gupta%2520and%2520Chenyu%2520Wang%2520and%2520Yifei%2520Wang%2520and%2520Tommi%2520Jaakkola%2520and%2520Stefanie%2520Jegelka%26entry.1292438233%3D%2520%2520At%2520the%2520core%2520of%2520self-supervised%2520learning%2520for%2520vision%2520is%2520the%2520idea%2520of%2520learning%250Ainvariant%2520or%2520equivariant%2520representations%2520with%2520respect%2520to%2520a%2520set%2520of%2520data%250Atransformations.%2520This%2520approach%252C%2520however%252C%2520introduces%2520strong%2520inductive%2520biases%252C%250Awhich%2520can%2520render%2520the%2520representations%2520fragile%2520in%2520downstream%2520tasks%2520that%2520do%2520not%250Aconform%2520to%2520these%2520symmetries.%2520In%2520this%2520work%252C%2520drawing%2520insights%2520from%2520world%2520models%252C%250Awe%2520propose%2520to%2520instead%2520learn%2520a%2520general%2520representation%2520that%2520can%2520adapt%2520to%2520be%250Ainvariant%2520or%2520equivariant%2520to%2520different%2520transformations%2520by%2520paying%2520attention%2520to%250Acontext%2520--%2520a%2520memory%2520module%2520that%2520tracks%2520task-specific%2520states%252C%2520actions%252C%2520and%250Afuture%2520states.%2520Here%252C%2520the%2520action%2520is%2520the%2520transformation%252C%2520while%2520the%2520current%2520and%250Afuture%2520states%2520respectively%2520represent%2520the%2520input%2527s%2520representation%2520before%2520and%250Aafter%2520the%2520transformation.%2520Our%2520proposed%2520algorithm%252C%2520Contextual%2520Self-Supervised%250ALearning%2520%2528ContextSSL%2529%252C%2520learns%2520equivariance%2520to%2520all%2520transformations%2520%2528as%2520opposed%250Ato%2520invariance%2529.%2520In%2520this%2520way%252C%2520the%2520model%2520can%2520learn%2520to%2520encode%2520all%2520relevant%250Afeatures%2520as%2520general%2520representations%2520while%2520having%2520the%2520versatility%2520to%2520tail%2520down%250Ato%2520task-wise%2520symmetries%2520when%2520given%2520a%2520few%2520examples%2520as%2520the%2520context.%2520Empirically%252C%250Awe%2520demonstrate%2520significant%2520performance%2520gains%2520over%2520existing%2520methods%2520on%250Aequivariance-related%2520tasks%252C%2520supported%2520by%2520both%2520qualitative%2520and%2520quantitative%250Aevaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Symmetries%3A%20Self-Supervised%20Learning%20through%20Contextual%20World%0A%20%20Models&entry.906535625=Sharut%20Gupta%20and%20Chenyu%20Wang%20and%20Yifei%20Wang%20and%20Tommi%20Jaakkola%20and%20Stefanie%20Jegelka&entry.1292438233=%20%20At%20the%20core%20of%20self-supervised%20learning%20for%20vision%20is%20the%20idea%20of%20learning%0Ainvariant%20or%20equivariant%20representations%20with%20respect%20to%20a%20set%20of%20data%0Atransformations.%20This%20approach%2C%20however%2C%20introduces%20strong%20inductive%20biases%2C%0Awhich%20can%20render%20the%20representations%20fragile%20in%20downstream%20tasks%20that%20do%20not%0Aconform%20to%20these%20symmetries.%20In%20this%20work%2C%20drawing%20insights%20from%20world%20models%2C%0Awe%20propose%20to%20instead%20learn%20a%20general%20representation%20that%20can%20adapt%20to%20be%0Ainvariant%20or%20equivariant%20to%20different%20transformations%20by%20paying%20attention%20to%0Acontext%20--%20a%20memory%20module%20that%20tracks%20task-specific%20states%2C%20actions%2C%20and%0Afuture%20states.%20Here%2C%20the%20action%20is%20the%20transformation%2C%20while%20the%20current%20and%0Afuture%20states%20respectively%20represent%20the%20input%27s%20representation%20before%20and%0Aafter%20the%20transformation.%20Our%20proposed%20algorithm%2C%20Contextual%20Self-Supervised%0ALearning%20%28ContextSSL%29%2C%20learns%20equivariance%20to%20all%20transformations%20%28as%20opposed%0Ato%20invariance%29.%20In%20this%20way%2C%20the%20model%20can%20learn%20to%20encode%20all%20relevant%0Afeatures%20as%20general%20representations%20while%20having%20the%20versatility%20to%20tail%20down%0Ato%20task-wise%20symmetries%20when%20given%20a%20few%20examples%20as%20the%20context.%20Empirically%2C%0Awe%20demonstrate%20significant%20performance%20gains%20over%20existing%20methods%20on%0Aequivariance-related%20tasks%2C%20supported%20by%20both%20qualitative%20and%20quantitative%0Aevaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18193v1&entry.124074799=Read"},
{"title": "PaDPaF: Partial Disentanglement with Partially-Federated GANs", "author": "Abdulla Jasem Almansoori and Samuel Horv\u00e1th and Martin Tak\u00e1\u010d", "abstract": "  Federated learning has become a popular machine learning paradigm with many\npotential real-life applications, including recommendation systems, the\nInternet of Things (IoT), healthcare, and self-driving cars. Though most\ncurrent applications focus on classification-based tasks, learning personalized\ngenerative models remains largely unexplored, and their benefits in the\nheterogeneous setting still need to be better understood. This work proposes a\nnovel architecture combining global client-agnostic and local client-specific\ngenerative models. We show that using standard techniques for training\nfederated models, our proposed model achieves privacy and personalization by\nimplicitly disentangling the globally consistent representation (i.e. content)\nfrom the client-dependent variations (i.e. style). Using such decomposition,\npersonalized models can generate locally unseen labels while preserving the\ngiven style of the client and can predict the labels for all clients with high\naccuracy by training a simple linear classifier on the global content features.\nFurthermore, disentanglement enables other essential applications, such as data\nanonymization, by sharing only the content. Extensive experimental evaluation\ncorroborates our findings, and we also discuss a theoretical motivation for the\nproposed approach.\n", "link": "http://arxiv.org/abs/2212.03836v2", "date": "2024-05-28", "relevancy": 2.6185, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5298}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5275}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PaDPaF%3A%20Partial%20Disentanglement%20with%20Partially-Federated%20GANs&body=Title%3A%20PaDPaF%3A%20Partial%20Disentanglement%20with%20Partially-Federated%20GANs%0AAuthor%3A%20Abdulla%20Jasem%20Almansoori%20and%20Samuel%20Horv%C3%A1th%20and%20Martin%20Tak%C3%A1%C4%8D%0AAbstract%3A%20%20%20Federated%20learning%20has%20become%20a%20popular%20machine%20learning%20paradigm%20with%20many%0Apotential%20real-life%20applications%2C%20including%20recommendation%20systems%2C%20the%0AInternet%20of%20Things%20%28IoT%29%2C%20healthcare%2C%20and%20self-driving%20cars.%20Though%20most%0Acurrent%20applications%20focus%20on%20classification-based%20tasks%2C%20learning%20personalized%0Agenerative%20models%20remains%20largely%20unexplored%2C%20and%20their%20benefits%20in%20the%0Aheterogeneous%20setting%20still%20need%20to%20be%20better%20understood.%20This%20work%20proposes%20a%0Anovel%20architecture%20combining%20global%20client-agnostic%20and%20local%20client-specific%0Agenerative%20models.%20We%20show%20that%20using%20standard%20techniques%20for%20training%0Afederated%20models%2C%20our%20proposed%20model%20achieves%20privacy%20and%20personalization%20by%0Aimplicitly%20disentangling%20the%20globally%20consistent%20representation%20%28i.e.%20content%29%0Afrom%20the%20client-dependent%20variations%20%28i.e.%20style%29.%20Using%20such%20decomposition%2C%0Apersonalized%20models%20can%20generate%20locally%20unseen%20labels%20while%20preserving%20the%0Agiven%20style%20of%20the%20client%20and%20can%20predict%20the%20labels%20for%20all%20clients%20with%20high%0Aaccuracy%20by%20training%20a%20simple%20linear%20classifier%20on%20the%20global%20content%20features.%0AFurthermore%2C%20disentanglement%20enables%20other%20essential%20applications%2C%20such%20as%20data%0Aanonymization%2C%20by%20sharing%20only%20the%20content.%20Extensive%20experimental%20evaluation%0Acorroborates%20our%20findings%2C%20and%20we%20also%20discuss%20a%20theoretical%20motivation%20for%20the%0Aproposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.03836v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaDPaF%253A%2520Partial%2520Disentanglement%2520with%2520Partially-Federated%2520GANs%26entry.906535625%3DAbdulla%2520Jasem%2520Almansoori%2520and%2520Samuel%2520Horv%25C3%25A1th%2520and%2520Martin%2520Tak%25C3%25A1%25C4%258D%26entry.1292438233%3D%2520%2520Federated%2520learning%2520has%2520become%2520a%2520popular%2520machine%2520learning%2520paradigm%2520with%2520many%250Apotential%2520real-life%2520applications%252C%2520including%2520recommendation%2520systems%252C%2520the%250AInternet%2520of%2520Things%2520%2528IoT%2529%252C%2520healthcare%252C%2520and%2520self-driving%2520cars.%2520Though%2520most%250Acurrent%2520applications%2520focus%2520on%2520classification-based%2520tasks%252C%2520learning%2520personalized%250Agenerative%2520models%2520remains%2520largely%2520unexplored%252C%2520and%2520their%2520benefits%2520in%2520the%250Aheterogeneous%2520setting%2520still%2520need%2520to%2520be%2520better%2520understood.%2520This%2520work%2520proposes%2520a%250Anovel%2520architecture%2520combining%2520global%2520client-agnostic%2520and%2520local%2520client-specific%250Agenerative%2520models.%2520We%2520show%2520that%2520using%2520standard%2520techniques%2520for%2520training%250Afederated%2520models%252C%2520our%2520proposed%2520model%2520achieves%2520privacy%2520and%2520personalization%2520by%250Aimplicitly%2520disentangling%2520the%2520globally%2520consistent%2520representation%2520%2528i.e.%2520content%2529%250Afrom%2520the%2520client-dependent%2520variations%2520%2528i.e.%2520style%2529.%2520Using%2520such%2520decomposition%252C%250Apersonalized%2520models%2520can%2520generate%2520locally%2520unseen%2520labels%2520while%2520preserving%2520the%250Agiven%2520style%2520of%2520the%2520client%2520and%2520can%2520predict%2520the%2520labels%2520for%2520all%2520clients%2520with%2520high%250Aaccuracy%2520by%2520training%2520a%2520simple%2520linear%2520classifier%2520on%2520the%2520global%2520content%2520features.%250AFurthermore%252C%2520disentanglement%2520enables%2520other%2520essential%2520applications%252C%2520such%2520as%2520data%250Aanonymization%252C%2520by%2520sharing%2520only%2520the%2520content.%2520Extensive%2520experimental%2520evaluation%250Acorroborates%2520our%2520findings%252C%2520and%2520we%2520also%2520discuss%2520a%2520theoretical%2520motivation%2520for%2520the%250Aproposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.03836v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PaDPaF%3A%20Partial%20Disentanglement%20with%20Partially-Federated%20GANs&entry.906535625=Abdulla%20Jasem%20Almansoori%20and%20Samuel%20Horv%C3%A1th%20and%20Martin%20Tak%C3%A1%C4%8D&entry.1292438233=%20%20Federated%20learning%20has%20become%20a%20popular%20machine%20learning%20paradigm%20with%20many%0Apotential%20real-life%20applications%2C%20including%20recommendation%20systems%2C%20the%0AInternet%20of%20Things%20%28IoT%29%2C%20healthcare%2C%20and%20self-driving%20cars.%20Though%20most%0Acurrent%20applications%20focus%20on%20classification-based%20tasks%2C%20learning%20personalized%0Agenerative%20models%20remains%20largely%20unexplored%2C%20and%20their%20benefits%20in%20the%0Aheterogeneous%20setting%20still%20need%20to%20be%20better%20understood.%20This%20work%20proposes%20a%0Anovel%20architecture%20combining%20global%20client-agnostic%20and%20local%20client-specific%0Agenerative%20models.%20We%20show%20that%20using%20standard%20techniques%20for%20training%0Afederated%20models%2C%20our%20proposed%20model%20achieves%20privacy%20and%20personalization%20by%0Aimplicitly%20disentangling%20the%20globally%20consistent%20representation%20%28i.e.%20content%29%0Afrom%20the%20client-dependent%20variations%20%28i.e.%20style%29.%20Using%20such%20decomposition%2C%0Apersonalized%20models%20can%20generate%20locally%20unseen%20labels%20while%20preserving%20the%0Agiven%20style%20of%20the%20client%20and%20can%20predict%20the%20labels%20for%20all%20clients%20with%20high%0Aaccuracy%20by%20training%20a%20simple%20linear%20classifier%20on%20the%20global%20content%20features.%0AFurthermore%2C%20disentanglement%20enables%20other%20essential%20applications%2C%20such%20as%20data%0Aanonymization%2C%20by%20sharing%20only%20the%20content.%20Extensive%20experimental%20evaluation%0Acorroborates%20our%20findings%2C%20and%20we%20also%20discuss%20a%20theoretical%20motivation%20for%20the%0Aproposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.03836v2&entry.124074799=Read"},
{"title": "EffoVPR: Effective Foundation Model Utilization for Visual Place\n  Recognition", "author": "Issar Tzachor and Boaz Lerner and Matan Levy and Michael Green and Tal Berkovitz Shalev and Gavriel Habib and Dvir Samuel and Noam Korngut Zailer and Or Shimshi and Nir Darshan and Rami Ben-Ari", "abstract": "  The task of Visual Place Recognition (VPR) is to predict the location of a\nquery image from a database of geo-tagged images. Recent studies in VPR have\nhighlighted the significant advantage of employing pre-trained foundation\nmodels like DINOv2 for the VPR task. However, these models are often deemed\ninadequate for VPR without further fine-tuning on task-specific data. In this\npaper, we propose a simple yet powerful approach to better exploit the\npotential of a foundation model for VPR. We first demonstrate that features\nextracted from self-attention layers can serve as a powerful re-ranker for VPR.\nUtilizing these features in a zero-shot manner, our method surpasses previous\nzero-shot methods and achieves competitive results compared to supervised\nmethods across multiple datasets. Subsequently, we demonstrate that a\nsingle-stage method leveraging internal ViT layers for pooling can generate\nglobal features that achieve state-of-the-art results, even when reduced to a\ndimensionality as low as 128D. Nevertheless, incorporating our local foundation\nfeatures for re-ranking, expands this gap. Our approach further demonstrates\nremarkable robustness and generalization, achieving state-of-the-art results,\nwith a significant gap, in challenging scenarios, involving occlusion,\nday-night variations, and seasonal changes.\n", "link": "http://arxiv.org/abs/2405.18065v1", "date": "2024-05-28", "relevancy": 2.6114, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5377}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5177}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EffoVPR%3A%20Effective%20Foundation%20Model%20Utilization%20for%20Visual%20Place%0A%20%20Recognition&body=Title%3A%20EffoVPR%3A%20Effective%20Foundation%20Model%20Utilization%20for%20Visual%20Place%0A%20%20Recognition%0AAuthor%3A%20Issar%20Tzachor%20and%20Boaz%20Lerner%20and%20Matan%20Levy%20and%20Michael%20Green%20and%20Tal%20Berkovitz%20Shalev%20and%20Gavriel%20Habib%20and%20Dvir%20Samuel%20and%20Noam%20Korngut%20Zailer%20and%20Or%20Shimshi%20and%20Nir%20Darshan%20and%20Rami%20Ben-Ari%0AAbstract%3A%20%20%20The%20task%20of%20Visual%20Place%20Recognition%20%28VPR%29%20is%20to%20predict%20the%20location%20of%20a%0Aquery%20image%20from%20a%20database%20of%20geo-tagged%20images.%20Recent%20studies%20in%20VPR%20have%0Ahighlighted%20the%20significant%20advantage%20of%20employing%20pre-trained%20foundation%0Amodels%20like%20DINOv2%20for%20the%20VPR%20task.%20However%2C%20these%20models%20are%20often%20deemed%0Ainadequate%20for%20VPR%20without%20further%20fine-tuning%20on%20task-specific%20data.%20In%20this%0Apaper%2C%20we%20propose%20a%20simple%20yet%20powerful%20approach%20to%20better%20exploit%20the%0Apotential%20of%20a%20foundation%20model%20for%20VPR.%20We%20first%20demonstrate%20that%20features%0Aextracted%20from%20self-attention%20layers%20can%20serve%20as%20a%20powerful%20re-ranker%20for%20VPR.%0AUtilizing%20these%20features%20in%20a%20zero-shot%20manner%2C%20our%20method%20surpasses%20previous%0Azero-shot%20methods%20and%20achieves%20competitive%20results%20compared%20to%20supervised%0Amethods%20across%20multiple%20datasets.%20Subsequently%2C%20we%20demonstrate%20that%20a%0Asingle-stage%20method%20leveraging%20internal%20ViT%20layers%20for%20pooling%20can%20generate%0Aglobal%20features%20that%20achieve%20state-of-the-art%20results%2C%20even%20when%20reduced%20to%20a%0Adimensionality%20as%20low%20as%20128D.%20Nevertheless%2C%20incorporating%20our%20local%20foundation%0Afeatures%20for%20re-ranking%2C%20expands%20this%20gap.%20Our%20approach%20further%20demonstrates%0Aremarkable%20robustness%20and%20generalization%2C%20achieving%20state-of-the-art%20results%2C%0Awith%20a%20significant%20gap%2C%20in%20challenging%20scenarios%2C%20involving%20occlusion%2C%0Aday-night%20variations%2C%20and%20seasonal%20changes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffoVPR%253A%2520Effective%2520Foundation%2520Model%2520Utilization%2520for%2520Visual%2520Place%250A%2520%2520Recognition%26entry.906535625%3DIssar%2520Tzachor%2520and%2520Boaz%2520Lerner%2520and%2520Matan%2520Levy%2520and%2520Michael%2520Green%2520and%2520Tal%2520Berkovitz%2520Shalev%2520and%2520Gavriel%2520Habib%2520and%2520Dvir%2520Samuel%2520and%2520Noam%2520Korngut%2520Zailer%2520and%2520Or%2520Shimshi%2520and%2520Nir%2520Darshan%2520and%2520Rami%2520Ben-Ari%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520Visual%2520Place%2520Recognition%2520%2528VPR%2529%2520is%2520to%2520predict%2520the%2520location%2520of%2520a%250Aquery%2520image%2520from%2520a%2520database%2520of%2520geo-tagged%2520images.%2520Recent%2520studies%2520in%2520VPR%2520have%250Ahighlighted%2520the%2520significant%2520advantage%2520of%2520employing%2520pre-trained%2520foundation%250Amodels%2520like%2520DINOv2%2520for%2520the%2520VPR%2520task.%2520However%252C%2520these%2520models%2520are%2520often%2520deemed%250Ainadequate%2520for%2520VPR%2520without%2520further%2520fine-tuning%2520on%2520task-specific%2520data.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520simple%2520yet%2520powerful%2520approach%2520to%2520better%2520exploit%2520the%250Apotential%2520of%2520a%2520foundation%2520model%2520for%2520VPR.%2520We%2520first%2520demonstrate%2520that%2520features%250Aextracted%2520from%2520self-attention%2520layers%2520can%2520serve%2520as%2520a%2520powerful%2520re-ranker%2520for%2520VPR.%250AUtilizing%2520these%2520features%2520in%2520a%2520zero-shot%2520manner%252C%2520our%2520method%2520surpasses%2520previous%250Azero-shot%2520methods%2520and%2520achieves%2520competitive%2520results%2520compared%2520to%2520supervised%250Amethods%2520across%2520multiple%2520datasets.%2520Subsequently%252C%2520we%2520demonstrate%2520that%2520a%250Asingle-stage%2520method%2520leveraging%2520internal%2520ViT%2520layers%2520for%2520pooling%2520can%2520generate%250Aglobal%2520features%2520that%2520achieve%2520state-of-the-art%2520results%252C%2520even%2520when%2520reduced%2520to%2520a%250Adimensionality%2520as%2520low%2520as%2520128D.%2520Nevertheless%252C%2520incorporating%2520our%2520local%2520foundation%250Afeatures%2520for%2520re-ranking%252C%2520expands%2520this%2520gap.%2520Our%2520approach%2520further%2520demonstrates%250Aremarkable%2520robustness%2520and%2520generalization%252C%2520achieving%2520state-of-the-art%2520results%252C%250Awith%2520a%2520significant%2520gap%252C%2520in%2520challenging%2520scenarios%252C%2520involving%2520occlusion%252C%250Aday-night%2520variations%252C%2520and%2520seasonal%2520changes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EffoVPR%3A%20Effective%20Foundation%20Model%20Utilization%20for%20Visual%20Place%0A%20%20Recognition&entry.906535625=Issar%20Tzachor%20and%20Boaz%20Lerner%20and%20Matan%20Levy%20and%20Michael%20Green%20and%20Tal%20Berkovitz%20Shalev%20and%20Gavriel%20Habib%20and%20Dvir%20Samuel%20and%20Noam%20Korngut%20Zailer%20and%20Or%20Shimshi%20and%20Nir%20Darshan%20and%20Rami%20Ben-Ari&entry.1292438233=%20%20The%20task%20of%20Visual%20Place%20Recognition%20%28VPR%29%20is%20to%20predict%20the%20location%20of%20a%0Aquery%20image%20from%20a%20database%20of%20geo-tagged%20images.%20Recent%20studies%20in%20VPR%20have%0Ahighlighted%20the%20significant%20advantage%20of%20employing%20pre-trained%20foundation%0Amodels%20like%20DINOv2%20for%20the%20VPR%20task.%20However%2C%20these%20models%20are%20often%20deemed%0Ainadequate%20for%20VPR%20without%20further%20fine-tuning%20on%20task-specific%20data.%20In%20this%0Apaper%2C%20we%20propose%20a%20simple%20yet%20powerful%20approach%20to%20better%20exploit%20the%0Apotential%20of%20a%20foundation%20model%20for%20VPR.%20We%20first%20demonstrate%20that%20features%0Aextracted%20from%20self-attention%20layers%20can%20serve%20as%20a%20powerful%20re-ranker%20for%20VPR.%0AUtilizing%20these%20features%20in%20a%20zero-shot%20manner%2C%20our%20method%20surpasses%20previous%0Azero-shot%20methods%20and%20achieves%20competitive%20results%20compared%20to%20supervised%0Amethods%20across%20multiple%20datasets.%20Subsequently%2C%20we%20demonstrate%20that%20a%0Asingle-stage%20method%20leveraging%20internal%20ViT%20layers%20for%20pooling%20can%20generate%0Aglobal%20features%20that%20achieve%20state-of-the-art%20results%2C%20even%20when%20reduced%20to%20a%0Adimensionality%20as%20low%20as%20128D.%20Nevertheless%2C%20incorporating%20our%20local%20foundation%0Afeatures%20for%20re-ranking%2C%20expands%20this%20gap.%20Our%20approach%20further%20demonstrates%0Aremarkable%20robustness%20and%20generalization%2C%20achieving%20state-of-the-art%20results%2C%0Awith%20a%20significant%20gap%2C%20in%20challenging%20scenarios%2C%20involving%20occlusion%2C%0Aday-night%20variations%2C%20and%20seasonal%20changes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18065v1&entry.124074799=Read"},
{"title": "NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields", "author": "Amandine Brunetto and Sascha Hornauer and Fabien Moutarde", "abstract": "  Sound plays a major role in human perception, providing essential scene\ninformation alongside vision for understanding our environment. Despite\nprogress in neural implicit representations, learning acoustics that match a\nvisual scene is still challenging. We propose NeRAF, a method that jointly\nlearns acoustic and radiance fields. NeRAF is designed as a Nerfstudio module\nfor convenient access to realistic audio-visual generation. It synthesizes both\nnovel views and spatialized audio at new positions, leveraging radiance field\ncapabilities to condition the acoustic field with 3D scene information. At\ninference, each modality can be rendered independently and at spatially\nseparated positions, providing greater versatility. We demonstrate the\nadvantages of our method on the SoundSpaces dataset. NeRAF achieves substantial\nperformance improvements over previous works while being more data-efficient.\nFurthermore, NeRAF enhances novel view synthesis of complex scenes trained with\nsparse data through cross-modal learning.\n", "link": "http://arxiv.org/abs/2405.18213v1", "date": "2024-05-28", "relevancy": 2.6076, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5421}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5112}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields&body=Title%3A%20NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields%0AAuthor%3A%20Amandine%20Brunetto%20and%20Sascha%20Hornauer%20and%20Fabien%20Moutarde%0AAbstract%3A%20%20%20Sound%20plays%20a%20major%20role%20in%20human%20perception%2C%20providing%20essential%20scene%0Ainformation%20alongside%20vision%20for%20understanding%20our%20environment.%20Despite%0Aprogress%20in%20neural%20implicit%20representations%2C%20learning%20acoustics%20that%20match%20a%0Avisual%20scene%20is%20still%20challenging.%20We%20propose%20NeRAF%2C%20a%20method%20that%20jointly%0Alearns%20acoustic%20and%20radiance%20fields.%20NeRAF%20is%20designed%20as%20a%20Nerfstudio%20module%0Afor%20convenient%20access%20to%20realistic%20audio-visual%20generation.%20It%20synthesizes%20both%0Anovel%20views%20and%20spatialized%20audio%20at%20new%20positions%2C%20leveraging%20radiance%20field%0Acapabilities%20to%20condition%20the%20acoustic%20field%20with%203D%20scene%20information.%20At%0Ainference%2C%20each%20modality%20can%20be%20rendered%20independently%20and%20at%20spatially%0Aseparated%20positions%2C%20providing%20greater%20versatility.%20We%20demonstrate%20the%0Aadvantages%20of%20our%20method%20on%20the%20SoundSpaces%20dataset.%20NeRAF%20achieves%20substantial%0Aperformance%20improvements%20over%20previous%20works%20while%20being%20more%20data-efficient.%0AFurthermore%2C%20NeRAF%20enhances%20novel%20view%20synthesis%20of%20complex%20scenes%20trained%20with%0Asparse%20data%20through%20cross-modal%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRAF%253A%25203D%2520Scene%2520Infused%2520Neural%2520Radiance%2520and%2520Acoustic%2520Fields%26entry.906535625%3DAmandine%2520Brunetto%2520and%2520Sascha%2520Hornauer%2520and%2520Fabien%2520Moutarde%26entry.1292438233%3D%2520%2520Sound%2520plays%2520a%2520major%2520role%2520in%2520human%2520perception%252C%2520providing%2520essential%2520scene%250Ainformation%2520alongside%2520vision%2520for%2520understanding%2520our%2520environment.%2520Despite%250Aprogress%2520in%2520neural%2520implicit%2520representations%252C%2520learning%2520acoustics%2520that%2520match%2520a%250Avisual%2520scene%2520is%2520still%2520challenging.%2520We%2520propose%2520NeRAF%252C%2520a%2520method%2520that%2520jointly%250Alearns%2520acoustic%2520and%2520radiance%2520fields.%2520NeRAF%2520is%2520designed%2520as%2520a%2520Nerfstudio%2520module%250Afor%2520convenient%2520access%2520to%2520realistic%2520audio-visual%2520generation.%2520It%2520synthesizes%2520both%250Anovel%2520views%2520and%2520spatialized%2520audio%2520at%2520new%2520positions%252C%2520leveraging%2520radiance%2520field%250Acapabilities%2520to%2520condition%2520the%2520acoustic%2520field%2520with%25203D%2520scene%2520information.%2520At%250Ainference%252C%2520each%2520modality%2520can%2520be%2520rendered%2520independently%2520and%2520at%2520spatially%250Aseparated%2520positions%252C%2520providing%2520greater%2520versatility.%2520We%2520demonstrate%2520the%250Aadvantages%2520of%2520our%2520method%2520on%2520the%2520SoundSpaces%2520dataset.%2520NeRAF%2520achieves%2520substantial%250Aperformance%2520improvements%2520over%2520previous%2520works%2520while%2520being%2520more%2520data-efficient.%250AFurthermore%252C%2520NeRAF%2520enhances%2520novel%2520view%2520synthesis%2520of%2520complex%2520scenes%2520trained%2520with%250Asparse%2520data%2520through%2520cross-modal%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields&entry.906535625=Amandine%20Brunetto%20and%20Sascha%20Hornauer%20and%20Fabien%20Moutarde&entry.1292438233=%20%20Sound%20plays%20a%20major%20role%20in%20human%20perception%2C%20providing%20essential%20scene%0Ainformation%20alongside%20vision%20for%20understanding%20our%20environment.%20Despite%0Aprogress%20in%20neural%20implicit%20representations%2C%20learning%20acoustics%20that%20match%20a%0Avisual%20scene%20is%20still%20challenging.%20We%20propose%20NeRAF%2C%20a%20method%20that%20jointly%0Alearns%20acoustic%20and%20radiance%20fields.%20NeRAF%20is%20designed%20as%20a%20Nerfstudio%20module%0Afor%20convenient%20access%20to%20realistic%20audio-visual%20generation.%20It%20synthesizes%20both%0Anovel%20views%20and%20spatialized%20audio%20at%20new%20positions%2C%20leveraging%20radiance%20field%0Acapabilities%20to%20condition%20the%20acoustic%20field%20with%203D%20scene%20information.%20At%0Ainference%2C%20each%20modality%20can%20be%20rendered%20independently%20and%20at%20spatially%0Aseparated%20positions%2C%20providing%20greater%20versatility.%20We%20demonstrate%20the%0Aadvantages%20of%20our%20method%20on%20the%20SoundSpaces%20dataset.%20NeRAF%20achieves%20substantial%0Aperformance%20improvements%20over%20previous%20works%20while%20being%20more%20data-efficient.%0AFurthermore%2C%20NeRAF%20enhances%20novel%20view%20synthesis%20of%20complex%20scenes%20trained%20with%0Asparse%20data%20through%20cross-modal%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18213v1&entry.124074799=Read"},
{"title": "Why are Visually-Grounded Language Models Bad at Image Classification?", "author": "Yuhui Zhang and Alyssa Unell and Xiaohan Wang and Dhruba Ghosh and Yuchang Su and Ludwig Schmidt and Serena Yeung-Levy", "abstract": "  Image classification is one of the most fundamental capabilities of machine\nvision intelligence. In this work, we revisit the image classification task\nusing visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We\nfind that existing proprietary and public VLMs, despite often using CLIP as a\nvision encoder and having many more parameters, significantly underperform CLIP\non standard image classification benchmarks like ImageNet. To understand the\nreason, we explore several hypotheses concerning the inference algorithms,\ntraining objectives, and data processing in VLMs. Our analysis reveals that the\nprimary cause is data-related: critical information for image classification is\nencoded in the VLM's latent space but can only be effectively decoded with\nenough training data. Specifically, there is a strong correlation between the\nfrequency of class exposure during VLM training and instruction-tuning and the\nVLM's performance in those classes; when trained with sufficient data, VLMs can\nmatch the accuracy of state-of-the-art classification models. Based on these\nfindings, we enhance a VLM by integrating classification-focused datasets into\nits training, and demonstrate that the enhanced classification performance of\nthe VLM transfers to its general capabilities, resulting in an improvement of\n11.8% on the newly collected ImageWikiQA dataset.\n", "link": "http://arxiv.org/abs/2405.18415v1", "date": "2024-05-28", "relevancy": 2.6047, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5472}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5097}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20are%20Visually-Grounded%20Language%20Models%20Bad%20at%20Image%20Classification%3F&body=Title%3A%20Why%20are%20Visually-Grounded%20Language%20Models%20Bad%20at%20Image%20Classification%3F%0AAuthor%3A%20Yuhui%20Zhang%20and%20Alyssa%20Unell%20and%20Xiaohan%20Wang%20and%20Dhruba%20Ghosh%20and%20Yuchang%20Su%20and%20Ludwig%20Schmidt%20and%20Serena%20Yeung-Levy%0AAbstract%3A%20%20%20Image%20classification%20is%20one%20of%20the%20most%20fundamental%20capabilities%20of%20machine%0Avision%20intelligence.%20In%20this%20work%2C%20we%20revisit%20the%20image%20classification%20task%0Ausing%20visually-grounded%20language%20models%20%28VLMs%29%20such%20as%20GPT-4V%20and%20LLaVA.%20We%0Afind%20that%20existing%20proprietary%20and%20public%20VLMs%2C%20despite%20often%20using%20CLIP%20as%20a%0Avision%20encoder%20and%20having%20many%20more%20parameters%2C%20significantly%20underperform%20CLIP%0Aon%20standard%20image%20classification%20benchmarks%20like%20ImageNet.%20To%20understand%20the%0Areason%2C%20we%20explore%20several%20hypotheses%20concerning%20the%20inference%20algorithms%2C%0Atraining%20objectives%2C%20and%20data%20processing%20in%20VLMs.%20Our%20analysis%20reveals%20that%20the%0Aprimary%20cause%20is%20data-related%3A%20critical%20information%20for%20image%20classification%20is%0Aencoded%20in%20the%20VLM%27s%20latent%20space%20but%20can%20only%20be%20effectively%20decoded%20with%0Aenough%20training%20data.%20Specifically%2C%20there%20is%20a%20strong%20correlation%20between%20the%0Afrequency%20of%20class%20exposure%20during%20VLM%20training%20and%20instruction-tuning%20and%20the%0AVLM%27s%20performance%20in%20those%20classes%3B%20when%20trained%20with%20sufficient%20data%2C%20VLMs%20can%0Amatch%20the%20accuracy%20of%20state-of-the-art%20classification%20models.%20Based%20on%20these%0Afindings%2C%20we%20enhance%20a%20VLM%20by%20integrating%20classification-focused%20datasets%20into%0Aits%20training%2C%20and%20demonstrate%20that%20the%20enhanced%20classification%20performance%20of%0Athe%20VLM%20transfers%20to%20its%20general%20capabilities%2C%20resulting%20in%20an%20improvement%20of%0A11.8%25%20on%20the%20newly%20collected%20ImageWikiQA%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520are%2520Visually-Grounded%2520Language%2520Models%2520Bad%2520at%2520Image%2520Classification%253F%26entry.906535625%3DYuhui%2520Zhang%2520and%2520Alyssa%2520Unell%2520and%2520Xiaohan%2520Wang%2520and%2520Dhruba%2520Ghosh%2520and%2520Yuchang%2520Su%2520and%2520Ludwig%2520Schmidt%2520and%2520Serena%2520Yeung-Levy%26entry.1292438233%3D%2520%2520Image%2520classification%2520is%2520one%2520of%2520the%2520most%2520fundamental%2520capabilities%2520of%2520machine%250Avision%2520intelligence.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520image%2520classification%2520task%250Ausing%2520visually-grounded%2520language%2520models%2520%2528VLMs%2529%2520such%2520as%2520GPT-4V%2520and%2520LLaVA.%2520We%250Afind%2520that%2520existing%2520proprietary%2520and%2520public%2520VLMs%252C%2520despite%2520often%2520using%2520CLIP%2520as%2520a%250Avision%2520encoder%2520and%2520having%2520many%2520more%2520parameters%252C%2520significantly%2520underperform%2520CLIP%250Aon%2520standard%2520image%2520classification%2520benchmarks%2520like%2520ImageNet.%2520To%2520understand%2520the%250Areason%252C%2520we%2520explore%2520several%2520hypotheses%2520concerning%2520the%2520inference%2520algorithms%252C%250Atraining%2520objectives%252C%2520and%2520data%2520processing%2520in%2520VLMs.%2520Our%2520analysis%2520reveals%2520that%2520the%250Aprimary%2520cause%2520is%2520data-related%253A%2520critical%2520information%2520for%2520image%2520classification%2520is%250Aencoded%2520in%2520the%2520VLM%2527s%2520latent%2520space%2520but%2520can%2520only%2520be%2520effectively%2520decoded%2520with%250Aenough%2520training%2520data.%2520Specifically%252C%2520there%2520is%2520a%2520strong%2520correlation%2520between%2520the%250Afrequency%2520of%2520class%2520exposure%2520during%2520VLM%2520training%2520and%2520instruction-tuning%2520and%2520the%250AVLM%2527s%2520performance%2520in%2520those%2520classes%253B%2520when%2520trained%2520with%2520sufficient%2520data%252C%2520VLMs%2520can%250Amatch%2520the%2520accuracy%2520of%2520state-of-the-art%2520classification%2520models.%2520Based%2520on%2520these%250Afindings%252C%2520we%2520enhance%2520a%2520VLM%2520by%2520integrating%2520classification-focused%2520datasets%2520into%250Aits%2520training%252C%2520and%2520demonstrate%2520that%2520the%2520enhanced%2520classification%2520performance%2520of%250Athe%2520VLM%2520transfers%2520to%2520its%2520general%2520capabilities%252C%2520resulting%2520in%2520an%2520improvement%2520of%250A11.8%2525%2520on%2520the%2520newly%2520collected%2520ImageWikiQA%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20are%20Visually-Grounded%20Language%20Models%20Bad%20at%20Image%20Classification%3F&entry.906535625=Yuhui%20Zhang%20and%20Alyssa%20Unell%20and%20Xiaohan%20Wang%20and%20Dhruba%20Ghosh%20and%20Yuchang%20Su%20and%20Ludwig%20Schmidt%20and%20Serena%20Yeung-Levy&entry.1292438233=%20%20Image%20classification%20is%20one%20of%20the%20most%20fundamental%20capabilities%20of%20machine%0Avision%20intelligence.%20In%20this%20work%2C%20we%20revisit%20the%20image%20classification%20task%0Ausing%20visually-grounded%20language%20models%20%28VLMs%29%20such%20as%20GPT-4V%20and%20LLaVA.%20We%0Afind%20that%20existing%20proprietary%20and%20public%20VLMs%2C%20despite%20often%20using%20CLIP%20as%20a%0Avision%20encoder%20and%20having%20many%20more%20parameters%2C%20significantly%20underperform%20CLIP%0Aon%20standard%20image%20classification%20benchmarks%20like%20ImageNet.%20To%20understand%20the%0Areason%2C%20we%20explore%20several%20hypotheses%20concerning%20the%20inference%20algorithms%2C%0Atraining%20objectives%2C%20and%20data%20processing%20in%20VLMs.%20Our%20analysis%20reveals%20that%20the%0Aprimary%20cause%20is%20data-related%3A%20critical%20information%20for%20image%20classification%20is%0Aencoded%20in%20the%20VLM%27s%20latent%20space%20but%20can%20only%20be%20effectively%20decoded%20with%0Aenough%20training%20data.%20Specifically%2C%20there%20is%20a%20strong%20correlation%20between%20the%0Afrequency%20of%20class%20exposure%20during%20VLM%20training%20and%20instruction-tuning%20and%20the%0AVLM%27s%20performance%20in%20those%20classes%3B%20when%20trained%20with%20sufficient%20data%2C%20VLMs%20can%0Amatch%20the%20accuracy%20of%20state-of-the-art%20classification%20models.%20Based%20on%20these%0Afindings%2C%20we%20enhance%20a%20VLM%20by%20integrating%20classification-focused%20datasets%20into%0Aits%20training%2C%20and%20demonstrate%20that%20the%20enhanced%20classification%20performance%20of%0Athe%20VLM%20transfers%20to%20its%20general%20capabilities%2C%20resulting%20in%20an%20improvement%20of%0A11.8%25%20on%20the%20newly%20collected%20ImageWikiQA%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18415v1&entry.124074799=Read"},
{"title": "Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models", "author": "Sangwon Jang and Jaehyeong Jo and Kimin Lee and Sung Ju Hwang", "abstract": "  Text-to-image diffusion models have shown remarkable success in generating\npersonalized subjects based on a few reference images. However, current methods\noften fail when generating multiple subjects simultaneously, resulting in mixed\nidentities with combined attributes from different subjects. In this work, we\npresent MuDI, a novel framework that enables multi-subject personalization by\neffectively decoupling identities from multiple subjects. Our main idea is to\nutilize segmented subjects generated by a foundation model for segmentation\n(Segment Anything) for both training and inference, as a form of data\naugmentation for training and initialization for the generation process.\nMoreover, we further introduce a new metric to better evaluate the performance\nof our method on multi-subject personalization. Experimental results show that\nour MuDI can produce high-quality personalized images without identity mixing,\neven for highly similar subjects as shown in Figure 1. Specifically, in human\nevaluation, MuDI obtains twice the success rate for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% against the strongest baseline.\n", "link": "http://arxiv.org/abs/2404.04243v2", "date": "2024-05-28", "relevancy": 2.5467, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6853}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.604}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identity%20Decoupling%20for%20Multi-Subject%20Personalization%20of%20Text-to-Image%0A%20%20Models&body=Title%3A%20Identity%20Decoupling%20for%20Multi-Subject%20Personalization%20of%20Text-to-Image%0A%20%20Models%0AAuthor%3A%20Sangwon%20Jang%20and%20Jaehyeong%20Jo%20and%20Kimin%20Lee%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20shown%20remarkable%20success%20in%20generating%0Apersonalized%20subjects%20based%20on%20a%20few%20reference%20images.%20However%2C%20current%20methods%0Aoften%20fail%20when%20generating%20multiple%20subjects%20simultaneously%2C%20resulting%20in%20mixed%0Aidentities%20with%20combined%20attributes%20from%20different%20subjects.%20In%20this%20work%2C%20we%0Apresent%20MuDI%2C%20a%20novel%20framework%20that%20enables%20multi-subject%20personalization%20by%0Aeffectively%20decoupling%20identities%20from%20multiple%20subjects.%20Our%20main%20idea%20is%20to%0Autilize%20segmented%20subjects%20generated%20by%20a%20foundation%20model%20for%20segmentation%0A%28Segment%20Anything%29%20for%20both%20training%20and%20inference%2C%20as%20a%20form%20of%20data%0Aaugmentation%20for%20training%20and%20initialization%20for%20the%20generation%20process.%0AMoreover%2C%20we%20further%20introduce%20a%20new%20metric%20to%20better%20evaluate%20the%20performance%0Aof%20our%20method%20on%20multi-subject%20personalization.%20Experimental%20results%20show%20that%0Aour%20MuDI%20can%20produce%20high-quality%20personalized%20images%20without%20identity%20mixing%2C%0Aeven%20for%20highly%20similar%20subjects%20as%20shown%20in%20Figure%201.%20Specifically%2C%20in%20human%0Aevaluation%2C%20MuDI%20obtains%20twice%20the%20success%20rate%20for%20personalizing%20multiple%0Asubjects%20without%20identity%20mixing%20over%20existing%20baselines%20and%20is%20preferred%20over%0A70%25%20against%20the%20strongest%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04243v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentity%2520Decoupling%2520for%2520Multi-Subject%2520Personalization%2520of%2520Text-to-Image%250A%2520%2520Models%26entry.906535625%3DSangwon%2520Jang%2520and%2520Jaehyeong%2520Jo%2520and%2520Kimin%2520Lee%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520have%2520shown%2520remarkable%2520success%2520in%2520generating%250Apersonalized%2520subjects%2520based%2520on%2520a%2520few%2520reference%2520images.%2520However%252C%2520current%2520methods%250Aoften%2520fail%2520when%2520generating%2520multiple%2520subjects%2520simultaneously%252C%2520resulting%2520in%2520mixed%250Aidentities%2520with%2520combined%2520attributes%2520from%2520different%2520subjects.%2520In%2520this%2520work%252C%2520we%250Apresent%2520MuDI%252C%2520a%2520novel%2520framework%2520that%2520enables%2520multi-subject%2520personalization%2520by%250Aeffectively%2520decoupling%2520identities%2520from%2520multiple%2520subjects.%2520Our%2520main%2520idea%2520is%2520to%250Autilize%2520segmented%2520subjects%2520generated%2520by%2520a%2520foundation%2520model%2520for%2520segmentation%250A%2528Segment%2520Anything%2529%2520for%2520both%2520training%2520and%2520inference%252C%2520as%2520a%2520form%2520of%2520data%250Aaugmentation%2520for%2520training%2520and%2520initialization%2520for%2520the%2520generation%2520process.%250AMoreover%252C%2520we%2520further%2520introduce%2520a%2520new%2520metric%2520to%2520better%2520evaluate%2520the%2520performance%250Aof%2520our%2520method%2520on%2520multi-subject%2520personalization.%2520Experimental%2520results%2520show%2520that%250Aour%2520MuDI%2520can%2520produce%2520high-quality%2520personalized%2520images%2520without%2520identity%2520mixing%252C%250Aeven%2520for%2520highly%2520similar%2520subjects%2520as%2520shown%2520in%2520Figure%25201.%2520Specifically%252C%2520in%2520human%250Aevaluation%252C%2520MuDI%2520obtains%2520twice%2520the%2520success%2520rate%2520for%2520personalizing%2520multiple%250Asubjects%2520without%2520identity%2520mixing%2520over%2520existing%2520baselines%2520and%2520is%2520preferred%2520over%250A70%2525%2520against%2520the%2520strongest%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04243v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identity%20Decoupling%20for%20Multi-Subject%20Personalization%20of%20Text-to-Image%0A%20%20Models&entry.906535625=Sangwon%20Jang%20and%20Jaehyeong%20Jo%20and%20Kimin%20Lee%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20shown%20remarkable%20success%20in%20generating%0Apersonalized%20subjects%20based%20on%20a%20few%20reference%20images.%20However%2C%20current%20methods%0Aoften%20fail%20when%20generating%20multiple%20subjects%20simultaneously%2C%20resulting%20in%20mixed%0Aidentities%20with%20combined%20attributes%20from%20different%20subjects.%20In%20this%20work%2C%20we%0Apresent%20MuDI%2C%20a%20novel%20framework%20that%20enables%20multi-subject%20personalization%20by%0Aeffectively%20decoupling%20identities%20from%20multiple%20subjects.%20Our%20main%20idea%20is%20to%0Autilize%20segmented%20subjects%20generated%20by%20a%20foundation%20model%20for%20segmentation%0A%28Segment%20Anything%29%20for%20both%20training%20and%20inference%2C%20as%20a%20form%20of%20data%0Aaugmentation%20for%20training%20and%20initialization%20for%20the%20generation%20process.%0AMoreover%2C%20we%20further%20introduce%20a%20new%20metric%20to%20better%20evaluate%20the%20performance%0Aof%20our%20method%20on%20multi-subject%20personalization.%20Experimental%20results%20show%20that%0Aour%20MuDI%20can%20produce%20high-quality%20personalized%20images%20without%20identity%20mixing%2C%0Aeven%20for%20highly%20similar%20subjects%20as%20shown%20in%20Figure%201.%20Specifically%2C%20in%20human%0Aevaluation%2C%20MuDI%20obtains%20twice%20the%20success%20rate%20for%20personalizing%20multiple%0Asubjects%20without%20identity%20mixing%20over%20existing%20baselines%20and%20is%20preferred%20over%0A70%25%20against%20the%20strongest%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04243v2&entry.124074799=Read"},
{"title": "A Calibration Tool for Refractive Underwater Vision", "author": "Felix Seegr\u00e4ber and Mengkun She and Felix Woelk and Kevin K\u00f6ser", "abstract": "  Many underwater robotic applications relying on vision sensors require proper\ncamera calibration, i.e. knowing the incoming light ray for each pixel in the\nimage. While for the ideal pinhole camera model all viewing rays intersect in a\nsingle 3D point, underwater cameras suffer from - possibly multiple -\nrefractions of light rays at the interfaces of water, glass and air. These\nchanges of direction depend on the position and orientation of the camera\ninside the water-proof housing, as well as on the shape and properties of the\noptical window, the port, itself. In recent years explicit models for\nunderwater vision behind common ports such as flat or dome port have been\nproposed, but the underwater community is still lacking a calibration tool\nwhich can determine port parameters through refractive calibration. With this\nwork we provide the first open source implementation of an underwater\nrefractive camera calibration toolbox. It allows end-to-end calibration of\nunderwater vision systems, including camera, stereo and housing calibration for\nsystems with dome or flat ports. The implementation is verified using rendered\ndatasets and real-world experiments.\n", "link": "http://arxiv.org/abs/2405.18018v1", "date": "2024-05-28", "relevancy": 2.5425, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5141}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5057}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Calibration%20Tool%20for%20Refractive%20Underwater%20Vision&body=Title%3A%20A%20Calibration%20Tool%20for%20Refractive%20Underwater%20Vision%0AAuthor%3A%20Felix%20Seegr%C3%A4ber%20and%20Mengkun%20She%20and%20Felix%20Woelk%20and%20Kevin%20K%C3%B6ser%0AAbstract%3A%20%20%20Many%20underwater%20robotic%20applications%20relying%20on%20vision%20sensors%20require%20proper%0Acamera%20calibration%2C%20i.e.%20knowing%20the%20incoming%20light%20ray%20for%20each%20pixel%20in%20the%0Aimage.%20While%20for%20the%20ideal%20pinhole%20camera%20model%20all%20viewing%20rays%20intersect%20in%20a%0Asingle%203D%20point%2C%20underwater%20cameras%20suffer%20from%20-%20possibly%20multiple%20-%0Arefractions%20of%20light%20rays%20at%20the%20interfaces%20of%20water%2C%20glass%20and%20air.%20These%0Achanges%20of%20direction%20depend%20on%20the%20position%20and%20orientation%20of%20the%20camera%0Ainside%20the%20water-proof%20housing%2C%20as%20well%20as%20on%20the%20shape%20and%20properties%20of%20the%0Aoptical%20window%2C%20the%20port%2C%20itself.%20In%20recent%20years%20explicit%20models%20for%0Aunderwater%20vision%20behind%20common%20ports%20such%20as%20flat%20or%20dome%20port%20have%20been%0Aproposed%2C%20but%20the%20underwater%20community%20is%20still%20lacking%20a%20calibration%20tool%0Awhich%20can%20determine%20port%20parameters%20through%20refractive%20calibration.%20With%20this%0Awork%20we%20provide%20the%20first%20open%20source%20implementation%20of%20an%20underwater%0Arefractive%20camera%20calibration%20toolbox.%20It%20allows%20end-to-end%20calibration%20of%0Aunderwater%20vision%20systems%2C%20including%20camera%2C%20stereo%20and%20housing%20calibration%20for%0Asystems%20with%20dome%20or%20flat%20ports.%20The%20implementation%20is%20verified%20using%20rendered%0Adatasets%20and%20real-world%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Calibration%2520Tool%2520for%2520Refractive%2520Underwater%2520Vision%26entry.906535625%3DFelix%2520Seegr%25C3%25A4ber%2520and%2520Mengkun%2520She%2520and%2520Felix%2520Woelk%2520and%2520Kevin%2520K%25C3%25B6ser%26entry.1292438233%3D%2520%2520Many%2520underwater%2520robotic%2520applications%2520relying%2520on%2520vision%2520sensors%2520require%2520proper%250Acamera%2520calibration%252C%2520i.e.%2520knowing%2520the%2520incoming%2520light%2520ray%2520for%2520each%2520pixel%2520in%2520the%250Aimage.%2520While%2520for%2520the%2520ideal%2520pinhole%2520camera%2520model%2520all%2520viewing%2520rays%2520intersect%2520in%2520a%250Asingle%25203D%2520point%252C%2520underwater%2520cameras%2520suffer%2520from%2520-%2520possibly%2520multiple%2520-%250Arefractions%2520of%2520light%2520rays%2520at%2520the%2520interfaces%2520of%2520water%252C%2520glass%2520and%2520air.%2520These%250Achanges%2520of%2520direction%2520depend%2520on%2520the%2520position%2520and%2520orientation%2520of%2520the%2520camera%250Ainside%2520the%2520water-proof%2520housing%252C%2520as%2520well%2520as%2520on%2520the%2520shape%2520and%2520properties%2520of%2520the%250Aoptical%2520window%252C%2520the%2520port%252C%2520itself.%2520In%2520recent%2520years%2520explicit%2520models%2520for%250Aunderwater%2520vision%2520behind%2520common%2520ports%2520such%2520as%2520flat%2520or%2520dome%2520port%2520have%2520been%250Aproposed%252C%2520but%2520the%2520underwater%2520community%2520is%2520still%2520lacking%2520a%2520calibration%2520tool%250Awhich%2520can%2520determine%2520port%2520parameters%2520through%2520refractive%2520calibration.%2520With%2520this%250Awork%2520we%2520provide%2520the%2520first%2520open%2520source%2520implementation%2520of%2520an%2520underwater%250Arefractive%2520camera%2520calibration%2520toolbox.%2520It%2520allows%2520end-to-end%2520calibration%2520of%250Aunderwater%2520vision%2520systems%252C%2520including%2520camera%252C%2520stereo%2520and%2520housing%2520calibration%2520for%250Asystems%2520with%2520dome%2520or%2520flat%2520ports.%2520The%2520implementation%2520is%2520verified%2520using%2520rendered%250Adatasets%2520and%2520real-world%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Calibration%20Tool%20for%20Refractive%20Underwater%20Vision&entry.906535625=Felix%20Seegr%C3%A4ber%20and%20Mengkun%20She%20and%20Felix%20Woelk%20and%20Kevin%20K%C3%B6ser&entry.1292438233=%20%20Many%20underwater%20robotic%20applications%20relying%20on%20vision%20sensors%20require%20proper%0Acamera%20calibration%2C%20i.e.%20knowing%20the%20incoming%20light%20ray%20for%20each%20pixel%20in%20the%0Aimage.%20While%20for%20the%20ideal%20pinhole%20camera%20model%20all%20viewing%20rays%20intersect%20in%20a%0Asingle%203D%20point%2C%20underwater%20cameras%20suffer%20from%20-%20possibly%20multiple%20-%0Arefractions%20of%20light%20rays%20at%20the%20interfaces%20of%20water%2C%20glass%20and%20air.%20These%0Achanges%20of%20direction%20depend%20on%20the%20position%20and%20orientation%20of%20the%20camera%0Ainside%20the%20water-proof%20housing%2C%20as%20well%20as%20on%20the%20shape%20and%20properties%20of%20the%0Aoptical%20window%2C%20the%20port%2C%20itself.%20In%20recent%20years%20explicit%20models%20for%0Aunderwater%20vision%20behind%20common%20ports%20such%20as%20flat%20or%20dome%20port%20have%20been%0Aproposed%2C%20but%20the%20underwater%20community%20is%20still%20lacking%20a%20calibration%20tool%0Awhich%20can%20determine%20port%20parameters%20through%20refractive%20calibration.%20With%20this%0Awork%20we%20provide%20the%20first%20open%20source%20implementation%20of%20an%20underwater%0Arefractive%20camera%20calibration%20toolbox.%20It%20allows%20end-to-end%20calibration%20of%0Aunderwater%20vision%20systems%2C%20including%20camera%2C%20stereo%20and%20housing%20calibration%20for%0Asystems%20with%20dome%20or%20flat%20ports.%20The%20implementation%20is%20verified%20using%20rendered%0Adatasets%20and%20real-world%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18018v1&entry.124074799=Read"},
{"title": "SpikeCV: Open a Continuous Computer Vision Era", "author": "Yajing Zheng and Jiyuan Zhang and Rui Zhao and Jianhao Ding and Shiyan Chen and Ruiqin Xiong and Zhaofei Yu and Tiejun Huang", "abstract": "  SpikeCV is a new open-source computer vision platform for the spike camera,\nwhich is a neuromorphic visual sensor that has developed rapidly in recent\nyears. In the spike camera, each pixel position directly accumulates the light\nintensity and asynchronously fires spikes. The output binary spikes can reach a\nfrequency of 40,000 Hz. As a new type of visual expression, spike sequence has\nhigh spatiotemporal completeness and preserves the continuous visual\ninformation of the external world. Taking advantage of the low latency and high\ndynamic range of the spike camera, many spike-based algorithms have made\nsignificant progress, such as high-quality imaging and ultra-high-speed target\ndetection.\n  To build up a community ecology for the spike vision to facilitate more users\nto take advantage of the spike camera, SpikeCV provides a variety of\nultra-high-speed scene datasets, hardware interfaces, and an easy-to-use\nmodules library. SpikeCV focuses on encapsulation for spike data,\nstandardization for dataset interfaces, modularization for vision tasks, and\nreal-time applications for challenging scenes. With the advent of the\nopen-source Python ecosystem, modules of SpikeCV can be used as a Python\nlibrary to fulfilled most of the numerical analysis needs of researchers. We\ndemonstrate the efficiency of the SpikeCV on offline inference and real-time\napplications. The project repository address are\n\\url{https://openi.pcl.ac.cn/Cordium/SpikeCV} and\n\\url{https://github.com/Zyj061/SpikeCV\n", "link": "http://arxiv.org/abs/2303.11684v2", "date": "2024-05-28", "relevancy": 2.539, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5103}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5065}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeCV%3A%20Open%20a%20Continuous%20Computer%20Vision%20Era&body=Title%3A%20SpikeCV%3A%20Open%20a%20Continuous%20Computer%20Vision%20Era%0AAuthor%3A%20Yajing%20Zheng%20and%20Jiyuan%20Zhang%20and%20Rui%20Zhao%20and%20Jianhao%20Ding%20and%20Shiyan%20Chen%20and%20Ruiqin%20Xiong%20and%20Zhaofei%20Yu%20and%20Tiejun%20Huang%0AAbstract%3A%20%20%20SpikeCV%20is%20a%20new%20open-source%20computer%20vision%20platform%20for%20the%20spike%20camera%2C%0Awhich%20is%20a%20neuromorphic%20visual%20sensor%20that%20has%20developed%20rapidly%20in%20recent%0Ayears.%20In%20the%20spike%20camera%2C%20each%20pixel%20position%20directly%20accumulates%20the%20light%0Aintensity%20and%20asynchronously%20fires%20spikes.%20The%20output%20binary%20spikes%20can%20reach%20a%0Afrequency%20of%2040%2C000%20Hz.%20As%20a%20new%20type%20of%20visual%20expression%2C%20spike%20sequence%20has%0Ahigh%20spatiotemporal%20completeness%20and%20preserves%20the%20continuous%20visual%0Ainformation%20of%20the%20external%20world.%20Taking%20advantage%20of%20the%20low%20latency%20and%20high%0Adynamic%20range%20of%20the%20spike%20camera%2C%20many%20spike-based%20algorithms%20have%20made%0Asignificant%20progress%2C%20such%20as%20high-quality%20imaging%20and%20ultra-high-speed%20target%0Adetection.%0A%20%20To%20build%20up%20a%20community%20ecology%20for%20the%20spike%20vision%20to%20facilitate%20more%20users%0Ato%20take%20advantage%20of%20the%20spike%20camera%2C%20SpikeCV%20provides%20a%20variety%20of%0Aultra-high-speed%20scene%20datasets%2C%20hardware%20interfaces%2C%20and%20an%20easy-to-use%0Amodules%20library.%20SpikeCV%20focuses%20on%20encapsulation%20for%20spike%20data%2C%0Astandardization%20for%20dataset%20interfaces%2C%20modularization%20for%20vision%20tasks%2C%20and%0Areal-time%20applications%20for%20challenging%20scenes.%20With%20the%20advent%20of%20the%0Aopen-source%20Python%20ecosystem%2C%20modules%20of%20SpikeCV%20can%20be%20used%20as%20a%20Python%0Alibrary%20to%20fulfilled%20most%20of%20the%20numerical%20analysis%20needs%20of%20researchers.%20We%0Ademonstrate%20the%20efficiency%20of%20the%20SpikeCV%20on%20offline%20inference%20and%20real-time%0Aapplications.%20The%20project%20repository%20address%20are%0A%5Curl%7Bhttps%3A//openi.pcl.ac.cn/Cordium/SpikeCV%7D%20and%0A%5Curl%7Bhttps%3A//github.com/Zyj061/SpikeCV%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.11684v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeCV%253A%2520Open%2520a%2520Continuous%2520Computer%2520Vision%2520Era%26entry.906535625%3DYajing%2520Zheng%2520and%2520Jiyuan%2520Zhang%2520and%2520Rui%2520Zhao%2520and%2520Jianhao%2520Ding%2520and%2520Shiyan%2520Chen%2520and%2520Ruiqin%2520Xiong%2520and%2520Zhaofei%2520Yu%2520and%2520Tiejun%2520Huang%26entry.1292438233%3D%2520%2520SpikeCV%2520is%2520a%2520new%2520open-source%2520computer%2520vision%2520platform%2520for%2520the%2520spike%2520camera%252C%250Awhich%2520is%2520a%2520neuromorphic%2520visual%2520sensor%2520that%2520has%2520developed%2520rapidly%2520in%2520recent%250Ayears.%2520In%2520the%2520spike%2520camera%252C%2520each%2520pixel%2520position%2520directly%2520accumulates%2520the%2520light%250Aintensity%2520and%2520asynchronously%2520fires%2520spikes.%2520The%2520output%2520binary%2520spikes%2520can%2520reach%2520a%250Afrequency%2520of%252040%252C000%2520Hz.%2520As%2520a%2520new%2520type%2520of%2520visual%2520expression%252C%2520spike%2520sequence%2520has%250Ahigh%2520spatiotemporal%2520completeness%2520and%2520preserves%2520the%2520continuous%2520visual%250Ainformation%2520of%2520the%2520external%2520world.%2520Taking%2520advantage%2520of%2520the%2520low%2520latency%2520and%2520high%250Adynamic%2520range%2520of%2520the%2520spike%2520camera%252C%2520many%2520spike-based%2520algorithms%2520have%2520made%250Asignificant%2520progress%252C%2520such%2520as%2520high-quality%2520imaging%2520and%2520ultra-high-speed%2520target%250Adetection.%250A%2520%2520To%2520build%2520up%2520a%2520community%2520ecology%2520for%2520the%2520spike%2520vision%2520to%2520facilitate%2520more%2520users%250Ato%2520take%2520advantage%2520of%2520the%2520spike%2520camera%252C%2520SpikeCV%2520provides%2520a%2520variety%2520of%250Aultra-high-speed%2520scene%2520datasets%252C%2520hardware%2520interfaces%252C%2520and%2520an%2520easy-to-use%250Amodules%2520library.%2520SpikeCV%2520focuses%2520on%2520encapsulation%2520for%2520spike%2520data%252C%250Astandardization%2520for%2520dataset%2520interfaces%252C%2520modularization%2520for%2520vision%2520tasks%252C%2520and%250Areal-time%2520applications%2520for%2520challenging%2520scenes.%2520With%2520the%2520advent%2520of%2520the%250Aopen-source%2520Python%2520ecosystem%252C%2520modules%2520of%2520SpikeCV%2520can%2520be%2520used%2520as%2520a%2520Python%250Alibrary%2520to%2520fulfilled%2520most%2520of%2520the%2520numerical%2520analysis%2520needs%2520of%2520researchers.%2520We%250Ademonstrate%2520the%2520efficiency%2520of%2520the%2520SpikeCV%2520on%2520offline%2520inference%2520and%2520real-time%250Aapplications.%2520The%2520project%2520repository%2520address%2520are%250A%255Curl%257Bhttps%253A//openi.pcl.ac.cn/Cordium/SpikeCV%257D%2520and%250A%255Curl%257Bhttps%253A//github.com/Zyj061/SpikeCV%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.11684v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeCV%3A%20Open%20a%20Continuous%20Computer%20Vision%20Era&entry.906535625=Yajing%20Zheng%20and%20Jiyuan%20Zhang%20and%20Rui%20Zhao%20and%20Jianhao%20Ding%20and%20Shiyan%20Chen%20and%20Ruiqin%20Xiong%20and%20Zhaofei%20Yu%20and%20Tiejun%20Huang&entry.1292438233=%20%20SpikeCV%20is%20a%20new%20open-source%20computer%20vision%20platform%20for%20the%20spike%20camera%2C%0Awhich%20is%20a%20neuromorphic%20visual%20sensor%20that%20has%20developed%20rapidly%20in%20recent%0Ayears.%20In%20the%20spike%20camera%2C%20each%20pixel%20position%20directly%20accumulates%20the%20light%0Aintensity%20and%20asynchronously%20fires%20spikes.%20The%20output%20binary%20spikes%20can%20reach%20a%0Afrequency%20of%2040%2C000%20Hz.%20As%20a%20new%20type%20of%20visual%20expression%2C%20spike%20sequence%20has%0Ahigh%20spatiotemporal%20completeness%20and%20preserves%20the%20continuous%20visual%0Ainformation%20of%20the%20external%20world.%20Taking%20advantage%20of%20the%20low%20latency%20and%20high%0Adynamic%20range%20of%20the%20spike%20camera%2C%20many%20spike-based%20algorithms%20have%20made%0Asignificant%20progress%2C%20such%20as%20high-quality%20imaging%20and%20ultra-high-speed%20target%0Adetection.%0A%20%20To%20build%20up%20a%20community%20ecology%20for%20the%20spike%20vision%20to%20facilitate%20more%20users%0Ato%20take%20advantage%20of%20the%20spike%20camera%2C%20SpikeCV%20provides%20a%20variety%20of%0Aultra-high-speed%20scene%20datasets%2C%20hardware%20interfaces%2C%20and%20an%20easy-to-use%0Amodules%20library.%20SpikeCV%20focuses%20on%20encapsulation%20for%20spike%20data%2C%0Astandardization%20for%20dataset%20interfaces%2C%20modularization%20for%20vision%20tasks%2C%20and%0Areal-time%20applications%20for%20challenging%20scenes.%20With%20the%20advent%20of%20the%0Aopen-source%20Python%20ecosystem%2C%20modules%20of%20SpikeCV%20can%20be%20used%20as%20a%20Python%0Alibrary%20to%20fulfilled%20most%20of%20the%20numerical%20analysis%20needs%20of%20researchers.%20We%0Ademonstrate%20the%20efficiency%20of%20the%20SpikeCV%20on%20offline%20inference%20and%20real-time%0Aapplications.%20The%20project%20repository%20address%20are%0A%5Curl%7Bhttps%3A//openi.pcl.ac.cn/Cordium/SpikeCV%7D%20and%0A%5Curl%7Bhttps%3A//github.com/Zyj061/SpikeCV%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.11684v2&entry.124074799=Read"},
{"title": "DyGPrompt: Learning Feature and Time Prompts on Dynamic Graphs", "author": "Xingtong Yu and Zhenghao Liu and Yuan Fang and Xinming Zhang", "abstract": "  Dynamic graphs are pervasive in the real world, modeling dynamic relations\nbetween objects across various fields. For dynamic graph modeling, dynamic\ngraph neural networks (DGNNs) have emerged as a mainstream technique, which are\ngenerally pre-trained on the link prediction task, leaving a significant gap\nfrom the objectives of downstream tasks such as node classification. To bridge\nthe gap, prompt-based learning has gained traction on graphs. However, existing\nefforts focus on static graphs, neglecting the evolution of dynamic graphs. In\nthis paper, we propose DyGPrompt, a novel pre-training and prompting framework\nfor dynamic graph modeling. First, we design dual prompts to address the gap in\nboth task objectives and dynamic variations across pre-training and downstream\ntasks. Second, we recognize that node and time features mutually characterize\neach other, and propose dual condition-nets to model the evolving node-time\npatterns in downstream tasks. Finally, we thoroughly evaluate and analyze\nDyGPrompt through extensive experiments on three public datasets.\n", "link": "http://arxiv.org/abs/2405.13937v3", "date": "2024-05-28", "relevancy": 2.4949, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5619}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4703}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyGPrompt%3A%20Learning%20Feature%20and%20Time%20Prompts%20on%20Dynamic%20Graphs&body=Title%3A%20DyGPrompt%3A%20Learning%20Feature%20and%20Time%20Prompts%20on%20Dynamic%20Graphs%0AAuthor%3A%20Xingtong%20Yu%20and%20Zhenghao%20Liu%20and%20Yuan%20Fang%20and%20Xinming%20Zhang%0AAbstract%3A%20%20%20Dynamic%20graphs%20are%20pervasive%20in%20the%20real%20world%2C%20modeling%20dynamic%20relations%0Abetween%20objects%20across%20various%20fields.%20For%20dynamic%20graph%20modeling%2C%20dynamic%0Agraph%20neural%20networks%20%28DGNNs%29%20have%20emerged%20as%20a%20mainstream%20technique%2C%20which%20are%0Agenerally%20pre-trained%20on%20the%20link%20prediction%20task%2C%20leaving%20a%20significant%20gap%0Afrom%20the%20objectives%20of%20downstream%20tasks%20such%20as%20node%20classification.%20To%20bridge%0Athe%20gap%2C%20prompt-based%20learning%20has%20gained%20traction%20on%20graphs.%20However%2C%20existing%0Aefforts%20focus%20on%20static%20graphs%2C%20neglecting%20the%20evolution%20of%20dynamic%20graphs.%20In%0Athis%20paper%2C%20we%20propose%20DyGPrompt%2C%20a%20novel%20pre-training%20and%20prompting%20framework%0Afor%20dynamic%20graph%20modeling.%20First%2C%20we%20design%20dual%20prompts%20to%20address%20the%20gap%20in%0Aboth%20task%20objectives%20and%20dynamic%20variations%20across%20pre-training%20and%20downstream%0Atasks.%20Second%2C%20we%20recognize%20that%20node%20and%20time%20features%20mutually%20characterize%0Aeach%20other%2C%20and%20propose%20dual%20condition-nets%20to%20model%20the%20evolving%20node-time%0Apatterns%20in%20downstream%20tasks.%20Finally%2C%20we%20thoroughly%20evaluate%20and%20analyze%0ADyGPrompt%20through%20extensive%20experiments%20on%20three%20public%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13937v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyGPrompt%253A%2520Learning%2520Feature%2520and%2520Time%2520Prompts%2520on%2520Dynamic%2520Graphs%26entry.906535625%3DXingtong%2520Yu%2520and%2520Zhenghao%2520Liu%2520and%2520Yuan%2520Fang%2520and%2520Xinming%2520Zhang%26entry.1292438233%3D%2520%2520Dynamic%2520graphs%2520are%2520pervasive%2520in%2520the%2520real%2520world%252C%2520modeling%2520dynamic%2520relations%250Abetween%2520objects%2520across%2520various%2520fields.%2520For%2520dynamic%2520graph%2520modeling%252C%2520dynamic%250Agraph%2520neural%2520networks%2520%2528DGNNs%2529%2520have%2520emerged%2520as%2520a%2520mainstream%2520technique%252C%2520which%2520are%250Agenerally%2520pre-trained%2520on%2520the%2520link%2520prediction%2520task%252C%2520leaving%2520a%2520significant%2520gap%250Afrom%2520the%2520objectives%2520of%2520downstream%2520tasks%2520such%2520as%2520node%2520classification.%2520To%2520bridge%250Athe%2520gap%252C%2520prompt-based%2520learning%2520has%2520gained%2520traction%2520on%2520graphs.%2520However%252C%2520existing%250Aefforts%2520focus%2520on%2520static%2520graphs%252C%2520neglecting%2520the%2520evolution%2520of%2520dynamic%2520graphs.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520DyGPrompt%252C%2520a%2520novel%2520pre-training%2520and%2520prompting%2520framework%250Afor%2520dynamic%2520graph%2520modeling.%2520First%252C%2520we%2520design%2520dual%2520prompts%2520to%2520address%2520the%2520gap%2520in%250Aboth%2520task%2520objectives%2520and%2520dynamic%2520variations%2520across%2520pre-training%2520and%2520downstream%250Atasks.%2520Second%252C%2520we%2520recognize%2520that%2520node%2520and%2520time%2520features%2520mutually%2520characterize%250Aeach%2520other%252C%2520and%2520propose%2520dual%2520condition-nets%2520to%2520model%2520the%2520evolving%2520node-time%250Apatterns%2520in%2520downstream%2520tasks.%2520Finally%252C%2520we%2520thoroughly%2520evaluate%2520and%2520analyze%250ADyGPrompt%2520through%2520extensive%2520experiments%2520on%2520three%2520public%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13937v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyGPrompt%3A%20Learning%20Feature%20and%20Time%20Prompts%20on%20Dynamic%20Graphs&entry.906535625=Xingtong%20Yu%20and%20Zhenghao%20Liu%20and%20Yuan%20Fang%20and%20Xinming%20Zhang&entry.1292438233=%20%20Dynamic%20graphs%20are%20pervasive%20in%20the%20real%20world%2C%20modeling%20dynamic%20relations%0Abetween%20objects%20across%20various%20fields.%20For%20dynamic%20graph%20modeling%2C%20dynamic%0Agraph%20neural%20networks%20%28DGNNs%29%20have%20emerged%20as%20a%20mainstream%20technique%2C%20which%20are%0Agenerally%20pre-trained%20on%20the%20link%20prediction%20task%2C%20leaving%20a%20significant%20gap%0Afrom%20the%20objectives%20of%20downstream%20tasks%20such%20as%20node%20classification.%20To%20bridge%0Athe%20gap%2C%20prompt-based%20learning%20has%20gained%20traction%20on%20graphs.%20However%2C%20existing%0Aefforts%20focus%20on%20static%20graphs%2C%20neglecting%20the%20evolution%20of%20dynamic%20graphs.%20In%0Athis%20paper%2C%20we%20propose%20DyGPrompt%2C%20a%20novel%20pre-training%20and%20prompting%20framework%0Afor%20dynamic%20graph%20modeling.%20First%2C%20we%20design%20dual%20prompts%20to%20address%20the%20gap%20in%0Aboth%20task%20objectives%20and%20dynamic%20variations%20across%20pre-training%20and%20downstream%0Atasks.%20Second%2C%20we%20recognize%20that%20node%20and%20time%20features%20mutually%20characterize%0Aeach%20other%2C%20and%20propose%20dual%20condition-nets%20to%20model%20the%20evolving%20node-time%0Apatterns%20in%20downstream%20tasks.%20Finally%2C%20we%20thoroughly%20evaluate%20and%20analyze%0ADyGPrompt%20through%20extensive%20experiments%20on%20three%20public%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13937v3&entry.124074799=Read"},
{"title": "BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models", "author": "Haotian Sun and Yuchen Zhuang and Wei Wei and Chao Zhang and Bo Dai", "abstract": "  Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini\nfor specific tasks is challenging. Due to the opacity in their parameters,\nembeddings, and even output probabilities, existing fine-tuning adaptation\nmethods are inapplicable. Consequently, adapting these black-box LLMs is only\npossible through their API services, raising concerns about transparency,\nprivacy, and cost. To address these challenges, we introduce BBox-Adapter, a\nnovel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target\nand source domain data by treating target data as positive and source data as\nnegative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to\npromote the likelihood of target domain data while penalizing that of the\nsource domain. Furthermore, it features an online adaptation mechanism, which\nincorporates real-time positive data sampling from ground-truth, human, or AI\nfeedback, coupled with negative data from previous adaptations. Extensive\nexperiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It\nimproves model performance by up to 6.77% across diverse tasks and domains,\nwhile reducing training and inference costs by 31.30x and 1.84x, respectively.\n", "link": "http://arxiv.org/abs/2402.08219v2", "date": "2024-05-28", "relevancy": 2.48, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5354}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4974}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BBox-Adapter%3A%20Lightweight%20Adapting%20for%20Black-Box%20Large%20Language%20Models&body=Title%3A%20BBox-Adapter%3A%20Lightweight%20Adapting%20for%20Black-Box%20Large%20Language%20Models%0AAuthor%3A%20Haotian%20Sun%20and%20Yuchen%20Zhuang%20and%20Wei%20Wei%20and%20Chao%20Zhang%20and%20Bo%20Dai%0AAbstract%3A%20%20%20Adapting%20state-of-the-art%20Large%20Language%20Models%20%28LLMs%29%20like%20GPT-4%20and%20Gemini%0Afor%20specific%20tasks%20is%20challenging.%20Due%20to%20the%20opacity%20in%20their%20parameters%2C%0Aembeddings%2C%20and%20even%20output%20probabilities%2C%20existing%20fine-tuning%20adaptation%0Amethods%20are%20inapplicable.%20Consequently%2C%20adapting%20these%20black-box%20LLMs%20is%20only%0Apossible%20through%20their%20API%20services%2C%20raising%20concerns%20about%20transparency%2C%0Aprivacy%2C%20and%20cost.%20To%20address%20these%20challenges%2C%20we%20introduce%20BBox-Adapter%2C%20a%0Anovel%20lightweight%20adapter%20for%20black-box%20LLMs.%20BBox-Adapter%20distinguishes%20target%0Aand%20source%20domain%20data%20by%20treating%20target%20data%20as%20positive%20and%20source%20data%20as%0Anegative.%20It%20employs%20a%20ranking-based%20Noise%20Contrastive%20Estimation%20%28NCE%29%20loss%20to%0Apromote%20the%20likelihood%20of%20target%20domain%20data%20while%20penalizing%20that%20of%20the%0Asource%20domain.%20Furthermore%2C%20it%20features%20an%20online%20adaptation%20mechanism%2C%20which%0Aincorporates%20real-time%20positive%20data%20sampling%20from%20ground-truth%2C%20human%2C%20or%20AI%0Afeedback%2C%20coupled%20with%20negative%20data%20from%20previous%20adaptations.%20Extensive%0Aexperiments%20demonstrate%20BBox-Adapter%27s%20effectiveness%20and%20cost%20efficiency.%20It%0Aimproves%20model%20performance%20by%20up%20to%206.77%25%20across%20diverse%20tasks%20and%20domains%2C%0Awhile%20reducing%20training%20and%20inference%20costs%20by%2031.30x%20and%201.84x%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08219v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBBox-Adapter%253A%2520Lightweight%2520Adapting%2520for%2520Black-Box%2520Large%2520Language%2520Models%26entry.906535625%3DHaotian%2520Sun%2520and%2520Yuchen%2520Zhuang%2520and%2520Wei%2520Wei%2520and%2520Chao%2520Zhang%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520Adapting%2520state-of-the-art%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520like%2520GPT-4%2520and%2520Gemini%250Afor%2520specific%2520tasks%2520is%2520challenging.%2520Due%2520to%2520the%2520opacity%2520in%2520their%2520parameters%252C%250Aembeddings%252C%2520and%2520even%2520output%2520probabilities%252C%2520existing%2520fine-tuning%2520adaptation%250Amethods%2520are%2520inapplicable.%2520Consequently%252C%2520adapting%2520these%2520black-box%2520LLMs%2520is%2520only%250Apossible%2520through%2520their%2520API%2520services%252C%2520raising%2520concerns%2520about%2520transparency%252C%250Aprivacy%252C%2520and%2520cost.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520BBox-Adapter%252C%2520a%250Anovel%2520lightweight%2520adapter%2520for%2520black-box%2520LLMs.%2520BBox-Adapter%2520distinguishes%2520target%250Aand%2520source%2520domain%2520data%2520by%2520treating%2520target%2520data%2520as%2520positive%2520and%2520source%2520data%2520as%250Anegative.%2520It%2520employs%2520a%2520ranking-based%2520Noise%2520Contrastive%2520Estimation%2520%2528NCE%2529%2520loss%2520to%250Apromote%2520the%2520likelihood%2520of%2520target%2520domain%2520data%2520while%2520penalizing%2520that%2520of%2520the%250Asource%2520domain.%2520Furthermore%252C%2520it%2520features%2520an%2520online%2520adaptation%2520mechanism%252C%2520which%250Aincorporates%2520real-time%2520positive%2520data%2520sampling%2520from%2520ground-truth%252C%2520human%252C%2520or%2520AI%250Afeedback%252C%2520coupled%2520with%2520negative%2520data%2520from%2520previous%2520adaptations.%2520Extensive%250Aexperiments%2520demonstrate%2520BBox-Adapter%2527s%2520effectiveness%2520and%2520cost%2520efficiency.%2520It%250Aimproves%2520model%2520performance%2520by%2520up%2520to%25206.77%2525%2520across%2520diverse%2520tasks%2520and%2520domains%252C%250Awhile%2520reducing%2520training%2520and%2520inference%2520costs%2520by%252031.30x%2520and%25201.84x%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08219v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BBox-Adapter%3A%20Lightweight%20Adapting%20for%20Black-Box%20Large%20Language%20Models&entry.906535625=Haotian%20Sun%20and%20Yuchen%20Zhuang%20and%20Wei%20Wei%20and%20Chao%20Zhang%20and%20Bo%20Dai&entry.1292438233=%20%20Adapting%20state-of-the-art%20Large%20Language%20Models%20%28LLMs%29%20like%20GPT-4%20and%20Gemini%0Afor%20specific%20tasks%20is%20challenging.%20Due%20to%20the%20opacity%20in%20their%20parameters%2C%0Aembeddings%2C%20and%20even%20output%20probabilities%2C%20existing%20fine-tuning%20adaptation%0Amethods%20are%20inapplicable.%20Consequently%2C%20adapting%20these%20black-box%20LLMs%20is%20only%0Apossible%20through%20their%20API%20services%2C%20raising%20concerns%20about%20transparency%2C%0Aprivacy%2C%20and%20cost.%20To%20address%20these%20challenges%2C%20we%20introduce%20BBox-Adapter%2C%20a%0Anovel%20lightweight%20adapter%20for%20black-box%20LLMs.%20BBox-Adapter%20distinguishes%20target%0Aand%20source%20domain%20data%20by%20treating%20target%20data%20as%20positive%20and%20source%20data%20as%0Anegative.%20It%20employs%20a%20ranking-based%20Noise%20Contrastive%20Estimation%20%28NCE%29%20loss%20to%0Apromote%20the%20likelihood%20of%20target%20domain%20data%20while%20penalizing%20that%20of%20the%0Asource%20domain.%20Furthermore%2C%20it%20features%20an%20online%20adaptation%20mechanism%2C%20which%0Aincorporates%20real-time%20positive%20data%20sampling%20from%20ground-truth%2C%20human%2C%20or%20AI%0Afeedback%2C%20coupled%20with%20negative%20data%20from%20previous%20adaptations.%20Extensive%0Aexperiments%20demonstrate%20BBox-Adapter%27s%20effectiveness%20and%20cost%20efficiency.%20It%0Aimproves%20model%20performance%20by%20up%20to%206.77%25%20across%20diverse%20tasks%20and%20domains%2C%0Awhile%20reducing%20training%20and%20inference%20costs%20by%2031.30x%20and%201.84x%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08219v2&entry.124074799=Read"},
{"title": "Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?", "author": "Yifan Bai and Dongming Wu and Yingfei Liu and Fan Jia and Weixin Mao and Ziheng Zhang and Yucheng Zhao and Jianbing Shen and Xing Wei and Tiancai Wang and Xiangyu Zhang", "abstract": "  Rapid advancements in Autonomous Driving (AD) tasks turned a significant\nshift toward end-to-end fashion, particularly in the utilization of\nvision-language models (VLMs) that integrate robust logical reasoning and\ncognitive abilities to enable comprehensive end-to-end planning. However, these\nVLM-based approaches tend to integrate 2D vision tokenizers and a large\nlanguage model (LLM) for ego-car planning, which lack 3D geometric priors as a\ncornerstone of reliable planning. Naturally, this observation raises a critical\nconcern: Can a 2D-tokenized LLM accurately perceive the 3D environment? Our\nevaluation of current VLM-based methods across 3D object detection, vectorized\nmap construction, and environmental caption suggests that the answer is,\nunfortunately, NO. In other words, 2D-tokenized LLM fails to provide reliable\nautonomous driving. In response, we introduce DETR-style 3D perceptrons as 3D\ntokenizers, which connect LLM with a one-layer linear projector. This simple\nyet elegant strategy, termed Atlas, harnesses the inherent priors of the 3D\nphysical world, enabling it to simultaneously process high-resolution\nmulti-view images and employ spatiotemporal modeling. Despite its simplicity,\nAtlas demonstrates superior performance in both 3D detection and ego planning\ntasks on nuScenes dataset, proving that 3D-tokenized LLM is the key to reliable\nautonomous driving. The code and datasets will be released.\n", "link": "http://arxiv.org/abs/2405.18361v1", "date": "2024-05-28", "relevancy": 2.4421, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6182}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6111}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20a%203D-Tokenized%20LLM%20the%20Key%20to%20Reliable%20Autonomous%20Driving%3F&body=Title%3A%20Is%20a%203D-Tokenized%20LLM%20the%20Key%20to%20Reliable%20Autonomous%20Driving%3F%0AAuthor%3A%20Yifan%20Bai%20and%20Dongming%20Wu%20and%20Yingfei%20Liu%20and%20Fan%20Jia%20and%20Weixin%20Mao%20and%20Ziheng%20Zhang%20and%20Yucheng%20Zhao%20and%20Jianbing%20Shen%20and%20Xing%20Wei%20and%20Tiancai%20Wang%20and%20Xiangyu%20Zhang%0AAbstract%3A%20%20%20Rapid%20advancements%20in%20Autonomous%20Driving%20%28AD%29%20tasks%20turned%20a%20significant%0Ashift%20toward%20end-to-end%20fashion%2C%20particularly%20in%20the%20utilization%20of%0Avision-language%20models%20%28VLMs%29%20that%20integrate%20robust%20logical%20reasoning%20and%0Acognitive%20abilities%20to%20enable%20comprehensive%20end-to-end%20planning.%20However%2C%20these%0AVLM-based%20approaches%20tend%20to%20integrate%202D%20vision%20tokenizers%20and%20a%20large%0Alanguage%20model%20%28LLM%29%20for%20ego-car%20planning%2C%20which%20lack%203D%20geometric%20priors%20as%20a%0Acornerstone%20of%20reliable%20planning.%20Naturally%2C%20this%20observation%20raises%20a%20critical%0Aconcern%3A%20Can%20a%202D-tokenized%20LLM%20accurately%20perceive%20the%203D%20environment%3F%20Our%0Aevaluation%20of%20current%20VLM-based%20methods%20across%203D%20object%20detection%2C%20vectorized%0Amap%20construction%2C%20and%20environmental%20caption%20suggests%20that%20the%20answer%20is%2C%0Aunfortunately%2C%20NO.%20In%20other%20words%2C%202D-tokenized%20LLM%20fails%20to%20provide%20reliable%0Aautonomous%20driving.%20In%20response%2C%20we%20introduce%20DETR-style%203D%20perceptrons%20as%203D%0Atokenizers%2C%20which%20connect%20LLM%20with%20a%20one-layer%20linear%20projector.%20This%20simple%0Ayet%20elegant%20strategy%2C%20termed%20Atlas%2C%20harnesses%20the%20inherent%20priors%20of%20the%203D%0Aphysical%20world%2C%20enabling%20it%20to%20simultaneously%20process%20high-resolution%0Amulti-view%20images%20and%20employ%20spatiotemporal%20modeling.%20Despite%20its%20simplicity%2C%0AAtlas%20demonstrates%20superior%20performance%20in%20both%203D%20detection%20and%20ego%20planning%0Atasks%20on%20nuScenes%20dataset%2C%20proving%20that%203D-tokenized%20LLM%20is%20the%20key%20to%20reliable%0Aautonomous%20driving.%20The%20code%20and%20datasets%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520a%25203D-Tokenized%2520LLM%2520the%2520Key%2520to%2520Reliable%2520Autonomous%2520Driving%253F%26entry.906535625%3DYifan%2520Bai%2520and%2520Dongming%2520Wu%2520and%2520Yingfei%2520Liu%2520and%2520Fan%2520Jia%2520and%2520Weixin%2520Mao%2520and%2520Ziheng%2520Zhang%2520and%2520Yucheng%2520Zhao%2520and%2520Jianbing%2520Shen%2520and%2520Xing%2520Wei%2520and%2520Tiancai%2520Wang%2520and%2520Xiangyu%2520Zhang%26entry.1292438233%3D%2520%2520Rapid%2520advancements%2520in%2520Autonomous%2520Driving%2520%2528AD%2529%2520tasks%2520turned%2520a%2520significant%250Ashift%2520toward%2520end-to-end%2520fashion%252C%2520particularly%2520in%2520the%2520utilization%2520of%250Avision-language%2520models%2520%2528VLMs%2529%2520that%2520integrate%2520robust%2520logical%2520reasoning%2520and%250Acognitive%2520abilities%2520to%2520enable%2520comprehensive%2520end-to-end%2520planning.%2520However%252C%2520these%250AVLM-based%2520approaches%2520tend%2520to%2520integrate%25202D%2520vision%2520tokenizers%2520and%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520for%2520ego-car%2520planning%252C%2520which%2520lack%25203D%2520geometric%2520priors%2520as%2520a%250Acornerstone%2520of%2520reliable%2520planning.%2520Naturally%252C%2520this%2520observation%2520raises%2520a%2520critical%250Aconcern%253A%2520Can%2520a%25202D-tokenized%2520LLM%2520accurately%2520perceive%2520the%25203D%2520environment%253F%2520Our%250Aevaluation%2520of%2520current%2520VLM-based%2520methods%2520across%25203D%2520object%2520detection%252C%2520vectorized%250Amap%2520construction%252C%2520and%2520environmental%2520caption%2520suggests%2520that%2520the%2520answer%2520is%252C%250Aunfortunately%252C%2520NO.%2520In%2520other%2520words%252C%25202D-tokenized%2520LLM%2520fails%2520to%2520provide%2520reliable%250Aautonomous%2520driving.%2520In%2520response%252C%2520we%2520introduce%2520DETR-style%25203D%2520perceptrons%2520as%25203D%250Atokenizers%252C%2520which%2520connect%2520LLM%2520with%2520a%2520one-layer%2520linear%2520projector.%2520This%2520simple%250Ayet%2520elegant%2520strategy%252C%2520termed%2520Atlas%252C%2520harnesses%2520the%2520inherent%2520priors%2520of%2520the%25203D%250Aphysical%2520world%252C%2520enabling%2520it%2520to%2520simultaneously%2520process%2520high-resolution%250Amulti-view%2520images%2520and%2520employ%2520spatiotemporal%2520modeling.%2520Despite%2520its%2520simplicity%252C%250AAtlas%2520demonstrates%2520superior%2520performance%2520in%2520both%25203D%2520detection%2520and%2520ego%2520planning%250Atasks%2520on%2520nuScenes%2520dataset%252C%2520proving%2520that%25203D-tokenized%2520LLM%2520is%2520the%2520key%2520to%2520reliable%250Aautonomous%2520driving.%2520The%2520code%2520and%2520datasets%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20a%203D-Tokenized%20LLM%20the%20Key%20to%20Reliable%20Autonomous%20Driving%3F&entry.906535625=Yifan%20Bai%20and%20Dongming%20Wu%20and%20Yingfei%20Liu%20and%20Fan%20Jia%20and%20Weixin%20Mao%20and%20Ziheng%20Zhang%20and%20Yucheng%20Zhao%20and%20Jianbing%20Shen%20and%20Xing%20Wei%20and%20Tiancai%20Wang%20and%20Xiangyu%20Zhang&entry.1292438233=%20%20Rapid%20advancements%20in%20Autonomous%20Driving%20%28AD%29%20tasks%20turned%20a%20significant%0Ashift%20toward%20end-to-end%20fashion%2C%20particularly%20in%20the%20utilization%20of%0Avision-language%20models%20%28VLMs%29%20that%20integrate%20robust%20logical%20reasoning%20and%0Acognitive%20abilities%20to%20enable%20comprehensive%20end-to-end%20planning.%20However%2C%20these%0AVLM-based%20approaches%20tend%20to%20integrate%202D%20vision%20tokenizers%20and%20a%20large%0Alanguage%20model%20%28LLM%29%20for%20ego-car%20planning%2C%20which%20lack%203D%20geometric%20priors%20as%20a%0Acornerstone%20of%20reliable%20planning.%20Naturally%2C%20this%20observation%20raises%20a%20critical%0Aconcern%3A%20Can%20a%202D-tokenized%20LLM%20accurately%20perceive%20the%203D%20environment%3F%20Our%0Aevaluation%20of%20current%20VLM-based%20methods%20across%203D%20object%20detection%2C%20vectorized%0Amap%20construction%2C%20and%20environmental%20caption%20suggests%20that%20the%20answer%20is%2C%0Aunfortunately%2C%20NO.%20In%20other%20words%2C%202D-tokenized%20LLM%20fails%20to%20provide%20reliable%0Aautonomous%20driving.%20In%20response%2C%20we%20introduce%20DETR-style%203D%20perceptrons%20as%203D%0Atokenizers%2C%20which%20connect%20LLM%20with%20a%20one-layer%20linear%20projector.%20This%20simple%0Ayet%20elegant%20strategy%2C%20termed%20Atlas%2C%20harnesses%20the%20inherent%20priors%20of%20the%203D%0Aphysical%20world%2C%20enabling%20it%20to%20simultaneously%20process%20high-resolution%0Amulti-view%20images%20and%20employ%20spatiotemporal%20modeling.%20Despite%20its%20simplicity%2C%0AAtlas%20demonstrates%20superior%20performance%20in%20both%203D%20detection%20and%20ego%20planning%0Atasks%20on%20nuScenes%20dataset%2C%20proving%20that%203D-tokenized%20LLM%20is%20the%20key%20to%20reliable%0Aautonomous%20driving.%20The%20code%20and%20datasets%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18361v1&entry.124074799=Read"},
{"title": "Bridging Mini-Batch and Asymptotic Analysis in Contrastive Learning:\n  From InfoNCE to Kernel-Based Losses", "author": "Panagiotis Koromilas and Giorgos Bouritsas and Theodoros Giannakopoulos and Mihalis Nicolaou and Yannis Panagakis", "abstract": "  What do different contrastive learning (CL) losses actually optimize for?\nAlthough multiple CL methods have demonstrated remarkable representation\nlearning capabilities, the differences in their inner workings remain largely\nopaque. In this work, we analyse several CL families and prove that, under\ncertain conditions, they admit the same minimisers when optimizing either their\nbatch-level objectives or their expectations asymptotically. In both cases, an\nintimate connection with the hyperspherical energy minimisation (HEM) problem\nresurfaces. Drawing inspiration from this, we introduce a novel CL objective,\ncoined Decoupled Hyperspherical Energy Loss (DHEL). DHEL simplifies the problem\nby decoupling the target hyperspherical energy from the alignment of positive\nexamples while preserving the same theoretical guarantees. Going one step\nfurther, we show the same results hold for another relevant CL family, namely\nkernel contrastive learning (KCL), with the additional advantage of the\nexpected loss being independent of batch size, thus identifying the minimisers\nin the non-asymptotic regime. Empirical results demonstrate improved downstream\nperformance and robustness across combinations of different batch sizes and\nhyperparameters and reduced dimensionality collapse, on several computer vision\ndatasets.\n", "link": "http://arxiv.org/abs/2405.18045v1", "date": "2024-05-28", "relevancy": 2.4388, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4907}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4877}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Mini-Batch%20and%20Asymptotic%20Analysis%20in%20Contrastive%20Learning%3A%0A%20%20From%20InfoNCE%20to%20Kernel-Based%20Losses&body=Title%3A%20Bridging%20Mini-Batch%20and%20Asymptotic%20Analysis%20in%20Contrastive%20Learning%3A%0A%20%20From%20InfoNCE%20to%20Kernel-Based%20Losses%0AAuthor%3A%20Panagiotis%20Koromilas%20and%20Giorgos%20Bouritsas%20and%20Theodoros%20Giannakopoulos%20and%20Mihalis%20Nicolaou%20and%20Yannis%20Panagakis%0AAbstract%3A%20%20%20What%20do%20different%20contrastive%20learning%20%28CL%29%20losses%20actually%20optimize%20for%3F%0AAlthough%20multiple%20CL%20methods%20have%20demonstrated%20remarkable%20representation%0Alearning%20capabilities%2C%20the%20differences%20in%20their%20inner%20workings%20remain%20largely%0Aopaque.%20In%20this%20work%2C%20we%20analyse%20several%20CL%20families%20and%20prove%20that%2C%20under%0Acertain%20conditions%2C%20they%20admit%20the%20same%20minimisers%20when%20optimizing%20either%20their%0Abatch-level%20objectives%20or%20their%20expectations%20asymptotically.%20In%20both%20cases%2C%20an%0Aintimate%20connection%20with%20the%20hyperspherical%20energy%20minimisation%20%28HEM%29%20problem%0Aresurfaces.%20Drawing%20inspiration%20from%20this%2C%20we%20introduce%20a%20novel%20CL%20objective%2C%0Acoined%20Decoupled%20Hyperspherical%20Energy%20Loss%20%28DHEL%29.%20DHEL%20simplifies%20the%20problem%0Aby%20decoupling%20the%20target%20hyperspherical%20energy%20from%20the%20alignment%20of%20positive%0Aexamples%20while%20preserving%20the%20same%20theoretical%20guarantees.%20Going%20one%20step%0Afurther%2C%20we%20show%20the%20same%20results%20hold%20for%20another%20relevant%20CL%20family%2C%20namely%0Akernel%20contrastive%20learning%20%28KCL%29%2C%20with%20the%20additional%20advantage%20of%20the%0Aexpected%20loss%20being%20independent%20of%20batch%20size%2C%20thus%20identifying%20the%20minimisers%0Ain%20the%20non-asymptotic%20regime.%20Empirical%20results%20demonstrate%20improved%20downstream%0Aperformance%20and%20robustness%20across%20combinations%20of%20different%20batch%20sizes%20and%0Ahyperparameters%20and%20reduced%20dimensionality%20collapse%2C%20on%20several%20computer%20vision%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Mini-Batch%2520and%2520Asymptotic%2520Analysis%2520in%2520Contrastive%2520Learning%253A%250A%2520%2520From%2520InfoNCE%2520to%2520Kernel-Based%2520Losses%26entry.906535625%3DPanagiotis%2520Koromilas%2520and%2520Giorgos%2520Bouritsas%2520and%2520Theodoros%2520Giannakopoulos%2520and%2520Mihalis%2520Nicolaou%2520and%2520Yannis%2520Panagakis%26entry.1292438233%3D%2520%2520What%2520do%2520different%2520contrastive%2520learning%2520%2528CL%2529%2520losses%2520actually%2520optimize%2520for%253F%250AAlthough%2520multiple%2520CL%2520methods%2520have%2520demonstrated%2520remarkable%2520representation%250Alearning%2520capabilities%252C%2520the%2520differences%2520in%2520their%2520inner%2520workings%2520remain%2520largely%250Aopaque.%2520In%2520this%2520work%252C%2520we%2520analyse%2520several%2520CL%2520families%2520and%2520prove%2520that%252C%2520under%250Acertain%2520conditions%252C%2520they%2520admit%2520the%2520same%2520minimisers%2520when%2520optimizing%2520either%2520their%250Abatch-level%2520objectives%2520or%2520their%2520expectations%2520asymptotically.%2520In%2520both%2520cases%252C%2520an%250Aintimate%2520connection%2520with%2520the%2520hyperspherical%2520energy%2520minimisation%2520%2528HEM%2529%2520problem%250Aresurfaces.%2520Drawing%2520inspiration%2520from%2520this%252C%2520we%2520introduce%2520a%2520novel%2520CL%2520objective%252C%250Acoined%2520Decoupled%2520Hyperspherical%2520Energy%2520Loss%2520%2528DHEL%2529.%2520DHEL%2520simplifies%2520the%2520problem%250Aby%2520decoupling%2520the%2520target%2520hyperspherical%2520energy%2520from%2520the%2520alignment%2520of%2520positive%250Aexamples%2520while%2520preserving%2520the%2520same%2520theoretical%2520guarantees.%2520Going%2520one%2520step%250Afurther%252C%2520we%2520show%2520the%2520same%2520results%2520hold%2520for%2520another%2520relevant%2520CL%2520family%252C%2520namely%250Akernel%2520contrastive%2520learning%2520%2528KCL%2529%252C%2520with%2520the%2520additional%2520advantage%2520of%2520the%250Aexpected%2520loss%2520being%2520independent%2520of%2520batch%2520size%252C%2520thus%2520identifying%2520the%2520minimisers%250Ain%2520the%2520non-asymptotic%2520regime.%2520Empirical%2520results%2520demonstrate%2520improved%2520downstream%250Aperformance%2520and%2520robustness%2520across%2520combinations%2520of%2520different%2520batch%2520sizes%2520and%250Ahyperparameters%2520and%2520reduced%2520dimensionality%2520collapse%252C%2520on%2520several%2520computer%2520vision%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Mini-Batch%20and%20Asymptotic%20Analysis%20in%20Contrastive%20Learning%3A%0A%20%20From%20InfoNCE%20to%20Kernel-Based%20Losses&entry.906535625=Panagiotis%20Koromilas%20and%20Giorgos%20Bouritsas%20and%20Theodoros%20Giannakopoulos%20and%20Mihalis%20Nicolaou%20and%20Yannis%20Panagakis&entry.1292438233=%20%20What%20do%20different%20contrastive%20learning%20%28CL%29%20losses%20actually%20optimize%20for%3F%0AAlthough%20multiple%20CL%20methods%20have%20demonstrated%20remarkable%20representation%0Alearning%20capabilities%2C%20the%20differences%20in%20their%20inner%20workings%20remain%20largely%0Aopaque.%20In%20this%20work%2C%20we%20analyse%20several%20CL%20families%20and%20prove%20that%2C%20under%0Acertain%20conditions%2C%20they%20admit%20the%20same%20minimisers%20when%20optimizing%20either%20their%0Abatch-level%20objectives%20or%20their%20expectations%20asymptotically.%20In%20both%20cases%2C%20an%0Aintimate%20connection%20with%20the%20hyperspherical%20energy%20minimisation%20%28HEM%29%20problem%0Aresurfaces.%20Drawing%20inspiration%20from%20this%2C%20we%20introduce%20a%20novel%20CL%20objective%2C%0Acoined%20Decoupled%20Hyperspherical%20Energy%20Loss%20%28DHEL%29.%20DHEL%20simplifies%20the%20problem%0Aby%20decoupling%20the%20target%20hyperspherical%20energy%20from%20the%20alignment%20of%20positive%0Aexamples%20while%20preserving%20the%20same%20theoretical%20guarantees.%20Going%20one%20step%0Afurther%2C%20we%20show%20the%20same%20results%20hold%20for%20another%20relevant%20CL%20family%2C%20namely%0Akernel%20contrastive%20learning%20%28KCL%29%2C%20with%20the%20additional%20advantage%20of%20the%0Aexpected%20loss%20being%20independent%20of%20batch%20size%2C%20thus%20identifying%20the%20minimisers%0Ain%20the%20non-asymptotic%20regime.%20Empirical%20results%20demonstrate%20improved%20downstream%0Aperformance%20and%20robustness%20across%20combinations%20of%20different%20batch%20sizes%20and%0Ahyperparameters%20and%20reduced%20dimensionality%20collapse%2C%20on%20several%20computer%20vision%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18045v1&entry.124074799=Read"},
{"title": "Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single\n  Process", "author": "Ermo Hua and Biqing Qi and Kaiyan Zhang and Yue Yu and Ning Ding and Xingtai Lv and Kai Tian and Bowen Zhou", "abstract": "  Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are two\nfundamental processes for enhancing the capabilities of Language Models (LMs)\npost pre-training, aligning them better with human preferences. Although SFT\nadvances in training efficiency, PO delivers better alignment, thus they are\noften combined. However, common practices simply apply them sequentially\nwithout integrating their optimization objectives, ignoring the opportunities\nto bridge their paradigm gap and take the strengths from both. To obtain a\nunified understanding, we interpret SFT and PO with two sub-processes --\nPreference Estimation and Transition Optimization -- defined at token level\nwithin the Markov Decision Process (MDP) framework. This modeling shows that\nSFT is only a specialized case of PO with inferior estimation and optimization.\nPO evaluates the quality of model's entire generated answer, whereas SFT only\nscores predicted tokens based on preceding tokens from target answers.\nTherefore, SFT overestimates the ability of model, leading to inferior\noptimization. Building on this view, we introduce Intuitive Fine-Tuning (IFT)\nto integrate SFT and Preference Optimization into a single process. IFT\ncaptures LMs' intuitive sense of the entire answers through a temporal residual\nconnection, but it solely relies on a single policy and the same volume of\nnon-preference-labeled data as SFT. Our experiments show that IFT performs\ncomparably or even superiorly to sequential recipes of SFT and some typical\nPreference Optimization methods across several tasks, particularly those\nrequires generation, reasoning, and fact-following abilities. An explainable\nFrozen Lake game further validates the effectiveness of IFT for getting\ncompetitive policy.\n", "link": "http://arxiv.org/abs/2405.11870v2", "date": "2024-05-28", "relevancy": 2.4215, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4937}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4853}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intuitive%20Fine-Tuning%3A%20Towards%20Simplifying%20Alignment%20into%20a%20Single%0A%20%20Process&body=Title%3A%20Intuitive%20Fine-Tuning%3A%20Towards%20Simplifying%20Alignment%20into%20a%20Single%0A%20%20Process%0AAuthor%3A%20Ermo%20Hua%20and%20Biqing%20Qi%20and%20Kaiyan%20Zhang%20and%20Yue%20Yu%20and%20Ning%20Ding%20and%20Xingtai%20Lv%20and%20Kai%20Tian%20and%20Bowen%20Zhou%0AAbstract%3A%20%20%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Preference%20Optimization%20%28PO%29%20are%20two%0Afundamental%20processes%20for%20enhancing%20the%20capabilities%20of%20Language%20Models%20%28LMs%29%0Apost%20pre-training%2C%20aligning%20them%20better%20with%20human%20preferences.%20Although%20SFT%0Aadvances%20in%20training%20efficiency%2C%20PO%20delivers%20better%20alignment%2C%20thus%20they%20are%0Aoften%20combined.%20However%2C%20common%20practices%20simply%20apply%20them%20sequentially%0Awithout%20integrating%20their%20optimization%20objectives%2C%20ignoring%20the%20opportunities%0Ato%20bridge%20their%20paradigm%20gap%20and%20take%20the%20strengths%20from%20both.%20To%20obtain%20a%0Aunified%20understanding%2C%20we%20interpret%20SFT%20and%20PO%20with%20two%20sub-processes%20--%0APreference%20Estimation%20and%20Transition%20Optimization%20--%20defined%20at%20token%20level%0Awithin%20the%20Markov%20Decision%20Process%20%28MDP%29%20framework.%20This%20modeling%20shows%20that%0ASFT%20is%20only%20a%20specialized%20case%20of%20PO%20with%20inferior%20estimation%20and%20optimization.%0APO%20evaluates%20the%20quality%20of%20model%27s%20entire%20generated%20answer%2C%20whereas%20SFT%20only%0Ascores%20predicted%20tokens%20based%20on%20preceding%20tokens%20from%20target%20answers.%0ATherefore%2C%20SFT%20overestimates%20the%20ability%20of%20model%2C%20leading%20to%20inferior%0Aoptimization.%20Building%20on%20this%20view%2C%20we%20introduce%20Intuitive%20Fine-Tuning%20%28IFT%29%0Ato%20integrate%20SFT%20and%20Preference%20Optimization%20into%20a%20single%20process.%20IFT%0Acaptures%20LMs%27%20intuitive%20sense%20of%20the%20entire%20answers%20through%20a%20temporal%20residual%0Aconnection%2C%20but%20it%20solely%20relies%20on%20a%20single%20policy%20and%20the%20same%20volume%20of%0Anon-preference-labeled%20data%20as%20SFT.%20Our%20experiments%20show%20that%20IFT%20performs%0Acomparably%20or%20even%20superiorly%20to%20sequential%20recipes%20of%20SFT%20and%20some%20typical%0APreference%20Optimization%20methods%20across%20several%20tasks%2C%20particularly%20those%0Arequires%20generation%2C%20reasoning%2C%20and%20fact-following%20abilities.%20An%20explainable%0AFrozen%20Lake%20game%20further%20validates%20the%20effectiveness%20of%20IFT%20for%20getting%0Acompetitive%20policy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntuitive%2520Fine-Tuning%253A%2520Towards%2520Simplifying%2520Alignment%2520into%2520a%2520Single%250A%2520%2520Process%26entry.906535625%3DErmo%2520Hua%2520and%2520Biqing%2520Qi%2520and%2520Kaiyan%2520Zhang%2520and%2520Yue%2520Yu%2520and%2520Ning%2520Ding%2520and%2520Xingtai%2520Lv%2520and%2520Kai%2520Tian%2520and%2520Bowen%2520Zhou%26entry.1292438233%3D%2520%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520and%2520Preference%2520Optimization%2520%2528PO%2529%2520are%2520two%250Afundamental%2520processes%2520for%2520enhancing%2520the%2520capabilities%2520of%2520Language%2520Models%2520%2528LMs%2529%250Apost%2520pre-training%252C%2520aligning%2520them%2520better%2520with%2520human%2520preferences.%2520Although%2520SFT%250Aadvances%2520in%2520training%2520efficiency%252C%2520PO%2520delivers%2520better%2520alignment%252C%2520thus%2520they%2520are%250Aoften%2520combined.%2520However%252C%2520common%2520practices%2520simply%2520apply%2520them%2520sequentially%250Awithout%2520integrating%2520their%2520optimization%2520objectives%252C%2520ignoring%2520the%2520opportunities%250Ato%2520bridge%2520their%2520paradigm%2520gap%2520and%2520take%2520the%2520strengths%2520from%2520both.%2520To%2520obtain%2520a%250Aunified%2520understanding%252C%2520we%2520interpret%2520SFT%2520and%2520PO%2520with%2520two%2520sub-processes%2520--%250APreference%2520Estimation%2520and%2520Transition%2520Optimization%2520--%2520defined%2520at%2520token%2520level%250Awithin%2520the%2520Markov%2520Decision%2520Process%2520%2528MDP%2529%2520framework.%2520This%2520modeling%2520shows%2520that%250ASFT%2520is%2520only%2520a%2520specialized%2520case%2520of%2520PO%2520with%2520inferior%2520estimation%2520and%2520optimization.%250APO%2520evaluates%2520the%2520quality%2520of%2520model%2527s%2520entire%2520generated%2520answer%252C%2520whereas%2520SFT%2520only%250Ascores%2520predicted%2520tokens%2520based%2520on%2520preceding%2520tokens%2520from%2520target%2520answers.%250ATherefore%252C%2520SFT%2520overestimates%2520the%2520ability%2520of%2520model%252C%2520leading%2520to%2520inferior%250Aoptimization.%2520Building%2520on%2520this%2520view%252C%2520we%2520introduce%2520Intuitive%2520Fine-Tuning%2520%2528IFT%2529%250Ato%2520integrate%2520SFT%2520and%2520Preference%2520Optimization%2520into%2520a%2520single%2520process.%2520IFT%250Acaptures%2520LMs%2527%2520intuitive%2520sense%2520of%2520the%2520entire%2520answers%2520through%2520a%2520temporal%2520residual%250Aconnection%252C%2520but%2520it%2520solely%2520relies%2520on%2520a%2520single%2520policy%2520and%2520the%2520same%2520volume%2520of%250Anon-preference-labeled%2520data%2520as%2520SFT.%2520Our%2520experiments%2520show%2520that%2520IFT%2520performs%250Acomparably%2520or%2520even%2520superiorly%2520to%2520sequential%2520recipes%2520of%2520SFT%2520and%2520some%2520typical%250APreference%2520Optimization%2520methods%2520across%2520several%2520tasks%252C%2520particularly%2520those%250Arequires%2520generation%252C%2520reasoning%252C%2520and%2520fact-following%2520abilities.%2520An%2520explainable%250AFrozen%2520Lake%2520game%2520further%2520validates%2520the%2520effectiveness%2520of%2520IFT%2520for%2520getting%250Acompetitive%2520policy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intuitive%20Fine-Tuning%3A%20Towards%20Simplifying%20Alignment%20into%20a%20Single%0A%20%20Process&entry.906535625=Ermo%20Hua%20and%20Biqing%20Qi%20and%20Kaiyan%20Zhang%20and%20Yue%20Yu%20and%20Ning%20Ding%20and%20Xingtai%20Lv%20and%20Kai%20Tian%20and%20Bowen%20Zhou&entry.1292438233=%20%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Preference%20Optimization%20%28PO%29%20are%20two%0Afundamental%20processes%20for%20enhancing%20the%20capabilities%20of%20Language%20Models%20%28LMs%29%0Apost%20pre-training%2C%20aligning%20them%20better%20with%20human%20preferences.%20Although%20SFT%0Aadvances%20in%20training%20efficiency%2C%20PO%20delivers%20better%20alignment%2C%20thus%20they%20are%0Aoften%20combined.%20However%2C%20common%20practices%20simply%20apply%20them%20sequentially%0Awithout%20integrating%20their%20optimization%20objectives%2C%20ignoring%20the%20opportunities%0Ato%20bridge%20their%20paradigm%20gap%20and%20take%20the%20strengths%20from%20both.%20To%20obtain%20a%0Aunified%20understanding%2C%20we%20interpret%20SFT%20and%20PO%20with%20two%20sub-processes%20--%0APreference%20Estimation%20and%20Transition%20Optimization%20--%20defined%20at%20token%20level%0Awithin%20the%20Markov%20Decision%20Process%20%28MDP%29%20framework.%20This%20modeling%20shows%20that%0ASFT%20is%20only%20a%20specialized%20case%20of%20PO%20with%20inferior%20estimation%20and%20optimization.%0APO%20evaluates%20the%20quality%20of%20model%27s%20entire%20generated%20answer%2C%20whereas%20SFT%20only%0Ascores%20predicted%20tokens%20based%20on%20preceding%20tokens%20from%20target%20answers.%0ATherefore%2C%20SFT%20overestimates%20the%20ability%20of%20model%2C%20leading%20to%20inferior%0Aoptimization.%20Building%20on%20this%20view%2C%20we%20introduce%20Intuitive%20Fine-Tuning%20%28IFT%29%0Ato%20integrate%20SFT%20and%20Preference%20Optimization%20into%20a%20single%20process.%20IFT%0Acaptures%20LMs%27%20intuitive%20sense%20of%20the%20entire%20answers%20through%20a%20temporal%20residual%0Aconnection%2C%20but%20it%20solely%20relies%20on%20a%20single%20policy%20and%20the%20same%20volume%20of%0Anon-preference-labeled%20data%20as%20SFT.%20Our%20experiments%20show%20that%20IFT%20performs%0Acomparably%20or%20even%20superiorly%20to%20sequential%20recipes%20of%20SFT%20and%20some%20typical%0APreference%20Optimization%20methods%20across%20several%20tasks%2C%20particularly%20those%0Arequires%20generation%2C%20reasoning%2C%20and%20fact-following%20abilities.%20An%20explainable%0AFrozen%20Lake%20game%20further%20validates%20the%20effectiveness%20of%20IFT%20for%20getting%0Acompetitive%20policy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11870v2&entry.124074799=Read"},
{"title": "AnyFit: Controllable Virtual Try-on for Any Combination of Attire Across\n  Any Scenario", "author": "Yuhan Li and Hao Zhou and Wenxiang Shang and Ran Lin and Xuanhong Chen and Bingbing Ni", "abstract": "  While image-based virtual try-on has made significant strides, emerging\napproaches still fall short of delivering high-fidelity and robust fitting\nimages across various scenarios, as their models suffer from issues of\nill-fitted garment styles and quality degrading during the training process,\nnot to mention the lack of support for various combinations of attire.\nTherefore, we first propose a lightweight, scalable, operator known as Hydra\nBlock for attire combinations. This is achieved through a parallel attention\nmechanism that facilitates the feature injection of multiple garments from\nconditionally encoded branches into the main network. Secondly, to\nsignificantly enhance the model's robustness and expressiveness in real-world\nscenarios, we evolve its potential across diverse settings by synthesizing the\nresiduals of multiple models, as well as implementing a mask region boost\nstrategy to overcome the instability caused by information leakage in existing\nmodels. Equipped with the above design, AnyFit surpasses all baselines on\nhigh-resolution benchmarks and real-world data by a large gap, excelling in\nproducing well-fitting garments replete with photorealistic and rich details.\nFurthermore, AnyFit's impressive performance on high-fidelity virtual try-ons\nin any scenario from any image, paves a new path for future research within the\nfashion community.\n", "link": "http://arxiv.org/abs/2405.18172v1", "date": "2024-05-28", "relevancy": 2.3855, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6157}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6061}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyFit%3A%20Controllable%20Virtual%20Try-on%20for%20Any%20Combination%20of%20Attire%20Across%0A%20%20Any%20Scenario&body=Title%3A%20AnyFit%3A%20Controllable%20Virtual%20Try-on%20for%20Any%20Combination%20of%20Attire%20Across%0A%20%20Any%20Scenario%0AAuthor%3A%20Yuhan%20Li%20and%20Hao%20Zhou%20and%20Wenxiang%20Shang%20and%20Ran%20Lin%20and%20Xuanhong%20Chen%20and%20Bingbing%20Ni%0AAbstract%3A%20%20%20While%20image-based%20virtual%20try-on%20has%20made%20significant%20strides%2C%20emerging%0Aapproaches%20still%20fall%20short%20of%20delivering%20high-fidelity%20and%20robust%20fitting%0Aimages%20across%20various%20scenarios%2C%20as%20their%20models%20suffer%20from%20issues%20of%0Aill-fitted%20garment%20styles%20and%20quality%20degrading%20during%20the%20training%20process%2C%0Anot%20to%20mention%20the%20lack%20of%20support%20for%20various%20combinations%20of%20attire.%0ATherefore%2C%20we%20first%20propose%20a%20lightweight%2C%20scalable%2C%20operator%20known%20as%20Hydra%0ABlock%20for%20attire%20combinations.%20This%20is%20achieved%20through%20a%20parallel%20attention%0Amechanism%20that%20facilitates%20the%20feature%20injection%20of%20multiple%20garments%20from%0Aconditionally%20encoded%20branches%20into%20the%20main%20network.%20Secondly%2C%20to%0Asignificantly%20enhance%20the%20model%27s%20robustness%20and%20expressiveness%20in%20real-world%0Ascenarios%2C%20we%20evolve%20its%20potential%20across%20diverse%20settings%20by%20synthesizing%20the%0Aresiduals%20of%20multiple%20models%2C%20as%20well%20as%20implementing%20a%20mask%20region%20boost%0Astrategy%20to%20overcome%20the%20instability%20caused%20by%20information%20leakage%20in%20existing%0Amodels.%20Equipped%20with%20the%20above%20design%2C%20AnyFit%20surpasses%20all%20baselines%20on%0Ahigh-resolution%20benchmarks%20and%20real-world%20data%20by%20a%20large%20gap%2C%20excelling%20in%0Aproducing%20well-fitting%20garments%20replete%20with%20photorealistic%20and%20rich%20details.%0AFurthermore%2C%20AnyFit%27s%20impressive%20performance%20on%20high-fidelity%20virtual%20try-ons%0Ain%20any%20scenario%20from%20any%20image%2C%20paves%20a%20new%20path%20for%20future%20research%20within%20the%0Afashion%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyFit%253A%2520Controllable%2520Virtual%2520Try-on%2520for%2520Any%2520Combination%2520of%2520Attire%2520Across%250A%2520%2520Any%2520Scenario%26entry.906535625%3DYuhan%2520Li%2520and%2520Hao%2520Zhou%2520and%2520Wenxiang%2520Shang%2520and%2520Ran%2520Lin%2520and%2520Xuanhong%2520Chen%2520and%2520Bingbing%2520Ni%26entry.1292438233%3D%2520%2520While%2520image-based%2520virtual%2520try-on%2520has%2520made%2520significant%2520strides%252C%2520emerging%250Aapproaches%2520still%2520fall%2520short%2520of%2520delivering%2520high-fidelity%2520and%2520robust%2520fitting%250Aimages%2520across%2520various%2520scenarios%252C%2520as%2520their%2520models%2520suffer%2520from%2520issues%2520of%250Aill-fitted%2520garment%2520styles%2520and%2520quality%2520degrading%2520during%2520the%2520training%2520process%252C%250Anot%2520to%2520mention%2520the%2520lack%2520of%2520support%2520for%2520various%2520combinations%2520of%2520attire.%250ATherefore%252C%2520we%2520first%2520propose%2520a%2520lightweight%252C%2520scalable%252C%2520operator%2520known%2520as%2520Hydra%250ABlock%2520for%2520attire%2520combinations.%2520This%2520is%2520achieved%2520through%2520a%2520parallel%2520attention%250Amechanism%2520that%2520facilitates%2520the%2520feature%2520injection%2520of%2520multiple%2520garments%2520from%250Aconditionally%2520encoded%2520branches%2520into%2520the%2520main%2520network.%2520Secondly%252C%2520to%250Asignificantly%2520enhance%2520the%2520model%2527s%2520robustness%2520and%2520expressiveness%2520in%2520real-world%250Ascenarios%252C%2520we%2520evolve%2520its%2520potential%2520across%2520diverse%2520settings%2520by%2520synthesizing%2520the%250Aresiduals%2520of%2520multiple%2520models%252C%2520as%2520well%2520as%2520implementing%2520a%2520mask%2520region%2520boost%250Astrategy%2520to%2520overcome%2520the%2520instability%2520caused%2520by%2520information%2520leakage%2520in%2520existing%250Amodels.%2520Equipped%2520with%2520the%2520above%2520design%252C%2520AnyFit%2520surpasses%2520all%2520baselines%2520on%250Ahigh-resolution%2520benchmarks%2520and%2520real-world%2520data%2520by%2520a%2520large%2520gap%252C%2520excelling%2520in%250Aproducing%2520well-fitting%2520garments%2520replete%2520with%2520photorealistic%2520and%2520rich%2520details.%250AFurthermore%252C%2520AnyFit%2527s%2520impressive%2520performance%2520on%2520high-fidelity%2520virtual%2520try-ons%250Ain%2520any%2520scenario%2520from%2520any%2520image%252C%2520paves%2520a%2520new%2520path%2520for%2520future%2520research%2520within%2520the%250Afashion%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyFit%3A%20Controllable%20Virtual%20Try-on%20for%20Any%20Combination%20of%20Attire%20Across%0A%20%20Any%20Scenario&entry.906535625=Yuhan%20Li%20and%20Hao%20Zhou%20and%20Wenxiang%20Shang%20and%20Ran%20Lin%20and%20Xuanhong%20Chen%20and%20Bingbing%20Ni&entry.1292438233=%20%20While%20image-based%20virtual%20try-on%20has%20made%20significant%20strides%2C%20emerging%0Aapproaches%20still%20fall%20short%20of%20delivering%20high-fidelity%20and%20robust%20fitting%0Aimages%20across%20various%20scenarios%2C%20as%20their%20models%20suffer%20from%20issues%20of%0Aill-fitted%20garment%20styles%20and%20quality%20degrading%20during%20the%20training%20process%2C%0Anot%20to%20mention%20the%20lack%20of%20support%20for%20various%20combinations%20of%20attire.%0ATherefore%2C%20we%20first%20propose%20a%20lightweight%2C%20scalable%2C%20operator%20known%20as%20Hydra%0ABlock%20for%20attire%20combinations.%20This%20is%20achieved%20through%20a%20parallel%20attention%0Amechanism%20that%20facilitates%20the%20feature%20injection%20of%20multiple%20garments%20from%0Aconditionally%20encoded%20branches%20into%20the%20main%20network.%20Secondly%2C%20to%0Asignificantly%20enhance%20the%20model%27s%20robustness%20and%20expressiveness%20in%20real-world%0Ascenarios%2C%20we%20evolve%20its%20potential%20across%20diverse%20settings%20by%20synthesizing%20the%0Aresiduals%20of%20multiple%20models%2C%20as%20well%20as%20implementing%20a%20mask%20region%20boost%0Astrategy%20to%20overcome%20the%20instability%20caused%20by%20information%20leakage%20in%20existing%0Amodels.%20Equipped%20with%20the%20above%20design%2C%20AnyFit%20surpasses%20all%20baselines%20on%0Ahigh-resolution%20benchmarks%20and%20real-world%20data%20by%20a%20large%20gap%2C%20excelling%20in%0Aproducing%20well-fitting%20garments%20replete%20with%20photorealistic%20and%20rich%20details.%0AFurthermore%2C%20AnyFit%27s%20impressive%20performance%20on%20high-fidelity%20virtual%20try-ons%0Ain%20any%20scenario%20from%20any%20image%2C%20paves%20a%20new%20path%20for%20future%20research%20within%20the%0Afashion%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18172v1&entry.124074799=Read"},
{"title": "MetaEarth: A Generative Foundation Model for Global-Scale Remote Sensing\n  Image Generation", "author": "Zhiping Yu and Chenyang Liu and Liqin Liu and Zhenwei Shi and Zhengxia Zou", "abstract": "  The recent advancement of generative foundational models has ushered in a new\nera of image generation in the realm of natural images, revolutionizing art\ndesign, entertainment, environment simulation, and beyond. Despite producing\nhigh-quality samples, existing methods are constrained to generating images of\nscenes at a limited scale. In this paper, we present MetaEarth, a generative\nfoundation model that breaks the barrier by scaling image generation to a\nglobal level, exploring the creation of worldwide, multi-resolution, unbounded,\nand virtually limitless remote sensing images. In MetaEarth, we propose a\nresolution-guided self-cascading generative framework, which enables the\ngenerating of images at any region with a wide range of geographical\nresolutions. To achieve unbounded and arbitrary-sized image generation, we\ndesign a novel noise sampling strategy for denoising diffusion models by\nanalyzing the generation conditions and initial noise. To train MetaEarth, we\nconstruct a large dataset comprising multi-resolution optical remote sensing\nimages with geographical information. Experiments have demonstrated the\npowerful capabilities of our method in generating global-scale images.\nAdditionally, the MetaEarth serves as a data engine that can provide\nhigh-quality and rich training data for downstream tasks. Our model opens up\nnew possibilities for constructing generative world models by simulating Earth\nvisuals from an innovative overhead perspective.\n", "link": "http://arxiv.org/abs/2405.13570v2", "date": "2024-05-28", "relevancy": 2.377, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6419}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5826}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaEarth%3A%20A%20Generative%20Foundation%20Model%20for%20Global-Scale%20Remote%20Sensing%0A%20%20Image%20Generation&body=Title%3A%20MetaEarth%3A%20A%20Generative%20Foundation%20Model%20for%20Global-Scale%20Remote%20Sensing%0A%20%20Image%20Generation%0AAuthor%3A%20Zhiping%20Yu%20and%20Chenyang%20Liu%20and%20Liqin%20Liu%20and%20Zhenwei%20Shi%20and%20Zhengxia%20Zou%0AAbstract%3A%20%20%20The%20recent%20advancement%20of%20generative%20foundational%20models%20has%20ushered%20in%20a%20new%0Aera%20of%20image%20generation%20in%20the%20realm%20of%20natural%20images%2C%20revolutionizing%20art%0Adesign%2C%20entertainment%2C%20environment%20simulation%2C%20and%20beyond.%20Despite%20producing%0Ahigh-quality%20samples%2C%20existing%20methods%20are%20constrained%20to%20generating%20images%20of%0Ascenes%20at%20a%20limited%20scale.%20In%20this%20paper%2C%20we%20present%20MetaEarth%2C%20a%20generative%0Afoundation%20model%20that%20breaks%20the%20barrier%20by%20scaling%20image%20generation%20to%20a%0Aglobal%20level%2C%20exploring%20the%20creation%20of%20worldwide%2C%20multi-resolution%2C%20unbounded%2C%0Aand%20virtually%20limitless%20remote%20sensing%20images.%20In%20MetaEarth%2C%20we%20propose%20a%0Aresolution-guided%20self-cascading%20generative%20framework%2C%20which%20enables%20the%0Agenerating%20of%20images%20at%20any%20region%20with%20a%20wide%20range%20of%20geographical%0Aresolutions.%20To%20achieve%20unbounded%20and%20arbitrary-sized%20image%20generation%2C%20we%0Adesign%20a%20novel%20noise%20sampling%20strategy%20for%20denoising%20diffusion%20models%20by%0Aanalyzing%20the%20generation%20conditions%20and%20initial%20noise.%20To%20train%20MetaEarth%2C%20we%0Aconstruct%20a%20large%20dataset%20comprising%20multi-resolution%20optical%20remote%20sensing%0Aimages%20with%20geographical%20information.%20Experiments%20have%20demonstrated%20the%0Apowerful%20capabilities%20of%20our%20method%20in%20generating%20global-scale%20images.%0AAdditionally%2C%20the%20MetaEarth%20serves%20as%20a%20data%20engine%20that%20can%20provide%0Ahigh-quality%20and%20rich%20training%20data%20for%20downstream%20tasks.%20Our%20model%20opens%20up%0Anew%20possibilities%20for%20constructing%20generative%20world%20models%20by%20simulating%20Earth%0Avisuals%20from%20an%20innovative%20overhead%20perspective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaEarth%253A%2520A%2520Generative%2520Foundation%2520Model%2520for%2520Global-Scale%2520Remote%2520Sensing%250A%2520%2520Image%2520Generation%26entry.906535625%3DZhiping%2520Yu%2520and%2520Chenyang%2520Liu%2520and%2520Liqin%2520Liu%2520and%2520Zhenwei%2520Shi%2520and%2520Zhengxia%2520Zou%26entry.1292438233%3D%2520%2520The%2520recent%2520advancement%2520of%2520generative%2520foundational%2520models%2520has%2520ushered%2520in%2520a%2520new%250Aera%2520of%2520image%2520generation%2520in%2520the%2520realm%2520of%2520natural%2520images%252C%2520revolutionizing%2520art%250Adesign%252C%2520entertainment%252C%2520environment%2520simulation%252C%2520and%2520beyond.%2520Despite%2520producing%250Ahigh-quality%2520samples%252C%2520existing%2520methods%2520are%2520constrained%2520to%2520generating%2520images%2520of%250Ascenes%2520at%2520a%2520limited%2520scale.%2520In%2520this%2520paper%252C%2520we%2520present%2520MetaEarth%252C%2520a%2520generative%250Afoundation%2520model%2520that%2520breaks%2520the%2520barrier%2520by%2520scaling%2520image%2520generation%2520to%2520a%250Aglobal%2520level%252C%2520exploring%2520the%2520creation%2520of%2520worldwide%252C%2520multi-resolution%252C%2520unbounded%252C%250Aand%2520virtually%2520limitless%2520remote%2520sensing%2520images.%2520In%2520MetaEarth%252C%2520we%2520propose%2520a%250Aresolution-guided%2520self-cascading%2520generative%2520framework%252C%2520which%2520enables%2520the%250Agenerating%2520of%2520images%2520at%2520any%2520region%2520with%2520a%2520wide%2520range%2520of%2520geographical%250Aresolutions.%2520To%2520achieve%2520unbounded%2520and%2520arbitrary-sized%2520image%2520generation%252C%2520we%250Adesign%2520a%2520novel%2520noise%2520sampling%2520strategy%2520for%2520denoising%2520diffusion%2520models%2520by%250Aanalyzing%2520the%2520generation%2520conditions%2520and%2520initial%2520noise.%2520To%2520train%2520MetaEarth%252C%2520we%250Aconstruct%2520a%2520large%2520dataset%2520comprising%2520multi-resolution%2520optical%2520remote%2520sensing%250Aimages%2520with%2520geographical%2520information.%2520Experiments%2520have%2520demonstrated%2520the%250Apowerful%2520capabilities%2520of%2520our%2520method%2520in%2520generating%2520global-scale%2520images.%250AAdditionally%252C%2520the%2520MetaEarth%2520serves%2520as%2520a%2520data%2520engine%2520that%2520can%2520provide%250Ahigh-quality%2520and%2520rich%2520training%2520data%2520for%2520downstream%2520tasks.%2520Our%2520model%2520opens%2520up%250Anew%2520possibilities%2520for%2520constructing%2520generative%2520world%2520models%2520by%2520simulating%2520Earth%250Avisuals%2520from%2520an%2520innovative%2520overhead%2520perspective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaEarth%3A%20A%20Generative%20Foundation%20Model%20for%20Global-Scale%20Remote%20Sensing%0A%20%20Image%20Generation&entry.906535625=Zhiping%20Yu%20and%20Chenyang%20Liu%20and%20Liqin%20Liu%20and%20Zhenwei%20Shi%20and%20Zhengxia%20Zou&entry.1292438233=%20%20The%20recent%20advancement%20of%20generative%20foundational%20models%20has%20ushered%20in%20a%20new%0Aera%20of%20image%20generation%20in%20the%20realm%20of%20natural%20images%2C%20revolutionizing%20art%0Adesign%2C%20entertainment%2C%20environment%20simulation%2C%20and%20beyond.%20Despite%20producing%0Ahigh-quality%20samples%2C%20existing%20methods%20are%20constrained%20to%20generating%20images%20of%0Ascenes%20at%20a%20limited%20scale.%20In%20this%20paper%2C%20we%20present%20MetaEarth%2C%20a%20generative%0Afoundation%20model%20that%20breaks%20the%20barrier%20by%20scaling%20image%20generation%20to%20a%0Aglobal%20level%2C%20exploring%20the%20creation%20of%20worldwide%2C%20multi-resolution%2C%20unbounded%2C%0Aand%20virtually%20limitless%20remote%20sensing%20images.%20In%20MetaEarth%2C%20we%20propose%20a%0Aresolution-guided%20self-cascading%20generative%20framework%2C%20which%20enables%20the%0Agenerating%20of%20images%20at%20any%20region%20with%20a%20wide%20range%20of%20geographical%0Aresolutions.%20To%20achieve%20unbounded%20and%20arbitrary-sized%20image%20generation%2C%20we%0Adesign%20a%20novel%20noise%20sampling%20strategy%20for%20denoising%20diffusion%20models%20by%0Aanalyzing%20the%20generation%20conditions%20and%20initial%20noise.%20To%20train%20MetaEarth%2C%20we%0Aconstruct%20a%20large%20dataset%20comprising%20multi-resolution%20optical%20remote%20sensing%0Aimages%20with%20geographical%20information.%20Experiments%20have%20demonstrated%20the%0Apowerful%20capabilities%20of%20our%20method%20in%20generating%20global-scale%20images.%0AAdditionally%2C%20the%20MetaEarth%20serves%20as%20a%20data%20engine%20that%20can%20provide%0Ahigh-quality%20and%20rich%20training%20data%20for%20downstream%20tasks.%20Our%20model%20opens%20up%0Anew%20possibilities%20for%20constructing%20generative%20world%20models%20by%20simulating%20Earth%0Avisuals%20from%20an%20innovative%20overhead%20perspective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13570v2&entry.124074799=Read"},
{"title": "Tactile-Driven Non-Prehensile Object Manipulation via Extrinsic Contact\n  Mode Control", "author": "Miquel Oller and Dmitry Berenson and Nima Fazeli", "abstract": "  In this paper, we consider the problem of non-prehensile manipulation using\ngrasped objects. This problem is a superset of many common manipulation skills\nincluding instances of tool-use (e.g., grasped spatula flipping a burger) and\nassembly (e.g., screwdriver tightening a screw). Here, we present an\nalgorithmic approach for non-prehensile manipulation leveraging a gripper with\nhighly compliant and high-resolution tactile sensors. Our approach solves for\nrobot actions that drive object poses and forces to desired values while\nobeying the complex dynamics induced by the sensors as well as the constraints\nimposed by static equilibrium, object kinematics, and frictional contact. Our\nmethod is able to produce a variety of manipulation skills and is amenable to\ngradient-based optimization by exploiting differentiability within contact\nmodes (e.g., specifications of sticking or sliding contacts). We evaluate 4\nvariants of controllers that attempt to realize these plans and demonstrate a\nnumber of complex skills including non-prehensile planar sliding and pivoting\non a variety of object geometries. The perception and controls capabilities\nthat drive these skills are the building blocks towards dexterous and reactive\nautonomy in unstructured environments.\n", "link": "http://arxiv.org/abs/2405.18214v1", "date": "2024-05-28", "relevancy": 2.3667, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5909}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tactile-Driven%20Non-Prehensile%20Object%20Manipulation%20via%20Extrinsic%20Contact%0A%20%20Mode%20Control&body=Title%3A%20Tactile-Driven%20Non-Prehensile%20Object%20Manipulation%20via%20Extrinsic%20Contact%0A%20%20Mode%20Control%0AAuthor%3A%20Miquel%20Oller%20and%20Dmitry%20Berenson%20and%20Nima%20Fazeli%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20the%20problem%20of%20non-prehensile%20manipulation%20using%0Agrasped%20objects.%20This%20problem%20is%20a%20superset%20of%20many%20common%20manipulation%20skills%0Aincluding%20instances%20of%20tool-use%20%28e.g.%2C%20grasped%20spatula%20flipping%20a%20burger%29%20and%0Aassembly%20%28e.g.%2C%20screwdriver%20tightening%20a%20screw%29.%20Here%2C%20we%20present%20an%0Aalgorithmic%20approach%20for%20non-prehensile%20manipulation%20leveraging%20a%20gripper%20with%0Ahighly%20compliant%20and%20high-resolution%20tactile%20sensors.%20Our%20approach%20solves%20for%0Arobot%20actions%20that%20drive%20object%20poses%20and%20forces%20to%20desired%20values%20while%0Aobeying%20the%20complex%20dynamics%20induced%20by%20the%20sensors%20as%20well%20as%20the%20constraints%0Aimposed%20by%20static%20equilibrium%2C%20object%20kinematics%2C%20and%20frictional%20contact.%20Our%0Amethod%20is%20able%20to%20produce%20a%20variety%20of%20manipulation%20skills%20and%20is%20amenable%20to%0Agradient-based%20optimization%20by%20exploiting%20differentiability%20within%20contact%0Amodes%20%28e.g.%2C%20specifications%20of%20sticking%20or%20sliding%20contacts%29.%20We%20evaluate%204%0Avariants%20of%20controllers%20that%20attempt%20to%20realize%20these%20plans%20and%20demonstrate%20a%0Anumber%20of%20complex%20skills%20including%20non-prehensile%20planar%20sliding%20and%20pivoting%0Aon%20a%20variety%20of%20object%20geometries.%20The%20perception%20and%20controls%20capabilities%0Athat%20drive%20these%20skills%20are%20the%20building%20blocks%20towards%20dexterous%20and%20reactive%0Aautonomy%20in%20unstructured%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTactile-Driven%2520Non-Prehensile%2520Object%2520Manipulation%2520via%2520Extrinsic%2520Contact%250A%2520%2520Mode%2520Control%26entry.906535625%3DMiquel%2520Oller%2520and%2520Dmitry%2520Berenson%2520and%2520Nima%2520Fazeli%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520the%2520problem%2520of%2520non-prehensile%2520manipulation%2520using%250Agrasped%2520objects.%2520This%2520problem%2520is%2520a%2520superset%2520of%2520many%2520common%2520manipulation%2520skills%250Aincluding%2520instances%2520of%2520tool-use%2520%2528e.g.%252C%2520grasped%2520spatula%2520flipping%2520a%2520burger%2529%2520and%250Aassembly%2520%2528e.g.%252C%2520screwdriver%2520tightening%2520a%2520screw%2529.%2520Here%252C%2520we%2520present%2520an%250Aalgorithmic%2520approach%2520for%2520non-prehensile%2520manipulation%2520leveraging%2520a%2520gripper%2520with%250Ahighly%2520compliant%2520and%2520high-resolution%2520tactile%2520sensors.%2520Our%2520approach%2520solves%2520for%250Arobot%2520actions%2520that%2520drive%2520object%2520poses%2520and%2520forces%2520to%2520desired%2520values%2520while%250Aobeying%2520the%2520complex%2520dynamics%2520induced%2520by%2520the%2520sensors%2520as%2520well%2520as%2520the%2520constraints%250Aimposed%2520by%2520static%2520equilibrium%252C%2520object%2520kinematics%252C%2520and%2520frictional%2520contact.%2520Our%250Amethod%2520is%2520able%2520to%2520produce%2520a%2520variety%2520of%2520manipulation%2520skills%2520and%2520is%2520amenable%2520to%250Agradient-based%2520optimization%2520by%2520exploiting%2520differentiability%2520within%2520contact%250Amodes%2520%2528e.g.%252C%2520specifications%2520of%2520sticking%2520or%2520sliding%2520contacts%2529.%2520We%2520evaluate%25204%250Avariants%2520of%2520controllers%2520that%2520attempt%2520to%2520realize%2520these%2520plans%2520and%2520demonstrate%2520a%250Anumber%2520of%2520complex%2520skills%2520including%2520non-prehensile%2520planar%2520sliding%2520and%2520pivoting%250Aon%2520a%2520variety%2520of%2520object%2520geometries.%2520The%2520perception%2520and%2520controls%2520capabilities%250Athat%2520drive%2520these%2520skills%2520are%2520the%2520building%2520blocks%2520towards%2520dexterous%2520and%2520reactive%250Aautonomy%2520in%2520unstructured%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tactile-Driven%20Non-Prehensile%20Object%20Manipulation%20via%20Extrinsic%20Contact%0A%20%20Mode%20Control&entry.906535625=Miquel%20Oller%20and%20Dmitry%20Berenson%20and%20Nima%20Fazeli&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20the%20problem%20of%20non-prehensile%20manipulation%20using%0Agrasped%20objects.%20This%20problem%20is%20a%20superset%20of%20many%20common%20manipulation%20skills%0Aincluding%20instances%20of%20tool-use%20%28e.g.%2C%20grasped%20spatula%20flipping%20a%20burger%29%20and%0Aassembly%20%28e.g.%2C%20screwdriver%20tightening%20a%20screw%29.%20Here%2C%20we%20present%20an%0Aalgorithmic%20approach%20for%20non-prehensile%20manipulation%20leveraging%20a%20gripper%20with%0Ahighly%20compliant%20and%20high-resolution%20tactile%20sensors.%20Our%20approach%20solves%20for%0Arobot%20actions%20that%20drive%20object%20poses%20and%20forces%20to%20desired%20values%20while%0Aobeying%20the%20complex%20dynamics%20induced%20by%20the%20sensors%20as%20well%20as%20the%20constraints%0Aimposed%20by%20static%20equilibrium%2C%20object%20kinematics%2C%20and%20frictional%20contact.%20Our%0Amethod%20is%20able%20to%20produce%20a%20variety%20of%20manipulation%20skills%20and%20is%20amenable%20to%0Agradient-based%20optimization%20by%20exploiting%20differentiability%20within%20contact%0Amodes%20%28e.g.%2C%20specifications%20of%20sticking%20or%20sliding%20contacts%29.%20We%20evaluate%204%0Avariants%20of%20controllers%20that%20attempt%20to%20realize%20these%20plans%20and%20demonstrate%20a%0Anumber%20of%20complex%20skills%20including%20non-prehensile%20planar%20sliding%20and%20pivoting%0Aon%20a%20variety%20of%20object%20geometries.%20The%20perception%20and%20controls%20capabilities%0Athat%20drive%20these%20skills%20are%20the%20building%20blocks%20towards%20dexterous%20and%20reactive%0Aautonomy%20in%20unstructured%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18214v1&entry.124074799=Read"},
{"title": "RACCooN: Remove, Add, and Change Video Content with Auto-Generated\n  Narratives", "author": "Jaehong Yoon and Shoubin Yu and Mohit Bansal", "abstract": "  Recent video generative models primarily rely on carefully written text\nprompts for specific tasks, like inpainting or style editing. They require\nlabor-intensive textual descriptions for input videos, hindering their\nflexibility to adapt personal/raw videos to user specifications. This paper\nproposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video\ngenerative framework that supports multiple video editing capabilities such as\nremoval, addition, and modification, through a unified pipeline. RACCooN\nconsists of two principal stages: Video-to-Paragraph (V2P) and\nParagraph-to-Video (P2V). In the V2P stage, we automatically describe video\nscenes in well-structured natural language, capturing both the holistic context\nand focused object details. Subsequently, in the P2V stage, users can\noptionally refine these descriptions to guide the video diffusion model,\nenabling various modifications to the input video, such as removing, changing\nsubjects, and/or adding new objects. The proposed approach stands out from\nother methods through several significant contributions: (1) RACCooN suggests a\nmulti-granular spatiotemporal pooling strategy to generate well-structured\nvideo descriptions, capturing both the broad context and object details without\nrequiring complex human annotations, simplifying precise video content editing\nbased on text for users. (2) Our video generative model incorporates\nauto-generated narratives or instructions to enhance the quality and accuracy\nof the generated content. It supports the addition of video objects,\ninpainting, and attribute modification within a unified framework, surpassing\nexisting video editing and inpainting benchmarks. The proposed framework\ndemonstrates impressive versatile capabilities in video-to-paragraph\ngeneration, video content editing, and can be incorporated into other SoTA\nvideo generative models for further enhancement.\n", "link": "http://arxiv.org/abs/2405.18406v1", "date": "2024-05-28", "relevancy": 2.3626, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.637}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6075}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RACCooN%3A%20Remove%2C%20Add%2C%20and%20Change%20Video%20Content%20with%20Auto-Generated%0A%20%20Narratives&body=Title%3A%20RACCooN%3A%20Remove%2C%20Add%2C%20and%20Change%20Video%20Content%20with%20Auto-Generated%0A%20%20Narratives%0AAuthor%3A%20Jaehong%20Yoon%20and%20Shoubin%20Yu%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Recent%20video%20generative%20models%20primarily%20rely%20on%20carefully%20written%20text%0Aprompts%20for%20specific%20tasks%2C%20like%20inpainting%20or%20style%20editing.%20They%20require%0Alabor-intensive%20textual%20descriptions%20for%20input%20videos%2C%20hindering%20their%0Aflexibility%20to%20adapt%20personal/raw%20videos%20to%20user%20specifications.%20This%20paper%0Aproposes%20RACCooN%2C%20a%20versatile%20and%20user-friendly%20video-to-paragraph-to-video%0Agenerative%20framework%20that%20supports%20multiple%20video%20editing%20capabilities%20such%20as%0Aremoval%2C%20addition%2C%20and%20modification%2C%20through%20a%20unified%20pipeline.%20RACCooN%0Aconsists%20of%20two%20principal%20stages%3A%20Video-to-Paragraph%20%28V2P%29%20and%0AParagraph-to-Video%20%28P2V%29.%20In%20the%20V2P%20stage%2C%20we%20automatically%20describe%20video%0Ascenes%20in%20well-structured%20natural%20language%2C%20capturing%20both%20the%20holistic%20context%0Aand%20focused%20object%20details.%20Subsequently%2C%20in%20the%20P2V%20stage%2C%20users%20can%0Aoptionally%20refine%20these%20descriptions%20to%20guide%20the%20video%20diffusion%20model%2C%0Aenabling%20various%20modifications%20to%20the%20input%20video%2C%20such%20as%20removing%2C%20changing%0Asubjects%2C%20and/or%20adding%20new%20objects.%20The%20proposed%20approach%20stands%20out%20from%0Aother%20methods%20through%20several%20significant%20contributions%3A%20%281%29%20RACCooN%20suggests%20a%0Amulti-granular%20spatiotemporal%20pooling%20strategy%20to%20generate%20well-structured%0Avideo%20descriptions%2C%20capturing%20both%20the%20broad%20context%20and%20object%20details%20without%0Arequiring%20complex%20human%20annotations%2C%20simplifying%20precise%20video%20content%20editing%0Abased%20on%20text%20for%20users.%20%282%29%20Our%20video%20generative%20model%20incorporates%0Aauto-generated%20narratives%20or%20instructions%20to%20enhance%20the%20quality%20and%20accuracy%0Aof%20the%20generated%20content.%20It%20supports%20the%20addition%20of%20video%20objects%2C%0Ainpainting%2C%20and%20attribute%20modification%20within%20a%20unified%20framework%2C%20surpassing%0Aexisting%20video%20editing%20and%20inpainting%20benchmarks.%20The%20proposed%20framework%0Ademonstrates%20impressive%20versatile%20capabilities%20in%20video-to-paragraph%0Ageneration%2C%20video%20content%20editing%2C%20and%20can%20be%20incorporated%20into%20other%20SoTA%0Avideo%20generative%20models%20for%20further%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRACCooN%253A%2520Remove%252C%2520Add%252C%2520and%2520Change%2520Video%2520Content%2520with%2520Auto-Generated%250A%2520%2520Narratives%26entry.906535625%3DJaehong%2520Yoon%2520and%2520Shoubin%2520Yu%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Recent%2520video%2520generative%2520models%2520primarily%2520rely%2520on%2520carefully%2520written%2520text%250Aprompts%2520for%2520specific%2520tasks%252C%2520like%2520inpainting%2520or%2520style%2520editing.%2520They%2520require%250Alabor-intensive%2520textual%2520descriptions%2520for%2520input%2520videos%252C%2520hindering%2520their%250Aflexibility%2520to%2520adapt%2520personal/raw%2520videos%2520to%2520user%2520specifications.%2520This%2520paper%250Aproposes%2520RACCooN%252C%2520a%2520versatile%2520and%2520user-friendly%2520video-to-paragraph-to-video%250Agenerative%2520framework%2520that%2520supports%2520multiple%2520video%2520editing%2520capabilities%2520such%2520as%250Aremoval%252C%2520addition%252C%2520and%2520modification%252C%2520through%2520a%2520unified%2520pipeline.%2520RACCooN%250Aconsists%2520of%2520two%2520principal%2520stages%253A%2520Video-to-Paragraph%2520%2528V2P%2529%2520and%250AParagraph-to-Video%2520%2528P2V%2529.%2520In%2520the%2520V2P%2520stage%252C%2520we%2520automatically%2520describe%2520video%250Ascenes%2520in%2520well-structured%2520natural%2520language%252C%2520capturing%2520both%2520the%2520holistic%2520context%250Aand%2520focused%2520object%2520details.%2520Subsequently%252C%2520in%2520the%2520P2V%2520stage%252C%2520users%2520can%250Aoptionally%2520refine%2520these%2520descriptions%2520to%2520guide%2520the%2520video%2520diffusion%2520model%252C%250Aenabling%2520various%2520modifications%2520to%2520the%2520input%2520video%252C%2520such%2520as%2520removing%252C%2520changing%250Asubjects%252C%2520and/or%2520adding%2520new%2520objects.%2520The%2520proposed%2520approach%2520stands%2520out%2520from%250Aother%2520methods%2520through%2520several%2520significant%2520contributions%253A%2520%25281%2529%2520RACCooN%2520suggests%2520a%250Amulti-granular%2520spatiotemporal%2520pooling%2520strategy%2520to%2520generate%2520well-structured%250Avideo%2520descriptions%252C%2520capturing%2520both%2520the%2520broad%2520context%2520and%2520object%2520details%2520without%250Arequiring%2520complex%2520human%2520annotations%252C%2520simplifying%2520precise%2520video%2520content%2520editing%250Abased%2520on%2520text%2520for%2520users.%2520%25282%2529%2520Our%2520video%2520generative%2520model%2520incorporates%250Aauto-generated%2520narratives%2520or%2520instructions%2520to%2520enhance%2520the%2520quality%2520and%2520accuracy%250Aof%2520the%2520generated%2520content.%2520It%2520supports%2520the%2520addition%2520of%2520video%2520objects%252C%250Ainpainting%252C%2520and%2520attribute%2520modification%2520within%2520a%2520unified%2520framework%252C%2520surpassing%250Aexisting%2520video%2520editing%2520and%2520inpainting%2520benchmarks.%2520The%2520proposed%2520framework%250Ademonstrates%2520impressive%2520versatile%2520capabilities%2520in%2520video-to-paragraph%250Ageneration%252C%2520video%2520content%2520editing%252C%2520and%2520can%2520be%2520incorporated%2520into%2520other%2520SoTA%250Avideo%2520generative%2520models%2520for%2520further%2520enhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RACCooN%3A%20Remove%2C%20Add%2C%20and%20Change%20Video%20Content%20with%20Auto-Generated%0A%20%20Narratives&entry.906535625=Jaehong%20Yoon%20and%20Shoubin%20Yu%20and%20Mohit%20Bansal&entry.1292438233=%20%20Recent%20video%20generative%20models%20primarily%20rely%20on%20carefully%20written%20text%0Aprompts%20for%20specific%20tasks%2C%20like%20inpainting%20or%20style%20editing.%20They%20require%0Alabor-intensive%20textual%20descriptions%20for%20input%20videos%2C%20hindering%20their%0Aflexibility%20to%20adapt%20personal/raw%20videos%20to%20user%20specifications.%20This%20paper%0Aproposes%20RACCooN%2C%20a%20versatile%20and%20user-friendly%20video-to-paragraph-to-video%0Agenerative%20framework%20that%20supports%20multiple%20video%20editing%20capabilities%20such%20as%0Aremoval%2C%20addition%2C%20and%20modification%2C%20through%20a%20unified%20pipeline.%20RACCooN%0Aconsists%20of%20two%20principal%20stages%3A%20Video-to-Paragraph%20%28V2P%29%20and%0AParagraph-to-Video%20%28P2V%29.%20In%20the%20V2P%20stage%2C%20we%20automatically%20describe%20video%0Ascenes%20in%20well-structured%20natural%20language%2C%20capturing%20both%20the%20holistic%20context%0Aand%20focused%20object%20details.%20Subsequently%2C%20in%20the%20P2V%20stage%2C%20users%20can%0Aoptionally%20refine%20these%20descriptions%20to%20guide%20the%20video%20diffusion%20model%2C%0Aenabling%20various%20modifications%20to%20the%20input%20video%2C%20such%20as%20removing%2C%20changing%0Asubjects%2C%20and/or%20adding%20new%20objects.%20The%20proposed%20approach%20stands%20out%20from%0Aother%20methods%20through%20several%20significant%20contributions%3A%20%281%29%20RACCooN%20suggests%20a%0Amulti-granular%20spatiotemporal%20pooling%20strategy%20to%20generate%20well-structured%0Avideo%20descriptions%2C%20capturing%20both%20the%20broad%20context%20and%20object%20details%20without%0Arequiring%20complex%20human%20annotations%2C%20simplifying%20precise%20video%20content%20editing%0Abased%20on%20text%20for%20users.%20%282%29%20Our%20video%20generative%20model%20incorporates%0Aauto-generated%20narratives%20or%20instructions%20to%20enhance%20the%20quality%20and%20accuracy%0Aof%20the%20generated%20content.%20It%20supports%20the%20addition%20of%20video%20objects%2C%0Ainpainting%2C%20and%20attribute%20modification%20within%20a%20unified%20framework%2C%20surpassing%0Aexisting%20video%20editing%20and%20inpainting%20benchmarks.%20The%20proposed%20framework%0Ademonstrates%20impressive%20versatile%20capabilities%20in%20video-to-paragraph%0Ageneration%2C%20video%20content%20editing%2C%20and%20can%20be%20incorporated%20into%20other%20SoTA%0Avideo%20generative%20models%20for%20further%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18406v1&entry.124074799=Read"},
{"title": "Guidance and Control Networks with Periodic Activation Functions", "author": "Sebastien Origer and Dario Izzo", "abstract": "  Inspired by the versatility of sinusoidal representation networks (SIRENs),\nwe present a modified Guidance & Control Networks (G&CNETs) variant using\nperiodic activation functions in the hidden layers. We demonstrate that the\nresulting G&CNETs train faster and achieve a lower overall training error on\nthree different control scenarios on which G&CNETs have been tested previously.\nA preliminary analysis is presented in an attempt to explain the superior\nperformance of the SIREN architecture for the particular types of tasks that\nG&CNETs excel on.\n", "link": "http://arxiv.org/abs/2405.18084v1", "date": "2024-05-28", "relevancy": 2.3539, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4843}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4688}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guidance%20and%20Control%20Networks%20with%20Periodic%20Activation%20Functions&body=Title%3A%20Guidance%20and%20Control%20Networks%20with%20Periodic%20Activation%20Functions%0AAuthor%3A%20Sebastien%20Origer%20and%20Dario%20Izzo%0AAbstract%3A%20%20%20Inspired%20by%20the%20versatility%20of%20sinusoidal%20representation%20networks%20%28SIRENs%29%2C%0Awe%20present%20a%20modified%20Guidance%20%26%20Control%20Networks%20%28G%26CNETs%29%20variant%20using%0Aperiodic%20activation%20functions%20in%20the%20hidden%20layers.%20We%20demonstrate%20that%20the%0Aresulting%20G%26CNETs%20train%20faster%20and%20achieve%20a%20lower%20overall%20training%20error%20on%0Athree%20different%20control%20scenarios%20on%20which%20G%26CNETs%20have%20been%20tested%20previously.%0AA%20preliminary%20analysis%20is%20presented%20in%20an%20attempt%20to%20explain%20the%20superior%0Aperformance%20of%20the%20SIREN%20architecture%20for%20the%20particular%20types%20of%20tasks%20that%0AG%26CNETs%20excel%20on.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuidance%2520and%2520Control%2520Networks%2520with%2520Periodic%2520Activation%2520Functions%26entry.906535625%3DSebastien%2520Origer%2520and%2520Dario%2520Izzo%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520versatility%2520of%2520sinusoidal%2520representation%2520networks%2520%2528SIRENs%2529%252C%250Awe%2520present%2520a%2520modified%2520Guidance%2520%2526%2520Control%2520Networks%2520%2528G%2526CNETs%2529%2520variant%2520using%250Aperiodic%2520activation%2520functions%2520in%2520the%2520hidden%2520layers.%2520We%2520demonstrate%2520that%2520the%250Aresulting%2520G%2526CNETs%2520train%2520faster%2520and%2520achieve%2520a%2520lower%2520overall%2520training%2520error%2520on%250Athree%2520different%2520control%2520scenarios%2520on%2520which%2520G%2526CNETs%2520have%2520been%2520tested%2520previously.%250AA%2520preliminary%2520analysis%2520is%2520presented%2520in%2520an%2520attempt%2520to%2520explain%2520the%2520superior%250Aperformance%2520of%2520the%2520SIREN%2520architecture%2520for%2520the%2520particular%2520types%2520of%2520tasks%2520that%250AG%2526CNETs%2520excel%2520on.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guidance%20and%20Control%20Networks%20with%20Periodic%20Activation%20Functions&entry.906535625=Sebastien%20Origer%20and%20Dario%20Izzo&entry.1292438233=%20%20Inspired%20by%20the%20versatility%20of%20sinusoidal%20representation%20networks%20%28SIRENs%29%2C%0Awe%20present%20a%20modified%20Guidance%20%26%20Control%20Networks%20%28G%26CNETs%29%20variant%20using%0Aperiodic%20activation%20functions%20in%20the%20hidden%20layers.%20We%20demonstrate%20that%20the%0Aresulting%20G%26CNETs%20train%20faster%20and%20achieve%20a%20lower%20overall%20training%20error%20on%0Athree%20different%20control%20scenarios%20on%20which%20G%26CNETs%20have%20been%20tested%20previously.%0AA%20preliminary%20analysis%20is%20presented%20in%20an%20attempt%20to%20explain%20the%20superior%0Aperformance%20of%20the%20SIREN%20architecture%20for%20the%20particular%20types%20of%20tasks%20that%0AG%26CNETs%20excel%20on.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18084v1&entry.124074799=Read"},
{"title": "The Battle of LLMs: A Comparative Study in Conversational QA Tasks", "author": "Aryan Rangapur and Aman Rangapur", "abstract": "  Large language models have gained considerable interest for their impressive\nperformance on various tasks. Within this domain, ChatGPT and GPT-4, developed\nby OpenAI, and the Gemini, developed by Google, have emerged as particularly\npopular among early adopters. Additionally, Mixtral by Mistral AI and Claude by\nAnthropic are newly released, further expanding the landscape of advanced\nlanguage models. These models are viewed as disruptive technologies with\napplications spanning customer service, education, healthcare, and finance.\nMore recently, Mistral has entered the scene, captivating users with its unique\nability to generate creative content. Understanding the perspectives of these\nusers is crucial, as they can offer valuable insights into the potential\nstrengths, weaknesses, and overall success or failure of these technologies in\nvarious domains. This research delves into the responses generated by ChatGPT,\nGPT-4, Gemini, Mixtral and Claude across different Conversational QA corpora.\nEvaluation scores were meticulously computed and subsequently compared to\nascertain the overall performance of these models. Our study pinpointed\ninstances where these models provided inaccurate answers to questions, offering\ninsights into potential areas where they might be susceptible to errors. In\nessence, this research provides a comprehensive comparison and evaluation of\nthese state of-the-art language models, shedding light on their capabilities\nwhile also highlighting potential areas for improvement\n", "link": "http://arxiv.org/abs/2405.18344v1", "date": "2024-05-28", "relevancy": 2.348, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4664}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Battle%20of%20LLMs%3A%20A%20Comparative%20Study%20in%20Conversational%20QA%20Tasks&body=Title%3A%20The%20Battle%20of%20LLMs%3A%20A%20Comparative%20Study%20in%20Conversational%20QA%20Tasks%0AAuthor%3A%20Aryan%20Rangapur%20and%20Aman%20Rangapur%0AAbstract%3A%20%20%20Large%20language%20models%20have%20gained%20considerable%20interest%20for%20their%20impressive%0Aperformance%20on%20various%20tasks.%20Within%20this%20domain%2C%20ChatGPT%20and%20GPT-4%2C%20developed%0Aby%20OpenAI%2C%20and%20the%20Gemini%2C%20developed%20by%20Google%2C%20have%20emerged%20as%20particularly%0Apopular%20among%20early%20adopters.%20Additionally%2C%20Mixtral%20by%20Mistral%20AI%20and%20Claude%20by%0AAnthropic%20are%20newly%20released%2C%20further%20expanding%20the%20landscape%20of%20advanced%0Alanguage%20models.%20These%20models%20are%20viewed%20as%20disruptive%20technologies%20with%0Aapplications%20spanning%20customer%20service%2C%20education%2C%20healthcare%2C%20and%20finance.%0AMore%20recently%2C%20Mistral%20has%20entered%20the%20scene%2C%20captivating%20users%20with%20its%20unique%0Aability%20to%20generate%20creative%20content.%20Understanding%20the%20perspectives%20of%20these%0Ausers%20is%20crucial%2C%20as%20they%20can%20offer%20valuable%20insights%20into%20the%20potential%0Astrengths%2C%20weaknesses%2C%20and%20overall%20success%20or%20failure%20of%20these%20technologies%20in%0Avarious%20domains.%20This%20research%20delves%20into%20the%20responses%20generated%20by%20ChatGPT%2C%0AGPT-4%2C%20Gemini%2C%20Mixtral%20and%20Claude%20across%20different%20Conversational%20QA%20corpora.%0AEvaluation%20scores%20were%20meticulously%20computed%20and%20subsequently%20compared%20to%0Aascertain%20the%20overall%20performance%20of%20these%20models.%20Our%20study%20pinpointed%0Ainstances%20where%20these%20models%20provided%20inaccurate%20answers%20to%20questions%2C%20offering%0Ainsights%20into%20potential%20areas%20where%20they%20might%20be%20susceptible%20to%20errors.%20In%0Aessence%2C%20this%20research%20provides%20a%20comprehensive%20comparison%20and%20evaluation%20of%0Athese%20state%20of-the-art%20language%20models%2C%20shedding%20light%20on%20their%20capabilities%0Awhile%20also%20highlighting%20potential%20areas%20for%20improvement%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Battle%2520of%2520LLMs%253A%2520A%2520Comparative%2520Study%2520in%2520Conversational%2520QA%2520Tasks%26entry.906535625%3DAryan%2520Rangapur%2520and%2520Aman%2520Rangapur%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520gained%2520considerable%2520interest%2520for%2520their%2520impressive%250Aperformance%2520on%2520various%2520tasks.%2520Within%2520this%2520domain%252C%2520ChatGPT%2520and%2520GPT-4%252C%2520developed%250Aby%2520OpenAI%252C%2520and%2520the%2520Gemini%252C%2520developed%2520by%2520Google%252C%2520have%2520emerged%2520as%2520particularly%250Apopular%2520among%2520early%2520adopters.%2520Additionally%252C%2520Mixtral%2520by%2520Mistral%2520AI%2520and%2520Claude%2520by%250AAnthropic%2520are%2520newly%2520released%252C%2520further%2520expanding%2520the%2520landscape%2520of%2520advanced%250Alanguage%2520models.%2520These%2520models%2520are%2520viewed%2520as%2520disruptive%2520technologies%2520with%250Aapplications%2520spanning%2520customer%2520service%252C%2520education%252C%2520healthcare%252C%2520and%2520finance.%250AMore%2520recently%252C%2520Mistral%2520has%2520entered%2520the%2520scene%252C%2520captivating%2520users%2520with%2520its%2520unique%250Aability%2520to%2520generate%2520creative%2520content.%2520Understanding%2520the%2520perspectives%2520of%2520these%250Ausers%2520is%2520crucial%252C%2520as%2520they%2520can%2520offer%2520valuable%2520insights%2520into%2520the%2520potential%250Astrengths%252C%2520weaknesses%252C%2520and%2520overall%2520success%2520or%2520failure%2520of%2520these%2520technologies%2520in%250Avarious%2520domains.%2520This%2520research%2520delves%2520into%2520the%2520responses%2520generated%2520by%2520ChatGPT%252C%250AGPT-4%252C%2520Gemini%252C%2520Mixtral%2520and%2520Claude%2520across%2520different%2520Conversational%2520QA%2520corpora.%250AEvaluation%2520scores%2520were%2520meticulously%2520computed%2520and%2520subsequently%2520compared%2520to%250Aascertain%2520the%2520overall%2520performance%2520of%2520these%2520models.%2520Our%2520study%2520pinpointed%250Ainstances%2520where%2520these%2520models%2520provided%2520inaccurate%2520answers%2520to%2520questions%252C%2520offering%250Ainsights%2520into%2520potential%2520areas%2520where%2520they%2520might%2520be%2520susceptible%2520to%2520errors.%2520In%250Aessence%252C%2520this%2520research%2520provides%2520a%2520comprehensive%2520comparison%2520and%2520evaluation%2520of%250Athese%2520state%2520of-the-art%2520language%2520models%252C%2520shedding%2520light%2520on%2520their%2520capabilities%250Awhile%2520also%2520highlighting%2520potential%2520areas%2520for%2520improvement%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Battle%20of%20LLMs%3A%20A%20Comparative%20Study%20in%20Conversational%20QA%20Tasks&entry.906535625=Aryan%20Rangapur%20and%20Aman%20Rangapur&entry.1292438233=%20%20Large%20language%20models%20have%20gained%20considerable%20interest%20for%20their%20impressive%0Aperformance%20on%20various%20tasks.%20Within%20this%20domain%2C%20ChatGPT%20and%20GPT-4%2C%20developed%0Aby%20OpenAI%2C%20and%20the%20Gemini%2C%20developed%20by%20Google%2C%20have%20emerged%20as%20particularly%0Apopular%20among%20early%20adopters.%20Additionally%2C%20Mixtral%20by%20Mistral%20AI%20and%20Claude%20by%0AAnthropic%20are%20newly%20released%2C%20further%20expanding%20the%20landscape%20of%20advanced%0Alanguage%20models.%20These%20models%20are%20viewed%20as%20disruptive%20technologies%20with%0Aapplications%20spanning%20customer%20service%2C%20education%2C%20healthcare%2C%20and%20finance.%0AMore%20recently%2C%20Mistral%20has%20entered%20the%20scene%2C%20captivating%20users%20with%20its%20unique%0Aability%20to%20generate%20creative%20content.%20Understanding%20the%20perspectives%20of%20these%0Ausers%20is%20crucial%2C%20as%20they%20can%20offer%20valuable%20insights%20into%20the%20potential%0Astrengths%2C%20weaknesses%2C%20and%20overall%20success%20or%20failure%20of%20these%20technologies%20in%0Avarious%20domains.%20This%20research%20delves%20into%20the%20responses%20generated%20by%20ChatGPT%2C%0AGPT-4%2C%20Gemini%2C%20Mixtral%20and%20Claude%20across%20different%20Conversational%20QA%20corpora.%0AEvaluation%20scores%20were%20meticulously%20computed%20and%20subsequently%20compared%20to%0Aascertain%20the%20overall%20performance%20of%20these%20models.%20Our%20study%20pinpointed%0Ainstances%20where%20these%20models%20provided%20inaccurate%20answers%20to%20questions%2C%20offering%0Ainsights%20into%20potential%20areas%20where%20they%20might%20be%20susceptible%20to%20errors.%20In%0Aessence%2C%20this%20research%20provides%20a%20comprehensive%20comparison%20and%20evaluation%20of%0Athese%20state%20of-the-art%20language%20models%2C%20shedding%20light%20on%20their%20capabilities%0Awhile%20also%20highlighting%20potential%20areas%20for%20improvement%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18344v1&entry.124074799=Read"},
{"title": "Universal and Extensible Language-Vision Models for Organ Segmentation\n  and Tumor Detection from Abdominal Computed Tomography", "author": "Jie Liu and Yixiao Zhang and Kang Wang and Mehmet Can Yavuz and Xiaoxi Chen and Yixuan Yuan and Haoliang Li and Yang Yang and Alan Yuille and Yucheng Tang and Zongwei Zhou", "abstract": "  The advancement of artificial intelligence (AI) for organ segmentation and\ntumor detection is propelled by the growing availability of computed tomography\n(CT) datasets with detailed, per-voxel annotations. However, these AI models\noften struggle with flexibility for partially annotated datasets and\nextensibility for new classes due to limitations in the one-hot encoding,\narchitectural design, and learning scheme. To overcome these limitations, we\npropose a universal, extensible framework enabling a single model, termed\nUniversal Model, to deal with multiple public datasets and adapt to new classes\n(e.g., organs/tumors). Firstly, we introduce a novel language-driven parameter\ngenerator that leverages language embeddings from large language models,\nenriching semantic encoding compared with one-hot encoding. Secondly, the\nconventional output layers are replaced with lightweight, class-specific heads,\nallowing Universal Model to simultaneously segment 25 organs and six types of\ntumors and ease the addition of new classes. We train our Universal Model on\n3,410 CT volumes assembled from 14 publicly available datasets and then test it\non 6,173 CT volumes from four external datasets. Universal Model achieves first\nplace on six CT tasks in the Medical Segmentation Decathlon (MSD) public\nleaderboard and leading performance on the Beyond The Cranial Vault (BTCV)\ndataset. In summary, Universal Model exhibits remarkable computational\nefficiency (6x faster than other dataset-specific models), demonstrates strong\ngeneralization across different hospitals, transfers well to numerous\ndownstream tasks, and more importantly, facilitates the extensibility to new\nclasses while alleviating the catastrophic forgetting of previously learned\nclasses. Codes, models, and datasets are available at\nhttps://github.com/ljwztc/CLIP-Driven-Universal-Model\n", "link": "http://arxiv.org/abs/2405.18356v1", "date": "2024-05-28", "relevancy": 2.3405, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.616}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.594}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20and%20Extensible%20Language-Vision%20Models%20for%20Organ%20Segmentation%0A%20%20and%20Tumor%20Detection%20from%20Abdominal%20Computed%20Tomography&body=Title%3A%20Universal%20and%20Extensible%20Language-Vision%20Models%20for%20Organ%20Segmentation%0A%20%20and%20Tumor%20Detection%20from%20Abdominal%20Computed%20Tomography%0AAuthor%3A%20Jie%20Liu%20and%20Yixiao%20Zhang%20and%20Kang%20Wang%20and%20Mehmet%20Can%20Yavuz%20and%20Xiaoxi%20Chen%20and%20Yixuan%20Yuan%20and%20Haoliang%20Li%20and%20Yang%20Yang%20and%20Alan%20Yuille%20and%20Yucheng%20Tang%20and%20Zongwei%20Zhou%0AAbstract%3A%20%20%20The%20advancement%20of%20artificial%20intelligence%20%28AI%29%20for%20organ%20segmentation%20and%0Atumor%20detection%20is%20propelled%20by%20the%20growing%20availability%20of%20computed%20tomography%0A%28CT%29%20datasets%20with%20detailed%2C%20per-voxel%20annotations.%20However%2C%20these%20AI%20models%0Aoften%20struggle%20with%20flexibility%20for%20partially%20annotated%20datasets%20and%0Aextensibility%20for%20new%20classes%20due%20to%20limitations%20in%20the%20one-hot%20encoding%2C%0Aarchitectural%20design%2C%20and%20learning%20scheme.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20universal%2C%20extensible%20framework%20enabling%20a%20single%20model%2C%20termed%0AUniversal%20Model%2C%20to%20deal%20with%20multiple%20public%20datasets%20and%20adapt%20to%20new%20classes%0A%28e.g.%2C%20organs/tumors%29.%20Firstly%2C%20we%20introduce%20a%20novel%20language-driven%20parameter%0Agenerator%20that%20leverages%20language%20embeddings%20from%20large%20language%20models%2C%0Aenriching%20semantic%20encoding%20compared%20with%20one-hot%20encoding.%20Secondly%2C%20the%0Aconventional%20output%20layers%20are%20replaced%20with%20lightweight%2C%20class-specific%20heads%2C%0Aallowing%20Universal%20Model%20to%20simultaneously%20segment%2025%20organs%20and%20six%20types%20of%0Atumors%20and%20ease%20the%20addition%20of%20new%20classes.%20We%20train%20our%20Universal%20Model%20on%0A3%2C410%20CT%20volumes%20assembled%20from%2014%20publicly%20available%20datasets%20and%20then%20test%20it%0Aon%206%2C173%20CT%20volumes%20from%20four%20external%20datasets.%20Universal%20Model%20achieves%20first%0Aplace%20on%20six%20CT%20tasks%20in%20the%20Medical%20Segmentation%20Decathlon%20%28MSD%29%20public%0Aleaderboard%20and%20leading%20performance%20on%20the%20Beyond%20The%20Cranial%20Vault%20%28BTCV%29%0Adataset.%20In%20summary%2C%20Universal%20Model%20exhibits%20remarkable%20computational%0Aefficiency%20%286x%20faster%20than%20other%20dataset-specific%20models%29%2C%20demonstrates%20strong%0Ageneralization%20across%20different%20hospitals%2C%20transfers%20well%20to%20numerous%0Adownstream%20tasks%2C%20and%20more%20importantly%2C%20facilitates%20the%20extensibility%20to%20new%0Aclasses%20while%20alleviating%20the%20catastrophic%20forgetting%20of%20previously%20learned%0Aclasses.%20Codes%2C%20models%2C%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/ljwztc/CLIP-Driven-Universal-Model%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520and%2520Extensible%2520Language-Vision%2520Models%2520for%2520Organ%2520Segmentation%250A%2520%2520and%2520Tumor%2520Detection%2520from%2520Abdominal%2520Computed%2520Tomography%26entry.906535625%3DJie%2520Liu%2520and%2520Yixiao%2520Zhang%2520and%2520Kang%2520Wang%2520and%2520Mehmet%2520Can%2520Yavuz%2520and%2520Xiaoxi%2520Chen%2520and%2520Yixuan%2520Yuan%2520and%2520Haoliang%2520Li%2520and%2520Yang%2520Yang%2520and%2520Alan%2520Yuille%2520and%2520Yucheng%2520Tang%2520and%2520Zongwei%2520Zhou%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520for%2520organ%2520segmentation%2520and%250Atumor%2520detection%2520is%2520propelled%2520by%2520the%2520growing%2520availability%2520of%2520computed%2520tomography%250A%2528CT%2529%2520datasets%2520with%2520detailed%252C%2520per-voxel%2520annotations.%2520However%252C%2520these%2520AI%2520models%250Aoften%2520struggle%2520with%2520flexibility%2520for%2520partially%2520annotated%2520datasets%2520and%250Aextensibility%2520for%2520new%2520classes%2520due%2520to%2520limitations%2520in%2520the%2520one-hot%2520encoding%252C%250Aarchitectural%2520design%252C%2520and%2520learning%2520scheme.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520universal%252C%2520extensible%2520framework%2520enabling%2520a%2520single%2520model%252C%2520termed%250AUniversal%2520Model%252C%2520to%2520deal%2520with%2520multiple%2520public%2520datasets%2520and%2520adapt%2520to%2520new%2520classes%250A%2528e.g.%252C%2520organs/tumors%2529.%2520Firstly%252C%2520we%2520introduce%2520a%2520novel%2520language-driven%2520parameter%250Agenerator%2520that%2520leverages%2520language%2520embeddings%2520from%2520large%2520language%2520models%252C%250Aenriching%2520semantic%2520encoding%2520compared%2520with%2520one-hot%2520encoding.%2520Secondly%252C%2520the%250Aconventional%2520output%2520layers%2520are%2520replaced%2520with%2520lightweight%252C%2520class-specific%2520heads%252C%250Aallowing%2520Universal%2520Model%2520to%2520simultaneously%2520segment%252025%2520organs%2520and%2520six%2520types%2520of%250Atumors%2520and%2520ease%2520the%2520addition%2520of%2520new%2520classes.%2520We%2520train%2520our%2520Universal%2520Model%2520on%250A3%252C410%2520CT%2520volumes%2520assembled%2520from%252014%2520publicly%2520available%2520datasets%2520and%2520then%2520test%2520it%250Aon%25206%252C173%2520CT%2520volumes%2520from%2520four%2520external%2520datasets.%2520Universal%2520Model%2520achieves%2520first%250Aplace%2520on%2520six%2520CT%2520tasks%2520in%2520the%2520Medical%2520Segmentation%2520Decathlon%2520%2528MSD%2529%2520public%250Aleaderboard%2520and%2520leading%2520performance%2520on%2520the%2520Beyond%2520The%2520Cranial%2520Vault%2520%2528BTCV%2529%250Adataset.%2520In%2520summary%252C%2520Universal%2520Model%2520exhibits%2520remarkable%2520computational%250Aefficiency%2520%25286x%2520faster%2520than%2520other%2520dataset-specific%2520models%2529%252C%2520demonstrates%2520strong%250Ageneralization%2520across%2520different%2520hospitals%252C%2520transfers%2520well%2520to%2520numerous%250Adownstream%2520tasks%252C%2520and%2520more%2520importantly%252C%2520facilitates%2520the%2520extensibility%2520to%2520new%250Aclasses%2520while%2520alleviating%2520the%2520catastrophic%2520forgetting%2520of%2520previously%2520learned%250Aclasses.%2520Codes%252C%2520models%252C%2520and%2520datasets%2520are%2520available%2520at%250Ahttps%253A//github.com/ljwztc/CLIP-Driven-Universal-Model%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20and%20Extensible%20Language-Vision%20Models%20for%20Organ%20Segmentation%0A%20%20and%20Tumor%20Detection%20from%20Abdominal%20Computed%20Tomography&entry.906535625=Jie%20Liu%20and%20Yixiao%20Zhang%20and%20Kang%20Wang%20and%20Mehmet%20Can%20Yavuz%20and%20Xiaoxi%20Chen%20and%20Yixuan%20Yuan%20and%20Haoliang%20Li%20and%20Yang%20Yang%20and%20Alan%20Yuille%20and%20Yucheng%20Tang%20and%20Zongwei%20Zhou&entry.1292438233=%20%20The%20advancement%20of%20artificial%20intelligence%20%28AI%29%20for%20organ%20segmentation%20and%0Atumor%20detection%20is%20propelled%20by%20the%20growing%20availability%20of%20computed%20tomography%0A%28CT%29%20datasets%20with%20detailed%2C%20per-voxel%20annotations.%20However%2C%20these%20AI%20models%0Aoften%20struggle%20with%20flexibility%20for%20partially%20annotated%20datasets%20and%0Aextensibility%20for%20new%20classes%20due%20to%20limitations%20in%20the%20one-hot%20encoding%2C%0Aarchitectural%20design%2C%20and%20learning%20scheme.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20universal%2C%20extensible%20framework%20enabling%20a%20single%20model%2C%20termed%0AUniversal%20Model%2C%20to%20deal%20with%20multiple%20public%20datasets%20and%20adapt%20to%20new%20classes%0A%28e.g.%2C%20organs/tumors%29.%20Firstly%2C%20we%20introduce%20a%20novel%20language-driven%20parameter%0Agenerator%20that%20leverages%20language%20embeddings%20from%20large%20language%20models%2C%0Aenriching%20semantic%20encoding%20compared%20with%20one-hot%20encoding.%20Secondly%2C%20the%0Aconventional%20output%20layers%20are%20replaced%20with%20lightweight%2C%20class-specific%20heads%2C%0Aallowing%20Universal%20Model%20to%20simultaneously%20segment%2025%20organs%20and%20six%20types%20of%0Atumors%20and%20ease%20the%20addition%20of%20new%20classes.%20We%20train%20our%20Universal%20Model%20on%0A3%2C410%20CT%20volumes%20assembled%20from%2014%20publicly%20available%20datasets%20and%20then%20test%20it%0Aon%206%2C173%20CT%20volumes%20from%20four%20external%20datasets.%20Universal%20Model%20achieves%20first%0Aplace%20on%20six%20CT%20tasks%20in%20the%20Medical%20Segmentation%20Decathlon%20%28MSD%29%20public%0Aleaderboard%20and%20leading%20performance%20on%20the%20Beyond%20The%20Cranial%20Vault%20%28BTCV%29%0Adataset.%20In%20summary%2C%20Universal%20Model%20exhibits%20remarkable%20computational%0Aefficiency%20%286x%20faster%20than%20other%20dataset-specific%20models%29%2C%20demonstrates%20strong%0Ageneralization%20across%20different%20hospitals%2C%20transfers%20well%20to%20numerous%0Adownstream%20tasks%2C%20and%20more%20importantly%2C%20facilitates%20the%20extensibility%20to%20new%0Aclasses%20while%20alleviating%20the%20catastrophic%20forgetting%20of%20previously%20learned%0Aclasses.%20Codes%2C%20models%2C%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/ljwztc/CLIP-Driven-Universal-Model%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18356v1&entry.124074799=Read"},
{"title": "Multi-modal Generation via Cross-Modal In-Context Learning", "author": "Amandeep Kumar and Muzammal Naseer and Sanath Narayan and Rao Muhammad Anwer and Salman Khan and Hisham Cholakkal", "abstract": "  In this work, we study the problem of generating novel images from complex\nmultimodal prompt sequences. While existing methods achieve promising results\nfor text-to-image generation, they often struggle to capture fine-grained\ndetails from lengthy prompts and maintain contextual coherence within prompt\nsequences. Moreover, they often result in misaligned image generation for\nprompt sequences featuring multiple objects. To address this, we propose a\nMulti-modal Generation via Cross-Modal In-Context Learning (MGCC) method that\ngenerates novel images from complex multimodal prompt sequences by leveraging\nthe combined capabilities of large language models (LLMs) and diffusion models.\nOur MGCC comprises a novel Cross-Modal Refinement module to explicitly learn\ncross-modal dependencies between the text and image in the LLM embedding space,\nand a contextual object grounding module to generate object bounding boxes\nspecifically targeting scenes with multiple objects. Our MGCC demonstrates a\ndiverse range of multimodal capabilities, like novel image generation, the\nfacilitation of multimodal dialogue, and generation of texts. Experimental\nevaluations on two benchmark datasets, demonstrate the effectiveness of our\nmethod. On Visual Story Generation (VIST) dataset with multimodal inputs, our\nMGCC achieves a CLIP Similarity score of $0.652$ compared to SOTA GILL $0.641$.\nSimilarly, on Visual Dialogue Context (VisDial) having lengthy dialogue\nsequences, our MGCC achieves an impressive CLIP score of $0.660$, largely\noutperforming existing SOTA method scoring $0.645$. Code:\nhttps://github.com/VIROBO-15/MGCC\n", "link": "http://arxiv.org/abs/2405.18304v1", "date": "2024-05-28", "relevancy": 2.3386, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.593}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5794}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Generation%20via%20Cross-Modal%20In-Context%20Learning&body=Title%3A%20Multi-modal%20Generation%20via%20Cross-Modal%20In-Context%20Learning%0AAuthor%3A%20Amandeep%20Kumar%20and%20Muzammal%20Naseer%20and%20Sanath%20Narayan%20and%20Rao%20Muhammad%20Anwer%20and%20Salman%20Khan%20and%20Hisham%20Cholakkal%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20study%20the%20problem%20of%20generating%20novel%20images%20from%20complex%0Amultimodal%20prompt%20sequences.%20While%20existing%20methods%20achieve%20promising%20results%0Afor%20text-to-image%20generation%2C%20they%20often%20struggle%20to%20capture%20fine-grained%0Adetails%20from%20lengthy%20prompts%20and%20maintain%20contextual%20coherence%20within%20prompt%0Asequences.%20Moreover%2C%20they%20often%20result%20in%20misaligned%20image%20generation%20for%0Aprompt%20sequences%20featuring%20multiple%20objects.%20To%20address%20this%2C%20we%20propose%20a%0AMulti-modal%20Generation%20via%20Cross-Modal%20In-Context%20Learning%20%28MGCC%29%20method%20that%0Agenerates%20novel%20images%20from%20complex%20multimodal%20prompt%20sequences%20by%20leveraging%0Athe%20combined%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20and%20diffusion%20models.%0AOur%20MGCC%20comprises%20a%20novel%20Cross-Modal%20Refinement%20module%20to%20explicitly%20learn%0Across-modal%20dependencies%20between%20the%20text%20and%20image%20in%20the%20LLM%20embedding%20space%2C%0Aand%20a%20contextual%20object%20grounding%20module%20to%20generate%20object%20bounding%20boxes%0Aspecifically%20targeting%20scenes%20with%20multiple%20objects.%20Our%20MGCC%20demonstrates%20a%0Adiverse%20range%20of%20multimodal%20capabilities%2C%20like%20novel%20image%20generation%2C%20the%0Afacilitation%20of%20multimodal%20dialogue%2C%20and%20generation%20of%20texts.%20Experimental%0Aevaluations%20on%20two%20benchmark%20datasets%2C%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%20On%20Visual%20Story%20Generation%20%28VIST%29%20dataset%20with%20multimodal%20inputs%2C%20our%0AMGCC%20achieves%20a%20CLIP%20Similarity%20score%20of%20%240.652%24%20compared%20to%20SOTA%20GILL%20%240.641%24.%0ASimilarly%2C%20on%20Visual%20Dialogue%20Context%20%28VisDial%29%20having%20lengthy%20dialogue%0Asequences%2C%20our%20MGCC%20achieves%20an%20impressive%20CLIP%20score%20of%20%240.660%24%2C%20largely%0Aoutperforming%20existing%20SOTA%20method%20scoring%20%240.645%24.%20Code%3A%0Ahttps%3A//github.com/VIROBO-15/MGCC%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Generation%2520via%2520Cross-Modal%2520In-Context%2520Learning%26entry.906535625%3DAmandeep%2520Kumar%2520and%2520Muzammal%2520Naseer%2520and%2520Sanath%2520Narayan%2520and%2520Rao%2520Muhammad%2520Anwer%2520and%2520Salman%2520Khan%2520and%2520Hisham%2520Cholakkal%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520problem%2520of%2520generating%2520novel%2520images%2520from%2520complex%250Amultimodal%2520prompt%2520sequences.%2520While%2520existing%2520methods%2520achieve%2520promising%2520results%250Afor%2520text-to-image%2520generation%252C%2520they%2520often%2520struggle%2520to%2520capture%2520fine-grained%250Adetails%2520from%2520lengthy%2520prompts%2520and%2520maintain%2520contextual%2520coherence%2520within%2520prompt%250Asequences.%2520Moreover%252C%2520they%2520often%2520result%2520in%2520misaligned%2520image%2520generation%2520for%250Aprompt%2520sequences%2520featuring%2520multiple%2520objects.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250AMulti-modal%2520Generation%2520via%2520Cross-Modal%2520In-Context%2520Learning%2520%2528MGCC%2529%2520method%2520that%250Agenerates%2520novel%2520images%2520from%2520complex%2520multimodal%2520prompt%2520sequences%2520by%2520leveraging%250Athe%2520combined%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520diffusion%2520models.%250AOur%2520MGCC%2520comprises%2520a%2520novel%2520Cross-Modal%2520Refinement%2520module%2520to%2520explicitly%2520learn%250Across-modal%2520dependencies%2520between%2520the%2520text%2520and%2520image%2520in%2520the%2520LLM%2520embedding%2520space%252C%250Aand%2520a%2520contextual%2520object%2520grounding%2520module%2520to%2520generate%2520object%2520bounding%2520boxes%250Aspecifically%2520targeting%2520scenes%2520with%2520multiple%2520objects.%2520Our%2520MGCC%2520demonstrates%2520a%250Adiverse%2520range%2520of%2520multimodal%2520capabilities%252C%2520like%2520novel%2520image%2520generation%252C%2520the%250Afacilitation%2520of%2520multimodal%2520dialogue%252C%2520and%2520generation%2520of%2520texts.%2520Experimental%250Aevaluations%2520on%2520two%2520benchmark%2520datasets%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod.%2520On%2520Visual%2520Story%2520Generation%2520%2528VIST%2529%2520dataset%2520with%2520multimodal%2520inputs%252C%2520our%250AMGCC%2520achieves%2520a%2520CLIP%2520Similarity%2520score%2520of%2520%25240.652%2524%2520compared%2520to%2520SOTA%2520GILL%2520%25240.641%2524.%250ASimilarly%252C%2520on%2520Visual%2520Dialogue%2520Context%2520%2528VisDial%2529%2520having%2520lengthy%2520dialogue%250Asequences%252C%2520our%2520MGCC%2520achieves%2520an%2520impressive%2520CLIP%2520score%2520of%2520%25240.660%2524%252C%2520largely%250Aoutperforming%2520existing%2520SOTA%2520method%2520scoring%2520%25240.645%2524.%2520Code%253A%250Ahttps%253A//github.com/VIROBO-15/MGCC%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Generation%20via%20Cross-Modal%20In-Context%20Learning&entry.906535625=Amandeep%20Kumar%20and%20Muzammal%20Naseer%20and%20Sanath%20Narayan%20and%20Rao%20Muhammad%20Anwer%20and%20Salman%20Khan%20and%20Hisham%20Cholakkal&entry.1292438233=%20%20In%20this%20work%2C%20we%20study%20the%20problem%20of%20generating%20novel%20images%20from%20complex%0Amultimodal%20prompt%20sequences.%20While%20existing%20methods%20achieve%20promising%20results%0Afor%20text-to-image%20generation%2C%20they%20often%20struggle%20to%20capture%20fine-grained%0Adetails%20from%20lengthy%20prompts%20and%20maintain%20contextual%20coherence%20within%20prompt%0Asequences.%20Moreover%2C%20they%20often%20result%20in%20misaligned%20image%20generation%20for%0Aprompt%20sequences%20featuring%20multiple%20objects.%20To%20address%20this%2C%20we%20propose%20a%0AMulti-modal%20Generation%20via%20Cross-Modal%20In-Context%20Learning%20%28MGCC%29%20method%20that%0Agenerates%20novel%20images%20from%20complex%20multimodal%20prompt%20sequences%20by%20leveraging%0Athe%20combined%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20and%20diffusion%20models.%0AOur%20MGCC%20comprises%20a%20novel%20Cross-Modal%20Refinement%20module%20to%20explicitly%20learn%0Across-modal%20dependencies%20between%20the%20text%20and%20image%20in%20the%20LLM%20embedding%20space%2C%0Aand%20a%20contextual%20object%20grounding%20module%20to%20generate%20object%20bounding%20boxes%0Aspecifically%20targeting%20scenes%20with%20multiple%20objects.%20Our%20MGCC%20demonstrates%20a%0Adiverse%20range%20of%20multimodal%20capabilities%2C%20like%20novel%20image%20generation%2C%20the%0Afacilitation%20of%20multimodal%20dialogue%2C%20and%20generation%20of%20texts.%20Experimental%0Aevaluations%20on%20two%20benchmark%20datasets%2C%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%20On%20Visual%20Story%20Generation%20%28VIST%29%20dataset%20with%20multimodal%20inputs%2C%20our%0AMGCC%20achieves%20a%20CLIP%20Similarity%20score%20of%20%240.652%24%20compared%20to%20SOTA%20GILL%20%240.641%24.%0ASimilarly%2C%20on%20Visual%20Dialogue%20Context%20%28VisDial%29%20having%20lengthy%20dialogue%0Asequences%2C%20our%20MGCC%20achieves%20an%20impressive%20CLIP%20score%20of%20%240.660%24%2C%20largely%0Aoutperforming%20existing%20SOTA%20method%20scoring%20%240.645%24.%20Code%3A%0Ahttps%3A//github.com/VIROBO-15/MGCC%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18304v1&entry.124074799=Read"},
{"title": "On the importance of cross-task features for class-incremental learning", "author": "Albin Soutif--Cormerais and Marc Masana and Joost van de Weijer and Bart\u0142omiej Twardowski", "abstract": "  In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features, and\nthe knowledge transfer between tasks. This is especially important when tasks\ncontain limited amount of data.\n", "link": "http://arxiv.org/abs/2106.11930v4", "date": "2024-05-28", "relevancy": 2.3346, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4738}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4652}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20importance%20of%20cross-task%20features%20for%20class-incremental%20learning&body=Title%3A%20On%20the%20importance%20of%20cross-task%20features%20for%20class-incremental%20learning%0AAuthor%3A%20Albin%20Soutif--Cormerais%20and%20Marc%20Masana%20and%20Joost%20van%20de%20Weijer%20and%20Bart%C5%82omiej%20Twardowski%0AAbstract%3A%20%20%20In%20class-incremental%20learning%2C%20an%20agent%20with%20limited%20resources%20needs%20to%20learn%0Aa%20sequence%20of%20classification%20tasks%2C%20forming%20an%20ever%20growing%20classification%0Aproblem%2C%20with%20the%20constraint%20of%20not%20being%20able%20to%20access%20data%20from%20previous%0Atasks.%20The%20main%20difference%20with%20task-incremental%20learning%2C%20where%20a%20task-ID%20is%0Aavailable%20at%20inference%20time%2C%20is%20that%20the%20learner%20also%20needs%20to%20perform%0Across-task%20discrimination%2C%20i.e.%20distinguish%20between%20classes%20that%20have%20not%20been%0Aseen%20together.%20Approaches%20to%20tackle%20this%20problem%20are%20numerous%20and%20mostly%20make%0Ause%20of%20an%20external%20memory%20%28buffer%29%20of%20non-negligible%20size.%20In%20this%20paper%2C%20we%0Aablate%20the%20learning%20of%20cross-task%20features%20and%20study%20its%20influence%20on%20the%0Aperformance%20of%20basic%20replay%20strategies%20used%20for%20class-IL.%20We%20also%20define%20a%20new%0Aforgetting%20measure%20for%20class-incremental%20learning%2C%20and%20see%20that%20forgetting%20is%0Anot%20the%20principal%20cause%20of%20low%20performance.%20Our%20experimental%20results%20show%20that%0Afuture%20algorithms%20for%20class-incremental%20learning%20should%20not%20only%20prevent%0Aforgetting%2C%20but%20also%20aim%20to%20improve%20the%20quality%20of%20the%20cross-task%20features%2C%20and%0Athe%20knowledge%20transfer%20between%20tasks.%20This%20is%20especially%20important%20when%20tasks%0Acontain%20limited%20amount%20of%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2106.11930v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520importance%2520of%2520cross-task%2520features%2520for%2520class-incremental%2520learning%26entry.906535625%3DAlbin%2520Soutif--Cormerais%2520and%2520Marc%2520Masana%2520and%2520Joost%2520van%2520de%2520Weijer%2520and%2520Bart%25C5%2582omiej%2520Twardowski%26entry.1292438233%3D%2520%2520In%2520class-incremental%2520learning%252C%2520an%2520agent%2520with%2520limited%2520resources%2520needs%2520to%2520learn%250Aa%2520sequence%2520of%2520classification%2520tasks%252C%2520forming%2520an%2520ever%2520growing%2520classification%250Aproblem%252C%2520with%2520the%2520constraint%2520of%2520not%2520being%2520able%2520to%2520access%2520data%2520from%2520previous%250Atasks.%2520The%2520main%2520difference%2520with%2520task-incremental%2520learning%252C%2520where%2520a%2520task-ID%2520is%250Aavailable%2520at%2520inference%2520time%252C%2520is%2520that%2520the%2520learner%2520also%2520needs%2520to%2520perform%250Across-task%2520discrimination%252C%2520i.e.%2520distinguish%2520between%2520classes%2520that%2520have%2520not%2520been%250Aseen%2520together.%2520Approaches%2520to%2520tackle%2520this%2520problem%2520are%2520numerous%2520and%2520mostly%2520make%250Ause%2520of%2520an%2520external%2520memory%2520%2528buffer%2529%2520of%2520non-negligible%2520size.%2520In%2520this%2520paper%252C%2520we%250Aablate%2520the%2520learning%2520of%2520cross-task%2520features%2520and%2520study%2520its%2520influence%2520on%2520the%250Aperformance%2520of%2520basic%2520replay%2520strategies%2520used%2520for%2520class-IL.%2520We%2520also%2520define%2520a%2520new%250Aforgetting%2520measure%2520for%2520class-incremental%2520learning%252C%2520and%2520see%2520that%2520forgetting%2520is%250Anot%2520the%2520principal%2520cause%2520of%2520low%2520performance.%2520Our%2520experimental%2520results%2520show%2520that%250Afuture%2520algorithms%2520for%2520class-incremental%2520learning%2520should%2520not%2520only%2520prevent%250Aforgetting%252C%2520but%2520also%2520aim%2520to%2520improve%2520the%2520quality%2520of%2520the%2520cross-task%2520features%252C%2520and%250Athe%2520knowledge%2520transfer%2520between%2520tasks.%2520This%2520is%2520especially%2520important%2520when%2520tasks%250Acontain%2520limited%2520amount%2520of%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2106.11930v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20importance%20of%20cross-task%20features%20for%20class-incremental%20learning&entry.906535625=Albin%20Soutif--Cormerais%20and%20Marc%20Masana%20and%20Joost%20van%20de%20Weijer%20and%20Bart%C5%82omiej%20Twardowski&entry.1292438233=%20%20In%20class-incremental%20learning%2C%20an%20agent%20with%20limited%20resources%20needs%20to%20learn%0Aa%20sequence%20of%20classification%20tasks%2C%20forming%20an%20ever%20growing%20classification%0Aproblem%2C%20with%20the%20constraint%20of%20not%20being%20able%20to%20access%20data%20from%20previous%0Atasks.%20The%20main%20difference%20with%20task-incremental%20learning%2C%20where%20a%20task-ID%20is%0Aavailable%20at%20inference%20time%2C%20is%20that%20the%20learner%20also%20needs%20to%20perform%0Across-task%20discrimination%2C%20i.e.%20distinguish%20between%20classes%20that%20have%20not%20been%0Aseen%20together.%20Approaches%20to%20tackle%20this%20problem%20are%20numerous%20and%20mostly%20make%0Ause%20of%20an%20external%20memory%20%28buffer%29%20of%20non-negligible%20size.%20In%20this%20paper%2C%20we%0Aablate%20the%20learning%20of%20cross-task%20features%20and%20study%20its%20influence%20on%20the%0Aperformance%20of%20basic%20replay%20strategies%20used%20for%20class-IL.%20We%20also%20define%20a%20new%0Aforgetting%20measure%20for%20class-incremental%20learning%2C%20and%20see%20that%20forgetting%20is%0Anot%20the%20principal%20cause%20of%20low%20performance.%20Our%20experimental%20results%20show%20that%0Afuture%20algorithms%20for%20class-incremental%20learning%20should%20not%20only%20prevent%0Aforgetting%2C%20but%20also%20aim%20to%20improve%20the%20quality%20of%20the%20cross-task%20features%2C%20and%0Athe%20knowledge%20transfer%20between%20tasks.%20This%20is%20especially%20important%20when%20tasks%0Acontain%20limited%20amount%20of%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2106.11930v4&entry.124074799=Read"},
{"title": "Intelligent Clinical Documentation: Harnessing Generative AI for\n  Patient-Centric Clinical Note Generation", "author": "Anjanava Biswas and Wrick Talukdar", "abstract": "  Comprehensive clinical documentation is crucial for effective healthcare\ndelivery, yet it poses a significant burden on healthcare professionals,\nleading to burnout, increased medical errors, and compromised patient safety.\nThis paper explores the potential of generative AI (Artificial Intelligence) to\nstreamline the clinical documentation process, specifically focusing on\ngenerating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior,\nIntervention, Response, Plan) notes. We present a case study demonstrating the\napplication of natural language processing (NLP) and automatic speech\nrecognition (ASR) technologies to transcribe patient-clinician interactions,\ncoupled with advanced prompting techniques to generate draft clinical notes\nusing large language models (LLMs). The study highlights the benefits of this\napproach, including time savings, improved documentation quality, and enhanced\npatient-centered care. Additionally, we discuss ethical considerations, such as\nmaintaining patient confidentiality and addressing model biases, underscoring\nthe need for responsible deployment of generative AI in healthcare settings.\nThe findings suggest that generative AI has the potential to revolutionize\nclinical documentation practices, alleviating administrative burdens and\nenabling healthcare professionals to focus more on direct patient care.\n", "link": "http://arxiv.org/abs/2405.18346v1", "date": "2024-05-28", "relevancy": 2.3213, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4836}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4742}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Clinical%20Documentation%3A%20Harnessing%20Generative%20AI%20for%0A%20%20Patient-Centric%20Clinical%20Note%20Generation&body=Title%3A%20Intelligent%20Clinical%20Documentation%3A%20Harnessing%20Generative%20AI%20for%0A%20%20Patient-Centric%20Clinical%20Note%20Generation%0AAuthor%3A%20Anjanava%20Biswas%20and%20Wrick%20Talukdar%0AAbstract%3A%20%20%20Comprehensive%20clinical%20documentation%20is%20crucial%20for%20effective%20healthcare%0Adelivery%2C%20yet%20it%20poses%20a%20significant%20burden%20on%20healthcare%20professionals%2C%0Aleading%20to%20burnout%2C%20increased%20medical%20errors%2C%20and%20compromised%20patient%20safety.%0AThis%20paper%20explores%20the%20potential%20of%20generative%20AI%20%28Artificial%20Intelligence%29%20to%0Astreamline%20the%20clinical%20documentation%20process%2C%20specifically%20focusing%20on%0Agenerating%20SOAP%20%28Subjective%2C%20Objective%2C%20Assessment%2C%20Plan%29%20and%20BIRP%20%28Behavior%2C%0AIntervention%2C%20Response%2C%20Plan%29%20notes.%20We%20present%20a%20case%20study%20demonstrating%20the%0Aapplication%20of%20natural%20language%20processing%20%28NLP%29%20and%20automatic%20speech%0Arecognition%20%28ASR%29%20technologies%20to%20transcribe%20patient-clinician%20interactions%2C%0Acoupled%20with%20advanced%20prompting%20techniques%20to%20generate%20draft%20clinical%20notes%0Ausing%20large%20language%20models%20%28LLMs%29.%20The%20study%20highlights%20the%20benefits%20of%20this%0Aapproach%2C%20including%20time%20savings%2C%20improved%20documentation%20quality%2C%20and%20enhanced%0Apatient-centered%20care.%20Additionally%2C%20we%20discuss%20ethical%20considerations%2C%20such%20as%0Amaintaining%20patient%20confidentiality%20and%20addressing%20model%20biases%2C%20underscoring%0Athe%20need%20for%20responsible%20deployment%20of%20generative%20AI%20in%20healthcare%20settings.%0AThe%20findings%20suggest%20that%20generative%20AI%20has%20the%20potential%20to%20revolutionize%0Aclinical%20documentation%20practices%2C%20alleviating%20administrative%20burdens%20and%0Aenabling%20healthcare%20professionals%20to%20focus%20more%20on%20direct%20patient%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Clinical%2520Documentation%253A%2520Harnessing%2520Generative%2520AI%2520for%250A%2520%2520Patient-Centric%2520Clinical%2520Note%2520Generation%26entry.906535625%3DAnjanava%2520Biswas%2520and%2520Wrick%2520Talukdar%26entry.1292438233%3D%2520%2520Comprehensive%2520clinical%2520documentation%2520is%2520crucial%2520for%2520effective%2520healthcare%250Adelivery%252C%2520yet%2520it%2520poses%2520a%2520significant%2520burden%2520on%2520healthcare%2520professionals%252C%250Aleading%2520to%2520burnout%252C%2520increased%2520medical%2520errors%252C%2520and%2520compromised%2520patient%2520safety.%250AThis%2520paper%2520explores%2520the%2520potential%2520of%2520generative%2520AI%2520%2528Artificial%2520Intelligence%2529%2520to%250Astreamline%2520the%2520clinical%2520documentation%2520process%252C%2520specifically%2520focusing%2520on%250Agenerating%2520SOAP%2520%2528Subjective%252C%2520Objective%252C%2520Assessment%252C%2520Plan%2529%2520and%2520BIRP%2520%2528Behavior%252C%250AIntervention%252C%2520Response%252C%2520Plan%2529%2520notes.%2520We%2520present%2520a%2520case%2520study%2520demonstrating%2520the%250Aapplication%2520of%2520natural%2520language%2520processing%2520%2528NLP%2529%2520and%2520automatic%2520speech%250Arecognition%2520%2528ASR%2529%2520technologies%2520to%2520transcribe%2520patient-clinician%2520interactions%252C%250Acoupled%2520with%2520advanced%2520prompting%2520techniques%2520to%2520generate%2520draft%2520clinical%2520notes%250Ausing%2520large%2520language%2520models%2520%2528LLMs%2529.%2520The%2520study%2520highlights%2520the%2520benefits%2520of%2520this%250Aapproach%252C%2520including%2520time%2520savings%252C%2520improved%2520documentation%2520quality%252C%2520and%2520enhanced%250Apatient-centered%2520care.%2520Additionally%252C%2520we%2520discuss%2520ethical%2520considerations%252C%2520such%2520as%250Amaintaining%2520patient%2520confidentiality%2520and%2520addressing%2520model%2520biases%252C%2520underscoring%250Athe%2520need%2520for%2520responsible%2520deployment%2520of%2520generative%2520AI%2520in%2520healthcare%2520settings.%250AThe%2520findings%2520suggest%2520that%2520generative%2520AI%2520has%2520the%2520potential%2520to%2520revolutionize%250Aclinical%2520documentation%2520practices%252C%2520alleviating%2520administrative%2520burdens%2520and%250Aenabling%2520healthcare%2520professionals%2520to%2520focus%2520more%2520on%2520direct%2520patient%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Clinical%20Documentation%3A%20Harnessing%20Generative%20AI%20for%0A%20%20Patient-Centric%20Clinical%20Note%20Generation&entry.906535625=Anjanava%20Biswas%20and%20Wrick%20Talukdar&entry.1292438233=%20%20Comprehensive%20clinical%20documentation%20is%20crucial%20for%20effective%20healthcare%0Adelivery%2C%20yet%20it%20poses%20a%20significant%20burden%20on%20healthcare%20professionals%2C%0Aleading%20to%20burnout%2C%20increased%20medical%20errors%2C%20and%20compromised%20patient%20safety.%0AThis%20paper%20explores%20the%20potential%20of%20generative%20AI%20%28Artificial%20Intelligence%29%20to%0Astreamline%20the%20clinical%20documentation%20process%2C%20specifically%20focusing%20on%0Agenerating%20SOAP%20%28Subjective%2C%20Objective%2C%20Assessment%2C%20Plan%29%20and%20BIRP%20%28Behavior%2C%0AIntervention%2C%20Response%2C%20Plan%29%20notes.%20We%20present%20a%20case%20study%20demonstrating%20the%0Aapplication%20of%20natural%20language%20processing%20%28NLP%29%20and%20automatic%20speech%0Arecognition%20%28ASR%29%20technologies%20to%20transcribe%20patient-clinician%20interactions%2C%0Acoupled%20with%20advanced%20prompting%20techniques%20to%20generate%20draft%20clinical%20notes%0Ausing%20large%20language%20models%20%28LLMs%29.%20The%20study%20highlights%20the%20benefits%20of%20this%0Aapproach%2C%20including%20time%20savings%2C%20improved%20documentation%20quality%2C%20and%20enhanced%0Apatient-centered%20care.%20Additionally%2C%20we%20discuss%20ethical%20considerations%2C%20such%20as%0Amaintaining%20patient%20confidentiality%20and%20addressing%20model%20biases%2C%20underscoring%0Athe%20need%20for%20responsible%20deployment%20of%20generative%20AI%20in%20healthcare%20settings.%0AThe%20findings%20suggest%20that%20generative%20AI%20has%20the%20potential%20to%20revolutionize%0Aclinical%20documentation%20practices%2C%20alleviating%20administrative%20burdens%20and%0Aenabling%20healthcare%20professionals%20to%20focus%20more%20on%20direct%20patient%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18346v1&entry.124074799=Read"},
{"title": "Lumen: Unleashing Versatile Vision-Centric Capabilities of Large\n  Multimodal Models", "author": "Yang Jiao and Shaoxiang Chen and Zequn Jie and Jingjing Chen and Lin Ma and Yu-Gang Jiang", "abstract": "  Large Multimodal Model (LMM) is a hot research topic in the computer vision\narea and has also demonstrated remarkable potential across multiple\ndisciplinary fields. A recent trend is to further extend and enhance the\nperception capabilities of LMMs. The current methods follow the paradigm of\nadapting the visual task outputs to the format of the language model, which is\nthe main component of a LMM. This adaptation leads to convenient development of\nsuch LMMs with minimal modifications, however, it overlooks the intrinsic\ncharacteristics of diverse visual tasks and hinders the learning of perception\ncapabilities. To address this issue, we propose a novel LMM architecture named\nLumen, a Large multimodal model with versatile vision-centric capability\nenhancement. We decouple the LMM's learning of perception capabilities into\ntask-agnostic and task-specific stages. Lumen first promotes fine-grained\nvision-language concept alignment, which is the fundamental capability for\nvarious visual tasks. Thus the output of the task-agnostic stage is a shared\nrepresentation for all the tasks we address in this paper. Then the\ntask-specific decoding is carried out by flexibly routing the shared\nrepresentation to lightweight task decoders with negligible training efforts.\nComprehensive experimental results on a series of vision-centric and VQA\nbenchmarks indicate that our Lumen model not only achieves or surpasses the\nperformance of existing LMM-based approaches in a range of vision-centric tasks\nwhile maintaining general visual understanding and instruction following\ncapabilities. The code will be released at https://github.com/SxJyJay/Lumen.\n", "link": "http://arxiv.org/abs/2403.07304v2", "date": "2024-05-28", "relevancy": 2.3062, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6096}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5549}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lumen%3A%20Unleashing%20Versatile%20Vision-Centric%20Capabilities%20of%20Large%0A%20%20Multimodal%20Models&body=Title%3A%20Lumen%3A%20Unleashing%20Versatile%20Vision-Centric%20Capabilities%20of%20Large%0A%20%20Multimodal%20Models%0AAuthor%3A%20Yang%20Jiao%20and%20Shaoxiang%20Chen%20and%20Zequn%20Jie%20and%20Jingjing%20Chen%20and%20Lin%20Ma%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Large%20Multimodal%20Model%20%28LMM%29%20is%20a%20hot%20research%20topic%20in%20the%20computer%20vision%0Aarea%20and%20has%20also%20demonstrated%20remarkable%20potential%20across%20multiple%0Adisciplinary%20fields.%20A%20recent%20trend%20is%20to%20further%20extend%20and%20enhance%20the%0Aperception%20capabilities%20of%20LMMs.%20The%20current%20methods%20follow%20the%20paradigm%20of%0Aadapting%20the%20visual%20task%20outputs%20to%20the%20format%20of%20the%20language%20model%2C%20which%20is%0Athe%20main%20component%20of%20a%20LMM.%20This%20adaptation%20leads%20to%20convenient%20development%20of%0Asuch%20LMMs%20with%20minimal%20modifications%2C%20however%2C%20it%20overlooks%20the%20intrinsic%0Acharacteristics%20of%20diverse%20visual%20tasks%20and%20hinders%20the%20learning%20of%20perception%0Acapabilities.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20LMM%20architecture%20named%0ALumen%2C%20a%20Large%20multimodal%20model%20with%20versatile%20vision-centric%20capability%0Aenhancement.%20We%20decouple%20the%20LMM%27s%20learning%20of%20perception%20capabilities%20into%0Atask-agnostic%20and%20task-specific%20stages.%20Lumen%20first%20promotes%20fine-grained%0Avision-language%20concept%20alignment%2C%20which%20is%20the%20fundamental%20capability%20for%0Avarious%20visual%20tasks.%20Thus%20the%20output%20of%20the%20task-agnostic%20stage%20is%20a%20shared%0Arepresentation%20for%20all%20the%20tasks%20we%20address%20in%20this%20paper.%20Then%20the%0Atask-specific%20decoding%20is%20carried%20out%20by%20flexibly%20routing%20the%20shared%0Arepresentation%20to%20lightweight%20task%20decoders%20with%20negligible%20training%20efforts.%0AComprehensive%20experimental%20results%20on%20a%20series%20of%20vision-centric%20and%20VQA%0Abenchmarks%20indicate%20that%20our%20Lumen%20model%20not%20only%20achieves%20or%20surpasses%20the%0Aperformance%20of%20existing%20LMM-based%20approaches%20in%20a%20range%20of%20vision-centric%20tasks%0Awhile%20maintaining%20general%20visual%20understanding%20and%20instruction%20following%0Acapabilities.%20The%20code%20will%20be%20released%20at%20https%3A//github.com/SxJyJay/Lumen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07304v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLumen%253A%2520Unleashing%2520Versatile%2520Vision-Centric%2520Capabilities%2520of%2520Large%250A%2520%2520Multimodal%2520Models%26entry.906535625%3DYang%2520Jiao%2520and%2520Shaoxiang%2520Chen%2520and%2520Zequn%2520Jie%2520and%2520Jingjing%2520Chen%2520and%2520Lin%2520Ma%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Model%2520%2528LMM%2529%2520is%2520a%2520hot%2520research%2520topic%2520in%2520the%2520computer%2520vision%250Aarea%2520and%2520has%2520also%2520demonstrated%2520remarkable%2520potential%2520across%2520multiple%250Adisciplinary%2520fields.%2520A%2520recent%2520trend%2520is%2520to%2520further%2520extend%2520and%2520enhance%2520the%250Aperception%2520capabilities%2520of%2520LMMs.%2520The%2520current%2520methods%2520follow%2520the%2520paradigm%2520of%250Aadapting%2520the%2520visual%2520task%2520outputs%2520to%2520the%2520format%2520of%2520the%2520language%2520model%252C%2520which%2520is%250Athe%2520main%2520component%2520of%2520a%2520LMM.%2520This%2520adaptation%2520leads%2520to%2520convenient%2520development%2520of%250Asuch%2520LMMs%2520with%2520minimal%2520modifications%252C%2520however%252C%2520it%2520overlooks%2520the%2520intrinsic%250Acharacteristics%2520of%2520diverse%2520visual%2520tasks%2520and%2520hinders%2520the%2520learning%2520of%2520perception%250Acapabilities.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520LMM%2520architecture%2520named%250ALumen%252C%2520a%2520Large%2520multimodal%2520model%2520with%2520versatile%2520vision-centric%2520capability%250Aenhancement.%2520We%2520decouple%2520the%2520LMM%2527s%2520learning%2520of%2520perception%2520capabilities%2520into%250Atask-agnostic%2520and%2520task-specific%2520stages.%2520Lumen%2520first%2520promotes%2520fine-grained%250Avision-language%2520concept%2520alignment%252C%2520which%2520is%2520the%2520fundamental%2520capability%2520for%250Avarious%2520visual%2520tasks.%2520Thus%2520the%2520output%2520of%2520the%2520task-agnostic%2520stage%2520is%2520a%2520shared%250Arepresentation%2520for%2520all%2520the%2520tasks%2520we%2520address%2520in%2520this%2520paper.%2520Then%2520the%250Atask-specific%2520decoding%2520is%2520carried%2520out%2520by%2520flexibly%2520routing%2520the%2520shared%250Arepresentation%2520to%2520lightweight%2520task%2520decoders%2520with%2520negligible%2520training%2520efforts.%250AComprehensive%2520experimental%2520results%2520on%2520a%2520series%2520of%2520vision-centric%2520and%2520VQA%250Abenchmarks%2520indicate%2520that%2520our%2520Lumen%2520model%2520not%2520only%2520achieves%2520or%2520surpasses%2520the%250Aperformance%2520of%2520existing%2520LMM-based%2520approaches%2520in%2520a%2520range%2520of%2520vision-centric%2520tasks%250Awhile%2520maintaining%2520general%2520visual%2520understanding%2520and%2520instruction%2520following%250Acapabilities.%2520The%2520code%2520will%2520be%2520released%2520at%2520https%253A//github.com/SxJyJay/Lumen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07304v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lumen%3A%20Unleashing%20Versatile%20Vision-Centric%20Capabilities%20of%20Large%0A%20%20Multimodal%20Models&entry.906535625=Yang%20Jiao%20and%20Shaoxiang%20Chen%20and%20Zequn%20Jie%20and%20Jingjing%20Chen%20and%20Lin%20Ma%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Large%20Multimodal%20Model%20%28LMM%29%20is%20a%20hot%20research%20topic%20in%20the%20computer%20vision%0Aarea%20and%20has%20also%20demonstrated%20remarkable%20potential%20across%20multiple%0Adisciplinary%20fields.%20A%20recent%20trend%20is%20to%20further%20extend%20and%20enhance%20the%0Aperception%20capabilities%20of%20LMMs.%20The%20current%20methods%20follow%20the%20paradigm%20of%0Aadapting%20the%20visual%20task%20outputs%20to%20the%20format%20of%20the%20language%20model%2C%20which%20is%0Athe%20main%20component%20of%20a%20LMM.%20This%20adaptation%20leads%20to%20convenient%20development%20of%0Asuch%20LMMs%20with%20minimal%20modifications%2C%20however%2C%20it%20overlooks%20the%20intrinsic%0Acharacteristics%20of%20diverse%20visual%20tasks%20and%20hinders%20the%20learning%20of%20perception%0Acapabilities.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20LMM%20architecture%20named%0ALumen%2C%20a%20Large%20multimodal%20model%20with%20versatile%20vision-centric%20capability%0Aenhancement.%20We%20decouple%20the%20LMM%27s%20learning%20of%20perception%20capabilities%20into%0Atask-agnostic%20and%20task-specific%20stages.%20Lumen%20first%20promotes%20fine-grained%0Avision-language%20concept%20alignment%2C%20which%20is%20the%20fundamental%20capability%20for%0Avarious%20visual%20tasks.%20Thus%20the%20output%20of%20the%20task-agnostic%20stage%20is%20a%20shared%0Arepresentation%20for%20all%20the%20tasks%20we%20address%20in%20this%20paper.%20Then%20the%0Atask-specific%20decoding%20is%20carried%20out%20by%20flexibly%20routing%20the%20shared%0Arepresentation%20to%20lightweight%20task%20decoders%20with%20negligible%20training%20efforts.%0AComprehensive%20experimental%20results%20on%20a%20series%20of%20vision-centric%20and%20VQA%0Abenchmarks%20indicate%20that%20our%20Lumen%20model%20not%20only%20achieves%20or%20surpasses%20the%0Aperformance%20of%20existing%20LMM-based%20approaches%20in%20a%20range%20of%20vision-centric%20tasks%0Awhile%20maintaining%20general%20visual%20understanding%20and%20instruction%20following%0Acapabilities.%20The%20code%20will%20be%20released%20at%20https%3A//github.com/SxJyJay/Lumen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07304v2&entry.124074799=Read"},
{"title": "A Generalization Theory of Cross-Modality Distillation with Contrastive\n  Learning", "author": "Hangyu Lin and Chen Liu and Chengming Xu and Zhengqi Gao and Yanwei Fu and Yuan Yao", "abstract": "  Cross-modality distillation arises as an important topic for data modalities\ncontaining limited knowledge such as depth maps and high-quality sketches. Such\ntechniques are of great importance, especially for memory and\nprivacy-restricted scenarios where labeled training data is generally\nunavailable. To solve the problem, existing label-free methods leverage a few\npairwise unlabeled data to distill the knowledge by aligning features or\nstatistics between the source and target modalities. For instance, one\ntypically aims to minimize the L2 distance or contrastive loss between the\nlearned features of pairs of samples in the source (e.g. image) and the target\n(e.g. sketch) modalities. However, most algorithms in this domain only focus on\nthe experimental results but lack theoretical insight. To bridge the gap\nbetween the theory and practical method of cross-modality distillation, we\nfirst formulate a general framework of cross-modality contrastive distillation\n(CMCD), built upon contrastive learning that leverages both positive and\nnegative correspondence, towards a better distillation of generalizable\nfeatures. Furthermore, we establish a thorough convergence analysis that\nreveals that the distance between source and target modalities significantly\nimpacts the test error on downstream tasks within the target modality which is\nalso validated by the empirical results. Extensive experimental results show\nthat our algorithm outperforms existing algorithms consistently by a margin of\n2-3\\% across diverse modalities and tasks, covering modalities of image,\nsketch, depth map, and audio and tasks of recognition and segmentation.\n", "link": "http://arxiv.org/abs/2405.03355v2", "date": "2024-05-28", "relevancy": 2.2869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6031}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5577}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Generalization%20Theory%20of%20Cross-Modality%20Distillation%20with%20Contrastive%0A%20%20Learning&body=Title%3A%20A%20Generalization%20Theory%20of%20Cross-Modality%20Distillation%20with%20Contrastive%0A%20%20Learning%0AAuthor%3A%20Hangyu%20Lin%20and%20Chen%20Liu%20and%20Chengming%20Xu%20and%20Zhengqi%20Gao%20and%20Yanwei%20Fu%20and%20Yuan%20Yao%0AAbstract%3A%20%20%20Cross-modality%20distillation%20arises%20as%20an%20important%20topic%20for%20data%20modalities%0Acontaining%20limited%20knowledge%20such%20as%20depth%20maps%20and%20high-quality%20sketches.%20Such%0Atechniques%20are%20of%20great%20importance%2C%20especially%20for%20memory%20and%0Aprivacy-restricted%20scenarios%20where%20labeled%20training%20data%20is%20generally%0Aunavailable.%20To%20solve%20the%20problem%2C%20existing%20label-free%20methods%20leverage%20a%20few%0Apairwise%20unlabeled%20data%20to%20distill%20the%20knowledge%20by%20aligning%20features%20or%0Astatistics%20between%20the%20source%20and%20target%20modalities.%20For%20instance%2C%20one%0Atypically%20aims%20to%20minimize%20the%20L2%20distance%20or%20contrastive%20loss%20between%20the%0Alearned%20features%20of%20pairs%20of%20samples%20in%20the%20source%20%28e.g.%20image%29%20and%20the%20target%0A%28e.g.%20sketch%29%20modalities.%20However%2C%20most%20algorithms%20in%20this%20domain%20only%20focus%20on%0Athe%20experimental%20results%20but%20lack%20theoretical%20insight.%20To%20bridge%20the%20gap%0Abetween%20the%20theory%20and%20practical%20method%20of%20cross-modality%20distillation%2C%20we%0Afirst%20formulate%20a%20general%20framework%20of%20cross-modality%20contrastive%20distillation%0A%28CMCD%29%2C%20built%20upon%20contrastive%20learning%20that%20leverages%20both%20positive%20and%0Anegative%20correspondence%2C%20towards%20a%20better%20distillation%20of%20generalizable%0Afeatures.%20Furthermore%2C%20we%20establish%20a%20thorough%20convergence%20analysis%20that%0Areveals%20that%20the%20distance%20between%20source%20and%20target%20modalities%20significantly%0Aimpacts%20the%20test%20error%20on%20downstream%20tasks%20within%20the%20target%20modality%20which%20is%0Aalso%20validated%20by%20the%20empirical%20results.%20Extensive%20experimental%20results%20show%0Athat%20our%20algorithm%20outperforms%20existing%20algorithms%20consistently%20by%20a%20margin%20of%0A2-3%5C%25%20across%20diverse%20modalities%20and%20tasks%2C%20covering%20modalities%20of%20image%2C%0Asketch%2C%20depth%20map%2C%20and%20audio%20and%20tasks%20of%20recognition%20and%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Generalization%2520Theory%2520of%2520Cross-Modality%2520Distillation%2520with%2520Contrastive%250A%2520%2520Learning%26entry.906535625%3DHangyu%2520Lin%2520and%2520Chen%2520Liu%2520and%2520Chengming%2520Xu%2520and%2520Zhengqi%2520Gao%2520and%2520Yanwei%2520Fu%2520and%2520Yuan%2520Yao%26entry.1292438233%3D%2520%2520Cross-modality%2520distillation%2520arises%2520as%2520an%2520important%2520topic%2520for%2520data%2520modalities%250Acontaining%2520limited%2520knowledge%2520such%2520as%2520depth%2520maps%2520and%2520high-quality%2520sketches.%2520Such%250Atechniques%2520are%2520of%2520great%2520importance%252C%2520especially%2520for%2520memory%2520and%250Aprivacy-restricted%2520scenarios%2520where%2520labeled%2520training%2520data%2520is%2520generally%250Aunavailable.%2520To%2520solve%2520the%2520problem%252C%2520existing%2520label-free%2520methods%2520leverage%2520a%2520few%250Apairwise%2520unlabeled%2520data%2520to%2520distill%2520the%2520knowledge%2520by%2520aligning%2520features%2520or%250Astatistics%2520between%2520the%2520source%2520and%2520target%2520modalities.%2520For%2520instance%252C%2520one%250Atypically%2520aims%2520to%2520minimize%2520the%2520L2%2520distance%2520or%2520contrastive%2520loss%2520between%2520the%250Alearned%2520features%2520of%2520pairs%2520of%2520samples%2520in%2520the%2520source%2520%2528e.g.%2520image%2529%2520and%2520the%2520target%250A%2528e.g.%2520sketch%2529%2520modalities.%2520However%252C%2520most%2520algorithms%2520in%2520this%2520domain%2520only%2520focus%2520on%250Athe%2520experimental%2520results%2520but%2520lack%2520theoretical%2520insight.%2520To%2520bridge%2520the%2520gap%250Abetween%2520the%2520theory%2520and%2520practical%2520method%2520of%2520cross-modality%2520distillation%252C%2520we%250Afirst%2520formulate%2520a%2520general%2520framework%2520of%2520cross-modality%2520contrastive%2520distillation%250A%2528CMCD%2529%252C%2520built%2520upon%2520contrastive%2520learning%2520that%2520leverages%2520both%2520positive%2520and%250Anegative%2520correspondence%252C%2520towards%2520a%2520better%2520distillation%2520of%2520generalizable%250Afeatures.%2520Furthermore%252C%2520we%2520establish%2520a%2520thorough%2520convergence%2520analysis%2520that%250Areveals%2520that%2520the%2520distance%2520between%2520source%2520and%2520target%2520modalities%2520significantly%250Aimpacts%2520the%2520test%2520error%2520on%2520downstream%2520tasks%2520within%2520the%2520target%2520modality%2520which%2520is%250Aalso%2520validated%2520by%2520the%2520empirical%2520results.%2520Extensive%2520experimental%2520results%2520show%250Athat%2520our%2520algorithm%2520outperforms%2520existing%2520algorithms%2520consistently%2520by%2520a%2520margin%2520of%250A2-3%255C%2525%2520across%2520diverse%2520modalities%2520and%2520tasks%252C%2520covering%2520modalities%2520of%2520image%252C%250Asketch%252C%2520depth%2520map%252C%2520and%2520audio%2520and%2520tasks%2520of%2520recognition%2520and%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generalization%20Theory%20of%20Cross-Modality%20Distillation%20with%20Contrastive%0A%20%20Learning&entry.906535625=Hangyu%20Lin%20and%20Chen%20Liu%20and%20Chengming%20Xu%20and%20Zhengqi%20Gao%20and%20Yanwei%20Fu%20and%20Yuan%20Yao&entry.1292438233=%20%20Cross-modality%20distillation%20arises%20as%20an%20important%20topic%20for%20data%20modalities%0Acontaining%20limited%20knowledge%20such%20as%20depth%20maps%20and%20high-quality%20sketches.%20Such%0Atechniques%20are%20of%20great%20importance%2C%20especially%20for%20memory%20and%0Aprivacy-restricted%20scenarios%20where%20labeled%20training%20data%20is%20generally%0Aunavailable.%20To%20solve%20the%20problem%2C%20existing%20label-free%20methods%20leverage%20a%20few%0Apairwise%20unlabeled%20data%20to%20distill%20the%20knowledge%20by%20aligning%20features%20or%0Astatistics%20between%20the%20source%20and%20target%20modalities.%20For%20instance%2C%20one%0Atypically%20aims%20to%20minimize%20the%20L2%20distance%20or%20contrastive%20loss%20between%20the%0Alearned%20features%20of%20pairs%20of%20samples%20in%20the%20source%20%28e.g.%20image%29%20and%20the%20target%0A%28e.g.%20sketch%29%20modalities.%20However%2C%20most%20algorithms%20in%20this%20domain%20only%20focus%20on%0Athe%20experimental%20results%20but%20lack%20theoretical%20insight.%20To%20bridge%20the%20gap%0Abetween%20the%20theory%20and%20practical%20method%20of%20cross-modality%20distillation%2C%20we%0Afirst%20formulate%20a%20general%20framework%20of%20cross-modality%20contrastive%20distillation%0A%28CMCD%29%2C%20built%20upon%20contrastive%20learning%20that%20leverages%20both%20positive%20and%0Anegative%20correspondence%2C%20towards%20a%20better%20distillation%20of%20generalizable%0Afeatures.%20Furthermore%2C%20we%20establish%20a%20thorough%20convergence%20analysis%20that%0Areveals%20that%20the%20distance%20between%20source%20and%20target%20modalities%20significantly%0Aimpacts%20the%20test%20error%20on%20downstream%20tasks%20within%20the%20target%20modality%20which%20is%0Aalso%20validated%20by%20the%20empirical%20results.%20Extensive%20experimental%20results%20show%0Athat%20our%20algorithm%20outperforms%20existing%20algorithms%20consistently%20by%20a%20margin%20of%0A2-3%5C%25%20across%20diverse%20modalities%20and%20tasks%2C%20covering%20modalities%20of%20image%2C%0Asketch%2C%20depth%20map%2C%20and%20audio%20and%20tasks%20of%20recognition%20and%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03355v2&entry.124074799=Read"},
{"title": "Dataset Growth", "author": "Ziheng Qin and Zhaopan Xu and Yukun Zhou and Zangwei Zheng and Zebang Cheng and Hao Tang and Lei Shang and Baigui Sun and Xiaojiang Peng and Radu Timofte and Hongxun Yao and Kai Wang and Yang You", "abstract": "  Deep learning benefits from the growing abundance of available data.\nMeanwhile, efficiently dealing with the growing data scale has become a\nchallenge. Data publicly available are from different sources with various\nqualities, and it is impractical to do manual cleaning against noise and\nredundancy given today's data scale. There are existing techniques for\ncleaning/selecting the collected data. However, these methods are mainly\nproposed for offline settings that target one of the cleanness and redundancy\nproblems. In practice, data are growing exponentially with both problems. This\nleads to repeated data curation with sub-optimal efficiency. To tackle this\nchallenge, we propose InfoGrowth, an efficient online algorithm for data\ncleaning and selection, resulting in a growing dataset that keeps up to date\nwith awareness of cleanliness and diversity. InfoGrowth can improve data\nquality/efficiency on both single-modal and multi-modal tasks, with an\nefficient and scalable design. Its framework makes it practical for real-world\ndata engines.\n", "link": "http://arxiv.org/abs/2405.18347v1", "date": "2024-05-28", "relevancy": 2.2749, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.465}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4569}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dataset%20Growth&body=Title%3A%20Dataset%20Growth%0AAuthor%3A%20Ziheng%20Qin%20and%20Zhaopan%20Xu%20and%20Yukun%20Zhou%20and%20Zangwei%20Zheng%20and%20Zebang%20Cheng%20and%20Hao%20Tang%20and%20Lei%20Shang%20and%20Baigui%20Sun%20and%20Xiaojiang%20Peng%20and%20Radu%20Timofte%20and%20Hongxun%20Yao%20and%20Kai%20Wang%20and%20Yang%20You%0AAbstract%3A%20%20%20Deep%20learning%20benefits%20from%20the%20growing%20abundance%20of%20available%20data.%0AMeanwhile%2C%20efficiently%20dealing%20with%20the%20growing%20data%20scale%20has%20become%20a%0Achallenge.%20Data%20publicly%20available%20are%20from%20different%20sources%20with%20various%0Aqualities%2C%20and%20it%20is%20impractical%20to%20do%20manual%20cleaning%20against%20noise%20and%0Aredundancy%20given%20today%27s%20data%20scale.%20There%20are%20existing%20techniques%20for%0Acleaning/selecting%20the%20collected%20data.%20However%2C%20these%20methods%20are%20mainly%0Aproposed%20for%20offline%20settings%20that%20target%20one%20of%20the%20cleanness%20and%20redundancy%0Aproblems.%20In%20practice%2C%20data%20are%20growing%20exponentially%20with%20both%20problems.%20This%0Aleads%20to%20repeated%20data%20curation%20with%20sub-optimal%20efficiency.%20To%20tackle%20this%0Achallenge%2C%20we%20propose%20InfoGrowth%2C%20an%20efficient%20online%20algorithm%20for%20data%0Acleaning%20and%20selection%2C%20resulting%20in%20a%20growing%20dataset%20that%20keeps%20up%20to%20date%0Awith%20awareness%20of%20cleanliness%20and%20diversity.%20InfoGrowth%20can%20improve%20data%0Aquality/efficiency%20on%20both%20single-modal%20and%20multi-modal%20tasks%2C%20with%20an%0Aefficient%20and%20scalable%20design.%20Its%20framework%20makes%20it%20practical%20for%20real-world%0Adata%20engines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataset%2520Growth%26entry.906535625%3DZiheng%2520Qin%2520and%2520Zhaopan%2520Xu%2520and%2520Yukun%2520Zhou%2520and%2520Zangwei%2520Zheng%2520and%2520Zebang%2520Cheng%2520and%2520Hao%2520Tang%2520and%2520Lei%2520Shang%2520and%2520Baigui%2520Sun%2520and%2520Xiaojiang%2520Peng%2520and%2520Radu%2520Timofte%2520and%2520Hongxun%2520Yao%2520and%2520Kai%2520Wang%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520Deep%2520learning%2520benefits%2520from%2520the%2520growing%2520abundance%2520of%2520available%2520data.%250AMeanwhile%252C%2520efficiently%2520dealing%2520with%2520the%2520growing%2520data%2520scale%2520has%2520become%2520a%250Achallenge.%2520Data%2520publicly%2520available%2520are%2520from%2520different%2520sources%2520with%2520various%250Aqualities%252C%2520and%2520it%2520is%2520impractical%2520to%2520do%2520manual%2520cleaning%2520against%2520noise%2520and%250Aredundancy%2520given%2520today%2527s%2520data%2520scale.%2520There%2520are%2520existing%2520techniques%2520for%250Acleaning/selecting%2520the%2520collected%2520data.%2520However%252C%2520these%2520methods%2520are%2520mainly%250Aproposed%2520for%2520offline%2520settings%2520that%2520target%2520one%2520of%2520the%2520cleanness%2520and%2520redundancy%250Aproblems.%2520In%2520practice%252C%2520data%2520are%2520growing%2520exponentially%2520with%2520both%2520problems.%2520This%250Aleads%2520to%2520repeated%2520data%2520curation%2520with%2520sub-optimal%2520efficiency.%2520To%2520tackle%2520this%250Achallenge%252C%2520we%2520propose%2520InfoGrowth%252C%2520an%2520efficient%2520online%2520algorithm%2520for%2520data%250Acleaning%2520and%2520selection%252C%2520resulting%2520in%2520a%2520growing%2520dataset%2520that%2520keeps%2520up%2520to%2520date%250Awith%2520awareness%2520of%2520cleanliness%2520and%2520diversity.%2520InfoGrowth%2520can%2520improve%2520data%250Aquality/efficiency%2520on%2520both%2520single-modal%2520and%2520multi-modal%2520tasks%252C%2520with%2520an%250Aefficient%2520and%2520scalable%2520design.%2520Its%2520framework%2520makes%2520it%2520practical%2520for%2520real-world%250Adata%2520engines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dataset%20Growth&entry.906535625=Ziheng%20Qin%20and%20Zhaopan%20Xu%20and%20Yukun%20Zhou%20and%20Zangwei%20Zheng%20and%20Zebang%20Cheng%20and%20Hao%20Tang%20and%20Lei%20Shang%20and%20Baigui%20Sun%20and%20Xiaojiang%20Peng%20and%20Radu%20Timofte%20and%20Hongxun%20Yao%20and%20Kai%20Wang%20and%20Yang%20You&entry.1292438233=%20%20Deep%20learning%20benefits%20from%20the%20growing%20abundance%20of%20available%20data.%0AMeanwhile%2C%20efficiently%20dealing%20with%20the%20growing%20data%20scale%20has%20become%20a%0Achallenge.%20Data%20publicly%20available%20are%20from%20different%20sources%20with%20various%0Aqualities%2C%20and%20it%20is%20impractical%20to%20do%20manual%20cleaning%20against%20noise%20and%0Aredundancy%20given%20today%27s%20data%20scale.%20There%20are%20existing%20techniques%20for%0Acleaning/selecting%20the%20collected%20data.%20However%2C%20these%20methods%20are%20mainly%0Aproposed%20for%20offline%20settings%20that%20target%20one%20of%20the%20cleanness%20and%20redundancy%0Aproblems.%20In%20practice%2C%20data%20are%20growing%20exponentially%20with%20both%20problems.%20This%0Aleads%20to%20repeated%20data%20curation%20with%20sub-optimal%20efficiency.%20To%20tackle%20this%0Achallenge%2C%20we%20propose%20InfoGrowth%2C%20an%20efficient%20online%20algorithm%20for%20data%0Acleaning%20and%20selection%2C%20resulting%20in%20a%20growing%20dataset%20that%20keeps%20up%20to%20date%0Awith%20awareness%20of%20cleanliness%20and%20diversity.%20InfoGrowth%20can%20improve%20data%0Aquality/efficiency%20on%20both%20single-modal%20and%20multi-modal%20tasks%2C%20with%20an%0Aefficient%20and%20scalable%20design.%20Its%20framework%20makes%20it%20practical%20for%20real-world%0Adata%20engines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18347v1&entry.124074799=Read"},
{"title": "Self-Supervised Learning Based Handwriting Verification", "author": "Mihir Chauhan and Mohammad Abuzar Shaikh and Bina Ramamurthy and Mingchen Gao and Siwei Lyu and Sargur Srihari", "abstract": "  We present SSL-HV: Self-Supervised Learning approaches applied to the task of\nHandwriting Verification. This task involves determining whether a given pair\nof handwritten images originate from the same or different writer distribution.\nWe have compared the performance of multiple generative, contrastive SSL\napproaches against handcrafted feature extractors and supervised learning on\nCEDAR AND dataset. We show that ResNet based Variational Auto-Encoder (VAE)\noutperforms other generative approaches achieving 76.3% accuracy, while\nResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization\n(VICReg) outperforms other contrastive approaches achieving 78% accuracy. Using\na pre-trained VAE and VICReg for the downstream task of writer verification we\nobserved a relative improvement in accuracy of 6.7% and 9% over ResNet-18\nsupervised baseline with 10% writer labels.\n", "link": "http://arxiv.org/abs/2405.18320v1", "date": "2024-05-28", "relevancy": 2.2706, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4954}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4351}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20Based%20Handwriting%20Verification&body=Title%3A%20Self-Supervised%20Learning%20Based%20Handwriting%20Verification%0AAuthor%3A%20Mihir%20Chauhan%20and%20Mohammad%20Abuzar%20Shaikh%20and%20Bina%20Ramamurthy%20and%20Mingchen%20Gao%20and%20Siwei%20Lyu%20and%20Sargur%20Srihari%0AAbstract%3A%20%20%20We%20present%20SSL-HV%3A%20Self-Supervised%20Learning%20approaches%20applied%20to%20the%20task%20of%0AHandwriting%20Verification.%20This%20task%20involves%20determining%20whether%20a%20given%20pair%0Aof%20handwritten%20images%20originate%20from%20the%20same%20or%20different%20writer%20distribution.%0AWe%20have%20compared%20the%20performance%20of%20multiple%20generative%2C%20contrastive%20SSL%0Aapproaches%20against%20handcrafted%20feature%20extractors%20and%20supervised%20learning%20on%0ACEDAR%20AND%20dataset.%20We%20show%20that%20ResNet%20based%20Variational%20Auto-Encoder%20%28VAE%29%0Aoutperforms%20other%20generative%20approaches%20achieving%2076.3%25%20accuracy%2C%20while%0AResNet-18%20fine-tuned%20using%20Variance-Invariance-Covariance%20Regularization%0A%28VICReg%29%20outperforms%20other%20contrastive%20approaches%20achieving%2078%25%20accuracy.%20Using%0Aa%20pre-trained%20VAE%20and%20VICReg%20for%20the%20downstream%20task%20of%20writer%20verification%20we%0Aobserved%20a%20relative%20improvement%20in%20accuracy%20of%206.7%25%20and%209%25%20over%20ResNet-18%0Asupervised%20baseline%20with%2010%25%20writer%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520Based%2520Handwriting%2520Verification%26entry.906535625%3DMihir%2520Chauhan%2520and%2520Mohammad%2520Abuzar%2520Shaikh%2520and%2520Bina%2520Ramamurthy%2520and%2520Mingchen%2520Gao%2520and%2520Siwei%2520Lyu%2520and%2520Sargur%2520Srihari%26entry.1292438233%3D%2520%2520We%2520present%2520SSL-HV%253A%2520Self-Supervised%2520Learning%2520approaches%2520applied%2520to%2520the%2520task%2520of%250AHandwriting%2520Verification.%2520This%2520task%2520involves%2520determining%2520whether%2520a%2520given%2520pair%250Aof%2520handwritten%2520images%2520originate%2520from%2520the%2520same%2520or%2520different%2520writer%2520distribution.%250AWe%2520have%2520compared%2520the%2520performance%2520of%2520multiple%2520generative%252C%2520contrastive%2520SSL%250Aapproaches%2520against%2520handcrafted%2520feature%2520extractors%2520and%2520supervised%2520learning%2520on%250ACEDAR%2520AND%2520dataset.%2520We%2520show%2520that%2520ResNet%2520based%2520Variational%2520Auto-Encoder%2520%2528VAE%2529%250Aoutperforms%2520other%2520generative%2520approaches%2520achieving%252076.3%2525%2520accuracy%252C%2520while%250AResNet-18%2520fine-tuned%2520using%2520Variance-Invariance-Covariance%2520Regularization%250A%2528VICReg%2529%2520outperforms%2520other%2520contrastive%2520approaches%2520achieving%252078%2525%2520accuracy.%2520Using%250Aa%2520pre-trained%2520VAE%2520and%2520VICReg%2520for%2520the%2520downstream%2520task%2520of%2520writer%2520verification%2520we%250Aobserved%2520a%2520relative%2520improvement%2520in%2520accuracy%2520of%25206.7%2525%2520and%25209%2525%2520over%2520ResNet-18%250Asupervised%2520baseline%2520with%252010%2525%2520writer%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20Based%20Handwriting%20Verification&entry.906535625=Mihir%20Chauhan%20and%20Mohammad%20Abuzar%20Shaikh%20and%20Bina%20Ramamurthy%20and%20Mingchen%20Gao%20and%20Siwei%20Lyu%20and%20Sargur%20Srihari&entry.1292438233=%20%20We%20present%20SSL-HV%3A%20Self-Supervised%20Learning%20approaches%20applied%20to%20the%20task%20of%0AHandwriting%20Verification.%20This%20task%20involves%20determining%20whether%20a%20given%20pair%0Aof%20handwritten%20images%20originate%20from%20the%20same%20or%20different%20writer%20distribution.%0AWe%20have%20compared%20the%20performance%20of%20multiple%20generative%2C%20contrastive%20SSL%0Aapproaches%20against%20handcrafted%20feature%20extractors%20and%20supervised%20learning%20on%0ACEDAR%20AND%20dataset.%20We%20show%20that%20ResNet%20based%20Variational%20Auto-Encoder%20%28VAE%29%0Aoutperforms%20other%20generative%20approaches%20achieving%2076.3%25%20accuracy%2C%20while%0AResNet-18%20fine-tuned%20using%20Variance-Invariance-Covariance%20Regularization%0A%28VICReg%29%20outperforms%20other%20contrastive%20approaches%20achieving%2078%25%20accuracy.%20Using%0Aa%20pre-trained%20VAE%20and%20VICReg%20for%20the%20downstream%20task%20of%20writer%20verification%20we%0Aobserved%20a%20relative%20improvement%20in%20accuracy%20of%206.7%25%20and%209%25%20over%20ResNet-18%0Asupervised%20baseline%20with%2010%25%20writer%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18320v1&entry.124074799=Read"},
{"title": "MULi-Ev: Maintaining Unperturbed LiDAR-Event Calibration", "author": "Mathieu Cocheteux and Julien Moreau and Franck Davoine", "abstract": "  Despite the increasing interest in enhancing perception systems for\nautonomous vehicles, the online calibration between event cameras and LiDAR -\ntwo sensors pivotal in capturing comprehensive environmental information -\nremains unexplored. We introduce MULi-Ev, the first online, deep learning-based\nframework tailored for the extrinsic calibration of event cameras with LiDAR.\nThis advancement is instrumental for the seamless integration of LiDAR and\nevent cameras, enabling dynamic, real-time calibration adjustments that are\nessential for maintaining optimal sensor alignment amidst varying operational\nconditions. Rigorously evaluated against the real-world scenarios presented in\nthe DSEC dataset, MULi-Ev not only achieves substantial improvements in\ncalibration accuracy but also sets a new standard for integrating LiDAR with\nevent cameras in mobile platforms. Our findings reveal the potential of MULi-Ev\nto bolster the safety, reliability, and overall performance of event-based\nperception systems in autonomous driving, marking a significant step forward in\ntheir real-world deployment and effectiveness.\n", "link": "http://arxiv.org/abs/2405.18021v1", "date": "2024-05-28", "relevancy": 2.2641, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5949}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5785}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MULi-Ev%3A%20Maintaining%20Unperturbed%20LiDAR-Event%20Calibration&body=Title%3A%20MULi-Ev%3A%20Maintaining%20Unperturbed%20LiDAR-Event%20Calibration%0AAuthor%3A%20Mathieu%20Cocheteux%20and%20Julien%20Moreau%20and%20Franck%20Davoine%0AAbstract%3A%20%20%20Despite%20the%20increasing%20interest%20in%20enhancing%20perception%20systems%20for%0Aautonomous%20vehicles%2C%20the%20online%20calibration%20between%20event%20cameras%20and%20LiDAR%20-%0Atwo%20sensors%20pivotal%20in%20capturing%20comprehensive%20environmental%20information%20-%0Aremains%20unexplored.%20We%20introduce%20MULi-Ev%2C%20the%20first%20online%2C%20deep%20learning-based%0Aframework%20tailored%20for%20the%20extrinsic%20calibration%20of%20event%20cameras%20with%20LiDAR.%0AThis%20advancement%20is%20instrumental%20for%20the%20seamless%20integration%20of%20LiDAR%20and%0Aevent%20cameras%2C%20enabling%20dynamic%2C%20real-time%20calibration%20adjustments%20that%20are%0Aessential%20for%20maintaining%20optimal%20sensor%20alignment%20amidst%20varying%20operational%0Aconditions.%20Rigorously%20evaluated%20against%20the%20real-world%20scenarios%20presented%20in%0Athe%20DSEC%20dataset%2C%20MULi-Ev%20not%20only%20achieves%20substantial%20improvements%20in%0Acalibration%20accuracy%20but%20also%20sets%20a%20new%20standard%20for%20integrating%20LiDAR%20with%0Aevent%20cameras%20in%20mobile%20platforms.%20Our%20findings%20reveal%20the%20potential%20of%20MULi-Ev%0Ato%20bolster%20the%20safety%2C%20reliability%2C%20and%20overall%20performance%20of%20event-based%0Aperception%20systems%20in%20autonomous%20driving%2C%20marking%20a%20significant%20step%20forward%20in%0Atheir%20real-world%20deployment%20and%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMULi-Ev%253A%2520Maintaining%2520Unperturbed%2520LiDAR-Event%2520Calibration%26entry.906535625%3DMathieu%2520Cocheteux%2520and%2520Julien%2520Moreau%2520and%2520Franck%2520Davoine%26entry.1292438233%3D%2520%2520Despite%2520the%2520increasing%2520interest%2520in%2520enhancing%2520perception%2520systems%2520for%250Aautonomous%2520vehicles%252C%2520the%2520online%2520calibration%2520between%2520event%2520cameras%2520and%2520LiDAR%2520-%250Atwo%2520sensors%2520pivotal%2520in%2520capturing%2520comprehensive%2520environmental%2520information%2520-%250Aremains%2520unexplored.%2520We%2520introduce%2520MULi-Ev%252C%2520the%2520first%2520online%252C%2520deep%2520learning-based%250Aframework%2520tailored%2520for%2520the%2520extrinsic%2520calibration%2520of%2520event%2520cameras%2520with%2520LiDAR.%250AThis%2520advancement%2520is%2520instrumental%2520for%2520the%2520seamless%2520integration%2520of%2520LiDAR%2520and%250Aevent%2520cameras%252C%2520enabling%2520dynamic%252C%2520real-time%2520calibration%2520adjustments%2520that%2520are%250Aessential%2520for%2520maintaining%2520optimal%2520sensor%2520alignment%2520amidst%2520varying%2520operational%250Aconditions.%2520Rigorously%2520evaluated%2520against%2520the%2520real-world%2520scenarios%2520presented%2520in%250Athe%2520DSEC%2520dataset%252C%2520MULi-Ev%2520not%2520only%2520achieves%2520substantial%2520improvements%2520in%250Acalibration%2520accuracy%2520but%2520also%2520sets%2520a%2520new%2520standard%2520for%2520integrating%2520LiDAR%2520with%250Aevent%2520cameras%2520in%2520mobile%2520platforms.%2520Our%2520findings%2520reveal%2520the%2520potential%2520of%2520MULi-Ev%250Ato%2520bolster%2520the%2520safety%252C%2520reliability%252C%2520and%2520overall%2520performance%2520of%2520event-based%250Aperception%2520systems%2520in%2520autonomous%2520driving%252C%2520marking%2520a%2520significant%2520step%2520forward%2520in%250Atheir%2520real-world%2520deployment%2520and%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MULi-Ev%3A%20Maintaining%20Unperturbed%20LiDAR-Event%20Calibration&entry.906535625=Mathieu%20Cocheteux%20and%20Julien%20Moreau%20and%20Franck%20Davoine&entry.1292438233=%20%20Despite%20the%20increasing%20interest%20in%20enhancing%20perception%20systems%20for%0Aautonomous%20vehicles%2C%20the%20online%20calibration%20between%20event%20cameras%20and%20LiDAR%20-%0Atwo%20sensors%20pivotal%20in%20capturing%20comprehensive%20environmental%20information%20-%0Aremains%20unexplored.%20We%20introduce%20MULi-Ev%2C%20the%20first%20online%2C%20deep%20learning-based%0Aframework%20tailored%20for%20the%20extrinsic%20calibration%20of%20event%20cameras%20with%20LiDAR.%0AThis%20advancement%20is%20instrumental%20for%20the%20seamless%20integration%20of%20LiDAR%20and%0Aevent%20cameras%2C%20enabling%20dynamic%2C%20real-time%20calibration%20adjustments%20that%20are%0Aessential%20for%20maintaining%20optimal%20sensor%20alignment%20amidst%20varying%20operational%0Aconditions.%20Rigorously%20evaluated%20against%20the%20real-world%20scenarios%20presented%20in%0Athe%20DSEC%20dataset%2C%20MULi-Ev%20not%20only%20achieves%20substantial%20improvements%20in%0Acalibration%20accuracy%20but%20also%20sets%20a%20new%20standard%20for%20integrating%20LiDAR%20with%0Aevent%20cameras%20in%20mobile%20platforms.%20Our%20findings%20reveal%20the%20potential%20of%20MULi-Ev%0Ato%20bolster%20the%20safety%2C%20reliability%2C%20and%20overall%20performance%20of%20event-based%0Aperception%20systems%20in%20autonomous%20driving%2C%20marking%20a%20significant%20step%20forward%20in%0Atheir%20real-world%20deployment%20and%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18021v1&entry.124074799=Read"},
{"title": "NeRO: Neural Road Surface Reconstruction", "author": "Ruibo Wang and Song Zhang and Ping Huang and Donghai Zhang and Haoyu Chen", "abstract": "  Accurately reconstructing road surfaces is pivotal for various applications\nespecially in autonomous driving. This paper introduces a position encoding\nMulti-Layer Perceptrons (MLPs) framework to reconstruct road surfaces, with\ninput as world coordinates x and y, and output as height, color, and semantic\ninformation. The effectiveness of this method is demonstrated through its\ncompatibility with a variety of road height sources like vehicle camera poses,\nLiDAR point clouds, and SFM point clouds, robust to the semantic noise of\nimages like sparse labels and noise semantic prediction, and fast training\nspeed, which indicates a promising application for rendering road surfaces with\nsemantics, particularly in applications demanding visualization of road\nsurface, 4D labeling, and semantic groupings.\n", "link": "http://arxiv.org/abs/2405.10554v2", "date": "2024-05-28", "relevancy": 2.2488, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5776}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5683}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRO%3A%20Neural%20Road%20Surface%20Reconstruction&body=Title%3A%20NeRO%3A%20Neural%20Road%20Surface%20Reconstruction%0AAuthor%3A%20Ruibo%20Wang%20and%20Song%20Zhang%20and%20Ping%20Huang%20and%20Donghai%20Zhang%20and%20Haoyu%20Chen%0AAbstract%3A%20%20%20Accurately%20reconstructing%20road%20surfaces%20is%20pivotal%20for%20various%20applications%0Aespecially%20in%20autonomous%20driving.%20This%20paper%20introduces%20a%20position%20encoding%0AMulti-Layer%20Perceptrons%20%28MLPs%29%20framework%20to%20reconstruct%20road%20surfaces%2C%20with%0Ainput%20as%20world%20coordinates%20x%20and%20y%2C%20and%20output%20as%20height%2C%20color%2C%20and%20semantic%0Ainformation.%20The%20effectiveness%20of%20this%20method%20is%20demonstrated%20through%20its%0Acompatibility%20with%20a%20variety%20of%20road%20height%20sources%20like%20vehicle%20camera%20poses%2C%0ALiDAR%20point%20clouds%2C%20and%20SFM%20point%20clouds%2C%20robust%20to%20the%20semantic%20noise%20of%0Aimages%20like%20sparse%20labels%20and%20noise%20semantic%20prediction%2C%20and%20fast%20training%0Aspeed%2C%20which%20indicates%20a%20promising%20application%20for%20rendering%20road%20surfaces%20with%0Asemantics%2C%20particularly%20in%20applications%20demanding%20visualization%20of%20road%0Asurface%2C%204D%20labeling%2C%20and%20semantic%20groupings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10554v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRO%253A%2520Neural%2520Road%2520Surface%2520Reconstruction%26entry.906535625%3DRuibo%2520Wang%2520and%2520Song%2520Zhang%2520and%2520Ping%2520Huang%2520and%2520Donghai%2520Zhang%2520and%2520Haoyu%2520Chen%26entry.1292438233%3D%2520%2520Accurately%2520reconstructing%2520road%2520surfaces%2520is%2520pivotal%2520for%2520various%2520applications%250Aespecially%2520in%2520autonomous%2520driving.%2520This%2520paper%2520introduces%2520a%2520position%2520encoding%250AMulti-Layer%2520Perceptrons%2520%2528MLPs%2529%2520framework%2520to%2520reconstruct%2520road%2520surfaces%252C%2520with%250Ainput%2520as%2520world%2520coordinates%2520x%2520and%2520y%252C%2520and%2520output%2520as%2520height%252C%2520color%252C%2520and%2520semantic%250Ainformation.%2520The%2520effectiveness%2520of%2520this%2520method%2520is%2520demonstrated%2520through%2520its%250Acompatibility%2520with%2520a%2520variety%2520of%2520road%2520height%2520sources%2520like%2520vehicle%2520camera%2520poses%252C%250ALiDAR%2520point%2520clouds%252C%2520and%2520SFM%2520point%2520clouds%252C%2520robust%2520to%2520the%2520semantic%2520noise%2520of%250Aimages%2520like%2520sparse%2520labels%2520and%2520noise%2520semantic%2520prediction%252C%2520and%2520fast%2520training%250Aspeed%252C%2520which%2520indicates%2520a%2520promising%2520application%2520for%2520rendering%2520road%2520surfaces%2520with%250Asemantics%252C%2520particularly%2520in%2520applications%2520demanding%2520visualization%2520of%2520road%250Asurface%252C%25204D%2520labeling%252C%2520and%2520semantic%2520groupings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10554v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRO%3A%20Neural%20Road%20Surface%20Reconstruction&entry.906535625=Ruibo%20Wang%20and%20Song%20Zhang%20and%20Ping%20Huang%20and%20Donghai%20Zhang%20and%20Haoyu%20Chen&entry.1292438233=%20%20Accurately%20reconstructing%20road%20surfaces%20is%20pivotal%20for%20various%20applications%0Aespecially%20in%20autonomous%20driving.%20This%20paper%20introduces%20a%20position%20encoding%0AMulti-Layer%20Perceptrons%20%28MLPs%29%20framework%20to%20reconstruct%20road%20surfaces%2C%20with%0Ainput%20as%20world%20coordinates%20x%20and%20y%2C%20and%20output%20as%20height%2C%20color%2C%20and%20semantic%0Ainformation.%20The%20effectiveness%20of%20this%20method%20is%20demonstrated%20through%20its%0Acompatibility%20with%20a%20variety%20of%20road%20height%20sources%20like%20vehicle%20camera%20poses%2C%0ALiDAR%20point%20clouds%2C%20and%20SFM%20point%20clouds%2C%20robust%20to%20the%20semantic%20noise%20of%0Aimages%20like%20sparse%20labels%20and%20noise%20semantic%20prediction%2C%20and%20fast%20training%0Aspeed%2C%20which%20indicates%20a%20promising%20application%20for%20rendering%20road%20surfaces%20with%0Asemantics%2C%20particularly%20in%20applications%20demanding%20visualization%20of%20road%0Asurface%2C%204D%20labeling%2C%20and%20semantic%20groupings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10554v2&entry.124074799=Read"},
{"title": "ViG: Linear-complexity Visual Sequence Learning with Gated Linear\n  Attention", "author": "Bencheng Liao and Xinggang Wang and Lianghui Zhu and Qian Zhang and Chang Huang", "abstract": "  Recently, linear complexity sequence modeling networks have achieved modeling\ncapabilities similar to Vision Transformers on a variety of computer vision\ntasks, while using fewer FLOPs and less memory. However, their advantage in\nterms of actual runtime speed is not significant. To address this issue, we\nintroduce Gated Linear Attention (GLA) for vision, leveraging its superior\nhardware-awareness and efficiency. We propose direction-wise gating to capture\n1D global context through bidirectional modeling and a 2D gating locality\ninjection to adaptively inject 2D local details into 1D global context. Our\nhardware-aware implementation further merges forward and backward scanning into\na single kernel, enhancing parallelism and reducing memory cost and latency.\nThe proposed model, \\name{}, offers a favorable trade-off in accuracy,\nparameters, and FLOPs on ImageNet and downstream tasks, outperforming popular\nTransformer and CNN-based models. Notably, \\name{}-S matches DeiT-B's accuracy\nwhile using only 27\\% of the parameters and 20\\% of the FLOPs, running\n2$\\times$ faster on $224\\times224$ images. At $1024\\times1024$ resolution,\n\\name{}-T uses 5.2$\\times$ fewer FLOPs, saves 90\\% GPU memory, runs 4.8$\\times$\nfaster, and achieves 20.7\\% higher top-1 accuracy than DeiT-T. These results\nposition \\name{} as an efficient and scalable solution for visual\nrepresentation learning. Code is available at\n\\url{https://github.com/hustvl/ViG}.\n", "link": "http://arxiv.org/abs/2405.18425v1", "date": "2024-05-28", "relevancy": 2.2435, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5658}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5657}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViG%3A%20Linear-complexity%20Visual%20Sequence%20Learning%20with%20Gated%20Linear%0A%20%20Attention&body=Title%3A%20ViG%3A%20Linear-complexity%20Visual%20Sequence%20Learning%20with%20Gated%20Linear%0A%20%20Attention%0AAuthor%3A%20Bencheng%20Liao%20and%20Xinggang%20Wang%20and%20Lianghui%20Zhu%20and%20Qian%20Zhang%20and%20Chang%20Huang%0AAbstract%3A%20%20%20Recently%2C%20linear%20complexity%20sequence%20modeling%20networks%20have%20achieved%20modeling%0Acapabilities%20similar%20to%20Vision%20Transformers%20on%20a%20variety%20of%20computer%20vision%0Atasks%2C%20while%20using%20fewer%20FLOPs%20and%20less%20memory.%20However%2C%20their%20advantage%20in%0Aterms%20of%20actual%20runtime%20speed%20is%20not%20significant.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20Gated%20Linear%20Attention%20%28GLA%29%20for%20vision%2C%20leveraging%20its%20superior%0Ahardware-awareness%20and%20efficiency.%20We%20propose%20direction-wise%20gating%20to%20capture%0A1D%20global%20context%20through%20bidirectional%20modeling%20and%20a%202D%20gating%20locality%0Ainjection%20to%20adaptively%20inject%202D%20local%20details%20into%201D%20global%20context.%20Our%0Ahardware-aware%20implementation%20further%20merges%20forward%20and%20backward%20scanning%20into%0Aa%20single%20kernel%2C%20enhancing%20parallelism%20and%20reducing%20memory%20cost%20and%20latency.%0AThe%20proposed%20model%2C%20%5Cname%7B%7D%2C%20offers%20a%20favorable%20trade-off%20in%20accuracy%2C%0Aparameters%2C%20and%20FLOPs%20on%20ImageNet%20and%20downstream%20tasks%2C%20outperforming%20popular%0ATransformer%20and%20CNN-based%20models.%20Notably%2C%20%5Cname%7B%7D-S%20matches%20DeiT-B%27s%20accuracy%0Awhile%20using%20only%2027%5C%25%20of%20the%20parameters%20and%2020%5C%25%20of%20the%20FLOPs%2C%20running%0A2%24%5Ctimes%24%20faster%20on%20%24224%5Ctimes224%24%20images.%20At%20%241024%5Ctimes1024%24%20resolution%2C%0A%5Cname%7B%7D-T%20uses%205.2%24%5Ctimes%24%20fewer%20FLOPs%2C%20saves%2090%5C%25%20GPU%20memory%2C%20runs%204.8%24%5Ctimes%24%0Afaster%2C%20and%20achieves%2020.7%5C%25%20higher%20top-1%20accuracy%20than%20DeiT-T.%20These%20results%0Aposition%20%5Cname%7B%7D%20as%20an%20efficient%20and%20scalable%20solution%20for%20visual%0Arepresentation%20learning.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/hustvl/ViG%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViG%253A%2520Linear-complexity%2520Visual%2520Sequence%2520Learning%2520with%2520Gated%2520Linear%250A%2520%2520Attention%26entry.906535625%3DBencheng%2520Liao%2520and%2520Xinggang%2520Wang%2520and%2520Lianghui%2520Zhu%2520and%2520Qian%2520Zhang%2520and%2520Chang%2520Huang%26entry.1292438233%3D%2520%2520Recently%252C%2520linear%2520complexity%2520sequence%2520modeling%2520networks%2520have%2520achieved%2520modeling%250Acapabilities%2520similar%2520to%2520Vision%2520Transformers%2520on%2520a%2520variety%2520of%2520computer%2520vision%250Atasks%252C%2520while%2520using%2520fewer%2520FLOPs%2520and%2520less%2520memory.%2520However%252C%2520their%2520advantage%2520in%250Aterms%2520of%2520actual%2520runtime%2520speed%2520is%2520not%2520significant.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520Gated%2520Linear%2520Attention%2520%2528GLA%2529%2520for%2520vision%252C%2520leveraging%2520its%2520superior%250Ahardware-awareness%2520and%2520efficiency.%2520We%2520propose%2520direction-wise%2520gating%2520to%2520capture%250A1D%2520global%2520context%2520through%2520bidirectional%2520modeling%2520and%2520a%25202D%2520gating%2520locality%250Ainjection%2520to%2520adaptively%2520inject%25202D%2520local%2520details%2520into%25201D%2520global%2520context.%2520Our%250Ahardware-aware%2520implementation%2520further%2520merges%2520forward%2520and%2520backward%2520scanning%2520into%250Aa%2520single%2520kernel%252C%2520enhancing%2520parallelism%2520and%2520reducing%2520memory%2520cost%2520and%2520latency.%250AThe%2520proposed%2520model%252C%2520%255Cname%257B%257D%252C%2520offers%2520a%2520favorable%2520trade-off%2520in%2520accuracy%252C%250Aparameters%252C%2520and%2520FLOPs%2520on%2520ImageNet%2520and%2520downstream%2520tasks%252C%2520outperforming%2520popular%250ATransformer%2520and%2520CNN-based%2520models.%2520Notably%252C%2520%255Cname%257B%257D-S%2520matches%2520DeiT-B%2527s%2520accuracy%250Awhile%2520using%2520only%252027%255C%2525%2520of%2520the%2520parameters%2520and%252020%255C%2525%2520of%2520the%2520FLOPs%252C%2520running%250A2%2524%255Ctimes%2524%2520faster%2520on%2520%2524224%255Ctimes224%2524%2520images.%2520At%2520%25241024%255Ctimes1024%2524%2520resolution%252C%250A%255Cname%257B%257D-T%2520uses%25205.2%2524%255Ctimes%2524%2520fewer%2520FLOPs%252C%2520saves%252090%255C%2525%2520GPU%2520memory%252C%2520runs%25204.8%2524%255Ctimes%2524%250Afaster%252C%2520and%2520achieves%252020.7%255C%2525%2520higher%2520top-1%2520accuracy%2520than%2520DeiT-T.%2520These%2520results%250Aposition%2520%255Cname%257B%257D%2520as%2520an%2520efficient%2520and%2520scalable%2520solution%2520for%2520visual%250Arepresentation%2520learning.%2520Code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/hustvl/ViG%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViG%3A%20Linear-complexity%20Visual%20Sequence%20Learning%20with%20Gated%20Linear%0A%20%20Attention&entry.906535625=Bencheng%20Liao%20and%20Xinggang%20Wang%20and%20Lianghui%20Zhu%20and%20Qian%20Zhang%20and%20Chang%20Huang&entry.1292438233=%20%20Recently%2C%20linear%20complexity%20sequence%20modeling%20networks%20have%20achieved%20modeling%0Acapabilities%20similar%20to%20Vision%20Transformers%20on%20a%20variety%20of%20computer%20vision%0Atasks%2C%20while%20using%20fewer%20FLOPs%20and%20less%20memory.%20However%2C%20their%20advantage%20in%0Aterms%20of%20actual%20runtime%20speed%20is%20not%20significant.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20Gated%20Linear%20Attention%20%28GLA%29%20for%20vision%2C%20leveraging%20its%20superior%0Ahardware-awareness%20and%20efficiency.%20We%20propose%20direction-wise%20gating%20to%20capture%0A1D%20global%20context%20through%20bidirectional%20modeling%20and%20a%202D%20gating%20locality%0Ainjection%20to%20adaptively%20inject%202D%20local%20details%20into%201D%20global%20context.%20Our%0Ahardware-aware%20implementation%20further%20merges%20forward%20and%20backward%20scanning%20into%0Aa%20single%20kernel%2C%20enhancing%20parallelism%20and%20reducing%20memory%20cost%20and%20latency.%0AThe%20proposed%20model%2C%20%5Cname%7B%7D%2C%20offers%20a%20favorable%20trade-off%20in%20accuracy%2C%0Aparameters%2C%20and%20FLOPs%20on%20ImageNet%20and%20downstream%20tasks%2C%20outperforming%20popular%0ATransformer%20and%20CNN-based%20models.%20Notably%2C%20%5Cname%7B%7D-S%20matches%20DeiT-B%27s%20accuracy%0Awhile%20using%20only%2027%5C%25%20of%20the%20parameters%20and%2020%5C%25%20of%20the%20FLOPs%2C%20running%0A2%24%5Ctimes%24%20faster%20on%20%24224%5Ctimes224%24%20images.%20At%20%241024%5Ctimes1024%24%20resolution%2C%0A%5Cname%7B%7D-T%20uses%205.2%24%5Ctimes%24%20fewer%20FLOPs%2C%20saves%2090%5C%25%20GPU%20memory%2C%20runs%204.8%24%5Ctimes%24%0Afaster%2C%20and%20achieves%2020.7%5C%25%20higher%20top-1%20accuracy%20than%20DeiT-T.%20These%20results%0Aposition%20%5Cname%7B%7D%20as%20an%20efficient%20and%20scalable%20solution%20for%20visual%0Arepresentation%20learning.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/hustvl/ViG%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18425v1&entry.124074799=Read"},
{"title": "CARL: A Framework for Equivariant Image Registration", "author": "Hastings Greer and Lin Tian and Francois-Xavier Vialard and Roland Kwitt and Raul San Jose Estepar and Marc Niethammer", "abstract": "  Image registration estimates spatial correspondences between a pair of\nimages. These estimates are typically obtained via numerical optimization or\nregression by a deep network. A desirable property of such estimators is that a\ncorrespondence estimate (e.g., the true oracle correspondence) for an image\npair is maintained under deformations of the input images. Formally, the\nestimator should be equivariant to a desired class of image transformations. In\nthis work, we present careful analyses of the desired equivariance properties\nin the context of multi-step deep registration networks. Based on these\nanalyses we 1) introduce the notions of $[U,U]$ equivariance (network\nequivariance to the same deformations of the input images) and $[W,U]$\nequivariance (where input images can undergo different deformations); we 2)\nshow that in a suitable multi-step registration setup it is sufficient for\noverall $[W,U]$ equivariance if the first step has $[W,U]$ equivariance and all\nothers have $[U,U]$ equivariance; we 3) show that common\ndisplacement-predicting networks only exhibit $[U,U]$ equivariance to\ntranslations instead of the more powerful $[W,U]$ equivariance; and we 4) show\nhow to achieve multi-step $[W,U]$ equivariance via a coordinate-attention\nmechanism combined with displacement-predicting refinement layers (CARL).\nOverall, our approach obtains excellent practical registration performance on\nseveral 3D medical image registration tasks and outperforms existing\nunsupervised approaches for the challenging problem of abdomen registration.\n", "link": "http://arxiv.org/abs/2405.16738v2", "date": "2024-05-28", "relevancy": 2.2163, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5632}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5513}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARL%3A%20A%20Framework%20for%20Equivariant%20Image%20Registration&body=Title%3A%20CARL%3A%20A%20Framework%20for%20Equivariant%20Image%20Registration%0AAuthor%3A%20Hastings%20Greer%20and%20Lin%20Tian%20and%20Francois-Xavier%20Vialard%20and%20Roland%20Kwitt%20and%20Raul%20San%20Jose%20Estepar%20and%20Marc%20Niethammer%0AAbstract%3A%20%20%20Image%20registration%20estimates%20spatial%20correspondences%20between%20a%20pair%20of%0Aimages.%20These%20estimates%20are%20typically%20obtained%20via%20numerical%20optimization%20or%0Aregression%20by%20a%20deep%20network.%20A%20desirable%20property%20of%20such%20estimators%20is%20that%20a%0Acorrespondence%20estimate%20%28e.g.%2C%20the%20true%20oracle%20correspondence%29%20for%20an%20image%0Apair%20is%20maintained%20under%20deformations%20of%20the%20input%20images.%20Formally%2C%20the%0Aestimator%20should%20be%20equivariant%20to%20a%20desired%20class%20of%20image%20transformations.%20In%0Athis%20work%2C%20we%20present%20careful%20analyses%20of%20the%20desired%20equivariance%20properties%0Ain%20the%20context%20of%20multi-step%20deep%20registration%20networks.%20Based%20on%20these%0Aanalyses%20we%201%29%20introduce%20the%20notions%20of%20%24%5BU%2CU%5D%24%20equivariance%20%28network%0Aequivariance%20to%20the%20same%20deformations%20of%20the%20input%20images%29%20and%20%24%5BW%2CU%5D%24%0Aequivariance%20%28where%20input%20images%20can%20undergo%20different%20deformations%29%3B%20we%202%29%0Ashow%20that%20in%20a%20suitable%20multi-step%20registration%20setup%20it%20is%20sufficient%20for%0Aoverall%20%24%5BW%2CU%5D%24%20equivariance%20if%20the%20first%20step%20has%20%24%5BW%2CU%5D%24%20equivariance%20and%20all%0Aothers%20have%20%24%5BU%2CU%5D%24%20equivariance%3B%20we%203%29%20show%20that%20common%0Adisplacement-predicting%20networks%20only%20exhibit%20%24%5BU%2CU%5D%24%20equivariance%20to%0Atranslations%20instead%20of%20the%20more%20powerful%20%24%5BW%2CU%5D%24%20equivariance%3B%20and%20we%204%29%20show%0Ahow%20to%20achieve%20multi-step%20%24%5BW%2CU%5D%24%20equivariance%20via%20a%20coordinate-attention%0Amechanism%20combined%20with%20displacement-predicting%20refinement%20layers%20%28CARL%29.%0AOverall%2C%20our%20approach%20obtains%20excellent%20practical%20registration%20performance%20on%0Aseveral%203D%20medical%20image%20registration%20tasks%20and%20outperforms%20existing%0Aunsupervised%20approaches%20for%20the%20challenging%20problem%20of%20abdomen%20registration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16738v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARL%253A%2520A%2520Framework%2520for%2520Equivariant%2520Image%2520Registration%26entry.906535625%3DHastings%2520Greer%2520and%2520Lin%2520Tian%2520and%2520Francois-Xavier%2520Vialard%2520and%2520Roland%2520Kwitt%2520and%2520Raul%2520San%2520Jose%2520Estepar%2520and%2520Marc%2520Niethammer%26entry.1292438233%3D%2520%2520Image%2520registration%2520estimates%2520spatial%2520correspondences%2520between%2520a%2520pair%2520of%250Aimages.%2520These%2520estimates%2520are%2520typically%2520obtained%2520via%2520numerical%2520optimization%2520or%250Aregression%2520by%2520a%2520deep%2520network.%2520A%2520desirable%2520property%2520of%2520such%2520estimators%2520is%2520that%2520a%250Acorrespondence%2520estimate%2520%2528e.g.%252C%2520the%2520true%2520oracle%2520correspondence%2529%2520for%2520an%2520image%250Apair%2520is%2520maintained%2520under%2520deformations%2520of%2520the%2520input%2520images.%2520Formally%252C%2520the%250Aestimator%2520should%2520be%2520equivariant%2520to%2520a%2520desired%2520class%2520of%2520image%2520transformations.%2520In%250Athis%2520work%252C%2520we%2520present%2520careful%2520analyses%2520of%2520the%2520desired%2520equivariance%2520properties%250Ain%2520the%2520context%2520of%2520multi-step%2520deep%2520registration%2520networks.%2520Based%2520on%2520these%250Aanalyses%2520we%25201%2529%2520introduce%2520the%2520notions%2520of%2520%2524%255BU%252CU%255D%2524%2520equivariance%2520%2528network%250Aequivariance%2520to%2520the%2520same%2520deformations%2520of%2520the%2520input%2520images%2529%2520and%2520%2524%255BW%252CU%255D%2524%250Aequivariance%2520%2528where%2520input%2520images%2520can%2520undergo%2520different%2520deformations%2529%253B%2520we%25202%2529%250Ashow%2520that%2520in%2520a%2520suitable%2520multi-step%2520registration%2520setup%2520it%2520is%2520sufficient%2520for%250Aoverall%2520%2524%255BW%252CU%255D%2524%2520equivariance%2520if%2520the%2520first%2520step%2520has%2520%2524%255BW%252CU%255D%2524%2520equivariance%2520and%2520all%250Aothers%2520have%2520%2524%255BU%252CU%255D%2524%2520equivariance%253B%2520we%25203%2529%2520show%2520that%2520common%250Adisplacement-predicting%2520networks%2520only%2520exhibit%2520%2524%255BU%252CU%255D%2524%2520equivariance%2520to%250Atranslations%2520instead%2520of%2520the%2520more%2520powerful%2520%2524%255BW%252CU%255D%2524%2520equivariance%253B%2520and%2520we%25204%2529%2520show%250Ahow%2520to%2520achieve%2520multi-step%2520%2524%255BW%252CU%255D%2524%2520equivariance%2520via%2520a%2520coordinate-attention%250Amechanism%2520combined%2520with%2520displacement-predicting%2520refinement%2520layers%2520%2528CARL%2529.%250AOverall%252C%2520our%2520approach%2520obtains%2520excellent%2520practical%2520registration%2520performance%2520on%250Aseveral%25203D%2520medical%2520image%2520registration%2520tasks%2520and%2520outperforms%2520existing%250Aunsupervised%2520approaches%2520for%2520the%2520challenging%2520problem%2520of%2520abdomen%2520registration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16738v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARL%3A%20A%20Framework%20for%20Equivariant%20Image%20Registration&entry.906535625=Hastings%20Greer%20and%20Lin%20Tian%20and%20Francois-Xavier%20Vialard%20and%20Roland%20Kwitt%20and%20Raul%20San%20Jose%20Estepar%20and%20Marc%20Niethammer&entry.1292438233=%20%20Image%20registration%20estimates%20spatial%20correspondences%20between%20a%20pair%20of%0Aimages.%20These%20estimates%20are%20typically%20obtained%20via%20numerical%20optimization%20or%0Aregression%20by%20a%20deep%20network.%20A%20desirable%20property%20of%20such%20estimators%20is%20that%20a%0Acorrespondence%20estimate%20%28e.g.%2C%20the%20true%20oracle%20correspondence%29%20for%20an%20image%0Apair%20is%20maintained%20under%20deformations%20of%20the%20input%20images.%20Formally%2C%20the%0Aestimator%20should%20be%20equivariant%20to%20a%20desired%20class%20of%20image%20transformations.%20In%0Athis%20work%2C%20we%20present%20careful%20analyses%20of%20the%20desired%20equivariance%20properties%0Ain%20the%20context%20of%20multi-step%20deep%20registration%20networks.%20Based%20on%20these%0Aanalyses%20we%201%29%20introduce%20the%20notions%20of%20%24%5BU%2CU%5D%24%20equivariance%20%28network%0Aequivariance%20to%20the%20same%20deformations%20of%20the%20input%20images%29%20and%20%24%5BW%2CU%5D%24%0Aequivariance%20%28where%20input%20images%20can%20undergo%20different%20deformations%29%3B%20we%202%29%0Ashow%20that%20in%20a%20suitable%20multi-step%20registration%20setup%20it%20is%20sufficient%20for%0Aoverall%20%24%5BW%2CU%5D%24%20equivariance%20if%20the%20first%20step%20has%20%24%5BW%2CU%5D%24%20equivariance%20and%20all%0Aothers%20have%20%24%5BU%2CU%5D%24%20equivariance%3B%20we%203%29%20show%20that%20common%0Adisplacement-predicting%20networks%20only%20exhibit%20%24%5BU%2CU%5D%24%20equivariance%20to%0Atranslations%20instead%20of%20the%20more%20powerful%20%24%5BW%2CU%5D%24%20equivariance%3B%20and%20we%204%29%20show%0Ahow%20to%20achieve%20multi-step%20%24%5BW%2CU%5D%24%20equivariance%20via%20a%20coordinate-attention%0Amechanism%20combined%20with%20displacement-predicting%20refinement%20layers%20%28CARL%29.%0AOverall%2C%20our%20approach%20obtains%20excellent%20practical%20registration%20performance%20on%0Aseveral%203D%20medical%20image%20registration%20tasks%20and%20outperforms%20existing%0Aunsupervised%20approaches%20for%20the%20challenging%20problem%20of%20abdomen%20registration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16738v2&entry.124074799=Read"},
{"title": "Stochastic Localization via Iterative Posterior Sampling", "author": "Louis Grenioux and Maxence Noble and Marylou Gabri\u00e9 and Alain Oliviero Durmus", "abstract": "  Building upon score-based learning, new interest in stochastic localization\ntechniques has recently emerged. In these models, one seeks to noise a sample\nfrom the data distribution through a stochastic process, called observation\nprocess, and progressively learns a denoiser associated to this dynamics. Apart\nfrom specific applications, the use of stochastic localization for the problem\nof sampling from an unnormalized target density has not been explored\nextensively. This work contributes to fill this gap. We consider a general\nstochastic localization framework and introduce an explicit class of\nobservation processes, associated with flexible denoising schedules. We provide\na complete methodology, $\\textit{Stochastic Localization via Iterative\nPosterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics,\nand as a by-product, samples from the target distribution. Our scheme is based\non a Markov chain Monte Carlo estimation of the denoiser and comes with\ndetailed practical guidelines. We illustrate the benefits and applicability of\nSLIPS on several benchmarks of multi-modal distributions, including Gaussian\nmixtures in increasing dimensions, Bayesian logistic regression and a\nhigh-dimensional field system from statistical-mechanics.\n", "link": "http://arxiv.org/abs/2402.10758v2", "date": "2024-05-28", "relevancy": 2.198, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5711}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5432}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Localization%20via%20Iterative%20Posterior%20Sampling&body=Title%3A%20Stochastic%20Localization%20via%20Iterative%20Posterior%20Sampling%0AAuthor%3A%20Louis%20Grenioux%20and%20Maxence%20Noble%20and%20Marylou%20Gabri%C3%A9%20and%20Alain%20Oliviero%20Durmus%0AAbstract%3A%20%20%20Building%20upon%20score-based%20learning%2C%20new%20interest%20in%20stochastic%20localization%0Atechniques%20has%20recently%20emerged.%20In%20these%20models%2C%20one%20seeks%20to%20noise%20a%20sample%0Afrom%20the%20data%20distribution%20through%20a%20stochastic%20process%2C%20called%20observation%0Aprocess%2C%20and%20progressively%20learns%20a%20denoiser%20associated%20to%20this%20dynamics.%20Apart%0Afrom%20specific%20applications%2C%20the%20use%20of%20stochastic%20localization%20for%20the%20problem%0Aof%20sampling%20from%20an%20unnormalized%20target%20density%20has%20not%20been%20explored%0Aextensively.%20This%20work%20contributes%20to%20fill%20this%20gap.%20We%20consider%20a%20general%0Astochastic%20localization%20framework%20and%20introduce%20an%20explicit%20class%20of%0Aobservation%20processes%2C%20associated%20with%20flexible%20denoising%20schedules.%20We%20provide%0Aa%20complete%20methodology%2C%20%24%5Ctextit%7BStochastic%20Localization%20via%20Iterative%0APosterior%20Sampling%7D%24%20%28SLIPS%29%2C%20to%20obtain%20approximate%20samples%20of%20this%20dynamics%2C%0Aand%20as%20a%20by-product%2C%20samples%20from%20the%20target%20distribution.%20Our%20scheme%20is%20based%0Aon%20a%20Markov%20chain%20Monte%20Carlo%20estimation%20of%20the%20denoiser%20and%20comes%20with%0Adetailed%20practical%20guidelines.%20We%20illustrate%20the%20benefits%20and%20applicability%20of%0ASLIPS%20on%20several%20benchmarks%20of%20multi-modal%20distributions%2C%20including%20Gaussian%0Amixtures%20in%20increasing%20dimensions%2C%20Bayesian%20logistic%20regression%20and%20a%0Ahigh-dimensional%20field%20system%20from%20statistical-mechanics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10758v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Localization%2520via%2520Iterative%2520Posterior%2520Sampling%26entry.906535625%3DLouis%2520Grenioux%2520and%2520Maxence%2520Noble%2520and%2520Marylou%2520Gabri%25C3%25A9%2520and%2520Alain%2520Oliviero%2520Durmus%26entry.1292438233%3D%2520%2520Building%2520upon%2520score-based%2520learning%252C%2520new%2520interest%2520in%2520stochastic%2520localization%250Atechniques%2520has%2520recently%2520emerged.%2520In%2520these%2520models%252C%2520one%2520seeks%2520to%2520noise%2520a%2520sample%250Afrom%2520the%2520data%2520distribution%2520through%2520a%2520stochastic%2520process%252C%2520called%2520observation%250Aprocess%252C%2520and%2520progressively%2520learns%2520a%2520denoiser%2520associated%2520to%2520this%2520dynamics.%2520Apart%250Afrom%2520specific%2520applications%252C%2520the%2520use%2520of%2520stochastic%2520localization%2520for%2520the%2520problem%250Aof%2520sampling%2520from%2520an%2520unnormalized%2520target%2520density%2520has%2520not%2520been%2520explored%250Aextensively.%2520This%2520work%2520contributes%2520to%2520fill%2520this%2520gap.%2520We%2520consider%2520a%2520general%250Astochastic%2520localization%2520framework%2520and%2520introduce%2520an%2520explicit%2520class%2520of%250Aobservation%2520processes%252C%2520associated%2520with%2520flexible%2520denoising%2520schedules.%2520We%2520provide%250Aa%2520complete%2520methodology%252C%2520%2524%255Ctextit%257BStochastic%2520Localization%2520via%2520Iterative%250APosterior%2520Sampling%257D%2524%2520%2528SLIPS%2529%252C%2520to%2520obtain%2520approximate%2520samples%2520of%2520this%2520dynamics%252C%250Aand%2520as%2520a%2520by-product%252C%2520samples%2520from%2520the%2520target%2520distribution.%2520Our%2520scheme%2520is%2520based%250Aon%2520a%2520Markov%2520chain%2520Monte%2520Carlo%2520estimation%2520of%2520the%2520denoiser%2520and%2520comes%2520with%250Adetailed%2520practical%2520guidelines.%2520We%2520illustrate%2520the%2520benefits%2520and%2520applicability%2520of%250ASLIPS%2520on%2520several%2520benchmarks%2520of%2520multi-modal%2520distributions%252C%2520including%2520Gaussian%250Amixtures%2520in%2520increasing%2520dimensions%252C%2520Bayesian%2520logistic%2520regression%2520and%2520a%250Ahigh-dimensional%2520field%2520system%2520from%2520statistical-mechanics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10758v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Localization%20via%20Iterative%20Posterior%20Sampling&entry.906535625=Louis%20Grenioux%20and%20Maxence%20Noble%20and%20Marylou%20Gabri%C3%A9%20and%20Alain%20Oliviero%20Durmus&entry.1292438233=%20%20Building%20upon%20score-based%20learning%2C%20new%20interest%20in%20stochastic%20localization%0Atechniques%20has%20recently%20emerged.%20In%20these%20models%2C%20one%20seeks%20to%20noise%20a%20sample%0Afrom%20the%20data%20distribution%20through%20a%20stochastic%20process%2C%20called%20observation%0Aprocess%2C%20and%20progressively%20learns%20a%20denoiser%20associated%20to%20this%20dynamics.%20Apart%0Afrom%20specific%20applications%2C%20the%20use%20of%20stochastic%20localization%20for%20the%20problem%0Aof%20sampling%20from%20an%20unnormalized%20target%20density%20has%20not%20been%20explored%0Aextensively.%20This%20work%20contributes%20to%20fill%20this%20gap.%20We%20consider%20a%20general%0Astochastic%20localization%20framework%20and%20introduce%20an%20explicit%20class%20of%0Aobservation%20processes%2C%20associated%20with%20flexible%20denoising%20schedules.%20We%20provide%0Aa%20complete%20methodology%2C%20%24%5Ctextit%7BStochastic%20Localization%20via%20Iterative%0APosterior%20Sampling%7D%24%20%28SLIPS%29%2C%20to%20obtain%20approximate%20samples%20of%20this%20dynamics%2C%0Aand%20as%20a%20by-product%2C%20samples%20from%20the%20target%20distribution.%20Our%20scheme%20is%20based%0Aon%20a%20Markov%20chain%20Monte%20Carlo%20estimation%20of%20the%20denoiser%20and%20comes%20with%0Adetailed%20practical%20guidelines.%20We%20illustrate%20the%20benefits%20and%20applicability%20of%0ASLIPS%20on%20several%20benchmarks%20of%20multi-modal%20distributions%2C%20including%20Gaussian%0Amixtures%20in%20increasing%20dimensions%2C%20Bayesian%20logistic%20regression%20and%20a%0Ahigh-dimensional%20field%20system%20from%20statistical-mechanics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10758v2&entry.124074799=Read"},
{"title": "CT-based brain ventricle segmentation via diffusion Schr\u00f6dinger Bridge\n  without target domain ground truths", "author": "Reihaneh Teimouri and Marta Kersten-Oertel and Yiming Xiao", "abstract": "  Efficient and accurate brain ventricle segmentation from clinical CT scans is\ncritical for emergency surgeries like ventriculostomy. With the challenges in\npoor soft tissue contrast and a scarcity of well-annotated databases for\nclinical brain CTs, we introduce a novel uncertainty-aware ventricle\nsegmentation technique without the need of CT segmentation ground truths by\nleveraging diffusion-model-based domain adaptation. Specifically, our method\nemploys the diffusion Schr\\\"odinger Bridge and an attention recurrent residual\nU-Net to capitalize on unpaired CT and MRI scans to derive automatic CT\nsegmentation from those of the MRIs, which are more accessible. Importantly, we\npropose an end-to-end, joint training framework of image translation and\nsegmentation tasks, and demonstrate its benefit over training individual tasks\nseparately. By comparing the proposed method against similar setups using two\ndifferent GAN models for domain adaptation (CycleGAN and CUT), we also reveal\nthe advantage of diffusion models towards improved segmentation and image\ntranslation quality. With a Dice score of 0.78$\\pm$0.27, our proposed method\noutperformed the compared methods, including SynSeg-Net, while providing\nintuitive uncertainty measures to further facilitate quality control of the\nautomatic segmentation outcomes.\n", "link": "http://arxiv.org/abs/2405.18267v1", "date": "2024-05-28", "relevancy": 2.1962, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5703}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5448}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CT-based%20brain%20ventricle%20segmentation%20via%20diffusion%20Schr%C3%B6dinger%20Bridge%0A%20%20without%20target%20domain%20ground%20truths&body=Title%3A%20CT-based%20brain%20ventricle%20segmentation%20via%20diffusion%20Schr%C3%B6dinger%20Bridge%0A%20%20without%20target%20domain%20ground%20truths%0AAuthor%3A%20Reihaneh%20Teimouri%20and%20Marta%20Kersten-Oertel%20and%20Yiming%20Xiao%0AAbstract%3A%20%20%20Efficient%20and%20accurate%20brain%20ventricle%20segmentation%20from%20clinical%20CT%20scans%20is%0Acritical%20for%20emergency%20surgeries%20like%20ventriculostomy.%20With%20the%20challenges%20in%0Apoor%20soft%20tissue%20contrast%20and%20a%20scarcity%20of%20well-annotated%20databases%20for%0Aclinical%20brain%20CTs%2C%20we%20introduce%20a%20novel%20uncertainty-aware%20ventricle%0Asegmentation%20technique%20without%20the%20need%20of%20CT%20segmentation%20ground%20truths%20by%0Aleveraging%20diffusion-model-based%20domain%20adaptation.%20Specifically%2C%20our%20method%0Aemploys%20the%20diffusion%20Schr%5C%22odinger%20Bridge%20and%20an%20attention%20recurrent%20residual%0AU-Net%20to%20capitalize%20on%20unpaired%20CT%20and%20MRI%20scans%20to%20derive%20automatic%20CT%0Asegmentation%20from%20those%20of%20the%20MRIs%2C%20which%20are%20more%20accessible.%20Importantly%2C%20we%0Apropose%20an%20end-to-end%2C%20joint%20training%20framework%20of%20image%20translation%20and%0Asegmentation%20tasks%2C%20and%20demonstrate%20its%20benefit%20over%20training%20individual%20tasks%0Aseparately.%20By%20comparing%20the%20proposed%20method%20against%20similar%20setups%20using%20two%0Adifferent%20GAN%20models%20for%20domain%20adaptation%20%28CycleGAN%20and%20CUT%29%2C%20we%20also%20reveal%0Athe%20advantage%20of%20diffusion%20models%20towards%20improved%20segmentation%20and%20image%0Atranslation%20quality.%20With%20a%20Dice%20score%20of%200.78%24%5Cpm%240.27%2C%20our%20proposed%20method%0Aoutperformed%20the%20compared%20methods%2C%20including%20SynSeg-Net%2C%20while%20providing%0Aintuitive%20uncertainty%20measures%20to%20further%20facilitate%20quality%20control%20of%20the%0Aautomatic%20segmentation%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCT-based%2520brain%2520ventricle%2520segmentation%2520via%2520diffusion%2520Schr%25C3%25B6dinger%2520Bridge%250A%2520%2520without%2520target%2520domain%2520ground%2520truths%26entry.906535625%3DReihaneh%2520Teimouri%2520and%2520Marta%2520Kersten-Oertel%2520and%2520Yiming%2520Xiao%26entry.1292438233%3D%2520%2520Efficient%2520and%2520accurate%2520brain%2520ventricle%2520segmentation%2520from%2520clinical%2520CT%2520scans%2520is%250Acritical%2520for%2520emergency%2520surgeries%2520like%2520ventriculostomy.%2520With%2520the%2520challenges%2520in%250Apoor%2520soft%2520tissue%2520contrast%2520and%2520a%2520scarcity%2520of%2520well-annotated%2520databases%2520for%250Aclinical%2520brain%2520CTs%252C%2520we%2520introduce%2520a%2520novel%2520uncertainty-aware%2520ventricle%250Asegmentation%2520technique%2520without%2520the%2520need%2520of%2520CT%2520segmentation%2520ground%2520truths%2520by%250Aleveraging%2520diffusion-model-based%2520domain%2520adaptation.%2520Specifically%252C%2520our%2520method%250Aemploys%2520the%2520diffusion%2520Schr%255C%2522odinger%2520Bridge%2520and%2520an%2520attention%2520recurrent%2520residual%250AU-Net%2520to%2520capitalize%2520on%2520unpaired%2520CT%2520and%2520MRI%2520scans%2520to%2520derive%2520automatic%2520CT%250Asegmentation%2520from%2520those%2520of%2520the%2520MRIs%252C%2520which%2520are%2520more%2520accessible.%2520Importantly%252C%2520we%250Apropose%2520an%2520end-to-end%252C%2520joint%2520training%2520framework%2520of%2520image%2520translation%2520and%250Asegmentation%2520tasks%252C%2520and%2520demonstrate%2520its%2520benefit%2520over%2520training%2520individual%2520tasks%250Aseparately.%2520By%2520comparing%2520the%2520proposed%2520method%2520against%2520similar%2520setups%2520using%2520two%250Adifferent%2520GAN%2520models%2520for%2520domain%2520adaptation%2520%2528CycleGAN%2520and%2520CUT%2529%252C%2520we%2520also%2520reveal%250Athe%2520advantage%2520of%2520diffusion%2520models%2520towards%2520improved%2520segmentation%2520and%2520image%250Atranslation%2520quality.%2520With%2520a%2520Dice%2520score%2520of%25200.78%2524%255Cpm%25240.27%252C%2520our%2520proposed%2520method%250Aoutperformed%2520the%2520compared%2520methods%252C%2520including%2520SynSeg-Net%252C%2520while%2520providing%250Aintuitive%2520uncertainty%2520measures%2520to%2520further%2520facilitate%2520quality%2520control%2520of%2520the%250Aautomatic%2520segmentation%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT-based%20brain%20ventricle%20segmentation%20via%20diffusion%20Schr%C3%B6dinger%20Bridge%0A%20%20without%20target%20domain%20ground%20truths&entry.906535625=Reihaneh%20Teimouri%20and%20Marta%20Kersten-Oertel%20and%20Yiming%20Xiao&entry.1292438233=%20%20Efficient%20and%20accurate%20brain%20ventricle%20segmentation%20from%20clinical%20CT%20scans%20is%0Acritical%20for%20emergency%20surgeries%20like%20ventriculostomy.%20With%20the%20challenges%20in%0Apoor%20soft%20tissue%20contrast%20and%20a%20scarcity%20of%20well-annotated%20databases%20for%0Aclinical%20brain%20CTs%2C%20we%20introduce%20a%20novel%20uncertainty-aware%20ventricle%0Asegmentation%20technique%20without%20the%20need%20of%20CT%20segmentation%20ground%20truths%20by%0Aleveraging%20diffusion-model-based%20domain%20adaptation.%20Specifically%2C%20our%20method%0Aemploys%20the%20diffusion%20Schr%5C%22odinger%20Bridge%20and%20an%20attention%20recurrent%20residual%0AU-Net%20to%20capitalize%20on%20unpaired%20CT%20and%20MRI%20scans%20to%20derive%20automatic%20CT%0Asegmentation%20from%20those%20of%20the%20MRIs%2C%20which%20are%20more%20accessible.%20Importantly%2C%20we%0Apropose%20an%20end-to-end%2C%20joint%20training%20framework%20of%20image%20translation%20and%0Asegmentation%20tasks%2C%20and%20demonstrate%20its%20benefit%20over%20training%20individual%20tasks%0Aseparately.%20By%20comparing%20the%20proposed%20method%20against%20similar%20setups%20using%20two%0Adifferent%20GAN%20models%20for%20domain%20adaptation%20%28CycleGAN%20and%20CUT%29%2C%20we%20also%20reveal%0Athe%20advantage%20of%20diffusion%20models%20towards%20improved%20segmentation%20and%20image%0Atranslation%20quality.%20With%20a%20Dice%20score%20of%200.78%24%5Cpm%240.27%2C%20our%20proposed%20method%0Aoutperformed%20the%20compared%20methods%2C%20including%20SynSeg-Net%2C%20while%20providing%0Aintuitive%20uncertainty%20measures%20to%20further%20facilitate%20quality%20control%20of%20the%0Aautomatic%20segmentation%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18267v1&entry.124074799=Read"},
{"title": "Explicit Formulae to Interchangeably use Hyperplanes and Hyperballs\n  using Inversive Geometry", "author": "Erik Thordsen and Erich Schubert", "abstract": "  Many algorithms require discriminative boundaries, such as separating\nhyperplanes or hyperballs, or are specifically designed to work on spherical\ndata. By applying inversive geometry, we show that the two discriminative\nboundaries can be used interchangeably, and that general Euclidean data can be\ntransformed into spherical data, whenever a change in point distances is\nacceptable. We provide explicit formulae to embed general Euclidean data into\nspherical data and to unembed it back. We further show a duality between\nhyperspherical caps, i.e., the volume created by a separating hyperplane on\nspherical data, and hyperballs and provide explicit formulae to map between the\ntwo. We further provide equations to translate inner products and Euclidean\ndistances between the two spaces, to avoid explicit embedding and unembedding.\nWe also provide a method to enforce projections of the general Euclidean space\nonto hemi-hyperspheres and propose an intrinsic dimensionality based method to\nobtain \"all-purpose\" parameters. To show the usefulness of the\ncap-ball-duality, we discuss example applications in machine learning and\nvector similarity search.\n", "link": "http://arxiv.org/abs/2405.18401v1", "date": "2024-05-28", "relevancy": 2.1915, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4603}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.431}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Formulae%20to%20Interchangeably%20use%20Hyperplanes%20and%20Hyperballs%0A%20%20using%20Inversive%20Geometry&body=Title%3A%20Explicit%20Formulae%20to%20Interchangeably%20use%20Hyperplanes%20and%20Hyperballs%0A%20%20using%20Inversive%20Geometry%0AAuthor%3A%20Erik%20Thordsen%20and%20Erich%20Schubert%0AAbstract%3A%20%20%20Many%20algorithms%20require%20discriminative%20boundaries%2C%20such%20as%20separating%0Ahyperplanes%20or%20hyperballs%2C%20or%20are%20specifically%20designed%20to%20work%20on%20spherical%0Adata.%20By%20applying%20inversive%20geometry%2C%20we%20show%20that%20the%20two%20discriminative%0Aboundaries%20can%20be%20used%20interchangeably%2C%20and%20that%20general%20Euclidean%20data%20can%20be%0Atransformed%20into%20spherical%20data%2C%20whenever%20a%20change%20in%20point%20distances%20is%0Aacceptable.%20We%20provide%20explicit%20formulae%20to%20embed%20general%20Euclidean%20data%20into%0Aspherical%20data%20and%20to%20unembed%20it%20back.%20We%20further%20show%20a%20duality%20between%0Ahyperspherical%20caps%2C%20i.e.%2C%20the%20volume%20created%20by%20a%20separating%20hyperplane%20on%0Aspherical%20data%2C%20and%20hyperballs%20and%20provide%20explicit%20formulae%20to%20map%20between%20the%0Atwo.%20We%20further%20provide%20equations%20to%20translate%20inner%20products%20and%20Euclidean%0Adistances%20between%20the%20two%20spaces%2C%20to%20avoid%20explicit%20embedding%20and%20unembedding.%0AWe%20also%20provide%20a%20method%20to%20enforce%20projections%20of%20the%20general%20Euclidean%20space%0Aonto%20hemi-hyperspheres%20and%20propose%20an%20intrinsic%20dimensionality%20based%20method%20to%0Aobtain%20%22all-purpose%22%20parameters.%20To%20show%20the%20usefulness%20of%20the%0Acap-ball-duality%2C%20we%20discuss%20example%20applications%20in%20machine%20learning%20and%0Avector%20similarity%20search.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Formulae%2520to%2520Interchangeably%2520use%2520Hyperplanes%2520and%2520Hyperballs%250A%2520%2520using%2520Inversive%2520Geometry%26entry.906535625%3DErik%2520Thordsen%2520and%2520Erich%2520Schubert%26entry.1292438233%3D%2520%2520Many%2520algorithms%2520require%2520discriminative%2520boundaries%252C%2520such%2520as%2520separating%250Ahyperplanes%2520or%2520hyperballs%252C%2520or%2520are%2520specifically%2520designed%2520to%2520work%2520on%2520spherical%250Adata.%2520By%2520applying%2520inversive%2520geometry%252C%2520we%2520show%2520that%2520the%2520two%2520discriminative%250Aboundaries%2520can%2520be%2520used%2520interchangeably%252C%2520and%2520that%2520general%2520Euclidean%2520data%2520can%2520be%250Atransformed%2520into%2520spherical%2520data%252C%2520whenever%2520a%2520change%2520in%2520point%2520distances%2520is%250Aacceptable.%2520We%2520provide%2520explicit%2520formulae%2520to%2520embed%2520general%2520Euclidean%2520data%2520into%250Aspherical%2520data%2520and%2520to%2520unembed%2520it%2520back.%2520We%2520further%2520show%2520a%2520duality%2520between%250Ahyperspherical%2520caps%252C%2520i.e.%252C%2520the%2520volume%2520created%2520by%2520a%2520separating%2520hyperplane%2520on%250Aspherical%2520data%252C%2520and%2520hyperballs%2520and%2520provide%2520explicit%2520formulae%2520to%2520map%2520between%2520the%250Atwo.%2520We%2520further%2520provide%2520equations%2520to%2520translate%2520inner%2520products%2520and%2520Euclidean%250Adistances%2520between%2520the%2520two%2520spaces%252C%2520to%2520avoid%2520explicit%2520embedding%2520and%2520unembedding.%250AWe%2520also%2520provide%2520a%2520method%2520to%2520enforce%2520projections%2520of%2520the%2520general%2520Euclidean%2520space%250Aonto%2520hemi-hyperspheres%2520and%2520propose%2520an%2520intrinsic%2520dimensionality%2520based%2520method%2520to%250Aobtain%2520%2522all-purpose%2522%2520parameters.%2520To%2520show%2520the%2520usefulness%2520of%2520the%250Acap-ball-duality%252C%2520we%2520discuss%2520example%2520applications%2520in%2520machine%2520learning%2520and%250Avector%2520similarity%2520search.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Formulae%20to%20Interchangeably%20use%20Hyperplanes%20and%20Hyperballs%0A%20%20using%20Inversive%20Geometry&entry.906535625=Erik%20Thordsen%20and%20Erich%20Schubert&entry.1292438233=%20%20Many%20algorithms%20require%20discriminative%20boundaries%2C%20such%20as%20separating%0Ahyperplanes%20or%20hyperballs%2C%20or%20are%20specifically%20designed%20to%20work%20on%20spherical%0Adata.%20By%20applying%20inversive%20geometry%2C%20we%20show%20that%20the%20two%20discriminative%0Aboundaries%20can%20be%20used%20interchangeably%2C%20and%20that%20general%20Euclidean%20data%20can%20be%0Atransformed%20into%20spherical%20data%2C%20whenever%20a%20change%20in%20point%20distances%20is%0Aacceptable.%20We%20provide%20explicit%20formulae%20to%20embed%20general%20Euclidean%20data%20into%0Aspherical%20data%20and%20to%20unembed%20it%20back.%20We%20further%20show%20a%20duality%20between%0Ahyperspherical%20caps%2C%20i.e.%2C%20the%20volume%20created%20by%20a%20separating%20hyperplane%20on%0Aspherical%20data%2C%20and%20hyperballs%20and%20provide%20explicit%20formulae%20to%20map%20between%20the%0Atwo.%20We%20further%20provide%20equations%20to%20translate%20inner%20products%20and%20Euclidean%0Adistances%20between%20the%20two%20spaces%2C%20to%20avoid%20explicit%20embedding%20and%20unembedding.%0AWe%20also%20provide%20a%20method%20to%20enforce%20projections%20of%20the%20general%20Euclidean%20space%0Aonto%20hemi-hyperspheres%20and%20propose%20an%20intrinsic%20dimensionality%20based%20method%20to%0Aobtain%20%22all-purpose%22%20parameters.%20To%20show%20the%20usefulness%20of%20the%0Acap-ball-duality%2C%20we%20discuss%20example%20applications%20in%20machine%20learning%20and%0Avector%20similarity%20search.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18401v1&entry.124074799=Read"},
{"title": "Online Calibration of a Single-Track Ground Vehicle Dynamics Model by\n  Tight Fusion with Visual-Inertial Odometry", "author": "Haolong Li and Joerg Stueckler", "abstract": "  Wheeled mobile robots need the ability to estimate their motion and the\neffect of their control actions for navigation planning. In this paper, we\npresent ST-VIO, a novel approach which tightly fuses a single-track dynamics\nmodel for wheeled ground vehicles with visual inertial odometry (VIO). Our\nmethod calibrates and adapts the dynamics model online to improve the accuracy\nof forward prediction conditioned on future control inputs. The single-track\ndynamics model approximates wheeled vehicle motion under specific control\ninputs on flat ground using ordinary differential equations. We use a\nsingularity-free and differentiable variant of the single-track model to enable\nseamless integration as dynamics factor into VIO and to optimize the model\nparameters online together with the VIO state variables. We validate our method\nwith real-world data in both indoor and outdoor environments with different\nterrain types and wheels. In experiments, we demonstrate that ST-VIO can not\nonly adapt to wheel or ground changes and improve the accuracy of prediction\nunder new control inputs, but can even improve tracking accuracy.\n", "link": "http://arxiv.org/abs/2309.11148v3", "date": "2024-05-28", "relevancy": 2.1844, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5771}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5751}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Calibration%20of%20a%20Single-Track%20Ground%20Vehicle%20Dynamics%20Model%20by%0A%20%20Tight%20Fusion%20with%20Visual-Inertial%20Odometry&body=Title%3A%20Online%20Calibration%20of%20a%20Single-Track%20Ground%20Vehicle%20Dynamics%20Model%20by%0A%20%20Tight%20Fusion%20with%20Visual-Inertial%20Odometry%0AAuthor%3A%20Haolong%20Li%20and%20Joerg%20Stueckler%0AAbstract%3A%20%20%20Wheeled%20mobile%20robots%20need%20the%20ability%20to%20estimate%20their%20motion%20and%20the%0Aeffect%20of%20their%20control%20actions%20for%20navigation%20planning.%20In%20this%20paper%2C%20we%0Apresent%20ST-VIO%2C%20a%20novel%20approach%20which%20tightly%20fuses%20a%20single-track%20dynamics%0Amodel%20for%20wheeled%20ground%20vehicles%20with%20visual%20inertial%20odometry%20%28VIO%29.%20Our%0Amethod%20calibrates%20and%20adapts%20the%20dynamics%20model%20online%20to%20improve%20the%20accuracy%0Aof%20forward%20prediction%20conditioned%20on%20future%20control%20inputs.%20The%20single-track%0Adynamics%20model%20approximates%20wheeled%20vehicle%20motion%20under%20specific%20control%0Ainputs%20on%20flat%20ground%20using%20ordinary%20differential%20equations.%20We%20use%20a%0Asingularity-free%20and%20differentiable%20variant%20of%20the%20single-track%20model%20to%20enable%0Aseamless%20integration%20as%20dynamics%20factor%20into%20VIO%20and%20to%20optimize%20the%20model%0Aparameters%20online%20together%20with%20the%20VIO%20state%20variables.%20We%20validate%20our%20method%0Awith%20real-world%20data%20in%20both%20indoor%20and%20outdoor%20environments%20with%20different%0Aterrain%20types%20and%20wheels.%20In%20experiments%2C%20we%20demonstrate%20that%20ST-VIO%20can%20not%0Aonly%20adapt%20to%20wheel%20or%20ground%20changes%20and%20improve%20the%20accuracy%20of%20prediction%0Aunder%20new%20control%20inputs%2C%20but%20can%20even%20improve%20tracking%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11148v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Calibration%2520of%2520a%2520Single-Track%2520Ground%2520Vehicle%2520Dynamics%2520Model%2520by%250A%2520%2520Tight%2520Fusion%2520with%2520Visual-Inertial%2520Odometry%26entry.906535625%3DHaolong%2520Li%2520and%2520Joerg%2520Stueckler%26entry.1292438233%3D%2520%2520Wheeled%2520mobile%2520robots%2520need%2520the%2520ability%2520to%2520estimate%2520their%2520motion%2520and%2520the%250Aeffect%2520of%2520their%2520control%2520actions%2520for%2520navigation%2520planning.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520ST-VIO%252C%2520a%2520novel%2520approach%2520which%2520tightly%2520fuses%2520a%2520single-track%2520dynamics%250Amodel%2520for%2520wheeled%2520ground%2520vehicles%2520with%2520visual%2520inertial%2520odometry%2520%2528VIO%2529.%2520Our%250Amethod%2520calibrates%2520and%2520adapts%2520the%2520dynamics%2520model%2520online%2520to%2520improve%2520the%2520accuracy%250Aof%2520forward%2520prediction%2520conditioned%2520on%2520future%2520control%2520inputs.%2520The%2520single-track%250Adynamics%2520model%2520approximates%2520wheeled%2520vehicle%2520motion%2520under%2520specific%2520control%250Ainputs%2520on%2520flat%2520ground%2520using%2520ordinary%2520differential%2520equations.%2520We%2520use%2520a%250Asingularity-free%2520and%2520differentiable%2520variant%2520of%2520the%2520single-track%2520model%2520to%2520enable%250Aseamless%2520integration%2520as%2520dynamics%2520factor%2520into%2520VIO%2520and%2520to%2520optimize%2520the%2520model%250Aparameters%2520online%2520together%2520with%2520the%2520VIO%2520state%2520variables.%2520We%2520validate%2520our%2520method%250Awith%2520real-world%2520data%2520in%2520both%2520indoor%2520and%2520outdoor%2520environments%2520with%2520different%250Aterrain%2520types%2520and%2520wheels.%2520In%2520experiments%252C%2520we%2520demonstrate%2520that%2520ST-VIO%2520can%2520not%250Aonly%2520adapt%2520to%2520wheel%2520or%2520ground%2520changes%2520and%2520improve%2520the%2520accuracy%2520of%2520prediction%250Aunder%2520new%2520control%2520inputs%252C%2520but%2520can%2520even%2520improve%2520tracking%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11148v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Calibration%20of%20a%20Single-Track%20Ground%20Vehicle%20Dynamics%20Model%20by%0A%20%20Tight%20Fusion%20with%20Visual-Inertial%20Odometry&entry.906535625=Haolong%20Li%20and%20Joerg%20Stueckler&entry.1292438233=%20%20Wheeled%20mobile%20robots%20need%20the%20ability%20to%20estimate%20their%20motion%20and%20the%0Aeffect%20of%20their%20control%20actions%20for%20navigation%20planning.%20In%20this%20paper%2C%20we%0Apresent%20ST-VIO%2C%20a%20novel%20approach%20which%20tightly%20fuses%20a%20single-track%20dynamics%0Amodel%20for%20wheeled%20ground%20vehicles%20with%20visual%20inertial%20odometry%20%28VIO%29.%20Our%0Amethod%20calibrates%20and%20adapts%20the%20dynamics%20model%20online%20to%20improve%20the%20accuracy%0Aof%20forward%20prediction%20conditioned%20on%20future%20control%20inputs.%20The%20single-track%0Adynamics%20model%20approximates%20wheeled%20vehicle%20motion%20under%20specific%20control%0Ainputs%20on%20flat%20ground%20using%20ordinary%20differential%20equations.%20We%20use%20a%0Asingularity-free%20and%20differentiable%20variant%20of%20the%20single-track%20model%20to%20enable%0Aseamless%20integration%20as%20dynamics%20factor%20into%20VIO%20and%20to%20optimize%20the%20model%0Aparameters%20online%20together%20with%20the%20VIO%20state%20variables.%20We%20validate%20our%20method%0Awith%20real-world%20data%20in%20both%20indoor%20and%20outdoor%20environments%20with%20different%0Aterrain%20types%20and%20wheels.%20In%20experiments%2C%20we%20demonstrate%20that%20ST-VIO%20can%20not%0Aonly%20adapt%20to%20wheel%20or%20ground%20changes%20and%20improve%20the%20accuracy%20of%20prediction%0Aunder%20new%20control%20inputs%2C%20but%20can%20even%20improve%20tracking%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11148v3&entry.124074799=Read"},
{"title": "Safe Multi-Agent Reinforcement Learning with Bilevel Optimization in\n  Autonomous Driving", "author": "Zhi Zheng and Shangding Gu", "abstract": "  Ensuring safety in MARL, particularly when deploying it in real-world\napplications such as autonomous driving, emerges as a critical challenge. To\naddress this challenge, traditional safe MARL methods extend MARL approaches to\nincorporate safety considerations, aiming to minimize safety risk values.\nHowever, these safe MARL algorithms often fail to model other agents and lack\nconvergence guarantees, particularly in dynamically complex environments. In\nthis study, we propose a safe MARL method grounded in a Stackelberg model with\nbi-level optimization, for which convergence analysis is provided. Derived from\nour theoretical analysis, we develop two practical algorithms, namely\nConstrained Stackelberg Q-learning (CSQ) and Constrained Stackelberg\nMulti-Agent Deep Deterministic Policy Gradient (CS-MADDPG), designed to\nfacilitate MARL decision-making in autonomous driving applications. To evaluate\nthe effectiveness of our algorithms, we developed a safe MARL autonomous\ndriving benchmark and conducted experiments on challenging autonomous driving\nscenarios, such as merges, roundabouts, intersections, and racetracks. The\nexperimental results indicate that our algorithms, CSQ and CS-MADDPG,\noutperform several strong MARL baselines, such as Bi-AC, MACPO, and MAPPO-L,\nregarding reward and safety performance. The demos and source code are\navailable at\n{https://github.com/SafeRL-Lab/Safe-MARL-in-Autonomous-Driving.git}.\n", "link": "http://arxiv.org/abs/2405.18209v1", "date": "2024-05-28", "relevancy": 2.1785, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5763}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5457}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Multi-Agent%20Reinforcement%20Learning%20with%20Bilevel%20Optimization%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20Safe%20Multi-Agent%20Reinforcement%20Learning%20with%20Bilevel%20Optimization%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Zhi%20Zheng%20and%20Shangding%20Gu%0AAbstract%3A%20%20%20Ensuring%20safety%20in%20MARL%2C%20particularly%20when%20deploying%20it%20in%20real-world%0Aapplications%20such%20as%20autonomous%20driving%2C%20emerges%20as%20a%20critical%20challenge.%20To%0Aaddress%20this%20challenge%2C%20traditional%20safe%20MARL%20methods%20extend%20MARL%20approaches%20to%0Aincorporate%20safety%20considerations%2C%20aiming%20to%20minimize%20safety%20risk%20values.%0AHowever%2C%20these%20safe%20MARL%20algorithms%20often%20fail%20to%20model%20other%20agents%20and%20lack%0Aconvergence%20guarantees%2C%20particularly%20in%20dynamically%20complex%20environments.%20In%0Athis%20study%2C%20we%20propose%20a%20safe%20MARL%20method%20grounded%20in%20a%20Stackelberg%20model%20with%0Abi-level%20optimization%2C%20for%20which%20convergence%20analysis%20is%20provided.%20Derived%20from%0Aour%20theoretical%20analysis%2C%20we%20develop%20two%20practical%20algorithms%2C%20namely%0AConstrained%20Stackelberg%20Q-learning%20%28CSQ%29%20and%20Constrained%20Stackelberg%0AMulti-Agent%20Deep%20Deterministic%20Policy%20Gradient%20%28CS-MADDPG%29%2C%20designed%20to%0Afacilitate%20MARL%20decision-making%20in%20autonomous%20driving%20applications.%20To%20evaluate%0Athe%20effectiveness%20of%20our%20algorithms%2C%20we%20developed%20a%20safe%20MARL%20autonomous%0Adriving%20benchmark%20and%20conducted%20experiments%20on%20challenging%20autonomous%20driving%0Ascenarios%2C%20such%20as%20merges%2C%20roundabouts%2C%20intersections%2C%20and%20racetracks.%20The%0Aexperimental%20results%20indicate%20that%20our%20algorithms%2C%20CSQ%20and%20CS-MADDPG%2C%0Aoutperform%20several%20strong%20MARL%20baselines%2C%20such%20as%20Bi-AC%2C%20MACPO%2C%20and%20MAPPO-L%2C%0Aregarding%20reward%20and%20safety%20performance.%20The%20demos%20and%20source%20code%20are%0Aavailable%20at%0A%7Bhttps%3A//github.com/SafeRL-Lab/Safe-MARL-in-Autonomous-Driving.git%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Multi-Agent%2520Reinforcement%2520Learning%2520with%2520Bilevel%2520Optimization%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DZhi%2520Zheng%2520and%2520Shangding%2520Gu%26entry.1292438233%3D%2520%2520Ensuring%2520safety%2520in%2520MARL%252C%2520particularly%2520when%2520deploying%2520it%2520in%2520real-world%250Aapplications%2520such%2520as%2520autonomous%2520driving%252C%2520emerges%2520as%2520a%2520critical%2520challenge.%2520To%250Aaddress%2520this%2520challenge%252C%2520traditional%2520safe%2520MARL%2520methods%2520extend%2520MARL%2520approaches%2520to%250Aincorporate%2520safety%2520considerations%252C%2520aiming%2520to%2520minimize%2520safety%2520risk%2520values.%250AHowever%252C%2520these%2520safe%2520MARL%2520algorithms%2520often%2520fail%2520to%2520model%2520other%2520agents%2520and%2520lack%250Aconvergence%2520guarantees%252C%2520particularly%2520in%2520dynamically%2520complex%2520environments.%2520In%250Athis%2520study%252C%2520we%2520propose%2520a%2520safe%2520MARL%2520method%2520grounded%2520in%2520a%2520Stackelberg%2520model%2520with%250Abi-level%2520optimization%252C%2520for%2520which%2520convergence%2520analysis%2520is%2520provided.%2520Derived%2520from%250Aour%2520theoretical%2520analysis%252C%2520we%2520develop%2520two%2520practical%2520algorithms%252C%2520namely%250AConstrained%2520Stackelberg%2520Q-learning%2520%2528CSQ%2529%2520and%2520Constrained%2520Stackelberg%250AMulti-Agent%2520Deep%2520Deterministic%2520Policy%2520Gradient%2520%2528CS-MADDPG%2529%252C%2520designed%2520to%250Afacilitate%2520MARL%2520decision-making%2520in%2520autonomous%2520driving%2520applications.%2520To%2520evaluate%250Athe%2520effectiveness%2520of%2520our%2520algorithms%252C%2520we%2520developed%2520a%2520safe%2520MARL%2520autonomous%250Adriving%2520benchmark%2520and%2520conducted%2520experiments%2520on%2520challenging%2520autonomous%2520driving%250Ascenarios%252C%2520such%2520as%2520merges%252C%2520roundabouts%252C%2520intersections%252C%2520and%2520racetracks.%2520The%250Aexperimental%2520results%2520indicate%2520that%2520our%2520algorithms%252C%2520CSQ%2520and%2520CS-MADDPG%252C%250Aoutperform%2520several%2520strong%2520MARL%2520baselines%252C%2520such%2520as%2520Bi-AC%252C%2520MACPO%252C%2520and%2520MAPPO-L%252C%250Aregarding%2520reward%2520and%2520safety%2520performance.%2520The%2520demos%2520and%2520source%2520code%2520are%250Aavailable%2520at%250A%257Bhttps%253A//github.com/SafeRL-Lab/Safe-MARL-in-Autonomous-Driving.git%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Multi-Agent%20Reinforcement%20Learning%20with%20Bilevel%20Optimization%20in%0A%20%20Autonomous%20Driving&entry.906535625=Zhi%20Zheng%20and%20Shangding%20Gu&entry.1292438233=%20%20Ensuring%20safety%20in%20MARL%2C%20particularly%20when%20deploying%20it%20in%20real-world%0Aapplications%20such%20as%20autonomous%20driving%2C%20emerges%20as%20a%20critical%20challenge.%20To%0Aaddress%20this%20challenge%2C%20traditional%20safe%20MARL%20methods%20extend%20MARL%20approaches%20to%0Aincorporate%20safety%20considerations%2C%20aiming%20to%20minimize%20safety%20risk%20values.%0AHowever%2C%20these%20safe%20MARL%20algorithms%20often%20fail%20to%20model%20other%20agents%20and%20lack%0Aconvergence%20guarantees%2C%20particularly%20in%20dynamically%20complex%20environments.%20In%0Athis%20study%2C%20we%20propose%20a%20safe%20MARL%20method%20grounded%20in%20a%20Stackelberg%20model%20with%0Abi-level%20optimization%2C%20for%20which%20convergence%20analysis%20is%20provided.%20Derived%20from%0Aour%20theoretical%20analysis%2C%20we%20develop%20two%20practical%20algorithms%2C%20namely%0AConstrained%20Stackelberg%20Q-learning%20%28CSQ%29%20and%20Constrained%20Stackelberg%0AMulti-Agent%20Deep%20Deterministic%20Policy%20Gradient%20%28CS-MADDPG%29%2C%20designed%20to%0Afacilitate%20MARL%20decision-making%20in%20autonomous%20driving%20applications.%20To%20evaluate%0Athe%20effectiveness%20of%20our%20algorithms%2C%20we%20developed%20a%20safe%20MARL%20autonomous%0Adriving%20benchmark%20and%20conducted%20experiments%20on%20challenging%20autonomous%20driving%0Ascenarios%2C%20such%20as%20merges%2C%20roundabouts%2C%20intersections%2C%20and%20racetracks.%20The%0Aexperimental%20results%20indicate%20that%20our%20algorithms%2C%20CSQ%20and%20CS-MADDPG%2C%0Aoutperform%20several%20strong%20MARL%20baselines%2C%20such%20as%20Bi-AC%2C%20MACPO%2C%20and%20MAPPO-L%2C%0Aregarding%20reward%20and%20safety%20performance.%20The%20demos%20and%20source%20code%20are%0Aavailable%20at%0A%7Bhttps%3A//github.com/SafeRL-Lab/Safe-MARL-in-Autonomous-Driving.git%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18209v1&entry.124074799=Read"},
{"title": "Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention", "author": "Weitai Kang and Mengxue Qu and Jyoti Kini and Yunchao Wei and Mubarak Shah and Yan Yan", "abstract": "  In real-life scenarios, humans seek out objects in the 3D world to fulfill\ntheir daily needs or intentions. This inspires us to introduce 3D intention\ngrounding, a new task in 3D object detection employing RGB-D, based on human\nintention, such as \"I want something to support my back\". Closely related, 3D\nvisual grounding focuses on understanding human reference. To achieve detection\nbased on human intention, it relies on humans to observe the scene, reason out\nthe target that aligns with their intention (\"pillow\" in this case), and\nfinally provide a reference to the AI system, such as \"A pillow on the couch\".\nInstead, 3D intention grounding challenges AI agents to automatically observe,\nreason and detect the desired target solely based on human intention. To tackle\nthis challenge, we introduce the new Intent3D dataset, consisting of 44,990\nintention texts associated with 209 fine-grained classes from 1,042 scenes of\nthe ScanNet dataset. We also establish several baselines based on different\nlanguage-based 3D object detection models on our benchmark. Finally, we propose\nIntentNet, our unique approach, designed to tackle this intention-based\ndetection problem. It focuses on three key aspects: intention understanding,\nreasoning to identify object candidates, and cascaded adaptive learning that\nleverages the intrinsic priority logic of different losses for multiple\nobjective optimization.\n", "link": "http://arxiv.org/abs/2405.18295v1", "date": "2024-05-28", "relevancy": 2.1737, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5649}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5375}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intent3D%3A%203D%20Object%20Detection%20in%20RGB-D%20Scans%20Based%20on%20Human%20Intention&body=Title%3A%20Intent3D%3A%203D%20Object%20Detection%20in%20RGB-D%20Scans%20Based%20on%20Human%20Intention%0AAuthor%3A%20Weitai%20Kang%20and%20Mengxue%20Qu%20and%20Jyoti%20Kini%20and%20Yunchao%20Wei%20and%20Mubarak%20Shah%20and%20Yan%20Yan%0AAbstract%3A%20%20%20In%20real-life%20scenarios%2C%20humans%20seek%20out%20objects%20in%20the%203D%20world%20to%20fulfill%0Atheir%20daily%20needs%20or%20intentions.%20This%20inspires%20us%20to%20introduce%203D%20intention%0Agrounding%2C%20a%20new%20task%20in%203D%20object%20detection%20employing%20RGB-D%2C%20based%20on%20human%0Aintention%2C%20such%20as%20%22I%20want%20something%20to%20support%20my%20back%22.%20Closely%20related%2C%203D%0Avisual%20grounding%20focuses%20on%20understanding%20human%20reference.%20To%20achieve%20detection%0Abased%20on%20human%20intention%2C%20it%20relies%20on%20humans%20to%20observe%20the%20scene%2C%20reason%20out%0Athe%20target%20that%20aligns%20with%20their%20intention%20%28%22pillow%22%20in%20this%20case%29%2C%20and%0Afinally%20provide%20a%20reference%20to%20the%20AI%20system%2C%20such%20as%20%22A%20pillow%20on%20the%20couch%22.%0AInstead%2C%203D%20intention%20grounding%20challenges%20AI%20agents%20to%20automatically%20observe%2C%0Areason%20and%20detect%20the%20desired%20target%20solely%20based%20on%20human%20intention.%20To%20tackle%0Athis%20challenge%2C%20we%20introduce%20the%20new%20Intent3D%20dataset%2C%20consisting%20of%2044%2C990%0Aintention%20texts%20associated%20with%20209%20fine-grained%20classes%20from%201%2C042%20scenes%20of%0Athe%20ScanNet%20dataset.%20We%20also%20establish%20several%20baselines%20based%20on%20different%0Alanguage-based%203D%20object%20detection%20models%20on%20our%20benchmark.%20Finally%2C%20we%20propose%0AIntentNet%2C%20our%20unique%20approach%2C%20designed%20to%20tackle%20this%20intention-based%0Adetection%20problem.%20It%20focuses%20on%20three%20key%20aspects%3A%20intention%20understanding%2C%0Areasoning%20to%20identify%20object%20candidates%2C%20and%20cascaded%20adaptive%20learning%20that%0Aleverages%20the%20intrinsic%20priority%20logic%20of%20different%20losses%20for%20multiple%0Aobjective%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntent3D%253A%25203D%2520Object%2520Detection%2520in%2520RGB-D%2520Scans%2520Based%2520on%2520Human%2520Intention%26entry.906535625%3DWeitai%2520Kang%2520and%2520Mengxue%2520Qu%2520and%2520Jyoti%2520Kini%2520and%2520Yunchao%2520Wei%2520and%2520Mubarak%2520Shah%2520and%2520Yan%2520Yan%26entry.1292438233%3D%2520%2520In%2520real-life%2520scenarios%252C%2520humans%2520seek%2520out%2520objects%2520in%2520the%25203D%2520world%2520to%2520fulfill%250Atheir%2520daily%2520needs%2520or%2520intentions.%2520This%2520inspires%2520us%2520to%2520introduce%25203D%2520intention%250Agrounding%252C%2520a%2520new%2520task%2520in%25203D%2520object%2520detection%2520employing%2520RGB-D%252C%2520based%2520on%2520human%250Aintention%252C%2520such%2520as%2520%2522I%2520want%2520something%2520to%2520support%2520my%2520back%2522.%2520Closely%2520related%252C%25203D%250Avisual%2520grounding%2520focuses%2520on%2520understanding%2520human%2520reference.%2520To%2520achieve%2520detection%250Abased%2520on%2520human%2520intention%252C%2520it%2520relies%2520on%2520humans%2520to%2520observe%2520the%2520scene%252C%2520reason%2520out%250Athe%2520target%2520that%2520aligns%2520with%2520their%2520intention%2520%2528%2522pillow%2522%2520in%2520this%2520case%2529%252C%2520and%250Afinally%2520provide%2520a%2520reference%2520to%2520the%2520AI%2520system%252C%2520such%2520as%2520%2522A%2520pillow%2520on%2520the%2520couch%2522.%250AInstead%252C%25203D%2520intention%2520grounding%2520challenges%2520AI%2520agents%2520to%2520automatically%2520observe%252C%250Areason%2520and%2520detect%2520the%2520desired%2520target%2520solely%2520based%2520on%2520human%2520intention.%2520To%2520tackle%250Athis%2520challenge%252C%2520we%2520introduce%2520the%2520new%2520Intent3D%2520dataset%252C%2520consisting%2520of%252044%252C990%250Aintention%2520texts%2520associated%2520with%2520209%2520fine-grained%2520classes%2520from%25201%252C042%2520scenes%2520of%250Athe%2520ScanNet%2520dataset.%2520We%2520also%2520establish%2520several%2520baselines%2520based%2520on%2520different%250Alanguage-based%25203D%2520object%2520detection%2520models%2520on%2520our%2520benchmark.%2520Finally%252C%2520we%2520propose%250AIntentNet%252C%2520our%2520unique%2520approach%252C%2520designed%2520to%2520tackle%2520this%2520intention-based%250Adetection%2520problem.%2520It%2520focuses%2520on%2520three%2520key%2520aspects%253A%2520intention%2520understanding%252C%250Areasoning%2520to%2520identify%2520object%2520candidates%252C%2520and%2520cascaded%2520adaptive%2520learning%2520that%250Aleverages%2520the%2520intrinsic%2520priority%2520logic%2520of%2520different%2520losses%2520for%2520multiple%250Aobjective%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intent3D%3A%203D%20Object%20Detection%20in%20RGB-D%20Scans%20Based%20on%20Human%20Intention&entry.906535625=Weitai%20Kang%20and%20Mengxue%20Qu%20and%20Jyoti%20Kini%20and%20Yunchao%20Wei%20and%20Mubarak%20Shah%20and%20Yan%20Yan&entry.1292438233=%20%20In%20real-life%20scenarios%2C%20humans%20seek%20out%20objects%20in%20the%203D%20world%20to%20fulfill%0Atheir%20daily%20needs%20or%20intentions.%20This%20inspires%20us%20to%20introduce%203D%20intention%0Agrounding%2C%20a%20new%20task%20in%203D%20object%20detection%20employing%20RGB-D%2C%20based%20on%20human%0Aintention%2C%20such%20as%20%22I%20want%20something%20to%20support%20my%20back%22.%20Closely%20related%2C%203D%0Avisual%20grounding%20focuses%20on%20understanding%20human%20reference.%20To%20achieve%20detection%0Abased%20on%20human%20intention%2C%20it%20relies%20on%20humans%20to%20observe%20the%20scene%2C%20reason%20out%0Athe%20target%20that%20aligns%20with%20their%20intention%20%28%22pillow%22%20in%20this%20case%29%2C%20and%0Afinally%20provide%20a%20reference%20to%20the%20AI%20system%2C%20such%20as%20%22A%20pillow%20on%20the%20couch%22.%0AInstead%2C%203D%20intention%20grounding%20challenges%20AI%20agents%20to%20automatically%20observe%2C%0Areason%20and%20detect%20the%20desired%20target%20solely%20based%20on%20human%20intention.%20To%20tackle%0Athis%20challenge%2C%20we%20introduce%20the%20new%20Intent3D%20dataset%2C%20consisting%20of%2044%2C990%0Aintention%20texts%20associated%20with%20209%20fine-grained%20classes%20from%201%2C042%20scenes%20of%0Athe%20ScanNet%20dataset.%20We%20also%20establish%20several%20baselines%20based%20on%20different%0Alanguage-based%203D%20object%20detection%20models%20on%20our%20benchmark.%20Finally%2C%20we%20propose%0AIntentNet%2C%20our%20unique%20approach%2C%20designed%20to%20tackle%20this%20intention-based%0Adetection%20problem.%20It%20focuses%20on%20three%20key%20aspects%3A%20intention%20understanding%2C%0Areasoning%20to%20identify%20object%20candidates%2C%20and%20cascaded%20adaptive%20learning%20that%0Aleverages%20the%20intrinsic%20priority%20logic%20of%20different%20losses%20for%20multiple%0Aobjective%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18295v1&entry.124074799=Read"},
{"title": "An adaptive transfer learning perspective on classification in\n  non-stationary environments", "author": "Henry W J Reeve", "abstract": "  We consider a semi-supervised classification problem with non-stationary\nlabel-shift in which we observe a labelled data set followed by a sequence of\nunlabelled covariate vectors in which the marginal probabilities of the class\nlabels may change over time. Our objective is to predict the corresponding\nclass-label for each covariate vector, without ever observing the ground-truth\nlabels, beyond the initial labelled data set. Previous work has demonstrated\nthe potential of sophisticated variants of online gradient descent to perform\ncompetitively with the optimal dynamic strategy (Bai et al. 2022). In this work\nwe explore an alternative approach grounded in statistical methods for adaptive\ntransfer learning. We demonstrate the merits of this alternative methodology by\nestablishing a high-probability regret bound on the test error at any given\nindividual test-time, which adapt automatically to the unknown dynamics of the\nmarginal label probabilities. Further more, we give bounds on the average\ndynamic regret which match the average guarantees of the online learning\nperspective for any given time interval.\n", "link": "http://arxiv.org/abs/2405.18091v1", "date": "2024-05-28", "relevancy": 2.1716, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5774}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.536}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20adaptive%20transfer%20learning%20perspective%20on%20classification%20in%0A%20%20non-stationary%20environments&body=Title%3A%20An%20adaptive%20transfer%20learning%20perspective%20on%20classification%20in%0A%20%20non-stationary%20environments%0AAuthor%3A%20Henry%20W%20J%20Reeve%0AAbstract%3A%20%20%20We%20consider%20a%20semi-supervised%20classification%20problem%20with%20non-stationary%0Alabel-shift%20in%20which%20we%20observe%20a%20labelled%20data%20set%20followed%20by%20a%20sequence%20of%0Aunlabelled%20covariate%20vectors%20in%20which%20the%20marginal%20probabilities%20of%20the%20class%0Alabels%20may%20change%20over%20time.%20Our%20objective%20is%20to%20predict%20the%20corresponding%0Aclass-label%20for%20each%20covariate%20vector%2C%20without%20ever%20observing%20the%20ground-truth%0Alabels%2C%20beyond%20the%20initial%20labelled%20data%20set.%20Previous%20work%20has%20demonstrated%0Athe%20potential%20of%20sophisticated%20variants%20of%20online%20gradient%20descent%20to%20perform%0Acompetitively%20with%20the%20optimal%20dynamic%20strategy%20%28Bai%20et%20al.%202022%29.%20In%20this%20work%0Awe%20explore%20an%20alternative%20approach%20grounded%20in%20statistical%20methods%20for%20adaptive%0Atransfer%20learning.%20We%20demonstrate%20the%20merits%20of%20this%20alternative%20methodology%20by%0Aestablishing%20a%20high-probability%20regret%20bound%20on%20the%20test%20error%20at%20any%20given%0Aindividual%20test-time%2C%20which%20adapt%20automatically%20to%20the%20unknown%20dynamics%20of%20the%0Amarginal%20label%20probabilities.%20Further%20more%2C%20we%20give%20bounds%20on%20the%20average%0Adynamic%20regret%20which%20match%20the%20average%20guarantees%20of%20the%20online%20learning%0Aperspective%20for%20any%20given%20time%20interval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520adaptive%2520transfer%2520learning%2520perspective%2520on%2520classification%2520in%250A%2520%2520non-stationary%2520environments%26entry.906535625%3DHenry%2520W%2520J%2520Reeve%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520semi-supervised%2520classification%2520problem%2520with%2520non-stationary%250Alabel-shift%2520in%2520which%2520we%2520observe%2520a%2520labelled%2520data%2520set%2520followed%2520by%2520a%2520sequence%2520of%250Aunlabelled%2520covariate%2520vectors%2520in%2520which%2520the%2520marginal%2520probabilities%2520of%2520the%2520class%250Alabels%2520may%2520change%2520over%2520time.%2520Our%2520objective%2520is%2520to%2520predict%2520the%2520corresponding%250Aclass-label%2520for%2520each%2520covariate%2520vector%252C%2520without%2520ever%2520observing%2520the%2520ground-truth%250Alabels%252C%2520beyond%2520the%2520initial%2520labelled%2520data%2520set.%2520Previous%2520work%2520has%2520demonstrated%250Athe%2520potential%2520of%2520sophisticated%2520variants%2520of%2520online%2520gradient%2520descent%2520to%2520perform%250Acompetitively%2520with%2520the%2520optimal%2520dynamic%2520strategy%2520%2528Bai%2520et%2520al.%25202022%2529.%2520In%2520this%2520work%250Awe%2520explore%2520an%2520alternative%2520approach%2520grounded%2520in%2520statistical%2520methods%2520for%2520adaptive%250Atransfer%2520learning.%2520We%2520demonstrate%2520the%2520merits%2520of%2520this%2520alternative%2520methodology%2520by%250Aestablishing%2520a%2520high-probability%2520regret%2520bound%2520on%2520the%2520test%2520error%2520at%2520any%2520given%250Aindividual%2520test-time%252C%2520which%2520adapt%2520automatically%2520to%2520the%2520unknown%2520dynamics%2520of%2520the%250Amarginal%2520label%2520probabilities.%2520Further%2520more%252C%2520we%2520give%2520bounds%2520on%2520the%2520average%250Adynamic%2520regret%2520which%2520match%2520the%2520average%2520guarantees%2520of%2520the%2520online%2520learning%250Aperspective%2520for%2520any%2520given%2520time%2520interval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20adaptive%20transfer%20learning%20perspective%20on%20classification%20in%0A%20%20non-stationary%20environments&entry.906535625=Henry%20W%20J%20Reeve&entry.1292438233=%20%20We%20consider%20a%20semi-supervised%20classification%20problem%20with%20non-stationary%0Alabel-shift%20in%20which%20we%20observe%20a%20labelled%20data%20set%20followed%20by%20a%20sequence%20of%0Aunlabelled%20covariate%20vectors%20in%20which%20the%20marginal%20probabilities%20of%20the%20class%0Alabels%20may%20change%20over%20time.%20Our%20objective%20is%20to%20predict%20the%20corresponding%0Aclass-label%20for%20each%20covariate%20vector%2C%20without%20ever%20observing%20the%20ground-truth%0Alabels%2C%20beyond%20the%20initial%20labelled%20data%20set.%20Previous%20work%20has%20demonstrated%0Athe%20potential%20of%20sophisticated%20variants%20of%20online%20gradient%20descent%20to%20perform%0Acompetitively%20with%20the%20optimal%20dynamic%20strategy%20%28Bai%20et%20al.%202022%29.%20In%20this%20work%0Awe%20explore%20an%20alternative%20approach%20grounded%20in%20statistical%20methods%20for%20adaptive%0Atransfer%20learning.%20We%20demonstrate%20the%20merits%20of%20this%20alternative%20methodology%20by%0Aestablishing%20a%20high-probability%20regret%20bound%20on%20the%20test%20error%20at%20any%20given%0Aindividual%20test-time%2C%20which%20adapt%20automatically%20to%20the%20unknown%20dynamics%20of%20the%0Amarginal%20label%20probabilities.%20Further%20more%2C%20we%20give%20bounds%20on%20the%20average%0Adynamic%20regret%20which%20match%20the%20average%20guarantees%20of%20the%20online%20learning%0Aperspective%20for%20any%20given%20time%20interval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18091v1&entry.124074799=Read"},
{"title": "Visualizing the loss landscape of Self-supervised Vision Transformer", "author": "Youngwan Lee and Jeffrey Ryan Willette and Jonghee Kim and Sung Ju Hwang", "abstract": "  The Masked autoencoder (MAE) has drawn attention as a representative\nself-supervised approach for masked image modeling with vision transformers.\nHowever, even though MAE shows better generalization capability than fully\nsupervised training from scratch, the reason why has not been explored. In\nanother line of work, the Reconstruction Consistent Masked Auto Encoder\n(RC-MAE), has been proposed which adopts a self-distillation scheme in the form\nof an exponential moving average (EMA) teacher into MAE, and it has been shown\nthat the EMA-teacher performs a conditional gradient correction during\noptimization. To further investigate the reason for better generalization of\nthe self-supervised ViT when trained by MAE (MAE-ViT) and the effect of the\ngradient correction of RC-MAE from the perspective of optimization, we\nvisualize the loss landscapes of the self-supervised vision transformer by both\nMAE and RC-MAE and compare them with the supervised ViT (Sup-ViT). Unlike\nprevious loss landscape visualizations of neural networks based on\nclassification task loss, we visualize the loss landscape of ViT by computing\npre-training task loss. Through the lens of loss landscapes, we find two\ninteresting observations: (1) MAE-ViT has a smoother and wider overall loss\ncurvature than Sup-ViT. (2) The EMA-teacher allows MAE to widen the region of\nconvexity in both pretraining and linear probing, leading to quicker\nconvergence. To the best of our knowledge, this work is the first to\ninvestigate the self-supervised ViT through the lens of the loss landscape.\n", "link": "http://arxiv.org/abs/2405.18042v1", "date": "2024-05-28", "relevancy": 2.1658, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5652}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5466}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visualizing%20the%20loss%20landscape%20of%20Self-supervised%20Vision%20Transformer&body=Title%3A%20Visualizing%20the%20loss%20landscape%20of%20Self-supervised%20Vision%20Transformer%0AAuthor%3A%20Youngwan%20Lee%20and%20Jeffrey%20Ryan%20Willette%20and%20Jonghee%20Kim%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20The%20Masked%20autoencoder%20%28MAE%29%20has%20drawn%20attention%20as%20a%20representative%0Aself-supervised%20approach%20for%20masked%20image%20modeling%20with%20vision%20transformers.%0AHowever%2C%20even%20though%20MAE%20shows%20better%20generalization%20capability%20than%20fully%0Asupervised%20training%20from%20scratch%2C%20the%20reason%20why%20has%20not%20been%20explored.%20In%0Aanother%20line%20of%20work%2C%20the%20Reconstruction%20Consistent%20Masked%20Auto%20Encoder%0A%28RC-MAE%29%2C%20has%20been%20proposed%20which%20adopts%20a%20self-distillation%20scheme%20in%20the%20form%0Aof%20an%20exponential%20moving%20average%20%28EMA%29%20teacher%20into%20MAE%2C%20and%20it%20has%20been%20shown%0Athat%20the%20EMA-teacher%20performs%20a%20conditional%20gradient%20correction%20during%0Aoptimization.%20To%20further%20investigate%20the%20reason%20for%20better%20generalization%20of%0Athe%20self-supervised%20ViT%20when%20trained%20by%20MAE%20%28MAE-ViT%29%20and%20the%20effect%20of%20the%0Agradient%20correction%20of%20RC-MAE%20from%20the%20perspective%20of%20optimization%2C%20we%0Avisualize%20the%20loss%20landscapes%20of%20the%20self-supervised%20vision%20transformer%20by%20both%0AMAE%20and%20RC-MAE%20and%20compare%20them%20with%20the%20supervised%20ViT%20%28Sup-ViT%29.%20Unlike%0Aprevious%20loss%20landscape%20visualizations%20of%20neural%20networks%20based%20on%0Aclassification%20task%20loss%2C%20we%20visualize%20the%20loss%20landscape%20of%20ViT%20by%20computing%0Apre-training%20task%20loss.%20Through%20the%20lens%20of%20loss%20landscapes%2C%20we%20find%20two%0Ainteresting%20observations%3A%20%281%29%20MAE-ViT%20has%20a%20smoother%20and%20wider%20overall%20loss%0Acurvature%20than%20Sup-ViT.%20%282%29%20The%20EMA-teacher%20allows%20MAE%20to%20widen%20the%20region%20of%0Aconvexity%20in%20both%20pretraining%20and%20linear%20probing%2C%20leading%20to%20quicker%0Aconvergence.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%0Ainvestigate%20the%20self-supervised%20ViT%20through%20the%20lens%20of%20the%20loss%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualizing%2520the%2520loss%2520landscape%2520of%2520Self-supervised%2520Vision%2520Transformer%26entry.906535625%3DYoungwan%2520Lee%2520and%2520Jeffrey%2520Ryan%2520Willette%2520and%2520Jonghee%2520Kim%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3D%2520%2520The%2520Masked%2520autoencoder%2520%2528MAE%2529%2520has%2520drawn%2520attention%2520as%2520a%2520representative%250Aself-supervised%2520approach%2520for%2520masked%2520image%2520modeling%2520with%2520vision%2520transformers.%250AHowever%252C%2520even%2520though%2520MAE%2520shows%2520better%2520generalization%2520capability%2520than%2520fully%250Asupervised%2520training%2520from%2520scratch%252C%2520the%2520reason%2520why%2520has%2520not%2520been%2520explored.%2520In%250Aanother%2520line%2520of%2520work%252C%2520the%2520Reconstruction%2520Consistent%2520Masked%2520Auto%2520Encoder%250A%2528RC-MAE%2529%252C%2520has%2520been%2520proposed%2520which%2520adopts%2520a%2520self-distillation%2520scheme%2520in%2520the%2520form%250Aof%2520an%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520teacher%2520into%2520MAE%252C%2520and%2520it%2520has%2520been%2520shown%250Athat%2520the%2520EMA-teacher%2520performs%2520a%2520conditional%2520gradient%2520correction%2520during%250Aoptimization.%2520To%2520further%2520investigate%2520the%2520reason%2520for%2520better%2520generalization%2520of%250Athe%2520self-supervised%2520ViT%2520when%2520trained%2520by%2520MAE%2520%2528MAE-ViT%2529%2520and%2520the%2520effect%2520of%2520the%250Agradient%2520correction%2520of%2520RC-MAE%2520from%2520the%2520perspective%2520of%2520optimization%252C%2520we%250Avisualize%2520the%2520loss%2520landscapes%2520of%2520the%2520self-supervised%2520vision%2520transformer%2520by%2520both%250AMAE%2520and%2520RC-MAE%2520and%2520compare%2520them%2520with%2520the%2520supervised%2520ViT%2520%2528Sup-ViT%2529.%2520Unlike%250Aprevious%2520loss%2520landscape%2520visualizations%2520of%2520neural%2520networks%2520based%2520on%250Aclassification%2520task%2520loss%252C%2520we%2520visualize%2520the%2520loss%2520landscape%2520of%2520ViT%2520by%2520computing%250Apre-training%2520task%2520loss.%2520Through%2520the%2520lens%2520of%2520loss%2520landscapes%252C%2520we%2520find%2520two%250Ainteresting%2520observations%253A%2520%25281%2529%2520MAE-ViT%2520has%2520a%2520smoother%2520and%2520wider%2520overall%2520loss%250Acurvature%2520than%2520Sup-ViT.%2520%25282%2529%2520The%2520EMA-teacher%2520allows%2520MAE%2520to%2520widen%2520the%2520region%2520of%250Aconvexity%2520in%2520both%2520pretraining%2520and%2520linear%2520probing%252C%2520leading%2520to%2520quicker%250Aconvergence.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520is%2520the%2520first%2520to%250Ainvestigate%2520the%2520self-supervised%2520ViT%2520through%2520the%2520lens%2520of%2520the%2520loss%2520landscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visualizing%20the%20loss%20landscape%20of%20Self-supervised%20Vision%20Transformer&entry.906535625=Youngwan%20Lee%20and%20Jeffrey%20Ryan%20Willette%20and%20Jonghee%20Kim%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20The%20Masked%20autoencoder%20%28MAE%29%20has%20drawn%20attention%20as%20a%20representative%0Aself-supervised%20approach%20for%20masked%20image%20modeling%20with%20vision%20transformers.%0AHowever%2C%20even%20though%20MAE%20shows%20better%20generalization%20capability%20than%20fully%0Asupervised%20training%20from%20scratch%2C%20the%20reason%20why%20has%20not%20been%20explored.%20In%0Aanother%20line%20of%20work%2C%20the%20Reconstruction%20Consistent%20Masked%20Auto%20Encoder%0A%28RC-MAE%29%2C%20has%20been%20proposed%20which%20adopts%20a%20self-distillation%20scheme%20in%20the%20form%0Aof%20an%20exponential%20moving%20average%20%28EMA%29%20teacher%20into%20MAE%2C%20and%20it%20has%20been%20shown%0Athat%20the%20EMA-teacher%20performs%20a%20conditional%20gradient%20correction%20during%0Aoptimization.%20To%20further%20investigate%20the%20reason%20for%20better%20generalization%20of%0Athe%20self-supervised%20ViT%20when%20trained%20by%20MAE%20%28MAE-ViT%29%20and%20the%20effect%20of%20the%0Agradient%20correction%20of%20RC-MAE%20from%20the%20perspective%20of%20optimization%2C%20we%0Avisualize%20the%20loss%20landscapes%20of%20the%20self-supervised%20vision%20transformer%20by%20both%0AMAE%20and%20RC-MAE%20and%20compare%20them%20with%20the%20supervised%20ViT%20%28Sup-ViT%29.%20Unlike%0Aprevious%20loss%20landscape%20visualizations%20of%20neural%20networks%20based%20on%0Aclassification%20task%20loss%2C%20we%20visualize%20the%20loss%20landscape%20of%20ViT%20by%20computing%0Apre-training%20task%20loss.%20Through%20the%20lens%20of%20loss%20landscapes%2C%20we%20find%20two%0Ainteresting%20observations%3A%20%281%29%20MAE-ViT%20has%20a%20smoother%20and%20wider%20overall%20loss%0Acurvature%20than%20Sup-ViT.%20%282%29%20The%20EMA-teacher%20allows%20MAE%20to%20widen%20the%20region%20of%0Aconvexity%20in%20both%20pretraining%20and%20linear%20probing%2C%20leading%20to%20quicker%0Aconvergence.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%0Ainvestigate%20the%20self-supervised%20ViT%20through%20the%20lens%20of%20the%20loss%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18042v1&entry.124074799=Read"},
{"title": "STELLA: Continual Audio-Video Pre-training with Spatio-Temporal\n  Localized Alignment", "author": "Jaewoo Lee and Jaehong Yoon and Wonjae Kim and Yunji Kim and Sung Ju Hwang", "abstract": "  Continuously learning a variety of audio-video semantics over time is crucial\nfor audio-related reasoning tasks in our ever-evolving world. However, this is\na nontrivial problem and poses two critical challenges: sparse spatio-temporal\ncorrelation between audio-video pairs and multimodal correlation overwriting\nthat forgets audio-video relations. To tackle this problem, we propose a new\ncontinual audio-video pre-training method with two novel ideas: (1) Localized\nPatch Importance Scoring: we introduce a multimodal encoder to determine the\nimportance score for each patch, emphasizing semantically intertwined\naudio-video patches. (2) Replay-guided Correlation Assessment: to reduce the\ncorruption of previously learned audiovisual knowledge due to drift, we propose\nto assess the correlation of the current patches on the past steps to identify\nthe patches exhibiting high correlations with the past steps. Based on the\nresults from the two ideas, we perform probabilistic patch selection for\neffective continual audio-video pre-training. Experimental validation on\nmultiple benchmarks shows that our method achieves a 3.69%p of relative\nperformance gain in zero-shot retrieval tasks compared to strong continual\nlearning baselines, while reducing memory consumption by ~45%.\n", "link": "http://arxiv.org/abs/2310.08204v3", "date": "2024-05-28", "relevancy": 2.1626, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5612}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5272}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STELLA%3A%20Continual%20Audio-Video%20Pre-training%20with%20Spatio-Temporal%0A%20%20Localized%20Alignment&body=Title%3A%20STELLA%3A%20Continual%20Audio-Video%20Pre-training%20with%20Spatio-Temporal%0A%20%20Localized%20Alignment%0AAuthor%3A%20Jaewoo%20Lee%20and%20Jaehong%20Yoon%20and%20Wonjae%20Kim%20and%20Yunji%20Kim%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20Continuously%20learning%20a%20variety%20of%20audio-video%20semantics%20over%20time%20is%20crucial%0Afor%20audio-related%20reasoning%20tasks%20in%20our%20ever-evolving%20world.%20However%2C%20this%20is%0Aa%20nontrivial%20problem%20and%20poses%20two%20critical%20challenges%3A%20sparse%20spatio-temporal%0Acorrelation%20between%20audio-video%20pairs%20and%20multimodal%20correlation%20overwriting%0Athat%20forgets%20audio-video%20relations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20new%0Acontinual%20audio-video%20pre-training%20method%20with%20two%20novel%20ideas%3A%20%281%29%20Localized%0APatch%20Importance%20Scoring%3A%20we%20introduce%20a%20multimodal%20encoder%20to%20determine%20the%0Aimportance%20score%20for%20each%20patch%2C%20emphasizing%20semantically%20intertwined%0Aaudio-video%20patches.%20%282%29%20Replay-guided%20Correlation%20Assessment%3A%20to%20reduce%20the%0Acorruption%20of%20previously%20learned%20audiovisual%20knowledge%20due%20to%20drift%2C%20we%20propose%0Ato%20assess%20the%20correlation%20of%20the%20current%20patches%20on%20the%20past%20steps%20to%20identify%0Athe%20patches%20exhibiting%20high%20correlations%20with%20the%20past%20steps.%20Based%20on%20the%0Aresults%20from%20the%20two%20ideas%2C%20we%20perform%20probabilistic%20patch%20selection%20for%0Aeffective%20continual%20audio-video%20pre-training.%20Experimental%20validation%20on%0Amultiple%20benchmarks%20shows%20that%20our%20method%20achieves%20a%203.69%25p%20of%20relative%0Aperformance%20gain%20in%20zero-shot%20retrieval%20tasks%20compared%20to%20strong%20continual%0Alearning%20baselines%2C%20while%20reducing%20memory%20consumption%20by%20~45%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08204v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTELLA%253A%2520Continual%2520Audio-Video%2520Pre-training%2520with%2520Spatio-Temporal%250A%2520%2520Localized%2520Alignment%26entry.906535625%3DJaewoo%2520Lee%2520and%2520Jaehong%2520Yoon%2520and%2520Wonjae%2520Kim%2520and%2520Yunji%2520Kim%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3D%2520%2520Continuously%2520learning%2520a%2520variety%2520of%2520audio-video%2520semantics%2520over%2520time%2520is%2520crucial%250Afor%2520audio-related%2520reasoning%2520tasks%2520in%2520our%2520ever-evolving%2520world.%2520However%252C%2520this%2520is%250Aa%2520nontrivial%2520problem%2520and%2520poses%2520two%2520critical%2520challenges%253A%2520sparse%2520spatio-temporal%250Acorrelation%2520between%2520audio-video%2520pairs%2520and%2520multimodal%2520correlation%2520overwriting%250Athat%2520forgets%2520audio-video%2520relations.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520a%2520new%250Acontinual%2520audio-video%2520pre-training%2520method%2520with%2520two%2520novel%2520ideas%253A%2520%25281%2529%2520Localized%250APatch%2520Importance%2520Scoring%253A%2520we%2520introduce%2520a%2520multimodal%2520encoder%2520to%2520determine%2520the%250Aimportance%2520score%2520for%2520each%2520patch%252C%2520emphasizing%2520semantically%2520intertwined%250Aaudio-video%2520patches.%2520%25282%2529%2520Replay-guided%2520Correlation%2520Assessment%253A%2520to%2520reduce%2520the%250Acorruption%2520of%2520previously%2520learned%2520audiovisual%2520knowledge%2520due%2520to%2520drift%252C%2520we%2520propose%250Ato%2520assess%2520the%2520correlation%2520of%2520the%2520current%2520patches%2520on%2520the%2520past%2520steps%2520to%2520identify%250Athe%2520patches%2520exhibiting%2520high%2520correlations%2520with%2520the%2520past%2520steps.%2520Based%2520on%2520the%250Aresults%2520from%2520the%2520two%2520ideas%252C%2520we%2520perform%2520probabilistic%2520patch%2520selection%2520for%250Aeffective%2520continual%2520audio-video%2520pre-training.%2520Experimental%2520validation%2520on%250Amultiple%2520benchmarks%2520shows%2520that%2520our%2520method%2520achieves%2520a%25203.69%2525p%2520of%2520relative%250Aperformance%2520gain%2520in%2520zero-shot%2520retrieval%2520tasks%2520compared%2520to%2520strong%2520continual%250Alearning%2520baselines%252C%2520while%2520reducing%2520memory%2520consumption%2520by%2520~45%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08204v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STELLA%3A%20Continual%20Audio-Video%20Pre-training%20with%20Spatio-Temporal%0A%20%20Localized%20Alignment&entry.906535625=Jaewoo%20Lee%20and%20Jaehong%20Yoon%20and%20Wonjae%20Kim%20and%20Yunji%20Kim%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20Continuously%20learning%20a%20variety%20of%20audio-video%20semantics%20over%20time%20is%20crucial%0Afor%20audio-related%20reasoning%20tasks%20in%20our%20ever-evolving%20world.%20However%2C%20this%20is%0Aa%20nontrivial%20problem%20and%20poses%20two%20critical%20challenges%3A%20sparse%20spatio-temporal%0Acorrelation%20between%20audio-video%20pairs%20and%20multimodal%20correlation%20overwriting%0Athat%20forgets%20audio-video%20relations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20new%0Acontinual%20audio-video%20pre-training%20method%20with%20two%20novel%20ideas%3A%20%281%29%20Localized%0APatch%20Importance%20Scoring%3A%20we%20introduce%20a%20multimodal%20encoder%20to%20determine%20the%0Aimportance%20score%20for%20each%20patch%2C%20emphasizing%20semantically%20intertwined%0Aaudio-video%20patches.%20%282%29%20Replay-guided%20Correlation%20Assessment%3A%20to%20reduce%20the%0Acorruption%20of%20previously%20learned%20audiovisual%20knowledge%20due%20to%20drift%2C%20we%20propose%0Ato%20assess%20the%20correlation%20of%20the%20current%20patches%20on%20the%20past%20steps%20to%20identify%0Athe%20patches%20exhibiting%20high%20correlations%20with%20the%20past%20steps.%20Based%20on%20the%0Aresults%20from%20the%20two%20ideas%2C%20we%20perform%20probabilistic%20patch%20selection%20for%0Aeffective%20continual%20audio-video%20pre-training.%20Experimental%20validation%20on%0Amultiple%20benchmarks%20shows%20that%20our%20method%20achieves%20a%203.69%25p%20of%20relative%0Aperformance%20gain%20in%20zero-shot%20retrieval%20tasks%20compared%20to%20strong%20continual%0Alearning%20baselines%2C%20while%20reducing%20memory%20consumption%20by%20~45%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08204v3&entry.124074799=Read"},
{"title": "Memory in Plain Sight: Surveying the Uncanny Resemblances of Associative\n  Memories and Diffusion Models", "author": "Benjamin Hoover and Hendrik Strobelt and Dmitry Krotov and Judy Hoffman and Zsolt Kira and Duen Horng Chau", "abstract": "  The generative process of Diffusion Models (DMs) has recently set\nstate-of-the-art on many AI generation benchmarks. Though the generative\nprocess is traditionally understood as an \"iterative denoiser\", there is no\nuniversally accepted language to describe it. We introduce a novel perspective\nto describe DMs using the mathematical language of memory retrieval from the\nfield of energy-based Associative Memories (AMs), making efforts to keep our\npresentation approachable to newcomers to both of these fields. Unifying these\ntwo fields provides insight that DMs can be seen as a particular kind of AM\nwhere Lyapunov stability guarantees are bypassed by intelligently engineering\nthe dynamics (i.e., the noise and step size schedules) of the denoising\nprocess. Finally, we present a growing body of evidence that records DMs\nexhibiting empirical behavior we would expect from AMs, and conclude by\ndiscussing research opportunities that are revealed by understanding DMs as a\nform of energy-based memory.\n", "link": "http://arxiv.org/abs/2309.16750v2", "date": "2024-05-28", "relevancy": 2.1502, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5571}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.535}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20in%20Plain%20Sight%3A%20Surveying%20the%20Uncanny%20Resemblances%20of%20Associative%0A%20%20Memories%20and%20Diffusion%20Models&body=Title%3A%20Memory%20in%20Plain%20Sight%3A%20Surveying%20the%20Uncanny%20Resemblances%20of%20Associative%0A%20%20Memories%20and%20Diffusion%20Models%0AAuthor%3A%20Benjamin%20Hoover%20and%20Hendrik%20Strobelt%20and%20Dmitry%20Krotov%20and%20Judy%20Hoffman%20and%20Zsolt%20Kira%20and%20Duen%20Horng%20Chau%0AAbstract%3A%20%20%20The%20generative%20process%20of%20Diffusion%20Models%20%28DMs%29%20has%20recently%20set%0Astate-of-the-art%20on%20many%20AI%20generation%20benchmarks.%20Though%20the%20generative%0Aprocess%20is%20traditionally%20understood%20as%20an%20%22iterative%20denoiser%22%2C%20there%20is%20no%0Auniversally%20accepted%20language%20to%20describe%20it.%20We%20introduce%20a%20novel%20perspective%0Ato%20describe%20DMs%20using%20the%20mathematical%20language%20of%20memory%20retrieval%20from%20the%0Afield%20of%20energy-based%20Associative%20Memories%20%28AMs%29%2C%20making%20efforts%20to%20keep%20our%0Apresentation%20approachable%20to%20newcomers%20to%20both%20of%20these%20fields.%20Unifying%20these%0Atwo%20fields%20provides%20insight%20that%20DMs%20can%20be%20seen%20as%20a%20particular%20kind%20of%20AM%0Awhere%20Lyapunov%20stability%20guarantees%20are%20bypassed%20by%20intelligently%20engineering%0Athe%20dynamics%20%28i.e.%2C%20the%20noise%20and%20step%20size%20schedules%29%20of%20the%20denoising%0Aprocess.%20Finally%2C%20we%20present%20a%20growing%20body%20of%20evidence%20that%20records%20DMs%0Aexhibiting%20empirical%20behavior%20we%20would%20expect%20from%20AMs%2C%20and%20conclude%20by%0Adiscussing%20research%20opportunities%20that%20are%20revealed%20by%20understanding%20DMs%20as%20a%0Aform%20of%20energy-based%20memory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16750v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520in%2520Plain%2520Sight%253A%2520Surveying%2520the%2520Uncanny%2520Resemblances%2520of%2520Associative%250A%2520%2520Memories%2520and%2520Diffusion%2520Models%26entry.906535625%3DBenjamin%2520Hoover%2520and%2520Hendrik%2520Strobelt%2520and%2520Dmitry%2520Krotov%2520and%2520Judy%2520Hoffman%2520and%2520Zsolt%2520Kira%2520and%2520Duen%2520Horng%2520Chau%26entry.1292438233%3D%2520%2520The%2520generative%2520process%2520of%2520Diffusion%2520Models%2520%2528DMs%2529%2520has%2520recently%2520set%250Astate-of-the-art%2520on%2520many%2520AI%2520generation%2520benchmarks.%2520Though%2520the%2520generative%250Aprocess%2520is%2520traditionally%2520understood%2520as%2520an%2520%2522iterative%2520denoiser%2522%252C%2520there%2520is%2520no%250Auniversally%2520accepted%2520language%2520to%2520describe%2520it.%2520We%2520introduce%2520a%2520novel%2520perspective%250Ato%2520describe%2520DMs%2520using%2520the%2520mathematical%2520language%2520of%2520memory%2520retrieval%2520from%2520the%250Afield%2520of%2520energy-based%2520Associative%2520Memories%2520%2528AMs%2529%252C%2520making%2520efforts%2520to%2520keep%2520our%250Apresentation%2520approachable%2520to%2520newcomers%2520to%2520both%2520of%2520these%2520fields.%2520Unifying%2520these%250Atwo%2520fields%2520provides%2520insight%2520that%2520DMs%2520can%2520be%2520seen%2520as%2520a%2520particular%2520kind%2520of%2520AM%250Awhere%2520Lyapunov%2520stability%2520guarantees%2520are%2520bypassed%2520by%2520intelligently%2520engineering%250Athe%2520dynamics%2520%2528i.e.%252C%2520the%2520noise%2520and%2520step%2520size%2520schedules%2529%2520of%2520the%2520denoising%250Aprocess.%2520Finally%252C%2520we%2520present%2520a%2520growing%2520body%2520of%2520evidence%2520that%2520records%2520DMs%250Aexhibiting%2520empirical%2520behavior%2520we%2520would%2520expect%2520from%2520AMs%252C%2520and%2520conclude%2520by%250Adiscussing%2520research%2520opportunities%2520that%2520are%2520revealed%2520by%2520understanding%2520DMs%2520as%2520a%250Aform%2520of%2520energy-based%2520memory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16750v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20in%20Plain%20Sight%3A%20Surveying%20the%20Uncanny%20Resemblances%20of%20Associative%0A%20%20Memories%20and%20Diffusion%20Models&entry.906535625=Benjamin%20Hoover%20and%20Hendrik%20Strobelt%20and%20Dmitry%20Krotov%20and%20Judy%20Hoffman%20and%20Zsolt%20Kira%20and%20Duen%20Horng%20Chau&entry.1292438233=%20%20The%20generative%20process%20of%20Diffusion%20Models%20%28DMs%29%20has%20recently%20set%0Astate-of-the-art%20on%20many%20AI%20generation%20benchmarks.%20Though%20the%20generative%0Aprocess%20is%20traditionally%20understood%20as%20an%20%22iterative%20denoiser%22%2C%20there%20is%20no%0Auniversally%20accepted%20language%20to%20describe%20it.%20We%20introduce%20a%20novel%20perspective%0Ato%20describe%20DMs%20using%20the%20mathematical%20language%20of%20memory%20retrieval%20from%20the%0Afield%20of%20energy-based%20Associative%20Memories%20%28AMs%29%2C%20making%20efforts%20to%20keep%20our%0Apresentation%20approachable%20to%20newcomers%20to%20both%20of%20these%20fields.%20Unifying%20these%0Atwo%20fields%20provides%20insight%20that%20DMs%20can%20be%20seen%20as%20a%20particular%20kind%20of%20AM%0Awhere%20Lyapunov%20stability%20guarantees%20are%20bypassed%20by%20intelligently%20engineering%0Athe%20dynamics%20%28i.e.%2C%20the%20noise%20and%20step%20size%20schedules%29%20of%20the%20denoising%0Aprocess.%20Finally%2C%20we%20present%20a%20growing%20body%20of%20evidence%20that%20records%20DMs%0Aexhibiting%20empirical%20behavior%20we%20would%20expect%20from%20AMs%2C%20and%20conclude%20by%0Adiscussing%20research%20opportunities%20that%20are%20revealed%20by%20understanding%20DMs%20as%20a%0Aform%20of%20energy-based%20memory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16750v2&entry.124074799=Read"},
{"title": "Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with\n  LLMs for Multi-modal Text Recognition", "author": "Chan-Jan Hsu and Yi-Chang Chen and Feng-Ting Liao and Pei-Chen Ho and Yu-Hsiang Wang and Po-Chun Hsu and Da-shan Shiu", "abstract": "  We introduce \"Generative Fusion Decoding\" (GFD), a novel shallow fusion\nframework, utilized to integrate Large Language Models (LLMs) into multi-modal\ntext recognition systems such as automatic speech recognition (ASR) and optical\ncharacter recognition (OCR). We derive the formulas necessary to enable GFD to\noperate across mismatched token spaces of different models by mapping text\ntoken space to byte token space, enabling seamless fusion during the decoding\nprocess. The framework is plug-and-play, compatible with various\nauto-regressive models, and does not require re-training for feature alignment,\nthus overcoming limitations of previous fusion techniques. We highlight three\nmain advantages of GFD: First, by simplifying the complexity of aligning\ndifferent model sample spaces, GFD allows LLMs to correct errors in tandem with\nthe recognition model, reducing computation latencies. Second, the in-context\nlearning ability of LLMs is fully capitalized by GFD, increasing robustness in\nlong-form speech recognition and instruction aware speech recognition. Third,\nGFD enables fusing recognition models deficient in Chinese text recognition\nwith LLMs extensively trained on Chinese. Our evaluation demonstrates that GFD\nsignificantly improves performance in ASR and OCR tasks, with ASR reaching\nstate-of-the-art in the NTUML2021 benchmark. GFD provides a significant step\nforward in model integration, offering a unified solution that could be widely\napplicable to leveraging existing pre-trained models through step by step\nfusion.\n", "link": "http://arxiv.org/abs/2405.14259v2", "date": "2024-05-28", "relevancy": 2.1481, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5596}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5356}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Fuse%20Step%20by%20Step%3A%20A%20Generative%20Fusion%20Decoding%20Algorithm%20with%0A%20%20LLMs%20for%20Multi-modal%20Text%20Recognition&body=Title%3A%20Let%27s%20Fuse%20Step%20by%20Step%3A%20A%20Generative%20Fusion%20Decoding%20Algorithm%20with%0A%20%20LLMs%20for%20Multi-modal%20Text%20Recognition%0AAuthor%3A%20Chan-Jan%20Hsu%20and%20Yi-Chang%20Chen%20and%20Feng-Ting%20Liao%20and%20Pei-Chen%20Ho%20and%20Yu-Hsiang%20Wang%20and%20Po-Chun%20Hsu%20and%20Da-shan%20Shiu%0AAbstract%3A%20%20%20We%20introduce%20%22Generative%20Fusion%20Decoding%22%20%28GFD%29%2C%20a%20novel%20shallow%20fusion%0Aframework%2C%20utilized%20to%20integrate%20Large%20Language%20Models%20%28LLMs%29%20into%20multi-modal%0Atext%20recognition%20systems%20such%20as%20automatic%20speech%20recognition%20%28ASR%29%20and%20optical%0Acharacter%20recognition%20%28OCR%29.%20We%20derive%20the%20formulas%20necessary%20to%20enable%20GFD%20to%0Aoperate%20across%20mismatched%20token%20spaces%20of%20different%20models%20by%20mapping%20text%0Atoken%20space%20to%20byte%20token%20space%2C%20enabling%20seamless%20fusion%20during%20the%20decoding%0Aprocess.%20The%20framework%20is%20plug-and-play%2C%20compatible%20with%20various%0Aauto-regressive%20models%2C%20and%20does%20not%20require%20re-training%20for%20feature%20alignment%2C%0Athus%20overcoming%20limitations%20of%20previous%20fusion%20techniques.%20We%20highlight%20three%0Amain%20advantages%20of%20GFD%3A%20First%2C%20by%20simplifying%20the%20complexity%20of%20aligning%0Adifferent%20model%20sample%20spaces%2C%20GFD%20allows%20LLMs%20to%20correct%20errors%20in%20tandem%20with%0Athe%20recognition%20model%2C%20reducing%20computation%20latencies.%20Second%2C%20the%20in-context%0Alearning%20ability%20of%20LLMs%20is%20fully%20capitalized%20by%20GFD%2C%20increasing%20robustness%20in%0Along-form%20speech%20recognition%20and%20instruction%20aware%20speech%20recognition.%20Third%2C%0AGFD%20enables%20fusing%20recognition%20models%20deficient%20in%20Chinese%20text%20recognition%0Awith%20LLMs%20extensively%20trained%20on%20Chinese.%20Our%20evaluation%20demonstrates%20that%20GFD%0Asignificantly%20improves%20performance%20in%20ASR%20and%20OCR%20tasks%2C%20with%20ASR%20reaching%0Astate-of-the-art%20in%20the%20NTUML2021%20benchmark.%20GFD%20provides%20a%20significant%20step%0Aforward%20in%20model%20integration%2C%20offering%20a%20unified%20solution%20that%20could%20be%20widely%0Aapplicable%20to%20leveraging%20existing%20pre-trained%20models%20through%20step%20by%20step%0Afusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14259v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Fuse%2520Step%2520by%2520Step%253A%2520A%2520Generative%2520Fusion%2520Decoding%2520Algorithm%2520with%250A%2520%2520LLMs%2520for%2520Multi-modal%2520Text%2520Recognition%26entry.906535625%3DChan-Jan%2520Hsu%2520and%2520Yi-Chang%2520Chen%2520and%2520Feng-Ting%2520Liao%2520and%2520Pei-Chen%2520Ho%2520and%2520Yu-Hsiang%2520Wang%2520and%2520Po-Chun%2520Hsu%2520and%2520Da-shan%2520Shiu%26entry.1292438233%3D%2520%2520We%2520introduce%2520%2522Generative%2520Fusion%2520Decoding%2522%2520%2528GFD%2529%252C%2520a%2520novel%2520shallow%2520fusion%250Aframework%252C%2520utilized%2520to%2520integrate%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520into%2520multi-modal%250Atext%2520recognition%2520systems%2520such%2520as%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520and%2520optical%250Acharacter%2520recognition%2520%2528OCR%2529.%2520We%2520derive%2520the%2520formulas%2520necessary%2520to%2520enable%2520GFD%2520to%250Aoperate%2520across%2520mismatched%2520token%2520spaces%2520of%2520different%2520models%2520by%2520mapping%2520text%250Atoken%2520space%2520to%2520byte%2520token%2520space%252C%2520enabling%2520seamless%2520fusion%2520during%2520the%2520decoding%250Aprocess.%2520The%2520framework%2520is%2520plug-and-play%252C%2520compatible%2520with%2520various%250Aauto-regressive%2520models%252C%2520and%2520does%2520not%2520require%2520re-training%2520for%2520feature%2520alignment%252C%250Athus%2520overcoming%2520limitations%2520of%2520previous%2520fusion%2520techniques.%2520We%2520highlight%2520three%250Amain%2520advantages%2520of%2520GFD%253A%2520First%252C%2520by%2520simplifying%2520the%2520complexity%2520of%2520aligning%250Adifferent%2520model%2520sample%2520spaces%252C%2520GFD%2520allows%2520LLMs%2520to%2520correct%2520errors%2520in%2520tandem%2520with%250Athe%2520recognition%2520model%252C%2520reducing%2520computation%2520latencies.%2520Second%252C%2520the%2520in-context%250Alearning%2520ability%2520of%2520LLMs%2520is%2520fully%2520capitalized%2520by%2520GFD%252C%2520increasing%2520robustness%2520in%250Along-form%2520speech%2520recognition%2520and%2520instruction%2520aware%2520speech%2520recognition.%2520Third%252C%250AGFD%2520enables%2520fusing%2520recognition%2520models%2520deficient%2520in%2520Chinese%2520text%2520recognition%250Awith%2520LLMs%2520extensively%2520trained%2520on%2520Chinese.%2520Our%2520evaluation%2520demonstrates%2520that%2520GFD%250Asignificantly%2520improves%2520performance%2520in%2520ASR%2520and%2520OCR%2520tasks%252C%2520with%2520ASR%2520reaching%250Astate-of-the-art%2520in%2520the%2520NTUML2021%2520benchmark.%2520GFD%2520provides%2520a%2520significant%2520step%250Aforward%2520in%2520model%2520integration%252C%2520offering%2520a%2520unified%2520solution%2520that%2520could%2520be%2520widely%250Aapplicable%2520to%2520leveraging%2520existing%2520pre-trained%2520models%2520through%2520step%2520by%2520step%250Afusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14259v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Fuse%20Step%20by%20Step%3A%20A%20Generative%20Fusion%20Decoding%20Algorithm%20with%0A%20%20LLMs%20for%20Multi-modal%20Text%20Recognition&entry.906535625=Chan-Jan%20Hsu%20and%20Yi-Chang%20Chen%20and%20Feng-Ting%20Liao%20and%20Pei-Chen%20Ho%20and%20Yu-Hsiang%20Wang%20and%20Po-Chun%20Hsu%20and%20Da-shan%20Shiu&entry.1292438233=%20%20We%20introduce%20%22Generative%20Fusion%20Decoding%22%20%28GFD%29%2C%20a%20novel%20shallow%20fusion%0Aframework%2C%20utilized%20to%20integrate%20Large%20Language%20Models%20%28LLMs%29%20into%20multi-modal%0Atext%20recognition%20systems%20such%20as%20automatic%20speech%20recognition%20%28ASR%29%20and%20optical%0Acharacter%20recognition%20%28OCR%29.%20We%20derive%20the%20formulas%20necessary%20to%20enable%20GFD%20to%0Aoperate%20across%20mismatched%20token%20spaces%20of%20different%20models%20by%20mapping%20text%0Atoken%20space%20to%20byte%20token%20space%2C%20enabling%20seamless%20fusion%20during%20the%20decoding%0Aprocess.%20The%20framework%20is%20plug-and-play%2C%20compatible%20with%20various%0Aauto-regressive%20models%2C%20and%20does%20not%20require%20re-training%20for%20feature%20alignment%2C%0Athus%20overcoming%20limitations%20of%20previous%20fusion%20techniques.%20We%20highlight%20three%0Amain%20advantages%20of%20GFD%3A%20First%2C%20by%20simplifying%20the%20complexity%20of%20aligning%0Adifferent%20model%20sample%20spaces%2C%20GFD%20allows%20LLMs%20to%20correct%20errors%20in%20tandem%20with%0Athe%20recognition%20model%2C%20reducing%20computation%20latencies.%20Second%2C%20the%20in-context%0Alearning%20ability%20of%20LLMs%20is%20fully%20capitalized%20by%20GFD%2C%20increasing%20robustness%20in%0Along-form%20speech%20recognition%20and%20instruction%20aware%20speech%20recognition.%20Third%2C%0AGFD%20enables%20fusing%20recognition%20models%20deficient%20in%20Chinese%20text%20recognition%0Awith%20LLMs%20extensively%20trained%20on%20Chinese.%20Our%20evaluation%20demonstrates%20that%20GFD%0Asignificantly%20improves%20performance%20in%20ASR%20and%20OCR%20tasks%2C%20with%20ASR%20reaching%0Astate-of-the-art%20in%20the%20NTUML2021%20benchmark.%20GFD%20provides%20a%20significant%20step%0Aforward%20in%20model%20integration%2C%20offering%20a%20unified%20solution%20that%20could%20be%20widely%0Aapplicable%20to%20leveraging%20existing%20pre-trained%20models%20through%20step%20by%20step%0Afusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14259v2&entry.124074799=Read"},
{"title": "A Study on Self-Supervised Pretraining for Vision Problems in\n  Gastrointestinal Endoscopy", "author": "Edward Sanderson and Bogdan J. Matuszewski", "abstract": "  Solutions to vision tasks in gastrointestinal endoscopy (GIE) conventionally\nuse image encoders pretrained in a supervised manner with ImageNet-1k as\nbackbones. However, the use of modern self-supervised pretraining algorithms\nand a recent dataset of 100k unlabelled GIE images (Hyperkvasir-unlabelled) may\nallow for improvements. In this work, we study the fine-tuned performance of\nmodels with ResNet50 and ViT-B backbones pretrained in self-supervised and\nsupervised manners with ImageNet-1k and Hyperkvasir-unlabelled (self-supervised\nonly) in a range of GIE vision tasks. In addition to identifying the most\nsuitable pretraining pipeline and backbone architecture for each task, out of\nthose considered, our results suggest three general principles. Firstly, that\nself-supervised pretraining generally produces more suitable backbones for GIE\nvision tasks than supervised pretraining. Secondly, that self-supervised\npretraining with ImageNet-1k is typically more suitable than pretraining with\nHyperkvasir-unlabelled, with the notable exception of monocular depth\nestimation in colonoscopy. Thirdly, that ViT-Bs are more suitable in polyp\nsegmentation and monocular depth estimation in colonoscopy, ResNet50s are more\nsuitable in polyp detection, and both architectures perform similarly in\nanatomical landmark recognition and pathological finding characterisation. We\nhope this work draws attention to the complexity of pretraining for GIE vision\ntasks, informs this development of more suitable approaches than the\nconvention, and inspires further research on this topic to help advance this\ndevelopment. Code available: \\underline{github.com/ESandML/SSL4GIE}\n", "link": "http://arxiv.org/abs/2401.06278v2", "date": "2024-05-28", "relevancy": 2.1426, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5435}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5408}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20on%20Self-Supervised%20Pretraining%20for%20Vision%20Problems%20in%0A%20%20Gastrointestinal%20Endoscopy&body=Title%3A%20A%20Study%20on%20Self-Supervised%20Pretraining%20for%20Vision%20Problems%20in%0A%20%20Gastrointestinal%20Endoscopy%0AAuthor%3A%20Edward%20Sanderson%20and%20Bogdan%20J.%20Matuszewski%0AAbstract%3A%20%20%20Solutions%20to%20vision%20tasks%20in%20gastrointestinal%20endoscopy%20%28GIE%29%20conventionally%0Ause%20image%20encoders%20pretrained%20in%20a%20supervised%20manner%20with%20ImageNet-1k%20as%0Abackbones.%20However%2C%20the%20use%20of%20modern%20self-supervised%20pretraining%20algorithms%0Aand%20a%20recent%20dataset%20of%20100k%20unlabelled%20GIE%20images%20%28Hyperkvasir-unlabelled%29%20may%0Aallow%20for%20improvements.%20In%20this%20work%2C%20we%20study%20the%20fine-tuned%20performance%20of%0Amodels%20with%20ResNet50%20and%20ViT-B%20backbones%20pretrained%20in%20self-supervised%20and%0Asupervised%20manners%20with%20ImageNet-1k%20and%20Hyperkvasir-unlabelled%20%28self-supervised%0Aonly%29%20in%20a%20range%20of%20GIE%20vision%20tasks.%20In%20addition%20to%20identifying%20the%20most%0Asuitable%20pretraining%20pipeline%20and%20backbone%20architecture%20for%20each%20task%2C%20out%20of%0Athose%20considered%2C%20our%20results%20suggest%20three%20general%20principles.%20Firstly%2C%20that%0Aself-supervised%20pretraining%20generally%20produces%20more%20suitable%20backbones%20for%20GIE%0Avision%20tasks%20than%20supervised%20pretraining.%20Secondly%2C%20that%20self-supervised%0Apretraining%20with%20ImageNet-1k%20is%20typically%20more%20suitable%20than%20pretraining%20with%0AHyperkvasir-unlabelled%2C%20with%20the%20notable%20exception%20of%20monocular%20depth%0Aestimation%20in%20colonoscopy.%20Thirdly%2C%20that%20ViT-Bs%20are%20more%20suitable%20in%20polyp%0Asegmentation%20and%20monocular%20depth%20estimation%20in%20colonoscopy%2C%20ResNet50s%20are%20more%0Asuitable%20in%20polyp%20detection%2C%20and%20both%20architectures%20perform%20similarly%20in%0Aanatomical%20landmark%20recognition%20and%20pathological%20finding%20characterisation.%20We%0Ahope%20this%20work%20draws%20attention%20to%20the%20complexity%20of%20pretraining%20for%20GIE%20vision%0Atasks%2C%20informs%20this%20development%20of%20more%20suitable%20approaches%20than%20the%0Aconvention%2C%20and%20inspires%20further%20research%20on%20this%20topic%20to%20help%20advance%20this%0Adevelopment.%20Code%20available%3A%20%5Cunderline%7Bgithub.com/ESandML/SSL4GIE%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520on%2520Self-Supervised%2520Pretraining%2520for%2520Vision%2520Problems%2520in%250A%2520%2520Gastrointestinal%2520Endoscopy%26entry.906535625%3DEdward%2520Sanderson%2520and%2520Bogdan%2520J.%2520Matuszewski%26entry.1292438233%3D%2520%2520Solutions%2520to%2520vision%2520tasks%2520in%2520gastrointestinal%2520endoscopy%2520%2528GIE%2529%2520conventionally%250Ause%2520image%2520encoders%2520pretrained%2520in%2520a%2520supervised%2520manner%2520with%2520ImageNet-1k%2520as%250Abackbones.%2520However%252C%2520the%2520use%2520of%2520modern%2520self-supervised%2520pretraining%2520algorithms%250Aand%2520a%2520recent%2520dataset%2520of%2520100k%2520unlabelled%2520GIE%2520images%2520%2528Hyperkvasir-unlabelled%2529%2520may%250Aallow%2520for%2520improvements.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520fine-tuned%2520performance%2520of%250Amodels%2520with%2520ResNet50%2520and%2520ViT-B%2520backbones%2520pretrained%2520in%2520self-supervised%2520and%250Asupervised%2520manners%2520with%2520ImageNet-1k%2520and%2520Hyperkvasir-unlabelled%2520%2528self-supervised%250Aonly%2529%2520in%2520a%2520range%2520of%2520GIE%2520vision%2520tasks.%2520In%2520addition%2520to%2520identifying%2520the%2520most%250Asuitable%2520pretraining%2520pipeline%2520and%2520backbone%2520architecture%2520for%2520each%2520task%252C%2520out%2520of%250Athose%2520considered%252C%2520our%2520results%2520suggest%2520three%2520general%2520principles.%2520Firstly%252C%2520that%250Aself-supervised%2520pretraining%2520generally%2520produces%2520more%2520suitable%2520backbones%2520for%2520GIE%250Avision%2520tasks%2520than%2520supervised%2520pretraining.%2520Secondly%252C%2520that%2520self-supervised%250Apretraining%2520with%2520ImageNet-1k%2520is%2520typically%2520more%2520suitable%2520than%2520pretraining%2520with%250AHyperkvasir-unlabelled%252C%2520with%2520the%2520notable%2520exception%2520of%2520monocular%2520depth%250Aestimation%2520in%2520colonoscopy.%2520Thirdly%252C%2520that%2520ViT-Bs%2520are%2520more%2520suitable%2520in%2520polyp%250Asegmentation%2520and%2520monocular%2520depth%2520estimation%2520in%2520colonoscopy%252C%2520ResNet50s%2520are%2520more%250Asuitable%2520in%2520polyp%2520detection%252C%2520and%2520both%2520architectures%2520perform%2520similarly%2520in%250Aanatomical%2520landmark%2520recognition%2520and%2520pathological%2520finding%2520characterisation.%2520We%250Ahope%2520this%2520work%2520draws%2520attention%2520to%2520the%2520complexity%2520of%2520pretraining%2520for%2520GIE%2520vision%250Atasks%252C%2520informs%2520this%2520development%2520of%2520more%2520suitable%2520approaches%2520than%2520the%250Aconvention%252C%2520and%2520inspires%2520further%2520research%2520on%2520this%2520topic%2520to%2520help%2520advance%2520this%250Adevelopment.%2520Code%2520available%253A%2520%255Cunderline%257Bgithub.com/ESandML/SSL4GIE%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20on%20Self-Supervised%20Pretraining%20for%20Vision%20Problems%20in%0A%20%20Gastrointestinal%20Endoscopy&entry.906535625=Edward%20Sanderson%20and%20Bogdan%20J.%20Matuszewski&entry.1292438233=%20%20Solutions%20to%20vision%20tasks%20in%20gastrointestinal%20endoscopy%20%28GIE%29%20conventionally%0Ause%20image%20encoders%20pretrained%20in%20a%20supervised%20manner%20with%20ImageNet-1k%20as%0Abackbones.%20However%2C%20the%20use%20of%20modern%20self-supervised%20pretraining%20algorithms%0Aand%20a%20recent%20dataset%20of%20100k%20unlabelled%20GIE%20images%20%28Hyperkvasir-unlabelled%29%20may%0Aallow%20for%20improvements.%20In%20this%20work%2C%20we%20study%20the%20fine-tuned%20performance%20of%0Amodels%20with%20ResNet50%20and%20ViT-B%20backbones%20pretrained%20in%20self-supervised%20and%0Asupervised%20manners%20with%20ImageNet-1k%20and%20Hyperkvasir-unlabelled%20%28self-supervised%0Aonly%29%20in%20a%20range%20of%20GIE%20vision%20tasks.%20In%20addition%20to%20identifying%20the%20most%0Asuitable%20pretraining%20pipeline%20and%20backbone%20architecture%20for%20each%20task%2C%20out%20of%0Athose%20considered%2C%20our%20results%20suggest%20three%20general%20principles.%20Firstly%2C%20that%0Aself-supervised%20pretraining%20generally%20produces%20more%20suitable%20backbones%20for%20GIE%0Avision%20tasks%20than%20supervised%20pretraining.%20Secondly%2C%20that%20self-supervised%0Apretraining%20with%20ImageNet-1k%20is%20typically%20more%20suitable%20than%20pretraining%20with%0AHyperkvasir-unlabelled%2C%20with%20the%20notable%20exception%20of%20monocular%20depth%0Aestimation%20in%20colonoscopy.%20Thirdly%2C%20that%20ViT-Bs%20are%20more%20suitable%20in%20polyp%0Asegmentation%20and%20monocular%20depth%20estimation%20in%20colonoscopy%2C%20ResNet50s%20are%20more%0Asuitable%20in%20polyp%20detection%2C%20and%20both%20architectures%20perform%20similarly%20in%0Aanatomical%20landmark%20recognition%20and%20pathological%20finding%20characterisation.%20We%0Ahope%20this%20work%20draws%20attention%20to%20the%20complexity%20of%20pretraining%20for%20GIE%20vision%0Atasks%2C%20informs%20this%20development%20of%20more%20suitable%20approaches%20than%20the%0Aconvention%2C%20and%20inspires%20further%20research%20on%20this%20topic%20to%20help%20advance%20this%0Adevelopment.%20Code%20available%3A%20%5Cunderline%7Bgithub.com/ESandML/SSL4GIE%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06278v2&entry.124074799=Read"},
{"title": "MODL: Multilearner Online Deep Learning", "author": "Antonios Valkanas and Boris N. Oreshkin and Mark Coates", "abstract": "  Online deep learning solves the problem of learning from streams of data,\nreconciling two opposing objectives: learn fast and learn deep. Existing work\nfocuses almost exclusively on exploring pure deep learning solutions, which are\nmuch better suited to handle the \"deep\" than the \"fast\" part of the online\nlearning equation. In our work, we propose a different paradigm, based on a\nhybrid multilearner approach. First, we develop a fast online logistic\nregression learner. This learner does not rely on backpropagation. Instead, it\nuses closed form recursive updates of model parameters, handling the fast\nlearning part of the online learning problem. We then analyze the existing\nonline deep learning theory and show that the widespread ODL approach,\ncurrently operating at complexity $O(L^2)$ in terms of the number of layers\n$L$, can be equivalently implemented in $O(L)$ complexity. This further leads\nus to the cascaded multilearner design, in which multiple shallow and deep\nlearners are co-trained to solve the online learning problem in a cooperative,\nsynergistic fashion. We show that this approach achieves state-of-the-art\nresults on common online learning datasets, while also being able to handle\nmissing features gracefully. Our code is publicly available at\nhttps://github.com/AntonValk/MODL.\n", "link": "http://arxiv.org/abs/2405.18281v1", "date": "2024-05-28", "relevancy": 2.1387, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5469}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5286}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MODL%3A%20Multilearner%20Online%20Deep%20Learning&body=Title%3A%20MODL%3A%20Multilearner%20Online%20Deep%20Learning%0AAuthor%3A%20Antonios%20Valkanas%20and%20Boris%20N.%20Oreshkin%20and%20Mark%20Coates%0AAbstract%3A%20%20%20Online%20deep%20learning%20solves%20the%20problem%20of%20learning%20from%20streams%20of%20data%2C%0Areconciling%20two%20opposing%20objectives%3A%20learn%20fast%20and%20learn%20deep.%20Existing%20work%0Afocuses%20almost%20exclusively%20on%20exploring%20pure%20deep%20learning%20solutions%2C%20which%20are%0Amuch%20better%20suited%20to%20handle%20the%20%22deep%22%20than%20the%20%22fast%22%20part%20of%20the%20online%0Alearning%20equation.%20In%20our%20work%2C%20we%20propose%20a%20different%20paradigm%2C%20based%20on%20a%0Ahybrid%20multilearner%20approach.%20First%2C%20we%20develop%20a%20fast%20online%20logistic%0Aregression%20learner.%20This%20learner%20does%20not%20rely%20on%20backpropagation.%20Instead%2C%20it%0Auses%20closed%20form%20recursive%20updates%20of%20model%20parameters%2C%20handling%20the%20fast%0Alearning%20part%20of%20the%20online%20learning%20problem.%20We%20then%20analyze%20the%20existing%0Aonline%20deep%20learning%20theory%20and%20show%20that%20the%20widespread%20ODL%20approach%2C%0Acurrently%20operating%20at%20complexity%20%24O%28L%5E2%29%24%20in%20terms%20of%20the%20number%20of%20layers%0A%24L%24%2C%20can%20be%20equivalently%20implemented%20in%20%24O%28L%29%24%20complexity.%20This%20further%20leads%0Aus%20to%20the%20cascaded%20multilearner%20design%2C%20in%20which%20multiple%20shallow%20and%20deep%0Alearners%20are%20co-trained%20to%20solve%20the%20online%20learning%20problem%20in%20a%20cooperative%2C%0Asynergistic%20fashion.%20We%20show%20that%20this%20approach%20achieves%20state-of-the-art%0Aresults%20on%20common%20online%20learning%20datasets%2C%20while%20also%20being%20able%20to%20handle%0Amissing%20features%20gracefully.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/AntonValk/MODL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMODL%253A%2520Multilearner%2520Online%2520Deep%2520Learning%26entry.906535625%3DAntonios%2520Valkanas%2520and%2520Boris%2520N.%2520Oreshkin%2520and%2520Mark%2520Coates%26entry.1292438233%3D%2520%2520Online%2520deep%2520learning%2520solves%2520the%2520problem%2520of%2520learning%2520from%2520streams%2520of%2520data%252C%250Areconciling%2520two%2520opposing%2520objectives%253A%2520learn%2520fast%2520and%2520learn%2520deep.%2520Existing%2520work%250Afocuses%2520almost%2520exclusively%2520on%2520exploring%2520pure%2520deep%2520learning%2520solutions%252C%2520which%2520are%250Amuch%2520better%2520suited%2520to%2520handle%2520the%2520%2522deep%2522%2520than%2520the%2520%2522fast%2522%2520part%2520of%2520the%2520online%250Alearning%2520equation.%2520In%2520our%2520work%252C%2520we%2520propose%2520a%2520different%2520paradigm%252C%2520based%2520on%2520a%250Ahybrid%2520multilearner%2520approach.%2520First%252C%2520we%2520develop%2520a%2520fast%2520online%2520logistic%250Aregression%2520learner.%2520This%2520learner%2520does%2520not%2520rely%2520on%2520backpropagation.%2520Instead%252C%2520it%250Auses%2520closed%2520form%2520recursive%2520updates%2520of%2520model%2520parameters%252C%2520handling%2520the%2520fast%250Alearning%2520part%2520of%2520the%2520online%2520learning%2520problem.%2520We%2520then%2520analyze%2520the%2520existing%250Aonline%2520deep%2520learning%2520theory%2520and%2520show%2520that%2520the%2520widespread%2520ODL%2520approach%252C%250Acurrently%2520operating%2520at%2520complexity%2520%2524O%2528L%255E2%2529%2524%2520in%2520terms%2520of%2520the%2520number%2520of%2520layers%250A%2524L%2524%252C%2520can%2520be%2520equivalently%2520implemented%2520in%2520%2524O%2528L%2529%2524%2520complexity.%2520This%2520further%2520leads%250Aus%2520to%2520the%2520cascaded%2520multilearner%2520design%252C%2520in%2520which%2520multiple%2520shallow%2520and%2520deep%250Alearners%2520are%2520co-trained%2520to%2520solve%2520the%2520online%2520learning%2520problem%2520in%2520a%2520cooperative%252C%250Asynergistic%2520fashion.%2520We%2520show%2520that%2520this%2520approach%2520achieves%2520state-of-the-art%250Aresults%2520on%2520common%2520online%2520learning%2520datasets%252C%2520while%2520also%2520being%2520able%2520to%2520handle%250Amissing%2520features%2520gracefully.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/AntonValk/MODL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MODL%3A%20Multilearner%20Online%20Deep%20Learning&entry.906535625=Antonios%20Valkanas%20and%20Boris%20N.%20Oreshkin%20and%20Mark%20Coates&entry.1292438233=%20%20Online%20deep%20learning%20solves%20the%20problem%20of%20learning%20from%20streams%20of%20data%2C%0Areconciling%20two%20opposing%20objectives%3A%20learn%20fast%20and%20learn%20deep.%20Existing%20work%0Afocuses%20almost%20exclusively%20on%20exploring%20pure%20deep%20learning%20solutions%2C%20which%20are%0Amuch%20better%20suited%20to%20handle%20the%20%22deep%22%20than%20the%20%22fast%22%20part%20of%20the%20online%0Alearning%20equation.%20In%20our%20work%2C%20we%20propose%20a%20different%20paradigm%2C%20based%20on%20a%0Ahybrid%20multilearner%20approach.%20First%2C%20we%20develop%20a%20fast%20online%20logistic%0Aregression%20learner.%20This%20learner%20does%20not%20rely%20on%20backpropagation.%20Instead%2C%20it%0Auses%20closed%20form%20recursive%20updates%20of%20model%20parameters%2C%20handling%20the%20fast%0Alearning%20part%20of%20the%20online%20learning%20problem.%20We%20then%20analyze%20the%20existing%0Aonline%20deep%20learning%20theory%20and%20show%20that%20the%20widespread%20ODL%20approach%2C%0Acurrently%20operating%20at%20complexity%20%24O%28L%5E2%29%24%20in%20terms%20of%20the%20number%20of%20layers%0A%24L%24%2C%20can%20be%20equivalently%20implemented%20in%20%24O%28L%29%24%20complexity.%20This%20further%20leads%0Aus%20to%20the%20cascaded%20multilearner%20design%2C%20in%20which%20multiple%20shallow%20and%20deep%0Alearners%20are%20co-trained%20to%20solve%20the%20online%20learning%20problem%20in%20a%20cooperative%2C%0Asynergistic%20fashion.%20We%20show%20that%20this%20approach%20achieves%20state-of-the-art%0Aresults%20on%20common%20online%20learning%20datasets%2C%20while%20also%20being%20able%20to%20handle%0Amissing%20features%20gracefully.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/AntonValk/MODL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18281v1&entry.124074799=Read"},
{"title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "author": "Jeongwhan Choi and Hyowon Wi and Jayoung Kim and Yehjin Shin and Kookjin Lee and Nathaniel Trask and Noseong Park", "abstract": "  Transformers, renowned for their self-attention mechanism, have achieved\nstate-of-the-art performance across various tasks in natural language\nprocessing, computer vision, time-series modeling, etc. However, one of the\nchallenges with deep Transformer models is the oversmoothing problem, where\nrepresentations across layers converge to indistinguishable values, leading to\nsignificant performance degradation. We interpret the original self-attention\nas a simple graph filter and redesign it from a graph signal processing (GSP)\nperspective. We propose a graph-filter-based self-attention (GFSA) to learn a\ngeneral yet effective one, whose complexity, however, is slightly larger than\nthat of the original self-attention mechanism. We demonstrate that GFSA\nimproves the performance of Transformers in various fields, including computer\nvision, natural language processing, graph regression, speech recognition, and\ncode classification.\n", "link": "http://arxiv.org/abs/2312.04234v3", "date": "2024-05-28", "relevancy": 2.1381, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5933}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5332}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Convolutions%20Enrich%20the%20Self-Attention%20in%20Transformers%21&body=Title%3A%20Graph%20Convolutions%20Enrich%20the%20Self-Attention%20in%20Transformers%21%0AAuthor%3A%20Jeongwhan%20Choi%20and%20Hyowon%20Wi%20and%20Jayoung%20Kim%20and%20Yehjin%20Shin%20and%20Kookjin%20Lee%20and%20Nathaniel%20Trask%20and%20Noseong%20Park%0AAbstract%3A%20%20%20Transformers%2C%20renowned%20for%20their%20self-attention%20mechanism%2C%20have%20achieved%0Astate-of-the-art%20performance%20across%20various%20tasks%20in%20natural%20language%0Aprocessing%2C%20computer%20vision%2C%20time-series%20modeling%2C%20etc.%20However%2C%20one%20of%20the%0Achallenges%20with%20deep%20Transformer%20models%20is%20the%20oversmoothing%20problem%2C%20where%0Arepresentations%20across%20layers%20converge%20to%20indistinguishable%20values%2C%20leading%20to%0Asignificant%20performance%20degradation.%20We%20interpret%20the%20original%20self-attention%0Aas%20a%20simple%20graph%20filter%20and%20redesign%20it%20from%20a%20graph%20signal%20processing%20%28GSP%29%0Aperspective.%20We%20propose%20a%20graph-filter-based%20self-attention%20%28GFSA%29%20to%20learn%20a%0Ageneral%20yet%20effective%20one%2C%20whose%20complexity%2C%20however%2C%20is%20slightly%20larger%20than%0Athat%20of%20the%20original%20self-attention%20mechanism.%20We%20demonstrate%20that%20GFSA%0Aimproves%20the%20performance%20of%20Transformers%20in%20various%20fields%2C%20including%20computer%0Avision%2C%20natural%20language%20processing%2C%20graph%20regression%2C%20speech%20recognition%2C%20and%0Acode%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04234v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Convolutions%2520Enrich%2520the%2520Self-Attention%2520in%2520Transformers%2521%26entry.906535625%3DJeongwhan%2520Choi%2520and%2520Hyowon%2520Wi%2520and%2520Jayoung%2520Kim%2520and%2520Yehjin%2520Shin%2520and%2520Kookjin%2520Lee%2520and%2520Nathaniel%2520Trask%2520and%2520Noseong%2520Park%26entry.1292438233%3D%2520%2520Transformers%252C%2520renowned%2520for%2520their%2520self-attention%2520mechanism%252C%2520have%2520achieved%250Astate-of-the-art%2520performance%2520across%2520various%2520tasks%2520in%2520natural%2520language%250Aprocessing%252C%2520computer%2520vision%252C%2520time-series%2520modeling%252C%2520etc.%2520However%252C%2520one%2520of%2520the%250Achallenges%2520with%2520deep%2520Transformer%2520models%2520is%2520the%2520oversmoothing%2520problem%252C%2520where%250Arepresentations%2520across%2520layers%2520converge%2520to%2520indistinguishable%2520values%252C%2520leading%2520to%250Asignificant%2520performance%2520degradation.%2520We%2520interpret%2520the%2520original%2520self-attention%250Aas%2520a%2520simple%2520graph%2520filter%2520and%2520redesign%2520it%2520from%2520a%2520graph%2520signal%2520processing%2520%2528GSP%2529%250Aperspective.%2520We%2520propose%2520a%2520graph-filter-based%2520self-attention%2520%2528GFSA%2529%2520to%2520learn%2520a%250Ageneral%2520yet%2520effective%2520one%252C%2520whose%2520complexity%252C%2520however%252C%2520is%2520slightly%2520larger%2520than%250Athat%2520of%2520the%2520original%2520self-attention%2520mechanism.%2520We%2520demonstrate%2520that%2520GFSA%250Aimproves%2520the%2520performance%2520of%2520Transformers%2520in%2520various%2520fields%252C%2520including%2520computer%250Avision%252C%2520natural%2520language%2520processing%252C%2520graph%2520regression%252C%2520speech%2520recognition%252C%2520and%250Acode%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04234v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Convolutions%20Enrich%20the%20Self-Attention%20in%20Transformers%21&entry.906535625=Jeongwhan%20Choi%20and%20Hyowon%20Wi%20and%20Jayoung%20Kim%20and%20Yehjin%20Shin%20and%20Kookjin%20Lee%20and%20Nathaniel%20Trask%20and%20Noseong%20Park&entry.1292438233=%20%20Transformers%2C%20renowned%20for%20their%20self-attention%20mechanism%2C%20have%20achieved%0Astate-of-the-art%20performance%20across%20various%20tasks%20in%20natural%20language%0Aprocessing%2C%20computer%20vision%2C%20time-series%20modeling%2C%20etc.%20However%2C%20one%20of%20the%0Achallenges%20with%20deep%20Transformer%20models%20is%20the%20oversmoothing%20problem%2C%20where%0Arepresentations%20across%20layers%20converge%20to%20indistinguishable%20values%2C%20leading%20to%0Asignificant%20performance%20degradation.%20We%20interpret%20the%20original%20self-attention%0Aas%20a%20simple%20graph%20filter%20and%20redesign%20it%20from%20a%20graph%20signal%20processing%20%28GSP%29%0Aperspective.%20We%20propose%20a%20graph-filter-based%20self-attention%20%28GFSA%29%20to%20learn%20a%0Ageneral%20yet%20effective%20one%2C%20whose%20complexity%2C%20however%2C%20is%20slightly%20larger%20than%0Athat%20of%20the%20original%20self-attention%20mechanism.%20We%20demonstrate%20that%20GFSA%0Aimproves%20the%20performance%20of%20Transformers%20in%20various%20fields%2C%20including%20computer%0Avision%2C%20natural%20language%20processing%2C%20graph%20regression%2C%20speech%20recognition%2C%20and%0Acode%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04234v3&entry.124074799=Read"},
{"title": "Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language\n  Models via Instruction Tuning", "author": "Yixiao Zhang and Yukara Ikemiya and Woosung Choi and Naoki Murata and Marco A. Mart\u00ednez-Ram\u00edrez and Liwei Lin and Gus Xia and Wei-Hsiang Liao and Yuki Mitsufuji and Simon Dixon", "abstract": "  Recent advances in text-to-music editing, which employ text queries to modify\nmusic (e.g.\\ by changing its style or adjusting instrumental components),\npresent unique challenges and opportunities for AI-assisted music creation.\nPrevious approaches in this domain have been constrained by the necessity to\ntrain specific editing models from scratch, which is both resource-intensive\nand inefficient; other research uses large language models to predict edited\nmusic, resulting in imprecise audio reconstruction. To Combine the strengths\nand address these limitations, we introduce Instruct-MusicGen, a novel approach\nthat finetunes a pretrained MusicGen model to efficiently follow editing\ninstructions such as adding, removing, or separating stems. Our approach\ninvolves a modification of the original MusicGen architecture by incorporating\na text fusion module and an audio fusion module, which allow the model to\nprocess instruction texts and audio inputs concurrently and yield the desired\nedited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters\nto the original MusicGen model and only trains for 5K steps, yet it achieves\nsuperior performance across all tasks compared to existing baselines, and\ndemonstrates performance comparable to the models trained for specific tasks.\nThis advancement not only enhances the efficiency of text-to-music editing but\nalso broadens the applicability of music language models in dynamic music\nproduction environments.\n", "link": "http://arxiv.org/abs/2405.18386v1", "date": "2024-05-28", "relevancy": 2.1205, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5559}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5147}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruct-MusicGen%3A%20Unlocking%20Text-to-Music%20Editing%20for%20Music%20Language%0A%20%20Models%20via%20Instruction%20Tuning&body=Title%3A%20Instruct-MusicGen%3A%20Unlocking%20Text-to-Music%20Editing%20for%20Music%20Language%0A%20%20Models%20via%20Instruction%20Tuning%0AAuthor%3A%20Yixiao%20Zhang%20and%20Yukara%20Ikemiya%20and%20Woosung%20Choi%20and%20Naoki%20Murata%20and%20Marco%20A.%20Mart%C3%ADnez-Ram%C3%ADrez%20and%20Liwei%20Lin%20and%20Gus%20Xia%20and%20Wei-Hsiang%20Liao%20and%20Yuki%20Mitsufuji%20and%20Simon%20Dixon%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-music%20editing%2C%20which%20employ%20text%20queries%20to%20modify%0Amusic%20%28e.g.%5C%20by%20changing%20its%20style%20or%20adjusting%20instrumental%20components%29%2C%0Apresent%20unique%20challenges%20and%20opportunities%20for%20AI-assisted%20music%20creation.%0APrevious%20approaches%20in%20this%20domain%20have%20been%20constrained%20by%20the%20necessity%20to%0Atrain%20specific%20editing%20models%20from%20scratch%2C%20which%20is%20both%20resource-intensive%0Aand%20inefficient%3B%20other%20research%20uses%20large%20language%20models%20to%20predict%20edited%0Amusic%2C%20resulting%20in%20imprecise%20audio%20reconstruction.%20To%20Combine%20the%20strengths%0Aand%20address%20these%20limitations%2C%20we%20introduce%20Instruct-MusicGen%2C%20a%20novel%20approach%0Athat%20finetunes%20a%20pretrained%20MusicGen%20model%20to%20efficiently%20follow%20editing%0Ainstructions%20such%20as%20adding%2C%20removing%2C%20or%20separating%20stems.%20Our%20approach%0Ainvolves%20a%20modification%20of%20the%20original%20MusicGen%20architecture%20by%20incorporating%0Aa%20text%20fusion%20module%20and%20an%20audio%20fusion%20module%2C%20which%20allow%20the%20model%20to%0Aprocess%20instruction%20texts%20and%20audio%20inputs%20concurrently%20and%20yield%20the%20desired%0Aedited%20music.%20Remarkably%2C%20Instruct-MusicGen%20only%20introduces%208%25%20new%20parameters%0Ato%20the%20original%20MusicGen%20model%20and%20only%20trains%20for%205K%20steps%2C%20yet%20it%20achieves%0Asuperior%20performance%20across%20all%20tasks%20compared%20to%20existing%20baselines%2C%20and%0Ademonstrates%20performance%20comparable%20to%20the%20models%20trained%20for%20specific%20tasks.%0AThis%20advancement%20not%20only%20enhances%20the%20efficiency%20of%20text-to-music%20editing%20but%0Aalso%20broadens%20the%20applicability%20of%20music%20language%20models%20in%20dynamic%20music%0Aproduction%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruct-MusicGen%253A%2520Unlocking%2520Text-to-Music%2520Editing%2520for%2520Music%2520Language%250A%2520%2520Models%2520via%2520Instruction%2520Tuning%26entry.906535625%3DYixiao%2520Zhang%2520and%2520Yukara%2520Ikemiya%2520and%2520Woosung%2520Choi%2520and%2520Naoki%2520Murata%2520and%2520Marco%2520A.%2520Mart%25C3%25ADnez-Ram%25C3%25ADrez%2520and%2520Liwei%2520Lin%2520and%2520Gus%2520Xia%2520and%2520Wei-Hsiang%2520Liao%2520and%2520Yuki%2520Mitsufuji%2520and%2520Simon%2520Dixon%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-music%2520editing%252C%2520which%2520employ%2520text%2520queries%2520to%2520modify%250Amusic%2520%2528e.g.%255C%2520by%2520changing%2520its%2520style%2520or%2520adjusting%2520instrumental%2520components%2529%252C%250Apresent%2520unique%2520challenges%2520and%2520opportunities%2520for%2520AI-assisted%2520music%2520creation.%250APrevious%2520approaches%2520in%2520this%2520domain%2520have%2520been%2520constrained%2520by%2520the%2520necessity%2520to%250Atrain%2520specific%2520editing%2520models%2520from%2520scratch%252C%2520which%2520is%2520both%2520resource-intensive%250Aand%2520inefficient%253B%2520other%2520research%2520uses%2520large%2520language%2520models%2520to%2520predict%2520edited%250Amusic%252C%2520resulting%2520in%2520imprecise%2520audio%2520reconstruction.%2520To%2520Combine%2520the%2520strengths%250Aand%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520Instruct-MusicGen%252C%2520a%2520novel%2520approach%250Athat%2520finetunes%2520a%2520pretrained%2520MusicGen%2520model%2520to%2520efficiently%2520follow%2520editing%250Ainstructions%2520such%2520as%2520adding%252C%2520removing%252C%2520or%2520separating%2520stems.%2520Our%2520approach%250Ainvolves%2520a%2520modification%2520of%2520the%2520original%2520MusicGen%2520architecture%2520by%2520incorporating%250Aa%2520text%2520fusion%2520module%2520and%2520an%2520audio%2520fusion%2520module%252C%2520which%2520allow%2520the%2520model%2520to%250Aprocess%2520instruction%2520texts%2520and%2520audio%2520inputs%2520concurrently%2520and%2520yield%2520the%2520desired%250Aedited%2520music.%2520Remarkably%252C%2520Instruct-MusicGen%2520only%2520introduces%25208%2525%2520new%2520parameters%250Ato%2520the%2520original%2520MusicGen%2520model%2520and%2520only%2520trains%2520for%25205K%2520steps%252C%2520yet%2520it%2520achieves%250Asuperior%2520performance%2520across%2520all%2520tasks%2520compared%2520to%2520existing%2520baselines%252C%2520and%250Ademonstrates%2520performance%2520comparable%2520to%2520the%2520models%2520trained%2520for%2520specific%2520tasks.%250AThis%2520advancement%2520not%2520only%2520enhances%2520the%2520efficiency%2520of%2520text-to-music%2520editing%2520but%250Aalso%2520broadens%2520the%2520applicability%2520of%2520music%2520language%2520models%2520in%2520dynamic%2520music%250Aproduction%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruct-MusicGen%3A%20Unlocking%20Text-to-Music%20Editing%20for%20Music%20Language%0A%20%20Models%20via%20Instruction%20Tuning&entry.906535625=Yixiao%20Zhang%20and%20Yukara%20Ikemiya%20and%20Woosung%20Choi%20and%20Naoki%20Murata%20and%20Marco%20A.%20Mart%C3%ADnez-Ram%C3%ADrez%20and%20Liwei%20Lin%20and%20Gus%20Xia%20and%20Wei-Hsiang%20Liao%20and%20Yuki%20Mitsufuji%20and%20Simon%20Dixon&entry.1292438233=%20%20Recent%20advances%20in%20text-to-music%20editing%2C%20which%20employ%20text%20queries%20to%20modify%0Amusic%20%28e.g.%5C%20by%20changing%20its%20style%20or%20adjusting%20instrumental%20components%29%2C%0Apresent%20unique%20challenges%20and%20opportunities%20for%20AI-assisted%20music%20creation.%0APrevious%20approaches%20in%20this%20domain%20have%20been%20constrained%20by%20the%20necessity%20to%0Atrain%20specific%20editing%20models%20from%20scratch%2C%20which%20is%20both%20resource-intensive%0Aand%20inefficient%3B%20other%20research%20uses%20large%20language%20models%20to%20predict%20edited%0Amusic%2C%20resulting%20in%20imprecise%20audio%20reconstruction.%20To%20Combine%20the%20strengths%0Aand%20address%20these%20limitations%2C%20we%20introduce%20Instruct-MusicGen%2C%20a%20novel%20approach%0Athat%20finetunes%20a%20pretrained%20MusicGen%20model%20to%20efficiently%20follow%20editing%0Ainstructions%20such%20as%20adding%2C%20removing%2C%20or%20separating%20stems.%20Our%20approach%0Ainvolves%20a%20modification%20of%20the%20original%20MusicGen%20architecture%20by%20incorporating%0Aa%20text%20fusion%20module%20and%20an%20audio%20fusion%20module%2C%20which%20allow%20the%20model%20to%0Aprocess%20instruction%20texts%20and%20audio%20inputs%20concurrently%20and%20yield%20the%20desired%0Aedited%20music.%20Remarkably%2C%20Instruct-MusicGen%20only%20introduces%208%25%20new%20parameters%0Ato%20the%20original%20MusicGen%20model%20and%20only%20trains%20for%205K%20steps%2C%20yet%20it%20achieves%0Asuperior%20performance%20across%20all%20tasks%20compared%20to%20existing%20baselines%2C%20and%0Ademonstrates%20performance%20comparable%20to%20the%20models%20trained%20for%20specific%20tasks.%0AThis%20advancement%20not%20only%20enhances%20the%20efficiency%20of%20text-to-music%20editing%20but%0Aalso%20broadens%20the%20applicability%20of%20music%20language%20models%20in%20dynamic%20music%0Aproduction%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18386v1&entry.124074799=Read"},
{"title": "Recurrent Complex-Weighted Autoencoders for Unsupervised Object\n  Discovery", "author": "Anand Gopalakrishnan and Aleksandar Stani\u0107 and J\u00fcrgen Schmidhuber and Michael Curtis Mozer", "abstract": "  Current state-of-the-art synchrony-based models encode object bindings with\ncomplex-valued activations and compute with real-valued weights in feedforward\narchitectures. We argue for the computational advantages of a recurrent\narchitecture with complex-valued weights. We propose a fully convolutional\nautoencoder, SynCx, that performs iterative constraint satisfaction: at each\niteration, a hidden layer bottleneck encodes statistically regular\nconfigurations of features in particular phase relationships; over iterations,\nlocal constraints propagate and the model converges to a globally consistent\nconfiguration of phase assignments. Binding is achieved simply by the\nmatrix-vector product operation between complex-valued weights and activations,\nwithout the need for additional mechanisms that have been incorporated into\ncurrent synchrony-based models. SynCx outperforms or is strongly competitive\nwith current models for unsupervised object discovery. SynCx also avoids\ncertain systematic grouping errors of current models, such as the inability to\nseparate similarly colored objects without additional supervision.\n", "link": "http://arxiv.org/abs/2405.17283v2", "date": "2024-05-28", "relevancy": 2.1186, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5379}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5373}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Complex-Weighted%20Autoencoders%20for%20Unsupervised%20Object%0A%20%20Discovery&body=Title%3A%20Recurrent%20Complex-Weighted%20Autoencoders%20for%20Unsupervised%20Object%0A%20%20Discovery%0AAuthor%3A%20Anand%20Gopalakrishnan%20and%20Aleksandar%20Stani%C4%87%20and%20J%C3%BCrgen%20Schmidhuber%20and%20Michael%20Curtis%20Mozer%0AAbstract%3A%20%20%20Current%20state-of-the-art%20synchrony-based%20models%20encode%20object%20bindings%20with%0Acomplex-valued%20activations%20and%20compute%20with%20real-valued%20weights%20in%20feedforward%0Aarchitectures.%20We%20argue%20for%20the%20computational%20advantages%20of%20a%20recurrent%0Aarchitecture%20with%20complex-valued%20weights.%20We%20propose%20a%20fully%20convolutional%0Aautoencoder%2C%20SynCx%2C%20that%20performs%20iterative%20constraint%20satisfaction%3A%20at%20each%0Aiteration%2C%20a%20hidden%20layer%20bottleneck%20encodes%20statistically%20regular%0Aconfigurations%20of%20features%20in%20particular%20phase%20relationships%3B%20over%20iterations%2C%0Alocal%20constraints%20propagate%20and%20the%20model%20converges%20to%20a%20globally%20consistent%0Aconfiguration%20of%20phase%20assignments.%20Binding%20is%20achieved%20simply%20by%20the%0Amatrix-vector%20product%20operation%20between%20complex-valued%20weights%20and%20activations%2C%0Awithout%20the%20need%20for%20additional%20mechanisms%20that%20have%20been%20incorporated%20into%0Acurrent%20synchrony-based%20models.%20SynCx%20outperforms%20or%20is%20strongly%20competitive%0Awith%20current%20models%20for%20unsupervised%20object%20discovery.%20SynCx%20also%20avoids%0Acertain%20systematic%20grouping%20errors%20of%20current%20models%2C%20such%20as%20the%20inability%20to%0Aseparate%20similarly%20colored%20objects%20without%20additional%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Complex-Weighted%2520Autoencoders%2520for%2520Unsupervised%2520Object%250A%2520%2520Discovery%26entry.906535625%3DAnand%2520Gopalakrishnan%2520and%2520Aleksandar%2520Stani%25C4%2587%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%2520and%2520Michael%2520Curtis%2520Mozer%26entry.1292438233%3D%2520%2520Current%2520state-of-the-art%2520synchrony-based%2520models%2520encode%2520object%2520bindings%2520with%250Acomplex-valued%2520activations%2520and%2520compute%2520with%2520real-valued%2520weights%2520in%2520feedforward%250Aarchitectures.%2520We%2520argue%2520for%2520the%2520computational%2520advantages%2520of%2520a%2520recurrent%250Aarchitecture%2520with%2520complex-valued%2520weights.%2520We%2520propose%2520a%2520fully%2520convolutional%250Aautoencoder%252C%2520SynCx%252C%2520that%2520performs%2520iterative%2520constraint%2520satisfaction%253A%2520at%2520each%250Aiteration%252C%2520a%2520hidden%2520layer%2520bottleneck%2520encodes%2520statistically%2520regular%250Aconfigurations%2520of%2520features%2520in%2520particular%2520phase%2520relationships%253B%2520over%2520iterations%252C%250Alocal%2520constraints%2520propagate%2520and%2520the%2520model%2520converges%2520to%2520a%2520globally%2520consistent%250Aconfiguration%2520of%2520phase%2520assignments.%2520Binding%2520is%2520achieved%2520simply%2520by%2520the%250Amatrix-vector%2520product%2520operation%2520between%2520complex-valued%2520weights%2520and%2520activations%252C%250Awithout%2520the%2520need%2520for%2520additional%2520mechanisms%2520that%2520have%2520been%2520incorporated%2520into%250Acurrent%2520synchrony-based%2520models.%2520SynCx%2520outperforms%2520or%2520is%2520strongly%2520competitive%250Awith%2520current%2520models%2520for%2520unsupervised%2520object%2520discovery.%2520SynCx%2520also%2520avoids%250Acertain%2520systematic%2520grouping%2520errors%2520of%2520current%2520models%252C%2520such%2520as%2520the%2520inability%2520to%250Aseparate%2520similarly%2520colored%2520objects%2520without%2520additional%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Complex-Weighted%20Autoencoders%20for%20Unsupervised%20Object%0A%20%20Discovery&entry.906535625=Anand%20Gopalakrishnan%20and%20Aleksandar%20Stani%C4%87%20and%20J%C3%BCrgen%20Schmidhuber%20and%20Michael%20Curtis%20Mozer&entry.1292438233=%20%20Current%20state-of-the-art%20synchrony-based%20models%20encode%20object%20bindings%20with%0Acomplex-valued%20activations%20and%20compute%20with%20real-valued%20weights%20in%20feedforward%0Aarchitectures.%20We%20argue%20for%20the%20computational%20advantages%20of%20a%20recurrent%0Aarchitecture%20with%20complex-valued%20weights.%20We%20propose%20a%20fully%20convolutional%0Aautoencoder%2C%20SynCx%2C%20that%20performs%20iterative%20constraint%20satisfaction%3A%20at%20each%0Aiteration%2C%20a%20hidden%20layer%20bottleneck%20encodes%20statistically%20regular%0Aconfigurations%20of%20features%20in%20particular%20phase%20relationships%3B%20over%20iterations%2C%0Alocal%20constraints%20propagate%20and%20the%20model%20converges%20to%20a%20globally%20consistent%0Aconfiguration%20of%20phase%20assignments.%20Binding%20is%20achieved%20simply%20by%20the%0Amatrix-vector%20product%20operation%20between%20complex-valued%20weights%20and%20activations%2C%0Awithout%20the%20need%20for%20additional%20mechanisms%20that%20have%20been%20incorporated%20into%0Acurrent%20synchrony-based%20models.%20SynCx%20outperforms%20or%20is%20strongly%20competitive%0Awith%20current%20models%20for%20unsupervised%20object%20discovery.%20SynCx%20also%20avoids%0Acertain%20systematic%20grouping%20errors%20of%20current%20models%2C%20such%20as%20the%20inability%20to%0Aseparate%20similarly%20colored%20objects%20without%20additional%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17283v2&entry.124074799=Read"},
{"title": "An approach to improve agent learning via guaranteeing goal reaching in\n  all episodes", "author": "Pavel Osinenko and Grigory Yaremenko and Georgiy Malaniya and Anton Bolychev", "abstract": "  Reinforcement learning is commonly concerned with problems of maximizing\naccumulated rewards in Markov decision processes. Oftentimes, a certain goal\nstate or a subset of the state space attain maximal reward. In such a case, the\nenvironment may be considered solved when the goal is reached. Whereas numerous\ntechniques, learning or non-learning based, exist for solving environments,\ndoing so optimally is the biggest challenge. Say, one may choose a reward rate\nwhich penalizes the action effort. Reinforcement learning is currently among\nthe most actively developed frameworks for solving environments optimally by\nvirtue of maximizing accumulated reward, in other words, returns. Yet, tuning\nagents is a notoriously hard task as reported in a series of works. Our aim\nhere is to help the agent learn a near-optimal policy efficiently while\nensuring a goal reaching property of some basis policy that merely solves the\nenvironment. We suggest an algorithm, which is fairly flexible, and can be used\nto augment practically any agent as long as it comprises of a critic. A formal\nproof of a goal reaching property is provided. Simulation experiments on six\nproblems under five agents, including the benchmarked one, provided an\nempirical evidence that the learning can indeed be boosted while ensuring goal\nreaching property.\n", "link": "http://arxiv.org/abs/2405.18118v1", "date": "2024-05-28", "relevancy": 2.1116, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5821}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4899}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20approach%20to%20improve%20agent%20learning%20via%20guaranteeing%20goal%20reaching%20in%0A%20%20all%20episodes&body=Title%3A%20An%20approach%20to%20improve%20agent%20learning%20via%20guaranteeing%20goal%20reaching%20in%0A%20%20all%20episodes%0AAuthor%3A%20Pavel%20Osinenko%20and%20Grigory%20Yaremenko%20and%20Georgiy%20Malaniya%20and%20Anton%20Bolychev%0AAbstract%3A%20%20%20Reinforcement%20learning%20is%20commonly%20concerned%20with%20problems%20of%20maximizing%0Aaccumulated%20rewards%20in%20Markov%20decision%20processes.%20Oftentimes%2C%20a%20certain%20goal%0Astate%20or%20a%20subset%20of%20the%20state%20space%20attain%20maximal%20reward.%20In%20such%20a%20case%2C%20the%0Aenvironment%20may%20be%20considered%20solved%20when%20the%20goal%20is%20reached.%20Whereas%20numerous%0Atechniques%2C%20learning%20or%20non-learning%20based%2C%20exist%20for%20solving%20environments%2C%0Adoing%20so%20optimally%20is%20the%20biggest%20challenge.%20Say%2C%20one%20may%20choose%20a%20reward%20rate%0Awhich%20penalizes%20the%20action%20effort.%20Reinforcement%20learning%20is%20currently%20among%0Athe%20most%20actively%20developed%20frameworks%20for%20solving%20environments%20optimally%20by%0Avirtue%20of%20maximizing%20accumulated%20reward%2C%20in%20other%20words%2C%20returns.%20Yet%2C%20tuning%0Aagents%20is%20a%20notoriously%20hard%20task%20as%20reported%20in%20a%20series%20of%20works.%20Our%20aim%0Ahere%20is%20to%20help%20the%20agent%20learn%20a%20near-optimal%20policy%20efficiently%20while%0Aensuring%20a%20goal%20reaching%20property%20of%20some%20basis%20policy%20that%20merely%20solves%20the%0Aenvironment.%20We%20suggest%20an%20algorithm%2C%20which%20is%20fairly%20flexible%2C%20and%20can%20be%20used%0Ato%20augment%20practically%20any%20agent%20as%20long%20as%20it%20comprises%20of%20a%20critic.%20A%20formal%0Aproof%20of%20a%20goal%20reaching%20property%20is%20provided.%20Simulation%20experiments%20on%20six%0Aproblems%20under%20five%20agents%2C%20including%20the%20benchmarked%20one%2C%20provided%20an%0Aempirical%20evidence%20that%20the%20learning%20can%20indeed%20be%20boosted%20while%20ensuring%20goal%0Areaching%20property.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520approach%2520to%2520improve%2520agent%2520learning%2520via%2520guaranteeing%2520goal%2520reaching%2520in%250A%2520%2520all%2520episodes%26entry.906535625%3DPavel%2520Osinenko%2520and%2520Grigory%2520Yaremenko%2520and%2520Georgiy%2520Malaniya%2520and%2520Anton%2520Bolychev%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520is%2520commonly%2520concerned%2520with%2520problems%2520of%2520maximizing%250Aaccumulated%2520rewards%2520in%2520Markov%2520decision%2520processes.%2520Oftentimes%252C%2520a%2520certain%2520goal%250Astate%2520or%2520a%2520subset%2520of%2520the%2520state%2520space%2520attain%2520maximal%2520reward.%2520In%2520such%2520a%2520case%252C%2520the%250Aenvironment%2520may%2520be%2520considered%2520solved%2520when%2520the%2520goal%2520is%2520reached.%2520Whereas%2520numerous%250Atechniques%252C%2520learning%2520or%2520non-learning%2520based%252C%2520exist%2520for%2520solving%2520environments%252C%250Adoing%2520so%2520optimally%2520is%2520the%2520biggest%2520challenge.%2520Say%252C%2520one%2520may%2520choose%2520a%2520reward%2520rate%250Awhich%2520penalizes%2520the%2520action%2520effort.%2520Reinforcement%2520learning%2520is%2520currently%2520among%250Athe%2520most%2520actively%2520developed%2520frameworks%2520for%2520solving%2520environments%2520optimally%2520by%250Avirtue%2520of%2520maximizing%2520accumulated%2520reward%252C%2520in%2520other%2520words%252C%2520returns.%2520Yet%252C%2520tuning%250Aagents%2520is%2520a%2520notoriously%2520hard%2520task%2520as%2520reported%2520in%2520a%2520series%2520of%2520works.%2520Our%2520aim%250Ahere%2520is%2520to%2520help%2520the%2520agent%2520learn%2520a%2520near-optimal%2520policy%2520efficiently%2520while%250Aensuring%2520a%2520goal%2520reaching%2520property%2520of%2520some%2520basis%2520policy%2520that%2520merely%2520solves%2520the%250Aenvironment.%2520We%2520suggest%2520an%2520algorithm%252C%2520which%2520is%2520fairly%2520flexible%252C%2520and%2520can%2520be%2520used%250Ato%2520augment%2520practically%2520any%2520agent%2520as%2520long%2520as%2520it%2520comprises%2520of%2520a%2520critic.%2520A%2520formal%250Aproof%2520of%2520a%2520goal%2520reaching%2520property%2520is%2520provided.%2520Simulation%2520experiments%2520on%2520six%250Aproblems%2520under%2520five%2520agents%252C%2520including%2520the%2520benchmarked%2520one%252C%2520provided%2520an%250Aempirical%2520evidence%2520that%2520the%2520learning%2520can%2520indeed%2520be%2520boosted%2520while%2520ensuring%2520goal%250Areaching%2520property.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20approach%20to%20improve%20agent%20learning%20via%20guaranteeing%20goal%20reaching%20in%0A%20%20all%20episodes&entry.906535625=Pavel%20Osinenko%20and%20Grigory%20Yaremenko%20and%20Georgiy%20Malaniya%20and%20Anton%20Bolychev&entry.1292438233=%20%20Reinforcement%20learning%20is%20commonly%20concerned%20with%20problems%20of%20maximizing%0Aaccumulated%20rewards%20in%20Markov%20decision%20processes.%20Oftentimes%2C%20a%20certain%20goal%0Astate%20or%20a%20subset%20of%20the%20state%20space%20attain%20maximal%20reward.%20In%20such%20a%20case%2C%20the%0Aenvironment%20may%20be%20considered%20solved%20when%20the%20goal%20is%20reached.%20Whereas%20numerous%0Atechniques%2C%20learning%20or%20non-learning%20based%2C%20exist%20for%20solving%20environments%2C%0Adoing%20so%20optimally%20is%20the%20biggest%20challenge.%20Say%2C%20one%20may%20choose%20a%20reward%20rate%0Awhich%20penalizes%20the%20action%20effort.%20Reinforcement%20learning%20is%20currently%20among%0Athe%20most%20actively%20developed%20frameworks%20for%20solving%20environments%20optimally%20by%0Avirtue%20of%20maximizing%20accumulated%20reward%2C%20in%20other%20words%2C%20returns.%20Yet%2C%20tuning%0Aagents%20is%20a%20notoriously%20hard%20task%20as%20reported%20in%20a%20series%20of%20works.%20Our%20aim%0Ahere%20is%20to%20help%20the%20agent%20learn%20a%20near-optimal%20policy%20efficiently%20while%0Aensuring%20a%20goal%20reaching%20property%20of%20some%20basis%20policy%20that%20merely%20solves%20the%0Aenvironment.%20We%20suggest%20an%20algorithm%2C%20which%20is%20fairly%20flexible%2C%20and%20can%20be%20used%0Ato%20augment%20practically%20any%20agent%20as%20long%20as%20it%20comprises%20of%20a%20critic.%20A%20formal%0Aproof%20of%20a%20goal%20reaching%20property%20is%20provided.%20Simulation%20experiments%20on%20six%0Aproblems%20under%20five%20agents%2C%20including%20the%20benchmarked%20one%2C%20provided%20an%0Aempirical%20evidence%20that%20the%20learning%20can%20indeed%20be%20boosted%20while%20ensuring%20goal%0Areaching%20property.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18118v1&entry.124074799=Read"},
{"title": "Cooperative Relative Localization in MAV Swarms with Ultra-wideband\n  Ranging", "author": "Changrui Liu and Sven U. Pfeiffer and Guido C. H. E. de Croon", "abstract": "  Relative localization (RL) is essential for the successful operation of micro\nair vehicle (MAV) swarms. Achieving accurate 3-D RL in infrastructure-free and\nGPS-denied environments with only distance information is a challenging problem\nthat has not been satisfactorily solved. In this work, based on the range-based\npeer-to-peer RL using the ultra-wideband (UWB) ranging technique, we develop a\nnovel UWB-based cooperative relative localization (CRL) solution that\nintegrates the relative motion dynamics of each host-neighbor pair to build a\nunified dynamic model and takes the distances between the neighbors as\n\\textit{bonus information}. Observability analysis using differential geometry\nshows that the proposed CRL scheme can expand the observable subspace compared\nto other alternatives using only direct distances between the host agent and\nits neighbors. In addition, we apply the kernel-induced extended Kalman filter\n(EKF) to the CRL state estimation problem with the novel-designed\nLogarithmic-Versoria (LV) kernel to tackle heavy-tailed UWB noise. Sufficient\nconditions for the convergence of the fixed-point iteration involved in the\nestimation algorithm are also derived. Comparative Monte Carlo simulations\ndemonstrate that the proposed CRL scheme combined with the LV-kernel EKF\nsignificantly improves the estimation accuracy owing to its robustness against\nboth measurement outliers and incorrect measurement covariance matrix\ninitialization. Moreover, with the LV kernel, the estimation is still\nsatisfactory when performing the fixed-point iteration only once for reduced\ncomputational complexity.\n", "link": "http://arxiv.org/abs/2405.18234v1", "date": "2024-05-28", "relevancy": 2.1089, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5742}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5112}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperative%20Relative%20Localization%20in%20MAV%20Swarms%20with%20Ultra-wideband%0A%20%20Ranging&body=Title%3A%20Cooperative%20Relative%20Localization%20in%20MAV%20Swarms%20with%20Ultra-wideband%0A%20%20Ranging%0AAuthor%3A%20Changrui%20Liu%20and%20Sven%20U.%20Pfeiffer%20and%20Guido%20C.%20H.%20E.%20de%20Croon%0AAbstract%3A%20%20%20Relative%20localization%20%28RL%29%20is%20essential%20for%20the%20successful%20operation%20of%20micro%0Aair%20vehicle%20%28MAV%29%20swarms.%20Achieving%20accurate%203-D%20RL%20in%20infrastructure-free%20and%0AGPS-denied%20environments%20with%20only%20distance%20information%20is%20a%20challenging%20problem%0Athat%20has%20not%20been%20satisfactorily%20solved.%20In%20this%20work%2C%20based%20on%20the%20range-based%0Apeer-to-peer%20RL%20using%20the%20ultra-wideband%20%28UWB%29%20ranging%20technique%2C%20we%20develop%20a%0Anovel%20UWB-based%20cooperative%20relative%20localization%20%28CRL%29%20solution%20that%0Aintegrates%20the%20relative%20motion%20dynamics%20of%20each%20host-neighbor%20pair%20to%20build%20a%0Aunified%20dynamic%20model%20and%20takes%20the%20distances%20between%20the%20neighbors%20as%0A%5Ctextit%7Bbonus%20information%7D.%20Observability%20analysis%20using%20differential%20geometry%0Ashows%20that%20the%20proposed%20CRL%20scheme%20can%20expand%20the%20observable%20subspace%20compared%0Ato%20other%20alternatives%20using%20only%20direct%20distances%20between%20the%20host%20agent%20and%0Aits%20neighbors.%20In%20addition%2C%20we%20apply%20the%20kernel-induced%20extended%20Kalman%20filter%0A%28EKF%29%20to%20the%20CRL%20state%20estimation%20problem%20with%20the%20novel-designed%0ALogarithmic-Versoria%20%28LV%29%20kernel%20to%20tackle%20heavy-tailed%20UWB%20noise.%20Sufficient%0Aconditions%20for%20the%20convergence%20of%20the%20fixed-point%20iteration%20involved%20in%20the%0Aestimation%20algorithm%20are%20also%20derived.%20Comparative%20Monte%20Carlo%20simulations%0Ademonstrate%20that%20the%20proposed%20CRL%20scheme%20combined%20with%20the%20LV-kernel%20EKF%0Asignificantly%20improves%20the%20estimation%20accuracy%20owing%20to%20its%20robustness%20against%0Aboth%20measurement%20outliers%20and%20incorrect%20measurement%20covariance%20matrix%0Ainitialization.%20Moreover%2C%20with%20the%20LV%20kernel%2C%20the%20estimation%20is%20still%0Asatisfactory%20when%20performing%20the%20fixed-point%20iteration%20only%20once%20for%20reduced%0Acomputational%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperative%2520Relative%2520Localization%2520in%2520MAV%2520Swarms%2520with%2520Ultra-wideband%250A%2520%2520Ranging%26entry.906535625%3DChangrui%2520Liu%2520and%2520Sven%2520U.%2520Pfeiffer%2520and%2520Guido%2520C.%2520H.%2520E.%2520de%2520Croon%26entry.1292438233%3D%2520%2520Relative%2520localization%2520%2528RL%2529%2520is%2520essential%2520for%2520the%2520successful%2520operation%2520of%2520micro%250Aair%2520vehicle%2520%2528MAV%2529%2520swarms.%2520Achieving%2520accurate%25203-D%2520RL%2520in%2520infrastructure-free%2520and%250AGPS-denied%2520environments%2520with%2520only%2520distance%2520information%2520is%2520a%2520challenging%2520problem%250Athat%2520has%2520not%2520been%2520satisfactorily%2520solved.%2520In%2520this%2520work%252C%2520based%2520on%2520the%2520range-based%250Apeer-to-peer%2520RL%2520using%2520the%2520ultra-wideband%2520%2528UWB%2529%2520ranging%2520technique%252C%2520we%2520develop%2520a%250Anovel%2520UWB-based%2520cooperative%2520relative%2520localization%2520%2528CRL%2529%2520solution%2520that%250Aintegrates%2520the%2520relative%2520motion%2520dynamics%2520of%2520each%2520host-neighbor%2520pair%2520to%2520build%2520a%250Aunified%2520dynamic%2520model%2520and%2520takes%2520the%2520distances%2520between%2520the%2520neighbors%2520as%250A%255Ctextit%257Bbonus%2520information%257D.%2520Observability%2520analysis%2520using%2520differential%2520geometry%250Ashows%2520that%2520the%2520proposed%2520CRL%2520scheme%2520can%2520expand%2520the%2520observable%2520subspace%2520compared%250Ato%2520other%2520alternatives%2520using%2520only%2520direct%2520distances%2520between%2520the%2520host%2520agent%2520and%250Aits%2520neighbors.%2520In%2520addition%252C%2520we%2520apply%2520the%2520kernel-induced%2520extended%2520Kalman%2520filter%250A%2528EKF%2529%2520to%2520the%2520CRL%2520state%2520estimation%2520problem%2520with%2520the%2520novel-designed%250ALogarithmic-Versoria%2520%2528LV%2529%2520kernel%2520to%2520tackle%2520heavy-tailed%2520UWB%2520noise.%2520Sufficient%250Aconditions%2520for%2520the%2520convergence%2520of%2520the%2520fixed-point%2520iteration%2520involved%2520in%2520the%250Aestimation%2520algorithm%2520are%2520also%2520derived.%2520Comparative%2520Monte%2520Carlo%2520simulations%250Ademonstrate%2520that%2520the%2520proposed%2520CRL%2520scheme%2520combined%2520with%2520the%2520LV-kernel%2520EKF%250Asignificantly%2520improves%2520the%2520estimation%2520accuracy%2520owing%2520to%2520its%2520robustness%2520against%250Aboth%2520measurement%2520outliers%2520and%2520incorrect%2520measurement%2520covariance%2520matrix%250Ainitialization.%2520Moreover%252C%2520with%2520the%2520LV%2520kernel%252C%2520the%2520estimation%2520is%2520still%250Asatisfactory%2520when%2520performing%2520the%2520fixed-point%2520iteration%2520only%2520once%2520for%2520reduced%250Acomputational%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20Relative%20Localization%20in%20MAV%20Swarms%20with%20Ultra-wideband%0A%20%20Ranging&entry.906535625=Changrui%20Liu%20and%20Sven%20U.%20Pfeiffer%20and%20Guido%20C.%20H.%20E.%20de%20Croon&entry.1292438233=%20%20Relative%20localization%20%28RL%29%20is%20essential%20for%20the%20successful%20operation%20of%20micro%0Aair%20vehicle%20%28MAV%29%20swarms.%20Achieving%20accurate%203-D%20RL%20in%20infrastructure-free%20and%0AGPS-denied%20environments%20with%20only%20distance%20information%20is%20a%20challenging%20problem%0Athat%20has%20not%20been%20satisfactorily%20solved.%20In%20this%20work%2C%20based%20on%20the%20range-based%0Apeer-to-peer%20RL%20using%20the%20ultra-wideband%20%28UWB%29%20ranging%20technique%2C%20we%20develop%20a%0Anovel%20UWB-based%20cooperative%20relative%20localization%20%28CRL%29%20solution%20that%0Aintegrates%20the%20relative%20motion%20dynamics%20of%20each%20host-neighbor%20pair%20to%20build%20a%0Aunified%20dynamic%20model%20and%20takes%20the%20distances%20between%20the%20neighbors%20as%0A%5Ctextit%7Bbonus%20information%7D.%20Observability%20analysis%20using%20differential%20geometry%0Ashows%20that%20the%20proposed%20CRL%20scheme%20can%20expand%20the%20observable%20subspace%20compared%0Ato%20other%20alternatives%20using%20only%20direct%20distances%20between%20the%20host%20agent%20and%0Aits%20neighbors.%20In%20addition%2C%20we%20apply%20the%20kernel-induced%20extended%20Kalman%20filter%0A%28EKF%29%20to%20the%20CRL%20state%20estimation%20problem%20with%20the%20novel-designed%0ALogarithmic-Versoria%20%28LV%29%20kernel%20to%20tackle%20heavy-tailed%20UWB%20noise.%20Sufficient%0Aconditions%20for%20the%20convergence%20of%20the%20fixed-point%20iteration%20involved%20in%20the%0Aestimation%20algorithm%20are%20also%20derived.%20Comparative%20Monte%20Carlo%20simulations%0Ademonstrate%20that%20the%20proposed%20CRL%20scheme%20combined%20with%20the%20LV-kernel%20EKF%0Asignificantly%20improves%20the%20estimation%20accuracy%20owing%20to%20its%20robustness%20against%0Aboth%20measurement%20outliers%20and%20incorrect%20measurement%20covariance%20matrix%0Ainitialization.%20Moreover%2C%20with%20the%20LV%20kernel%2C%20the%20estimation%20is%20still%0Asatisfactory%20when%20performing%20the%20fixed-point%20iteration%20only%20once%20for%20reduced%0Acomputational%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18234v1&entry.124074799=Read"},
{"title": "MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any\n  Resolution", "author": "Wenzhuo Liu and Fei Zhu and Shijie Ma and Cheng-Lin Liu", "abstract": "  Although Vision Transformers (ViTs) have recently advanced computer vision\ntasks significantly, an important real-world problem was overlooked: adapting\nto variable input resolutions. Typically, images are resized to a fixed\nresolution, such as 224x224, for efficiency during training and inference.\nHowever, uniform input size conflicts with real-world scenarios where images\nnaturally vary in resolution. Modifying the preset resolution of a model may\nseverely degrade the performance. In this work, we propose to enhance the model\nadaptability to resolution variation by optimizing the patch embedding. The\nproposed method, called Multi-Scale Patch Embedding (MSPE), substitutes the\nstandard patch embedding with multiple variable-sized patch kernels and selects\nthe best parameters for different resolutions, eliminating the need to resize\nthe original image. Our method does not require high-cost training or\nmodifications to other parts, making it easy to apply to most ViT models.\nExperiments in image classification, segmentation, and detection tasks\ndemonstrate the effectiveness of MSPE, yielding superior performance on\nlow-resolution inputs and performing comparably on high-resolution inputs with\nexisting methods.\n", "link": "http://arxiv.org/abs/2405.18240v1", "date": "2024-05-28", "relevancy": 2.1046, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5385}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5232}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSPE%3A%20Multi-Scale%20Patch%20Embedding%20Prompts%20Vision%20Transformers%20to%20Any%0A%20%20Resolution&body=Title%3A%20MSPE%3A%20Multi-Scale%20Patch%20Embedding%20Prompts%20Vision%20Transformers%20to%20Any%0A%20%20Resolution%0AAuthor%3A%20Wenzhuo%20Liu%20and%20Fei%20Zhu%20and%20Shijie%20Ma%20and%20Cheng-Lin%20Liu%0AAbstract%3A%20%20%20Although%20Vision%20Transformers%20%28ViTs%29%20have%20recently%20advanced%20computer%20vision%0Atasks%20significantly%2C%20an%20important%20real-world%20problem%20was%20overlooked%3A%20adapting%0Ato%20variable%20input%20resolutions.%20Typically%2C%20images%20are%20resized%20to%20a%20fixed%0Aresolution%2C%20such%20as%20224x224%2C%20for%20efficiency%20during%20training%20and%20inference.%0AHowever%2C%20uniform%20input%20size%20conflicts%20with%20real-world%20scenarios%20where%20images%0Anaturally%20vary%20in%20resolution.%20Modifying%20the%20preset%20resolution%20of%20a%20model%20may%0Aseverely%20degrade%20the%20performance.%20In%20this%20work%2C%20we%20propose%20to%20enhance%20the%20model%0Aadaptability%20to%20resolution%20variation%20by%20optimizing%20the%20patch%20embedding.%20The%0Aproposed%20method%2C%20called%20Multi-Scale%20Patch%20Embedding%20%28MSPE%29%2C%20substitutes%20the%0Astandard%20patch%20embedding%20with%20multiple%20variable-sized%20patch%20kernels%20and%20selects%0Athe%20best%20parameters%20for%20different%20resolutions%2C%20eliminating%20the%20need%20to%20resize%0Athe%20original%20image.%20Our%20method%20does%20not%20require%20high-cost%20training%20or%0Amodifications%20to%20other%20parts%2C%20making%20it%20easy%20to%20apply%20to%20most%20ViT%20models.%0AExperiments%20in%20image%20classification%2C%20segmentation%2C%20and%20detection%20tasks%0Ademonstrate%20the%20effectiveness%20of%20MSPE%2C%20yielding%20superior%20performance%20on%0Alow-resolution%20inputs%20and%20performing%20comparably%20on%20high-resolution%20inputs%20with%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSPE%253A%2520Multi-Scale%2520Patch%2520Embedding%2520Prompts%2520Vision%2520Transformers%2520to%2520Any%250A%2520%2520Resolution%26entry.906535625%3DWenzhuo%2520Liu%2520and%2520Fei%2520Zhu%2520and%2520Shijie%2520Ma%2520and%2520Cheng-Lin%2520Liu%26entry.1292438233%3D%2520%2520Although%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520recently%2520advanced%2520computer%2520vision%250Atasks%2520significantly%252C%2520an%2520important%2520real-world%2520problem%2520was%2520overlooked%253A%2520adapting%250Ato%2520variable%2520input%2520resolutions.%2520Typically%252C%2520images%2520are%2520resized%2520to%2520a%2520fixed%250Aresolution%252C%2520such%2520as%2520224x224%252C%2520for%2520efficiency%2520during%2520training%2520and%2520inference.%250AHowever%252C%2520uniform%2520input%2520size%2520conflicts%2520with%2520real-world%2520scenarios%2520where%2520images%250Anaturally%2520vary%2520in%2520resolution.%2520Modifying%2520the%2520preset%2520resolution%2520of%2520a%2520model%2520may%250Aseverely%2520degrade%2520the%2520performance.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520enhance%2520the%2520model%250Aadaptability%2520to%2520resolution%2520variation%2520by%2520optimizing%2520the%2520patch%2520embedding.%2520The%250Aproposed%2520method%252C%2520called%2520Multi-Scale%2520Patch%2520Embedding%2520%2528MSPE%2529%252C%2520substitutes%2520the%250Astandard%2520patch%2520embedding%2520with%2520multiple%2520variable-sized%2520patch%2520kernels%2520and%2520selects%250Athe%2520best%2520parameters%2520for%2520different%2520resolutions%252C%2520eliminating%2520the%2520need%2520to%2520resize%250Athe%2520original%2520image.%2520Our%2520method%2520does%2520not%2520require%2520high-cost%2520training%2520or%250Amodifications%2520to%2520other%2520parts%252C%2520making%2520it%2520easy%2520to%2520apply%2520to%2520most%2520ViT%2520models.%250AExperiments%2520in%2520image%2520classification%252C%2520segmentation%252C%2520and%2520detection%2520tasks%250Ademonstrate%2520the%2520effectiveness%2520of%2520MSPE%252C%2520yielding%2520superior%2520performance%2520on%250Alow-resolution%2520inputs%2520and%2520performing%2520comparably%2520on%2520high-resolution%2520inputs%2520with%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSPE%3A%20Multi-Scale%20Patch%20Embedding%20Prompts%20Vision%20Transformers%20to%20Any%0A%20%20Resolution&entry.906535625=Wenzhuo%20Liu%20and%20Fei%20Zhu%20and%20Shijie%20Ma%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20Although%20Vision%20Transformers%20%28ViTs%29%20have%20recently%20advanced%20computer%20vision%0Atasks%20significantly%2C%20an%20important%20real-world%20problem%20was%20overlooked%3A%20adapting%0Ato%20variable%20input%20resolutions.%20Typically%2C%20images%20are%20resized%20to%20a%20fixed%0Aresolution%2C%20such%20as%20224x224%2C%20for%20efficiency%20during%20training%20and%20inference.%0AHowever%2C%20uniform%20input%20size%20conflicts%20with%20real-world%20scenarios%20where%20images%0Anaturally%20vary%20in%20resolution.%20Modifying%20the%20preset%20resolution%20of%20a%20model%20may%0Aseverely%20degrade%20the%20performance.%20In%20this%20work%2C%20we%20propose%20to%20enhance%20the%20model%0Aadaptability%20to%20resolution%20variation%20by%20optimizing%20the%20patch%20embedding.%20The%0Aproposed%20method%2C%20called%20Multi-Scale%20Patch%20Embedding%20%28MSPE%29%2C%20substitutes%20the%0Astandard%20patch%20embedding%20with%20multiple%20variable-sized%20patch%20kernels%20and%20selects%0Athe%20best%20parameters%20for%20different%20resolutions%2C%20eliminating%20the%20need%20to%20resize%0Athe%20original%20image.%20Our%20method%20does%20not%20require%20high-cost%20training%20or%0Amodifications%20to%20other%20parts%2C%20making%20it%20easy%20to%20apply%20to%20most%20ViT%20models.%0AExperiments%20in%20image%20classification%2C%20segmentation%2C%20and%20detection%20tasks%0Ademonstrate%20the%20effectiveness%20of%20MSPE%2C%20yielding%20superior%20performance%20on%0Alow-resolution%20inputs%20and%20performing%20comparably%20on%20high-resolution%20inputs%20with%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18240v1&entry.124074799=Read"},
{"title": "Time Series Representation Models", "author": "Robert Leppich and Vanessa Borst and Veronika Lesch and Samuel Kounev", "abstract": "  Time series analysis remains a major challenge due to its sparse\ncharacteristics, high dimensionality, and inconsistent data quality. Recent\nadvancements in transformer-based techniques have enhanced capabilities in\nforecasting and imputation; however, these methods are still resource-heavy,\nlack adaptability, and face difficulties in integrating both local and global\nattributes of time series. To tackle these challenges, we propose a new\narchitectural concept for time series analysis based on introspection. Central\nto this concept is the self-supervised pretraining of Time Series\nRepresentation Models (TSRMs), which once learned can be easily tailored and\nfine-tuned for specific tasks, such as forecasting and imputation, in an\nautomated and resource-efficient manner. Our architecture is equipped with a\nflexible and hierarchical representation learning process, which is robust\nagainst missing data and outliers. It can capture and learn both local and\nglobal features of the structure, semantics, and crucial patterns of a given\ntime series category, such as heart rate data. Our learned time series\nrepresentation models can be efficiently adapted to a specific task, such as\nforecasting or imputation, without manual intervention. Furthermore, our\narchitecture's design supports explainability by highlighting the significance\nof each input value for the task at hand. Our empirical study using four\nbenchmark datasets shows that, compared to investigated state-of-the-art\nbaseline methods, our architecture improves imputation and forecasting errors\nby up to 90.34% and 71.54%, respectively, while reducing the required trainable\nparameters by up to 92.43%. The source code is available at\nhttps://github.com/RobertLeppich/TSRM.\n", "link": "http://arxiv.org/abs/2405.18165v1", "date": "2024-05-28", "relevancy": 2.1024, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.534}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5228}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%20Series%20Representation%20Models&body=Title%3A%20Time%20Series%20Representation%20Models%0AAuthor%3A%20Robert%20Leppich%20and%20Vanessa%20Borst%20and%20Veronika%20Lesch%20and%20Samuel%20Kounev%0AAbstract%3A%20%20%20Time%20series%20analysis%20remains%20a%20major%20challenge%20due%20to%20its%20sparse%0Acharacteristics%2C%20high%20dimensionality%2C%20and%20inconsistent%20data%20quality.%20Recent%0Aadvancements%20in%20transformer-based%20techniques%20have%20enhanced%20capabilities%20in%0Aforecasting%20and%20imputation%3B%20however%2C%20these%20methods%20are%20still%20resource-heavy%2C%0Alack%20adaptability%2C%20and%20face%20difficulties%20in%20integrating%20both%20local%20and%20global%0Aattributes%20of%20time%20series.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20new%0Aarchitectural%20concept%20for%20time%20series%20analysis%20based%20on%20introspection.%20Central%0Ato%20this%20concept%20is%20the%20self-supervised%20pretraining%20of%20Time%20Series%0ARepresentation%20Models%20%28TSRMs%29%2C%20which%20once%20learned%20can%20be%20easily%20tailored%20and%0Afine-tuned%20for%20specific%20tasks%2C%20such%20as%20forecasting%20and%20imputation%2C%20in%20an%0Aautomated%20and%20resource-efficient%20manner.%20Our%20architecture%20is%20equipped%20with%20a%0Aflexible%20and%20hierarchical%20representation%20learning%20process%2C%20which%20is%20robust%0Aagainst%20missing%20data%20and%20outliers.%20It%20can%20capture%20and%20learn%20both%20local%20and%0Aglobal%20features%20of%20the%20structure%2C%20semantics%2C%20and%20crucial%20patterns%20of%20a%20given%0Atime%20series%20category%2C%20such%20as%20heart%20rate%20data.%20Our%20learned%20time%20series%0Arepresentation%20models%20can%20be%20efficiently%20adapted%20to%20a%20specific%20task%2C%20such%20as%0Aforecasting%20or%20imputation%2C%20without%20manual%20intervention.%20Furthermore%2C%20our%0Aarchitecture%27s%20design%20supports%20explainability%20by%20highlighting%20the%20significance%0Aof%20each%20input%20value%20for%20the%20task%20at%20hand.%20Our%20empirical%20study%20using%20four%0Abenchmark%20datasets%20shows%20that%2C%20compared%20to%20investigated%20state-of-the-art%0Abaseline%20methods%2C%20our%20architecture%20improves%20imputation%20and%20forecasting%20errors%0Aby%20up%20to%2090.34%25%20and%2071.54%25%2C%20respectively%2C%20while%20reducing%20the%20required%20trainable%0Aparameters%20by%20up%20to%2092.43%25.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/RobertLeppich/TSRM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2520Series%2520Representation%2520Models%26entry.906535625%3DRobert%2520Leppich%2520and%2520Vanessa%2520Borst%2520and%2520Veronika%2520Lesch%2520and%2520Samuel%2520Kounev%26entry.1292438233%3D%2520%2520Time%2520series%2520analysis%2520remains%2520a%2520major%2520challenge%2520due%2520to%2520its%2520sparse%250Acharacteristics%252C%2520high%2520dimensionality%252C%2520and%2520inconsistent%2520data%2520quality.%2520Recent%250Aadvancements%2520in%2520transformer-based%2520techniques%2520have%2520enhanced%2520capabilities%2520in%250Aforecasting%2520and%2520imputation%253B%2520however%252C%2520these%2520methods%2520are%2520still%2520resource-heavy%252C%250Alack%2520adaptability%252C%2520and%2520face%2520difficulties%2520in%2520integrating%2520both%2520local%2520and%2520global%250Aattributes%2520of%2520time%2520series.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520a%2520new%250Aarchitectural%2520concept%2520for%2520time%2520series%2520analysis%2520based%2520on%2520introspection.%2520Central%250Ato%2520this%2520concept%2520is%2520the%2520self-supervised%2520pretraining%2520of%2520Time%2520Series%250ARepresentation%2520Models%2520%2528TSRMs%2529%252C%2520which%2520once%2520learned%2520can%2520be%2520easily%2520tailored%2520and%250Afine-tuned%2520for%2520specific%2520tasks%252C%2520such%2520as%2520forecasting%2520and%2520imputation%252C%2520in%2520an%250Aautomated%2520and%2520resource-efficient%2520manner.%2520Our%2520architecture%2520is%2520equipped%2520with%2520a%250Aflexible%2520and%2520hierarchical%2520representation%2520learning%2520process%252C%2520which%2520is%2520robust%250Aagainst%2520missing%2520data%2520and%2520outliers.%2520It%2520can%2520capture%2520and%2520learn%2520both%2520local%2520and%250Aglobal%2520features%2520of%2520the%2520structure%252C%2520semantics%252C%2520and%2520crucial%2520patterns%2520of%2520a%2520given%250Atime%2520series%2520category%252C%2520such%2520as%2520heart%2520rate%2520data.%2520Our%2520learned%2520time%2520series%250Arepresentation%2520models%2520can%2520be%2520efficiently%2520adapted%2520to%2520a%2520specific%2520task%252C%2520such%2520as%250Aforecasting%2520or%2520imputation%252C%2520without%2520manual%2520intervention.%2520Furthermore%252C%2520our%250Aarchitecture%2527s%2520design%2520supports%2520explainability%2520by%2520highlighting%2520the%2520significance%250Aof%2520each%2520input%2520value%2520for%2520the%2520task%2520at%2520hand.%2520Our%2520empirical%2520study%2520using%2520four%250Abenchmark%2520datasets%2520shows%2520that%252C%2520compared%2520to%2520investigated%2520state-of-the-art%250Abaseline%2520methods%252C%2520our%2520architecture%2520improves%2520imputation%2520and%2520forecasting%2520errors%250Aby%2520up%2520to%252090.34%2525%2520and%252071.54%2525%252C%2520respectively%252C%2520while%2520reducing%2520the%2520required%2520trainable%250Aparameters%2520by%2520up%2520to%252092.43%2525.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/RobertLeppich/TSRM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20Series%20Representation%20Models&entry.906535625=Robert%20Leppich%20and%20Vanessa%20Borst%20and%20Veronika%20Lesch%20and%20Samuel%20Kounev&entry.1292438233=%20%20Time%20series%20analysis%20remains%20a%20major%20challenge%20due%20to%20its%20sparse%0Acharacteristics%2C%20high%20dimensionality%2C%20and%20inconsistent%20data%20quality.%20Recent%0Aadvancements%20in%20transformer-based%20techniques%20have%20enhanced%20capabilities%20in%0Aforecasting%20and%20imputation%3B%20however%2C%20these%20methods%20are%20still%20resource-heavy%2C%0Alack%20adaptability%2C%20and%20face%20difficulties%20in%20integrating%20both%20local%20and%20global%0Aattributes%20of%20time%20series.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20new%0Aarchitectural%20concept%20for%20time%20series%20analysis%20based%20on%20introspection.%20Central%0Ato%20this%20concept%20is%20the%20self-supervised%20pretraining%20of%20Time%20Series%0ARepresentation%20Models%20%28TSRMs%29%2C%20which%20once%20learned%20can%20be%20easily%20tailored%20and%0Afine-tuned%20for%20specific%20tasks%2C%20such%20as%20forecasting%20and%20imputation%2C%20in%20an%0Aautomated%20and%20resource-efficient%20manner.%20Our%20architecture%20is%20equipped%20with%20a%0Aflexible%20and%20hierarchical%20representation%20learning%20process%2C%20which%20is%20robust%0Aagainst%20missing%20data%20and%20outliers.%20It%20can%20capture%20and%20learn%20both%20local%20and%0Aglobal%20features%20of%20the%20structure%2C%20semantics%2C%20and%20crucial%20patterns%20of%20a%20given%0Atime%20series%20category%2C%20such%20as%20heart%20rate%20data.%20Our%20learned%20time%20series%0Arepresentation%20models%20can%20be%20efficiently%20adapted%20to%20a%20specific%20task%2C%20such%20as%0Aforecasting%20or%20imputation%2C%20without%20manual%20intervention.%20Furthermore%2C%20our%0Aarchitecture%27s%20design%20supports%20explainability%20by%20highlighting%20the%20significance%0Aof%20each%20input%20value%20for%20the%20task%20at%20hand.%20Our%20empirical%20study%20using%20four%0Abenchmark%20datasets%20shows%20that%2C%20compared%20to%20investigated%20state-of-the-art%0Abaseline%20methods%2C%20our%20architecture%20improves%20imputation%20and%20forecasting%20errors%0Aby%20up%20to%2090.34%25%20and%2071.54%25%2C%20respectively%2C%20while%20reducing%20the%20required%20trainable%0Aparameters%20by%20up%20to%2092.43%25.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/RobertLeppich/TSRM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18165v1&entry.124074799=Read"},
{"title": "Automated Real-World Sustainability Data Generation from Images of\n  Buildings", "author": "Peter J Bentley and Soo Ling Lim and Rajat Mathur and Sid Narang", "abstract": "  When data on building features is unavailable, the task of determining how to\nimprove that building in terms of carbon emissions becomes infeasible. We show\nthat from only a set of images, a Large Language Model with appropriate prompt\nengineering and domain knowledge can successfully estimate a range of building\nfeatures relevant for sustainability calculations. We compare our novel\nimage-to-data method with a ground truth comprising real building data for 47\napartments and achieve accuracy better than a human performing the same task.\nWe also demonstrate that the method can generate tailored recommendations to\nthe owner on how best to improve their properties and discuss methods to scale\nthe approach.\n", "link": "http://arxiv.org/abs/2405.18064v1", "date": "2024-05-28", "relevancy": 2.0988, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5384}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5352}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Real-World%20Sustainability%20Data%20Generation%20from%20Images%20of%0A%20%20Buildings&body=Title%3A%20Automated%20Real-World%20Sustainability%20Data%20Generation%20from%20Images%20of%0A%20%20Buildings%0AAuthor%3A%20Peter%20J%20Bentley%20and%20Soo%20Ling%20Lim%20and%20Rajat%20Mathur%20and%20Sid%20Narang%0AAbstract%3A%20%20%20When%20data%20on%20building%20features%20is%20unavailable%2C%20the%20task%20of%20determining%20how%20to%0Aimprove%20that%20building%20in%20terms%20of%20carbon%20emissions%20becomes%20infeasible.%20We%20show%0Athat%20from%20only%20a%20set%20of%20images%2C%20a%20Large%20Language%20Model%20with%20appropriate%20prompt%0Aengineering%20and%20domain%20knowledge%20can%20successfully%20estimate%20a%20range%20of%20building%0Afeatures%20relevant%20for%20sustainability%20calculations.%20We%20compare%20our%20novel%0Aimage-to-data%20method%20with%20a%20ground%20truth%20comprising%20real%20building%20data%20for%2047%0Aapartments%20and%20achieve%20accuracy%20better%20than%20a%20human%20performing%20the%20same%20task.%0AWe%20also%20demonstrate%20that%20the%20method%20can%20generate%20tailored%20recommendations%20to%0Athe%20owner%20on%20how%20best%20to%20improve%20their%20properties%20and%20discuss%20methods%20to%20scale%0Athe%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18064v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Real-World%2520Sustainability%2520Data%2520Generation%2520from%2520Images%2520of%250A%2520%2520Buildings%26entry.906535625%3DPeter%2520J%2520Bentley%2520and%2520Soo%2520Ling%2520Lim%2520and%2520Rajat%2520Mathur%2520and%2520Sid%2520Narang%26entry.1292438233%3D%2520%2520When%2520data%2520on%2520building%2520features%2520is%2520unavailable%252C%2520the%2520task%2520of%2520determining%2520how%2520to%250Aimprove%2520that%2520building%2520in%2520terms%2520of%2520carbon%2520emissions%2520becomes%2520infeasible.%2520We%2520show%250Athat%2520from%2520only%2520a%2520set%2520of%2520images%252C%2520a%2520Large%2520Language%2520Model%2520with%2520appropriate%2520prompt%250Aengineering%2520and%2520domain%2520knowledge%2520can%2520successfully%2520estimate%2520a%2520range%2520of%2520building%250Afeatures%2520relevant%2520for%2520sustainability%2520calculations.%2520We%2520compare%2520our%2520novel%250Aimage-to-data%2520method%2520with%2520a%2520ground%2520truth%2520comprising%2520real%2520building%2520data%2520for%252047%250Aapartments%2520and%2520achieve%2520accuracy%2520better%2520than%2520a%2520human%2520performing%2520the%2520same%2520task.%250AWe%2520also%2520demonstrate%2520that%2520the%2520method%2520can%2520generate%2520tailored%2520recommendations%2520to%250Athe%2520owner%2520on%2520how%2520best%2520to%2520improve%2520their%2520properties%2520and%2520discuss%2520methods%2520to%2520scale%250Athe%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18064v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Real-World%20Sustainability%20Data%20Generation%20from%20Images%20of%0A%20%20Buildings&entry.906535625=Peter%20J%20Bentley%20and%20Soo%20Ling%20Lim%20and%20Rajat%20Mathur%20and%20Sid%20Narang&entry.1292438233=%20%20When%20data%20on%20building%20features%20is%20unavailable%2C%20the%20task%20of%20determining%20how%20to%0Aimprove%20that%20building%20in%20terms%20of%20carbon%20emissions%20becomes%20infeasible.%20We%20show%0Athat%20from%20only%20a%20set%20of%20images%2C%20a%20Large%20Language%20Model%20with%20appropriate%20prompt%0Aengineering%20and%20domain%20knowledge%20can%20successfully%20estimate%20a%20range%20of%20building%0Afeatures%20relevant%20for%20sustainability%20calculations.%20We%20compare%20our%20novel%0Aimage-to-data%20method%20with%20a%20ground%20truth%20comprising%20real%20building%20data%20for%2047%0Aapartments%20and%20achieve%20accuracy%20better%20than%20a%20human%20performing%20the%20same%20task.%0AWe%20also%20demonstrate%20that%20the%20method%20can%20generate%20tailored%20recommendations%20to%0Athe%20owner%20on%20how%20best%20to%20improve%20their%20properties%20and%20discuss%20methods%20to%20scale%0Athe%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18064v1&entry.124074799=Read"},
{"title": "Adaptive debiased SGD in high-dimensional GLMs with steaming data", "author": "Ruijian Han and Lan Luo and Yuanhang Luo and Yuanyuan Lin and Jian Huang", "abstract": "  Online statistical inference facilitates real-time analysis of sequentially\ncollected data, making it different from traditional methods that rely on\nstatic datasets. This paper introduces a novel approach to online inference in\nhigh-dimensional generalized linear models, where we update regression\ncoefficient estimates and their standard errors upon each new data arrival. In\ncontrast to existing methods that either require full dataset access or\nlarge-dimensional summary statistics storage, our method operates in a\nsingle-pass mode, significantly reducing both time and space complexity. The\ncore of our methodological innovation lies in an adaptive stochastic gradient\ndescent algorithm tailored for dynamic objective functions, coupled with a\nnovel online debiasing procedure. This allows us to maintain low-dimensional\nsummary statistics while effectively controlling optimization errors introduced\nby the dynamically changing loss functions. We demonstrate that our method,\ntermed the Approximated Debiased Lasso (ADL), not only mitigates the need for\nthe bounded individual probability condition but also significantly improves\nnumerical performance. Numerical experiments demonstrate that the proposed ADL\nmethod consistently exhibits robust performance across various covariance\nmatrix structures.\n", "link": "http://arxiv.org/abs/2405.18284v1", "date": "2024-05-28", "relevancy": 2.0982, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5306}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5267}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20debiased%20SGD%20in%20high-dimensional%20GLMs%20with%20steaming%20data&body=Title%3A%20Adaptive%20debiased%20SGD%20in%20high-dimensional%20GLMs%20with%20steaming%20data%0AAuthor%3A%20Ruijian%20Han%20and%20Lan%20Luo%20and%20Yuanhang%20Luo%20and%20Yuanyuan%20Lin%20and%20Jian%20Huang%0AAbstract%3A%20%20%20Online%20statistical%20inference%20facilitates%20real-time%20analysis%20of%20sequentially%0Acollected%20data%2C%20making%20it%20different%20from%20traditional%20methods%20that%20rely%20on%0Astatic%20datasets.%20This%20paper%20introduces%20a%20novel%20approach%20to%20online%20inference%20in%0Ahigh-dimensional%20generalized%20linear%20models%2C%20where%20we%20update%20regression%0Acoefficient%20estimates%20and%20their%20standard%20errors%20upon%20each%20new%20data%20arrival.%20In%0Acontrast%20to%20existing%20methods%20that%20either%20require%20full%20dataset%20access%20or%0Alarge-dimensional%20summary%20statistics%20storage%2C%20our%20method%20operates%20in%20a%0Asingle-pass%20mode%2C%20significantly%20reducing%20both%20time%20and%20space%20complexity.%20The%0Acore%20of%20our%20methodological%20innovation%20lies%20in%20an%20adaptive%20stochastic%20gradient%0Adescent%20algorithm%20tailored%20for%20dynamic%20objective%20functions%2C%20coupled%20with%20a%0Anovel%20online%20debiasing%20procedure.%20This%20allows%20us%20to%20maintain%20low-dimensional%0Asummary%20statistics%20while%20effectively%20controlling%20optimization%20errors%20introduced%0Aby%20the%20dynamically%20changing%20loss%20functions.%20We%20demonstrate%20that%20our%20method%2C%0Atermed%20the%20Approximated%20Debiased%20Lasso%20%28ADL%29%2C%20not%20only%20mitigates%20the%20need%20for%0Athe%20bounded%20individual%20probability%20condition%20but%20also%20significantly%20improves%0Anumerical%20performance.%20Numerical%20experiments%20demonstrate%20that%20the%20proposed%20ADL%0Amethod%20consistently%20exhibits%20robust%20performance%20across%20various%20covariance%0Amatrix%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520debiased%2520SGD%2520in%2520high-dimensional%2520GLMs%2520with%2520steaming%2520data%26entry.906535625%3DRuijian%2520Han%2520and%2520Lan%2520Luo%2520and%2520Yuanhang%2520Luo%2520and%2520Yuanyuan%2520Lin%2520and%2520Jian%2520Huang%26entry.1292438233%3D%2520%2520Online%2520statistical%2520inference%2520facilitates%2520real-time%2520analysis%2520of%2520sequentially%250Acollected%2520data%252C%2520making%2520it%2520different%2520from%2520traditional%2520methods%2520that%2520rely%2520on%250Astatic%2520datasets.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520online%2520inference%2520in%250Ahigh-dimensional%2520generalized%2520linear%2520models%252C%2520where%2520we%2520update%2520regression%250Acoefficient%2520estimates%2520and%2520their%2520standard%2520errors%2520upon%2520each%2520new%2520data%2520arrival.%2520In%250Acontrast%2520to%2520existing%2520methods%2520that%2520either%2520require%2520full%2520dataset%2520access%2520or%250Alarge-dimensional%2520summary%2520statistics%2520storage%252C%2520our%2520method%2520operates%2520in%2520a%250Asingle-pass%2520mode%252C%2520significantly%2520reducing%2520both%2520time%2520and%2520space%2520complexity.%2520The%250Acore%2520of%2520our%2520methodological%2520innovation%2520lies%2520in%2520an%2520adaptive%2520stochastic%2520gradient%250Adescent%2520algorithm%2520tailored%2520for%2520dynamic%2520objective%2520functions%252C%2520coupled%2520with%2520a%250Anovel%2520online%2520debiasing%2520procedure.%2520This%2520allows%2520us%2520to%2520maintain%2520low-dimensional%250Asummary%2520statistics%2520while%2520effectively%2520controlling%2520optimization%2520errors%2520introduced%250Aby%2520the%2520dynamically%2520changing%2520loss%2520functions.%2520We%2520demonstrate%2520that%2520our%2520method%252C%250Atermed%2520the%2520Approximated%2520Debiased%2520Lasso%2520%2528ADL%2529%252C%2520not%2520only%2520mitigates%2520the%2520need%2520for%250Athe%2520bounded%2520individual%2520probability%2520condition%2520but%2520also%2520significantly%2520improves%250Anumerical%2520performance.%2520Numerical%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520ADL%250Amethod%2520consistently%2520exhibits%2520robust%2520performance%2520across%2520various%2520covariance%250Amatrix%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20debiased%20SGD%20in%20high-dimensional%20GLMs%20with%20steaming%20data&entry.906535625=Ruijian%20Han%20and%20Lan%20Luo%20and%20Yuanhang%20Luo%20and%20Yuanyuan%20Lin%20and%20Jian%20Huang&entry.1292438233=%20%20Online%20statistical%20inference%20facilitates%20real-time%20analysis%20of%20sequentially%0Acollected%20data%2C%20making%20it%20different%20from%20traditional%20methods%20that%20rely%20on%0Astatic%20datasets.%20This%20paper%20introduces%20a%20novel%20approach%20to%20online%20inference%20in%0Ahigh-dimensional%20generalized%20linear%20models%2C%20where%20we%20update%20regression%0Acoefficient%20estimates%20and%20their%20standard%20errors%20upon%20each%20new%20data%20arrival.%20In%0Acontrast%20to%20existing%20methods%20that%20either%20require%20full%20dataset%20access%20or%0Alarge-dimensional%20summary%20statistics%20storage%2C%20our%20method%20operates%20in%20a%0Asingle-pass%20mode%2C%20significantly%20reducing%20both%20time%20and%20space%20complexity.%20The%0Acore%20of%20our%20methodological%20innovation%20lies%20in%20an%20adaptive%20stochastic%20gradient%0Adescent%20algorithm%20tailored%20for%20dynamic%20objective%20functions%2C%20coupled%20with%20a%0Anovel%20online%20debiasing%20procedure.%20This%20allows%20us%20to%20maintain%20low-dimensional%0Asummary%20statistics%20while%20effectively%20controlling%20optimization%20errors%20introduced%0Aby%20the%20dynamically%20changing%20loss%20functions.%20We%20demonstrate%20that%20our%20method%2C%0Atermed%20the%20Approximated%20Debiased%20Lasso%20%28ADL%29%2C%20not%20only%20mitigates%20the%20need%20for%0Athe%20bounded%20individual%20probability%20condition%20but%20also%20significantly%20improves%0Anumerical%20performance.%20Numerical%20experiments%20demonstrate%20that%20the%20proposed%20ADL%0Amethod%20consistently%20exhibits%20robust%20performance%20across%20various%20covariance%0Amatrix%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18284v1&entry.124074799=Read"},
{"title": "MambaVC: Learned Visual Compression with Selective State Spaces", "author": "Shiyu Qin and Jinpeng Wang and Yimin Zhou and Bin Chen and Tianci Luo and Baoyi An and Tao Dai and Shutao Xia and Yaowei Wang", "abstract": "  Learned visual compression is an important and active task in multimedia.\nExisting approaches have explored various CNN- and Transformer-based designs to\nmodel content distribution and eliminate redundancy, where balancing efficacy\n(i.e., rate-distortion trade-off) and efficiency remains a challenge. Recently,\nstate-space models (SSMs) have shown promise due to their long-range modeling\ncapacity and efficiency. Inspired by this, we take the first step to explore\nSSMs for visual compression. We introduce MambaVC, a simple, strong and\nefficient compression network based on SSM. MambaVC develops a visual state\nspace (VSS) block with a 2D selective scanning (2DSS) module as the nonlinear\nactivation function after each downsampling, which helps to capture informative\nglobal contexts and enhances compression. On compression benchmark datasets,\nMambaVC achieves superior rate-distortion performance with lower computational\nand memory overheads. Specifically, it outperforms CNN and Transformer variants\nby 9.3% and 15.6% on Kodak, respectively, while reducing computation by 42% and\n24%, and saving 12% and 71% of memory. MambaVC shows even greater improvements\nwith high-resolution images, highlighting its potential and scalability in\nreal-world applications. We also provide a comprehensive comparison of\ndifferent network designs, underscoring MambaVC's advantages. Code is available\nat https://github.com/QinSY123/2024-MambaVC.\n", "link": "http://arxiv.org/abs/2405.15413v3", "date": "2024-05-28", "relevancy": 2.0943, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5493}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5207}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaVC%3A%20Learned%20Visual%20Compression%20with%20Selective%20State%20Spaces&body=Title%3A%20MambaVC%3A%20Learned%20Visual%20Compression%20with%20Selective%20State%20Spaces%0AAuthor%3A%20Shiyu%20Qin%20and%20Jinpeng%20Wang%20and%20Yimin%20Zhou%20and%20Bin%20Chen%20and%20Tianci%20Luo%20and%20Baoyi%20An%20and%20Tao%20Dai%20and%20Shutao%20Xia%20and%20Yaowei%20Wang%0AAbstract%3A%20%20%20Learned%20visual%20compression%20is%20an%20important%20and%20active%20task%20in%20multimedia.%0AExisting%20approaches%20have%20explored%20various%20CNN-%20and%20Transformer-based%20designs%20to%0Amodel%20content%20distribution%20and%20eliminate%20redundancy%2C%20where%20balancing%20efficacy%0A%28i.e.%2C%20rate-distortion%20trade-off%29%20and%20efficiency%20remains%20a%20challenge.%20Recently%2C%0Astate-space%20models%20%28SSMs%29%20have%20shown%20promise%20due%20to%20their%20long-range%20modeling%0Acapacity%20and%20efficiency.%20Inspired%20by%20this%2C%20we%20take%20the%20first%20step%20to%20explore%0ASSMs%20for%20visual%20compression.%20We%20introduce%20MambaVC%2C%20a%20simple%2C%20strong%20and%0Aefficient%20compression%20network%20based%20on%20SSM.%20MambaVC%20develops%20a%20visual%20state%0Aspace%20%28VSS%29%20block%20with%20a%202D%20selective%20scanning%20%282DSS%29%20module%20as%20the%20nonlinear%0Aactivation%20function%20after%20each%20downsampling%2C%20which%20helps%20to%20capture%20informative%0Aglobal%20contexts%20and%20enhances%20compression.%20On%20compression%20benchmark%20datasets%2C%0AMambaVC%20achieves%20superior%20rate-distortion%20performance%20with%20lower%20computational%0Aand%20memory%20overheads.%20Specifically%2C%20it%20outperforms%20CNN%20and%20Transformer%20variants%0Aby%209.3%25%20and%2015.6%25%20on%20Kodak%2C%20respectively%2C%20while%20reducing%20computation%20by%2042%25%20and%0A24%25%2C%20and%20saving%2012%25%20and%2071%25%20of%20memory.%20MambaVC%20shows%20even%20greater%20improvements%0Awith%20high-resolution%20images%2C%20highlighting%20its%20potential%20and%20scalability%20in%0Areal-world%20applications.%20We%20also%20provide%20a%20comprehensive%20comparison%20of%0Adifferent%20network%20designs%2C%20underscoring%20MambaVC%27s%20advantages.%20Code%20is%20available%0Aat%20https%3A//github.com/QinSY123/2024-MambaVC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15413v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaVC%253A%2520Learned%2520Visual%2520Compression%2520with%2520Selective%2520State%2520Spaces%26entry.906535625%3DShiyu%2520Qin%2520and%2520Jinpeng%2520Wang%2520and%2520Yimin%2520Zhou%2520and%2520Bin%2520Chen%2520and%2520Tianci%2520Luo%2520and%2520Baoyi%2520An%2520and%2520Tao%2520Dai%2520and%2520Shutao%2520Xia%2520and%2520Yaowei%2520Wang%26entry.1292438233%3D%2520%2520Learned%2520visual%2520compression%2520is%2520an%2520important%2520and%2520active%2520task%2520in%2520multimedia.%250AExisting%2520approaches%2520have%2520explored%2520various%2520CNN-%2520and%2520Transformer-based%2520designs%2520to%250Amodel%2520content%2520distribution%2520and%2520eliminate%2520redundancy%252C%2520where%2520balancing%2520efficacy%250A%2528i.e.%252C%2520rate-distortion%2520trade-off%2529%2520and%2520efficiency%2520remains%2520a%2520challenge.%2520Recently%252C%250Astate-space%2520models%2520%2528SSMs%2529%2520have%2520shown%2520promise%2520due%2520to%2520their%2520long-range%2520modeling%250Acapacity%2520and%2520efficiency.%2520Inspired%2520by%2520this%252C%2520we%2520take%2520the%2520first%2520step%2520to%2520explore%250ASSMs%2520for%2520visual%2520compression.%2520We%2520introduce%2520MambaVC%252C%2520a%2520simple%252C%2520strong%2520and%250Aefficient%2520compression%2520network%2520based%2520on%2520SSM.%2520MambaVC%2520develops%2520a%2520visual%2520state%250Aspace%2520%2528VSS%2529%2520block%2520with%2520a%25202D%2520selective%2520scanning%2520%25282DSS%2529%2520module%2520as%2520the%2520nonlinear%250Aactivation%2520function%2520after%2520each%2520downsampling%252C%2520which%2520helps%2520to%2520capture%2520informative%250Aglobal%2520contexts%2520and%2520enhances%2520compression.%2520On%2520compression%2520benchmark%2520datasets%252C%250AMambaVC%2520achieves%2520superior%2520rate-distortion%2520performance%2520with%2520lower%2520computational%250Aand%2520memory%2520overheads.%2520Specifically%252C%2520it%2520outperforms%2520CNN%2520and%2520Transformer%2520variants%250Aby%25209.3%2525%2520and%252015.6%2525%2520on%2520Kodak%252C%2520respectively%252C%2520while%2520reducing%2520computation%2520by%252042%2525%2520and%250A24%2525%252C%2520and%2520saving%252012%2525%2520and%252071%2525%2520of%2520memory.%2520MambaVC%2520shows%2520even%2520greater%2520improvements%250Awith%2520high-resolution%2520images%252C%2520highlighting%2520its%2520potential%2520and%2520scalability%2520in%250Areal-world%2520applications.%2520We%2520also%2520provide%2520a%2520comprehensive%2520comparison%2520of%250Adifferent%2520network%2520designs%252C%2520underscoring%2520MambaVC%2527s%2520advantages.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/QinSY123/2024-MambaVC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15413v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaVC%3A%20Learned%20Visual%20Compression%20with%20Selective%20State%20Spaces&entry.906535625=Shiyu%20Qin%20and%20Jinpeng%20Wang%20and%20Yimin%20Zhou%20and%20Bin%20Chen%20and%20Tianci%20Luo%20and%20Baoyi%20An%20and%20Tao%20Dai%20and%20Shutao%20Xia%20and%20Yaowei%20Wang&entry.1292438233=%20%20Learned%20visual%20compression%20is%20an%20important%20and%20active%20task%20in%20multimedia.%0AExisting%20approaches%20have%20explored%20various%20CNN-%20and%20Transformer-based%20designs%20to%0Amodel%20content%20distribution%20and%20eliminate%20redundancy%2C%20where%20balancing%20efficacy%0A%28i.e.%2C%20rate-distortion%20trade-off%29%20and%20efficiency%20remains%20a%20challenge.%20Recently%2C%0Astate-space%20models%20%28SSMs%29%20have%20shown%20promise%20due%20to%20their%20long-range%20modeling%0Acapacity%20and%20efficiency.%20Inspired%20by%20this%2C%20we%20take%20the%20first%20step%20to%20explore%0ASSMs%20for%20visual%20compression.%20We%20introduce%20MambaVC%2C%20a%20simple%2C%20strong%20and%0Aefficient%20compression%20network%20based%20on%20SSM.%20MambaVC%20develops%20a%20visual%20state%0Aspace%20%28VSS%29%20block%20with%20a%202D%20selective%20scanning%20%282DSS%29%20module%20as%20the%20nonlinear%0Aactivation%20function%20after%20each%20downsampling%2C%20which%20helps%20to%20capture%20informative%0Aglobal%20contexts%20and%20enhances%20compression.%20On%20compression%20benchmark%20datasets%2C%0AMambaVC%20achieves%20superior%20rate-distortion%20performance%20with%20lower%20computational%0Aand%20memory%20overheads.%20Specifically%2C%20it%20outperforms%20CNN%20and%20Transformer%20variants%0Aby%209.3%25%20and%2015.6%25%20on%20Kodak%2C%20respectively%2C%20while%20reducing%20computation%20by%2042%25%20and%0A24%25%2C%20and%20saving%2012%25%20and%2071%25%20of%20memory.%20MambaVC%20shows%20even%20greater%20improvements%0Awith%20high-resolution%20images%2C%20highlighting%20its%20potential%20and%20scalability%20in%0Areal-world%20applications.%20We%20also%20provide%20a%20comprehensive%20comparison%20of%0Adifferent%20network%20designs%2C%20underscoring%20MambaVC%27s%20advantages.%20Code%20is%20available%0Aat%20https%3A//github.com/QinSY123/2024-MambaVC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15413v3&entry.124074799=Read"},
{"title": "One for All: Toward Unified Foundation Models for Earth Vision", "author": "Zhitong Xiong and Yi Wang and Fahong Zhang and Xiao Xiang Zhu", "abstract": "  Foundation models characterized by extensive parameters and trained on\nlarge-scale datasets have demonstrated remarkable efficacy across various\ndownstream tasks for remote sensing data. Current remote sensing foundation\nmodels typically specialize in a single modality or a specific spatial\nresolution range, limiting their versatility for downstream datasets. While\nthere have been attempts to develop multi-modal remote sensing foundation\nmodels, they typically employ separate vision encoders for each modality or\nspatial resolution, necessitating a switch in backbones contingent upon the\ninput data. To address this issue, we introduce a simple yet effective method,\ntermed OFA-Net (One-For-All Network): employing a single, shared Transformer\nbackbone for multiple data modalities with different spatial resolutions. Using\nthe masked image modeling mechanism, we pre-train a single Transformer backbone\non a curated multi-modal dataset with this simple design. Then the backbone\nmodel can be used in different downstream tasks, thus forging a path towards a\nunified foundation backbone model in Earth vision. The proposed method is\nevaluated on 12 distinct downstream tasks and demonstrates promising\nperformance.\n", "link": "http://arxiv.org/abs/2401.07527v2", "date": "2024-05-28", "relevancy": 2.0943, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5336}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5189}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20for%20All%3A%20Toward%20Unified%20Foundation%20Models%20for%20Earth%20Vision&body=Title%3A%20One%20for%20All%3A%20Toward%20Unified%20Foundation%20Models%20for%20Earth%20Vision%0AAuthor%3A%20Zhitong%20Xiong%20and%20Yi%20Wang%20and%20Fahong%20Zhang%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%20Foundation%20models%20characterized%20by%20extensive%20parameters%20and%20trained%20on%0Alarge-scale%20datasets%20have%20demonstrated%20remarkable%20efficacy%20across%20various%0Adownstream%20tasks%20for%20remote%20sensing%20data.%20Current%20remote%20sensing%20foundation%0Amodels%20typically%20specialize%20in%20a%20single%20modality%20or%20a%20specific%20spatial%0Aresolution%20range%2C%20limiting%20their%20versatility%20for%20downstream%20datasets.%20While%0Athere%20have%20been%20attempts%20to%20develop%20multi-modal%20remote%20sensing%20foundation%0Amodels%2C%20they%20typically%20employ%20separate%20vision%20encoders%20for%20each%20modality%20or%0Aspatial%20resolution%2C%20necessitating%20a%20switch%20in%20backbones%20contingent%20upon%20the%0Ainput%20data.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20simple%20yet%20effective%20method%2C%0Atermed%20OFA-Net%20%28One-For-All%20Network%29%3A%20employing%20a%20single%2C%20shared%20Transformer%0Abackbone%20for%20multiple%20data%20modalities%20with%20different%20spatial%20resolutions.%20Using%0Athe%20masked%20image%20modeling%20mechanism%2C%20we%20pre-train%20a%20single%20Transformer%20backbone%0Aon%20a%20curated%20multi-modal%20dataset%20with%20this%20simple%20design.%20Then%20the%20backbone%0Amodel%20can%20be%20used%20in%20different%20downstream%20tasks%2C%20thus%20forging%20a%20path%20towards%20a%0Aunified%20foundation%20backbone%20model%20in%20Earth%20vision.%20The%20proposed%20method%20is%0Aevaluated%20on%2012%20distinct%20downstream%20tasks%20and%20demonstrates%20promising%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07527v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520for%2520All%253A%2520Toward%2520Unified%2520Foundation%2520Models%2520for%2520Earth%2520Vision%26entry.906535625%3DZhitong%2520Xiong%2520and%2520Yi%2520Wang%2520and%2520Fahong%2520Zhang%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3D%2520%2520Foundation%2520models%2520characterized%2520by%2520extensive%2520parameters%2520and%2520trained%2520on%250Alarge-scale%2520datasets%2520have%2520demonstrated%2520remarkable%2520efficacy%2520across%2520various%250Adownstream%2520tasks%2520for%2520remote%2520sensing%2520data.%2520Current%2520remote%2520sensing%2520foundation%250Amodels%2520typically%2520specialize%2520in%2520a%2520single%2520modality%2520or%2520a%2520specific%2520spatial%250Aresolution%2520range%252C%2520limiting%2520their%2520versatility%2520for%2520downstream%2520datasets.%2520While%250Athere%2520have%2520been%2520attempts%2520to%2520develop%2520multi-modal%2520remote%2520sensing%2520foundation%250Amodels%252C%2520they%2520typically%2520employ%2520separate%2520vision%2520encoders%2520for%2520each%2520modality%2520or%250Aspatial%2520resolution%252C%2520necessitating%2520a%2520switch%2520in%2520backbones%2520contingent%2520upon%2520the%250Ainput%2520data.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520simple%2520yet%2520effective%2520method%252C%250Atermed%2520OFA-Net%2520%2528One-For-All%2520Network%2529%253A%2520employing%2520a%2520single%252C%2520shared%2520Transformer%250Abackbone%2520for%2520multiple%2520data%2520modalities%2520with%2520different%2520spatial%2520resolutions.%2520Using%250Athe%2520masked%2520image%2520modeling%2520mechanism%252C%2520we%2520pre-train%2520a%2520single%2520Transformer%2520backbone%250Aon%2520a%2520curated%2520multi-modal%2520dataset%2520with%2520this%2520simple%2520design.%2520Then%2520the%2520backbone%250Amodel%2520can%2520be%2520used%2520in%2520different%2520downstream%2520tasks%252C%2520thus%2520forging%2520a%2520path%2520towards%2520a%250Aunified%2520foundation%2520backbone%2520model%2520in%2520Earth%2520vision.%2520The%2520proposed%2520method%2520is%250Aevaluated%2520on%252012%2520distinct%2520downstream%2520tasks%2520and%2520demonstrates%2520promising%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07527v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20for%20All%3A%20Toward%20Unified%20Foundation%20Models%20for%20Earth%20Vision&entry.906535625=Zhitong%20Xiong%20and%20Yi%20Wang%20and%20Fahong%20Zhang%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20Foundation%20models%20characterized%20by%20extensive%20parameters%20and%20trained%20on%0Alarge-scale%20datasets%20have%20demonstrated%20remarkable%20efficacy%20across%20various%0Adownstream%20tasks%20for%20remote%20sensing%20data.%20Current%20remote%20sensing%20foundation%0Amodels%20typically%20specialize%20in%20a%20single%20modality%20or%20a%20specific%20spatial%0Aresolution%20range%2C%20limiting%20their%20versatility%20for%20downstream%20datasets.%20While%0Athere%20have%20been%20attempts%20to%20develop%20multi-modal%20remote%20sensing%20foundation%0Amodels%2C%20they%20typically%20employ%20separate%20vision%20encoders%20for%20each%20modality%20or%0Aspatial%20resolution%2C%20necessitating%20a%20switch%20in%20backbones%20contingent%20upon%20the%0Ainput%20data.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20simple%20yet%20effective%20method%2C%0Atermed%20OFA-Net%20%28One-For-All%20Network%29%3A%20employing%20a%20single%2C%20shared%20Transformer%0Abackbone%20for%20multiple%20data%20modalities%20with%20different%20spatial%20resolutions.%20Using%0Athe%20masked%20image%20modeling%20mechanism%2C%20we%20pre-train%20a%20single%20Transformer%20backbone%0Aon%20a%20curated%20multi-modal%20dataset%20with%20this%20simple%20design.%20Then%20the%20backbone%0Amodel%20can%20be%20used%20in%20different%20downstream%20tasks%2C%20thus%20forging%20a%20path%20towards%20a%0Aunified%20foundation%20backbone%20model%20in%20Earth%20vision.%20The%20proposed%20method%20is%0Aevaluated%20on%2012%20distinct%20downstream%20tasks%20and%20demonstrates%20promising%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07527v2&entry.124074799=Read"},
{"title": "X-Light: Cross-City Traffic Signal Control Using Transformer on\n  Transformer as Meta Multi-Agent Reinforcement Learner", "author": "Haoyuan Jiang and Ziyue Li and Hua Wei and Xuantang Xiong and Jingqing Ruan and Jiaming Lu and Hangyu Mao and Rui Zhao", "abstract": "  The effectiveness of traffic light control has been significantly improved by\ncurrent reinforcement learning-based approaches via better cooperation among\nmultiple traffic lights. However, a persisting issue remains: how to obtain a\nmulti-agent traffic signal control algorithm with remarkable transferability\nacross diverse cities? In this paper, we propose a Transformer on Transformer\n(TonT) model for cross-city meta multi-agent traffic signal control, named as\nX-Light: We input the full Markov Decision Process trajectories, and the Lower\nTransformer aggregates the states, actions, rewards among the target\nintersection and its neighbors within a city, and the Upper Transformer learns\nthe general decision trajectories across different cities. This dual-level\napproach bolsters the model's robust generalization and transferability.\nNotably, when directly transferring to unseen scenarios, ours surpasses all\nbaseline methods with +7.91% on average, and even +16.3% in some cases,\nyielding the best results.\n", "link": "http://arxiv.org/abs/2404.12090v2", "date": "2024-05-28", "relevancy": 2.0734, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5542}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5026}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-Light%3A%20Cross-City%20Traffic%20Signal%20Control%20Using%20Transformer%20on%0A%20%20Transformer%20as%20Meta%20Multi-Agent%20Reinforcement%20Learner&body=Title%3A%20X-Light%3A%20Cross-City%20Traffic%20Signal%20Control%20Using%20Transformer%20on%0A%20%20Transformer%20as%20Meta%20Multi-Agent%20Reinforcement%20Learner%0AAuthor%3A%20Haoyuan%20Jiang%20and%20Ziyue%20Li%20and%20Hua%20Wei%20and%20Xuantang%20Xiong%20and%20Jingqing%20Ruan%20and%20Jiaming%20Lu%20and%20Hangyu%20Mao%20and%20Rui%20Zhao%0AAbstract%3A%20%20%20The%20effectiveness%20of%20traffic%20light%20control%20has%20been%20significantly%20improved%20by%0Acurrent%20reinforcement%20learning-based%20approaches%20via%20better%20cooperation%20among%0Amultiple%20traffic%20lights.%20However%2C%20a%20persisting%20issue%20remains%3A%20how%20to%20obtain%20a%0Amulti-agent%20traffic%20signal%20control%20algorithm%20with%20remarkable%20transferability%0Aacross%20diverse%20cities%3F%20In%20this%20paper%2C%20we%20propose%20a%20Transformer%20on%20Transformer%0A%28TonT%29%20model%20for%20cross-city%20meta%20multi-agent%20traffic%20signal%20control%2C%20named%20as%0AX-Light%3A%20We%20input%20the%20full%20Markov%20Decision%20Process%20trajectories%2C%20and%20the%20Lower%0ATransformer%20aggregates%20the%20states%2C%20actions%2C%20rewards%20among%20the%20target%0Aintersection%20and%20its%20neighbors%20within%20a%20city%2C%20and%20the%20Upper%20Transformer%20learns%0Athe%20general%20decision%20trajectories%20across%20different%20cities.%20This%20dual-level%0Aapproach%20bolsters%20the%20model%27s%20robust%20generalization%20and%20transferability.%0ANotably%2C%20when%20directly%20transferring%20to%20unseen%20scenarios%2C%20ours%20surpasses%20all%0Abaseline%20methods%20with%20%2B7.91%25%20on%20average%2C%20and%20even%20%2B16.3%25%20in%20some%20cases%2C%0Ayielding%20the%20best%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-Light%253A%2520Cross-City%2520Traffic%2520Signal%2520Control%2520Using%2520Transformer%2520on%250A%2520%2520Transformer%2520as%2520Meta%2520Multi-Agent%2520Reinforcement%2520Learner%26entry.906535625%3DHaoyuan%2520Jiang%2520and%2520Ziyue%2520Li%2520and%2520Hua%2520Wei%2520and%2520Xuantang%2520Xiong%2520and%2520Jingqing%2520Ruan%2520and%2520Jiaming%2520Lu%2520and%2520Hangyu%2520Mao%2520and%2520Rui%2520Zhao%26entry.1292438233%3D%2520%2520The%2520effectiveness%2520of%2520traffic%2520light%2520control%2520has%2520been%2520significantly%2520improved%2520by%250Acurrent%2520reinforcement%2520learning-based%2520approaches%2520via%2520better%2520cooperation%2520among%250Amultiple%2520traffic%2520lights.%2520However%252C%2520a%2520persisting%2520issue%2520remains%253A%2520how%2520to%2520obtain%2520a%250Amulti-agent%2520traffic%2520signal%2520control%2520algorithm%2520with%2520remarkable%2520transferability%250Aacross%2520diverse%2520cities%253F%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Transformer%2520on%2520Transformer%250A%2528TonT%2529%2520model%2520for%2520cross-city%2520meta%2520multi-agent%2520traffic%2520signal%2520control%252C%2520named%2520as%250AX-Light%253A%2520We%2520input%2520the%2520full%2520Markov%2520Decision%2520Process%2520trajectories%252C%2520and%2520the%2520Lower%250ATransformer%2520aggregates%2520the%2520states%252C%2520actions%252C%2520rewards%2520among%2520the%2520target%250Aintersection%2520and%2520its%2520neighbors%2520within%2520a%2520city%252C%2520and%2520the%2520Upper%2520Transformer%2520learns%250Athe%2520general%2520decision%2520trajectories%2520across%2520different%2520cities.%2520This%2520dual-level%250Aapproach%2520bolsters%2520the%2520model%2527s%2520robust%2520generalization%2520and%2520transferability.%250ANotably%252C%2520when%2520directly%2520transferring%2520to%2520unseen%2520scenarios%252C%2520ours%2520surpasses%2520all%250Abaseline%2520methods%2520with%2520%252B7.91%2525%2520on%2520average%252C%2520and%2520even%2520%252B16.3%2525%2520in%2520some%2520cases%252C%250Ayielding%2520the%2520best%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Light%3A%20Cross-City%20Traffic%20Signal%20Control%20Using%20Transformer%20on%0A%20%20Transformer%20as%20Meta%20Multi-Agent%20Reinforcement%20Learner&entry.906535625=Haoyuan%20Jiang%20and%20Ziyue%20Li%20and%20Hua%20Wei%20and%20Xuantang%20Xiong%20and%20Jingqing%20Ruan%20and%20Jiaming%20Lu%20and%20Hangyu%20Mao%20and%20Rui%20Zhao&entry.1292438233=%20%20The%20effectiveness%20of%20traffic%20light%20control%20has%20been%20significantly%20improved%20by%0Acurrent%20reinforcement%20learning-based%20approaches%20via%20better%20cooperation%20among%0Amultiple%20traffic%20lights.%20However%2C%20a%20persisting%20issue%20remains%3A%20how%20to%20obtain%20a%0Amulti-agent%20traffic%20signal%20control%20algorithm%20with%20remarkable%20transferability%0Aacross%20diverse%20cities%3F%20In%20this%20paper%2C%20we%20propose%20a%20Transformer%20on%20Transformer%0A%28TonT%29%20model%20for%20cross-city%20meta%20multi-agent%20traffic%20signal%20control%2C%20named%20as%0AX-Light%3A%20We%20input%20the%20full%20Markov%20Decision%20Process%20trajectories%2C%20and%20the%20Lower%0ATransformer%20aggregates%20the%20states%2C%20actions%2C%20rewards%20among%20the%20target%0Aintersection%20and%20its%20neighbors%20within%20a%20city%2C%20and%20the%20Upper%20Transformer%20learns%0Athe%20general%20decision%20trajectories%20across%20different%20cities.%20This%20dual-level%0Aapproach%20bolsters%20the%20model%27s%20robust%20generalization%20and%20transferability.%0ANotably%2C%20when%20directly%20transferring%20to%20unseen%20scenarios%2C%20ours%20surpasses%20all%0Abaseline%20methods%20with%20%2B7.91%25%20on%20average%2C%20and%20even%20%2B16.3%25%20in%20some%20cases%2C%0Ayielding%20the%20best%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12090v2&entry.124074799=Read"},
{"title": "Position: Foundation Agents as the Paradigm Shift for Decision Making", "author": "Xiaoqian Liu and Xingzhou Lou and Jianbin Jiao and Junge Zhang", "abstract": "  Decision making demands intricate interplay between perception, memory, and\nreasoning to discern optimal policies. Conventional approaches to decision\nmaking face challenges related to low sample efficiency and poor\ngeneralization. In contrast, foundation models in language and vision have\nshowcased rapid adaptation to diverse new tasks. Therefore, we advocate for the\nconstruction of foundation agents as a transformative shift in the learning\nparadigm of agents. This proposal is underpinned by the formulation of\nfoundation agents with their fundamental characteristics and challenges\nmotivated by the success of large language models (LLMs). Moreover, we specify\nthe roadmap of foundation agents from large interactive data collection or\ngeneration, to self-supervised pretraining and adaptation, and knowledge and\nvalue alignment with LLMs. Lastly, we pinpoint critical research questions\nderived from the formulation and delineate trends for foundation agents\nsupported by real-world use cases, addressing both technical and theoretical\naspects to propel the field towards a more comprehensive and impactful future.\n", "link": "http://arxiv.org/abs/2405.17009v2", "date": "2024-05-28", "relevancy": 2.0697, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5218}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5216}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Foundation%20Agents%20as%20the%20Paradigm%20Shift%20for%20Decision%20Making&body=Title%3A%20Position%3A%20Foundation%20Agents%20as%20the%20Paradigm%20Shift%20for%20Decision%20Making%0AAuthor%3A%20Xiaoqian%20Liu%20and%20Xingzhou%20Lou%20and%20Jianbin%20Jiao%20and%20Junge%20Zhang%0AAbstract%3A%20%20%20Decision%20making%20demands%20intricate%20interplay%20between%20perception%2C%20memory%2C%20and%0Areasoning%20to%20discern%20optimal%20policies.%20Conventional%20approaches%20to%20decision%0Amaking%20face%20challenges%20related%20to%20low%20sample%20efficiency%20and%20poor%0Ageneralization.%20In%20contrast%2C%20foundation%20models%20in%20language%20and%20vision%20have%0Ashowcased%20rapid%20adaptation%20to%20diverse%20new%20tasks.%20Therefore%2C%20we%20advocate%20for%20the%0Aconstruction%20of%20foundation%20agents%20as%20a%20transformative%20shift%20in%20the%20learning%0Aparadigm%20of%20agents.%20This%20proposal%20is%20underpinned%20by%20the%20formulation%20of%0Afoundation%20agents%20with%20their%20fundamental%20characteristics%20and%20challenges%0Amotivated%20by%20the%20success%20of%20large%20language%20models%20%28LLMs%29.%20Moreover%2C%20we%20specify%0Athe%20roadmap%20of%20foundation%20agents%20from%20large%20interactive%20data%20collection%20or%0Ageneration%2C%20to%20self-supervised%20pretraining%20and%20adaptation%2C%20and%20knowledge%20and%0Avalue%20alignment%20with%20LLMs.%20Lastly%2C%20we%20pinpoint%20critical%20research%20questions%0Aderived%20from%20the%20formulation%20and%20delineate%20trends%20for%20foundation%20agents%0Asupported%20by%20real-world%20use%20cases%2C%20addressing%20both%20technical%20and%20theoretical%0Aaspects%20to%20propel%20the%20field%20towards%20a%20more%20comprehensive%20and%20impactful%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17009v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Foundation%2520Agents%2520as%2520the%2520Paradigm%2520Shift%2520for%2520Decision%2520Making%26entry.906535625%3DXiaoqian%2520Liu%2520and%2520Xingzhou%2520Lou%2520and%2520Jianbin%2520Jiao%2520and%2520Junge%2520Zhang%26entry.1292438233%3D%2520%2520Decision%2520making%2520demands%2520intricate%2520interplay%2520between%2520perception%252C%2520memory%252C%2520and%250Areasoning%2520to%2520discern%2520optimal%2520policies.%2520Conventional%2520approaches%2520to%2520decision%250Amaking%2520face%2520challenges%2520related%2520to%2520low%2520sample%2520efficiency%2520and%2520poor%250Ageneralization.%2520In%2520contrast%252C%2520foundation%2520models%2520in%2520language%2520and%2520vision%2520have%250Ashowcased%2520rapid%2520adaptation%2520to%2520diverse%2520new%2520tasks.%2520Therefore%252C%2520we%2520advocate%2520for%2520the%250Aconstruction%2520of%2520foundation%2520agents%2520as%2520a%2520transformative%2520shift%2520in%2520the%2520learning%250Aparadigm%2520of%2520agents.%2520This%2520proposal%2520is%2520underpinned%2520by%2520the%2520formulation%2520of%250Afoundation%2520agents%2520with%2520their%2520fundamental%2520characteristics%2520and%2520challenges%250Amotivated%2520by%2520the%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Moreover%252C%2520we%2520specify%250Athe%2520roadmap%2520of%2520foundation%2520agents%2520from%2520large%2520interactive%2520data%2520collection%2520or%250Ageneration%252C%2520to%2520self-supervised%2520pretraining%2520and%2520adaptation%252C%2520and%2520knowledge%2520and%250Avalue%2520alignment%2520with%2520LLMs.%2520Lastly%252C%2520we%2520pinpoint%2520critical%2520research%2520questions%250Aderived%2520from%2520the%2520formulation%2520and%2520delineate%2520trends%2520for%2520foundation%2520agents%250Asupported%2520by%2520real-world%2520use%2520cases%252C%2520addressing%2520both%2520technical%2520and%2520theoretical%250Aaspects%2520to%2520propel%2520the%2520field%2520towards%2520a%2520more%2520comprehensive%2520and%2520impactful%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17009v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Foundation%20Agents%20as%20the%20Paradigm%20Shift%20for%20Decision%20Making&entry.906535625=Xiaoqian%20Liu%20and%20Xingzhou%20Lou%20and%20Jianbin%20Jiao%20and%20Junge%20Zhang&entry.1292438233=%20%20Decision%20making%20demands%20intricate%20interplay%20between%20perception%2C%20memory%2C%20and%0Areasoning%20to%20discern%20optimal%20policies.%20Conventional%20approaches%20to%20decision%0Amaking%20face%20challenges%20related%20to%20low%20sample%20efficiency%20and%20poor%0Ageneralization.%20In%20contrast%2C%20foundation%20models%20in%20language%20and%20vision%20have%0Ashowcased%20rapid%20adaptation%20to%20diverse%20new%20tasks.%20Therefore%2C%20we%20advocate%20for%20the%0Aconstruction%20of%20foundation%20agents%20as%20a%20transformative%20shift%20in%20the%20learning%0Aparadigm%20of%20agents.%20This%20proposal%20is%20underpinned%20by%20the%20formulation%20of%0Afoundation%20agents%20with%20their%20fundamental%20characteristics%20and%20challenges%0Amotivated%20by%20the%20success%20of%20large%20language%20models%20%28LLMs%29.%20Moreover%2C%20we%20specify%0Athe%20roadmap%20of%20foundation%20agents%20from%20large%20interactive%20data%20collection%20or%0Ageneration%2C%20to%20self-supervised%20pretraining%20and%20adaptation%2C%20and%20knowledge%20and%0Avalue%20alignment%20with%20LLMs.%20Lastly%2C%20we%20pinpoint%20critical%20research%20questions%0Aderived%20from%20the%20formulation%20and%20delineate%20trends%20for%20foundation%20agents%0Asupported%20by%20real-world%20use%20cases%2C%20addressing%20both%20technical%20and%20theoretical%0Aaspects%20to%20propel%20the%20field%20towards%20a%20more%20comprehensive%20and%20impactful%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17009v2&entry.124074799=Read"},
{"title": "OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for\n  Memory-Efficient LLM Fine-tuning", "author": "Pengxiang Li and Lu Yin and Xiaowei Gao and Shiwei Liu", "abstract": "  The rapid advancements in Large Language Models (LLMs) have revolutionized\nvarious natural language processing tasks. However, the substantial size of\nLLMs presents significant challenges in training or fine-tuning. While\nparameter-efficient approaches such as low-rank adaptation (LoRA) have gained\npopularity, they often compromise performance compared to full-rank\nfine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampled\nLow-Rank Projection (OwLore), a new memory-efficient fine-tuning approach,\ninspired by the layerwise outlier distribution of LLMs, which dynamically\nsamples pre-trained layers to fine-tune instead of adding additional adaptors.\nWe first interpret the outlier phenomenon through the lens of Heavy-Tailed\nSelf-Regularization theory (HT-SR), discovering that layers with more outliers\ntend to be more heavy-tailed and consequently better trained. Inspired by this\nfinding, OwLore strategically assigns higher sampling probabilities to layers\nwith more outliers to better leverage the knowledge stored in pre-trained LLMs.\nTo further mitigate the memory demands of fine-tuning, we integrate gradient\nlow-rank projection into our approach, which facilitates each layer to be\nefficiently trained in a low-rank manner. By incorporating the efficient\ncharacteristics of low-rank and optimal layerwise sampling, OwLore\nsignificantly improves the memory-performance trade-off in LLM pruning. Our\nextensive experiments across various architectures, including LLaMa2, LLaMa3,\nand Mistral, demonstrate that OwLore consistently outperforms baseline\napproaches, including full fine-tuning. Specifically, it achieves up to a 1.1%\naverage accuracy gain on the Commonsense Reasoning benchmark, a 3.0%\nimprovement on MMLU, and a notable 10% boost on MT-Bench, while being more\nmemory efficient. OwLore allows us to fine-tune LLaMa2-7B with only 21GB of\nmemory.\n", "link": "http://arxiv.org/abs/2405.18380v1", "date": "2024-05-28", "relevancy": 2.0646, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5206}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5164}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OwLore%3A%20Outlier-weighed%20Layerwise%20Sampled%20Low-Rank%20Projection%20for%0A%20%20Memory-Efficient%20LLM%20Fine-tuning&body=Title%3A%20OwLore%3A%20Outlier-weighed%20Layerwise%20Sampled%20Low-Rank%20Projection%20for%0A%20%20Memory-Efficient%20LLM%20Fine-tuning%0AAuthor%3A%20Pengxiang%20Li%20and%20Lu%20Yin%20and%20Xiaowei%20Gao%20and%20Shiwei%20Liu%0AAbstract%3A%20%20%20The%20rapid%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%0Avarious%20natural%20language%20processing%20tasks.%20However%2C%20the%20substantial%20size%20of%0ALLMs%20presents%20significant%20challenges%20in%20training%20or%20fine-tuning.%20While%0Aparameter-efficient%20approaches%20such%20as%20low-rank%20adaptation%20%28LoRA%29%20have%20gained%0Apopularity%2C%20they%20often%20compromise%20performance%20compared%20to%20full-rank%0Afine-tuning.%20In%20this%20paper%2C%20we%20propose%20Outlier-weighed%20Layerwise%20Sampled%0ALow-Rank%20Projection%20%28OwLore%29%2C%20a%20new%20memory-efficient%20fine-tuning%20approach%2C%0Ainspired%20by%20the%20layerwise%20outlier%20distribution%20of%20LLMs%2C%20which%20dynamically%0Asamples%20pre-trained%20layers%20to%20fine-tune%20instead%20of%20adding%20additional%20adaptors.%0AWe%20first%20interpret%20the%20outlier%20phenomenon%20through%20the%20lens%20of%20Heavy-Tailed%0ASelf-Regularization%20theory%20%28HT-SR%29%2C%20discovering%20that%20layers%20with%20more%20outliers%0Atend%20to%20be%20more%20heavy-tailed%20and%20consequently%20better%20trained.%20Inspired%20by%20this%0Afinding%2C%20OwLore%20strategically%20assigns%20higher%20sampling%20probabilities%20to%20layers%0Awith%20more%20outliers%20to%20better%20leverage%20the%20knowledge%20stored%20in%20pre-trained%20LLMs.%0ATo%20further%20mitigate%20the%20memory%20demands%20of%20fine-tuning%2C%20we%20integrate%20gradient%0Alow-rank%20projection%20into%20our%20approach%2C%20which%20facilitates%20each%20layer%20to%20be%0Aefficiently%20trained%20in%20a%20low-rank%20manner.%20By%20incorporating%20the%20efficient%0Acharacteristics%20of%20low-rank%20and%20optimal%20layerwise%20sampling%2C%20OwLore%0Asignificantly%20improves%20the%20memory-performance%20trade-off%20in%20LLM%20pruning.%20Our%0Aextensive%20experiments%20across%20various%20architectures%2C%20including%20LLaMa2%2C%20LLaMa3%2C%0Aand%20Mistral%2C%20demonstrate%20that%20OwLore%20consistently%20outperforms%20baseline%0Aapproaches%2C%20including%20full%20fine-tuning.%20Specifically%2C%20it%20achieves%20up%20to%20a%201.1%25%0Aaverage%20accuracy%20gain%20on%20the%20Commonsense%20Reasoning%20benchmark%2C%20a%203.0%25%0Aimprovement%20on%20MMLU%2C%20and%20a%20notable%2010%25%20boost%20on%20MT-Bench%2C%20while%20being%20more%0Amemory%20efficient.%20OwLore%20allows%20us%20to%20fine-tune%20LLaMa2-7B%20with%20only%2021GB%20of%0Amemory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOwLore%253A%2520Outlier-weighed%2520Layerwise%2520Sampled%2520Low-Rank%2520Projection%2520for%250A%2520%2520Memory-Efficient%2520LLM%2520Fine-tuning%26entry.906535625%3DPengxiang%2520Li%2520and%2520Lu%2520Yin%2520and%2520Xiaowei%2520Gao%2520and%2520Shiwei%2520Liu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%250Avarious%2520natural%2520language%2520processing%2520tasks.%2520However%252C%2520the%2520substantial%2520size%2520of%250ALLMs%2520presents%2520significant%2520challenges%2520in%2520training%2520or%2520fine-tuning.%2520While%250Aparameter-efficient%2520approaches%2520such%2520as%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520have%2520gained%250Apopularity%252C%2520they%2520often%2520compromise%2520performance%2520compared%2520to%2520full-rank%250Afine-tuning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Outlier-weighed%2520Layerwise%2520Sampled%250ALow-Rank%2520Projection%2520%2528OwLore%2529%252C%2520a%2520new%2520memory-efficient%2520fine-tuning%2520approach%252C%250Ainspired%2520by%2520the%2520layerwise%2520outlier%2520distribution%2520of%2520LLMs%252C%2520which%2520dynamically%250Asamples%2520pre-trained%2520layers%2520to%2520fine-tune%2520instead%2520of%2520adding%2520additional%2520adaptors.%250AWe%2520first%2520interpret%2520the%2520outlier%2520phenomenon%2520through%2520the%2520lens%2520of%2520Heavy-Tailed%250ASelf-Regularization%2520theory%2520%2528HT-SR%2529%252C%2520discovering%2520that%2520layers%2520with%2520more%2520outliers%250Atend%2520to%2520be%2520more%2520heavy-tailed%2520and%2520consequently%2520better%2520trained.%2520Inspired%2520by%2520this%250Afinding%252C%2520OwLore%2520strategically%2520assigns%2520higher%2520sampling%2520probabilities%2520to%2520layers%250Awith%2520more%2520outliers%2520to%2520better%2520leverage%2520the%2520knowledge%2520stored%2520in%2520pre-trained%2520LLMs.%250ATo%2520further%2520mitigate%2520the%2520memory%2520demands%2520of%2520fine-tuning%252C%2520we%2520integrate%2520gradient%250Alow-rank%2520projection%2520into%2520our%2520approach%252C%2520which%2520facilitates%2520each%2520layer%2520to%2520be%250Aefficiently%2520trained%2520in%2520a%2520low-rank%2520manner.%2520By%2520incorporating%2520the%2520efficient%250Acharacteristics%2520of%2520low-rank%2520and%2520optimal%2520layerwise%2520sampling%252C%2520OwLore%250Asignificantly%2520improves%2520the%2520memory-performance%2520trade-off%2520in%2520LLM%2520pruning.%2520Our%250Aextensive%2520experiments%2520across%2520various%2520architectures%252C%2520including%2520LLaMa2%252C%2520LLaMa3%252C%250Aand%2520Mistral%252C%2520demonstrate%2520that%2520OwLore%2520consistently%2520outperforms%2520baseline%250Aapproaches%252C%2520including%2520full%2520fine-tuning.%2520Specifically%252C%2520it%2520achieves%2520up%2520to%2520a%25201.1%2525%250Aaverage%2520accuracy%2520gain%2520on%2520the%2520Commonsense%2520Reasoning%2520benchmark%252C%2520a%25203.0%2525%250Aimprovement%2520on%2520MMLU%252C%2520and%2520a%2520notable%252010%2525%2520boost%2520on%2520MT-Bench%252C%2520while%2520being%2520more%250Amemory%2520efficient.%2520OwLore%2520allows%2520us%2520to%2520fine-tune%2520LLaMa2-7B%2520with%2520only%252021GB%2520of%250Amemory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OwLore%3A%20Outlier-weighed%20Layerwise%20Sampled%20Low-Rank%20Projection%20for%0A%20%20Memory-Efficient%20LLM%20Fine-tuning&entry.906535625=Pengxiang%20Li%20and%20Lu%20Yin%20and%20Xiaowei%20Gao%20and%20Shiwei%20Liu&entry.1292438233=%20%20The%20rapid%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%0Avarious%20natural%20language%20processing%20tasks.%20However%2C%20the%20substantial%20size%20of%0ALLMs%20presents%20significant%20challenges%20in%20training%20or%20fine-tuning.%20While%0Aparameter-efficient%20approaches%20such%20as%20low-rank%20adaptation%20%28LoRA%29%20have%20gained%0Apopularity%2C%20they%20often%20compromise%20performance%20compared%20to%20full-rank%0Afine-tuning.%20In%20this%20paper%2C%20we%20propose%20Outlier-weighed%20Layerwise%20Sampled%0ALow-Rank%20Projection%20%28OwLore%29%2C%20a%20new%20memory-efficient%20fine-tuning%20approach%2C%0Ainspired%20by%20the%20layerwise%20outlier%20distribution%20of%20LLMs%2C%20which%20dynamically%0Asamples%20pre-trained%20layers%20to%20fine-tune%20instead%20of%20adding%20additional%20adaptors.%0AWe%20first%20interpret%20the%20outlier%20phenomenon%20through%20the%20lens%20of%20Heavy-Tailed%0ASelf-Regularization%20theory%20%28HT-SR%29%2C%20discovering%20that%20layers%20with%20more%20outliers%0Atend%20to%20be%20more%20heavy-tailed%20and%20consequently%20better%20trained.%20Inspired%20by%20this%0Afinding%2C%20OwLore%20strategically%20assigns%20higher%20sampling%20probabilities%20to%20layers%0Awith%20more%20outliers%20to%20better%20leverage%20the%20knowledge%20stored%20in%20pre-trained%20LLMs.%0ATo%20further%20mitigate%20the%20memory%20demands%20of%20fine-tuning%2C%20we%20integrate%20gradient%0Alow-rank%20projection%20into%20our%20approach%2C%20which%20facilitates%20each%20layer%20to%20be%0Aefficiently%20trained%20in%20a%20low-rank%20manner.%20By%20incorporating%20the%20efficient%0Acharacteristics%20of%20low-rank%20and%20optimal%20layerwise%20sampling%2C%20OwLore%0Asignificantly%20improves%20the%20memory-performance%20trade-off%20in%20LLM%20pruning.%20Our%0Aextensive%20experiments%20across%20various%20architectures%2C%20including%20LLaMa2%2C%20LLaMa3%2C%0Aand%20Mistral%2C%20demonstrate%20that%20OwLore%20consistently%20outperforms%20baseline%0Aapproaches%2C%20including%20full%20fine-tuning.%20Specifically%2C%20it%20achieves%20up%20to%20a%201.1%25%0Aaverage%20accuracy%20gain%20on%20the%20Commonsense%20Reasoning%20benchmark%2C%20a%203.0%25%0Aimprovement%20on%20MMLU%2C%20and%20a%20notable%2010%25%20boost%20on%20MT-Bench%2C%20while%20being%20more%0Amemory%20efficient.%20OwLore%20allows%20us%20to%20fine-tune%20LLaMa2-7B%20with%20only%2021GB%20of%0Amemory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18380v1&entry.124074799=Read"},
{"title": "Pipette: Automatic Fine-grained Large Language Model Training\n  Configurator for Real-World Clusters", "author": "Jinkyu Yim and Jaeyong Song and Yerim Choi and Jaebeen Lee and Jaewon Jung and Hongsun Jang and Jinho Lee", "abstract": "  Training large language models (LLMs) is known to be challenging because of\nthe huge computational and memory capacity requirements. To address these\nissues, it is common to use a cluster of GPUs with 3D parallelism, which splits\na model along the data batch, pipeline stage, and intra-layer tensor\ndimensions. However, the use of 3D parallelism produces the additional\nchallenge of finding the optimal number of ways on each dimension and mapping\nthe split models onto the GPUs. Several previous studies have attempted to\nautomatically find the optimal configuration, but many of these lacked several\nimportant aspects. For instance, the heterogeneous nature of the interconnect\nspeeds is often ignored. While the peak bandwidths for the interconnects are\nusually made equal, the actual attained bandwidth varies per link in real-world\nclusters. Combined with the critical path modeling that does not properly\nconsider the communication, they easily fall into sub-optimal configurations.\nIn addition, they often fail to consider the memory requirement per GPU, often\nrecommending solutions that could not be executed. To address these challenges,\nwe propose Pipette, which is an automatic fine-grained LLM training\nconfigurator for real-world clusters. By devising better performance models\nalong with the memory estimator and fine-grained individual GPU assignment,\nPipette achieves faster configurations that satisfy the memory constraints. We\nevaluated Pipette on large clusters to show that it provides a significant\nspeedup over the prior art. The implementation of Pipette is available at\nhttps://github.com/yimjinkyu1/date2024_pipette.\n", "link": "http://arxiv.org/abs/2405.18093v1", "date": "2024-05-28", "relevancy": 2.063, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5289}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5138}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pipette%3A%20Automatic%20Fine-grained%20Large%20Language%20Model%20Training%0A%20%20Configurator%20for%20Real-World%20Clusters&body=Title%3A%20Pipette%3A%20Automatic%20Fine-grained%20Large%20Language%20Model%20Training%0A%20%20Configurator%20for%20Real-World%20Clusters%0AAuthor%3A%20Jinkyu%20Yim%20and%20Jaeyong%20Song%20and%20Yerim%20Choi%20and%20Jaebeen%20Lee%20and%20Jaewon%20Jung%20and%20Hongsun%20Jang%20and%20Jinho%20Lee%0AAbstract%3A%20%20%20Training%20large%20language%20models%20%28LLMs%29%20is%20known%20to%20be%20challenging%20because%20of%0Athe%20huge%20computational%20and%20memory%20capacity%20requirements.%20To%20address%20these%0Aissues%2C%20it%20is%20common%20to%20use%20a%20cluster%20of%20GPUs%20with%203D%20parallelism%2C%20which%20splits%0Aa%20model%20along%20the%20data%20batch%2C%20pipeline%20stage%2C%20and%20intra-layer%20tensor%0Adimensions.%20However%2C%20the%20use%20of%203D%20parallelism%20produces%20the%20additional%0Achallenge%20of%20finding%20the%20optimal%20number%20of%20ways%20on%20each%20dimension%20and%20mapping%0Athe%20split%20models%20onto%20the%20GPUs.%20Several%20previous%20studies%20have%20attempted%20to%0Aautomatically%20find%20the%20optimal%20configuration%2C%20but%20many%20of%20these%20lacked%20several%0Aimportant%20aspects.%20For%20instance%2C%20the%20heterogeneous%20nature%20of%20the%20interconnect%0Aspeeds%20is%20often%20ignored.%20While%20the%20peak%20bandwidths%20for%20the%20interconnects%20are%0Ausually%20made%20equal%2C%20the%20actual%20attained%20bandwidth%20varies%20per%20link%20in%20real-world%0Aclusters.%20Combined%20with%20the%20critical%20path%20modeling%20that%20does%20not%20properly%0Aconsider%20the%20communication%2C%20they%20easily%20fall%20into%20sub-optimal%20configurations.%0AIn%20addition%2C%20they%20often%20fail%20to%20consider%20the%20memory%20requirement%20per%20GPU%2C%20often%0Arecommending%20solutions%20that%20could%20not%20be%20executed.%20To%20address%20these%20challenges%2C%0Awe%20propose%20Pipette%2C%20which%20is%20an%20automatic%20fine-grained%20LLM%20training%0Aconfigurator%20for%20real-world%20clusters.%20By%20devising%20better%20performance%20models%0Aalong%20with%20the%20memory%20estimator%20and%20fine-grained%20individual%20GPU%20assignment%2C%0APipette%20achieves%20faster%20configurations%20that%20satisfy%20the%20memory%20constraints.%20We%0Aevaluated%20Pipette%20on%20large%20clusters%20to%20show%20that%20it%20provides%20a%20significant%0Aspeedup%20over%20the%20prior%20art.%20The%20implementation%20of%20Pipette%20is%20available%20at%0Ahttps%3A//github.com/yimjinkyu1/date2024_pipette.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPipette%253A%2520Automatic%2520Fine-grained%2520Large%2520Language%2520Model%2520Training%250A%2520%2520Configurator%2520for%2520Real-World%2520Clusters%26entry.906535625%3DJinkyu%2520Yim%2520and%2520Jaeyong%2520Song%2520and%2520Yerim%2520Choi%2520and%2520Jaebeen%2520Lee%2520and%2520Jaewon%2520Jung%2520and%2520Hongsun%2520Jang%2520and%2520Jinho%2520Lee%26entry.1292438233%3D%2520%2520Training%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520known%2520to%2520be%2520challenging%2520because%2520of%250Athe%2520huge%2520computational%2520and%2520memory%2520capacity%2520requirements.%2520To%2520address%2520these%250Aissues%252C%2520it%2520is%2520common%2520to%2520use%2520a%2520cluster%2520of%2520GPUs%2520with%25203D%2520parallelism%252C%2520which%2520splits%250Aa%2520model%2520along%2520the%2520data%2520batch%252C%2520pipeline%2520stage%252C%2520and%2520intra-layer%2520tensor%250Adimensions.%2520However%252C%2520the%2520use%2520of%25203D%2520parallelism%2520produces%2520the%2520additional%250Achallenge%2520of%2520finding%2520the%2520optimal%2520number%2520of%2520ways%2520on%2520each%2520dimension%2520and%2520mapping%250Athe%2520split%2520models%2520onto%2520the%2520GPUs.%2520Several%2520previous%2520studies%2520have%2520attempted%2520to%250Aautomatically%2520find%2520the%2520optimal%2520configuration%252C%2520but%2520many%2520of%2520these%2520lacked%2520several%250Aimportant%2520aspects.%2520For%2520instance%252C%2520the%2520heterogeneous%2520nature%2520of%2520the%2520interconnect%250Aspeeds%2520is%2520often%2520ignored.%2520While%2520the%2520peak%2520bandwidths%2520for%2520the%2520interconnects%2520are%250Ausually%2520made%2520equal%252C%2520the%2520actual%2520attained%2520bandwidth%2520varies%2520per%2520link%2520in%2520real-world%250Aclusters.%2520Combined%2520with%2520the%2520critical%2520path%2520modeling%2520that%2520does%2520not%2520properly%250Aconsider%2520the%2520communication%252C%2520they%2520easily%2520fall%2520into%2520sub-optimal%2520configurations.%250AIn%2520addition%252C%2520they%2520often%2520fail%2520to%2520consider%2520the%2520memory%2520requirement%2520per%2520GPU%252C%2520often%250Arecommending%2520solutions%2520that%2520could%2520not%2520be%2520executed.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520Pipette%252C%2520which%2520is%2520an%2520automatic%2520fine-grained%2520LLM%2520training%250Aconfigurator%2520for%2520real-world%2520clusters.%2520By%2520devising%2520better%2520performance%2520models%250Aalong%2520with%2520the%2520memory%2520estimator%2520and%2520fine-grained%2520individual%2520GPU%2520assignment%252C%250APipette%2520achieves%2520faster%2520configurations%2520that%2520satisfy%2520the%2520memory%2520constraints.%2520We%250Aevaluated%2520Pipette%2520on%2520large%2520clusters%2520to%2520show%2520that%2520it%2520provides%2520a%2520significant%250Aspeedup%2520over%2520the%2520prior%2520art.%2520The%2520implementation%2520of%2520Pipette%2520is%2520available%2520at%250Ahttps%253A//github.com/yimjinkyu1/date2024_pipette.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pipette%3A%20Automatic%20Fine-grained%20Large%20Language%20Model%20Training%0A%20%20Configurator%20for%20Real-World%20Clusters&entry.906535625=Jinkyu%20Yim%20and%20Jaeyong%20Song%20and%20Yerim%20Choi%20and%20Jaebeen%20Lee%20and%20Jaewon%20Jung%20and%20Hongsun%20Jang%20and%20Jinho%20Lee&entry.1292438233=%20%20Training%20large%20language%20models%20%28LLMs%29%20is%20known%20to%20be%20challenging%20because%20of%0Athe%20huge%20computational%20and%20memory%20capacity%20requirements.%20To%20address%20these%0Aissues%2C%20it%20is%20common%20to%20use%20a%20cluster%20of%20GPUs%20with%203D%20parallelism%2C%20which%20splits%0Aa%20model%20along%20the%20data%20batch%2C%20pipeline%20stage%2C%20and%20intra-layer%20tensor%0Adimensions.%20However%2C%20the%20use%20of%203D%20parallelism%20produces%20the%20additional%0Achallenge%20of%20finding%20the%20optimal%20number%20of%20ways%20on%20each%20dimension%20and%20mapping%0Athe%20split%20models%20onto%20the%20GPUs.%20Several%20previous%20studies%20have%20attempted%20to%0Aautomatically%20find%20the%20optimal%20configuration%2C%20but%20many%20of%20these%20lacked%20several%0Aimportant%20aspects.%20For%20instance%2C%20the%20heterogeneous%20nature%20of%20the%20interconnect%0Aspeeds%20is%20often%20ignored.%20While%20the%20peak%20bandwidths%20for%20the%20interconnects%20are%0Ausually%20made%20equal%2C%20the%20actual%20attained%20bandwidth%20varies%20per%20link%20in%20real-world%0Aclusters.%20Combined%20with%20the%20critical%20path%20modeling%20that%20does%20not%20properly%0Aconsider%20the%20communication%2C%20they%20easily%20fall%20into%20sub-optimal%20configurations.%0AIn%20addition%2C%20they%20often%20fail%20to%20consider%20the%20memory%20requirement%20per%20GPU%2C%20often%0Arecommending%20solutions%20that%20could%20not%20be%20executed.%20To%20address%20these%20challenges%2C%0Awe%20propose%20Pipette%2C%20which%20is%20an%20automatic%20fine-grained%20LLM%20training%0Aconfigurator%20for%20real-world%20clusters.%20By%20devising%20better%20performance%20models%0Aalong%20with%20the%20memory%20estimator%20and%20fine-grained%20individual%20GPU%20assignment%2C%0APipette%20achieves%20faster%20configurations%20that%20satisfy%20the%20memory%20constraints.%20We%0Aevaluated%20Pipette%20on%20large%20clusters%20to%20show%20that%20it%20provides%20a%20significant%0Aspeedup%20over%20the%20prior%20art.%20The%20implementation%20of%20Pipette%20is%20available%20at%0Ahttps%3A//github.com/yimjinkyu1/date2024_pipette.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18093v1&entry.124074799=Read"},
{"title": "PatchAD: A Lightweight Patch-based MLP-Mixer for Time Series Anomaly\n  Detection", "author": "Zhijie Zhong and Zhiwen Yu and Yiyuan Yang and Weizheng Wang and Kaixiang Yang", "abstract": "  Anomaly detection in time series analysis is a pivotal task, yet it poses the\nchallenge of discerning normal and abnormal patterns in label-deficient\nscenarios. While prior studies have largely employed reconstruction-based\napproaches, which limits the models' representational capacities. Moreover,\nexisting deep learning-based methods are not sufficiently lightweight.\nAddressing these issues, we present PatchAD, our novel, highly efficient\nmultiscale patch-based MLP-Mixer architecture that utilizes contrastive\nlearning for representation extraction and anomaly detection. With its four\ndistinct MLP Mixers and innovative dual project constraint module, PatchAD\nmitigates potential model degradation and offers a lightweight solution,\nrequiring only $3.2$MB. Its efficacy is demonstrated by state-of-the-art\nresults across $9$ datasets sourced from different application scenarios,\noutperforming over $30$ comparative algorithms. PatchAD significantly improves\nthe classical F1 score by $50.5\\%$, the Aff-F1 score by $7.8\\%$, and the AUC by\n$10.0\\%$. The code is publicly available.\n\\url{https://github.com/EmorZz1G/PatchAD}\n", "link": "http://arxiv.org/abs/2401.09793v5", "date": "2024-05-28", "relevancy": 2.0627, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5297}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5077}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PatchAD%3A%20A%20Lightweight%20Patch-based%20MLP-Mixer%20for%20Time%20Series%20Anomaly%0A%20%20Detection&body=Title%3A%20PatchAD%3A%20A%20Lightweight%20Patch-based%20MLP-Mixer%20for%20Time%20Series%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Zhijie%20Zhong%20and%20Zhiwen%20Yu%20and%20Yiyuan%20Yang%20and%20Weizheng%20Wang%20and%20Kaixiang%20Yang%0AAbstract%3A%20%20%20Anomaly%20detection%20in%20time%20series%20analysis%20is%20a%20pivotal%20task%2C%20yet%20it%20poses%20the%0Achallenge%20of%20discerning%20normal%20and%20abnormal%20patterns%20in%20label-deficient%0Ascenarios.%20While%20prior%20studies%20have%20largely%20employed%20reconstruction-based%0Aapproaches%2C%20which%20limits%20the%20models%27%20representational%20capacities.%20Moreover%2C%0Aexisting%20deep%20learning-based%20methods%20are%20not%20sufficiently%20lightweight.%0AAddressing%20these%20issues%2C%20we%20present%20PatchAD%2C%20our%20novel%2C%20highly%20efficient%0Amultiscale%20patch-based%20MLP-Mixer%20architecture%20that%20utilizes%20contrastive%0Alearning%20for%20representation%20extraction%20and%20anomaly%20detection.%20With%20its%20four%0Adistinct%20MLP%20Mixers%20and%20innovative%20dual%20project%20constraint%20module%2C%20PatchAD%0Amitigates%20potential%20model%20degradation%20and%20offers%20a%20lightweight%20solution%2C%0Arequiring%20only%20%243.2%24MB.%20Its%20efficacy%20is%20demonstrated%20by%20state-of-the-art%0Aresults%20across%20%249%24%20datasets%20sourced%20from%20different%20application%20scenarios%2C%0Aoutperforming%20over%20%2430%24%20comparative%20algorithms.%20PatchAD%20significantly%20improves%0Athe%20classical%20F1%20score%20by%20%2450.5%5C%25%24%2C%20the%20Aff-F1%20score%20by%20%247.8%5C%25%24%2C%20and%20the%20AUC%20by%0A%2410.0%5C%25%24.%20The%20code%20is%20publicly%20available.%0A%5Curl%7Bhttps%3A//github.com/EmorZz1G/PatchAD%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09793v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatchAD%253A%2520A%2520Lightweight%2520Patch-based%2520MLP-Mixer%2520for%2520Time%2520Series%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DZhijie%2520Zhong%2520and%2520Zhiwen%2520Yu%2520and%2520Yiyuan%2520Yang%2520and%2520Weizheng%2520Wang%2520and%2520Kaixiang%2520Yang%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520in%2520time%2520series%2520analysis%2520is%2520a%2520pivotal%2520task%252C%2520yet%2520it%2520poses%2520the%250Achallenge%2520of%2520discerning%2520normal%2520and%2520abnormal%2520patterns%2520in%2520label-deficient%250Ascenarios.%2520While%2520prior%2520studies%2520have%2520largely%2520employed%2520reconstruction-based%250Aapproaches%252C%2520which%2520limits%2520the%2520models%2527%2520representational%2520capacities.%2520Moreover%252C%250Aexisting%2520deep%2520learning-based%2520methods%2520are%2520not%2520sufficiently%2520lightweight.%250AAddressing%2520these%2520issues%252C%2520we%2520present%2520PatchAD%252C%2520our%2520novel%252C%2520highly%2520efficient%250Amultiscale%2520patch-based%2520MLP-Mixer%2520architecture%2520that%2520utilizes%2520contrastive%250Alearning%2520for%2520representation%2520extraction%2520and%2520anomaly%2520detection.%2520With%2520its%2520four%250Adistinct%2520MLP%2520Mixers%2520and%2520innovative%2520dual%2520project%2520constraint%2520module%252C%2520PatchAD%250Amitigates%2520potential%2520model%2520degradation%2520and%2520offers%2520a%2520lightweight%2520solution%252C%250Arequiring%2520only%2520%25243.2%2524MB.%2520Its%2520efficacy%2520is%2520demonstrated%2520by%2520state-of-the-art%250Aresults%2520across%2520%25249%2524%2520datasets%2520sourced%2520from%2520different%2520application%2520scenarios%252C%250Aoutperforming%2520over%2520%252430%2524%2520comparative%2520algorithms.%2520PatchAD%2520significantly%2520improves%250Athe%2520classical%2520F1%2520score%2520by%2520%252450.5%255C%2525%2524%252C%2520the%2520Aff-F1%2520score%2520by%2520%25247.8%255C%2525%2524%252C%2520and%2520the%2520AUC%2520by%250A%252410.0%255C%2525%2524.%2520The%2520code%2520is%2520publicly%2520available.%250A%255Curl%257Bhttps%253A//github.com/EmorZz1G/PatchAD%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09793v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PatchAD%3A%20A%20Lightweight%20Patch-based%20MLP-Mixer%20for%20Time%20Series%20Anomaly%0A%20%20Detection&entry.906535625=Zhijie%20Zhong%20and%20Zhiwen%20Yu%20and%20Yiyuan%20Yang%20and%20Weizheng%20Wang%20and%20Kaixiang%20Yang&entry.1292438233=%20%20Anomaly%20detection%20in%20time%20series%20analysis%20is%20a%20pivotal%20task%2C%20yet%20it%20poses%20the%0Achallenge%20of%20discerning%20normal%20and%20abnormal%20patterns%20in%20label-deficient%0Ascenarios.%20While%20prior%20studies%20have%20largely%20employed%20reconstruction-based%0Aapproaches%2C%20which%20limits%20the%20models%27%20representational%20capacities.%20Moreover%2C%0Aexisting%20deep%20learning-based%20methods%20are%20not%20sufficiently%20lightweight.%0AAddressing%20these%20issues%2C%20we%20present%20PatchAD%2C%20our%20novel%2C%20highly%20efficient%0Amultiscale%20patch-based%20MLP-Mixer%20architecture%20that%20utilizes%20contrastive%0Alearning%20for%20representation%20extraction%20and%20anomaly%20detection.%20With%20its%20four%0Adistinct%20MLP%20Mixers%20and%20innovative%20dual%20project%20constraint%20module%2C%20PatchAD%0Amitigates%20potential%20model%20degradation%20and%20offers%20a%20lightweight%20solution%2C%0Arequiring%20only%20%243.2%24MB.%20Its%20efficacy%20is%20demonstrated%20by%20state-of-the-art%0Aresults%20across%20%249%24%20datasets%20sourced%20from%20different%20application%20scenarios%2C%0Aoutperforming%20over%20%2430%24%20comparative%20algorithms.%20PatchAD%20significantly%20improves%0Athe%20classical%20F1%20score%20by%20%2450.5%5C%25%24%2C%20the%20Aff-F1%20score%20by%20%247.8%5C%25%24%2C%20and%20the%20AUC%20by%0A%2410.0%5C%25%24.%20The%20code%20is%20publicly%20available.%0A%5Curl%7Bhttps%3A//github.com/EmorZz1G/PatchAD%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09793v5&entry.124074799=Read"},
{"title": "Token-level Direct Preference Optimization", "author": "Yongcheng Zeng and Guoqing Liu and Weiyu Ma and Ning Yang and Haifeng Zhang and Jun Wang", "abstract": "  Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.\n", "link": "http://arxiv.org/abs/2404.11999v2", "date": "2024-05-28", "relevancy": 2.0587, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5554}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.491}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-level%20Direct%20Preference%20Optimization&body=Title%3A%20Token-level%20Direct%20Preference%20Optimization%0AAuthor%3A%20Yongcheng%20Zeng%20and%20Guoqing%20Liu%20and%20Weiyu%20Ma%20and%20Ning%20Yang%20and%20Haifeng%20Zhang%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Fine-tuning%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20is%20essential%20to%20align%0Athem%20with%20human%20values%20and%20intentions.%20This%20process%20often%20utilizes%20methods%20like%0Apairwise%20comparisons%20and%20KL%20divergence%20against%20a%20reference%20LLM%2C%20focusing%20on%20the%0Aevaluation%20of%20full%20answers%20generated%20by%20the%20models.%20However%2C%20the%20generation%20of%0Athese%20responses%20occurs%20in%20a%20token%20level%2C%20following%20a%20sequential%2C%0Aauto-regressive%20fashion.%20In%20this%20paper%2C%20we%20introduce%20Token-level%20Direct%0APreference%20Optimization%20%28TDPO%29%2C%20a%20novel%20approach%20to%20align%20LLMs%20with%20human%0Apreferences%20by%20optimizing%20policy%20at%20the%20token%20level.%20Unlike%20previous%20methods%2C%0Awhich%20face%20challenges%20in%20divergence%20efficiency%2C%20TDPO%20incorporates%20forward%20KL%0Adivergence%20constraints%20for%20each%20token%2C%20improving%20alignment%20and%20diversity.%0AUtilizing%20the%20Bradley-Terry%20model%20for%20a%20token-based%20reward%20system%2C%20TDPO%0Aenhances%20the%20regulation%20of%20KL%20divergence%2C%20while%20preserving%20simplicity%20without%0Athe%20need%20for%20explicit%20reward%20modeling.%20Experimental%20results%20across%20various%20text%0Atasks%20demonstrate%20TDPO%27s%20superior%20performance%20in%20balancing%20alignment%20with%0Ageneration%20diversity.%20Notably%2C%20fine-tuning%20with%20TDPO%20strikes%20a%20better%20balance%0Athan%20DPO%20in%20the%20controlled%20sentiment%20generation%20and%20single-turn%20dialogue%0Adatasets%2C%20and%20significantly%20improves%20the%20quality%20of%20generated%20responses%0Acompared%20to%20both%20DPO%20and%20PPO-based%20RLHF%20methods.%20Our%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/Vance0124/Token-level-Direct-Preference-Optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11999v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-level%2520Direct%2520Preference%2520Optimization%26entry.906535625%3DYongcheng%2520Zeng%2520and%2520Guoqing%2520Liu%2520and%2520Weiyu%2520Ma%2520and%2520Ning%2520Yang%2520and%2520Haifeng%2520Zhang%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Fine-tuning%2520pre-trained%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520essential%2520to%2520align%250Athem%2520with%2520human%2520values%2520and%2520intentions.%2520This%2520process%2520often%2520utilizes%2520methods%2520like%250Apairwise%2520comparisons%2520and%2520KL%2520divergence%2520against%2520a%2520reference%2520LLM%252C%2520focusing%2520on%2520the%250Aevaluation%2520of%2520full%2520answers%2520generated%2520by%2520the%2520models.%2520However%252C%2520the%2520generation%2520of%250Athese%2520responses%2520occurs%2520in%2520a%2520token%2520level%252C%2520following%2520a%2520sequential%252C%250Aauto-regressive%2520fashion.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Token-level%2520Direct%250APreference%2520Optimization%2520%2528TDPO%2529%252C%2520a%2520novel%2520approach%2520to%2520align%2520LLMs%2520with%2520human%250Apreferences%2520by%2520optimizing%2520policy%2520at%2520the%2520token%2520level.%2520Unlike%2520previous%2520methods%252C%250Awhich%2520face%2520challenges%2520in%2520divergence%2520efficiency%252C%2520TDPO%2520incorporates%2520forward%2520KL%250Adivergence%2520constraints%2520for%2520each%2520token%252C%2520improving%2520alignment%2520and%2520diversity.%250AUtilizing%2520the%2520Bradley-Terry%2520model%2520for%2520a%2520token-based%2520reward%2520system%252C%2520TDPO%250Aenhances%2520the%2520regulation%2520of%2520KL%2520divergence%252C%2520while%2520preserving%2520simplicity%2520without%250Athe%2520need%2520for%2520explicit%2520reward%2520modeling.%2520Experimental%2520results%2520across%2520various%2520text%250Atasks%2520demonstrate%2520TDPO%2527s%2520superior%2520performance%2520in%2520balancing%2520alignment%2520with%250Ageneration%2520diversity.%2520Notably%252C%2520fine-tuning%2520with%2520TDPO%2520strikes%2520a%2520better%2520balance%250Athan%2520DPO%2520in%2520the%2520controlled%2520sentiment%2520generation%2520and%2520single-turn%2520dialogue%250Adatasets%252C%2520and%2520significantly%2520improves%2520the%2520quality%2520of%2520generated%2520responses%250Acompared%2520to%2520both%2520DPO%2520and%2520PPO-based%2520RLHF%2520methods.%2520Our%2520code%2520is%2520open-sourced%2520at%250Ahttps%253A//github.com/Vance0124/Token-level-Direct-Preference-Optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11999v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-level%20Direct%20Preference%20Optimization&entry.906535625=Yongcheng%20Zeng%20and%20Guoqing%20Liu%20and%20Weiyu%20Ma%20and%20Ning%20Yang%20and%20Haifeng%20Zhang%20and%20Jun%20Wang&entry.1292438233=%20%20Fine-tuning%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20is%20essential%20to%20align%0Athem%20with%20human%20values%20and%20intentions.%20This%20process%20often%20utilizes%20methods%20like%0Apairwise%20comparisons%20and%20KL%20divergence%20against%20a%20reference%20LLM%2C%20focusing%20on%20the%0Aevaluation%20of%20full%20answers%20generated%20by%20the%20models.%20However%2C%20the%20generation%20of%0Athese%20responses%20occurs%20in%20a%20token%20level%2C%20following%20a%20sequential%2C%0Aauto-regressive%20fashion.%20In%20this%20paper%2C%20we%20introduce%20Token-level%20Direct%0APreference%20Optimization%20%28TDPO%29%2C%20a%20novel%20approach%20to%20align%20LLMs%20with%20human%0Apreferences%20by%20optimizing%20policy%20at%20the%20token%20level.%20Unlike%20previous%20methods%2C%0Awhich%20face%20challenges%20in%20divergence%20efficiency%2C%20TDPO%20incorporates%20forward%20KL%0Adivergence%20constraints%20for%20each%20token%2C%20improving%20alignment%20and%20diversity.%0AUtilizing%20the%20Bradley-Terry%20model%20for%20a%20token-based%20reward%20system%2C%20TDPO%0Aenhances%20the%20regulation%20of%20KL%20divergence%2C%20while%20preserving%20simplicity%20without%0Athe%20need%20for%20explicit%20reward%20modeling.%20Experimental%20results%20across%20various%20text%0Atasks%20demonstrate%20TDPO%27s%20superior%20performance%20in%20balancing%20alignment%20with%0Ageneration%20diversity.%20Notably%2C%20fine-tuning%20with%20TDPO%20strikes%20a%20better%20balance%0Athan%20DPO%20in%20the%20controlled%20sentiment%20generation%20and%20single-turn%20dialogue%0Adatasets%2C%20and%20significantly%20improves%20the%20quality%20of%20generated%20responses%0Acompared%20to%20both%20DPO%20and%20PPO-based%20RLHF%20methods.%20Our%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/Vance0124/Token-level-Direct-Preference-Optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11999v2&entry.124074799=Read"},
{"title": "Bias in Motion: Theoretical Insights into the Dynamics of Bias in SGD\n  Training", "author": "Anchit Jain and Rozhin Nobahari and Aristide Baratin and Stefano Sarao Mannelli", "abstract": "  Machine learning systems often acquire biases by leveraging undesired\nfeatures in the data, impacting accuracy variably across different\nsub-populations. Current understanding of bias formation mostly focuses on the\ninitial and final stages of learning, leaving a gap in knowledge regarding the\ntransient dynamics. To address this gap, this paper explores the evolution of\nbias in a teacher-student setup modeling different data sub-populations with a\nGaussian-mixture model. We provide an analytical description of the stochastic\ngradient descent dynamics of a linear classifier in this setting, which we\nprove to be exact in high dimension. Notably, our analysis reveals how\ndifferent properties of sub-populations influence bias at different timescales,\nshowing a shifting preference of the classifier during training. Applying our\nfindings to fairness and robustness, we delineate how and when heterogeneous\ndata and spurious features can generate and amplify bias. We empirically\nvalidate our results in more complex scenarios by training deeper networks on\nsynthetic and real datasets, including CIFAR10, MNIST, and CelebA.\n", "link": "http://arxiv.org/abs/2405.18296v1", "date": "2024-05-28", "relevancy": 2.0569, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5241}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bias%20in%20Motion%3A%20Theoretical%20Insights%20into%20the%20Dynamics%20of%20Bias%20in%20SGD%0A%20%20Training&body=Title%3A%20Bias%20in%20Motion%3A%20Theoretical%20Insights%20into%20the%20Dynamics%20of%20Bias%20in%20SGD%0A%20%20Training%0AAuthor%3A%20Anchit%20Jain%20and%20Rozhin%20Nobahari%20and%20Aristide%20Baratin%20and%20Stefano%20Sarao%20Mannelli%0AAbstract%3A%20%20%20Machine%20learning%20systems%20often%20acquire%20biases%20by%20leveraging%20undesired%0Afeatures%20in%20the%20data%2C%20impacting%20accuracy%20variably%20across%20different%0Asub-populations.%20Current%20understanding%20of%20bias%20formation%20mostly%20focuses%20on%20the%0Ainitial%20and%20final%20stages%20of%20learning%2C%20leaving%20a%20gap%20in%20knowledge%20regarding%20the%0Atransient%20dynamics.%20To%20address%20this%20gap%2C%20this%20paper%20explores%20the%20evolution%20of%0Abias%20in%20a%20teacher-student%20setup%20modeling%20different%20data%20sub-populations%20with%20a%0AGaussian-mixture%20model.%20We%20provide%20an%20analytical%20description%20of%20the%20stochastic%0Agradient%20descent%20dynamics%20of%20a%20linear%20classifier%20in%20this%20setting%2C%20which%20we%0Aprove%20to%20be%20exact%20in%20high%20dimension.%20Notably%2C%20our%20analysis%20reveals%20how%0Adifferent%20properties%20of%20sub-populations%20influence%20bias%20at%20different%20timescales%2C%0Ashowing%20a%20shifting%20preference%20of%20the%20classifier%20during%20training.%20Applying%20our%0Afindings%20to%20fairness%20and%20robustness%2C%20we%20delineate%20how%20and%20when%20heterogeneous%0Adata%20and%20spurious%20features%20can%20generate%20and%20amplify%20bias.%20We%20empirically%0Avalidate%20our%20results%20in%20more%20complex%20scenarios%20by%20training%20deeper%20networks%20on%0Asynthetic%20and%20real%20datasets%2C%20including%20CIFAR10%2C%20MNIST%2C%20and%20CelebA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBias%2520in%2520Motion%253A%2520Theoretical%2520Insights%2520into%2520the%2520Dynamics%2520of%2520Bias%2520in%2520SGD%250A%2520%2520Training%26entry.906535625%3DAnchit%2520Jain%2520and%2520Rozhin%2520Nobahari%2520and%2520Aristide%2520Baratin%2520and%2520Stefano%2520Sarao%2520Mannelli%26entry.1292438233%3D%2520%2520Machine%2520learning%2520systems%2520often%2520acquire%2520biases%2520by%2520leveraging%2520undesired%250Afeatures%2520in%2520the%2520data%252C%2520impacting%2520accuracy%2520variably%2520across%2520different%250Asub-populations.%2520Current%2520understanding%2520of%2520bias%2520formation%2520mostly%2520focuses%2520on%2520the%250Ainitial%2520and%2520final%2520stages%2520of%2520learning%252C%2520leaving%2520a%2520gap%2520in%2520knowledge%2520regarding%2520the%250Atransient%2520dynamics.%2520To%2520address%2520this%2520gap%252C%2520this%2520paper%2520explores%2520the%2520evolution%2520of%250Abias%2520in%2520a%2520teacher-student%2520setup%2520modeling%2520different%2520data%2520sub-populations%2520with%2520a%250AGaussian-mixture%2520model.%2520We%2520provide%2520an%2520analytical%2520description%2520of%2520the%2520stochastic%250Agradient%2520descent%2520dynamics%2520of%2520a%2520linear%2520classifier%2520in%2520this%2520setting%252C%2520which%2520we%250Aprove%2520to%2520be%2520exact%2520in%2520high%2520dimension.%2520Notably%252C%2520our%2520analysis%2520reveals%2520how%250Adifferent%2520properties%2520of%2520sub-populations%2520influence%2520bias%2520at%2520different%2520timescales%252C%250Ashowing%2520a%2520shifting%2520preference%2520of%2520the%2520classifier%2520during%2520training.%2520Applying%2520our%250Afindings%2520to%2520fairness%2520and%2520robustness%252C%2520we%2520delineate%2520how%2520and%2520when%2520heterogeneous%250Adata%2520and%2520spurious%2520features%2520can%2520generate%2520and%2520amplify%2520bias.%2520We%2520empirically%250Avalidate%2520our%2520results%2520in%2520more%2520complex%2520scenarios%2520by%2520training%2520deeper%2520networks%2520on%250Asynthetic%2520and%2520real%2520datasets%252C%2520including%2520CIFAR10%252C%2520MNIST%252C%2520and%2520CelebA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bias%20in%20Motion%3A%20Theoretical%20Insights%20into%20the%20Dynamics%20of%20Bias%20in%20SGD%0A%20%20Training&entry.906535625=Anchit%20Jain%20and%20Rozhin%20Nobahari%20and%20Aristide%20Baratin%20and%20Stefano%20Sarao%20Mannelli&entry.1292438233=%20%20Machine%20learning%20systems%20often%20acquire%20biases%20by%20leveraging%20undesired%0Afeatures%20in%20the%20data%2C%20impacting%20accuracy%20variably%20across%20different%0Asub-populations.%20Current%20understanding%20of%20bias%20formation%20mostly%20focuses%20on%20the%0Ainitial%20and%20final%20stages%20of%20learning%2C%20leaving%20a%20gap%20in%20knowledge%20regarding%20the%0Atransient%20dynamics.%20To%20address%20this%20gap%2C%20this%20paper%20explores%20the%20evolution%20of%0Abias%20in%20a%20teacher-student%20setup%20modeling%20different%20data%20sub-populations%20with%20a%0AGaussian-mixture%20model.%20We%20provide%20an%20analytical%20description%20of%20the%20stochastic%0Agradient%20descent%20dynamics%20of%20a%20linear%20classifier%20in%20this%20setting%2C%20which%20we%0Aprove%20to%20be%20exact%20in%20high%20dimension.%20Notably%2C%20our%20analysis%20reveals%20how%0Adifferent%20properties%20of%20sub-populations%20influence%20bias%20at%20different%20timescales%2C%0Ashowing%20a%20shifting%20preference%20of%20the%20classifier%20during%20training.%20Applying%20our%0Afindings%20to%20fairness%20and%20robustness%2C%20we%20delineate%20how%20and%20when%20heterogeneous%0Adata%20and%20spurious%20features%20can%20generate%20and%20amplify%20bias.%20We%20empirically%0Avalidate%20our%20results%20in%20more%20complex%20scenarios%20by%20training%20deeper%20networks%20on%0Asynthetic%20and%20real%20datasets%2C%20including%20CIFAR10%2C%20MNIST%2C%20and%20CelebA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18296v1&entry.124074799=Read"},
{"title": "The Impact of Geometric Complexity on Neural Collapse in Transfer\n  Learning", "author": "Michael Munn and Benoit Dherin and Javier Gonzalvo", "abstract": "  Many of the recent remarkable advances in computer vision and language models\ncan be attributed to the success of transfer learning via the pre-training of\nlarge foundation models. However, a theoretical framework which explains this\nempirical success is incomplete and remains an active area of research.\nFlatness of the loss surface and neural collapse have recently emerged as\nuseful pre-training metrics which shed light on the implicit biases underlying\npre-training. In this paper, we explore the geometric complexity of a model's\nlearned representations as a fundamental mechanism that relates these two\nconcepts. We show through experiments and theory that mechanisms which affect\nthe geometric complexity of the pre-trained network also influence the neural\ncollapse. Furthermore, we show how this effect of the geometric complexity\ngeneralizes to the neural collapse of new classes as well, thus encouraging\nbetter performance on downstream tasks, particularly in the few-shot setting.\n", "link": "http://arxiv.org/abs/2405.15706v2", "date": "2024-05-28", "relevancy": 2.0553, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5242}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5188}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Geometric%20Complexity%20on%20Neural%20Collapse%20in%20Transfer%0A%20%20Learning&body=Title%3A%20The%20Impact%20of%20Geometric%20Complexity%20on%20Neural%20Collapse%20in%20Transfer%0A%20%20Learning%0AAuthor%3A%20Michael%20Munn%20and%20Benoit%20Dherin%20and%20Javier%20Gonzalvo%0AAbstract%3A%20%20%20Many%20of%20the%20recent%20remarkable%20advances%20in%20computer%20vision%20and%20language%20models%0Acan%20be%20attributed%20to%20the%20success%20of%20transfer%20learning%20via%20the%20pre-training%20of%0Alarge%20foundation%20models.%20However%2C%20a%20theoretical%20framework%20which%20explains%20this%0Aempirical%20success%20is%20incomplete%20and%20remains%20an%20active%20area%20of%20research.%0AFlatness%20of%20the%20loss%20surface%20and%20neural%20collapse%20have%20recently%20emerged%20as%0Auseful%20pre-training%20metrics%20which%20shed%20light%20on%20the%20implicit%20biases%20underlying%0Apre-training.%20In%20this%20paper%2C%20we%20explore%20the%20geometric%20complexity%20of%20a%20model%27s%0Alearned%20representations%20as%20a%20fundamental%20mechanism%20that%20relates%20these%20two%0Aconcepts.%20We%20show%20through%20experiments%20and%20theory%20that%20mechanisms%20which%20affect%0Athe%20geometric%20complexity%20of%20the%20pre-trained%20network%20also%20influence%20the%20neural%0Acollapse.%20Furthermore%2C%20we%20show%20how%20this%20effect%20of%20the%20geometric%20complexity%0Ageneralizes%20to%20the%20neural%20collapse%20of%20new%20classes%20as%20well%2C%20thus%20encouraging%0Abetter%20performance%20on%20downstream%20tasks%2C%20particularly%20in%20the%20few-shot%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15706v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Geometric%2520Complexity%2520on%2520Neural%2520Collapse%2520in%2520Transfer%250A%2520%2520Learning%26entry.906535625%3DMichael%2520Munn%2520and%2520Benoit%2520Dherin%2520and%2520Javier%2520Gonzalvo%26entry.1292438233%3D%2520%2520Many%2520of%2520the%2520recent%2520remarkable%2520advances%2520in%2520computer%2520vision%2520and%2520language%2520models%250Acan%2520be%2520attributed%2520to%2520the%2520success%2520of%2520transfer%2520learning%2520via%2520the%2520pre-training%2520of%250Alarge%2520foundation%2520models.%2520However%252C%2520a%2520theoretical%2520framework%2520which%2520explains%2520this%250Aempirical%2520success%2520is%2520incomplete%2520and%2520remains%2520an%2520active%2520area%2520of%2520research.%250AFlatness%2520of%2520the%2520loss%2520surface%2520and%2520neural%2520collapse%2520have%2520recently%2520emerged%2520as%250Auseful%2520pre-training%2520metrics%2520which%2520shed%2520light%2520on%2520the%2520implicit%2520biases%2520underlying%250Apre-training.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520geometric%2520complexity%2520of%2520a%2520model%2527s%250Alearned%2520representations%2520as%2520a%2520fundamental%2520mechanism%2520that%2520relates%2520these%2520two%250Aconcepts.%2520We%2520show%2520through%2520experiments%2520and%2520theory%2520that%2520mechanisms%2520which%2520affect%250Athe%2520geometric%2520complexity%2520of%2520the%2520pre-trained%2520network%2520also%2520influence%2520the%2520neural%250Acollapse.%2520Furthermore%252C%2520we%2520show%2520how%2520this%2520effect%2520of%2520the%2520geometric%2520complexity%250Ageneralizes%2520to%2520the%2520neural%2520collapse%2520of%2520new%2520classes%2520as%2520well%252C%2520thus%2520encouraging%250Abetter%2520performance%2520on%2520downstream%2520tasks%252C%2520particularly%2520in%2520the%2520few-shot%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15706v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Geometric%20Complexity%20on%20Neural%20Collapse%20in%20Transfer%0A%20%20Learning&entry.906535625=Michael%20Munn%20and%20Benoit%20Dherin%20and%20Javier%20Gonzalvo&entry.1292438233=%20%20Many%20of%20the%20recent%20remarkable%20advances%20in%20computer%20vision%20and%20language%20models%0Acan%20be%20attributed%20to%20the%20success%20of%20transfer%20learning%20via%20the%20pre-training%20of%0Alarge%20foundation%20models.%20However%2C%20a%20theoretical%20framework%20which%20explains%20this%0Aempirical%20success%20is%20incomplete%20and%20remains%20an%20active%20area%20of%20research.%0AFlatness%20of%20the%20loss%20surface%20and%20neural%20collapse%20have%20recently%20emerged%20as%0Auseful%20pre-training%20metrics%20which%20shed%20light%20on%20the%20implicit%20biases%20underlying%0Apre-training.%20In%20this%20paper%2C%20we%20explore%20the%20geometric%20complexity%20of%20a%20model%27s%0Alearned%20representations%20as%20a%20fundamental%20mechanism%20that%20relates%20these%20two%0Aconcepts.%20We%20show%20through%20experiments%20and%20theory%20that%20mechanisms%20which%20affect%0Athe%20geometric%20complexity%20of%20the%20pre-trained%20network%20also%20influence%20the%20neural%0Acollapse.%20Furthermore%2C%20we%20show%20how%20this%20effect%20of%20the%20geometric%20complexity%0Ageneralizes%20to%20the%20neural%20collapse%20of%20new%20classes%20as%20well%2C%20thus%20encouraging%0Abetter%20performance%20on%20downstream%20tasks%2C%20particularly%20in%20the%20few-shot%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15706v2&entry.124074799=Read"},
{"title": "Text-only Synthesis for Image Captioning", "author": "Qing Zhou and Junlin Huang and Qiang Li and Junyu Gao and Qi Wang", "abstract": "  From paired image-text training to text-only training for image captioning,\nthe pursuit of relaxing the requirements for high-cost and large-scale\nannotation of good quality data remains consistent. In this paper, we propose\nText-only Synthesis for Image Captioning (ToCa), which further advances this\nrelaxation with fewer human labor and less computing time. Specifically, we\ndeconstruct caption text into structures and lexical words, which serve as the\nfundamental components of the caption. By combining different structures and\nlexical words as inputs to the large language model, massive captions that\ncontain various patterns of lexical words are generated. This method not only\napproaches the target domain but also surpasses it by generating new captions,\nthereby enhancing the zero-shot generalization ability of the model.\nConsidering the different levels of data access in the real world, we define\nthree synthesis scenarios: cross-domain synthesis, in-domain synthesis, and\ndata-efficient synthesis. Experiments in these scenarios demonstrate the\ngeneralizability, transferability and practicability of ToCa with a nearly 5\nCIDEr improvement for zero-shot cross-domain captioning and a maximum increase\nof over 20 CIDEr for data-efficient captioning.\n", "link": "http://arxiv.org/abs/2405.18258v1", "date": "2024-05-28", "relevancy": 2.0517, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5269}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5122}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-only%20Synthesis%20for%20Image%20Captioning&body=Title%3A%20Text-only%20Synthesis%20for%20Image%20Captioning%0AAuthor%3A%20Qing%20Zhou%20and%20Junlin%20Huang%20and%20Qiang%20Li%20and%20Junyu%20Gao%20and%20Qi%20Wang%0AAbstract%3A%20%20%20From%20paired%20image-text%20training%20to%20text-only%20training%20for%20image%20captioning%2C%0Athe%20pursuit%20of%20relaxing%20the%20requirements%20for%20high-cost%20and%20large-scale%0Aannotation%20of%20good%20quality%20data%20remains%20consistent.%20In%20this%20paper%2C%20we%20propose%0AText-only%20Synthesis%20for%20Image%20Captioning%20%28ToCa%29%2C%20which%20further%20advances%20this%0Arelaxation%20with%20fewer%20human%20labor%20and%20less%20computing%20time.%20Specifically%2C%20we%0Adeconstruct%20caption%20text%20into%20structures%20and%20lexical%20words%2C%20which%20serve%20as%20the%0Afundamental%20components%20of%20the%20caption.%20By%20combining%20different%20structures%20and%0Alexical%20words%20as%20inputs%20to%20the%20large%20language%20model%2C%20massive%20captions%20that%0Acontain%20various%20patterns%20of%20lexical%20words%20are%20generated.%20This%20method%20not%20only%0Aapproaches%20the%20target%20domain%20but%20also%20surpasses%20it%20by%20generating%20new%20captions%2C%0Athereby%20enhancing%20the%20zero-shot%20generalization%20ability%20of%20the%20model.%0AConsidering%20the%20different%20levels%20of%20data%20access%20in%20the%20real%20world%2C%20we%20define%0Athree%20synthesis%20scenarios%3A%20cross-domain%20synthesis%2C%20in-domain%20synthesis%2C%20and%0Adata-efficient%20synthesis.%20Experiments%20in%20these%20scenarios%20demonstrate%20the%0Ageneralizability%2C%20transferability%20and%20practicability%20of%20ToCa%20with%20a%20nearly%205%0ACIDEr%20improvement%20for%20zero-shot%20cross-domain%20captioning%20and%20a%20maximum%20increase%0Aof%20over%2020%20CIDEr%20for%20data-efficient%20captioning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-only%2520Synthesis%2520for%2520Image%2520Captioning%26entry.906535625%3DQing%2520Zhou%2520and%2520Junlin%2520Huang%2520and%2520Qiang%2520Li%2520and%2520Junyu%2520Gao%2520and%2520Qi%2520Wang%26entry.1292438233%3D%2520%2520From%2520paired%2520image-text%2520training%2520to%2520text-only%2520training%2520for%2520image%2520captioning%252C%250Athe%2520pursuit%2520of%2520relaxing%2520the%2520requirements%2520for%2520high-cost%2520and%2520large-scale%250Aannotation%2520of%2520good%2520quality%2520data%2520remains%2520consistent.%2520In%2520this%2520paper%252C%2520we%2520propose%250AText-only%2520Synthesis%2520for%2520Image%2520Captioning%2520%2528ToCa%2529%252C%2520which%2520further%2520advances%2520this%250Arelaxation%2520with%2520fewer%2520human%2520labor%2520and%2520less%2520computing%2520time.%2520Specifically%252C%2520we%250Adeconstruct%2520caption%2520text%2520into%2520structures%2520and%2520lexical%2520words%252C%2520which%2520serve%2520as%2520the%250Afundamental%2520components%2520of%2520the%2520caption.%2520By%2520combining%2520different%2520structures%2520and%250Alexical%2520words%2520as%2520inputs%2520to%2520the%2520large%2520language%2520model%252C%2520massive%2520captions%2520that%250Acontain%2520various%2520patterns%2520of%2520lexical%2520words%2520are%2520generated.%2520This%2520method%2520not%2520only%250Aapproaches%2520the%2520target%2520domain%2520but%2520also%2520surpasses%2520it%2520by%2520generating%2520new%2520captions%252C%250Athereby%2520enhancing%2520the%2520zero-shot%2520generalization%2520ability%2520of%2520the%2520model.%250AConsidering%2520the%2520different%2520levels%2520of%2520data%2520access%2520in%2520the%2520real%2520world%252C%2520we%2520define%250Athree%2520synthesis%2520scenarios%253A%2520cross-domain%2520synthesis%252C%2520in-domain%2520synthesis%252C%2520and%250Adata-efficient%2520synthesis.%2520Experiments%2520in%2520these%2520scenarios%2520demonstrate%2520the%250Ageneralizability%252C%2520transferability%2520and%2520practicability%2520of%2520ToCa%2520with%2520a%2520nearly%25205%250ACIDEr%2520improvement%2520for%2520zero-shot%2520cross-domain%2520captioning%2520and%2520a%2520maximum%2520increase%250Aof%2520over%252020%2520CIDEr%2520for%2520data-efficient%2520captioning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-only%20Synthesis%20for%20Image%20Captioning&entry.906535625=Qing%20Zhou%20and%20Junlin%20Huang%20and%20Qiang%20Li%20and%20Junyu%20Gao%20and%20Qi%20Wang&entry.1292438233=%20%20From%20paired%20image-text%20training%20to%20text-only%20training%20for%20image%20captioning%2C%0Athe%20pursuit%20of%20relaxing%20the%20requirements%20for%20high-cost%20and%20large-scale%0Aannotation%20of%20good%20quality%20data%20remains%20consistent.%20In%20this%20paper%2C%20we%20propose%0AText-only%20Synthesis%20for%20Image%20Captioning%20%28ToCa%29%2C%20which%20further%20advances%20this%0Arelaxation%20with%20fewer%20human%20labor%20and%20less%20computing%20time.%20Specifically%2C%20we%0Adeconstruct%20caption%20text%20into%20structures%20and%20lexical%20words%2C%20which%20serve%20as%20the%0Afundamental%20components%20of%20the%20caption.%20By%20combining%20different%20structures%20and%0Alexical%20words%20as%20inputs%20to%20the%20large%20language%20model%2C%20massive%20captions%20that%0Acontain%20various%20patterns%20of%20lexical%20words%20are%20generated.%20This%20method%20not%20only%0Aapproaches%20the%20target%20domain%20but%20also%20surpasses%20it%20by%20generating%20new%20captions%2C%0Athereby%20enhancing%20the%20zero-shot%20generalization%20ability%20of%20the%20model.%0AConsidering%20the%20different%20levels%20of%20data%20access%20in%20the%20real%20world%2C%20we%20define%0Athree%20synthesis%20scenarios%3A%20cross-domain%20synthesis%2C%20in-domain%20synthesis%2C%20and%0Adata-efficient%20synthesis.%20Experiments%20in%20these%20scenarios%20demonstrate%20the%0Ageneralizability%2C%20transferability%20and%20practicability%20of%20ToCa%20with%20a%20nearly%205%0ACIDEr%20improvement%20for%20zero-shot%20cross-domain%20captioning%20and%20a%20maximum%20increase%0Aof%20over%2020%20CIDEr%20for%20data-efficient%20captioning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18258v1&entry.124074799=Read"},
{"title": "A General Framework for Learning from Weak Supervision", "author": "Hao Chen and Jindong Wang and Lei Feng and Xiang Li and Yidong Wang and Xing Xie and Masashi Sugiyama and Rita Singh and Bhiksha Raj", "abstract": "  Weakly supervised learning generally faces challenges in applicability to\nvarious scenarios with diverse weak supervision and in scalability due to the\ncomplexity of existing algorithms, thereby hindering the practical deployment.\nThis paper introduces a general framework for learning from weak supervision\n(GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization\n(EM) formulation, adeptly accommodating various weak supervision sources,\nincluding instance partial labels, aggregate statistics, pairwise observations,\nand unlabeled data. We further present an advanced algorithm that significantly\nsimplifies the EM computational demands using a Non-deterministic Finite\nAutomaton (NFA) along with a forward-backward algorithm, which effectively\nreduces time complexity from quadratic or factorial often required in existing\nsolutions to linear scale. The problem of learning from arbitrary weak\nsupervision is therefore converted to the NFA modeling of them. GLWS not only\nenhances the scalability of machine learning models but also demonstrates\nsuperior performance and versatility across 11 weak supervision scenarios. We\nhope our work paves the way for further advancements and practical deployment\nin this field.\n", "link": "http://arxiv.org/abs/2402.01922v2", "date": "2024-05-28", "relevancy": 2.0483, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5471}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4891}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20General%20Framework%20for%20Learning%20from%20Weak%20Supervision&body=Title%3A%20A%20General%20Framework%20for%20Learning%20from%20Weak%20Supervision%0AAuthor%3A%20Hao%20Chen%20and%20Jindong%20Wang%20and%20Lei%20Feng%20and%20Xiang%20Li%20and%20Yidong%20Wang%20and%20Xing%20Xie%20and%20Masashi%20Sugiyama%20and%20Rita%20Singh%20and%20Bhiksha%20Raj%0AAbstract%3A%20%20%20Weakly%20supervised%20learning%20generally%20faces%20challenges%20in%20applicability%20to%0Avarious%20scenarios%20with%20diverse%20weak%20supervision%20and%20in%20scalability%20due%20to%20the%0Acomplexity%20of%20existing%20algorithms%2C%20thereby%20hindering%20the%20practical%20deployment.%0AThis%20paper%20introduces%20a%20general%20framework%20for%20learning%20from%20weak%20supervision%0A%28GLWS%29%20with%20a%20novel%20algorithm.%20Central%20to%20GLWS%20is%20an%20Expectation-Maximization%0A%28EM%29%20formulation%2C%20adeptly%20accommodating%20various%20weak%20supervision%20sources%2C%0Aincluding%20instance%20partial%20labels%2C%20aggregate%20statistics%2C%20pairwise%20observations%2C%0Aand%20unlabeled%20data.%20We%20further%20present%20an%20advanced%20algorithm%20that%20significantly%0Asimplifies%20the%20EM%20computational%20demands%20using%20a%20Non-deterministic%20Finite%0AAutomaton%20%28NFA%29%20along%20with%20a%20forward-backward%20algorithm%2C%20which%20effectively%0Areduces%20time%20complexity%20from%20quadratic%20or%20factorial%20often%20required%20in%20existing%0Asolutions%20to%20linear%20scale.%20The%20problem%20of%20learning%20from%20arbitrary%20weak%0Asupervision%20is%20therefore%20converted%20to%20the%20NFA%20modeling%20of%20them.%20GLWS%20not%20only%0Aenhances%20the%20scalability%20of%20machine%20learning%20models%20but%20also%20demonstrates%0Asuperior%20performance%20and%20versatility%20across%2011%20weak%20supervision%20scenarios.%20We%0Ahope%20our%20work%20paves%20the%20way%20for%20further%20advancements%20and%20practical%20deployment%0Ain%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01922v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520General%2520Framework%2520for%2520Learning%2520from%2520Weak%2520Supervision%26entry.906535625%3DHao%2520Chen%2520and%2520Jindong%2520Wang%2520and%2520Lei%2520Feng%2520and%2520Xiang%2520Li%2520and%2520Yidong%2520Wang%2520and%2520Xing%2520Xie%2520and%2520Masashi%2520Sugiyama%2520and%2520Rita%2520Singh%2520and%2520Bhiksha%2520Raj%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520learning%2520generally%2520faces%2520challenges%2520in%2520applicability%2520to%250Avarious%2520scenarios%2520with%2520diverse%2520weak%2520supervision%2520and%2520in%2520scalability%2520due%2520to%2520the%250Acomplexity%2520of%2520existing%2520algorithms%252C%2520thereby%2520hindering%2520the%2520practical%2520deployment.%250AThis%2520paper%2520introduces%2520a%2520general%2520framework%2520for%2520learning%2520from%2520weak%2520supervision%250A%2528GLWS%2529%2520with%2520a%2520novel%2520algorithm.%2520Central%2520to%2520GLWS%2520is%2520an%2520Expectation-Maximization%250A%2528EM%2529%2520formulation%252C%2520adeptly%2520accommodating%2520various%2520weak%2520supervision%2520sources%252C%250Aincluding%2520instance%2520partial%2520labels%252C%2520aggregate%2520statistics%252C%2520pairwise%2520observations%252C%250Aand%2520unlabeled%2520data.%2520We%2520further%2520present%2520an%2520advanced%2520algorithm%2520that%2520significantly%250Asimplifies%2520the%2520EM%2520computational%2520demands%2520using%2520a%2520Non-deterministic%2520Finite%250AAutomaton%2520%2528NFA%2529%2520along%2520with%2520a%2520forward-backward%2520algorithm%252C%2520which%2520effectively%250Areduces%2520time%2520complexity%2520from%2520quadratic%2520or%2520factorial%2520often%2520required%2520in%2520existing%250Asolutions%2520to%2520linear%2520scale.%2520The%2520problem%2520of%2520learning%2520from%2520arbitrary%2520weak%250Asupervision%2520is%2520therefore%2520converted%2520to%2520the%2520NFA%2520modeling%2520of%2520them.%2520GLWS%2520not%2520only%250Aenhances%2520the%2520scalability%2520of%2520machine%2520learning%2520models%2520but%2520also%2520demonstrates%250Asuperior%2520performance%2520and%2520versatility%2520across%252011%2520weak%2520supervision%2520scenarios.%2520We%250Ahope%2520our%2520work%2520paves%2520the%2520way%2520for%2520further%2520advancements%2520and%2520practical%2520deployment%250Ain%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01922v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20Framework%20for%20Learning%20from%20Weak%20Supervision&entry.906535625=Hao%20Chen%20and%20Jindong%20Wang%20and%20Lei%20Feng%20and%20Xiang%20Li%20and%20Yidong%20Wang%20and%20Xing%20Xie%20and%20Masashi%20Sugiyama%20and%20Rita%20Singh%20and%20Bhiksha%20Raj&entry.1292438233=%20%20Weakly%20supervised%20learning%20generally%20faces%20challenges%20in%20applicability%20to%0Avarious%20scenarios%20with%20diverse%20weak%20supervision%20and%20in%20scalability%20due%20to%20the%0Acomplexity%20of%20existing%20algorithms%2C%20thereby%20hindering%20the%20practical%20deployment.%0AThis%20paper%20introduces%20a%20general%20framework%20for%20learning%20from%20weak%20supervision%0A%28GLWS%29%20with%20a%20novel%20algorithm.%20Central%20to%20GLWS%20is%20an%20Expectation-Maximization%0A%28EM%29%20formulation%2C%20adeptly%20accommodating%20various%20weak%20supervision%20sources%2C%0Aincluding%20instance%20partial%20labels%2C%20aggregate%20statistics%2C%20pairwise%20observations%2C%0Aand%20unlabeled%20data.%20We%20further%20present%20an%20advanced%20algorithm%20that%20significantly%0Asimplifies%20the%20EM%20computational%20demands%20using%20a%20Non-deterministic%20Finite%0AAutomaton%20%28NFA%29%20along%20with%20a%20forward-backward%20algorithm%2C%20which%20effectively%0Areduces%20time%20complexity%20from%20quadratic%20or%20factorial%20often%20required%20in%20existing%0Asolutions%20to%20linear%20scale.%20The%20problem%20of%20learning%20from%20arbitrary%20weak%0Asupervision%20is%20therefore%20converted%20to%20the%20NFA%20modeling%20of%20them.%20GLWS%20not%20only%0Aenhances%20the%20scalability%20of%20machine%20learning%20models%20but%20also%20demonstrates%0Asuperior%20performance%20and%20versatility%20across%2011%20weak%20supervision%20scenarios.%20We%0Ahope%20our%20work%20paves%20the%20way%20for%20further%20advancements%20and%20practical%20deployment%0Ain%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01922v2&entry.124074799=Read"},
{"title": "Implicitly Guided Design with PropEn: Match your Data to Follow the\n  Gradient", "author": "Nata\u0161a Tagasovska and Vladimir Gligorijevi\u0107 and Kyunghyun Cho and Andreas Loukas", "abstract": "  Across scientific domains, generating new models or optimizing existing ones\nwhile meeting specific criteria is crucial. Traditional machine learning\nframeworks for guided design use a generative model and a surrogate model\n(discriminator), requiring large datasets. However, real-world scientific\napplications often have limited data and complex landscapes, making data-hungry\nmodels inefficient or impractical. We propose a new framework, PropEn, inspired\nby ``matching'', which enables implicit guidance without training a\ndiscriminator. By matching each sample with a similar one that has a better\nproperty value, we create a larger training dataset that inherently indicates\nthe direction of improvement. Matching, combined with an encoder-decoder\narchitecture, forms a domain-agnostic generative framework for property\nenhancement. We show that training with a matched dataset approximates the\ngradient of the property of interest while remaining within the data\ndistribution, allowing efficient design optimization. Extensive evaluations in\ntoy problems and scientific applications, such as therapeutic protein design\nand airfoil optimization, demonstrate PropEn's advantages over common\nbaselines. Notably, the protein design results are validated with wet lab\nexperiments, confirming the competitiveness and effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2405.18075v1", "date": "2024-05-28", "relevancy": 2.0461, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5172}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5143}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicitly%20Guided%20Design%20with%20PropEn%3A%20Match%20your%20Data%20to%20Follow%20the%0A%20%20Gradient&body=Title%3A%20Implicitly%20Guided%20Design%20with%20PropEn%3A%20Match%20your%20Data%20to%20Follow%20the%0A%20%20Gradient%0AAuthor%3A%20Nata%C5%A1a%20Tagasovska%20and%20Vladimir%20Gligorijevi%C4%87%20and%20Kyunghyun%20Cho%20and%20Andreas%20Loukas%0AAbstract%3A%20%20%20Across%20scientific%20domains%2C%20generating%20new%20models%20or%20optimizing%20existing%20ones%0Awhile%20meeting%20specific%20criteria%20is%20crucial.%20Traditional%20machine%20learning%0Aframeworks%20for%20guided%20design%20use%20a%20generative%20model%20and%20a%20surrogate%20model%0A%28discriminator%29%2C%20requiring%20large%20datasets.%20However%2C%20real-world%20scientific%0Aapplications%20often%20have%20limited%20data%20and%20complex%20landscapes%2C%20making%20data-hungry%0Amodels%20inefficient%20or%20impractical.%20We%20propose%20a%20new%20framework%2C%20PropEn%2C%20inspired%0Aby%20%60%60matching%27%27%2C%20which%20enables%20implicit%20guidance%20without%20training%20a%0Adiscriminator.%20By%20matching%20each%20sample%20with%20a%20similar%20one%20that%20has%20a%20better%0Aproperty%20value%2C%20we%20create%20a%20larger%20training%20dataset%20that%20inherently%20indicates%0Athe%20direction%20of%20improvement.%20Matching%2C%20combined%20with%20an%20encoder-decoder%0Aarchitecture%2C%20forms%20a%20domain-agnostic%20generative%20framework%20for%20property%0Aenhancement.%20We%20show%20that%20training%20with%20a%20matched%20dataset%20approximates%20the%0Agradient%20of%20the%20property%20of%20interest%20while%20remaining%20within%20the%20data%0Adistribution%2C%20allowing%20efficient%20design%20optimization.%20Extensive%20evaluations%20in%0Atoy%20problems%20and%20scientific%20applications%2C%20such%20as%20therapeutic%20protein%20design%0Aand%20airfoil%20optimization%2C%20demonstrate%20PropEn%27s%20advantages%20over%20common%0Abaselines.%20Notably%2C%20the%20protein%20design%20results%20are%20validated%20with%20wet%20lab%0Aexperiments%2C%20confirming%20the%20competitiveness%20and%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicitly%2520Guided%2520Design%2520with%2520PropEn%253A%2520Match%2520your%2520Data%2520to%2520Follow%2520the%250A%2520%2520Gradient%26entry.906535625%3DNata%25C5%25A1a%2520Tagasovska%2520and%2520Vladimir%2520Gligorijevi%25C4%2587%2520and%2520Kyunghyun%2520Cho%2520and%2520Andreas%2520Loukas%26entry.1292438233%3D%2520%2520Across%2520scientific%2520domains%252C%2520generating%2520new%2520models%2520or%2520optimizing%2520existing%2520ones%250Awhile%2520meeting%2520specific%2520criteria%2520is%2520crucial.%2520Traditional%2520machine%2520learning%250Aframeworks%2520for%2520guided%2520design%2520use%2520a%2520generative%2520model%2520and%2520a%2520surrogate%2520model%250A%2528discriminator%2529%252C%2520requiring%2520large%2520datasets.%2520However%252C%2520real-world%2520scientific%250Aapplications%2520often%2520have%2520limited%2520data%2520and%2520complex%2520landscapes%252C%2520making%2520data-hungry%250Amodels%2520inefficient%2520or%2520impractical.%2520We%2520propose%2520a%2520new%2520framework%252C%2520PropEn%252C%2520inspired%250Aby%2520%2560%2560matching%2527%2527%252C%2520which%2520enables%2520implicit%2520guidance%2520without%2520training%2520a%250Adiscriminator.%2520By%2520matching%2520each%2520sample%2520with%2520a%2520similar%2520one%2520that%2520has%2520a%2520better%250Aproperty%2520value%252C%2520we%2520create%2520a%2520larger%2520training%2520dataset%2520that%2520inherently%2520indicates%250Athe%2520direction%2520of%2520improvement.%2520Matching%252C%2520combined%2520with%2520an%2520encoder-decoder%250Aarchitecture%252C%2520forms%2520a%2520domain-agnostic%2520generative%2520framework%2520for%2520property%250Aenhancement.%2520We%2520show%2520that%2520training%2520with%2520a%2520matched%2520dataset%2520approximates%2520the%250Agradient%2520of%2520the%2520property%2520of%2520interest%2520while%2520remaining%2520within%2520the%2520data%250Adistribution%252C%2520allowing%2520efficient%2520design%2520optimization.%2520Extensive%2520evaluations%2520in%250Atoy%2520problems%2520and%2520scientific%2520applications%252C%2520such%2520as%2520therapeutic%2520protein%2520design%250Aand%2520airfoil%2520optimization%252C%2520demonstrate%2520PropEn%2527s%2520advantages%2520over%2520common%250Abaselines.%2520Notably%252C%2520the%2520protein%2520design%2520results%2520are%2520validated%2520with%2520wet%2520lab%250Aexperiments%252C%2520confirming%2520the%2520competitiveness%2520and%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicitly%20Guided%20Design%20with%20PropEn%3A%20Match%20your%20Data%20to%20Follow%20the%0A%20%20Gradient&entry.906535625=Nata%C5%A1a%20Tagasovska%20and%20Vladimir%20Gligorijevi%C4%87%20and%20Kyunghyun%20Cho%20and%20Andreas%20Loukas&entry.1292438233=%20%20Across%20scientific%20domains%2C%20generating%20new%20models%20or%20optimizing%20existing%20ones%0Awhile%20meeting%20specific%20criteria%20is%20crucial.%20Traditional%20machine%20learning%0Aframeworks%20for%20guided%20design%20use%20a%20generative%20model%20and%20a%20surrogate%20model%0A%28discriminator%29%2C%20requiring%20large%20datasets.%20However%2C%20real-world%20scientific%0Aapplications%20often%20have%20limited%20data%20and%20complex%20landscapes%2C%20making%20data-hungry%0Amodels%20inefficient%20or%20impractical.%20We%20propose%20a%20new%20framework%2C%20PropEn%2C%20inspired%0Aby%20%60%60matching%27%27%2C%20which%20enables%20implicit%20guidance%20without%20training%20a%0Adiscriminator.%20By%20matching%20each%20sample%20with%20a%20similar%20one%20that%20has%20a%20better%0Aproperty%20value%2C%20we%20create%20a%20larger%20training%20dataset%20that%20inherently%20indicates%0Athe%20direction%20of%20improvement.%20Matching%2C%20combined%20with%20an%20encoder-decoder%0Aarchitecture%2C%20forms%20a%20domain-agnostic%20generative%20framework%20for%20property%0Aenhancement.%20We%20show%20that%20training%20with%20a%20matched%20dataset%20approximates%20the%0Agradient%20of%20the%20property%20of%20interest%20while%20remaining%20within%20the%20data%0Adistribution%2C%20allowing%20efficient%20design%20optimization.%20Extensive%20evaluations%20in%0Atoy%20problems%20and%20scientific%20applications%2C%20such%20as%20therapeutic%20protein%20design%0Aand%20airfoil%20optimization%2C%20demonstrate%20PropEn%27s%20advantages%20over%20common%0Abaselines.%20Notably%2C%20the%20protein%20design%20results%20are%20validated%20with%20wet%20lab%0Aexperiments%2C%20confirming%20the%20competitiveness%20and%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18075v1&entry.124074799=Read"},
{"title": "Synchronization on circles and spheres with nonlinear interactions", "author": "Christopher Criscitiello and Quentin Rebjock and Andrew D. McRae and Nicolas Boumal", "abstract": "  We consider the dynamics of $n$ points on a sphere in $\\mathbb{R}^d$ ($d \\geq\n2$) which attract each other according to a function $\\varphi$ of their inner\nproducts. When $\\varphi$ is linear ($\\varphi(t) = t$), the points converge to a\ncommon value (i.e., synchronize) in various connectivity scenarios: this is\npart of classical work on Kuramoto oscillator networks. When $\\varphi$ is\nexponential ($\\varphi(t) = e^{\\beta t}$), these dynamics correspond to a limit\nof how idealized transformers process data, as described by Geshkovski et al.\n(2024). Accordingly, they ask whether synchronization occurs for exponential\n$\\varphi$.\n  In the context of consensus for multi-agent control, Markdahl et al. (2018)\nshow that for $d \\geq 3$ (spheres), if the interaction graph is connected and\n$\\varphi$ is increasing and convex, then the system synchronizes. What is the\nsituation on circles ($d=2$)? First, we show that $\\varphi$ being increasing\nand convex is no longer sufficient. Then we identify a new condition (that the\nTaylor coefficients of $\\varphi'$ are decreasing) under which we do have\nsynchronization on the circle. In so doing, we provide some answers to the open\nproblems posed by Geshkovski et al. (2024).\n", "link": "http://arxiv.org/abs/2405.18273v1", "date": "2024-05-28", "relevancy": 2.0459, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4276}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4067}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synchronization%20on%20circles%20and%20spheres%20with%20nonlinear%20interactions&body=Title%3A%20Synchronization%20on%20circles%20and%20spheres%20with%20nonlinear%20interactions%0AAuthor%3A%20Christopher%20Criscitiello%20and%20Quentin%20Rebjock%20and%20Andrew%20D.%20McRae%20and%20Nicolas%20Boumal%0AAbstract%3A%20%20%20We%20consider%20the%20dynamics%20of%20%24n%24%20points%20on%20a%20sphere%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%20%28%24d%20%5Cgeq%0A2%24%29%20which%20attract%20each%20other%20according%20to%20a%20function%20%24%5Cvarphi%24%20of%20their%20inner%0Aproducts.%20When%20%24%5Cvarphi%24%20is%20linear%20%28%24%5Cvarphi%28t%29%20%3D%20t%24%29%2C%20the%20points%20converge%20to%20a%0Acommon%20value%20%28i.e.%2C%20synchronize%29%20in%20various%20connectivity%20scenarios%3A%20this%20is%0Apart%20of%20classical%20work%20on%20Kuramoto%20oscillator%20networks.%20When%20%24%5Cvarphi%24%20is%0Aexponential%20%28%24%5Cvarphi%28t%29%20%3D%20e%5E%7B%5Cbeta%20t%7D%24%29%2C%20these%20dynamics%20correspond%20to%20a%20limit%0Aof%20how%20idealized%20transformers%20process%20data%2C%20as%20described%20by%20Geshkovski%20et%20al.%0A%282024%29.%20Accordingly%2C%20they%20ask%20whether%20synchronization%20occurs%20for%20exponential%0A%24%5Cvarphi%24.%0A%20%20In%20the%20context%20of%20consensus%20for%20multi-agent%20control%2C%20Markdahl%20et%20al.%20%282018%29%0Ashow%20that%20for%20%24d%20%5Cgeq%203%24%20%28spheres%29%2C%20if%20the%20interaction%20graph%20is%20connected%20and%0A%24%5Cvarphi%24%20is%20increasing%20and%20convex%2C%20then%20the%20system%20synchronizes.%20What%20is%20the%0Asituation%20on%20circles%20%28%24d%3D2%24%29%3F%20First%2C%20we%20show%20that%20%24%5Cvarphi%24%20being%20increasing%0Aand%20convex%20is%20no%20longer%20sufficient.%20Then%20we%20identify%20a%20new%20condition%20%28that%20the%0ATaylor%20coefficients%20of%20%24%5Cvarphi%27%24%20are%20decreasing%29%20under%20which%20we%20do%20have%0Asynchronization%20on%20the%20circle.%20In%20so%20doing%2C%20we%20provide%20some%20answers%20to%20the%20open%0Aproblems%20posed%20by%20Geshkovski%20et%20al.%20%282024%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynchronization%2520on%2520circles%2520and%2520spheres%2520with%2520nonlinear%2520interactions%26entry.906535625%3DChristopher%2520Criscitiello%2520and%2520Quentin%2520Rebjock%2520and%2520Andrew%2520D.%2520McRae%2520and%2520Nicolas%2520Boumal%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520dynamics%2520of%2520%2524n%2524%2520points%2520on%2520a%2520sphere%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%2520%2528%2524d%2520%255Cgeq%250A2%2524%2529%2520which%2520attract%2520each%2520other%2520according%2520to%2520a%2520function%2520%2524%255Cvarphi%2524%2520of%2520their%2520inner%250Aproducts.%2520When%2520%2524%255Cvarphi%2524%2520is%2520linear%2520%2528%2524%255Cvarphi%2528t%2529%2520%253D%2520t%2524%2529%252C%2520the%2520points%2520converge%2520to%2520a%250Acommon%2520value%2520%2528i.e.%252C%2520synchronize%2529%2520in%2520various%2520connectivity%2520scenarios%253A%2520this%2520is%250Apart%2520of%2520classical%2520work%2520on%2520Kuramoto%2520oscillator%2520networks.%2520When%2520%2524%255Cvarphi%2524%2520is%250Aexponential%2520%2528%2524%255Cvarphi%2528t%2529%2520%253D%2520e%255E%257B%255Cbeta%2520t%257D%2524%2529%252C%2520these%2520dynamics%2520correspond%2520to%2520a%2520limit%250Aof%2520how%2520idealized%2520transformers%2520process%2520data%252C%2520as%2520described%2520by%2520Geshkovski%2520et%2520al.%250A%25282024%2529.%2520Accordingly%252C%2520they%2520ask%2520whether%2520synchronization%2520occurs%2520for%2520exponential%250A%2524%255Cvarphi%2524.%250A%2520%2520In%2520the%2520context%2520of%2520consensus%2520for%2520multi-agent%2520control%252C%2520Markdahl%2520et%2520al.%2520%25282018%2529%250Ashow%2520that%2520for%2520%2524d%2520%255Cgeq%25203%2524%2520%2528spheres%2529%252C%2520if%2520the%2520interaction%2520graph%2520is%2520connected%2520and%250A%2524%255Cvarphi%2524%2520is%2520increasing%2520and%2520convex%252C%2520then%2520the%2520system%2520synchronizes.%2520What%2520is%2520the%250Asituation%2520on%2520circles%2520%2528%2524d%253D2%2524%2529%253F%2520First%252C%2520we%2520show%2520that%2520%2524%255Cvarphi%2524%2520being%2520increasing%250Aand%2520convex%2520is%2520no%2520longer%2520sufficient.%2520Then%2520we%2520identify%2520a%2520new%2520condition%2520%2528that%2520the%250ATaylor%2520coefficients%2520of%2520%2524%255Cvarphi%2527%2524%2520are%2520decreasing%2529%2520under%2520which%2520we%2520do%2520have%250Asynchronization%2520on%2520the%2520circle.%2520In%2520so%2520doing%252C%2520we%2520provide%2520some%2520answers%2520to%2520the%2520open%250Aproblems%2520posed%2520by%2520Geshkovski%2520et%2520al.%2520%25282024%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synchronization%20on%20circles%20and%20spheres%20with%20nonlinear%20interactions&entry.906535625=Christopher%20Criscitiello%20and%20Quentin%20Rebjock%20and%20Andrew%20D.%20McRae%20and%20Nicolas%20Boumal&entry.1292438233=%20%20We%20consider%20the%20dynamics%20of%20%24n%24%20points%20on%20a%20sphere%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%20%28%24d%20%5Cgeq%0A2%24%29%20which%20attract%20each%20other%20according%20to%20a%20function%20%24%5Cvarphi%24%20of%20their%20inner%0Aproducts.%20When%20%24%5Cvarphi%24%20is%20linear%20%28%24%5Cvarphi%28t%29%20%3D%20t%24%29%2C%20the%20points%20converge%20to%20a%0Acommon%20value%20%28i.e.%2C%20synchronize%29%20in%20various%20connectivity%20scenarios%3A%20this%20is%0Apart%20of%20classical%20work%20on%20Kuramoto%20oscillator%20networks.%20When%20%24%5Cvarphi%24%20is%0Aexponential%20%28%24%5Cvarphi%28t%29%20%3D%20e%5E%7B%5Cbeta%20t%7D%24%29%2C%20these%20dynamics%20correspond%20to%20a%20limit%0Aof%20how%20idealized%20transformers%20process%20data%2C%20as%20described%20by%20Geshkovski%20et%20al.%0A%282024%29.%20Accordingly%2C%20they%20ask%20whether%20synchronization%20occurs%20for%20exponential%0A%24%5Cvarphi%24.%0A%20%20In%20the%20context%20of%20consensus%20for%20multi-agent%20control%2C%20Markdahl%20et%20al.%20%282018%29%0Ashow%20that%20for%20%24d%20%5Cgeq%203%24%20%28spheres%29%2C%20if%20the%20interaction%20graph%20is%20connected%20and%0A%24%5Cvarphi%24%20is%20increasing%20and%20convex%2C%20then%20the%20system%20synchronizes.%20What%20is%20the%0Asituation%20on%20circles%20%28%24d%3D2%24%29%3F%20First%2C%20we%20show%20that%20%24%5Cvarphi%24%20being%20increasing%0Aand%20convex%20is%20no%20longer%20sufficient.%20Then%20we%20identify%20a%20new%20condition%20%28that%20the%0ATaylor%20coefficients%20of%20%24%5Cvarphi%27%24%20are%20decreasing%29%20under%20which%20we%20do%20have%0Asynchronization%20on%20the%20circle.%20In%20so%20doing%2C%20we%20provide%20some%20answers%20to%20the%20open%0Aproblems%20posed%20by%20Geshkovski%20et%20al.%20%282024%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18273v1&entry.124074799=Read"},
{"title": "KOALA: Empirical Lessons Toward Memory-Efficient and Fast Diffusion\n  Models for Text-to-Image Synthesis", "author": "Youngwan Lee and Kwanyong Park and Yoorhim Cho and Yong-Ju Lee and Sung Ju Hwang", "abstract": "  As text-to-image (T2I) synthesis models increase in size, they demand higher\ninference costs due to the need for more expensive GPUs with larger memory,\nwhich makes it challenging to reproduce these models in addition to the\nrestricted access to training datasets. Our study aims to reduce these\ninference costs and explores how far the generative capabilities of T2I models\ncan be extended using only publicly available datasets and open-source models.\nTo this end, by using the de facto standard text-to-image model, Stable\nDiffusion XL (SDXL), we present three key practices in building an efficient\nT2I model: (1) Knowledge distillation: we explore how to effectively distill\nthe generation capability of SDXL into an efficient U-Net and find that\nself-attention is the most crucial part. (2) Data: despite fewer samples,\nhigh-resolution images with rich captions are more crucial than a larger number\nof low-resolution images with short captions. (3) Teacher: Step-distilled\nTeacher allows T2I models to reduce the noising steps. Based on these findings,\nwe build two types of efficient text-to-image models, called KOALA-Turbo\n&-Lightning, with two compact U-Nets (1B & 700M), reducing the model size up to\n54% and 69% of the SDXL U-Net. In particular, the KOALA-Lightning-700M is 4x\nfaster than SDXL while still maintaining satisfactory generation quality.\nMoreover, unlike SDXL, our KOALA models can generate 1024px high-resolution\nimages on consumer-grade GPUs with 8GB of VRAMs (3060Ti). We believe that our\nKOALA models will have a significant practical impact, serving as\ncost-effective alternatives to SDXL for academic researchers and general users\nin resource-constrained environments.\n", "link": "http://arxiv.org/abs/2312.04005v2", "date": "2024-05-28", "relevancy": 2.0364, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7171}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6765}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KOALA%3A%20Empirical%20Lessons%20Toward%20Memory-Efficient%20and%20Fast%20Diffusion%0A%20%20Models%20for%20Text-to-Image%20Synthesis&body=Title%3A%20KOALA%3A%20Empirical%20Lessons%20Toward%20Memory-Efficient%20and%20Fast%20Diffusion%0A%20%20Models%20for%20Text-to-Image%20Synthesis%0AAuthor%3A%20Youngwan%20Lee%20and%20Kwanyong%20Park%20and%20Yoorhim%20Cho%20and%20Yong-Ju%20Lee%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20As%20text-to-image%20%28T2I%29%20synthesis%20models%20increase%20in%20size%2C%20they%20demand%20higher%0Ainference%20costs%20due%20to%20the%20need%20for%20more%20expensive%20GPUs%20with%20larger%20memory%2C%0Awhich%20makes%20it%20challenging%20to%20reproduce%20these%20models%20in%20addition%20to%20the%0Arestricted%20access%20to%20training%20datasets.%20Our%20study%20aims%20to%20reduce%20these%0Ainference%20costs%20and%20explores%20how%20far%20the%20generative%20capabilities%20of%20T2I%20models%0Acan%20be%20extended%20using%20only%20publicly%20available%20datasets%20and%20open-source%20models.%0ATo%20this%20end%2C%20by%20using%20the%20de%20facto%20standard%20text-to-image%20model%2C%20Stable%0ADiffusion%20XL%20%28SDXL%29%2C%20we%20present%20three%20key%20practices%20in%20building%20an%20efficient%0AT2I%20model%3A%20%281%29%20Knowledge%20distillation%3A%20we%20explore%20how%20to%20effectively%20distill%0Athe%20generation%20capability%20of%20SDXL%20into%20an%20efficient%20U-Net%20and%20find%20that%0Aself-attention%20is%20the%20most%20crucial%20part.%20%282%29%20Data%3A%20despite%20fewer%20samples%2C%0Ahigh-resolution%20images%20with%20rich%20captions%20are%20more%20crucial%20than%20a%20larger%20number%0Aof%20low-resolution%20images%20with%20short%20captions.%20%283%29%20Teacher%3A%20Step-distilled%0ATeacher%20allows%20T2I%20models%20to%20reduce%20the%20noising%20steps.%20Based%20on%20these%20findings%2C%0Awe%20build%20two%20types%20of%20efficient%20text-to-image%20models%2C%20called%20KOALA-Turbo%0A%26-Lightning%2C%20with%20two%20compact%20U-Nets%20%281B%20%26%20700M%29%2C%20reducing%20the%20model%20size%20up%20to%0A54%25%20and%2069%25%20of%20the%20SDXL%20U-Net.%20In%20particular%2C%20the%20KOALA-Lightning-700M%20is%204x%0Afaster%20than%20SDXL%20while%20still%20maintaining%20satisfactory%20generation%20quality.%0AMoreover%2C%20unlike%20SDXL%2C%20our%20KOALA%20models%20can%20generate%201024px%20high-resolution%0Aimages%20on%20consumer-grade%20GPUs%20with%208GB%20of%20VRAMs%20%283060Ti%29.%20We%20believe%20that%20our%0AKOALA%20models%20will%20have%20a%20significant%20practical%20impact%2C%20serving%20as%0Acost-effective%20alternatives%20to%20SDXL%20for%20academic%20researchers%20and%20general%20users%0Ain%20resource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04005v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKOALA%253A%2520Empirical%2520Lessons%2520Toward%2520Memory-Efficient%2520and%2520Fast%2520Diffusion%250A%2520%2520Models%2520for%2520Text-to-Image%2520Synthesis%26entry.906535625%3DYoungwan%2520Lee%2520and%2520Kwanyong%2520Park%2520and%2520Yoorhim%2520Cho%2520and%2520Yong-Ju%2520Lee%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3D%2520%2520As%2520text-to-image%2520%2528T2I%2529%2520synthesis%2520models%2520increase%2520in%2520size%252C%2520they%2520demand%2520higher%250Ainference%2520costs%2520due%2520to%2520the%2520need%2520for%2520more%2520expensive%2520GPUs%2520with%2520larger%2520memory%252C%250Awhich%2520makes%2520it%2520challenging%2520to%2520reproduce%2520these%2520models%2520in%2520addition%2520to%2520the%250Arestricted%2520access%2520to%2520training%2520datasets.%2520Our%2520study%2520aims%2520to%2520reduce%2520these%250Ainference%2520costs%2520and%2520explores%2520how%2520far%2520the%2520generative%2520capabilities%2520of%2520T2I%2520models%250Acan%2520be%2520extended%2520using%2520only%2520publicly%2520available%2520datasets%2520and%2520open-source%2520models.%250ATo%2520this%2520end%252C%2520by%2520using%2520the%2520de%2520facto%2520standard%2520text-to-image%2520model%252C%2520Stable%250ADiffusion%2520XL%2520%2528SDXL%2529%252C%2520we%2520present%2520three%2520key%2520practices%2520in%2520building%2520an%2520efficient%250AT2I%2520model%253A%2520%25281%2529%2520Knowledge%2520distillation%253A%2520we%2520explore%2520how%2520to%2520effectively%2520distill%250Athe%2520generation%2520capability%2520of%2520SDXL%2520into%2520an%2520efficient%2520U-Net%2520and%2520find%2520that%250Aself-attention%2520is%2520the%2520most%2520crucial%2520part.%2520%25282%2529%2520Data%253A%2520despite%2520fewer%2520samples%252C%250Ahigh-resolution%2520images%2520with%2520rich%2520captions%2520are%2520more%2520crucial%2520than%2520a%2520larger%2520number%250Aof%2520low-resolution%2520images%2520with%2520short%2520captions.%2520%25283%2529%2520Teacher%253A%2520Step-distilled%250ATeacher%2520allows%2520T2I%2520models%2520to%2520reduce%2520the%2520noising%2520steps.%2520Based%2520on%2520these%2520findings%252C%250Awe%2520build%2520two%2520types%2520of%2520efficient%2520text-to-image%2520models%252C%2520called%2520KOALA-Turbo%250A%2526-Lightning%252C%2520with%2520two%2520compact%2520U-Nets%2520%25281B%2520%2526%2520700M%2529%252C%2520reducing%2520the%2520model%2520size%2520up%2520to%250A54%2525%2520and%252069%2525%2520of%2520the%2520SDXL%2520U-Net.%2520In%2520particular%252C%2520the%2520KOALA-Lightning-700M%2520is%25204x%250Afaster%2520than%2520SDXL%2520while%2520still%2520maintaining%2520satisfactory%2520generation%2520quality.%250AMoreover%252C%2520unlike%2520SDXL%252C%2520our%2520KOALA%2520models%2520can%2520generate%25201024px%2520high-resolution%250Aimages%2520on%2520consumer-grade%2520GPUs%2520with%25208GB%2520of%2520VRAMs%2520%25283060Ti%2529.%2520We%2520believe%2520that%2520our%250AKOALA%2520models%2520will%2520have%2520a%2520significant%2520practical%2520impact%252C%2520serving%2520as%250Acost-effective%2520alternatives%2520to%2520SDXL%2520for%2520academic%2520researchers%2520and%2520general%2520users%250Ain%2520resource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04005v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KOALA%3A%20Empirical%20Lessons%20Toward%20Memory-Efficient%20and%20Fast%20Diffusion%0A%20%20Models%20for%20Text-to-Image%20Synthesis&entry.906535625=Youngwan%20Lee%20and%20Kwanyong%20Park%20and%20Yoorhim%20Cho%20and%20Yong-Ju%20Lee%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20As%20text-to-image%20%28T2I%29%20synthesis%20models%20increase%20in%20size%2C%20they%20demand%20higher%0Ainference%20costs%20due%20to%20the%20need%20for%20more%20expensive%20GPUs%20with%20larger%20memory%2C%0Awhich%20makes%20it%20challenging%20to%20reproduce%20these%20models%20in%20addition%20to%20the%0Arestricted%20access%20to%20training%20datasets.%20Our%20study%20aims%20to%20reduce%20these%0Ainference%20costs%20and%20explores%20how%20far%20the%20generative%20capabilities%20of%20T2I%20models%0Acan%20be%20extended%20using%20only%20publicly%20available%20datasets%20and%20open-source%20models.%0ATo%20this%20end%2C%20by%20using%20the%20de%20facto%20standard%20text-to-image%20model%2C%20Stable%0ADiffusion%20XL%20%28SDXL%29%2C%20we%20present%20three%20key%20practices%20in%20building%20an%20efficient%0AT2I%20model%3A%20%281%29%20Knowledge%20distillation%3A%20we%20explore%20how%20to%20effectively%20distill%0Athe%20generation%20capability%20of%20SDXL%20into%20an%20efficient%20U-Net%20and%20find%20that%0Aself-attention%20is%20the%20most%20crucial%20part.%20%282%29%20Data%3A%20despite%20fewer%20samples%2C%0Ahigh-resolution%20images%20with%20rich%20captions%20are%20more%20crucial%20than%20a%20larger%20number%0Aof%20low-resolution%20images%20with%20short%20captions.%20%283%29%20Teacher%3A%20Step-distilled%0ATeacher%20allows%20T2I%20models%20to%20reduce%20the%20noising%20steps.%20Based%20on%20these%20findings%2C%0Awe%20build%20two%20types%20of%20efficient%20text-to-image%20models%2C%20called%20KOALA-Turbo%0A%26-Lightning%2C%20with%20two%20compact%20U-Nets%20%281B%20%26%20700M%29%2C%20reducing%20the%20model%20size%20up%20to%0A54%25%20and%2069%25%20of%20the%20SDXL%20U-Net.%20In%20particular%2C%20the%20KOALA-Lightning-700M%20is%204x%0Afaster%20than%20SDXL%20while%20still%20maintaining%20satisfactory%20generation%20quality.%0AMoreover%2C%20unlike%20SDXL%2C%20our%20KOALA%20models%20can%20generate%201024px%20high-resolution%0Aimages%20on%20consumer-grade%20GPUs%20with%208GB%20of%20VRAMs%20%283060Ti%29.%20We%20believe%20that%20our%0AKOALA%20models%20will%20have%20a%20significant%20practical%20impact%2C%20serving%20as%0Acost-effective%20alternatives%20to%20SDXL%20for%20academic%20researchers%20and%20general%20users%0Ain%20resource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04005v2&entry.124074799=Read"},
{"title": "Redefining Safety for Autonomous Vehicles", "author": "Philip Koopman and William Widen", "abstract": "  Existing definitions and associated conceptual frameworks for computer-based\nsystem safety should be revisited in light of real-world experiences from\ndeploying autonomous vehicles. Current terminology used by industry safety\nstandards emphasizes mitigation of risk from specifically identified hazards,\nand carries assumptions based on human-supervised vehicle operation. Operation\nwithout a human driver dramatically increases the scope of safety concerns,\nespecially due to operation in an open world environment, a requirement to\nself-enforce operational limits, participation in an ad hoc sociotechnical\nsystem of systems, and a requirement to conform to both legal and ethical\nconstraints. Existing standards and terminology only partially address these\nnew challenges. We propose updated definitions for core system safety concepts\nthat encompass these additional considerations as a starting point for evolving\nsafe-ty approaches to address these additional safety challenges. These results\nmight additionally inform framing safety terminology for other autonomous\nsystem applications.\n", "link": "http://arxiv.org/abs/2404.16768v3", "date": "2024-05-28", "relevancy": 1.955, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.511}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.474}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Redefining%20Safety%20for%20Autonomous%20Vehicles&body=Title%3A%20Redefining%20Safety%20for%20Autonomous%20Vehicles%0AAuthor%3A%20Philip%20Koopman%20and%20William%20Widen%0AAbstract%3A%20%20%20Existing%20definitions%20and%20associated%20conceptual%20frameworks%20for%20computer-based%0Asystem%20safety%20should%20be%20revisited%20in%20light%20of%20real-world%20experiences%20from%0Adeploying%20autonomous%20vehicles.%20Current%20terminology%20used%20by%20industry%20safety%0Astandards%20emphasizes%20mitigation%20of%20risk%20from%20specifically%20identified%20hazards%2C%0Aand%20carries%20assumptions%20based%20on%20human-supervised%20vehicle%20operation.%20Operation%0Awithout%20a%20human%20driver%20dramatically%20increases%20the%20scope%20of%20safety%20concerns%2C%0Aespecially%20due%20to%20operation%20in%20an%20open%20world%20environment%2C%20a%20requirement%20to%0Aself-enforce%20operational%20limits%2C%20participation%20in%20an%20ad%20hoc%20sociotechnical%0Asystem%20of%20systems%2C%20and%20a%20requirement%20to%20conform%20to%20both%20legal%20and%20ethical%0Aconstraints.%20Existing%20standards%20and%20terminology%20only%20partially%20address%20these%0Anew%20challenges.%20We%20propose%20updated%20definitions%20for%20core%20system%20safety%20concepts%0Athat%20encompass%20these%20additional%20considerations%20as%20a%20starting%20point%20for%20evolving%0Asafe-ty%20approaches%20to%20address%20these%20additional%20safety%20challenges.%20These%20results%0Amight%20additionally%20inform%20framing%20safety%20terminology%20for%20other%20autonomous%0Asystem%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16768v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedefining%2520Safety%2520for%2520Autonomous%2520Vehicles%26entry.906535625%3DPhilip%2520Koopman%2520and%2520William%2520Widen%26entry.1292438233%3D%2520%2520Existing%2520definitions%2520and%2520associated%2520conceptual%2520frameworks%2520for%2520computer-based%250Asystem%2520safety%2520should%2520be%2520revisited%2520in%2520light%2520of%2520real-world%2520experiences%2520from%250Adeploying%2520autonomous%2520vehicles.%2520Current%2520terminology%2520used%2520by%2520industry%2520safety%250Astandards%2520emphasizes%2520mitigation%2520of%2520risk%2520from%2520specifically%2520identified%2520hazards%252C%250Aand%2520carries%2520assumptions%2520based%2520on%2520human-supervised%2520vehicle%2520operation.%2520Operation%250Awithout%2520a%2520human%2520driver%2520dramatically%2520increases%2520the%2520scope%2520of%2520safety%2520concerns%252C%250Aespecially%2520due%2520to%2520operation%2520in%2520an%2520open%2520world%2520environment%252C%2520a%2520requirement%2520to%250Aself-enforce%2520operational%2520limits%252C%2520participation%2520in%2520an%2520ad%2520hoc%2520sociotechnical%250Asystem%2520of%2520systems%252C%2520and%2520a%2520requirement%2520to%2520conform%2520to%2520both%2520legal%2520and%2520ethical%250Aconstraints.%2520Existing%2520standards%2520and%2520terminology%2520only%2520partially%2520address%2520these%250Anew%2520challenges.%2520We%2520propose%2520updated%2520definitions%2520for%2520core%2520system%2520safety%2520concepts%250Athat%2520encompass%2520these%2520additional%2520considerations%2520as%2520a%2520starting%2520point%2520for%2520evolving%250Asafe-ty%2520approaches%2520to%2520address%2520these%2520additional%2520safety%2520challenges.%2520These%2520results%250Amight%2520additionally%2520inform%2520framing%2520safety%2520terminology%2520for%2520other%2520autonomous%250Asystem%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16768v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redefining%20Safety%20for%20Autonomous%20Vehicles&entry.906535625=Philip%20Koopman%20and%20William%20Widen&entry.1292438233=%20%20Existing%20definitions%20and%20associated%20conceptual%20frameworks%20for%20computer-based%0Asystem%20safety%20should%20be%20revisited%20in%20light%20of%20real-world%20experiences%20from%0Adeploying%20autonomous%20vehicles.%20Current%20terminology%20used%20by%20industry%20safety%0Astandards%20emphasizes%20mitigation%20of%20risk%20from%20specifically%20identified%20hazards%2C%0Aand%20carries%20assumptions%20based%20on%20human-supervised%20vehicle%20operation.%20Operation%0Awithout%20a%20human%20driver%20dramatically%20increases%20the%20scope%20of%20safety%20concerns%2C%0Aespecially%20due%20to%20operation%20in%20an%20open%20world%20environment%2C%20a%20requirement%20to%0Aself-enforce%20operational%20limits%2C%20participation%20in%20an%20ad%20hoc%20sociotechnical%0Asystem%20of%20systems%2C%20and%20a%20requirement%20to%20conform%20to%20both%20legal%20and%20ethical%0Aconstraints.%20Existing%20standards%20and%20terminology%20only%20partially%20address%20these%0Anew%20challenges.%20We%20propose%20updated%20definitions%20for%20core%20system%20safety%20concepts%0Athat%20encompass%20these%20additional%20considerations%20as%20a%20starting%20point%20for%20evolving%0Asafe-ty%20approaches%20to%20address%20these%20additional%20safety%20challenges.%20These%20results%0Amight%20additionally%20inform%20framing%20safety%20terminology%20for%20other%20autonomous%0Asystem%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16768v3&entry.124074799=Read"},
{"title": "Controllable Continual Test-Time Adaptation", "author": "Ziqi Shi and Fan Lyu and Ye Liu and Fanhua Shang and Fuyuan Hu and Wei Feng and Zhang Zhang and Liang Wang", "abstract": "  Continual Test-Time Adaptation (CTTA) is an emerging and challenging task\nwhere a model trained in a source domain must adapt to continuously changing\nconditions during testing, without access to the original source data. CTTA is\nprone to error accumulation due to uncontrollable domain shifts, leading to\nblurred decision boundaries between categories. Existing CTTA methods primarily\nfocus on suppressing domain shifts, which proves inadequate during the\nunsupervised test phase. In contrast, we introduce a novel approach that guides\nrather than suppresses these shifts. Specifically, we propose\n$\\textbf{C}$ontrollable $\\textbf{Co}$ntinual $\\textbf{T}$est-$\\textbf{T}$ime\n$\\textbf{A}$daptation (C-CoTTA), which explicitly prevents any single category\nfrom encroaching on others, thereby mitigating the mutual influence between\ncategories caused by uncontrollable shifts. Moreover, our method reduces the\nsensitivity of model to domain transformations, thereby minimizing the\nmagnitude of category shifts. Extensive quantitative experiments demonstrate\nthe effectiveness of our method, while qualitative analyses, such as t-SNE\nplots, confirm the theoretical validity of our approach.\n", "link": "http://arxiv.org/abs/2405.14602v3", "date": "2024-05-28", "relevancy": 1.9647, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.509}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4955}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controllable%20Continual%20Test-Time%20Adaptation&body=Title%3A%20Controllable%20Continual%20Test-Time%20Adaptation%0AAuthor%3A%20Ziqi%20Shi%20and%20Fan%20Lyu%20and%20Ye%20Liu%20and%20Fanhua%20Shang%20and%20Fuyuan%20Hu%20and%20Wei%20Feng%20and%20Zhang%20Zhang%20and%20Liang%20Wang%0AAbstract%3A%20%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20is%20an%20emerging%20and%20challenging%20task%0Awhere%20a%20model%20trained%20in%20a%20source%20domain%20must%20adapt%20to%20continuously%20changing%0Aconditions%20during%20testing%2C%20without%20access%20to%20the%20original%20source%20data.%20CTTA%20is%0Aprone%20to%20error%20accumulation%20due%20to%20uncontrollable%20domain%20shifts%2C%20leading%20to%0Ablurred%20decision%20boundaries%20between%20categories.%20Existing%20CTTA%20methods%20primarily%0Afocus%20on%20suppressing%20domain%20shifts%2C%20which%20proves%20inadequate%20during%20the%0Aunsupervised%20test%20phase.%20In%20contrast%2C%20we%20introduce%20a%20novel%20approach%20that%20guides%0Arather%20than%20suppresses%20these%20shifts.%20Specifically%2C%20we%20propose%0A%24%5Ctextbf%7BC%7D%24ontrollable%20%24%5Ctextbf%7BCo%7D%24ntinual%20%24%5Ctextbf%7BT%7D%24est-%24%5Ctextbf%7BT%7D%24ime%0A%24%5Ctextbf%7BA%7D%24daptation%20%28C-CoTTA%29%2C%20which%20explicitly%20prevents%20any%20single%20category%0Afrom%20encroaching%20on%20others%2C%20thereby%20mitigating%20the%20mutual%20influence%20between%0Acategories%20caused%20by%20uncontrollable%20shifts.%20Moreover%2C%20our%20method%20reduces%20the%0Asensitivity%20of%20model%20to%20domain%20transformations%2C%20thereby%20minimizing%20the%0Amagnitude%20of%20category%20shifts.%20Extensive%20quantitative%20experiments%20demonstrate%0Athe%20effectiveness%20of%20our%20method%2C%20while%20qualitative%20analyses%2C%20such%20as%20t-SNE%0Aplots%2C%20confirm%20the%20theoretical%20validity%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14602v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControllable%2520Continual%2520Test-Time%2520Adaptation%26entry.906535625%3DZiqi%2520Shi%2520and%2520Fan%2520Lyu%2520and%2520Ye%2520Liu%2520and%2520Fanhua%2520Shang%2520and%2520Fuyuan%2520Hu%2520and%2520Wei%2520Feng%2520and%2520Zhang%2520Zhang%2520and%2520Liang%2520Wang%26entry.1292438233%3D%2520%2520Continual%2520Test-Time%2520Adaptation%2520%2528CTTA%2529%2520is%2520an%2520emerging%2520and%2520challenging%2520task%250Awhere%2520a%2520model%2520trained%2520in%2520a%2520source%2520domain%2520must%2520adapt%2520to%2520continuously%2520changing%250Aconditions%2520during%2520testing%252C%2520without%2520access%2520to%2520the%2520original%2520source%2520data.%2520CTTA%2520is%250Aprone%2520to%2520error%2520accumulation%2520due%2520to%2520uncontrollable%2520domain%2520shifts%252C%2520leading%2520to%250Ablurred%2520decision%2520boundaries%2520between%2520categories.%2520Existing%2520CTTA%2520methods%2520primarily%250Afocus%2520on%2520suppressing%2520domain%2520shifts%252C%2520which%2520proves%2520inadequate%2520during%2520the%250Aunsupervised%2520test%2520phase.%2520In%2520contrast%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520that%2520guides%250Arather%2520than%2520suppresses%2520these%2520shifts.%2520Specifically%252C%2520we%2520propose%250A%2524%255Ctextbf%257BC%257D%2524ontrollable%2520%2524%255Ctextbf%257BCo%257D%2524ntinual%2520%2524%255Ctextbf%257BT%257D%2524est-%2524%255Ctextbf%257BT%257D%2524ime%250A%2524%255Ctextbf%257BA%257D%2524daptation%2520%2528C-CoTTA%2529%252C%2520which%2520explicitly%2520prevents%2520any%2520single%2520category%250Afrom%2520encroaching%2520on%2520others%252C%2520thereby%2520mitigating%2520the%2520mutual%2520influence%2520between%250Acategories%2520caused%2520by%2520uncontrollable%2520shifts.%2520Moreover%252C%2520our%2520method%2520reduces%2520the%250Asensitivity%2520of%2520model%2520to%2520domain%2520transformations%252C%2520thereby%2520minimizing%2520the%250Amagnitude%2520of%2520category%2520shifts.%2520Extensive%2520quantitative%2520experiments%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520method%252C%2520while%2520qualitative%2520analyses%252C%2520such%2520as%2520t-SNE%250Aplots%252C%2520confirm%2520the%2520theoretical%2520validity%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14602v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20Continual%20Test-Time%20Adaptation&entry.906535625=Ziqi%20Shi%20and%20Fan%20Lyu%20and%20Ye%20Liu%20and%20Fanhua%20Shang%20and%20Fuyuan%20Hu%20and%20Wei%20Feng%20and%20Zhang%20Zhang%20and%20Liang%20Wang&entry.1292438233=%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20is%20an%20emerging%20and%20challenging%20task%0Awhere%20a%20model%20trained%20in%20a%20source%20domain%20must%20adapt%20to%20continuously%20changing%0Aconditions%20during%20testing%2C%20without%20access%20to%20the%20original%20source%20data.%20CTTA%20is%0Aprone%20to%20error%20accumulation%20due%20to%20uncontrollable%20domain%20shifts%2C%20leading%20to%0Ablurred%20decision%20boundaries%20between%20categories.%20Existing%20CTTA%20methods%20primarily%0Afocus%20on%20suppressing%20domain%20shifts%2C%20which%20proves%20inadequate%20during%20the%0Aunsupervised%20test%20phase.%20In%20contrast%2C%20we%20introduce%20a%20novel%20approach%20that%20guides%0Arather%20than%20suppresses%20these%20shifts.%20Specifically%2C%20we%20propose%0A%24%5Ctextbf%7BC%7D%24ontrollable%20%24%5Ctextbf%7BCo%7D%24ntinual%20%24%5Ctextbf%7BT%7D%24est-%24%5Ctextbf%7BT%7D%24ime%0A%24%5Ctextbf%7BA%7D%24daptation%20%28C-CoTTA%29%2C%20which%20explicitly%20prevents%20any%20single%20category%0Afrom%20encroaching%20on%20others%2C%20thereby%20mitigating%20the%20mutual%20influence%20between%0Acategories%20caused%20by%20uncontrollable%20shifts.%20Moreover%2C%20our%20method%20reduces%20the%0Asensitivity%20of%20model%20to%20domain%20transformations%2C%20thereby%20minimizing%20the%0Amagnitude%20of%20category%20shifts.%20Extensive%20quantitative%20experiments%20demonstrate%0Athe%20effectiveness%20of%20our%20method%2C%20while%20qualitative%20analyses%2C%20such%20as%20t-SNE%0Aplots%2C%20confirm%20the%20theoretical%20validity%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14602v3&entry.124074799=Read"},
{"title": "AlignIQL: Policy Alignment in Implicit Q-Learning through Constrained\n  Optimization", "author": "Longxiang He and Li Shen and Junbo Tan and Xueqian Wang", "abstract": "  Implicit Q-learning (IQL) serves as a strong baseline for offline RL, which\nlearns the value function using only dataset actions through quantile\nregression. However, it is unclear how to recover the implicit policy from the\nlearned implicit Q-function and why IQL can utilize weighted regression for\npolicy extraction. IDQL reinterprets IQL as an actor-critic method and gets\nweights of implicit policy, however, this weight only holds for the optimal\nvalue function. In this work, we introduce a different way to solve the\nimplicit policy-finding problem (IPF) by formulating this problem as an\noptimization problem. Based on this optimization problem, we further propose\ntwo practical algorithms AlignIQL and AlignIQL-hard, which inherit the\nadvantages of decoupling actor from critic in IQL and provide insights into why\nIQL can use weighted regression for policy extraction. Compared with IQL and\nIDQL, we find our method keeps the simplicity of IQL and solves the implicit\npolicy-finding problem. Experimental results on D4RL datasets show that our\nmethod achieves competitive or superior results compared with other SOTA\noffline RL methods. Especially in complex sparse reward tasks like Antmaze and\nAdroit, our method outperforms IQL and IDQL by a significant margin.\n", "link": "http://arxiv.org/abs/2405.18187v1", "date": "2024-05-28", "relevancy": 1.4364, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.504}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4552}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignIQL%3A%20Policy%20Alignment%20in%20Implicit%20Q-Learning%20through%20Constrained%0A%20%20Optimization&body=Title%3A%20AlignIQL%3A%20Policy%20Alignment%20in%20Implicit%20Q-Learning%20through%20Constrained%0A%20%20Optimization%0AAuthor%3A%20Longxiang%20He%20and%20Li%20Shen%20and%20Junbo%20Tan%20and%20Xueqian%20Wang%0AAbstract%3A%20%20%20Implicit%20Q-learning%20%28IQL%29%20serves%20as%20a%20strong%20baseline%20for%20offline%20RL%2C%20which%0Alearns%20the%20value%20function%20using%20only%20dataset%20actions%20through%20quantile%0Aregression.%20However%2C%20it%20is%20unclear%20how%20to%20recover%20the%20implicit%20policy%20from%20the%0Alearned%20implicit%20Q-function%20and%20why%20IQL%20can%20utilize%20weighted%20regression%20for%0Apolicy%20extraction.%20IDQL%20reinterprets%20IQL%20as%20an%20actor-critic%20method%20and%20gets%0Aweights%20of%20implicit%20policy%2C%20however%2C%20this%20weight%20only%20holds%20for%20the%20optimal%0Avalue%20function.%20In%20this%20work%2C%20we%20introduce%20a%20different%20way%20to%20solve%20the%0Aimplicit%20policy-finding%20problem%20%28IPF%29%20by%20formulating%20this%20problem%20as%20an%0Aoptimization%20problem.%20Based%20on%20this%20optimization%20problem%2C%20we%20further%20propose%0Atwo%20practical%20algorithms%20AlignIQL%20and%20AlignIQL-hard%2C%20which%20inherit%20the%0Aadvantages%20of%20decoupling%20actor%20from%20critic%20in%20IQL%20and%20provide%20insights%20into%20why%0AIQL%20can%20use%20weighted%20regression%20for%20policy%20extraction.%20Compared%20with%20IQL%20and%0AIDQL%2C%20we%20find%20our%20method%20keeps%20the%20simplicity%20of%20IQL%20and%20solves%20the%20implicit%0Apolicy-finding%20problem.%20Experimental%20results%20on%20D4RL%20datasets%20show%20that%20our%0Amethod%20achieves%20competitive%20or%20superior%20results%20compared%20with%20other%20SOTA%0Aoffline%20RL%20methods.%20Especially%20in%20complex%20sparse%20reward%20tasks%20like%20Antmaze%20and%0AAdroit%2C%20our%20method%20outperforms%20IQL%20and%20IDQL%20by%20a%20significant%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignIQL%253A%2520Policy%2520Alignment%2520in%2520Implicit%2520Q-Learning%2520through%2520Constrained%250A%2520%2520Optimization%26entry.906535625%3DLongxiang%2520He%2520and%2520Li%2520Shen%2520and%2520Junbo%2520Tan%2520and%2520Xueqian%2520Wang%26entry.1292438233%3D%2520%2520Implicit%2520Q-learning%2520%2528IQL%2529%2520serves%2520as%2520a%2520strong%2520baseline%2520for%2520offline%2520RL%252C%2520which%250Alearns%2520the%2520value%2520function%2520using%2520only%2520dataset%2520actions%2520through%2520quantile%250Aregression.%2520However%252C%2520it%2520is%2520unclear%2520how%2520to%2520recover%2520the%2520implicit%2520policy%2520from%2520the%250Alearned%2520implicit%2520Q-function%2520and%2520why%2520IQL%2520can%2520utilize%2520weighted%2520regression%2520for%250Apolicy%2520extraction.%2520IDQL%2520reinterprets%2520IQL%2520as%2520an%2520actor-critic%2520method%2520and%2520gets%250Aweights%2520of%2520implicit%2520policy%252C%2520however%252C%2520this%2520weight%2520only%2520holds%2520for%2520the%2520optimal%250Avalue%2520function.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520different%2520way%2520to%2520solve%2520the%250Aimplicit%2520policy-finding%2520problem%2520%2528IPF%2529%2520by%2520formulating%2520this%2520problem%2520as%2520an%250Aoptimization%2520problem.%2520Based%2520on%2520this%2520optimization%2520problem%252C%2520we%2520further%2520propose%250Atwo%2520practical%2520algorithms%2520AlignIQL%2520and%2520AlignIQL-hard%252C%2520which%2520inherit%2520the%250Aadvantages%2520of%2520decoupling%2520actor%2520from%2520critic%2520in%2520IQL%2520and%2520provide%2520insights%2520into%2520why%250AIQL%2520can%2520use%2520weighted%2520regression%2520for%2520policy%2520extraction.%2520Compared%2520with%2520IQL%2520and%250AIDQL%252C%2520we%2520find%2520our%2520method%2520keeps%2520the%2520simplicity%2520of%2520IQL%2520and%2520solves%2520the%2520implicit%250Apolicy-finding%2520problem.%2520Experimental%2520results%2520on%2520D4RL%2520datasets%2520show%2520that%2520our%250Amethod%2520achieves%2520competitive%2520or%2520superior%2520results%2520compared%2520with%2520other%2520SOTA%250Aoffline%2520RL%2520methods.%2520Especially%2520in%2520complex%2520sparse%2520reward%2520tasks%2520like%2520Antmaze%2520and%250AAdroit%252C%2520our%2520method%2520outperforms%2520IQL%2520and%2520IDQL%2520by%2520a%2520significant%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignIQL%3A%20Policy%20Alignment%20in%20Implicit%20Q-Learning%20through%20Constrained%0A%20%20Optimization&entry.906535625=Longxiang%20He%20and%20Li%20Shen%20and%20Junbo%20Tan%20and%20Xueqian%20Wang&entry.1292438233=%20%20Implicit%20Q-learning%20%28IQL%29%20serves%20as%20a%20strong%20baseline%20for%20offline%20RL%2C%20which%0Alearns%20the%20value%20function%20using%20only%20dataset%20actions%20through%20quantile%0Aregression.%20However%2C%20it%20is%20unclear%20how%20to%20recover%20the%20implicit%20policy%20from%20the%0Alearned%20implicit%20Q-function%20and%20why%20IQL%20can%20utilize%20weighted%20regression%20for%0Apolicy%20extraction.%20IDQL%20reinterprets%20IQL%20as%20an%20actor-critic%20method%20and%20gets%0Aweights%20of%20implicit%20policy%2C%20however%2C%20this%20weight%20only%20holds%20for%20the%20optimal%0Avalue%20function.%20In%20this%20work%2C%20we%20introduce%20a%20different%20way%20to%20solve%20the%0Aimplicit%20policy-finding%20problem%20%28IPF%29%20by%20formulating%20this%20problem%20as%20an%0Aoptimization%20problem.%20Based%20on%20this%20optimization%20problem%2C%20we%20further%20propose%0Atwo%20practical%20algorithms%20AlignIQL%20and%20AlignIQL-hard%2C%20which%20inherit%20the%0Aadvantages%20of%20decoupling%20actor%20from%20critic%20in%20IQL%20and%20provide%20insights%20into%20why%0AIQL%20can%20use%20weighted%20regression%20for%20policy%20extraction.%20Compared%20with%20IQL%20and%0AIDQL%2C%20we%20find%20our%20method%20keeps%20the%20simplicity%20of%20IQL%20and%20solves%20the%20implicit%0Apolicy-finding%20problem.%20Experimental%20results%20on%20D4RL%20datasets%20show%20that%20our%0Amethod%20achieves%20competitive%20or%20superior%20results%20compared%20with%20other%20SOTA%0Aoffline%20RL%20methods.%20Especially%20in%20complex%20sparse%20reward%20tasks%20like%20Antmaze%20and%0AAdroit%2C%20our%20method%20outperforms%20IQL%20and%20IDQL%20by%20a%20significant%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18187v1&entry.124074799=Read"},
{"title": "Warm Start Marginal Likelihood Optimisation for Iterative Gaussian\n  Processes", "author": "Jihao Andreas Lin and Shreyas Padhy and Bruno Mlodozeniec and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "abstract": "  Gaussian processes are a versatile probabilistic machine learning model whose\neffectiveness often depends on good hyperparameters, which are typically\nlearned by maximising the marginal likelihood. In this work, we consider\niterative methods, which use iterative linear system solvers to approximate\nmarginal likelihood gradients up to a specified numerical precision, allowing a\ntrade-off between compute time and accuracy of a solution. We introduce a\nthree-level hierarchy of marginal likelihood optimisation for iterative\nGaussian processes, and identify that the computational costs are dominated by\nsolving sequential batches of large positive-definite systems of linear\nequations. We then propose to amortise computations by reusing solutions of\nlinear system solvers as initialisations in the next step, providing a\n$\\textit{warm start}$. Finally, we discuss the necessary conditions and\nquantify the consequences of warm starts and demonstrate their effectiveness on\nregression tasks, where warm starts achieve the same results as the\nconventional procedure while providing up to a $16 \\times$ average speed-up\namong datasets.\n", "link": "http://arxiv.org/abs/2405.18328v1", "date": "2024-05-28", "relevancy": 1.802, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4612}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4434}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Warm%20Start%20Marginal%20Likelihood%20Optimisation%20for%20Iterative%20Gaussian%0A%20%20Processes&body=Title%3A%20Warm%20Start%20Marginal%20Likelihood%20Optimisation%20for%20Iterative%20Gaussian%0A%20%20Processes%0AAuthor%3A%20Jihao%20Andreas%20Lin%20and%20Shreyas%20Padhy%20and%20Bruno%20Mlodozeniec%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20%20%20Gaussian%20processes%20are%20a%20versatile%20probabilistic%20machine%20learning%20model%20whose%0Aeffectiveness%20often%20depends%20on%20good%20hyperparameters%2C%20which%20are%20typically%0Alearned%20by%20maximising%20the%20marginal%20likelihood.%20In%20this%20work%2C%20we%20consider%0Aiterative%20methods%2C%20which%20use%20iterative%20linear%20system%20solvers%20to%20approximate%0Amarginal%20likelihood%20gradients%20up%20to%20a%20specified%20numerical%20precision%2C%20allowing%20a%0Atrade-off%20between%20compute%20time%20and%20accuracy%20of%20a%20solution.%20We%20introduce%20a%0Athree-level%20hierarchy%20of%20marginal%20likelihood%20optimisation%20for%20iterative%0AGaussian%20processes%2C%20and%20identify%20that%20the%20computational%20costs%20are%20dominated%20by%0Asolving%20sequential%20batches%20of%20large%20positive-definite%20systems%20of%20linear%0Aequations.%20We%20then%20propose%20to%20amortise%20computations%20by%20reusing%20solutions%20of%0Alinear%20system%20solvers%20as%20initialisations%20in%20the%20next%20step%2C%20providing%20a%0A%24%5Ctextit%7Bwarm%20start%7D%24.%20Finally%2C%20we%20discuss%20the%20necessary%20conditions%20and%0Aquantify%20the%20consequences%20of%20warm%20starts%20and%20demonstrate%20their%20effectiveness%20on%0Aregression%20tasks%2C%20where%20warm%20starts%20achieve%20the%20same%20results%20as%20the%0Aconventional%20procedure%20while%20providing%20up%20to%20a%20%2416%20%5Ctimes%24%20average%20speed-up%0Aamong%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWarm%2520Start%2520Marginal%2520Likelihood%2520Optimisation%2520for%2520Iterative%2520Gaussian%250A%2520%2520Processes%26entry.906535625%3DJihao%2520Andreas%2520Lin%2520and%2520Shreyas%2520Padhy%2520and%2520Bruno%2520Mlodozeniec%2520and%2520Jos%25C3%25A9%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520are%2520a%2520versatile%2520probabilistic%2520machine%2520learning%2520model%2520whose%250Aeffectiveness%2520often%2520depends%2520on%2520good%2520hyperparameters%252C%2520which%2520are%2520typically%250Alearned%2520by%2520maximising%2520the%2520marginal%2520likelihood.%2520In%2520this%2520work%252C%2520we%2520consider%250Aiterative%2520methods%252C%2520which%2520use%2520iterative%2520linear%2520system%2520solvers%2520to%2520approximate%250Amarginal%2520likelihood%2520gradients%2520up%2520to%2520a%2520specified%2520numerical%2520precision%252C%2520allowing%2520a%250Atrade-off%2520between%2520compute%2520time%2520and%2520accuracy%2520of%2520a%2520solution.%2520We%2520introduce%2520a%250Athree-level%2520hierarchy%2520of%2520marginal%2520likelihood%2520optimisation%2520for%2520iterative%250AGaussian%2520processes%252C%2520and%2520identify%2520that%2520the%2520computational%2520costs%2520are%2520dominated%2520by%250Asolving%2520sequential%2520batches%2520of%2520large%2520positive-definite%2520systems%2520of%2520linear%250Aequations.%2520We%2520then%2520propose%2520to%2520amortise%2520computations%2520by%2520reusing%2520solutions%2520of%250Alinear%2520system%2520solvers%2520as%2520initialisations%2520in%2520the%2520next%2520step%252C%2520providing%2520a%250A%2524%255Ctextit%257Bwarm%2520start%257D%2524.%2520Finally%252C%2520we%2520discuss%2520the%2520necessary%2520conditions%2520and%250Aquantify%2520the%2520consequences%2520of%2520warm%2520starts%2520and%2520demonstrate%2520their%2520effectiveness%2520on%250Aregression%2520tasks%252C%2520where%2520warm%2520starts%2520achieve%2520the%2520same%2520results%2520as%2520the%250Aconventional%2520procedure%2520while%2520providing%2520up%2520to%2520a%2520%252416%2520%255Ctimes%2524%2520average%2520speed-up%250Aamong%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Warm%20Start%20Marginal%20Likelihood%20Optimisation%20for%20Iterative%20Gaussian%0A%20%20Processes&entry.906535625=Jihao%20Andreas%20Lin%20and%20Shreyas%20Padhy%20and%20Bruno%20Mlodozeniec%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=%20%20Gaussian%20processes%20are%20a%20versatile%20probabilistic%20machine%20learning%20model%20whose%0Aeffectiveness%20often%20depends%20on%20good%20hyperparameters%2C%20which%20are%20typically%0Alearned%20by%20maximising%20the%20marginal%20likelihood.%20In%20this%20work%2C%20we%20consider%0Aiterative%20methods%2C%20which%20use%20iterative%20linear%20system%20solvers%20to%20approximate%0Amarginal%20likelihood%20gradients%20up%20to%20a%20specified%20numerical%20precision%2C%20allowing%20a%0Atrade-off%20between%20compute%20time%20and%20accuracy%20of%20a%20solution.%20We%20introduce%20a%0Athree-level%20hierarchy%20of%20marginal%20likelihood%20optimisation%20for%20iterative%0AGaussian%20processes%2C%20and%20identify%20that%20the%20computational%20costs%20are%20dominated%20by%0Asolving%20sequential%20batches%20of%20large%20positive-definite%20systems%20of%20linear%0Aequations.%20We%20then%20propose%20to%20amortise%20computations%20by%20reusing%20solutions%20of%0Alinear%20system%20solvers%20as%20initialisations%20in%20the%20next%20step%2C%20providing%20a%0A%24%5Ctextit%7Bwarm%20start%7D%24.%20Finally%2C%20we%20discuss%20the%20necessary%20conditions%20and%0Aquantify%20the%20consequences%20of%20warm%20starts%20and%20demonstrate%20their%20effectiveness%20on%0Aregression%20tasks%2C%20where%20warm%20starts%20achieve%20the%20same%20results%20as%20the%0Aconventional%20procedure%20while%20providing%20up%20to%20a%20%2416%20%5Ctimes%24%20average%20speed-up%0Aamong%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18328v1&entry.124074799=Read"},
{"title": "Highway Reinforcement Learning", "author": "Yuhui Wang and Miroslav Strupl and Francesco Faccio and Qingyuan Wu and Haozhe Liu and Micha\u0142 Grudzie\u0144 and Xiaoyang Tan and J\u00fcrgen Schmidhuber", "abstract": "  Learning from multi-step off-policy data collected by a set of policies is a\ncore problem of reinforcement learning (RL). Approaches based on importance\nsampling (IS) often suffer from large variances due to products of IS ratios.\nTypical IS-free methods, such as $n$-step Q-learning, look ahead for $n$ time\nsteps along the trajectory of actions (where $n$ is called the lookahead depth)\nand utilize off-policy data directly without any additional adjustment. They\nwork well for proper choices of $n$. We show, however, that such IS-free\nmethods underestimate the optimal value function (VF), especially for large\n$n$, restricting their capacity to efficiently utilize information from distant\nfuture time steps. To overcome this problem, we introduce a novel, IS-free,\nmulti-step off-policy method that avoids the underestimation issue and\nconverges to the optimal VF. At its core lies a simple but non-trivial\n\\emph{highway gate}, which controls the information flow from the distant\nfuture by comparing it to a threshold. The highway gate guarantees convergence\nto the optimal VF for arbitrary $n$ and arbitrary behavioral policies. It gives\nrise to a novel family of off-policy RL algorithms that safely learn even when\n$n$ is very large, facilitating rapid credit assignment from the far future to\nthe past. On tasks with greatly delayed rewards, including video games where\nthe reward is given only at the end of the game, our new methods outperform\nmany existing multi-step off-policy algorithms.\n", "link": "http://arxiv.org/abs/2405.18289v1", "date": "2024-05-28", "relevancy": 1.8374, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4818}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4684}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Highway%20Reinforcement%20Learning&body=Title%3A%20Highway%20Reinforcement%20Learning%0AAuthor%3A%20Yuhui%20Wang%20and%20Miroslav%20Strupl%20and%20Francesco%20Faccio%20and%20Qingyuan%20Wu%20and%20Haozhe%20Liu%20and%20Micha%C5%82%20Grudzie%C5%84%20and%20Xiaoyang%20Tan%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Learning%20from%20multi-step%20off-policy%20data%20collected%20by%20a%20set%20of%20policies%20is%20a%0Acore%20problem%20of%20reinforcement%20learning%20%28RL%29.%20Approaches%20based%20on%20importance%0Asampling%20%28IS%29%20often%20suffer%20from%20large%20variances%20due%20to%20products%20of%20IS%20ratios.%0ATypical%20IS-free%20methods%2C%20such%20as%20%24n%24-step%20Q-learning%2C%20look%20ahead%20for%20%24n%24%20time%0Asteps%20along%20the%20trajectory%20of%20actions%20%28where%20%24n%24%20is%20called%20the%20lookahead%20depth%29%0Aand%20utilize%20off-policy%20data%20directly%20without%20any%20additional%20adjustment.%20They%0Awork%20well%20for%20proper%20choices%20of%20%24n%24.%20We%20show%2C%20however%2C%20that%20such%20IS-free%0Amethods%20underestimate%20the%20optimal%20value%20function%20%28VF%29%2C%20especially%20for%20large%0A%24n%24%2C%20restricting%20their%20capacity%20to%20efficiently%20utilize%20information%20from%20distant%0Afuture%20time%20steps.%20To%20overcome%20this%20problem%2C%20we%20introduce%20a%20novel%2C%20IS-free%2C%0Amulti-step%20off-policy%20method%20that%20avoids%20the%20underestimation%20issue%20and%0Aconverges%20to%20the%20optimal%20VF.%20At%20its%20core%20lies%20a%20simple%20but%20non-trivial%0A%5Cemph%7Bhighway%20gate%7D%2C%20which%20controls%20the%20information%20flow%20from%20the%20distant%0Afuture%20by%20comparing%20it%20to%20a%20threshold.%20The%20highway%20gate%20guarantees%20convergence%0Ato%20the%20optimal%20VF%20for%20arbitrary%20%24n%24%20and%20arbitrary%20behavioral%20policies.%20It%20gives%0Arise%20to%20a%20novel%20family%20of%20off-policy%20RL%20algorithms%20that%20safely%20learn%20even%20when%0A%24n%24%20is%20very%20large%2C%20facilitating%20rapid%20credit%20assignment%20from%20the%20far%20future%20to%0Athe%20past.%20On%20tasks%20with%20greatly%20delayed%20rewards%2C%20including%20video%20games%20where%0Athe%20reward%20is%20given%20only%20at%20the%20end%20of%20the%20game%2C%20our%20new%20methods%20outperform%0Amany%20existing%20multi-step%20off-policy%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHighway%2520Reinforcement%2520Learning%26entry.906535625%3DYuhui%2520Wang%2520and%2520Miroslav%2520Strupl%2520and%2520Francesco%2520Faccio%2520and%2520Qingyuan%2520Wu%2520and%2520Haozhe%2520Liu%2520and%2520Micha%25C5%2582%2520Grudzie%25C5%2584%2520and%2520Xiaoyang%2520Tan%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%26entry.1292438233%3D%2520%2520Learning%2520from%2520multi-step%2520off-policy%2520data%2520collected%2520by%2520a%2520set%2520of%2520policies%2520is%2520a%250Acore%2520problem%2520of%2520reinforcement%2520learning%2520%2528RL%2529.%2520Approaches%2520based%2520on%2520importance%250Asampling%2520%2528IS%2529%2520often%2520suffer%2520from%2520large%2520variances%2520due%2520to%2520products%2520of%2520IS%2520ratios.%250ATypical%2520IS-free%2520methods%252C%2520such%2520as%2520%2524n%2524-step%2520Q-learning%252C%2520look%2520ahead%2520for%2520%2524n%2524%2520time%250Asteps%2520along%2520the%2520trajectory%2520of%2520actions%2520%2528where%2520%2524n%2524%2520is%2520called%2520the%2520lookahead%2520depth%2529%250Aand%2520utilize%2520off-policy%2520data%2520directly%2520without%2520any%2520additional%2520adjustment.%2520They%250Awork%2520well%2520for%2520proper%2520choices%2520of%2520%2524n%2524.%2520We%2520show%252C%2520however%252C%2520that%2520such%2520IS-free%250Amethods%2520underestimate%2520the%2520optimal%2520value%2520function%2520%2528VF%2529%252C%2520especially%2520for%2520large%250A%2524n%2524%252C%2520restricting%2520their%2520capacity%2520to%2520efficiently%2520utilize%2520information%2520from%2520distant%250Afuture%2520time%2520steps.%2520To%2520overcome%2520this%2520problem%252C%2520we%2520introduce%2520a%2520novel%252C%2520IS-free%252C%250Amulti-step%2520off-policy%2520method%2520that%2520avoids%2520the%2520underestimation%2520issue%2520and%250Aconverges%2520to%2520the%2520optimal%2520VF.%2520At%2520its%2520core%2520lies%2520a%2520simple%2520but%2520non-trivial%250A%255Cemph%257Bhighway%2520gate%257D%252C%2520which%2520controls%2520the%2520information%2520flow%2520from%2520the%2520distant%250Afuture%2520by%2520comparing%2520it%2520to%2520a%2520threshold.%2520The%2520highway%2520gate%2520guarantees%2520convergence%250Ato%2520the%2520optimal%2520VF%2520for%2520arbitrary%2520%2524n%2524%2520and%2520arbitrary%2520behavioral%2520policies.%2520It%2520gives%250Arise%2520to%2520a%2520novel%2520family%2520of%2520off-policy%2520RL%2520algorithms%2520that%2520safely%2520learn%2520even%2520when%250A%2524n%2524%2520is%2520very%2520large%252C%2520facilitating%2520rapid%2520credit%2520assignment%2520from%2520the%2520far%2520future%2520to%250Athe%2520past.%2520On%2520tasks%2520with%2520greatly%2520delayed%2520rewards%252C%2520including%2520video%2520games%2520where%250Athe%2520reward%2520is%2520given%2520only%2520at%2520the%2520end%2520of%2520the%2520game%252C%2520our%2520new%2520methods%2520outperform%250Amany%2520existing%2520multi-step%2520off-policy%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Highway%20Reinforcement%20Learning&entry.906535625=Yuhui%20Wang%20and%20Miroslav%20Strupl%20and%20Francesco%20Faccio%20and%20Qingyuan%20Wu%20and%20Haozhe%20Liu%20and%20Micha%C5%82%20Grudzie%C5%84%20and%20Xiaoyang%20Tan%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Learning%20from%20multi-step%20off-policy%20data%20collected%20by%20a%20set%20of%20policies%20is%20a%0Acore%20problem%20of%20reinforcement%20learning%20%28RL%29.%20Approaches%20based%20on%20importance%0Asampling%20%28IS%29%20often%20suffer%20from%20large%20variances%20due%20to%20products%20of%20IS%20ratios.%0ATypical%20IS-free%20methods%2C%20such%20as%20%24n%24-step%20Q-learning%2C%20look%20ahead%20for%20%24n%24%20time%0Asteps%20along%20the%20trajectory%20of%20actions%20%28where%20%24n%24%20is%20called%20the%20lookahead%20depth%29%0Aand%20utilize%20off-policy%20data%20directly%20without%20any%20additional%20adjustment.%20They%0Awork%20well%20for%20proper%20choices%20of%20%24n%24.%20We%20show%2C%20however%2C%20that%20such%20IS-free%0Amethods%20underestimate%20the%20optimal%20value%20function%20%28VF%29%2C%20especially%20for%20large%0A%24n%24%2C%20restricting%20their%20capacity%20to%20efficiently%20utilize%20information%20from%20distant%0Afuture%20time%20steps.%20To%20overcome%20this%20problem%2C%20we%20introduce%20a%20novel%2C%20IS-free%2C%0Amulti-step%20off-policy%20method%20that%20avoids%20the%20underestimation%20issue%20and%0Aconverges%20to%20the%20optimal%20VF.%20At%20its%20core%20lies%20a%20simple%20but%20non-trivial%0A%5Cemph%7Bhighway%20gate%7D%2C%20which%20controls%20the%20information%20flow%20from%20the%20distant%0Afuture%20by%20comparing%20it%20to%20a%20threshold.%20The%20highway%20gate%20guarantees%20convergence%0Ato%20the%20optimal%20VF%20for%20arbitrary%20%24n%24%20and%20arbitrary%20behavioral%20policies.%20It%20gives%0Arise%20to%20a%20novel%20family%20of%20off-policy%20RL%20algorithms%20that%20safely%20learn%20even%20when%0A%24n%24%20is%20very%20large%2C%20facilitating%20rapid%20credit%20assignment%20from%20the%20far%20future%20to%0Athe%20past.%20On%20tasks%20with%20greatly%20delayed%20rewards%2C%20including%20video%20games%20where%0Athe%20reward%20is%20given%20only%20at%20the%20end%20of%20the%20game%2C%20our%20new%20methods%20outperform%0Amany%20existing%20multi-step%20off-policy%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18289v1&entry.124074799=Read"},
{"title": "Sensor-Based Distributionally Robust Control for Safe Robot Navigation\n  in Dynamic Environments", "author": "Kehan Long and Yinzhuang Yi and Zhirui Dai and Sylvia Herbert and Jorge Cort\u00e9s and Nikolay Atanasov", "abstract": "  We introduce a novel method for safe mobile robot navigation in dynamic,\nunknown environments, utilizing onboard sensing to impose safety constraints\nwithout the need for accurate map reconstruction. Traditional methods typically\nrely on detailed map information to synthesize safe stabilizing controls for\nmobile robots, which can be computationally demanding and less effective,\nparticularly in dynamic operational conditions. By leveraging recent advances\nin distributionally robust optimization, we develop a distributionally robust\ncontrol barrier function (DR-CBF) constraint that directly processes range\nsensor data to impose safety constraints. Coupling this with a control Lyapunov\nfunction (CLF) for path tracking, we demonstrate that our CLF-DR-CBF control\nsynthesis method achieves safe, efficient, and robust navigation in uncertain\ndynamic environments. We demonstrate the effectiveness of our approach in\nsimulated and real autonomous robot navigation experiments, marking a\nsubstantial advancement in real-time safety guarantees for mobile robots.\n", "link": "http://arxiv.org/abs/2405.18251v1", "date": "2024-05-28", "relevancy": 1.7785, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5998}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5959}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sensor-Based%20Distributionally%20Robust%20Control%20for%20Safe%20Robot%20Navigation%0A%20%20in%20Dynamic%20Environments&body=Title%3A%20Sensor-Based%20Distributionally%20Robust%20Control%20for%20Safe%20Robot%20Navigation%0A%20%20in%20Dynamic%20Environments%0AAuthor%3A%20Kehan%20Long%20and%20Yinzhuang%20Yi%20and%20Zhirui%20Dai%20and%20Sylvia%20Herbert%20and%20Jorge%20Cort%C3%A9s%20and%20Nikolay%20Atanasov%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20method%20for%20safe%20mobile%20robot%20navigation%20in%20dynamic%2C%0Aunknown%20environments%2C%20utilizing%20onboard%20sensing%20to%20impose%20safety%20constraints%0Awithout%20the%20need%20for%20accurate%20map%20reconstruction.%20Traditional%20methods%20typically%0Arely%20on%20detailed%20map%20information%20to%20synthesize%20safe%20stabilizing%20controls%20for%0Amobile%20robots%2C%20which%20can%20be%20computationally%20demanding%20and%20less%20effective%2C%0Aparticularly%20in%20dynamic%20operational%20conditions.%20By%20leveraging%20recent%20advances%0Ain%20distributionally%20robust%20optimization%2C%20we%20develop%20a%20distributionally%20robust%0Acontrol%20barrier%20function%20%28DR-CBF%29%20constraint%20that%20directly%20processes%20range%0Asensor%20data%20to%20impose%20safety%20constraints.%20Coupling%20this%20with%20a%20control%20Lyapunov%0Afunction%20%28CLF%29%20for%20path%20tracking%2C%20we%20demonstrate%20that%20our%20CLF-DR-CBF%20control%0Asynthesis%20method%20achieves%20safe%2C%20efficient%2C%20and%20robust%20navigation%20in%20uncertain%0Adynamic%20environments.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%0Asimulated%20and%20real%20autonomous%20robot%20navigation%20experiments%2C%20marking%20a%0Asubstantial%20advancement%20in%20real-time%20safety%20guarantees%20for%20mobile%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSensor-Based%2520Distributionally%2520Robust%2520Control%2520for%2520Safe%2520Robot%2520Navigation%250A%2520%2520in%2520Dynamic%2520Environments%26entry.906535625%3DKehan%2520Long%2520and%2520Yinzhuang%2520Yi%2520and%2520Zhirui%2520Dai%2520and%2520Sylvia%2520Herbert%2520and%2520Jorge%2520Cort%25C3%25A9s%2520and%2520Nikolay%2520Atanasov%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520method%2520for%2520safe%2520mobile%2520robot%2520navigation%2520in%2520dynamic%252C%250Aunknown%2520environments%252C%2520utilizing%2520onboard%2520sensing%2520to%2520impose%2520safety%2520constraints%250Awithout%2520the%2520need%2520for%2520accurate%2520map%2520reconstruction.%2520Traditional%2520methods%2520typically%250Arely%2520on%2520detailed%2520map%2520information%2520to%2520synthesize%2520safe%2520stabilizing%2520controls%2520for%250Amobile%2520robots%252C%2520which%2520can%2520be%2520computationally%2520demanding%2520and%2520less%2520effective%252C%250Aparticularly%2520in%2520dynamic%2520operational%2520conditions.%2520By%2520leveraging%2520recent%2520advances%250Ain%2520distributionally%2520robust%2520optimization%252C%2520we%2520develop%2520a%2520distributionally%2520robust%250Acontrol%2520barrier%2520function%2520%2528DR-CBF%2529%2520constraint%2520that%2520directly%2520processes%2520range%250Asensor%2520data%2520to%2520impose%2520safety%2520constraints.%2520Coupling%2520this%2520with%2520a%2520control%2520Lyapunov%250Afunction%2520%2528CLF%2529%2520for%2520path%2520tracking%252C%2520we%2520demonstrate%2520that%2520our%2520CLF-DR-CBF%2520control%250Asynthesis%2520method%2520achieves%2520safe%252C%2520efficient%252C%2520and%2520robust%2520navigation%2520in%2520uncertain%250Adynamic%2520environments.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%250Asimulated%2520and%2520real%2520autonomous%2520robot%2520navigation%2520experiments%252C%2520marking%2520a%250Asubstantial%2520advancement%2520in%2520real-time%2520safety%2520guarantees%2520for%2520mobile%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sensor-Based%20Distributionally%20Robust%20Control%20for%20Safe%20Robot%20Navigation%0A%20%20in%20Dynamic%20Environments&entry.906535625=Kehan%20Long%20and%20Yinzhuang%20Yi%20and%20Zhirui%20Dai%20and%20Sylvia%20Herbert%20and%20Jorge%20Cort%C3%A9s%20and%20Nikolay%20Atanasov&entry.1292438233=%20%20We%20introduce%20a%20novel%20method%20for%20safe%20mobile%20robot%20navigation%20in%20dynamic%2C%0Aunknown%20environments%2C%20utilizing%20onboard%20sensing%20to%20impose%20safety%20constraints%0Awithout%20the%20need%20for%20accurate%20map%20reconstruction.%20Traditional%20methods%20typically%0Arely%20on%20detailed%20map%20information%20to%20synthesize%20safe%20stabilizing%20controls%20for%0Amobile%20robots%2C%20which%20can%20be%20computationally%20demanding%20and%20less%20effective%2C%0Aparticularly%20in%20dynamic%20operational%20conditions.%20By%20leveraging%20recent%20advances%0Ain%20distributionally%20robust%20optimization%2C%20we%20develop%20a%20distributionally%20robust%0Acontrol%20barrier%20function%20%28DR-CBF%29%20constraint%20that%20directly%20processes%20range%0Asensor%20data%20to%20impose%20safety%20constraints.%20Coupling%20this%20with%20a%20control%20Lyapunov%0Afunction%20%28CLF%29%20for%20path%20tracking%2C%20we%20demonstrate%20that%20our%20CLF-DR-CBF%20control%0Asynthesis%20method%20achieves%20safe%2C%20efficient%2C%20and%20robust%20navigation%20in%20uncertain%0Adynamic%20environments.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%0Asimulated%20and%20real%20autonomous%20robot%20navigation%20experiments%2C%20marking%20a%0Asubstantial%20advancement%20in%20real-time%20safety%20guarantees%20for%20mobile%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18251v1&entry.124074799=Read"},
{"title": "DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention", "author": "Lianghui Zhu and Zilong Huang and Bencheng Liao and Jun Hao Liew and Hanshu Yan and Jiashi Feng and Xinggang Wang", "abstract": "  Diffusion models with large-scale pre-training have achieved significant\nsuccess in the field of visual content generation, particularly exemplified by\nDiffusion Transformers (DiT). However, DiT models have faced challenges with\nscalability and quadratic complexity efficiency. In this paper, we aim to\nleverage the long sequence modeling capability of Gated Linear Attention (GLA)\nTransformers, expanding its applicability to diffusion models. We introduce\nDiffusion Gated Linear Attention Transformers (DiG), a simple, adoptable\nsolution with minimal parameter overhead, following the DiT design, but\noffering superior efficiency and effectiveness. In addition to better\nperformance than DiT, DiG-S/2 exhibits $2.5\\times$ higher training speed than\nDiT-S/2 and saves $75.7\\%$ GPU memory at a resolution of $1792 \\times 1792$.\nMoreover, we analyze the scalability of DiG across a variety of computational\ncomplexity. DiG models, with increased depth/width or augmentation of input\ntokens, consistently exhibit decreasing FID. We further compare DiG with other\nsubquadratic-time diffusion models. With the same model size, DiG-XL/2 is\n$4.2\\times$ faster than the recent Mamba-based diffusion model at a $1024$\nresolution, and is $1.8\\times$ faster than DiT with CUDA-optimized\nFlashAttention-2 under the $2048$ resolution. All these results demonstrate its\nsuperior efficiency among the latest diffusion models. Code is released at\nhttps://github.com/hustvl/DiG.\n", "link": "http://arxiv.org/abs/2405.18428v1", "date": "2024-05-28", "relevancy": 1.9758, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6936}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6685}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiG%3A%20Scalable%20and%20Efficient%20Diffusion%20Models%20with%20Gated%20Linear%20Attention&body=Title%3A%20DiG%3A%20Scalable%20and%20Efficient%20Diffusion%20Models%20with%20Gated%20Linear%20Attention%0AAuthor%3A%20Lianghui%20Zhu%20and%20Zilong%20Huang%20and%20Bencheng%20Liao%20and%20Jun%20Hao%20Liew%20and%20Hanshu%20Yan%20and%20Jiashi%20Feng%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Diffusion%20models%20with%20large-scale%20pre-training%20have%20achieved%20significant%0Asuccess%20in%20the%20field%20of%20visual%20content%20generation%2C%20particularly%20exemplified%20by%0ADiffusion%20Transformers%20%28DiT%29.%20However%2C%20DiT%20models%20have%20faced%20challenges%20with%0Ascalability%20and%20quadratic%20complexity%20efficiency.%20In%20this%20paper%2C%20we%20aim%20to%0Aleverage%20the%20long%20sequence%20modeling%20capability%20of%20Gated%20Linear%20Attention%20%28GLA%29%0ATransformers%2C%20expanding%20its%20applicability%20to%20diffusion%20models.%20We%20introduce%0ADiffusion%20Gated%20Linear%20Attention%20Transformers%20%28DiG%29%2C%20a%20simple%2C%20adoptable%0Asolution%20with%20minimal%20parameter%20overhead%2C%20following%20the%20DiT%20design%2C%20but%0Aoffering%20superior%20efficiency%20and%20effectiveness.%20In%20addition%20to%20better%0Aperformance%20than%20DiT%2C%20DiG-S/2%20exhibits%20%242.5%5Ctimes%24%20higher%20training%20speed%20than%0ADiT-S/2%20and%20saves%20%2475.7%5C%25%24%20GPU%20memory%20at%20a%20resolution%20of%20%241792%20%5Ctimes%201792%24.%0AMoreover%2C%20we%20analyze%20the%20scalability%20of%20DiG%20across%20a%20variety%20of%20computational%0Acomplexity.%20DiG%20models%2C%20with%20increased%20depth/width%20or%20augmentation%20of%20input%0Atokens%2C%20consistently%20exhibit%20decreasing%20FID.%20We%20further%20compare%20DiG%20with%20other%0Asubquadratic-time%20diffusion%20models.%20With%20the%20same%20model%20size%2C%20DiG-XL/2%20is%0A%244.2%5Ctimes%24%20faster%20than%20the%20recent%20Mamba-based%20diffusion%20model%20at%20a%20%241024%24%0Aresolution%2C%20and%20is%20%241.8%5Ctimes%24%20faster%20than%20DiT%20with%20CUDA-optimized%0AFlashAttention-2%20under%20the%20%242048%24%20resolution.%20All%20these%20results%20demonstrate%20its%0Asuperior%20efficiency%20among%20the%20latest%20diffusion%20models.%20Code%20is%20released%20at%0Ahttps%3A//github.com/hustvl/DiG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiG%253A%2520Scalable%2520and%2520Efficient%2520Diffusion%2520Models%2520with%2520Gated%2520Linear%2520Attention%26entry.906535625%3DLianghui%2520Zhu%2520and%2520Zilong%2520Huang%2520and%2520Bencheng%2520Liao%2520and%2520Jun%2520Hao%2520Liew%2520and%2520Hanshu%2520Yan%2520and%2520Jiashi%2520Feng%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520with%2520large-scale%2520pre-training%2520have%2520achieved%2520significant%250Asuccess%2520in%2520the%2520field%2520of%2520visual%2520content%2520generation%252C%2520particularly%2520exemplified%2520by%250ADiffusion%2520Transformers%2520%2528DiT%2529.%2520However%252C%2520DiT%2520models%2520have%2520faced%2520challenges%2520with%250Ascalability%2520and%2520quadratic%2520complexity%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%250Aleverage%2520the%2520long%2520sequence%2520modeling%2520capability%2520of%2520Gated%2520Linear%2520Attention%2520%2528GLA%2529%250ATransformers%252C%2520expanding%2520its%2520applicability%2520to%2520diffusion%2520models.%2520We%2520introduce%250ADiffusion%2520Gated%2520Linear%2520Attention%2520Transformers%2520%2528DiG%2529%252C%2520a%2520simple%252C%2520adoptable%250Asolution%2520with%2520minimal%2520parameter%2520overhead%252C%2520following%2520the%2520DiT%2520design%252C%2520but%250Aoffering%2520superior%2520efficiency%2520and%2520effectiveness.%2520In%2520addition%2520to%2520better%250Aperformance%2520than%2520DiT%252C%2520DiG-S/2%2520exhibits%2520%25242.5%255Ctimes%2524%2520higher%2520training%2520speed%2520than%250ADiT-S/2%2520and%2520saves%2520%252475.7%255C%2525%2524%2520GPU%2520memory%2520at%2520a%2520resolution%2520of%2520%25241792%2520%255Ctimes%25201792%2524.%250AMoreover%252C%2520we%2520analyze%2520the%2520scalability%2520of%2520DiG%2520across%2520a%2520variety%2520of%2520computational%250Acomplexity.%2520DiG%2520models%252C%2520with%2520increased%2520depth/width%2520or%2520augmentation%2520of%2520input%250Atokens%252C%2520consistently%2520exhibit%2520decreasing%2520FID.%2520We%2520further%2520compare%2520DiG%2520with%2520other%250Asubquadratic-time%2520diffusion%2520models.%2520With%2520the%2520same%2520model%2520size%252C%2520DiG-XL/2%2520is%250A%25244.2%255Ctimes%2524%2520faster%2520than%2520the%2520recent%2520Mamba-based%2520diffusion%2520model%2520at%2520a%2520%25241024%2524%250Aresolution%252C%2520and%2520is%2520%25241.8%255Ctimes%2524%2520faster%2520than%2520DiT%2520with%2520CUDA-optimized%250AFlashAttention-2%2520under%2520the%2520%25242048%2524%2520resolution.%2520All%2520these%2520results%2520demonstrate%2520its%250Asuperior%2520efficiency%2520among%2520the%2520latest%2520diffusion%2520models.%2520Code%2520is%2520released%2520at%250Ahttps%253A//github.com/hustvl/DiG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiG%3A%20Scalable%20and%20Efficient%20Diffusion%20Models%20with%20Gated%20Linear%20Attention&entry.906535625=Lianghui%20Zhu%20and%20Zilong%20Huang%20and%20Bencheng%20Liao%20and%20Jun%20Hao%20Liew%20and%20Hanshu%20Yan%20and%20Jiashi%20Feng%20and%20Xinggang%20Wang&entry.1292438233=%20%20Diffusion%20models%20with%20large-scale%20pre-training%20have%20achieved%20significant%0Asuccess%20in%20the%20field%20of%20visual%20content%20generation%2C%20particularly%20exemplified%20by%0ADiffusion%20Transformers%20%28DiT%29.%20However%2C%20DiT%20models%20have%20faced%20challenges%20with%0Ascalability%20and%20quadratic%20complexity%20efficiency.%20In%20this%20paper%2C%20we%20aim%20to%0Aleverage%20the%20long%20sequence%20modeling%20capability%20of%20Gated%20Linear%20Attention%20%28GLA%29%0ATransformers%2C%20expanding%20its%20applicability%20to%20diffusion%20models.%20We%20introduce%0ADiffusion%20Gated%20Linear%20Attention%20Transformers%20%28DiG%29%2C%20a%20simple%2C%20adoptable%0Asolution%20with%20minimal%20parameter%20overhead%2C%20following%20the%20DiT%20design%2C%20but%0Aoffering%20superior%20efficiency%20and%20effectiveness.%20In%20addition%20to%20better%0Aperformance%20than%20DiT%2C%20DiG-S/2%20exhibits%20%242.5%5Ctimes%24%20higher%20training%20speed%20than%0ADiT-S/2%20and%20saves%20%2475.7%5C%25%24%20GPU%20memory%20at%20a%20resolution%20of%20%241792%20%5Ctimes%201792%24.%0AMoreover%2C%20we%20analyze%20the%20scalability%20of%20DiG%20across%20a%20variety%20of%20computational%0Acomplexity.%20DiG%20models%2C%20with%20increased%20depth/width%20or%20augmentation%20of%20input%0Atokens%2C%20consistently%20exhibit%20decreasing%20FID.%20We%20further%20compare%20DiG%20with%20other%0Asubquadratic-time%20diffusion%20models.%20With%20the%20same%20model%20size%2C%20DiG-XL/2%20is%0A%244.2%5Ctimes%24%20faster%20than%20the%20recent%20Mamba-based%20diffusion%20model%20at%20a%20%241024%24%0Aresolution%2C%20and%20is%20%241.8%5Ctimes%24%20faster%20than%20DiT%20with%20CUDA-optimized%0AFlashAttention-2%20under%20the%20%242048%24%20resolution.%20All%20these%20results%20demonstrate%20its%0Asuperior%20efficiency%20among%20the%20latest%20diffusion%20models.%20Code%20is%20released%20at%0Ahttps%3A//github.com/hustvl/DiG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18428v1&entry.124074799=Read"},
{"title": "Whole Genome Transformer for Gene Interaction Effects in Microbiome\n  Habitat Specificity", "author": "Zhufeng Li and Sandeep S Cranganore and Nicholas Youngblut and Niki Kilbertus", "abstract": "  Leveraging the vast genetic diversity within microbiomes offers unparalleled\ninsights into complex phenotypes, yet the task of accurately predicting and\nunderstanding such traits from genomic data remains challenging. We propose a\nframework taking advantage of existing large models for gene vectorization to\npredict habitat specificity from entire microbial genome sequences. Based on\nour model, we develop attribution techniques to elucidate gene interaction\neffects that drive microbial adaptation to diverse environments. We train and\nvalidate our approach on a large dataset of high quality microbiome genomes\nfrom different habitats. We not only demonstrate solid predictive performance,\nbut also how sequence-level information of entire genomes allows us to identify\ngene associations underlying complex phenotypes. Our attribution recovers known\nimportant interaction networks and proposes new candidates for experimental\nfollow up.\n", "link": "http://arxiv.org/abs/2405.05998v2", "date": "2024-05-28", "relevancy": 1.9076, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5158}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4498}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whole%20Genome%20Transformer%20for%20Gene%20Interaction%20Effects%20in%20Microbiome%0A%20%20Habitat%20Specificity&body=Title%3A%20Whole%20Genome%20Transformer%20for%20Gene%20Interaction%20Effects%20in%20Microbiome%0A%20%20Habitat%20Specificity%0AAuthor%3A%20Zhufeng%20Li%20and%20Sandeep%20S%20Cranganore%20and%20Nicholas%20Youngblut%20and%20Niki%20Kilbertus%0AAbstract%3A%20%20%20Leveraging%20the%20vast%20genetic%20diversity%20within%20microbiomes%20offers%20unparalleled%0Ainsights%20into%20complex%20phenotypes%2C%20yet%20the%20task%20of%20accurately%20predicting%20and%0Aunderstanding%20such%20traits%20from%20genomic%20data%20remains%20challenging.%20We%20propose%20a%0Aframework%20taking%20advantage%20of%20existing%20large%20models%20for%20gene%20vectorization%20to%0Apredict%20habitat%20specificity%20from%20entire%20microbial%20genome%20sequences.%20Based%20on%0Aour%20model%2C%20we%20develop%20attribution%20techniques%20to%20elucidate%20gene%20interaction%0Aeffects%20that%20drive%20microbial%20adaptation%20to%20diverse%20environments.%20We%20train%20and%0Avalidate%20our%20approach%20on%20a%20large%20dataset%20of%20high%20quality%20microbiome%20genomes%0Afrom%20different%20habitats.%20We%20not%20only%20demonstrate%20solid%20predictive%20performance%2C%0Abut%20also%20how%20sequence-level%20information%20of%20entire%20genomes%20allows%20us%20to%20identify%0Agene%20associations%20underlying%20complex%20phenotypes.%20Our%20attribution%20recovers%20known%0Aimportant%20interaction%20networks%20and%20proposes%20new%20candidates%20for%20experimental%0Afollow%20up.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05998v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhole%2520Genome%2520Transformer%2520for%2520Gene%2520Interaction%2520Effects%2520in%2520Microbiome%250A%2520%2520Habitat%2520Specificity%26entry.906535625%3DZhufeng%2520Li%2520and%2520Sandeep%2520S%2520Cranganore%2520and%2520Nicholas%2520Youngblut%2520and%2520Niki%2520Kilbertus%26entry.1292438233%3D%2520%2520Leveraging%2520the%2520vast%2520genetic%2520diversity%2520within%2520microbiomes%2520offers%2520unparalleled%250Ainsights%2520into%2520complex%2520phenotypes%252C%2520yet%2520the%2520task%2520of%2520accurately%2520predicting%2520and%250Aunderstanding%2520such%2520traits%2520from%2520genomic%2520data%2520remains%2520challenging.%2520We%2520propose%2520a%250Aframework%2520taking%2520advantage%2520of%2520existing%2520large%2520models%2520for%2520gene%2520vectorization%2520to%250Apredict%2520habitat%2520specificity%2520from%2520entire%2520microbial%2520genome%2520sequences.%2520Based%2520on%250Aour%2520model%252C%2520we%2520develop%2520attribution%2520techniques%2520to%2520elucidate%2520gene%2520interaction%250Aeffects%2520that%2520drive%2520microbial%2520adaptation%2520to%2520diverse%2520environments.%2520We%2520train%2520and%250Avalidate%2520our%2520approach%2520on%2520a%2520large%2520dataset%2520of%2520high%2520quality%2520microbiome%2520genomes%250Afrom%2520different%2520habitats.%2520We%2520not%2520only%2520demonstrate%2520solid%2520predictive%2520performance%252C%250Abut%2520also%2520how%2520sequence-level%2520information%2520of%2520entire%2520genomes%2520allows%2520us%2520to%2520identify%250Agene%2520associations%2520underlying%2520complex%2520phenotypes.%2520Our%2520attribution%2520recovers%2520known%250Aimportant%2520interaction%2520networks%2520and%2520proposes%2520new%2520candidates%2520for%2520experimental%250Afollow%2520up.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05998v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whole%20Genome%20Transformer%20for%20Gene%20Interaction%20Effects%20in%20Microbiome%0A%20%20Habitat%20Specificity&entry.906535625=Zhufeng%20Li%20and%20Sandeep%20S%20Cranganore%20and%20Nicholas%20Youngblut%20and%20Niki%20Kilbertus&entry.1292438233=%20%20Leveraging%20the%20vast%20genetic%20diversity%20within%20microbiomes%20offers%20unparalleled%0Ainsights%20into%20complex%20phenotypes%2C%20yet%20the%20task%20of%20accurately%20predicting%20and%0Aunderstanding%20such%20traits%20from%20genomic%20data%20remains%20challenging.%20We%20propose%20a%0Aframework%20taking%20advantage%20of%20existing%20large%20models%20for%20gene%20vectorization%20to%0Apredict%20habitat%20specificity%20from%20entire%20microbial%20genome%20sequences.%20Based%20on%0Aour%20model%2C%20we%20develop%20attribution%20techniques%20to%20elucidate%20gene%20interaction%0Aeffects%20that%20drive%20microbial%20adaptation%20to%20diverse%20environments.%20We%20train%20and%0Avalidate%20our%20approach%20on%20a%20large%20dataset%20of%20high%20quality%20microbiome%20genomes%0Afrom%20different%20habitats.%20We%20not%20only%20demonstrate%20solid%20predictive%20performance%2C%0Abut%20also%20how%20sequence-level%20information%20of%20entire%20genomes%20allows%20us%20to%20identify%0Agene%20associations%20underlying%20complex%20phenotypes.%20Our%20attribution%20recovers%20known%0Aimportant%20interaction%20networks%20and%20proposes%20new%20candidates%20for%20experimental%0Afollow%20up.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05998v2&entry.124074799=Read"},
{"title": "HarmoDT: Harmony Multi-Task Decision Transformer for Offline\n  Reinforcement Learning", "author": "Shengchao Hu and Ziqing Fan and Li Shen and Ya Zhang and Yanfeng Wang and Dacheng Tao", "abstract": "  The purpose of offline multi-task reinforcement learning (MTRL) is to develop\na unified policy applicable to diverse tasks without the need for online\nenvironmental interaction. Recent advancements approach this through sequence\nmodeling, leveraging the Transformer architecture's scalability and the\nbenefits of parameter sharing to exploit task similarities. However, variations\nin task content and complexity pose significant challenges in policy\nformulation, necessitating judicious parameter sharing and management of\nconflicting gradients for optimal policy performance. In this work, we\nintroduce the Harmony Multi-Task Decision Transformer (HarmoDT), a novel\nsolution designed to identify an optimal harmony subspace of parameters for\neach task. We approach this as a bi-level optimization problem, employing a\nmeta-learning framework that leverages gradient-based techniques. The upper\nlevel of this framework is dedicated to learning a task-specific mask that\ndelineates the harmony subspace, while the inner level focuses on updating\nparameters to enhance the overall performance of the unified policy. Empirical\nevaluations on a series of benchmarks demonstrate the superiority of HarmoDT,\nverifying the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2405.18080v1", "date": "2024-05-28", "relevancy": 1.8983, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5005}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4576}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HarmoDT%3A%20Harmony%20Multi-Task%20Decision%20Transformer%20for%20Offline%0A%20%20Reinforcement%20Learning&body=Title%3A%20HarmoDT%3A%20Harmony%20Multi-Task%20Decision%20Transformer%20for%20Offline%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Shengchao%20Hu%20and%20Ziqing%20Fan%20and%20Li%20Shen%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20The%20purpose%20of%20offline%20multi-task%20reinforcement%20learning%20%28MTRL%29%20is%20to%20develop%0Aa%20unified%20policy%20applicable%20to%20diverse%20tasks%20without%20the%20need%20for%20online%0Aenvironmental%20interaction.%20Recent%20advancements%20approach%20this%20through%20sequence%0Amodeling%2C%20leveraging%20the%20Transformer%20architecture%27s%20scalability%20and%20the%0Abenefits%20of%20parameter%20sharing%20to%20exploit%20task%20similarities.%20However%2C%20variations%0Ain%20task%20content%20and%20complexity%20pose%20significant%20challenges%20in%20policy%0Aformulation%2C%20necessitating%20judicious%20parameter%20sharing%20and%20management%20of%0Aconflicting%20gradients%20for%20optimal%20policy%20performance.%20In%20this%20work%2C%20we%0Aintroduce%20the%20Harmony%20Multi-Task%20Decision%20Transformer%20%28HarmoDT%29%2C%20a%20novel%0Asolution%20designed%20to%20identify%20an%20optimal%20harmony%20subspace%20of%20parameters%20for%0Aeach%20task.%20We%20approach%20this%20as%20a%20bi-level%20optimization%20problem%2C%20employing%20a%0Ameta-learning%20framework%20that%20leverages%20gradient-based%20techniques.%20The%20upper%0Alevel%20of%20this%20framework%20is%20dedicated%20to%20learning%20a%20task-specific%20mask%20that%0Adelineates%20the%20harmony%20subspace%2C%20while%20the%20inner%20level%20focuses%20on%20updating%0Aparameters%20to%20enhance%20the%20overall%20performance%20of%20the%20unified%20policy.%20Empirical%0Aevaluations%20on%20a%20series%20of%20benchmarks%20demonstrate%20the%20superiority%20of%20HarmoDT%2C%0Averifying%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmoDT%253A%2520Harmony%2520Multi-Task%2520Decision%2520Transformer%2520for%2520Offline%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DShengchao%2520Hu%2520and%2520Ziqing%2520Fan%2520and%2520Li%2520Shen%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520The%2520purpose%2520of%2520offline%2520multi-task%2520reinforcement%2520learning%2520%2528MTRL%2529%2520is%2520to%2520develop%250Aa%2520unified%2520policy%2520applicable%2520to%2520diverse%2520tasks%2520without%2520the%2520need%2520for%2520online%250Aenvironmental%2520interaction.%2520Recent%2520advancements%2520approach%2520this%2520through%2520sequence%250Amodeling%252C%2520leveraging%2520the%2520Transformer%2520architecture%2527s%2520scalability%2520and%2520the%250Abenefits%2520of%2520parameter%2520sharing%2520to%2520exploit%2520task%2520similarities.%2520However%252C%2520variations%250Ain%2520task%2520content%2520and%2520complexity%2520pose%2520significant%2520challenges%2520in%2520policy%250Aformulation%252C%2520necessitating%2520judicious%2520parameter%2520sharing%2520and%2520management%2520of%250Aconflicting%2520gradients%2520for%2520optimal%2520policy%2520performance.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520the%2520Harmony%2520Multi-Task%2520Decision%2520Transformer%2520%2528HarmoDT%2529%252C%2520a%2520novel%250Asolution%2520designed%2520to%2520identify%2520an%2520optimal%2520harmony%2520subspace%2520of%2520parameters%2520for%250Aeach%2520task.%2520We%2520approach%2520this%2520as%2520a%2520bi-level%2520optimization%2520problem%252C%2520employing%2520a%250Ameta-learning%2520framework%2520that%2520leverages%2520gradient-based%2520techniques.%2520The%2520upper%250Alevel%2520of%2520this%2520framework%2520is%2520dedicated%2520to%2520learning%2520a%2520task-specific%2520mask%2520that%250Adelineates%2520the%2520harmony%2520subspace%252C%2520while%2520the%2520inner%2520level%2520focuses%2520on%2520updating%250Aparameters%2520to%2520enhance%2520the%2520overall%2520performance%2520of%2520the%2520unified%2520policy.%2520Empirical%250Aevaluations%2520on%2520a%2520series%2520of%2520benchmarks%2520demonstrate%2520the%2520superiority%2520of%2520HarmoDT%252C%250Averifying%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HarmoDT%3A%20Harmony%20Multi-Task%20Decision%20Transformer%20for%20Offline%0A%20%20Reinforcement%20Learning&entry.906535625=Shengchao%20Hu%20and%20Ziqing%20Fan%20and%20Li%20Shen%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Dacheng%20Tao&entry.1292438233=%20%20The%20purpose%20of%20offline%20multi-task%20reinforcement%20learning%20%28MTRL%29%20is%20to%20develop%0Aa%20unified%20policy%20applicable%20to%20diverse%20tasks%20without%20the%20need%20for%20online%0Aenvironmental%20interaction.%20Recent%20advancements%20approach%20this%20through%20sequence%0Amodeling%2C%20leveraging%20the%20Transformer%20architecture%27s%20scalability%20and%20the%0Abenefits%20of%20parameter%20sharing%20to%20exploit%20task%20similarities.%20However%2C%20variations%0Ain%20task%20content%20and%20complexity%20pose%20significant%20challenges%20in%20policy%0Aformulation%2C%20necessitating%20judicious%20parameter%20sharing%20and%20management%20of%0Aconflicting%20gradients%20for%20optimal%20policy%20performance.%20In%20this%20work%2C%20we%0Aintroduce%20the%20Harmony%20Multi-Task%20Decision%20Transformer%20%28HarmoDT%29%2C%20a%20novel%0Asolution%20designed%20to%20identify%20an%20optimal%20harmony%20subspace%20of%20parameters%20for%0Aeach%20task.%20We%20approach%20this%20as%20a%20bi-level%20optimization%20problem%2C%20employing%20a%0Ameta-learning%20framework%20that%20leverages%20gradient-based%20techniques.%20The%20upper%0Alevel%20of%20this%20framework%20is%20dedicated%20to%20learning%20a%20task-specific%20mask%20that%0Adelineates%20the%20harmony%20subspace%2C%20while%20the%20inner%20level%20focuses%20on%20updating%0Aparameters%20to%20enhance%20the%20overall%20performance%20of%20the%20unified%20policy.%20Empirical%0Aevaluations%20on%20a%20series%20of%20benchmarks%20demonstrate%20the%20superiority%20of%20HarmoDT%2C%0Averifying%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18080v1&entry.124074799=Read"},
{"title": "Position Paper: Think Globally, React Locally -- Bringing Real-time\n  Reference-based Website Phishing Detection on macOS", "author": "Ivan Petrukha and Nataliia Stulova and Sergii Kryvoblotskyi", "abstract": "  Background. The recent surge in phishing attacks keeps undermining the\neffectiveness of the traditional anti-phishing blacklist approaches. On-device\nanti-phishing solutions are gaining popularity as they offer faster phishing\ndetection locally. Aim. We aim to eliminate the delay in recognizing and\nrecording phishing campaigns in databases via on-device solutions that identify\nphishing sites immediately when encountered by the user rather than waiting for\na web crawler's scan to finish. Additionally, utilizing operating\nsystem-specific resources and frameworks, we aim to minimize the impact on\nsystem performance and depend on local processing to protect user privacy.\nMethod. We propose a phishing detection solution that uses a combination of\ncomputer vision and on-device machine learning models to analyze websites in\nreal time. Our reference-based approach analyzes the visual content of\nwebpages, identifying phishing attempts through layout analysis, credential\ninput areas detection, and brand impersonation criteria combination. Results.\nOur case study shows it's feasible to perform background processing on-device\ncontinuously, for the case of the web browser requiring the resource use of 16%\nof a single CPU core and less than 84MB of RAM on Apple M1 while maintaining\nthe accuracy of brand logo detection at 46.6% (comparable with baselines), and\nof Credential Requiring Page detection at 98.1% (improving the baseline by\n3.1%), within the test dataset. Conclusions. Our results demonstrate the\npotential of on-device, real-time phishing detection systems to enhance\ncybersecurity defensive technologies and extend the scope of phishing detection\nto more similar regions of interest, e.g., email clients and messenger windows.\n", "link": "http://arxiv.org/abs/2405.18236v1", "date": "2024-05-28", "relevancy": 1.2386, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4188}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4129}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%20Paper%3A%20Think%20Globally%2C%20React%20Locally%20--%20Bringing%20Real-time%0A%20%20Reference-based%20Website%20Phishing%20Detection%20on%20macOS&body=Title%3A%20Position%20Paper%3A%20Think%20Globally%2C%20React%20Locally%20--%20Bringing%20Real-time%0A%20%20Reference-based%20Website%20Phishing%20Detection%20on%20macOS%0AAuthor%3A%20Ivan%20Petrukha%20and%20Nataliia%20Stulova%20and%20Sergii%20Kryvoblotskyi%0AAbstract%3A%20%20%20Background.%20The%20recent%20surge%20in%20phishing%20attacks%20keeps%20undermining%20the%0Aeffectiveness%20of%20the%20traditional%20anti-phishing%20blacklist%20approaches.%20On-device%0Aanti-phishing%20solutions%20are%20gaining%20popularity%20as%20they%20offer%20faster%20phishing%0Adetection%20locally.%20Aim.%20We%20aim%20to%20eliminate%20the%20delay%20in%20recognizing%20and%0Arecording%20phishing%20campaigns%20in%20databases%20via%20on-device%20solutions%20that%20identify%0Aphishing%20sites%20immediately%20when%20encountered%20by%20the%20user%20rather%20than%20waiting%20for%0Aa%20web%20crawler%27s%20scan%20to%20finish.%20Additionally%2C%20utilizing%20operating%0Asystem-specific%20resources%20and%20frameworks%2C%20we%20aim%20to%20minimize%20the%20impact%20on%0Asystem%20performance%20and%20depend%20on%20local%20processing%20to%20protect%20user%20privacy.%0AMethod.%20We%20propose%20a%20phishing%20detection%20solution%20that%20uses%20a%20combination%20of%0Acomputer%20vision%20and%20on-device%20machine%20learning%20models%20to%20analyze%20websites%20in%0Areal%20time.%20Our%20reference-based%20approach%20analyzes%20the%20visual%20content%20of%0Awebpages%2C%20identifying%20phishing%20attempts%20through%20layout%20analysis%2C%20credential%0Ainput%20areas%20detection%2C%20and%20brand%20impersonation%20criteria%20combination.%20Results.%0AOur%20case%20study%20shows%20it%27s%20feasible%20to%20perform%20background%20processing%20on-device%0Acontinuously%2C%20for%20the%20case%20of%20the%20web%20browser%20requiring%20the%20resource%20use%20of%2016%25%0Aof%20a%20single%20CPU%20core%20and%20less%20than%2084MB%20of%20RAM%20on%20Apple%20M1%20while%20maintaining%0Athe%20accuracy%20of%20brand%20logo%20detection%20at%2046.6%25%20%28comparable%20with%20baselines%29%2C%20and%0Aof%20Credential%20Requiring%20Page%20detection%20at%2098.1%25%20%28improving%20the%20baseline%20by%0A3.1%25%29%2C%20within%20the%20test%20dataset.%20Conclusions.%20Our%20results%20demonstrate%20the%0Apotential%20of%20on-device%2C%20real-time%20phishing%20detection%20systems%20to%20enhance%0Acybersecurity%20defensive%20technologies%20and%20extend%20the%20scope%20of%20phishing%20detection%0Ato%20more%20similar%20regions%20of%20interest%2C%20e.g.%2C%20email%20clients%20and%20messenger%20windows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%2520Paper%253A%2520Think%2520Globally%252C%2520React%2520Locally%2520--%2520Bringing%2520Real-time%250A%2520%2520Reference-based%2520Website%2520Phishing%2520Detection%2520on%2520macOS%26entry.906535625%3DIvan%2520Petrukha%2520and%2520Nataliia%2520Stulova%2520and%2520Sergii%2520Kryvoblotskyi%26entry.1292438233%3D%2520%2520Background.%2520The%2520recent%2520surge%2520in%2520phishing%2520attacks%2520keeps%2520undermining%2520the%250Aeffectiveness%2520of%2520the%2520traditional%2520anti-phishing%2520blacklist%2520approaches.%2520On-device%250Aanti-phishing%2520solutions%2520are%2520gaining%2520popularity%2520as%2520they%2520offer%2520faster%2520phishing%250Adetection%2520locally.%2520Aim.%2520We%2520aim%2520to%2520eliminate%2520the%2520delay%2520in%2520recognizing%2520and%250Arecording%2520phishing%2520campaigns%2520in%2520databases%2520via%2520on-device%2520solutions%2520that%2520identify%250Aphishing%2520sites%2520immediately%2520when%2520encountered%2520by%2520the%2520user%2520rather%2520than%2520waiting%2520for%250Aa%2520web%2520crawler%2527s%2520scan%2520to%2520finish.%2520Additionally%252C%2520utilizing%2520operating%250Asystem-specific%2520resources%2520and%2520frameworks%252C%2520we%2520aim%2520to%2520minimize%2520the%2520impact%2520on%250Asystem%2520performance%2520and%2520depend%2520on%2520local%2520processing%2520to%2520protect%2520user%2520privacy.%250AMethod.%2520We%2520propose%2520a%2520phishing%2520detection%2520solution%2520that%2520uses%2520a%2520combination%2520of%250Acomputer%2520vision%2520and%2520on-device%2520machine%2520learning%2520models%2520to%2520analyze%2520websites%2520in%250Areal%2520time.%2520Our%2520reference-based%2520approach%2520analyzes%2520the%2520visual%2520content%2520of%250Awebpages%252C%2520identifying%2520phishing%2520attempts%2520through%2520layout%2520analysis%252C%2520credential%250Ainput%2520areas%2520detection%252C%2520and%2520brand%2520impersonation%2520criteria%2520combination.%2520Results.%250AOur%2520case%2520study%2520shows%2520it%2527s%2520feasible%2520to%2520perform%2520background%2520processing%2520on-device%250Acontinuously%252C%2520for%2520the%2520case%2520of%2520the%2520web%2520browser%2520requiring%2520the%2520resource%2520use%2520of%252016%2525%250Aof%2520a%2520single%2520CPU%2520core%2520and%2520less%2520than%252084MB%2520of%2520RAM%2520on%2520Apple%2520M1%2520while%2520maintaining%250Athe%2520accuracy%2520of%2520brand%2520logo%2520detection%2520at%252046.6%2525%2520%2528comparable%2520with%2520baselines%2529%252C%2520and%250Aof%2520Credential%2520Requiring%2520Page%2520detection%2520at%252098.1%2525%2520%2528improving%2520the%2520baseline%2520by%250A3.1%2525%2529%252C%2520within%2520the%2520test%2520dataset.%2520Conclusions.%2520Our%2520results%2520demonstrate%2520the%250Apotential%2520of%2520on-device%252C%2520real-time%2520phishing%2520detection%2520systems%2520to%2520enhance%250Acybersecurity%2520defensive%2520technologies%2520and%2520extend%2520the%2520scope%2520of%2520phishing%2520detection%250Ato%2520more%2520similar%2520regions%2520of%2520interest%252C%2520e.g.%252C%2520email%2520clients%2520and%2520messenger%2520windows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%20Paper%3A%20Think%20Globally%2C%20React%20Locally%20--%20Bringing%20Real-time%0A%20%20Reference-based%20Website%20Phishing%20Detection%20on%20macOS&entry.906535625=Ivan%20Petrukha%20and%20Nataliia%20Stulova%20and%20Sergii%20Kryvoblotskyi&entry.1292438233=%20%20Background.%20The%20recent%20surge%20in%20phishing%20attacks%20keeps%20undermining%20the%0Aeffectiveness%20of%20the%20traditional%20anti-phishing%20blacklist%20approaches.%20On-device%0Aanti-phishing%20solutions%20are%20gaining%20popularity%20as%20they%20offer%20faster%20phishing%0Adetection%20locally.%20Aim.%20We%20aim%20to%20eliminate%20the%20delay%20in%20recognizing%20and%0Arecording%20phishing%20campaigns%20in%20databases%20via%20on-device%20solutions%20that%20identify%0Aphishing%20sites%20immediately%20when%20encountered%20by%20the%20user%20rather%20than%20waiting%20for%0Aa%20web%20crawler%27s%20scan%20to%20finish.%20Additionally%2C%20utilizing%20operating%0Asystem-specific%20resources%20and%20frameworks%2C%20we%20aim%20to%20minimize%20the%20impact%20on%0Asystem%20performance%20and%20depend%20on%20local%20processing%20to%20protect%20user%20privacy.%0AMethod.%20We%20propose%20a%20phishing%20detection%20solution%20that%20uses%20a%20combination%20of%0Acomputer%20vision%20and%20on-device%20machine%20learning%20models%20to%20analyze%20websites%20in%0Areal%20time.%20Our%20reference-based%20approach%20analyzes%20the%20visual%20content%20of%0Awebpages%2C%20identifying%20phishing%20attempts%20through%20layout%20analysis%2C%20credential%0Ainput%20areas%20detection%2C%20and%20brand%20impersonation%20criteria%20combination.%20Results.%0AOur%20case%20study%20shows%20it%27s%20feasible%20to%20perform%20background%20processing%20on-device%0Acontinuously%2C%20for%20the%20case%20of%20the%20web%20browser%20requiring%20the%20resource%20use%20of%2016%25%0Aof%20a%20single%20CPU%20core%20and%20less%20than%2084MB%20of%20RAM%20on%20Apple%20M1%20while%20maintaining%0Athe%20accuracy%20of%20brand%20logo%20detection%20at%2046.6%25%20%28comparable%20with%20baselines%29%2C%20and%0Aof%20Credential%20Requiring%20Page%20detection%20at%2098.1%25%20%28improving%20the%20baseline%20by%0A3.1%25%29%2C%20within%20the%20test%20dataset.%20Conclusions.%20Our%20results%20demonstrate%20the%0Apotential%20of%20on-device%2C%20real-time%20phishing%20detection%20systems%20to%20enhance%0Acybersecurity%20defensive%20technologies%20and%20extend%20the%20scope%20of%20phishing%20detection%0Ato%20more%20similar%20regions%20of%20interest%2C%20e.g.%2C%20email%20clients%20and%20messenger%20windows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18236v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


