<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240709.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models", "author": "Bowen Zhang and Yiji Cheng and Chunyu Wang and Ting Zhang and Jiaolong Yang and Yansong Tang and Feng Zhao and Dong Chen and Baining Guo", "abstract": "  We present RodinHD, which can generate high-fidelity 3D avatars from a\nportrait image. Existing methods fail to capture intricate details such as\nhairstyles which we tackle in this paper. We first identify an overlooked\nproblem of catastrophic forgetting that arises when fitting triplanes\nsequentially on many avatars, caused by the MLP decoder sharing scheme. To\novercome this issue, we raise a novel data scheduling strategy and a weight\nconsolidation regularization term, which improves the decoder's capability of\nrendering sharper details. Additionally, we optimize the guiding effect of the\nportrait image by computing a finer-grained hierarchical representation that\ncaptures rich 2D texture cues, and injecting them to the 3D diffusion model at\nmultiple layers via cross-attention. When trained on 46K avatars with a noise\nschedule optimized for triplanes, the resulting model can generate 3D avatars\nwith notably better details than previous methods and can generalize to\nin-the-wild portrait input.\n", "link": "http://arxiv.org/abs/2407.06938v1", "date": "2024-07-09", "relevancy": 3.1713, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6355}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6355}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RodinHD%3A%20High-Fidelity%203D%20Avatar%20Generation%20with%20Diffusion%20Models&body=Title%3A%20RodinHD%3A%20High-Fidelity%203D%20Avatar%20Generation%20with%20Diffusion%20Models%0AAuthor%3A%20Bowen%20Zhang%20and%20Yiji%20Cheng%20and%20Chunyu%20Wang%20and%20Ting%20Zhang%20and%20Jiaolong%20Yang%20and%20Yansong%20Tang%20and%20Feng%20Zhao%20and%20Dong%20Chen%20and%20Baining%20Guo%0AAbstract%3A%20%20%20We%20present%20RodinHD%2C%20which%20can%20generate%20high-fidelity%203D%20avatars%20from%20a%0Aportrait%20image.%20Existing%20methods%20fail%20to%20capture%20intricate%20details%20such%20as%0Ahairstyles%20which%20we%20tackle%20in%20this%20paper.%20We%20first%20identify%20an%20overlooked%0Aproblem%20of%20catastrophic%20forgetting%20that%20arises%20when%20fitting%20triplanes%0Asequentially%20on%20many%20avatars%2C%20caused%20by%20the%20MLP%20decoder%20sharing%20scheme.%20To%0Aovercome%20this%20issue%2C%20we%20raise%20a%20novel%20data%20scheduling%20strategy%20and%20a%20weight%0Aconsolidation%20regularization%20term%2C%20which%20improves%20the%20decoder%27s%20capability%20of%0Arendering%20sharper%20details.%20Additionally%2C%20we%20optimize%20the%20guiding%20effect%20of%20the%0Aportrait%20image%20by%20computing%20a%20finer-grained%20hierarchical%20representation%20that%0Acaptures%20rich%202D%20texture%20cues%2C%20and%20injecting%20them%20to%20the%203D%20diffusion%20model%20at%0Amultiple%20layers%20via%20cross-attention.%20When%20trained%20on%2046K%20avatars%20with%20a%20noise%0Aschedule%20optimized%20for%20triplanes%2C%20the%20resulting%20model%20can%20generate%203D%20avatars%0Awith%20notably%20better%20details%20than%20previous%20methods%20and%20can%20generalize%20to%0Ain-the-wild%20portrait%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRodinHD%253A%2520High-Fidelity%25203D%2520Avatar%2520Generation%2520with%2520Diffusion%2520Models%26entry.906535625%3DBowen%2520Zhang%2520and%2520Yiji%2520Cheng%2520and%2520Chunyu%2520Wang%2520and%2520Ting%2520Zhang%2520and%2520Jiaolong%2520Yang%2520and%2520Yansong%2520Tang%2520and%2520Feng%2520Zhao%2520and%2520Dong%2520Chen%2520and%2520Baining%2520Guo%26entry.1292438233%3D%2520%2520We%2520present%2520RodinHD%252C%2520which%2520can%2520generate%2520high-fidelity%25203D%2520avatars%2520from%2520a%250Aportrait%2520image.%2520Existing%2520methods%2520fail%2520to%2520capture%2520intricate%2520details%2520such%2520as%250Ahairstyles%2520which%2520we%2520tackle%2520in%2520this%2520paper.%2520We%2520first%2520identify%2520an%2520overlooked%250Aproblem%2520of%2520catastrophic%2520forgetting%2520that%2520arises%2520when%2520fitting%2520triplanes%250Asequentially%2520on%2520many%2520avatars%252C%2520caused%2520by%2520the%2520MLP%2520decoder%2520sharing%2520scheme.%2520To%250Aovercome%2520this%2520issue%252C%2520we%2520raise%2520a%2520novel%2520data%2520scheduling%2520strategy%2520and%2520a%2520weight%250Aconsolidation%2520regularization%2520term%252C%2520which%2520improves%2520the%2520decoder%2527s%2520capability%2520of%250Arendering%2520sharper%2520details.%2520Additionally%252C%2520we%2520optimize%2520the%2520guiding%2520effect%2520of%2520the%250Aportrait%2520image%2520by%2520computing%2520a%2520finer-grained%2520hierarchical%2520representation%2520that%250Acaptures%2520rich%25202D%2520texture%2520cues%252C%2520and%2520injecting%2520them%2520to%2520the%25203D%2520diffusion%2520model%2520at%250Amultiple%2520layers%2520via%2520cross-attention.%2520When%2520trained%2520on%252046K%2520avatars%2520with%2520a%2520noise%250Aschedule%2520optimized%2520for%2520triplanes%252C%2520the%2520resulting%2520model%2520can%2520generate%25203D%2520avatars%250Awith%2520notably%2520better%2520details%2520than%2520previous%2520methods%2520and%2520can%2520generalize%2520to%250Ain-the-wild%2520portrait%2520input.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RodinHD%3A%20High-Fidelity%203D%20Avatar%20Generation%20with%20Diffusion%20Models&entry.906535625=Bowen%20Zhang%20and%20Yiji%20Cheng%20and%20Chunyu%20Wang%20and%20Ting%20Zhang%20and%20Jiaolong%20Yang%20and%20Yansong%20Tang%20and%20Feng%20Zhao%20and%20Dong%20Chen%20and%20Baining%20Guo&entry.1292438233=%20%20We%20present%20RodinHD%2C%20which%20can%20generate%20high-fidelity%203D%20avatars%20from%20a%0Aportrait%20image.%20Existing%20methods%20fail%20to%20capture%20intricate%20details%20such%20as%0Ahairstyles%20which%20we%20tackle%20in%20this%20paper.%20We%20first%20identify%20an%20overlooked%0Aproblem%20of%20catastrophic%20forgetting%20that%20arises%20when%20fitting%20triplanes%0Asequentially%20on%20many%20avatars%2C%20caused%20by%20the%20MLP%20decoder%20sharing%20scheme.%20To%0Aovercome%20this%20issue%2C%20we%20raise%20a%20novel%20data%20scheduling%20strategy%20and%20a%20weight%0Aconsolidation%20regularization%20term%2C%20which%20improves%20the%20decoder%27s%20capability%20of%0Arendering%20sharper%20details.%20Additionally%2C%20we%20optimize%20the%20guiding%20effect%20of%20the%0Aportrait%20image%20by%20computing%20a%20finer-grained%20hierarchical%20representation%20that%0Acaptures%20rich%202D%20texture%20cues%2C%20and%20injecting%20them%20to%20the%203D%20diffusion%20model%20at%0Amultiple%20layers%20via%20cross-attention.%20When%20trained%20on%2046K%20avatars%20with%20a%20noise%0Aschedule%20optimized%20for%20triplanes%2C%20the%20resulting%20model%20can%20generate%203D%20avatars%0Awith%20notably%20better%20details%20than%20previous%20methods%20and%20can%20generalize%20to%0Ain-the-wild%20portrait%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06938v1&entry.124074799=Read"},
{"title": "3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes", "author": "Nicolas Moenne-Loccoz and Ashkan Mirzaei and Or Perel and Riccardo de Lutio and Janick Martinez Esturo and Gavriel State and Sanja Fidler and Nicholas Sharp and Zan Gojcic", "abstract": "  Particle-based representations of radiance fields such as 3D Gaussian\nSplatting have found great success for reconstructing and re-rendering of\ncomplex scenes. Most existing methods render particles via rasterization,\nprojecting them to screen space tiles for processing in a sorted order. This\nwork instead considers ray tracing the particles, building a bounding volume\nhierarchy and casting a ray for each pixel using high-performance GPU ray\ntracing hardware. To efficiently handle large numbers of semi-transparent\nparticles, we describe a specialized rendering algorithm which encapsulates\nparticles with bounding meshes to leverage fast ray-triangle intersections, and\nshades batches of intersections in depth-order. The benefits of ray tracing are\nwell-known in computer graphics: processing incoherent rays for secondary\nlighting effects such as shadows and reflections, rendering from\nhighly-distorted cameras common in robotics, stochastically sampling rays, and\nmore. With our renderer, this flexibility comes at little cost compared to\nrasterization. Experiments demonstrate the speed and accuracy of our approach,\nas well as several applications in computer graphics and vision. We further\npropose related improvements to the basic Gaussian representation, including a\nsimple use of generalized kernel functions which significantly reduces particle\nhit counts.\n", "link": "http://arxiv.org/abs/2407.07090v1", "date": "2024-07-09", "relevancy": 2.9387, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6682}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5593}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Gaussian%20Ray%20Tracing%3A%20Fast%20Tracing%20of%20Particle%20Scenes&body=Title%3A%203D%20Gaussian%20Ray%20Tracing%3A%20Fast%20Tracing%20of%20Particle%20Scenes%0AAuthor%3A%20Nicolas%20Moenne-Loccoz%20and%20Ashkan%20Mirzaei%20and%20Or%20Perel%20and%20Riccardo%20de%20Lutio%20and%20Janick%20Martinez%20Esturo%20and%20Gavriel%20State%20and%20Sanja%20Fidler%20and%20Nicholas%20Sharp%20and%20Zan%20Gojcic%0AAbstract%3A%20%20%20Particle-based%20representations%20of%20radiance%20fields%20such%20as%203D%20Gaussian%0ASplatting%20have%20found%20great%20success%20for%20reconstructing%20and%20re-rendering%20of%0Acomplex%20scenes.%20Most%20existing%20methods%20render%20particles%20via%20rasterization%2C%0Aprojecting%20them%20to%20screen%20space%20tiles%20for%20processing%20in%20a%20sorted%20order.%20This%0Awork%20instead%20considers%20ray%20tracing%20the%20particles%2C%20building%20a%20bounding%20volume%0Ahierarchy%20and%20casting%20a%20ray%20for%20each%20pixel%20using%20high-performance%20GPU%20ray%0Atracing%20hardware.%20To%20efficiently%20handle%20large%20numbers%20of%20semi-transparent%0Aparticles%2C%20we%20describe%20a%20specialized%20rendering%20algorithm%20which%20encapsulates%0Aparticles%20with%20bounding%20meshes%20to%20leverage%20fast%20ray-triangle%20intersections%2C%20and%0Ashades%20batches%20of%20intersections%20in%20depth-order.%20The%20benefits%20of%20ray%20tracing%20are%0Awell-known%20in%20computer%20graphics%3A%20processing%20incoherent%20rays%20for%20secondary%0Alighting%20effects%20such%20as%20shadows%20and%20reflections%2C%20rendering%20from%0Ahighly-distorted%20cameras%20common%20in%20robotics%2C%20stochastically%20sampling%20rays%2C%20and%0Amore.%20With%20our%20renderer%2C%20this%20flexibility%20comes%20at%20little%20cost%20compared%20to%0Arasterization.%20Experiments%20demonstrate%20the%20speed%20and%20accuracy%20of%20our%20approach%2C%0Aas%20well%20as%20several%20applications%20in%20computer%20graphics%20and%20vision.%20We%20further%0Apropose%20related%20improvements%20to%20the%20basic%20Gaussian%20representation%2C%20including%20a%0Asimple%20use%20of%20generalized%20kernel%20functions%20which%20significantly%20reduces%20particle%0Ahit%20counts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Gaussian%2520Ray%2520Tracing%253A%2520Fast%2520Tracing%2520of%2520Particle%2520Scenes%26entry.906535625%3DNicolas%2520Moenne-Loccoz%2520and%2520Ashkan%2520Mirzaei%2520and%2520Or%2520Perel%2520and%2520Riccardo%2520de%2520Lutio%2520and%2520Janick%2520Martinez%2520Esturo%2520and%2520Gavriel%2520State%2520and%2520Sanja%2520Fidler%2520and%2520Nicholas%2520Sharp%2520and%2520Zan%2520Gojcic%26entry.1292438233%3D%2520%2520Particle-based%2520representations%2520of%2520radiance%2520fields%2520such%2520as%25203D%2520Gaussian%250ASplatting%2520have%2520found%2520great%2520success%2520for%2520reconstructing%2520and%2520re-rendering%2520of%250Acomplex%2520scenes.%2520Most%2520existing%2520methods%2520render%2520particles%2520via%2520rasterization%252C%250Aprojecting%2520them%2520to%2520screen%2520space%2520tiles%2520for%2520processing%2520in%2520a%2520sorted%2520order.%2520This%250Awork%2520instead%2520considers%2520ray%2520tracing%2520the%2520particles%252C%2520building%2520a%2520bounding%2520volume%250Ahierarchy%2520and%2520casting%2520a%2520ray%2520for%2520each%2520pixel%2520using%2520high-performance%2520GPU%2520ray%250Atracing%2520hardware.%2520To%2520efficiently%2520handle%2520large%2520numbers%2520of%2520semi-transparent%250Aparticles%252C%2520we%2520describe%2520a%2520specialized%2520rendering%2520algorithm%2520which%2520encapsulates%250Aparticles%2520with%2520bounding%2520meshes%2520to%2520leverage%2520fast%2520ray-triangle%2520intersections%252C%2520and%250Ashades%2520batches%2520of%2520intersections%2520in%2520depth-order.%2520The%2520benefits%2520of%2520ray%2520tracing%2520are%250Awell-known%2520in%2520computer%2520graphics%253A%2520processing%2520incoherent%2520rays%2520for%2520secondary%250Alighting%2520effects%2520such%2520as%2520shadows%2520and%2520reflections%252C%2520rendering%2520from%250Ahighly-distorted%2520cameras%2520common%2520in%2520robotics%252C%2520stochastically%2520sampling%2520rays%252C%2520and%250Amore.%2520With%2520our%2520renderer%252C%2520this%2520flexibility%2520comes%2520at%2520little%2520cost%2520compared%2520to%250Arasterization.%2520Experiments%2520demonstrate%2520the%2520speed%2520and%2520accuracy%2520of%2520our%2520approach%252C%250Aas%2520well%2520as%2520several%2520applications%2520in%2520computer%2520graphics%2520and%2520vision.%2520We%2520further%250Apropose%2520related%2520improvements%2520to%2520the%2520basic%2520Gaussian%2520representation%252C%2520including%2520a%250Asimple%2520use%2520of%2520generalized%2520kernel%2520functions%2520which%2520significantly%2520reduces%2520particle%250Ahit%2520counts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Gaussian%20Ray%20Tracing%3A%20Fast%20Tracing%20of%20Particle%20Scenes&entry.906535625=Nicolas%20Moenne-Loccoz%20and%20Ashkan%20Mirzaei%20and%20Or%20Perel%20and%20Riccardo%20de%20Lutio%20and%20Janick%20Martinez%20Esturo%20and%20Gavriel%20State%20and%20Sanja%20Fidler%20and%20Nicholas%20Sharp%20and%20Zan%20Gojcic&entry.1292438233=%20%20Particle-based%20representations%20of%20radiance%20fields%20such%20as%203D%20Gaussian%0ASplatting%20have%20found%20great%20success%20for%20reconstructing%20and%20re-rendering%20of%0Acomplex%20scenes.%20Most%20existing%20methods%20render%20particles%20via%20rasterization%2C%0Aprojecting%20them%20to%20screen%20space%20tiles%20for%20processing%20in%20a%20sorted%20order.%20This%0Awork%20instead%20considers%20ray%20tracing%20the%20particles%2C%20building%20a%20bounding%20volume%0Ahierarchy%20and%20casting%20a%20ray%20for%20each%20pixel%20using%20high-performance%20GPU%20ray%0Atracing%20hardware.%20To%20efficiently%20handle%20large%20numbers%20of%20semi-transparent%0Aparticles%2C%20we%20describe%20a%20specialized%20rendering%20algorithm%20which%20encapsulates%0Aparticles%20with%20bounding%20meshes%20to%20leverage%20fast%20ray-triangle%20intersections%2C%20and%0Ashades%20batches%20of%20intersections%20in%20depth-order.%20The%20benefits%20of%20ray%20tracing%20are%0Awell-known%20in%20computer%20graphics%3A%20processing%20incoherent%20rays%20for%20secondary%0Alighting%20effects%20such%20as%20shadows%20and%20reflections%2C%20rendering%20from%0Ahighly-distorted%20cameras%20common%20in%20robotics%2C%20stochastically%20sampling%20rays%2C%20and%0Amore.%20With%20our%20renderer%2C%20this%20flexibility%20comes%20at%20little%20cost%20compared%20to%0Arasterization.%20Experiments%20demonstrate%20the%20speed%20and%20accuracy%20of%20our%20approach%2C%0Aas%20well%20as%20several%20applications%20in%20computer%20graphics%20and%20vision.%20We%20further%0Apropose%20related%20improvements%20to%20the%20basic%20Gaussian%20representation%2C%20including%20a%0Asimple%20use%20of%20generalized%20kernel%20functions%20which%20significantly%20reduces%20particle%0Ahit%20counts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07090v1&entry.124074799=Read"},
{"title": "Category-level Object Detection, Pose Estimation and Reconstruction from\n  Stereo Images", "author": "Chuanrui Zhang and Yonggen Ling and Minglei Lu and Minghan Qin and Haoqian Wang", "abstract": "  We study the 3D object understanding task for manipulating everyday objects\nwith different material properties (diffuse, specular, transparent and mixed).\nExisting monocular and RGB-D methods suffer from scale ambiguity due to missing\nor imprecise depth measurements. We present CODERS, a one-stage approach for\nCategory-level Object Detection, pose Estimation and Reconstruction from Stereo\nimages. The base of our pipeline is an implicit stereo matching module that\ncombines stereo image features with 3D position information. Concatenating this\npresented module and the following transform-decoder architecture leads to\nend-to-end learning of multiple tasks required by robot manipulation. Our\napproach significantly outperforms all competing methods in the public TOD\ndataset. Furthermore, trained on simulated data, CODERS generalize well to\nunseen category-level object instances in real-world robot manipulation\nexperiments. Our dataset, code, and demos will be available on our project\npage.\n", "link": "http://arxiv.org/abs/2407.06984v1", "date": "2024-07-09", "relevancy": 2.8551, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5759}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5759}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Category-level%20Object%20Detection%2C%20Pose%20Estimation%20and%20Reconstruction%20from%0A%20%20Stereo%20Images&body=Title%3A%20Category-level%20Object%20Detection%2C%20Pose%20Estimation%20and%20Reconstruction%20from%0A%20%20Stereo%20Images%0AAuthor%3A%20Chuanrui%20Zhang%20and%20Yonggen%20Ling%20and%20Minglei%20Lu%20and%20Minghan%20Qin%20and%20Haoqian%20Wang%0AAbstract%3A%20%20%20We%20study%20the%203D%20object%20understanding%20task%20for%20manipulating%20everyday%20objects%0Awith%20different%20material%20properties%20%28diffuse%2C%20specular%2C%20transparent%20and%20mixed%29.%0AExisting%20monocular%20and%20RGB-D%20methods%20suffer%20from%20scale%20ambiguity%20due%20to%20missing%0Aor%20imprecise%20depth%20measurements.%20We%20present%20CODERS%2C%20a%20one-stage%20approach%20for%0ACategory-level%20Object%20Detection%2C%20pose%20Estimation%20and%20Reconstruction%20from%20Stereo%0Aimages.%20The%20base%20of%20our%20pipeline%20is%20an%20implicit%20stereo%20matching%20module%20that%0Acombines%20stereo%20image%20features%20with%203D%20position%20information.%20Concatenating%20this%0Apresented%20module%20and%20the%20following%20transform-decoder%20architecture%20leads%20to%0Aend-to-end%20learning%20of%20multiple%20tasks%20required%20by%20robot%20manipulation.%20Our%0Aapproach%20significantly%20outperforms%20all%20competing%20methods%20in%20the%20public%20TOD%0Adataset.%20Furthermore%2C%20trained%20on%20simulated%20data%2C%20CODERS%20generalize%20well%20to%0Aunseen%20category-level%20object%20instances%20in%20real-world%20robot%20manipulation%0Aexperiments.%20Our%20dataset%2C%20code%2C%20and%20demos%20will%20be%20available%20on%20our%20project%0Apage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCategory-level%2520Object%2520Detection%252C%2520Pose%2520Estimation%2520and%2520Reconstruction%2520from%250A%2520%2520Stereo%2520Images%26entry.906535625%3DChuanrui%2520Zhang%2520and%2520Yonggen%2520Ling%2520and%2520Minglei%2520Lu%2520and%2520Minghan%2520Qin%2520and%2520Haoqian%2520Wang%26entry.1292438233%3D%2520%2520We%2520study%2520the%25203D%2520object%2520understanding%2520task%2520for%2520manipulating%2520everyday%2520objects%250Awith%2520different%2520material%2520properties%2520%2528diffuse%252C%2520specular%252C%2520transparent%2520and%2520mixed%2529.%250AExisting%2520monocular%2520and%2520RGB-D%2520methods%2520suffer%2520from%2520scale%2520ambiguity%2520due%2520to%2520missing%250Aor%2520imprecise%2520depth%2520measurements.%2520We%2520present%2520CODERS%252C%2520a%2520one-stage%2520approach%2520for%250ACategory-level%2520Object%2520Detection%252C%2520pose%2520Estimation%2520and%2520Reconstruction%2520from%2520Stereo%250Aimages.%2520The%2520base%2520of%2520our%2520pipeline%2520is%2520an%2520implicit%2520stereo%2520matching%2520module%2520that%250Acombines%2520stereo%2520image%2520features%2520with%25203D%2520position%2520information.%2520Concatenating%2520this%250Apresented%2520module%2520and%2520the%2520following%2520transform-decoder%2520architecture%2520leads%2520to%250Aend-to-end%2520learning%2520of%2520multiple%2520tasks%2520required%2520by%2520robot%2520manipulation.%2520Our%250Aapproach%2520significantly%2520outperforms%2520all%2520competing%2520methods%2520in%2520the%2520public%2520TOD%250Adataset.%2520Furthermore%252C%2520trained%2520on%2520simulated%2520data%252C%2520CODERS%2520generalize%2520well%2520to%250Aunseen%2520category-level%2520object%2520instances%2520in%2520real-world%2520robot%2520manipulation%250Aexperiments.%2520Our%2520dataset%252C%2520code%252C%2520and%2520demos%2520will%2520be%2520available%2520on%2520our%2520project%250Apage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Category-level%20Object%20Detection%2C%20Pose%20Estimation%20and%20Reconstruction%20from%0A%20%20Stereo%20Images&entry.906535625=Chuanrui%20Zhang%20and%20Yonggen%20Ling%20and%20Minglei%20Lu%20and%20Minghan%20Qin%20and%20Haoqian%20Wang&entry.1292438233=%20%20We%20study%20the%203D%20object%20understanding%20task%20for%20manipulating%20everyday%20objects%0Awith%20different%20material%20properties%20%28diffuse%2C%20specular%2C%20transparent%20and%20mixed%29.%0AExisting%20monocular%20and%20RGB-D%20methods%20suffer%20from%20scale%20ambiguity%20due%20to%20missing%0Aor%20imprecise%20depth%20measurements.%20We%20present%20CODERS%2C%20a%20one-stage%20approach%20for%0ACategory-level%20Object%20Detection%2C%20pose%20Estimation%20and%20Reconstruction%20from%20Stereo%0Aimages.%20The%20base%20of%20our%20pipeline%20is%20an%20implicit%20stereo%20matching%20module%20that%0Acombines%20stereo%20image%20features%20with%203D%20position%20information.%20Concatenating%20this%0Apresented%20module%20and%20the%20following%20transform-decoder%20architecture%20leads%20to%0Aend-to-end%20learning%20of%20multiple%20tasks%20required%20by%20robot%20manipulation.%20Our%0Aapproach%20significantly%20outperforms%20all%20competing%20methods%20in%20the%20public%20TOD%0Adataset.%20Furthermore%2C%20trained%20on%20simulated%20data%2C%20CODERS%20generalize%20well%20to%0Aunseen%20category-level%20object%20instances%20in%20real-world%20robot%20manipulation%0Aexperiments.%20Our%20dataset%2C%20code%2C%20and%20demos%20will%20be%20available%20on%20our%20project%0Apage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06984v1&entry.124074799=Read"},
{"title": "V-VIPE: Variational View Invariant Pose Embedding", "author": "Mara Levy and Abhinav Shrivastava", "abstract": "  Learning to represent three dimensional (3D) human pose given a two\ndimensional (2D) image of a person, is a challenging problem. In order to make\nthe problem less ambiguous it has become common practice to estimate 3D pose in\nthe camera coordinate space. However, this makes the task of comparing two 3D\nposes difficult. In this paper, we address this challenge by separating the\nproblem of estimating 3D pose from 2D images into two steps. We use a\nvariational autoencoder (VAE) to find an embedding that represents 3D poses in\ncanonical coordinate space. We refer to this embedding as variational\nview-invariant pose embedding V-VIPE. Using V-VIPE we can encode 2D and 3D\nposes and use the embedding for downstream tasks, like retrieval and\nclassification. We can estimate 3D poses from these embeddings using the\ndecoder as well as generate unseen 3D poses. The variability of our encoding\nallows it to generalize well to unseen camera views when mapping from 2D space.\nTo the best of our knowledge, V-VIPE is the only representation to offer this\ndiversity of applications. Code and more information can be found at\nhttps://v-vipe.github.io/.\n", "link": "http://arxiv.org/abs/2407.07092v1", "date": "2024-07-09", "relevancy": 2.8406, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5672}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-VIPE%3A%20Variational%20View%20Invariant%20Pose%20Embedding&body=Title%3A%20V-VIPE%3A%20Variational%20View%20Invariant%20Pose%20Embedding%0AAuthor%3A%20Mara%20Levy%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20Learning%20to%20represent%20three%20dimensional%20%283D%29%20human%20pose%20given%20a%20two%0Adimensional%20%282D%29%20image%20of%20a%20person%2C%20is%20a%20challenging%20problem.%20In%20order%20to%20make%0Athe%20problem%20less%20ambiguous%20it%20has%20become%20common%20practice%20to%20estimate%203D%20pose%20in%0Athe%20camera%20coordinate%20space.%20However%2C%20this%20makes%20the%20task%20of%20comparing%20two%203D%0Aposes%20difficult.%20In%20this%20paper%2C%20we%20address%20this%20challenge%20by%20separating%20the%0Aproblem%20of%20estimating%203D%20pose%20from%202D%20images%20into%20two%20steps.%20We%20use%20a%0Avariational%20autoencoder%20%28VAE%29%20to%20find%20an%20embedding%20that%20represents%203D%20poses%20in%0Acanonical%20coordinate%20space.%20We%20refer%20to%20this%20embedding%20as%20variational%0Aview-invariant%20pose%20embedding%20V-VIPE.%20Using%20V-VIPE%20we%20can%20encode%202D%20and%203D%0Aposes%20and%20use%20the%20embedding%20for%20downstream%20tasks%2C%20like%20retrieval%20and%0Aclassification.%20We%20can%20estimate%203D%20poses%20from%20these%20embeddings%20using%20the%0Adecoder%20as%20well%20as%20generate%20unseen%203D%20poses.%20The%20variability%20of%20our%20encoding%0Aallows%20it%20to%20generalize%20well%20to%20unseen%20camera%20views%20when%20mapping%20from%202D%20space.%0ATo%20the%20best%20of%20our%20knowledge%2C%20V-VIPE%20is%20the%20only%20representation%20to%20offer%20this%0Adiversity%20of%20applications.%20Code%20and%20more%20information%20can%20be%20found%20at%0Ahttps%3A//v-vipe.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-VIPE%253A%2520Variational%2520View%2520Invariant%2520Pose%2520Embedding%26entry.906535625%3DMara%2520Levy%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520Learning%2520to%2520represent%2520three%2520dimensional%2520%25283D%2529%2520human%2520pose%2520given%2520a%2520two%250Adimensional%2520%25282D%2529%2520image%2520of%2520a%2520person%252C%2520is%2520a%2520challenging%2520problem.%2520In%2520order%2520to%2520make%250Athe%2520problem%2520less%2520ambiguous%2520it%2520has%2520become%2520common%2520practice%2520to%2520estimate%25203D%2520pose%2520in%250Athe%2520camera%2520coordinate%2520space.%2520However%252C%2520this%2520makes%2520the%2520task%2520of%2520comparing%2520two%25203D%250Aposes%2520difficult.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520challenge%2520by%2520separating%2520the%250Aproblem%2520of%2520estimating%25203D%2520pose%2520from%25202D%2520images%2520into%2520two%2520steps.%2520We%2520use%2520a%250Avariational%2520autoencoder%2520%2528VAE%2529%2520to%2520find%2520an%2520embedding%2520that%2520represents%25203D%2520poses%2520in%250Acanonical%2520coordinate%2520space.%2520We%2520refer%2520to%2520this%2520embedding%2520as%2520variational%250Aview-invariant%2520pose%2520embedding%2520V-VIPE.%2520Using%2520V-VIPE%2520we%2520can%2520encode%25202D%2520and%25203D%250Aposes%2520and%2520use%2520the%2520embedding%2520for%2520downstream%2520tasks%252C%2520like%2520retrieval%2520and%250Aclassification.%2520We%2520can%2520estimate%25203D%2520poses%2520from%2520these%2520embeddings%2520using%2520the%250Adecoder%2520as%2520well%2520as%2520generate%2520unseen%25203D%2520poses.%2520The%2520variability%2520of%2520our%2520encoding%250Aallows%2520it%2520to%2520generalize%2520well%2520to%2520unseen%2520camera%2520views%2520when%2520mapping%2520from%25202D%2520space.%250ATo%2520the%2520best%2520of%2520our%2520knowledge%252C%2520V-VIPE%2520is%2520the%2520only%2520representation%2520to%2520offer%2520this%250Adiversity%2520of%2520applications.%2520Code%2520and%2520more%2520information%2520can%2520be%2520found%2520at%250Ahttps%253A//v-vipe.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-VIPE%3A%20Variational%20View%20Invariant%20Pose%20Embedding&entry.906535625=Mara%20Levy%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Learning%20to%20represent%20three%20dimensional%20%283D%29%20human%20pose%20given%20a%20two%0Adimensional%20%282D%29%20image%20of%20a%20person%2C%20is%20a%20challenging%20problem.%20In%20order%20to%20make%0Athe%20problem%20less%20ambiguous%20it%20has%20become%20common%20practice%20to%20estimate%203D%20pose%20in%0Athe%20camera%20coordinate%20space.%20However%2C%20this%20makes%20the%20task%20of%20comparing%20two%203D%0Aposes%20difficult.%20In%20this%20paper%2C%20we%20address%20this%20challenge%20by%20separating%20the%0Aproblem%20of%20estimating%203D%20pose%20from%202D%20images%20into%20two%20steps.%20We%20use%20a%0Avariational%20autoencoder%20%28VAE%29%20to%20find%20an%20embedding%20that%20represents%203D%20poses%20in%0Acanonical%20coordinate%20space.%20We%20refer%20to%20this%20embedding%20as%20variational%0Aview-invariant%20pose%20embedding%20V-VIPE.%20Using%20V-VIPE%20we%20can%20encode%202D%20and%203D%0Aposes%20and%20use%20the%20embedding%20for%20downstream%20tasks%2C%20like%20retrieval%20and%0Aclassification.%20We%20can%20estimate%203D%20poses%20from%20these%20embeddings%20using%20the%0Adecoder%20as%20well%20as%20generate%20unseen%203D%20poses.%20The%20variability%20of%20our%20encoding%0Aallows%20it%20to%20generalize%20well%20to%20unseen%20camera%20views%20when%20mapping%20from%202D%20space.%0ATo%20the%20best%20of%20our%20knowledge%2C%20V-VIPE%20is%20the%20only%20representation%20to%20offer%20this%0Adiversity%20of%20applications.%20Code%20and%20more%20information%20can%20be%20found%20at%0Ahttps%3A//v-vipe.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07092v1&entry.124074799=Read"},
{"title": "Momentum Auxiliary Network for Supervised Local Learning", "author": "Junhao Su and Changpeng Cai and Feiyu Zhu and Chenghao He and Xiaojie Xu and Dongzhi Guan and Chenyang Si", "abstract": "  Deep neural networks conventionally employ end-to-end backpropagation for\ntheir training process, which lacks biological credibility and triggers a\nlocking dilemma during network parameter updates, leading to significant GPU\nmemory use. Supervised local learning, which segments the network into multiple\nlocal blocks updated by independent auxiliary networks. However, these methods\ncannot replace end-to-end training due to lower accuracy, as gradients only\npropagate within their local block, creating a lack of information exchange\nbetween blocks. To address this issue and establish information transfer across\nblocks, we propose a Momentum Auxiliary Network (MAN) that establishes a\ndynamic interaction mechanism. The MAN leverages an exponential moving average\n(EMA) of the parameters from adjacent local blocks to enhance information flow.\nThis auxiliary network, updated through EMA, helps bridge the informational gap\nbetween blocks. Nevertheless, we observe that directly applying EMA parameters\nhas certain limitations due to feature discrepancies among local blocks. To\novercome this, we introduce learnable biases, further boosting performance. We\nhave validated our method on four image classification datasets (CIFAR-10,\nSTL-10, SVHN, ImageNet), attaining superior performance and substantial memory\nsavings. Notably, our method can reduce GPU memory usage by more than 45\\% on\nthe ImageNet dataset compared to end-to-end training, while achieving higher\nperformance. The Momentum Auxiliary Network thus offers a new perspective for\nsupervised local learning. Our code is available at:\nhttps://github.com/JunhaoSu0/MAN.\n", "link": "http://arxiv.org/abs/2407.05623v2", "date": "2024-07-09", "relevancy": 2.8336, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5877}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5738}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Momentum%20Auxiliary%20Network%20for%20Supervised%20Local%20Learning&body=Title%3A%20Momentum%20Auxiliary%20Network%20for%20Supervised%20Local%20Learning%0AAuthor%3A%20Junhao%20Su%20and%20Changpeng%20Cai%20and%20Feiyu%20Zhu%20and%20Chenghao%20He%20and%20Xiaojie%20Xu%20and%20Dongzhi%20Guan%20and%20Chenyang%20Si%0AAbstract%3A%20%20%20Deep%20neural%20networks%20conventionally%20employ%20end-to-end%20backpropagation%20for%0Atheir%20training%20process%2C%20which%20lacks%20biological%20credibility%20and%20triggers%20a%0Alocking%20dilemma%20during%20network%20parameter%20updates%2C%20leading%20to%20significant%20GPU%0Amemory%20use.%20Supervised%20local%20learning%2C%20which%20segments%20the%20network%20into%20multiple%0Alocal%20blocks%20updated%20by%20independent%20auxiliary%20networks.%20However%2C%20these%20methods%0Acannot%20replace%20end-to-end%20training%20due%20to%20lower%20accuracy%2C%20as%20gradients%20only%0Apropagate%20within%20their%20local%20block%2C%20creating%20a%20lack%20of%20information%20exchange%0Abetween%20blocks.%20To%20address%20this%20issue%20and%20establish%20information%20transfer%20across%0Ablocks%2C%20we%20propose%20a%20Momentum%20Auxiliary%20Network%20%28MAN%29%20that%20establishes%20a%0Adynamic%20interaction%20mechanism.%20The%20MAN%20leverages%20an%20exponential%20moving%20average%0A%28EMA%29%20of%20the%20parameters%20from%20adjacent%20local%20blocks%20to%20enhance%20information%20flow.%0AThis%20auxiliary%20network%2C%20updated%20through%20EMA%2C%20helps%20bridge%20the%20informational%20gap%0Abetween%20blocks.%20Nevertheless%2C%20we%20observe%20that%20directly%20applying%20EMA%20parameters%0Ahas%20certain%20limitations%20due%20to%20feature%20discrepancies%20among%20local%20blocks.%20To%0Aovercome%20this%2C%20we%20introduce%20learnable%20biases%2C%20further%20boosting%20performance.%20We%0Ahave%20validated%20our%20method%20on%20four%20image%20classification%20datasets%20%28CIFAR-10%2C%0ASTL-10%2C%20SVHN%2C%20ImageNet%29%2C%20attaining%20superior%20performance%20and%20substantial%20memory%0Asavings.%20Notably%2C%20our%20method%20can%20reduce%20GPU%20memory%20usage%20by%20more%20than%2045%5C%25%20on%0Athe%20ImageNet%20dataset%20compared%20to%20end-to-end%20training%2C%20while%20achieving%20higher%0Aperformance.%20The%20Momentum%20Auxiliary%20Network%20thus%20offers%20a%20new%20perspective%20for%0Asupervised%20local%20learning.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/JunhaoSu0/MAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05623v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMomentum%2520Auxiliary%2520Network%2520for%2520Supervised%2520Local%2520Learning%26entry.906535625%3DJunhao%2520Su%2520and%2520Changpeng%2520Cai%2520and%2520Feiyu%2520Zhu%2520and%2520Chenghao%2520He%2520and%2520Xiaojie%2520Xu%2520and%2520Dongzhi%2520Guan%2520and%2520Chenyang%2520Si%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520conventionally%2520employ%2520end-to-end%2520backpropagation%2520for%250Atheir%2520training%2520process%252C%2520which%2520lacks%2520biological%2520credibility%2520and%2520triggers%2520a%250Alocking%2520dilemma%2520during%2520network%2520parameter%2520updates%252C%2520leading%2520to%2520significant%2520GPU%250Amemory%2520use.%2520Supervised%2520local%2520learning%252C%2520which%2520segments%2520the%2520network%2520into%2520multiple%250Alocal%2520blocks%2520updated%2520by%2520independent%2520auxiliary%2520networks.%2520However%252C%2520these%2520methods%250Acannot%2520replace%2520end-to-end%2520training%2520due%2520to%2520lower%2520accuracy%252C%2520as%2520gradients%2520only%250Apropagate%2520within%2520their%2520local%2520block%252C%2520creating%2520a%2520lack%2520of%2520information%2520exchange%250Abetween%2520blocks.%2520To%2520address%2520this%2520issue%2520and%2520establish%2520information%2520transfer%2520across%250Ablocks%252C%2520we%2520propose%2520a%2520Momentum%2520Auxiliary%2520Network%2520%2528MAN%2529%2520that%2520establishes%2520a%250Adynamic%2520interaction%2520mechanism.%2520The%2520MAN%2520leverages%2520an%2520exponential%2520moving%2520average%250A%2528EMA%2529%2520of%2520the%2520parameters%2520from%2520adjacent%2520local%2520blocks%2520to%2520enhance%2520information%2520flow.%250AThis%2520auxiliary%2520network%252C%2520updated%2520through%2520EMA%252C%2520helps%2520bridge%2520the%2520informational%2520gap%250Abetween%2520blocks.%2520Nevertheless%252C%2520we%2520observe%2520that%2520directly%2520applying%2520EMA%2520parameters%250Ahas%2520certain%2520limitations%2520due%2520to%2520feature%2520discrepancies%2520among%2520local%2520blocks.%2520To%250Aovercome%2520this%252C%2520we%2520introduce%2520learnable%2520biases%252C%2520further%2520boosting%2520performance.%2520We%250Ahave%2520validated%2520our%2520method%2520on%2520four%2520image%2520classification%2520datasets%2520%2528CIFAR-10%252C%250ASTL-10%252C%2520SVHN%252C%2520ImageNet%2529%252C%2520attaining%2520superior%2520performance%2520and%2520substantial%2520memory%250Asavings.%2520Notably%252C%2520our%2520method%2520can%2520reduce%2520GPU%2520memory%2520usage%2520by%2520more%2520than%252045%255C%2525%2520on%250Athe%2520ImageNet%2520dataset%2520compared%2520to%2520end-to-end%2520training%252C%2520while%2520achieving%2520higher%250Aperformance.%2520The%2520Momentum%2520Auxiliary%2520Network%2520thus%2520offers%2520a%2520new%2520perspective%2520for%250Asupervised%2520local%2520learning.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/JunhaoSu0/MAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05623v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Momentum%20Auxiliary%20Network%20for%20Supervised%20Local%20Learning&entry.906535625=Junhao%20Su%20and%20Changpeng%20Cai%20and%20Feiyu%20Zhu%20and%20Chenghao%20He%20and%20Xiaojie%20Xu%20and%20Dongzhi%20Guan%20and%20Chenyang%20Si&entry.1292438233=%20%20Deep%20neural%20networks%20conventionally%20employ%20end-to-end%20backpropagation%20for%0Atheir%20training%20process%2C%20which%20lacks%20biological%20credibility%20and%20triggers%20a%0Alocking%20dilemma%20during%20network%20parameter%20updates%2C%20leading%20to%20significant%20GPU%0Amemory%20use.%20Supervised%20local%20learning%2C%20which%20segments%20the%20network%20into%20multiple%0Alocal%20blocks%20updated%20by%20independent%20auxiliary%20networks.%20However%2C%20these%20methods%0Acannot%20replace%20end-to-end%20training%20due%20to%20lower%20accuracy%2C%20as%20gradients%20only%0Apropagate%20within%20their%20local%20block%2C%20creating%20a%20lack%20of%20information%20exchange%0Abetween%20blocks.%20To%20address%20this%20issue%20and%20establish%20information%20transfer%20across%0Ablocks%2C%20we%20propose%20a%20Momentum%20Auxiliary%20Network%20%28MAN%29%20that%20establishes%20a%0Adynamic%20interaction%20mechanism.%20The%20MAN%20leverages%20an%20exponential%20moving%20average%0A%28EMA%29%20of%20the%20parameters%20from%20adjacent%20local%20blocks%20to%20enhance%20information%20flow.%0AThis%20auxiliary%20network%2C%20updated%20through%20EMA%2C%20helps%20bridge%20the%20informational%20gap%0Abetween%20blocks.%20Nevertheless%2C%20we%20observe%20that%20directly%20applying%20EMA%20parameters%0Ahas%20certain%20limitations%20due%20to%20feature%20discrepancies%20among%20local%20blocks.%20To%0Aovercome%20this%2C%20we%20introduce%20learnable%20biases%2C%20further%20boosting%20performance.%20We%0Ahave%20validated%20our%20method%20on%20four%20image%20classification%20datasets%20%28CIFAR-10%2C%0ASTL-10%2C%20SVHN%2C%20ImageNet%29%2C%20attaining%20superior%20performance%20and%20substantial%20memory%0Asavings.%20Notably%2C%20our%20method%20can%20reduce%20GPU%20memory%20usage%20by%20more%20than%2045%5C%25%20on%0Athe%20ImageNet%20dataset%20compared%20to%20end-to-end%20training%2C%20while%20achieving%20higher%0Aperformance.%20The%20Momentum%20Auxiliary%20Network%20thus%20offers%20a%20new%20perspective%20for%0Asupervised%20local%20learning.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/JunhaoSu0/MAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05623v2&entry.124074799=Read"},
{"title": "City-Scale Multi-Camera Vehicle Tracking System with Improved\n  Self-Supervised Camera Link Model", "author": "Yuqiang Lin and Sam Lockyer and Adrian Evans and Markus Zarbock and Nic Zhang", "abstract": "  Multi-Target Multi-Camera Tracking (MTMCT) has broad applications and forms\nthe basis for numerous future city-wide systems (e.g. traffic management, crash\ndetection, etc.). However, the challenge of matching vehicle trajectories\nacross different cameras based solely on feature extraction poses significant\ndifficulties. This article introduces an innovative multi-camera vehicle\ntracking system that utilizes a self-supervised camera link model. In contrast\nto related works that rely on manual spatial-temporal annotations, our model\nautomatically extracts crucial multi-camera relationships for vehicle matching.\nThe camera link is established through a pre-matching process that evaluates\nfeature similarities, pair numbers, and time variance for high-quality tracks.\nThis process calculates the probability of spatial linkage for all camera\ncombinations, selecting the highest scoring pairs to create camera links. Our\napproach significantly improves deployment times by eliminating the need for\nhuman annotation, offering substantial improvements in efficiency and\ncost-effectiveness when it comes to real-world application. This pairing\nprocess supports cross camera matching by setting spatial-temporal constraints,\nreducing the searching space for potential vehicle matches. According to our\nexperimental results, the proposed method achieves a new state-of-the-art among\nautomatic camera-link based methods in CityFlow V2 benchmarks with 61.07% IDF1\nScore.\n", "link": "http://arxiv.org/abs/2405.11345v2", "date": "2024-07-09", "relevancy": 2.8298, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5652}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20City-Scale%20Multi-Camera%20Vehicle%20Tracking%20System%20with%20Improved%0A%20%20Self-Supervised%20Camera%20Link%20Model&body=Title%3A%20City-Scale%20Multi-Camera%20Vehicle%20Tracking%20System%20with%20Improved%0A%20%20Self-Supervised%20Camera%20Link%20Model%0AAuthor%3A%20Yuqiang%20Lin%20and%20Sam%20Lockyer%20and%20Adrian%20Evans%20and%20Markus%20Zarbock%20and%20Nic%20Zhang%0AAbstract%3A%20%20%20Multi-Target%20Multi-Camera%20Tracking%20%28MTMCT%29%20has%20broad%20applications%20and%20forms%0Athe%20basis%20for%20numerous%20future%20city-wide%20systems%20%28e.g.%20traffic%20management%2C%20crash%0Adetection%2C%20etc.%29.%20However%2C%20the%20challenge%20of%20matching%20vehicle%20trajectories%0Aacross%20different%20cameras%20based%20solely%20on%20feature%20extraction%20poses%20significant%0Adifficulties.%20This%20article%20introduces%20an%20innovative%20multi-camera%20vehicle%0Atracking%20system%20that%20utilizes%20a%20self-supervised%20camera%20link%20model.%20In%20contrast%0Ato%20related%20works%20that%20rely%20on%20manual%20spatial-temporal%20annotations%2C%20our%20model%0Aautomatically%20extracts%20crucial%20multi-camera%20relationships%20for%20vehicle%20matching.%0AThe%20camera%20link%20is%20established%20through%20a%20pre-matching%20process%20that%20evaluates%0Afeature%20similarities%2C%20pair%20numbers%2C%20and%20time%20variance%20for%20high-quality%20tracks.%0AThis%20process%20calculates%20the%20probability%20of%20spatial%20linkage%20for%20all%20camera%0Acombinations%2C%20selecting%20the%20highest%20scoring%20pairs%20to%20create%20camera%20links.%20Our%0Aapproach%20significantly%20improves%20deployment%20times%20by%20eliminating%20the%20need%20for%0Ahuman%20annotation%2C%20offering%20substantial%20improvements%20in%20efficiency%20and%0Acost-effectiveness%20when%20it%20comes%20to%20real-world%20application.%20This%20pairing%0Aprocess%20supports%20cross%20camera%20matching%20by%20setting%20spatial-temporal%20constraints%2C%0Areducing%20the%20searching%20space%20for%20potential%20vehicle%20matches.%20According%20to%20our%0Aexperimental%20results%2C%20the%20proposed%20method%20achieves%20a%20new%20state-of-the-art%20among%0Aautomatic%20camera-link%20based%20methods%20in%20CityFlow%20V2%20benchmarks%20with%2061.07%25%20IDF1%0AScore.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11345v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCity-Scale%2520Multi-Camera%2520Vehicle%2520Tracking%2520System%2520with%2520Improved%250A%2520%2520Self-Supervised%2520Camera%2520Link%2520Model%26entry.906535625%3DYuqiang%2520Lin%2520and%2520Sam%2520Lockyer%2520and%2520Adrian%2520Evans%2520and%2520Markus%2520Zarbock%2520and%2520Nic%2520Zhang%26entry.1292438233%3D%2520%2520Multi-Target%2520Multi-Camera%2520Tracking%2520%2528MTMCT%2529%2520has%2520broad%2520applications%2520and%2520forms%250Athe%2520basis%2520for%2520numerous%2520future%2520city-wide%2520systems%2520%2528e.g.%2520traffic%2520management%252C%2520crash%250Adetection%252C%2520etc.%2529.%2520However%252C%2520the%2520challenge%2520of%2520matching%2520vehicle%2520trajectories%250Aacross%2520different%2520cameras%2520based%2520solely%2520on%2520feature%2520extraction%2520poses%2520significant%250Adifficulties.%2520This%2520article%2520introduces%2520an%2520innovative%2520multi-camera%2520vehicle%250Atracking%2520system%2520that%2520utilizes%2520a%2520self-supervised%2520camera%2520link%2520model.%2520In%2520contrast%250Ato%2520related%2520works%2520that%2520rely%2520on%2520manual%2520spatial-temporal%2520annotations%252C%2520our%2520model%250Aautomatically%2520extracts%2520crucial%2520multi-camera%2520relationships%2520for%2520vehicle%2520matching.%250AThe%2520camera%2520link%2520is%2520established%2520through%2520a%2520pre-matching%2520process%2520that%2520evaluates%250Afeature%2520similarities%252C%2520pair%2520numbers%252C%2520and%2520time%2520variance%2520for%2520high-quality%2520tracks.%250AThis%2520process%2520calculates%2520the%2520probability%2520of%2520spatial%2520linkage%2520for%2520all%2520camera%250Acombinations%252C%2520selecting%2520the%2520highest%2520scoring%2520pairs%2520to%2520create%2520camera%2520links.%2520Our%250Aapproach%2520significantly%2520improves%2520deployment%2520times%2520by%2520eliminating%2520the%2520need%2520for%250Ahuman%2520annotation%252C%2520offering%2520substantial%2520improvements%2520in%2520efficiency%2520and%250Acost-effectiveness%2520when%2520it%2520comes%2520to%2520real-world%2520application.%2520This%2520pairing%250Aprocess%2520supports%2520cross%2520camera%2520matching%2520by%2520setting%2520spatial-temporal%2520constraints%252C%250Areducing%2520the%2520searching%2520space%2520for%2520potential%2520vehicle%2520matches.%2520According%2520to%2520our%250Aexperimental%2520results%252C%2520the%2520proposed%2520method%2520achieves%2520a%2520new%2520state-of-the-art%2520among%250Aautomatic%2520camera-link%2520based%2520methods%2520in%2520CityFlow%2520V2%2520benchmarks%2520with%252061.07%2525%2520IDF1%250AScore.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11345v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=City-Scale%20Multi-Camera%20Vehicle%20Tracking%20System%20with%20Improved%0A%20%20Self-Supervised%20Camera%20Link%20Model&entry.906535625=Yuqiang%20Lin%20and%20Sam%20Lockyer%20and%20Adrian%20Evans%20and%20Markus%20Zarbock%20and%20Nic%20Zhang&entry.1292438233=%20%20Multi-Target%20Multi-Camera%20Tracking%20%28MTMCT%29%20has%20broad%20applications%20and%20forms%0Athe%20basis%20for%20numerous%20future%20city-wide%20systems%20%28e.g.%20traffic%20management%2C%20crash%0Adetection%2C%20etc.%29.%20However%2C%20the%20challenge%20of%20matching%20vehicle%20trajectories%0Aacross%20different%20cameras%20based%20solely%20on%20feature%20extraction%20poses%20significant%0Adifficulties.%20This%20article%20introduces%20an%20innovative%20multi-camera%20vehicle%0Atracking%20system%20that%20utilizes%20a%20self-supervised%20camera%20link%20model.%20In%20contrast%0Ato%20related%20works%20that%20rely%20on%20manual%20spatial-temporal%20annotations%2C%20our%20model%0Aautomatically%20extracts%20crucial%20multi-camera%20relationships%20for%20vehicle%20matching.%0AThe%20camera%20link%20is%20established%20through%20a%20pre-matching%20process%20that%20evaluates%0Afeature%20similarities%2C%20pair%20numbers%2C%20and%20time%20variance%20for%20high-quality%20tracks.%0AThis%20process%20calculates%20the%20probability%20of%20spatial%20linkage%20for%20all%20camera%0Acombinations%2C%20selecting%20the%20highest%20scoring%20pairs%20to%20create%20camera%20links.%20Our%0Aapproach%20significantly%20improves%20deployment%20times%20by%20eliminating%20the%20need%20for%0Ahuman%20annotation%2C%20offering%20substantial%20improvements%20in%20efficiency%20and%0Acost-effectiveness%20when%20it%20comes%20to%20real-world%20application.%20This%20pairing%0Aprocess%20supports%20cross%20camera%20matching%20by%20setting%20spatial-temporal%20constraints%2C%0Areducing%20the%20searching%20space%20for%20potential%20vehicle%20matches.%20According%20to%20our%0Aexperimental%20results%2C%20the%20proposed%20method%20achieves%20a%20new%20state-of-the-art%20among%0Aautomatic%20camera-link%20based%20methods%20in%20CityFlow%20V2%20benchmarks%20with%2061.07%25%20IDF1%0AScore.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11345v2&entry.124074799=Read"},
{"title": "Plasticine3D: 3D Non-Rigid Editing with Text Guidance by Multi-View\n  Embedding Optimization", "author": "Yige Chen and Teng Hu and Yizhe Tang and Siyuan Chen and Ang Chen and Ran Yi", "abstract": "  With the help of Score Distillation Sampling (SDS) and the rapid development\nof neural 3D representations, some methods have been proposed to perform 3D\nediting such as adding additional geometries, or overwriting textures. However,\ngeneralized 3D non-rigid editing task, which requires changing both the\nstructure (posture or composition) and appearance (texture) of the original\nobject, remains to be challenging in 3D editing field. In this paper, we\npropose Plasticine3D, a novel text-guided fine-grained controlled 3D editing\npipeline that can perform 3D non-rigid editing with large structure\ndeformations. Our work divides the editing process into a geometry editing\nstage and a texture editing stage to achieve separate control of structure and\nappearance. In order to maintain the details of the original object from\ndifferent viewpoints, we propose a Multi-View-Embedding (MVE) Optimization\nstrategy to ensure that the guidance model learns the features of the original\nobject from various viewpoints. For the purpose of fine-grained control, we\npropose Embedding-Fusion (EF) to blend the original characteristics with the\nediting objectives in the embedding space, and control the extent of editing by\nadjusting the fusion rate. Furthermore, in order to address the issue of\ngradual loss of details during the generation process under high editing\nintensity, as well as the problem of insignificant editing effects in some\nscenarios, we propose Score Projection Sampling (SPS) as a replacement of score\ndistillation sampling, which introduces additional optimization phases for\nediting target enhancement and original detail maintenance, leading to better\nediting quality. Extensive experiments demonstrate the effectiveness of our\nmethod on 3D non-rigid editing tasks\n", "link": "http://arxiv.org/abs/2312.10111v2", "date": "2024-07-09", "relevancy": 2.8179, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5708}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5708}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Plasticine3D%3A%203D%20Non-Rigid%20Editing%20with%20Text%20Guidance%20by%20Multi-View%0A%20%20Embedding%20Optimization&body=Title%3A%20Plasticine3D%3A%203D%20Non-Rigid%20Editing%20with%20Text%20Guidance%20by%20Multi-View%0A%20%20Embedding%20Optimization%0AAuthor%3A%20Yige%20Chen%20and%20Teng%20Hu%20and%20Yizhe%20Tang%20and%20Siyuan%20Chen%20and%20Ang%20Chen%20and%20Ran%20Yi%0AAbstract%3A%20%20%20With%20the%20help%20of%20Score%20Distillation%20Sampling%20%28SDS%29%20and%20the%20rapid%20development%0Aof%20neural%203D%20representations%2C%20some%20methods%20have%20been%20proposed%20to%20perform%203D%0Aediting%20such%20as%20adding%20additional%20geometries%2C%20or%20overwriting%20textures.%20However%2C%0Ageneralized%203D%20non-rigid%20editing%20task%2C%20which%20requires%20changing%20both%20the%0Astructure%20%28posture%20or%20composition%29%20and%20appearance%20%28texture%29%20of%20the%20original%0Aobject%2C%20remains%20to%20be%20challenging%20in%203D%20editing%20field.%20In%20this%20paper%2C%20we%0Apropose%20Plasticine3D%2C%20a%20novel%20text-guided%20fine-grained%20controlled%203D%20editing%0Apipeline%20that%20can%20perform%203D%20non-rigid%20editing%20with%20large%20structure%0Adeformations.%20Our%20work%20divides%20the%20editing%20process%20into%20a%20geometry%20editing%0Astage%20and%20a%20texture%20editing%20stage%20to%20achieve%20separate%20control%20of%20structure%20and%0Aappearance.%20In%20order%20to%20maintain%20the%20details%20of%20the%20original%20object%20from%0Adifferent%20viewpoints%2C%20we%20propose%20a%20Multi-View-Embedding%20%28MVE%29%20Optimization%0Astrategy%20to%20ensure%20that%20the%20guidance%20model%20learns%20the%20features%20of%20the%20original%0Aobject%20from%20various%20viewpoints.%20For%20the%20purpose%20of%20fine-grained%20control%2C%20we%0Apropose%20Embedding-Fusion%20%28EF%29%20to%20blend%20the%20original%20characteristics%20with%20the%0Aediting%20objectives%20in%20the%20embedding%20space%2C%20and%20control%20the%20extent%20of%20editing%20by%0Aadjusting%20the%20fusion%20rate.%20Furthermore%2C%20in%20order%20to%20address%20the%20issue%20of%0Agradual%20loss%20of%20details%20during%20the%20generation%20process%20under%20high%20editing%0Aintensity%2C%20as%20well%20as%20the%20problem%20of%20insignificant%20editing%20effects%20in%20some%0Ascenarios%2C%20we%20propose%20Score%20Projection%20Sampling%20%28SPS%29%20as%20a%20replacement%20of%20score%0Adistillation%20sampling%2C%20which%20introduces%20additional%20optimization%20phases%20for%0Aediting%20target%20enhancement%20and%20original%20detail%20maintenance%2C%20leading%20to%20better%0Aediting%20quality.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20on%203D%20non-rigid%20editing%20tasks%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10111v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlasticine3D%253A%25203D%2520Non-Rigid%2520Editing%2520with%2520Text%2520Guidance%2520by%2520Multi-View%250A%2520%2520Embedding%2520Optimization%26entry.906535625%3DYige%2520Chen%2520and%2520Teng%2520Hu%2520and%2520Yizhe%2520Tang%2520and%2520Siyuan%2520Chen%2520and%2520Ang%2520Chen%2520and%2520Ran%2520Yi%26entry.1292438233%3D%2520%2520With%2520the%2520help%2520of%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520and%2520the%2520rapid%2520development%250Aof%2520neural%25203D%2520representations%252C%2520some%2520methods%2520have%2520been%2520proposed%2520to%2520perform%25203D%250Aediting%2520such%2520as%2520adding%2520additional%2520geometries%252C%2520or%2520overwriting%2520textures.%2520However%252C%250Ageneralized%25203D%2520non-rigid%2520editing%2520task%252C%2520which%2520requires%2520changing%2520both%2520the%250Astructure%2520%2528posture%2520or%2520composition%2529%2520and%2520appearance%2520%2528texture%2529%2520of%2520the%2520original%250Aobject%252C%2520remains%2520to%2520be%2520challenging%2520in%25203D%2520editing%2520field.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Plasticine3D%252C%2520a%2520novel%2520text-guided%2520fine-grained%2520controlled%25203D%2520editing%250Apipeline%2520that%2520can%2520perform%25203D%2520non-rigid%2520editing%2520with%2520large%2520structure%250Adeformations.%2520Our%2520work%2520divides%2520the%2520editing%2520process%2520into%2520a%2520geometry%2520editing%250Astage%2520and%2520a%2520texture%2520editing%2520stage%2520to%2520achieve%2520separate%2520control%2520of%2520structure%2520and%250Aappearance.%2520In%2520order%2520to%2520maintain%2520the%2520details%2520of%2520the%2520original%2520object%2520from%250Adifferent%2520viewpoints%252C%2520we%2520propose%2520a%2520Multi-View-Embedding%2520%2528MVE%2529%2520Optimization%250Astrategy%2520to%2520ensure%2520that%2520the%2520guidance%2520model%2520learns%2520the%2520features%2520of%2520the%2520original%250Aobject%2520from%2520various%2520viewpoints.%2520For%2520the%2520purpose%2520of%2520fine-grained%2520control%252C%2520we%250Apropose%2520Embedding-Fusion%2520%2528EF%2529%2520to%2520blend%2520the%2520original%2520characteristics%2520with%2520the%250Aediting%2520objectives%2520in%2520the%2520embedding%2520space%252C%2520and%2520control%2520the%2520extent%2520of%2520editing%2520by%250Aadjusting%2520the%2520fusion%2520rate.%2520Furthermore%252C%2520in%2520order%2520to%2520address%2520the%2520issue%2520of%250Agradual%2520loss%2520of%2520details%2520during%2520the%2520generation%2520process%2520under%2520high%2520editing%250Aintensity%252C%2520as%2520well%2520as%2520the%2520problem%2520of%2520insignificant%2520editing%2520effects%2520in%2520some%250Ascenarios%252C%2520we%2520propose%2520Score%2520Projection%2520Sampling%2520%2528SPS%2529%2520as%2520a%2520replacement%2520of%2520score%250Adistillation%2520sampling%252C%2520which%2520introduces%2520additional%2520optimization%2520phases%2520for%250Aediting%2520target%2520enhancement%2520and%2520original%2520detail%2520maintenance%252C%2520leading%2520to%2520better%250Aediting%2520quality.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod%2520on%25203D%2520non-rigid%2520editing%2520tasks%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10111v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plasticine3D%3A%203D%20Non-Rigid%20Editing%20with%20Text%20Guidance%20by%20Multi-View%0A%20%20Embedding%20Optimization&entry.906535625=Yige%20Chen%20and%20Teng%20Hu%20and%20Yizhe%20Tang%20and%20Siyuan%20Chen%20and%20Ang%20Chen%20and%20Ran%20Yi&entry.1292438233=%20%20With%20the%20help%20of%20Score%20Distillation%20Sampling%20%28SDS%29%20and%20the%20rapid%20development%0Aof%20neural%203D%20representations%2C%20some%20methods%20have%20been%20proposed%20to%20perform%203D%0Aediting%20such%20as%20adding%20additional%20geometries%2C%20or%20overwriting%20textures.%20However%2C%0Ageneralized%203D%20non-rigid%20editing%20task%2C%20which%20requires%20changing%20both%20the%0Astructure%20%28posture%20or%20composition%29%20and%20appearance%20%28texture%29%20of%20the%20original%0Aobject%2C%20remains%20to%20be%20challenging%20in%203D%20editing%20field.%20In%20this%20paper%2C%20we%0Apropose%20Plasticine3D%2C%20a%20novel%20text-guided%20fine-grained%20controlled%203D%20editing%0Apipeline%20that%20can%20perform%203D%20non-rigid%20editing%20with%20large%20structure%0Adeformations.%20Our%20work%20divides%20the%20editing%20process%20into%20a%20geometry%20editing%0Astage%20and%20a%20texture%20editing%20stage%20to%20achieve%20separate%20control%20of%20structure%20and%0Aappearance.%20In%20order%20to%20maintain%20the%20details%20of%20the%20original%20object%20from%0Adifferent%20viewpoints%2C%20we%20propose%20a%20Multi-View-Embedding%20%28MVE%29%20Optimization%0Astrategy%20to%20ensure%20that%20the%20guidance%20model%20learns%20the%20features%20of%20the%20original%0Aobject%20from%20various%20viewpoints.%20For%20the%20purpose%20of%20fine-grained%20control%2C%20we%0Apropose%20Embedding-Fusion%20%28EF%29%20to%20blend%20the%20original%20characteristics%20with%20the%0Aediting%20objectives%20in%20the%20embedding%20space%2C%20and%20control%20the%20extent%20of%20editing%20by%0Aadjusting%20the%20fusion%20rate.%20Furthermore%2C%20in%20order%20to%20address%20the%20issue%20of%0Agradual%20loss%20of%20details%20during%20the%20generation%20process%20under%20high%20editing%0Aintensity%2C%20as%20well%20as%20the%20problem%20of%20insignificant%20editing%20effects%20in%20some%0Ascenarios%2C%20we%20propose%20Score%20Projection%20Sampling%20%28SPS%29%20as%20a%20replacement%20of%20score%0Adistillation%20sampling%2C%20which%20introduces%20additional%20optimization%20phases%20for%0Aediting%20target%20enhancement%20and%20original%20detail%20maintenance%2C%20leading%20to%20better%0Aediting%20quality.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20on%203D%20non-rigid%20editing%20tasks%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10111v2&entry.124074799=Read"},
{"title": "LVLM-empowered Multi-modal Representation Learning for Visual Place\n  Recognition", "author": "Teng Wang and Lingquan Meng and Lei Cheng and Changyin Sun", "abstract": "  Visual place recognition (VPR) remains challenging due to significant\nviewpoint changes and appearance variations. Mainstream works tackle these\nchallenges by developing various feature aggregation methods to transform deep\nfeatures into robust and compact global representations. Unfortunately,\nsatisfactory results cannot be achieved under challenging conditions. We start\nfrom a new perspective and attempt to build a discriminative global\nrepresentations by fusing image data and text descriptions of the the visual\nscene. The motivation is twofold: (1) Current Large Vision-Language Models\n(LVLMs) demonstrate extraordinary emergent capability in visual instruction\nfollowing, and thus provide an efficient and flexible manner in generating text\ndescriptions of images; (2) The text descriptions, which provide high-level\nscene understanding, show strong robustness against environment variations.\nAlthough promising, leveraging LVLMs to build multi-modal VPR solutions remains\nchallenging in efficient multi-modal fusion. Furthermore, LVLMs will inevitably\nproduces some inaccurate descriptions, making it even harder. To tackle these\nchallenges, we propose a novel multi-modal VPR solution. It first adapts\npre-trained visual and language foundation models to VPR for extracting image\nand text features, which are then fed into the feature combiner to enhance each\nother. As the main component, the feature combiner first propose a token-wise\nattention block to adaptively recalibrate text tokens according to their\nrelevance to the image data, and then develop an efficient cross-attention\nfusion module to propagate information across different modalities. The\nenhanced multi-modal features are compressed into the feature descriptor for\nperforming retrieval. Experimental results show that our method outperforms\nstate-of-the-art methods by a large margin with significantly smaller image\ndescriptor dimension.\n", "link": "http://arxiv.org/abs/2407.06730v1", "date": "2024-07-09", "relevancy": 2.7764, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5882}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5446}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LVLM-empowered%20Multi-modal%20Representation%20Learning%20for%20Visual%20Place%0A%20%20Recognition&body=Title%3A%20LVLM-empowered%20Multi-modal%20Representation%20Learning%20for%20Visual%20Place%0A%20%20Recognition%0AAuthor%3A%20Teng%20Wang%20and%20Lingquan%20Meng%20and%20Lei%20Cheng%20and%20Changyin%20Sun%0AAbstract%3A%20%20%20Visual%20place%20recognition%20%28VPR%29%20remains%20challenging%20due%20to%20significant%0Aviewpoint%20changes%20and%20appearance%20variations.%20Mainstream%20works%20tackle%20these%0Achallenges%20by%20developing%20various%20feature%20aggregation%20methods%20to%20transform%20deep%0Afeatures%20into%20robust%20and%20compact%20global%20representations.%20Unfortunately%2C%0Asatisfactory%20results%20cannot%20be%20achieved%20under%20challenging%20conditions.%20We%20start%0Afrom%20a%20new%20perspective%20and%20attempt%20to%20build%20a%20discriminative%20global%0Arepresentations%20by%20fusing%20image%20data%20and%20text%20descriptions%20of%20the%20the%20visual%0Ascene.%20The%20motivation%20is%20twofold%3A%20%281%29%20Current%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20demonstrate%20extraordinary%20emergent%20capability%20in%20visual%20instruction%0Afollowing%2C%20and%20thus%20provide%20an%20efficient%20and%20flexible%20manner%20in%20generating%20text%0Adescriptions%20of%20images%3B%20%282%29%20The%20text%20descriptions%2C%20which%20provide%20high-level%0Ascene%20understanding%2C%20show%20strong%20robustness%20against%20environment%20variations.%0AAlthough%20promising%2C%20leveraging%20LVLMs%20to%20build%20multi-modal%20VPR%20solutions%20remains%0Achallenging%20in%20efficient%20multi-modal%20fusion.%20Furthermore%2C%20LVLMs%20will%20inevitably%0Aproduces%20some%20inaccurate%20descriptions%2C%20making%20it%20even%20harder.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20a%20novel%20multi-modal%20VPR%20solution.%20It%20first%20adapts%0Apre-trained%20visual%20and%20language%20foundation%20models%20to%20VPR%20for%20extracting%20image%0Aand%20text%20features%2C%20which%20are%20then%20fed%20into%20the%20feature%20combiner%20to%20enhance%20each%0Aother.%20As%20the%20main%20component%2C%20the%20feature%20combiner%20first%20propose%20a%20token-wise%0Aattention%20block%20to%20adaptively%20recalibrate%20text%20tokens%20according%20to%20their%0Arelevance%20to%20the%20image%20data%2C%20and%20then%20develop%20an%20efficient%20cross-attention%0Afusion%20module%20to%20propagate%20information%20across%20different%20modalities.%20The%0Aenhanced%20multi-modal%20features%20are%20compressed%20into%20the%20feature%20descriptor%20for%0Aperforming%20retrieval.%20Experimental%20results%20show%20that%20our%20method%20outperforms%0Astate-of-the-art%20methods%20by%20a%20large%20margin%20with%20significantly%20smaller%20image%0Adescriptor%20dimension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLVLM-empowered%2520Multi-modal%2520Representation%2520Learning%2520for%2520Visual%2520Place%250A%2520%2520Recognition%26entry.906535625%3DTeng%2520Wang%2520and%2520Lingquan%2520Meng%2520and%2520Lei%2520Cheng%2520and%2520Changyin%2520Sun%26entry.1292438233%3D%2520%2520Visual%2520place%2520recognition%2520%2528VPR%2529%2520remains%2520challenging%2520due%2520to%2520significant%250Aviewpoint%2520changes%2520and%2520appearance%2520variations.%2520Mainstream%2520works%2520tackle%2520these%250Achallenges%2520by%2520developing%2520various%2520feature%2520aggregation%2520methods%2520to%2520transform%2520deep%250Afeatures%2520into%2520robust%2520and%2520compact%2520global%2520representations.%2520Unfortunately%252C%250Asatisfactory%2520results%2520cannot%2520be%2520achieved%2520under%2520challenging%2520conditions.%2520We%2520start%250Afrom%2520a%2520new%2520perspective%2520and%2520attempt%2520to%2520build%2520a%2520discriminative%2520global%250Arepresentations%2520by%2520fusing%2520image%2520data%2520and%2520text%2520descriptions%2520of%2520the%2520the%2520visual%250Ascene.%2520The%2520motivation%2520is%2520twofold%253A%2520%25281%2529%2520Current%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529%2520demonstrate%2520extraordinary%2520emergent%2520capability%2520in%2520visual%2520instruction%250Afollowing%252C%2520and%2520thus%2520provide%2520an%2520efficient%2520and%2520flexible%2520manner%2520in%2520generating%2520text%250Adescriptions%2520of%2520images%253B%2520%25282%2529%2520The%2520text%2520descriptions%252C%2520which%2520provide%2520high-level%250Ascene%2520understanding%252C%2520show%2520strong%2520robustness%2520against%2520environment%2520variations.%250AAlthough%2520promising%252C%2520leveraging%2520LVLMs%2520to%2520build%2520multi-modal%2520VPR%2520solutions%2520remains%250Achallenging%2520in%2520efficient%2520multi-modal%2520fusion.%2520Furthermore%252C%2520LVLMs%2520will%2520inevitably%250Aproduces%2520some%2520inaccurate%2520descriptions%252C%2520making%2520it%2520even%2520harder.%2520To%2520tackle%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520multi-modal%2520VPR%2520solution.%2520It%2520first%2520adapts%250Apre-trained%2520visual%2520and%2520language%2520foundation%2520models%2520to%2520VPR%2520for%2520extracting%2520image%250Aand%2520text%2520features%252C%2520which%2520are%2520then%2520fed%2520into%2520the%2520feature%2520combiner%2520to%2520enhance%2520each%250Aother.%2520As%2520the%2520main%2520component%252C%2520the%2520feature%2520combiner%2520first%2520propose%2520a%2520token-wise%250Aattention%2520block%2520to%2520adaptively%2520recalibrate%2520text%2520tokens%2520according%2520to%2520their%250Arelevance%2520to%2520the%2520image%2520data%252C%2520and%2520then%2520develop%2520an%2520efficient%2520cross-attention%250Afusion%2520module%2520to%2520propagate%2520information%2520across%2520different%2520modalities.%2520The%250Aenhanced%2520multi-modal%2520features%2520are%2520compressed%2520into%2520the%2520feature%2520descriptor%2520for%250Aperforming%2520retrieval.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520outperforms%250Astate-of-the-art%2520methods%2520by%2520a%2520large%2520margin%2520with%2520significantly%2520smaller%2520image%250Adescriptor%2520dimension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVLM-empowered%20Multi-modal%20Representation%20Learning%20for%20Visual%20Place%0A%20%20Recognition&entry.906535625=Teng%20Wang%20and%20Lingquan%20Meng%20and%20Lei%20Cheng%20and%20Changyin%20Sun&entry.1292438233=%20%20Visual%20place%20recognition%20%28VPR%29%20remains%20challenging%20due%20to%20significant%0Aviewpoint%20changes%20and%20appearance%20variations.%20Mainstream%20works%20tackle%20these%0Achallenges%20by%20developing%20various%20feature%20aggregation%20methods%20to%20transform%20deep%0Afeatures%20into%20robust%20and%20compact%20global%20representations.%20Unfortunately%2C%0Asatisfactory%20results%20cannot%20be%20achieved%20under%20challenging%20conditions.%20We%20start%0Afrom%20a%20new%20perspective%20and%20attempt%20to%20build%20a%20discriminative%20global%0Arepresentations%20by%20fusing%20image%20data%20and%20text%20descriptions%20of%20the%20the%20visual%0Ascene.%20The%20motivation%20is%20twofold%3A%20%281%29%20Current%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20demonstrate%20extraordinary%20emergent%20capability%20in%20visual%20instruction%0Afollowing%2C%20and%20thus%20provide%20an%20efficient%20and%20flexible%20manner%20in%20generating%20text%0Adescriptions%20of%20images%3B%20%282%29%20The%20text%20descriptions%2C%20which%20provide%20high-level%0Ascene%20understanding%2C%20show%20strong%20robustness%20against%20environment%20variations.%0AAlthough%20promising%2C%20leveraging%20LVLMs%20to%20build%20multi-modal%20VPR%20solutions%20remains%0Achallenging%20in%20efficient%20multi-modal%20fusion.%20Furthermore%2C%20LVLMs%20will%20inevitably%0Aproduces%20some%20inaccurate%20descriptions%2C%20making%20it%20even%20harder.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20a%20novel%20multi-modal%20VPR%20solution.%20It%20first%20adapts%0Apre-trained%20visual%20and%20language%20foundation%20models%20to%20VPR%20for%20extracting%20image%0Aand%20text%20features%2C%20which%20are%20then%20fed%20into%20the%20feature%20combiner%20to%20enhance%20each%0Aother.%20As%20the%20main%20component%2C%20the%20feature%20combiner%20first%20propose%20a%20token-wise%0Aattention%20block%20to%20adaptively%20recalibrate%20text%20tokens%20according%20to%20their%0Arelevance%20to%20the%20image%20data%2C%20and%20then%20develop%20an%20efficient%20cross-attention%0Afusion%20module%20to%20propagate%20information%20across%20different%20modalities.%20The%0Aenhanced%20multi-modal%20features%20are%20compressed%20into%20the%20feature%20descriptor%20for%0Aperforming%20retrieval.%20Experimental%20results%20show%20that%20our%20method%20outperforms%0Astate-of-the-art%20methods%20by%20a%20large%20margin%20with%20significantly%20smaller%20image%0Adescriptor%20dimension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06730v1&entry.124074799=Read"},
{"title": "Expanding continual few-shot learning benchmarks to include recognition\n  of specific instances", "author": "Gideon Kowadlo and Abdelrahman Ahmed and Amir Mayan and David Rawlinson", "abstract": "  Continual learning and few-shot learning are important frontiers in progress\ntoward broader Machine Learning (ML) capabilities. Recently, there has been\nintense interest in combining both. One of the first examples to do so was the\nContinual few-shot Learning (CFSL) framework of Antoniou et al.\narXiv:2004.11967. In this study, we extend CFSL in two ways that capture a\nbroader range of challenges, important for intelligent agent behaviour in\nreal-world conditions. First, we increased the number of classes by an order of\nmagnitude, making the results more comparable to standard continual learning\nexperiments. Second, we introduced an 'instance test' which requires\nrecognition of specific instances of classes -- a capability of animal\ncognition that is usually neglected in ML. For an initial exploration of ML\nmodel performance under these conditions, we selected representative baseline\nmodels from the original CFSL work and added a model variant with replay. As\nexpected, learning more classes is more difficult than the original CFSL\nexperiments, and interestingly, the way in which image instances and classes\nare presented affects classification performance. Surprisingly, accuracy in the\nbaseline instance test is comparable to other classification tasks, but poor\ngiven significant occlusion and noise. The use of replay for consolidation\nsubstantially improves performance for both types of tasks, but particularly\nfor the instance test.\n", "link": "http://arxiv.org/abs/2209.07863v4", "date": "2024-07-09", "relevancy": 2.7249, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.557}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5421}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expanding%20continual%20few-shot%20learning%20benchmarks%20to%20include%20recognition%0A%20%20of%20specific%20instances&body=Title%3A%20Expanding%20continual%20few-shot%20learning%20benchmarks%20to%20include%20recognition%0A%20%20of%20specific%20instances%0AAuthor%3A%20Gideon%20Kowadlo%20and%20Abdelrahman%20Ahmed%20and%20Amir%20Mayan%20and%20David%20Rawlinson%0AAbstract%3A%20%20%20Continual%20learning%20and%20few-shot%20learning%20are%20important%20frontiers%20in%20progress%0Atoward%20broader%20Machine%20Learning%20%28ML%29%20capabilities.%20Recently%2C%20there%20has%20been%0Aintense%20interest%20in%20combining%20both.%20One%20of%20the%20first%20examples%20to%20do%20so%20was%20the%0AContinual%20few-shot%20Learning%20%28CFSL%29%20framework%20of%20Antoniou%20et%20al.%0AarXiv%3A2004.11967.%20In%20this%20study%2C%20we%20extend%20CFSL%20in%20two%20ways%20that%20capture%20a%0Abroader%20range%20of%20challenges%2C%20important%20for%20intelligent%20agent%20behaviour%20in%0Areal-world%20conditions.%20First%2C%20we%20increased%20the%20number%20of%20classes%20by%20an%20order%20of%0Amagnitude%2C%20making%20the%20results%20more%20comparable%20to%20standard%20continual%20learning%0Aexperiments.%20Second%2C%20we%20introduced%20an%20%27instance%20test%27%20which%20requires%0Arecognition%20of%20specific%20instances%20of%20classes%20--%20a%20capability%20of%20animal%0Acognition%20that%20is%20usually%20neglected%20in%20ML.%20For%20an%20initial%20exploration%20of%20ML%0Amodel%20performance%20under%20these%20conditions%2C%20we%20selected%20representative%20baseline%0Amodels%20from%20the%20original%20CFSL%20work%20and%20added%20a%20model%20variant%20with%20replay.%20As%0Aexpected%2C%20learning%20more%20classes%20is%20more%20difficult%20than%20the%20original%20CFSL%0Aexperiments%2C%20and%20interestingly%2C%20the%20way%20in%20which%20image%20instances%20and%20classes%0Aare%20presented%20affects%20classification%20performance.%20Surprisingly%2C%20accuracy%20in%20the%0Abaseline%20instance%20test%20is%20comparable%20to%20other%20classification%20tasks%2C%20but%20poor%0Agiven%20significant%20occlusion%20and%20noise.%20The%20use%20of%20replay%20for%20consolidation%0Asubstantially%20improves%20performance%20for%20both%20types%20of%20tasks%2C%20but%20particularly%0Afor%20the%20instance%20test.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.07863v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpanding%2520continual%2520few-shot%2520learning%2520benchmarks%2520to%2520include%2520recognition%250A%2520%2520of%2520specific%2520instances%26entry.906535625%3DGideon%2520Kowadlo%2520and%2520Abdelrahman%2520Ahmed%2520and%2520Amir%2520Mayan%2520and%2520David%2520Rawlinson%26entry.1292438233%3D%2520%2520Continual%2520learning%2520and%2520few-shot%2520learning%2520are%2520important%2520frontiers%2520in%2520progress%250Atoward%2520broader%2520Machine%2520Learning%2520%2528ML%2529%2520capabilities.%2520Recently%252C%2520there%2520has%2520been%250Aintense%2520interest%2520in%2520combining%2520both.%2520One%2520of%2520the%2520first%2520examples%2520to%2520do%2520so%2520was%2520the%250AContinual%2520few-shot%2520Learning%2520%2528CFSL%2529%2520framework%2520of%2520Antoniou%2520et%2520al.%250AarXiv%253A2004.11967.%2520In%2520this%2520study%252C%2520we%2520extend%2520CFSL%2520in%2520two%2520ways%2520that%2520capture%2520a%250Abroader%2520range%2520of%2520challenges%252C%2520important%2520for%2520intelligent%2520agent%2520behaviour%2520in%250Areal-world%2520conditions.%2520First%252C%2520we%2520increased%2520the%2520number%2520of%2520classes%2520by%2520an%2520order%2520of%250Amagnitude%252C%2520making%2520the%2520results%2520more%2520comparable%2520to%2520standard%2520continual%2520learning%250Aexperiments.%2520Second%252C%2520we%2520introduced%2520an%2520%2527instance%2520test%2527%2520which%2520requires%250Arecognition%2520of%2520specific%2520instances%2520of%2520classes%2520--%2520a%2520capability%2520of%2520animal%250Acognition%2520that%2520is%2520usually%2520neglected%2520in%2520ML.%2520For%2520an%2520initial%2520exploration%2520of%2520ML%250Amodel%2520performance%2520under%2520these%2520conditions%252C%2520we%2520selected%2520representative%2520baseline%250Amodels%2520from%2520the%2520original%2520CFSL%2520work%2520and%2520added%2520a%2520model%2520variant%2520with%2520replay.%2520As%250Aexpected%252C%2520learning%2520more%2520classes%2520is%2520more%2520difficult%2520than%2520the%2520original%2520CFSL%250Aexperiments%252C%2520and%2520interestingly%252C%2520the%2520way%2520in%2520which%2520image%2520instances%2520and%2520classes%250Aare%2520presented%2520affects%2520classification%2520performance.%2520Surprisingly%252C%2520accuracy%2520in%2520the%250Abaseline%2520instance%2520test%2520is%2520comparable%2520to%2520other%2520classification%2520tasks%252C%2520but%2520poor%250Agiven%2520significant%2520occlusion%2520and%2520noise.%2520The%2520use%2520of%2520replay%2520for%2520consolidation%250Asubstantially%2520improves%2520performance%2520for%2520both%2520types%2520of%2520tasks%252C%2520but%2520particularly%250Afor%2520the%2520instance%2520test.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.07863v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expanding%20continual%20few-shot%20learning%20benchmarks%20to%20include%20recognition%0A%20%20of%20specific%20instances&entry.906535625=Gideon%20Kowadlo%20and%20Abdelrahman%20Ahmed%20and%20Amir%20Mayan%20and%20David%20Rawlinson&entry.1292438233=%20%20Continual%20learning%20and%20few-shot%20learning%20are%20important%20frontiers%20in%20progress%0Atoward%20broader%20Machine%20Learning%20%28ML%29%20capabilities.%20Recently%2C%20there%20has%20been%0Aintense%20interest%20in%20combining%20both.%20One%20of%20the%20first%20examples%20to%20do%20so%20was%20the%0AContinual%20few-shot%20Learning%20%28CFSL%29%20framework%20of%20Antoniou%20et%20al.%0AarXiv%3A2004.11967.%20In%20this%20study%2C%20we%20extend%20CFSL%20in%20two%20ways%20that%20capture%20a%0Abroader%20range%20of%20challenges%2C%20important%20for%20intelligent%20agent%20behaviour%20in%0Areal-world%20conditions.%20First%2C%20we%20increased%20the%20number%20of%20classes%20by%20an%20order%20of%0Amagnitude%2C%20making%20the%20results%20more%20comparable%20to%20standard%20continual%20learning%0Aexperiments.%20Second%2C%20we%20introduced%20an%20%27instance%20test%27%20which%20requires%0Arecognition%20of%20specific%20instances%20of%20classes%20--%20a%20capability%20of%20animal%0Acognition%20that%20is%20usually%20neglected%20in%20ML.%20For%20an%20initial%20exploration%20of%20ML%0Amodel%20performance%20under%20these%20conditions%2C%20we%20selected%20representative%20baseline%0Amodels%20from%20the%20original%20CFSL%20work%20and%20added%20a%20model%20variant%20with%20replay.%20As%0Aexpected%2C%20learning%20more%20classes%20is%20more%20difficult%20than%20the%20original%20CFSL%0Aexperiments%2C%20and%20interestingly%2C%20the%20way%20in%20which%20image%20instances%20and%20classes%0Aare%20presented%20affects%20classification%20performance.%20Surprisingly%2C%20accuracy%20in%20the%0Abaseline%20instance%20test%20is%20comparable%20to%20other%20classification%20tasks%2C%20but%20poor%0Agiven%20significant%20occlusion%20and%20noise.%20The%20use%20of%20replay%20for%20consolidation%0Asubstantially%20improves%20performance%20for%20both%20types%20of%20tasks%2C%20but%20particularly%0Afor%20the%20instance%20test.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.07863v4&entry.124074799=Read"},
{"title": "Does CLIP Know My Face?", "author": "Dominik Hintersdorf and Lukas Struppek and Manuel Brack and Felix Friedrich and Patrick Schramowski and Kristian Kersting", "abstract": "  With the rise of deep learning in various applications, privacy concerns\naround the protection of training data have become a critical area of research.\nWhereas prior studies have focused on privacy risks in single-modal models, we\nintroduce a novel method to assess privacy for multi-modal models, specifically\nvision-language models like CLIP. The proposed Identity Inference Attack (IDIA)\nreveals whether an individual was included in the training data by querying the\nmodel with images of the same person. Letting the model choose from a wide\nvariety of possible text labels, the model reveals whether it recognizes the\nperson and, therefore, was used for training. Our large-scale experiments on\nCLIP demonstrate that individuals used for training can be identified with very\nhigh accuracy. We confirm that the model has learned to associate names with\ndepicted individuals, implying the existence of sensitive information that can\nbe extracted by adversaries. Our results highlight the need for stronger\nprivacy protection in large-scale models and suggest that IDIAs can be used to\nprove the unauthorized use of data for training and to enforce privacy laws.\n", "link": "http://arxiv.org/abs/2209.07341v4", "date": "2024-07-09", "relevancy": 2.7128, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5461}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5427}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20CLIP%20Know%20My%20Face%3F&body=Title%3A%20Does%20CLIP%20Know%20My%20Face%3F%0AAuthor%3A%20Dominik%20Hintersdorf%20and%20Lukas%20Struppek%20and%20Manuel%20Brack%20and%20Felix%20Friedrich%20and%20Patrick%20Schramowski%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20With%20the%20rise%20of%20deep%20learning%20in%20various%20applications%2C%20privacy%20concerns%0Aaround%20the%20protection%20of%20training%20data%20have%20become%20a%20critical%20area%20of%20research.%0AWhereas%20prior%20studies%20have%20focused%20on%20privacy%20risks%20in%20single-modal%20models%2C%20we%0Aintroduce%20a%20novel%20method%20to%20assess%20privacy%20for%20multi-modal%20models%2C%20specifically%0Avision-language%20models%20like%20CLIP.%20The%20proposed%20Identity%20Inference%20Attack%20%28IDIA%29%0Areveals%20whether%20an%20individual%20was%20included%20in%20the%20training%20data%20by%20querying%20the%0Amodel%20with%20images%20of%20the%20same%20person.%20Letting%20the%20model%20choose%20from%20a%20wide%0Avariety%20of%20possible%20text%20labels%2C%20the%20model%20reveals%20whether%20it%20recognizes%20the%0Aperson%20and%2C%20therefore%2C%20was%20used%20for%20training.%20Our%20large-scale%20experiments%20on%0ACLIP%20demonstrate%20that%20individuals%20used%20for%20training%20can%20be%20identified%20with%20very%0Ahigh%20accuracy.%20We%20confirm%20that%20the%20model%20has%20learned%20to%20associate%20names%20with%0Adepicted%20individuals%2C%20implying%20the%20existence%20of%20sensitive%20information%20that%20can%0Abe%20extracted%20by%20adversaries.%20Our%20results%20highlight%20the%20need%20for%20stronger%0Aprivacy%20protection%20in%20large-scale%20models%20and%20suggest%20that%20IDIAs%20can%20be%20used%20to%0Aprove%20the%20unauthorized%20use%20of%20data%20for%20training%20and%20to%20enforce%20privacy%20laws.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.07341v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520CLIP%2520Know%2520My%2520Face%253F%26entry.906535625%3DDominik%2520Hintersdorf%2520and%2520Lukas%2520Struppek%2520and%2520Manuel%2520Brack%2520and%2520Felix%2520Friedrich%2520and%2520Patrick%2520Schramowski%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520With%2520the%2520rise%2520of%2520deep%2520learning%2520in%2520various%2520applications%252C%2520privacy%2520concerns%250Aaround%2520the%2520protection%2520of%2520training%2520data%2520have%2520become%2520a%2520critical%2520area%2520of%2520research.%250AWhereas%2520prior%2520studies%2520have%2520focused%2520on%2520privacy%2520risks%2520in%2520single-modal%2520models%252C%2520we%250Aintroduce%2520a%2520novel%2520method%2520to%2520assess%2520privacy%2520for%2520multi-modal%2520models%252C%2520specifically%250Avision-language%2520models%2520like%2520CLIP.%2520The%2520proposed%2520Identity%2520Inference%2520Attack%2520%2528IDIA%2529%250Areveals%2520whether%2520an%2520individual%2520was%2520included%2520in%2520the%2520training%2520data%2520by%2520querying%2520the%250Amodel%2520with%2520images%2520of%2520the%2520same%2520person.%2520Letting%2520the%2520model%2520choose%2520from%2520a%2520wide%250Avariety%2520of%2520possible%2520text%2520labels%252C%2520the%2520model%2520reveals%2520whether%2520it%2520recognizes%2520the%250Aperson%2520and%252C%2520therefore%252C%2520was%2520used%2520for%2520training.%2520Our%2520large-scale%2520experiments%2520on%250ACLIP%2520demonstrate%2520that%2520individuals%2520used%2520for%2520training%2520can%2520be%2520identified%2520with%2520very%250Ahigh%2520accuracy.%2520We%2520confirm%2520that%2520the%2520model%2520has%2520learned%2520to%2520associate%2520names%2520with%250Adepicted%2520individuals%252C%2520implying%2520the%2520existence%2520of%2520sensitive%2520information%2520that%2520can%250Abe%2520extracted%2520by%2520adversaries.%2520Our%2520results%2520highlight%2520the%2520need%2520for%2520stronger%250Aprivacy%2520protection%2520in%2520large-scale%2520models%2520and%2520suggest%2520that%2520IDIAs%2520can%2520be%2520used%2520to%250Aprove%2520the%2520unauthorized%2520use%2520of%2520data%2520for%2520training%2520and%2520to%2520enforce%2520privacy%2520laws.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.07341v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20CLIP%20Know%20My%20Face%3F&entry.906535625=Dominik%20Hintersdorf%20and%20Lukas%20Struppek%20and%20Manuel%20Brack%20and%20Felix%20Friedrich%20and%20Patrick%20Schramowski%20and%20Kristian%20Kersting&entry.1292438233=%20%20With%20the%20rise%20of%20deep%20learning%20in%20various%20applications%2C%20privacy%20concerns%0Aaround%20the%20protection%20of%20training%20data%20have%20become%20a%20critical%20area%20of%20research.%0AWhereas%20prior%20studies%20have%20focused%20on%20privacy%20risks%20in%20single-modal%20models%2C%20we%0Aintroduce%20a%20novel%20method%20to%20assess%20privacy%20for%20multi-modal%20models%2C%20specifically%0Avision-language%20models%20like%20CLIP.%20The%20proposed%20Identity%20Inference%20Attack%20%28IDIA%29%0Areveals%20whether%20an%20individual%20was%20included%20in%20the%20training%20data%20by%20querying%20the%0Amodel%20with%20images%20of%20the%20same%20person.%20Letting%20the%20model%20choose%20from%20a%20wide%0Avariety%20of%20possible%20text%20labels%2C%20the%20model%20reveals%20whether%20it%20recognizes%20the%0Aperson%20and%2C%20therefore%2C%20was%20used%20for%20training.%20Our%20large-scale%20experiments%20on%0ACLIP%20demonstrate%20that%20individuals%20used%20for%20training%20can%20be%20identified%20with%20very%0Ahigh%20accuracy.%20We%20confirm%20that%20the%20model%20has%20learned%20to%20associate%20names%20with%0Adepicted%20individuals%2C%20implying%20the%20existence%20of%20sensitive%20information%20that%20can%0Abe%20extracted%20by%20adversaries.%20Our%20results%20highlight%20the%20need%20for%20stronger%0Aprivacy%20protection%20in%20large-scale%20models%20and%20suggest%20that%20IDIAs%20can%20be%20used%20to%0Aprove%20the%20unauthorized%20use%20of%20data%20for%20training%20and%20to%20enforce%20privacy%20laws.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.07341v4&entry.124074799=Read"},
{"title": "Window-to-Window BEV Representation Learning for Limited FoV Cross-View\n  Geo-localization", "author": "Lei Cheng and Teng Wang and Lingquan Meng and Changyin Sun", "abstract": "  Cross-view geo-localization confronts significant challenges due to large\nperspective changes, especially when the ground-view query image has a limited\nfield of view with unknown orientation. To bridge the cross-view domain gap, we\nfor the first time explore to learn a BEV representation directly from the\nground query image. However, the unknown orientation between ground and aerial\nimages combined with the absence of camera parameters led to ambiguity between\nBEV queries and ground references. To tackle this challenge, we propose a novel\nWindow-to-Window BEV representation learning method, termed W2W-BEV, which\nadaptively matches BEV queries to ground reference at window-scale.\nSpecifically, predefined BEV embeddings and extracted ground features are\nsegmented into a fixed number of windows, and then most similar ground window\nis chosen for each BEV feature based on the context-aware window matching\nstrategy. Subsequently, the cross-attention is performed between the matched\nBEV and ground windows to learn the robust BEV representation. Additionally, we\nuse ground features along with predicted depth information to initialize the\nBEV embeddings, helping learn more powerful BEV representations. Extensive\nexperimental results on benchmark datasets demonstrate significant superiority\nof our W2W-BEV over previous state-of-the-art methods under challenging\nconditions of unknown orientation and limited FoV. Specifically, on the CVUSA\ndataset with limited Fov of 90 degree and unknown orientation, the W2W-BEV\nachieve an significant improvement from 47.24% to 64.73 %(+17.49%) in R@1\naccuracy.\n", "link": "http://arxiv.org/abs/2407.06861v1", "date": "2024-07-09", "relevancy": 2.6945, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5519}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Window-to-Window%20BEV%20Representation%20Learning%20for%20Limited%20FoV%20Cross-View%0A%20%20Geo-localization&body=Title%3A%20Window-to-Window%20BEV%20Representation%20Learning%20for%20Limited%20FoV%20Cross-View%0A%20%20Geo-localization%0AAuthor%3A%20Lei%20Cheng%20and%20Teng%20Wang%20and%20Lingquan%20Meng%20and%20Changyin%20Sun%0AAbstract%3A%20%20%20Cross-view%20geo-localization%20confronts%20significant%20challenges%20due%20to%20large%0Aperspective%20changes%2C%20especially%20when%20the%20ground-view%20query%20image%20has%20a%20limited%0Afield%20of%20view%20with%20unknown%20orientation.%20To%20bridge%20the%20cross-view%20domain%20gap%2C%20we%0Afor%20the%20first%20time%20explore%20to%20learn%20a%20BEV%20representation%20directly%20from%20the%0Aground%20query%20image.%20However%2C%20the%20unknown%20orientation%20between%20ground%20and%20aerial%0Aimages%20combined%20with%20the%20absence%20of%20camera%20parameters%20led%20to%20ambiguity%20between%0ABEV%20queries%20and%20ground%20references.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20novel%0AWindow-to-Window%20BEV%20representation%20learning%20method%2C%20termed%20W2W-BEV%2C%20which%0Aadaptively%20matches%20BEV%20queries%20to%20ground%20reference%20at%20window-scale.%0ASpecifically%2C%20predefined%20BEV%20embeddings%20and%20extracted%20ground%20features%20are%0Asegmented%20into%20a%20fixed%20number%20of%20windows%2C%20and%20then%20most%20similar%20ground%20window%0Ais%20chosen%20for%20each%20BEV%20feature%20based%20on%20the%20context-aware%20window%20matching%0Astrategy.%20Subsequently%2C%20the%20cross-attention%20is%20performed%20between%20the%20matched%0ABEV%20and%20ground%20windows%20to%20learn%20the%20robust%20BEV%20representation.%20Additionally%2C%20we%0Ause%20ground%20features%20along%20with%20predicted%20depth%20information%20to%20initialize%20the%0ABEV%20embeddings%2C%20helping%20learn%20more%20powerful%20BEV%20representations.%20Extensive%0Aexperimental%20results%20on%20benchmark%20datasets%20demonstrate%20significant%20superiority%0Aof%20our%20W2W-BEV%20over%20previous%20state-of-the-art%20methods%20under%20challenging%0Aconditions%20of%20unknown%20orientation%20and%20limited%20FoV.%20Specifically%2C%20on%20the%20CVUSA%0Adataset%20with%20limited%20Fov%20of%2090%20degree%20and%20unknown%20orientation%2C%20the%20W2W-BEV%0Aachieve%20an%20significant%20improvement%20from%2047.24%25%20to%2064.73%20%25%28%2B17.49%25%29%20in%20R%401%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWindow-to-Window%2520BEV%2520Representation%2520Learning%2520for%2520Limited%2520FoV%2520Cross-View%250A%2520%2520Geo-localization%26entry.906535625%3DLei%2520Cheng%2520and%2520Teng%2520Wang%2520and%2520Lingquan%2520Meng%2520and%2520Changyin%2520Sun%26entry.1292438233%3D%2520%2520Cross-view%2520geo-localization%2520confronts%2520significant%2520challenges%2520due%2520to%2520large%250Aperspective%2520changes%252C%2520especially%2520when%2520the%2520ground-view%2520query%2520image%2520has%2520a%2520limited%250Afield%2520of%2520view%2520with%2520unknown%2520orientation.%2520To%2520bridge%2520the%2520cross-view%2520domain%2520gap%252C%2520we%250Afor%2520the%2520first%2520time%2520explore%2520to%2520learn%2520a%2520BEV%2520representation%2520directly%2520from%2520the%250Aground%2520query%2520image.%2520However%252C%2520the%2520unknown%2520orientation%2520between%2520ground%2520and%2520aerial%250Aimages%2520combined%2520with%2520the%2520absence%2520of%2520camera%2520parameters%2520led%2520to%2520ambiguity%2520between%250ABEV%2520queries%2520and%2520ground%2520references.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%250AWindow-to-Window%2520BEV%2520representation%2520learning%2520method%252C%2520termed%2520W2W-BEV%252C%2520which%250Aadaptively%2520matches%2520BEV%2520queries%2520to%2520ground%2520reference%2520at%2520window-scale.%250ASpecifically%252C%2520predefined%2520BEV%2520embeddings%2520and%2520extracted%2520ground%2520features%2520are%250Asegmented%2520into%2520a%2520fixed%2520number%2520of%2520windows%252C%2520and%2520then%2520most%2520similar%2520ground%2520window%250Ais%2520chosen%2520for%2520each%2520BEV%2520feature%2520based%2520on%2520the%2520context-aware%2520window%2520matching%250Astrategy.%2520Subsequently%252C%2520the%2520cross-attention%2520is%2520performed%2520between%2520the%2520matched%250ABEV%2520and%2520ground%2520windows%2520to%2520learn%2520the%2520robust%2520BEV%2520representation.%2520Additionally%252C%2520we%250Ause%2520ground%2520features%2520along%2520with%2520predicted%2520depth%2520information%2520to%2520initialize%2520the%250ABEV%2520embeddings%252C%2520helping%2520learn%2520more%2520powerful%2520BEV%2520representations.%2520Extensive%250Aexperimental%2520results%2520on%2520benchmark%2520datasets%2520demonstrate%2520significant%2520superiority%250Aof%2520our%2520W2W-BEV%2520over%2520previous%2520state-of-the-art%2520methods%2520under%2520challenging%250Aconditions%2520of%2520unknown%2520orientation%2520and%2520limited%2520FoV.%2520Specifically%252C%2520on%2520the%2520CVUSA%250Adataset%2520with%2520limited%2520Fov%2520of%252090%2520degree%2520and%2520unknown%2520orientation%252C%2520the%2520W2W-BEV%250Aachieve%2520an%2520significant%2520improvement%2520from%252047.24%2525%2520to%252064.73%2520%2525%2528%252B17.49%2525%2529%2520in%2520R%25401%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Window-to-Window%20BEV%20Representation%20Learning%20for%20Limited%20FoV%20Cross-View%0A%20%20Geo-localization&entry.906535625=Lei%20Cheng%20and%20Teng%20Wang%20and%20Lingquan%20Meng%20and%20Changyin%20Sun&entry.1292438233=%20%20Cross-view%20geo-localization%20confronts%20significant%20challenges%20due%20to%20large%0Aperspective%20changes%2C%20especially%20when%20the%20ground-view%20query%20image%20has%20a%20limited%0Afield%20of%20view%20with%20unknown%20orientation.%20To%20bridge%20the%20cross-view%20domain%20gap%2C%20we%0Afor%20the%20first%20time%20explore%20to%20learn%20a%20BEV%20representation%20directly%20from%20the%0Aground%20query%20image.%20However%2C%20the%20unknown%20orientation%20between%20ground%20and%20aerial%0Aimages%20combined%20with%20the%20absence%20of%20camera%20parameters%20led%20to%20ambiguity%20between%0ABEV%20queries%20and%20ground%20references.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20novel%0AWindow-to-Window%20BEV%20representation%20learning%20method%2C%20termed%20W2W-BEV%2C%20which%0Aadaptively%20matches%20BEV%20queries%20to%20ground%20reference%20at%20window-scale.%0ASpecifically%2C%20predefined%20BEV%20embeddings%20and%20extracted%20ground%20features%20are%0Asegmented%20into%20a%20fixed%20number%20of%20windows%2C%20and%20then%20most%20similar%20ground%20window%0Ais%20chosen%20for%20each%20BEV%20feature%20based%20on%20the%20context-aware%20window%20matching%0Astrategy.%20Subsequently%2C%20the%20cross-attention%20is%20performed%20between%20the%20matched%0ABEV%20and%20ground%20windows%20to%20learn%20the%20robust%20BEV%20representation.%20Additionally%2C%20we%0Ause%20ground%20features%20along%20with%20predicted%20depth%20information%20to%20initialize%20the%0ABEV%20embeddings%2C%20helping%20learn%20more%20powerful%20BEV%20representations.%20Extensive%0Aexperimental%20results%20on%20benchmark%20datasets%20demonstrate%20significant%20superiority%0Aof%20our%20W2W-BEV%20over%20previous%20state-of-the-art%20methods%20under%20challenging%0Aconditions%20of%20unknown%20orientation%20and%20limited%20FoV.%20Specifically%2C%20on%20the%20CVUSA%0Adataset%20with%20limited%20Fov%20of%2090%20degree%20and%20unknown%20orientation%2C%20the%20W2W-BEV%0Aachieve%20an%20significant%20improvement%20from%2047.24%25%20to%2064.73%20%25%28%2B17.49%25%29%20in%20R%401%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06861v1&entry.124074799=Read"},
{"title": "High-Quality Medical Image Generation from Free-hand Sketch", "author": "Quan Huu Cap and Atsushi Fukuda", "abstract": "  Generating medical images from human-drawn free-hand sketches holds promise\nfor various important medical imaging applications. Due to the extreme\ndifficulty in collecting free-hand sketch data in the medical domain, most deep\nlearning-based methods have been proposed to generate medical images from the\nsynthesized sketches (e.g., edge maps or contours of segmentation masks from\nreal images). However, these models often fail to generalize on the free-hand\nsketches, leading to unsatisfactory results. In this paper, we propose a\npractical free-hand sketch-to-image generation model called Sketch2MedI that\nlearns to represent sketches in StyleGAN's latent space and generate medical\nimages from it. Thanks to the ability to encode sketches into this meaningful\nrepresentation space, Sketch2MedI only requires synthesized sketches for\ntraining, enabling a cost-effective learning process. Our Sketch2MedI\ndemonstrates a robust generalization to free-hand sketches, resulting in\nhigh-quality and realistic medical image generations. Comparative evaluations\nof Sketch2MedI against the pix2pix, CycleGAN, UNIT, and U-GAT-IT models show\nsuperior performance in generating pharyngeal images, both quantitative and\nqualitative across various metrics.\n", "link": "http://arxiv.org/abs/2402.00353v2", "date": "2024-07-09", "relevancy": 2.61, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5224}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5222}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Quality%20Medical%20Image%20Generation%20from%20Free-hand%20Sketch&body=Title%3A%20High-Quality%20Medical%20Image%20Generation%20from%20Free-hand%20Sketch%0AAuthor%3A%20Quan%20Huu%20Cap%20and%20Atsushi%20Fukuda%0AAbstract%3A%20%20%20Generating%20medical%20images%20from%20human-drawn%20free-hand%20sketches%20holds%20promise%0Afor%20various%20important%20medical%20imaging%20applications.%20Due%20to%20the%20extreme%0Adifficulty%20in%20collecting%20free-hand%20sketch%20data%20in%20the%20medical%20domain%2C%20most%20deep%0Alearning-based%20methods%20have%20been%20proposed%20to%20generate%20medical%20images%20from%20the%0Asynthesized%20sketches%20%28e.g.%2C%20edge%20maps%20or%20contours%20of%20segmentation%20masks%20from%0Areal%20images%29.%20However%2C%20these%20models%20often%20fail%20to%20generalize%20on%20the%20free-hand%0Asketches%2C%20leading%20to%20unsatisfactory%20results.%20In%20this%20paper%2C%20we%20propose%20a%0Apractical%20free-hand%20sketch-to-image%20generation%20model%20called%20Sketch2MedI%20that%0Alearns%20to%20represent%20sketches%20in%20StyleGAN%27s%20latent%20space%20and%20generate%20medical%0Aimages%20from%20it.%20Thanks%20to%20the%20ability%20to%20encode%20sketches%20into%20this%20meaningful%0Arepresentation%20space%2C%20Sketch2MedI%20only%20requires%20synthesized%20sketches%20for%0Atraining%2C%20enabling%20a%20cost-effective%20learning%20process.%20Our%20Sketch2MedI%0Ademonstrates%20a%20robust%20generalization%20to%20free-hand%20sketches%2C%20resulting%20in%0Ahigh-quality%20and%20realistic%20medical%20image%20generations.%20Comparative%20evaluations%0Aof%20Sketch2MedI%20against%20the%20pix2pix%2C%20CycleGAN%2C%20UNIT%2C%20and%20U-GAT-IT%20models%20show%0Asuperior%20performance%20in%20generating%20pharyngeal%20images%2C%20both%20quantitative%20and%0Aqualitative%20across%20various%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00353v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Quality%2520Medical%2520Image%2520Generation%2520from%2520Free-hand%2520Sketch%26entry.906535625%3DQuan%2520Huu%2520Cap%2520and%2520Atsushi%2520Fukuda%26entry.1292438233%3D%2520%2520Generating%2520medical%2520images%2520from%2520human-drawn%2520free-hand%2520sketches%2520holds%2520promise%250Afor%2520various%2520important%2520medical%2520imaging%2520applications.%2520Due%2520to%2520the%2520extreme%250Adifficulty%2520in%2520collecting%2520free-hand%2520sketch%2520data%2520in%2520the%2520medical%2520domain%252C%2520most%2520deep%250Alearning-based%2520methods%2520have%2520been%2520proposed%2520to%2520generate%2520medical%2520images%2520from%2520the%250Asynthesized%2520sketches%2520%2528e.g.%252C%2520edge%2520maps%2520or%2520contours%2520of%2520segmentation%2520masks%2520from%250Areal%2520images%2529.%2520However%252C%2520these%2520models%2520often%2520fail%2520to%2520generalize%2520on%2520the%2520free-hand%250Asketches%252C%2520leading%2520to%2520unsatisfactory%2520results.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Apractical%2520free-hand%2520sketch-to-image%2520generation%2520model%2520called%2520Sketch2MedI%2520that%250Alearns%2520to%2520represent%2520sketches%2520in%2520StyleGAN%2527s%2520latent%2520space%2520and%2520generate%2520medical%250Aimages%2520from%2520it.%2520Thanks%2520to%2520the%2520ability%2520to%2520encode%2520sketches%2520into%2520this%2520meaningful%250Arepresentation%2520space%252C%2520Sketch2MedI%2520only%2520requires%2520synthesized%2520sketches%2520for%250Atraining%252C%2520enabling%2520a%2520cost-effective%2520learning%2520process.%2520Our%2520Sketch2MedI%250Ademonstrates%2520a%2520robust%2520generalization%2520to%2520free-hand%2520sketches%252C%2520resulting%2520in%250Ahigh-quality%2520and%2520realistic%2520medical%2520image%2520generations.%2520Comparative%2520evaluations%250Aof%2520Sketch2MedI%2520against%2520the%2520pix2pix%252C%2520CycleGAN%252C%2520UNIT%252C%2520and%2520U-GAT-IT%2520models%2520show%250Asuperior%2520performance%2520in%2520generating%2520pharyngeal%2520images%252C%2520both%2520quantitative%2520and%250Aqualitative%2520across%2520various%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00353v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Quality%20Medical%20Image%20Generation%20from%20Free-hand%20Sketch&entry.906535625=Quan%20Huu%20Cap%20and%20Atsushi%20Fukuda&entry.1292438233=%20%20Generating%20medical%20images%20from%20human-drawn%20free-hand%20sketches%20holds%20promise%0Afor%20various%20important%20medical%20imaging%20applications.%20Due%20to%20the%20extreme%0Adifficulty%20in%20collecting%20free-hand%20sketch%20data%20in%20the%20medical%20domain%2C%20most%20deep%0Alearning-based%20methods%20have%20been%20proposed%20to%20generate%20medical%20images%20from%20the%0Asynthesized%20sketches%20%28e.g.%2C%20edge%20maps%20or%20contours%20of%20segmentation%20masks%20from%0Areal%20images%29.%20However%2C%20these%20models%20often%20fail%20to%20generalize%20on%20the%20free-hand%0Asketches%2C%20leading%20to%20unsatisfactory%20results.%20In%20this%20paper%2C%20we%20propose%20a%0Apractical%20free-hand%20sketch-to-image%20generation%20model%20called%20Sketch2MedI%20that%0Alearns%20to%20represent%20sketches%20in%20StyleGAN%27s%20latent%20space%20and%20generate%20medical%0Aimages%20from%20it.%20Thanks%20to%20the%20ability%20to%20encode%20sketches%20into%20this%20meaningful%0Arepresentation%20space%2C%20Sketch2MedI%20only%20requires%20synthesized%20sketches%20for%0Atraining%2C%20enabling%20a%20cost-effective%20learning%20process.%20Our%20Sketch2MedI%0Ademonstrates%20a%20robust%20generalization%20to%20free-hand%20sketches%2C%20resulting%20in%0Ahigh-quality%20and%20realistic%20medical%20image%20generations.%20Comparative%20evaluations%0Aof%20Sketch2MedI%20against%20the%20pix2pix%2C%20CycleGAN%2C%20UNIT%2C%20and%20U-GAT-IT%20models%20show%0Asuperior%20performance%20in%20generating%20pharyngeal%20images%2C%20both%20quantitative%20and%0Aqualitative%20across%20various%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00353v2&entry.124074799=Read"},
{"title": "Chat-Edit-3D: Interactive 3D Scene Editing via Text Prompts", "author": "Shuangkang Fang and Yufeng Wang and Yi-Hsuan Tsai and Yi Yang and Wenrui Ding and Shuchang Zhou and Ming-Hsuan Yang", "abstract": "  Recent work on image content manipulation based on vision-language\npre-training models has been effectively extended to text-driven 3D scene\nediting. However, existing schemes for 3D scene editing still exhibit certain\nshortcomings, hindering their further interactive design. Such schemes\ntypically adhere to fixed input patterns, limiting users' flexibility in text\ninput. Moreover, their editing capabilities are constrained by a single or a\nfew 2D visual models and require intricate pipeline design to integrate these\nmodels into 3D reconstruction processes. To address the aforementioned issues,\nwe propose a dialogue-based 3D scene editing approach, termed CE3D, which is\ncentered around a large language model that allows for arbitrary textual input\nfrom users and interprets their intentions, subsequently facilitating the\nautonomous invocation of the corresponding visual expert models. Furthermore,\nwe design a scheme utilizing Hash-Atlas to represent 3D scene views, which\ntransfers the editing of 3D scenes onto 2D atlas images. This design achieves\ncomplete decoupling between the 2D editing and 3D reconstruction processes,\nenabling CE3D to flexibly integrate a wide range of existing 2D or 3D visual\nmodels without necessitating intricate fusion designs. Experimental results\ndemonstrate that CE3D effectively integrates multiple visual models to achieve\ndiverse editing visual effects, possessing strong scene comprehension and\nmulti-round dialog capabilities. Code is available at <a\nhref=\"https://sk-fun.fun/CE3D\"> this https URL.</a>\n", "link": "http://arxiv.org/abs/2407.06842v1", "date": "2024-07-09", "relevancy": 2.5938, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6578}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6578}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chat-Edit-3D%3A%20Interactive%203D%20Scene%20Editing%20via%20Text%20Prompts&body=Title%3A%20Chat-Edit-3D%3A%20Interactive%203D%20Scene%20Editing%20via%20Text%20Prompts%0AAuthor%3A%20Shuangkang%20Fang%20and%20Yufeng%20Wang%20and%20Yi-Hsuan%20Tsai%20and%20Yi%20Yang%20and%20Wenrui%20Ding%20and%20Shuchang%20Zhou%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Recent%20work%20on%20image%20content%20manipulation%20based%20on%20vision-language%0Apre-training%20models%20has%20been%20effectively%20extended%20to%20text-driven%203D%20scene%0Aediting.%20However%2C%20existing%20schemes%20for%203D%20scene%20editing%20still%20exhibit%20certain%0Ashortcomings%2C%20hindering%20their%20further%20interactive%20design.%20Such%20schemes%0Atypically%20adhere%20to%20fixed%20input%20patterns%2C%20limiting%20users%27%20flexibility%20in%20text%0Ainput.%20Moreover%2C%20their%20editing%20capabilities%20are%20constrained%20by%20a%20single%20or%20a%0Afew%202D%20visual%20models%20and%20require%20intricate%20pipeline%20design%20to%20integrate%20these%0Amodels%20into%203D%20reconstruction%20processes.%20To%20address%20the%20aforementioned%20issues%2C%0Awe%20propose%20a%20dialogue-based%203D%20scene%20editing%20approach%2C%20termed%20CE3D%2C%20which%20is%0Acentered%20around%20a%20large%20language%20model%20that%20allows%20for%20arbitrary%20textual%20input%0Afrom%20users%20and%20interprets%20their%20intentions%2C%20subsequently%20facilitating%20the%0Aautonomous%20invocation%20of%20the%20corresponding%20visual%20expert%20models.%20Furthermore%2C%0Awe%20design%20a%20scheme%20utilizing%20Hash-Atlas%20to%20represent%203D%20scene%20views%2C%20which%0Atransfers%20the%20editing%20of%203D%20scenes%20onto%202D%20atlas%20images.%20This%20design%20achieves%0Acomplete%20decoupling%20between%20the%202D%20editing%20and%203D%20reconstruction%20processes%2C%0Aenabling%20CE3D%20to%20flexibly%20integrate%20a%20wide%20range%20of%20existing%202D%20or%203D%20visual%0Amodels%20without%20necessitating%20intricate%20fusion%20designs.%20Experimental%20results%0Ademonstrate%20that%20CE3D%20effectively%20integrates%20multiple%20visual%20models%20to%20achieve%0Adiverse%20editing%20visual%20effects%2C%20possessing%20strong%20scene%20comprehension%20and%0Amulti-round%20dialog%20capabilities.%20Code%20is%20available%20at%20%3Ca%0Ahref%3D%22https%3A//sk-fun.fun/CE3D%22%3E%20this%20https%20URL.%3C/a%3E%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChat-Edit-3D%253A%2520Interactive%25203D%2520Scene%2520Editing%2520via%2520Text%2520Prompts%26entry.906535625%3DShuangkang%2520Fang%2520and%2520Yufeng%2520Wang%2520and%2520Yi-Hsuan%2520Tsai%2520and%2520Yi%2520Yang%2520and%2520Wenrui%2520Ding%2520and%2520Shuchang%2520Zhou%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520work%2520on%2520image%2520content%2520manipulation%2520based%2520on%2520vision-language%250Apre-training%2520models%2520has%2520been%2520effectively%2520extended%2520to%2520text-driven%25203D%2520scene%250Aediting.%2520However%252C%2520existing%2520schemes%2520for%25203D%2520scene%2520editing%2520still%2520exhibit%2520certain%250Ashortcomings%252C%2520hindering%2520their%2520further%2520interactive%2520design.%2520Such%2520schemes%250Atypically%2520adhere%2520to%2520fixed%2520input%2520patterns%252C%2520limiting%2520users%2527%2520flexibility%2520in%2520text%250Ainput.%2520Moreover%252C%2520their%2520editing%2520capabilities%2520are%2520constrained%2520by%2520a%2520single%2520or%2520a%250Afew%25202D%2520visual%2520models%2520and%2520require%2520intricate%2520pipeline%2520design%2520to%2520integrate%2520these%250Amodels%2520into%25203D%2520reconstruction%2520processes.%2520To%2520address%2520the%2520aforementioned%2520issues%252C%250Awe%2520propose%2520a%2520dialogue-based%25203D%2520scene%2520editing%2520approach%252C%2520termed%2520CE3D%252C%2520which%2520is%250Acentered%2520around%2520a%2520large%2520language%2520model%2520that%2520allows%2520for%2520arbitrary%2520textual%2520input%250Afrom%2520users%2520and%2520interprets%2520their%2520intentions%252C%2520subsequently%2520facilitating%2520the%250Aautonomous%2520invocation%2520of%2520the%2520corresponding%2520visual%2520expert%2520models.%2520Furthermore%252C%250Awe%2520design%2520a%2520scheme%2520utilizing%2520Hash-Atlas%2520to%2520represent%25203D%2520scene%2520views%252C%2520which%250Atransfers%2520the%2520editing%2520of%25203D%2520scenes%2520onto%25202D%2520atlas%2520images.%2520This%2520design%2520achieves%250Acomplete%2520decoupling%2520between%2520the%25202D%2520editing%2520and%25203D%2520reconstruction%2520processes%252C%250Aenabling%2520CE3D%2520to%2520flexibly%2520integrate%2520a%2520wide%2520range%2520of%2520existing%25202D%2520or%25203D%2520visual%250Amodels%2520without%2520necessitating%2520intricate%2520fusion%2520designs.%2520Experimental%2520results%250Ademonstrate%2520that%2520CE3D%2520effectively%2520integrates%2520multiple%2520visual%2520models%2520to%2520achieve%250Adiverse%2520editing%2520visual%2520effects%252C%2520possessing%2520strong%2520scene%2520comprehension%2520and%250Amulti-round%2520dialog%2520capabilities.%2520Code%2520is%2520available%2520at%2520%253Ca%250Ahref%253D%2522https%253A//sk-fun.fun/CE3D%2522%253E%2520this%2520https%2520URL.%253C/a%253E%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chat-Edit-3D%3A%20Interactive%203D%20Scene%20Editing%20via%20Text%20Prompts&entry.906535625=Shuangkang%20Fang%20and%20Yufeng%20Wang%20and%20Yi-Hsuan%20Tsai%20and%20Yi%20Yang%20and%20Wenrui%20Ding%20and%20Shuchang%20Zhou%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Recent%20work%20on%20image%20content%20manipulation%20based%20on%20vision-language%0Apre-training%20models%20has%20been%20effectively%20extended%20to%20text-driven%203D%20scene%0Aediting.%20However%2C%20existing%20schemes%20for%203D%20scene%20editing%20still%20exhibit%20certain%0Ashortcomings%2C%20hindering%20their%20further%20interactive%20design.%20Such%20schemes%0Atypically%20adhere%20to%20fixed%20input%20patterns%2C%20limiting%20users%27%20flexibility%20in%20text%0Ainput.%20Moreover%2C%20their%20editing%20capabilities%20are%20constrained%20by%20a%20single%20or%20a%0Afew%202D%20visual%20models%20and%20require%20intricate%20pipeline%20design%20to%20integrate%20these%0Amodels%20into%203D%20reconstruction%20processes.%20To%20address%20the%20aforementioned%20issues%2C%0Awe%20propose%20a%20dialogue-based%203D%20scene%20editing%20approach%2C%20termed%20CE3D%2C%20which%20is%0Acentered%20around%20a%20large%20language%20model%20that%20allows%20for%20arbitrary%20textual%20input%0Afrom%20users%20and%20interprets%20their%20intentions%2C%20subsequently%20facilitating%20the%0Aautonomous%20invocation%20of%20the%20corresponding%20visual%20expert%20models.%20Furthermore%2C%0Awe%20design%20a%20scheme%20utilizing%20Hash-Atlas%20to%20represent%203D%20scene%20views%2C%20which%0Atransfers%20the%20editing%20of%203D%20scenes%20onto%202D%20atlas%20images.%20This%20design%20achieves%0Acomplete%20decoupling%20between%20the%202D%20editing%20and%203D%20reconstruction%20processes%2C%0Aenabling%20CE3D%20to%20flexibly%20integrate%20a%20wide%20range%20of%20existing%202D%20or%203D%20visual%0Amodels%20without%20necessitating%20intricate%20fusion%20designs.%20Experimental%20results%0Ademonstrate%20that%20CE3D%20effectively%20integrates%20multiple%20visual%20models%20to%20achieve%0Adiverse%20editing%20visual%20effects%2C%20possessing%20strong%20scene%20comprehension%20and%0Amulti-round%20dialog%20capabilities.%20Code%20is%20available%20at%20%3Ca%0Ahref%3D%22https%3A//sk-fun.fun/CE3D%22%3E%20this%20https%20URL.%3C/a%3E%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06842v1&entry.124074799=Read"},
{"title": "Approaches of large-scale images recognition with more than 50,000\n  categoris", "author": "Wanhong Huang and Rui Geng", "abstract": "  Though current CV models have been able to achieve high levels of accuracy on\nsmall-scale images classification dataset with hundreds or thousands of\ncategories, many models become infeasible in computational or space consumption\nwhen it comes to large-scale dataset with more than 50,000 categories. In this\npaper, we provide a viable solution for classifying large-scale species\ndatasets using traditional CV techniques such as.features extraction and\nprocessing, BOVW(Bag of Visual Words) and some statistical learning technics\nlike Mini-Batch K-Means,SVM which are used in our works. And then mixed with a\nneural network model. When applying these techniques, we have done some\noptimization in time and memory consumption, so that it can be feasible for\nlarge-scale dataset. And we also use some technics to reduce the impact of\nmislabeling data. We use a dataset with more than 50, 000 categories, and all\noperations are done on common computer with l 6GB RAM and a CPU of 3. OGHz. Our\ncontributions are: 1) analysis what problems may meet in the training\nprocesses, and presents several feasible ways to solve these problems. 2) Make\ntraditional CV models combined with neural network models provide some feasible\nscenarios for training large-scale classified datasets within the constraints\nof time and spatial resources.\n", "link": "http://arxiv.org/abs/2007.13072v2", "date": "2024-07-09", "relevancy": 2.5727, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.532}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5104}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approaches%20of%20large-scale%20images%20recognition%20with%20more%20than%2050%2C000%0A%20%20categoris&body=Title%3A%20Approaches%20of%20large-scale%20images%20recognition%20with%20more%20than%2050%2C000%0A%20%20categoris%0AAuthor%3A%20Wanhong%20Huang%20and%20Rui%20Geng%0AAbstract%3A%20%20%20Though%20current%20CV%20models%20have%20been%20able%20to%20achieve%20high%20levels%20of%20accuracy%20on%0Asmall-scale%20images%20classification%20dataset%20with%20hundreds%20or%20thousands%20of%0Acategories%2C%20many%20models%20become%20infeasible%20in%20computational%20or%20space%20consumption%0Awhen%20it%20comes%20to%20large-scale%20dataset%20with%20more%20than%2050%2C000%20categories.%20In%20this%0Apaper%2C%20we%20provide%20a%20viable%20solution%20for%20classifying%20large-scale%20species%0Adatasets%20using%20traditional%20CV%20techniques%20such%20as.features%20extraction%20and%0Aprocessing%2C%20BOVW%28Bag%20of%20Visual%20Words%29%20and%20some%20statistical%20learning%20technics%0Alike%20Mini-Batch%20K-Means%2CSVM%20which%20are%20used%20in%20our%20works.%20And%20then%20mixed%20with%20a%0Aneural%20network%20model.%20When%20applying%20these%20techniques%2C%20we%20have%20done%20some%0Aoptimization%20in%20time%20and%20memory%20consumption%2C%20so%20that%20it%20can%20be%20feasible%20for%0Alarge-scale%20dataset.%20And%20we%20also%20use%20some%20technics%20to%20reduce%20the%20impact%20of%0Amislabeling%20data.%20We%20use%20a%20dataset%20with%20more%20than%2050%2C%20000%20categories%2C%20and%20all%0Aoperations%20are%20done%20on%20common%20computer%20with%20l%206GB%20RAM%20and%20a%20CPU%20of%203.%20OGHz.%20Our%0Acontributions%20are%3A%201%29%20analysis%20what%20problems%20may%20meet%20in%20the%20training%0Aprocesses%2C%20and%20presents%20several%20feasible%20ways%20to%20solve%20these%20problems.%202%29%20Make%0Atraditional%20CV%20models%20combined%20with%20neural%20network%20models%20provide%20some%20feasible%0Ascenarios%20for%20training%20large-scale%20classified%20datasets%20within%20the%20constraints%0Aof%20time%20and%20spatial%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2007.13072v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproaches%2520of%2520large-scale%2520images%2520recognition%2520with%2520more%2520than%252050%252C000%250A%2520%2520categoris%26entry.906535625%3DWanhong%2520Huang%2520and%2520Rui%2520Geng%26entry.1292438233%3D%2520%2520Though%2520current%2520CV%2520models%2520have%2520been%2520able%2520to%2520achieve%2520high%2520levels%2520of%2520accuracy%2520on%250Asmall-scale%2520images%2520classification%2520dataset%2520with%2520hundreds%2520or%2520thousands%2520of%250Acategories%252C%2520many%2520models%2520become%2520infeasible%2520in%2520computational%2520or%2520space%2520consumption%250Awhen%2520it%2520comes%2520to%2520large-scale%2520dataset%2520with%2520more%2520than%252050%252C000%2520categories.%2520In%2520this%250Apaper%252C%2520we%2520provide%2520a%2520viable%2520solution%2520for%2520classifying%2520large-scale%2520species%250Adatasets%2520using%2520traditional%2520CV%2520techniques%2520such%2520as.features%2520extraction%2520and%250Aprocessing%252C%2520BOVW%2528Bag%2520of%2520Visual%2520Words%2529%2520and%2520some%2520statistical%2520learning%2520technics%250Alike%2520Mini-Batch%2520K-Means%252CSVM%2520which%2520are%2520used%2520in%2520our%2520works.%2520And%2520then%2520mixed%2520with%2520a%250Aneural%2520network%2520model.%2520When%2520applying%2520these%2520techniques%252C%2520we%2520have%2520done%2520some%250Aoptimization%2520in%2520time%2520and%2520memory%2520consumption%252C%2520so%2520that%2520it%2520can%2520be%2520feasible%2520for%250Alarge-scale%2520dataset.%2520And%2520we%2520also%2520use%2520some%2520technics%2520to%2520reduce%2520the%2520impact%2520of%250Amislabeling%2520data.%2520We%2520use%2520a%2520dataset%2520with%2520more%2520than%252050%252C%2520000%2520categories%252C%2520and%2520all%250Aoperations%2520are%2520done%2520on%2520common%2520computer%2520with%2520l%25206GB%2520RAM%2520and%2520a%2520CPU%2520of%25203.%2520OGHz.%2520Our%250Acontributions%2520are%253A%25201%2529%2520analysis%2520what%2520problems%2520may%2520meet%2520in%2520the%2520training%250Aprocesses%252C%2520and%2520presents%2520several%2520feasible%2520ways%2520to%2520solve%2520these%2520problems.%25202%2529%2520Make%250Atraditional%2520CV%2520models%2520combined%2520with%2520neural%2520network%2520models%2520provide%2520some%2520feasible%250Ascenarios%2520for%2520training%2520large-scale%2520classified%2520datasets%2520within%2520the%2520constraints%250Aof%2520time%2520and%2520spatial%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2007.13072v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approaches%20of%20large-scale%20images%20recognition%20with%20more%20than%2050%2C000%0A%20%20categoris&entry.906535625=Wanhong%20Huang%20and%20Rui%20Geng&entry.1292438233=%20%20Though%20current%20CV%20models%20have%20been%20able%20to%20achieve%20high%20levels%20of%20accuracy%20on%0Asmall-scale%20images%20classification%20dataset%20with%20hundreds%20or%20thousands%20of%0Acategories%2C%20many%20models%20become%20infeasible%20in%20computational%20or%20space%20consumption%0Awhen%20it%20comes%20to%20large-scale%20dataset%20with%20more%20than%2050%2C000%20categories.%20In%20this%0Apaper%2C%20we%20provide%20a%20viable%20solution%20for%20classifying%20large-scale%20species%0Adatasets%20using%20traditional%20CV%20techniques%20such%20as.features%20extraction%20and%0Aprocessing%2C%20BOVW%28Bag%20of%20Visual%20Words%29%20and%20some%20statistical%20learning%20technics%0Alike%20Mini-Batch%20K-Means%2CSVM%20which%20are%20used%20in%20our%20works.%20And%20then%20mixed%20with%20a%0Aneural%20network%20model.%20When%20applying%20these%20techniques%2C%20we%20have%20done%20some%0Aoptimization%20in%20time%20and%20memory%20consumption%2C%20so%20that%20it%20can%20be%20feasible%20for%0Alarge-scale%20dataset.%20And%20we%20also%20use%20some%20technics%20to%20reduce%20the%20impact%20of%0Amislabeling%20data.%20We%20use%20a%20dataset%20with%20more%20than%2050%2C%20000%20categories%2C%20and%20all%0Aoperations%20are%20done%20on%20common%20computer%20with%20l%206GB%20RAM%20and%20a%20CPU%20of%203.%20OGHz.%20Our%0Acontributions%20are%3A%201%29%20analysis%20what%20problems%20may%20meet%20in%20the%20training%0Aprocesses%2C%20and%20presents%20several%20feasible%20ways%20to%20solve%20these%20problems.%202%29%20Make%0Atraditional%20CV%20models%20combined%20with%20neural%20network%20models%20provide%20some%20feasible%0Ascenarios%20for%20training%20large-scale%20classified%20datasets%20within%20the%20constraints%0Aof%20time%20and%20spatial%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2007.13072v2&entry.124074799=Read"},
{"title": "GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting", "author": "Xinjie Zhang and Xingtong Ge and Tongda Xu and Dailan He and Yan Wang and Hongwei Qin and Guo Lu and Jing Geng and Jun Zhang", "abstract": "  Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 2000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding. Code is available at\nhttps://github.com/Xinjie-Q/GaussianImage.\n", "link": "http://arxiv.org/abs/2403.08551v5", "date": "2024-07-09", "relevancy": 2.5062, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6665}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6083}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianImage%3A%201000%20FPS%20Image%20Representation%20and%20Compression%20by%202D%0A%20%20Gaussian%20Splatting&body=Title%3A%20GaussianImage%3A%201000%20FPS%20Image%20Representation%20and%20Compression%20by%202D%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Xinjie%20Zhang%20and%20Xingtong%20Ge%20and%20Tongda%20Xu%20and%20Dailan%20He%20and%20Yan%20Wang%20and%20Hongwei%20Qin%20and%20Guo%20Lu%20and%20Jing%20Geng%20and%20Jun%20Zhang%0AAbstract%3A%20%20%20Implicit%20neural%20representations%20%28INRs%29%20recently%20achieved%20great%20success%20in%0Aimage%20representation%20and%20compression%2C%20offering%20high%20visual%20quality%20and%20fast%0Arendering%20speeds%20with%2010-1000%20FPS%2C%20assuming%20sufficient%20GPU%20resources%20are%0Aavailable.%20However%2C%20this%20requirement%20often%20hinders%20their%20use%20on%20low-end%20devices%0Awith%20limited%20memory.%20In%20response%2C%20we%20propose%20a%20groundbreaking%20paradigm%20of%20image%0Arepresentation%20and%20compression%20by%202D%20Gaussian%20Splatting%2C%20named%20GaussianImage.%0AWe%20first%20introduce%202D%20Gaussian%20to%20represent%20the%20image%2C%20where%20each%20Gaussian%20has%0A8%20parameters%20including%20position%2C%20covariance%20and%20color.%20Subsequently%2C%20we%20unveil%0Aa%20novel%20rendering%20algorithm%20based%20on%20accumulated%20summation.%20Remarkably%2C%20our%0Amethod%20with%20a%20minimum%20of%203%24%5Ctimes%24%20lower%20GPU%20memory%20usage%20and%205%24%5Ctimes%24%20faster%0Afitting%20time%20not%20only%20rivals%20INRs%20%28e.g.%2C%20WIRE%2C%20I-NGP%29%20in%20representation%0Aperformance%2C%20but%20also%20delivers%20a%20faster%20rendering%20speed%20of%201500-2000%20FPS%0Aregardless%20of%20parameter%20size.%20Furthermore%2C%20we%20integrate%20existing%20vector%0Aquantization%20technique%20to%20build%20an%20image%20codec.%20Experimental%20results%0Ademonstrate%20that%20our%20codec%20attains%20rate-distortion%20performance%20comparable%20to%0Acompression-based%20INRs%20such%20as%20COIN%20and%20COIN%2B%2B%2C%20while%20facilitating%20decoding%0Aspeeds%20of%20approximately%202000%20FPS.%20Additionally%2C%20preliminary%20proof%20of%20concept%0Ashows%20that%20our%20codec%20surpasses%20COIN%20and%20COIN%2B%2B%20in%20performance%20when%20using%0Apartial%20bits-back%20coding.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Xinjie-Q/GaussianImage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08551v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianImage%253A%25201000%2520FPS%2520Image%2520Representation%2520and%2520Compression%2520by%25202D%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DXinjie%2520Zhang%2520and%2520Xingtong%2520Ge%2520and%2520Tongda%2520Xu%2520and%2520Dailan%2520He%2520and%2520Yan%2520Wang%2520and%2520Hongwei%2520Qin%2520and%2520Guo%2520Lu%2520and%2520Jing%2520Geng%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%2520Implicit%2520neural%2520representations%2520%2528INRs%2529%2520recently%2520achieved%2520great%2520success%2520in%250Aimage%2520representation%2520and%2520compression%252C%2520offering%2520high%2520visual%2520quality%2520and%2520fast%250Arendering%2520speeds%2520with%252010-1000%2520FPS%252C%2520assuming%2520sufficient%2520GPU%2520resources%2520are%250Aavailable.%2520However%252C%2520this%2520requirement%2520often%2520hinders%2520their%2520use%2520on%2520low-end%2520devices%250Awith%2520limited%2520memory.%2520In%2520response%252C%2520we%2520propose%2520a%2520groundbreaking%2520paradigm%2520of%2520image%250Arepresentation%2520and%2520compression%2520by%25202D%2520Gaussian%2520Splatting%252C%2520named%2520GaussianImage.%250AWe%2520first%2520introduce%25202D%2520Gaussian%2520to%2520represent%2520the%2520image%252C%2520where%2520each%2520Gaussian%2520has%250A8%2520parameters%2520including%2520position%252C%2520covariance%2520and%2520color.%2520Subsequently%252C%2520we%2520unveil%250Aa%2520novel%2520rendering%2520algorithm%2520based%2520on%2520accumulated%2520summation.%2520Remarkably%252C%2520our%250Amethod%2520with%2520a%2520minimum%2520of%25203%2524%255Ctimes%2524%2520lower%2520GPU%2520memory%2520usage%2520and%25205%2524%255Ctimes%2524%2520faster%250Afitting%2520time%2520not%2520only%2520rivals%2520INRs%2520%2528e.g.%252C%2520WIRE%252C%2520I-NGP%2529%2520in%2520representation%250Aperformance%252C%2520but%2520also%2520delivers%2520a%2520faster%2520rendering%2520speed%2520of%25201500-2000%2520FPS%250Aregardless%2520of%2520parameter%2520size.%2520Furthermore%252C%2520we%2520integrate%2520existing%2520vector%250Aquantization%2520technique%2520to%2520build%2520an%2520image%2520codec.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520codec%2520attains%2520rate-distortion%2520performance%2520comparable%2520to%250Acompression-based%2520INRs%2520such%2520as%2520COIN%2520and%2520COIN%252B%252B%252C%2520while%2520facilitating%2520decoding%250Aspeeds%2520of%2520approximately%25202000%2520FPS.%2520Additionally%252C%2520preliminary%2520proof%2520of%2520concept%250Ashows%2520that%2520our%2520codec%2520surpasses%2520COIN%2520and%2520COIN%252B%252B%2520in%2520performance%2520when%2520using%250Apartial%2520bits-back%2520coding.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Xinjie-Q/GaussianImage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08551v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianImage%3A%201000%20FPS%20Image%20Representation%20and%20Compression%20by%202D%0A%20%20Gaussian%20Splatting&entry.906535625=Xinjie%20Zhang%20and%20Xingtong%20Ge%20and%20Tongda%20Xu%20and%20Dailan%20He%20and%20Yan%20Wang%20and%20Hongwei%20Qin%20and%20Guo%20Lu%20and%20Jing%20Geng%20and%20Jun%20Zhang&entry.1292438233=%20%20Implicit%20neural%20representations%20%28INRs%29%20recently%20achieved%20great%20success%20in%0Aimage%20representation%20and%20compression%2C%20offering%20high%20visual%20quality%20and%20fast%0Arendering%20speeds%20with%2010-1000%20FPS%2C%20assuming%20sufficient%20GPU%20resources%20are%0Aavailable.%20However%2C%20this%20requirement%20often%20hinders%20their%20use%20on%20low-end%20devices%0Awith%20limited%20memory.%20In%20response%2C%20we%20propose%20a%20groundbreaking%20paradigm%20of%20image%0Arepresentation%20and%20compression%20by%202D%20Gaussian%20Splatting%2C%20named%20GaussianImage.%0AWe%20first%20introduce%202D%20Gaussian%20to%20represent%20the%20image%2C%20where%20each%20Gaussian%20has%0A8%20parameters%20including%20position%2C%20covariance%20and%20color.%20Subsequently%2C%20we%20unveil%0Aa%20novel%20rendering%20algorithm%20based%20on%20accumulated%20summation.%20Remarkably%2C%20our%0Amethod%20with%20a%20minimum%20of%203%24%5Ctimes%24%20lower%20GPU%20memory%20usage%20and%205%24%5Ctimes%24%20faster%0Afitting%20time%20not%20only%20rivals%20INRs%20%28e.g.%2C%20WIRE%2C%20I-NGP%29%20in%20representation%0Aperformance%2C%20but%20also%20delivers%20a%20faster%20rendering%20speed%20of%201500-2000%20FPS%0Aregardless%20of%20parameter%20size.%20Furthermore%2C%20we%20integrate%20existing%20vector%0Aquantization%20technique%20to%20build%20an%20image%20codec.%20Experimental%20results%0Ademonstrate%20that%20our%20codec%20attains%20rate-distortion%20performance%20comparable%20to%0Acompression-based%20INRs%20such%20as%20COIN%20and%20COIN%2B%2B%2C%20while%20facilitating%20decoding%0Aspeeds%20of%20approximately%202000%20FPS.%20Additionally%2C%20preliminary%20proof%20of%20concept%0Ashows%20that%20our%20codec%20surpasses%20COIN%20and%20COIN%2B%2B%20in%20performance%20when%20using%0Apartial%20bits-back%20coding.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Xinjie-Q/GaussianImage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08551v5&entry.124074799=Read"},
{"title": "Multicell-Fold: geometric learning in folding multicellular life", "author": "Haiqian Yang and Anh Q. Nguyen and Dapeng Bi and Markus J. Buehler and Ming Guo", "abstract": "  During developmental processes such as embryogenesis, how a group of cells\nfold into specific structures, is a central question in biology that defines\nhow living organisms form. Establishing tissue-level morphology critically\nrelies on how every single cell decides to position itself relative to its\nneighboring cells. Despite its importance, it remains a major challenge to\nunderstand and predict the behavior of every cell within the living tissue over\ntime during such intricate processes. To tackle this question, we propose a\ngeometric deep learning model that can predict multicellular folding and\nembryogenesis, accurately capturing the highly convoluted spatial interactions\namong cells. We demonstrate that multicellular data can be represented with\nboth granular and foam-like physical pictures through a unified graph data\nstructure, considering both cellular interactions and cell junction networks.\nWe successfully use our model to achieve two important tasks, interpretable 4-D\nmorphological sequence alignment, and predicting local cell rearrangements\nbefore they occur at single-cell resolution. Furthermore, using an activation\nmap and ablation studies, we demonstrate that cell geometries and cell junction\nnetworks together regulate local cell rearrangement which is critical for\nembryo morphogenesis. This approach provides a novel paradigm to study\nmorphogenesis, highlighting a unified data structure and harnessing the power\nof geometric deep learning to accurately model the mechanisms and behaviors of\ncells during development. It offers a pathway toward creating a unified dynamic\nmorphological atlas for a variety of developmental processes such as\nembryogenesis.\n", "link": "http://arxiv.org/abs/2407.07055v1", "date": "2024-07-09", "relevancy": 2.4966, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5027}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multicell-Fold%3A%20geometric%20learning%20in%20folding%20multicellular%20life&body=Title%3A%20Multicell-Fold%3A%20geometric%20learning%20in%20folding%20multicellular%20life%0AAuthor%3A%20Haiqian%20Yang%20and%20Anh%20Q.%20Nguyen%20and%20Dapeng%20Bi%20and%20Markus%20J.%20Buehler%20and%20Ming%20Guo%0AAbstract%3A%20%20%20During%20developmental%20processes%20such%20as%20embryogenesis%2C%20how%20a%20group%20of%20cells%0Afold%20into%20specific%20structures%2C%20is%20a%20central%20question%20in%20biology%20that%20defines%0Ahow%20living%20organisms%20form.%20Establishing%20tissue-level%20morphology%20critically%0Arelies%20on%20how%20every%20single%20cell%20decides%20to%20position%20itself%20relative%20to%20its%0Aneighboring%20cells.%20Despite%20its%20importance%2C%20it%20remains%20a%20major%20challenge%20to%0Aunderstand%20and%20predict%20the%20behavior%20of%20every%20cell%20within%20the%20living%20tissue%20over%0Atime%20during%20such%20intricate%20processes.%20To%20tackle%20this%20question%2C%20we%20propose%20a%0Ageometric%20deep%20learning%20model%20that%20can%20predict%20multicellular%20folding%20and%0Aembryogenesis%2C%20accurately%20capturing%20the%20highly%20convoluted%20spatial%20interactions%0Aamong%20cells.%20We%20demonstrate%20that%20multicellular%20data%20can%20be%20represented%20with%0Aboth%20granular%20and%20foam-like%20physical%20pictures%20through%20a%20unified%20graph%20data%0Astructure%2C%20considering%20both%20cellular%20interactions%20and%20cell%20junction%20networks.%0AWe%20successfully%20use%20our%20model%20to%20achieve%20two%20important%20tasks%2C%20interpretable%204-D%0Amorphological%20sequence%20alignment%2C%20and%20predicting%20local%20cell%20rearrangements%0Abefore%20they%20occur%20at%20single-cell%20resolution.%20Furthermore%2C%20using%20an%20activation%0Amap%20and%20ablation%20studies%2C%20we%20demonstrate%20that%20cell%20geometries%20and%20cell%20junction%0Anetworks%20together%20regulate%20local%20cell%20rearrangement%20which%20is%20critical%20for%0Aembryo%20morphogenesis.%20This%20approach%20provides%20a%20novel%20paradigm%20to%20study%0Amorphogenesis%2C%20highlighting%20a%20unified%20data%20structure%20and%20harnessing%20the%20power%0Aof%20geometric%20deep%20learning%20to%20accurately%20model%20the%20mechanisms%20and%20behaviors%20of%0Acells%20during%20development.%20It%20offers%20a%20pathway%20toward%20creating%20a%20unified%20dynamic%0Amorphological%20atlas%20for%20a%20variety%20of%20developmental%20processes%20such%20as%0Aembryogenesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulticell-Fold%253A%2520geometric%2520learning%2520in%2520folding%2520multicellular%2520life%26entry.906535625%3DHaiqian%2520Yang%2520and%2520Anh%2520Q.%2520Nguyen%2520and%2520Dapeng%2520Bi%2520and%2520Markus%2520J.%2520Buehler%2520and%2520Ming%2520Guo%26entry.1292438233%3D%2520%2520During%2520developmental%2520processes%2520such%2520as%2520embryogenesis%252C%2520how%2520a%2520group%2520of%2520cells%250Afold%2520into%2520specific%2520structures%252C%2520is%2520a%2520central%2520question%2520in%2520biology%2520that%2520defines%250Ahow%2520living%2520organisms%2520form.%2520Establishing%2520tissue-level%2520morphology%2520critically%250Arelies%2520on%2520how%2520every%2520single%2520cell%2520decides%2520to%2520position%2520itself%2520relative%2520to%2520its%250Aneighboring%2520cells.%2520Despite%2520its%2520importance%252C%2520it%2520remains%2520a%2520major%2520challenge%2520to%250Aunderstand%2520and%2520predict%2520the%2520behavior%2520of%2520every%2520cell%2520within%2520the%2520living%2520tissue%2520over%250Atime%2520during%2520such%2520intricate%2520processes.%2520To%2520tackle%2520this%2520question%252C%2520we%2520propose%2520a%250Ageometric%2520deep%2520learning%2520model%2520that%2520can%2520predict%2520multicellular%2520folding%2520and%250Aembryogenesis%252C%2520accurately%2520capturing%2520the%2520highly%2520convoluted%2520spatial%2520interactions%250Aamong%2520cells.%2520We%2520demonstrate%2520that%2520multicellular%2520data%2520can%2520be%2520represented%2520with%250Aboth%2520granular%2520and%2520foam-like%2520physical%2520pictures%2520through%2520a%2520unified%2520graph%2520data%250Astructure%252C%2520considering%2520both%2520cellular%2520interactions%2520and%2520cell%2520junction%2520networks.%250AWe%2520successfully%2520use%2520our%2520model%2520to%2520achieve%2520two%2520important%2520tasks%252C%2520interpretable%25204-D%250Amorphological%2520sequence%2520alignment%252C%2520and%2520predicting%2520local%2520cell%2520rearrangements%250Abefore%2520they%2520occur%2520at%2520single-cell%2520resolution.%2520Furthermore%252C%2520using%2520an%2520activation%250Amap%2520and%2520ablation%2520studies%252C%2520we%2520demonstrate%2520that%2520cell%2520geometries%2520and%2520cell%2520junction%250Anetworks%2520together%2520regulate%2520local%2520cell%2520rearrangement%2520which%2520is%2520critical%2520for%250Aembryo%2520morphogenesis.%2520This%2520approach%2520provides%2520a%2520novel%2520paradigm%2520to%2520study%250Amorphogenesis%252C%2520highlighting%2520a%2520unified%2520data%2520structure%2520and%2520harnessing%2520the%2520power%250Aof%2520geometric%2520deep%2520learning%2520to%2520accurately%2520model%2520the%2520mechanisms%2520and%2520behaviors%2520of%250Acells%2520during%2520development.%2520It%2520offers%2520a%2520pathway%2520toward%2520creating%2520a%2520unified%2520dynamic%250Amorphological%2520atlas%2520for%2520a%2520variety%2520of%2520developmental%2520processes%2520such%2520as%250Aembryogenesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multicell-Fold%3A%20geometric%20learning%20in%20folding%20multicellular%20life&entry.906535625=Haiqian%20Yang%20and%20Anh%20Q.%20Nguyen%20and%20Dapeng%20Bi%20and%20Markus%20J.%20Buehler%20and%20Ming%20Guo&entry.1292438233=%20%20During%20developmental%20processes%20such%20as%20embryogenesis%2C%20how%20a%20group%20of%20cells%0Afold%20into%20specific%20structures%2C%20is%20a%20central%20question%20in%20biology%20that%20defines%0Ahow%20living%20organisms%20form.%20Establishing%20tissue-level%20morphology%20critically%0Arelies%20on%20how%20every%20single%20cell%20decides%20to%20position%20itself%20relative%20to%20its%0Aneighboring%20cells.%20Despite%20its%20importance%2C%20it%20remains%20a%20major%20challenge%20to%0Aunderstand%20and%20predict%20the%20behavior%20of%20every%20cell%20within%20the%20living%20tissue%20over%0Atime%20during%20such%20intricate%20processes.%20To%20tackle%20this%20question%2C%20we%20propose%20a%0Ageometric%20deep%20learning%20model%20that%20can%20predict%20multicellular%20folding%20and%0Aembryogenesis%2C%20accurately%20capturing%20the%20highly%20convoluted%20spatial%20interactions%0Aamong%20cells.%20We%20demonstrate%20that%20multicellular%20data%20can%20be%20represented%20with%0Aboth%20granular%20and%20foam-like%20physical%20pictures%20through%20a%20unified%20graph%20data%0Astructure%2C%20considering%20both%20cellular%20interactions%20and%20cell%20junction%20networks.%0AWe%20successfully%20use%20our%20model%20to%20achieve%20two%20important%20tasks%2C%20interpretable%204-D%0Amorphological%20sequence%20alignment%2C%20and%20predicting%20local%20cell%20rearrangements%0Abefore%20they%20occur%20at%20single-cell%20resolution.%20Furthermore%2C%20using%20an%20activation%0Amap%20and%20ablation%20studies%2C%20we%20demonstrate%20that%20cell%20geometries%20and%20cell%20junction%0Anetworks%20together%20regulate%20local%20cell%20rearrangement%20which%20is%20critical%20for%0Aembryo%20morphogenesis.%20This%20approach%20provides%20a%20novel%20paradigm%20to%20study%0Amorphogenesis%2C%20highlighting%20a%20unified%20data%20structure%20and%20harnessing%20the%20power%0Aof%20geometric%20deep%20learning%20to%20accurately%20model%20the%20mechanisms%20and%20behaviors%20of%0Acells%20during%20development.%20It%20offers%20a%20pathway%20toward%20creating%20a%20unified%20dynamic%0Amorphological%20atlas%20for%20a%20variety%20of%20developmental%20processes%20such%20as%0Aembryogenesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07055v1&entry.124074799=Read"},
{"title": "Hypergraph based Understanding for Document Semantic Entity Recognition", "author": "Qiwei Li and Zuchao Li and Ping Wang and Haojun Ai and Hai Zhao", "abstract": "  Semantic entity recognition is an important task in the field of\nvisually-rich document understanding. It distinguishes the semantic types of\ntext by analyzing the position relationship between text nodes and the relation\nbetween text content. The existing document understanding models mainly focus\non entity categories while ignoring the extraction of entity boundaries. We\nbuild a novel hypergraph attention document semantic entity recognition\nframework, HGA, which uses hypergraph attention to focus on entity boundaries\nand entity categories at the same time. It can conduct a more detailed analysis\nof the document text representation analyzed by the upstream model and achieves\na better performance of semantic information. We apply this method on the basis\nof GraphLayoutLM to construct a new semantic entity recognition model\nHGALayoutLM. Our experiment results on FUNSD, CORD, XFUND and SROIE show that\nour method can effectively improve the performance of semantic entity\nrecognition tasks based on the original model. The results of HGALayoutLM on\nFUNSD and XFUND reach the new state-of-the-art results.\n", "link": "http://arxiv.org/abs/2407.06904v1", "date": "2024-07-09", "relevancy": 2.4847, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5434}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4739}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypergraph%20based%20Understanding%20for%20Document%20Semantic%20Entity%20Recognition&body=Title%3A%20Hypergraph%20based%20Understanding%20for%20Document%20Semantic%20Entity%20Recognition%0AAuthor%3A%20Qiwei%20Li%20and%20Zuchao%20Li%20and%20Ping%20Wang%20and%20Haojun%20Ai%20and%20Hai%20Zhao%0AAbstract%3A%20%20%20Semantic%20entity%20recognition%20is%20an%20important%20task%20in%20the%20field%20of%0Avisually-rich%20document%20understanding.%20It%20distinguishes%20the%20semantic%20types%20of%0Atext%20by%20analyzing%20the%20position%20relationship%20between%20text%20nodes%20and%20the%20relation%0Abetween%20text%20content.%20The%20existing%20document%20understanding%20models%20mainly%20focus%0Aon%20entity%20categories%20while%20ignoring%20the%20extraction%20of%20entity%20boundaries.%20We%0Abuild%20a%20novel%20hypergraph%20attention%20document%20semantic%20entity%20recognition%0Aframework%2C%20HGA%2C%20which%20uses%20hypergraph%20attention%20to%20focus%20on%20entity%20boundaries%0Aand%20entity%20categories%20at%20the%20same%20time.%20It%20can%20conduct%20a%20more%20detailed%20analysis%0Aof%20the%20document%20text%20representation%20analyzed%20by%20the%20upstream%20model%20and%20achieves%0Aa%20better%20performance%20of%20semantic%20information.%20We%20apply%20this%20method%20on%20the%20basis%0Aof%20GraphLayoutLM%20to%20construct%20a%20new%20semantic%20entity%20recognition%20model%0AHGALayoutLM.%20Our%20experiment%20results%20on%20FUNSD%2C%20CORD%2C%20XFUND%20and%20SROIE%20show%20that%0Aour%20method%20can%20effectively%20improve%20the%20performance%20of%20semantic%20entity%0Arecognition%20tasks%20based%20on%20the%20original%20model.%20The%20results%20of%20HGALayoutLM%20on%0AFUNSD%20and%20XFUND%20reach%20the%20new%20state-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypergraph%2520based%2520Understanding%2520for%2520Document%2520Semantic%2520Entity%2520Recognition%26entry.906535625%3DQiwei%2520Li%2520and%2520Zuchao%2520Li%2520and%2520Ping%2520Wang%2520and%2520Haojun%2520Ai%2520and%2520Hai%2520Zhao%26entry.1292438233%3D%2520%2520Semantic%2520entity%2520recognition%2520is%2520an%2520important%2520task%2520in%2520the%2520field%2520of%250Avisually-rich%2520document%2520understanding.%2520It%2520distinguishes%2520the%2520semantic%2520types%2520of%250Atext%2520by%2520analyzing%2520the%2520position%2520relationship%2520between%2520text%2520nodes%2520and%2520the%2520relation%250Abetween%2520text%2520content.%2520The%2520existing%2520document%2520understanding%2520models%2520mainly%2520focus%250Aon%2520entity%2520categories%2520while%2520ignoring%2520the%2520extraction%2520of%2520entity%2520boundaries.%2520We%250Abuild%2520a%2520novel%2520hypergraph%2520attention%2520document%2520semantic%2520entity%2520recognition%250Aframework%252C%2520HGA%252C%2520which%2520uses%2520hypergraph%2520attention%2520to%2520focus%2520on%2520entity%2520boundaries%250Aand%2520entity%2520categories%2520at%2520the%2520same%2520time.%2520It%2520can%2520conduct%2520a%2520more%2520detailed%2520analysis%250Aof%2520the%2520document%2520text%2520representation%2520analyzed%2520by%2520the%2520upstream%2520model%2520and%2520achieves%250Aa%2520better%2520performance%2520of%2520semantic%2520information.%2520We%2520apply%2520this%2520method%2520on%2520the%2520basis%250Aof%2520GraphLayoutLM%2520to%2520construct%2520a%2520new%2520semantic%2520entity%2520recognition%2520model%250AHGALayoutLM.%2520Our%2520experiment%2520results%2520on%2520FUNSD%252C%2520CORD%252C%2520XFUND%2520and%2520SROIE%2520show%2520that%250Aour%2520method%2520can%2520effectively%2520improve%2520the%2520performance%2520of%2520semantic%2520entity%250Arecognition%2520tasks%2520based%2520on%2520the%2520original%2520model.%2520The%2520results%2520of%2520HGALayoutLM%2520on%250AFUNSD%2520and%2520XFUND%2520reach%2520the%2520new%2520state-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypergraph%20based%20Understanding%20for%20Document%20Semantic%20Entity%20Recognition&entry.906535625=Qiwei%20Li%20and%20Zuchao%20Li%20and%20Ping%20Wang%20and%20Haojun%20Ai%20and%20Hai%20Zhao&entry.1292438233=%20%20Semantic%20entity%20recognition%20is%20an%20important%20task%20in%20the%20field%20of%0Avisually-rich%20document%20understanding.%20It%20distinguishes%20the%20semantic%20types%20of%0Atext%20by%20analyzing%20the%20position%20relationship%20between%20text%20nodes%20and%20the%20relation%0Abetween%20text%20content.%20The%20existing%20document%20understanding%20models%20mainly%20focus%0Aon%20entity%20categories%20while%20ignoring%20the%20extraction%20of%20entity%20boundaries.%20We%0Abuild%20a%20novel%20hypergraph%20attention%20document%20semantic%20entity%20recognition%0Aframework%2C%20HGA%2C%20which%20uses%20hypergraph%20attention%20to%20focus%20on%20entity%20boundaries%0Aand%20entity%20categories%20at%20the%20same%20time.%20It%20can%20conduct%20a%20more%20detailed%20analysis%0Aof%20the%20document%20text%20representation%20analyzed%20by%20the%20upstream%20model%20and%20achieves%0Aa%20better%20performance%20of%20semantic%20information.%20We%20apply%20this%20method%20on%20the%20basis%0Aof%20GraphLayoutLM%20to%20construct%20a%20new%20semantic%20entity%20recognition%20model%0AHGALayoutLM.%20Our%20experiment%20results%20on%20FUNSD%2C%20CORD%2C%20XFUND%20and%20SROIE%20show%20that%0Aour%20method%20can%20effectively%20improve%20the%20performance%20of%20semantic%20entity%0Arecognition%20tasks%20based%20on%20the%20original%20model.%20The%20results%20of%20HGALayoutLM%20on%0AFUNSD%20and%20XFUND%20reach%20the%20new%20state-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06904v1&entry.124074799=Read"},
{"title": "Zero-shot Object Counting with Good Exemplars", "author": "Huilin Zhu and Jingling Yuan and Zhengwei Yang and Yu Guo and Zheng Wang and Xian Zhong and Shengfeng He", "abstract": "  Zero-shot object counting (ZOC) aims to enumerate objects in images using\nonly the names of object classes during testing, without the need for manual\nannotations. However, a critical challenge in current ZOC methods lies in their\ninability to identify high-quality exemplars effectively. This deficiency\nhampers scalability across diverse classes and undermines the development of\nstrong visual associations between the identified classes and image content. To\nthis end, we propose the Visual Association-based Zero-shot Object Counting\n(VA-Count) framework. VA-Count consists of an Exemplar Enhancement Module (EEM)\nand a Noise Suppression Module (NSM) that synergistically refine the process of\nclass exemplar identification while minimizing the consequences of incorrect\nobject identification. The EEM utilizes advanced vision-language pretaining\nmodels to discover potential exemplars, ensuring the framework's adaptability\nto various classes. Meanwhile, the NSM employs contrastive learning to\ndifferentiate between optimal and suboptimal exemplar pairs, reducing the\nnegative effects of erroneous exemplars. VA-Count demonstrates its\neffectiveness and scalability in zero-shot contexts with superior performance\non two object counting datasets.\n", "link": "http://arxiv.org/abs/2407.04948v2", "date": "2024-07-09", "relevancy": 2.4515, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4994}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4931}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Object%20Counting%20with%20Good%20Exemplars&body=Title%3A%20Zero-shot%20Object%20Counting%20with%20Good%20Exemplars%0AAuthor%3A%20Huilin%20Zhu%20and%20Jingling%20Yuan%20and%20Zhengwei%20Yang%20and%20Yu%20Guo%20and%20Zheng%20Wang%20and%20Xian%20Zhong%20and%20Shengfeng%20He%0AAbstract%3A%20%20%20Zero-shot%20object%20counting%20%28ZOC%29%20aims%20to%20enumerate%20objects%20in%20images%20using%0Aonly%20the%20names%20of%20object%20classes%20during%20testing%2C%20without%20the%20need%20for%20manual%0Aannotations.%20However%2C%20a%20critical%20challenge%20in%20current%20ZOC%20methods%20lies%20in%20their%0Ainability%20to%20identify%20high-quality%20exemplars%20effectively.%20This%20deficiency%0Ahampers%20scalability%20across%20diverse%20classes%20and%20undermines%20the%20development%20of%0Astrong%20visual%20associations%20between%20the%20identified%20classes%20and%20image%20content.%20To%0Athis%20end%2C%20we%20propose%20the%20Visual%20Association-based%20Zero-shot%20Object%20Counting%0A%28VA-Count%29%20framework.%20VA-Count%20consists%20of%20an%20Exemplar%20Enhancement%20Module%20%28EEM%29%0Aand%20a%20Noise%20Suppression%20Module%20%28NSM%29%20that%20synergistically%20refine%20the%20process%20of%0Aclass%20exemplar%20identification%20while%20minimizing%20the%20consequences%20of%20incorrect%0Aobject%20identification.%20The%20EEM%20utilizes%20advanced%20vision-language%20pretaining%0Amodels%20to%20discover%20potential%20exemplars%2C%20ensuring%20the%20framework%27s%20adaptability%0Ato%20various%20classes.%20Meanwhile%2C%20the%20NSM%20employs%20contrastive%20learning%20to%0Adifferentiate%20between%20optimal%20and%20suboptimal%20exemplar%20pairs%2C%20reducing%20the%0Anegative%20effects%20of%20erroneous%20exemplars.%20VA-Count%20demonstrates%20its%0Aeffectiveness%20and%20scalability%20in%20zero-shot%20contexts%20with%20superior%20performance%0Aon%20two%20object%20counting%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04948v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Object%2520Counting%2520with%2520Good%2520Exemplars%26entry.906535625%3DHuilin%2520Zhu%2520and%2520Jingling%2520Yuan%2520and%2520Zhengwei%2520Yang%2520and%2520Yu%2520Guo%2520and%2520Zheng%2520Wang%2520and%2520Xian%2520Zhong%2520and%2520Shengfeng%2520He%26entry.1292438233%3D%2520%2520Zero-shot%2520object%2520counting%2520%2528ZOC%2529%2520aims%2520to%2520enumerate%2520objects%2520in%2520images%2520using%250Aonly%2520the%2520names%2520of%2520object%2520classes%2520during%2520testing%252C%2520without%2520the%2520need%2520for%2520manual%250Aannotations.%2520However%252C%2520a%2520critical%2520challenge%2520in%2520current%2520ZOC%2520methods%2520lies%2520in%2520their%250Ainability%2520to%2520identify%2520high-quality%2520exemplars%2520effectively.%2520This%2520deficiency%250Ahampers%2520scalability%2520across%2520diverse%2520classes%2520and%2520undermines%2520the%2520development%2520of%250Astrong%2520visual%2520associations%2520between%2520the%2520identified%2520classes%2520and%2520image%2520content.%2520To%250Athis%2520end%252C%2520we%2520propose%2520the%2520Visual%2520Association-based%2520Zero-shot%2520Object%2520Counting%250A%2528VA-Count%2529%2520framework.%2520VA-Count%2520consists%2520of%2520an%2520Exemplar%2520Enhancement%2520Module%2520%2528EEM%2529%250Aand%2520a%2520Noise%2520Suppression%2520Module%2520%2528NSM%2529%2520that%2520synergistically%2520refine%2520the%2520process%2520of%250Aclass%2520exemplar%2520identification%2520while%2520minimizing%2520the%2520consequences%2520of%2520incorrect%250Aobject%2520identification.%2520The%2520EEM%2520utilizes%2520advanced%2520vision-language%2520pretaining%250Amodels%2520to%2520discover%2520potential%2520exemplars%252C%2520ensuring%2520the%2520framework%2527s%2520adaptability%250Ato%2520various%2520classes.%2520Meanwhile%252C%2520the%2520NSM%2520employs%2520contrastive%2520learning%2520to%250Adifferentiate%2520between%2520optimal%2520and%2520suboptimal%2520exemplar%2520pairs%252C%2520reducing%2520the%250Anegative%2520effects%2520of%2520erroneous%2520exemplars.%2520VA-Count%2520demonstrates%2520its%250Aeffectiveness%2520and%2520scalability%2520in%2520zero-shot%2520contexts%2520with%2520superior%2520performance%250Aon%2520two%2520object%2520counting%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04948v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Object%20Counting%20with%20Good%20Exemplars&entry.906535625=Huilin%20Zhu%20and%20Jingling%20Yuan%20and%20Zhengwei%20Yang%20and%20Yu%20Guo%20and%20Zheng%20Wang%20and%20Xian%20Zhong%20and%20Shengfeng%20He&entry.1292438233=%20%20Zero-shot%20object%20counting%20%28ZOC%29%20aims%20to%20enumerate%20objects%20in%20images%20using%0Aonly%20the%20names%20of%20object%20classes%20during%20testing%2C%20without%20the%20need%20for%20manual%0Aannotations.%20However%2C%20a%20critical%20challenge%20in%20current%20ZOC%20methods%20lies%20in%20their%0Ainability%20to%20identify%20high-quality%20exemplars%20effectively.%20This%20deficiency%0Ahampers%20scalability%20across%20diverse%20classes%20and%20undermines%20the%20development%20of%0Astrong%20visual%20associations%20between%20the%20identified%20classes%20and%20image%20content.%20To%0Athis%20end%2C%20we%20propose%20the%20Visual%20Association-based%20Zero-shot%20Object%20Counting%0A%28VA-Count%29%20framework.%20VA-Count%20consists%20of%20an%20Exemplar%20Enhancement%20Module%20%28EEM%29%0Aand%20a%20Noise%20Suppression%20Module%20%28NSM%29%20that%20synergistically%20refine%20the%20process%20of%0Aclass%20exemplar%20identification%20while%20minimizing%20the%20consequences%20of%20incorrect%0Aobject%20identification.%20The%20EEM%20utilizes%20advanced%20vision-language%20pretaining%0Amodels%20to%20discover%20potential%20exemplars%2C%20ensuring%20the%20framework%27s%20adaptability%0Ato%20various%20classes.%20Meanwhile%2C%20the%20NSM%20employs%20contrastive%20learning%20to%0Adifferentiate%20between%20optimal%20and%20suboptimal%20exemplar%20pairs%2C%20reducing%20the%0Anegative%20effects%20of%20erroneous%20exemplars.%20VA-Count%20demonstrates%20its%0Aeffectiveness%20and%20scalability%20in%20zero-shot%20contexts%20with%20superior%20performance%0Aon%20two%20object%20counting%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04948v2&entry.124074799=Read"},
{"title": "HumanRefiner: Benchmarking Abnormal Human Generation and Refining with\n  Coarse-to-fine Pose-Reversible Guidance", "author": "Guian Fang and Wenbiao Yan and Yuanfan Guo and Jianhua Han and Zutao Jiang and Hang Xu and Shengcai Liao and Xiaodan Liang", "abstract": "  Text-to-image diffusion models have significantly advanced in conditional\nimage generation. However, these models usually struggle with accurately\nrendering images featuring humans, resulting in distorted limbs and other\nanomalies. This issue primarily stems from the insufficient recognition and\nevaluation of limb qualities in diffusion models. To address this issue, we\nintroduce AbHuman, the first large-scale synthesized human benchmark focusing\non anatomical anomalies. This benchmark consists of 56K synthesized human\nimages, each annotated with detailed, bounding-box level labels identifying\n147K human anomalies in 18 different categories. Based on this, the recognition\nof human anomalies can be established, which in turn enhances image generation\nthrough traditional techniques such as negative prompting and guidance. To\nfurther boost the improvement, we propose HumanRefiner, a novel plug-and-play\napproach for the coarse-to-fine refinement of human anomalies in text-to-image\ngeneration. Specifically, HumanRefiner utilizes a self-diagnostic procedure to\ndetect and correct issues related to both coarse-grained abnormal human poses\nand fine-grained anomaly levels, facilitating pose-reversible diffusion\ngeneration. Experimental results on the AbHuman benchmark demonstrate that\nHumanRefiner significantly reduces generative discrepancies, achieving a 2.9x\nimprovement in limb quality compared to the state-of-the-art open-source\ngenerator SDXL and a 1.4x improvement over DALL-E 3 in human evaluations. Our\ndata and code are available at https://github.com/Enderfga/HumanRefiner.\n", "link": "http://arxiv.org/abs/2407.06937v1", "date": "2024-07-09", "relevancy": 2.4402, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6205}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6114}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanRefiner%3A%20Benchmarking%20Abnormal%20Human%20Generation%20and%20Refining%20with%0A%20%20Coarse-to-fine%20Pose-Reversible%20Guidance&body=Title%3A%20HumanRefiner%3A%20Benchmarking%20Abnormal%20Human%20Generation%20and%20Refining%20with%0A%20%20Coarse-to-fine%20Pose-Reversible%20Guidance%0AAuthor%3A%20Guian%20Fang%20and%20Wenbiao%20Yan%20and%20Yuanfan%20Guo%20and%20Jianhua%20Han%20and%20Zutao%20Jiang%20and%20Hang%20Xu%20and%20Shengcai%20Liao%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20significantly%20advanced%20in%20conditional%0Aimage%20generation.%20However%2C%20these%20models%20usually%20struggle%20with%20accurately%0Arendering%20images%20featuring%20humans%2C%20resulting%20in%20distorted%20limbs%20and%20other%0Aanomalies.%20This%20issue%20primarily%20stems%20from%20the%20insufficient%20recognition%20and%0Aevaluation%20of%20limb%20qualities%20in%20diffusion%20models.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20AbHuman%2C%20the%20first%20large-scale%20synthesized%20human%20benchmark%20focusing%0Aon%20anatomical%20anomalies.%20This%20benchmark%20consists%20of%2056K%20synthesized%20human%0Aimages%2C%20each%20annotated%20with%20detailed%2C%20bounding-box%20level%20labels%20identifying%0A147K%20human%20anomalies%20in%2018%20different%20categories.%20Based%20on%20this%2C%20the%20recognition%0Aof%20human%20anomalies%20can%20be%20established%2C%20which%20in%20turn%20enhances%20image%20generation%0Athrough%20traditional%20techniques%20such%20as%20negative%20prompting%20and%20guidance.%20To%0Afurther%20boost%20the%20improvement%2C%20we%20propose%20HumanRefiner%2C%20a%20novel%20plug-and-play%0Aapproach%20for%20the%20coarse-to-fine%20refinement%20of%20human%20anomalies%20in%20text-to-image%0Ageneration.%20Specifically%2C%20HumanRefiner%20utilizes%20a%20self-diagnostic%20procedure%20to%0Adetect%20and%20correct%20issues%20related%20to%20both%20coarse-grained%20abnormal%20human%20poses%0Aand%20fine-grained%20anomaly%20levels%2C%20facilitating%20pose-reversible%20diffusion%0Ageneration.%20Experimental%20results%20on%20the%20AbHuman%20benchmark%20demonstrate%20that%0AHumanRefiner%20significantly%20reduces%20generative%20discrepancies%2C%20achieving%20a%202.9x%0Aimprovement%20in%20limb%20quality%20compared%20to%20the%20state-of-the-art%20open-source%0Agenerator%20SDXL%20and%20a%201.4x%20improvement%20over%20DALL-E%203%20in%20human%20evaluations.%20Our%0Adata%20and%20code%20are%20available%20at%20https%3A//github.com/Enderfga/HumanRefiner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanRefiner%253A%2520Benchmarking%2520Abnormal%2520Human%2520Generation%2520and%2520Refining%2520with%250A%2520%2520Coarse-to-fine%2520Pose-Reversible%2520Guidance%26entry.906535625%3DGuian%2520Fang%2520and%2520Wenbiao%2520Yan%2520and%2520Yuanfan%2520Guo%2520and%2520Jianhua%2520Han%2520and%2520Zutao%2520Jiang%2520and%2520Hang%2520Xu%2520and%2520Shengcai%2520Liao%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520have%2520significantly%2520advanced%2520in%2520conditional%250Aimage%2520generation.%2520However%252C%2520these%2520models%2520usually%2520struggle%2520with%2520accurately%250Arendering%2520images%2520featuring%2520humans%252C%2520resulting%2520in%2520distorted%2520limbs%2520and%2520other%250Aanomalies.%2520This%2520issue%2520primarily%2520stems%2520from%2520the%2520insufficient%2520recognition%2520and%250Aevaluation%2520of%2520limb%2520qualities%2520in%2520diffusion%2520models.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520AbHuman%252C%2520the%2520first%2520large-scale%2520synthesized%2520human%2520benchmark%2520focusing%250Aon%2520anatomical%2520anomalies.%2520This%2520benchmark%2520consists%2520of%252056K%2520synthesized%2520human%250Aimages%252C%2520each%2520annotated%2520with%2520detailed%252C%2520bounding-box%2520level%2520labels%2520identifying%250A147K%2520human%2520anomalies%2520in%252018%2520different%2520categories.%2520Based%2520on%2520this%252C%2520the%2520recognition%250Aof%2520human%2520anomalies%2520can%2520be%2520established%252C%2520which%2520in%2520turn%2520enhances%2520image%2520generation%250Athrough%2520traditional%2520techniques%2520such%2520as%2520negative%2520prompting%2520and%2520guidance.%2520To%250Afurther%2520boost%2520the%2520improvement%252C%2520we%2520propose%2520HumanRefiner%252C%2520a%2520novel%2520plug-and-play%250Aapproach%2520for%2520the%2520coarse-to-fine%2520refinement%2520of%2520human%2520anomalies%2520in%2520text-to-image%250Ageneration.%2520Specifically%252C%2520HumanRefiner%2520utilizes%2520a%2520self-diagnostic%2520procedure%2520to%250Adetect%2520and%2520correct%2520issues%2520related%2520to%2520both%2520coarse-grained%2520abnormal%2520human%2520poses%250Aand%2520fine-grained%2520anomaly%2520levels%252C%2520facilitating%2520pose-reversible%2520diffusion%250Ageneration.%2520Experimental%2520results%2520on%2520the%2520AbHuman%2520benchmark%2520demonstrate%2520that%250AHumanRefiner%2520significantly%2520reduces%2520generative%2520discrepancies%252C%2520achieving%2520a%25202.9x%250Aimprovement%2520in%2520limb%2520quality%2520compared%2520to%2520the%2520state-of-the-art%2520open-source%250Agenerator%2520SDXL%2520and%2520a%25201.4x%2520improvement%2520over%2520DALL-E%25203%2520in%2520human%2520evaluations.%2520Our%250Adata%2520and%2520code%2520are%2520available%2520at%2520https%253A//github.com/Enderfga/HumanRefiner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanRefiner%3A%20Benchmarking%20Abnormal%20Human%20Generation%20and%20Refining%20with%0A%20%20Coarse-to-fine%20Pose-Reversible%20Guidance&entry.906535625=Guian%20Fang%20and%20Wenbiao%20Yan%20and%20Yuanfan%20Guo%20and%20Jianhua%20Han%20and%20Zutao%20Jiang%20and%20Hang%20Xu%20and%20Shengcai%20Liao%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20significantly%20advanced%20in%20conditional%0Aimage%20generation.%20However%2C%20these%20models%20usually%20struggle%20with%20accurately%0Arendering%20images%20featuring%20humans%2C%20resulting%20in%20distorted%20limbs%20and%20other%0Aanomalies.%20This%20issue%20primarily%20stems%20from%20the%20insufficient%20recognition%20and%0Aevaluation%20of%20limb%20qualities%20in%20diffusion%20models.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20AbHuman%2C%20the%20first%20large-scale%20synthesized%20human%20benchmark%20focusing%0Aon%20anatomical%20anomalies.%20This%20benchmark%20consists%20of%2056K%20synthesized%20human%0Aimages%2C%20each%20annotated%20with%20detailed%2C%20bounding-box%20level%20labels%20identifying%0A147K%20human%20anomalies%20in%2018%20different%20categories.%20Based%20on%20this%2C%20the%20recognition%0Aof%20human%20anomalies%20can%20be%20established%2C%20which%20in%20turn%20enhances%20image%20generation%0Athrough%20traditional%20techniques%20such%20as%20negative%20prompting%20and%20guidance.%20To%0Afurther%20boost%20the%20improvement%2C%20we%20propose%20HumanRefiner%2C%20a%20novel%20plug-and-play%0Aapproach%20for%20the%20coarse-to-fine%20refinement%20of%20human%20anomalies%20in%20text-to-image%0Ageneration.%20Specifically%2C%20HumanRefiner%20utilizes%20a%20self-diagnostic%20procedure%20to%0Adetect%20and%20correct%20issues%20related%20to%20both%20coarse-grained%20abnormal%20human%20poses%0Aand%20fine-grained%20anomaly%20levels%2C%20facilitating%20pose-reversible%20diffusion%0Ageneration.%20Experimental%20results%20on%20the%20AbHuman%20benchmark%20demonstrate%20that%0AHumanRefiner%20significantly%20reduces%20generative%20discrepancies%2C%20achieving%20a%202.9x%0Aimprovement%20in%20limb%20quality%20compared%20to%20the%20state-of-the-art%20open-source%0Agenerator%20SDXL%20and%20a%201.4x%20improvement%20over%20DALL-E%203%20in%20human%20evaluations.%20Our%0Adata%20and%20code%20are%20available%20at%20https%3A//github.com/Enderfga/HumanRefiner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06937v1&entry.124074799=Read"},
{"title": "CS3: Cascade SAM for Sperm Segmentation", "author": "Yi Shi and Xu-Peng Tian and Yun-Kai Wang and Tie-Yi Zhang and Bin Yao and Hui Wang and Yong Shao and Cen-Cen Wang and Rong Zeng and De-Chuan Zhan", "abstract": "  Automated sperm morphology analysis plays a crucial role in the assessment of\nmale fertility, yet its efficacy is often compromised by the challenges in\naccurately segmenting sperm images. Existing segmentation techniques, including\nthe Segment Anything Model(SAM), are notably inadequate in addressing the\ncomplex issue of sperm overlap-a frequent occurrence in clinical samples. Our\nexploratory studies reveal that modifying image characteristics by removing\nsperm heads and easily segmentable areas, alongside enhancing the visibility of\noverlapping regions, markedly enhances SAM's efficiency in segmenting intricate\nsperm structures. Motivated by these findings, we present the Cascade SAM for\nSperm Segmentation (CS3), an unsupervised approach specifically designed to\ntackle the issue of sperm overlap. This method employs a cascade application of\nSAM to segment sperm heads, simple tails, and complex tails in stages.\nSubsequently, these segmented masks are meticulously matched and joined to\nconstruct complete sperm masks. In collaboration with leading medical\ninstitutions, we have compiled a dataset comprising approximately 2,000\nunlabeled sperm images to fine-tune our method, and secured expert annotations\nfor an additional 240 images to facilitate comprehensive model assessment.\nExperimental results demonstrate superior performance of CS3 compared to\nexisting methods.\n", "link": "http://arxiv.org/abs/2407.03772v2", "date": "2024-07-09", "relevancy": 2.4349, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4991}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4809}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CS3%3A%20Cascade%20SAM%20for%20Sperm%20Segmentation&body=Title%3A%20CS3%3A%20Cascade%20SAM%20for%20Sperm%20Segmentation%0AAuthor%3A%20Yi%20Shi%20and%20Xu-Peng%20Tian%20and%20Yun-Kai%20Wang%20and%20Tie-Yi%20Zhang%20and%20Bin%20Yao%20and%20Hui%20Wang%20and%20Yong%20Shao%20and%20Cen-Cen%20Wang%20and%20Rong%20Zeng%20and%20De-Chuan%20Zhan%0AAbstract%3A%20%20%20Automated%20sperm%20morphology%20analysis%20plays%20a%20crucial%20role%20in%20the%20assessment%20of%0Amale%20fertility%2C%20yet%20its%20efficacy%20is%20often%20compromised%20by%20the%20challenges%20in%0Aaccurately%20segmenting%20sperm%20images.%20Existing%20segmentation%20techniques%2C%20including%0Athe%20Segment%20Anything%20Model%28SAM%29%2C%20are%20notably%20inadequate%20in%20addressing%20the%0Acomplex%20issue%20of%20sperm%20overlap-a%20frequent%20occurrence%20in%20clinical%20samples.%20Our%0Aexploratory%20studies%20reveal%20that%20modifying%20image%20characteristics%20by%20removing%0Asperm%20heads%20and%20easily%20segmentable%20areas%2C%20alongside%20enhancing%20the%20visibility%20of%0Aoverlapping%20regions%2C%20markedly%20enhances%20SAM%27s%20efficiency%20in%20segmenting%20intricate%0Asperm%20structures.%20Motivated%20by%20these%20findings%2C%20we%20present%20the%20Cascade%20SAM%20for%0ASperm%20Segmentation%20%28CS3%29%2C%20an%20unsupervised%20approach%20specifically%20designed%20to%0Atackle%20the%20issue%20of%20sperm%20overlap.%20This%20method%20employs%20a%20cascade%20application%20of%0ASAM%20to%20segment%20sperm%20heads%2C%20simple%20tails%2C%20and%20complex%20tails%20in%20stages.%0ASubsequently%2C%20these%20segmented%20masks%20are%20meticulously%20matched%20and%20joined%20to%0Aconstruct%20complete%20sperm%20masks.%20In%20collaboration%20with%20leading%20medical%0Ainstitutions%2C%20we%20have%20compiled%20a%20dataset%20comprising%20approximately%202%2C000%0Aunlabeled%20sperm%20images%20to%20fine-tune%20our%20method%2C%20and%20secured%20expert%20annotations%0Afor%20an%20additional%20240%20images%20to%20facilitate%20comprehensive%20model%20assessment.%0AExperimental%20results%20demonstrate%20superior%20performance%20of%20CS3%20compared%20to%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03772v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCS3%253A%2520Cascade%2520SAM%2520for%2520Sperm%2520Segmentation%26entry.906535625%3DYi%2520Shi%2520and%2520Xu-Peng%2520Tian%2520and%2520Yun-Kai%2520Wang%2520and%2520Tie-Yi%2520Zhang%2520and%2520Bin%2520Yao%2520and%2520Hui%2520Wang%2520and%2520Yong%2520Shao%2520and%2520Cen-Cen%2520Wang%2520and%2520Rong%2520Zeng%2520and%2520De-Chuan%2520Zhan%26entry.1292438233%3D%2520%2520Automated%2520sperm%2520morphology%2520analysis%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520assessment%2520of%250Amale%2520fertility%252C%2520yet%2520its%2520efficacy%2520is%2520often%2520compromised%2520by%2520the%2520challenges%2520in%250Aaccurately%2520segmenting%2520sperm%2520images.%2520Existing%2520segmentation%2520techniques%252C%2520including%250Athe%2520Segment%2520Anything%2520Model%2528SAM%2529%252C%2520are%2520notably%2520inadequate%2520in%2520addressing%2520the%250Acomplex%2520issue%2520of%2520sperm%2520overlap-a%2520frequent%2520occurrence%2520in%2520clinical%2520samples.%2520Our%250Aexploratory%2520studies%2520reveal%2520that%2520modifying%2520image%2520characteristics%2520by%2520removing%250Asperm%2520heads%2520and%2520easily%2520segmentable%2520areas%252C%2520alongside%2520enhancing%2520the%2520visibility%2520of%250Aoverlapping%2520regions%252C%2520markedly%2520enhances%2520SAM%2527s%2520efficiency%2520in%2520segmenting%2520intricate%250Asperm%2520structures.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520present%2520the%2520Cascade%2520SAM%2520for%250ASperm%2520Segmentation%2520%2528CS3%2529%252C%2520an%2520unsupervised%2520approach%2520specifically%2520designed%2520to%250Atackle%2520the%2520issue%2520of%2520sperm%2520overlap.%2520This%2520method%2520employs%2520a%2520cascade%2520application%2520of%250ASAM%2520to%2520segment%2520sperm%2520heads%252C%2520simple%2520tails%252C%2520and%2520complex%2520tails%2520in%2520stages.%250ASubsequently%252C%2520these%2520segmented%2520masks%2520are%2520meticulously%2520matched%2520and%2520joined%2520to%250Aconstruct%2520complete%2520sperm%2520masks.%2520In%2520collaboration%2520with%2520leading%2520medical%250Ainstitutions%252C%2520we%2520have%2520compiled%2520a%2520dataset%2520comprising%2520approximately%25202%252C000%250Aunlabeled%2520sperm%2520images%2520to%2520fine-tune%2520our%2520method%252C%2520and%2520secured%2520expert%2520annotations%250Afor%2520an%2520additional%2520240%2520images%2520to%2520facilitate%2520comprehensive%2520model%2520assessment.%250AExperimental%2520results%2520demonstrate%2520superior%2520performance%2520of%2520CS3%2520compared%2520to%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03772v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CS3%3A%20Cascade%20SAM%20for%20Sperm%20Segmentation&entry.906535625=Yi%20Shi%20and%20Xu-Peng%20Tian%20and%20Yun-Kai%20Wang%20and%20Tie-Yi%20Zhang%20and%20Bin%20Yao%20and%20Hui%20Wang%20and%20Yong%20Shao%20and%20Cen-Cen%20Wang%20and%20Rong%20Zeng%20and%20De-Chuan%20Zhan&entry.1292438233=%20%20Automated%20sperm%20morphology%20analysis%20plays%20a%20crucial%20role%20in%20the%20assessment%20of%0Amale%20fertility%2C%20yet%20its%20efficacy%20is%20often%20compromised%20by%20the%20challenges%20in%0Aaccurately%20segmenting%20sperm%20images.%20Existing%20segmentation%20techniques%2C%20including%0Athe%20Segment%20Anything%20Model%28SAM%29%2C%20are%20notably%20inadequate%20in%20addressing%20the%0Acomplex%20issue%20of%20sperm%20overlap-a%20frequent%20occurrence%20in%20clinical%20samples.%20Our%0Aexploratory%20studies%20reveal%20that%20modifying%20image%20characteristics%20by%20removing%0Asperm%20heads%20and%20easily%20segmentable%20areas%2C%20alongside%20enhancing%20the%20visibility%20of%0Aoverlapping%20regions%2C%20markedly%20enhances%20SAM%27s%20efficiency%20in%20segmenting%20intricate%0Asperm%20structures.%20Motivated%20by%20these%20findings%2C%20we%20present%20the%20Cascade%20SAM%20for%0ASperm%20Segmentation%20%28CS3%29%2C%20an%20unsupervised%20approach%20specifically%20designed%20to%0Atackle%20the%20issue%20of%20sperm%20overlap.%20This%20method%20employs%20a%20cascade%20application%20of%0ASAM%20to%20segment%20sperm%20heads%2C%20simple%20tails%2C%20and%20complex%20tails%20in%20stages.%0ASubsequently%2C%20these%20segmented%20masks%20are%20meticulously%20matched%20and%20joined%20to%0Aconstruct%20complete%20sperm%20masks.%20In%20collaboration%20with%20leading%20medical%0Ainstitutions%2C%20we%20have%20compiled%20a%20dataset%20comprising%20approximately%202%2C000%0Aunlabeled%20sperm%20images%20to%20fine-tune%20our%20method%2C%20and%20secured%20expert%20annotations%0Afor%20an%20additional%20240%20images%20to%20facilitate%20comprehensive%20model%20assessment.%0AExperimental%20results%20demonstrate%20superior%20performance%20of%20CS3%20compared%20to%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03772v2&entry.124074799=Read"},
{"title": "Rethinking Image-to-Video Adaptation: An Object-centric Perspective", "author": "Rui Qian and Shuangrui Ding and Dahua Lin", "abstract": "  Image-to-video adaptation seeks to efficiently adapt image models for use in\nthe video domain. Instead of finetuning the entire image backbone, many\nimage-to-video adaptation paradigms use lightweight adapters for temporal\nmodeling on top of the spatial module. However, these attempts are subject to\nlimitations in efficiency and interpretability. In this paper, we propose a\nnovel and efficient image-to-video adaptation strategy from the object-centric\nperspective. Inspired by human perception, which identifies objects as key\ncomponents for video understanding, we integrate a proxy task of object\ndiscovery into image-to-video transfer learning. Specifically, we adopt slot\nattention with learnable queries to distill each frame into a compact set of\nobject tokens. These object-centric tokens are then processed through\nobject-time interaction layers to model object state changes across time.\nIntegrated with two novel object-level losses, we demonstrate the feasibility\nof performing efficient temporal reasoning solely on the compressed\nobject-centric representations for video downstream tasks. Our method achieves\nstate-of-the-art performance with fewer tunable parameters, only 5\\% of fully\nfinetuned models and 50\\% of efficient tuning methods, on action recognition\nbenchmarks. In addition, our model performs favorably in zero-shot video object\nsegmentation without further retraining or object annotations, proving the\neffectiveness of object-centric video understanding.\n", "link": "http://arxiv.org/abs/2407.06871v1", "date": "2024-07-09", "relevancy": 2.4059, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6448}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6048}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Image-to-Video%20Adaptation%3A%20An%20Object-centric%20Perspective&body=Title%3A%20Rethinking%20Image-to-Video%20Adaptation%3A%20An%20Object-centric%20Perspective%0AAuthor%3A%20Rui%20Qian%20and%20Shuangrui%20Ding%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20Image-to-video%20adaptation%20seeks%20to%20efficiently%20adapt%20image%20models%20for%20use%20in%0Athe%20video%20domain.%20Instead%20of%20finetuning%20the%20entire%20image%20backbone%2C%20many%0Aimage-to-video%20adaptation%20paradigms%20use%20lightweight%20adapters%20for%20temporal%0Amodeling%20on%20top%20of%20the%20spatial%20module.%20However%2C%20these%20attempts%20are%20subject%20to%0Alimitations%20in%20efficiency%20and%20interpretability.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20and%20efficient%20image-to-video%20adaptation%20strategy%20from%20the%20object-centric%0Aperspective.%20Inspired%20by%20human%20perception%2C%20which%20identifies%20objects%20as%20key%0Acomponents%20for%20video%20understanding%2C%20we%20integrate%20a%20proxy%20task%20of%20object%0Adiscovery%20into%20image-to-video%20transfer%20learning.%20Specifically%2C%20we%20adopt%20slot%0Aattention%20with%20learnable%20queries%20to%20distill%20each%20frame%20into%20a%20compact%20set%20of%0Aobject%20tokens.%20These%20object-centric%20tokens%20are%20then%20processed%20through%0Aobject-time%20interaction%20layers%20to%20model%20object%20state%20changes%20across%20time.%0AIntegrated%20with%20two%20novel%20object-level%20losses%2C%20we%20demonstrate%20the%20feasibility%0Aof%20performing%20efficient%20temporal%20reasoning%20solely%20on%20the%20compressed%0Aobject-centric%20representations%20for%20video%20downstream%20tasks.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20with%20fewer%20tunable%20parameters%2C%20only%205%5C%25%20of%20fully%0Afinetuned%20models%20and%2050%5C%25%20of%20efficient%20tuning%20methods%2C%20on%20action%20recognition%0Abenchmarks.%20In%20addition%2C%20our%20model%20performs%20favorably%20in%20zero-shot%20video%20object%0Asegmentation%20without%20further%20retraining%20or%20object%20annotations%2C%20proving%20the%0Aeffectiveness%20of%20object-centric%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Image-to-Video%2520Adaptation%253A%2520An%2520Object-centric%2520Perspective%26entry.906535625%3DRui%2520Qian%2520and%2520Shuangrui%2520Ding%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%2520Image-to-video%2520adaptation%2520seeks%2520to%2520efficiently%2520adapt%2520image%2520models%2520for%2520use%2520in%250Athe%2520video%2520domain.%2520Instead%2520of%2520finetuning%2520the%2520entire%2520image%2520backbone%252C%2520many%250Aimage-to-video%2520adaptation%2520paradigms%2520use%2520lightweight%2520adapters%2520for%2520temporal%250Amodeling%2520on%2520top%2520of%2520the%2520spatial%2520module.%2520However%252C%2520these%2520attempts%2520are%2520subject%2520to%250Alimitations%2520in%2520efficiency%2520and%2520interpretability.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520and%2520efficient%2520image-to-video%2520adaptation%2520strategy%2520from%2520the%2520object-centric%250Aperspective.%2520Inspired%2520by%2520human%2520perception%252C%2520which%2520identifies%2520objects%2520as%2520key%250Acomponents%2520for%2520video%2520understanding%252C%2520we%2520integrate%2520a%2520proxy%2520task%2520of%2520object%250Adiscovery%2520into%2520image-to-video%2520transfer%2520learning.%2520Specifically%252C%2520we%2520adopt%2520slot%250Aattention%2520with%2520learnable%2520queries%2520to%2520distill%2520each%2520frame%2520into%2520a%2520compact%2520set%2520of%250Aobject%2520tokens.%2520These%2520object-centric%2520tokens%2520are%2520then%2520processed%2520through%250Aobject-time%2520interaction%2520layers%2520to%2520model%2520object%2520state%2520changes%2520across%2520time.%250AIntegrated%2520with%2520two%2520novel%2520object-level%2520losses%252C%2520we%2520demonstrate%2520the%2520feasibility%250Aof%2520performing%2520efficient%2520temporal%2520reasoning%2520solely%2520on%2520the%2520compressed%250Aobject-centric%2520representations%2520for%2520video%2520downstream%2520tasks.%2520Our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520with%2520fewer%2520tunable%2520parameters%252C%2520only%25205%255C%2525%2520of%2520fully%250Afinetuned%2520models%2520and%252050%255C%2525%2520of%2520efficient%2520tuning%2520methods%252C%2520on%2520action%2520recognition%250Abenchmarks.%2520In%2520addition%252C%2520our%2520model%2520performs%2520favorably%2520in%2520zero-shot%2520video%2520object%250Asegmentation%2520without%2520further%2520retraining%2520or%2520object%2520annotations%252C%2520proving%2520the%250Aeffectiveness%2520of%2520object-centric%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Image-to-Video%20Adaptation%3A%20An%20Object-centric%20Perspective&entry.906535625=Rui%20Qian%20and%20Shuangrui%20Ding%20and%20Dahua%20Lin&entry.1292438233=%20%20Image-to-video%20adaptation%20seeks%20to%20efficiently%20adapt%20image%20models%20for%20use%20in%0Athe%20video%20domain.%20Instead%20of%20finetuning%20the%20entire%20image%20backbone%2C%20many%0Aimage-to-video%20adaptation%20paradigms%20use%20lightweight%20adapters%20for%20temporal%0Amodeling%20on%20top%20of%20the%20spatial%20module.%20However%2C%20these%20attempts%20are%20subject%20to%0Alimitations%20in%20efficiency%20and%20interpretability.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20and%20efficient%20image-to-video%20adaptation%20strategy%20from%20the%20object-centric%0Aperspective.%20Inspired%20by%20human%20perception%2C%20which%20identifies%20objects%20as%20key%0Acomponents%20for%20video%20understanding%2C%20we%20integrate%20a%20proxy%20task%20of%20object%0Adiscovery%20into%20image-to-video%20transfer%20learning.%20Specifically%2C%20we%20adopt%20slot%0Aattention%20with%20learnable%20queries%20to%20distill%20each%20frame%20into%20a%20compact%20set%20of%0Aobject%20tokens.%20These%20object-centric%20tokens%20are%20then%20processed%20through%0Aobject-time%20interaction%20layers%20to%20model%20object%20state%20changes%20across%20time.%0AIntegrated%20with%20two%20novel%20object-level%20losses%2C%20we%20demonstrate%20the%20feasibility%0Aof%20performing%20efficient%20temporal%20reasoning%20solely%20on%20the%20compressed%0Aobject-centric%20representations%20for%20video%20downstream%20tasks.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20with%20fewer%20tunable%20parameters%2C%20only%205%5C%25%20of%20fully%0Afinetuned%20models%20and%2050%5C%25%20of%20efficient%20tuning%20methods%2C%20on%20action%20recognition%0Abenchmarks.%20In%20addition%2C%20our%20model%20performs%20favorably%20in%20zero-shot%20video%20object%0Asegmentation%20without%20further%20retraining%20or%20object%20annotations%2C%20proving%20the%0Aeffectiveness%20of%20object-centric%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06871v1&entry.124074799=Read"},
{"title": "Changepoint Detection in Highly-Attributed Dynamic Graphs", "author": "Emiliano Penaloza and Nathaniel Stevens", "abstract": "  Detecting anomalous behavior in dynamic networks remains a constant\nchallenge. This problem is further exacerbated when the underlying topology of\nthese networks is affected by individual highly-dimensional node attributes. We\naddress this issue by tracking a network's modularity as a proxy of its\ncommunity structure. We leverage Graph Neural Networks (GNNs) to estimate each\nsnapshot's modularity. GNNs can account for both network structure and\nhigh-dimensional node attributes, providing a comprehensive approach for\nestimating network statistics. Our method is validated through simulations that\ndemonstrate its ability to detect changes in highly-attributed networks by\nanalyzing shifts in modularity. Moreover, we find our method is able to detect\na real-world event within the \\#Iran Twitter reply network, where each node has\nhigh-dimensional textual attributes.\n", "link": "http://arxiv.org/abs/2407.06998v1", "date": "2024-07-09", "relevancy": 2.3994, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5158}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4666}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Changepoint%20Detection%20in%20Highly-Attributed%20Dynamic%20Graphs&body=Title%3A%20Changepoint%20Detection%20in%20Highly-Attributed%20Dynamic%20Graphs%0AAuthor%3A%20Emiliano%20Penaloza%20and%20Nathaniel%20Stevens%0AAbstract%3A%20%20%20Detecting%20anomalous%20behavior%20in%20dynamic%20networks%20remains%20a%20constant%0Achallenge.%20This%20problem%20is%20further%20exacerbated%20when%20the%20underlying%20topology%20of%0Athese%20networks%20is%20affected%20by%20individual%20highly-dimensional%20node%20attributes.%20We%0Aaddress%20this%20issue%20by%20tracking%20a%20network%27s%20modularity%20as%20a%20proxy%20of%20its%0Acommunity%20structure.%20We%20leverage%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20estimate%20each%0Asnapshot%27s%20modularity.%20GNNs%20can%20account%20for%20both%20network%20structure%20and%0Ahigh-dimensional%20node%20attributes%2C%20providing%20a%20comprehensive%20approach%20for%0Aestimating%20network%20statistics.%20Our%20method%20is%20validated%20through%20simulations%20that%0Ademonstrate%20its%20ability%20to%20detect%20changes%20in%20highly-attributed%20networks%20by%0Aanalyzing%20shifts%20in%20modularity.%20Moreover%2C%20we%20find%20our%20method%20is%20able%20to%20detect%0Aa%20real-world%20event%20within%20the%20%5C%23Iran%20Twitter%20reply%20network%2C%20where%20each%20node%20has%0Ahigh-dimensional%20textual%20attributes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChangepoint%2520Detection%2520in%2520Highly-Attributed%2520Dynamic%2520Graphs%26entry.906535625%3DEmiliano%2520Penaloza%2520and%2520Nathaniel%2520Stevens%26entry.1292438233%3D%2520%2520Detecting%2520anomalous%2520behavior%2520in%2520dynamic%2520networks%2520remains%2520a%2520constant%250Achallenge.%2520This%2520problem%2520is%2520further%2520exacerbated%2520when%2520the%2520underlying%2520topology%2520of%250Athese%2520networks%2520is%2520affected%2520by%2520individual%2520highly-dimensional%2520node%2520attributes.%2520We%250Aaddress%2520this%2520issue%2520by%2520tracking%2520a%2520network%2527s%2520modularity%2520as%2520a%2520proxy%2520of%2520its%250Acommunity%2520structure.%2520We%2520leverage%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%2520estimate%2520each%250Asnapshot%2527s%2520modularity.%2520GNNs%2520can%2520account%2520for%2520both%2520network%2520structure%2520and%250Ahigh-dimensional%2520node%2520attributes%252C%2520providing%2520a%2520comprehensive%2520approach%2520for%250Aestimating%2520network%2520statistics.%2520Our%2520method%2520is%2520validated%2520through%2520simulations%2520that%250Ademonstrate%2520its%2520ability%2520to%2520detect%2520changes%2520in%2520highly-attributed%2520networks%2520by%250Aanalyzing%2520shifts%2520in%2520modularity.%2520Moreover%252C%2520we%2520find%2520our%2520method%2520is%2520able%2520to%2520detect%250Aa%2520real-world%2520event%2520within%2520the%2520%255C%2523Iran%2520Twitter%2520reply%2520network%252C%2520where%2520each%2520node%2520has%250Ahigh-dimensional%2520textual%2520attributes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Changepoint%20Detection%20in%20Highly-Attributed%20Dynamic%20Graphs&entry.906535625=Emiliano%20Penaloza%20and%20Nathaniel%20Stevens&entry.1292438233=%20%20Detecting%20anomalous%20behavior%20in%20dynamic%20networks%20remains%20a%20constant%0Achallenge.%20This%20problem%20is%20further%20exacerbated%20when%20the%20underlying%20topology%20of%0Athese%20networks%20is%20affected%20by%20individual%20highly-dimensional%20node%20attributes.%20We%0Aaddress%20this%20issue%20by%20tracking%20a%20network%27s%20modularity%20as%20a%20proxy%20of%20its%0Acommunity%20structure.%20We%20leverage%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20estimate%20each%0Asnapshot%27s%20modularity.%20GNNs%20can%20account%20for%20both%20network%20structure%20and%0Ahigh-dimensional%20node%20attributes%2C%20providing%20a%20comprehensive%20approach%20for%0Aestimating%20network%20statistics.%20Our%20method%20is%20validated%20through%20simulations%20that%0Ademonstrate%20its%20ability%20to%20detect%20changes%20in%20highly-attributed%20networks%20by%0Aanalyzing%20shifts%20in%20modularity.%20Moreover%2C%20we%20find%20our%20method%20is%20able%20to%20detect%0Aa%20real-world%20event%20within%20the%20%5C%23Iran%20Twitter%20reply%20network%2C%20where%20each%20node%20has%0Ahigh-dimensional%20textual%20attributes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06998v1&entry.124074799=Read"},
{"title": "Improved Block Merging for 3D Point Cloud Instance Segmentation", "author": "Leon Denis and Remco Royen and Adrian Munteanu", "abstract": "  This paper proposes a novel block merging algorithm suitable for any\nblock-based 3D instance segmentation technique. The proposed work improves over\nthe state-of-the-art by allowing wrongly labelled points of already processed\nblocks to be corrected through label propagation. By doing so, instance overlap\nbetween blocks is not anymore necessary to produce the desirable results, which\nis the main limitation of the current art. Our experiments show that the\nproposed block merging algorithm significantly and consistently improves the\nobtained accuracy for all evaluation metrics employed in literature, regardless\nof the underlying network architecture.\n", "link": "http://arxiv.org/abs/2407.06991v1", "date": "2024-07-09", "relevancy": 2.3918, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5158}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4612}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Block%20Merging%20for%203D%20Point%20Cloud%20Instance%20Segmentation&body=Title%3A%20Improved%20Block%20Merging%20for%203D%20Point%20Cloud%20Instance%20Segmentation%0AAuthor%3A%20Leon%20Denis%20and%20Remco%20Royen%20and%20Adrian%20Munteanu%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20block%20merging%20algorithm%20suitable%20for%20any%0Ablock-based%203D%20instance%20segmentation%20technique.%20The%20proposed%20work%20improves%20over%0Athe%20state-of-the-art%20by%20allowing%20wrongly%20labelled%20points%20of%20already%20processed%0Ablocks%20to%20be%20corrected%20through%20label%20propagation.%20By%20doing%20so%2C%20instance%20overlap%0Abetween%20blocks%20is%20not%20anymore%20necessary%20to%20produce%20the%20desirable%20results%2C%20which%0Ais%20the%20main%20limitation%20of%20the%20current%20art.%20Our%20experiments%20show%20that%20the%0Aproposed%20block%20merging%20algorithm%20significantly%20and%20consistently%20improves%20the%0Aobtained%20accuracy%20for%20all%20evaluation%20metrics%20employed%20in%20literature%2C%20regardless%0Aof%20the%20underlying%20network%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Block%2520Merging%2520for%25203D%2520Point%2520Cloud%2520Instance%2520Segmentation%26entry.906535625%3DLeon%2520Denis%2520and%2520Remco%2520Royen%2520and%2520Adrian%2520Munteanu%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520block%2520merging%2520algorithm%2520suitable%2520for%2520any%250Ablock-based%25203D%2520instance%2520segmentation%2520technique.%2520The%2520proposed%2520work%2520improves%2520over%250Athe%2520state-of-the-art%2520by%2520allowing%2520wrongly%2520labelled%2520points%2520of%2520already%2520processed%250Ablocks%2520to%2520be%2520corrected%2520through%2520label%2520propagation.%2520By%2520doing%2520so%252C%2520instance%2520overlap%250Abetween%2520blocks%2520is%2520not%2520anymore%2520necessary%2520to%2520produce%2520the%2520desirable%2520results%252C%2520which%250Ais%2520the%2520main%2520limitation%2520of%2520the%2520current%2520art.%2520Our%2520experiments%2520show%2520that%2520the%250Aproposed%2520block%2520merging%2520algorithm%2520significantly%2520and%2520consistently%2520improves%2520the%250Aobtained%2520accuracy%2520for%2520all%2520evaluation%2520metrics%2520employed%2520in%2520literature%252C%2520regardless%250Aof%2520the%2520underlying%2520network%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Block%20Merging%20for%203D%20Point%20Cloud%20Instance%20Segmentation&entry.906535625=Leon%20Denis%20and%20Remco%20Royen%20and%20Adrian%20Munteanu&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20block%20merging%20algorithm%20suitable%20for%20any%0Ablock-based%203D%20instance%20segmentation%20technique.%20The%20proposed%20work%20improves%20over%0Athe%20state-of-the-art%20by%20allowing%20wrongly%20labelled%20points%20of%20already%20processed%0Ablocks%20to%20be%20corrected%20through%20label%20propagation.%20By%20doing%20so%2C%20instance%20overlap%0Abetween%20blocks%20is%20not%20anymore%20necessary%20to%20produce%20the%20desirable%20results%2C%20which%0Ais%20the%20main%20limitation%20of%20the%20current%20art.%20Our%20experiments%20show%20that%20the%0Aproposed%20block%20merging%20algorithm%20significantly%20and%20consistently%20improves%20the%0Aobtained%20accuracy%20for%20all%20evaluation%20metrics%20employed%20in%20literature%2C%20regardless%0Aof%20the%20underlying%20network%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06991v1&entry.124074799=Read"},
{"title": "A Neurosymbolic Approach to Adaptive Feature Extraction in SLAM", "author": "Yasra Chandio and Momin A. Khan and Khotso Selialia and Luis Garcia and Joseph DeGol and Fatima M. Anwar", "abstract": "  Autonomous robots, autonomous vehicles, and humans wearing mixed-reality\nheadsets require accurate and reliable tracking services for safety-critical\napplications in dynamically changing real-world environments. However, the\nexisting tracking approaches, such as Simultaneous Localization and Mapping\n(SLAM), do not adapt well to environmental changes and boundary conditions\ndespite extensive manual tuning. On the other hand, while deep learning-based\napproaches can better adapt to environmental changes, they typically demand\nsubstantial data for training and often lack flexibility in adapting to new\ndomains. To solve this problem, we propose leveraging the neurosymbolic program\nsynthesis approach to construct adaptable SLAM pipelines that integrate the\ndomain knowledge from traditional SLAM approaches while leveraging data to\nlearn complex relationships. While the approach can synthesize end-to-end SLAM\npipelines, we focus on synthesizing the feature extraction module. We first\ndevise a domain-specific language (DSL) that can encapsulate domain knowledge\non the important attributes for feature extraction and the real-world\nperformance of various feature extractors. Our neurosymbolic architecture then\nundertakes adaptive feature extraction, optimizing parameters via learning\nwhile employing symbolic reasoning to select the most suitable feature\nextractor. Our evaluations demonstrate that our approach, neurosymbolic Feature\nEXtraction (nFEX), yields higher-quality features. It also reduces the pose\nerror observed for the state-of-the-art baseline feature extractors ORB and\nSIFT by up to 90% and up to 66%, respectively, thereby enhancing the system's\nefficiency and adaptability to novel environments.\n", "link": "http://arxiv.org/abs/2407.06889v1", "date": "2024-07-09", "relevancy": 2.3821, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5988}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5976}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Neurosymbolic%20Approach%20to%20Adaptive%20Feature%20Extraction%20in%20SLAM&body=Title%3A%20A%20Neurosymbolic%20Approach%20to%20Adaptive%20Feature%20Extraction%20in%20SLAM%0AAuthor%3A%20Yasra%20Chandio%20and%20Momin%20A.%20Khan%20and%20Khotso%20Selialia%20and%20Luis%20Garcia%20and%20Joseph%20DeGol%20and%20Fatima%20M.%20Anwar%0AAbstract%3A%20%20%20Autonomous%20robots%2C%20autonomous%20vehicles%2C%20and%20humans%20wearing%20mixed-reality%0Aheadsets%20require%20accurate%20and%20reliable%20tracking%20services%20for%20safety-critical%0Aapplications%20in%20dynamically%20changing%20real-world%20environments.%20However%2C%20the%0Aexisting%20tracking%20approaches%2C%20such%20as%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%2C%20do%20not%20adapt%20well%20to%20environmental%20changes%20and%20boundary%20conditions%0Adespite%20extensive%20manual%20tuning.%20On%20the%20other%20hand%2C%20while%20deep%20learning-based%0Aapproaches%20can%20better%20adapt%20to%20environmental%20changes%2C%20they%20typically%20demand%0Asubstantial%20data%20for%20training%20and%20often%20lack%20flexibility%20in%20adapting%20to%20new%0Adomains.%20To%20solve%20this%20problem%2C%20we%20propose%20leveraging%20the%20neurosymbolic%20program%0Asynthesis%20approach%20to%20construct%20adaptable%20SLAM%20pipelines%20that%20integrate%20the%0Adomain%20knowledge%20from%20traditional%20SLAM%20approaches%20while%20leveraging%20data%20to%0Alearn%20complex%20relationships.%20While%20the%20approach%20can%20synthesize%20end-to-end%20SLAM%0Apipelines%2C%20we%20focus%20on%20synthesizing%20the%20feature%20extraction%20module.%20We%20first%0Adevise%20a%20domain-specific%20language%20%28DSL%29%20that%20can%20encapsulate%20domain%20knowledge%0Aon%20the%20important%20attributes%20for%20feature%20extraction%20and%20the%20real-world%0Aperformance%20of%20various%20feature%20extractors.%20Our%20neurosymbolic%20architecture%20then%0Aundertakes%20adaptive%20feature%20extraction%2C%20optimizing%20parameters%20via%20learning%0Awhile%20employing%20symbolic%20reasoning%20to%20select%20the%20most%20suitable%20feature%0Aextractor.%20Our%20evaluations%20demonstrate%20that%20our%20approach%2C%20neurosymbolic%20Feature%0AEXtraction%20%28nFEX%29%2C%20yields%20higher-quality%20features.%20It%20also%20reduces%20the%20pose%0Aerror%20observed%20for%20the%20state-of-the-art%20baseline%20feature%20extractors%20ORB%20and%0ASIFT%20by%20up%20to%2090%25%20and%20up%20to%2066%25%2C%20respectively%2C%20thereby%20enhancing%20the%20system%27s%0Aefficiency%20and%20adaptability%20to%20novel%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Neurosymbolic%2520Approach%2520to%2520Adaptive%2520Feature%2520Extraction%2520in%2520SLAM%26entry.906535625%3DYasra%2520Chandio%2520and%2520Momin%2520A.%2520Khan%2520and%2520Khotso%2520Selialia%2520and%2520Luis%2520Garcia%2520and%2520Joseph%2520DeGol%2520and%2520Fatima%2520M.%2520Anwar%26entry.1292438233%3D%2520%2520Autonomous%2520robots%252C%2520autonomous%2520vehicles%252C%2520and%2520humans%2520wearing%2520mixed-reality%250Aheadsets%2520require%2520accurate%2520and%2520reliable%2520tracking%2520services%2520for%2520safety-critical%250Aapplications%2520in%2520dynamically%2520changing%2520real-world%2520environments.%2520However%252C%2520the%250Aexisting%2520tracking%2520approaches%252C%2520such%2520as%2520Simultaneous%2520Localization%2520and%2520Mapping%250A%2528SLAM%2529%252C%2520do%2520not%2520adapt%2520well%2520to%2520environmental%2520changes%2520and%2520boundary%2520conditions%250Adespite%2520extensive%2520manual%2520tuning.%2520On%2520the%2520other%2520hand%252C%2520while%2520deep%2520learning-based%250Aapproaches%2520can%2520better%2520adapt%2520to%2520environmental%2520changes%252C%2520they%2520typically%2520demand%250Asubstantial%2520data%2520for%2520training%2520and%2520often%2520lack%2520flexibility%2520in%2520adapting%2520to%2520new%250Adomains.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520leveraging%2520the%2520neurosymbolic%2520program%250Asynthesis%2520approach%2520to%2520construct%2520adaptable%2520SLAM%2520pipelines%2520that%2520integrate%2520the%250Adomain%2520knowledge%2520from%2520traditional%2520SLAM%2520approaches%2520while%2520leveraging%2520data%2520to%250Alearn%2520complex%2520relationships.%2520While%2520the%2520approach%2520can%2520synthesize%2520end-to-end%2520SLAM%250Apipelines%252C%2520we%2520focus%2520on%2520synthesizing%2520the%2520feature%2520extraction%2520module.%2520We%2520first%250Adevise%2520a%2520domain-specific%2520language%2520%2528DSL%2529%2520that%2520can%2520encapsulate%2520domain%2520knowledge%250Aon%2520the%2520important%2520attributes%2520for%2520feature%2520extraction%2520and%2520the%2520real-world%250Aperformance%2520of%2520various%2520feature%2520extractors.%2520Our%2520neurosymbolic%2520architecture%2520then%250Aundertakes%2520adaptive%2520feature%2520extraction%252C%2520optimizing%2520parameters%2520via%2520learning%250Awhile%2520employing%2520symbolic%2520reasoning%2520to%2520select%2520the%2520most%2520suitable%2520feature%250Aextractor.%2520Our%2520evaluations%2520demonstrate%2520that%2520our%2520approach%252C%2520neurosymbolic%2520Feature%250AEXtraction%2520%2528nFEX%2529%252C%2520yields%2520higher-quality%2520features.%2520It%2520also%2520reduces%2520the%2520pose%250Aerror%2520observed%2520for%2520the%2520state-of-the-art%2520baseline%2520feature%2520extractors%2520ORB%2520and%250ASIFT%2520by%2520up%2520to%252090%2525%2520and%2520up%2520to%252066%2525%252C%2520respectively%252C%2520thereby%2520enhancing%2520the%2520system%2527s%250Aefficiency%2520and%2520adaptability%2520to%2520novel%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Neurosymbolic%20Approach%20to%20Adaptive%20Feature%20Extraction%20in%20SLAM&entry.906535625=Yasra%20Chandio%20and%20Momin%20A.%20Khan%20and%20Khotso%20Selialia%20and%20Luis%20Garcia%20and%20Joseph%20DeGol%20and%20Fatima%20M.%20Anwar&entry.1292438233=%20%20Autonomous%20robots%2C%20autonomous%20vehicles%2C%20and%20humans%20wearing%20mixed-reality%0Aheadsets%20require%20accurate%20and%20reliable%20tracking%20services%20for%20safety-critical%0Aapplications%20in%20dynamically%20changing%20real-world%20environments.%20However%2C%20the%0Aexisting%20tracking%20approaches%2C%20such%20as%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%2C%20do%20not%20adapt%20well%20to%20environmental%20changes%20and%20boundary%20conditions%0Adespite%20extensive%20manual%20tuning.%20On%20the%20other%20hand%2C%20while%20deep%20learning-based%0Aapproaches%20can%20better%20adapt%20to%20environmental%20changes%2C%20they%20typically%20demand%0Asubstantial%20data%20for%20training%20and%20often%20lack%20flexibility%20in%20adapting%20to%20new%0Adomains.%20To%20solve%20this%20problem%2C%20we%20propose%20leveraging%20the%20neurosymbolic%20program%0Asynthesis%20approach%20to%20construct%20adaptable%20SLAM%20pipelines%20that%20integrate%20the%0Adomain%20knowledge%20from%20traditional%20SLAM%20approaches%20while%20leveraging%20data%20to%0Alearn%20complex%20relationships.%20While%20the%20approach%20can%20synthesize%20end-to-end%20SLAM%0Apipelines%2C%20we%20focus%20on%20synthesizing%20the%20feature%20extraction%20module.%20We%20first%0Adevise%20a%20domain-specific%20language%20%28DSL%29%20that%20can%20encapsulate%20domain%20knowledge%0Aon%20the%20important%20attributes%20for%20feature%20extraction%20and%20the%20real-world%0Aperformance%20of%20various%20feature%20extractors.%20Our%20neurosymbolic%20architecture%20then%0Aundertakes%20adaptive%20feature%20extraction%2C%20optimizing%20parameters%20via%20learning%0Awhile%20employing%20symbolic%20reasoning%20to%20select%20the%20most%20suitable%20feature%0Aextractor.%20Our%20evaluations%20demonstrate%20that%20our%20approach%2C%20neurosymbolic%20Feature%0AEXtraction%20%28nFEX%29%2C%20yields%20higher-quality%20features.%20It%20also%20reduces%20the%20pose%0Aerror%20observed%20for%20the%20state-of-the-art%20baseline%20feature%20extractors%20ORB%20and%0ASIFT%20by%20up%20to%2090%25%20and%20up%20to%2066%25%2C%20respectively%2C%20thereby%20enhancing%20the%20system%27s%0Aefficiency%20and%20adaptability%20to%20novel%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06889v1&entry.124074799=Read"},
{"title": "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning\n  Instruction Using Language Model", "author": "Wenqi Zhang and Zhenglin Cheng and Yuanyu He and Mengna Wang and Yongliang Shen and Zeqi Tan and Guiyang Hou and Mingqian He and Yanna Ma and Weiming Lu and Yueting Zhuang", "abstract": "  Although most current large multimodal models (LMMs) can already understand\nphotos of natural scenes and portraits, their understanding of abstract images,\ne.g., charts, maps, or layouts, and visual reasoning capabilities remains quite\nrudimentary. They often struggle with simple daily tasks, such as reading time\nfrom a clock, understanding a flowchart, or planning a route using a road map.\nIn light of this, we design a multi-modal self-instruct, utilizing large\nlanguage models and their code capabilities to synthesize massive abstract\nimages and visual reasoning instructions across daily scenarios. Our strategy\neffortlessly creates a multimodal benchmark with 11,193 instructions for eight\nvisual scenarios: charts, tables, simulated maps, dashboards, flowcharts,\nrelation graphs, floor plans, and visual puzzles. \\textbf{This benchmark,\nconstructed with simple lines and geometric elements, exposes the shortcomings\nof most advanced LMMs} like Claude-3.5-Sonnet and GPT-4o in abstract image\nunderstanding, spatial relations reasoning, and visual element induction.\nBesides, to verify the quality of our synthetic data, we fine-tune an LMM using\n62,476 synthetic chart, table and road map instructions. The results\ndemonstrate improved chart understanding and map navigation performance, and\nalso demonstrate potential benefits for other visual reasoning tasks. Our code\nis available at: \\url{https://github.com/zwq2018/Multi-modal-Self-instruct}.\n", "link": "http://arxiv.org/abs/2407.07053v1", "date": "2024-07-09", "relevancy": 2.3753, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.601}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5972}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Self-Instruct%3A%20Synthetic%20Abstract%20Image%20and%20Visual%20Reasoning%0A%20%20Instruction%20Using%20Language%20Model&body=Title%3A%20Multimodal%20Self-Instruct%3A%20Synthetic%20Abstract%20Image%20and%20Visual%20Reasoning%0A%20%20Instruction%20Using%20Language%20Model%0AAuthor%3A%20Wenqi%20Zhang%20and%20Zhenglin%20Cheng%20and%20Yuanyu%20He%20and%20Mengna%20Wang%20and%20Yongliang%20Shen%20and%20Zeqi%20Tan%20and%20Guiyang%20Hou%20and%20Mingqian%20He%20and%20Yanna%20Ma%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Although%20most%20current%20large%20multimodal%20models%20%28LMMs%29%20can%20already%20understand%0Aphotos%20of%20natural%20scenes%20and%20portraits%2C%20their%20understanding%20of%20abstract%20images%2C%0Ae.g.%2C%20charts%2C%20maps%2C%20or%20layouts%2C%20and%20visual%20reasoning%20capabilities%20remains%20quite%0Arudimentary.%20They%20often%20struggle%20with%20simple%20daily%20tasks%2C%20such%20as%20reading%20time%0Afrom%20a%20clock%2C%20understanding%20a%20flowchart%2C%20or%20planning%20a%20route%20using%20a%20road%20map.%0AIn%20light%20of%20this%2C%20we%20design%20a%20multi-modal%20self-instruct%2C%20utilizing%20large%0Alanguage%20models%20and%20their%20code%20capabilities%20to%20synthesize%20massive%20abstract%0Aimages%20and%20visual%20reasoning%20instructions%20across%20daily%20scenarios.%20Our%20strategy%0Aeffortlessly%20creates%20a%20multimodal%20benchmark%20with%2011%2C193%20instructions%20for%20eight%0Avisual%20scenarios%3A%20charts%2C%20tables%2C%20simulated%20maps%2C%20dashboards%2C%20flowcharts%2C%0Arelation%20graphs%2C%20floor%20plans%2C%20and%20visual%20puzzles.%20%5Ctextbf%7BThis%20benchmark%2C%0Aconstructed%20with%20simple%20lines%20and%20geometric%20elements%2C%20exposes%20the%20shortcomings%0Aof%20most%20advanced%20LMMs%7D%20like%20Claude-3.5-Sonnet%20and%20GPT-4o%20in%20abstract%20image%0Aunderstanding%2C%20spatial%20relations%20reasoning%2C%20and%20visual%20element%20induction.%0ABesides%2C%20to%20verify%20the%20quality%20of%20our%20synthetic%20data%2C%20we%20fine-tune%20an%20LMM%20using%0A62%2C476%20synthetic%20chart%2C%20table%20and%20road%20map%20instructions.%20The%20results%0Ademonstrate%20improved%20chart%20understanding%20and%20map%20navigation%20performance%2C%20and%0Aalso%20demonstrate%20potential%20benefits%20for%20other%20visual%20reasoning%20tasks.%20Our%20code%0Ais%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/zwq2018/Multi-modal-Self-instruct%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Self-Instruct%253A%2520Synthetic%2520Abstract%2520Image%2520and%2520Visual%2520Reasoning%250A%2520%2520Instruction%2520Using%2520Language%2520Model%26entry.906535625%3DWenqi%2520Zhang%2520and%2520Zhenglin%2520Cheng%2520and%2520Yuanyu%2520He%2520and%2520Mengna%2520Wang%2520and%2520Yongliang%2520Shen%2520and%2520Zeqi%2520Tan%2520and%2520Guiyang%2520Hou%2520and%2520Mingqian%2520He%2520and%2520Yanna%2520Ma%2520and%2520Weiming%2520Lu%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Although%2520most%2520current%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520can%2520already%2520understand%250Aphotos%2520of%2520natural%2520scenes%2520and%2520portraits%252C%2520their%2520understanding%2520of%2520abstract%2520images%252C%250Ae.g.%252C%2520charts%252C%2520maps%252C%2520or%2520layouts%252C%2520and%2520visual%2520reasoning%2520capabilities%2520remains%2520quite%250Arudimentary.%2520They%2520often%2520struggle%2520with%2520simple%2520daily%2520tasks%252C%2520such%2520as%2520reading%2520time%250Afrom%2520a%2520clock%252C%2520understanding%2520a%2520flowchart%252C%2520or%2520planning%2520a%2520route%2520using%2520a%2520road%2520map.%250AIn%2520light%2520of%2520this%252C%2520we%2520design%2520a%2520multi-modal%2520self-instruct%252C%2520utilizing%2520large%250Alanguage%2520models%2520and%2520their%2520code%2520capabilities%2520to%2520synthesize%2520massive%2520abstract%250Aimages%2520and%2520visual%2520reasoning%2520instructions%2520across%2520daily%2520scenarios.%2520Our%2520strategy%250Aeffortlessly%2520creates%2520a%2520multimodal%2520benchmark%2520with%252011%252C193%2520instructions%2520for%2520eight%250Avisual%2520scenarios%253A%2520charts%252C%2520tables%252C%2520simulated%2520maps%252C%2520dashboards%252C%2520flowcharts%252C%250Arelation%2520graphs%252C%2520floor%2520plans%252C%2520and%2520visual%2520puzzles.%2520%255Ctextbf%257BThis%2520benchmark%252C%250Aconstructed%2520with%2520simple%2520lines%2520and%2520geometric%2520elements%252C%2520exposes%2520the%2520shortcomings%250Aof%2520most%2520advanced%2520LMMs%257D%2520like%2520Claude-3.5-Sonnet%2520and%2520GPT-4o%2520in%2520abstract%2520image%250Aunderstanding%252C%2520spatial%2520relations%2520reasoning%252C%2520and%2520visual%2520element%2520induction.%250ABesides%252C%2520to%2520verify%2520the%2520quality%2520of%2520our%2520synthetic%2520data%252C%2520we%2520fine-tune%2520an%2520LMM%2520using%250A62%252C476%2520synthetic%2520chart%252C%2520table%2520and%2520road%2520map%2520instructions.%2520The%2520results%250Ademonstrate%2520improved%2520chart%2520understanding%2520and%2520map%2520navigation%2520performance%252C%2520and%250Aalso%2520demonstrate%2520potential%2520benefits%2520for%2520other%2520visual%2520reasoning%2520tasks.%2520Our%2520code%250Ais%2520available%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/zwq2018/Multi-modal-Self-instruct%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Self-Instruct%3A%20Synthetic%20Abstract%20Image%20and%20Visual%20Reasoning%0A%20%20Instruction%20Using%20Language%20Model&entry.906535625=Wenqi%20Zhang%20and%20Zhenglin%20Cheng%20and%20Yuanyu%20He%20and%20Mengna%20Wang%20and%20Yongliang%20Shen%20and%20Zeqi%20Tan%20and%20Guiyang%20Hou%20and%20Mingqian%20He%20and%20Yanna%20Ma%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Although%20most%20current%20large%20multimodal%20models%20%28LMMs%29%20can%20already%20understand%0Aphotos%20of%20natural%20scenes%20and%20portraits%2C%20their%20understanding%20of%20abstract%20images%2C%0Ae.g.%2C%20charts%2C%20maps%2C%20or%20layouts%2C%20and%20visual%20reasoning%20capabilities%20remains%20quite%0Arudimentary.%20They%20often%20struggle%20with%20simple%20daily%20tasks%2C%20such%20as%20reading%20time%0Afrom%20a%20clock%2C%20understanding%20a%20flowchart%2C%20or%20planning%20a%20route%20using%20a%20road%20map.%0AIn%20light%20of%20this%2C%20we%20design%20a%20multi-modal%20self-instruct%2C%20utilizing%20large%0Alanguage%20models%20and%20their%20code%20capabilities%20to%20synthesize%20massive%20abstract%0Aimages%20and%20visual%20reasoning%20instructions%20across%20daily%20scenarios.%20Our%20strategy%0Aeffortlessly%20creates%20a%20multimodal%20benchmark%20with%2011%2C193%20instructions%20for%20eight%0Avisual%20scenarios%3A%20charts%2C%20tables%2C%20simulated%20maps%2C%20dashboards%2C%20flowcharts%2C%0Arelation%20graphs%2C%20floor%20plans%2C%20and%20visual%20puzzles.%20%5Ctextbf%7BThis%20benchmark%2C%0Aconstructed%20with%20simple%20lines%20and%20geometric%20elements%2C%20exposes%20the%20shortcomings%0Aof%20most%20advanced%20LMMs%7D%20like%20Claude-3.5-Sonnet%20and%20GPT-4o%20in%20abstract%20image%0Aunderstanding%2C%20spatial%20relations%20reasoning%2C%20and%20visual%20element%20induction.%0ABesides%2C%20to%20verify%20the%20quality%20of%20our%20synthetic%20data%2C%20we%20fine-tune%20an%20LMM%20using%0A62%2C476%20synthetic%20chart%2C%20table%20and%20road%20map%20instructions.%20The%20results%0Ademonstrate%20improved%20chart%20understanding%20and%20map%20navigation%20performance%2C%20and%0Aalso%20demonstrate%20potential%20benefits%20for%20other%20visual%20reasoning%20tasks.%20Our%20code%0Ais%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/zwq2018/Multi-modal-Self-instruct%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07053v1&entry.124074799=Read"},
{"title": "Latent Space Imaging", "author": "Matheus Souza and Yidan Zheng and Kaizhang Kang and Yogeshwar Nath Mishra and Qiang Fu and Wolfgang Heidrich", "abstract": "  Digital imaging systems have classically been based on brute-force measuring\nand processing of pixels organized on regular grids. The human visual system,\non the other hand, performs a massive data reduction from the number of\nphoto-receptors to the optic nerve, essentially encoding the image information\ninto a low bandwidth latent space representation suitable for processing by the\nhuman brain. In this work, we propose to follow a similar approach for the\ndevelopment of artificial vision systems. Latent Space Imaging is a new\nparadigm that, through a combination of optics and software, directly encodes\nthe image information into the semantically rich latent space of a generative\nmodel, thus substantially reducing bandwidth and memory requirements during the\ncapture process. We demonstrate this new principle through an initial hardware\nprototype based on the single pixel camera. By designing an amplitude\nmodulation scheme that encodes into the latent space of a generative model, we\nachieve compression ratios from 1:100 to 1:1,000 during the imaging process,\nillustrating the potential of latent space imaging for highly efficient imaging\nhardware, to enable future applications in high speed imaging, or task-specific\ncameras with substantially reduced hardware complexity.\n", "link": "http://arxiv.org/abs/2407.07052v1", "date": "2024-07-09", "relevancy": 2.3554, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6355}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6019}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Space%20Imaging&body=Title%3A%20Latent%20Space%20Imaging%0AAuthor%3A%20Matheus%20Souza%20and%20Yidan%20Zheng%20and%20Kaizhang%20Kang%20and%20Yogeshwar%20Nath%20Mishra%20and%20Qiang%20Fu%20and%20Wolfgang%20Heidrich%0AAbstract%3A%20%20%20Digital%20imaging%20systems%20have%20classically%20been%20based%20on%20brute-force%20measuring%0Aand%20processing%20of%20pixels%20organized%20on%20regular%20grids.%20The%20human%20visual%20system%2C%0Aon%20the%20other%20hand%2C%20performs%20a%20massive%20data%20reduction%20from%20the%20number%20of%0Aphoto-receptors%20to%20the%20optic%20nerve%2C%20essentially%20encoding%20the%20image%20information%0Ainto%20a%20low%20bandwidth%20latent%20space%20representation%20suitable%20for%20processing%20by%20the%0Ahuman%20brain.%20In%20this%20work%2C%20we%20propose%20to%20follow%20a%20similar%20approach%20for%20the%0Adevelopment%20of%20artificial%20vision%20systems.%20Latent%20Space%20Imaging%20is%20a%20new%0Aparadigm%20that%2C%20through%20a%20combination%20of%20optics%20and%20software%2C%20directly%20encodes%0Athe%20image%20information%20into%20the%20semantically%20rich%20latent%20space%20of%20a%20generative%0Amodel%2C%20thus%20substantially%20reducing%20bandwidth%20and%20memory%20requirements%20during%20the%0Acapture%20process.%20We%20demonstrate%20this%20new%20principle%20through%20an%20initial%20hardware%0Aprototype%20based%20on%20the%20single%20pixel%20camera.%20By%20designing%20an%20amplitude%0Amodulation%20scheme%20that%20encodes%20into%20the%20latent%20space%20of%20a%20generative%20model%2C%20we%0Aachieve%20compression%20ratios%20from%201%3A100%20to%201%3A1%2C000%20during%20the%20imaging%20process%2C%0Aillustrating%20the%20potential%20of%20latent%20space%20imaging%20for%20highly%20efficient%20imaging%0Ahardware%2C%20to%20enable%20future%20applications%20in%20high%20speed%20imaging%2C%20or%20task-specific%0Acameras%20with%20substantially%20reduced%20hardware%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Space%2520Imaging%26entry.906535625%3DMatheus%2520Souza%2520and%2520Yidan%2520Zheng%2520and%2520Kaizhang%2520Kang%2520and%2520Yogeshwar%2520Nath%2520Mishra%2520and%2520Qiang%2520Fu%2520and%2520Wolfgang%2520Heidrich%26entry.1292438233%3D%2520%2520Digital%2520imaging%2520systems%2520have%2520classically%2520been%2520based%2520on%2520brute-force%2520measuring%250Aand%2520processing%2520of%2520pixels%2520organized%2520on%2520regular%2520grids.%2520The%2520human%2520visual%2520system%252C%250Aon%2520the%2520other%2520hand%252C%2520performs%2520a%2520massive%2520data%2520reduction%2520from%2520the%2520number%2520of%250Aphoto-receptors%2520to%2520the%2520optic%2520nerve%252C%2520essentially%2520encoding%2520the%2520image%2520information%250Ainto%2520a%2520low%2520bandwidth%2520latent%2520space%2520representation%2520suitable%2520for%2520processing%2520by%2520the%250Ahuman%2520brain.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520follow%2520a%2520similar%2520approach%2520for%2520the%250Adevelopment%2520of%2520artificial%2520vision%2520systems.%2520Latent%2520Space%2520Imaging%2520is%2520a%2520new%250Aparadigm%2520that%252C%2520through%2520a%2520combination%2520of%2520optics%2520and%2520software%252C%2520directly%2520encodes%250Athe%2520image%2520information%2520into%2520the%2520semantically%2520rich%2520latent%2520space%2520of%2520a%2520generative%250Amodel%252C%2520thus%2520substantially%2520reducing%2520bandwidth%2520and%2520memory%2520requirements%2520during%2520the%250Acapture%2520process.%2520We%2520demonstrate%2520this%2520new%2520principle%2520through%2520an%2520initial%2520hardware%250Aprototype%2520based%2520on%2520the%2520single%2520pixel%2520camera.%2520By%2520designing%2520an%2520amplitude%250Amodulation%2520scheme%2520that%2520encodes%2520into%2520the%2520latent%2520space%2520of%2520a%2520generative%2520model%252C%2520we%250Aachieve%2520compression%2520ratios%2520from%25201%253A100%2520to%25201%253A1%252C000%2520during%2520the%2520imaging%2520process%252C%250Aillustrating%2520the%2520potential%2520of%2520latent%2520space%2520imaging%2520for%2520highly%2520efficient%2520imaging%250Ahardware%252C%2520to%2520enable%2520future%2520applications%2520in%2520high%2520speed%2520imaging%252C%2520or%2520task-specific%250Acameras%2520with%2520substantially%2520reduced%2520hardware%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Space%20Imaging&entry.906535625=Matheus%20Souza%20and%20Yidan%20Zheng%20and%20Kaizhang%20Kang%20and%20Yogeshwar%20Nath%20Mishra%20and%20Qiang%20Fu%20and%20Wolfgang%20Heidrich&entry.1292438233=%20%20Digital%20imaging%20systems%20have%20classically%20been%20based%20on%20brute-force%20measuring%0Aand%20processing%20of%20pixels%20organized%20on%20regular%20grids.%20The%20human%20visual%20system%2C%0Aon%20the%20other%20hand%2C%20performs%20a%20massive%20data%20reduction%20from%20the%20number%20of%0Aphoto-receptors%20to%20the%20optic%20nerve%2C%20essentially%20encoding%20the%20image%20information%0Ainto%20a%20low%20bandwidth%20latent%20space%20representation%20suitable%20for%20processing%20by%20the%0Ahuman%20brain.%20In%20this%20work%2C%20we%20propose%20to%20follow%20a%20similar%20approach%20for%20the%0Adevelopment%20of%20artificial%20vision%20systems.%20Latent%20Space%20Imaging%20is%20a%20new%0Aparadigm%20that%2C%20through%20a%20combination%20of%20optics%20and%20software%2C%20directly%20encodes%0Athe%20image%20information%20into%20the%20semantically%20rich%20latent%20space%20of%20a%20generative%0Amodel%2C%20thus%20substantially%20reducing%20bandwidth%20and%20memory%20requirements%20during%20the%0Acapture%20process.%20We%20demonstrate%20this%20new%20principle%20through%20an%20initial%20hardware%0Aprototype%20based%20on%20the%20single%20pixel%20camera.%20By%20designing%20an%20amplitude%0Amodulation%20scheme%20that%20encodes%20into%20the%20latent%20space%20of%20a%20generative%20model%2C%20we%0Aachieve%20compression%20ratios%20from%201%3A100%20to%201%3A1%2C000%20during%20the%20imaging%20process%2C%0Aillustrating%20the%20potential%20of%20latent%20space%20imaging%20for%20highly%20efficient%20imaging%0Ahardware%2C%20to%20enable%20future%20applications%20in%20high%20speed%20imaging%2C%20or%20task-specific%0Acameras%20with%20substantially%20reduced%20hardware%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07052v1&entry.124074799=Read"},
{"title": "Hyperion - A fast, versatile symbolic Gaussian Belief Propagation\n  framework for Continuous-Time SLAM", "author": "David Hug and Ignacio Alzugaray and Margarita Chli", "abstract": "  Continuous-Time Simultaneous Localization And Mapping (CTSLAM) has become a\npromising approach for fusing asynchronous and multi-modal sensor suites.\nUnlike discrete-time SLAM, which estimates poses discretely, CTSLAM uses\ncontinuous-time motion parametrizations, facilitating the integration of a\nvariety of sensors such as rolling-shutter cameras, event cameras and Inertial\nMeasurement Units (IMUs). However, CTSLAM approaches remain computationally\ndemanding and are conventionally posed as centralized Non-Linear Least Squares\n(NLLS) optimizations. Targeting these limitations, we not only present the\nfastest SymForce-based [Martiros et al., RSS 2022] B- and Z-Spline\nimplementations achieving speedups between 2.43x and 110.31x over Sommer et al.\n[CVPR 2020] but also implement a novel continuous-time Gaussian Belief\nPropagation (GBP) framework, coined Hyperion, which targets decentralized\nprobabilistic inference across agents. We demonstrate the efficacy of our\nmethod in motion tracking and localization settings, complemented by empirical\nablation studies.\n", "link": "http://arxiv.org/abs/2407.07074v1", "date": "2024-07-09", "relevancy": 2.3538, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6155}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5779}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperion%20-%20A%20fast%2C%20versatile%20symbolic%20Gaussian%20Belief%20Propagation%0A%20%20framework%20for%20Continuous-Time%20SLAM&body=Title%3A%20Hyperion%20-%20A%20fast%2C%20versatile%20symbolic%20Gaussian%20Belief%20Propagation%0A%20%20framework%20for%20Continuous-Time%20SLAM%0AAuthor%3A%20David%20Hug%20and%20Ignacio%20Alzugaray%20and%20Margarita%20Chli%0AAbstract%3A%20%20%20Continuous-Time%20Simultaneous%20Localization%20And%20Mapping%20%28CTSLAM%29%20has%20become%20a%0Apromising%20approach%20for%20fusing%20asynchronous%20and%20multi-modal%20sensor%20suites.%0AUnlike%20discrete-time%20SLAM%2C%20which%20estimates%20poses%20discretely%2C%20CTSLAM%20uses%0Acontinuous-time%20motion%20parametrizations%2C%20facilitating%20the%20integration%20of%20a%0Avariety%20of%20sensors%20such%20as%20rolling-shutter%20cameras%2C%20event%20cameras%20and%20Inertial%0AMeasurement%20Units%20%28IMUs%29.%20However%2C%20CTSLAM%20approaches%20remain%20computationally%0Ademanding%20and%20are%20conventionally%20posed%20as%20centralized%20Non-Linear%20Least%20Squares%0A%28NLLS%29%20optimizations.%20Targeting%20these%20limitations%2C%20we%20not%20only%20present%20the%0Afastest%20SymForce-based%20%5BMartiros%20et%20al.%2C%20RSS%202022%5D%20B-%20and%20Z-Spline%0Aimplementations%20achieving%20speedups%20between%202.43x%20and%20110.31x%20over%20Sommer%20et%20al.%0A%5BCVPR%202020%5D%20but%20also%20implement%20a%20novel%20continuous-time%20Gaussian%20Belief%0APropagation%20%28GBP%29%20framework%2C%20coined%20Hyperion%2C%20which%20targets%20decentralized%0Aprobabilistic%20inference%20across%20agents.%20We%20demonstrate%20the%20efficacy%20of%20our%0Amethod%20in%20motion%20tracking%20and%20localization%20settings%2C%20complemented%20by%20empirical%0Aablation%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperion%2520-%2520A%2520fast%252C%2520versatile%2520symbolic%2520Gaussian%2520Belief%2520Propagation%250A%2520%2520framework%2520for%2520Continuous-Time%2520SLAM%26entry.906535625%3DDavid%2520Hug%2520and%2520Ignacio%2520Alzugaray%2520and%2520Margarita%2520Chli%26entry.1292438233%3D%2520%2520Continuous-Time%2520Simultaneous%2520Localization%2520And%2520Mapping%2520%2528CTSLAM%2529%2520has%2520become%2520a%250Apromising%2520approach%2520for%2520fusing%2520asynchronous%2520and%2520multi-modal%2520sensor%2520suites.%250AUnlike%2520discrete-time%2520SLAM%252C%2520which%2520estimates%2520poses%2520discretely%252C%2520CTSLAM%2520uses%250Acontinuous-time%2520motion%2520parametrizations%252C%2520facilitating%2520the%2520integration%2520of%2520a%250Avariety%2520of%2520sensors%2520such%2520as%2520rolling-shutter%2520cameras%252C%2520event%2520cameras%2520and%2520Inertial%250AMeasurement%2520Units%2520%2528IMUs%2529.%2520However%252C%2520CTSLAM%2520approaches%2520remain%2520computationally%250Ademanding%2520and%2520are%2520conventionally%2520posed%2520as%2520centralized%2520Non-Linear%2520Least%2520Squares%250A%2528NLLS%2529%2520optimizations.%2520Targeting%2520these%2520limitations%252C%2520we%2520not%2520only%2520present%2520the%250Afastest%2520SymForce-based%2520%255BMartiros%2520et%2520al.%252C%2520RSS%25202022%255D%2520B-%2520and%2520Z-Spline%250Aimplementations%2520achieving%2520speedups%2520between%25202.43x%2520and%2520110.31x%2520over%2520Sommer%2520et%2520al.%250A%255BCVPR%25202020%255D%2520but%2520also%2520implement%2520a%2520novel%2520continuous-time%2520Gaussian%2520Belief%250APropagation%2520%2528GBP%2529%2520framework%252C%2520coined%2520Hyperion%252C%2520which%2520targets%2520decentralized%250Aprobabilistic%2520inference%2520across%2520agents.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%250Amethod%2520in%2520motion%2520tracking%2520and%2520localization%2520settings%252C%2520complemented%2520by%2520empirical%250Aablation%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperion%20-%20A%20fast%2C%20versatile%20symbolic%20Gaussian%20Belief%20Propagation%0A%20%20framework%20for%20Continuous-Time%20SLAM&entry.906535625=David%20Hug%20and%20Ignacio%20Alzugaray%20and%20Margarita%20Chli&entry.1292438233=%20%20Continuous-Time%20Simultaneous%20Localization%20And%20Mapping%20%28CTSLAM%29%20has%20become%20a%0Apromising%20approach%20for%20fusing%20asynchronous%20and%20multi-modal%20sensor%20suites.%0AUnlike%20discrete-time%20SLAM%2C%20which%20estimates%20poses%20discretely%2C%20CTSLAM%20uses%0Acontinuous-time%20motion%20parametrizations%2C%20facilitating%20the%20integration%20of%20a%0Avariety%20of%20sensors%20such%20as%20rolling-shutter%20cameras%2C%20event%20cameras%20and%20Inertial%0AMeasurement%20Units%20%28IMUs%29.%20However%2C%20CTSLAM%20approaches%20remain%20computationally%0Ademanding%20and%20are%20conventionally%20posed%20as%20centralized%20Non-Linear%20Least%20Squares%0A%28NLLS%29%20optimizations.%20Targeting%20these%20limitations%2C%20we%20not%20only%20present%20the%0Afastest%20SymForce-based%20%5BMartiros%20et%20al.%2C%20RSS%202022%5D%20B-%20and%20Z-Spline%0Aimplementations%20achieving%20speedups%20between%202.43x%20and%20110.31x%20over%20Sommer%20et%20al.%0A%5BCVPR%202020%5D%20but%20also%20implement%20a%20novel%20continuous-time%20Gaussian%20Belief%0APropagation%20%28GBP%29%20framework%2C%20coined%20Hyperion%2C%20which%20targets%20decentralized%0Aprobabilistic%20inference%20across%20agents.%20We%20demonstrate%20the%20efficacy%20of%20our%0Amethod%20in%20motion%20tracking%20and%20localization%20settings%2C%20complemented%20by%20empirical%0Aablation%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07074v1&entry.124074799=Read"},
{"title": "AstroSpy: On detecting Fake Images in Astronomy via Joint Image-Spectral\n  Representations", "author": "Mohammed Talha Alam and Raza Imam and Mohsen Guizani and Fakhri Karray", "abstract": "  The prevalence of AI-generated imagery has raised concerns about the\nauthenticity of astronomical images, especially with advanced text-to-image\nmodels like Stable Diffusion producing highly realistic synthetic samples.\nExisting detection methods, primarily based on convolutional neural networks\n(CNNs) or spectral analysis, have limitations when used independently. We\npresent AstroSpy, a hybrid model that integrates both spectral and image\nfeatures to distinguish real from synthetic astronomical images. Trained on a\nunique dataset of real NASA images and AI-generated fakes (approximately 18k\nsamples), AstroSpy utilizes a dual-pathway architecture to fuse spatial and\nspectral information. This approach enables AstroSpy to achieve superior\nperformance in identifying authentic astronomical images. Extensive evaluations\ndemonstrate AstroSpy's effectiveness and robustness, significantly\noutperforming baseline models in both in-domain and cross-domain tasks,\nhighlighting its potential to combat misinformation in astronomy.\n", "link": "http://arxiv.org/abs/2407.06817v1", "date": "2024-07-09", "relevancy": 2.3488, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.474}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4676}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AstroSpy%3A%20On%20detecting%20Fake%20Images%20in%20Astronomy%20via%20Joint%20Image-Spectral%0A%20%20Representations&body=Title%3A%20AstroSpy%3A%20On%20detecting%20Fake%20Images%20in%20Astronomy%20via%20Joint%20Image-Spectral%0A%20%20Representations%0AAuthor%3A%20Mohammed%20Talha%20Alam%20and%20Raza%20Imam%20and%20Mohsen%20Guizani%20and%20Fakhri%20Karray%0AAbstract%3A%20%20%20The%20prevalence%20of%20AI-generated%20imagery%20has%20raised%20concerns%20about%20the%0Aauthenticity%20of%20astronomical%20images%2C%20especially%20with%20advanced%20text-to-image%0Amodels%20like%20Stable%20Diffusion%20producing%20highly%20realistic%20synthetic%20samples.%0AExisting%20detection%20methods%2C%20primarily%20based%20on%20convolutional%20neural%20networks%0A%28CNNs%29%20or%20spectral%20analysis%2C%20have%20limitations%20when%20used%20independently.%20We%0Apresent%20AstroSpy%2C%20a%20hybrid%20model%20that%20integrates%20both%20spectral%20and%20image%0Afeatures%20to%20distinguish%20real%20from%20synthetic%20astronomical%20images.%20Trained%20on%20a%0Aunique%20dataset%20of%20real%20NASA%20images%20and%20AI-generated%20fakes%20%28approximately%2018k%0Asamples%29%2C%20AstroSpy%20utilizes%20a%20dual-pathway%20architecture%20to%20fuse%20spatial%20and%0Aspectral%20information.%20This%20approach%20enables%20AstroSpy%20to%20achieve%20superior%0Aperformance%20in%20identifying%20authentic%20astronomical%20images.%20Extensive%20evaluations%0Ademonstrate%20AstroSpy%27s%20effectiveness%20and%20robustness%2C%20significantly%0Aoutperforming%20baseline%20models%20in%20both%20in-domain%20and%20cross-domain%20tasks%2C%0Ahighlighting%20its%20potential%20to%20combat%20misinformation%20in%20astronomy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAstroSpy%253A%2520On%2520detecting%2520Fake%2520Images%2520in%2520Astronomy%2520via%2520Joint%2520Image-Spectral%250A%2520%2520Representations%26entry.906535625%3DMohammed%2520Talha%2520Alam%2520and%2520Raza%2520Imam%2520and%2520Mohsen%2520Guizani%2520and%2520Fakhri%2520Karray%26entry.1292438233%3D%2520%2520The%2520prevalence%2520of%2520AI-generated%2520imagery%2520has%2520raised%2520concerns%2520about%2520the%250Aauthenticity%2520of%2520astronomical%2520images%252C%2520especially%2520with%2520advanced%2520text-to-image%250Amodels%2520like%2520Stable%2520Diffusion%2520producing%2520highly%2520realistic%2520synthetic%2520samples.%250AExisting%2520detection%2520methods%252C%2520primarily%2520based%2520on%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%2520or%2520spectral%2520analysis%252C%2520have%2520limitations%2520when%2520used%2520independently.%2520We%250Apresent%2520AstroSpy%252C%2520a%2520hybrid%2520model%2520that%2520integrates%2520both%2520spectral%2520and%2520image%250Afeatures%2520to%2520distinguish%2520real%2520from%2520synthetic%2520astronomical%2520images.%2520Trained%2520on%2520a%250Aunique%2520dataset%2520of%2520real%2520NASA%2520images%2520and%2520AI-generated%2520fakes%2520%2528approximately%252018k%250Asamples%2529%252C%2520AstroSpy%2520utilizes%2520a%2520dual-pathway%2520architecture%2520to%2520fuse%2520spatial%2520and%250Aspectral%2520information.%2520This%2520approach%2520enables%2520AstroSpy%2520to%2520achieve%2520superior%250Aperformance%2520in%2520identifying%2520authentic%2520astronomical%2520images.%2520Extensive%2520evaluations%250Ademonstrate%2520AstroSpy%2527s%2520effectiveness%2520and%2520robustness%252C%2520significantly%250Aoutperforming%2520baseline%2520models%2520in%2520both%2520in-domain%2520and%2520cross-domain%2520tasks%252C%250Ahighlighting%2520its%2520potential%2520to%2520combat%2520misinformation%2520in%2520astronomy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AstroSpy%3A%20On%20detecting%20Fake%20Images%20in%20Astronomy%20via%20Joint%20Image-Spectral%0A%20%20Representations&entry.906535625=Mohammed%20Talha%20Alam%20and%20Raza%20Imam%20and%20Mohsen%20Guizani%20and%20Fakhri%20Karray&entry.1292438233=%20%20The%20prevalence%20of%20AI-generated%20imagery%20has%20raised%20concerns%20about%20the%0Aauthenticity%20of%20astronomical%20images%2C%20especially%20with%20advanced%20text-to-image%0Amodels%20like%20Stable%20Diffusion%20producing%20highly%20realistic%20synthetic%20samples.%0AExisting%20detection%20methods%2C%20primarily%20based%20on%20convolutional%20neural%20networks%0A%28CNNs%29%20or%20spectral%20analysis%2C%20have%20limitations%20when%20used%20independently.%20We%0Apresent%20AstroSpy%2C%20a%20hybrid%20model%20that%20integrates%20both%20spectral%20and%20image%0Afeatures%20to%20distinguish%20real%20from%20synthetic%20astronomical%20images.%20Trained%20on%20a%0Aunique%20dataset%20of%20real%20NASA%20images%20and%20AI-generated%20fakes%20%28approximately%2018k%0Asamples%29%2C%20AstroSpy%20utilizes%20a%20dual-pathway%20architecture%20to%20fuse%20spatial%20and%0Aspectral%20information.%20This%20approach%20enables%20AstroSpy%20to%20achieve%20superior%0Aperformance%20in%20identifying%20authentic%20astronomical%20images.%20Extensive%20evaluations%0Ademonstrate%20AstroSpy%27s%20effectiveness%20and%20robustness%2C%20significantly%0Aoutperforming%20baseline%20models%20in%20both%20in-domain%20and%20cross-domain%20tasks%2C%0Ahighlighting%20its%20potential%20to%20combat%20misinformation%20in%20astronomy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06817v1&entry.124074799=Read"},
{"title": "Neuromorphic Perception and Navigation for Mobile Robots: A Review", "author": "A. Novo and F. Lobon and H. G. De Marina and S. Romero and F. Barranco", "abstract": "  With the fast and unstoppable evolution of robotics and artificial\nintelligence, effective autonomous navigation in real-world scenarios has\nbecome one of the most pressing challenges in the literature. However,\ndemanding requirements, such as real-time operation, energy and computational\nefficiency, robustness, and reliability, make most current solutions unsuitable\nfor real-world challenges. Thus, researchers are forced to seek innovative\napproaches, such as bio-inspired solutions. Indeed, animals have the intrinsic\nability to efficiently perceive, understand, and navigate their unstructured\nsurroundings. To do so, they exploit self-motion cues, proprioception, and\nvisual flow in a cognitive process to map their environment and locate\nthemselves within it. Computational neuroscientists aim to answer ''how'' and\n''why'' such cognitive processes occur in the brain, to design novel\nneuromorphic sensors and methods that imitate biological processing. This\nsurvey aims to comprehensively review the application of brain-inspired\nstrategies to autonomous navigation, considering: neuromorphic perception and\nasynchronous event processing, energy-efficient and adaptive learning, or the\nimitation of the working principles of brain areas that play a crucial role in\nnavigation such as the hippocampus or the entorhinal cortex.\n", "link": "http://arxiv.org/abs/2407.06792v1", "date": "2024-07-09", "relevancy": 2.3124, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.603}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5747}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuromorphic%20Perception%20and%20Navigation%20for%20Mobile%20Robots%3A%20A%20Review&body=Title%3A%20Neuromorphic%20Perception%20and%20Navigation%20for%20Mobile%20Robots%3A%20A%20Review%0AAuthor%3A%20A.%20Novo%20and%20F.%20Lobon%20and%20H.%20G.%20De%20Marina%20and%20S.%20Romero%20and%20F.%20Barranco%0AAbstract%3A%20%20%20With%20the%20fast%20and%20unstoppable%20evolution%20of%20robotics%20and%20artificial%0Aintelligence%2C%20effective%20autonomous%20navigation%20in%20real-world%20scenarios%20has%0Abecome%20one%20of%20the%20most%20pressing%20challenges%20in%20the%20literature.%20However%2C%0Ademanding%20requirements%2C%20such%20as%20real-time%20operation%2C%20energy%20and%20computational%0Aefficiency%2C%20robustness%2C%20and%20reliability%2C%20make%20most%20current%20solutions%20unsuitable%0Afor%20real-world%20challenges.%20Thus%2C%20researchers%20are%20forced%20to%20seek%20innovative%0Aapproaches%2C%20such%20as%20bio-inspired%20solutions.%20Indeed%2C%20animals%20have%20the%20intrinsic%0Aability%20to%20efficiently%20perceive%2C%20understand%2C%20and%20navigate%20their%20unstructured%0Asurroundings.%20To%20do%20so%2C%20they%20exploit%20self-motion%20cues%2C%20proprioception%2C%20and%0Avisual%20flow%20in%20a%20cognitive%20process%20to%20map%20their%20environment%20and%20locate%0Athemselves%20within%20it.%20Computational%20neuroscientists%20aim%20to%20answer%20%27%27how%27%27%20and%0A%27%27why%27%27%20such%20cognitive%20processes%20occur%20in%20the%20brain%2C%20to%20design%20novel%0Aneuromorphic%20sensors%20and%20methods%20that%20imitate%20biological%20processing.%20This%0Asurvey%20aims%20to%20comprehensively%20review%20the%20application%20of%20brain-inspired%0Astrategies%20to%20autonomous%20navigation%2C%20considering%3A%20neuromorphic%20perception%20and%0Aasynchronous%20event%20processing%2C%20energy-efficient%20and%20adaptive%20learning%2C%20or%20the%0Aimitation%20of%20the%20working%20principles%20of%20brain%20areas%20that%20play%20a%20crucial%20role%20in%0Anavigation%20such%20as%20the%20hippocampus%20or%20the%20entorhinal%20cortex.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuromorphic%2520Perception%2520and%2520Navigation%2520for%2520Mobile%2520Robots%253A%2520A%2520Review%26entry.906535625%3DA.%2520Novo%2520and%2520F.%2520Lobon%2520and%2520H.%2520G.%2520De%2520Marina%2520and%2520S.%2520Romero%2520and%2520F.%2520Barranco%26entry.1292438233%3D%2520%2520With%2520the%2520fast%2520and%2520unstoppable%2520evolution%2520of%2520robotics%2520and%2520artificial%250Aintelligence%252C%2520effective%2520autonomous%2520navigation%2520in%2520real-world%2520scenarios%2520has%250Abecome%2520one%2520of%2520the%2520most%2520pressing%2520challenges%2520in%2520the%2520literature.%2520However%252C%250Ademanding%2520requirements%252C%2520such%2520as%2520real-time%2520operation%252C%2520energy%2520and%2520computational%250Aefficiency%252C%2520robustness%252C%2520and%2520reliability%252C%2520make%2520most%2520current%2520solutions%2520unsuitable%250Afor%2520real-world%2520challenges.%2520Thus%252C%2520researchers%2520are%2520forced%2520to%2520seek%2520innovative%250Aapproaches%252C%2520such%2520as%2520bio-inspired%2520solutions.%2520Indeed%252C%2520animals%2520have%2520the%2520intrinsic%250Aability%2520to%2520efficiently%2520perceive%252C%2520understand%252C%2520and%2520navigate%2520their%2520unstructured%250Asurroundings.%2520To%2520do%2520so%252C%2520they%2520exploit%2520self-motion%2520cues%252C%2520proprioception%252C%2520and%250Avisual%2520flow%2520in%2520a%2520cognitive%2520process%2520to%2520map%2520their%2520environment%2520and%2520locate%250Athemselves%2520within%2520it.%2520Computational%2520neuroscientists%2520aim%2520to%2520answer%2520%2527%2527how%2527%2527%2520and%250A%2527%2527why%2527%2527%2520such%2520cognitive%2520processes%2520occur%2520in%2520the%2520brain%252C%2520to%2520design%2520novel%250Aneuromorphic%2520sensors%2520and%2520methods%2520that%2520imitate%2520biological%2520processing.%2520This%250Asurvey%2520aims%2520to%2520comprehensively%2520review%2520the%2520application%2520of%2520brain-inspired%250Astrategies%2520to%2520autonomous%2520navigation%252C%2520considering%253A%2520neuromorphic%2520perception%2520and%250Aasynchronous%2520event%2520processing%252C%2520energy-efficient%2520and%2520adaptive%2520learning%252C%2520or%2520the%250Aimitation%2520of%2520the%2520working%2520principles%2520of%2520brain%2520areas%2520that%2520play%2520a%2520crucial%2520role%2520in%250Anavigation%2520such%2520as%2520the%2520hippocampus%2520or%2520the%2520entorhinal%2520cortex.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuromorphic%20Perception%20and%20Navigation%20for%20Mobile%20Robots%3A%20A%20Review&entry.906535625=A.%20Novo%20and%20F.%20Lobon%20and%20H.%20G.%20De%20Marina%20and%20S.%20Romero%20and%20F.%20Barranco&entry.1292438233=%20%20With%20the%20fast%20and%20unstoppable%20evolution%20of%20robotics%20and%20artificial%0Aintelligence%2C%20effective%20autonomous%20navigation%20in%20real-world%20scenarios%20has%0Abecome%20one%20of%20the%20most%20pressing%20challenges%20in%20the%20literature.%20However%2C%0Ademanding%20requirements%2C%20such%20as%20real-time%20operation%2C%20energy%20and%20computational%0Aefficiency%2C%20robustness%2C%20and%20reliability%2C%20make%20most%20current%20solutions%20unsuitable%0Afor%20real-world%20challenges.%20Thus%2C%20researchers%20are%20forced%20to%20seek%20innovative%0Aapproaches%2C%20such%20as%20bio-inspired%20solutions.%20Indeed%2C%20animals%20have%20the%20intrinsic%0Aability%20to%20efficiently%20perceive%2C%20understand%2C%20and%20navigate%20their%20unstructured%0Asurroundings.%20To%20do%20so%2C%20they%20exploit%20self-motion%20cues%2C%20proprioception%2C%20and%0Avisual%20flow%20in%20a%20cognitive%20process%20to%20map%20their%20environment%20and%20locate%0Athemselves%20within%20it.%20Computational%20neuroscientists%20aim%20to%20answer%20%27%27how%27%27%20and%0A%27%27why%27%27%20such%20cognitive%20processes%20occur%20in%20the%20brain%2C%20to%20design%20novel%0Aneuromorphic%20sensors%20and%20methods%20that%20imitate%20biological%20processing.%20This%0Asurvey%20aims%20to%20comprehensively%20review%20the%20application%20of%20brain-inspired%0Astrategies%20to%20autonomous%20navigation%2C%20considering%3A%20neuromorphic%20perception%20and%0Aasynchronous%20event%20processing%2C%20energy-efficient%20and%20adaptive%20learning%2C%20or%20the%0Aimitation%20of%20the%20working%20principles%20of%20brain%20areas%20that%20play%20a%20crucial%20role%20in%0Anavigation%20such%20as%20the%20hippocampus%20or%20the%20entorhinal%20cortex.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06792v1&entry.124074799=Read"},
{"title": "Exploring Scalability of Self-Training for Open-Vocabulary Temporal\n  Action Localization", "author": "Jeongseok Hyun and Su Ho Han and Hyolim Kang and Joon-Young Lee and Seon Joo Kim", "abstract": "  The vocabulary size in temporal action localization (TAL) is constrained by\nthe scarcity of large-scale annotated datasets. To address this, recent works\nincorporate powerful pre-trained vision-language models (VLMs), such as CLIP,\nto perform open-vocabulary TAL (OV-TAL). However, unlike VLMs trained on\nextensive image/video-text pairs, existing OV-TAL methods still rely on small,\nfully labeled TAL datasets for training an action localizer. In this paper, we\nexplore the scalability of self-training with unlabeled YouTube videos for\nOV-TAL. Our self-training approach consists of two stages. First, a\nclass-agnostic action localizer is trained on a human-labeled TAL dataset and\nused to generate pseudo-labels for unlabeled videos. Second, the large-scale\npseudo-labeled dataset is combined with the human-labeled dataset to train the\nlocalizer. Extensive experiments demonstrate that leveraging web-scale videos\nin self-training significantly enhances the generalizability of an action\nlocalizer. Additionally, we highlighted issues with existing OV-TAL evaluation\nschemes and proposed a new evaluation protocol. Code is released at\nhttps://github.com/HYUNJS/STOV-TAL\n", "link": "http://arxiv.org/abs/2407.07024v1", "date": "2024-07-09", "relevancy": 2.3027, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5985}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5671}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Scalability%20of%20Self-Training%20for%20Open-Vocabulary%20Temporal%0A%20%20Action%20Localization&body=Title%3A%20Exploring%20Scalability%20of%20Self-Training%20for%20Open-Vocabulary%20Temporal%0A%20%20Action%20Localization%0AAuthor%3A%20Jeongseok%20Hyun%20and%20Su%20Ho%20Han%20and%20Hyolim%20Kang%20and%20Joon-Young%20Lee%20and%20Seon%20Joo%20Kim%0AAbstract%3A%20%20%20The%20vocabulary%20size%20in%20temporal%20action%20localization%20%28TAL%29%20is%20constrained%20by%0Athe%20scarcity%20of%20large-scale%20annotated%20datasets.%20To%20address%20this%2C%20recent%20works%0Aincorporate%20powerful%20pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%0Ato%20perform%20open-vocabulary%20TAL%20%28OV-TAL%29.%20However%2C%20unlike%20VLMs%20trained%20on%0Aextensive%20image/video-text%20pairs%2C%20existing%20OV-TAL%20methods%20still%20rely%20on%20small%2C%0Afully%20labeled%20TAL%20datasets%20for%20training%20an%20action%20localizer.%20In%20this%20paper%2C%20we%0Aexplore%20the%20scalability%20of%20self-training%20with%20unlabeled%20YouTube%20videos%20for%0AOV-TAL.%20Our%20self-training%20approach%20consists%20of%20two%20stages.%20First%2C%20a%0Aclass-agnostic%20action%20localizer%20is%20trained%20on%20a%20human-labeled%20TAL%20dataset%20and%0Aused%20to%20generate%20pseudo-labels%20for%20unlabeled%20videos.%20Second%2C%20the%20large-scale%0Apseudo-labeled%20dataset%20is%20combined%20with%20the%20human-labeled%20dataset%20to%20train%20the%0Alocalizer.%20Extensive%20experiments%20demonstrate%20that%20leveraging%20web-scale%20videos%0Ain%20self-training%20significantly%20enhances%20the%20generalizability%20of%20an%20action%0Alocalizer.%20Additionally%2C%20we%20highlighted%20issues%20with%20existing%20OV-TAL%20evaluation%0Aschemes%20and%20proposed%20a%20new%20evaluation%20protocol.%20Code%20is%20released%20at%0Ahttps%3A//github.com/HYUNJS/STOV-TAL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Scalability%2520of%2520Self-Training%2520for%2520Open-Vocabulary%2520Temporal%250A%2520%2520Action%2520Localization%26entry.906535625%3DJeongseok%2520Hyun%2520and%2520Su%2520Ho%2520Han%2520and%2520Hyolim%2520Kang%2520and%2520Joon-Young%2520Lee%2520and%2520Seon%2520Joo%2520Kim%26entry.1292438233%3D%2520%2520The%2520vocabulary%2520size%2520in%2520temporal%2520action%2520localization%2520%2528TAL%2529%2520is%2520constrained%2520by%250Athe%2520scarcity%2520of%2520large-scale%2520annotated%2520datasets.%2520To%2520address%2520this%252C%2520recent%2520works%250Aincorporate%2520powerful%2520pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%250Ato%2520perform%2520open-vocabulary%2520TAL%2520%2528OV-TAL%2529.%2520However%252C%2520unlike%2520VLMs%2520trained%2520on%250Aextensive%2520image/video-text%2520pairs%252C%2520existing%2520OV-TAL%2520methods%2520still%2520rely%2520on%2520small%252C%250Afully%2520labeled%2520TAL%2520datasets%2520for%2520training%2520an%2520action%2520localizer.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520the%2520scalability%2520of%2520self-training%2520with%2520unlabeled%2520YouTube%2520videos%2520for%250AOV-TAL.%2520Our%2520self-training%2520approach%2520consists%2520of%2520two%2520stages.%2520First%252C%2520a%250Aclass-agnostic%2520action%2520localizer%2520is%2520trained%2520on%2520a%2520human-labeled%2520TAL%2520dataset%2520and%250Aused%2520to%2520generate%2520pseudo-labels%2520for%2520unlabeled%2520videos.%2520Second%252C%2520the%2520large-scale%250Apseudo-labeled%2520dataset%2520is%2520combined%2520with%2520the%2520human-labeled%2520dataset%2520to%2520train%2520the%250Alocalizer.%2520Extensive%2520experiments%2520demonstrate%2520that%2520leveraging%2520web-scale%2520videos%250Ain%2520self-training%2520significantly%2520enhances%2520the%2520generalizability%2520of%2520an%2520action%250Alocalizer.%2520Additionally%252C%2520we%2520highlighted%2520issues%2520with%2520existing%2520OV-TAL%2520evaluation%250Aschemes%2520and%2520proposed%2520a%2520new%2520evaluation%2520protocol.%2520Code%2520is%2520released%2520at%250Ahttps%253A//github.com/HYUNJS/STOV-TAL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Scalability%20of%20Self-Training%20for%20Open-Vocabulary%20Temporal%0A%20%20Action%20Localization&entry.906535625=Jeongseok%20Hyun%20and%20Su%20Ho%20Han%20and%20Hyolim%20Kang%20and%20Joon-Young%20Lee%20and%20Seon%20Joo%20Kim&entry.1292438233=%20%20The%20vocabulary%20size%20in%20temporal%20action%20localization%20%28TAL%29%20is%20constrained%20by%0Athe%20scarcity%20of%20large-scale%20annotated%20datasets.%20To%20address%20this%2C%20recent%20works%0Aincorporate%20powerful%20pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%0Ato%20perform%20open-vocabulary%20TAL%20%28OV-TAL%29.%20However%2C%20unlike%20VLMs%20trained%20on%0Aextensive%20image/video-text%20pairs%2C%20existing%20OV-TAL%20methods%20still%20rely%20on%20small%2C%0Afully%20labeled%20TAL%20datasets%20for%20training%20an%20action%20localizer.%20In%20this%20paper%2C%20we%0Aexplore%20the%20scalability%20of%20self-training%20with%20unlabeled%20YouTube%20videos%20for%0AOV-TAL.%20Our%20self-training%20approach%20consists%20of%20two%20stages.%20First%2C%20a%0Aclass-agnostic%20action%20localizer%20is%20trained%20on%20a%20human-labeled%20TAL%20dataset%20and%0Aused%20to%20generate%20pseudo-labels%20for%20unlabeled%20videos.%20Second%2C%20the%20large-scale%0Apseudo-labeled%20dataset%20is%20combined%20with%20the%20human-labeled%20dataset%20to%20train%20the%0Alocalizer.%20Extensive%20experiments%20demonstrate%20that%20leveraging%20web-scale%20videos%0Ain%20self-training%20significantly%20enhances%20the%20generalizability%20of%20an%20action%0Alocalizer.%20Additionally%2C%20we%20highlighted%20issues%20with%20existing%20OV-TAL%20evaluation%0Aschemes%20and%20proposed%20a%20new%20evaluation%20protocol.%20Code%20is%20released%20at%0Ahttps%3A//github.com/HYUNJS/STOV-TAL%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07024v1&entry.124074799=Read"},
{"title": "Dynamic Correlation Learning and Regularization for Multi-Label\n  Confidence Calibration", "author": "Tianshui Chen and Weihang Wang and Tao Pu and Jinghui Qin and Zhijing Yang and Jie Liu and Liang Lin", "abstract": "  Modern visual recognition models often display overconfidence due to their\nreliance on complex deep neural networks and one-hot target supervision,\nresulting in unreliable confidence scores that necessitate calibration. While\ncurrent confidence calibration techniques primarily address single-label\nscenarios, there is a lack of focus on more practical and generalizable\nmulti-label contexts. This paper introduces the Multi-Label Confidence\nCalibration (MLCC) task, aiming to provide well-calibrated confidence scores in\nmulti-label scenarios. Unlike single-label images, multi-label images contain\nmultiple objects, leading to semantic confusion and further unreliability in\nconfidence scores. Existing single-label calibration methods, based on label\nsmoothing, fail to account for category correlations, which are crucial for\naddressing semantic confusion, thereby yielding sub-optimal performance. To\novercome these limitations, we propose the Dynamic Correlation Learning and\nRegularization (DCLR) algorithm, which leverages multi-grained semantic\ncorrelations to better model semantic confusion for adaptive regularization.\nDCLR learns dynamic instance-level and prototype-level similarities specific to\neach category, using these to measure semantic correlations across different\ncategories. With this understanding, we construct adaptive label vectors that\nassign higher values to categories with strong correlations, thereby\nfacilitating more effective regularization. We establish an evaluation\nbenchmark, re-implementing several advanced confidence calibration algorithms\nand applying them to leading multi-label recognition (MLR) models for fair\ncomparison. Through extensive experiments, we demonstrate the superior\nperformance of DCLR over existing methods in providing reliable confidence\nscores in multi-label scenarios.\n", "link": "http://arxiv.org/abs/2407.06844v1", "date": "2024-07-09", "relevancy": 2.2928, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6047}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5812}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Correlation%20Learning%20and%20Regularization%20for%20Multi-Label%0A%20%20Confidence%20Calibration&body=Title%3A%20Dynamic%20Correlation%20Learning%20and%20Regularization%20for%20Multi-Label%0A%20%20Confidence%20Calibration%0AAuthor%3A%20Tianshui%20Chen%20and%20Weihang%20Wang%20and%20Tao%20Pu%20and%20Jinghui%20Qin%20and%20Zhijing%20Yang%20and%20Jie%20Liu%20and%20Liang%20Lin%0AAbstract%3A%20%20%20Modern%20visual%20recognition%20models%20often%20display%20overconfidence%20due%20to%20their%0Areliance%20on%20complex%20deep%20neural%20networks%20and%20one-hot%20target%20supervision%2C%0Aresulting%20in%20unreliable%20confidence%20scores%20that%20necessitate%20calibration.%20While%0Acurrent%20confidence%20calibration%20techniques%20primarily%20address%20single-label%0Ascenarios%2C%20there%20is%20a%20lack%20of%20focus%20on%20more%20practical%20and%20generalizable%0Amulti-label%20contexts.%20This%20paper%20introduces%20the%20Multi-Label%20Confidence%0ACalibration%20%28MLCC%29%20task%2C%20aiming%20to%20provide%20well-calibrated%20confidence%20scores%20in%0Amulti-label%20scenarios.%20Unlike%20single-label%20images%2C%20multi-label%20images%20contain%0Amultiple%20objects%2C%20leading%20to%20semantic%20confusion%20and%20further%20unreliability%20in%0Aconfidence%20scores.%20Existing%20single-label%20calibration%20methods%2C%20based%20on%20label%0Asmoothing%2C%20fail%20to%20account%20for%20category%20correlations%2C%20which%20are%20crucial%20for%0Aaddressing%20semantic%20confusion%2C%20thereby%20yielding%20sub-optimal%20performance.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20the%20Dynamic%20Correlation%20Learning%20and%0ARegularization%20%28DCLR%29%20algorithm%2C%20which%20leverages%20multi-grained%20semantic%0Acorrelations%20to%20better%20model%20semantic%20confusion%20for%20adaptive%20regularization.%0ADCLR%20learns%20dynamic%20instance-level%20and%20prototype-level%20similarities%20specific%20to%0Aeach%20category%2C%20using%20these%20to%20measure%20semantic%20correlations%20across%20different%0Acategories.%20With%20this%20understanding%2C%20we%20construct%20adaptive%20label%20vectors%20that%0Aassign%20higher%20values%20to%20categories%20with%20strong%20correlations%2C%20thereby%0Afacilitating%20more%20effective%20regularization.%20We%20establish%20an%20evaluation%0Abenchmark%2C%20re-implementing%20several%20advanced%20confidence%20calibration%20algorithms%0Aand%20applying%20them%20to%20leading%20multi-label%20recognition%20%28MLR%29%20models%20for%20fair%0Acomparison.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20the%20superior%0Aperformance%20of%20DCLR%20over%20existing%20methods%20in%20providing%20reliable%20confidence%0Ascores%20in%20multi-label%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Correlation%2520Learning%2520and%2520Regularization%2520for%2520Multi-Label%250A%2520%2520Confidence%2520Calibration%26entry.906535625%3DTianshui%2520Chen%2520and%2520Weihang%2520Wang%2520and%2520Tao%2520Pu%2520and%2520Jinghui%2520Qin%2520and%2520Zhijing%2520Yang%2520and%2520Jie%2520Liu%2520and%2520Liang%2520Lin%26entry.1292438233%3D%2520%2520Modern%2520visual%2520recognition%2520models%2520often%2520display%2520overconfidence%2520due%2520to%2520their%250Areliance%2520on%2520complex%2520deep%2520neural%2520networks%2520and%2520one-hot%2520target%2520supervision%252C%250Aresulting%2520in%2520unreliable%2520confidence%2520scores%2520that%2520necessitate%2520calibration.%2520While%250Acurrent%2520confidence%2520calibration%2520techniques%2520primarily%2520address%2520single-label%250Ascenarios%252C%2520there%2520is%2520a%2520lack%2520of%2520focus%2520on%2520more%2520practical%2520and%2520generalizable%250Amulti-label%2520contexts.%2520This%2520paper%2520introduces%2520the%2520Multi-Label%2520Confidence%250ACalibration%2520%2528MLCC%2529%2520task%252C%2520aiming%2520to%2520provide%2520well-calibrated%2520confidence%2520scores%2520in%250Amulti-label%2520scenarios.%2520Unlike%2520single-label%2520images%252C%2520multi-label%2520images%2520contain%250Amultiple%2520objects%252C%2520leading%2520to%2520semantic%2520confusion%2520and%2520further%2520unreliability%2520in%250Aconfidence%2520scores.%2520Existing%2520single-label%2520calibration%2520methods%252C%2520based%2520on%2520label%250Asmoothing%252C%2520fail%2520to%2520account%2520for%2520category%2520correlations%252C%2520which%2520are%2520crucial%2520for%250Aaddressing%2520semantic%2520confusion%252C%2520thereby%2520yielding%2520sub-optimal%2520performance.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520propose%2520the%2520Dynamic%2520Correlation%2520Learning%2520and%250ARegularization%2520%2528DCLR%2529%2520algorithm%252C%2520which%2520leverages%2520multi-grained%2520semantic%250Acorrelations%2520to%2520better%2520model%2520semantic%2520confusion%2520for%2520adaptive%2520regularization.%250ADCLR%2520learns%2520dynamic%2520instance-level%2520and%2520prototype-level%2520similarities%2520specific%2520to%250Aeach%2520category%252C%2520using%2520these%2520to%2520measure%2520semantic%2520correlations%2520across%2520different%250Acategories.%2520With%2520this%2520understanding%252C%2520we%2520construct%2520adaptive%2520label%2520vectors%2520that%250Aassign%2520higher%2520values%2520to%2520categories%2520with%2520strong%2520correlations%252C%2520thereby%250Afacilitating%2520more%2520effective%2520regularization.%2520We%2520establish%2520an%2520evaluation%250Abenchmark%252C%2520re-implementing%2520several%2520advanced%2520confidence%2520calibration%2520algorithms%250Aand%2520applying%2520them%2520to%2520leading%2520multi-label%2520recognition%2520%2528MLR%2529%2520models%2520for%2520fair%250Acomparison.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520DCLR%2520over%2520existing%2520methods%2520in%2520providing%2520reliable%2520confidence%250Ascores%2520in%2520multi-label%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Correlation%20Learning%20and%20Regularization%20for%20Multi-Label%0A%20%20Confidence%20Calibration&entry.906535625=Tianshui%20Chen%20and%20Weihang%20Wang%20and%20Tao%20Pu%20and%20Jinghui%20Qin%20and%20Zhijing%20Yang%20and%20Jie%20Liu%20and%20Liang%20Lin&entry.1292438233=%20%20Modern%20visual%20recognition%20models%20often%20display%20overconfidence%20due%20to%20their%0Areliance%20on%20complex%20deep%20neural%20networks%20and%20one-hot%20target%20supervision%2C%0Aresulting%20in%20unreliable%20confidence%20scores%20that%20necessitate%20calibration.%20While%0Acurrent%20confidence%20calibration%20techniques%20primarily%20address%20single-label%0Ascenarios%2C%20there%20is%20a%20lack%20of%20focus%20on%20more%20practical%20and%20generalizable%0Amulti-label%20contexts.%20This%20paper%20introduces%20the%20Multi-Label%20Confidence%0ACalibration%20%28MLCC%29%20task%2C%20aiming%20to%20provide%20well-calibrated%20confidence%20scores%20in%0Amulti-label%20scenarios.%20Unlike%20single-label%20images%2C%20multi-label%20images%20contain%0Amultiple%20objects%2C%20leading%20to%20semantic%20confusion%20and%20further%20unreliability%20in%0Aconfidence%20scores.%20Existing%20single-label%20calibration%20methods%2C%20based%20on%20label%0Asmoothing%2C%20fail%20to%20account%20for%20category%20correlations%2C%20which%20are%20crucial%20for%0Aaddressing%20semantic%20confusion%2C%20thereby%20yielding%20sub-optimal%20performance.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20the%20Dynamic%20Correlation%20Learning%20and%0ARegularization%20%28DCLR%29%20algorithm%2C%20which%20leverages%20multi-grained%20semantic%0Acorrelations%20to%20better%20model%20semantic%20confusion%20for%20adaptive%20regularization.%0ADCLR%20learns%20dynamic%20instance-level%20and%20prototype-level%20similarities%20specific%20to%0Aeach%20category%2C%20using%20these%20to%20measure%20semantic%20correlations%20across%20different%0Acategories.%20With%20this%20understanding%2C%20we%20construct%20adaptive%20label%20vectors%20that%0Aassign%20higher%20values%20to%20categories%20with%20strong%20correlations%2C%20thereby%0Afacilitating%20more%20effective%20regularization.%20We%20establish%20an%20evaluation%0Abenchmark%2C%20re-implementing%20several%20advanced%20confidence%20calibration%20algorithms%0Aand%20applying%20them%20to%20leading%20multi-label%20recognition%20%28MLR%29%20models%20for%20fair%0Acomparison.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20the%20superior%0Aperformance%20of%20DCLR%20over%20existing%20methods%20in%20providing%20reliable%20confidence%0Ascores%20in%20multi-label%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06844v1&entry.124074799=Read"},
{"title": "Stable Diffusion Segmentation for Biomedical Images with Single-step\n  Reverse Process", "author": "Tianyu Lin and Zhiguang Chen and Zhonghao Yan and Weijiang Yu and Fudan Zheng", "abstract": "  Diffusion models have demonstrated their effectiveness across various\ngenerative tasks. However, when applied to medical image segmentation, these\nmodels encounter several challenges, including significant resource and time\nrequirements. They also necessitate a multi-step reverse process and multiple\nsamples to produce reliable predictions. To address these challenges, we\nintroduce the first latent diffusion segmentation model, named SDSeg, built\nupon stable diffusion (SD). SDSeg incorporates a straightforward latent\nestimation strategy to facilitate a single-step reverse process and utilizes\nlatent fusion concatenation to remove the necessity for multiple samples.\nExtensive experiments indicate that SDSeg surpasses existing state-of-the-art\nmethods on five benchmark datasets featuring diverse imaging modalities.\nRemarkably, SDSeg is capable of generating stable predictions with a solitary\nreverse step and sample, epitomizing the model's stability as implied by its\nname. The code is available at\nhttps://github.com/lin-tianyu/Stable-Diffusion-Seg\n", "link": "http://arxiv.org/abs/2406.18361v3", "date": "2024-07-09", "relevancy": 2.2899, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6744}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5714}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Diffusion%20Segmentation%20for%20Biomedical%20Images%20with%20Single-step%0A%20%20Reverse%20Process&body=Title%3A%20Stable%20Diffusion%20Segmentation%20for%20Biomedical%20Images%20with%20Single-step%0A%20%20Reverse%20Process%0AAuthor%3A%20Tianyu%20Lin%20and%20Zhiguang%20Chen%20and%20Zhonghao%20Yan%20and%20Weijiang%20Yu%20and%20Fudan%20Zheng%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20their%20effectiveness%20across%20various%0Agenerative%20tasks.%20However%2C%20when%20applied%20to%20medical%20image%20segmentation%2C%20these%0Amodels%20encounter%20several%20challenges%2C%20including%20significant%20resource%20and%20time%0Arequirements.%20They%20also%20necessitate%20a%20multi-step%20reverse%20process%20and%20multiple%0Asamples%20to%20produce%20reliable%20predictions.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20the%20first%20latent%20diffusion%20segmentation%20model%2C%20named%20SDSeg%2C%20built%0Aupon%20stable%20diffusion%20%28SD%29.%20SDSeg%20incorporates%20a%20straightforward%20latent%0Aestimation%20strategy%20to%20facilitate%20a%20single-step%20reverse%20process%20and%20utilizes%0Alatent%20fusion%20concatenation%20to%20remove%20the%20necessity%20for%20multiple%20samples.%0AExtensive%20experiments%20indicate%20that%20SDSeg%20surpasses%20existing%20state-of-the-art%0Amethods%20on%20five%20benchmark%20datasets%20featuring%20diverse%20imaging%20modalities.%0ARemarkably%2C%20SDSeg%20is%20capable%20of%20generating%20stable%20predictions%20with%20a%20solitary%0Areverse%20step%20and%20sample%2C%20epitomizing%20the%20model%27s%20stability%20as%20implied%20by%20its%0Aname.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lin-tianyu/Stable-Diffusion-Seg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18361v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Diffusion%2520Segmentation%2520for%2520Biomedical%2520Images%2520with%2520Single-step%250A%2520%2520Reverse%2520Process%26entry.906535625%3DTianyu%2520Lin%2520and%2520Zhiguang%2520Chen%2520and%2520Zhonghao%2520Yan%2520and%2520Weijiang%2520Yu%2520and%2520Fudan%2520Zheng%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520their%2520effectiveness%2520across%2520various%250Agenerative%2520tasks.%2520However%252C%2520when%2520applied%2520to%2520medical%2520image%2520segmentation%252C%2520these%250Amodels%2520encounter%2520several%2520challenges%252C%2520including%2520significant%2520resource%2520and%2520time%250Arequirements.%2520They%2520also%2520necessitate%2520a%2520multi-step%2520reverse%2520process%2520and%2520multiple%250Asamples%2520to%2520produce%2520reliable%2520predictions.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520the%2520first%2520latent%2520diffusion%2520segmentation%2520model%252C%2520named%2520SDSeg%252C%2520built%250Aupon%2520stable%2520diffusion%2520%2528SD%2529.%2520SDSeg%2520incorporates%2520a%2520straightforward%2520latent%250Aestimation%2520strategy%2520to%2520facilitate%2520a%2520single-step%2520reverse%2520process%2520and%2520utilizes%250Alatent%2520fusion%2520concatenation%2520to%2520remove%2520the%2520necessity%2520for%2520multiple%2520samples.%250AExtensive%2520experiments%2520indicate%2520that%2520SDSeg%2520surpasses%2520existing%2520state-of-the-art%250Amethods%2520on%2520five%2520benchmark%2520datasets%2520featuring%2520diverse%2520imaging%2520modalities.%250ARemarkably%252C%2520SDSeg%2520is%2520capable%2520of%2520generating%2520stable%2520predictions%2520with%2520a%2520solitary%250Areverse%2520step%2520and%2520sample%252C%2520epitomizing%2520the%2520model%2527s%2520stability%2520as%2520implied%2520by%2520its%250Aname.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/lin-tianyu/Stable-Diffusion-Seg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18361v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Diffusion%20Segmentation%20for%20Biomedical%20Images%20with%20Single-step%0A%20%20Reverse%20Process&entry.906535625=Tianyu%20Lin%20and%20Zhiguang%20Chen%20and%20Zhonghao%20Yan%20and%20Weijiang%20Yu%20and%20Fudan%20Zheng&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20their%20effectiveness%20across%20various%0Agenerative%20tasks.%20However%2C%20when%20applied%20to%20medical%20image%20segmentation%2C%20these%0Amodels%20encounter%20several%20challenges%2C%20including%20significant%20resource%20and%20time%0Arequirements.%20They%20also%20necessitate%20a%20multi-step%20reverse%20process%20and%20multiple%0Asamples%20to%20produce%20reliable%20predictions.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20the%20first%20latent%20diffusion%20segmentation%20model%2C%20named%20SDSeg%2C%20built%0Aupon%20stable%20diffusion%20%28SD%29.%20SDSeg%20incorporates%20a%20straightforward%20latent%0Aestimation%20strategy%20to%20facilitate%20a%20single-step%20reverse%20process%20and%20utilizes%0Alatent%20fusion%20concatenation%20to%20remove%20the%20necessity%20for%20multiple%20samples.%0AExtensive%20experiments%20indicate%20that%20SDSeg%20surpasses%20existing%20state-of-the-art%0Amethods%20on%20five%20benchmark%20datasets%20featuring%20diverse%20imaging%20modalities.%0ARemarkably%2C%20SDSeg%20is%20capable%20of%20generating%20stable%20predictions%20with%20a%20solitary%0Areverse%20step%20and%20sample%2C%20epitomizing%20the%20model%27s%20stability%20as%20implied%20by%20its%0Aname.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lin-tianyu/Stable-Diffusion-Seg%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18361v3&entry.124074799=Read"},
{"title": "PartSTAD: 2D-to-3D Part Segmentation Task Adaptation", "author": "Hyunjin Kim and Minhyuk Sung", "abstract": "  We introduce PartSTAD, a method designed for the task adaptation of 2D-to-3D\nsegmentation lifting. Recent studies have highlighted the advantages of\nutilizing 2D segmentation models to achieve high-quality 3D segmentation\nthrough few-shot adaptation. However, previous approaches have focused on\nadapting 2D segmentation models for domain shift to rendered images and\nsynthetic text descriptions, rather than optimizing the model specifically for\n3D segmentation. Our proposed task adaptation method finetunes a 2D bounding\nbox prediction model with an objective function for 3D segmentation. We\nintroduce weights for 2D bounding boxes for adaptive merging and learn the\nweights using a small additional neural network. Additionally, we incorporate\nSAM, a foreground segmentation model on a bounding box, to improve the\nboundaries of 2D segments and consequently those of 3D segmentation. Our\nexperiments on the PartNet-Mobility dataset show significant improvements with\nour task adaptation approach, achieving a 7.0%p increase in mIoU and a 5.2%p\nimprovement in mAP@50 for semantic and instance segmentation compared to the\nSotA few-shot 3D segmentation model.\n", "link": "http://arxiv.org/abs/2401.05906v2", "date": "2024-07-09", "relevancy": 2.2352, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6001}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5401}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartSTAD%3A%202D-to-3D%20Part%20Segmentation%20Task%20Adaptation&body=Title%3A%20PartSTAD%3A%202D-to-3D%20Part%20Segmentation%20Task%20Adaptation%0AAuthor%3A%20Hyunjin%20Kim%20and%20Minhyuk%20Sung%0AAbstract%3A%20%20%20We%20introduce%20PartSTAD%2C%20a%20method%20designed%20for%20the%20task%20adaptation%20of%202D-to-3D%0Asegmentation%20lifting.%20Recent%20studies%20have%20highlighted%20the%20advantages%20of%0Autilizing%202D%20segmentation%20models%20to%20achieve%20high-quality%203D%20segmentation%0Athrough%20few-shot%20adaptation.%20However%2C%20previous%20approaches%20have%20focused%20on%0Aadapting%202D%20segmentation%20models%20for%20domain%20shift%20to%20rendered%20images%20and%0Asynthetic%20text%20descriptions%2C%20rather%20than%20optimizing%20the%20model%20specifically%20for%0A3D%20segmentation.%20Our%20proposed%20task%20adaptation%20method%20finetunes%20a%202D%20bounding%0Abox%20prediction%20model%20with%20an%20objective%20function%20for%203D%20segmentation.%20We%0Aintroduce%20weights%20for%202D%20bounding%20boxes%20for%20adaptive%20merging%20and%20learn%20the%0Aweights%20using%20a%20small%20additional%20neural%20network.%20Additionally%2C%20we%20incorporate%0ASAM%2C%20a%20foreground%20segmentation%20model%20on%20a%20bounding%20box%2C%20to%20improve%20the%0Aboundaries%20of%202D%20segments%20and%20consequently%20those%20of%203D%20segmentation.%20Our%0Aexperiments%20on%20the%20PartNet-Mobility%20dataset%20show%20significant%20improvements%20with%0Aour%20task%20adaptation%20approach%2C%20achieving%20a%207.0%25p%20increase%20in%20mIoU%20and%20a%205.2%25p%0Aimprovement%20in%20mAP%4050%20for%20semantic%20and%20instance%20segmentation%20compared%20to%20the%0ASotA%20few-shot%203D%20segmentation%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.05906v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartSTAD%253A%25202D-to-3D%2520Part%2520Segmentation%2520Task%2520Adaptation%26entry.906535625%3DHyunjin%2520Kim%2520and%2520Minhyuk%2520Sung%26entry.1292438233%3D%2520%2520We%2520introduce%2520PartSTAD%252C%2520a%2520method%2520designed%2520for%2520the%2520task%2520adaptation%2520of%25202D-to-3D%250Asegmentation%2520lifting.%2520Recent%2520studies%2520have%2520highlighted%2520the%2520advantages%2520of%250Autilizing%25202D%2520segmentation%2520models%2520to%2520achieve%2520high-quality%25203D%2520segmentation%250Athrough%2520few-shot%2520adaptation.%2520However%252C%2520previous%2520approaches%2520have%2520focused%2520on%250Aadapting%25202D%2520segmentation%2520models%2520for%2520domain%2520shift%2520to%2520rendered%2520images%2520and%250Asynthetic%2520text%2520descriptions%252C%2520rather%2520than%2520optimizing%2520the%2520model%2520specifically%2520for%250A3D%2520segmentation.%2520Our%2520proposed%2520task%2520adaptation%2520method%2520finetunes%2520a%25202D%2520bounding%250Abox%2520prediction%2520model%2520with%2520an%2520objective%2520function%2520for%25203D%2520segmentation.%2520We%250Aintroduce%2520weights%2520for%25202D%2520bounding%2520boxes%2520for%2520adaptive%2520merging%2520and%2520learn%2520the%250Aweights%2520using%2520a%2520small%2520additional%2520neural%2520network.%2520Additionally%252C%2520we%2520incorporate%250ASAM%252C%2520a%2520foreground%2520segmentation%2520model%2520on%2520a%2520bounding%2520box%252C%2520to%2520improve%2520the%250Aboundaries%2520of%25202D%2520segments%2520and%2520consequently%2520those%2520of%25203D%2520segmentation.%2520Our%250Aexperiments%2520on%2520the%2520PartNet-Mobility%2520dataset%2520show%2520significant%2520improvements%2520with%250Aour%2520task%2520adaptation%2520approach%252C%2520achieving%2520a%25207.0%2525p%2520increase%2520in%2520mIoU%2520and%2520a%25205.2%2525p%250Aimprovement%2520in%2520mAP%254050%2520for%2520semantic%2520and%2520instance%2520segmentation%2520compared%2520to%2520the%250ASotA%2520few-shot%25203D%2520segmentation%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.05906v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartSTAD%3A%202D-to-3D%20Part%20Segmentation%20Task%20Adaptation&entry.906535625=Hyunjin%20Kim%20and%20Minhyuk%20Sung&entry.1292438233=%20%20We%20introduce%20PartSTAD%2C%20a%20method%20designed%20for%20the%20task%20adaptation%20of%202D-to-3D%0Asegmentation%20lifting.%20Recent%20studies%20have%20highlighted%20the%20advantages%20of%0Autilizing%202D%20segmentation%20models%20to%20achieve%20high-quality%203D%20segmentation%0Athrough%20few-shot%20adaptation.%20However%2C%20previous%20approaches%20have%20focused%20on%0Aadapting%202D%20segmentation%20models%20for%20domain%20shift%20to%20rendered%20images%20and%0Asynthetic%20text%20descriptions%2C%20rather%20than%20optimizing%20the%20model%20specifically%20for%0A3D%20segmentation.%20Our%20proposed%20task%20adaptation%20method%20finetunes%20a%202D%20bounding%0Abox%20prediction%20model%20with%20an%20objective%20function%20for%203D%20segmentation.%20We%0Aintroduce%20weights%20for%202D%20bounding%20boxes%20for%20adaptive%20merging%20and%20learn%20the%0Aweights%20using%20a%20small%20additional%20neural%20network.%20Additionally%2C%20we%20incorporate%0ASAM%2C%20a%20foreground%20segmentation%20model%20on%20a%20bounding%20box%2C%20to%20improve%20the%0Aboundaries%20of%202D%20segments%20and%20consequently%20those%20of%203D%20segmentation.%20Our%0Aexperiments%20on%20the%20PartNet-Mobility%20dataset%20show%20significant%20improvements%20with%0Aour%20task%20adaptation%20approach%2C%20achieving%20a%207.0%25p%20increase%20in%20mIoU%20and%20a%205.2%25p%0Aimprovement%20in%20mAP%4050%20for%20semantic%20and%20instance%20segmentation%20compared%20to%20the%0ASotA%20few-shot%203D%20segmentation%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.05906v2&entry.124074799=Read"},
{"title": "Gaussian Interpolation Flows", "author": "Yuan Gao and Jian Huang and Yuling Jiao", "abstract": "  Gaussian denoising has emerged as a powerful method for constructing\nsimulation-free continuous normalizing flows for generative modeling. Despite\ntheir empirical successes, theoretical properties of these flows and the\nregularizing effect of Gaussian denoising have remained largely unexplored. In\nthis work, we aim to address this gap by investigating the well-posedness of\nsimulation-free continuous normalizing flows built on Gaussian denoising.\nThrough a unified framework termed Gaussian interpolation flow, we establish\nthe Lipschitz regularity of the flow velocity field, the existence and\nuniqueness of the flow, and the Lipschitz continuity of the flow map and the\ntime-reversed flow map for several rich classes of target distributions. This\nanalysis also sheds light on the auto-encoding and cycle consistency properties\nof Gaussian interpolation flows. Additionally, we study the stability of these\nflows in source distributions and perturbations of the velocity field, using\nthe quadratic Wasserstein distance as a metric. Our findings offer valuable\ninsights into the learning techniques employed in Gaussian interpolation flows\nfor generative modeling, providing a solid theoretical foundation for\nend-to-end error analyses of learning Gaussian interpolation flows with\nempirical observations.\n", "link": "http://arxiv.org/abs/2311.11475v2", "date": "2024-07-09", "relevancy": 2.1979, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5978}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5589}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Interpolation%20Flows&body=Title%3A%20Gaussian%20Interpolation%20Flows%0AAuthor%3A%20Yuan%20Gao%20and%20Jian%20Huang%20and%20Yuling%20Jiao%0AAbstract%3A%20%20%20Gaussian%20denoising%20has%20emerged%20as%20a%20powerful%20method%20for%20constructing%0Asimulation-free%20continuous%20normalizing%20flows%20for%20generative%20modeling.%20Despite%0Atheir%20empirical%20successes%2C%20theoretical%20properties%20of%20these%20flows%20and%20the%0Aregularizing%20effect%20of%20Gaussian%20denoising%20have%20remained%20largely%20unexplored.%20In%0Athis%20work%2C%20we%20aim%20to%20address%20this%20gap%20by%20investigating%20the%20well-posedness%20of%0Asimulation-free%20continuous%20normalizing%20flows%20built%20on%20Gaussian%20denoising.%0AThrough%20a%20unified%20framework%20termed%20Gaussian%20interpolation%20flow%2C%20we%20establish%0Athe%20Lipschitz%20regularity%20of%20the%20flow%20velocity%20field%2C%20the%20existence%20and%0Auniqueness%20of%20the%20flow%2C%20and%20the%20Lipschitz%20continuity%20of%20the%20flow%20map%20and%20the%0Atime-reversed%20flow%20map%20for%20several%20rich%20classes%20of%20target%20distributions.%20This%0Aanalysis%20also%20sheds%20light%20on%20the%20auto-encoding%20and%20cycle%20consistency%20properties%0Aof%20Gaussian%20interpolation%20flows.%20Additionally%2C%20we%20study%20the%20stability%20of%20these%0Aflows%20in%20source%20distributions%20and%20perturbations%20of%20the%20velocity%20field%2C%20using%0Athe%20quadratic%20Wasserstein%20distance%20as%20a%20metric.%20Our%20findings%20offer%20valuable%0Ainsights%20into%20the%20learning%20techniques%20employed%20in%20Gaussian%20interpolation%20flows%0Afor%20generative%20modeling%2C%20providing%20a%20solid%20theoretical%20foundation%20for%0Aend-to-end%20error%20analyses%20of%20learning%20Gaussian%20interpolation%20flows%20with%0Aempirical%20observations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11475v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Interpolation%2520Flows%26entry.906535625%3DYuan%2520Gao%2520and%2520Jian%2520Huang%2520and%2520Yuling%2520Jiao%26entry.1292438233%3D%2520%2520Gaussian%2520denoising%2520has%2520emerged%2520as%2520a%2520powerful%2520method%2520for%2520constructing%250Asimulation-free%2520continuous%2520normalizing%2520flows%2520for%2520generative%2520modeling.%2520Despite%250Atheir%2520empirical%2520successes%252C%2520theoretical%2520properties%2520of%2520these%2520flows%2520and%2520the%250Aregularizing%2520effect%2520of%2520Gaussian%2520denoising%2520have%2520remained%2520largely%2520unexplored.%2520In%250Athis%2520work%252C%2520we%2520aim%2520to%2520address%2520this%2520gap%2520by%2520investigating%2520the%2520well-posedness%2520of%250Asimulation-free%2520continuous%2520normalizing%2520flows%2520built%2520on%2520Gaussian%2520denoising.%250AThrough%2520a%2520unified%2520framework%2520termed%2520Gaussian%2520interpolation%2520flow%252C%2520we%2520establish%250Athe%2520Lipschitz%2520regularity%2520of%2520the%2520flow%2520velocity%2520field%252C%2520the%2520existence%2520and%250Auniqueness%2520of%2520the%2520flow%252C%2520and%2520the%2520Lipschitz%2520continuity%2520of%2520the%2520flow%2520map%2520and%2520the%250Atime-reversed%2520flow%2520map%2520for%2520several%2520rich%2520classes%2520of%2520target%2520distributions.%2520This%250Aanalysis%2520also%2520sheds%2520light%2520on%2520the%2520auto-encoding%2520and%2520cycle%2520consistency%2520properties%250Aof%2520Gaussian%2520interpolation%2520flows.%2520Additionally%252C%2520we%2520study%2520the%2520stability%2520of%2520these%250Aflows%2520in%2520source%2520distributions%2520and%2520perturbations%2520of%2520the%2520velocity%2520field%252C%2520using%250Athe%2520quadratic%2520Wasserstein%2520distance%2520as%2520a%2520metric.%2520Our%2520findings%2520offer%2520valuable%250Ainsights%2520into%2520the%2520learning%2520techniques%2520employed%2520in%2520Gaussian%2520interpolation%2520flows%250Afor%2520generative%2520modeling%252C%2520providing%2520a%2520solid%2520theoretical%2520foundation%2520for%250Aend-to-end%2520error%2520analyses%2520of%2520learning%2520Gaussian%2520interpolation%2520flows%2520with%250Aempirical%2520observations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11475v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Interpolation%20Flows&entry.906535625=Yuan%20Gao%20and%20Jian%20Huang%20and%20Yuling%20Jiao&entry.1292438233=%20%20Gaussian%20denoising%20has%20emerged%20as%20a%20powerful%20method%20for%20constructing%0Asimulation-free%20continuous%20normalizing%20flows%20for%20generative%20modeling.%20Despite%0Atheir%20empirical%20successes%2C%20theoretical%20properties%20of%20these%20flows%20and%20the%0Aregularizing%20effect%20of%20Gaussian%20denoising%20have%20remained%20largely%20unexplored.%20In%0Athis%20work%2C%20we%20aim%20to%20address%20this%20gap%20by%20investigating%20the%20well-posedness%20of%0Asimulation-free%20continuous%20normalizing%20flows%20built%20on%20Gaussian%20denoising.%0AThrough%20a%20unified%20framework%20termed%20Gaussian%20interpolation%20flow%2C%20we%20establish%0Athe%20Lipschitz%20regularity%20of%20the%20flow%20velocity%20field%2C%20the%20existence%20and%0Auniqueness%20of%20the%20flow%2C%20and%20the%20Lipschitz%20continuity%20of%20the%20flow%20map%20and%20the%0Atime-reversed%20flow%20map%20for%20several%20rich%20classes%20of%20target%20distributions.%20This%0Aanalysis%20also%20sheds%20light%20on%20the%20auto-encoding%20and%20cycle%20consistency%20properties%0Aof%20Gaussian%20interpolation%20flows.%20Additionally%2C%20we%20study%20the%20stability%20of%20these%0Aflows%20in%20source%20distributions%20and%20perturbations%20of%20the%20velocity%20field%2C%20using%0Athe%20quadratic%20Wasserstein%20distance%20as%20a%20metric.%20Our%20findings%20offer%20valuable%0Ainsights%20into%20the%20learning%20techniques%20employed%20in%20Gaussian%20interpolation%20flows%0Afor%20generative%20modeling%2C%20providing%20a%20solid%20theoretical%20foundation%20for%0Aend-to-end%20error%20analyses%20of%20learning%20Gaussian%20interpolation%20flows%20with%0Aempirical%20observations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11475v2&entry.124074799=Read"},
{"title": "MoSt-DSA: Modeling Motion and Structural Interactions for Direct\n  Multi-Frame Interpolation in DSA Images", "author": "Ziyang Xu and Huangxuan Zhao and Ziwei Cui and Wenyu Liu and Chuansheng Zheng and Xinggang Wang", "abstract": "  Artificial intelligence has become a crucial tool for medical image analysis.\nAs an advanced cerebral angiography technique, Digital Subtraction Angiography\n(DSA) poses a challenge where the radiation dose to humans is proportional to\nthe image count. By reducing images and using AI interpolation instead, the\nradiation can be cut significantly. However, DSA images present more complex\nmotion and structural features than natural scenes, making interpolation more\nchallenging. We propose MoSt-DSA, the first work that uses deep learning for\nDSA frame interpolation. Unlike natural scene Video Frame Interpolation (VFI)\nmethods that extract unclear or coarse-grained features, we devise a general\nmodule that models motion and structural context interactions between frames in\nan efficient full convolution manner by adjusting optimal context range and\ntransforming contexts into linear functions. Benefiting from this, MoSt-DSA is\nalso the first method that directly achieves any number of interpolations at\nany time steps with just one forward pass during both training and testing. We\nconduct extensive comparisons with 7 representative VFI models for\ninterpolating 1 to 3 frames, MoSt-DSA demonstrates robust results across 470\nDSA image sequences (each typically 152 images), with average SSIM over 0.93,\naverage PSNR over 38 (standard deviations of less than 0.030 and 3.6,\nrespectively), comprehensively achieving state-of-the-art performance in\naccuracy, speed, visual effect, and memory usage. Our code is available at\nhttps://github.com/ZyoungXu/MoSt-DSA.\n", "link": "http://arxiv.org/abs/2407.07078v1", "date": "2024-07-09", "relevancy": 2.1977, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6088}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5469}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoSt-DSA%3A%20Modeling%20Motion%20and%20Structural%20Interactions%20for%20Direct%0A%20%20Multi-Frame%20Interpolation%20in%20DSA%20Images&body=Title%3A%20MoSt-DSA%3A%20Modeling%20Motion%20and%20Structural%20Interactions%20for%20Direct%0A%20%20Multi-Frame%20Interpolation%20in%20DSA%20Images%0AAuthor%3A%20Ziyang%20Xu%20and%20Huangxuan%20Zhao%20and%20Ziwei%20Cui%20and%20Wenyu%20Liu%20and%20Chuansheng%20Zheng%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Artificial%20intelligence%20has%20become%20a%20crucial%20tool%20for%20medical%20image%20analysis.%0AAs%20an%20advanced%20cerebral%20angiography%20technique%2C%20Digital%20Subtraction%20Angiography%0A%28DSA%29%20poses%20a%20challenge%20where%20the%20radiation%20dose%20to%20humans%20is%20proportional%20to%0Athe%20image%20count.%20By%20reducing%20images%20and%20using%20AI%20interpolation%20instead%2C%20the%0Aradiation%20can%20be%20cut%20significantly.%20However%2C%20DSA%20images%20present%20more%20complex%0Amotion%20and%20structural%20features%20than%20natural%20scenes%2C%20making%20interpolation%20more%0Achallenging.%20We%20propose%20MoSt-DSA%2C%20the%20first%20work%20that%20uses%20deep%20learning%20for%0ADSA%20frame%20interpolation.%20Unlike%20natural%20scene%20Video%20Frame%20Interpolation%20%28VFI%29%0Amethods%20that%20extract%20unclear%20or%20coarse-grained%20features%2C%20we%20devise%20a%20general%0Amodule%20that%20models%20motion%20and%20structural%20context%20interactions%20between%20frames%20in%0Aan%20efficient%20full%20convolution%20manner%20by%20adjusting%20optimal%20context%20range%20and%0Atransforming%20contexts%20into%20linear%20functions.%20Benefiting%20from%20this%2C%20MoSt-DSA%20is%0Aalso%20the%20first%20method%20that%20directly%20achieves%20any%20number%20of%20interpolations%20at%0Aany%20time%20steps%20with%20just%20one%20forward%20pass%20during%20both%20training%20and%20testing.%20We%0Aconduct%20extensive%20comparisons%20with%207%20representative%20VFI%20models%20for%0Ainterpolating%201%20to%203%20frames%2C%20MoSt-DSA%20demonstrates%20robust%20results%20across%20470%0ADSA%20image%20sequences%20%28each%20typically%20152%20images%29%2C%20with%20average%20SSIM%20over%200.93%2C%0Aaverage%20PSNR%20over%2038%20%28standard%20deviations%20of%20less%20than%200.030%20and%203.6%2C%0Arespectively%29%2C%20comprehensively%20achieving%20state-of-the-art%20performance%20in%0Aaccuracy%2C%20speed%2C%20visual%20effect%2C%20and%20memory%20usage.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ZyoungXu/MoSt-DSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoSt-DSA%253A%2520Modeling%2520Motion%2520and%2520Structural%2520Interactions%2520for%2520Direct%250A%2520%2520Multi-Frame%2520Interpolation%2520in%2520DSA%2520Images%26entry.906535625%3DZiyang%2520Xu%2520and%2520Huangxuan%2520Zhao%2520and%2520Ziwei%2520Cui%2520and%2520Wenyu%2520Liu%2520and%2520Chuansheng%2520Zheng%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520has%2520become%2520a%2520crucial%2520tool%2520for%2520medical%2520image%2520analysis.%250AAs%2520an%2520advanced%2520cerebral%2520angiography%2520technique%252C%2520Digital%2520Subtraction%2520Angiography%250A%2528DSA%2529%2520poses%2520a%2520challenge%2520where%2520the%2520radiation%2520dose%2520to%2520humans%2520is%2520proportional%2520to%250Athe%2520image%2520count.%2520By%2520reducing%2520images%2520and%2520using%2520AI%2520interpolation%2520instead%252C%2520the%250Aradiation%2520can%2520be%2520cut%2520significantly.%2520However%252C%2520DSA%2520images%2520present%2520more%2520complex%250Amotion%2520and%2520structural%2520features%2520than%2520natural%2520scenes%252C%2520making%2520interpolation%2520more%250Achallenging.%2520We%2520propose%2520MoSt-DSA%252C%2520the%2520first%2520work%2520that%2520uses%2520deep%2520learning%2520for%250ADSA%2520frame%2520interpolation.%2520Unlike%2520natural%2520scene%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%250Amethods%2520that%2520extract%2520unclear%2520or%2520coarse-grained%2520features%252C%2520we%2520devise%2520a%2520general%250Amodule%2520that%2520models%2520motion%2520and%2520structural%2520context%2520interactions%2520between%2520frames%2520in%250Aan%2520efficient%2520full%2520convolution%2520manner%2520by%2520adjusting%2520optimal%2520context%2520range%2520and%250Atransforming%2520contexts%2520into%2520linear%2520functions.%2520Benefiting%2520from%2520this%252C%2520MoSt-DSA%2520is%250Aalso%2520the%2520first%2520method%2520that%2520directly%2520achieves%2520any%2520number%2520of%2520interpolations%2520at%250Aany%2520time%2520steps%2520with%2520just%2520one%2520forward%2520pass%2520during%2520both%2520training%2520and%2520testing.%2520We%250Aconduct%2520extensive%2520comparisons%2520with%25207%2520representative%2520VFI%2520models%2520for%250Ainterpolating%25201%2520to%25203%2520frames%252C%2520MoSt-DSA%2520demonstrates%2520robust%2520results%2520across%2520470%250ADSA%2520image%2520sequences%2520%2528each%2520typically%2520152%2520images%2529%252C%2520with%2520average%2520SSIM%2520over%25200.93%252C%250Aaverage%2520PSNR%2520over%252038%2520%2528standard%2520deviations%2520of%2520less%2520than%25200.030%2520and%25203.6%252C%250Arespectively%2529%252C%2520comprehensively%2520achieving%2520state-of-the-art%2520performance%2520in%250Aaccuracy%252C%2520speed%252C%2520visual%2520effect%252C%2520and%2520memory%2520usage.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ZyoungXu/MoSt-DSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoSt-DSA%3A%20Modeling%20Motion%20and%20Structural%20Interactions%20for%20Direct%0A%20%20Multi-Frame%20Interpolation%20in%20DSA%20Images&entry.906535625=Ziyang%20Xu%20and%20Huangxuan%20Zhao%20and%20Ziwei%20Cui%20and%20Wenyu%20Liu%20and%20Chuansheng%20Zheng%20and%20Xinggang%20Wang&entry.1292438233=%20%20Artificial%20intelligence%20has%20become%20a%20crucial%20tool%20for%20medical%20image%20analysis.%0AAs%20an%20advanced%20cerebral%20angiography%20technique%2C%20Digital%20Subtraction%20Angiography%0A%28DSA%29%20poses%20a%20challenge%20where%20the%20radiation%20dose%20to%20humans%20is%20proportional%20to%0Athe%20image%20count.%20By%20reducing%20images%20and%20using%20AI%20interpolation%20instead%2C%20the%0Aradiation%20can%20be%20cut%20significantly.%20However%2C%20DSA%20images%20present%20more%20complex%0Amotion%20and%20structural%20features%20than%20natural%20scenes%2C%20making%20interpolation%20more%0Achallenging.%20We%20propose%20MoSt-DSA%2C%20the%20first%20work%20that%20uses%20deep%20learning%20for%0ADSA%20frame%20interpolation.%20Unlike%20natural%20scene%20Video%20Frame%20Interpolation%20%28VFI%29%0Amethods%20that%20extract%20unclear%20or%20coarse-grained%20features%2C%20we%20devise%20a%20general%0Amodule%20that%20models%20motion%20and%20structural%20context%20interactions%20between%20frames%20in%0Aan%20efficient%20full%20convolution%20manner%20by%20adjusting%20optimal%20context%20range%20and%0Atransforming%20contexts%20into%20linear%20functions.%20Benefiting%20from%20this%2C%20MoSt-DSA%20is%0Aalso%20the%20first%20method%20that%20directly%20achieves%20any%20number%20of%20interpolations%20at%0Aany%20time%20steps%20with%20just%20one%20forward%20pass%20during%20both%20training%20and%20testing.%20We%0Aconduct%20extensive%20comparisons%20with%207%20representative%20VFI%20models%20for%0Ainterpolating%201%20to%203%20frames%2C%20MoSt-DSA%20demonstrates%20robust%20results%20across%20470%0ADSA%20image%20sequences%20%28each%20typically%20152%20images%29%2C%20with%20average%20SSIM%20over%200.93%2C%0Aaverage%20PSNR%20over%2038%20%28standard%20deviations%20of%20less%20than%200.030%20and%203.6%2C%0Arespectively%29%2C%20comprehensively%20achieving%20state-of-the-art%20performance%20in%0Aaccuracy%2C%20speed%2C%20visual%20effect%2C%20and%20memory%20usage.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ZyoungXu/MoSt-DSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07078v1&entry.124074799=Read"},
{"title": "Less is More: Efficient Brain-Inspired Learning for Autonomous Driving\n  Trajectory Prediction", "author": "Haicheng Liao and Yongkang Li and Zhenning Li and Chengyue Wang and Chunlin Tian and Yuming Huang and Zilin Bian and Kaiqun Zhu and Guofa Li and Ziyuan Pu and Jia Hu and Zhiyong Cui and Chengzhong Xu", "abstract": "  Accurately and safely predicting the trajectories of surrounding vehicles is\nessential for fully realizing autonomous driving (AD). This paper presents the\nHuman-Like Trajectory Prediction model (HLTP++), which emulates human cognitive\nprocesses to improve trajectory prediction in AD. HLTP++ incorporates a novel\nteacher-student knowledge distillation framework. The \"teacher\" model equipped\nwith an adaptive visual sector, mimics the dynamic allocation of attention\nhuman drivers exhibit based on factors like spatial orientation, proximity, and\ndriving speed. On the other hand, the \"student\" model focuses on real-time\ninteraction and human decision-making, drawing parallels to the human memory\nstorage mechanism. Furthermore, we improve the model's efficiency by\nintroducing a new Fourier Adaptive Spike Neural Network (FA-SNN), allowing for\nfaster and more precise predictions with fewer parameters. Evaluated using the\nNGSIM, HighD, and MoCAD benchmarks, HLTP++ demonstrates superior performance\ncompared to existing models, which reduces the predicted trajectory error with\nover 11% on the NGSIM dataset and 25% on the HighD datasets. Moreover, HLTP++\ndemonstrates strong adaptability in challenging environments with incomplete\ninput data. This marks a significant stride in the journey towards fully AD\nsystems.\n", "link": "http://arxiv.org/abs/2407.07020v1", "date": "2024-07-09", "relevancy": 2.1919, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5802}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5444}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20Efficient%20Brain-Inspired%20Learning%20for%20Autonomous%20Driving%0A%20%20Trajectory%20Prediction&body=Title%3A%20Less%20is%20More%3A%20Efficient%20Brain-Inspired%20Learning%20for%20Autonomous%20Driving%0A%20%20Trajectory%20Prediction%0AAuthor%3A%20Haicheng%20Liao%20and%20Yongkang%20Li%20and%20Zhenning%20Li%20and%20Chengyue%20Wang%20and%20Chunlin%20Tian%20and%20Yuming%20Huang%20and%20Zilin%20Bian%20and%20Kaiqun%20Zhu%20and%20Guofa%20Li%20and%20Ziyuan%20Pu%20and%20Jia%20Hu%20and%20Zhiyong%20Cui%20and%20Chengzhong%20Xu%0AAbstract%3A%20%20%20Accurately%20and%20safely%20predicting%20the%20trajectories%20of%20surrounding%20vehicles%20is%0Aessential%20for%20fully%20realizing%20autonomous%20driving%20%28AD%29.%20This%20paper%20presents%20the%0AHuman-Like%20Trajectory%20Prediction%20model%20%28HLTP%2B%2B%29%2C%20which%20emulates%20human%20cognitive%0Aprocesses%20to%20improve%20trajectory%20prediction%20in%20AD.%20HLTP%2B%2B%20incorporates%20a%20novel%0Ateacher-student%20knowledge%20distillation%20framework.%20The%20%22teacher%22%20model%20equipped%0Awith%20an%20adaptive%20visual%20sector%2C%20mimics%20the%20dynamic%20allocation%20of%20attention%0Ahuman%20drivers%20exhibit%20based%20on%20factors%20like%20spatial%20orientation%2C%20proximity%2C%20and%0Adriving%20speed.%20On%20the%20other%20hand%2C%20the%20%22student%22%20model%20focuses%20on%20real-time%0Ainteraction%20and%20human%20decision-making%2C%20drawing%20parallels%20to%20the%20human%20memory%0Astorage%20mechanism.%20Furthermore%2C%20we%20improve%20the%20model%27s%20efficiency%20by%0Aintroducing%20a%20new%20Fourier%20Adaptive%20Spike%20Neural%20Network%20%28FA-SNN%29%2C%20allowing%20for%0Afaster%20and%20more%20precise%20predictions%20with%20fewer%20parameters.%20Evaluated%20using%20the%0ANGSIM%2C%20HighD%2C%20and%20MoCAD%20benchmarks%2C%20HLTP%2B%2B%20demonstrates%20superior%20performance%0Acompared%20to%20existing%20models%2C%20which%20reduces%20the%20predicted%20trajectory%20error%20with%0Aover%2011%25%20on%20the%20NGSIM%20dataset%20and%2025%25%20on%20the%20HighD%20datasets.%20Moreover%2C%20HLTP%2B%2B%0Ademonstrates%20strong%20adaptability%20in%20challenging%20environments%20with%20incomplete%0Ainput%20data.%20This%20marks%20a%20significant%20stride%20in%20the%20journey%20towards%20fully%20AD%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520Efficient%2520Brain-Inspired%2520Learning%2520for%2520Autonomous%2520Driving%250A%2520%2520Trajectory%2520Prediction%26entry.906535625%3DHaicheng%2520Liao%2520and%2520Yongkang%2520Li%2520and%2520Zhenning%2520Li%2520and%2520Chengyue%2520Wang%2520and%2520Chunlin%2520Tian%2520and%2520Yuming%2520Huang%2520and%2520Zilin%2520Bian%2520and%2520Kaiqun%2520Zhu%2520and%2520Guofa%2520Li%2520and%2520Ziyuan%2520Pu%2520and%2520Jia%2520Hu%2520and%2520Zhiyong%2520Cui%2520and%2520Chengzhong%2520Xu%26entry.1292438233%3D%2520%2520Accurately%2520and%2520safely%2520predicting%2520the%2520trajectories%2520of%2520surrounding%2520vehicles%2520is%250Aessential%2520for%2520fully%2520realizing%2520autonomous%2520driving%2520%2528AD%2529.%2520This%2520paper%2520presents%2520the%250AHuman-Like%2520Trajectory%2520Prediction%2520model%2520%2528HLTP%252B%252B%2529%252C%2520which%2520emulates%2520human%2520cognitive%250Aprocesses%2520to%2520improve%2520trajectory%2520prediction%2520in%2520AD.%2520HLTP%252B%252B%2520incorporates%2520a%2520novel%250Ateacher-student%2520knowledge%2520distillation%2520framework.%2520The%2520%2522teacher%2522%2520model%2520equipped%250Awith%2520an%2520adaptive%2520visual%2520sector%252C%2520mimics%2520the%2520dynamic%2520allocation%2520of%2520attention%250Ahuman%2520drivers%2520exhibit%2520based%2520on%2520factors%2520like%2520spatial%2520orientation%252C%2520proximity%252C%2520and%250Adriving%2520speed.%2520On%2520the%2520other%2520hand%252C%2520the%2520%2522student%2522%2520model%2520focuses%2520on%2520real-time%250Ainteraction%2520and%2520human%2520decision-making%252C%2520drawing%2520parallels%2520to%2520the%2520human%2520memory%250Astorage%2520mechanism.%2520Furthermore%252C%2520we%2520improve%2520the%2520model%2527s%2520efficiency%2520by%250Aintroducing%2520a%2520new%2520Fourier%2520Adaptive%2520Spike%2520Neural%2520Network%2520%2528FA-SNN%2529%252C%2520allowing%2520for%250Afaster%2520and%2520more%2520precise%2520predictions%2520with%2520fewer%2520parameters.%2520Evaluated%2520using%2520the%250ANGSIM%252C%2520HighD%252C%2520and%2520MoCAD%2520benchmarks%252C%2520HLTP%252B%252B%2520demonstrates%2520superior%2520performance%250Acompared%2520to%2520existing%2520models%252C%2520which%2520reduces%2520the%2520predicted%2520trajectory%2520error%2520with%250Aover%252011%2525%2520on%2520the%2520NGSIM%2520dataset%2520and%252025%2525%2520on%2520the%2520HighD%2520datasets.%2520Moreover%252C%2520HLTP%252B%252B%250Ademonstrates%2520strong%2520adaptability%2520in%2520challenging%2520environments%2520with%2520incomplete%250Ainput%2520data.%2520This%2520marks%2520a%2520significant%2520stride%2520in%2520the%2520journey%2520towards%2520fully%2520AD%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20Efficient%20Brain-Inspired%20Learning%20for%20Autonomous%20Driving%0A%20%20Trajectory%20Prediction&entry.906535625=Haicheng%20Liao%20and%20Yongkang%20Li%20and%20Zhenning%20Li%20and%20Chengyue%20Wang%20and%20Chunlin%20Tian%20and%20Yuming%20Huang%20and%20Zilin%20Bian%20and%20Kaiqun%20Zhu%20and%20Guofa%20Li%20and%20Ziyuan%20Pu%20and%20Jia%20Hu%20and%20Zhiyong%20Cui%20and%20Chengzhong%20Xu&entry.1292438233=%20%20Accurately%20and%20safely%20predicting%20the%20trajectories%20of%20surrounding%20vehicles%20is%0Aessential%20for%20fully%20realizing%20autonomous%20driving%20%28AD%29.%20This%20paper%20presents%20the%0AHuman-Like%20Trajectory%20Prediction%20model%20%28HLTP%2B%2B%29%2C%20which%20emulates%20human%20cognitive%0Aprocesses%20to%20improve%20trajectory%20prediction%20in%20AD.%20HLTP%2B%2B%20incorporates%20a%20novel%0Ateacher-student%20knowledge%20distillation%20framework.%20The%20%22teacher%22%20model%20equipped%0Awith%20an%20adaptive%20visual%20sector%2C%20mimics%20the%20dynamic%20allocation%20of%20attention%0Ahuman%20drivers%20exhibit%20based%20on%20factors%20like%20spatial%20orientation%2C%20proximity%2C%20and%0Adriving%20speed.%20On%20the%20other%20hand%2C%20the%20%22student%22%20model%20focuses%20on%20real-time%0Ainteraction%20and%20human%20decision-making%2C%20drawing%20parallels%20to%20the%20human%20memory%0Astorage%20mechanism.%20Furthermore%2C%20we%20improve%20the%20model%27s%20efficiency%20by%0Aintroducing%20a%20new%20Fourier%20Adaptive%20Spike%20Neural%20Network%20%28FA-SNN%29%2C%20allowing%20for%0Afaster%20and%20more%20precise%20predictions%20with%20fewer%20parameters.%20Evaluated%20using%20the%0ANGSIM%2C%20HighD%2C%20and%20MoCAD%20benchmarks%2C%20HLTP%2B%2B%20demonstrates%20superior%20performance%0Acompared%20to%20existing%20models%2C%20which%20reduces%20the%20predicted%20trajectory%20error%20with%0Aover%2011%25%20on%20the%20NGSIM%20dataset%20and%2025%25%20on%20the%20HighD%20datasets.%20Moreover%2C%20HLTP%2B%2B%0Ademonstrates%20strong%20adaptability%20in%20challenging%20environments%20with%20incomplete%0Ainput%20data.%20This%20marks%20a%20significant%20stride%20in%20the%20journey%20towards%20fully%20AD%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07020v1&entry.124074799=Read"},
{"title": "Joint prototype and coefficient prediction for 3D instance segmentation", "author": "Remco Royen and Leon Denis and Adrian Munteanu", "abstract": "  3D instance segmentation is crucial for applications demanding comprehensive\n3D scene understanding. In this paper, we introduce a novel method that\nsimultaneously learns coefficients and prototypes. Employing an overcomplete\nsampling strategy, our method produces an overcomplete set of instance\npredictions, from which the optimal ones are selected through a Non-Maximum\nSuppression (NMS) algorithm during inference. The obtained prototypes are\nvisualizable and interpretable. Our method demonstrates superior performance on\nS3DIS-blocks, consistently outperforming existing methods in mRec and mPrec.\nMoreover, it operates 32.9% faster than the state-of-the-art. Notably, with\nonly 0.8% of the total inference time, our method exhibits an over 20-fold\nreduction in the variance of inference time compared to existing methods. These\nattributes render our method well-suited for practical applications requiring\nboth rapid inference and high reliability.\n", "link": "http://arxiv.org/abs/2407.06958v1", "date": "2024-07-09", "relevancy": 2.1848, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5462}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20prototype%20and%20coefficient%20prediction%20for%203D%20instance%20segmentation&body=Title%3A%20Joint%20prototype%20and%20coefficient%20prediction%20for%203D%20instance%20segmentation%0AAuthor%3A%20Remco%20Royen%20and%20Leon%20Denis%20and%20Adrian%20Munteanu%0AAbstract%3A%20%20%203D%20instance%20segmentation%20is%20crucial%20for%20applications%20demanding%20comprehensive%0A3D%20scene%20understanding.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20method%20that%0Asimultaneously%20learns%20coefficients%20and%20prototypes.%20Employing%20an%20overcomplete%0Asampling%20strategy%2C%20our%20method%20produces%20an%20overcomplete%20set%20of%20instance%0Apredictions%2C%20from%20which%20the%20optimal%20ones%20are%20selected%20through%20a%20Non-Maximum%0ASuppression%20%28NMS%29%20algorithm%20during%20inference.%20The%20obtained%20prototypes%20are%0Avisualizable%20and%20interpretable.%20Our%20method%20demonstrates%20superior%20performance%20on%0AS3DIS-blocks%2C%20consistently%20outperforming%20existing%20methods%20in%20mRec%20and%20mPrec.%0AMoreover%2C%20it%20operates%2032.9%25%20faster%20than%20the%20state-of-the-art.%20Notably%2C%20with%0Aonly%200.8%25%20of%20the%20total%20inference%20time%2C%20our%20method%20exhibits%20an%20over%2020-fold%0Areduction%20in%20the%20variance%20of%20inference%20time%20compared%20to%20existing%20methods.%20These%0Aattributes%20render%20our%20method%20well-suited%20for%20practical%20applications%20requiring%0Aboth%20rapid%20inference%20and%20high%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520prototype%2520and%2520coefficient%2520prediction%2520for%25203D%2520instance%2520segmentation%26entry.906535625%3DRemco%2520Royen%2520and%2520Leon%2520Denis%2520and%2520Adrian%2520Munteanu%26entry.1292438233%3D%2520%25203D%2520instance%2520segmentation%2520is%2520crucial%2520for%2520applications%2520demanding%2520comprehensive%250A3D%2520scene%2520understanding.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520method%2520that%250Asimultaneously%2520learns%2520coefficients%2520and%2520prototypes.%2520Employing%2520an%2520overcomplete%250Asampling%2520strategy%252C%2520our%2520method%2520produces%2520an%2520overcomplete%2520set%2520of%2520instance%250Apredictions%252C%2520from%2520which%2520the%2520optimal%2520ones%2520are%2520selected%2520through%2520a%2520Non-Maximum%250ASuppression%2520%2528NMS%2529%2520algorithm%2520during%2520inference.%2520The%2520obtained%2520prototypes%2520are%250Avisualizable%2520and%2520interpretable.%2520Our%2520method%2520demonstrates%2520superior%2520performance%2520on%250AS3DIS-blocks%252C%2520consistently%2520outperforming%2520existing%2520methods%2520in%2520mRec%2520and%2520mPrec.%250AMoreover%252C%2520it%2520operates%252032.9%2525%2520faster%2520than%2520the%2520state-of-the-art.%2520Notably%252C%2520with%250Aonly%25200.8%2525%2520of%2520the%2520total%2520inference%2520time%252C%2520our%2520method%2520exhibits%2520an%2520over%252020-fold%250Areduction%2520in%2520the%2520variance%2520of%2520inference%2520time%2520compared%2520to%2520existing%2520methods.%2520These%250Aattributes%2520render%2520our%2520method%2520well-suited%2520for%2520practical%2520applications%2520requiring%250Aboth%2520rapid%2520inference%2520and%2520high%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20prototype%20and%20coefficient%20prediction%20for%203D%20instance%20segmentation&entry.906535625=Remco%20Royen%20and%20Leon%20Denis%20and%20Adrian%20Munteanu&entry.1292438233=%20%203D%20instance%20segmentation%20is%20crucial%20for%20applications%20demanding%20comprehensive%0A3D%20scene%20understanding.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20method%20that%0Asimultaneously%20learns%20coefficients%20and%20prototypes.%20Employing%20an%20overcomplete%0Asampling%20strategy%2C%20our%20method%20produces%20an%20overcomplete%20set%20of%20instance%0Apredictions%2C%20from%20which%20the%20optimal%20ones%20are%20selected%20through%20a%20Non-Maximum%0ASuppression%20%28NMS%29%20algorithm%20during%20inference.%20The%20obtained%20prototypes%20are%0Avisualizable%20and%20interpretable.%20Our%20method%20demonstrates%20superior%20performance%20on%0AS3DIS-blocks%2C%20consistently%20outperforming%20existing%20methods%20in%20mRec%20and%20mPrec.%0AMoreover%2C%20it%20operates%2032.9%25%20faster%20than%20the%20state-of-the-art.%20Notably%2C%20with%0Aonly%200.8%25%20of%20the%20total%20inference%20time%2C%20our%20method%20exhibits%20an%20over%2020-fold%0Areduction%20in%20the%20variance%20of%20inference%20time%20compared%20to%20existing%20methods.%20These%0Aattributes%20render%20our%20method%20well-suited%20for%20practical%20applications%20requiring%0Aboth%20rapid%20inference%20and%20high%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06958v1&entry.124074799=Read"},
{"title": "Convergence rates for Poisson learning to a Poisson equation with\n  measure data", "author": "Leon Bungert and Jeff Calder and Max Mihailescu and Kodjo Houssou and Amber Yuan", "abstract": "  In this paper we prove discrete to continuum convergence rates for Poisson\nLearning, a graph-based semi-supervised learning algorithm that is based on\nsolving the graph Poisson equation with a source term consisting of a linear\ncombination of Dirac deltas located at labeled points and carrying label\ninformation. The corresponding continuum equation is a Poisson equation with\nmeasure data in a Euclidean domain $\\Omega \\subset \\mathbb{R}^d$. The singular\nnature of these equations is challenging and requires an approach with several\ndistinct parts: (1) We prove quantitative error estimates when convolving the\nmeasure data of a Poisson equation with (approximately) radial function\nsupported on balls. (2) We use quantitative variational techniques to prove\ndiscrete to continuum convergence rates on random geometric graphs with\nbandwidth $\\varepsilon>0$ for bounded source terms. (3) We show how to\nregularize the graph Poisson equation via mollification with the graph heat\nkernel, and we study fine asymptotics of the heat kernel on random geometric\ngraphs. Combining these three pillars we obtain $L^1$ convergence rates that\nscale, up to logarithmic factors, like $O(\\varepsilon^{\\frac{1}{d+2}})$ for\ngeneral data distributions, and $O(\\varepsilon^{\\frac{2-\\sigma}{d+4}})$ for\nuniformly distributed data, where $\\sigma>0$. These rates are valid with high\nprobability if $\\varepsilon\\gg\\left({\\log n}/{n}\\right)^q$ where $n$ denotes\nthe number of vertices of the graph and $q \\approx \\frac{1}{3d}$.\n", "link": "http://arxiv.org/abs/2407.06783v1", "date": "2024-07-09", "relevancy": 2.177, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4422}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4332}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20rates%20for%20Poisson%20learning%20to%20a%20Poisson%20equation%20with%0A%20%20measure%20data&body=Title%3A%20Convergence%20rates%20for%20Poisson%20learning%20to%20a%20Poisson%20equation%20with%0A%20%20measure%20data%0AAuthor%3A%20Leon%20Bungert%20and%20Jeff%20Calder%20and%20Max%20Mihailescu%20and%20Kodjo%20Houssou%20and%20Amber%20Yuan%0AAbstract%3A%20%20%20In%20this%20paper%20we%20prove%20discrete%20to%20continuum%20convergence%20rates%20for%20Poisson%0ALearning%2C%20a%20graph-based%20semi-supervised%20learning%20algorithm%20that%20is%20based%20on%0Asolving%20the%20graph%20Poisson%20equation%20with%20a%20source%20term%20consisting%20of%20a%20linear%0Acombination%20of%20Dirac%20deltas%20located%20at%20labeled%20points%20and%20carrying%20label%0Ainformation.%20The%20corresponding%20continuum%20equation%20is%20a%20Poisson%20equation%20with%0Ameasure%20data%20in%20a%20Euclidean%20domain%20%24%5COmega%20%5Csubset%20%5Cmathbb%7BR%7D%5Ed%24.%20The%20singular%0Anature%20of%20these%20equations%20is%20challenging%20and%20requires%20an%20approach%20with%20several%0Adistinct%20parts%3A%20%281%29%20We%20prove%20quantitative%20error%20estimates%20when%20convolving%20the%0Ameasure%20data%20of%20a%20Poisson%20equation%20with%20%28approximately%29%20radial%20function%0Asupported%20on%20balls.%20%282%29%20We%20use%20quantitative%20variational%20techniques%20to%20prove%0Adiscrete%20to%20continuum%20convergence%20rates%20on%20random%20geometric%20graphs%20with%0Abandwidth%20%24%5Cvarepsilon%3E0%24%20for%20bounded%20source%20terms.%20%283%29%20We%20show%20how%20to%0Aregularize%20the%20graph%20Poisson%20equation%20via%20mollification%20with%20the%20graph%20heat%0Akernel%2C%20and%20we%20study%20fine%20asymptotics%20of%20the%20heat%20kernel%20on%20random%20geometric%0Agraphs.%20Combining%20these%20three%20pillars%20we%20obtain%20%24L%5E1%24%20convergence%20rates%20that%0Ascale%2C%20up%20to%20logarithmic%20factors%2C%20like%20%24O%28%5Cvarepsilon%5E%7B%5Cfrac%7B1%7D%7Bd%2B2%7D%7D%29%24%20for%0Ageneral%20data%20distributions%2C%20and%20%24O%28%5Cvarepsilon%5E%7B%5Cfrac%7B2-%5Csigma%7D%7Bd%2B4%7D%7D%29%24%20for%0Auniformly%20distributed%20data%2C%20where%20%24%5Csigma%3E0%24.%20These%20rates%20are%20valid%20with%20high%0Aprobability%20if%20%24%5Cvarepsilon%5Cgg%5Cleft%28%7B%5Clog%20n%7D/%7Bn%7D%5Cright%29%5Eq%24%20where%20%24n%24%20denotes%0Athe%20number%20of%20vertices%20of%20the%20graph%20and%20%24q%20%5Capprox%20%5Cfrac%7B1%7D%7B3d%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520rates%2520for%2520Poisson%2520learning%2520to%2520a%2520Poisson%2520equation%2520with%250A%2520%2520measure%2520data%26entry.906535625%3DLeon%2520Bungert%2520and%2520Jeff%2520Calder%2520and%2520Max%2520Mihailescu%2520and%2520Kodjo%2520Houssou%2520and%2520Amber%2520Yuan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520prove%2520discrete%2520to%2520continuum%2520convergence%2520rates%2520for%2520Poisson%250ALearning%252C%2520a%2520graph-based%2520semi-supervised%2520learning%2520algorithm%2520that%2520is%2520based%2520on%250Asolving%2520the%2520graph%2520Poisson%2520equation%2520with%2520a%2520source%2520term%2520consisting%2520of%2520a%2520linear%250Acombination%2520of%2520Dirac%2520deltas%2520located%2520at%2520labeled%2520points%2520and%2520carrying%2520label%250Ainformation.%2520The%2520corresponding%2520continuum%2520equation%2520is%2520a%2520Poisson%2520equation%2520with%250Ameasure%2520data%2520in%2520a%2520Euclidean%2520domain%2520%2524%255COmega%2520%255Csubset%2520%255Cmathbb%257BR%257D%255Ed%2524.%2520The%2520singular%250Anature%2520of%2520these%2520equations%2520is%2520challenging%2520and%2520requires%2520an%2520approach%2520with%2520several%250Adistinct%2520parts%253A%2520%25281%2529%2520We%2520prove%2520quantitative%2520error%2520estimates%2520when%2520convolving%2520the%250Ameasure%2520data%2520of%2520a%2520Poisson%2520equation%2520with%2520%2528approximately%2529%2520radial%2520function%250Asupported%2520on%2520balls.%2520%25282%2529%2520We%2520use%2520quantitative%2520variational%2520techniques%2520to%2520prove%250Adiscrete%2520to%2520continuum%2520convergence%2520rates%2520on%2520random%2520geometric%2520graphs%2520with%250Abandwidth%2520%2524%255Cvarepsilon%253E0%2524%2520for%2520bounded%2520source%2520terms.%2520%25283%2529%2520We%2520show%2520how%2520to%250Aregularize%2520the%2520graph%2520Poisson%2520equation%2520via%2520mollification%2520with%2520the%2520graph%2520heat%250Akernel%252C%2520and%2520we%2520study%2520fine%2520asymptotics%2520of%2520the%2520heat%2520kernel%2520on%2520random%2520geometric%250Agraphs.%2520Combining%2520these%2520three%2520pillars%2520we%2520obtain%2520%2524L%255E1%2524%2520convergence%2520rates%2520that%250Ascale%252C%2520up%2520to%2520logarithmic%2520factors%252C%2520like%2520%2524O%2528%255Cvarepsilon%255E%257B%255Cfrac%257B1%257D%257Bd%252B2%257D%257D%2529%2524%2520for%250Ageneral%2520data%2520distributions%252C%2520and%2520%2524O%2528%255Cvarepsilon%255E%257B%255Cfrac%257B2-%255Csigma%257D%257Bd%252B4%257D%257D%2529%2524%2520for%250Auniformly%2520distributed%2520data%252C%2520where%2520%2524%255Csigma%253E0%2524.%2520These%2520rates%2520are%2520valid%2520with%2520high%250Aprobability%2520if%2520%2524%255Cvarepsilon%255Cgg%255Cleft%2528%257B%255Clog%2520n%257D/%257Bn%257D%255Cright%2529%255Eq%2524%2520where%2520%2524n%2524%2520denotes%250Athe%2520number%2520of%2520vertices%2520of%2520the%2520graph%2520and%2520%2524q%2520%255Capprox%2520%255Cfrac%257B1%257D%257B3d%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20rates%20for%20Poisson%20learning%20to%20a%20Poisson%20equation%20with%0A%20%20measure%20data&entry.906535625=Leon%20Bungert%20and%20Jeff%20Calder%20and%20Max%20Mihailescu%20and%20Kodjo%20Houssou%20and%20Amber%20Yuan&entry.1292438233=%20%20In%20this%20paper%20we%20prove%20discrete%20to%20continuum%20convergence%20rates%20for%20Poisson%0ALearning%2C%20a%20graph-based%20semi-supervised%20learning%20algorithm%20that%20is%20based%20on%0Asolving%20the%20graph%20Poisson%20equation%20with%20a%20source%20term%20consisting%20of%20a%20linear%0Acombination%20of%20Dirac%20deltas%20located%20at%20labeled%20points%20and%20carrying%20label%0Ainformation.%20The%20corresponding%20continuum%20equation%20is%20a%20Poisson%20equation%20with%0Ameasure%20data%20in%20a%20Euclidean%20domain%20%24%5COmega%20%5Csubset%20%5Cmathbb%7BR%7D%5Ed%24.%20The%20singular%0Anature%20of%20these%20equations%20is%20challenging%20and%20requires%20an%20approach%20with%20several%0Adistinct%20parts%3A%20%281%29%20We%20prove%20quantitative%20error%20estimates%20when%20convolving%20the%0Ameasure%20data%20of%20a%20Poisson%20equation%20with%20%28approximately%29%20radial%20function%0Asupported%20on%20balls.%20%282%29%20We%20use%20quantitative%20variational%20techniques%20to%20prove%0Adiscrete%20to%20continuum%20convergence%20rates%20on%20random%20geometric%20graphs%20with%0Abandwidth%20%24%5Cvarepsilon%3E0%24%20for%20bounded%20source%20terms.%20%283%29%20We%20show%20how%20to%0Aregularize%20the%20graph%20Poisson%20equation%20via%20mollification%20with%20the%20graph%20heat%0Akernel%2C%20and%20we%20study%20fine%20asymptotics%20of%20the%20heat%20kernel%20on%20random%20geometric%0Agraphs.%20Combining%20these%20three%20pillars%20we%20obtain%20%24L%5E1%24%20convergence%20rates%20that%0Ascale%2C%20up%20to%20logarithmic%20factors%2C%20like%20%24O%28%5Cvarepsilon%5E%7B%5Cfrac%7B1%7D%7Bd%2B2%7D%7D%29%24%20for%0Ageneral%20data%20distributions%2C%20and%20%24O%28%5Cvarepsilon%5E%7B%5Cfrac%7B2-%5Csigma%7D%7Bd%2B4%7D%7D%29%24%20for%0Auniformly%20distributed%20data%2C%20where%20%24%5Csigma%3E0%24.%20These%20rates%20are%20valid%20with%20high%0Aprobability%20if%20%24%5Cvarepsilon%5Cgg%5Cleft%28%7B%5Clog%20n%7D/%7Bn%7D%5Cright%29%5Eq%24%20where%20%24n%24%20denotes%0Athe%20number%20of%20vertices%20of%20the%20graph%20and%20%24q%20%5Capprox%20%5Cfrac%7B1%7D%7B3d%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06783v1&entry.124074799=Read"},
{"title": "Beyond Aesthetics: Cultural Competence in Text-to-Image Models", "author": "Nithish Kannen and Arif Ahmad and Marco Andreetto and Vinodkumar Prabhakaran and Utsav Prabhu and Adji Bousso Dieng and Pushpak Bhattacharyya and Shachi Dave", "abstract": "  Text-to-Image (T2I) models are being increasingly adopted in diverse global\ncommunities where they create visual representations of their unique cultures.\nCurrent T2I benchmarks primarily focus on faithfulness, aesthetics, and realism\nof generated images, overlooking the critical dimension of cultural competence.\nIn this work, we introduce a framework to evaluate cultural competence of T2I\nmodels along two crucial dimensions: cultural awareness and cultural diversity,\nand present a scalable approach using a combination of structured knowledge\nbases and large language models to build a large dataset of cultural artifacts\nto enable this evaluation. In particular, we apply this approach to build CUBE\n(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to\nevaluate cultural competence of T2I models. CUBE covers cultural artifacts\nassociated with 8 countries across different geo-cultural regions and along 3\nconcepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of\nhigh-quality prompts that enable the evaluation of cultural awareness, and 2)\nCUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to\nevaluate cultural diversity. We also introduce cultural diversity as a novel\nT2I evaluation component, leveraging quality-weighted Vendi score. Our\nevaluations reveal significant gaps in the cultural awareness of existing\nmodels across countries and provide valuable insights into the cultural\ndiversity of T2I outputs for under-specified prompts. Our methodology is\nextendable to other cultural regions and concepts, and can facilitate the\ndevelopment of T2I models that better cater to the global population.\n", "link": "http://arxiv.org/abs/2407.06863v1", "date": "2024-07-09", "relevancy": 2.1701, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5503}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5475}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Aesthetics%3A%20Cultural%20Competence%20in%20Text-to-Image%20Models&body=Title%3A%20Beyond%20Aesthetics%3A%20Cultural%20Competence%20in%20Text-to-Image%20Models%0AAuthor%3A%20Nithish%20Kannen%20and%20Arif%20Ahmad%20and%20Marco%20Andreetto%20and%20Vinodkumar%20Prabhakaran%20and%20Utsav%20Prabhu%20and%20Adji%20Bousso%20Dieng%20and%20Pushpak%20Bhattacharyya%20and%20Shachi%20Dave%0AAbstract%3A%20%20%20Text-to-Image%20%28T2I%29%20models%20are%20being%20increasingly%20adopted%20in%20diverse%20global%0Acommunities%20where%20they%20create%20visual%20representations%20of%20their%20unique%20cultures.%0ACurrent%20T2I%20benchmarks%20primarily%20focus%20on%20faithfulness%2C%20aesthetics%2C%20and%20realism%0Aof%20generated%20images%2C%20overlooking%20the%20critical%20dimension%20of%20cultural%20competence.%0AIn%20this%20work%2C%20we%20introduce%20a%20framework%20to%20evaluate%20cultural%20competence%20of%20T2I%0Amodels%20along%20two%20crucial%20dimensions%3A%20cultural%20awareness%20and%20cultural%20diversity%2C%0Aand%20present%20a%20scalable%20approach%20using%20a%20combination%20of%20structured%20knowledge%0Abases%20and%20large%20language%20models%20to%20build%20a%20large%20dataset%20of%20cultural%20artifacts%0Ato%20enable%20this%20evaluation.%20In%20particular%2C%20we%20apply%20this%20approach%20to%20build%20CUBE%0A%28CUltural%20BEnchmark%20for%20Text-to-Image%20models%29%2C%20a%20first-of-its-kind%20benchmark%20to%0Aevaluate%20cultural%20competence%20of%20T2I%20models.%20CUBE%20covers%20cultural%20artifacts%0Aassociated%20with%208%20countries%20across%20different%20geo-cultural%20regions%20and%20along%203%0Aconcepts%3A%20cuisine%2C%20landmarks%2C%20and%20art.%20CUBE%20consists%20of%201%29%20CUBE-1K%2C%20a%20set%20of%0Ahigh-quality%20prompts%20that%20enable%20the%20evaluation%20of%20cultural%20awareness%2C%20and%202%29%0ACUBE-CSpace%2C%20a%20larger%20dataset%20of%20cultural%20artifacts%20that%20serves%20as%20grounding%20to%0Aevaluate%20cultural%20diversity.%20We%20also%20introduce%20cultural%20diversity%20as%20a%20novel%0AT2I%20evaluation%20component%2C%20leveraging%20quality-weighted%20Vendi%20score.%20Our%0Aevaluations%20reveal%20significant%20gaps%20in%20the%20cultural%20awareness%20of%20existing%0Amodels%20across%20countries%20and%20provide%20valuable%20insights%20into%20the%20cultural%0Adiversity%20of%20T2I%20outputs%20for%20under-specified%20prompts.%20Our%20methodology%20is%0Aextendable%20to%20other%20cultural%20regions%20and%20concepts%2C%20and%20can%20facilitate%20the%0Adevelopment%20of%20T2I%20models%20that%20better%20cater%20to%20the%20global%20population.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Aesthetics%253A%2520Cultural%2520Competence%2520in%2520Text-to-Image%2520Models%26entry.906535625%3DNithish%2520Kannen%2520and%2520Arif%2520Ahmad%2520and%2520Marco%2520Andreetto%2520and%2520Vinodkumar%2520Prabhakaran%2520and%2520Utsav%2520Prabhu%2520and%2520Adji%2520Bousso%2520Dieng%2520and%2520Pushpak%2520Bhattacharyya%2520and%2520Shachi%2520Dave%26entry.1292438233%3D%2520%2520Text-to-Image%2520%2528T2I%2529%2520models%2520are%2520being%2520increasingly%2520adopted%2520in%2520diverse%2520global%250Acommunities%2520where%2520they%2520create%2520visual%2520representations%2520of%2520their%2520unique%2520cultures.%250ACurrent%2520T2I%2520benchmarks%2520primarily%2520focus%2520on%2520faithfulness%252C%2520aesthetics%252C%2520and%2520realism%250Aof%2520generated%2520images%252C%2520overlooking%2520the%2520critical%2520dimension%2520of%2520cultural%2520competence.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520a%2520framework%2520to%2520evaluate%2520cultural%2520competence%2520of%2520T2I%250Amodels%2520along%2520two%2520crucial%2520dimensions%253A%2520cultural%2520awareness%2520and%2520cultural%2520diversity%252C%250Aand%2520present%2520a%2520scalable%2520approach%2520using%2520a%2520combination%2520of%2520structured%2520knowledge%250Abases%2520and%2520large%2520language%2520models%2520to%2520build%2520a%2520large%2520dataset%2520of%2520cultural%2520artifacts%250Ato%2520enable%2520this%2520evaluation.%2520In%2520particular%252C%2520we%2520apply%2520this%2520approach%2520to%2520build%2520CUBE%250A%2528CUltural%2520BEnchmark%2520for%2520Text-to-Image%2520models%2529%252C%2520a%2520first-of-its-kind%2520benchmark%2520to%250Aevaluate%2520cultural%2520competence%2520of%2520T2I%2520models.%2520CUBE%2520covers%2520cultural%2520artifacts%250Aassociated%2520with%25208%2520countries%2520across%2520different%2520geo-cultural%2520regions%2520and%2520along%25203%250Aconcepts%253A%2520cuisine%252C%2520landmarks%252C%2520and%2520art.%2520CUBE%2520consists%2520of%25201%2529%2520CUBE-1K%252C%2520a%2520set%2520of%250Ahigh-quality%2520prompts%2520that%2520enable%2520the%2520evaluation%2520of%2520cultural%2520awareness%252C%2520and%25202%2529%250ACUBE-CSpace%252C%2520a%2520larger%2520dataset%2520of%2520cultural%2520artifacts%2520that%2520serves%2520as%2520grounding%2520to%250Aevaluate%2520cultural%2520diversity.%2520We%2520also%2520introduce%2520cultural%2520diversity%2520as%2520a%2520novel%250AT2I%2520evaluation%2520component%252C%2520leveraging%2520quality-weighted%2520Vendi%2520score.%2520Our%250Aevaluations%2520reveal%2520significant%2520gaps%2520in%2520the%2520cultural%2520awareness%2520of%2520existing%250Amodels%2520across%2520countries%2520and%2520provide%2520valuable%2520insights%2520into%2520the%2520cultural%250Adiversity%2520of%2520T2I%2520outputs%2520for%2520under-specified%2520prompts.%2520Our%2520methodology%2520is%250Aextendable%2520to%2520other%2520cultural%2520regions%2520and%2520concepts%252C%2520and%2520can%2520facilitate%2520the%250Adevelopment%2520of%2520T2I%2520models%2520that%2520better%2520cater%2520to%2520the%2520global%2520population.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Aesthetics%3A%20Cultural%20Competence%20in%20Text-to-Image%20Models&entry.906535625=Nithish%20Kannen%20and%20Arif%20Ahmad%20and%20Marco%20Andreetto%20and%20Vinodkumar%20Prabhakaran%20and%20Utsav%20Prabhu%20and%20Adji%20Bousso%20Dieng%20and%20Pushpak%20Bhattacharyya%20and%20Shachi%20Dave&entry.1292438233=%20%20Text-to-Image%20%28T2I%29%20models%20are%20being%20increasingly%20adopted%20in%20diverse%20global%0Acommunities%20where%20they%20create%20visual%20representations%20of%20their%20unique%20cultures.%0ACurrent%20T2I%20benchmarks%20primarily%20focus%20on%20faithfulness%2C%20aesthetics%2C%20and%20realism%0Aof%20generated%20images%2C%20overlooking%20the%20critical%20dimension%20of%20cultural%20competence.%0AIn%20this%20work%2C%20we%20introduce%20a%20framework%20to%20evaluate%20cultural%20competence%20of%20T2I%0Amodels%20along%20two%20crucial%20dimensions%3A%20cultural%20awareness%20and%20cultural%20diversity%2C%0Aand%20present%20a%20scalable%20approach%20using%20a%20combination%20of%20structured%20knowledge%0Abases%20and%20large%20language%20models%20to%20build%20a%20large%20dataset%20of%20cultural%20artifacts%0Ato%20enable%20this%20evaluation.%20In%20particular%2C%20we%20apply%20this%20approach%20to%20build%20CUBE%0A%28CUltural%20BEnchmark%20for%20Text-to-Image%20models%29%2C%20a%20first-of-its-kind%20benchmark%20to%0Aevaluate%20cultural%20competence%20of%20T2I%20models.%20CUBE%20covers%20cultural%20artifacts%0Aassociated%20with%208%20countries%20across%20different%20geo-cultural%20regions%20and%20along%203%0Aconcepts%3A%20cuisine%2C%20landmarks%2C%20and%20art.%20CUBE%20consists%20of%201%29%20CUBE-1K%2C%20a%20set%20of%0Ahigh-quality%20prompts%20that%20enable%20the%20evaluation%20of%20cultural%20awareness%2C%20and%202%29%0ACUBE-CSpace%2C%20a%20larger%20dataset%20of%20cultural%20artifacts%20that%20serves%20as%20grounding%20to%0Aevaluate%20cultural%20diversity.%20We%20also%20introduce%20cultural%20diversity%20as%20a%20novel%0AT2I%20evaluation%20component%2C%20leveraging%20quality-weighted%20Vendi%20score.%20Our%0Aevaluations%20reveal%20significant%20gaps%20in%20the%20cultural%20awareness%20of%20existing%0Amodels%20across%20countries%20and%20provide%20valuable%20insights%20into%20the%20cultural%0Adiversity%20of%20T2I%20outputs%20for%20under-specified%20prompts.%20Our%20methodology%20is%0Aextendable%20to%20other%20cultural%20regions%20and%20concepts%2C%20and%20can%20facilitate%20the%0Adevelopment%20of%20T2I%20models%20that%20better%20cater%20to%20the%20global%20population.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06863v1&entry.124074799=Read"},
{"title": "HTD-Mamba: Efficient Hyperspectral Target Detection with Pyramid State\n  Space Model", "author": "Dunbin Shen and Xuanbing Zhu and Jiacheng Tian and Jianjun Liu and Zhenrong Du and Hongyu Wang and Xiaorui Ma", "abstract": "  Hyperspectral target detection (HTD) identifies objects of interest from\ncomplex backgrounds at the pixel level, playing a vital role in Earth\nobservation. However, HTD faces challenges due to limited prior knowledge and\nspectral variations, leading to underfitting models and unreliable performance.\nTo address these challenges, this paper proposes an efficient self-supervised\nHTD method with a pyramid state space model (SSM), named HTD-Mamba, which\nemploys spectrally contrastive learning to distinguish between target and\nbackground based on the similarity measurement of intrinsic features.\nSpecifically, to obtain sufficient training samples and leverage spatial\ncontextual information, we propose a spatial-encoded spectral augmentation\ntechnique that encodes all surrounding pixels within a patch into a transformed\nview of the central pixel. Additionally, to explore global band correlations,\nwe divide pixels into continuous group-wise spectral embeddings and introduce\nMamba to HTD for the first time to model long-range dependencies of the\nspectral sequence with linear complexity. Furthermore, to alleviate spectral\nvariation and enhance robust representation, we propose a pyramid SSM as a\nbackbone to capture and fuse multiresolution spectral-wise intrinsic features.\nExtensive experiments conducted on four public datasets demonstrate that the\nproposed method outperforms state-of-the-art methods in both quantitative and\nqualitative evaluations. Code is available at\n\\url{https://github.com/shendb2022/HTD-Mamba}.\n", "link": "http://arxiv.org/abs/2407.06841v1", "date": "2024-07-09", "relevancy": 2.1569, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.56}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5251}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HTD-Mamba%3A%20Efficient%20Hyperspectral%20Target%20Detection%20with%20Pyramid%20State%0A%20%20Space%20Model&body=Title%3A%20HTD-Mamba%3A%20Efficient%20Hyperspectral%20Target%20Detection%20with%20Pyramid%20State%0A%20%20Space%20Model%0AAuthor%3A%20Dunbin%20Shen%20and%20Xuanbing%20Zhu%20and%20Jiacheng%20Tian%20and%20Jianjun%20Liu%20and%20Zhenrong%20Du%20and%20Hongyu%20Wang%20and%20Xiaorui%20Ma%0AAbstract%3A%20%20%20Hyperspectral%20target%20detection%20%28HTD%29%20identifies%20objects%20of%20interest%20from%0Acomplex%20backgrounds%20at%20the%20pixel%20level%2C%20playing%20a%20vital%20role%20in%20Earth%0Aobservation.%20However%2C%20HTD%20faces%20challenges%20due%20to%20limited%20prior%20knowledge%20and%0Aspectral%20variations%2C%20leading%20to%20underfitting%20models%20and%20unreliable%20performance.%0ATo%20address%20these%20challenges%2C%20this%20paper%20proposes%20an%20efficient%20self-supervised%0AHTD%20method%20with%20a%20pyramid%20state%20space%20model%20%28SSM%29%2C%20named%20HTD-Mamba%2C%20which%0Aemploys%20spectrally%20contrastive%20learning%20to%20distinguish%20between%20target%20and%0Abackground%20based%20on%20the%20similarity%20measurement%20of%20intrinsic%20features.%0ASpecifically%2C%20to%20obtain%20sufficient%20training%20samples%20and%20leverage%20spatial%0Acontextual%20information%2C%20we%20propose%20a%20spatial-encoded%20spectral%20augmentation%0Atechnique%20that%20encodes%20all%20surrounding%20pixels%20within%20a%20patch%20into%20a%20transformed%0Aview%20of%20the%20central%20pixel.%20Additionally%2C%20to%20explore%20global%20band%20correlations%2C%0Awe%20divide%20pixels%20into%20continuous%20group-wise%20spectral%20embeddings%20and%20introduce%0AMamba%20to%20HTD%20for%20the%20first%20time%20to%20model%20long-range%20dependencies%20of%20the%0Aspectral%20sequence%20with%20linear%20complexity.%20Furthermore%2C%20to%20alleviate%20spectral%0Avariation%20and%20enhance%20robust%20representation%2C%20we%20propose%20a%20pyramid%20SSM%20as%20a%0Abackbone%20to%20capture%20and%20fuse%20multiresolution%20spectral-wise%20intrinsic%20features.%0AExtensive%20experiments%20conducted%20on%20four%20public%20datasets%20demonstrate%20that%20the%0Aproposed%20method%20outperforms%20state-of-the-art%20methods%20in%20both%20quantitative%20and%0Aqualitative%20evaluations.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/shendb2022/HTD-Mamba%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHTD-Mamba%253A%2520Efficient%2520Hyperspectral%2520Target%2520Detection%2520with%2520Pyramid%2520State%250A%2520%2520Space%2520Model%26entry.906535625%3DDunbin%2520Shen%2520and%2520Xuanbing%2520Zhu%2520and%2520Jiacheng%2520Tian%2520and%2520Jianjun%2520Liu%2520and%2520Zhenrong%2520Du%2520and%2520Hongyu%2520Wang%2520and%2520Xiaorui%2520Ma%26entry.1292438233%3D%2520%2520Hyperspectral%2520target%2520detection%2520%2528HTD%2529%2520identifies%2520objects%2520of%2520interest%2520from%250Acomplex%2520backgrounds%2520at%2520the%2520pixel%2520level%252C%2520playing%2520a%2520vital%2520role%2520in%2520Earth%250Aobservation.%2520However%252C%2520HTD%2520faces%2520challenges%2520due%2520to%2520limited%2520prior%2520knowledge%2520and%250Aspectral%2520variations%252C%2520leading%2520to%2520underfitting%2520models%2520and%2520unreliable%2520performance.%250ATo%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520an%2520efficient%2520self-supervised%250AHTD%2520method%2520with%2520a%2520pyramid%2520state%2520space%2520model%2520%2528SSM%2529%252C%2520named%2520HTD-Mamba%252C%2520which%250Aemploys%2520spectrally%2520contrastive%2520learning%2520to%2520distinguish%2520between%2520target%2520and%250Abackground%2520based%2520on%2520the%2520similarity%2520measurement%2520of%2520intrinsic%2520features.%250ASpecifically%252C%2520to%2520obtain%2520sufficient%2520training%2520samples%2520and%2520leverage%2520spatial%250Acontextual%2520information%252C%2520we%2520propose%2520a%2520spatial-encoded%2520spectral%2520augmentation%250Atechnique%2520that%2520encodes%2520all%2520surrounding%2520pixels%2520within%2520a%2520patch%2520into%2520a%2520transformed%250Aview%2520of%2520the%2520central%2520pixel.%2520Additionally%252C%2520to%2520explore%2520global%2520band%2520correlations%252C%250Awe%2520divide%2520pixels%2520into%2520continuous%2520group-wise%2520spectral%2520embeddings%2520and%2520introduce%250AMamba%2520to%2520HTD%2520for%2520the%2520first%2520time%2520to%2520model%2520long-range%2520dependencies%2520of%2520the%250Aspectral%2520sequence%2520with%2520linear%2520complexity.%2520Furthermore%252C%2520to%2520alleviate%2520spectral%250Avariation%2520and%2520enhance%2520robust%2520representation%252C%2520we%2520propose%2520a%2520pyramid%2520SSM%2520as%2520a%250Abackbone%2520to%2520capture%2520and%2520fuse%2520multiresolution%2520spectral-wise%2520intrinsic%2520features.%250AExtensive%2520experiments%2520conducted%2520on%2520four%2520public%2520datasets%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520quantitative%2520and%250Aqualitative%2520evaluations.%2520Code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/shendb2022/HTD-Mamba%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HTD-Mamba%3A%20Efficient%20Hyperspectral%20Target%20Detection%20with%20Pyramid%20State%0A%20%20Space%20Model&entry.906535625=Dunbin%20Shen%20and%20Xuanbing%20Zhu%20and%20Jiacheng%20Tian%20and%20Jianjun%20Liu%20and%20Zhenrong%20Du%20and%20Hongyu%20Wang%20and%20Xiaorui%20Ma&entry.1292438233=%20%20Hyperspectral%20target%20detection%20%28HTD%29%20identifies%20objects%20of%20interest%20from%0Acomplex%20backgrounds%20at%20the%20pixel%20level%2C%20playing%20a%20vital%20role%20in%20Earth%0Aobservation.%20However%2C%20HTD%20faces%20challenges%20due%20to%20limited%20prior%20knowledge%20and%0Aspectral%20variations%2C%20leading%20to%20underfitting%20models%20and%20unreliable%20performance.%0ATo%20address%20these%20challenges%2C%20this%20paper%20proposes%20an%20efficient%20self-supervised%0AHTD%20method%20with%20a%20pyramid%20state%20space%20model%20%28SSM%29%2C%20named%20HTD-Mamba%2C%20which%0Aemploys%20spectrally%20contrastive%20learning%20to%20distinguish%20between%20target%20and%0Abackground%20based%20on%20the%20similarity%20measurement%20of%20intrinsic%20features.%0ASpecifically%2C%20to%20obtain%20sufficient%20training%20samples%20and%20leverage%20spatial%0Acontextual%20information%2C%20we%20propose%20a%20spatial-encoded%20spectral%20augmentation%0Atechnique%20that%20encodes%20all%20surrounding%20pixels%20within%20a%20patch%20into%20a%20transformed%0Aview%20of%20the%20central%20pixel.%20Additionally%2C%20to%20explore%20global%20band%20correlations%2C%0Awe%20divide%20pixels%20into%20continuous%20group-wise%20spectral%20embeddings%20and%20introduce%0AMamba%20to%20HTD%20for%20the%20first%20time%20to%20model%20long-range%20dependencies%20of%20the%0Aspectral%20sequence%20with%20linear%20complexity.%20Furthermore%2C%20to%20alleviate%20spectral%0Avariation%20and%20enhance%20robust%20representation%2C%20we%20propose%20a%20pyramid%20SSM%20as%20a%0Abackbone%20to%20capture%20and%20fuse%20multiresolution%20spectral-wise%20intrinsic%20features.%0AExtensive%20experiments%20conducted%20on%20four%20public%20datasets%20demonstrate%20that%20the%0Aproposed%20method%20outperforms%20state-of-the-art%20methods%20in%20both%20quantitative%20and%0Aqualitative%20evaluations.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/shendb2022/HTD-Mamba%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06841v1&entry.124074799=Read"},
{"title": "Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline", "author": "Yangguang Li and Bin Huang and Zeren Chen and Yufeng Cui and Feng Liang and Mingzhu Shen and Fenggang Liu and Enze Xie and Lu Sheng and Wanli Ouyang and Jing Shao", "abstract": "  Recently, perception task based on Bird's-Eye View (BEV) representation has\ndrawn more and more attention, and BEV representation is promising as the\nfoundation for next-generation Autonomous Vehicle (AV) perception. However,\nmost existing BEV solutions either require considerable resources to execute\non-vehicle inference or suffer from modest performance. This paper proposes a\nsimple yet effective framework, termed Fast-BEV , which is capable of\nperforming faster BEV perception on the on-vehicle chips. Towards this goal, we\nfirst empirically find that the BEV representation can be sufficiently powerful\nwithout expensive transformer based transformation nor depth representation.\nOur Fast-BEV consists of five parts, We novelly propose (1) a lightweight\ndeployment-friendly view transformation which fast transfers 2D image feature\nto 3D voxel space, (2) an multi-scale image encoder which leverages multi-scale\ninformation for better performance, (3) an efficient BEV encoder which is\nparticularly designed to speed up on-vehicle inference. We further introduce\n(4) a strong data augmentation strategy for both image and BEV space to avoid\nover-fitting, (5) a multi-frame feature fusion mechanism to leverage the\ntemporal information. Through experiments, on 2080Ti platform, our R50 model\ncan run 52.6 FPS with 47.3% NDS on the nuScenes validation set, exceeding the\n41.3 FPS and 47.5% NDS of the BEVDepth-R50 model and 30.2 FPS and 45.7% NDS of\nthe BEVDet4D-R50 model. Our largest model (R101@900x1600) establishes a\ncompetitive 53.5% NDS on the nuScenes validation set. We further develop a\nbenchmark with considerable accuracy and efficiency on current popular\non-vehicle chips. The code is released at:\nhttps://github.com/Sense-GVT/Fast-BEV.\n", "link": "http://arxiv.org/abs/2301.12511v2", "date": "2024-07-09", "relevancy": 2.155, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5455}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5374}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast-BEV%3A%20A%20Fast%20and%20Strong%20Bird%27s-Eye%20View%20Perception%20Baseline&body=Title%3A%20Fast-BEV%3A%20A%20Fast%20and%20Strong%20Bird%27s-Eye%20View%20Perception%20Baseline%0AAuthor%3A%20Yangguang%20Li%20and%20Bin%20Huang%20and%20Zeren%20Chen%20and%20Yufeng%20Cui%20and%20Feng%20Liang%20and%20Mingzhu%20Shen%20and%20Fenggang%20Liu%20and%20Enze%20Xie%20and%20Lu%20Sheng%20and%20Wanli%20Ouyang%20and%20Jing%20Shao%0AAbstract%3A%20%20%20Recently%2C%20perception%20task%20based%20on%20Bird%27s-Eye%20View%20%28BEV%29%20representation%20has%0Adrawn%20more%20and%20more%20attention%2C%20and%20BEV%20representation%20is%20promising%20as%20the%0Afoundation%20for%20next-generation%20Autonomous%20Vehicle%20%28AV%29%20perception.%20However%2C%0Amost%20existing%20BEV%20solutions%20either%20require%20considerable%20resources%20to%20execute%0Aon-vehicle%20inference%20or%20suffer%20from%20modest%20performance.%20This%20paper%20proposes%20a%0Asimple%20yet%20effective%20framework%2C%20termed%20Fast-BEV%20%2C%20which%20is%20capable%20of%0Aperforming%20faster%20BEV%20perception%20on%20the%20on-vehicle%20chips.%20Towards%20this%20goal%2C%20we%0Afirst%20empirically%20find%20that%20the%20BEV%20representation%20can%20be%20sufficiently%20powerful%0Awithout%20expensive%20transformer%20based%20transformation%20nor%20depth%20representation.%0AOur%20Fast-BEV%20consists%20of%20five%20parts%2C%20We%20novelly%20propose%20%281%29%20a%20lightweight%0Adeployment-friendly%20view%20transformation%20which%20fast%20transfers%202D%20image%20feature%0Ato%203D%20voxel%20space%2C%20%282%29%20an%20multi-scale%20image%20encoder%20which%20leverages%20multi-scale%0Ainformation%20for%20better%20performance%2C%20%283%29%20an%20efficient%20BEV%20encoder%20which%20is%0Aparticularly%20designed%20to%20speed%20up%20on-vehicle%20inference.%20We%20further%20introduce%0A%284%29%20a%20strong%20data%20augmentation%20strategy%20for%20both%20image%20and%20BEV%20space%20to%20avoid%0Aover-fitting%2C%20%285%29%20a%20multi-frame%20feature%20fusion%20mechanism%20to%20leverage%20the%0Atemporal%20information.%20Through%20experiments%2C%20on%202080Ti%20platform%2C%20our%20R50%20model%0Acan%20run%2052.6%20FPS%20with%2047.3%25%20NDS%20on%20the%20nuScenes%20validation%20set%2C%20exceeding%20the%0A41.3%20FPS%20and%2047.5%25%20NDS%20of%20the%20BEVDepth-R50%20model%20and%2030.2%20FPS%20and%2045.7%25%20NDS%20of%0Athe%20BEVDet4D-R50%20model.%20Our%20largest%20model%20%28R101%40900x1600%29%20establishes%20a%0Acompetitive%2053.5%25%20NDS%20on%20the%20nuScenes%20validation%20set.%20We%20further%20develop%20a%0Abenchmark%20with%20considerable%20accuracy%20and%20efficiency%20on%20current%20popular%0Aon-vehicle%20chips.%20The%20code%20is%20released%20at%3A%0Ahttps%3A//github.com/Sense-GVT/Fast-BEV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.12511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast-BEV%253A%2520A%2520Fast%2520and%2520Strong%2520Bird%2527s-Eye%2520View%2520Perception%2520Baseline%26entry.906535625%3DYangguang%2520Li%2520and%2520Bin%2520Huang%2520and%2520Zeren%2520Chen%2520and%2520Yufeng%2520Cui%2520and%2520Feng%2520Liang%2520and%2520Mingzhu%2520Shen%2520and%2520Fenggang%2520Liu%2520and%2520Enze%2520Xie%2520and%2520Lu%2520Sheng%2520and%2520Wanli%2520Ouyang%2520and%2520Jing%2520Shao%26entry.1292438233%3D%2520%2520Recently%252C%2520perception%2520task%2520based%2520on%2520Bird%2527s-Eye%2520View%2520%2528BEV%2529%2520representation%2520has%250Adrawn%2520more%2520and%2520more%2520attention%252C%2520and%2520BEV%2520representation%2520is%2520promising%2520as%2520the%250Afoundation%2520for%2520next-generation%2520Autonomous%2520Vehicle%2520%2528AV%2529%2520perception.%2520However%252C%250Amost%2520existing%2520BEV%2520solutions%2520either%2520require%2520considerable%2520resources%2520to%2520execute%250Aon-vehicle%2520inference%2520or%2520suffer%2520from%2520modest%2520performance.%2520This%2520paper%2520proposes%2520a%250Asimple%2520yet%2520effective%2520framework%252C%2520termed%2520Fast-BEV%2520%252C%2520which%2520is%2520capable%2520of%250Aperforming%2520faster%2520BEV%2520perception%2520on%2520the%2520on-vehicle%2520chips.%2520Towards%2520this%2520goal%252C%2520we%250Afirst%2520empirically%2520find%2520that%2520the%2520BEV%2520representation%2520can%2520be%2520sufficiently%2520powerful%250Awithout%2520expensive%2520transformer%2520based%2520transformation%2520nor%2520depth%2520representation.%250AOur%2520Fast-BEV%2520consists%2520of%2520five%2520parts%252C%2520We%2520novelly%2520propose%2520%25281%2529%2520a%2520lightweight%250Adeployment-friendly%2520view%2520transformation%2520which%2520fast%2520transfers%25202D%2520image%2520feature%250Ato%25203D%2520voxel%2520space%252C%2520%25282%2529%2520an%2520multi-scale%2520image%2520encoder%2520which%2520leverages%2520multi-scale%250Ainformation%2520for%2520better%2520performance%252C%2520%25283%2529%2520an%2520efficient%2520BEV%2520encoder%2520which%2520is%250Aparticularly%2520designed%2520to%2520speed%2520up%2520on-vehicle%2520inference.%2520We%2520further%2520introduce%250A%25284%2529%2520a%2520strong%2520data%2520augmentation%2520strategy%2520for%2520both%2520image%2520and%2520BEV%2520space%2520to%2520avoid%250Aover-fitting%252C%2520%25285%2529%2520a%2520multi-frame%2520feature%2520fusion%2520mechanism%2520to%2520leverage%2520the%250Atemporal%2520information.%2520Through%2520experiments%252C%2520on%25202080Ti%2520platform%252C%2520our%2520R50%2520model%250Acan%2520run%252052.6%2520FPS%2520with%252047.3%2525%2520NDS%2520on%2520the%2520nuScenes%2520validation%2520set%252C%2520exceeding%2520the%250A41.3%2520FPS%2520and%252047.5%2525%2520NDS%2520of%2520the%2520BEVDepth-R50%2520model%2520and%252030.2%2520FPS%2520and%252045.7%2525%2520NDS%2520of%250Athe%2520BEVDet4D-R50%2520model.%2520Our%2520largest%2520model%2520%2528R101%2540900x1600%2529%2520establishes%2520a%250Acompetitive%252053.5%2525%2520NDS%2520on%2520the%2520nuScenes%2520validation%2520set.%2520We%2520further%2520develop%2520a%250Abenchmark%2520with%2520considerable%2520accuracy%2520and%2520efficiency%2520on%2520current%2520popular%250Aon-vehicle%2520chips.%2520The%2520code%2520is%2520released%2520at%253A%250Ahttps%253A//github.com/Sense-GVT/Fast-BEV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.12511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast-BEV%3A%20A%20Fast%20and%20Strong%20Bird%27s-Eye%20View%20Perception%20Baseline&entry.906535625=Yangguang%20Li%20and%20Bin%20Huang%20and%20Zeren%20Chen%20and%20Yufeng%20Cui%20and%20Feng%20Liang%20and%20Mingzhu%20Shen%20and%20Fenggang%20Liu%20and%20Enze%20Xie%20and%20Lu%20Sheng%20and%20Wanli%20Ouyang%20and%20Jing%20Shao&entry.1292438233=%20%20Recently%2C%20perception%20task%20based%20on%20Bird%27s-Eye%20View%20%28BEV%29%20representation%20has%0Adrawn%20more%20and%20more%20attention%2C%20and%20BEV%20representation%20is%20promising%20as%20the%0Afoundation%20for%20next-generation%20Autonomous%20Vehicle%20%28AV%29%20perception.%20However%2C%0Amost%20existing%20BEV%20solutions%20either%20require%20considerable%20resources%20to%20execute%0Aon-vehicle%20inference%20or%20suffer%20from%20modest%20performance.%20This%20paper%20proposes%20a%0Asimple%20yet%20effective%20framework%2C%20termed%20Fast-BEV%20%2C%20which%20is%20capable%20of%0Aperforming%20faster%20BEV%20perception%20on%20the%20on-vehicle%20chips.%20Towards%20this%20goal%2C%20we%0Afirst%20empirically%20find%20that%20the%20BEV%20representation%20can%20be%20sufficiently%20powerful%0Awithout%20expensive%20transformer%20based%20transformation%20nor%20depth%20representation.%0AOur%20Fast-BEV%20consists%20of%20five%20parts%2C%20We%20novelly%20propose%20%281%29%20a%20lightweight%0Adeployment-friendly%20view%20transformation%20which%20fast%20transfers%202D%20image%20feature%0Ato%203D%20voxel%20space%2C%20%282%29%20an%20multi-scale%20image%20encoder%20which%20leverages%20multi-scale%0Ainformation%20for%20better%20performance%2C%20%283%29%20an%20efficient%20BEV%20encoder%20which%20is%0Aparticularly%20designed%20to%20speed%20up%20on-vehicle%20inference.%20We%20further%20introduce%0A%284%29%20a%20strong%20data%20augmentation%20strategy%20for%20both%20image%20and%20BEV%20space%20to%20avoid%0Aover-fitting%2C%20%285%29%20a%20multi-frame%20feature%20fusion%20mechanism%20to%20leverage%20the%0Atemporal%20information.%20Through%20experiments%2C%20on%202080Ti%20platform%2C%20our%20R50%20model%0Acan%20run%2052.6%20FPS%20with%2047.3%25%20NDS%20on%20the%20nuScenes%20validation%20set%2C%20exceeding%20the%0A41.3%20FPS%20and%2047.5%25%20NDS%20of%20the%20BEVDepth-R50%20model%20and%2030.2%20FPS%20and%2045.7%25%20NDS%20of%0Athe%20BEVDet4D-R50%20model.%20Our%20largest%20model%20%28R101%40900x1600%29%20establishes%20a%0Acompetitive%2053.5%25%20NDS%20on%20the%20nuScenes%20validation%20set.%20We%20further%20develop%20a%0Abenchmark%20with%20considerable%20accuracy%20and%20efficiency%20on%20current%20popular%0Aon-vehicle%20chips.%20The%20code%20is%20released%20at%3A%0Ahttps%3A//github.com/Sense-GVT/Fast-BEV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.12511v2&entry.124074799=Read"},
{"title": "Intercepting Unauthorized Aerial Robots in Controlled Airspace Using\n  Reinforcement Learning", "author": "Francisco Giral and Ignacio G\u00f3mez and Soledad Le Clainche", "abstract": "  The proliferation of unmanned aerial vehicles (UAVs) in controlled airspace\npresents significant risks, including potential collisions, disruptions to air\ntraffic, and security threats. Ensuring the safe and efficient operation of\nairspace, particularly in urban environments and near critical infrastructure,\nnecessitates effective methods to intercept unauthorized or non-cooperative\nUAVs. This work addresses the critical need for robust, adaptive systems\ncapable of managing such threats through the use of Reinforcement Learning\n(RL). We present a novel approach utilizing RL to train fixed-wing UAV pursuer\nagents for intercepting dynamic evader targets. Our methodology explores both\nmodel-based and model-free RL algorithms, specifically DreamerV3, Truncated\nQuantile Critics (TQC), and Soft Actor-Critic (SAC). The training and\nevaluation of these algorithms were conducted under diverse scenarios,\nincluding unseen evasion strategies and environmental perturbations. Our\napproach leverages high-fidelity flight dynamics simulations to create\nrealistic training environments. This research underscores the importance of\ndeveloping intelligent, adaptive control systems for UAV interception,\nsignificantly contributing to the advancement of secure and efficient airspace\nmanagement. It demonstrates the potential of RL to train systems capable of\nautonomously achieving these critical tasks.\n", "link": "http://arxiv.org/abs/2407.06909v1", "date": "2024-07-09", "relevancy": 2.153, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5332}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intercepting%20Unauthorized%20Aerial%20Robots%20in%20Controlled%20Airspace%20Using%0A%20%20Reinforcement%20Learning&body=Title%3A%20Intercepting%20Unauthorized%20Aerial%20Robots%20in%20Controlled%20Airspace%20Using%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Francisco%20Giral%20and%20Ignacio%20G%C3%B3mez%20and%20Soledad%20Le%20Clainche%0AAbstract%3A%20%20%20The%20proliferation%20of%20unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20controlled%20airspace%0Apresents%20significant%20risks%2C%20including%20potential%20collisions%2C%20disruptions%20to%20air%0Atraffic%2C%20and%20security%20threats.%20Ensuring%20the%20safe%20and%20efficient%20operation%20of%0Aairspace%2C%20particularly%20in%20urban%20environments%20and%20near%20critical%20infrastructure%2C%0Anecessitates%20effective%20methods%20to%20intercept%20unauthorized%20or%20non-cooperative%0AUAVs.%20This%20work%20addresses%20the%20critical%20need%20for%20robust%2C%20adaptive%20systems%0Acapable%20of%20managing%20such%20threats%20through%20the%20use%20of%20Reinforcement%20Learning%0A%28RL%29.%20We%20present%20a%20novel%20approach%20utilizing%20RL%20to%20train%20fixed-wing%20UAV%20pursuer%0Aagents%20for%20intercepting%20dynamic%20evader%20targets.%20Our%20methodology%20explores%20both%0Amodel-based%20and%20model-free%20RL%20algorithms%2C%20specifically%20DreamerV3%2C%20Truncated%0AQuantile%20Critics%20%28TQC%29%2C%20and%20Soft%20Actor-Critic%20%28SAC%29.%20The%20training%20and%0Aevaluation%20of%20these%20algorithms%20were%20conducted%20under%20diverse%20scenarios%2C%0Aincluding%20unseen%20evasion%20strategies%20and%20environmental%20perturbations.%20Our%0Aapproach%20leverages%20high-fidelity%20flight%20dynamics%20simulations%20to%20create%0Arealistic%20training%20environments.%20This%20research%20underscores%20the%20importance%20of%0Adeveloping%20intelligent%2C%20adaptive%20control%20systems%20for%20UAV%20interception%2C%0Asignificantly%20contributing%20to%20the%20advancement%20of%20secure%20and%20efficient%20airspace%0Amanagement.%20It%20demonstrates%20the%20potential%20of%20RL%20to%20train%20systems%20capable%20of%0Aautonomously%20achieving%20these%20critical%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntercepting%2520Unauthorized%2520Aerial%2520Robots%2520in%2520Controlled%2520Airspace%2520Using%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DFrancisco%2520Giral%2520and%2520Ignacio%2520G%25C3%25B3mez%2520and%2520Soledad%2520Le%2520Clainche%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520in%2520controlled%2520airspace%250Apresents%2520significant%2520risks%252C%2520including%2520potential%2520collisions%252C%2520disruptions%2520to%2520air%250Atraffic%252C%2520and%2520security%2520threats.%2520Ensuring%2520the%2520safe%2520and%2520efficient%2520operation%2520of%250Aairspace%252C%2520particularly%2520in%2520urban%2520environments%2520and%2520near%2520critical%2520infrastructure%252C%250Anecessitates%2520effective%2520methods%2520to%2520intercept%2520unauthorized%2520or%2520non-cooperative%250AUAVs.%2520This%2520work%2520addresses%2520the%2520critical%2520need%2520for%2520robust%252C%2520adaptive%2520systems%250Acapable%2520of%2520managing%2520such%2520threats%2520through%2520the%2520use%2520of%2520Reinforcement%2520Learning%250A%2528RL%2529.%2520We%2520present%2520a%2520novel%2520approach%2520utilizing%2520RL%2520to%2520train%2520fixed-wing%2520UAV%2520pursuer%250Aagents%2520for%2520intercepting%2520dynamic%2520evader%2520targets.%2520Our%2520methodology%2520explores%2520both%250Amodel-based%2520and%2520model-free%2520RL%2520algorithms%252C%2520specifically%2520DreamerV3%252C%2520Truncated%250AQuantile%2520Critics%2520%2528TQC%2529%252C%2520and%2520Soft%2520Actor-Critic%2520%2528SAC%2529.%2520The%2520training%2520and%250Aevaluation%2520of%2520these%2520algorithms%2520were%2520conducted%2520under%2520diverse%2520scenarios%252C%250Aincluding%2520unseen%2520evasion%2520strategies%2520and%2520environmental%2520perturbations.%2520Our%250Aapproach%2520leverages%2520high-fidelity%2520flight%2520dynamics%2520simulations%2520to%2520create%250Arealistic%2520training%2520environments.%2520This%2520research%2520underscores%2520the%2520importance%2520of%250Adeveloping%2520intelligent%252C%2520adaptive%2520control%2520systems%2520for%2520UAV%2520interception%252C%250Asignificantly%2520contributing%2520to%2520the%2520advancement%2520of%2520secure%2520and%2520efficient%2520airspace%250Amanagement.%2520It%2520demonstrates%2520the%2520potential%2520of%2520RL%2520to%2520train%2520systems%2520capable%2520of%250Aautonomously%2520achieving%2520these%2520critical%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intercepting%20Unauthorized%20Aerial%20Robots%20in%20Controlled%20Airspace%20Using%0A%20%20Reinforcement%20Learning&entry.906535625=Francisco%20Giral%20and%20Ignacio%20G%C3%B3mez%20and%20Soledad%20Le%20Clainche&entry.1292438233=%20%20The%20proliferation%20of%20unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20controlled%20airspace%0Apresents%20significant%20risks%2C%20including%20potential%20collisions%2C%20disruptions%20to%20air%0Atraffic%2C%20and%20security%20threats.%20Ensuring%20the%20safe%20and%20efficient%20operation%20of%0Aairspace%2C%20particularly%20in%20urban%20environments%20and%20near%20critical%20infrastructure%2C%0Anecessitates%20effective%20methods%20to%20intercept%20unauthorized%20or%20non-cooperative%0AUAVs.%20This%20work%20addresses%20the%20critical%20need%20for%20robust%2C%20adaptive%20systems%0Acapable%20of%20managing%20such%20threats%20through%20the%20use%20of%20Reinforcement%20Learning%0A%28RL%29.%20We%20present%20a%20novel%20approach%20utilizing%20RL%20to%20train%20fixed-wing%20UAV%20pursuer%0Aagents%20for%20intercepting%20dynamic%20evader%20targets.%20Our%20methodology%20explores%20both%0Amodel-based%20and%20model-free%20RL%20algorithms%2C%20specifically%20DreamerV3%2C%20Truncated%0AQuantile%20Critics%20%28TQC%29%2C%20and%20Soft%20Actor-Critic%20%28SAC%29.%20The%20training%20and%0Aevaluation%20of%20these%20algorithms%20were%20conducted%20under%20diverse%20scenarios%2C%0Aincluding%20unseen%20evasion%20strategies%20and%20environmental%20perturbations.%20Our%0Aapproach%20leverages%20high-fidelity%20flight%20dynamics%20simulations%20to%20create%0Arealistic%20training%20environments.%20This%20research%20underscores%20the%20importance%20of%0Adeveloping%20intelligent%2C%20adaptive%20control%20systems%20for%20UAV%20interception%2C%0Asignificantly%20contributing%20to%20the%20advancement%20of%20secure%20and%20efficient%20airspace%0Amanagement.%20It%20demonstrates%20the%20potential%20of%20RL%20to%20train%20systems%20capable%20of%0Aautonomously%20achieving%20these%20critical%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06909v1&entry.124074799=Read"},
{"title": "Convergence of the Chambolle-Pock Algorithm in the Absence of\n  Monotonicity", "author": "Brecht Evens and Puya Latafat and Panagiotis Patrinos", "abstract": "  The Chambolle-Pock algorithm (CPA), also known as the primal-dual hybrid\ngradient method, has gained popularity over the last decade due to its success\nin solving large-scale convex structured problems. This work extends its\nconvergence analysis for problems with varying degrees of (non)monotonicity,\nquantified through a so-called oblique weak Minty condition on the associated\nprimal-dual operator. Our results reveal novel stepsize and relaxation\nparameter ranges which do not only depend on the norm of the linear mapping,\nbut also on its other singular values. In particular, in nonmonotone settings,\nin addition to the classical stepsize conditions, extra bounds on the stepsizes\nand relaxation parameters are required. On the other hand, in the strongly\nmonotone setting, the relaxation parameter is allowed to exceed the classical\nupper bound of two. Moreover, we build upon the recently introduced class of\nsemimonotone operators, providing sufficient convergence conditions for CPA\nwhen the individual operators are semimonotone. Since this class of operators\nencompasses traditional operator classes including (hypo)- and\nco(hypo)-monotone operators, this analysis recovers and extends existing\nresults for CPA. Tightness of the proposed stepsize ranges is demonstrated\nthrough several examples.\n", "link": "http://arxiv.org/abs/2312.06540v2", "date": "2024-07-09", "relevancy": 2.1526, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4411}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4372}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20of%20the%20Chambolle-Pock%20Algorithm%20in%20the%20Absence%20of%0A%20%20Monotonicity&body=Title%3A%20Convergence%20of%20the%20Chambolle-Pock%20Algorithm%20in%20the%20Absence%20of%0A%20%20Monotonicity%0AAuthor%3A%20Brecht%20Evens%20and%20Puya%20Latafat%20and%20Panagiotis%20Patrinos%0AAbstract%3A%20%20%20The%20Chambolle-Pock%20algorithm%20%28CPA%29%2C%20also%20known%20as%20the%20primal-dual%20hybrid%0Agradient%20method%2C%20has%20gained%20popularity%20over%20the%20last%20decade%20due%20to%20its%20success%0Ain%20solving%20large-scale%20convex%20structured%20problems.%20This%20work%20extends%20its%0Aconvergence%20analysis%20for%20problems%20with%20varying%20degrees%20of%20%28non%29monotonicity%2C%0Aquantified%20through%20a%20so-called%20oblique%20weak%20Minty%20condition%20on%20the%20associated%0Aprimal-dual%20operator.%20Our%20results%20reveal%20novel%20stepsize%20and%20relaxation%0Aparameter%20ranges%20which%20do%20not%20only%20depend%20on%20the%20norm%20of%20the%20linear%20mapping%2C%0Abut%20also%20on%20its%20other%20singular%20values.%20In%20particular%2C%20in%20nonmonotone%20settings%2C%0Ain%20addition%20to%20the%20classical%20stepsize%20conditions%2C%20extra%20bounds%20on%20the%20stepsizes%0Aand%20relaxation%20parameters%20are%20required.%20On%20the%20other%20hand%2C%20in%20the%20strongly%0Amonotone%20setting%2C%20the%20relaxation%20parameter%20is%20allowed%20to%20exceed%20the%20classical%0Aupper%20bound%20of%20two.%20Moreover%2C%20we%20build%20upon%20the%20recently%20introduced%20class%20of%0Asemimonotone%20operators%2C%20providing%20sufficient%20convergence%20conditions%20for%20CPA%0Awhen%20the%20individual%20operators%20are%20semimonotone.%20Since%20this%20class%20of%20operators%0Aencompasses%20traditional%20operator%20classes%20including%20%28hypo%29-%20and%0Aco%28hypo%29-monotone%20operators%2C%20this%20analysis%20recovers%20and%20extends%20existing%0Aresults%20for%20CPA.%20Tightness%20of%20the%20proposed%20stepsize%20ranges%20is%20demonstrated%0Athrough%20several%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06540v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520of%2520the%2520Chambolle-Pock%2520Algorithm%2520in%2520the%2520Absence%2520of%250A%2520%2520Monotonicity%26entry.906535625%3DBrecht%2520Evens%2520and%2520Puya%2520Latafat%2520and%2520Panagiotis%2520Patrinos%26entry.1292438233%3D%2520%2520The%2520Chambolle-Pock%2520algorithm%2520%2528CPA%2529%252C%2520also%2520known%2520as%2520the%2520primal-dual%2520hybrid%250Agradient%2520method%252C%2520has%2520gained%2520popularity%2520over%2520the%2520last%2520decade%2520due%2520to%2520its%2520success%250Ain%2520solving%2520large-scale%2520convex%2520structured%2520problems.%2520This%2520work%2520extends%2520its%250Aconvergence%2520analysis%2520for%2520problems%2520with%2520varying%2520degrees%2520of%2520%2528non%2529monotonicity%252C%250Aquantified%2520through%2520a%2520so-called%2520oblique%2520weak%2520Minty%2520condition%2520on%2520the%2520associated%250Aprimal-dual%2520operator.%2520Our%2520results%2520reveal%2520novel%2520stepsize%2520and%2520relaxation%250Aparameter%2520ranges%2520which%2520do%2520not%2520only%2520depend%2520on%2520the%2520norm%2520of%2520the%2520linear%2520mapping%252C%250Abut%2520also%2520on%2520its%2520other%2520singular%2520values.%2520In%2520particular%252C%2520in%2520nonmonotone%2520settings%252C%250Ain%2520addition%2520to%2520the%2520classical%2520stepsize%2520conditions%252C%2520extra%2520bounds%2520on%2520the%2520stepsizes%250Aand%2520relaxation%2520parameters%2520are%2520required.%2520On%2520the%2520other%2520hand%252C%2520in%2520the%2520strongly%250Amonotone%2520setting%252C%2520the%2520relaxation%2520parameter%2520is%2520allowed%2520to%2520exceed%2520the%2520classical%250Aupper%2520bound%2520of%2520two.%2520Moreover%252C%2520we%2520build%2520upon%2520the%2520recently%2520introduced%2520class%2520of%250Asemimonotone%2520operators%252C%2520providing%2520sufficient%2520convergence%2520conditions%2520for%2520CPA%250Awhen%2520the%2520individual%2520operators%2520are%2520semimonotone.%2520Since%2520this%2520class%2520of%2520operators%250Aencompasses%2520traditional%2520operator%2520classes%2520including%2520%2528hypo%2529-%2520and%250Aco%2528hypo%2529-monotone%2520operators%252C%2520this%2520analysis%2520recovers%2520and%2520extends%2520existing%250Aresults%2520for%2520CPA.%2520Tightness%2520of%2520the%2520proposed%2520stepsize%2520ranges%2520is%2520demonstrated%250Athrough%2520several%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06540v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20of%20the%20Chambolle-Pock%20Algorithm%20in%20the%20Absence%20of%0A%20%20Monotonicity&entry.906535625=Brecht%20Evens%20and%20Puya%20Latafat%20and%20Panagiotis%20Patrinos&entry.1292438233=%20%20The%20Chambolle-Pock%20algorithm%20%28CPA%29%2C%20also%20known%20as%20the%20primal-dual%20hybrid%0Agradient%20method%2C%20has%20gained%20popularity%20over%20the%20last%20decade%20due%20to%20its%20success%0Ain%20solving%20large-scale%20convex%20structured%20problems.%20This%20work%20extends%20its%0Aconvergence%20analysis%20for%20problems%20with%20varying%20degrees%20of%20%28non%29monotonicity%2C%0Aquantified%20through%20a%20so-called%20oblique%20weak%20Minty%20condition%20on%20the%20associated%0Aprimal-dual%20operator.%20Our%20results%20reveal%20novel%20stepsize%20and%20relaxation%0Aparameter%20ranges%20which%20do%20not%20only%20depend%20on%20the%20norm%20of%20the%20linear%20mapping%2C%0Abut%20also%20on%20its%20other%20singular%20values.%20In%20particular%2C%20in%20nonmonotone%20settings%2C%0Ain%20addition%20to%20the%20classical%20stepsize%20conditions%2C%20extra%20bounds%20on%20the%20stepsizes%0Aand%20relaxation%20parameters%20are%20required.%20On%20the%20other%20hand%2C%20in%20the%20strongly%0Amonotone%20setting%2C%20the%20relaxation%20parameter%20is%20allowed%20to%20exceed%20the%20classical%0Aupper%20bound%20of%20two.%20Moreover%2C%20we%20build%20upon%20the%20recently%20introduced%20class%20of%0Asemimonotone%20operators%2C%20providing%20sufficient%20convergence%20conditions%20for%20CPA%0Awhen%20the%20individual%20operators%20are%20semimonotone.%20Since%20this%20class%20of%20operators%0Aencompasses%20traditional%20operator%20classes%20including%20%28hypo%29-%20and%0Aco%28hypo%29-monotone%20operators%2C%20this%20analysis%20recovers%20and%20extends%20existing%0Aresults%20for%20CPA.%20Tightness%20of%20the%20proposed%20stepsize%20ranges%20is%20demonstrated%0Athrough%20several%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06540v2&entry.124074799=Read"},
{"title": "Parameter-Efficient and Memory-Efficient Tuning for Vision Transformer:\n  A Disentangled Approach", "author": "Taolin Zhang and Jiawang Bai and Zhihe Lu and Dongze Lian and Genping Wang and Xinchao Wang and Shu-Tao Xia", "abstract": "  Recent works on parameter-efficient transfer learning (PETL) show the\npotential to adapt a pre-trained Vision Transformer to downstream recognition\ntasks with only a few learnable parameters. However, since they usually insert\nnew structures into the pre-trained model, entire intermediate features of that\nmodel are changed and thus need to be stored to be involved in\nback-propagation, resulting in memory-heavy training. We solve this problem\nfrom a novel disentangled perspective, i.e., dividing PETL into two aspects:\ntask-specific learning and pre-trained knowledge utilization. Specifically, we\nsynthesize the task-specific query with a learnable and lightweight module,\nwhich is independent of the pre-trained model. The synthesized query equipped\nwith task-specific knowledge serves to extract the useful features for\ndownstream tasks from the intermediate representations of the pre-trained model\nin a query-only manner. Built upon these features, a customized classification\nhead is proposed to make the prediction for the input sample. lightweight\narchitecture and avoids the use of heavy intermediate features for running\ngradient descent, it demonstrates limited memory usage in training. Extensive\nexperiments manifest that our method achieves state-of-the-art performance\nunder memory constraints, showcasing its applicability in real-world\nsituations.\n", "link": "http://arxiv.org/abs/2407.06964v1", "date": "2024-07-09", "relevancy": 2.1461, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5457}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5341}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Efficient%20and%20Memory-Efficient%20Tuning%20for%20Vision%20Transformer%3A%0A%20%20A%20Disentangled%20Approach&body=Title%3A%20Parameter-Efficient%20and%20Memory-Efficient%20Tuning%20for%20Vision%20Transformer%3A%0A%20%20A%20Disentangled%20Approach%0AAuthor%3A%20Taolin%20Zhang%20and%20Jiawang%20Bai%20and%20Zhihe%20Lu%20and%20Dongze%20Lian%20and%20Genping%20Wang%20and%20Xinchao%20Wang%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20Recent%20works%20on%20parameter-efficient%20transfer%20learning%20%28PETL%29%20show%20the%0Apotential%20to%20adapt%20a%20pre-trained%20Vision%20Transformer%20to%20downstream%20recognition%0Atasks%20with%20only%20a%20few%20learnable%20parameters.%20However%2C%20since%20they%20usually%20insert%0Anew%20structures%20into%20the%20pre-trained%20model%2C%20entire%20intermediate%20features%20of%20that%0Amodel%20are%20changed%20and%20thus%20need%20to%20be%20stored%20to%20be%20involved%20in%0Aback-propagation%2C%20resulting%20in%20memory-heavy%20training.%20We%20solve%20this%20problem%0Afrom%20a%20novel%20disentangled%20perspective%2C%20i.e.%2C%20dividing%20PETL%20into%20two%20aspects%3A%0Atask-specific%20learning%20and%20pre-trained%20knowledge%20utilization.%20Specifically%2C%20we%0Asynthesize%20the%20task-specific%20query%20with%20a%20learnable%20and%20lightweight%20module%2C%0Awhich%20is%20independent%20of%20the%20pre-trained%20model.%20The%20synthesized%20query%20equipped%0Awith%20task-specific%20knowledge%20serves%20to%20extract%20the%20useful%20features%20for%0Adownstream%20tasks%20from%20the%20intermediate%20representations%20of%20the%20pre-trained%20model%0Ain%20a%20query-only%20manner.%20Built%20upon%20these%20features%2C%20a%20customized%20classification%0Ahead%20is%20proposed%20to%20make%20the%20prediction%20for%20the%20input%20sample.%20lightweight%0Aarchitecture%20and%20avoids%20the%20use%20of%20heavy%20intermediate%20features%20for%20running%0Agradient%20descent%2C%20it%20demonstrates%20limited%20memory%20usage%20in%20training.%20Extensive%0Aexperiments%20manifest%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Aunder%20memory%20constraints%2C%20showcasing%20its%20applicability%20in%20real-world%0Asituations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Efficient%2520and%2520Memory-Efficient%2520Tuning%2520for%2520Vision%2520Transformer%253A%250A%2520%2520A%2520Disentangled%2520Approach%26entry.906535625%3DTaolin%2520Zhang%2520and%2520Jiawang%2520Bai%2520and%2520Zhihe%2520Lu%2520and%2520Dongze%2520Lian%2520and%2520Genping%2520Wang%2520and%2520Xinchao%2520Wang%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520Recent%2520works%2520on%2520parameter-efficient%2520transfer%2520learning%2520%2528PETL%2529%2520show%2520the%250Apotential%2520to%2520adapt%2520a%2520pre-trained%2520Vision%2520Transformer%2520to%2520downstream%2520recognition%250Atasks%2520with%2520only%2520a%2520few%2520learnable%2520parameters.%2520However%252C%2520since%2520they%2520usually%2520insert%250Anew%2520structures%2520into%2520the%2520pre-trained%2520model%252C%2520entire%2520intermediate%2520features%2520of%2520that%250Amodel%2520are%2520changed%2520and%2520thus%2520need%2520to%2520be%2520stored%2520to%2520be%2520involved%2520in%250Aback-propagation%252C%2520resulting%2520in%2520memory-heavy%2520training.%2520We%2520solve%2520this%2520problem%250Afrom%2520a%2520novel%2520disentangled%2520perspective%252C%2520i.e.%252C%2520dividing%2520PETL%2520into%2520two%2520aspects%253A%250Atask-specific%2520learning%2520and%2520pre-trained%2520knowledge%2520utilization.%2520Specifically%252C%2520we%250Asynthesize%2520the%2520task-specific%2520query%2520with%2520a%2520learnable%2520and%2520lightweight%2520module%252C%250Awhich%2520is%2520independent%2520of%2520the%2520pre-trained%2520model.%2520The%2520synthesized%2520query%2520equipped%250Awith%2520task-specific%2520knowledge%2520serves%2520to%2520extract%2520the%2520useful%2520features%2520for%250Adownstream%2520tasks%2520from%2520the%2520intermediate%2520representations%2520of%2520the%2520pre-trained%2520model%250Ain%2520a%2520query-only%2520manner.%2520Built%2520upon%2520these%2520features%252C%2520a%2520customized%2520classification%250Ahead%2520is%2520proposed%2520to%2520make%2520the%2520prediction%2520for%2520the%2520input%2520sample.%2520lightweight%250Aarchitecture%2520and%2520avoids%2520the%2520use%2520of%2520heavy%2520intermediate%2520features%2520for%2520running%250Agradient%2520descent%252C%2520it%2520demonstrates%2520limited%2520memory%2520usage%2520in%2520training.%2520Extensive%250Aexperiments%2520manifest%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%250Aunder%2520memory%2520constraints%252C%2520showcasing%2520its%2520applicability%2520in%2520real-world%250Asituations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Efficient%20and%20Memory-Efficient%20Tuning%20for%20Vision%20Transformer%3A%0A%20%20A%20Disentangled%20Approach&entry.906535625=Taolin%20Zhang%20and%20Jiawang%20Bai%20and%20Zhihe%20Lu%20and%20Dongze%20Lian%20and%20Genping%20Wang%20and%20Xinchao%20Wang%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20Recent%20works%20on%20parameter-efficient%20transfer%20learning%20%28PETL%29%20show%20the%0Apotential%20to%20adapt%20a%20pre-trained%20Vision%20Transformer%20to%20downstream%20recognition%0Atasks%20with%20only%20a%20few%20learnable%20parameters.%20However%2C%20since%20they%20usually%20insert%0Anew%20structures%20into%20the%20pre-trained%20model%2C%20entire%20intermediate%20features%20of%20that%0Amodel%20are%20changed%20and%20thus%20need%20to%20be%20stored%20to%20be%20involved%20in%0Aback-propagation%2C%20resulting%20in%20memory-heavy%20training.%20We%20solve%20this%20problem%0Afrom%20a%20novel%20disentangled%20perspective%2C%20i.e.%2C%20dividing%20PETL%20into%20two%20aspects%3A%0Atask-specific%20learning%20and%20pre-trained%20knowledge%20utilization.%20Specifically%2C%20we%0Asynthesize%20the%20task-specific%20query%20with%20a%20learnable%20and%20lightweight%20module%2C%0Awhich%20is%20independent%20of%20the%20pre-trained%20model.%20The%20synthesized%20query%20equipped%0Awith%20task-specific%20knowledge%20serves%20to%20extract%20the%20useful%20features%20for%0Adownstream%20tasks%20from%20the%20intermediate%20representations%20of%20the%20pre-trained%20model%0Ain%20a%20query-only%20manner.%20Built%20upon%20these%20features%2C%20a%20customized%20classification%0Ahead%20is%20proposed%20to%20make%20the%20prediction%20for%20the%20input%20sample.%20lightweight%0Aarchitecture%20and%20avoids%20the%20use%20of%20heavy%20intermediate%20features%20for%20running%0Agradient%20descent%2C%20it%20demonstrates%20limited%20memory%20usage%20in%20training.%20Extensive%0Aexperiments%20manifest%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Aunder%20memory%20constraints%2C%20showcasing%20its%20applicability%20in%20real-world%0Asituations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06964v1&entry.124074799=Read"},
{"title": "Learn and Don't Forget: Adding a New Language to ASR Foundation Models", "author": "Mengjie Qian and Siyuan Tang and Rao Ma and Kate M. Knill and Mark J. F. Gales", "abstract": "  Foundation ASR models often support many languages, e.g. 100 languages in\nWhisper. However, there has been limited work on integrating an additional,\ntypically low-resource, language, while maintaining performance on the original\nlanguage set. Fine-tuning, while simple, may degrade the accuracy of the\noriginal set. We compare three approaches that exploit adaptation parameters:\nsoft language code tuning, train only the language code; soft prompt tuning,\ntrain prepended tokens; and LoRA where a small set of additional parameters are\noptimised. Elastic Weight Consolidation (EWC) offers an alternative compromise\nwith the potential to maintain performance in specific target languages.\nResults show that direct fine-tuning yields the best performance for the new\nlanguage but degrades existing language capabilities. EWC can address this\nissue for specific languages. If only adaptation parameters are used, the\nlanguage capabilities are maintained but at the cost of performance in the new\nlanguage.\n", "link": "http://arxiv.org/abs/2407.06800v1", "date": "2024-07-09", "relevancy": 2.1384, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4373}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4318}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20and%20Don%27t%20Forget%3A%20Adding%20a%20New%20Language%20to%20ASR%20Foundation%20Models&body=Title%3A%20Learn%20and%20Don%27t%20Forget%3A%20Adding%20a%20New%20Language%20to%20ASR%20Foundation%20Models%0AAuthor%3A%20Mengjie%20Qian%20and%20Siyuan%20Tang%20and%20Rao%20Ma%20and%20Kate%20M.%20Knill%20and%20Mark%20J.%20F.%20Gales%0AAbstract%3A%20%20%20Foundation%20ASR%20models%20often%20support%20many%20languages%2C%20e.g.%20100%20languages%20in%0AWhisper.%20However%2C%20there%20has%20been%20limited%20work%20on%20integrating%20an%20additional%2C%0Atypically%20low-resource%2C%20language%2C%20while%20maintaining%20performance%20on%20the%20original%0Alanguage%20set.%20Fine-tuning%2C%20while%20simple%2C%20may%20degrade%20the%20accuracy%20of%20the%0Aoriginal%20set.%20We%20compare%20three%20approaches%20that%20exploit%20adaptation%20parameters%3A%0Asoft%20language%20code%20tuning%2C%20train%20only%20the%20language%20code%3B%20soft%20prompt%20tuning%2C%0Atrain%20prepended%20tokens%3B%20and%20LoRA%20where%20a%20small%20set%20of%20additional%20parameters%20are%0Aoptimised.%20Elastic%20Weight%20Consolidation%20%28EWC%29%20offers%20an%20alternative%20compromise%0Awith%20the%20potential%20to%20maintain%20performance%20in%20specific%20target%20languages.%0AResults%20show%20that%20direct%20fine-tuning%20yields%20the%20best%20performance%20for%20the%20new%0Alanguage%20but%20degrades%20existing%20language%20capabilities.%20EWC%20can%20address%20this%0Aissue%20for%20specific%20languages.%20If%20only%20adaptation%20parameters%20are%20used%2C%20the%0Alanguage%20capabilities%20are%20maintained%20but%20at%20the%20cost%20of%20performance%20in%20the%20new%0Alanguage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520and%2520Don%2527t%2520Forget%253A%2520Adding%2520a%2520New%2520Language%2520to%2520ASR%2520Foundation%2520Models%26entry.906535625%3DMengjie%2520Qian%2520and%2520Siyuan%2520Tang%2520and%2520Rao%2520Ma%2520and%2520Kate%2520M.%2520Knill%2520and%2520Mark%2520J.%2520F.%2520Gales%26entry.1292438233%3D%2520%2520Foundation%2520ASR%2520models%2520often%2520support%2520many%2520languages%252C%2520e.g.%2520100%2520languages%2520in%250AWhisper.%2520However%252C%2520there%2520has%2520been%2520limited%2520work%2520on%2520integrating%2520an%2520additional%252C%250Atypically%2520low-resource%252C%2520language%252C%2520while%2520maintaining%2520performance%2520on%2520the%2520original%250Alanguage%2520set.%2520Fine-tuning%252C%2520while%2520simple%252C%2520may%2520degrade%2520the%2520accuracy%2520of%2520the%250Aoriginal%2520set.%2520We%2520compare%2520three%2520approaches%2520that%2520exploit%2520adaptation%2520parameters%253A%250Asoft%2520language%2520code%2520tuning%252C%2520train%2520only%2520the%2520language%2520code%253B%2520soft%2520prompt%2520tuning%252C%250Atrain%2520prepended%2520tokens%253B%2520and%2520LoRA%2520where%2520a%2520small%2520set%2520of%2520additional%2520parameters%2520are%250Aoptimised.%2520Elastic%2520Weight%2520Consolidation%2520%2528EWC%2529%2520offers%2520an%2520alternative%2520compromise%250Awith%2520the%2520potential%2520to%2520maintain%2520performance%2520in%2520specific%2520target%2520languages.%250AResults%2520show%2520that%2520direct%2520fine-tuning%2520yields%2520the%2520best%2520performance%2520for%2520the%2520new%250Alanguage%2520but%2520degrades%2520existing%2520language%2520capabilities.%2520EWC%2520can%2520address%2520this%250Aissue%2520for%2520specific%2520languages.%2520If%2520only%2520adaptation%2520parameters%2520are%2520used%252C%2520the%250Alanguage%2520capabilities%2520are%2520maintained%2520but%2520at%2520the%2520cost%2520of%2520performance%2520in%2520the%2520new%250Alanguage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20and%20Don%27t%20Forget%3A%20Adding%20a%20New%20Language%20to%20ASR%20Foundation%20Models&entry.906535625=Mengjie%20Qian%20and%20Siyuan%20Tang%20and%20Rao%20Ma%20and%20Kate%20M.%20Knill%20and%20Mark%20J.%20F.%20Gales&entry.1292438233=%20%20Foundation%20ASR%20models%20often%20support%20many%20languages%2C%20e.g.%20100%20languages%20in%0AWhisper.%20However%2C%20there%20has%20been%20limited%20work%20on%20integrating%20an%20additional%2C%0Atypically%20low-resource%2C%20language%2C%20while%20maintaining%20performance%20on%20the%20original%0Alanguage%20set.%20Fine-tuning%2C%20while%20simple%2C%20may%20degrade%20the%20accuracy%20of%20the%0Aoriginal%20set.%20We%20compare%20three%20approaches%20that%20exploit%20adaptation%20parameters%3A%0Asoft%20language%20code%20tuning%2C%20train%20only%20the%20language%20code%3B%20soft%20prompt%20tuning%2C%0Atrain%20prepended%20tokens%3B%20and%20LoRA%20where%20a%20small%20set%20of%20additional%20parameters%20are%0Aoptimised.%20Elastic%20Weight%20Consolidation%20%28EWC%29%20offers%20an%20alternative%20compromise%0Awith%20the%20potential%20to%20maintain%20performance%20in%20specific%20target%20languages.%0AResults%20show%20that%20direct%20fine-tuning%20yields%20the%20best%20performance%20for%20the%20new%0Alanguage%20but%20degrades%20existing%20language%20capabilities.%20EWC%20can%20address%20this%0Aissue%20for%20specific%20languages.%20If%20only%20adaptation%20parameters%20are%20used%2C%20the%0Alanguage%20capabilities%20are%20maintained%20but%20at%20the%20cost%20of%20performance%20in%20the%20new%0Alanguage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06800v1&entry.124074799=Read"},
{"title": "AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit\n  Detectors", "author": "Kaishen Yuan and Zitong Yu and Xin Liu and Weicheng Xie and Huanjing Yue and Jingyu Yang", "abstract": "  Facial Action Units (AU) is a vital concept in the realm of affective\ncomputing, and AU detection has always been a hot research topic. Existing\nmethods suffer from overfitting issues due to the utilization of a large number\nof learnable parameters on scarce AU-annotated datasets or heavy reliance on\nsubstantial additional relevant data. Parameter-Efficient Transfer Learning\n(PETL) provides a promising paradigm to address these challenges, whereas its\nexisting methods lack design for AU characteristics. Therefore, we innovatively\ninvestigate PETL paradigm to AU detection, introducing AUFormer and proposing a\nnovel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual\nMoKE specific to a certain AU with minimal learnable parameters first\nintegrates personalized multi-scale and correlation knowledge. Then the MoKE\ncollaborates with other MoKEs in the expert group to obtain aggregated\ninformation and inject it into the frozen Vision Transformer (ViT) to achieve\nparameter-efficient AU detection. Additionally, we design a Margin-truncated\nDifficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the\nmodel to focus more on activated AUs, differentiate the difficulty of\nunactivated AUs, and discard potential mislabeled samples. Extensive\nexperiments from various perspectives, including within-domain, cross-domain,\ndata efficiency, and micro-expression domain, demonstrate AUFormer's\nstate-of-the-art performance and robust generalization abilities without\nrelying on additional relevant data. The code for AUFormer is available at\nhttps://github.com/yuankaishen2001/AUFormer.\n", "link": "http://arxiv.org/abs/2403.04697v2", "date": "2024-07-09", "relevancy": 2.1314, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5346}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AUFormer%3A%20Vision%20Transformers%20are%20Parameter-Efficient%20Facial%20Action%20Unit%0A%20%20Detectors&body=Title%3A%20AUFormer%3A%20Vision%20Transformers%20are%20Parameter-Efficient%20Facial%20Action%20Unit%0A%20%20Detectors%0AAuthor%3A%20Kaishen%20Yuan%20and%20Zitong%20Yu%20and%20Xin%20Liu%20and%20Weicheng%20Xie%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang%0AAbstract%3A%20%20%20Facial%20Action%20Units%20%28AU%29%20is%20a%20vital%20concept%20in%20the%20realm%20of%20affective%0Acomputing%2C%20and%20AU%20detection%20has%20always%20been%20a%20hot%20research%20topic.%20Existing%0Amethods%20suffer%20from%20overfitting%20issues%20due%20to%20the%20utilization%20of%20a%20large%20number%0Aof%20learnable%20parameters%20on%20scarce%20AU-annotated%20datasets%20or%20heavy%20reliance%20on%0Asubstantial%20additional%20relevant%20data.%20Parameter-Efficient%20Transfer%20Learning%0A%28PETL%29%20provides%20a%20promising%20paradigm%20to%20address%20these%20challenges%2C%20whereas%20its%0Aexisting%20methods%20lack%20design%20for%20AU%20characteristics.%20Therefore%2C%20we%20innovatively%0Ainvestigate%20PETL%20paradigm%20to%20AU%20detection%2C%20introducing%20AUFormer%20and%20proposing%20a%0Anovel%20Mixture-of-Knowledge%20Expert%20%28MoKE%29%20collaboration%20mechanism.%20An%20individual%0AMoKE%20specific%20to%20a%20certain%20AU%20with%20minimal%20learnable%20parameters%20first%0Aintegrates%20personalized%20multi-scale%20and%20correlation%20knowledge.%20Then%20the%20MoKE%0Acollaborates%20with%20other%20MoKEs%20in%20the%20expert%20group%20to%20obtain%20aggregated%0Ainformation%20and%20inject%20it%20into%20the%20frozen%20Vision%20Transformer%20%28ViT%29%20to%20achieve%0Aparameter-efficient%20AU%20detection.%20Additionally%2C%20we%20design%20a%20Margin-truncated%0ADifficulty-aware%20Weighted%20Asymmetric%20Loss%20%28MDWA-Loss%29%2C%20which%20can%20encourage%20the%0Amodel%20to%20focus%20more%20on%20activated%20AUs%2C%20differentiate%20the%20difficulty%20of%0Aunactivated%20AUs%2C%20and%20discard%20potential%20mislabeled%20samples.%20Extensive%0Aexperiments%20from%20various%20perspectives%2C%20including%20within-domain%2C%20cross-domain%2C%0Adata%20efficiency%2C%20and%20micro-expression%20domain%2C%20demonstrate%20AUFormer%27s%0Astate-of-the-art%20performance%20and%20robust%20generalization%20abilities%20without%0Arelying%20on%20additional%20relevant%20data.%20The%20code%20for%20AUFormer%20is%20available%20at%0Ahttps%3A//github.com/yuankaishen2001/AUFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAUFormer%253A%2520Vision%2520Transformers%2520are%2520Parameter-Efficient%2520Facial%2520Action%2520Unit%250A%2520%2520Detectors%26entry.906535625%3DKaishen%2520Yuan%2520and%2520Zitong%2520Yu%2520and%2520Xin%2520Liu%2520and%2520Weicheng%2520Xie%2520and%2520Huanjing%2520Yue%2520and%2520Jingyu%2520Yang%26entry.1292438233%3D%2520%2520Facial%2520Action%2520Units%2520%2528AU%2529%2520is%2520a%2520vital%2520concept%2520in%2520the%2520realm%2520of%2520affective%250Acomputing%252C%2520and%2520AU%2520detection%2520has%2520always%2520been%2520a%2520hot%2520research%2520topic.%2520Existing%250Amethods%2520suffer%2520from%2520overfitting%2520issues%2520due%2520to%2520the%2520utilization%2520of%2520a%2520large%2520number%250Aof%2520learnable%2520parameters%2520on%2520scarce%2520AU-annotated%2520datasets%2520or%2520heavy%2520reliance%2520on%250Asubstantial%2520additional%2520relevant%2520data.%2520Parameter-Efficient%2520Transfer%2520Learning%250A%2528PETL%2529%2520provides%2520a%2520promising%2520paradigm%2520to%2520address%2520these%2520challenges%252C%2520whereas%2520its%250Aexisting%2520methods%2520lack%2520design%2520for%2520AU%2520characteristics.%2520Therefore%252C%2520we%2520innovatively%250Ainvestigate%2520PETL%2520paradigm%2520to%2520AU%2520detection%252C%2520introducing%2520AUFormer%2520and%2520proposing%2520a%250Anovel%2520Mixture-of-Knowledge%2520Expert%2520%2528MoKE%2529%2520collaboration%2520mechanism.%2520An%2520individual%250AMoKE%2520specific%2520to%2520a%2520certain%2520AU%2520with%2520minimal%2520learnable%2520parameters%2520first%250Aintegrates%2520personalized%2520multi-scale%2520and%2520correlation%2520knowledge.%2520Then%2520the%2520MoKE%250Acollaborates%2520with%2520other%2520MoKEs%2520in%2520the%2520expert%2520group%2520to%2520obtain%2520aggregated%250Ainformation%2520and%2520inject%2520it%2520into%2520the%2520frozen%2520Vision%2520Transformer%2520%2528ViT%2529%2520to%2520achieve%250Aparameter-efficient%2520AU%2520detection.%2520Additionally%252C%2520we%2520design%2520a%2520Margin-truncated%250ADifficulty-aware%2520Weighted%2520Asymmetric%2520Loss%2520%2528MDWA-Loss%2529%252C%2520which%2520can%2520encourage%2520the%250Amodel%2520to%2520focus%2520more%2520on%2520activated%2520AUs%252C%2520differentiate%2520the%2520difficulty%2520of%250Aunactivated%2520AUs%252C%2520and%2520discard%2520potential%2520mislabeled%2520samples.%2520Extensive%250Aexperiments%2520from%2520various%2520perspectives%252C%2520including%2520within-domain%252C%2520cross-domain%252C%250Adata%2520efficiency%252C%2520and%2520micro-expression%2520domain%252C%2520demonstrate%2520AUFormer%2527s%250Astate-of-the-art%2520performance%2520and%2520robust%2520generalization%2520abilities%2520without%250Arelying%2520on%2520additional%2520relevant%2520data.%2520The%2520code%2520for%2520AUFormer%2520is%2520available%2520at%250Ahttps%253A//github.com/yuankaishen2001/AUFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AUFormer%3A%20Vision%20Transformers%20are%20Parameter-Efficient%20Facial%20Action%20Unit%0A%20%20Detectors&entry.906535625=Kaishen%20Yuan%20and%20Zitong%20Yu%20and%20Xin%20Liu%20and%20Weicheng%20Xie%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang&entry.1292438233=%20%20Facial%20Action%20Units%20%28AU%29%20is%20a%20vital%20concept%20in%20the%20realm%20of%20affective%0Acomputing%2C%20and%20AU%20detection%20has%20always%20been%20a%20hot%20research%20topic.%20Existing%0Amethods%20suffer%20from%20overfitting%20issues%20due%20to%20the%20utilization%20of%20a%20large%20number%0Aof%20learnable%20parameters%20on%20scarce%20AU-annotated%20datasets%20or%20heavy%20reliance%20on%0Asubstantial%20additional%20relevant%20data.%20Parameter-Efficient%20Transfer%20Learning%0A%28PETL%29%20provides%20a%20promising%20paradigm%20to%20address%20these%20challenges%2C%20whereas%20its%0Aexisting%20methods%20lack%20design%20for%20AU%20characteristics.%20Therefore%2C%20we%20innovatively%0Ainvestigate%20PETL%20paradigm%20to%20AU%20detection%2C%20introducing%20AUFormer%20and%20proposing%20a%0Anovel%20Mixture-of-Knowledge%20Expert%20%28MoKE%29%20collaboration%20mechanism.%20An%20individual%0AMoKE%20specific%20to%20a%20certain%20AU%20with%20minimal%20learnable%20parameters%20first%0Aintegrates%20personalized%20multi-scale%20and%20correlation%20knowledge.%20Then%20the%20MoKE%0Acollaborates%20with%20other%20MoKEs%20in%20the%20expert%20group%20to%20obtain%20aggregated%0Ainformation%20and%20inject%20it%20into%20the%20frozen%20Vision%20Transformer%20%28ViT%29%20to%20achieve%0Aparameter-efficient%20AU%20detection.%20Additionally%2C%20we%20design%20a%20Margin-truncated%0ADifficulty-aware%20Weighted%20Asymmetric%20Loss%20%28MDWA-Loss%29%2C%20which%20can%20encourage%20the%0Amodel%20to%20focus%20more%20on%20activated%20AUs%2C%20differentiate%20the%20difficulty%20of%0Aunactivated%20AUs%2C%20and%20discard%20potential%20mislabeled%20samples.%20Extensive%0Aexperiments%20from%20various%20perspectives%2C%20including%20within-domain%2C%20cross-domain%2C%0Adata%20efficiency%2C%20and%20micro-expression%20domain%2C%20demonstrate%20AUFormer%27s%0Astate-of-the-art%20performance%20and%20robust%20generalization%20abilities%20without%0Arelying%20on%20additional%20relevant%20data.%20The%20code%20for%20AUFormer%20is%20available%20at%0Ahttps%3A//github.com/yuankaishen2001/AUFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04697v2&entry.124074799=Read"},
{"title": "A Generalization Bound for Nearly-Linear Networks", "author": "Eugene Golikov", "abstract": "  We consider nonlinear networks as perturbations of linear ones. Based on this\napproach, we present novel generalization bounds that become non-vacuous for\nnetworks that are close to being linear. The main advantage over the previous\nworks which propose non-vacuous generalization bounds is that our bounds are\na-priori: performing the actual training is not required for evaluating the\nbounds. To the best of our knowledge, they are the first non-vacuous\ngeneralization bounds for neural nets possessing this property.\n", "link": "http://arxiv.org/abs/2407.06765v1", "date": "2024-07-09", "relevancy": 2.1161, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4552}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.413}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Generalization%20Bound%20for%20Nearly-Linear%20Networks&body=Title%3A%20A%20Generalization%20Bound%20for%20Nearly-Linear%20Networks%0AAuthor%3A%20Eugene%20Golikov%0AAbstract%3A%20%20%20We%20consider%20nonlinear%20networks%20as%20perturbations%20of%20linear%20ones.%20Based%20on%20this%0Aapproach%2C%20we%20present%20novel%20generalization%20bounds%20that%20become%20non-vacuous%20for%0Anetworks%20that%20are%20close%20to%20being%20linear.%20The%20main%20advantage%20over%20the%20previous%0Aworks%20which%20propose%20non-vacuous%20generalization%20bounds%20is%20that%20our%20bounds%20are%0Aa-priori%3A%20performing%20the%20actual%20training%20is%20not%20required%20for%20evaluating%20the%0Abounds.%20To%20the%20best%20of%20our%20knowledge%2C%20they%20are%20the%20first%20non-vacuous%0Ageneralization%20bounds%20for%20neural%20nets%20possessing%20this%20property.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Generalization%2520Bound%2520for%2520Nearly-Linear%2520Networks%26entry.906535625%3DEugene%2520Golikov%26entry.1292438233%3D%2520%2520We%2520consider%2520nonlinear%2520networks%2520as%2520perturbations%2520of%2520linear%2520ones.%2520Based%2520on%2520this%250Aapproach%252C%2520we%2520present%2520novel%2520generalization%2520bounds%2520that%2520become%2520non-vacuous%2520for%250Anetworks%2520that%2520are%2520close%2520to%2520being%2520linear.%2520The%2520main%2520advantage%2520over%2520the%2520previous%250Aworks%2520which%2520propose%2520non-vacuous%2520generalization%2520bounds%2520is%2520that%2520our%2520bounds%2520are%250Aa-priori%253A%2520performing%2520the%2520actual%2520training%2520is%2520not%2520required%2520for%2520evaluating%2520the%250Abounds.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520they%2520are%2520the%2520first%2520non-vacuous%250Ageneralization%2520bounds%2520for%2520neural%2520nets%2520possessing%2520this%2520property.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generalization%20Bound%20for%20Nearly-Linear%20Networks&entry.906535625=Eugene%20Golikov&entry.1292438233=%20%20We%20consider%20nonlinear%20networks%20as%20perturbations%20of%20linear%20ones.%20Based%20on%20this%0Aapproach%2C%20we%20present%20novel%20generalization%20bounds%20that%20become%20non-vacuous%20for%0Anetworks%20that%20are%20close%20to%20being%20linear.%20The%20main%20advantage%20over%20the%20previous%0Aworks%20which%20propose%20non-vacuous%20generalization%20bounds%20is%20that%20our%20bounds%20are%0Aa-priori%3A%20performing%20the%20actual%20training%20is%20not%20required%20for%20evaluating%20the%0Abounds.%20To%20the%20best%20of%20our%20knowledge%2C%20they%20are%20the%20first%20non-vacuous%0Ageneralization%20bounds%20for%20neural%20nets%20possessing%20this%20property.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06765v1&entry.124074799=Read"},
{"title": "Region-Adaptive Transform with Segmentation Prior for Image Compression", "author": "Yuxi Liu and Wenhan Yang and Huihui Bai and Yunchao Wei and Yao Zhao", "abstract": "  Learned Image Compression (LIC) has shown remarkable progress in recent\nyears. Existing works commonly employ CNN-based or self-attention-based modules\nas transform methods for compression. However, there is no prior research on\nneural transform that focuses on specific regions. In response, we introduce\nthe class-agnostic segmentation masks (i.e. semantic masks without category\nlabels) for extracting region-adaptive contextual information. Our proposed\nmodule, Region-Adaptive Transform, applies adaptive convolutions on different\nregions guided by the masks. Additionally, we introduce a plug-and-play module\nnamed Scale Affine Layer to incorporate rich contexts from various regions.\nWhile there have been prior image compression efforts that involve segmentation\nmasks as additional intermediate inputs, our approach differs significantly\nfrom them. Our advantages lie in that, to avoid extra bitrate overhead, we\ntreat these masks as privilege information, which is accessible during the\nmodel training stage but not required during the inference phase. To the best\nof our knowledge, we are the first to employ class-agnostic masks as privilege\ninformation and achieve superior performance in pixel-fidelity metrics, such as\nPeak Signal to Noise Ratio (PSNR). The experimental results demonstrate our\nimprovement compared to previously well-performing methods, with about 8.2%\nbitrate saving compared to VTM-17.0. The source code is available at\nhttps://github.com/GityuxiLiu/SegPIC-for-Image-Compression.\n", "link": "http://arxiv.org/abs/2403.00628v2", "date": "2024-07-09", "relevancy": 2.103, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5284}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Region-Adaptive%20Transform%20with%20Segmentation%20Prior%20for%20Image%20Compression&body=Title%3A%20Region-Adaptive%20Transform%20with%20Segmentation%20Prior%20for%20Image%20Compression%0AAuthor%3A%20Yuxi%20Liu%20and%20Wenhan%20Yang%20and%20Huihui%20Bai%20and%20Yunchao%20Wei%20and%20Yao%20Zhao%0AAbstract%3A%20%20%20Learned%20Image%20Compression%20%28LIC%29%20has%20shown%20remarkable%20progress%20in%20recent%0Ayears.%20Existing%20works%20commonly%20employ%20CNN-based%20or%20self-attention-based%20modules%0Aas%20transform%20methods%20for%20compression.%20However%2C%20there%20is%20no%20prior%20research%20on%0Aneural%20transform%20that%20focuses%20on%20specific%20regions.%20In%20response%2C%20we%20introduce%0Athe%20class-agnostic%20segmentation%20masks%20%28i.e.%20semantic%20masks%20without%20category%0Alabels%29%20for%20extracting%20region-adaptive%20contextual%20information.%20Our%20proposed%0Amodule%2C%20Region-Adaptive%20Transform%2C%20applies%20adaptive%20convolutions%20on%20different%0Aregions%20guided%20by%20the%20masks.%20Additionally%2C%20we%20introduce%20a%20plug-and-play%20module%0Anamed%20Scale%20Affine%20Layer%20to%20incorporate%20rich%20contexts%20from%20various%20regions.%0AWhile%20there%20have%20been%20prior%20image%20compression%20efforts%20that%20involve%20segmentation%0Amasks%20as%20additional%20intermediate%20inputs%2C%20our%20approach%20differs%20significantly%0Afrom%20them.%20Our%20advantages%20lie%20in%20that%2C%20to%20avoid%20extra%20bitrate%20overhead%2C%20we%0Atreat%20these%20masks%20as%20privilege%20information%2C%20which%20is%20accessible%20during%20the%0Amodel%20training%20stage%20but%20not%20required%20during%20the%20inference%20phase.%20To%20the%20best%0Aof%20our%20knowledge%2C%20we%20are%20the%20first%20to%20employ%20class-agnostic%20masks%20as%20privilege%0Ainformation%20and%20achieve%20superior%20performance%20in%20pixel-fidelity%20metrics%2C%20such%20as%0APeak%20Signal%20to%20Noise%20Ratio%20%28PSNR%29.%20The%20experimental%20results%20demonstrate%20our%0Aimprovement%20compared%20to%20previously%20well-performing%20methods%2C%20with%20about%208.2%25%0Abitrate%20saving%20compared%20to%20VTM-17.0.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/GityuxiLiu/SegPIC-for-Image-Compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00628v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegion-Adaptive%2520Transform%2520with%2520Segmentation%2520Prior%2520for%2520Image%2520Compression%26entry.906535625%3DYuxi%2520Liu%2520and%2520Wenhan%2520Yang%2520and%2520Huihui%2520Bai%2520and%2520Yunchao%2520Wei%2520and%2520Yao%2520Zhao%26entry.1292438233%3D%2520%2520Learned%2520Image%2520Compression%2520%2528LIC%2529%2520has%2520shown%2520remarkable%2520progress%2520in%2520recent%250Ayears.%2520Existing%2520works%2520commonly%2520employ%2520CNN-based%2520or%2520self-attention-based%2520modules%250Aas%2520transform%2520methods%2520for%2520compression.%2520However%252C%2520there%2520is%2520no%2520prior%2520research%2520on%250Aneural%2520transform%2520that%2520focuses%2520on%2520specific%2520regions.%2520In%2520response%252C%2520we%2520introduce%250Athe%2520class-agnostic%2520segmentation%2520masks%2520%2528i.e.%2520semantic%2520masks%2520without%2520category%250Alabels%2529%2520for%2520extracting%2520region-adaptive%2520contextual%2520information.%2520Our%2520proposed%250Amodule%252C%2520Region-Adaptive%2520Transform%252C%2520applies%2520adaptive%2520convolutions%2520on%2520different%250Aregions%2520guided%2520by%2520the%2520masks.%2520Additionally%252C%2520we%2520introduce%2520a%2520plug-and-play%2520module%250Anamed%2520Scale%2520Affine%2520Layer%2520to%2520incorporate%2520rich%2520contexts%2520from%2520various%2520regions.%250AWhile%2520there%2520have%2520been%2520prior%2520image%2520compression%2520efforts%2520that%2520involve%2520segmentation%250Amasks%2520as%2520additional%2520intermediate%2520inputs%252C%2520our%2520approach%2520differs%2520significantly%250Afrom%2520them.%2520Our%2520advantages%2520lie%2520in%2520that%252C%2520to%2520avoid%2520extra%2520bitrate%2520overhead%252C%2520we%250Atreat%2520these%2520masks%2520as%2520privilege%2520information%252C%2520which%2520is%2520accessible%2520during%2520the%250Amodel%2520training%2520stage%2520but%2520not%2520required%2520during%2520the%2520inference%2520phase.%2520To%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520we%2520are%2520the%2520first%2520to%2520employ%2520class-agnostic%2520masks%2520as%2520privilege%250Ainformation%2520and%2520achieve%2520superior%2520performance%2520in%2520pixel-fidelity%2520metrics%252C%2520such%2520as%250APeak%2520Signal%2520to%2520Noise%2520Ratio%2520%2528PSNR%2529.%2520The%2520experimental%2520results%2520demonstrate%2520our%250Aimprovement%2520compared%2520to%2520previously%2520well-performing%2520methods%252C%2520with%2520about%25208.2%2525%250Abitrate%2520saving%2520compared%2520to%2520VTM-17.0.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/GityuxiLiu/SegPIC-for-Image-Compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00628v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Region-Adaptive%20Transform%20with%20Segmentation%20Prior%20for%20Image%20Compression&entry.906535625=Yuxi%20Liu%20and%20Wenhan%20Yang%20and%20Huihui%20Bai%20and%20Yunchao%20Wei%20and%20Yao%20Zhao&entry.1292438233=%20%20Learned%20Image%20Compression%20%28LIC%29%20has%20shown%20remarkable%20progress%20in%20recent%0Ayears.%20Existing%20works%20commonly%20employ%20CNN-based%20or%20self-attention-based%20modules%0Aas%20transform%20methods%20for%20compression.%20However%2C%20there%20is%20no%20prior%20research%20on%0Aneural%20transform%20that%20focuses%20on%20specific%20regions.%20In%20response%2C%20we%20introduce%0Athe%20class-agnostic%20segmentation%20masks%20%28i.e.%20semantic%20masks%20without%20category%0Alabels%29%20for%20extracting%20region-adaptive%20contextual%20information.%20Our%20proposed%0Amodule%2C%20Region-Adaptive%20Transform%2C%20applies%20adaptive%20convolutions%20on%20different%0Aregions%20guided%20by%20the%20masks.%20Additionally%2C%20we%20introduce%20a%20plug-and-play%20module%0Anamed%20Scale%20Affine%20Layer%20to%20incorporate%20rich%20contexts%20from%20various%20regions.%0AWhile%20there%20have%20been%20prior%20image%20compression%20efforts%20that%20involve%20segmentation%0Amasks%20as%20additional%20intermediate%20inputs%2C%20our%20approach%20differs%20significantly%0Afrom%20them.%20Our%20advantages%20lie%20in%20that%2C%20to%20avoid%20extra%20bitrate%20overhead%2C%20we%0Atreat%20these%20masks%20as%20privilege%20information%2C%20which%20is%20accessible%20during%20the%0Amodel%20training%20stage%20but%20not%20required%20during%20the%20inference%20phase.%20To%20the%20best%0Aof%20our%20knowledge%2C%20we%20are%20the%20first%20to%20employ%20class-agnostic%20masks%20as%20privilege%0Ainformation%20and%20achieve%20superior%20performance%20in%20pixel-fidelity%20metrics%2C%20such%20as%0APeak%20Signal%20to%20Noise%20Ratio%20%28PSNR%29.%20The%20experimental%20results%20demonstrate%20our%0Aimprovement%20compared%20to%20previously%20well-performing%20methods%2C%20with%20about%208.2%25%0Abitrate%20saving%20compared%20to%20VTM-17.0.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/GityuxiLiu/SegPIC-for-Image-Compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00628v2&entry.124074799=Read"},
{"title": "Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting\n  Region Captions", "author": "Yu-Guan Hsieh and Cheng-Yu Hsieh and Shih-Ying Yeh and Louis B\u00e9thune and Hadi Pour Ansari and Pavan Kumar Anasosalu Vasu and Chun-Liang Li and Ranjay Krishna and Oncel Tuzel and Marco Cuturi", "abstract": "  Humans describe complex scenes with compositionality, using simple text\ndescriptions enriched with links and relationships. While vision-language\nresearch has aimed to develop models with compositional understanding\ncapabilities, this is not reflected yet in existing datasets which, for the\nmost part, still use plain text to describe images. In this work, we propose a\nnew annotation strategy, graph-based captioning (GBC) that describes an image\nusing a labelled graph structure, with nodes of various types. The nodes in GBC\nare created using, in a first stage, object detection and dense captioning\ntools nested recursively to uncover and describe entity nodes, further linked\ntogether in a second stage by highlighting, using new types of nodes,\ncompositions and relations among entities. Since all GBC nodes hold plain text\ndescriptions, GBC retains the flexibility found in natural language, but can\nalso encode hierarchical information in its edges. We demonstrate that GBC can\nbe produced automatically, using off-the-shelf multimodal LLMs and\nopen-vocabulary detection models, by building a new dataset, GBC10M, gathering\nGBC annotations for about 10M images of the CC12M dataset. We use GBC10M to\nshowcase the wealth of node captions uncovered by GBC, as measured with CLIP\ntraining. We show that using GBC nodes' annotations -- notably those stored in\ncomposition and relation nodes -- results in significant performance boost on\ndownstream models when compared to other dataset formats. To further explore\nthe opportunities provided by GBC, we also propose a new attention mechanism\nthat can leverage the entire GBC graph, with encouraging experimental results\nthat show the extra benefits of incorporating the graph structure. Our datasets\nare released at \\url{https://huggingface.co/graph-based-captions}.\n", "link": "http://arxiv.org/abs/2407.06723v1", "date": "2024-07-09", "relevancy": 2.0999, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5605}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5225}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Based%20Captioning%3A%20Enhancing%20Visual%20Descriptions%20by%20Interconnecting%0A%20%20Region%20Captions&body=Title%3A%20Graph-Based%20Captioning%3A%20Enhancing%20Visual%20Descriptions%20by%20Interconnecting%0A%20%20Region%20Captions%0AAuthor%3A%20Yu-Guan%20Hsieh%20and%20Cheng-Yu%20Hsieh%20and%20Shih-Ying%20Yeh%20and%20Louis%20B%C3%A9thune%20and%20Hadi%20Pour%20Ansari%20and%20Pavan%20Kumar%20Anasosalu%20Vasu%20and%20Chun-Liang%20Li%20and%20Ranjay%20Krishna%20and%20Oncel%20Tuzel%20and%20Marco%20Cuturi%0AAbstract%3A%20%20%20Humans%20describe%20complex%20scenes%20with%20compositionality%2C%20using%20simple%20text%0Adescriptions%20enriched%20with%20links%20and%20relationships.%20While%20vision-language%0Aresearch%20has%20aimed%20to%20develop%20models%20with%20compositional%20understanding%0Acapabilities%2C%20this%20is%20not%20reflected%20yet%20in%20existing%20datasets%20which%2C%20for%20the%0Amost%20part%2C%20still%20use%20plain%20text%20to%20describe%20images.%20In%20this%20work%2C%20we%20propose%20a%0Anew%20annotation%20strategy%2C%20graph-based%20captioning%20%28GBC%29%20that%20describes%20an%20image%0Ausing%20a%20labelled%20graph%20structure%2C%20with%20nodes%20of%20various%20types.%20The%20nodes%20in%20GBC%0Aare%20created%20using%2C%20in%20a%20first%20stage%2C%20object%20detection%20and%20dense%20captioning%0Atools%20nested%20recursively%20to%20uncover%20and%20describe%20entity%20nodes%2C%20further%20linked%0Atogether%20in%20a%20second%20stage%20by%20highlighting%2C%20using%20new%20types%20of%20nodes%2C%0Acompositions%20and%20relations%20among%20entities.%20Since%20all%20GBC%20nodes%20hold%20plain%20text%0Adescriptions%2C%20GBC%20retains%20the%20flexibility%20found%20in%20natural%20language%2C%20but%20can%0Aalso%20encode%20hierarchical%20information%20in%20its%20edges.%20We%20demonstrate%20that%20GBC%20can%0Abe%20produced%20automatically%2C%20using%20off-the-shelf%20multimodal%20LLMs%20and%0Aopen-vocabulary%20detection%20models%2C%20by%20building%20a%20new%20dataset%2C%20GBC10M%2C%20gathering%0AGBC%20annotations%20for%20about%2010M%20images%20of%20the%20CC12M%20dataset.%20We%20use%20GBC10M%20to%0Ashowcase%20the%20wealth%20of%20node%20captions%20uncovered%20by%20GBC%2C%20as%20measured%20with%20CLIP%0Atraining.%20We%20show%20that%20using%20GBC%20nodes%27%20annotations%20--%20notably%20those%20stored%20in%0Acomposition%20and%20relation%20nodes%20--%20results%20in%20significant%20performance%20boost%20on%0Adownstream%20models%20when%20compared%20to%20other%20dataset%20formats.%20To%20further%20explore%0Athe%20opportunities%20provided%20by%20GBC%2C%20we%20also%20propose%20a%20new%20attention%20mechanism%0Athat%20can%20leverage%20the%20entire%20GBC%20graph%2C%20with%20encouraging%20experimental%20results%0Athat%20show%20the%20extra%20benefits%20of%20incorporating%20the%20graph%20structure.%20Our%20datasets%0Aare%20released%20at%20%5Curl%7Bhttps%3A//huggingface.co/graph-based-captions%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Based%2520Captioning%253A%2520Enhancing%2520Visual%2520Descriptions%2520by%2520Interconnecting%250A%2520%2520Region%2520Captions%26entry.906535625%3DYu-Guan%2520Hsieh%2520and%2520Cheng-Yu%2520Hsieh%2520and%2520Shih-Ying%2520Yeh%2520and%2520Louis%2520B%25C3%25A9thune%2520and%2520Hadi%2520Pour%2520Ansari%2520and%2520Pavan%2520Kumar%2520Anasosalu%2520Vasu%2520and%2520Chun-Liang%2520Li%2520and%2520Ranjay%2520Krishna%2520and%2520Oncel%2520Tuzel%2520and%2520Marco%2520Cuturi%26entry.1292438233%3D%2520%2520Humans%2520describe%2520complex%2520scenes%2520with%2520compositionality%252C%2520using%2520simple%2520text%250Adescriptions%2520enriched%2520with%2520links%2520and%2520relationships.%2520While%2520vision-language%250Aresearch%2520has%2520aimed%2520to%2520develop%2520models%2520with%2520compositional%2520understanding%250Acapabilities%252C%2520this%2520is%2520not%2520reflected%2520yet%2520in%2520existing%2520datasets%2520which%252C%2520for%2520the%250Amost%2520part%252C%2520still%2520use%2520plain%2520text%2520to%2520describe%2520images.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anew%2520annotation%2520strategy%252C%2520graph-based%2520captioning%2520%2528GBC%2529%2520that%2520describes%2520an%2520image%250Ausing%2520a%2520labelled%2520graph%2520structure%252C%2520with%2520nodes%2520of%2520various%2520types.%2520The%2520nodes%2520in%2520GBC%250Aare%2520created%2520using%252C%2520in%2520a%2520first%2520stage%252C%2520object%2520detection%2520and%2520dense%2520captioning%250Atools%2520nested%2520recursively%2520to%2520uncover%2520and%2520describe%2520entity%2520nodes%252C%2520further%2520linked%250Atogether%2520in%2520a%2520second%2520stage%2520by%2520highlighting%252C%2520using%2520new%2520types%2520of%2520nodes%252C%250Acompositions%2520and%2520relations%2520among%2520entities.%2520Since%2520all%2520GBC%2520nodes%2520hold%2520plain%2520text%250Adescriptions%252C%2520GBC%2520retains%2520the%2520flexibility%2520found%2520in%2520natural%2520language%252C%2520but%2520can%250Aalso%2520encode%2520hierarchical%2520information%2520in%2520its%2520edges.%2520We%2520demonstrate%2520that%2520GBC%2520can%250Abe%2520produced%2520automatically%252C%2520using%2520off-the-shelf%2520multimodal%2520LLMs%2520and%250Aopen-vocabulary%2520detection%2520models%252C%2520by%2520building%2520a%2520new%2520dataset%252C%2520GBC10M%252C%2520gathering%250AGBC%2520annotations%2520for%2520about%252010M%2520images%2520of%2520the%2520CC12M%2520dataset.%2520We%2520use%2520GBC10M%2520to%250Ashowcase%2520the%2520wealth%2520of%2520node%2520captions%2520uncovered%2520by%2520GBC%252C%2520as%2520measured%2520with%2520CLIP%250Atraining.%2520We%2520show%2520that%2520using%2520GBC%2520nodes%2527%2520annotations%2520--%2520notably%2520those%2520stored%2520in%250Acomposition%2520and%2520relation%2520nodes%2520--%2520results%2520in%2520significant%2520performance%2520boost%2520on%250Adownstream%2520models%2520when%2520compared%2520to%2520other%2520dataset%2520formats.%2520To%2520further%2520explore%250Athe%2520opportunities%2520provided%2520by%2520GBC%252C%2520we%2520also%2520propose%2520a%2520new%2520attention%2520mechanism%250Athat%2520can%2520leverage%2520the%2520entire%2520GBC%2520graph%252C%2520with%2520encouraging%2520experimental%2520results%250Athat%2520show%2520the%2520extra%2520benefits%2520of%2520incorporating%2520the%2520graph%2520structure.%2520Our%2520datasets%250Aare%2520released%2520at%2520%255Curl%257Bhttps%253A//huggingface.co/graph-based-captions%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Based%20Captioning%3A%20Enhancing%20Visual%20Descriptions%20by%20Interconnecting%0A%20%20Region%20Captions&entry.906535625=Yu-Guan%20Hsieh%20and%20Cheng-Yu%20Hsieh%20and%20Shih-Ying%20Yeh%20and%20Louis%20B%C3%A9thune%20and%20Hadi%20Pour%20Ansari%20and%20Pavan%20Kumar%20Anasosalu%20Vasu%20and%20Chun-Liang%20Li%20and%20Ranjay%20Krishna%20and%20Oncel%20Tuzel%20and%20Marco%20Cuturi&entry.1292438233=%20%20Humans%20describe%20complex%20scenes%20with%20compositionality%2C%20using%20simple%20text%0Adescriptions%20enriched%20with%20links%20and%20relationships.%20While%20vision-language%0Aresearch%20has%20aimed%20to%20develop%20models%20with%20compositional%20understanding%0Acapabilities%2C%20this%20is%20not%20reflected%20yet%20in%20existing%20datasets%20which%2C%20for%20the%0Amost%20part%2C%20still%20use%20plain%20text%20to%20describe%20images.%20In%20this%20work%2C%20we%20propose%20a%0Anew%20annotation%20strategy%2C%20graph-based%20captioning%20%28GBC%29%20that%20describes%20an%20image%0Ausing%20a%20labelled%20graph%20structure%2C%20with%20nodes%20of%20various%20types.%20The%20nodes%20in%20GBC%0Aare%20created%20using%2C%20in%20a%20first%20stage%2C%20object%20detection%20and%20dense%20captioning%0Atools%20nested%20recursively%20to%20uncover%20and%20describe%20entity%20nodes%2C%20further%20linked%0Atogether%20in%20a%20second%20stage%20by%20highlighting%2C%20using%20new%20types%20of%20nodes%2C%0Acompositions%20and%20relations%20among%20entities.%20Since%20all%20GBC%20nodes%20hold%20plain%20text%0Adescriptions%2C%20GBC%20retains%20the%20flexibility%20found%20in%20natural%20language%2C%20but%20can%0Aalso%20encode%20hierarchical%20information%20in%20its%20edges.%20We%20demonstrate%20that%20GBC%20can%0Abe%20produced%20automatically%2C%20using%20off-the-shelf%20multimodal%20LLMs%20and%0Aopen-vocabulary%20detection%20models%2C%20by%20building%20a%20new%20dataset%2C%20GBC10M%2C%20gathering%0AGBC%20annotations%20for%20about%2010M%20images%20of%20the%20CC12M%20dataset.%20We%20use%20GBC10M%20to%0Ashowcase%20the%20wealth%20of%20node%20captions%20uncovered%20by%20GBC%2C%20as%20measured%20with%20CLIP%0Atraining.%20We%20show%20that%20using%20GBC%20nodes%27%20annotations%20--%20notably%20those%20stored%20in%0Acomposition%20and%20relation%20nodes%20--%20results%20in%20significant%20performance%20boost%20on%0Adownstream%20models%20when%20compared%20to%20other%20dataset%20formats.%20To%20further%20explore%0Athe%20opportunities%20provided%20by%20GBC%2C%20we%20also%20propose%20a%20new%20attention%20mechanism%0Athat%20can%20leverage%20the%20entire%20GBC%20graph%2C%20with%20encouraging%20experimental%20results%0Athat%20show%20the%20extra%20benefits%20of%20incorporating%20the%20graph%20structure.%20Our%20datasets%0Aare%20released%20at%20%5Curl%7Bhttps%3A//huggingface.co/graph-based-captions%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06723v1&entry.124074799=Read"},
{"title": "Positive-Unlabelled Learning for Improving Image-based Recommender\n  System Explainability", "author": "\u00c1lvaro Fern\u00e1ndez-Campa-Gonz\u00e1lez and Jorge Paz-Ruza and Amparo Alonso-Betanzos and Bertha Guijarro-Berdi\u00f1as", "abstract": "  Among the existing approaches for visual-based Recommender System (RS)\nexplainability, utilizing user-uploaded item images as efficient, trustable\nexplanations is a promising option. However, current models following this\nparadigm assume that, for any user, all images uploaded by other users can be\nconsidered negative training examples (i.e. bad explanatory images), an\ninadvertedly naive labelling assumption that contradicts the rationale of the\napproach. This work proposes a new explainer training pipeline by leveraging\nPositive-Unlabelled (PU) Learning techniques to train image-based explainer\nwith refined subsets of reliable negative examples for each user selected\nthrough a novel user-personalized, two-step, similarity-based PU Learning\nalgorithm. Computational experiments show this PU-based approach outperforms\nthe state-of-the-art non-PU method in six popular real-world datasets, proving\nthat an improvement of visual-based RS explainability can be achieved by\nmaximizing training data quality rather than increasing model complexity.\n", "link": "http://arxiv.org/abs/2407.06740v1", "date": "2024-07-09", "relevancy": 2.0944, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5267}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5247}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Positive-Unlabelled%20Learning%20for%20Improving%20Image-based%20Recommender%0A%20%20System%20Explainability&body=Title%3A%20Positive-Unlabelled%20Learning%20for%20Improving%20Image-based%20Recommender%0A%20%20System%20Explainability%0AAuthor%3A%20%C3%81lvaro%20Fern%C3%A1ndez-Campa-Gonz%C3%A1lez%20and%20Jorge%20Paz-Ruza%20and%20Amparo%20Alonso-Betanzos%20and%20Bertha%20Guijarro-Berdi%C3%B1as%0AAbstract%3A%20%20%20Among%20the%20existing%20approaches%20for%20visual-based%20Recommender%20System%20%28RS%29%0Aexplainability%2C%20utilizing%20user-uploaded%20item%20images%20as%20efficient%2C%20trustable%0Aexplanations%20is%20a%20promising%20option.%20However%2C%20current%20models%20following%20this%0Aparadigm%20assume%20that%2C%20for%20any%20user%2C%20all%20images%20uploaded%20by%20other%20users%20can%20be%0Aconsidered%20negative%20training%20examples%20%28i.e.%20bad%20explanatory%20images%29%2C%20an%0Ainadvertedly%20naive%20labelling%20assumption%20that%20contradicts%20the%20rationale%20of%20the%0Aapproach.%20This%20work%20proposes%20a%20new%20explainer%20training%20pipeline%20by%20leveraging%0APositive-Unlabelled%20%28PU%29%20Learning%20techniques%20to%20train%20image-based%20explainer%0Awith%20refined%20subsets%20of%20reliable%20negative%20examples%20for%20each%20user%20selected%0Athrough%20a%20novel%20user-personalized%2C%20two-step%2C%20similarity-based%20PU%20Learning%0Aalgorithm.%20Computational%20experiments%20show%20this%20PU-based%20approach%20outperforms%0Athe%20state-of-the-art%20non-PU%20method%20in%20six%20popular%20real-world%20datasets%2C%20proving%0Athat%20an%20improvement%20of%20visual-based%20RS%20explainability%20can%20be%20achieved%20by%0Amaximizing%20training%20data%20quality%20rather%20than%20increasing%20model%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPositive-Unlabelled%2520Learning%2520for%2520Improving%2520Image-based%2520Recommender%250A%2520%2520System%2520Explainability%26entry.906535625%3D%25C3%2581lvaro%2520Fern%25C3%25A1ndez-Campa-Gonz%25C3%25A1lez%2520and%2520Jorge%2520Paz-Ruza%2520and%2520Amparo%2520Alonso-Betanzos%2520and%2520Bertha%2520Guijarro-Berdi%25C3%25B1as%26entry.1292438233%3D%2520%2520Among%2520the%2520existing%2520approaches%2520for%2520visual-based%2520Recommender%2520System%2520%2528RS%2529%250Aexplainability%252C%2520utilizing%2520user-uploaded%2520item%2520images%2520as%2520efficient%252C%2520trustable%250Aexplanations%2520is%2520a%2520promising%2520option.%2520However%252C%2520current%2520models%2520following%2520this%250Aparadigm%2520assume%2520that%252C%2520for%2520any%2520user%252C%2520all%2520images%2520uploaded%2520by%2520other%2520users%2520can%2520be%250Aconsidered%2520negative%2520training%2520examples%2520%2528i.e.%2520bad%2520explanatory%2520images%2529%252C%2520an%250Ainadvertedly%2520naive%2520labelling%2520assumption%2520that%2520contradicts%2520the%2520rationale%2520of%2520the%250Aapproach.%2520This%2520work%2520proposes%2520a%2520new%2520explainer%2520training%2520pipeline%2520by%2520leveraging%250APositive-Unlabelled%2520%2528PU%2529%2520Learning%2520techniques%2520to%2520train%2520image-based%2520explainer%250Awith%2520refined%2520subsets%2520of%2520reliable%2520negative%2520examples%2520for%2520each%2520user%2520selected%250Athrough%2520a%2520novel%2520user-personalized%252C%2520two-step%252C%2520similarity-based%2520PU%2520Learning%250Aalgorithm.%2520Computational%2520experiments%2520show%2520this%2520PU-based%2520approach%2520outperforms%250Athe%2520state-of-the-art%2520non-PU%2520method%2520in%2520six%2520popular%2520real-world%2520datasets%252C%2520proving%250Athat%2520an%2520improvement%2520of%2520visual-based%2520RS%2520explainability%2520can%2520be%2520achieved%2520by%250Amaximizing%2520training%2520data%2520quality%2520rather%2520than%2520increasing%2520model%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Positive-Unlabelled%20Learning%20for%20Improving%20Image-based%20Recommender%0A%20%20System%20Explainability&entry.906535625=%C3%81lvaro%20Fern%C3%A1ndez-Campa-Gonz%C3%A1lez%20and%20Jorge%20Paz-Ruza%20and%20Amparo%20Alonso-Betanzos%20and%20Bertha%20Guijarro-Berdi%C3%B1as&entry.1292438233=%20%20Among%20the%20existing%20approaches%20for%20visual-based%20Recommender%20System%20%28RS%29%0Aexplainability%2C%20utilizing%20user-uploaded%20item%20images%20as%20efficient%2C%20trustable%0Aexplanations%20is%20a%20promising%20option.%20However%2C%20current%20models%20following%20this%0Aparadigm%20assume%20that%2C%20for%20any%20user%2C%20all%20images%20uploaded%20by%20other%20users%20can%20be%0Aconsidered%20negative%20training%20examples%20%28i.e.%20bad%20explanatory%20images%29%2C%20an%0Ainadvertedly%20naive%20labelling%20assumption%20that%20contradicts%20the%20rationale%20of%20the%0Aapproach.%20This%20work%20proposes%20a%20new%20explainer%20training%20pipeline%20by%20leveraging%0APositive-Unlabelled%20%28PU%29%20Learning%20techniques%20to%20train%20image-based%20explainer%0Awith%20refined%20subsets%20of%20reliable%20negative%20examples%20for%20each%20user%20selected%0Athrough%20a%20novel%20user-personalized%2C%20two-step%2C%20similarity-based%20PU%20Learning%0Aalgorithm.%20Computational%20experiments%20show%20this%20PU-based%20approach%20outperforms%0Athe%20state-of-the-art%20non-PU%20method%20in%20six%20popular%20real-world%20datasets%2C%20proving%0Athat%20an%20improvement%20of%20visual-based%20RS%20explainability%20can%20be%20achieved%20by%0Amaximizing%20training%20data%20quality%20rather%20than%20increasing%20model%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06740v1&entry.124074799=Read"},
{"title": "AdaNCA: Neural Cellular Automata As Adaptors For More Robust Vision\n  Transformer", "author": "Yitao Xu and Tong Zhang and Sabine S\u00fcsstrunk", "abstract": "  Vision Transformers (ViTs) have demonstrated remarkable performance in image\nclassification tasks, particularly when equipped with local information via\nregion attention or convolutions. While such architectures improve the feature\naggregation from different granularities, they often fail to contribute to the\nrobustness of the networks. Neural Cellular Automata (NCA) enables the modeling\nof global cell representations through local interactions, with its training\nstrategies and architecture design conferring strong generalization ability and\nrobustness against noisy inputs. In this paper, we propose Adaptor Neural\nCellular Automata (AdaNCA) for Vision Transformer that uses NCA as plug-in-play\nadaptors between ViT layers, enhancing ViT's performance and robustness against\nadversarial samples as well as out-of-distribution inputs. To overcome the\nlarge computational overhead of standard NCAs, we propose Dynamic Interaction\nfor more efficient interaction learning. Furthermore, we develop an algorithm\nfor identifying the most effective insertion points for AdaNCA based on our\nanalysis of AdaNCA placement and robustness improvement. With less than a 3%\nincrease in parameters, AdaNCA contributes to more than 10% absolute\nimprovement in accuracy under adversarial attacks on the ImageNet1K benchmark.\nMoreover, we demonstrate with extensive evaluations across 8 robustness\nbenchmarks and 4 ViT architectures that AdaNCA, as a plug-in-play module,\nconsistently improves the robustness of ViTs.\n", "link": "http://arxiv.org/abs/2406.08298v4", "date": "2024-07-09", "relevancy": 2.0877, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5124}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaNCA%3A%20Neural%20Cellular%20Automata%20As%20Adaptors%20For%20More%20Robust%20Vision%0A%20%20Transformer&body=Title%3A%20AdaNCA%3A%20Neural%20Cellular%20Automata%20As%20Adaptors%20For%20More%20Robust%20Vision%0A%20%20Transformer%0AAuthor%3A%20Yitao%20Xu%20and%20Tong%20Zhang%20and%20Sabine%20S%C3%BCsstrunk%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20remarkable%20performance%20in%20image%0Aclassification%20tasks%2C%20particularly%20when%20equipped%20with%20local%20information%20via%0Aregion%20attention%20or%20convolutions.%20While%20such%20architectures%20improve%20the%20feature%0Aaggregation%20from%20different%20granularities%2C%20they%20often%20fail%20to%20contribute%20to%20the%0Arobustness%20of%20the%20networks.%20Neural%20Cellular%20Automata%20%28NCA%29%20enables%20the%20modeling%0Aof%20global%20cell%20representations%20through%20local%20interactions%2C%20with%20its%20training%0Astrategies%20and%20architecture%20design%20conferring%20strong%20generalization%20ability%20and%0Arobustness%20against%20noisy%20inputs.%20In%20this%20paper%2C%20we%20propose%20Adaptor%20Neural%0ACellular%20Automata%20%28AdaNCA%29%20for%20Vision%20Transformer%20that%20uses%20NCA%20as%20plug-in-play%0Aadaptors%20between%20ViT%20layers%2C%20enhancing%20ViT%27s%20performance%20and%20robustness%20against%0Aadversarial%20samples%20as%20well%20as%20out-of-distribution%20inputs.%20To%20overcome%20the%0Alarge%20computational%20overhead%20of%20standard%20NCAs%2C%20we%20propose%20Dynamic%20Interaction%0Afor%20more%20efficient%20interaction%20learning.%20Furthermore%2C%20we%20develop%20an%20algorithm%0Afor%20identifying%20the%20most%20effective%20insertion%20points%20for%20AdaNCA%20based%20on%20our%0Aanalysis%20of%20AdaNCA%20placement%20and%20robustness%20improvement.%20With%20less%20than%20a%203%25%0Aincrease%20in%20parameters%2C%20AdaNCA%20contributes%20to%20more%20than%2010%25%20absolute%0Aimprovement%20in%20accuracy%20under%20adversarial%20attacks%20on%20the%20ImageNet1K%20benchmark.%0AMoreover%2C%20we%20demonstrate%20with%20extensive%20evaluations%20across%208%20robustness%0Abenchmarks%20and%204%20ViT%20architectures%20that%20AdaNCA%2C%20as%20a%20plug-in-play%20module%2C%0Aconsistently%20improves%20the%20robustness%20of%20ViTs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08298v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaNCA%253A%2520Neural%2520Cellular%2520Automata%2520As%2520Adaptors%2520For%2520More%2520Robust%2520Vision%250A%2520%2520Transformer%26entry.906535625%3DYitao%2520Xu%2520and%2520Tong%2520Zhang%2520and%2520Sabine%2520S%25C3%25BCsstrunk%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520in%2520image%250Aclassification%2520tasks%252C%2520particularly%2520when%2520equipped%2520with%2520local%2520information%2520via%250Aregion%2520attention%2520or%2520convolutions.%2520While%2520such%2520architectures%2520improve%2520the%2520feature%250Aaggregation%2520from%2520different%2520granularities%252C%2520they%2520often%2520fail%2520to%2520contribute%2520to%2520the%250Arobustness%2520of%2520the%2520networks.%2520Neural%2520Cellular%2520Automata%2520%2528NCA%2529%2520enables%2520the%2520modeling%250Aof%2520global%2520cell%2520representations%2520through%2520local%2520interactions%252C%2520with%2520its%2520training%250Astrategies%2520and%2520architecture%2520design%2520conferring%2520strong%2520generalization%2520ability%2520and%250Arobustness%2520against%2520noisy%2520inputs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Adaptor%2520Neural%250ACellular%2520Automata%2520%2528AdaNCA%2529%2520for%2520Vision%2520Transformer%2520that%2520uses%2520NCA%2520as%2520plug-in-play%250Aadaptors%2520between%2520ViT%2520layers%252C%2520enhancing%2520ViT%2527s%2520performance%2520and%2520robustness%2520against%250Aadversarial%2520samples%2520as%2520well%2520as%2520out-of-distribution%2520inputs.%2520To%2520overcome%2520the%250Alarge%2520computational%2520overhead%2520of%2520standard%2520NCAs%252C%2520we%2520propose%2520Dynamic%2520Interaction%250Afor%2520more%2520efficient%2520interaction%2520learning.%2520Furthermore%252C%2520we%2520develop%2520an%2520algorithm%250Afor%2520identifying%2520the%2520most%2520effective%2520insertion%2520points%2520for%2520AdaNCA%2520based%2520on%2520our%250Aanalysis%2520of%2520AdaNCA%2520placement%2520and%2520robustness%2520improvement.%2520With%2520less%2520than%2520a%25203%2525%250Aincrease%2520in%2520parameters%252C%2520AdaNCA%2520contributes%2520to%2520more%2520than%252010%2525%2520absolute%250Aimprovement%2520in%2520accuracy%2520under%2520adversarial%2520attacks%2520on%2520the%2520ImageNet1K%2520benchmark.%250AMoreover%252C%2520we%2520demonstrate%2520with%2520extensive%2520evaluations%2520across%25208%2520robustness%250Abenchmarks%2520and%25204%2520ViT%2520architectures%2520that%2520AdaNCA%252C%2520as%2520a%2520plug-in-play%2520module%252C%250Aconsistently%2520improves%2520the%2520robustness%2520of%2520ViTs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08298v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaNCA%3A%20Neural%20Cellular%20Automata%20As%20Adaptors%20For%20More%20Robust%20Vision%0A%20%20Transformer&entry.906535625=Yitao%20Xu%20and%20Tong%20Zhang%20and%20Sabine%20S%C3%BCsstrunk&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20remarkable%20performance%20in%20image%0Aclassification%20tasks%2C%20particularly%20when%20equipped%20with%20local%20information%20via%0Aregion%20attention%20or%20convolutions.%20While%20such%20architectures%20improve%20the%20feature%0Aaggregation%20from%20different%20granularities%2C%20they%20often%20fail%20to%20contribute%20to%20the%0Arobustness%20of%20the%20networks.%20Neural%20Cellular%20Automata%20%28NCA%29%20enables%20the%20modeling%0Aof%20global%20cell%20representations%20through%20local%20interactions%2C%20with%20its%20training%0Astrategies%20and%20architecture%20design%20conferring%20strong%20generalization%20ability%20and%0Arobustness%20against%20noisy%20inputs.%20In%20this%20paper%2C%20we%20propose%20Adaptor%20Neural%0ACellular%20Automata%20%28AdaNCA%29%20for%20Vision%20Transformer%20that%20uses%20NCA%20as%20plug-in-play%0Aadaptors%20between%20ViT%20layers%2C%20enhancing%20ViT%27s%20performance%20and%20robustness%20against%0Aadversarial%20samples%20as%20well%20as%20out-of-distribution%20inputs.%20To%20overcome%20the%0Alarge%20computational%20overhead%20of%20standard%20NCAs%2C%20we%20propose%20Dynamic%20Interaction%0Afor%20more%20efficient%20interaction%20learning.%20Furthermore%2C%20we%20develop%20an%20algorithm%0Afor%20identifying%20the%20most%20effective%20insertion%20points%20for%20AdaNCA%20based%20on%20our%0Aanalysis%20of%20AdaNCA%20placement%20and%20robustness%20improvement.%20With%20less%20than%20a%203%25%0Aincrease%20in%20parameters%2C%20AdaNCA%20contributes%20to%20more%20than%2010%25%20absolute%0Aimprovement%20in%20accuracy%20under%20adversarial%20attacks%20on%20the%20ImageNet1K%20benchmark.%0AMoreover%2C%20we%20demonstrate%20with%20extensive%20evaluations%20across%208%20robustness%0Abenchmarks%20and%204%20ViT%20architectures%20that%20AdaNCA%2C%20as%20a%20plug-in-play%20module%2C%0Aconsistently%20improves%20the%20robustness%20of%20ViTs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08298v4&entry.124074799=Read"},
{"title": "EvolBA: Evolutionary Boundary Attack under Hard-label Black Box\n  condition", "author": "Ayane Tajima and Satoshi Ono", "abstract": "  Research has shown that deep neural networks (DNNs) have vulnerabilities that\ncan lead to the misrecognition of Adversarial Examples (AEs) with specifically\ndesigned perturbations. Various adversarial attack methods have been proposed\nto detect vulnerabilities under hard-label black box (HL-BB) conditions in the\nabsence of loss gradients and confidence scores.However, these methods fall\ninto local solutions because they search only local regions of the search\nspace. Therefore, this study proposes an adversarial attack method named EvolBA\nto generate AEs using Covariance Matrix Adaptation Evolution Strategy (CMA-ES)\nunder the HL-BB condition, where only a class label predicted by the target DNN\nmodel is available. Inspired by formula-driven supervised learning, the\nproposed method introduces domain-independent operators for the initialization\nprocess and a jump that enhances search exploration. Experimental results\nconfirmed that the proposed method could determine AEs with smaller\nperturbations than previous methods in images where the previous methods have\ndifficulty.\n", "link": "http://arxiv.org/abs/2407.02248v3", "date": "2024-07-09", "relevancy": 2.0708, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5301}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvolBA%3A%20Evolutionary%20Boundary%20Attack%20under%20Hard-label%20Black%20Box%0A%20%20condition&body=Title%3A%20EvolBA%3A%20Evolutionary%20Boundary%20Attack%20under%20Hard-label%20Black%20Box%0A%20%20condition%0AAuthor%3A%20Ayane%20Tajima%20and%20Satoshi%20Ono%0AAbstract%3A%20%20%20Research%20has%20shown%20that%20deep%20neural%20networks%20%28DNNs%29%20have%20vulnerabilities%20that%0Acan%20lead%20to%20the%20misrecognition%20of%20Adversarial%20Examples%20%28AEs%29%20with%20specifically%0Adesigned%20perturbations.%20Various%20adversarial%20attack%20methods%20have%20been%20proposed%0Ato%20detect%20vulnerabilities%20under%20hard-label%20black%20box%20%28HL-BB%29%20conditions%20in%20the%0Aabsence%20of%20loss%20gradients%20and%20confidence%20scores.However%2C%20these%20methods%20fall%0Ainto%20local%20solutions%20because%20they%20search%20only%20local%20regions%20of%20the%20search%0Aspace.%20Therefore%2C%20this%20study%20proposes%20an%20adversarial%20attack%20method%20named%20EvolBA%0Ato%20generate%20AEs%20using%20Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%20%28CMA-ES%29%0Aunder%20the%20HL-BB%20condition%2C%20where%20only%20a%20class%20label%20predicted%20by%20the%20target%20DNN%0Amodel%20is%20available.%20Inspired%20by%20formula-driven%20supervised%20learning%2C%20the%0Aproposed%20method%20introduces%20domain-independent%20operators%20for%20the%20initialization%0Aprocess%20and%20a%20jump%20that%20enhances%20search%20exploration.%20Experimental%20results%0Aconfirmed%20that%20the%20proposed%20method%20could%20determine%20AEs%20with%20smaller%0Aperturbations%20than%20previous%20methods%20in%20images%20where%20the%20previous%20methods%20have%0Adifficulty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02248v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolBA%253A%2520Evolutionary%2520Boundary%2520Attack%2520under%2520Hard-label%2520Black%2520Box%250A%2520%2520condition%26entry.906535625%3DAyane%2520Tajima%2520and%2520Satoshi%2520Ono%26entry.1292438233%3D%2520%2520Research%2520has%2520shown%2520that%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520vulnerabilities%2520that%250Acan%2520lead%2520to%2520the%2520misrecognition%2520of%2520Adversarial%2520Examples%2520%2528AEs%2529%2520with%2520specifically%250Adesigned%2520perturbations.%2520Various%2520adversarial%2520attack%2520methods%2520have%2520been%2520proposed%250Ato%2520detect%2520vulnerabilities%2520under%2520hard-label%2520black%2520box%2520%2528HL-BB%2529%2520conditions%2520in%2520the%250Aabsence%2520of%2520loss%2520gradients%2520and%2520confidence%2520scores.However%252C%2520these%2520methods%2520fall%250Ainto%2520local%2520solutions%2520because%2520they%2520search%2520only%2520local%2520regions%2520of%2520the%2520search%250Aspace.%2520Therefore%252C%2520this%2520study%2520proposes%2520an%2520adversarial%2520attack%2520method%2520named%2520EvolBA%250Ato%2520generate%2520AEs%2520using%2520Covariance%2520Matrix%2520Adaptation%2520Evolution%2520Strategy%2520%2528CMA-ES%2529%250Aunder%2520the%2520HL-BB%2520condition%252C%2520where%2520only%2520a%2520class%2520label%2520predicted%2520by%2520the%2520target%2520DNN%250Amodel%2520is%2520available.%2520Inspired%2520by%2520formula-driven%2520supervised%2520learning%252C%2520the%250Aproposed%2520method%2520introduces%2520domain-independent%2520operators%2520for%2520the%2520initialization%250Aprocess%2520and%2520a%2520jump%2520that%2520enhances%2520search%2520exploration.%2520Experimental%2520results%250Aconfirmed%2520that%2520the%2520proposed%2520method%2520could%2520determine%2520AEs%2520with%2520smaller%250Aperturbations%2520than%2520previous%2520methods%2520in%2520images%2520where%2520the%2520previous%2520methods%2520have%250Adifficulty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02248v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvolBA%3A%20Evolutionary%20Boundary%20Attack%20under%20Hard-label%20Black%20Box%0A%20%20condition&entry.906535625=Ayane%20Tajima%20and%20Satoshi%20Ono&entry.1292438233=%20%20Research%20has%20shown%20that%20deep%20neural%20networks%20%28DNNs%29%20have%20vulnerabilities%20that%0Acan%20lead%20to%20the%20misrecognition%20of%20Adversarial%20Examples%20%28AEs%29%20with%20specifically%0Adesigned%20perturbations.%20Various%20adversarial%20attack%20methods%20have%20been%20proposed%0Ato%20detect%20vulnerabilities%20under%20hard-label%20black%20box%20%28HL-BB%29%20conditions%20in%20the%0Aabsence%20of%20loss%20gradients%20and%20confidence%20scores.However%2C%20these%20methods%20fall%0Ainto%20local%20solutions%20because%20they%20search%20only%20local%20regions%20of%20the%20search%0Aspace.%20Therefore%2C%20this%20study%20proposes%20an%20adversarial%20attack%20method%20named%20EvolBA%0Ato%20generate%20AEs%20using%20Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%20%28CMA-ES%29%0Aunder%20the%20HL-BB%20condition%2C%20where%20only%20a%20class%20label%20predicted%20by%20the%20target%20DNN%0Amodel%20is%20available.%20Inspired%20by%20formula-driven%20supervised%20learning%2C%20the%0Aproposed%20method%20introduces%20domain-independent%20operators%20for%20the%20initialization%0Aprocess%20and%20a%20jump%20that%20enhances%20search%20exploration.%20Experimental%20results%0Aconfirmed%20that%20the%20proposed%20method%20could%20determine%20AEs%20with%20smaller%0Aperturbations%20than%20previous%20methods%20in%20images%20where%20the%20previous%20methods%20have%0Adifficulty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02248v3&entry.124074799=Read"},
{"title": "CARL: Congestion-Aware Reinforcement Learning for Imitation-based\n  Perturbations in Mixed Traffic Control", "author": "Bibek Poudel and Weizi Li and Shuai Li", "abstract": "  Human-driven vehicles (HVs) exhibit complex and diverse behaviors. Accurately\nmodeling such behavior is crucial for validating Robot Vehicles (RVs) in\nsimulation and realizing the potential of mixed traffic control. However,\nexisting approaches like parameterized models and data-driven techniques\nstruggle to capture the full complexity and diversity. To address this, in this\nwork, we introduce CARL, a hybrid approach that combines imitation learning for\nclose proximity car-following and probabilistic sampling for larger headways.\nWe also propose two classes of RL-based RVs: a safety RV focused on maximizing\nsafety and an efficiency RV focused on maximizing efficiency. Our experiments\nshow that the safety RV increases Time-to-Collision above the critical 4-second\nthreshold and reduces Deceleration Rate to Avoid a Crash by up to 80%, while\nthe efficiency RV achieves improvements in throughput of up to 49%. These\nresults demonstrate the effectiveness of CARL in enhancing both safety and\nefficiency in mixed traffic.\n", "link": "http://arxiv.org/abs/2404.00796v2", "date": "2024-07-09", "relevancy": 2.0706, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5378}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5225}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARL%3A%20Congestion-Aware%20Reinforcement%20Learning%20for%20Imitation-based%0A%20%20Perturbations%20in%20Mixed%20Traffic%20Control&body=Title%3A%20CARL%3A%20Congestion-Aware%20Reinforcement%20Learning%20for%20Imitation-based%0A%20%20Perturbations%20in%20Mixed%20Traffic%20Control%0AAuthor%3A%20Bibek%20Poudel%20and%20Weizi%20Li%20and%20Shuai%20Li%0AAbstract%3A%20%20%20Human-driven%20vehicles%20%28HVs%29%20exhibit%20complex%20and%20diverse%20behaviors.%20Accurately%0Amodeling%20such%20behavior%20is%20crucial%20for%20validating%20Robot%20Vehicles%20%28RVs%29%20in%0Asimulation%20and%20realizing%20the%20potential%20of%20mixed%20traffic%20control.%20However%2C%0Aexisting%20approaches%20like%20parameterized%20models%20and%20data-driven%20techniques%0Astruggle%20to%20capture%20the%20full%20complexity%20and%20diversity.%20To%20address%20this%2C%20in%20this%0Awork%2C%20we%20introduce%20CARL%2C%20a%20hybrid%20approach%20that%20combines%20imitation%20learning%20for%0Aclose%20proximity%20car-following%20and%20probabilistic%20sampling%20for%20larger%20headways.%0AWe%20also%20propose%20two%20classes%20of%20RL-based%20RVs%3A%20a%20safety%20RV%20focused%20on%20maximizing%0Asafety%20and%20an%20efficiency%20RV%20focused%20on%20maximizing%20efficiency.%20Our%20experiments%0Ashow%20that%20the%20safety%20RV%20increases%20Time-to-Collision%20above%20the%20critical%204-second%0Athreshold%20and%20reduces%20Deceleration%20Rate%20to%20Avoid%20a%20Crash%20by%20up%20to%2080%25%2C%20while%0Athe%20efficiency%20RV%20achieves%20improvements%20in%20throughput%20of%20up%20to%2049%25.%20These%0Aresults%20demonstrate%20the%20effectiveness%20of%20CARL%20in%20enhancing%20both%20safety%20and%0Aefficiency%20in%20mixed%20traffic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00796v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARL%253A%2520Congestion-Aware%2520Reinforcement%2520Learning%2520for%2520Imitation-based%250A%2520%2520Perturbations%2520in%2520Mixed%2520Traffic%2520Control%26entry.906535625%3DBibek%2520Poudel%2520and%2520Weizi%2520Li%2520and%2520Shuai%2520Li%26entry.1292438233%3D%2520%2520Human-driven%2520vehicles%2520%2528HVs%2529%2520exhibit%2520complex%2520and%2520diverse%2520behaviors.%2520Accurately%250Amodeling%2520such%2520behavior%2520is%2520crucial%2520for%2520validating%2520Robot%2520Vehicles%2520%2528RVs%2529%2520in%250Asimulation%2520and%2520realizing%2520the%2520potential%2520of%2520mixed%2520traffic%2520control.%2520However%252C%250Aexisting%2520approaches%2520like%2520parameterized%2520models%2520and%2520data-driven%2520techniques%250Astruggle%2520to%2520capture%2520the%2520full%2520complexity%2520and%2520diversity.%2520To%2520address%2520this%252C%2520in%2520this%250Awork%252C%2520we%2520introduce%2520CARL%252C%2520a%2520hybrid%2520approach%2520that%2520combines%2520imitation%2520learning%2520for%250Aclose%2520proximity%2520car-following%2520and%2520probabilistic%2520sampling%2520for%2520larger%2520headways.%250AWe%2520also%2520propose%2520two%2520classes%2520of%2520RL-based%2520RVs%253A%2520a%2520safety%2520RV%2520focused%2520on%2520maximizing%250Asafety%2520and%2520an%2520efficiency%2520RV%2520focused%2520on%2520maximizing%2520efficiency.%2520Our%2520experiments%250Ashow%2520that%2520the%2520safety%2520RV%2520increases%2520Time-to-Collision%2520above%2520the%2520critical%25204-second%250Athreshold%2520and%2520reduces%2520Deceleration%2520Rate%2520to%2520Avoid%2520a%2520Crash%2520by%2520up%2520to%252080%2525%252C%2520while%250Athe%2520efficiency%2520RV%2520achieves%2520improvements%2520in%2520throughput%2520of%2520up%2520to%252049%2525.%2520These%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520CARL%2520in%2520enhancing%2520both%2520safety%2520and%250Aefficiency%2520in%2520mixed%2520traffic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00796v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARL%3A%20Congestion-Aware%20Reinforcement%20Learning%20for%20Imitation-based%0A%20%20Perturbations%20in%20Mixed%20Traffic%20Control&entry.906535625=Bibek%20Poudel%20and%20Weizi%20Li%20and%20Shuai%20Li&entry.1292438233=%20%20Human-driven%20vehicles%20%28HVs%29%20exhibit%20complex%20and%20diverse%20behaviors.%20Accurately%0Amodeling%20such%20behavior%20is%20crucial%20for%20validating%20Robot%20Vehicles%20%28RVs%29%20in%0Asimulation%20and%20realizing%20the%20potential%20of%20mixed%20traffic%20control.%20However%2C%0Aexisting%20approaches%20like%20parameterized%20models%20and%20data-driven%20techniques%0Astruggle%20to%20capture%20the%20full%20complexity%20and%20diversity.%20To%20address%20this%2C%20in%20this%0Awork%2C%20we%20introduce%20CARL%2C%20a%20hybrid%20approach%20that%20combines%20imitation%20learning%20for%0Aclose%20proximity%20car-following%20and%20probabilistic%20sampling%20for%20larger%20headways.%0AWe%20also%20propose%20two%20classes%20of%20RL-based%20RVs%3A%20a%20safety%20RV%20focused%20on%20maximizing%0Asafety%20and%20an%20efficiency%20RV%20focused%20on%20maximizing%20efficiency.%20Our%20experiments%0Ashow%20that%20the%20safety%20RV%20increases%20Time-to-Collision%20above%20the%20critical%204-second%0Athreshold%20and%20reduces%20Deceleration%20Rate%20to%20Avoid%20a%20Crash%20by%20up%20to%2080%25%2C%20while%0Athe%20efficiency%20RV%20achieves%20improvements%20in%20throughput%20of%20up%20to%2049%25.%20These%0Aresults%20demonstrate%20the%20effectiveness%20of%20CARL%20in%20enhancing%20both%20safety%20and%0Aefficiency%20in%20mixed%20traffic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00796v2&entry.124074799=Read"},
{"title": "FE-GUT: Factor Graph Optimization hybrid with Extended Kalman Filter for\n  tightly coupled GNSS/UWB Integration", "author": "Qijia Zhao and Shaolin L\u00fc and Jianan Lou and Rong Zhang", "abstract": "  Precise positioning and navigation information has been increasingly\nimportant with the development of the consumer electronics market. Due to some\ndeficits of Global Navigation Satellite System (GNSS), such as susceptible to\ninterferences, integrating of GNSS with additional alternative sensors is a\npromising approach to overcome the performance limitations of GNSS-based\nlocalization systems. Ultra-Wideband (UWB) can be used to enhance GNSS in\nconstructing an integrated localization system. However, most low-cost UWB\ndevices lack a hardware-level time synchronization feature, which necessitates\nthe estimation and compensation of the time-offset in the tightly coupled\nGNSS/UWB integration. Given the flexibility of probabilistic graphical models,\nthe time-offset can be modeled as an invariant constant in the discretization\nof the continuous model. This work proposes a novel architecture in which\nFactor Graph Optimization (FGO) is hybrid with Extend Kalman Filter (EKF) for\ntightly coupled GNSS/UWB integration with online Temporal calibration (FE-GUT).\nFGO is utilized to precisely estimate the time-offset, while EKF provides\ninitailization for the new factors and performs time-offset compensation.\nSimulation-based experiments validate the integrated localization performance\nof FE-GUT. In a four-wheeled robot scenario, the results demonstrate that,\ncompared to EKF, FE-GUT can improve horizontal and vertical localization\naccuracy by 58.59\\% and 34.80\\%, respectively, while the time-offset estimation\naccuracy is improved by 76.80\\%. All the source codes and datasets can be\ngotten via https://github.com/zhaoqj23/FE-GUT/.\n", "link": "http://arxiv.org/abs/2407.06915v1", "date": "2024-07-09", "relevancy": 2.0675, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5505}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4975}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FE-GUT%3A%20Factor%20Graph%20Optimization%20hybrid%20with%20Extended%20Kalman%20Filter%20for%0A%20%20tightly%20coupled%20GNSS/UWB%20Integration&body=Title%3A%20FE-GUT%3A%20Factor%20Graph%20Optimization%20hybrid%20with%20Extended%20Kalman%20Filter%20for%0A%20%20tightly%20coupled%20GNSS/UWB%20Integration%0AAuthor%3A%20Qijia%20Zhao%20and%20Shaolin%20L%C3%BC%20and%20Jianan%20Lou%20and%20Rong%20Zhang%0AAbstract%3A%20%20%20Precise%20positioning%20and%20navigation%20information%20has%20been%20increasingly%0Aimportant%20with%20the%20development%20of%20the%20consumer%20electronics%20market.%20Due%20to%20some%0Adeficits%20of%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%2C%20such%20as%20susceptible%20to%0Ainterferences%2C%20integrating%20of%20GNSS%20with%20additional%20alternative%20sensors%20is%20a%0Apromising%20approach%20to%20overcome%20the%20performance%20limitations%20of%20GNSS-based%0Alocalization%20systems.%20Ultra-Wideband%20%28UWB%29%20can%20be%20used%20to%20enhance%20GNSS%20in%0Aconstructing%20an%20integrated%20localization%20system.%20However%2C%20most%20low-cost%20UWB%0Adevices%20lack%20a%20hardware-level%20time%20synchronization%20feature%2C%20which%20necessitates%0Athe%20estimation%20and%20compensation%20of%20the%20time-offset%20in%20the%20tightly%20coupled%0AGNSS/UWB%20integration.%20Given%20the%20flexibility%20of%20probabilistic%20graphical%20models%2C%0Athe%20time-offset%20can%20be%20modeled%20as%20an%20invariant%20constant%20in%20the%20discretization%0Aof%20the%20continuous%20model.%20This%20work%20proposes%20a%20novel%20architecture%20in%20which%0AFactor%20Graph%20Optimization%20%28FGO%29%20is%20hybrid%20with%20Extend%20Kalman%20Filter%20%28EKF%29%20for%0Atightly%20coupled%20GNSS/UWB%20integration%20with%20online%20Temporal%20calibration%20%28FE-GUT%29.%0AFGO%20is%20utilized%20to%20precisely%20estimate%20the%20time-offset%2C%20while%20EKF%20provides%0Ainitailization%20for%20the%20new%20factors%20and%20performs%20time-offset%20compensation.%0ASimulation-based%20experiments%20validate%20the%20integrated%20localization%20performance%0Aof%20FE-GUT.%20In%20a%20four-wheeled%20robot%20scenario%2C%20the%20results%20demonstrate%20that%2C%0Acompared%20to%20EKF%2C%20FE-GUT%20can%20improve%20horizontal%20and%20vertical%20localization%0Aaccuracy%20by%2058.59%5C%25%20and%2034.80%5C%25%2C%20respectively%2C%20while%20the%20time-offset%20estimation%0Aaccuracy%20is%20improved%20by%2076.80%5C%25.%20All%20the%20source%20codes%20and%20datasets%20can%20be%0Agotten%20via%20https%3A//github.com/zhaoqj23/FE-GUT/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFE-GUT%253A%2520Factor%2520Graph%2520Optimization%2520hybrid%2520with%2520Extended%2520Kalman%2520Filter%2520for%250A%2520%2520tightly%2520coupled%2520GNSS/UWB%2520Integration%26entry.906535625%3DQijia%2520Zhao%2520and%2520Shaolin%2520L%25C3%25BC%2520and%2520Jianan%2520Lou%2520and%2520Rong%2520Zhang%26entry.1292438233%3D%2520%2520Precise%2520positioning%2520and%2520navigation%2520information%2520has%2520been%2520increasingly%250Aimportant%2520with%2520the%2520development%2520of%2520the%2520consumer%2520electronics%2520market.%2520Due%2520to%2520some%250Adeficits%2520of%2520Global%2520Navigation%2520Satellite%2520System%2520%2528GNSS%2529%252C%2520such%2520as%2520susceptible%2520to%250Ainterferences%252C%2520integrating%2520of%2520GNSS%2520with%2520additional%2520alternative%2520sensors%2520is%2520a%250Apromising%2520approach%2520to%2520overcome%2520the%2520performance%2520limitations%2520of%2520GNSS-based%250Alocalization%2520systems.%2520Ultra-Wideband%2520%2528UWB%2529%2520can%2520be%2520used%2520to%2520enhance%2520GNSS%2520in%250Aconstructing%2520an%2520integrated%2520localization%2520system.%2520However%252C%2520most%2520low-cost%2520UWB%250Adevices%2520lack%2520a%2520hardware-level%2520time%2520synchronization%2520feature%252C%2520which%2520necessitates%250Athe%2520estimation%2520and%2520compensation%2520of%2520the%2520time-offset%2520in%2520the%2520tightly%2520coupled%250AGNSS/UWB%2520integration.%2520Given%2520the%2520flexibility%2520of%2520probabilistic%2520graphical%2520models%252C%250Athe%2520time-offset%2520can%2520be%2520modeled%2520as%2520an%2520invariant%2520constant%2520in%2520the%2520discretization%250Aof%2520the%2520continuous%2520model.%2520This%2520work%2520proposes%2520a%2520novel%2520architecture%2520in%2520which%250AFactor%2520Graph%2520Optimization%2520%2528FGO%2529%2520is%2520hybrid%2520with%2520Extend%2520Kalman%2520Filter%2520%2528EKF%2529%2520for%250Atightly%2520coupled%2520GNSS/UWB%2520integration%2520with%2520online%2520Temporal%2520calibration%2520%2528FE-GUT%2529.%250AFGO%2520is%2520utilized%2520to%2520precisely%2520estimate%2520the%2520time-offset%252C%2520while%2520EKF%2520provides%250Ainitailization%2520for%2520the%2520new%2520factors%2520and%2520performs%2520time-offset%2520compensation.%250ASimulation-based%2520experiments%2520validate%2520the%2520integrated%2520localization%2520performance%250Aof%2520FE-GUT.%2520In%2520a%2520four-wheeled%2520robot%2520scenario%252C%2520the%2520results%2520demonstrate%2520that%252C%250Acompared%2520to%2520EKF%252C%2520FE-GUT%2520can%2520improve%2520horizontal%2520and%2520vertical%2520localization%250Aaccuracy%2520by%252058.59%255C%2525%2520and%252034.80%255C%2525%252C%2520respectively%252C%2520while%2520the%2520time-offset%2520estimation%250Aaccuracy%2520is%2520improved%2520by%252076.80%255C%2525.%2520All%2520the%2520source%2520codes%2520and%2520datasets%2520can%2520be%250Agotten%2520via%2520https%253A//github.com/zhaoqj23/FE-GUT/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FE-GUT%3A%20Factor%20Graph%20Optimization%20hybrid%20with%20Extended%20Kalman%20Filter%20for%0A%20%20tightly%20coupled%20GNSS/UWB%20Integration&entry.906535625=Qijia%20Zhao%20and%20Shaolin%20L%C3%BC%20and%20Jianan%20Lou%20and%20Rong%20Zhang&entry.1292438233=%20%20Precise%20positioning%20and%20navigation%20information%20has%20been%20increasingly%0Aimportant%20with%20the%20development%20of%20the%20consumer%20electronics%20market.%20Due%20to%20some%0Adeficits%20of%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%2C%20such%20as%20susceptible%20to%0Ainterferences%2C%20integrating%20of%20GNSS%20with%20additional%20alternative%20sensors%20is%20a%0Apromising%20approach%20to%20overcome%20the%20performance%20limitations%20of%20GNSS-based%0Alocalization%20systems.%20Ultra-Wideband%20%28UWB%29%20can%20be%20used%20to%20enhance%20GNSS%20in%0Aconstructing%20an%20integrated%20localization%20system.%20However%2C%20most%20low-cost%20UWB%0Adevices%20lack%20a%20hardware-level%20time%20synchronization%20feature%2C%20which%20necessitates%0Athe%20estimation%20and%20compensation%20of%20the%20time-offset%20in%20the%20tightly%20coupled%0AGNSS/UWB%20integration.%20Given%20the%20flexibility%20of%20probabilistic%20graphical%20models%2C%0Athe%20time-offset%20can%20be%20modeled%20as%20an%20invariant%20constant%20in%20the%20discretization%0Aof%20the%20continuous%20model.%20This%20work%20proposes%20a%20novel%20architecture%20in%20which%0AFactor%20Graph%20Optimization%20%28FGO%29%20is%20hybrid%20with%20Extend%20Kalman%20Filter%20%28EKF%29%20for%0Atightly%20coupled%20GNSS/UWB%20integration%20with%20online%20Temporal%20calibration%20%28FE-GUT%29.%0AFGO%20is%20utilized%20to%20precisely%20estimate%20the%20time-offset%2C%20while%20EKF%20provides%0Ainitailization%20for%20the%20new%20factors%20and%20performs%20time-offset%20compensation.%0ASimulation-based%20experiments%20validate%20the%20integrated%20localization%20performance%0Aof%20FE-GUT.%20In%20a%20four-wheeled%20robot%20scenario%2C%20the%20results%20demonstrate%20that%2C%0Acompared%20to%20EKF%2C%20FE-GUT%20can%20improve%20horizontal%20and%20vertical%20localization%0Aaccuracy%20by%2058.59%5C%25%20and%2034.80%5C%25%2C%20respectively%2C%20while%20the%20time-offset%20estimation%0Aaccuracy%20is%20improved%20by%2076.80%5C%25.%20All%20the%20source%20codes%20and%20datasets%20can%20be%0Agotten%20via%20https%3A//github.com/zhaoqj23/FE-GUT/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06915v1&entry.124074799=Read"},
{"title": "Lorentz-Equivariant Geometric Algebra Transformers for High-Energy\n  Physics", "author": "Jonas Spinner and Victor Bres\u00f3 and Pim de Haan and Tilman Plehn and Jesse Thaler and Johann Brehmer", "abstract": "  Extracting scientific understanding from particle-physics experiments\nrequires solving diverse learning problems with high precision and good data\nefficiency. We propose the Lorentz Geometric Algebra Transformer (L-GATr), a\nnew multi-purpose architecture for high-energy physics. L-GATr represents\nhigh-energy data in a geometric algebra over four-dimensional space-time and is\nequivariant under Lorentz transformations, the symmetry group of relativistic\nkinematics. At the same time, the architecture is a Transformer, which makes it\nversatile and scalable to large systems. L-GATr is first demonstrated on\nregression and classification tasks from particle physics. We then construct\nthe first Lorentz-equivariant generative model: a continuous normalizing flow\nbased on an L-GATr network, trained with Riemannian flow matching. Across our\nexperiments, L-GATr is on par with or outperforms strong domain-specific\nbaselines.\n", "link": "http://arxiv.org/abs/2405.14806v2", "date": "2024-07-09", "relevancy": 2.061, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5851}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5093}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lorentz-Equivariant%20Geometric%20Algebra%20Transformers%20for%20High-Energy%0A%20%20Physics&body=Title%3A%20Lorentz-Equivariant%20Geometric%20Algebra%20Transformers%20for%20High-Energy%0A%20%20Physics%0AAuthor%3A%20Jonas%20Spinner%20and%20Victor%20Bres%C3%B3%20and%20Pim%20de%20Haan%20and%20Tilman%20Plehn%20and%20Jesse%20Thaler%20and%20Johann%20Brehmer%0AAbstract%3A%20%20%20Extracting%20scientific%20understanding%20from%20particle-physics%20experiments%0Arequires%20solving%20diverse%20learning%20problems%20with%20high%20precision%20and%20good%20data%0Aefficiency.%20We%20propose%20the%20Lorentz%20Geometric%20Algebra%20Transformer%20%28L-GATr%29%2C%20a%0Anew%20multi-purpose%20architecture%20for%20high-energy%20physics.%20L-GATr%20represents%0Ahigh-energy%20data%20in%20a%20geometric%20algebra%20over%20four-dimensional%20space-time%20and%20is%0Aequivariant%20under%20Lorentz%20transformations%2C%20the%20symmetry%20group%20of%20relativistic%0Akinematics.%20At%20the%20same%20time%2C%20the%20architecture%20is%20a%20Transformer%2C%20which%20makes%20it%0Aversatile%20and%20scalable%20to%20large%20systems.%20L-GATr%20is%20first%20demonstrated%20on%0Aregression%20and%20classification%20tasks%20from%20particle%20physics.%20We%20then%20construct%0Athe%20first%20Lorentz-equivariant%20generative%20model%3A%20a%20continuous%20normalizing%20flow%0Abased%20on%20an%20L-GATr%20network%2C%20trained%20with%20Riemannian%20flow%20matching.%20Across%20our%0Aexperiments%2C%20L-GATr%20is%20on%20par%20with%20or%20outperforms%20strong%20domain-specific%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14806v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLorentz-Equivariant%2520Geometric%2520Algebra%2520Transformers%2520for%2520High-Energy%250A%2520%2520Physics%26entry.906535625%3DJonas%2520Spinner%2520and%2520Victor%2520Bres%25C3%25B3%2520and%2520Pim%2520de%2520Haan%2520and%2520Tilman%2520Plehn%2520and%2520Jesse%2520Thaler%2520and%2520Johann%2520Brehmer%26entry.1292438233%3D%2520%2520Extracting%2520scientific%2520understanding%2520from%2520particle-physics%2520experiments%250Arequires%2520solving%2520diverse%2520learning%2520problems%2520with%2520high%2520precision%2520and%2520good%2520data%250Aefficiency.%2520We%2520propose%2520the%2520Lorentz%2520Geometric%2520Algebra%2520Transformer%2520%2528L-GATr%2529%252C%2520a%250Anew%2520multi-purpose%2520architecture%2520for%2520high-energy%2520physics.%2520L-GATr%2520represents%250Ahigh-energy%2520data%2520in%2520a%2520geometric%2520algebra%2520over%2520four-dimensional%2520space-time%2520and%2520is%250Aequivariant%2520under%2520Lorentz%2520transformations%252C%2520the%2520symmetry%2520group%2520of%2520relativistic%250Akinematics.%2520At%2520the%2520same%2520time%252C%2520the%2520architecture%2520is%2520a%2520Transformer%252C%2520which%2520makes%2520it%250Aversatile%2520and%2520scalable%2520to%2520large%2520systems.%2520L-GATr%2520is%2520first%2520demonstrated%2520on%250Aregression%2520and%2520classification%2520tasks%2520from%2520particle%2520physics.%2520We%2520then%2520construct%250Athe%2520first%2520Lorentz-equivariant%2520generative%2520model%253A%2520a%2520continuous%2520normalizing%2520flow%250Abased%2520on%2520an%2520L-GATr%2520network%252C%2520trained%2520with%2520Riemannian%2520flow%2520matching.%2520Across%2520our%250Aexperiments%252C%2520L-GATr%2520is%2520on%2520par%2520with%2520or%2520outperforms%2520strong%2520domain-specific%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14806v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lorentz-Equivariant%20Geometric%20Algebra%20Transformers%20for%20High-Energy%0A%20%20Physics&entry.906535625=Jonas%20Spinner%20and%20Victor%20Bres%C3%B3%20and%20Pim%20de%20Haan%20and%20Tilman%20Plehn%20and%20Jesse%20Thaler%20and%20Johann%20Brehmer&entry.1292438233=%20%20Extracting%20scientific%20understanding%20from%20particle-physics%20experiments%0Arequires%20solving%20diverse%20learning%20problems%20with%20high%20precision%20and%20good%20data%0Aefficiency.%20We%20propose%20the%20Lorentz%20Geometric%20Algebra%20Transformer%20%28L-GATr%29%2C%20a%0Anew%20multi-purpose%20architecture%20for%20high-energy%20physics.%20L-GATr%20represents%0Ahigh-energy%20data%20in%20a%20geometric%20algebra%20over%20four-dimensional%20space-time%20and%20is%0Aequivariant%20under%20Lorentz%20transformations%2C%20the%20symmetry%20group%20of%20relativistic%0Akinematics.%20At%20the%20same%20time%2C%20the%20architecture%20is%20a%20Transformer%2C%20which%20makes%20it%0Aversatile%20and%20scalable%20to%20large%20systems.%20L-GATr%20is%20first%20demonstrated%20on%0Aregression%20and%20classification%20tasks%20from%20particle%20physics.%20We%20then%20construct%0Athe%20first%20Lorentz-equivariant%20generative%20model%3A%20a%20continuous%20normalizing%20flow%0Abased%20on%20an%20L-GATr%20network%2C%20trained%20with%20Riemannian%20flow%20matching.%20Across%20our%0Aexperiments%2C%20L-GATr%20is%20on%20par%20with%20or%20outperforms%20strong%20domain-specific%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14806v2&entry.124074799=Read"},
{"title": "Towards physics-informed neural networks for landslide prediction", "author": "Ashok Dahal and Luigi Lombardo", "abstract": "  For decades, solutions to regional scale landslide prediction have mostly\nrelied on data-driven models, by definition, disconnected from the physics of\nthe failure mechanism. The success and spread of such tools came from the\nability to exploit proxy variables rather than explicit geotechnical ones, as\nthe latter are prohibitive to acquire over broad landscapes. Our work\nimplements a Physics Informed Neural Network (PINN) approach, thereby adding to\na standard data-driven architecture, an intermediate constraint to solve for\nthe permanent deformation typical of Newmark slope stability methods. This\ntranslates into a neural network tasked with explicitly retrieving geotechnical\nparameters from common proxy variables and then minimize a loss function with\nrespect to the available coseismic landside inventory. The results are very\npromising, because our model not only produces excellent predictive performance\nin the form of standard susceptibility output, but in the process, also\ngenerates maps of the expected geotechnical properties at a regional scale.\nSuch architecture is therefore framed to tackle coseismic landslide prediction,\nsomething that, if confirmed in other studies, could open up towards PINN-based\nnear-real-time predictions.\n", "link": "http://arxiv.org/abs/2407.06785v1", "date": "2024-07-09", "relevancy": 2.0592, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5642}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.53}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20physics-informed%20neural%20networks%20for%20landslide%20prediction&body=Title%3A%20Towards%20physics-informed%20neural%20networks%20for%20landslide%20prediction%0AAuthor%3A%20Ashok%20Dahal%20and%20Luigi%20Lombardo%0AAbstract%3A%20%20%20For%20decades%2C%20solutions%20to%20regional%20scale%20landslide%20prediction%20have%20mostly%0Arelied%20on%20data-driven%20models%2C%20by%20definition%2C%20disconnected%20from%20the%20physics%20of%0Athe%20failure%20mechanism.%20The%20success%20and%20spread%20of%20such%20tools%20came%20from%20the%0Aability%20to%20exploit%20proxy%20variables%20rather%20than%20explicit%20geotechnical%20ones%2C%20as%0Athe%20latter%20are%20prohibitive%20to%20acquire%20over%20broad%20landscapes.%20Our%20work%0Aimplements%20a%20Physics%20Informed%20Neural%20Network%20%28PINN%29%20approach%2C%20thereby%20adding%20to%0Aa%20standard%20data-driven%20architecture%2C%20an%20intermediate%20constraint%20to%20solve%20for%0Athe%20permanent%20deformation%20typical%20of%20Newmark%20slope%20stability%20methods.%20This%0Atranslates%20into%20a%20neural%20network%20tasked%20with%20explicitly%20retrieving%20geotechnical%0Aparameters%20from%20common%20proxy%20variables%20and%20then%20minimize%20a%20loss%20function%20with%0Arespect%20to%20the%20available%20coseismic%20landside%20inventory.%20The%20results%20are%20very%0Apromising%2C%20because%20our%20model%20not%20only%20produces%20excellent%20predictive%20performance%0Ain%20the%20form%20of%20standard%20susceptibility%20output%2C%20but%20in%20the%20process%2C%20also%0Agenerates%20maps%20of%20the%20expected%20geotechnical%20properties%20at%20a%20regional%20scale.%0ASuch%20architecture%20is%20therefore%20framed%20to%20tackle%20coseismic%20landslide%20prediction%2C%0Asomething%20that%2C%20if%20confirmed%20in%20other%20studies%2C%20could%20open%20up%20towards%20PINN-based%0Anear-real-time%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520physics-informed%2520neural%2520networks%2520for%2520landslide%2520prediction%26entry.906535625%3DAshok%2520Dahal%2520and%2520Luigi%2520Lombardo%26entry.1292438233%3D%2520%2520For%2520decades%252C%2520solutions%2520to%2520regional%2520scale%2520landslide%2520prediction%2520have%2520mostly%250Arelied%2520on%2520data-driven%2520models%252C%2520by%2520definition%252C%2520disconnected%2520from%2520the%2520physics%2520of%250Athe%2520failure%2520mechanism.%2520The%2520success%2520and%2520spread%2520of%2520such%2520tools%2520came%2520from%2520the%250Aability%2520to%2520exploit%2520proxy%2520variables%2520rather%2520than%2520explicit%2520geotechnical%2520ones%252C%2520as%250Athe%2520latter%2520are%2520prohibitive%2520to%2520acquire%2520over%2520broad%2520landscapes.%2520Our%2520work%250Aimplements%2520a%2520Physics%2520Informed%2520Neural%2520Network%2520%2528PINN%2529%2520approach%252C%2520thereby%2520adding%2520to%250Aa%2520standard%2520data-driven%2520architecture%252C%2520an%2520intermediate%2520constraint%2520to%2520solve%2520for%250Athe%2520permanent%2520deformation%2520typical%2520of%2520Newmark%2520slope%2520stability%2520methods.%2520This%250Atranslates%2520into%2520a%2520neural%2520network%2520tasked%2520with%2520explicitly%2520retrieving%2520geotechnical%250Aparameters%2520from%2520common%2520proxy%2520variables%2520and%2520then%2520minimize%2520a%2520loss%2520function%2520with%250Arespect%2520to%2520the%2520available%2520coseismic%2520landside%2520inventory.%2520The%2520results%2520are%2520very%250Apromising%252C%2520because%2520our%2520model%2520not%2520only%2520produces%2520excellent%2520predictive%2520performance%250Ain%2520the%2520form%2520of%2520standard%2520susceptibility%2520output%252C%2520but%2520in%2520the%2520process%252C%2520also%250Agenerates%2520maps%2520of%2520the%2520expected%2520geotechnical%2520properties%2520at%2520a%2520regional%2520scale.%250ASuch%2520architecture%2520is%2520therefore%2520framed%2520to%2520tackle%2520coseismic%2520landslide%2520prediction%252C%250Asomething%2520that%252C%2520if%2520confirmed%2520in%2520other%2520studies%252C%2520could%2520open%2520up%2520towards%2520PINN-based%250Anear-real-time%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20physics-informed%20neural%20networks%20for%20landslide%20prediction&entry.906535625=Ashok%20Dahal%20and%20Luigi%20Lombardo&entry.1292438233=%20%20For%20decades%2C%20solutions%20to%20regional%20scale%20landslide%20prediction%20have%20mostly%0Arelied%20on%20data-driven%20models%2C%20by%20definition%2C%20disconnected%20from%20the%20physics%20of%0Athe%20failure%20mechanism.%20The%20success%20and%20spread%20of%20such%20tools%20came%20from%20the%0Aability%20to%20exploit%20proxy%20variables%20rather%20than%20explicit%20geotechnical%20ones%2C%20as%0Athe%20latter%20are%20prohibitive%20to%20acquire%20over%20broad%20landscapes.%20Our%20work%0Aimplements%20a%20Physics%20Informed%20Neural%20Network%20%28PINN%29%20approach%2C%20thereby%20adding%20to%0Aa%20standard%20data-driven%20architecture%2C%20an%20intermediate%20constraint%20to%20solve%20for%0Athe%20permanent%20deformation%20typical%20of%20Newmark%20slope%20stability%20methods.%20This%0Atranslates%20into%20a%20neural%20network%20tasked%20with%20explicitly%20retrieving%20geotechnical%0Aparameters%20from%20common%20proxy%20variables%20and%20then%20minimize%20a%20loss%20function%20with%0Arespect%20to%20the%20available%20coseismic%20landside%20inventory.%20The%20results%20are%20very%0Apromising%2C%20because%20our%20model%20not%20only%20produces%20excellent%20predictive%20performance%0Ain%20the%20form%20of%20standard%20susceptibility%20output%2C%20but%20in%20the%20process%2C%20also%0Agenerates%20maps%20of%20the%20expected%20geotechnical%20properties%20at%20a%20regional%20scale.%0ASuch%20architecture%20is%20therefore%20framed%20to%20tackle%20coseismic%20landslide%20prediction%2C%0Asomething%20that%2C%20if%20confirmed%20in%20other%20studies%2C%20could%20open%20up%20towards%20PINN-based%0Anear-real-time%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06785v1&entry.124074799=Read"},
{"title": "CAPformer: Compression-Aware Pre-trained Transformer for Low-Light Image\n  Enhancement", "author": "Wang Wei and Jin Zhi", "abstract": "  Low-Light Image Enhancement (LLIE) has advanced with the surge in phone\nphotography demand, yet many existing methods neglect compression, a crucial\nconcern for resource-constrained phone photography. Most LLIE methods overlook\nthis, hindering their effectiveness. In this study, we investigate the effects\nof JPEG compression on low-light images and reveal substantial information loss\ncaused by JPEG due to widespread low pixel values in dark areas. Hence, we\npropose the Compression-Aware Pre-trained Transformer (CAPformer), employing a\nnovel pre-training strategy to learn lossless information from uncompressed\nlow-light images. Additionally, the proposed Brightness-Guided Self-Attention\n(BGSA) mechanism enhances rational information gathering. Experiments\ndemonstrate the superiority of our approach in mitigating compression effects\non LLIE, showcasing its potential for improving LLIE in resource-constrained\nscenarios.\n", "link": "http://arxiv.org/abs/2407.07056v1", "date": "2024-07-09", "relevancy": 2.0565, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5193}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5167}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAPformer%3A%20Compression-Aware%20Pre-trained%20Transformer%20for%20Low-Light%20Image%0A%20%20Enhancement&body=Title%3A%20CAPformer%3A%20Compression-Aware%20Pre-trained%20Transformer%20for%20Low-Light%20Image%0A%20%20Enhancement%0AAuthor%3A%20Wang%20Wei%20and%20Jin%20Zhi%0AAbstract%3A%20%20%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20has%20advanced%20with%20the%20surge%20in%20phone%0Aphotography%20demand%2C%20yet%20many%20existing%20methods%20neglect%20compression%2C%20a%20crucial%0Aconcern%20for%20resource-constrained%20phone%20photography.%20Most%20LLIE%20methods%20overlook%0Athis%2C%20hindering%20their%20effectiveness.%20In%20this%20study%2C%20we%20investigate%20the%20effects%0Aof%20JPEG%20compression%20on%20low-light%20images%20and%20reveal%20substantial%20information%20loss%0Acaused%20by%20JPEG%20due%20to%20widespread%20low%20pixel%20values%20in%20dark%20areas.%20Hence%2C%20we%0Apropose%20the%20Compression-Aware%20Pre-trained%20Transformer%20%28CAPformer%29%2C%20employing%20a%0Anovel%20pre-training%20strategy%20to%20learn%20lossless%20information%20from%20uncompressed%0Alow-light%20images.%20Additionally%2C%20the%20proposed%20Brightness-Guided%20Self-Attention%0A%28BGSA%29%20mechanism%20enhances%20rational%20information%20gathering.%20Experiments%0Ademonstrate%20the%20superiority%20of%20our%20approach%20in%20mitigating%20compression%20effects%0Aon%20LLIE%2C%20showcasing%20its%20potential%20for%20improving%20LLIE%20in%20resource-constrained%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAPformer%253A%2520Compression-Aware%2520Pre-trained%2520Transformer%2520for%2520Low-Light%2520Image%250A%2520%2520Enhancement%26entry.906535625%3DWang%2520Wei%2520and%2520Jin%2520Zhi%26entry.1292438233%3D%2520%2520Low-Light%2520Image%2520Enhancement%2520%2528LLIE%2529%2520has%2520advanced%2520with%2520the%2520surge%2520in%2520phone%250Aphotography%2520demand%252C%2520yet%2520many%2520existing%2520methods%2520neglect%2520compression%252C%2520a%2520crucial%250Aconcern%2520for%2520resource-constrained%2520phone%2520photography.%2520Most%2520LLIE%2520methods%2520overlook%250Athis%252C%2520hindering%2520their%2520effectiveness.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520effects%250Aof%2520JPEG%2520compression%2520on%2520low-light%2520images%2520and%2520reveal%2520substantial%2520information%2520loss%250Acaused%2520by%2520JPEG%2520due%2520to%2520widespread%2520low%2520pixel%2520values%2520in%2520dark%2520areas.%2520Hence%252C%2520we%250Apropose%2520the%2520Compression-Aware%2520Pre-trained%2520Transformer%2520%2528CAPformer%2529%252C%2520employing%2520a%250Anovel%2520pre-training%2520strategy%2520to%2520learn%2520lossless%2520information%2520from%2520uncompressed%250Alow-light%2520images.%2520Additionally%252C%2520the%2520proposed%2520Brightness-Guided%2520Self-Attention%250A%2528BGSA%2529%2520mechanism%2520enhances%2520rational%2520information%2520gathering.%2520Experiments%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520in%2520mitigating%2520compression%2520effects%250Aon%2520LLIE%252C%2520showcasing%2520its%2520potential%2520for%2520improving%2520LLIE%2520in%2520resource-constrained%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAPformer%3A%20Compression-Aware%20Pre-trained%20Transformer%20for%20Low-Light%20Image%0A%20%20Enhancement&entry.906535625=Wang%20Wei%20and%20Jin%20Zhi&entry.1292438233=%20%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20has%20advanced%20with%20the%20surge%20in%20phone%0Aphotography%20demand%2C%20yet%20many%20existing%20methods%20neglect%20compression%2C%20a%20crucial%0Aconcern%20for%20resource-constrained%20phone%20photography.%20Most%20LLIE%20methods%20overlook%0Athis%2C%20hindering%20their%20effectiveness.%20In%20this%20study%2C%20we%20investigate%20the%20effects%0Aof%20JPEG%20compression%20on%20low-light%20images%20and%20reveal%20substantial%20information%20loss%0Acaused%20by%20JPEG%20due%20to%20widespread%20low%20pixel%20values%20in%20dark%20areas.%20Hence%2C%20we%0Apropose%20the%20Compression-Aware%20Pre-trained%20Transformer%20%28CAPformer%29%2C%20employing%20a%0Anovel%20pre-training%20strategy%20to%20learn%20lossless%20information%20from%20uncompressed%0Alow-light%20images.%20Additionally%2C%20the%20proposed%20Brightness-Guided%20Self-Attention%0A%28BGSA%29%20mechanism%20enhances%20rational%20information%20gathering.%20Experiments%0Ademonstrate%20the%20superiority%20of%20our%20approach%20in%20mitigating%20compression%20effects%0Aon%20LLIE%2C%20showcasing%20its%20potential%20for%20improving%20LLIE%20in%20resource-constrained%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07056v1&entry.124074799=Read"},
{"title": "TE-SSL: Time and Event-aware Self Supervised Learning for Alzheimer's\n  Disease Progression Analysis", "author": "Jacob Thrasher and Alina Devkota and Ahmed Tafti and Binod Bhattarai and Prashnna Gyawali", "abstract": "  Alzheimer's Dementia (AD) represents one of the most pressing challenges in\nthe field of neurodegenerative disorders, with its progression analysis being\ncrucial for understanding disease dynamics and developing targeted\ninterventions. Recent advancements in deep learning and various representation\nlearning strategies, including self-supervised learning (SSL), have shown\nsignificant promise in enhancing medical image analysis, providing innovative\nways to extract meaningful patterns from complex data. Notably, the computer\nvision literature has demonstrated that incorporating supervisory signals into\nSSL can further augment model performance by guiding the learning process with\nadditional relevant information. However, the application of such supervisory\nsignals in the context of disease progression analysis remains largely\nunexplored. This gap is particularly pronounced given the inherent challenges\nof incorporating both event and time-to-event information into the learning\nparadigm. Addressing this, we propose a novel framework, Time and Even-aware\nSSL (TE-SSL), which integrates time-to-event and event data as supervisory\nsignals to refine the learning process. Our comparative analysis with existing\nSSL-based methods in the downstream task of survival analysis shows superior\nperformance across standard metrics.\n", "link": "http://arxiv.org/abs/2407.06852v1", "date": "2024-07-09", "relevancy": 2.0399, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5346}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4985}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TE-SSL%3A%20Time%20and%20Event-aware%20Self%20Supervised%20Learning%20for%20Alzheimer%27s%0A%20%20Disease%20Progression%20Analysis&body=Title%3A%20TE-SSL%3A%20Time%20and%20Event-aware%20Self%20Supervised%20Learning%20for%20Alzheimer%27s%0A%20%20Disease%20Progression%20Analysis%0AAuthor%3A%20Jacob%20Thrasher%20and%20Alina%20Devkota%20and%20Ahmed%20Tafti%20and%20Binod%20Bhattarai%20and%20Prashnna%20Gyawali%0AAbstract%3A%20%20%20Alzheimer%27s%20Dementia%20%28AD%29%20represents%20one%20of%20the%20most%20pressing%20challenges%20in%0Athe%20field%20of%20neurodegenerative%20disorders%2C%20with%20its%20progression%20analysis%20being%0Acrucial%20for%20understanding%20disease%20dynamics%20and%20developing%20targeted%0Ainterventions.%20Recent%20advancements%20in%20deep%20learning%20and%20various%20representation%0Alearning%20strategies%2C%20including%20self-supervised%20learning%20%28SSL%29%2C%20have%20shown%0Asignificant%20promise%20in%20enhancing%20medical%20image%20analysis%2C%20providing%20innovative%0Aways%20to%20extract%20meaningful%20patterns%20from%20complex%20data.%20Notably%2C%20the%20computer%0Avision%20literature%20has%20demonstrated%20that%20incorporating%20supervisory%20signals%20into%0ASSL%20can%20further%20augment%20model%20performance%20by%20guiding%20the%20learning%20process%20with%0Aadditional%20relevant%20information.%20However%2C%20the%20application%20of%20such%20supervisory%0Asignals%20in%20the%20context%20of%20disease%20progression%20analysis%20remains%20largely%0Aunexplored.%20This%20gap%20is%20particularly%20pronounced%20given%20the%20inherent%20challenges%0Aof%20incorporating%20both%20event%20and%20time-to-event%20information%20into%20the%20learning%0Aparadigm.%20Addressing%20this%2C%20we%20propose%20a%20novel%20framework%2C%20Time%20and%20Even-aware%0ASSL%20%28TE-SSL%29%2C%20which%20integrates%20time-to-event%20and%20event%20data%20as%20supervisory%0Asignals%20to%20refine%20the%20learning%20process.%20Our%20comparative%20analysis%20with%20existing%0ASSL-based%20methods%20in%20the%20downstream%20task%20of%20survival%20analysis%20shows%20superior%0Aperformance%20across%20standard%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTE-SSL%253A%2520Time%2520and%2520Event-aware%2520Self%2520Supervised%2520Learning%2520for%2520Alzheimer%2527s%250A%2520%2520Disease%2520Progression%2520Analysis%26entry.906535625%3DJacob%2520Thrasher%2520and%2520Alina%2520Devkota%2520and%2520Ahmed%2520Tafti%2520and%2520Binod%2520Bhattarai%2520and%2520Prashnna%2520Gyawali%26entry.1292438233%3D%2520%2520Alzheimer%2527s%2520Dementia%2520%2528AD%2529%2520represents%2520one%2520of%2520the%2520most%2520pressing%2520challenges%2520in%250Athe%2520field%2520of%2520neurodegenerative%2520disorders%252C%2520with%2520its%2520progression%2520analysis%2520being%250Acrucial%2520for%2520understanding%2520disease%2520dynamics%2520and%2520developing%2520targeted%250Ainterventions.%2520Recent%2520advancements%2520in%2520deep%2520learning%2520and%2520various%2520representation%250Alearning%2520strategies%252C%2520including%2520self-supervised%2520learning%2520%2528SSL%2529%252C%2520have%2520shown%250Asignificant%2520promise%2520in%2520enhancing%2520medical%2520image%2520analysis%252C%2520providing%2520innovative%250Aways%2520to%2520extract%2520meaningful%2520patterns%2520from%2520complex%2520data.%2520Notably%252C%2520the%2520computer%250Avision%2520literature%2520has%2520demonstrated%2520that%2520incorporating%2520supervisory%2520signals%2520into%250ASSL%2520can%2520further%2520augment%2520model%2520performance%2520by%2520guiding%2520the%2520learning%2520process%2520with%250Aadditional%2520relevant%2520information.%2520However%252C%2520the%2520application%2520of%2520such%2520supervisory%250Asignals%2520in%2520the%2520context%2520of%2520disease%2520progression%2520analysis%2520remains%2520largely%250Aunexplored.%2520This%2520gap%2520is%2520particularly%2520pronounced%2520given%2520the%2520inherent%2520challenges%250Aof%2520incorporating%2520both%2520event%2520and%2520time-to-event%2520information%2520into%2520the%2520learning%250Aparadigm.%2520Addressing%2520this%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520Time%2520and%2520Even-aware%250ASSL%2520%2528TE-SSL%2529%252C%2520which%2520integrates%2520time-to-event%2520and%2520event%2520data%2520as%2520supervisory%250Asignals%2520to%2520refine%2520the%2520learning%2520process.%2520Our%2520comparative%2520analysis%2520with%2520existing%250ASSL-based%2520methods%2520in%2520the%2520downstream%2520task%2520of%2520survival%2520analysis%2520shows%2520superior%250Aperformance%2520across%2520standard%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TE-SSL%3A%20Time%20and%20Event-aware%20Self%20Supervised%20Learning%20for%20Alzheimer%27s%0A%20%20Disease%20Progression%20Analysis&entry.906535625=Jacob%20Thrasher%20and%20Alina%20Devkota%20and%20Ahmed%20Tafti%20and%20Binod%20Bhattarai%20and%20Prashnna%20Gyawali&entry.1292438233=%20%20Alzheimer%27s%20Dementia%20%28AD%29%20represents%20one%20of%20the%20most%20pressing%20challenges%20in%0Athe%20field%20of%20neurodegenerative%20disorders%2C%20with%20its%20progression%20analysis%20being%0Acrucial%20for%20understanding%20disease%20dynamics%20and%20developing%20targeted%0Ainterventions.%20Recent%20advancements%20in%20deep%20learning%20and%20various%20representation%0Alearning%20strategies%2C%20including%20self-supervised%20learning%20%28SSL%29%2C%20have%20shown%0Asignificant%20promise%20in%20enhancing%20medical%20image%20analysis%2C%20providing%20innovative%0Aways%20to%20extract%20meaningful%20patterns%20from%20complex%20data.%20Notably%2C%20the%20computer%0Avision%20literature%20has%20demonstrated%20that%20incorporating%20supervisory%20signals%20into%0ASSL%20can%20further%20augment%20model%20performance%20by%20guiding%20the%20learning%20process%20with%0Aadditional%20relevant%20information.%20However%2C%20the%20application%20of%20such%20supervisory%0Asignals%20in%20the%20context%20of%20disease%20progression%20analysis%20remains%20largely%0Aunexplored.%20This%20gap%20is%20particularly%20pronounced%20given%20the%20inherent%20challenges%0Aof%20incorporating%20both%20event%20and%20time-to-event%20information%20into%20the%20learning%0Aparadigm.%20Addressing%20this%2C%20we%20propose%20a%20novel%20framework%2C%20Time%20and%20Even-aware%0ASSL%20%28TE-SSL%29%2C%20which%20integrates%20time-to-event%20and%20event%20data%20as%20supervisory%0Asignals%20to%20refine%20the%20learning%20process.%20Our%20comparative%20analysis%20with%20existing%0ASSL-based%20methods%20in%20the%20downstream%20task%20of%20survival%20analysis%20shows%20superior%0Aperformance%20across%20standard%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06852v1&entry.124074799=Read"},
{"title": "AllMatch: Exploiting All Unlabeled Data for Semi-Supervised Learning", "author": "Zhiyu Wu and Jinshi Cui", "abstract": "  Existing semi-supervised learning algorithms adopt pseudo-labeling and\nconsistency regulation techniques to introduce supervision signals for\nunlabeled samples. To overcome the inherent limitation of threshold-based\npseudo-labeling, prior studies have attempted to align the confidence threshold\nwith the evolving learning status of the model, which is estimated through the\npredictions made on the unlabeled data. In this paper, we further reveal that\nclassifier weights can reflect the differentiated learning status across\ncategories and consequently propose a class-specific adaptive threshold\nmechanism. Additionally, considering that even the optimal threshold scheme\ncannot resolve the problem of discarding unlabeled samples, a binary\nclassification consistency regulation approach is designed to distinguish\ncandidate classes from negative options for all unlabeled samples. By combining\nthe above strategies, we present a novel SSL algorithm named AllMatch, which\nachieves improved pseudo-label accuracy and a 100% utilization ratio for the\nunlabeled data. We extensively evaluate our approach on multiple benchmarks,\nencompassing both balanced and imbalanced settings. The results demonstrate\nthat AllMatch consistently outperforms existing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2406.15763v2", "date": "2024-07-09", "relevancy": 2.0319, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5293}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5145}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AllMatch%3A%20Exploiting%20All%20Unlabeled%20Data%20for%20Semi-Supervised%20Learning&body=Title%3A%20AllMatch%3A%20Exploiting%20All%20Unlabeled%20Data%20for%20Semi-Supervised%20Learning%0AAuthor%3A%20Zhiyu%20Wu%20and%20Jinshi%20Cui%0AAbstract%3A%20%20%20Existing%20semi-supervised%20learning%20algorithms%20adopt%20pseudo-labeling%20and%0Aconsistency%20regulation%20techniques%20to%20introduce%20supervision%20signals%20for%0Aunlabeled%20samples.%20To%20overcome%20the%20inherent%20limitation%20of%20threshold-based%0Apseudo-labeling%2C%20prior%20studies%20have%20attempted%20to%20align%20the%20confidence%20threshold%0Awith%20the%20evolving%20learning%20status%20of%20the%20model%2C%20which%20is%20estimated%20through%20the%0Apredictions%20made%20on%20the%20unlabeled%20data.%20In%20this%20paper%2C%20we%20further%20reveal%20that%0Aclassifier%20weights%20can%20reflect%20the%20differentiated%20learning%20status%20across%0Acategories%20and%20consequently%20propose%20a%20class-specific%20adaptive%20threshold%0Amechanism.%20Additionally%2C%20considering%20that%20even%20the%20optimal%20threshold%20scheme%0Acannot%20resolve%20the%20problem%20of%20discarding%20unlabeled%20samples%2C%20a%20binary%0Aclassification%20consistency%20regulation%20approach%20is%20designed%20to%20distinguish%0Acandidate%20classes%20from%20negative%20options%20for%20all%20unlabeled%20samples.%20By%20combining%0Athe%20above%20strategies%2C%20we%20present%20a%20novel%20SSL%20algorithm%20named%20AllMatch%2C%20which%0Aachieves%20improved%20pseudo-label%20accuracy%20and%20a%20100%25%20utilization%20ratio%20for%20the%0Aunlabeled%20data.%20We%20extensively%20evaluate%20our%20approach%20on%20multiple%20benchmarks%2C%0Aencompassing%20both%20balanced%20and%20imbalanced%20settings.%20The%20results%20demonstrate%0Athat%20AllMatch%20consistently%20outperforms%20existing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15763v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAllMatch%253A%2520Exploiting%2520All%2520Unlabeled%2520Data%2520for%2520Semi-Supervised%2520Learning%26entry.906535625%3DZhiyu%2520Wu%2520and%2520Jinshi%2520Cui%26entry.1292438233%3D%2520%2520Existing%2520semi-supervised%2520learning%2520algorithms%2520adopt%2520pseudo-labeling%2520and%250Aconsistency%2520regulation%2520techniques%2520to%2520introduce%2520supervision%2520signals%2520for%250Aunlabeled%2520samples.%2520To%2520overcome%2520the%2520inherent%2520limitation%2520of%2520threshold-based%250Apseudo-labeling%252C%2520prior%2520studies%2520have%2520attempted%2520to%2520align%2520the%2520confidence%2520threshold%250Awith%2520the%2520evolving%2520learning%2520status%2520of%2520the%2520model%252C%2520which%2520is%2520estimated%2520through%2520the%250Apredictions%2520made%2520on%2520the%2520unlabeled%2520data.%2520In%2520this%2520paper%252C%2520we%2520further%2520reveal%2520that%250Aclassifier%2520weights%2520can%2520reflect%2520the%2520differentiated%2520learning%2520status%2520across%250Acategories%2520and%2520consequently%2520propose%2520a%2520class-specific%2520adaptive%2520threshold%250Amechanism.%2520Additionally%252C%2520considering%2520that%2520even%2520the%2520optimal%2520threshold%2520scheme%250Acannot%2520resolve%2520the%2520problem%2520of%2520discarding%2520unlabeled%2520samples%252C%2520a%2520binary%250Aclassification%2520consistency%2520regulation%2520approach%2520is%2520designed%2520to%2520distinguish%250Acandidate%2520classes%2520from%2520negative%2520options%2520for%2520all%2520unlabeled%2520samples.%2520By%2520combining%250Athe%2520above%2520strategies%252C%2520we%2520present%2520a%2520novel%2520SSL%2520algorithm%2520named%2520AllMatch%252C%2520which%250Aachieves%2520improved%2520pseudo-label%2520accuracy%2520and%2520a%2520100%2525%2520utilization%2520ratio%2520for%2520the%250Aunlabeled%2520data.%2520We%2520extensively%2520evaluate%2520our%2520approach%2520on%2520multiple%2520benchmarks%252C%250Aencompassing%2520both%2520balanced%2520and%2520imbalanced%2520settings.%2520The%2520results%2520demonstrate%250Athat%2520AllMatch%2520consistently%2520outperforms%2520existing%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15763v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AllMatch%3A%20Exploiting%20All%20Unlabeled%20Data%20for%20Semi-Supervised%20Learning&entry.906535625=Zhiyu%20Wu%20and%20Jinshi%20Cui&entry.1292438233=%20%20Existing%20semi-supervised%20learning%20algorithms%20adopt%20pseudo-labeling%20and%0Aconsistency%20regulation%20techniques%20to%20introduce%20supervision%20signals%20for%0Aunlabeled%20samples.%20To%20overcome%20the%20inherent%20limitation%20of%20threshold-based%0Apseudo-labeling%2C%20prior%20studies%20have%20attempted%20to%20align%20the%20confidence%20threshold%0Awith%20the%20evolving%20learning%20status%20of%20the%20model%2C%20which%20is%20estimated%20through%20the%0Apredictions%20made%20on%20the%20unlabeled%20data.%20In%20this%20paper%2C%20we%20further%20reveal%20that%0Aclassifier%20weights%20can%20reflect%20the%20differentiated%20learning%20status%20across%0Acategories%20and%20consequently%20propose%20a%20class-specific%20adaptive%20threshold%0Amechanism.%20Additionally%2C%20considering%20that%20even%20the%20optimal%20threshold%20scheme%0Acannot%20resolve%20the%20problem%20of%20discarding%20unlabeled%20samples%2C%20a%20binary%0Aclassification%20consistency%20regulation%20approach%20is%20designed%20to%20distinguish%0Acandidate%20classes%20from%20negative%20options%20for%20all%20unlabeled%20samples.%20By%20combining%0Athe%20above%20strategies%2C%20we%20present%20a%20novel%20SSL%20algorithm%20named%20AllMatch%2C%20which%0Aachieves%20improved%20pseudo-label%20accuracy%20and%20a%20100%25%20utilization%20ratio%20for%20the%0Aunlabeled%20data.%20We%20extensively%20evaluate%20our%20approach%20on%20multiple%20benchmarks%2C%0Aencompassing%20both%20balanced%20and%20imbalanced%20settings.%20The%20results%20demonstrate%0Athat%20AllMatch%20consistently%20outperforms%20existing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15763v2&entry.124074799=Read"},
{"title": "OpenCIL: Benchmarking Out-of-Distribution Detection in Class-Incremental\n  Learning", "author": "Wenjun Miao and Guansong Pang and Trong-Tung Nguyen and Ruohang Fang and Jin Zheng and Xiao Bai", "abstract": "  Class incremental learning (CIL) aims to learn a model that can not only\nincrementally accommodate new classes, but also maintain the learned knowledge\nof old classes. Out-of-distribution (OOD) detection in CIL is to retain this\nincremental learning ability, while being able to reject unknown samples that\nare drawn from different distributions of the learned classes. This capability\nis crucial to the safety of deploying CIL models in open worlds. However,\ndespite remarkable advancements in the respective CIL and OOD detection, there\nlacks a systematic and large-scale benchmark to assess the capability of\nadvanced CIL models in detecting OOD samples. To fill this gap, in this study\nwe design a comprehensive empirical study to establish such a benchmark, named\n$\\textbf{OpenCIL}$. To this end, we propose two principled frameworks for\nenabling four representative CIL models with 15 diverse OOD detection methods,\nresulting in 60 baseline models for OOD detection in CIL. The empirical\nevaluation is performed on two popular CIL datasets with six commonly-used OOD\ndatasets. One key observation we find through our comprehensive evaluation is\nthat the CIL models can be severely biased towards the OOD samples and newly\nadded classes when they are exposed to open environments. Motivated by this, we\nfurther propose a new baseline for OOD detection in CIL, namely Bi-directional\nEnergy Regularization ($\\textbf{BER}$), which is specially designed to mitigate\nthese two biases in different CIL models by having energy regularization on\nboth old and new classes. Its superior performance is justified in our\nexperiments. All codes and datasets are open-source at\nhttps://github.com/mala-lab/OpenCIL.\n", "link": "http://arxiv.org/abs/2407.06045v2", "date": "2024-07-09", "relevancy": 2.0173, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5093}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5066}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenCIL%3A%20Benchmarking%20Out-of-Distribution%20Detection%20in%20Class-Incremental%0A%20%20Learning&body=Title%3A%20OpenCIL%3A%20Benchmarking%20Out-of-Distribution%20Detection%20in%20Class-Incremental%0A%20%20Learning%0AAuthor%3A%20Wenjun%20Miao%20and%20Guansong%20Pang%20and%20Trong-Tung%20Nguyen%20and%20Ruohang%20Fang%20and%20Jin%20Zheng%20and%20Xiao%20Bai%0AAbstract%3A%20%20%20Class%20incremental%20learning%20%28CIL%29%20aims%20to%20learn%20a%20model%20that%20can%20not%20only%0Aincrementally%20accommodate%20new%20classes%2C%20but%20also%20maintain%20the%20learned%20knowledge%0Aof%20old%20classes.%20Out-of-distribution%20%28OOD%29%20detection%20in%20CIL%20is%20to%20retain%20this%0Aincremental%20learning%20ability%2C%20while%20being%20able%20to%20reject%20unknown%20samples%20that%0Aare%20drawn%20from%20different%20distributions%20of%20the%20learned%20classes.%20This%20capability%0Ais%20crucial%20to%20the%20safety%20of%20deploying%20CIL%20models%20in%20open%20worlds.%20However%2C%0Adespite%20remarkable%20advancements%20in%20the%20respective%20CIL%20and%20OOD%20detection%2C%20there%0Alacks%20a%20systematic%20and%20large-scale%20benchmark%20to%20assess%20the%20capability%20of%0Aadvanced%20CIL%20models%20in%20detecting%20OOD%20samples.%20To%20fill%20this%20gap%2C%20in%20this%20study%0Awe%20design%20a%20comprehensive%20empirical%20study%20to%20establish%20such%20a%20benchmark%2C%20named%0A%24%5Ctextbf%7BOpenCIL%7D%24.%20To%20this%20end%2C%20we%20propose%20two%20principled%20frameworks%20for%0Aenabling%20four%20representative%20CIL%20models%20with%2015%20diverse%20OOD%20detection%20methods%2C%0Aresulting%20in%2060%20baseline%20models%20for%20OOD%20detection%20in%20CIL.%20The%20empirical%0Aevaluation%20is%20performed%20on%20two%20popular%20CIL%20datasets%20with%20six%20commonly-used%20OOD%0Adatasets.%20One%20key%20observation%20we%20find%20through%20our%20comprehensive%20evaluation%20is%0Athat%20the%20CIL%20models%20can%20be%20severely%20biased%20towards%20the%20OOD%20samples%20and%20newly%0Aadded%20classes%20when%20they%20are%20exposed%20to%20open%20environments.%20Motivated%20by%20this%2C%20we%0Afurther%20propose%20a%20new%20baseline%20for%20OOD%20detection%20in%20CIL%2C%20namely%20Bi-directional%0AEnergy%20Regularization%20%28%24%5Ctextbf%7BBER%7D%24%29%2C%20which%20is%20specially%20designed%20to%20mitigate%0Athese%20two%20biases%20in%20different%20CIL%20models%20by%20having%20energy%20regularization%20on%0Aboth%20old%20and%20new%20classes.%20Its%20superior%20performance%20is%20justified%20in%20our%0Aexperiments.%20All%20codes%20and%20datasets%20are%20open-source%20at%0Ahttps%3A//github.com/mala-lab/OpenCIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06045v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenCIL%253A%2520Benchmarking%2520Out-of-Distribution%2520Detection%2520in%2520Class-Incremental%250A%2520%2520Learning%26entry.906535625%3DWenjun%2520Miao%2520and%2520Guansong%2520Pang%2520and%2520Trong-Tung%2520Nguyen%2520and%2520Ruohang%2520Fang%2520and%2520Jin%2520Zheng%2520and%2520Xiao%2520Bai%26entry.1292438233%3D%2520%2520Class%2520incremental%2520learning%2520%2528CIL%2529%2520aims%2520to%2520learn%2520a%2520model%2520that%2520can%2520not%2520only%250Aincrementally%2520accommodate%2520new%2520classes%252C%2520but%2520also%2520maintain%2520the%2520learned%2520knowledge%250Aof%2520old%2520classes.%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520in%2520CIL%2520is%2520to%2520retain%2520this%250Aincremental%2520learning%2520ability%252C%2520while%2520being%2520able%2520to%2520reject%2520unknown%2520samples%2520that%250Aare%2520drawn%2520from%2520different%2520distributions%2520of%2520the%2520learned%2520classes.%2520This%2520capability%250Ais%2520crucial%2520to%2520the%2520safety%2520of%2520deploying%2520CIL%2520models%2520in%2520open%2520worlds.%2520However%252C%250Adespite%2520remarkable%2520advancements%2520in%2520the%2520respective%2520CIL%2520and%2520OOD%2520detection%252C%2520there%250Alacks%2520a%2520systematic%2520and%2520large-scale%2520benchmark%2520to%2520assess%2520the%2520capability%2520of%250Aadvanced%2520CIL%2520models%2520in%2520detecting%2520OOD%2520samples.%2520To%2520fill%2520this%2520gap%252C%2520in%2520this%2520study%250Awe%2520design%2520a%2520comprehensive%2520empirical%2520study%2520to%2520establish%2520such%2520a%2520benchmark%252C%2520named%250A%2524%255Ctextbf%257BOpenCIL%257D%2524.%2520To%2520this%2520end%252C%2520we%2520propose%2520two%2520principled%2520frameworks%2520for%250Aenabling%2520four%2520representative%2520CIL%2520models%2520with%252015%2520diverse%2520OOD%2520detection%2520methods%252C%250Aresulting%2520in%252060%2520baseline%2520models%2520for%2520OOD%2520detection%2520in%2520CIL.%2520The%2520empirical%250Aevaluation%2520is%2520performed%2520on%2520two%2520popular%2520CIL%2520datasets%2520with%2520six%2520commonly-used%2520OOD%250Adatasets.%2520One%2520key%2520observation%2520we%2520find%2520through%2520our%2520comprehensive%2520evaluation%2520is%250Athat%2520the%2520CIL%2520models%2520can%2520be%2520severely%2520biased%2520towards%2520the%2520OOD%2520samples%2520and%2520newly%250Aadded%2520classes%2520when%2520they%2520are%2520exposed%2520to%2520open%2520environments.%2520Motivated%2520by%2520this%252C%2520we%250Afurther%2520propose%2520a%2520new%2520baseline%2520for%2520OOD%2520detection%2520in%2520CIL%252C%2520namely%2520Bi-directional%250AEnergy%2520Regularization%2520%2528%2524%255Ctextbf%257BBER%257D%2524%2529%252C%2520which%2520is%2520specially%2520designed%2520to%2520mitigate%250Athese%2520two%2520biases%2520in%2520different%2520CIL%2520models%2520by%2520having%2520energy%2520regularization%2520on%250Aboth%2520old%2520and%2520new%2520classes.%2520Its%2520superior%2520performance%2520is%2520justified%2520in%2520our%250Aexperiments.%2520All%2520codes%2520and%2520datasets%2520are%2520open-source%2520at%250Ahttps%253A//github.com/mala-lab/OpenCIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06045v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenCIL%3A%20Benchmarking%20Out-of-Distribution%20Detection%20in%20Class-Incremental%0A%20%20Learning&entry.906535625=Wenjun%20Miao%20and%20Guansong%20Pang%20and%20Trong-Tung%20Nguyen%20and%20Ruohang%20Fang%20and%20Jin%20Zheng%20and%20Xiao%20Bai&entry.1292438233=%20%20Class%20incremental%20learning%20%28CIL%29%20aims%20to%20learn%20a%20model%20that%20can%20not%20only%0Aincrementally%20accommodate%20new%20classes%2C%20but%20also%20maintain%20the%20learned%20knowledge%0Aof%20old%20classes.%20Out-of-distribution%20%28OOD%29%20detection%20in%20CIL%20is%20to%20retain%20this%0Aincremental%20learning%20ability%2C%20while%20being%20able%20to%20reject%20unknown%20samples%20that%0Aare%20drawn%20from%20different%20distributions%20of%20the%20learned%20classes.%20This%20capability%0Ais%20crucial%20to%20the%20safety%20of%20deploying%20CIL%20models%20in%20open%20worlds.%20However%2C%0Adespite%20remarkable%20advancements%20in%20the%20respective%20CIL%20and%20OOD%20detection%2C%20there%0Alacks%20a%20systematic%20and%20large-scale%20benchmark%20to%20assess%20the%20capability%20of%0Aadvanced%20CIL%20models%20in%20detecting%20OOD%20samples.%20To%20fill%20this%20gap%2C%20in%20this%20study%0Awe%20design%20a%20comprehensive%20empirical%20study%20to%20establish%20such%20a%20benchmark%2C%20named%0A%24%5Ctextbf%7BOpenCIL%7D%24.%20To%20this%20end%2C%20we%20propose%20two%20principled%20frameworks%20for%0Aenabling%20four%20representative%20CIL%20models%20with%2015%20diverse%20OOD%20detection%20methods%2C%0Aresulting%20in%2060%20baseline%20models%20for%20OOD%20detection%20in%20CIL.%20The%20empirical%0Aevaluation%20is%20performed%20on%20two%20popular%20CIL%20datasets%20with%20six%20commonly-used%20OOD%0Adatasets.%20One%20key%20observation%20we%20find%20through%20our%20comprehensive%20evaluation%20is%0Athat%20the%20CIL%20models%20can%20be%20severely%20biased%20towards%20the%20OOD%20samples%20and%20newly%0Aadded%20classes%20when%20they%20are%20exposed%20to%20open%20environments.%20Motivated%20by%20this%2C%20we%0Afurther%20propose%20a%20new%20baseline%20for%20OOD%20detection%20in%20CIL%2C%20namely%20Bi-directional%0AEnergy%20Regularization%20%28%24%5Ctextbf%7BBER%7D%24%29%2C%20which%20is%20specially%20designed%20to%20mitigate%0Athese%20two%20biases%20in%20different%20CIL%20models%20by%20having%20energy%20regularization%20on%0Aboth%20old%20and%20new%20classes.%20Its%20superior%20performance%20is%20justified%20in%20our%0Aexperiments.%20All%20codes%20and%20datasets%20are%20open-source%20at%0Ahttps%3A//github.com/mala-lab/OpenCIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06045v2&entry.124074799=Read"},
{"title": "ProtoSAM - One Shot Medical Image Segmentation With Foundational Models", "author": "Lev Ayzenberg and Raja Giryes and Hayit Greenspan", "abstract": "  This work introduces a new framework, ProtoSAM, for one-shot medical image\nsegmentation. It combines the use of prototypical networks, known for few-shot\nsegmentation, with SAM - a natural image foundation model. The method proposed\ncreates an initial coarse segmentation mask using the ALPnet prototypical\nnetwork, augmented with a DINOv2 encoder. Following the extraction of an\ninitial mask, prompts are extracted, such as points and bounding boxes, which\nare then input into the Segment Anything Model (SAM). State-of-the-art results\nare shown on several medical image datasets and demonstrate automated\nsegmentation capabilities using a single image example (one shot) with no need\nfor fine-tuning of the foundation model.\n", "link": "http://arxiv.org/abs/2407.07042v1", "date": "2024-07-09", "relevancy": 2.0115, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5246}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5061}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProtoSAM%20-%20One%20Shot%20Medical%20Image%20Segmentation%20With%20Foundational%20Models&body=Title%3A%20ProtoSAM%20-%20One%20Shot%20Medical%20Image%20Segmentation%20With%20Foundational%20Models%0AAuthor%3A%20Lev%20Ayzenberg%20and%20Raja%20Giryes%20and%20Hayit%20Greenspan%0AAbstract%3A%20%20%20This%20work%20introduces%20a%20new%20framework%2C%20ProtoSAM%2C%20for%20one-shot%20medical%20image%0Asegmentation.%20It%20combines%20the%20use%20of%20prototypical%20networks%2C%20known%20for%20few-shot%0Asegmentation%2C%20with%20SAM%20-%20a%20natural%20image%20foundation%20model.%20The%20method%20proposed%0Acreates%20an%20initial%20coarse%20segmentation%20mask%20using%20the%20ALPnet%20prototypical%0Anetwork%2C%20augmented%20with%20a%20DINOv2%20encoder.%20Following%20the%20extraction%20of%20an%0Ainitial%20mask%2C%20prompts%20are%20extracted%2C%20such%20as%20points%20and%20bounding%20boxes%2C%20which%0Aare%20then%20input%20into%20the%20Segment%20Anything%20Model%20%28SAM%29.%20State-of-the-art%20results%0Aare%20shown%20on%20several%20medical%20image%20datasets%20and%20demonstrate%20automated%0Asegmentation%20capabilities%20using%20a%20single%20image%20example%20%28one%20shot%29%20with%20no%20need%0Afor%20fine-tuning%20of%20the%20foundation%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtoSAM%2520-%2520One%2520Shot%2520Medical%2520Image%2520Segmentation%2520With%2520Foundational%2520Models%26entry.906535625%3DLev%2520Ayzenberg%2520and%2520Raja%2520Giryes%2520and%2520Hayit%2520Greenspan%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520a%2520new%2520framework%252C%2520ProtoSAM%252C%2520for%2520one-shot%2520medical%2520image%250Asegmentation.%2520It%2520combines%2520the%2520use%2520of%2520prototypical%2520networks%252C%2520known%2520for%2520few-shot%250Asegmentation%252C%2520with%2520SAM%2520-%2520a%2520natural%2520image%2520foundation%2520model.%2520The%2520method%2520proposed%250Acreates%2520an%2520initial%2520coarse%2520segmentation%2520mask%2520using%2520the%2520ALPnet%2520prototypical%250Anetwork%252C%2520augmented%2520with%2520a%2520DINOv2%2520encoder.%2520Following%2520the%2520extraction%2520of%2520an%250Ainitial%2520mask%252C%2520prompts%2520are%2520extracted%252C%2520such%2520as%2520points%2520and%2520bounding%2520boxes%252C%2520which%250Aare%2520then%2520input%2520into%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529.%2520State-of-the-art%2520results%250Aare%2520shown%2520on%2520several%2520medical%2520image%2520datasets%2520and%2520demonstrate%2520automated%250Asegmentation%2520capabilities%2520using%2520a%2520single%2520image%2520example%2520%2528one%2520shot%2529%2520with%2520no%2520need%250Afor%2520fine-tuning%2520of%2520the%2520foundation%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProtoSAM%20-%20One%20Shot%20Medical%20Image%20Segmentation%20With%20Foundational%20Models&entry.906535625=Lev%20Ayzenberg%20and%20Raja%20Giryes%20and%20Hayit%20Greenspan&entry.1292438233=%20%20This%20work%20introduces%20a%20new%20framework%2C%20ProtoSAM%2C%20for%20one-shot%20medical%20image%0Asegmentation.%20It%20combines%20the%20use%20of%20prototypical%20networks%2C%20known%20for%20few-shot%0Asegmentation%2C%20with%20SAM%20-%20a%20natural%20image%20foundation%20model.%20The%20method%20proposed%0Acreates%20an%20initial%20coarse%20segmentation%20mask%20using%20the%20ALPnet%20prototypical%0Anetwork%2C%20augmented%20with%20a%20DINOv2%20encoder.%20Following%20the%20extraction%20of%20an%0Ainitial%20mask%2C%20prompts%20are%20extracted%2C%20such%20as%20points%20and%20bounding%20boxes%2C%20which%0Aare%20then%20input%20into%20the%20Segment%20Anything%20Model%20%28SAM%29.%20State-of-the-art%20results%0Aare%20shown%20on%20several%20medical%20image%20datasets%20and%20demonstrate%20automated%0Asegmentation%20capabilities%20using%20a%20single%20image%20example%20%28one%20shot%29%20with%20no%20need%0Afor%20fine-tuning%20of%20the%20foundation%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07042v1&entry.124074799=Read"},
{"title": "CycleSAM: One-Shot Surgical Scene Segmentation using Cycle-Consistent\n  Feature Matching to Prompt SAM", "author": "Aditya Murali and Pietro Mascagni and Didier Mutter and Nicolas Padoy", "abstract": "  The recently introduced Segment-Anything Model (SAM) has the potential to\ngreatly accelerate the development of segmentation models. However, directly\napplying SAM to surgical images has key limitations including (1) the\nrequirement of image-specific prompts at test-time, thereby preventing fully\nautomated segmentation, and (2) ineffectiveness due to substantial domain gap\nbetween natural and surgical images. In this work, we propose CycleSAM, an\napproach for one-shot surgical scene segmentation that uses the training\nimage-mask pair at test-time to automatically identify points in the test\nimages that correspond to each object class, which can then be used to prompt\nSAM to produce object masks. To produce high-fidelity matches, we introduce a\nnovel spatial cycle-consistency constraint that enforces point proposals in the\ntest image to rematch to points within the object foreground region in the\ntraining image. Then, to address the domain gap, rather than directly using the\nvisual features from SAM, we employ a ResNet50 encoder pretrained on surgical\nimages in a self-supervised fashion, thereby maintaining high label-efficiency.\nWe evaluate CycleSAM for one-shot segmentation on two diverse surgical semantic\nsegmentation datasets, comprehensively outperforming baseline approaches and\nreaching up to 50% of fully-supervised performance.\n", "link": "http://arxiv.org/abs/2407.06795v1", "date": "2024-07-09", "relevancy": 2.009, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5364}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.482}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CycleSAM%3A%20One-Shot%20Surgical%20Scene%20Segmentation%20using%20Cycle-Consistent%0A%20%20Feature%20Matching%20to%20Prompt%20SAM&body=Title%3A%20CycleSAM%3A%20One-Shot%20Surgical%20Scene%20Segmentation%20using%20Cycle-Consistent%0A%20%20Feature%20Matching%20to%20Prompt%20SAM%0AAuthor%3A%20Aditya%20Murali%20and%20Pietro%20Mascagni%20and%20Didier%20Mutter%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20The%20recently%20introduced%20Segment-Anything%20Model%20%28SAM%29%20has%20the%20potential%20to%0Agreatly%20accelerate%20the%20development%20of%20segmentation%20models.%20However%2C%20directly%0Aapplying%20SAM%20to%20surgical%20images%20has%20key%20limitations%20including%20%281%29%20the%0Arequirement%20of%20image-specific%20prompts%20at%20test-time%2C%20thereby%20preventing%20fully%0Aautomated%20segmentation%2C%20and%20%282%29%20ineffectiveness%20due%20to%20substantial%20domain%20gap%0Abetween%20natural%20and%20surgical%20images.%20In%20this%20work%2C%20we%20propose%20CycleSAM%2C%20an%0Aapproach%20for%20one-shot%20surgical%20scene%20segmentation%20that%20uses%20the%20training%0Aimage-mask%20pair%20at%20test-time%20to%20automatically%20identify%20points%20in%20the%20test%0Aimages%20that%20correspond%20to%20each%20object%20class%2C%20which%20can%20then%20be%20used%20to%20prompt%0ASAM%20to%20produce%20object%20masks.%20To%20produce%20high-fidelity%20matches%2C%20we%20introduce%20a%0Anovel%20spatial%20cycle-consistency%20constraint%20that%20enforces%20point%20proposals%20in%20the%0Atest%20image%20to%20rematch%20to%20points%20within%20the%20object%20foreground%20region%20in%20the%0Atraining%20image.%20Then%2C%20to%20address%20the%20domain%20gap%2C%20rather%20than%20directly%20using%20the%0Avisual%20features%20from%20SAM%2C%20we%20employ%20a%20ResNet50%20encoder%20pretrained%20on%20surgical%0Aimages%20in%20a%20self-supervised%20fashion%2C%20thereby%20maintaining%20high%20label-efficiency.%0AWe%20evaluate%20CycleSAM%20for%20one-shot%20segmentation%20on%20two%20diverse%20surgical%20semantic%0Asegmentation%20datasets%2C%20comprehensively%20outperforming%20baseline%20approaches%20and%0Areaching%20up%20to%2050%25%20of%20fully-supervised%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCycleSAM%253A%2520One-Shot%2520Surgical%2520Scene%2520Segmentation%2520using%2520Cycle-Consistent%250A%2520%2520Feature%2520Matching%2520to%2520Prompt%2520SAM%26entry.906535625%3DAditya%2520Murali%2520and%2520Pietro%2520Mascagni%2520and%2520Didier%2520Mutter%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520The%2520recently%2520introduced%2520Segment-Anything%2520Model%2520%2528SAM%2529%2520has%2520the%2520potential%2520to%250Agreatly%2520accelerate%2520the%2520development%2520of%2520segmentation%2520models.%2520However%252C%2520directly%250Aapplying%2520SAM%2520to%2520surgical%2520images%2520has%2520key%2520limitations%2520including%2520%25281%2529%2520the%250Arequirement%2520of%2520image-specific%2520prompts%2520at%2520test-time%252C%2520thereby%2520preventing%2520fully%250Aautomated%2520segmentation%252C%2520and%2520%25282%2529%2520ineffectiveness%2520due%2520to%2520substantial%2520domain%2520gap%250Abetween%2520natural%2520and%2520surgical%2520images.%2520In%2520this%2520work%252C%2520we%2520propose%2520CycleSAM%252C%2520an%250Aapproach%2520for%2520one-shot%2520surgical%2520scene%2520segmentation%2520that%2520uses%2520the%2520training%250Aimage-mask%2520pair%2520at%2520test-time%2520to%2520automatically%2520identify%2520points%2520in%2520the%2520test%250Aimages%2520that%2520correspond%2520to%2520each%2520object%2520class%252C%2520which%2520can%2520then%2520be%2520used%2520to%2520prompt%250ASAM%2520to%2520produce%2520object%2520masks.%2520To%2520produce%2520high-fidelity%2520matches%252C%2520we%2520introduce%2520a%250Anovel%2520spatial%2520cycle-consistency%2520constraint%2520that%2520enforces%2520point%2520proposals%2520in%2520the%250Atest%2520image%2520to%2520rematch%2520to%2520points%2520within%2520the%2520object%2520foreground%2520region%2520in%2520the%250Atraining%2520image.%2520Then%252C%2520to%2520address%2520the%2520domain%2520gap%252C%2520rather%2520than%2520directly%2520using%2520the%250Avisual%2520features%2520from%2520SAM%252C%2520we%2520employ%2520a%2520ResNet50%2520encoder%2520pretrained%2520on%2520surgical%250Aimages%2520in%2520a%2520self-supervised%2520fashion%252C%2520thereby%2520maintaining%2520high%2520label-efficiency.%250AWe%2520evaluate%2520CycleSAM%2520for%2520one-shot%2520segmentation%2520on%2520two%2520diverse%2520surgical%2520semantic%250Asegmentation%2520datasets%252C%2520comprehensively%2520outperforming%2520baseline%2520approaches%2520and%250Areaching%2520up%2520to%252050%2525%2520of%2520fully-supervised%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CycleSAM%3A%20One-Shot%20Surgical%20Scene%20Segmentation%20using%20Cycle-Consistent%0A%20%20Feature%20Matching%20to%20Prompt%20SAM&entry.906535625=Aditya%20Murali%20and%20Pietro%20Mascagni%20and%20Didier%20Mutter%20and%20Nicolas%20Padoy&entry.1292438233=%20%20The%20recently%20introduced%20Segment-Anything%20Model%20%28SAM%29%20has%20the%20potential%20to%0Agreatly%20accelerate%20the%20development%20of%20segmentation%20models.%20However%2C%20directly%0Aapplying%20SAM%20to%20surgical%20images%20has%20key%20limitations%20including%20%281%29%20the%0Arequirement%20of%20image-specific%20prompts%20at%20test-time%2C%20thereby%20preventing%20fully%0Aautomated%20segmentation%2C%20and%20%282%29%20ineffectiveness%20due%20to%20substantial%20domain%20gap%0Abetween%20natural%20and%20surgical%20images.%20In%20this%20work%2C%20we%20propose%20CycleSAM%2C%20an%0Aapproach%20for%20one-shot%20surgical%20scene%20segmentation%20that%20uses%20the%20training%0Aimage-mask%20pair%20at%20test-time%20to%20automatically%20identify%20points%20in%20the%20test%0Aimages%20that%20correspond%20to%20each%20object%20class%2C%20which%20can%20then%20be%20used%20to%20prompt%0ASAM%20to%20produce%20object%20masks.%20To%20produce%20high-fidelity%20matches%2C%20we%20introduce%20a%0Anovel%20spatial%20cycle-consistency%20constraint%20that%20enforces%20point%20proposals%20in%20the%0Atest%20image%20to%20rematch%20to%20points%20within%20the%20object%20foreground%20region%20in%20the%0Atraining%20image.%20Then%2C%20to%20address%20the%20domain%20gap%2C%20rather%20than%20directly%20using%20the%0Avisual%20features%20from%20SAM%2C%20we%20employ%20a%20ResNet50%20encoder%20pretrained%20on%20surgical%0Aimages%20in%20a%20self-supervised%20fashion%2C%20thereby%20maintaining%20high%20label-efficiency.%0AWe%20evaluate%20CycleSAM%20for%20one-shot%20segmentation%20on%20two%20diverse%20surgical%20semantic%0Asegmentation%20datasets%2C%20comprehensively%20outperforming%20baseline%20approaches%20and%0Areaching%20up%20to%2050%25%20of%20fully-supervised%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06795v1&entry.124074799=Read"},
{"title": "ED-VAE: Entropy Decomposition of ELBO in Variational Autoencoders", "author": "Fotios Lygerakis and Elmar Rueckert", "abstract": "  Traditional Variational Autoencoders (VAEs) are constrained by the\nlimitations of the Evidence Lower Bound (ELBO) formulation, particularly when\nutilizing simplistic, non-analytic, or unknown prior distributions. These\nlimitations inhibit the VAE's ability to generate high-quality samples and\nprovide clear, interpretable latent representations. This work introduces the\nEntropy Decomposed Variational Autoencoder (ED-VAE), a novel re-formulation of\nthe ELBO that explicitly includes entropy and cross-entropy components. This\nreformulation significantly enhances model flexibility, allowing for the\nintegration of complex and non-standard priors. By providing more detailed\ncontrol over the encoding and regularization of latent spaces, ED-VAE not only\nimproves interpretability but also effectively captures the complex\ninteractions between latent variables and observed data, thus leading to better\ngenerative performance.\n", "link": "http://arxiv.org/abs/2407.06797v1", "date": "2024-07-09", "relevancy": 2.0024, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5314}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5121}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ED-VAE%3A%20Entropy%20Decomposition%20of%20ELBO%20in%20Variational%20Autoencoders&body=Title%3A%20ED-VAE%3A%20Entropy%20Decomposition%20of%20ELBO%20in%20Variational%20Autoencoders%0AAuthor%3A%20Fotios%20Lygerakis%20and%20Elmar%20Rueckert%0AAbstract%3A%20%20%20Traditional%20Variational%20Autoencoders%20%28VAEs%29%20are%20constrained%20by%20the%0Alimitations%20of%20the%20Evidence%20Lower%20Bound%20%28ELBO%29%20formulation%2C%20particularly%20when%0Autilizing%20simplistic%2C%20non-analytic%2C%20or%20unknown%20prior%20distributions.%20These%0Alimitations%20inhibit%20the%20VAE%27s%20ability%20to%20generate%20high-quality%20samples%20and%0Aprovide%20clear%2C%20interpretable%20latent%20representations.%20This%20work%20introduces%20the%0AEntropy%20Decomposed%20Variational%20Autoencoder%20%28ED-VAE%29%2C%20a%20novel%20re-formulation%20of%0Athe%20ELBO%20that%20explicitly%20includes%20entropy%20and%20cross-entropy%20components.%20This%0Areformulation%20significantly%20enhances%20model%20flexibility%2C%20allowing%20for%20the%0Aintegration%20of%20complex%20and%20non-standard%20priors.%20By%20providing%20more%20detailed%0Acontrol%20over%20the%20encoding%20and%20regularization%20of%20latent%20spaces%2C%20ED-VAE%20not%20only%0Aimproves%20interpretability%20but%20also%20effectively%20captures%20the%20complex%0Ainteractions%20between%20latent%20variables%20and%20observed%20data%2C%20thus%20leading%20to%20better%0Agenerative%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DED-VAE%253A%2520Entropy%2520Decomposition%2520of%2520ELBO%2520in%2520Variational%2520Autoencoders%26entry.906535625%3DFotios%2520Lygerakis%2520and%2520Elmar%2520Rueckert%26entry.1292438233%3D%2520%2520Traditional%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520are%2520constrained%2520by%2520the%250Alimitations%2520of%2520the%2520Evidence%2520Lower%2520Bound%2520%2528ELBO%2529%2520formulation%252C%2520particularly%2520when%250Autilizing%2520simplistic%252C%2520non-analytic%252C%2520or%2520unknown%2520prior%2520distributions.%2520These%250Alimitations%2520inhibit%2520the%2520VAE%2527s%2520ability%2520to%2520generate%2520high-quality%2520samples%2520and%250Aprovide%2520clear%252C%2520interpretable%2520latent%2520representations.%2520This%2520work%2520introduces%2520the%250AEntropy%2520Decomposed%2520Variational%2520Autoencoder%2520%2528ED-VAE%2529%252C%2520a%2520novel%2520re-formulation%2520of%250Athe%2520ELBO%2520that%2520explicitly%2520includes%2520entropy%2520and%2520cross-entropy%2520components.%2520This%250Areformulation%2520significantly%2520enhances%2520model%2520flexibility%252C%2520allowing%2520for%2520the%250Aintegration%2520of%2520complex%2520and%2520non-standard%2520priors.%2520By%2520providing%2520more%2520detailed%250Acontrol%2520over%2520the%2520encoding%2520and%2520regularization%2520of%2520latent%2520spaces%252C%2520ED-VAE%2520not%2520only%250Aimproves%2520interpretability%2520but%2520also%2520effectively%2520captures%2520the%2520complex%250Ainteractions%2520between%2520latent%2520variables%2520and%2520observed%2520data%252C%2520thus%2520leading%2520to%2520better%250Agenerative%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ED-VAE%3A%20Entropy%20Decomposition%20of%20ELBO%20in%20Variational%20Autoencoders&entry.906535625=Fotios%20Lygerakis%20and%20Elmar%20Rueckert&entry.1292438233=%20%20Traditional%20Variational%20Autoencoders%20%28VAEs%29%20are%20constrained%20by%20the%0Alimitations%20of%20the%20Evidence%20Lower%20Bound%20%28ELBO%29%20formulation%2C%20particularly%20when%0Autilizing%20simplistic%2C%20non-analytic%2C%20or%20unknown%20prior%20distributions.%20These%0Alimitations%20inhibit%20the%20VAE%27s%20ability%20to%20generate%20high-quality%20samples%20and%0Aprovide%20clear%2C%20interpretable%20latent%20representations.%20This%20work%20introduces%20the%0AEntropy%20Decomposed%20Variational%20Autoencoder%20%28ED-VAE%29%2C%20a%20novel%20re-formulation%20of%0Athe%20ELBO%20that%20explicitly%20includes%20entropy%20and%20cross-entropy%20components.%20This%0Areformulation%20significantly%20enhances%20model%20flexibility%2C%20allowing%20for%20the%0Aintegration%20of%20complex%20and%20non-standard%20priors.%20By%20providing%20more%20detailed%0Acontrol%20over%20the%20encoding%20and%20regularization%20of%20latent%20spaces%2C%20ED-VAE%20not%20only%0Aimproves%20interpretability%20but%20also%20effectively%20captures%20the%20complex%0Ainteractions%20between%20latent%20variables%20and%20observed%20data%2C%20thus%20leading%20to%20better%0Agenerative%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06797v1&entry.124074799=Read"},
{"title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization\n  with Lightweight Uncertainty Estimation", "author": "Xiaoying Zhang and Jean-Francois Ton and Wei Shen and Hongning Wang and Yang Liu", "abstract": "  We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the\npervasive issue of reward over-optimization in Reinforcement Learning from\nHuman Feedback (RLHF) for Large Language Models (LLMs). Over-optimization\noccurs when a reward model serves as an imperfect proxy for human preference,\nand RL-driven policy optimization erroneously exploits reward inaccuracies. In\nthis paper, we begin by introducing a lightweight way to quantify uncertainties\nin rewards, relying solely on the last layer embeddings of the reward model,\nwithout the need for computationally expensive reward ensembles. AdvPO then\naddresses a distributionally robust optimization problem centred around the\nconfidence interval of the reward model's predictions for policy improvement.\nThrough comprehensive experiments on the Anthropic HH and TL;DR summarization\ndatasets, we illustrate the efficacy of AdvPO in mitigating the\noveroptimization issue, consequently resulting in enhanced performance as\nevaluated through human-assisted evaluation.\n", "link": "http://arxiv.org/abs/2403.05171v2", "date": "2024-07-09", "relevancy": 2.0011, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5211}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5059}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20Reward%20Overoptimization%20via%20Adversarial%20Policy%20Optimization%0A%20%20with%20Lightweight%20Uncertainty%20Estimation&body=Title%3A%20Overcoming%20Reward%20Overoptimization%20via%20Adversarial%20Policy%20Optimization%0A%20%20with%20Lightweight%20Uncertainty%20Estimation%0AAuthor%3A%20Xiaoying%20Zhang%20and%20Jean-Francois%20Ton%20and%20Wei%20Shen%20and%20Hongning%20Wang%20and%20Yang%20Liu%0AAbstract%3A%20%20%20We%20introduce%20Adversarial%20Policy%20Optimization%20%28AdvPO%29%2C%20a%20novel%20solution%20to%20the%0Apervasive%20issue%20of%20reward%20over-optimization%20in%20Reinforcement%20Learning%20from%0AHuman%20Feedback%20%28RLHF%29%20for%20Large%20Language%20Models%20%28LLMs%29.%20Over-optimization%0Aoccurs%20when%20a%20reward%20model%20serves%20as%20an%20imperfect%20proxy%20for%20human%20preference%2C%0Aand%20RL-driven%20policy%20optimization%20erroneously%20exploits%20reward%20inaccuracies.%20In%0Athis%20paper%2C%20we%20begin%20by%20introducing%20a%20lightweight%20way%20to%20quantify%20uncertainties%0Ain%20rewards%2C%20relying%20solely%20on%20the%20last%20layer%20embeddings%20of%20the%20reward%20model%2C%0Awithout%20the%20need%20for%20computationally%20expensive%20reward%20ensembles.%20AdvPO%20then%0Aaddresses%20a%20distributionally%20robust%20optimization%20problem%20centred%20around%20the%0Aconfidence%20interval%20of%20the%20reward%20model%27s%20predictions%20for%20policy%20improvement.%0AThrough%20comprehensive%20experiments%20on%20the%20Anthropic%20HH%20and%20TL%3BDR%20summarization%0Adatasets%2C%20we%20illustrate%20the%20efficacy%20of%20AdvPO%20in%20mitigating%20the%0Aoveroptimization%20issue%2C%20consequently%20resulting%20in%20enhanced%20performance%20as%0Aevaluated%20through%20human-assisted%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520Reward%2520Overoptimization%2520via%2520Adversarial%2520Policy%2520Optimization%250A%2520%2520with%2520Lightweight%2520Uncertainty%2520Estimation%26entry.906535625%3DXiaoying%2520Zhang%2520and%2520Jean-Francois%2520Ton%2520and%2520Wei%2520Shen%2520and%2520Hongning%2520Wang%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520We%2520introduce%2520Adversarial%2520Policy%2520Optimization%2520%2528AdvPO%2529%252C%2520a%2520novel%2520solution%2520to%2520the%250Apervasive%2520issue%2520of%2520reward%2520over-optimization%2520in%2520Reinforcement%2520Learning%2520from%250AHuman%2520Feedback%2520%2528RLHF%2529%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Over-optimization%250Aoccurs%2520when%2520a%2520reward%2520model%2520serves%2520as%2520an%2520imperfect%2520proxy%2520for%2520human%2520preference%252C%250Aand%2520RL-driven%2520policy%2520optimization%2520erroneously%2520exploits%2520reward%2520inaccuracies.%2520In%250Athis%2520paper%252C%2520we%2520begin%2520by%2520introducing%2520a%2520lightweight%2520way%2520to%2520quantify%2520uncertainties%250Ain%2520rewards%252C%2520relying%2520solely%2520on%2520the%2520last%2520layer%2520embeddings%2520of%2520the%2520reward%2520model%252C%250Awithout%2520the%2520need%2520for%2520computationally%2520expensive%2520reward%2520ensembles.%2520AdvPO%2520then%250Aaddresses%2520a%2520distributionally%2520robust%2520optimization%2520problem%2520centred%2520around%2520the%250Aconfidence%2520interval%2520of%2520the%2520reward%2520model%2527s%2520predictions%2520for%2520policy%2520improvement.%250AThrough%2520comprehensive%2520experiments%2520on%2520the%2520Anthropic%2520HH%2520and%2520TL%253BDR%2520summarization%250Adatasets%252C%2520we%2520illustrate%2520the%2520efficacy%2520of%2520AdvPO%2520in%2520mitigating%2520the%250Aoveroptimization%2520issue%252C%2520consequently%2520resulting%2520in%2520enhanced%2520performance%2520as%250Aevaluated%2520through%2520human-assisted%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Reward%20Overoptimization%20via%20Adversarial%20Policy%20Optimization%0A%20%20with%20Lightweight%20Uncertainty%20Estimation&entry.906535625=Xiaoying%20Zhang%20and%20Jean-Francois%20Ton%20and%20Wei%20Shen%20and%20Hongning%20Wang%20and%20Yang%20Liu&entry.1292438233=%20%20We%20introduce%20Adversarial%20Policy%20Optimization%20%28AdvPO%29%2C%20a%20novel%20solution%20to%20the%0Apervasive%20issue%20of%20reward%20over-optimization%20in%20Reinforcement%20Learning%20from%0AHuman%20Feedback%20%28RLHF%29%20for%20Large%20Language%20Models%20%28LLMs%29.%20Over-optimization%0Aoccurs%20when%20a%20reward%20model%20serves%20as%20an%20imperfect%20proxy%20for%20human%20preference%2C%0Aand%20RL-driven%20policy%20optimization%20erroneously%20exploits%20reward%20inaccuracies.%20In%0Athis%20paper%2C%20we%20begin%20by%20introducing%20a%20lightweight%20way%20to%20quantify%20uncertainties%0Ain%20rewards%2C%20relying%20solely%20on%20the%20last%20layer%20embeddings%20of%20the%20reward%20model%2C%0Awithout%20the%20need%20for%20computationally%20expensive%20reward%20ensembles.%20AdvPO%20then%0Aaddresses%20a%20distributionally%20robust%20optimization%20problem%20centred%20around%20the%0Aconfidence%20interval%20of%20the%20reward%20model%27s%20predictions%20for%20policy%20improvement.%0AThrough%20comprehensive%20experiments%20on%20the%20Anthropic%20HH%20and%20TL%3BDR%20summarization%0Adatasets%2C%20we%20illustrate%20the%20efficacy%20of%20AdvPO%20in%20mitigating%20the%0Aoveroptimization%20issue%2C%20consequently%20resulting%20in%20enhanced%20performance%20as%0Aevaluated%20through%20human-assisted%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05171v2&entry.124074799=Read"},
{"title": "AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning", "author": "Jiaxi Cui and Wentao Zhang and Jing Tang and Xudong Tong and Zhenwei Zhang and  Amie and Jing Wen and Rongsheng Wang and Pengfei Wu", "abstract": "  The pervasive deployment of Large Language Models-LLMs in various sectors\noften neglects the nuanced requirements of individuals and small organizations,\nwho benefit more from models precisely tailored to their specific business\ncontexts rather than those with broadly superior general capabilities. This\nwork introduces \\textbf{AnyTaskTune}, a novel fine-tuning methodology coined as\n\\textbf{Task-Fine-Tune}, specifically developed to elevate model performance on\na diverse array of domain-specific tasks. This method involves a meticulous\nprocess to identify and define targeted sub-tasks within a domain, followed by\nthe creation of specialized enhancement datasets for fine-tuning, thereby\noptimizing task-specific model performance. We conducted comprehensive\nfine-tuning experiments not only in the legal domain for tasks such as keyword\nextraction and sentence prediction but across over twenty different sub-tasks\nderived from the domains of finance, healthcare, law, psychology, consumer\nservices, and human resources. To substantiate our approach and facilitate\ncommunity engagement, we will open-source these bilingual task datasets. Our\nfindings demonstrate that models fine-tuned using the \\textbf{Task-Fine-Tune}\nmethodology not only achieve superior performance on these specific tasks but\nalso significantly outperform models with higher general capabilities in their\nrespective domains. Our work is publicly available at\n\\url{https://github.com/PandaVT/DataTager}.\n", "link": "http://arxiv.org/abs/2407.07094v1", "date": "2024-07-09", "relevancy": 2.0007, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.518}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4904}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyTaskTune%3A%20Advanced%20Domain-Specific%20Solutions%20through%20Task-Fine-Tuning&body=Title%3A%20AnyTaskTune%3A%20Advanced%20Domain-Specific%20Solutions%20through%20Task-Fine-Tuning%0AAuthor%3A%20Jiaxi%20Cui%20and%20Wentao%20Zhang%20and%20Jing%20Tang%20and%20Xudong%20Tong%20and%20Zhenwei%20Zhang%20and%20%20Amie%20and%20Jing%20Wen%20and%20Rongsheng%20Wang%20and%20Pengfei%20Wu%0AAbstract%3A%20%20%20The%20pervasive%20deployment%20of%20Large%20Language%20Models-LLMs%20in%20various%20sectors%0Aoften%20neglects%20the%20nuanced%20requirements%20of%20individuals%20and%20small%20organizations%2C%0Awho%20benefit%20more%20from%20models%20precisely%20tailored%20to%20their%20specific%20business%0Acontexts%20rather%20than%20those%20with%20broadly%20superior%20general%20capabilities.%20This%0Awork%20introduces%20%5Ctextbf%7BAnyTaskTune%7D%2C%20a%20novel%20fine-tuning%20methodology%20coined%20as%0A%5Ctextbf%7BTask-Fine-Tune%7D%2C%20specifically%20developed%20to%20elevate%20model%20performance%20on%0Aa%20diverse%20array%20of%20domain-specific%20tasks.%20This%20method%20involves%20a%20meticulous%0Aprocess%20to%20identify%20and%20define%20targeted%20sub-tasks%20within%20a%20domain%2C%20followed%20by%0Athe%20creation%20of%20specialized%20enhancement%20datasets%20for%20fine-tuning%2C%20thereby%0Aoptimizing%20task-specific%20model%20performance.%20We%20conducted%20comprehensive%0Afine-tuning%20experiments%20not%20only%20in%20the%20legal%20domain%20for%20tasks%20such%20as%20keyword%0Aextraction%20and%20sentence%20prediction%20but%20across%20over%20twenty%20different%20sub-tasks%0Aderived%20from%20the%20domains%20of%20finance%2C%20healthcare%2C%20law%2C%20psychology%2C%20consumer%0Aservices%2C%20and%20human%20resources.%20To%20substantiate%20our%20approach%20and%20facilitate%0Acommunity%20engagement%2C%20we%20will%20open-source%20these%20bilingual%20task%20datasets.%20Our%0Afindings%20demonstrate%20that%20models%20fine-tuned%20using%20the%20%5Ctextbf%7BTask-Fine-Tune%7D%0Amethodology%20not%20only%20achieve%20superior%20performance%20on%20these%20specific%20tasks%20but%0Aalso%20significantly%20outperform%20models%20with%20higher%20general%20capabilities%20in%20their%0Arespective%20domains.%20Our%20work%20is%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/PandaVT/DataTager%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyTaskTune%253A%2520Advanced%2520Domain-Specific%2520Solutions%2520through%2520Task-Fine-Tuning%26entry.906535625%3DJiaxi%2520Cui%2520and%2520Wentao%2520Zhang%2520and%2520Jing%2520Tang%2520and%2520Xudong%2520Tong%2520and%2520Zhenwei%2520Zhang%2520and%2520%2520Amie%2520and%2520Jing%2520Wen%2520and%2520Rongsheng%2520Wang%2520and%2520Pengfei%2520Wu%26entry.1292438233%3D%2520%2520The%2520pervasive%2520deployment%2520of%2520Large%2520Language%2520Models-LLMs%2520in%2520various%2520sectors%250Aoften%2520neglects%2520the%2520nuanced%2520requirements%2520of%2520individuals%2520and%2520small%2520organizations%252C%250Awho%2520benefit%2520more%2520from%2520models%2520precisely%2520tailored%2520to%2520their%2520specific%2520business%250Acontexts%2520rather%2520than%2520those%2520with%2520broadly%2520superior%2520general%2520capabilities.%2520This%250Awork%2520introduces%2520%255Ctextbf%257BAnyTaskTune%257D%252C%2520a%2520novel%2520fine-tuning%2520methodology%2520coined%2520as%250A%255Ctextbf%257BTask-Fine-Tune%257D%252C%2520specifically%2520developed%2520to%2520elevate%2520model%2520performance%2520on%250Aa%2520diverse%2520array%2520of%2520domain-specific%2520tasks.%2520This%2520method%2520involves%2520a%2520meticulous%250Aprocess%2520to%2520identify%2520and%2520define%2520targeted%2520sub-tasks%2520within%2520a%2520domain%252C%2520followed%2520by%250Athe%2520creation%2520of%2520specialized%2520enhancement%2520datasets%2520for%2520fine-tuning%252C%2520thereby%250Aoptimizing%2520task-specific%2520model%2520performance.%2520We%2520conducted%2520comprehensive%250Afine-tuning%2520experiments%2520not%2520only%2520in%2520the%2520legal%2520domain%2520for%2520tasks%2520such%2520as%2520keyword%250Aextraction%2520and%2520sentence%2520prediction%2520but%2520across%2520over%2520twenty%2520different%2520sub-tasks%250Aderived%2520from%2520the%2520domains%2520of%2520finance%252C%2520healthcare%252C%2520law%252C%2520psychology%252C%2520consumer%250Aservices%252C%2520and%2520human%2520resources.%2520To%2520substantiate%2520our%2520approach%2520and%2520facilitate%250Acommunity%2520engagement%252C%2520we%2520will%2520open-source%2520these%2520bilingual%2520task%2520datasets.%2520Our%250Afindings%2520demonstrate%2520that%2520models%2520fine-tuned%2520using%2520the%2520%255Ctextbf%257BTask-Fine-Tune%257D%250Amethodology%2520not%2520only%2520achieve%2520superior%2520performance%2520on%2520these%2520specific%2520tasks%2520but%250Aalso%2520significantly%2520outperform%2520models%2520with%2520higher%2520general%2520capabilities%2520in%2520their%250Arespective%2520domains.%2520Our%2520work%2520is%2520publicly%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/PandaVT/DataTager%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyTaskTune%3A%20Advanced%20Domain-Specific%20Solutions%20through%20Task-Fine-Tuning&entry.906535625=Jiaxi%20Cui%20and%20Wentao%20Zhang%20and%20Jing%20Tang%20and%20Xudong%20Tong%20and%20Zhenwei%20Zhang%20and%20%20Amie%20and%20Jing%20Wen%20and%20Rongsheng%20Wang%20and%20Pengfei%20Wu&entry.1292438233=%20%20The%20pervasive%20deployment%20of%20Large%20Language%20Models-LLMs%20in%20various%20sectors%0Aoften%20neglects%20the%20nuanced%20requirements%20of%20individuals%20and%20small%20organizations%2C%0Awho%20benefit%20more%20from%20models%20precisely%20tailored%20to%20their%20specific%20business%0Acontexts%20rather%20than%20those%20with%20broadly%20superior%20general%20capabilities.%20This%0Awork%20introduces%20%5Ctextbf%7BAnyTaskTune%7D%2C%20a%20novel%20fine-tuning%20methodology%20coined%20as%0A%5Ctextbf%7BTask-Fine-Tune%7D%2C%20specifically%20developed%20to%20elevate%20model%20performance%20on%0Aa%20diverse%20array%20of%20domain-specific%20tasks.%20This%20method%20involves%20a%20meticulous%0Aprocess%20to%20identify%20and%20define%20targeted%20sub-tasks%20within%20a%20domain%2C%20followed%20by%0Athe%20creation%20of%20specialized%20enhancement%20datasets%20for%20fine-tuning%2C%20thereby%0Aoptimizing%20task-specific%20model%20performance.%20We%20conducted%20comprehensive%0Afine-tuning%20experiments%20not%20only%20in%20the%20legal%20domain%20for%20tasks%20such%20as%20keyword%0Aextraction%20and%20sentence%20prediction%20but%20across%20over%20twenty%20different%20sub-tasks%0Aderived%20from%20the%20domains%20of%20finance%2C%20healthcare%2C%20law%2C%20psychology%2C%20consumer%0Aservices%2C%20and%20human%20resources.%20To%20substantiate%20our%20approach%20and%20facilitate%0Acommunity%20engagement%2C%20we%20will%20open-source%20these%20bilingual%20task%20datasets.%20Our%0Afindings%20demonstrate%20that%20models%20fine-tuned%20using%20the%20%5Ctextbf%7BTask-Fine-Tune%7D%0Amethodology%20not%20only%20achieve%20superior%20performance%20on%20these%20specific%20tasks%20but%0Aalso%20significantly%20outperform%20models%20with%20higher%20general%20capabilities%20in%20their%0Arespective%20domains.%20Our%20work%20is%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/PandaVT/DataTager%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07094v1&entry.124074799=Read"},
{"title": "ERQ: Error Reduction for Post-Training Quantization of Vision\n  Transformers", "author": "Yunshan Zhong and Jiawei Hu and You Huang and Yuxin Zhang and Rongrong Ji", "abstract": "  Post-training quantization (PTQ) for vision transformers (ViTs) has garnered\nsignificant attention due to its efficiency in compressing models. However,\nexisting methods typically overlook the intricate interdependence between\nquantized weight and activation, leading to considerable quantization error. In\nthis paper, we propose ERQ, a two-step PTQ approach meticulously crafted to\nsequentially reduce the quantization error arising from activation and weight\nquantization. ERQ first introduces Activation quantization error reduction\n(Aqer) that strategically formulates the minimization of activation\nquantization error as a Ridge Regression problem, tackling it by updating\nweights with full-precision. Subsequently, ERQ introduces Weight quantization\nerror reduction (Wqer) that adopts an iterative approach to mitigate the\nquantization error induced by weight quantization. In each iteration, an\nempirically derived, efficient proxy is employed to refine the rounding\ndirections of quantized weights, coupled with a Ridge Regression solver to\ncurtail weight quantization error. Experimental results attest to the\neffectiveness of our approach. Notably, ERQ surpasses the state-of-the-art GPTQ\nby 22.36% in accuracy for W3A4 ViT-S.\n", "link": "http://arxiv.org/abs/2407.06794v1", "date": "2024-07-09", "relevancy": 1.9953, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.538}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4934}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ERQ%3A%20Error%20Reduction%20for%20Post-Training%20Quantization%20of%20Vision%0A%20%20Transformers&body=Title%3A%20ERQ%3A%20Error%20Reduction%20for%20Post-Training%20Quantization%20of%20Vision%0A%20%20Transformers%0AAuthor%3A%20Yunshan%20Zhong%20and%20Jiawei%20Hu%20and%20You%20Huang%20and%20Yuxin%20Zhang%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Post-training%20quantization%20%28PTQ%29%20for%20vision%20transformers%20%28ViTs%29%20has%20garnered%0Asignificant%20attention%20due%20to%20its%20efficiency%20in%20compressing%20models.%20However%2C%0Aexisting%20methods%20typically%20overlook%20the%20intricate%20interdependence%20between%0Aquantized%20weight%20and%20activation%2C%20leading%20to%20considerable%20quantization%20error.%20In%0Athis%20paper%2C%20we%20propose%20ERQ%2C%20a%20two-step%20PTQ%20approach%20meticulously%20crafted%20to%0Asequentially%20reduce%20the%20quantization%20error%20arising%20from%20activation%20and%20weight%0Aquantization.%20ERQ%20first%20introduces%20Activation%20quantization%20error%20reduction%0A%28Aqer%29%20that%20strategically%20formulates%20the%20minimization%20of%20activation%0Aquantization%20error%20as%20a%20Ridge%20Regression%20problem%2C%20tackling%20it%20by%20updating%0Aweights%20with%20full-precision.%20Subsequently%2C%20ERQ%20introduces%20Weight%20quantization%0Aerror%20reduction%20%28Wqer%29%20that%20adopts%20an%20iterative%20approach%20to%20mitigate%20the%0Aquantization%20error%20induced%20by%20weight%20quantization.%20In%20each%20iteration%2C%20an%0Aempirically%20derived%2C%20efficient%20proxy%20is%20employed%20to%20refine%20the%20rounding%0Adirections%20of%20quantized%20weights%2C%20coupled%20with%20a%20Ridge%20Regression%20solver%20to%0Acurtail%20weight%20quantization%20error.%20Experimental%20results%20attest%20to%20the%0Aeffectiveness%20of%20our%20approach.%20Notably%2C%20ERQ%20surpasses%20the%20state-of-the-art%20GPTQ%0Aby%2022.36%25%20in%20accuracy%20for%20W3A4%20ViT-S.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DERQ%253A%2520Error%2520Reduction%2520for%2520Post-Training%2520Quantization%2520of%2520Vision%250A%2520%2520Transformers%26entry.906535625%3DYunshan%2520Zhong%2520and%2520Jiawei%2520Hu%2520and%2520You%2520Huang%2520and%2520Yuxin%2520Zhang%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Post-training%2520quantization%2520%2528PTQ%2529%2520for%2520vision%2520transformers%2520%2528ViTs%2529%2520has%2520garnered%250Asignificant%2520attention%2520due%2520to%2520its%2520efficiency%2520in%2520compressing%2520models.%2520However%252C%250Aexisting%2520methods%2520typically%2520overlook%2520the%2520intricate%2520interdependence%2520between%250Aquantized%2520weight%2520and%2520activation%252C%2520leading%2520to%2520considerable%2520quantization%2520error.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520ERQ%252C%2520a%2520two-step%2520PTQ%2520approach%2520meticulously%2520crafted%2520to%250Asequentially%2520reduce%2520the%2520quantization%2520error%2520arising%2520from%2520activation%2520and%2520weight%250Aquantization.%2520ERQ%2520first%2520introduces%2520Activation%2520quantization%2520error%2520reduction%250A%2528Aqer%2529%2520that%2520strategically%2520formulates%2520the%2520minimization%2520of%2520activation%250Aquantization%2520error%2520as%2520a%2520Ridge%2520Regression%2520problem%252C%2520tackling%2520it%2520by%2520updating%250Aweights%2520with%2520full-precision.%2520Subsequently%252C%2520ERQ%2520introduces%2520Weight%2520quantization%250Aerror%2520reduction%2520%2528Wqer%2529%2520that%2520adopts%2520an%2520iterative%2520approach%2520to%2520mitigate%2520the%250Aquantization%2520error%2520induced%2520by%2520weight%2520quantization.%2520In%2520each%2520iteration%252C%2520an%250Aempirically%2520derived%252C%2520efficient%2520proxy%2520is%2520employed%2520to%2520refine%2520the%2520rounding%250Adirections%2520of%2520quantized%2520weights%252C%2520coupled%2520with%2520a%2520Ridge%2520Regression%2520solver%2520to%250Acurtail%2520weight%2520quantization%2520error.%2520Experimental%2520results%2520attest%2520to%2520the%250Aeffectiveness%2520of%2520our%2520approach.%2520Notably%252C%2520ERQ%2520surpasses%2520the%2520state-of-the-art%2520GPTQ%250Aby%252022.36%2525%2520in%2520accuracy%2520for%2520W3A4%2520ViT-S.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ERQ%3A%20Error%20Reduction%20for%20Post-Training%20Quantization%20of%20Vision%0A%20%20Transformers&entry.906535625=Yunshan%20Zhong%20and%20Jiawei%20Hu%20and%20You%20Huang%20and%20Yuxin%20Zhang%20and%20Rongrong%20Ji&entry.1292438233=%20%20Post-training%20quantization%20%28PTQ%29%20for%20vision%20transformers%20%28ViTs%29%20has%20garnered%0Asignificant%20attention%20due%20to%20its%20efficiency%20in%20compressing%20models.%20However%2C%0Aexisting%20methods%20typically%20overlook%20the%20intricate%20interdependence%20between%0Aquantized%20weight%20and%20activation%2C%20leading%20to%20considerable%20quantization%20error.%20In%0Athis%20paper%2C%20we%20propose%20ERQ%2C%20a%20two-step%20PTQ%20approach%20meticulously%20crafted%20to%0Asequentially%20reduce%20the%20quantization%20error%20arising%20from%20activation%20and%20weight%0Aquantization.%20ERQ%20first%20introduces%20Activation%20quantization%20error%20reduction%0A%28Aqer%29%20that%20strategically%20formulates%20the%20minimization%20of%20activation%0Aquantization%20error%20as%20a%20Ridge%20Regression%20problem%2C%20tackling%20it%20by%20updating%0Aweights%20with%20full-precision.%20Subsequently%2C%20ERQ%20introduces%20Weight%20quantization%0Aerror%20reduction%20%28Wqer%29%20that%20adopts%20an%20iterative%20approach%20to%20mitigate%20the%0Aquantization%20error%20induced%20by%20weight%20quantization.%20In%20each%20iteration%2C%20an%0Aempirically%20derived%2C%20efficient%20proxy%20is%20employed%20to%20refine%20the%20rounding%0Adirections%20of%20quantized%20weights%2C%20coupled%20with%20a%20Ridge%20Regression%20solver%20to%0Acurtail%20weight%20quantization%20error.%20Experimental%20results%20attest%20to%20the%0Aeffectiveness%20of%20our%20approach.%20Notably%2C%20ERQ%20surpasses%20the%20state-of-the-art%20GPTQ%0Aby%2022.36%25%20in%20accuracy%20for%20W3A4%20ViT-S.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06794v1&entry.124074799=Read"},
{"title": "FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive\n  Distillation", "author": "Liqun Ma and Mingjie Sun and Zhiqiang Shen", "abstract": "  This work presents a Fully BInarized Large Language Model (FBI-LLM),\ndemonstrating for the first time how to train a large-scale binary language\nmodel from scratch (not the partial binary or ternary LLM like BitNet b1.58) to\nmatch the performance of its full-precision counterparts (e.g., FP16 or BF16)\nin transformer-based LLMs. It achieves this by employing an autoregressive\ndistillation (AD) loss with maintaining equivalent model dimensions (130M,\n1.3B, 7B) and training data volume as regular LLM pretraining, while delivering\ncompetitive results in terms of perplexity and task-specific effectiveness.\nIntriguingly, by analyzing the training trajectory, we find that the pretrained\nweight is not necessary for training binarized LLMs from scratch. This research\nencourages a new computational framework and may facilitate the future design\nof specialized hardware tailored for fully 1-bit LLMs. We make all models,\ncode, and training dataset fully accessible and transparent to support further\nresearch (Code: https://github.com/LiqunMa/FBI-LLM. Model:\nhttps://huggingface.co/LiqunMa/).\n", "link": "http://arxiv.org/abs/2407.07093v1", "date": "2024-07-09", "relevancy": 1.9947, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5136}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5053}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FBI-LLM%3A%20Scaling%20Up%20Fully%20Binarized%20LLMs%20from%20Scratch%20via%20Autoregressive%0A%20%20Distillation&body=Title%3A%20FBI-LLM%3A%20Scaling%20Up%20Fully%20Binarized%20LLMs%20from%20Scratch%20via%20Autoregressive%0A%20%20Distillation%0AAuthor%3A%20Liqun%20Ma%20and%20Mingjie%20Sun%20and%20Zhiqiang%20Shen%0AAbstract%3A%20%20%20This%20work%20presents%20a%20Fully%20BInarized%20Large%20Language%20Model%20%28FBI-LLM%29%2C%0Ademonstrating%20for%20the%20first%20time%20how%20to%20train%20a%20large-scale%20binary%20language%0Amodel%20from%20scratch%20%28not%20the%20partial%20binary%20or%20ternary%20LLM%20like%20BitNet%20b1.58%29%20to%0Amatch%20the%20performance%20of%20its%20full-precision%20counterparts%20%28e.g.%2C%20FP16%20or%20BF16%29%0Ain%20transformer-based%20LLMs.%20It%20achieves%20this%20by%20employing%20an%20autoregressive%0Adistillation%20%28AD%29%20loss%20with%20maintaining%20equivalent%20model%20dimensions%20%28130M%2C%0A1.3B%2C%207B%29%20and%20training%20data%20volume%20as%20regular%20LLM%20pretraining%2C%20while%20delivering%0Acompetitive%20results%20in%20terms%20of%20perplexity%20and%20task-specific%20effectiveness.%0AIntriguingly%2C%20by%20analyzing%20the%20training%20trajectory%2C%20we%20find%20that%20the%20pretrained%0Aweight%20is%20not%20necessary%20for%20training%20binarized%20LLMs%20from%20scratch.%20This%20research%0Aencourages%20a%20new%20computational%20framework%20and%20may%20facilitate%20the%20future%20design%0Aof%20specialized%20hardware%20tailored%20for%20fully%201-bit%20LLMs.%20We%20make%20all%20models%2C%0Acode%2C%20and%20training%20dataset%20fully%20accessible%20and%20transparent%20to%20support%20further%0Aresearch%20%28Code%3A%20https%3A//github.com/LiqunMa/FBI-LLM.%20Model%3A%0Ahttps%3A//huggingface.co/LiqunMa/%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFBI-LLM%253A%2520Scaling%2520Up%2520Fully%2520Binarized%2520LLMs%2520from%2520Scratch%2520via%2520Autoregressive%250A%2520%2520Distillation%26entry.906535625%3DLiqun%2520Ma%2520and%2520Mingjie%2520Sun%2520and%2520Zhiqiang%2520Shen%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520Fully%2520BInarized%2520Large%2520Language%2520Model%2520%2528FBI-LLM%2529%252C%250Ademonstrating%2520for%2520the%2520first%2520time%2520how%2520to%2520train%2520a%2520large-scale%2520binary%2520language%250Amodel%2520from%2520scratch%2520%2528not%2520the%2520partial%2520binary%2520or%2520ternary%2520LLM%2520like%2520BitNet%2520b1.58%2529%2520to%250Amatch%2520the%2520performance%2520of%2520its%2520full-precision%2520counterparts%2520%2528e.g.%252C%2520FP16%2520or%2520BF16%2529%250Ain%2520transformer-based%2520LLMs.%2520It%2520achieves%2520this%2520by%2520employing%2520an%2520autoregressive%250Adistillation%2520%2528AD%2529%2520loss%2520with%2520maintaining%2520equivalent%2520model%2520dimensions%2520%2528130M%252C%250A1.3B%252C%25207B%2529%2520and%2520training%2520data%2520volume%2520as%2520regular%2520LLM%2520pretraining%252C%2520while%2520delivering%250Acompetitive%2520results%2520in%2520terms%2520of%2520perplexity%2520and%2520task-specific%2520effectiveness.%250AIntriguingly%252C%2520by%2520analyzing%2520the%2520training%2520trajectory%252C%2520we%2520find%2520that%2520the%2520pretrained%250Aweight%2520is%2520not%2520necessary%2520for%2520training%2520binarized%2520LLMs%2520from%2520scratch.%2520This%2520research%250Aencourages%2520a%2520new%2520computational%2520framework%2520and%2520may%2520facilitate%2520the%2520future%2520design%250Aof%2520specialized%2520hardware%2520tailored%2520for%2520fully%25201-bit%2520LLMs.%2520We%2520make%2520all%2520models%252C%250Acode%252C%2520and%2520training%2520dataset%2520fully%2520accessible%2520and%2520transparent%2520to%2520support%2520further%250Aresearch%2520%2528Code%253A%2520https%253A//github.com/LiqunMa/FBI-LLM.%2520Model%253A%250Ahttps%253A//huggingface.co/LiqunMa/%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FBI-LLM%3A%20Scaling%20Up%20Fully%20Binarized%20LLMs%20from%20Scratch%20via%20Autoregressive%0A%20%20Distillation&entry.906535625=Liqun%20Ma%20and%20Mingjie%20Sun%20and%20Zhiqiang%20Shen&entry.1292438233=%20%20This%20work%20presents%20a%20Fully%20BInarized%20Large%20Language%20Model%20%28FBI-LLM%29%2C%0Ademonstrating%20for%20the%20first%20time%20how%20to%20train%20a%20large-scale%20binary%20language%0Amodel%20from%20scratch%20%28not%20the%20partial%20binary%20or%20ternary%20LLM%20like%20BitNet%20b1.58%29%20to%0Amatch%20the%20performance%20of%20its%20full-precision%20counterparts%20%28e.g.%2C%20FP16%20or%20BF16%29%0Ain%20transformer-based%20LLMs.%20It%20achieves%20this%20by%20employing%20an%20autoregressive%0Adistillation%20%28AD%29%20loss%20with%20maintaining%20equivalent%20model%20dimensions%20%28130M%2C%0A1.3B%2C%207B%29%20and%20training%20data%20volume%20as%20regular%20LLM%20pretraining%2C%20while%20delivering%0Acompetitive%20results%20in%20terms%20of%20perplexity%20and%20task-specific%20effectiveness.%0AIntriguingly%2C%20by%20analyzing%20the%20training%20trajectory%2C%20we%20find%20that%20the%20pretrained%0Aweight%20is%20not%20necessary%20for%20training%20binarized%20LLMs%20from%20scratch.%20This%20research%0Aencourages%20a%20new%20computational%20framework%20and%20may%20facilitate%20the%20future%20design%0Aof%20specialized%20hardware%20tailored%20for%20fully%201-bit%20LLMs.%20We%20make%20all%20models%2C%0Acode%2C%20and%20training%20dataset%20fully%20accessible%20and%20transparent%20to%20support%20further%0Aresearch%20%28Code%3A%20https%3A//github.com/LiqunMa/FBI-LLM.%20Model%3A%0Ahttps%3A//huggingface.co/LiqunMa/%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07093v1&entry.124074799=Read"},
{"title": "PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and\n  Tuning Methods", "author": "Yiying Wang and Xiaojing Li and Binzhu Wang and Yueyang Zhou and Han Ji and Hong Chen and Jinshi Zhang and Fei Yu and Zewei Zhao and Song Jin and Renji Gong and Wanqing Xu", "abstract": "  In domain-specific applications, GPT-4, augmented with precise prompts or\nRetrieval-Augmented Generation (RAG), shows notable potential but faces the\ncritical tri-lemma of performance, cost, and data privacy. High performance\nrequires sophisticated processing techniques, yet managing multiple agents\nwithin a complex workflow often proves costly and challenging. To address this,\nwe introduce the PEER (Plan, Execute, Express, Review) multi-agent framework.\nThis systematizes domain-specific tasks by integrating precise question\ndecomposition, advanced information retrieval, comprehensive summarization, and\nrigorous self-assessment. Given the concerns of cost and data privacy,\nenterprises are shifting from proprietary models like GPT-4 to custom models,\nstriking a balance between cost, security, and performance. We developed\nindustrial practices leveraging online data and user feedback for efficient\nmodel tuning. This study provides best practice guidelines for applying\nmulti-agent systems in domain-specific problem-solving and implementing\neffective agent tuning strategies. Our empirical studies, particularly in the\nfinancial question-answering domain, demonstrate that our approach achieves\n95.0% of GPT-4's performance, while effectively managing costs and ensuring\ndata privacy.\n", "link": "http://arxiv.org/abs/2407.06985v1", "date": "2024-07-09", "relevancy": 1.9735, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.498}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.491}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEER%3A%20Expertizing%20Domain-Specific%20Tasks%20with%20a%20Multi-Agent%20Framework%20and%0A%20%20Tuning%20Methods&body=Title%3A%20PEER%3A%20Expertizing%20Domain-Specific%20Tasks%20with%20a%20Multi-Agent%20Framework%20and%0A%20%20Tuning%20Methods%0AAuthor%3A%20Yiying%20Wang%20and%20Xiaojing%20Li%20and%20Binzhu%20Wang%20and%20Yueyang%20Zhou%20and%20Han%20Ji%20and%20Hong%20Chen%20and%20Jinshi%20Zhang%20and%20Fei%20Yu%20and%20Zewei%20Zhao%20and%20Song%20Jin%20and%20Renji%20Gong%20and%20Wanqing%20Xu%0AAbstract%3A%20%20%20In%20domain-specific%20applications%2C%20GPT-4%2C%20augmented%20with%20precise%20prompts%20or%0ARetrieval-Augmented%20Generation%20%28RAG%29%2C%20shows%20notable%20potential%20but%20faces%20the%0Acritical%20tri-lemma%20of%20performance%2C%20cost%2C%20and%20data%20privacy.%20High%20performance%0Arequires%20sophisticated%20processing%20techniques%2C%20yet%20managing%20multiple%20agents%0Awithin%20a%20complex%20workflow%20often%20proves%20costly%20and%20challenging.%20To%20address%20this%2C%0Awe%20introduce%20the%20PEER%20%28Plan%2C%20Execute%2C%20Express%2C%20Review%29%20multi-agent%20framework.%0AThis%20systematizes%20domain-specific%20tasks%20by%20integrating%20precise%20question%0Adecomposition%2C%20advanced%20information%20retrieval%2C%20comprehensive%20summarization%2C%20and%0Arigorous%20self-assessment.%20Given%20the%20concerns%20of%20cost%20and%20data%20privacy%2C%0Aenterprises%20are%20shifting%20from%20proprietary%20models%20like%20GPT-4%20to%20custom%20models%2C%0Astriking%20a%20balance%20between%20cost%2C%20security%2C%20and%20performance.%20We%20developed%0Aindustrial%20practices%20leveraging%20online%20data%20and%20user%20feedback%20for%20efficient%0Amodel%20tuning.%20This%20study%20provides%20best%20practice%20guidelines%20for%20applying%0Amulti-agent%20systems%20in%20domain-specific%20problem-solving%20and%20implementing%0Aeffective%20agent%20tuning%20strategies.%20Our%20empirical%20studies%2C%20particularly%20in%20the%0Afinancial%20question-answering%20domain%2C%20demonstrate%20that%20our%20approach%20achieves%0A95.0%25%20of%20GPT-4%27s%20performance%2C%20while%20effectively%20managing%20costs%20and%20ensuring%0Adata%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEER%253A%2520Expertizing%2520Domain-Specific%2520Tasks%2520with%2520a%2520Multi-Agent%2520Framework%2520and%250A%2520%2520Tuning%2520Methods%26entry.906535625%3DYiying%2520Wang%2520and%2520Xiaojing%2520Li%2520and%2520Binzhu%2520Wang%2520and%2520Yueyang%2520Zhou%2520and%2520Han%2520Ji%2520and%2520Hong%2520Chen%2520and%2520Jinshi%2520Zhang%2520and%2520Fei%2520Yu%2520and%2520Zewei%2520Zhao%2520and%2520Song%2520Jin%2520and%2520Renji%2520Gong%2520and%2520Wanqing%2520Xu%26entry.1292438233%3D%2520%2520In%2520domain-specific%2520applications%252C%2520GPT-4%252C%2520augmented%2520with%2520precise%2520prompts%2520or%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%252C%2520shows%2520notable%2520potential%2520but%2520faces%2520the%250Acritical%2520tri-lemma%2520of%2520performance%252C%2520cost%252C%2520and%2520data%2520privacy.%2520High%2520performance%250Arequires%2520sophisticated%2520processing%2520techniques%252C%2520yet%2520managing%2520multiple%2520agents%250Awithin%2520a%2520complex%2520workflow%2520often%2520proves%2520costly%2520and%2520challenging.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520the%2520PEER%2520%2528Plan%252C%2520Execute%252C%2520Express%252C%2520Review%2529%2520multi-agent%2520framework.%250AThis%2520systematizes%2520domain-specific%2520tasks%2520by%2520integrating%2520precise%2520question%250Adecomposition%252C%2520advanced%2520information%2520retrieval%252C%2520comprehensive%2520summarization%252C%2520and%250Arigorous%2520self-assessment.%2520Given%2520the%2520concerns%2520of%2520cost%2520and%2520data%2520privacy%252C%250Aenterprises%2520are%2520shifting%2520from%2520proprietary%2520models%2520like%2520GPT-4%2520to%2520custom%2520models%252C%250Astriking%2520a%2520balance%2520between%2520cost%252C%2520security%252C%2520and%2520performance.%2520We%2520developed%250Aindustrial%2520practices%2520leveraging%2520online%2520data%2520and%2520user%2520feedback%2520for%2520efficient%250Amodel%2520tuning.%2520This%2520study%2520provides%2520best%2520practice%2520guidelines%2520for%2520applying%250Amulti-agent%2520systems%2520in%2520domain-specific%2520problem-solving%2520and%2520implementing%250Aeffective%2520agent%2520tuning%2520strategies.%2520Our%2520empirical%2520studies%252C%2520particularly%2520in%2520the%250Afinancial%2520question-answering%2520domain%252C%2520demonstrate%2520that%2520our%2520approach%2520achieves%250A95.0%2525%2520of%2520GPT-4%2527s%2520performance%252C%2520while%2520effectively%2520managing%2520costs%2520and%2520ensuring%250Adata%2520privacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEER%3A%20Expertizing%20Domain-Specific%20Tasks%20with%20a%20Multi-Agent%20Framework%20and%0A%20%20Tuning%20Methods&entry.906535625=Yiying%20Wang%20and%20Xiaojing%20Li%20and%20Binzhu%20Wang%20and%20Yueyang%20Zhou%20and%20Han%20Ji%20and%20Hong%20Chen%20and%20Jinshi%20Zhang%20and%20Fei%20Yu%20and%20Zewei%20Zhao%20and%20Song%20Jin%20and%20Renji%20Gong%20and%20Wanqing%20Xu&entry.1292438233=%20%20In%20domain-specific%20applications%2C%20GPT-4%2C%20augmented%20with%20precise%20prompts%20or%0ARetrieval-Augmented%20Generation%20%28RAG%29%2C%20shows%20notable%20potential%20but%20faces%20the%0Acritical%20tri-lemma%20of%20performance%2C%20cost%2C%20and%20data%20privacy.%20High%20performance%0Arequires%20sophisticated%20processing%20techniques%2C%20yet%20managing%20multiple%20agents%0Awithin%20a%20complex%20workflow%20often%20proves%20costly%20and%20challenging.%20To%20address%20this%2C%0Awe%20introduce%20the%20PEER%20%28Plan%2C%20Execute%2C%20Express%2C%20Review%29%20multi-agent%20framework.%0AThis%20systematizes%20domain-specific%20tasks%20by%20integrating%20precise%20question%0Adecomposition%2C%20advanced%20information%20retrieval%2C%20comprehensive%20summarization%2C%20and%0Arigorous%20self-assessment.%20Given%20the%20concerns%20of%20cost%20and%20data%20privacy%2C%0Aenterprises%20are%20shifting%20from%20proprietary%20models%20like%20GPT-4%20to%20custom%20models%2C%0Astriking%20a%20balance%20between%20cost%2C%20security%2C%20and%20performance.%20We%20developed%0Aindustrial%20practices%20leveraging%20online%20data%20and%20user%20feedback%20for%20efficient%0Amodel%20tuning.%20This%20study%20provides%20best%20practice%20guidelines%20for%20applying%0Amulti-agent%20systems%20in%20domain-specific%20problem-solving%20and%20implementing%0Aeffective%20agent%20tuning%20strategies.%20Our%20empirical%20studies%2C%20particularly%20in%20the%0Afinancial%20question-answering%20domain%2C%20demonstrate%20that%20our%20approach%20achieves%0A95.0%25%20of%20GPT-4%27s%20performance%2C%20while%20effectively%20managing%20costs%20and%20ensuring%0Adata%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06985v1&entry.124074799=Read"},
{"title": "Cue Point Estimation using Object Detection", "author": "Giulia Arg\u00fcello and Luca A. Lanzend\u00f6rfer and Roger Wattenhofer", "abstract": "  Cue points indicate possible temporal boundaries in a transition between two\npieces of music in DJ mixing and constitute a crucial element in autonomous DJ\nsystems as well as for live mixing. In this work, we present a novel method for\nautomatic cue point estimation, interpreted as a computer vision object\ndetection task. Our proposed system is based on a pre-trained object detection\ntransformer which we fine-tune on our novel cue point dataset. Our provided\ndataset contains 21k manually annotated cue points from human experts as well\nas metronome information for nearly 5k individual tracks, making this dataset\n35x larger than the previously available cue point dataset. Unlike previous\nmethods, our approach does not require low-level musical information analysis,\nwhile demonstrating increased precision in retrieving cue point positions.\nMoreover, our proposed method demonstrates high adherence to phrasing, a type\nof high-level music structure commonly emphasized in electronic dance music.\nThe code, model checkpoints, and dataset are made publicly available.\n", "link": "http://arxiv.org/abs/2407.06823v1", "date": "2024-07-09", "relevancy": 1.9691, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4965}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4933}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cue%20Point%20Estimation%20using%20Object%20Detection&body=Title%3A%20Cue%20Point%20Estimation%20using%20Object%20Detection%0AAuthor%3A%20Giulia%20Arg%C3%BCello%20and%20Luca%20A.%20Lanzend%C3%B6rfer%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20Cue%20points%20indicate%20possible%20temporal%20boundaries%20in%20a%20transition%20between%20two%0Apieces%20of%20music%20in%20DJ%20mixing%20and%20constitute%20a%20crucial%20element%20in%20autonomous%20DJ%0Asystems%20as%20well%20as%20for%20live%20mixing.%20In%20this%20work%2C%20we%20present%20a%20novel%20method%20for%0Aautomatic%20cue%20point%20estimation%2C%20interpreted%20as%20a%20computer%20vision%20object%0Adetection%20task.%20Our%20proposed%20system%20is%20based%20on%20a%20pre-trained%20object%20detection%0Atransformer%20which%20we%20fine-tune%20on%20our%20novel%20cue%20point%20dataset.%20Our%20provided%0Adataset%20contains%2021k%20manually%20annotated%20cue%20points%20from%20human%20experts%20as%20well%0Aas%20metronome%20information%20for%20nearly%205k%20individual%20tracks%2C%20making%20this%20dataset%0A35x%20larger%20than%20the%20previously%20available%20cue%20point%20dataset.%20Unlike%20previous%0Amethods%2C%20our%20approach%20does%20not%20require%20low-level%20musical%20information%20analysis%2C%0Awhile%20demonstrating%20increased%20precision%20in%20retrieving%20cue%20point%20positions.%0AMoreover%2C%20our%20proposed%20method%20demonstrates%20high%20adherence%20to%20phrasing%2C%20a%20type%0Aof%20high-level%20music%20structure%20commonly%20emphasized%20in%20electronic%20dance%20music.%0AThe%20code%2C%20model%20checkpoints%2C%20and%20dataset%20are%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCue%2520Point%2520Estimation%2520using%2520Object%2520Detection%26entry.906535625%3DGiulia%2520Arg%25C3%25BCello%2520and%2520Luca%2520A.%2520Lanzend%25C3%25B6rfer%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520Cue%2520points%2520indicate%2520possible%2520temporal%2520boundaries%2520in%2520a%2520transition%2520between%2520two%250Apieces%2520of%2520music%2520in%2520DJ%2520mixing%2520and%2520constitute%2520a%2520crucial%2520element%2520in%2520autonomous%2520DJ%250Asystems%2520as%2520well%2520as%2520for%2520live%2520mixing.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520method%2520for%250Aautomatic%2520cue%2520point%2520estimation%252C%2520interpreted%2520as%2520a%2520computer%2520vision%2520object%250Adetection%2520task.%2520Our%2520proposed%2520system%2520is%2520based%2520on%2520a%2520pre-trained%2520object%2520detection%250Atransformer%2520which%2520we%2520fine-tune%2520on%2520our%2520novel%2520cue%2520point%2520dataset.%2520Our%2520provided%250Adataset%2520contains%252021k%2520manually%2520annotated%2520cue%2520points%2520from%2520human%2520experts%2520as%2520well%250Aas%2520metronome%2520information%2520for%2520nearly%25205k%2520individual%2520tracks%252C%2520making%2520this%2520dataset%250A35x%2520larger%2520than%2520the%2520previously%2520available%2520cue%2520point%2520dataset.%2520Unlike%2520previous%250Amethods%252C%2520our%2520approach%2520does%2520not%2520require%2520low-level%2520musical%2520information%2520analysis%252C%250Awhile%2520demonstrating%2520increased%2520precision%2520in%2520retrieving%2520cue%2520point%2520positions.%250AMoreover%252C%2520our%2520proposed%2520method%2520demonstrates%2520high%2520adherence%2520to%2520phrasing%252C%2520a%2520type%250Aof%2520high-level%2520music%2520structure%2520commonly%2520emphasized%2520in%2520electronic%2520dance%2520music.%250AThe%2520code%252C%2520model%2520checkpoints%252C%2520and%2520dataset%2520are%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cue%20Point%20Estimation%20using%20Object%20Detection&entry.906535625=Giulia%20Arg%C3%BCello%20and%20Luca%20A.%20Lanzend%C3%B6rfer%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20Cue%20points%20indicate%20possible%20temporal%20boundaries%20in%20a%20transition%20between%20two%0Apieces%20of%20music%20in%20DJ%20mixing%20and%20constitute%20a%20crucial%20element%20in%20autonomous%20DJ%0Asystems%20as%20well%20as%20for%20live%20mixing.%20In%20this%20work%2C%20we%20present%20a%20novel%20method%20for%0Aautomatic%20cue%20point%20estimation%2C%20interpreted%20as%20a%20computer%20vision%20object%0Adetection%20task.%20Our%20proposed%20system%20is%20based%20on%20a%20pre-trained%20object%20detection%0Atransformer%20which%20we%20fine-tune%20on%20our%20novel%20cue%20point%20dataset.%20Our%20provided%0Adataset%20contains%2021k%20manually%20annotated%20cue%20points%20from%20human%20experts%20as%20well%0Aas%20metronome%20information%20for%20nearly%205k%20individual%20tracks%2C%20making%20this%20dataset%0A35x%20larger%20than%20the%20previously%20available%20cue%20point%20dataset.%20Unlike%20previous%0Amethods%2C%20our%20approach%20does%20not%20require%20low-level%20musical%20information%20analysis%2C%0Awhile%20demonstrating%20increased%20precision%20in%20retrieving%20cue%20point%20positions.%0AMoreover%2C%20our%20proposed%20method%20demonstrates%20high%20adherence%20to%20phrasing%2C%20a%20type%0Aof%20high-level%20music%20structure%20commonly%20emphasized%20in%20electronic%20dance%20music.%0AThe%20code%2C%20model%20checkpoints%2C%20and%20dataset%20are%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06823v1&entry.124074799=Read"},
{"title": "Spanish TrOCR: Leveraging Transfer Learning for Language Adaptation", "author": "Filipe Lauar and Valentin Laurent", "abstract": "  This study explores the transfer learning capabilities of the TrOCR\narchitecture to Spanish. TrOCR is a transformer-based Optical Character\nRecognition (OCR) model renowned for its state-of-the-art performance in\nEnglish benchmarks. Inspired by Li et al. assertion regarding its adaptability\nto multilingual text recognition, we investigate two distinct approaches to\nadapt the model to a new language: integrating an English TrOCR encoder with a\nlanguage specific decoder and train the model on this specific language, and\nfine-tuning the English base TrOCR model on a new language data. Due to the\nscarcity of publicly available datasets, we present a resource-efficient\npipeline for creating OCR datasets in any language, along with a comprehensive\nbenchmark of the different image generation methods employed with a focus on\nVisual Rich Documents (VRDs). Additionally, we offer a comparative analysis of\nthe two approaches for the Spanish language, demonstrating that fine-tuning the\nEnglish TrOCR on Spanish yields superior recognition than the language specific\ndecoder for a fixed dataset size. We evaluate our model employing character and\nword error rate metrics on a public available printed dataset, comparing the\nperformance against other open-source and cloud OCR spanish models. As far as\nwe know, these resources represent the best open-source model for OCR in\nSpanish. The Spanish TrOCR models are publicly available on HuggingFace [20]\nand the code to generate the dataset is available on Github [25].\n", "link": "http://arxiv.org/abs/2407.06950v1", "date": "2024-07-09", "relevancy": 1.9657, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5101}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5008}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spanish%20TrOCR%3A%20Leveraging%20Transfer%20Learning%20for%20Language%20Adaptation&body=Title%3A%20Spanish%20TrOCR%3A%20Leveraging%20Transfer%20Learning%20for%20Language%20Adaptation%0AAuthor%3A%20Filipe%20Lauar%20and%20Valentin%20Laurent%0AAbstract%3A%20%20%20This%20study%20explores%20the%20transfer%20learning%20capabilities%20of%20the%20TrOCR%0Aarchitecture%20to%20Spanish.%20TrOCR%20is%20a%20transformer-based%20Optical%20Character%0ARecognition%20%28OCR%29%20model%20renowned%20for%20its%20state-of-the-art%20performance%20in%0AEnglish%20benchmarks.%20Inspired%20by%20Li%20et%20al.%20assertion%20regarding%20its%20adaptability%0Ato%20multilingual%20text%20recognition%2C%20we%20investigate%20two%20distinct%20approaches%20to%0Aadapt%20the%20model%20to%20a%20new%20language%3A%20integrating%20an%20English%20TrOCR%20encoder%20with%20a%0Alanguage%20specific%20decoder%20and%20train%20the%20model%20on%20this%20specific%20language%2C%20and%0Afine-tuning%20the%20English%20base%20TrOCR%20model%20on%20a%20new%20language%20data.%20Due%20to%20the%0Ascarcity%20of%20publicly%20available%20datasets%2C%20we%20present%20a%20resource-efficient%0Apipeline%20for%20creating%20OCR%20datasets%20in%20any%20language%2C%20along%20with%20a%20comprehensive%0Abenchmark%20of%20the%20different%20image%20generation%20methods%20employed%20with%20a%20focus%20on%0AVisual%20Rich%20Documents%20%28VRDs%29.%20Additionally%2C%20we%20offer%20a%20comparative%20analysis%20of%0Athe%20two%20approaches%20for%20the%20Spanish%20language%2C%20demonstrating%20that%20fine-tuning%20the%0AEnglish%20TrOCR%20on%20Spanish%20yields%20superior%20recognition%20than%20the%20language%20specific%0Adecoder%20for%20a%20fixed%20dataset%20size.%20We%20evaluate%20our%20model%20employing%20character%20and%0Aword%20error%20rate%20metrics%20on%20a%20public%20available%20printed%20dataset%2C%20comparing%20the%0Aperformance%20against%20other%20open-source%20and%20cloud%20OCR%20spanish%20models.%20As%20far%20as%0Awe%20know%2C%20these%20resources%20represent%20the%20best%20open-source%20model%20for%20OCR%20in%0ASpanish.%20The%20Spanish%20TrOCR%20models%20are%20publicly%20available%20on%20HuggingFace%20%5B20%5D%0Aand%20the%20code%20to%20generate%20the%20dataset%20is%20available%20on%20Github%20%5B25%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpanish%2520TrOCR%253A%2520Leveraging%2520Transfer%2520Learning%2520for%2520Language%2520Adaptation%26entry.906535625%3DFilipe%2520Lauar%2520and%2520Valentin%2520Laurent%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520transfer%2520learning%2520capabilities%2520of%2520the%2520TrOCR%250Aarchitecture%2520to%2520Spanish.%2520TrOCR%2520is%2520a%2520transformer-based%2520Optical%2520Character%250ARecognition%2520%2528OCR%2529%2520model%2520renowned%2520for%2520its%2520state-of-the-art%2520performance%2520in%250AEnglish%2520benchmarks.%2520Inspired%2520by%2520Li%2520et%2520al.%2520assertion%2520regarding%2520its%2520adaptability%250Ato%2520multilingual%2520text%2520recognition%252C%2520we%2520investigate%2520two%2520distinct%2520approaches%2520to%250Aadapt%2520the%2520model%2520to%2520a%2520new%2520language%253A%2520integrating%2520an%2520English%2520TrOCR%2520encoder%2520with%2520a%250Alanguage%2520specific%2520decoder%2520and%2520train%2520the%2520model%2520on%2520this%2520specific%2520language%252C%2520and%250Afine-tuning%2520the%2520English%2520base%2520TrOCR%2520model%2520on%2520a%2520new%2520language%2520data.%2520Due%2520to%2520the%250Ascarcity%2520of%2520publicly%2520available%2520datasets%252C%2520we%2520present%2520a%2520resource-efficient%250Apipeline%2520for%2520creating%2520OCR%2520datasets%2520in%2520any%2520language%252C%2520along%2520with%2520a%2520comprehensive%250Abenchmark%2520of%2520the%2520different%2520image%2520generation%2520methods%2520employed%2520with%2520a%2520focus%2520on%250AVisual%2520Rich%2520Documents%2520%2528VRDs%2529.%2520Additionally%252C%2520we%2520offer%2520a%2520comparative%2520analysis%2520of%250Athe%2520two%2520approaches%2520for%2520the%2520Spanish%2520language%252C%2520demonstrating%2520that%2520fine-tuning%2520the%250AEnglish%2520TrOCR%2520on%2520Spanish%2520yields%2520superior%2520recognition%2520than%2520the%2520language%2520specific%250Adecoder%2520for%2520a%2520fixed%2520dataset%2520size.%2520We%2520evaluate%2520our%2520model%2520employing%2520character%2520and%250Aword%2520error%2520rate%2520metrics%2520on%2520a%2520public%2520available%2520printed%2520dataset%252C%2520comparing%2520the%250Aperformance%2520against%2520other%2520open-source%2520and%2520cloud%2520OCR%2520spanish%2520models.%2520As%2520far%2520as%250Awe%2520know%252C%2520these%2520resources%2520represent%2520the%2520best%2520open-source%2520model%2520for%2520OCR%2520in%250ASpanish.%2520The%2520Spanish%2520TrOCR%2520models%2520are%2520publicly%2520available%2520on%2520HuggingFace%2520%255B20%255D%250Aand%2520the%2520code%2520to%2520generate%2520the%2520dataset%2520is%2520available%2520on%2520Github%2520%255B25%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spanish%20TrOCR%3A%20Leveraging%20Transfer%20Learning%20for%20Language%20Adaptation&entry.906535625=Filipe%20Lauar%20and%20Valentin%20Laurent&entry.1292438233=%20%20This%20study%20explores%20the%20transfer%20learning%20capabilities%20of%20the%20TrOCR%0Aarchitecture%20to%20Spanish.%20TrOCR%20is%20a%20transformer-based%20Optical%20Character%0ARecognition%20%28OCR%29%20model%20renowned%20for%20its%20state-of-the-art%20performance%20in%0AEnglish%20benchmarks.%20Inspired%20by%20Li%20et%20al.%20assertion%20regarding%20its%20adaptability%0Ato%20multilingual%20text%20recognition%2C%20we%20investigate%20two%20distinct%20approaches%20to%0Aadapt%20the%20model%20to%20a%20new%20language%3A%20integrating%20an%20English%20TrOCR%20encoder%20with%20a%0Alanguage%20specific%20decoder%20and%20train%20the%20model%20on%20this%20specific%20language%2C%20and%0Afine-tuning%20the%20English%20base%20TrOCR%20model%20on%20a%20new%20language%20data.%20Due%20to%20the%0Ascarcity%20of%20publicly%20available%20datasets%2C%20we%20present%20a%20resource-efficient%0Apipeline%20for%20creating%20OCR%20datasets%20in%20any%20language%2C%20along%20with%20a%20comprehensive%0Abenchmark%20of%20the%20different%20image%20generation%20methods%20employed%20with%20a%20focus%20on%0AVisual%20Rich%20Documents%20%28VRDs%29.%20Additionally%2C%20we%20offer%20a%20comparative%20analysis%20of%0Athe%20two%20approaches%20for%20the%20Spanish%20language%2C%20demonstrating%20that%20fine-tuning%20the%0AEnglish%20TrOCR%20on%20Spanish%20yields%20superior%20recognition%20than%20the%20language%20specific%0Adecoder%20for%20a%20fixed%20dataset%20size.%20We%20evaluate%20our%20model%20employing%20character%20and%0Aword%20error%20rate%20metrics%20on%20a%20public%20available%20printed%20dataset%2C%20comparing%20the%0Aperformance%20against%20other%20open-source%20and%20cloud%20OCR%20spanish%20models.%20As%20far%20as%0Awe%20know%2C%20these%20resources%20represent%20the%20best%20open-source%20model%20for%20OCR%20in%0ASpanish.%20The%20Spanish%20TrOCR%20models%20are%20publicly%20available%20on%20HuggingFace%20%5B20%5D%0Aand%20the%20code%20to%20generate%20the%20dataset%20is%20available%20on%20Github%20%5B25%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06950v1&entry.124074799=Read"},
{"title": "Synthesizing Realistic Data for Table Recognition", "author": "Qiyu Hou and Jun Wang and Meixuan Qiao and Lujun Tian", "abstract": "  To overcome the limitations and challenges of current automatic table data\nannotation methods and random table data synthesis approaches, we propose a\nnovel method for synthesizing annotation data specifically designed for table\nrecognition. This method utilizes the structure and content of existing complex\ntables, facilitating the efficient creation of tables that closely replicate\nthe authentic styles found in the target domain. By leveraging the actual\nstructure and content of tables from Chinese financial announcements, we have\ndeveloped the first extensive table annotation dataset in this domain. We used\nthis dataset to train several recent deep learning-based end-to-end table\nrecognition models. Additionally, we have established the inaugural benchmark\nfor real-world complex tables in the Chinese financial announcement domain,\nusing it to assess the performance of models trained on our synthetic data,\nthereby effectively validating our method's practicality and effectiveness.\nFurthermore, we applied our synthesis method to augment the FinTabNet dataset,\nextracted from English financial announcements, by increasing the proportion of\ntables with multiple spanning cells to introduce greater complexity. Our\nexperiments show that models trained on this augmented dataset achieve\ncomprehensive improvements in performance, especially in the recognition of\ntables with multiple spanning cells.\n", "link": "http://arxiv.org/abs/2404.11100v2", "date": "2024-07-09", "relevancy": 1.9655, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5012}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4875}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesizing%20Realistic%20Data%20for%20Table%20Recognition&body=Title%3A%20Synthesizing%20Realistic%20Data%20for%20Table%20Recognition%0AAuthor%3A%20Qiyu%20Hou%20and%20Jun%20Wang%20and%20Meixuan%20Qiao%20and%20Lujun%20Tian%0AAbstract%3A%20%20%20To%20overcome%20the%20limitations%20and%20challenges%20of%20current%20automatic%20table%20data%0Aannotation%20methods%20and%20random%20table%20data%20synthesis%20approaches%2C%20we%20propose%20a%0Anovel%20method%20for%20synthesizing%20annotation%20data%20specifically%20designed%20for%20table%0Arecognition.%20This%20method%20utilizes%20the%20structure%20and%20content%20of%20existing%20complex%0Atables%2C%20facilitating%20the%20efficient%20creation%20of%20tables%20that%20closely%20replicate%0Athe%20authentic%20styles%20found%20in%20the%20target%20domain.%20By%20leveraging%20the%20actual%0Astructure%20and%20content%20of%20tables%20from%20Chinese%20financial%20announcements%2C%20we%20have%0Adeveloped%20the%20first%20extensive%20table%20annotation%20dataset%20in%20this%20domain.%20We%20used%0Athis%20dataset%20to%20train%20several%20recent%20deep%20learning-based%20end-to-end%20table%0Arecognition%20models.%20Additionally%2C%20we%20have%20established%20the%20inaugural%20benchmark%0Afor%20real-world%20complex%20tables%20in%20the%20Chinese%20financial%20announcement%20domain%2C%0Ausing%20it%20to%20assess%20the%20performance%20of%20models%20trained%20on%20our%20synthetic%20data%2C%0Athereby%20effectively%20validating%20our%20method%27s%20practicality%20and%20effectiveness.%0AFurthermore%2C%20we%20applied%20our%20synthesis%20method%20to%20augment%20the%20FinTabNet%20dataset%2C%0Aextracted%20from%20English%20financial%20announcements%2C%20by%20increasing%20the%20proportion%20of%0Atables%20with%20multiple%20spanning%20cells%20to%20introduce%20greater%20complexity.%20Our%0Aexperiments%20show%20that%20models%20trained%20on%20this%20augmented%20dataset%20achieve%0Acomprehensive%20improvements%20in%20performance%2C%20especially%20in%20the%20recognition%20of%0Atables%20with%20multiple%20spanning%20cells.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizing%2520Realistic%2520Data%2520for%2520Table%2520Recognition%26entry.906535625%3DQiyu%2520Hou%2520and%2520Jun%2520Wang%2520and%2520Meixuan%2520Qiao%2520and%2520Lujun%2520Tian%26entry.1292438233%3D%2520%2520To%2520overcome%2520the%2520limitations%2520and%2520challenges%2520of%2520current%2520automatic%2520table%2520data%250Aannotation%2520methods%2520and%2520random%2520table%2520data%2520synthesis%2520approaches%252C%2520we%2520propose%2520a%250Anovel%2520method%2520for%2520synthesizing%2520annotation%2520data%2520specifically%2520designed%2520for%2520table%250Arecognition.%2520This%2520method%2520utilizes%2520the%2520structure%2520and%2520content%2520of%2520existing%2520complex%250Atables%252C%2520facilitating%2520the%2520efficient%2520creation%2520of%2520tables%2520that%2520closely%2520replicate%250Athe%2520authentic%2520styles%2520found%2520in%2520the%2520target%2520domain.%2520By%2520leveraging%2520the%2520actual%250Astructure%2520and%2520content%2520of%2520tables%2520from%2520Chinese%2520financial%2520announcements%252C%2520we%2520have%250Adeveloped%2520the%2520first%2520extensive%2520table%2520annotation%2520dataset%2520in%2520this%2520domain.%2520We%2520used%250Athis%2520dataset%2520to%2520train%2520several%2520recent%2520deep%2520learning-based%2520end-to-end%2520table%250Arecognition%2520models.%2520Additionally%252C%2520we%2520have%2520established%2520the%2520inaugural%2520benchmark%250Afor%2520real-world%2520complex%2520tables%2520in%2520the%2520Chinese%2520financial%2520announcement%2520domain%252C%250Ausing%2520it%2520to%2520assess%2520the%2520performance%2520of%2520models%2520trained%2520on%2520our%2520synthetic%2520data%252C%250Athereby%2520effectively%2520validating%2520our%2520method%2527s%2520practicality%2520and%2520effectiveness.%250AFurthermore%252C%2520we%2520applied%2520our%2520synthesis%2520method%2520to%2520augment%2520the%2520FinTabNet%2520dataset%252C%250Aextracted%2520from%2520English%2520financial%2520announcements%252C%2520by%2520increasing%2520the%2520proportion%2520of%250Atables%2520with%2520multiple%2520spanning%2520cells%2520to%2520introduce%2520greater%2520complexity.%2520Our%250Aexperiments%2520show%2520that%2520models%2520trained%2520on%2520this%2520augmented%2520dataset%2520achieve%250Acomprehensive%2520improvements%2520in%2520performance%252C%2520especially%2520in%2520the%2520recognition%2520of%250Atables%2520with%2520multiple%2520spanning%2520cells.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesizing%20Realistic%20Data%20for%20Table%20Recognition&entry.906535625=Qiyu%20Hou%20and%20Jun%20Wang%20and%20Meixuan%20Qiao%20and%20Lujun%20Tian&entry.1292438233=%20%20To%20overcome%20the%20limitations%20and%20challenges%20of%20current%20automatic%20table%20data%0Aannotation%20methods%20and%20random%20table%20data%20synthesis%20approaches%2C%20we%20propose%20a%0Anovel%20method%20for%20synthesizing%20annotation%20data%20specifically%20designed%20for%20table%0Arecognition.%20This%20method%20utilizes%20the%20structure%20and%20content%20of%20existing%20complex%0Atables%2C%20facilitating%20the%20efficient%20creation%20of%20tables%20that%20closely%20replicate%0Athe%20authentic%20styles%20found%20in%20the%20target%20domain.%20By%20leveraging%20the%20actual%0Astructure%20and%20content%20of%20tables%20from%20Chinese%20financial%20announcements%2C%20we%20have%0Adeveloped%20the%20first%20extensive%20table%20annotation%20dataset%20in%20this%20domain.%20We%20used%0Athis%20dataset%20to%20train%20several%20recent%20deep%20learning-based%20end-to-end%20table%0Arecognition%20models.%20Additionally%2C%20we%20have%20established%20the%20inaugural%20benchmark%0Afor%20real-world%20complex%20tables%20in%20the%20Chinese%20financial%20announcement%20domain%2C%0Ausing%20it%20to%20assess%20the%20performance%20of%20models%20trained%20on%20our%20synthetic%20data%2C%0Athereby%20effectively%20validating%20our%20method%27s%20practicality%20and%20effectiveness.%0AFurthermore%2C%20we%20applied%20our%20synthesis%20method%20to%20augment%20the%20FinTabNet%20dataset%2C%0Aextracted%20from%20English%20financial%20announcements%2C%20by%20increasing%20the%20proportion%20of%0Atables%20with%20multiple%20spanning%20cells%20to%20introduce%20greater%20complexity.%20Our%0Aexperiments%20show%20that%20models%20trained%20on%20this%20augmented%20dataset%20achieve%0Acomprehensive%20improvements%20in%20performance%2C%20especially%20in%20the%20recognition%20of%0Atables%20with%20multiple%20spanning%20cells.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11100v2&entry.124074799=Read"},
{"title": "Biomechanics-informed Non-rigid Medical Image Registration and its\n  Inverse Material Property Estimation with Linear and Nonlinear Elasticity", "author": "Zhe Min and Zachary M. C. Baum and Shaheer U. Saeed and Mark Emberton and Dean C. Barratt and Zeike A. Taylor and Yipeng Hu", "abstract": "  This paper investigates both biomechanical-constrained non-rigid medical\nimage registrations and accurate identifications of material properties for\nsoft tissues, using physics-informed neural networks (PINNs). The complex\nnonlinear elasticity theory is leveraged to formally establish the partial\ndifferential equations (PDEs) representing physics laws of biomechanical\nconstraints that need to be satisfied, with which registration and\nidentification tasks are treated as forward (i.e., data-driven solutions of\nPDEs) and inverse (i.e., parameter estimation) problems under PINNs\nrespectively. Two net configurations (i.e., Cfg1 and Cfg2) have also been\ncompared for both linear and nonlinear physics model. Two sets of experiments\nhave been conducted, using pairs of undeformed and deformed MR images from\nclinical cases of prostate cancer biopsy.\n  Our contributions are summarised as follows. 1) We developed a learning-based\nbiomechanical-constrained non-rigid registration algorithm using PINNs, where\nlinear elasticity is generalised to the nonlinear version. 2) We demonstrated\nextensively that nonlinear elasticity shows no statistical significance against\nlinear models in computing point-wise displacement vectors but their respective\nbenefits may depend on specific patients, with finite-element (FE) computed\nground-truth. 3) We formulated and solved the inverse parameter estimation\nproblem, under the joint optimisation scheme of registration and parameter\nidentification using PINNs, whose solutions can be accurately found by locating\nsaddle points.\n", "link": "http://arxiv.org/abs/2407.03292v2", "date": "2024-07-09", "relevancy": 1.9624, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5356}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4905}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biomechanics-informed%20Non-rigid%20Medical%20Image%20Registration%20and%20its%0A%20%20Inverse%20Material%20Property%20Estimation%20with%20Linear%20and%20Nonlinear%20Elasticity&body=Title%3A%20Biomechanics-informed%20Non-rigid%20Medical%20Image%20Registration%20and%20its%0A%20%20Inverse%20Material%20Property%20Estimation%20with%20Linear%20and%20Nonlinear%20Elasticity%0AAuthor%3A%20Zhe%20Min%20and%20Zachary%20M.%20C.%20Baum%20and%20Shaheer%20U.%20Saeed%20and%20Mark%20Emberton%20and%20Dean%20C.%20Barratt%20and%20Zeike%20A.%20Taylor%20and%20Yipeng%20Hu%0AAbstract%3A%20%20%20This%20paper%20investigates%20both%20biomechanical-constrained%20non-rigid%20medical%0Aimage%20registrations%20and%20accurate%20identifications%20of%20material%20properties%20for%0Asoft%20tissues%2C%20using%20physics-informed%20neural%20networks%20%28PINNs%29.%20The%20complex%0Anonlinear%20elasticity%20theory%20is%20leveraged%20to%20formally%20establish%20the%20partial%0Adifferential%20equations%20%28PDEs%29%20representing%20physics%20laws%20of%20biomechanical%0Aconstraints%20that%20need%20to%20be%20satisfied%2C%20with%20which%20registration%20and%0Aidentification%20tasks%20are%20treated%20as%20forward%20%28i.e.%2C%20data-driven%20solutions%20of%0APDEs%29%20and%20inverse%20%28i.e.%2C%20parameter%20estimation%29%20problems%20under%20PINNs%0Arespectively.%20Two%20net%20configurations%20%28i.e.%2C%20Cfg1%20and%20Cfg2%29%20have%20also%20been%0Acompared%20for%20both%20linear%20and%20nonlinear%20physics%20model.%20Two%20sets%20of%20experiments%0Ahave%20been%20conducted%2C%20using%20pairs%20of%20undeformed%20and%20deformed%20MR%20images%20from%0Aclinical%20cases%20of%20prostate%20cancer%20biopsy.%0A%20%20Our%20contributions%20are%20summarised%20as%20follows.%201%29%20We%20developed%20a%20learning-based%0Abiomechanical-constrained%20non-rigid%20registration%20algorithm%20using%20PINNs%2C%20where%0Alinear%20elasticity%20is%20generalised%20to%20the%20nonlinear%20version.%202%29%20We%20demonstrated%0Aextensively%20that%20nonlinear%20elasticity%20shows%20no%20statistical%20significance%20against%0Alinear%20models%20in%20computing%20point-wise%20displacement%20vectors%20but%20their%20respective%0Abenefits%20may%20depend%20on%20specific%20patients%2C%20with%20finite-element%20%28FE%29%20computed%0Aground-truth.%203%29%20We%20formulated%20and%20solved%20the%20inverse%20parameter%20estimation%0Aproblem%2C%20under%20the%20joint%20optimisation%20scheme%20of%20registration%20and%20parameter%0Aidentification%20using%20PINNs%2C%20whose%20solutions%20can%20be%20accurately%20found%20by%20locating%0Asaddle%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03292v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiomechanics-informed%2520Non-rigid%2520Medical%2520Image%2520Registration%2520and%2520its%250A%2520%2520Inverse%2520Material%2520Property%2520Estimation%2520with%2520Linear%2520and%2520Nonlinear%2520Elasticity%26entry.906535625%3DZhe%2520Min%2520and%2520Zachary%2520M.%2520C.%2520Baum%2520and%2520Shaheer%2520U.%2520Saeed%2520and%2520Mark%2520Emberton%2520and%2520Dean%2520C.%2520Barratt%2520and%2520Zeike%2520A.%2520Taylor%2520and%2520Yipeng%2520Hu%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520both%2520biomechanical-constrained%2520non-rigid%2520medical%250Aimage%2520registrations%2520and%2520accurate%2520identifications%2520of%2520material%2520properties%2520for%250Asoft%2520tissues%252C%2520using%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529.%2520The%2520complex%250Anonlinear%2520elasticity%2520theory%2520is%2520leveraged%2520to%2520formally%2520establish%2520the%2520partial%250Adifferential%2520equations%2520%2528PDEs%2529%2520representing%2520physics%2520laws%2520of%2520biomechanical%250Aconstraints%2520that%2520need%2520to%2520be%2520satisfied%252C%2520with%2520which%2520registration%2520and%250Aidentification%2520tasks%2520are%2520treated%2520as%2520forward%2520%2528i.e.%252C%2520data-driven%2520solutions%2520of%250APDEs%2529%2520and%2520inverse%2520%2528i.e.%252C%2520parameter%2520estimation%2529%2520problems%2520under%2520PINNs%250Arespectively.%2520Two%2520net%2520configurations%2520%2528i.e.%252C%2520Cfg1%2520and%2520Cfg2%2529%2520have%2520also%2520been%250Acompared%2520for%2520both%2520linear%2520and%2520nonlinear%2520physics%2520model.%2520Two%2520sets%2520of%2520experiments%250Ahave%2520been%2520conducted%252C%2520using%2520pairs%2520of%2520undeformed%2520and%2520deformed%2520MR%2520images%2520from%250Aclinical%2520cases%2520of%2520prostate%2520cancer%2520biopsy.%250A%2520%2520Our%2520contributions%2520are%2520summarised%2520as%2520follows.%25201%2529%2520We%2520developed%2520a%2520learning-based%250Abiomechanical-constrained%2520non-rigid%2520registration%2520algorithm%2520using%2520PINNs%252C%2520where%250Alinear%2520elasticity%2520is%2520generalised%2520to%2520the%2520nonlinear%2520version.%25202%2529%2520We%2520demonstrated%250Aextensively%2520that%2520nonlinear%2520elasticity%2520shows%2520no%2520statistical%2520significance%2520against%250Alinear%2520models%2520in%2520computing%2520point-wise%2520displacement%2520vectors%2520but%2520their%2520respective%250Abenefits%2520may%2520depend%2520on%2520specific%2520patients%252C%2520with%2520finite-element%2520%2528FE%2529%2520computed%250Aground-truth.%25203%2529%2520We%2520formulated%2520and%2520solved%2520the%2520inverse%2520parameter%2520estimation%250Aproblem%252C%2520under%2520the%2520joint%2520optimisation%2520scheme%2520of%2520registration%2520and%2520parameter%250Aidentification%2520using%2520PINNs%252C%2520whose%2520solutions%2520can%2520be%2520accurately%2520found%2520by%2520locating%250Asaddle%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03292v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biomechanics-informed%20Non-rigid%20Medical%20Image%20Registration%20and%20its%0A%20%20Inverse%20Material%20Property%20Estimation%20with%20Linear%20and%20Nonlinear%20Elasticity&entry.906535625=Zhe%20Min%20and%20Zachary%20M.%20C.%20Baum%20and%20Shaheer%20U.%20Saeed%20and%20Mark%20Emberton%20and%20Dean%20C.%20Barratt%20and%20Zeike%20A.%20Taylor%20and%20Yipeng%20Hu&entry.1292438233=%20%20This%20paper%20investigates%20both%20biomechanical-constrained%20non-rigid%20medical%0Aimage%20registrations%20and%20accurate%20identifications%20of%20material%20properties%20for%0Asoft%20tissues%2C%20using%20physics-informed%20neural%20networks%20%28PINNs%29.%20The%20complex%0Anonlinear%20elasticity%20theory%20is%20leveraged%20to%20formally%20establish%20the%20partial%0Adifferential%20equations%20%28PDEs%29%20representing%20physics%20laws%20of%20biomechanical%0Aconstraints%20that%20need%20to%20be%20satisfied%2C%20with%20which%20registration%20and%0Aidentification%20tasks%20are%20treated%20as%20forward%20%28i.e.%2C%20data-driven%20solutions%20of%0APDEs%29%20and%20inverse%20%28i.e.%2C%20parameter%20estimation%29%20problems%20under%20PINNs%0Arespectively.%20Two%20net%20configurations%20%28i.e.%2C%20Cfg1%20and%20Cfg2%29%20have%20also%20been%0Acompared%20for%20both%20linear%20and%20nonlinear%20physics%20model.%20Two%20sets%20of%20experiments%0Ahave%20been%20conducted%2C%20using%20pairs%20of%20undeformed%20and%20deformed%20MR%20images%20from%0Aclinical%20cases%20of%20prostate%20cancer%20biopsy.%0A%20%20Our%20contributions%20are%20summarised%20as%20follows.%201%29%20We%20developed%20a%20learning-based%0Abiomechanical-constrained%20non-rigid%20registration%20algorithm%20using%20PINNs%2C%20where%0Alinear%20elasticity%20is%20generalised%20to%20the%20nonlinear%20version.%202%29%20We%20demonstrated%0Aextensively%20that%20nonlinear%20elasticity%20shows%20no%20statistical%20significance%20against%0Alinear%20models%20in%20computing%20point-wise%20displacement%20vectors%20but%20their%20respective%0Abenefits%20may%20depend%20on%20specific%20patients%2C%20with%20finite-element%20%28FE%29%20computed%0Aground-truth.%203%29%20We%20formulated%20and%20solved%20the%20inverse%20parameter%20estimation%0Aproblem%2C%20under%20the%20joint%20optimisation%20scheme%20of%20registration%20and%20parameter%0Aidentification%20using%20PINNs%2C%20whose%20solutions%20can%20be%20accurately%20found%20by%20locating%0Asaddle%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03292v2&entry.124074799=Read"},
{"title": "Fine-Tuning Linear Layers Only Is a Simple yet Effective Way for Task\n  Arithmetic", "author": "Ruochen Jin and Bojian Hou and Jiancong Xiao and Weijie Su and Li Shen", "abstract": "  Task arithmetic has recently emerged as a cost-effective and scalable\napproach to edit pre-trained models directly in weight space, by adding the\nfine-tuned weights of different tasks. The performance has been further\nimproved by a linear property which is illustrated by weight disentanglement.\nYet, conventional linearization methods (e.g., NTK linearization) not only\ndouble the time and training cost but also have a disadvantage on single-task\nperformance. We propose a simple yet effective and efficient method that only\nfine-tunes linear layers, which improves weight disentanglement and efficiency\nsimultaneously. Specifically, our study reveals that only fine-tuning the\nlinear layers in the attention modules makes the whole model occur in a linear\nregime, significantly improving weight disentanglement. To further understand\nhow our method improves the disentanglement of task arithmetic, we present a\ncomprehensive study of task arithmetic by differentiating the role of\nrepresentation model and task-specific model. In particular, we find that the\nrepresentation model plays an important role in improving weight\ndisentanglement whereas the task-specific models such as the classification\nheads can degenerate the weight disentanglement performance. Overall, our work\nuncovers novel insights into the fundamental mechanisms of task arithmetic and\noffers a more reliable and effective approach to editing pre-trained models.\n", "link": "http://arxiv.org/abs/2407.07089v1", "date": "2024-07-09", "relevancy": 1.9565, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5113}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4828}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuning%20Linear%20Layers%20Only%20Is%20a%20Simple%20yet%20Effective%20Way%20for%20Task%0A%20%20Arithmetic&body=Title%3A%20Fine-Tuning%20Linear%20Layers%20Only%20Is%20a%20Simple%20yet%20Effective%20Way%20for%20Task%0A%20%20Arithmetic%0AAuthor%3A%20Ruochen%20Jin%20and%20Bojian%20Hou%20and%20Jiancong%20Xiao%20and%20Weijie%20Su%20and%20Li%20Shen%0AAbstract%3A%20%20%20Task%20arithmetic%20has%20recently%20emerged%20as%20a%20cost-effective%20and%20scalable%0Aapproach%20to%20edit%20pre-trained%20models%20directly%20in%20weight%20space%2C%20by%20adding%20the%0Afine-tuned%20weights%20of%20different%20tasks.%20The%20performance%20has%20been%20further%0Aimproved%20by%20a%20linear%20property%20which%20is%20illustrated%20by%20weight%20disentanglement.%0AYet%2C%20conventional%20linearization%20methods%20%28e.g.%2C%20NTK%20linearization%29%20not%20only%0Adouble%20the%20time%20and%20training%20cost%20but%20also%20have%20a%20disadvantage%20on%20single-task%0Aperformance.%20We%20propose%20a%20simple%20yet%20effective%20and%20efficient%20method%20that%20only%0Afine-tunes%20linear%20layers%2C%20which%20improves%20weight%20disentanglement%20and%20efficiency%0Asimultaneously.%20Specifically%2C%20our%20study%20reveals%20that%20only%20fine-tuning%20the%0Alinear%20layers%20in%20the%20attention%20modules%20makes%20the%20whole%20model%20occur%20in%20a%20linear%0Aregime%2C%20significantly%20improving%20weight%20disentanglement.%20To%20further%20understand%0Ahow%20our%20method%20improves%20the%20disentanglement%20of%20task%20arithmetic%2C%20we%20present%20a%0Acomprehensive%20study%20of%20task%20arithmetic%20by%20differentiating%20the%20role%20of%0Arepresentation%20model%20and%20task-specific%20model.%20In%20particular%2C%20we%20find%20that%20the%0Arepresentation%20model%20plays%20an%20important%20role%20in%20improving%20weight%0Adisentanglement%20whereas%20the%20task-specific%20models%20such%20as%20the%20classification%0Aheads%20can%20degenerate%20the%20weight%20disentanglement%20performance.%20Overall%2C%20our%20work%0Auncovers%20novel%20insights%20into%20the%20fundamental%20mechanisms%20of%20task%20arithmetic%20and%0Aoffers%20a%20more%20reliable%20and%20effective%20approach%20to%20editing%20pre-trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuning%2520Linear%2520Layers%2520Only%2520Is%2520a%2520Simple%2520yet%2520Effective%2520Way%2520for%2520Task%250A%2520%2520Arithmetic%26entry.906535625%3DRuochen%2520Jin%2520and%2520Bojian%2520Hou%2520and%2520Jiancong%2520Xiao%2520and%2520Weijie%2520Su%2520and%2520Li%2520Shen%26entry.1292438233%3D%2520%2520Task%2520arithmetic%2520has%2520recently%2520emerged%2520as%2520a%2520cost-effective%2520and%2520scalable%250Aapproach%2520to%2520edit%2520pre-trained%2520models%2520directly%2520in%2520weight%2520space%252C%2520by%2520adding%2520the%250Afine-tuned%2520weights%2520of%2520different%2520tasks.%2520The%2520performance%2520has%2520been%2520further%250Aimproved%2520by%2520a%2520linear%2520property%2520which%2520is%2520illustrated%2520by%2520weight%2520disentanglement.%250AYet%252C%2520conventional%2520linearization%2520methods%2520%2528e.g.%252C%2520NTK%2520linearization%2529%2520not%2520only%250Adouble%2520the%2520time%2520and%2520training%2520cost%2520but%2520also%2520have%2520a%2520disadvantage%2520on%2520single-task%250Aperformance.%2520We%2520propose%2520a%2520simple%2520yet%2520effective%2520and%2520efficient%2520method%2520that%2520only%250Afine-tunes%2520linear%2520layers%252C%2520which%2520improves%2520weight%2520disentanglement%2520and%2520efficiency%250Asimultaneously.%2520Specifically%252C%2520our%2520study%2520reveals%2520that%2520only%2520fine-tuning%2520the%250Alinear%2520layers%2520in%2520the%2520attention%2520modules%2520makes%2520the%2520whole%2520model%2520occur%2520in%2520a%2520linear%250Aregime%252C%2520significantly%2520improving%2520weight%2520disentanglement.%2520To%2520further%2520understand%250Ahow%2520our%2520method%2520improves%2520the%2520disentanglement%2520of%2520task%2520arithmetic%252C%2520we%2520present%2520a%250Acomprehensive%2520study%2520of%2520task%2520arithmetic%2520by%2520differentiating%2520the%2520role%2520of%250Arepresentation%2520model%2520and%2520task-specific%2520model.%2520In%2520particular%252C%2520we%2520find%2520that%2520the%250Arepresentation%2520model%2520plays%2520an%2520important%2520role%2520in%2520improving%2520weight%250Adisentanglement%2520whereas%2520the%2520task-specific%2520models%2520such%2520as%2520the%2520classification%250Aheads%2520can%2520degenerate%2520the%2520weight%2520disentanglement%2520performance.%2520Overall%252C%2520our%2520work%250Auncovers%2520novel%2520insights%2520into%2520the%2520fundamental%2520mechanisms%2520of%2520task%2520arithmetic%2520and%250Aoffers%2520a%2520more%2520reliable%2520and%2520effective%2520approach%2520to%2520editing%2520pre-trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuning%20Linear%20Layers%20Only%20Is%20a%20Simple%20yet%20Effective%20Way%20for%20Task%0A%20%20Arithmetic&entry.906535625=Ruochen%20Jin%20and%20Bojian%20Hou%20and%20Jiancong%20Xiao%20and%20Weijie%20Su%20and%20Li%20Shen&entry.1292438233=%20%20Task%20arithmetic%20has%20recently%20emerged%20as%20a%20cost-effective%20and%20scalable%0Aapproach%20to%20edit%20pre-trained%20models%20directly%20in%20weight%20space%2C%20by%20adding%20the%0Afine-tuned%20weights%20of%20different%20tasks.%20The%20performance%20has%20been%20further%0Aimproved%20by%20a%20linear%20property%20which%20is%20illustrated%20by%20weight%20disentanglement.%0AYet%2C%20conventional%20linearization%20methods%20%28e.g.%2C%20NTK%20linearization%29%20not%20only%0Adouble%20the%20time%20and%20training%20cost%20but%20also%20have%20a%20disadvantage%20on%20single-task%0Aperformance.%20We%20propose%20a%20simple%20yet%20effective%20and%20efficient%20method%20that%20only%0Afine-tunes%20linear%20layers%2C%20which%20improves%20weight%20disentanglement%20and%20efficiency%0Asimultaneously.%20Specifically%2C%20our%20study%20reveals%20that%20only%20fine-tuning%20the%0Alinear%20layers%20in%20the%20attention%20modules%20makes%20the%20whole%20model%20occur%20in%20a%20linear%0Aregime%2C%20significantly%20improving%20weight%20disentanglement.%20To%20further%20understand%0Ahow%20our%20method%20improves%20the%20disentanglement%20of%20task%20arithmetic%2C%20we%20present%20a%0Acomprehensive%20study%20of%20task%20arithmetic%20by%20differentiating%20the%20role%20of%0Arepresentation%20model%20and%20task-specific%20model.%20In%20particular%2C%20we%20find%20that%20the%0Arepresentation%20model%20plays%20an%20important%20role%20in%20improving%20weight%0Adisentanglement%20whereas%20the%20task-specific%20models%20such%20as%20the%20classification%0Aheads%20can%20degenerate%20the%20weight%20disentanglement%20performance.%20Overall%2C%20our%20work%0Auncovers%20novel%20insights%20into%20the%20fundamental%20mechanisms%20of%20task%20arithmetic%20and%0Aoffers%20a%20more%20reliable%20and%20effective%20approach%20to%20editing%20pre-trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07089v1&entry.124074799=Read"},
{"title": "Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era\n  of Foundation Models", "author": "Yue Zhang and Ziqiao Ma and Jialu Li and Yanyuan Qiao and Zun Wang and Joyce Chai and Qi Wu and Mohit Bansal and Parisa Kordjamshidi", "abstract": "  Vision-and-Language Navigation (VLN) has gained increasing attention over\nrecent years and many approaches have emerged to advance their development. The\nremarkable achievements of foundation models have shaped the challenges and\nproposed methods for VLN research. In this survey, we provide a top-down review\nthat adopts a principled framework for embodied planning and reasoning, and\nemphasizes the current methods and future opportunities leveraging foundation\nmodels to address VLN challenges. We hope our in-depth discussions could\nprovide valuable resources and insights: on one hand, to milestone the progress\nand explore opportunities and potential roles for foundation models in this\nfield, and on the other, to organize different challenges and solutions in VLN\nto foundation model researchers.\n", "link": "http://arxiv.org/abs/2407.07035v1", "date": "2024-07-09", "relevancy": 1.955, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4922}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4911}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-and-Language%20Navigation%20Today%20and%20Tomorrow%3A%20A%20Survey%20in%20the%20Era%0A%20%20of%20Foundation%20Models&body=Title%3A%20Vision-and-Language%20Navigation%20Today%20and%20Tomorrow%3A%20A%20Survey%20in%20the%20Era%0A%20%20of%20Foundation%20Models%0AAuthor%3A%20Yue%20Zhang%20and%20Ziqiao%20Ma%20and%20Jialu%20Li%20and%20Yanyuan%20Qiao%20and%20Zun%20Wang%20and%20Joyce%20Chai%20and%20Qi%20Wu%20and%20Mohit%20Bansal%20and%20Parisa%20Kordjamshidi%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20%28VLN%29%20has%20gained%20increasing%20attention%20over%0Arecent%20years%20and%20many%20approaches%20have%20emerged%20to%20advance%20their%20development.%20The%0Aremarkable%20achievements%20of%20foundation%20models%20have%20shaped%20the%20challenges%20and%0Aproposed%20methods%20for%20VLN%20research.%20In%20this%20survey%2C%20we%20provide%20a%20top-down%20review%0Athat%20adopts%20a%20principled%20framework%20for%20embodied%20planning%20and%20reasoning%2C%20and%0Aemphasizes%20the%20current%20methods%20and%20future%20opportunities%20leveraging%20foundation%0Amodels%20to%20address%20VLN%20challenges.%20We%20hope%20our%20in-depth%20discussions%20could%0Aprovide%20valuable%20resources%20and%20insights%3A%20on%20one%20hand%2C%20to%20milestone%20the%20progress%0Aand%20explore%20opportunities%20and%20potential%20roles%20for%20foundation%20models%20in%20this%0Afield%2C%20and%20on%20the%20other%2C%20to%20organize%20different%20challenges%20and%20solutions%20in%20VLN%0Ato%20foundation%20model%20researchers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-and-Language%2520Navigation%2520Today%2520and%2520Tomorrow%253A%2520A%2520Survey%2520in%2520the%2520Era%250A%2520%2520of%2520Foundation%2520Models%26entry.906535625%3DYue%2520Zhang%2520and%2520Ziqiao%2520Ma%2520and%2520Jialu%2520Li%2520and%2520Yanyuan%2520Qiao%2520and%2520Zun%2520Wang%2520and%2520Joyce%2520Chai%2520and%2520Qi%2520Wu%2520and%2520Mohit%2520Bansal%2520and%2520Parisa%2520Kordjamshidi%26entry.1292438233%3D%2520%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520has%2520gained%2520increasing%2520attention%2520over%250Arecent%2520years%2520and%2520many%2520approaches%2520have%2520emerged%2520to%2520advance%2520their%2520development.%2520The%250Aremarkable%2520achievements%2520of%2520foundation%2520models%2520have%2520shaped%2520the%2520challenges%2520and%250Aproposed%2520methods%2520for%2520VLN%2520research.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%2520top-down%2520review%250Athat%2520adopts%2520a%2520principled%2520framework%2520for%2520embodied%2520planning%2520and%2520reasoning%252C%2520and%250Aemphasizes%2520the%2520current%2520methods%2520and%2520future%2520opportunities%2520leveraging%2520foundation%250Amodels%2520to%2520address%2520VLN%2520challenges.%2520We%2520hope%2520our%2520in-depth%2520discussions%2520could%250Aprovide%2520valuable%2520resources%2520and%2520insights%253A%2520on%2520one%2520hand%252C%2520to%2520milestone%2520the%2520progress%250Aand%2520explore%2520opportunities%2520and%2520potential%2520roles%2520for%2520foundation%2520models%2520in%2520this%250Afield%252C%2520and%2520on%2520the%2520other%252C%2520to%2520organize%2520different%2520challenges%2520and%2520solutions%2520in%2520VLN%250Ato%2520foundation%2520model%2520researchers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-and-Language%20Navigation%20Today%20and%20Tomorrow%3A%20A%20Survey%20in%20the%20Era%0A%20%20of%20Foundation%20Models&entry.906535625=Yue%20Zhang%20and%20Ziqiao%20Ma%20and%20Jialu%20Li%20and%20Yanyuan%20Qiao%20and%20Zun%20Wang%20and%20Joyce%20Chai%20and%20Qi%20Wu%20and%20Mohit%20Bansal%20and%20Parisa%20Kordjamshidi&entry.1292438233=%20%20Vision-and-Language%20Navigation%20%28VLN%29%20has%20gained%20increasing%20attention%20over%0Arecent%20years%20and%20many%20approaches%20have%20emerged%20to%20advance%20their%20development.%20The%0Aremarkable%20achievements%20of%20foundation%20models%20have%20shaped%20the%20challenges%20and%0Aproposed%20methods%20for%20VLN%20research.%20In%20this%20survey%2C%20we%20provide%20a%20top-down%20review%0Athat%20adopts%20a%20principled%20framework%20for%20embodied%20planning%20and%20reasoning%2C%20and%0Aemphasizes%20the%20current%20methods%20and%20future%20opportunities%20leveraging%20foundation%0Amodels%20to%20address%20VLN%20challenges.%20We%20hope%20our%20in-depth%20discussions%20could%0Aprovide%20valuable%20resources%20and%20insights%3A%20on%20one%20hand%2C%20to%20milestone%20the%20progress%0Aand%20explore%20opportunities%20and%20potential%20roles%20for%20foundation%20models%20in%20this%0Afield%2C%20and%20on%20the%20other%2C%20to%20organize%20different%20challenges%20and%20solutions%20in%20VLN%0Ato%20foundation%20model%20researchers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07035v1&entry.124074799=Read"},
{"title": "Low latency optical-based mode tracking with machine learning deployed\n  on FPGAs on a tokamak", "author": "Yumou Wei and Ryan F. Forelli and Chris Hansen and Jeffrey P. Levesque and Nhan Tran and Joshua C. Agar and Giuseppe Di Guglielmo and Michael E. Mauel and Gerald A. Navratil", "abstract": "  Active feedback control in magnetic confinement fusion devices is desirable\nto mitigate plasma instabilities and enable robust operation. Optical\nhigh-speed cameras provide a powerful, non-invasive diagnostic and can be\nsuitable for these applications. In this study, we process fast camera data, at\nrates exceeding 100kfps, on $\\textit{in situ}$ Field Programmable Gate Array\n(FPGA) hardware to track magnetohydrodynamic (MHD) mode evolution and generate\ncontrol signals in real-time. Our system utilizes a convolutional neural\nnetwork (CNN) model which predicts the $n$=1 MHD mode amplitude and phase using\ncamera images with better accuracy than other tested non-deep-learning-based\nmethods. By implementing this model directly within the standard FPGA readout\nhardware of the high-speed camera diagnostic, our mode tracking system achieves\na total trigger-to-output latency of 17.6$\\mu$s and a throughput of up to\n120kfps. This study at the High Beta Tokamak-Extended Pulse (HBT-EP) experiment\ndemonstrates an FPGA-based high-speed camera data acquisition and processing\nsystem, enabling application in real-time machine-learning-based tokamak\ndiagnostic and control as well as potential applications in other scientific\ndomains.\n", "link": "http://arxiv.org/abs/2312.00128v3", "date": "2024-07-09", "relevancy": 1.9492, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.495}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4898}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low%20latency%20optical-based%20mode%20tracking%20with%20machine%20learning%20deployed%0A%20%20on%20FPGAs%20on%20a%20tokamak&body=Title%3A%20Low%20latency%20optical-based%20mode%20tracking%20with%20machine%20learning%20deployed%0A%20%20on%20FPGAs%20on%20a%20tokamak%0AAuthor%3A%20Yumou%20Wei%20and%20Ryan%20F.%20Forelli%20and%20Chris%20Hansen%20and%20Jeffrey%20P.%20Levesque%20and%20Nhan%20Tran%20and%20Joshua%20C.%20Agar%20and%20Giuseppe%20Di%20Guglielmo%20and%20Michael%20E.%20Mauel%20and%20Gerald%20A.%20Navratil%0AAbstract%3A%20%20%20Active%20feedback%20control%20in%20magnetic%20confinement%20fusion%20devices%20is%20desirable%0Ato%20mitigate%20plasma%20instabilities%20and%20enable%20robust%20operation.%20Optical%0Ahigh-speed%20cameras%20provide%20a%20powerful%2C%20non-invasive%20diagnostic%20and%20can%20be%0Asuitable%20for%20these%20applications.%20In%20this%20study%2C%20we%20process%20fast%20camera%20data%2C%20at%0Arates%20exceeding%20100kfps%2C%20on%20%24%5Ctextit%7Bin%20situ%7D%24%20Field%20Programmable%20Gate%20Array%0A%28FPGA%29%20hardware%20to%20track%20magnetohydrodynamic%20%28MHD%29%20mode%20evolution%20and%20generate%0Acontrol%20signals%20in%20real-time.%20Our%20system%20utilizes%20a%20convolutional%20neural%0Anetwork%20%28CNN%29%20model%20which%20predicts%20the%20%24n%24%3D1%20MHD%20mode%20amplitude%20and%20phase%20using%0Acamera%20images%20with%20better%20accuracy%20than%20other%20tested%20non-deep-learning-based%0Amethods.%20By%20implementing%20this%20model%20directly%20within%20the%20standard%20FPGA%20readout%0Ahardware%20of%20the%20high-speed%20camera%20diagnostic%2C%20our%20mode%20tracking%20system%20achieves%0Aa%20total%20trigger-to-output%20latency%20of%2017.6%24%5Cmu%24s%20and%20a%20throughput%20of%20up%20to%0A120kfps.%20This%20study%20at%20the%20High%20Beta%20Tokamak-Extended%20Pulse%20%28HBT-EP%29%20experiment%0Ademonstrates%20an%20FPGA-based%20high-speed%20camera%20data%20acquisition%20and%20processing%0Asystem%2C%20enabling%20application%20in%20real-time%20machine-learning-based%20tokamak%0Adiagnostic%20and%20control%20as%20well%20as%20potential%20applications%20in%20other%20scientific%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00128v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow%2520latency%2520optical-based%2520mode%2520tracking%2520with%2520machine%2520learning%2520deployed%250A%2520%2520on%2520FPGAs%2520on%2520a%2520tokamak%26entry.906535625%3DYumou%2520Wei%2520and%2520Ryan%2520F.%2520Forelli%2520and%2520Chris%2520Hansen%2520and%2520Jeffrey%2520P.%2520Levesque%2520and%2520Nhan%2520Tran%2520and%2520Joshua%2520C.%2520Agar%2520and%2520Giuseppe%2520Di%2520Guglielmo%2520and%2520Michael%2520E.%2520Mauel%2520and%2520Gerald%2520A.%2520Navratil%26entry.1292438233%3D%2520%2520Active%2520feedback%2520control%2520in%2520magnetic%2520confinement%2520fusion%2520devices%2520is%2520desirable%250Ato%2520mitigate%2520plasma%2520instabilities%2520and%2520enable%2520robust%2520operation.%2520Optical%250Ahigh-speed%2520cameras%2520provide%2520a%2520powerful%252C%2520non-invasive%2520diagnostic%2520and%2520can%2520be%250Asuitable%2520for%2520these%2520applications.%2520In%2520this%2520study%252C%2520we%2520process%2520fast%2520camera%2520data%252C%2520at%250Arates%2520exceeding%2520100kfps%252C%2520on%2520%2524%255Ctextit%257Bin%2520situ%257D%2524%2520Field%2520Programmable%2520Gate%2520Array%250A%2528FPGA%2529%2520hardware%2520to%2520track%2520magnetohydrodynamic%2520%2528MHD%2529%2520mode%2520evolution%2520and%2520generate%250Acontrol%2520signals%2520in%2520real-time.%2520Our%2520system%2520utilizes%2520a%2520convolutional%2520neural%250Anetwork%2520%2528CNN%2529%2520model%2520which%2520predicts%2520the%2520%2524n%2524%253D1%2520MHD%2520mode%2520amplitude%2520and%2520phase%2520using%250Acamera%2520images%2520with%2520better%2520accuracy%2520than%2520other%2520tested%2520non-deep-learning-based%250Amethods.%2520By%2520implementing%2520this%2520model%2520directly%2520within%2520the%2520standard%2520FPGA%2520readout%250Ahardware%2520of%2520the%2520high-speed%2520camera%2520diagnostic%252C%2520our%2520mode%2520tracking%2520system%2520achieves%250Aa%2520total%2520trigger-to-output%2520latency%2520of%252017.6%2524%255Cmu%2524s%2520and%2520a%2520throughput%2520of%2520up%2520to%250A120kfps.%2520This%2520study%2520at%2520the%2520High%2520Beta%2520Tokamak-Extended%2520Pulse%2520%2528HBT-EP%2529%2520experiment%250Ademonstrates%2520an%2520FPGA-based%2520high-speed%2520camera%2520data%2520acquisition%2520and%2520processing%250Asystem%252C%2520enabling%2520application%2520in%2520real-time%2520machine-learning-based%2520tokamak%250Adiagnostic%2520and%2520control%2520as%2520well%2520as%2520potential%2520applications%2520in%2520other%2520scientific%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00128v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low%20latency%20optical-based%20mode%20tracking%20with%20machine%20learning%20deployed%0A%20%20on%20FPGAs%20on%20a%20tokamak&entry.906535625=Yumou%20Wei%20and%20Ryan%20F.%20Forelli%20and%20Chris%20Hansen%20and%20Jeffrey%20P.%20Levesque%20and%20Nhan%20Tran%20and%20Joshua%20C.%20Agar%20and%20Giuseppe%20Di%20Guglielmo%20and%20Michael%20E.%20Mauel%20and%20Gerald%20A.%20Navratil&entry.1292438233=%20%20Active%20feedback%20control%20in%20magnetic%20confinement%20fusion%20devices%20is%20desirable%0Ato%20mitigate%20plasma%20instabilities%20and%20enable%20robust%20operation.%20Optical%0Ahigh-speed%20cameras%20provide%20a%20powerful%2C%20non-invasive%20diagnostic%20and%20can%20be%0Asuitable%20for%20these%20applications.%20In%20this%20study%2C%20we%20process%20fast%20camera%20data%2C%20at%0Arates%20exceeding%20100kfps%2C%20on%20%24%5Ctextit%7Bin%20situ%7D%24%20Field%20Programmable%20Gate%20Array%0A%28FPGA%29%20hardware%20to%20track%20magnetohydrodynamic%20%28MHD%29%20mode%20evolution%20and%20generate%0Acontrol%20signals%20in%20real-time.%20Our%20system%20utilizes%20a%20convolutional%20neural%0Anetwork%20%28CNN%29%20model%20which%20predicts%20the%20%24n%24%3D1%20MHD%20mode%20amplitude%20and%20phase%20using%0Acamera%20images%20with%20better%20accuracy%20than%20other%20tested%20non-deep-learning-based%0Amethods.%20By%20implementing%20this%20model%20directly%20within%20the%20standard%20FPGA%20readout%0Ahardware%20of%20the%20high-speed%20camera%20diagnostic%2C%20our%20mode%20tracking%20system%20achieves%0Aa%20total%20trigger-to-output%20latency%20of%2017.6%24%5Cmu%24s%20and%20a%20throughput%20of%20up%20to%0A120kfps.%20This%20study%20at%20the%20High%20Beta%20Tokamak-Extended%20Pulse%20%28HBT-EP%29%20experiment%0Ademonstrates%20an%20FPGA-based%20high-speed%20camera%20data%20acquisition%20and%20processing%0Asystem%2C%20enabling%20application%20in%20real-time%20machine-learning-based%20tokamak%0Adiagnostic%20and%20control%20as%20well%20as%20potential%20applications%20in%20other%20scientific%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00128v3&entry.124074799=Read"},
{"title": "Subject-Adaptive Transfer Learning Using Resting State EEG Signals for\n  Cross-Subject EEG Motor Imagery Classification", "author": "Sion An and Myeongkyun Kang and Soopil Kim and Philip Chikontwe and Li Shen and Sang Hyun Park", "abstract": "  Electroencephalography (EEG) motor imagery (MI) classification is a\nfundamental, yet challenging task due to the variation of signals between\nindividuals i.e., inter-subject variability. Previous approaches try to\nmitigate this using task-specific (TS) EEG signals from the target subject in\ntraining. However, recording TS EEG signals requires time and limits its\napplicability in various fields. In contrast, resting state (RS) EEG signals\nare a viable alternative due to ease of acquisition with rich subject\ninformation. In this paper, we propose a novel subject-adaptive transfer\nlearning strategy that utilizes RS EEG signals to adapt models on unseen\nsubject data. Specifically, we disentangle extracted features into task- and\nsubject-dependent features and use them to calibrate RS EEG signals for\nobtaining task information while preserving subject characteristics. The\ncalibrated signals are then used to adapt the model to the target subject,\nenabling the model to simulate processing TS EEG signals of the target subject.\nThe proposed method achieves state-of-the-art accuracy on three public\nbenchmarks, demonstrating the effectiveness of our method in cross-subject EEG\nMI classification. Our findings highlight the potential of leveraging RS EEG\nsignals to advance practical brain-computer interface systems. The code is\navailable at https://github.com/SionAn/MICCAI2024-ResTL.\n", "link": "http://arxiv.org/abs/2405.19346v2", "date": "2024-07-09", "relevancy": 1.9457, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4919}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subject-Adaptive%20Transfer%20Learning%20Using%20Resting%20State%20EEG%20Signals%20for%0A%20%20Cross-Subject%20EEG%20Motor%20Imagery%20Classification&body=Title%3A%20Subject-Adaptive%20Transfer%20Learning%20Using%20Resting%20State%20EEG%20Signals%20for%0A%20%20Cross-Subject%20EEG%20Motor%20Imagery%20Classification%0AAuthor%3A%20Sion%20An%20and%20Myeongkyun%20Kang%20and%20Soopil%20Kim%20and%20Philip%20Chikontwe%20and%20Li%20Shen%20and%20Sang%20Hyun%20Park%0AAbstract%3A%20%20%20Electroencephalography%20%28EEG%29%20motor%20imagery%20%28MI%29%20classification%20is%20a%0Afundamental%2C%20yet%20challenging%20task%20due%20to%20the%20variation%20of%20signals%20between%0Aindividuals%20i.e.%2C%20inter-subject%20variability.%20Previous%20approaches%20try%20to%0Amitigate%20this%20using%20task-specific%20%28TS%29%20EEG%20signals%20from%20the%20target%20subject%20in%0Atraining.%20However%2C%20recording%20TS%20EEG%20signals%20requires%20time%20and%20limits%20its%0Aapplicability%20in%20various%20fields.%20In%20contrast%2C%20resting%20state%20%28RS%29%20EEG%20signals%0Aare%20a%20viable%20alternative%20due%20to%20ease%20of%20acquisition%20with%20rich%20subject%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20subject-adaptive%20transfer%0Alearning%20strategy%20that%20utilizes%20RS%20EEG%20signals%20to%20adapt%20models%20on%20unseen%0Asubject%20data.%20Specifically%2C%20we%20disentangle%20extracted%20features%20into%20task-%20and%0Asubject-dependent%20features%20and%20use%20them%20to%20calibrate%20RS%20EEG%20signals%20for%0Aobtaining%20task%20information%20while%20preserving%20subject%20characteristics.%20The%0Acalibrated%20signals%20are%20then%20used%20to%20adapt%20the%20model%20to%20the%20target%20subject%2C%0Aenabling%20the%20model%20to%20simulate%20processing%20TS%20EEG%20signals%20of%20the%20target%20subject.%0AThe%20proposed%20method%20achieves%20state-of-the-art%20accuracy%20on%20three%20public%0Abenchmarks%2C%20demonstrating%20the%20effectiveness%20of%20our%20method%20in%20cross-subject%20EEG%0AMI%20classification.%20Our%20findings%20highlight%20the%20potential%20of%20leveraging%20RS%20EEG%0Asignals%20to%20advance%20practical%20brain-computer%20interface%20systems.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/SionAn/MICCAI2024-ResTL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubject-Adaptive%2520Transfer%2520Learning%2520Using%2520Resting%2520State%2520EEG%2520Signals%2520for%250A%2520%2520Cross-Subject%2520EEG%2520Motor%2520Imagery%2520Classification%26entry.906535625%3DSion%2520An%2520and%2520Myeongkyun%2520Kang%2520and%2520Soopil%2520Kim%2520and%2520Philip%2520Chikontwe%2520and%2520Li%2520Shen%2520and%2520Sang%2520Hyun%2520Park%26entry.1292438233%3D%2520%2520Electroencephalography%2520%2528EEG%2529%2520motor%2520imagery%2520%2528MI%2529%2520classification%2520is%2520a%250Afundamental%252C%2520yet%2520challenging%2520task%2520due%2520to%2520the%2520variation%2520of%2520signals%2520between%250Aindividuals%2520i.e.%252C%2520inter-subject%2520variability.%2520Previous%2520approaches%2520try%2520to%250Amitigate%2520this%2520using%2520task-specific%2520%2528TS%2529%2520EEG%2520signals%2520from%2520the%2520target%2520subject%2520in%250Atraining.%2520However%252C%2520recording%2520TS%2520EEG%2520signals%2520requires%2520time%2520and%2520limits%2520its%250Aapplicability%2520in%2520various%2520fields.%2520In%2520contrast%252C%2520resting%2520state%2520%2528RS%2529%2520EEG%2520signals%250Aare%2520a%2520viable%2520alternative%2520due%2520to%2520ease%2520of%2520acquisition%2520with%2520rich%2520subject%250Ainformation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520subject-adaptive%2520transfer%250Alearning%2520strategy%2520that%2520utilizes%2520RS%2520EEG%2520signals%2520to%2520adapt%2520models%2520on%2520unseen%250Asubject%2520data.%2520Specifically%252C%2520we%2520disentangle%2520extracted%2520features%2520into%2520task-%2520and%250Asubject-dependent%2520features%2520and%2520use%2520them%2520to%2520calibrate%2520RS%2520EEG%2520signals%2520for%250Aobtaining%2520task%2520information%2520while%2520preserving%2520subject%2520characteristics.%2520The%250Acalibrated%2520signals%2520are%2520then%2520used%2520to%2520adapt%2520the%2520model%2520to%2520the%2520target%2520subject%252C%250Aenabling%2520the%2520model%2520to%2520simulate%2520processing%2520TS%2520EEG%2520signals%2520of%2520the%2520target%2520subject.%250AThe%2520proposed%2520method%2520achieves%2520state-of-the-art%2520accuracy%2520on%2520three%2520public%250Abenchmarks%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520cross-subject%2520EEG%250AMI%2520classification.%2520Our%2520findings%2520highlight%2520the%2520potential%2520of%2520leveraging%2520RS%2520EEG%250Asignals%2520to%2520advance%2520practical%2520brain-computer%2520interface%2520systems.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/SionAn/MICCAI2024-ResTL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subject-Adaptive%20Transfer%20Learning%20Using%20Resting%20State%20EEG%20Signals%20for%0A%20%20Cross-Subject%20EEG%20Motor%20Imagery%20Classification&entry.906535625=Sion%20An%20and%20Myeongkyun%20Kang%20and%20Soopil%20Kim%20and%20Philip%20Chikontwe%20and%20Li%20Shen%20and%20Sang%20Hyun%20Park&entry.1292438233=%20%20Electroencephalography%20%28EEG%29%20motor%20imagery%20%28MI%29%20classification%20is%20a%0Afundamental%2C%20yet%20challenging%20task%20due%20to%20the%20variation%20of%20signals%20between%0Aindividuals%20i.e.%2C%20inter-subject%20variability.%20Previous%20approaches%20try%20to%0Amitigate%20this%20using%20task-specific%20%28TS%29%20EEG%20signals%20from%20the%20target%20subject%20in%0Atraining.%20However%2C%20recording%20TS%20EEG%20signals%20requires%20time%20and%20limits%20its%0Aapplicability%20in%20various%20fields.%20In%20contrast%2C%20resting%20state%20%28RS%29%20EEG%20signals%0Aare%20a%20viable%20alternative%20due%20to%20ease%20of%20acquisition%20with%20rich%20subject%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20subject-adaptive%20transfer%0Alearning%20strategy%20that%20utilizes%20RS%20EEG%20signals%20to%20adapt%20models%20on%20unseen%0Asubject%20data.%20Specifically%2C%20we%20disentangle%20extracted%20features%20into%20task-%20and%0Asubject-dependent%20features%20and%20use%20them%20to%20calibrate%20RS%20EEG%20signals%20for%0Aobtaining%20task%20information%20while%20preserving%20subject%20characteristics.%20The%0Acalibrated%20signals%20are%20then%20used%20to%20adapt%20the%20model%20to%20the%20target%20subject%2C%0Aenabling%20the%20model%20to%20simulate%20processing%20TS%20EEG%20signals%20of%20the%20target%20subject.%0AThe%20proposed%20method%20achieves%20state-of-the-art%20accuracy%20on%20three%20public%0Abenchmarks%2C%20demonstrating%20the%20effectiveness%20of%20our%20method%20in%20cross-subject%20EEG%0AMI%20classification.%20Our%20findings%20highlight%20the%20potential%20of%20leveraging%20RS%20EEG%0Asignals%20to%20advance%20practical%20brain-computer%20interface%20systems.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/SionAn/MICCAI2024-ResTL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19346v2&entry.124074799=Read"},
{"title": "Representation Learning on Hyper-Relational and Numeric Knowledge Graphs\n  with Transformers", "author": "Chanyoung Chung and Jaejun Lee and Joyce Jiyoung Whang", "abstract": "  A hyper-relational knowledge graph has been recently studied where a triplet\nis associated with a set of qualifiers; a qualifier is composed of a relation\nand an entity, providing auxiliary information for a triplet. While existing\nhyper-relational knowledge graph embedding methods assume that the entities are\ndiscrete objects, some information should be represented using numeric values,\ne.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford\nUniv.) can be associated with a qualifier such as (start time, 1911). In this\npaper, we propose a unified framework named HyNT that learns representations of\na hyper-relational knowledge graph containing numeric literals in either\ntriplets or qualifiers. We define a context transformer and a prediction\ntransformer to learn the representations based not only on the correlations\nbetween a triplet and its qualifiers but also on the numeric information. By\nlearning compact representations of triplets and qualifiers and feeding them\ninto the transformers, we reduce the computation cost of using transformers.\nUsing HyNT, we can predict missing numeric values in addition to missing\nentities or relations in a hyper-relational knowledge graph. Experimental\nresults show that HyNT significantly outperforms state-of-the-art methods on\nreal-world datasets.\n", "link": "http://arxiv.org/abs/2305.18256v4", "date": "2024-07-09", "relevancy": 1.9443, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5036}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4872}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Learning%20on%20Hyper-Relational%20and%20Numeric%20Knowledge%20Graphs%0A%20%20with%20Transformers&body=Title%3A%20Representation%20Learning%20on%20Hyper-Relational%20and%20Numeric%20Knowledge%20Graphs%0A%20%20with%20Transformers%0AAuthor%3A%20Chanyoung%20Chung%20and%20Jaejun%20Lee%20and%20Joyce%20Jiyoung%20Whang%0AAbstract%3A%20%20%20A%20hyper-relational%20knowledge%20graph%20has%20been%20recently%20studied%20where%20a%20triplet%0Ais%20associated%20with%20a%20set%20of%20qualifiers%3B%20a%20qualifier%20is%20composed%20of%20a%20relation%0Aand%20an%20entity%2C%20providing%20auxiliary%20information%20for%20a%20triplet.%20While%20existing%0Ahyper-relational%20knowledge%20graph%20embedding%20methods%20assume%20that%20the%20entities%20are%0Adiscrete%20objects%2C%20some%20information%20should%20be%20represented%20using%20numeric%20values%2C%0Ae.g.%2C%20%28J.R.R.%2C%20was%20born%20in%2C%201892%29.%20Also%2C%20a%20triplet%20%28J.R.R.%2C%20educated%20at%2C%20Oxford%0AUniv.%29%20can%20be%20associated%20with%20a%20qualifier%20such%20as%20%28start%20time%2C%201911%29.%20In%20this%0Apaper%2C%20we%20propose%20a%20unified%20framework%20named%20HyNT%20that%20learns%20representations%20of%0Aa%20hyper-relational%20knowledge%20graph%20containing%20numeric%20literals%20in%20either%0Atriplets%20or%20qualifiers.%20We%20define%20a%20context%20transformer%20and%20a%20prediction%0Atransformer%20to%20learn%20the%20representations%20based%20not%20only%20on%20the%20correlations%0Abetween%20a%20triplet%20and%20its%20qualifiers%20but%20also%20on%20the%20numeric%20information.%20By%0Alearning%20compact%20representations%20of%20triplets%20and%20qualifiers%20and%20feeding%20them%0Ainto%20the%20transformers%2C%20we%20reduce%20the%20computation%20cost%20of%20using%20transformers.%0AUsing%20HyNT%2C%20we%20can%20predict%20missing%20numeric%20values%20in%20addition%20to%20missing%0Aentities%20or%20relations%20in%20a%20hyper-relational%20knowledge%20graph.%20Experimental%0Aresults%20show%20that%20HyNT%20significantly%20outperforms%20state-of-the-art%20methods%20on%0Areal-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.18256v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Learning%2520on%2520Hyper-Relational%2520and%2520Numeric%2520Knowledge%2520Graphs%250A%2520%2520with%2520Transformers%26entry.906535625%3DChanyoung%2520Chung%2520and%2520Jaejun%2520Lee%2520and%2520Joyce%2520Jiyoung%2520Whang%26entry.1292438233%3D%2520%2520A%2520hyper-relational%2520knowledge%2520graph%2520has%2520been%2520recently%2520studied%2520where%2520a%2520triplet%250Ais%2520associated%2520with%2520a%2520set%2520of%2520qualifiers%253B%2520a%2520qualifier%2520is%2520composed%2520of%2520a%2520relation%250Aand%2520an%2520entity%252C%2520providing%2520auxiliary%2520information%2520for%2520a%2520triplet.%2520While%2520existing%250Ahyper-relational%2520knowledge%2520graph%2520embedding%2520methods%2520assume%2520that%2520the%2520entities%2520are%250Adiscrete%2520objects%252C%2520some%2520information%2520should%2520be%2520represented%2520using%2520numeric%2520values%252C%250Ae.g.%252C%2520%2528J.R.R.%252C%2520was%2520born%2520in%252C%25201892%2529.%2520Also%252C%2520a%2520triplet%2520%2528J.R.R.%252C%2520educated%2520at%252C%2520Oxford%250AUniv.%2529%2520can%2520be%2520associated%2520with%2520a%2520qualifier%2520such%2520as%2520%2528start%2520time%252C%25201911%2529.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520unified%2520framework%2520named%2520HyNT%2520that%2520learns%2520representations%2520of%250Aa%2520hyper-relational%2520knowledge%2520graph%2520containing%2520numeric%2520literals%2520in%2520either%250Atriplets%2520or%2520qualifiers.%2520We%2520define%2520a%2520context%2520transformer%2520and%2520a%2520prediction%250Atransformer%2520to%2520learn%2520the%2520representations%2520based%2520not%2520only%2520on%2520the%2520correlations%250Abetween%2520a%2520triplet%2520and%2520its%2520qualifiers%2520but%2520also%2520on%2520the%2520numeric%2520information.%2520By%250Alearning%2520compact%2520representations%2520of%2520triplets%2520and%2520qualifiers%2520and%2520feeding%2520them%250Ainto%2520the%2520transformers%252C%2520we%2520reduce%2520the%2520computation%2520cost%2520of%2520using%2520transformers.%250AUsing%2520HyNT%252C%2520we%2520can%2520predict%2520missing%2520numeric%2520values%2520in%2520addition%2520to%2520missing%250Aentities%2520or%2520relations%2520in%2520a%2520hyper-relational%2520knowledge%2520graph.%2520Experimental%250Aresults%2520show%2520that%2520HyNT%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520on%250Areal-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.18256v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Learning%20on%20Hyper-Relational%20and%20Numeric%20Knowledge%20Graphs%0A%20%20with%20Transformers&entry.906535625=Chanyoung%20Chung%20and%20Jaejun%20Lee%20and%20Joyce%20Jiyoung%20Whang&entry.1292438233=%20%20A%20hyper-relational%20knowledge%20graph%20has%20been%20recently%20studied%20where%20a%20triplet%0Ais%20associated%20with%20a%20set%20of%20qualifiers%3B%20a%20qualifier%20is%20composed%20of%20a%20relation%0Aand%20an%20entity%2C%20providing%20auxiliary%20information%20for%20a%20triplet.%20While%20existing%0Ahyper-relational%20knowledge%20graph%20embedding%20methods%20assume%20that%20the%20entities%20are%0Adiscrete%20objects%2C%20some%20information%20should%20be%20represented%20using%20numeric%20values%2C%0Ae.g.%2C%20%28J.R.R.%2C%20was%20born%20in%2C%201892%29.%20Also%2C%20a%20triplet%20%28J.R.R.%2C%20educated%20at%2C%20Oxford%0AUniv.%29%20can%20be%20associated%20with%20a%20qualifier%20such%20as%20%28start%20time%2C%201911%29.%20In%20this%0Apaper%2C%20we%20propose%20a%20unified%20framework%20named%20HyNT%20that%20learns%20representations%20of%0Aa%20hyper-relational%20knowledge%20graph%20containing%20numeric%20literals%20in%20either%0Atriplets%20or%20qualifiers.%20We%20define%20a%20context%20transformer%20and%20a%20prediction%0Atransformer%20to%20learn%20the%20representations%20based%20not%20only%20on%20the%20correlations%0Abetween%20a%20triplet%20and%20its%20qualifiers%20but%20also%20on%20the%20numeric%20information.%20By%0Alearning%20compact%20representations%20of%20triplets%20and%20qualifiers%20and%20feeding%20them%0Ainto%20the%20transformers%2C%20we%20reduce%20the%20computation%20cost%20of%20using%20transformers.%0AUsing%20HyNT%2C%20we%20can%20predict%20missing%20numeric%20values%20in%20addition%20to%20missing%0Aentities%20or%20relations%20in%20a%20hyper-relational%20knowledge%20graph.%20Experimental%0Aresults%20show%20that%20HyNT%20significantly%20outperforms%20state-of-the-art%20methods%20on%0Areal-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.18256v4&entry.124074799=Read"},
{"title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps", "author": "Yung-Sung Chuang and Linlu Qiu and Cheng-Yu Hsieh and Ranjay Krishna and Yoon Kim and James Glass", "abstract": "  When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.\n", "link": "http://arxiv.org/abs/2407.07071v1", "date": "2024-07-09", "relevancy": 1.9322, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4937}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4853}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lookback%20Lens%3A%20Detecting%20and%20Mitigating%20Contextual%20Hallucinations%20in%0A%20%20Large%20Language%20Models%20Using%20Only%20Attention%20Maps&body=Title%3A%20Lookback%20Lens%3A%20Detecting%20and%20Mitigating%20Contextual%20Hallucinations%20in%0A%20%20Large%20Language%20Models%20Using%20Only%20Attention%20Maps%0AAuthor%3A%20Yung-Sung%20Chuang%20and%20Linlu%20Qiu%20and%20Cheng-Yu%20Hsieh%20and%20Ranjay%20Krishna%20and%20Yoon%20Kim%20and%20James%20Glass%0AAbstract%3A%20%20%20When%20asked%20to%20summarize%20articles%20or%20answer%20questions%20given%20a%20passage%2C%20large%0Alanguage%20models%20%28LLMs%29%20can%20hallucinate%20details%20and%20respond%20with%20unsubstantiated%0Aanswers%20that%20are%20inaccurate%20with%20respect%20to%20the%20input%20context.%20This%20paper%0Adescribes%20a%20simple%20approach%20for%20detecting%20such%20contextual%20hallucinations.%20We%0Ahypothesize%20that%20contextual%20hallucinations%20are%20related%20to%20the%20extent%20to%20which%0Aan%20LLM%20attends%20to%20information%20in%20the%20provided%20context%20versus%20its%20own%0Agenerations.%20Based%20on%20this%20intuition%2C%20we%20propose%20a%20simple%20hallucination%0Adetection%20model%20whose%20input%20features%20are%20given%20by%20the%20ratio%20of%20attention%0Aweights%20on%20the%20context%20versus%20newly%20generated%20tokens%20%28for%20each%20attention%20head%29.%0AWe%20find%20that%20a%20linear%20classifier%20based%20on%20these%20lookback%20ratio%20features%20is%20as%0Aeffective%20as%20a%20richer%20detector%20that%20utilizes%20the%20entire%20hidden%20states%20of%20an%20LLM%0Aor%20a%20text-based%20entailment%20model.%20The%20lookback%20ratio-based%20detector%20--%20Lookback%0ALens%20--%20is%20found%20to%20transfer%20across%20tasks%20and%20even%20models%2C%20allowing%20a%20detector%0Athat%20is%20trained%20on%20a%207B%20model%20to%20be%20applied%20%28without%20retraining%29%20to%20a%20larger%0A13B%20model.%20We%20further%20apply%20this%20detector%20to%20mitigate%20contextual%0Ahallucinations%2C%20and%20find%20that%20a%20simple%20classifier-guided%20decoding%20approach%20is%0Aable%20to%20reduce%20the%20amount%20of%20hallucination%2C%20for%20example%20by%209.6%25%20in%20the%20XSum%0Asummarization%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLookback%2520Lens%253A%2520Detecting%2520and%2520Mitigating%2520Contextual%2520Hallucinations%2520in%250A%2520%2520Large%2520Language%2520Models%2520Using%2520Only%2520Attention%2520Maps%26entry.906535625%3DYung-Sung%2520Chuang%2520and%2520Linlu%2520Qiu%2520and%2520Cheng-Yu%2520Hsieh%2520and%2520Ranjay%2520Krishna%2520and%2520Yoon%2520Kim%2520and%2520James%2520Glass%26entry.1292438233%3D%2520%2520When%2520asked%2520to%2520summarize%2520articles%2520or%2520answer%2520questions%2520given%2520a%2520passage%252C%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520can%2520hallucinate%2520details%2520and%2520respond%2520with%2520unsubstantiated%250Aanswers%2520that%2520are%2520inaccurate%2520with%2520respect%2520to%2520the%2520input%2520context.%2520This%2520paper%250Adescribes%2520a%2520simple%2520approach%2520for%2520detecting%2520such%2520contextual%2520hallucinations.%2520We%250Ahypothesize%2520that%2520contextual%2520hallucinations%2520are%2520related%2520to%2520the%2520extent%2520to%2520which%250Aan%2520LLM%2520attends%2520to%2520information%2520in%2520the%2520provided%2520context%2520versus%2520its%2520own%250Agenerations.%2520Based%2520on%2520this%2520intuition%252C%2520we%2520propose%2520a%2520simple%2520hallucination%250Adetection%2520model%2520whose%2520input%2520features%2520are%2520given%2520by%2520the%2520ratio%2520of%2520attention%250Aweights%2520on%2520the%2520context%2520versus%2520newly%2520generated%2520tokens%2520%2528for%2520each%2520attention%2520head%2529.%250AWe%2520find%2520that%2520a%2520linear%2520classifier%2520based%2520on%2520these%2520lookback%2520ratio%2520features%2520is%2520as%250Aeffective%2520as%2520a%2520richer%2520detector%2520that%2520utilizes%2520the%2520entire%2520hidden%2520states%2520of%2520an%2520LLM%250Aor%2520a%2520text-based%2520entailment%2520model.%2520The%2520lookback%2520ratio-based%2520detector%2520--%2520Lookback%250ALens%2520--%2520is%2520found%2520to%2520transfer%2520across%2520tasks%2520and%2520even%2520models%252C%2520allowing%2520a%2520detector%250Athat%2520is%2520trained%2520on%2520a%25207B%2520model%2520to%2520be%2520applied%2520%2528without%2520retraining%2529%2520to%2520a%2520larger%250A13B%2520model.%2520We%2520further%2520apply%2520this%2520detector%2520to%2520mitigate%2520contextual%250Ahallucinations%252C%2520and%2520find%2520that%2520a%2520simple%2520classifier-guided%2520decoding%2520approach%2520is%250Aable%2520to%2520reduce%2520the%2520amount%2520of%2520hallucination%252C%2520for%2520example%2520by%25209.6%2525%2520in%2520the%2520XSum%250Asummarization%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lookback%20Lens%3A%20Detecting%20and%20Mitigating%20Contextual%20Hallucinations%20in%0A%20%20Large%20Language%20Models%20Using%20Only%20Attention%20Maps&entry.906535625=Yung-Sung%20Chuang%20and%20Linlu%20Qiu%20and%20Cheng-Yu%20Hsieh%20and%20Ranjay%20Krishna%20and%20Yoon%20Kim%20and%20James%20Glass&entry.1292438233=%20%20When%20asked%20to%20summarize%20articles%20or%20answer%20questions%20given%20a%20passage%2C%20large%0Alanguage%20models%20%28LLMs%29%20can%20hallucinate%20details%20and%20respond%20with%20unsubstantiated%0Aanswers%20that%20are%20inaccurate%20with%20respect%20to%20the%20input%20context.%20This%20paper%0Adescribes%20a%20simple%20approach%20for%20detecting%20such%20contextual%20hallucinations.%20We%0Ahypothesize%20that%20contextual%20hallucinations%20are%20related%20to%20the%20extent%20to%20which%0Aan%20LLM%20attends%20to%20information%20in%20the%20provided%20context%20versus%20its%20own%0Agenerations.%20Based%20on%20this%20intuition%2C%20we%20propose%20a%20simple%20hallucination%0Adetection%20model%20whose%20input%20features%20are%20given%20by%20the%20ratio%20of%20attention%0Aweights%20on%20the%20context%20versus%20newly%20generated%20tokens%20%28for%20each%20attention%20head%29.%0AWe%20find%20that%20a%20linear%20classifier%20based%20on%20these%20lookback%20ratio%20features%20is%20as%0Aeffective%20as%20a%20richer%20detector%20that%20utilizes%20the%20entire%20hidden%20states%20of%20an%20LLM%0Aor%20a%20text-based%20entailment%20model.%20The%20lookback%20ratio-based%20detector%20--%20Lookback%0ALens%20--%20is%20found%20to%20transfer%20across%20tasks%20and%20even%20models%2C%20allowing%20a%20detector%0Athat%20is%20trained%20on%20a%207B%20model%20to%20be%20applied%20%28without%20retraining%29%20to%20a%20larger%0A13B%20model.%20We%20further%20apply%20this%20detector%20to%20mitigate%20contextual%0Ahallucinations%2C%20and%20find%20that%20a%20simple%20classifier-guided%20decoding%20approach%20is%0Aable%20to%20reduce%20the%20amount%20of%20hallucination%2C%20for%20example%20by%209.6%25%20in%20the%20XSum%0Asummarization%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07071v1&entry.124074799=Read"},
{"title": "A representation learning approach to probe for dynamical dark energy in\n  matter power spectra", "author": "Davide Piras and Lucas Lombriser", "abstract": "  We present DE-VAE, a variational autoencoder (VAE) architecture to search for\na compressed representation of dynamical dark energy (DE) models in\nobservational studies of the cosmic large-scale structure. DE-VAE is trained on\nmatter power spectra boosts generated at wavenumbers $k\\in(0.01-2.5) \\\nh/\\rm{Mpc}$ and at four redshift values $z\\in(0.1,0.48,0.78,1.5)$ for the most\ntypical dynamical DE parametrization with two extra parameters describing an\nevolving DE equation of state. The boosts are compressed to a lower-dimensional\nrepresentation, which is concatenated with standard cold dark matter (CDM)\nparameters and then mapped back to reconstructed boosts; both the compression\nand the reconstruction components are parametrized as neural networks.\nRemarkably, we find that a single latent parameter is sufficient to predict 95%\n(99%) of DE power spectra generated over a broad range of cosmological\nparameters within $1\\sigma$ ($2\\sigma$) of a Gaussian error which includes\ncosmic variance, shot noise and systematic effects for a Stage IV-like survey.\nThis single parameter shows a high mutual information with the two DE\nparameters, and these three variables can be linked together with an explicit\nequation through symbolic regression. Considering a model with two latent\nvariables only marginally improves the accuracy of the predictions, and adding\na third latent variable has no significant impact on the model's performance.\nWe discuss how the DE-VAE architecture can be extended from a proof of concept\nto a general framework to be employed in the search for a common\nlower-dimensional parametrization of a wide range of beyond-$\\Lambda$CDM models\nand for different cosmological datasets. Such a framework could then both\ninform the development of cosmological surveys by targeting optimal probes, and\nprovide theoretical insight into the common phenomenological aspects of\nbeyond-$\\Lambda$CDM models.\n", "link": "http://arxiv.org/abs/2310.10717v2", "date": "2024-07-09", "relevancy": 1.931, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5214}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4646}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20representation%20learning%20approach%20to%20probe%20for%20dynamical%20dark%20energy%20in%0A%20%20matter%20power%20spectra&body=Title%3A%20A%20representation%20learning%20approach%20to%20probe%20for%20dynamical%20dark%20energy%20in%0A%20%20matter%20power%20spectra%0AAuthor%3A%20Davide%20Piras%20and%20Lucas%20Lombriser%0AAbstract%3A%20%20%20We%20present%20DE-VAE%2C%20a%20variational%20autoencoder%20%28VAE%29%20architecture%20to%20search%20for%0Aa%20compressed%20representation%20of%20dynamical%20dark%20energy%20%28DE%29%20models%20in%0Aobservational%20studies%20of%20the%20cosmic%20large-scale%20structure.%20DE-VAE%20is%20trained%20on%0Amatter%20power%20spectra%20boosts%20generated%20at%20wavenumbers%20%24k%5Cin%280.01-2.5%29%20%5C%0Ah/%5Crm%7BMpc%7D%24%20and%20at%20four%20redshift%20values%20%24z%5Cin%280.1%2C0.48%2C0.78%2C1.5%29%24%20for%20the%20most%0Atypical%20dynamical%20DE%20parametrization%20with%20two%20extra%20parameters%20describing%20an%0Aevolving%20DE%20equation%20of%20state.%20The%20boosts%20are%20compressed%20to%20a%20lower-dimensional%0Arepresentation%2C%20which%20is%20concatenated%20with%20standard%20cold%20dark%20matter%20%28CDM%29%0Aparameters%20and%20then%20mapped%20back%20to%20reconstructed%20boosts%3B%20both%20the%20compression%0Aand%20the%20reconstruction%20components%20are%20parametrized%20as%20neural%20networks.%0ARemarkably%2C%20we%20find%20that%20a%20single%20latent%20parameter%20is%20sufficient%20to%20predict%2095%25%0A%2899%25%29%20of%20DE%20power%20spectra%20generated%20over%20a%20broad%20range%20of%20cosmological%0Aparameters%20within%20%241%5Csigma%24%20%28%242%5Csigma%24%29%20of%20a%20Gaussian%20error%20which%20includes%0Acosmic%20variance%2C%20shot%20noise%20and%20systematic%20effects%20for%20a%20Stage%20IV-like%20survey.%0AThis%20single%20parameter%20shows%20a%20high%20mutual%20information%20with%20the%20two%20DE%0Aparameters%2C%20and%20these%20three%20variables%20can%20be%20linked%20together%20with%20an%20explicit%0Aequation%20through%20symbolic%20regression.%20Considering%20a%20model%20with%20two%20latent%0Avariables%20only%20marginally%20improves%20the%20accuracy%20of%20the%20predictions%2C%20and%20adding%0Aa%20third%20latent%20variable%20has%20no%20significant%20impact%20on%20the%20model%27s%20performance.%0AWe%20discuss%20how%20the%20DE-VAE%20architecture%20can%20be%20extended%20from%20a%20proof%20of%20concept%0Ato%20a%20general%20framework%20to%20be%20employed%20in%20the%20search%20for%20a%20common%0Alower-dimensional%20parametrization%20of%20a%20wide%20range%20of%20beyond-%24%5CLambda%24CDM%20models%0Aand%20for%20different%20cosmological%20datasets.%20Such%20a%20framework%20could%20then%20both%0Ainform%20the%20development%20of%20cosmological%20surveys%20by%20targeting%20optimal%20probes%2C%20and%0Aprovide%20theoretical%20insight%20into%20the%20common%20phenomenological%20aspects%20of%0Abeyond-%24%5CLambda%24CDM%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520representation%2520learning%2520approach%2520to%2520probe%2520for%2520dynamical%2520dark%2520energy%2520in%250A%2520%2520matter%2520power%2520spectra%26entry.906535625%3DDavide%2520Piras%2520and%2520Lucas%2520Lombriser%26entry.1292438233%3D%2520%2520We%2520present%2520DE-VAE%252C%2520a%2520variational%2520autoencoder%2520%2528VAE%2529%2520architecture%2520to%2520search%2520for%250Aa%2520compressed%2520representation%2520of%2520dynamical%2520dark%2520energy%2520%2528DE%2529%2520models%2520in%250Aobservational%2520studies%2520of%2520the%2520cosmic%2520large-scale%2520structure.%2520DE-VAE%2520is%2520trained%2520on%250Amatter%2520power%2520spectra%2520boosts%2520generated%2520at%2520wavenumbers%2520%2524k%255Cin%25280.01-2.5%2529%2520%255C%250Ah/%255Crm%257BMpc%257D%2524%2520and%2520at%2520four%2520redshift%2520values%2520%2524z%255Cin%25280.1%252C0.48%252C0.78%252C1.5%2529%2524%2520for%2520the%2520most%250Atypical%2520dynamical%2520DE%2520parametrization%2520with%2520two%2520extra%2520parameters%2520describing%2520an%250Aevolving%2520DE%2520equation%2520of%2520state.%2520The%2520boosts%2520are%2520compressed%2520to%2520a%2520lower-dimensional%250Arepresentation%252C%2520which%2520is%2520concatenated%2520with%2520standard%2520cold%2520dark%2520matter%2520%2528CDM%2529%250Aparameters%2520and%2520then%2520mapped%2520back%2520to%2520reconstructed%2520boosts%253B%2520both%2520the%2520compression%250Aand%2520the%2520reconstruction%2520components%2520are%2520parametrized%2520as%2520neural%2520networks.%250ARemarkably%252C%2520we%2520find%2520that%2520a%2520single%2520latent%2520parameter%2520is%2520sufficient%2520to%2520predict%252095%2525%250A%252899%2525%2529%2520of%2520DE%2520power%2520spectra%2520generated%2520over%2520a%2520broad%2520range%2520of%2520cosmological%250Aparameters%2520within%2520%25241%255Csigma%2524%2520%2528%25242%255Csigma%2524%2529%2520of%2520a%2520Gaussian%2520error%2520which%2520includes%250Acosmic%2520variance%252C%2520shot%2520noise%2520and%2520systematic%2520effects%2520for%2520a%2520Stage%2520IV-like%2520survey.%250AThis%2520single%2520parameter%2520shows%2520a%2520high%2520mutual%2520information%2520with%2520the%2520two%2520DE%250Aparameters%252C%2520and%2520these%2520three%2520variables%2520can%2520be%2520linked%2520together%2520with%2520an%2520explicit%250Aequation%2520through%2520symbolic%2520regression.%2520Considering%2520a%2520model%2520with%2520two%2520latent%250Avariables%2520only%2520marginally%2520improves%2520the%2520accuracy%2520of%2520the%2520predictions%252C%2520and%2520adding%250Aa%2520third%2520latent%2520variable%2520has%2520no%2520significant%2520impact%2520on%2520the%2520model%2527s%2520performance.%250AWe%2520discuss%2520how%2520the%2520DE-VAE%2520architecture%2520can%2520be%2520extended%2520from%2520a%2520proof%2520of%2520concept%250Ato%2520a%2520general%2520framework%2520to%2520be%2520employed%2520in%2520the%2520search%2520for%2520a%2520common%250Alower-dimensional%2520parametrization%2520of%2520a%2520wide%2520range%2520of%2520beyond-%2524%255CLambda%2524CDM%2520models%250Aand%2520for%2520different%2520cosmological%2520datasets.%2520Such%2520a%2520framework%2520could%2520then%2520both%250Ainform%2520the%2520development%2520of%2520cosmological%2520surveys%2520by%2520targeting%2520optimal%2520probes%252C%2520and%250Aprovide%2520theoretical%2520insight%2520into%2520the%2520common%2520phenomenological%2520aspects%2520of%250Abeyond-%2524%255CLambda%2524CDM%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20representation%20learning%20approach%20to%20probe%20for%20dynamical%20dark%20energy%20in%0A%20%20matter%20power%20spectra&entry.906535625=Davide%20Piras%20and%20Lucas%20Lombriser&entry.1292438233=%20%20We%20present%20DE-VAE%2C%20a%20variational%20autoencoder%20%28VAE%29%20architecture%20to%20search%20for%0Aa%20compressed%20representation%20of%20dynamical%20dark%20energy%20%28DE%29%20models%20in%0Aobservational%20studies%20of%20the%20cosmic%20large-scale%20structure.%20DE-VAE%20is%20trained%20on%0Amatter%20power%20spectra%20boosts%20generated%20at%20wavenumbers%20%24k%5Cin%280.01-2.5%29%20%5C%0Ah/%5Crm%7BMpc%7D%24%20and%20at%20four%20redshift%20values%20%24z%5Cin%280.1%2C0.48%2C0.78%2C1.5%29%24%20for%20the%20most%0Atypical%20dynamical%20DE%20parametrization%20with%20two%20extra%20parameters%20describing%20an%0Aevolving%20DE%20equation%20of%20state.%20The%20boosts%20are%20compressed%20to%20a%20lower-dimensional%0Arepresentation%2C%20which%20is%20concatenated%20with%20standard%20cold%20dark%20matter%20%28CDM%29%0Aparameters%20and%20then%20mapped%20back%20to%20reconstructed%20boosts%3B%20both%20the%20compression%0Aand%20the%20reconstruction%20components%20are%20parametrized%20as%20neural%20networks.%0ARemarkably%2C%20we%20find%20that%20a%20single%20latent%20parameter%20is%20sufficient%20to%20predict%2095%25%0A%2899%25%29%20of%20DE%20power%20spectra%20generated%20over%20a%20broad%20range%20of%20cosmological%0Aparameters%20within%20%241%5Csigma%24%20%28%242%5Csigma%24%29%20of%20a%20Gaussian%20error%20which%20includes%0Acosmic%20variance%2C%20shot%20noise%20and%20systematic%20effects%20for%20a%20Stage%20IV-like%20survey.%0AThis%20single%20parameter%20shows%20a%20high%20mutual%20information%20with%20the%20two%20DE%0Aparameters%2C%20and%20these%20three%20variables%20can%20be%20linked%20together%20with%20an%20explicit%0Aequation%20through%20symbolic%20regression.%20Considering%20a%20model%20with%20two%20latent%0Avariables%20only%20marginally%20improves%20the%20accuracy%20of%20the%20predictions%2C%20and%20adding%0Aa%20third%20latent%20variable%20has%20no%20significant%20impact%20on%20the%20model%27s%20performance.%0AWe%20discuss%20how%20the%20DE-VAE%20architecture%20can%20be%20extended%20from%20a%20proof%20of%20concept%0Ato%20a%20general%20framework%20to%20be%20employed%20in%20the%20search%20for%20a%20common%0Alower-dimensional%20parametrization%20of%20a%20wide%20range%20of%20beyond-%24%5CLambda%24CDM%20models%0Aand%20for%20different%20cosmological%20datasets.%20Such%20a%20framework%20could%20then%20both%0Ainform%20the%20development%20of%20cosmological%20surveys%20by%20targeting%20optimal%20probes%2C%20and%0Aprovide%20theoretical%20insight%20into%20the%20common%20phenomenological%20aspects%20of%0Abeyond-%24%5CLambda%24CDM%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10717v2&entry.124074799=Read"},
{"title": "Learning-Based Difficulty Calibration for Enhanced Membership Inference\n  Attacks", "author": "Haonan Shi and Tu Ouyang and An Wang", "abstract": "  Machine learning models, in particular deep neural networks, are currently an\nintegral part of various applications, from healthcare to finance. However,\nusing sensitive data to train these models raises concerns about privacy and\nsecurity. One method that has emerged to verify if the trained models are\nprivacy-preserving is Membership Inference Attacks (MIA), which allows\nadversaries to determine whether a specific data point was part of a model's\ntraining dataset. While a series of MIAs have been proposed in the literature,\nonly a few can achieve high True Positive Rates (TPR) in the low False Positive\nRate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA\nto be practically useful in real-world settings. In this paper, we present a\nnovel approach to MIA that is aimed at significantly improving TPR at low FPRs.\nOur method, named learning-based difficulty calibration for MIA(LDC-MIA),\ncharacterizes data records by their hardness levels using a neural network\nclassifier to determine membership. The experiment results show that LDC-MIA\ncan improve TPR at low FPR by up to 4x compared to the other difficulty\ncalibration based MIAs. It also has the highest Area Under ROC curve (AUC)\nacross all datasets. Our method's cost is comparable with most of the existing\nMIAs, but is orders of magnitude more efficient than one of the\nstate-of-the-art methods, LiRA, while achieving similar performance.\n", "link": "http://arxiv.org/abs/2401.04929v3", "date": "2024-07-09", "relevancy": 1.9298, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5278}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4742}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-Based%20Difficulty%20Calibration%20for%20Enhanced%20Membership%20Inference%0A%20%20Attacks&body=Title%3A%20Learning-Based%20Difficulty%20Calibration%20for%20Enhanced%20Membership%20Inference%0A%20%20Attacks%0AAuthor%3A%20Haonan%20Shi%20and%20Tu%20Ouyang%20and%20An%20Wang%0AAbstract%3A%20%20%20Machine%20learning%20models%2C%20in%20particular%20deep%20neural%20networks%2C%20are%20currently%20an%0Aintegral%20part%20of%20various%20applications%2C%20from%20healthcare%20to%20finance.%20However%2C%0Ausing%20sensitive%20data%20to%20train%20these%20models%20raises%20concerns%20about%20privacy%20and%0Asecurity.%20One%20method%20that%20has%20emerged%20to%20verify%20if%20the%20trained%20models%20are%0Aprivacy-preserving%20is%20Membership%20Inference%20Attacks%20%28MIA%29%2C%20which%20allows%0Aadversaries%20to%20determine%20whether%20a%20specific%20data%20point%20was%20part%20of%20a%20model%27s%0Atraining%20dataset.%20While%20a%20series%20of%20MIAs%20have%20been%20proposed%20in%20the%20literature%2C%0Aonly%20a%20few%20can%20achieve%20high%20True%20Positive%20Rates%20%28TPR%29%20in%20the%20low%20False%20Positive%0ARate%20%28FPR%29%20region%20%280.01%25~1%25%29.%20This%20is%20a%20crucial%20factor%20to%20consider%20for%20an%20MIA%0Ato%20be%20practically%20useful%20in%20real-world%20settings.%20In%20this%20paper%2C%20we%20present%20a%0Anovel%20approach%20to%20MIA%20that%20is%20aimed%20at%20significantly%20improving%20TPR%20at%20low%20FPRs.%0AOur%20method%2C%20named%20learning-based%20difficulty%20calibration%20for%20MIA%28LDC-MIA%29%2C%0Acharacterizes%20data%20records%20by%20their%20hardness%20levels%20using%20a%20neural%20network%0Aclassifier%20to%20determine%20membership.%20The%20experiment%20results%20show%20that%20LDC-MIA%0Acan%20improve%20TPR%20at%20low%20FPR%20by%20up%20to%204x%20compared%20to%20the%20other%20difficulty%0Acalibration%20based%20MIAs.%20It%20also%20has%20the%20highest%20Area%20Under%20ROC%20curve%20%28AUC%29%0Aacross%20all%20datasets.%20Our%20method%27s%20cost%20is%20comparable%20with%20most%20of%20the%20existing%0AMIAs%2C%20but%20is%20orders%20of%20magnitude%20more%20efficient%20than%20one%20of%20the%0Astate-of-the-art%20methods%2C%20LiRA%2C%20while%20achieving%20similar%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04929v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-Based%2520Difficulty%2520Calibration%2520for%2520Enhanced%2520Membership%2520Inference%250A%2520%2520Attacks%26entry.906535625%3DHaonan%2520Shi%2520and%2520Tu%2520Ouyang%2520and%2520An%2520Wang%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%252C%2520in%2520particular%2520deep%2520neural%2520networks%252C%2520are%2520currently%2520an%250Aintegral%2520part%2520of%2520various%2520applications%252C%2520from%2520healthcare%2520to%2520finance.%2520However%252C%250Ausing%2520sensitive%2520data%2520to%2520train%2520these%2520models%2520raises%2520concerns%2520about%2520privacy%2520and%250Asecurity.%2520One%2520method%2520that%2520has%2520emerged%2520to%2520verify%2520if%2520the%2520trained%2520models%2520are%250Aprivacy-preserving%2520is%2520Membership%2520Inference%2520Attacks%2520%2528MIA%2529%252C%2520which%2520allows%250Aadversaries%2520to%2520determine%2520whether%2520a%2520specific%2520data%2520point%2520was%2520part%2520of%2520a%2520model%2527s%250Atraining%2520dataset.%2520While%2520a%2520series%2520of%2520MIAs%2520have%2520been%2520proposed%2520in%2520the%2520literature%252C%250Aonly%2520a%2520few%2520can%2520achieve%2520high%2520True%2520Positive%2520Rates%2520%2528TPR%2529%2520in%2520the%2520low%2520False%2520Positive%250ARate%2520%2528FPR%2529%2520region%2520%25280.01%2525~1%2525%2529.%2520This%2520is%2520a%2520crucial%2520factor%2520to%2520consider%2520for%2520an%2520MIA%250Ato%2520be%2520practically%2520useful%2520in%2520real-world%2520settings.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Anovel%2520approach%2520to%2520MIA%2520that%2520is%2520aimed%2520at%2520significantly%2520improving%2520TPR%2520at%2520low%2520FPRs.%250AOur%2520method%252C%2520named%2520learning-based%2520difficulty%2520calibration%2520for%2520MIA%2528LDC-MIA%2529%252C%250Acharacterizes%2520data%2520records%2520by%2520their%2520hardness%2520levels%2520using%2520a%2520neural%2520network%250Aclassifier%2520to%2520determine%2520membership.%2520The%2520experiment%2520results%2520show%2520that%2520LDC-MIA%250Acan%2520improve%2520TPR%2520at%2520low%2520FPR%2520by%2520up%2520to%25204x%2520compared%2520to%2520the%2520other%2520difficulty%250Acalibration%2520based%2520MIAs.%2520It%2520also%2520has%2520the%2520highest%2520Area%2520Under%2520ROC%2520curve%2520%2528AUC%2529%250Aacross%2520all%2520datasets.%2520Our%2520method%2527s%2520cost%2520is%2520comparable%2520with%2520most%2520of%2520the%2520existing%250AMIAs%252C%2520but%2520is%2520orders%2520of%2520magnitude%2520more%2520efficient%2520than%2520one%2520of%2520the%250Astate-of-the-art%2520methods%252C%2520LiRA%252C%2520while%2520achieving%2520similar%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04929v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-Based%20Difficulty%20Calibration%20for%20Enhanced%20Membership%20Inference%0A%20%20Attacks&entry.906535625=Haonan%20Shi%20and%20Tu%20Ouyang%20and%20An%20Wang&entry.1292438233=%20%20Machine%20learning%20models%2C%20in%20particular%20deep%20neural%20networks%2C%20are%20currently%20an%0Aintegral%20part%20of%20various%20applications%2C%20from%20healthcare%20to%20finance.%20However%2C%0Ausing%20sensitive%20data%20to%20train%20these%20models%20raises%20concerns%20about%20privacy%20and%0Asecurity.%20One%20method%20that%20has%20emerged%20to%20verify%20if%20the%20trained%20models%20are%0Aprivacy-preserving%20is%20Membership%20Inference%20Attacks%20%28MIA%29%2C%20which%20allows%0Aadversaries%20to%20determine%20whether%20a%20specific%20data%20point%20was%20part%20of%20a%20model%27s%0Atraining%20dataset.%20While%20a%20series%20of%20MIAs%20have%20been%20proposed%20in%20the%20literature%2C%0Aonly%20a%20few%20can%20achieve%20high%20True%20Positive%20Rates%20%28TPR%29%20in%20the%20low%20False%20Positive%0ARate%20%28FPR%29%20region%20%280.01%25~1%25%29.%20This%20is%20a%20crucial%20factor%20to%20consider%20for%20an%20MIA%0Ato%20be%20practically%20useful%20in%20real-world%20settings.%20In%20this%20paper%2C%20we%20present%20a%0Anovel%20approach%20to%20MIA%20that%20is%20aimed%20at%20significantly%20improving%20TPR%20at%20low%20FPRs.%0AOur%20method%2C%20named%20learning-based%20difficulty%20calibration%20for%20MIA%28LDC-MIA%29%2C%0Acharacterizes%20data%20records%20by%20their%20hardness%20levels%20using%20a%20neural%20network%0Aclassifier%20to%20determine%20membership.%20The%20experiment%20results%20show%20that%20LDC-MIA%0Acan%20improve%20TPR%20at%20low%20FPR%20by%20up%20to%204x%20compared%20to%20the%20other%20difficulty%0Acalibration%20based%20MIAs.%20It%20also%20has%20the%20highest%20Area%20Under%20ROC%20curve%20%28AUC%29%0Aacross%20all%20datasets.%20Our%20method%27s%20cost%20is%20comparable%20with%20most%20of%20the%20existing%0AMIAs%2C%20but%20is%20orders%20of%20magnitude%20more%20efficient%20than%20one%20of%20the%0Astate-of-the-art%20methods%2C%20LiRA%2C%20while%20achieving%20similar%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04929v3&entry.124074799=Read"},
{"title": "Event Trojan: Asynchronous Event-based Backdoor Attacks", "author": "Ruofei Wang and Qing Guo and Haoliang Li and Renjie Wan", "abstract": "  As asynchronous event data is more frequently engaged in various vision\ntasks, the risk of backdoor attacks becomes more evident. However, research\ninto the potential risk associated with backdoor attacks in asynchronous event\ndata has been scarce, leaving related tasks vulnerable to potential threats.\nThis paper has uncovered the possibility of directly poisoning event data\nstreams by proposing Event Trojan framework, including two kinds of triggers,\ni.e., immutable and mutable triggers. Specifically, our two types of event\ntriggers are based on a sequence of simulated event spikes, which can be easily\nincorporated into any event stream to initiate backdoor attacks. Additionally,\nfor the mutable trigger, we design an adaptive learning mechanism to maximize\nits aggressiveness. To improve the stealthiness, we introduce a novel loss\nfunction that constrains the generated contents of mutable triggers, minimizing\nthe difference between triggers and original events while maintaining\neffectiveness. Extensive experiments on public event datasets show the\neffectiveness of the proposed backdoor triggers. We hope that this paper can\ndraw greater attention to the potential threats posed by backdoor attacks on\nevent-based tasks. Our code is available at\nhttps://github.com/rfww/EventTrojan.\n", "link": "http://arxiv.org/abs/2407.06838v1", "date": "2024-07-09", "relevancy": 1.9296, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4085}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.375}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event%20Trojan%3A%20Asynchronous%20Event-based%20Backdoor%20Attacks&body=Title%3A%20Event%20Trojan%3A%20Asynchronous%20Event-based%20Backdoor%20Attacks%0AAuthor%3A%20Ruofei%20Wang%20and%20Qing%20Guo%20and%20Haoliang%20Li%20and%20Renjie%20Wan%0AAbstract%3A%20%20%20As%20asynchronous%20event%20data%20is%20more%20frequently%20engaged%20in%20various%20vision%0Atasks%2C%20the%20risk%20of%20backdoor%20attacks%20becomes%20more%20evident.%20However%2C%20research%0Ainto%20the%20potential%20risk%20associated%20with%20backdoor%20attacks%20in%20asynchronous%20event%0Adata%20has%20been%20scarce%2C%20leaving%20related%20tasks%20vulnerable%20to%20potential%20threats.%0AThis%20paper%20has%20uncovered%20the%20possibility%20of%20directly%20poisoning%20event%20data%0Astreams%20by%20proposing%20Event%20Trojan%20framework%2C%20including%20two%20kinds%20of%20triggers%2C%0Ai.e.%2C%20immutable%20and%20mutable%20triggers.%20Specifically%2C%20our%20two%20types%20of%20event%0Atriggers%20are%20based%20on%20a%20sequence%20of%20simulated%20event%20spikes%2C%20which%20can%20be%20easily%0Aincorporated%20into%20any%20event%20stream%20to%20initiate%20backdoor%20attacks.%20Additionally%2C%0Afor%20the%20mutable%20trigger%2C%20we%20design%20an%20adaptive%20learning%20mechanism%20to%20maximize%0Aits%20aggressiveness.%20To%20improve%20the%20stealthiness%2C%20we%20introduce%20a%20novel%20loss%0Afunction%20that%20constrains%20the%20generated%20contents%20of%20mutable%20triggers%2C%20minimizing%0Athe%20difference%20between%20triggers%20and%20original%20events%20while%20maintaining%0Aeffectiveness.%20Extensive%20experiments%20on%20public%20event%20datasets%20show%20the%0Aeffectiveness%20of%20the%20proposed%20backdoor%20triggers.%20We%20hope%20that%20this%20paper%20can%0Adraw%20greater%20attention%20to%20the%20potential%20threats%20posed%20by%20backdoor%20attacks%20on%0Aevent-based%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/rfww/EventTrojan.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent%2520Trojan%253A%2520Asynchronous%2520Event-based%2520Backdoor%2520Attacks%26entry.906535625%3DRuofei%2520Wang%2520and%2520Qing%2520Guo%2520and%2520Haoliang%2520Li%2520and%2520Renjie%2520Wan%26entry.1292438233%3D%2520%2520As%2520asynchronous%2520event%2520data%2520is%2520more%2520frequently%2520engaged%2520in%2520various%2520vision%250Atasks%252C%2520the%2520risk%2520of%2520backdoor%2520attacks%2520becomes%2520more%2520evident.%2520However%252C%2520research%250Ainto%2520the%2520potential%2520risk%2520associated%2520with%2520backdoor%2520attacks%2520in%2520asynchronous%2520event%250Adata%2520has%2520been%2520scarce%252C%2520leaving%2520related%2520tasks%2520vulnerable%2520to%2520potential%2520threats.%250AThis%2520paper%2520has%2520uncovered%2520the%2520possibility%2520of%2520directly%2520poisoning%2520event%2520data%250Astreams%2520by%2520proposing%2520Event%2520Trojan%2520framework%252C%2520including%2520two%2520kinds%2520of%2520triggers%252C%250Ai.e.%252C%2520immutable%2520and%2520mutable%2520triggers.%2520Specifically%252C%2520our%2520two%2520types%2520of%2520event%250Atriggers%2520are%2520based%2520on%2520a%2520sequence%2520of%2520simulated%2520event%2520spikes%252C%2520which%2520can%2520be%2520easily%250Aincorporated%2520into%2520any%2520event%2520stream%2520to%2520initiate%2520backdoor%2520attacks.%2520Additionally%252C%250Afor%2520the%2520mutable%2520trigger%252C%2520we%2520design%2520an%2520adaptive%2520learning%2520mechanism%2520to%2520maximize%250Aits%2520aggressiveness.%2520To%2520improve%2520the%2520stealthiness%252C%2520we%2520introduce%2520a%2520novel%2520loss%250Afunction%2520that%2520constrains%2520the%2520generated%2520contents%2520of%2520mutable%2520triggers%252C%2520minimizing%250Athe%2520difference%2520between%2520triggers%2520and%2520original%2520events%2520while%2520maintaining%250Aeffectiveness.%2520Extensive%2520experiments%2520on%2520public%2520event%2520datasets%2520show%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520backdoor%2520triggers.%2520We%2520hope%2520that%2520this%2520paper%2520can%250Adraw%2520greater%2520attention%2520to%2520the%2520potential%2520threats%2520posed%2520by%2520backdoor%2520attacks%2520on%250Aevent-based%2520tasks.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/rfww/EventTrojan.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event%20Trojan%3A%20Asynchronous%20Event-based%20Backdoor%20Attacks&entry.906535625=Ruofei%20Wang%20and%20Qing%20Guo%20and%20Haoliang%20Li%20and%20Renjie%20Wan&entry.1292438233=%20%20As%20asynchronous%20event%20data%20is%20more%20frequently%20engaged%20in%20various%20vision%0Atasks%2C%20the%20risk%20of%20backdoor%20attacks%20becomes%20more%20evident.%20However%2C%20research%0Ainto%20the%20potential%20risk%20associated%20with%20backdoor%20attacks%20in%20asynchronous%20event%0Adata%20has%20been%20scarce%2C%20leaving%20related%20tasks%20vulnerable%20to%20potential%20threats.%0AThis%20paper%20has%20uncovered%20the%20possibility%20of%20directly%20poisoning%20event%20data%0Astreams%20by%20proposing%20Event%20Trojan%20framework%2C%20including%20two%20kinds%20of%20triggers%2C%0Ai.e.%2C%20immutable%20and%20mutable%20triggers.%20Specifically%2C%20our%20two%20types%20of%20event%0Atriggers%20are%20based%20on%20a%20sequence%20of%20simulated%20event%20spikes%2C%20which%20can%20be%20easily%0Aincorporated%20into%20any%20event%20stream%20to%20initiate%20backdoor%20attacks.%20Additionally%2C%0Afor%20the%20mutable%20trigger%2C%20we%20design%20an%20adaptive%20learning%20mechanism%20to%20maximize%0Aits%20aggressiveness.%20To%20improve%20the%20stealthiness%2C%20we%20introduce%20a%20novel%20loss%0Afunction%20that%20constrains%20the%20generated%20contents%20of%20mutable%20triggers%2C%20minimizing%0Athe%20difference%20between%20triggers%20and%20original%20events%20while%20maintaining%0Aeffectiveness.%20Extensive%20experiments%20on%20public%20event%20datasets%20show%20the%0Aeffectiveness%20of%20the%20proposed%20backdoor%20triggers.%20We%20hope%20that%20this%20paper%20can%0Adraw%20greater%20attention%20to%20the%20potential%20threats%20posed%20by%20backdoor%20attacks%20on%0Aevent-based%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/rfww/EventTrojan.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06838v1&entry.124074799=Read"},
{"title": "A Hybrid Training-time and Run-time Defense Against Adversarial Attacks\n  in Modulation Classification", "author": "Lu Zhang and Sangarapillai Lambotharan and Gan Zheng and Guisheng Liao and Ambra Demontis and Fabio Roli", "abstract": "  Motivated by the superior performance of deep learning in many applications\nincluding computer vision and natural language processing, several recent\nstudies have focused on applying deep neural network for devising future\ngenerations of wireless networks. However, several recent works have pointed\nout that imperceptible and carefully designed adversarial examples (attacks)\ncan significantly deteriorate the classification accuracy. In this paper, we\ninvestigate a defense mechanism based on both training-time and run-time\ndefense techniques for protecting machine learning-based radio signal\n(modulation) classification against adversarial attacks. The training-time\ndefense consists of adversarial training and label smoothing, while the\nrun-time defense employs a support vector machine-based neural rejection (NR).\nConsidering a white-box scenario and real datasets, we demonstrate that our\nproposed techniques outperform existing state-of-the-art technologies.\n", "link": "http://arxiv.org/abs/2407.06807v1", "date": "2024-07-09", "relevancy": 1.9261, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.493}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4817}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Training-time%20and%20Run-time%20Defense%20Against%20Adversarial%20Attacks%0A%20%20in%20Modulation%20Classification&body=Title%3A%20A%20Hybrid%20Training-time%20and%20Run-time%20Defense%20Against%20Adversarial%20Attacks%0A%20%20in%20Modulation%20Classification%0AAuthor%3A%20Lu%20Zhang%20and%20Sangarapillai%20Lambotharan%20and%20Gan%20Zheng%20and%20Guisheng%20Liao%20and%20Ambra%20Demontis%20and%20Fabio%20Roli%0AAbstract%3A%20%20%20Motivated%20by%20the%20superior%20performance%20of%20deep%20learning%20in%20many%20applications%0Aincluding%20computer%20vision%20and%20natural%20language%20processing%2C%20several%20recent%0Astudies%20have%20focused%20on%20applying%20deep%20neural%20network%20for%20devising%20future%0Agenerations%20of%20wireless%20networks.%20However%2C%20several%20recent%20works%20have%20pointed%0Aout%20that%20imperceptible%20and%20carefully%20designed%20adversarial%20examples%20%28attacks%29%0Acan%20significantly%20deteriorate%20the%20classification%20accuracy.%20In%20this%20paper%2C%20we%0Ainvestigate%20a%20defense%20mechanism%20based%20on%20both%20training-time%20and%20run-time%0Adefense%20techniques%20for%20protecting%20machine%20learning-based%20radio%20signal%0A%28modulation%29%20classification%20against%20adversarial%20attacks.%20The%20training-time%0Adefense%20consists%20of%20adversarial%20training%20and%20label%20smoothing%2C%20while%20the%0Arun-time%20defense%20employs%20a%20support%20vector%20machine-based%20neural%20rejection%20%28NR%29.%0AConsidering%20a%20white-box%20scenario%20and%20real%20datasets%2C%20we%20demonstrate%20that%20our%0Aproposed%20techniques%20outperform%20existing%20state-of-the-art%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Training-time%2520and%2520Run-time%2520Defense%2520Against%2520Adversarial%2520Attacks%250A%2520%2520in%2520Modulation%2520Classification%26entry.906535625%3DLu%2520Zhang%2520and%2520Sangarapillai%2520Lambotharan%2520and%2520Gan%2520Zheng%2520and%2520Guisheng%2520Liao%2520and%2520Ambra%2520Demontis%2520and%2520Fabio%2520Roli%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520superior%2520performance%2520of%2520deep%2520learning%2520in%2520many%2520applications%250Aincluding%2520computer%2520vision%2520and%2520natural%2520language%2520processing%252C%2520several%2520recent%250Astudies%2520have%2520focused%2520on%2520applying%2520deep%2520neural%2520network%2520for%2520devising%2520future%250Agenerations%2520of%2520wireless%2520networks.%2520However%252C%2520several%2520recent%2520works%2520have%2520pointed%250Aout%2520that%2520imperceptible%2520and%2520carefully%2520designed%2520adversarial%2520examples%2520%2528attacks%2529%250Acan%2520significantly%2520deteriorate%2520the%2520classification%2520accuracy.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520a%2520defense%2520mechanism%2520based%2520on%2520both%2520training-time%2520and%2520run-time%250Adefense%2520techniques%2520for%2520protecting%2520machine%2520learning-based%2520radio%2520signal%250A%2528modulation%2529%2520classification%2520against%2520adversarial%2520attacks.%2520The%2520training-time%250Adefense%2520consists%2520of%2520adversarial%2520training%2520and%2520label%2520smoothing%252C%2520while%2520the%250Arun-time%2520defense%2520employs%2520a%2520support%2520vector%2520machine-based%2520neural%2520rejection%2520%2528NR%2529.%250AConsidering%2520a%2520white-box%2520scenario%2520and%2520real%2520datasets%252C%2520we%2520demonstrate%2520that%2520our%250Aproposed%2520techniques%2520outperform%2520existing%2520state-of-the-art%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Training-time%20and%20Run-time%20Defense%20Against%20Adversarial%20Attacks%0A%20%20in%20Modulation%20Classification&entry.906535625=Lu%20Zhang%20and%20Sangarapillai%20Lambotharan%20and%20Gan%20Zheng%20and%20Guisheng%20Liao%20and%20Ambra%20Demontis%20and%20Fabio%20Roli&entry.1292438233=%20%20Motivated%20by%20the%20superior%20performance%20of%20deep%20learning%20in%20many%20applications%0Aincluding%20computer%20vision%20and%20natural%20language%20processing%2C%20several%20recent%0Astudies%20have%20focused%20on%20applying%20deep%20neural%20network%20for%20devising%20future%0Agenerations%20of%20wireless%20networks.%20However%2C%20several%20recent%20works%20have%20pointed%0Aout%20that%20imperceptible%20and%20carefully%20designed%20adversarial%20examples%20%28attacks%29%0Acan%20significantly%20deteriorate%20the%20classification%20accuracy.%20In%20this%20paper%2C%20we%0Ainvestigate%20a%20defense%20mechanism%20based%20on%20both%20training-time%20and%20run-time%0Adefense%20techniques%20for%20protecting%20machine%20learning-based%20radio%20signal%0A%28modulation%29%20classification%20against%20adversarial%20attacks.%20The%20training-time%0Adefense%20consists%20of%20adversarial%20training%20and%20label%20smoothing%2C%20while%20the%0Arun-time%20defense%20employs%20a%20support%20vector%20machine-based%20neural%20rejection%20%28NR%29.%0AConsidering%20a%20white-box%20scenario%20and%20real%20datasets%2C%20we%20demonstrate%20that%20our%0Aproposed%20techniques%20outperform%20existing%20state-of-the-art%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06807v1&entry.124074799=Read"},
{"title": "Self-Recognition in Language Models", "author": "Tim R. Davidson and Viacheslav Surkov and Veniamin Veselovsky and Giuseppe Russo and Robert West and Caglar Gulcehre", "abstract": "  A rapidly growing number of applications rely on a small set of closed-source\nlanguage models (LMs). This dependency might introduce novel security risks if\nLMs develop self-recognition capabilities. Inspired by human identity\nverification methods, we propose a novel approach for assessing\nself-recognition in LMs using model-generated \"security questions\". Our test\ncan be externally administered to keep track of frontier models as it does not\nrequire access to internal model parameters or output probabilities. We use our\ntest to examine self-recognition in ten of the most capable open- and\nclosed-source LMs currently publicly available. Our extensive experiments found\nno empirical evidence of general or consistent self-recognition in any examined\nLM. Instead, our results suggest that given a set of alternatives, LMs seek to\npick the \"best\" answer, regardless of its origin. Moreover, we find indications\nthat preferences about which models produce the best answers are consistent\nacross LMs. We additionally uncover novel insights on position bias\nconsiderations for LMs in multiple-choice settings.\n", "link": "http://arxiv.org/abs/2407.06946v1", "date": "2024-07-09", "relevancy": 1.9227, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5102}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4683}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Recognition%20in%20Language%20Models&body=Title%3A%20Self-Recognition%20in%20Language%20Models%0AAuthor%3A%20Tim%20R.%20Davidson%20and%20Viacheslav%20Surkov%20and%20Veniamin%20Veselovsky%20and%20Giuseppe%20Russo%20and%20Robert%20West%20and%20Caglar%20Gulcehre%0AAbstract%3A%20%20%20A%20rapidly%20growing%20number%20of%20applications%20rely%20on%20a%20small%20set%20of%20closed-source%0Alanguage%20models%20%28LMs%29.%20This%20dependency%20might%20introduce%20novel%20security%20risks%20if%0ALMs%20develop%20self-recognition%20capabilities.%20Inspired%20by%20human%20identity%0Averification%20methods%2C%20we%20propose%20a%20novel%20approach%20for%20assessing%0Aself-recognition%20in%20LMs%20using%20model-generated%20%22security%20questions%22.%20Our%20test%0Acan%20be%20externally%20administered%20to%20keep%20track%20of%20frontier%20models%20as%20it%20does%20not%0Arequire%20access%20to%20internal%20model%20parameters%20or%20output%20probabilities.%20We%20use%20our%0Atest%20to%20examine%20self-recognition%20in%20ten%20of%20the%20most%20capable%20open-%20and%0Aclosed-source%20LMs%20currently%20publicly%20available.%20Our%20extensive%20experiments%20found%0Ano%20empirical%20evidence%20of%20general%20or%20consistent%20self-recognition%20in%20any%20examined%0ALM.%20Instead%2C%20our%20results%20suggest%20that%20given%20a%20set%20of%20alternatives%2C%20LMs%20seek%20to%0Apick%20the%20%22best%22%20answer%2C%20regardless%20of%20its%20origin.%20Moreover%2C%20we%20find%20indications%0Athat%20preferences%20about%20which%20models%20produce%20the%20best%20answers%20are%20consistent%0Aacross%20LMs.%20We%20additionally%20uncover%20novel%20insights%20on%20position%20bias%0Aconsiderations%20for%20LMs%20in%20multiple-choice%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Recognition%2520in%2520Language%2520Models%26entry.906535625%3DTim%2520R.%2520Davidson%2520and%2520Viacheslav%2520Surkov%2520and%2520Veniamin%2520Veselovsky%2520and%2520Giuseppe%2520Russo%2520and%2520Robert%2520West%2520and%2520Caglar%2520Gulcehre%26entry.1292438233%3D%2520%2520A%2520rapidly%2520growing%2520number%2520of%2520applications%2520rely%2520on%2520a%2520small%2520set%2520of%2520closed-source%250Alanguage%2520models%2520%2528LMs%2529.%2520This%2520dependency%2520might%2520introduce%2520novel%2520security%2520risks%2520if%250ALMs%2520develop%2520self-recognition%2520capabilities.%2520Inspired%2520by%2520human%2520identity%250Averification%2520methods%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520assessing%250Aself-recognition%2520in%2520LMs%2520using%2520model-generated%2520%2522security%2520questions%2522.%2520Our%2520test%250Acan%2520be%2520externally%2520administered%2520to%2520keep%2520track%2520of%2520frontier%2520models%2520as%2520it%2520does%2520not%250Arequire%2520access%2520to%2520internal%2520model%2520parameters%2520or%2520output%2520probabilities.%2520We%2520use%2520our%250Atest%2520to%2520examine%2520self-recognition%2520in%2520ten%2520of%2520the%2520most%2520capable%2520open-%2520and%250Aclosed-source%2520LMs%2520currently%2520publicly%2520available.%2520Our%2520extensive%2520experiments%2520found%250Ano%2520empirical%2520evidence%2520of%2520general%2520or%2520consistent%2520self-recognition%2520in%2520any%2520examined%250ALM.%2520Instead%252C%2520our%2520results%2520suggest%2520that%2520given%2520a%2520set%2520of%2520alternatives%252C%2520LMs%2520seek%2520to%250Apick%2520the%2520%2522best%2522%2520answer%252C%2520regardless%2520of%2520its%2520origin.%2520Moreover%252C%2520we%2520find%2520indications%250Athat%2520preferences%2520about%2520which%2520models%2520produce%2520the%2520best%2520answers%2520are%2520consistent%250Aacross%2520LMs.%2520We%2520additionally%2520uncover%2520novel%2520insights%2520on%2520position%2520bias%250Aconsiderations%2520for%2520LMs%2520in%2520multiple-choice%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Recognition%20in%20Language%20Models&entry.906535625=Tim%20R.%20Davidson%20and%20Viacheslav%20Surkov%20and%20Veniamin%20Veselovsky%20and%20Giuseppe%20Russo%20and%20Robert%20West%20and%20Caglar%20Gulcehre&entry.1292438233=%20%20A%20rapidly%20growing%20number%20of%20applications%20rely%20on%20a%20small%20set%20of%20closed-source%0Alanguage%20models%20%28LMs%29.%20This%20dependency%20might%20introduce%20novel%20security%20risks%20if%0ALMs%20develop%20self-recognition%20capabilities.%20Inspired%20by%20human%20identity%0Averification%20methods%2C%20we%20propose%20a%20novel%20approach%20for%20assessing%0Aself-recognition%20in%20LMs%20using%20model-generated%20%22security%20questions%22.%20Our%20test%0Acan%20be%20externally%20administered%20to%20keep%20track%20of%20frontier%20models%20as%20it%20does%20not%0Arequire%20access%20to%20internal%20model%20parameters%20or%20output%20probabilities.%20We%20use%20our%0Atest%20to%20examine%20self-recognition%20in%20ten%20of%20the%20most%20capable%20open-%20and%0Aclosed-source%20LMs%20currently%20publicly%20available.%20Our%20extensive%20experiments%20found%0Ano%20empirical%20evidence%20of%20general%20or%20consistent%20self-recognition%20in%20any%20examined%0ALM.%20Instead%2C%20our%20results%20suggest%20that%20given%20a%20set%20of%20alternatives%2C%20LMs%20seek%20to%0Apick%20the%20%22best%22%20answer%2C%20regardless%20of%20its%20origin.%20Moreover%2C%20we%20find%20indications%0Athat%20preferences%20about%20which%20models%20produce%20the%20best%20answers%20are%20consistent%0Aacross%20LMs.%20We%20additionally%20uncover%20novel%20insights%20on%20position%20bias%0Aconsiderations%20for%20LMs%20in%20multiple-choice%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06946v1&entry.124074799=Read"},
{"title": "Pixel-Aware Stable Diffusion for Realistic Image Super-resolution and\n  Personalized Stylization", "author": "Tao Yang and Rongyuan Wu and Peiran Ren and Xuansong Xie and Lei Zhang", "abstract": "  Diffusion models have demonstrated impressive performance in various image\ngeneration, editing, enhancement and translation tasks. In particular, the\npre-trained text-to-image stable diffusion models provide a potential solution\nto the challenging realistic image super-resolution (Real-ISR) and image\nstylization problems with their strong generative priors. However, the existing\nmethods along this line often fail to keep faithful pixel-wise image\nstructures. If extra skip connections between the encoder and the decoder of a\nVAE are used to reproduce details, additional training in image space will be\nrequired, limiting the application to tasks in latent space such as image\nstylization. In this work, we propose a pixel-aware stable diffusion (PASD)\nnetwork to achieve robust Real-ISR and personalized image stylization.\nSpecifically, a pixel-aware cross attention module is introduced to enable\ndiffusion models perceiving image local structures in pixel-wise level, while a\ndegradation removal module is used to extract degradation insensitive features\nto guide the diffusion process together with image high level information. An\nadjustable noise schedule is introduced to further improve the image\nrestoration results. By simply replacing the base diffusion model with a\nstylized one, PASD can generate diverse stylized images without collecting\npairwise training data, and by shifting the base model with an aesthetic one,\nPASD can bring old photos back to life. Extensive experiments in a variety of\nimage enhancement and stylization tasks demonstrate the effectiveness of our\nproposed PASD approach. Our source codes are available at\n\\url{https://github.com/yangxy/PASD/}.\n", "link": "http://arxiv.org/abs/2308.14469v4", "date": "2024-07-09", "relevancy": 1.9161, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6765}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.636}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixel-Aware%20Stable%20Diffusion%20for%20Realistic%20Image%20Super-resolution%20and%0A%20%20Personalized%20Stylization&body=Title%3A%20Pixel-Aware%20Stable%20Diffusion%20for%20Realistic%20Image%20Super-resolution%20and%0A%20%20Personalized%20Stylization%0AAuthor%3A%20Tao%20Yang%20and%20Rongyuan%20Wu%20and%20Peiran%20Ren%20and%20Xuansong%20Xie%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20impressive%20performance%20in%20various%20image%0Ageneration%2C%20editing%2C%20enhancement%20and%20translation%20tasks.%20In%20particular%2C%20the%0Apre-trained%20text-to-image%20stable%20diffusion%20models%20provide%20a%20potential%20solution%0Ato%20the%20challenging%20realistic%20image%20super-resolution%20%28Real-ISR%29%20and%20image%0Astylization%20problems%20with%20their%20strong%20generative%20priors.%20However%2C%20the%20existing%0Amethods%20along%20this%20line%20often%20fail%20to%20keep%20faithful%20pixel-wise%20image%0Astructures.%20If%20extra%20skip%20connections%20between%20the%20encoder%20and%20the%20decoder%20of%20a%0AVAE%20are%20used%20to%20reproduce%20details%2C%20additional%20training%20in%20image%20space%20will%20be%0Arequired%2C%20limiting%20the%20application%20to%20tasks%20in%20latent%20space%20such%20as%20image%0Astylization.%20In%20this%20work%2C%20we%20propose%20a%20pixel-aware%20stable%20diffusion%20%28PASD%29%0Anetwork%20to%20achieve%20robust%20Real-ISR%20and%20personalized%20image%20stylization.%0ASpecifically%2C%20a%20pixel-aware%20cross%20attention%20module%20is%20introduced%20to%20enable%0Adiffusion%20models%20perceiving%20image%20local%20structures%20in%20pixel-wise%20level%2C%20while%20a%0Adegradation%20removal%20module%20is%20used%20to%20extract%20degradation%20insensitive%20features%0Ato%20guide%20the%20diffusion%20process%20together%20with%20image%20high%20level%20information.%20An%0Aadjustable%20noise%20schedule%20is%20introduced%20to%20further%20improve%20the%20image%0Arestoration%20results.%20By%20simply%20replacing%20the%20base%20diffusion%20model%20with%20a%0Astylized%20one%2C%20PASD%20can%20generate%20diverse%20stylized%20images%20without%20collecting%0Apairwise%20training%20data%2C%20and%20by%20shifting%20the%20base%20model%20with%20an%20aesthetic%20one%2C%0APASD%20can%20bring%20old%20photos%20back%20to%20life.%20Extensive%20experiments%20in%20a%20variety%20of%0Aimage%20enhancement%20and%20stylization%20tasks%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20PASD%20approach.%20Our%20source%20codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yangxy/PASD/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.14469v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixel-Aware%2520Stable%2520Diffusion%2520for%2520Realistic%2520Image%2520Super-resolution%2520and%250A%2520%2520Personalized%2520Stylization%26entry.906535625%3DTao%2520Yang%2520and%2520Rongyuan%2520Wu%2520and%2520Peiran%2520Ren%2520and%2520Xuansong%2520Xie%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520impressive%2520performance%2520in%2520various%2520image%250Ageneration%252C%2520editing%252C%2520enhancement%2520and%2520translation%2520tasks.%2520In%2520particular%252C%2520the%250Apre-trained%2520text-to-image%2520stable%2520diffusion%2520models%2520provide%2520a%2520potential%2520solution%250Ato%2520the%2520challenging%2520realistic%2520image%2520super-resolution%2520%2528Real-ISR%2529%2520and%2520image%250Astylization%2520problems%2520with%2520their%2520strong%2520generative%2520priors.%2520However%252C%2520the%2520existing%250Amethods%2520along%2520this%2520line%2520often%2520fail%2520to%2520keep%2520faithful%2520pixel-wise%2520image%250Astructures.%2520If%2520extra%2520skip%2520connections%2520between%2520the%2520encoder%2520and%2520the%2520decoder%2520of%2520a%250AVAE%2520are%2520used%2520to%2520reproduce%2520details%252C%2520additional%2520training%2520in%2520image%2520space%2520will%2520be%250Arequired%252C%2520limiting%2520the%2520application%2520to%2520tasks%2520in%2520latent%2520space%2520such%2520as%2520image%250Astylization.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520pixel-aware%2520stable%2520diffusion%2520%2528PASD%2529%250Anetwork%2520to%2520achieve%2520robust%2520Real-ISR%2520and%2520personalized%2520image%2520stylization.%250ASpecifically%252C%2520a%2520pixel-aware%2520cross%2520attention%2520module%2520is%2520introduced%2520to%2520enable%250Adiffusion%2520models%2520perceiving%2520image%2520local%2520structures%2520in%2520pixel-wise%2520level%252C%2520while%2520a%250Adegradation%2520removal%2520module%2520is%2520used%2520to%2520extract%2520degradation%2520insensitive%2520features%250Ato%2520guide%2520the%2520diffusion%2520process%2520together%2520with%2520image%2520high%2520level%2520information.%2520An%250Aadjustable%2520noise%2520schedule%2520is%2520introduced%2520to%2520further%2520improve%2520the%2520image%250Arestoration%2520results.%2520By%2520simply%2520replacing%2520the%2520base%2520diffusion%2520model%2520with%2520a%250Astylized%2520one%252C%2520PASD%2520can%2520generate%2520diverse%2520stylized%2520images%2520without%2520collecting%250Apairwise%2520training%2520data%252C%2520and%2520by%2520shifting%2520the%2520base%2520model%2520with%2520an%2520aesthetic%2520one%252C%250APASD%2520can%2520bring%2520old%2520photos%2520back%2520to%2520life.%2520Extensive%2520experiments%2520in%2520a%2520variety%2520of%250Aimage%2520enhancement%2520and%2520stylization%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520PASD%2520approach.%2520Our%2520source%2520codes%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/yangxy/PASD/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.14469v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel-Aware%20Stable%20Diffusion%20for%20Realistic%20Image%20Super-resolution%20and%0A%20%20Personalized%20Stylization&entry.906535625=Tao%20Yang%20and%20Rongyuan%20Wu%20and%20Peiran%20Ren%20and%20Xuansong%20Xie%20and%20Lei%20Zhang&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20impressive%20performance%20in%20various%20image%0Ageneration%2C%20editing%2C%20enhancement%20and%20translation%20tasks.%20In%20particular%2C%20the%0Apre-trained%20text-to-image%20stable%20diffusion%20models%20provide%20a%20potential%20solution%0Ato%20the%20challenging%20realistic%20image%20super-resolution%20%28Real-ISR%29%20and%20image%0Astylization%20problems%20with%20their%20strong%20generative%20priors.%20However%2C%20the%20existing%0Amethods%20along%20this%20line%20often%20fail%20to%20keep%20faithful%20pixel-wise%20image%0Astructures.%20If%20extra%20skip%20connections%20between%20the%20encoder%20and%20the%20decoder%20of%20a%0AVAE%20are%20used%20to%20reproduce%20details%2C%20additional%20training%20in%20image%20space%20will%20be%0Arequired%2C%20limiting%20the%20application%20to%20tasks%20in%20latent%20space%20such%20as%20image%0Astylization.%20In%20this%20work%2C%20we%20propose%20a%20pixel-aware%20stable%20diffusion%20%28PASD%29%0Anetwork%20to%20achieve%20robust%20Real-ISR%20and%20personalized%20image%20stylization.%0ASpecifically%2C%20a%20pixel-aware%20cross%20attention%20module%20is%20introduced%20to%20enable%0Adiffusion%20models%20perceiving%20image%20local%20structures%20in%20pixel-wise%20level%2C%20while%20a%0Adegradation%20removal%20module%20is%20used%20to%20extract%20degradation%20insensitive%20features%0Ato%20guide%20the%20diffusion%20process%20together%20with%20image%20high%20level%20information.%20An%0Aadjustable%20noise%20schedule%20is%20introduced%20to%20further%20improve%20the%20image%0Arestoration%20results.%20By%20simply%20replacing%20the%20base%20diffusion%20model%20with%20a%0Astylized%20one%2C%20PASD%20can%20generate%20diverse%20stylized%20images%20without%20collecting%0Apairwise%20training%20data%2C%20and%20by%20shifting%20the%20base%20model%20with%20an%20aesthetic%20one%2C%0APASD%20can%20bring%20old%20photos%20back%20to%20life.%20Extensive%20experiments%20in%20a%20variety%20of%0Aimage%20enhancement%20and%20stylization%20tasks%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20PASD%20approach.%20Our%20source%20codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yangxy/PASD/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.14469v4&entry.124074799=Read"},
{"title": "A Unified Learn-to-Distort-Data Framework for Privacy-Utility Trade-off\n  in Trustworthy Federated Learning", "author": "Xiaojin Zhang and Mingcong Xu and Wei Chen", "abstract": "  In this paper, we first give an introduction to the theoretical basis of the\nprivacy-utility equilibrium in federated learning based on Bayesian privacy\ndefinitions and total variation distance privacy definitions. We then present\nthe \\textit{Learn-to-Distort-Data} framework, which provides a principled\napproach to navigate the privacy-utility equilibrium by explicitly modeling the\ndistortion introduced by the privacy-preserving mechanism as a learnable\nvariable and optimizing it jointly with the model parameters. We demonstrate\nthe applicability of our framework to a variety of privacy-preserving\nmechanisms on the basis of data distortion and highlight its connections to\nrelated areas such as adversarial training, input robustness, and unlearnable\nexamples. These connections enable leveraging techniques from these areas to\ndesign effective algorithms for privacy-utility equilibrium in federated\nlearning under the \\textit{Learn-to-Distort-Data} framework.\n", "link": "http://arxiv.org/abs/2407.04751v2", "date": "2024-07-09", "relevancy": 1.9134, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5382}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4703}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Learn-to-Distort-Data%20Framework%20for%20Privacy-Utility%20Trade-off%0A%20%20in%20Trustworthy%20Federated%20Learning&body=Title%3A%20A%20Unified%20Learn-to-Distort-Data%20Framework%20for%20Privacy-Utility%20Trade-off%0A%20%20in%20Trustworthy%20Federated%20Learning%0AAuthor%3A%20Xiaojin%20Zhang%20and%20Mingcong%20Xu%20and%20Wei%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20first%20give%20an%20introduction%20to%20the%20theoretical%20basis%20of%20the%0Aprivacy-utility%20equilibrium%20in%20federated%20learning%20based%20on%20Bayesian%20privacy%0Adefinitions%20and%20total%20variation%20distance%20privacy%20definitions.%20We%20then%20present%0Athe%20%5Ctextit%7BLearn-to-Distort-Data%7D%20framework%2C%20which%20provides%20a%20principled%0Aapproach%20to%20navigate%20the%20privacy-utility%20equilibrium%20by%20explicitly%20modeling%20the%0Adistortion%20introduced%20by%20the%20privacy-preserving%20mechanism%20as%20a%20learnable%0Avariable%20and%20optimizing%20it%20jointly%20with%20the%20model%20parameters.%20We%20demonstrate%0Athe%20applicability%20of%20our%20framework%20to%20a%20variety%20of%20privacy-preserving%0Amechanisms%20on%20the%20basis%20of%20data%20distortion%20and%20highlight%20its%20connections%20to%0Arelated%20areas%20such%20as%20adversarial%20training%2C%20input%20robustness%2C%20and%20unlearnable%0Aexamples.%20These%20connections%20enable%20leveraging%20techniques%20from%20these%20areas%20to%0Adesign%20effective%20algorithms%20for%20privacy-utility%20equilibrium%20in%20federated%0Alearning%20under%20the%20%5Ctextit%7BLearn-to-Distort-Data%7D%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Learn-to-Distort-Data%2520Framework%2520for%2520Privacy-Utility%2520Trade-off%250A%2520%2520in%2520Trustworthy%2520Federated%2520Learning%26entry.906535625%3DXiaojin%2520Zhang%2520and%2520Mingcong%2520Xu%2520and%2520Wei%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520first%2520give%2520an%2520introduction%2520to%2520the%2520theoretical%2520basis%2520of%2520the%250Aprivacy-utility%2520equilibrium%2520in%2520federated%2520learning%2520based%2520on%2520Bayesian%2520privacy%250Adefinitions%2520and%2520total%2520variation%2520distance%2520privacy%2520definitions.%2520We%2520then%2520present%250Athe%2520%255Ctextit%257BLearn-to-Distort-Data%257D%2520framework%252C%2520which%2520provides%2520a%2520principled%250Aapproach%2520to%2520navigate%2520the%2520privacy-utility%2520equilibrium%2520by%2520explicitly%2520modeling%2520the%250Adistortion%2520introduced%2520by%2520the%2520privacy-preserving%2520mechanism%2520as%2520a%2520learnable%250Avariable%2520and%2520optimizing%2520it%2520jointly%2520with%2520the%2520model%2520parameters.%2520We%2520demonstrate%250Athe%2520applicability%2520of%2520our%2520framework%2520to%2520a%2520variety%2520of%2520privacy-preserving%250Amechanisms%2520on%2520the%2520basis%2520of%2520data%2520distortion%2520and%2520highlight%2520its%2520connections%2520to%250Arelated%2520areas%2520such%2520as%2520adversarial%2520training%252C%2520input%2520robustness%252C%2520and%2520unlearnable%250Aexamples.%2520These%2520connections%2520enable%2520leveraging%2520techniques%2520from%2520these%2520areas%2520to%250Adesign%2520effective%2520algorithms%2520for%2520privacy-utility%2520equilibrium%2520in%2520federated%250Alearning%2520under%2520the%2520%255Ctextit%257BLearn-to-Distort-Data%257D%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Learn-to-Distort-Data%20Framework%20for%20Privacy-Utility%20Trade-off%0A%20%20in%20Trustworthy%20Federated%20Learning&entry.906535625=Xiaojin%20Zhang%20and%20Mingcong%20Xu%20and%20Wei%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20first%20give%20an%20introduction%20to%20the%20theoretical%20basis%20of%20the%0Aprivacy-utility%20equilibrium%20in%20federated%20learning%20based%20on%20Bayesian%20privacy%0Adefinitions%20and%20total%20variation%20distance%20privacy%20definitions.%20We%20then%20present%0Athe%20%5Ctextit%7BLearn-to-Distort-Data%7D%20framework%2C%20which%20provides%20a%20principled%0Aapproach%20to%20navigate%20the%20privacy-utility%20equilibrium%20by%20explicitly%20modeling%20the%0Adistortion%20introduced%20by%20the%20privacy-preserving%20mechanism%20as%20a%20learnable%0Avariable%20and%20optimizing%20it%20jointly%20with%20the%20model%20parameters.%20We%20demonstrate%0Athe%20applicability%20of%20our%20framework%20to%20a%20variety%20of%20privacy-preserving%0Amechanisms%20on%20the%20basis%20of%20data%20distortion%20and%20highlight%20its%20connections%20to%0Arelated%20areas%20such%20as%20adversarial%20training%2C%20input%20robustness%2C%20and%20unlearnable%0Aexamples.%20These%20connections%20enable%20leveraging%20techniques%20from%20these%20areas%20to%0Adesign%20effective%20algorithms%20for%20privacy-utility%20equilibrium%20in%20federated%0Alearning%20under%20the%20%5Ctextit%7BLearn-to-Distort-Data%7D%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04751v2&entry.124074799=Read"},
{"title": "Bayesian Federated Learning with Hamiltonian Monte Carlo: Algorithm and\n  Theory", "author": "Jiajun Liang and Qian Zhang and Wei Deng and Qifan Song and Guang Lin", "abstract": "  This work introduces a novel and efficient Bayesian federated learning\nalgorithm, namely, the Federated Averaging stochastic Hamiltonian Monte Carlo\n(FA-HMC), for parameter estimation and uncertainty quantification. We establish\nrigorous convergence guarantees of FA-HMC on non-iid distributed data sets,\nunder the strong convexity and Hessian smoothness assumptions. Our analysis\ninvestigates the effects of parameter space dimension, noise on gradients and\nmomentum, and the frequency of communication (between the central node and\nlocal nodes) on the convergence and communication costs of FA-HMC. Beyond that,\nwe establish the tightness of our analysis by showing that the convergence rate\ncannot be improved even for continuous FA-HMC process. Moreover, extensive\nempirical studies demonstrate that FA-HMC outperforms the existing Federated\nAveraging-Langevin Monte Carlo (FA-LD) algorithm.\n", "link": "http://arxiv.org/abs/2407.06935v1", "date": "2024-07-09", "relevancy": 1.9106, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5334}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5016}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Federated%20Learning%20with%20Hamiltonian%20Monte%20Carlo%3A%20Algorithm%20and%0A%20%20Theory&body=Title%3A%20Bayesian%20Federated%20Learning%20with%20Hamiltonian%20Monte%20Carlo%3A%20Algorithm%20and%0A%20%20Theory%0AAuthor%3A%20Jiajun%20Liang%20and%20Qian%20Zhang%20and%20Wei%20Deng%20and%20Qifan%20Song%20and%20Guang%20Lin%0AAbstract%3A%20%20%20This%20work%20introduces%20a%20novel%20and%20efficient%20Bayesian%20federated%20learning%0Aalgorithm%2C%20namely%2C%20the%20Federated%20Averaging%20stochastic%20Hamiltonian%20Monte%20Carlo%0A%28FA-HMC%29%2C%20for%20parameter%20estimation%20and%20uncertainty%20quantification.%20We%20establish%0Arigorous%20convergence%20guarantees%20of%20FA-HMC%20on%20non-iid%20distributed%20data%20sets%2C%0Aunder%20the%20strong%20convexity%20and%20Hessian%20smoothness%20assumptions.%20Our%20analysis%0Ainvestigates%20the%20effects%20of%20parameter%20space%20dimension%2C%20noise%20on%20gradients%20and%0Amomentum%2C%20and%20the%20frequency%20of%20communication%20%28between%20the%20central%20node%20and%0Alocal%20nodes%29%20on%20the%20convergence%20and%20communication%20costs%20of%20FA-HMC.%20Beyond%20that%2C%0Awe%20establish%20the%20tightness%20of%20our%20analysis%20by%20showing%20that%20the%20convergence%20rate%0Acannot%20be%20improved%20even%20for%20continuous%20FA-HMC%20process.%20Moreover%2C%20extensive%0Aempirical%20studies%20demonstrate%20that%20FA-HMC%20outperforms%20the%20existing%20Federated%0AAveraging-Langevin%20Monte%20Carlo%20%28FA-LD%29%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Federated%2520Learning%2520with%2520Hamiltonian%2520Monte%2520Carlo%253A%2520Algorithm%2520and%250A%2520%2520Theory%26entry.906535625%3DJiajun%2520Liang%2520and%2520Qian%2520Zhang%2520and%2520Wei%2520Deng%2520and%2520Qifan%2520Song%2520and%2520Guang%2520Lin%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520a%2520novel%2520and%2520efficient%2520Bayesian%2520federated%2520learning%250Aalgorithm%252C%2520namely%252C%2520the%2520Federated%2520Averaging%2520stochastic%2520Hamiltonian%2520Monte%2520Carlo%250A%2528FA-HMC%2529%252C%2520for%2520parameter%2520estimation%2520and%2520uncertainty%2520quantification.%2520We%2520establish%250Arigorous%2520convergence%2520guarantees%2520of%2520FA-HMC%2520on%2520non-iid%2520distributed%2520data%2520sets%252C%250Aunder%2520the%2520strong%2520convexity%2520and%2520Hessian%2520smoothness%2520assumptions.%2520Our%2520analysis%250Ainvestigates%2520the%2520effects%2520of%2520parameter%2520space%2520dimension%252C%2520noise%2520on%2520gradients%2520and%250Amomentum%252C%2520and%2520the%2520frequency%2520of%2520communication%2520%2528between%2520the%2520central%2520node%2520and%250Alocal%2520nodes%2529%2520on%2520the%2520convergence%2520and%2520communication%2520costs%2520of%2520FA-HMC.%2520Beyond%2520that%252C%250Awe%2520establish%2520the%2520tightness%2520of%2520our%2520analysis%2520by%2520showing%2520that%2520the%2520convergence%2520rate%250Acannot%2520be%2520improved%2520even%2520for%2520continuous%2520FA-HMC%2520process.%2520Moreover%252C%2520extensive%250Aempirical%2520studies%2520demonstrate%2520that%2520FA-HMC%2520outperforms%2520the%2520existing%2520Federated%250AAveraging-Langevin%2520Monte%2520Carlo%2520%2528FA-LD%2529%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Federated%20Learning%20with%20Hamiltonian%20Monte%20Carlo%3A%20Algorithm%20and%0A%20%20Theory&entry.906535625=Jiajun%20Liang%20and%20Qian%20Zhang%20and%20Wei%20Deng%20and%20Qifan%20Song%20and%20Guang%20Lin&entry.1292438233=%20%20This%20work%20introduces%20a%20novel%20and%20efficient%20Bayesian%20federated%20learning%0Aalgorithm%2C%20namely%2C%20the%20Federated%20Averaging%20stochastic%20Hamiltonian%20Monte%20Carlo%0A%28FA-HMC%29%2C%20for%20parameter%20estimation%20and%20uncertainty%20quantification.%20We%20establish%0Arigorous%20convergence%20guarantees%20of%20FA-HMC%20on%20non-iid%20distributed%20data%20sets%2C%0Aunder%20the%20strong%20convexity%20and%20Hessian%20smoothness%20assumptions.%20Our%20analysis%0Ainvestigates%20the%20effects%20of%20parameter%20space%20dimension%2C%20noise%20on%20gradients%20and%0Amomentum%2C%20and%20the%20frequency%20of%20communication%20%28between%20the%20central%20node%20and%0Alocal%20nodes%29%20on%20the%20convergence%20and%20communication%20costs%20of%20FA-HMC.%20Beyond%20that%2C%0Awe%20establish%20the%20tightness%20of%20our%20analysis%20by%20showing%20that%20the%20convergence%20rate%0Acannot%20be%20improved%20even%20for%20continuous%20FA-HMC%20process.%20Moreover%2C%20extensive%0Aempirical%20studies%20demonstrate%20that%20FA-HMC%20outperforms%20the%20existing%20Federated%0AAveraging-Langevin%20Monte%20Carlo%20%28FA-LD%29%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06935v1&entry.124074799=Read"},
{"title": "Sampling for Model Predictive Trajectory Planning in Autonomous Driving\n  using Normalizing Flows", "author": "Georg Rabenstein and Lars Ullrich and Knut Graichen", "abstract": "  Alongside optimization-based planners, sampling-based approaches are often\nused in trajectory planning for autonomous driving due to their simplicity.\nModel predictive path integral control is a framework that builds upon\noptimization principles while incorporating stochastic sampling of input\ntrajectories. This paper investigates several sampling approaches for\ntrajectory generation. In this context, normalizing flows originating from the\nfield of variational inference are considered for the generation of sampling\ndistributions, as they model transformations of simple to more complex\ndistributions. Accordingly, learning-based normalizing flow models are trained\nfor a more efficient exploration of the input domain for the task at hand. The\ndeveloped algorithm and the proposed sampling distributions are evaluated in\ntwo simulation scenarios.\n", "link": "http://arxiv.org/abs/2404.09657v2", "date": "2024-07-09", "relevancy": 1.901, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.498}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4779}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling%20for%20Model%20Predictive%20Trajectory%20Planning%20in%20Autonomous%20Driving%0A%20%20using%20Normalizing%20Flows&body=Title%3A%20Sampling%20for%20Model%20Predictive%20Trajectory%20Planning%20in%20Autonomous%20Driving%0A%20%20using%20Normalizing%20Flows%0AAuthor%3A%20Georg%20Rabenstein%20and%20Lars%20Ullrich%20and%20Knut%20Graichen%0AAbstract%3A%20%20%20Alongside%20optimization-based%20planners%2C%20sampling-based%20approaches%20are%20often%0Aused%20in%20trajectory%20planning%20for%20autonomous%20driving%20due%20to%20their%20simplicity.%0AModel%20predictive%20path%20integral%20control%20is%20a%20framework%20that%20builds%20upon%0Aoptimization%20principles%20while%20incorporating%20stochastic%20sampling%20of%20input%0Atrajectories.%20This%20paper%20investigates%20several%20sampling%20approaches%20for%0Atrajectory%20generation.%20In%20this%20context%2C%20normalizing%20flows%20originating%20from%20the%0Afield%20of%20variational%20inference%20are%20considered%20for%20the%20generation%20of%20sampling%0Adistributions%2C%20as%20they%20model%20transformations%20of%20simple%20to%20more%20complex%0Adistributions.%20Accordingly%2C%20learning-based%20normalizing%20flow%20models%20are%20trained%0Afor%20a%20more%20efficient%20exploration%20of%20the%20input%20domain%20for%20the%20task%20at%20hand.%20The%0Adeveloped%20algorithm%20and%20the%20proposed%20sampling%20distributions%20are%20evaluated%20in%0Atwo%20simulation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09657v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling%2520for%2520Model%2520Predictive%2520Trajectory%2520Planning%2520in%2520Autonomous%2520Driving%250A%2520%2520using%2520Normalizing%2520Flows%26entry.906535625%3DGeorg%2520Rabenstein%2520and%2520Lars%2520Ullrich%2520and%2520Knut%2520Graichen%26entry.1292438233%3D%2520%2520Alongside%2520optimization-based%2520planners%252C%2520sampling-based%2520approaches%2520are%2520often%250Aused%2520in%2520trajectory%2520planning%2520for%2520autonomous%2520driving%2520due%2520to%2520their%2520simplicity.%250AModel%2520predictive%2520path%2520integral%2520control%2520is%2520a%2520framework%2520that%2520builds%2520upon%250Aoptimization%2520principles%2520while%2520incorporating%2520stochastic%2520sampling%2520of%2520input%250Atrajectories.%2520This%2520paper%2520investigates%2520several%2520sampling%2520approaches%2520for%250Atrajectory%2520generation.%2520In%2520this%2520context%252C%2520normalizing%2520flows%2520originating%2520from%2520the%250Afield%2520of%2520variational%2520inference%2520are%2520considered%2520for%2520the%2520generation%2520of%2520sampling%250Adistributions%252C%2520as%2520they%2520model%2520transformations%2520of%2520simple%2520to%2520more%2520complex%250Adistributions.%2520Accordingly%252C%2520learning-based%2520normalizing%2520flow%2520models%2520are%2520trained%250Afor%2520a%2520more%2520efficient%2520exploration%2520of%2520the%2520input%2520domain%2520for%2520the%2520task%2520at%2520hand.%2520The%250Adeveloped%2520algorithm%2520and%2520the%2520proposed%2520sampling%2520distributions%2520are%2520evaluated%2520in%250Atwo%2520simulation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09657v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling%20for%20Model%20Predictive%20Trajectory%20Planning%20in%20Autonomous%20Driving%0A%20%20using%20Normalizing%20Flows&entry.906535625=Georg%20Rabenstein%20and%20Lars%20Ullrich%20and%20Knut%20Graichen&entry.1292438233=%20%20Alongside%20optimization-based%20planners%2C%20sampling-based%20approaches%20are%20often%0Aused%20in%20trajectory%20planning%20for%20autonomous%20driving%20due%20to%20their%20simplicity.%0AModel%20predictive%20path%20integral%20control%20is%20a%20framework%20that%20builds%20upon%0Aoptimization%20principles%20while%20incorporating%20stochastic%20sampling%20of%20input%0Atrajectories.%20This%20paper%20investigates%20several%20sampling%20approaches%20for%0Atrajectory%20generation.%20In%20this%20context%2C%20normalizing%20flows%20originating%20from%20the%0Afield%20of%20variational%20inference%20are%20considered%20for%20the%20generation%20of%20sampling%0Adistributions%2C%20as%20they%20model%20transformations%20of%20simple%20to%20more%20complex%0Adistributions.%20Accordingly%2C%20learning-based%20normalizing%20flow%20models%20are%20trained%0Afor%20a%20more%20efficient%20exploration%20of%20the%20input%20domain%20for%20the%20task%20at%20hand.%20The%0Adeveloped%20algorithm%20and%20the%20proposed%20sampling%20distributions%20are%20evaluated%20in%0Atwo%20simulation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09657v2&entry.124074799=Read"},
{"title": "Frieren: Efficient Video-to-Audio Generation with Rectified Flow\n  Matching", "author": "Yongqi Wang and Wenxiang Guo and Rongjie Huang and Jiawei Huang and Zehan Wang and Fuming You and Ruiqi Li and Zhou Zhao", "abstract": "  Video-to-audio (V2A) generation aims to synthesize content-matching audio\nfrom silent video, and it remains challenging to build V2A models with high\ngeneration quality, efficiency, and visual-audio temporal synchrony. We propose\nFrieren, a V2A model based on rectified flow matching. Frieren regresses the\nconditional transport vector field from noise to spectrogram latent with\nstraight paths and conducts sampling by solving ODE, outperforming\nautoregressive and score-based models in terms of audio quality. By employing a\nnon-autoregressive vector field estimator based on a feed-forward transformer\nand channel-level cross-modal feature fusion with strong temporal alignment,\nour model generates audio that is highly synchronized with the input video.\nFurthermore, through reflow and one-step distillation with guided vector field,\nour model can generate decent audio in a few, or even only one sampling step.\nExperiments indicate that Frieren achieves state-of-the-art performance in both\ngeneration quality and temporal alignment on VGGSound, with alignment accuracy\nreaching 97.22%, and 6.2% improvement in inception score over the strong\ndiffusion-based baseline. Audio samples are available at\nhttp://frieren-v2a.github.io .\n", "link": "http://arxiv.org/abs/2406.00320v2", "date": "2024-07-09", "relevancy": 1.8986, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6667}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.631}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frieren%3A%20Efficient%20Video-to-Audio%20Generation%20with%20Rectified%20Flow%0A%20%20Matching&body=Title%3A%20Frieren%3A%20Efficient%20Video-to-Audio%20Generation%20with%20Rectified%20Flow%0A%20%20Matching%0AAuthor%3A%20Yongqi%20Wang%20and%20Wenxiang%20Guo%20and%20Rongjie%20Huang%20and%20Jiawei%20Huang%20and%20Zehan%20Wang%20and%20Fuming%20You%20and%20Ruiqi%20Li%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Video-to-audio%20%28V2A%29%20generation%20aims%20to%20synthesize%20content-matching%20audio%0Afrom%20silent%20video%2C%20and%20it%20remains%20challenging%20to%20build%20V2A%20models%20with%20high%0Ageneration%20quality%2C%20efficiency%2C%20and%20visual-audio%20temporal%20synchrony.%20We%20propose%0AFrieren%2C%20a%20V2A%20model%20based%20on%20rectified%20flow%20matching.%20Frieren%20regresses%20the%0Aconditional%20transport%20vector%20field%20from%20noise%20to%20spectrogram%20latent%20with%0Astraight%20paths%20and%20conducts%20sampling%20by%20solving%20ODE%2C%20outperforming%0Aautoregressive%20and%20score-based%20models%20in%20terms%20of%20audio%20quality.%20By%20employing%20a%0Anon-autoregressive%20vector%20field%20estimator%20based%20on%20a%20feed-forward%20transformer%0Aand%20channel-level%20cross-modal%20feature%20fusion%20with%20strong%20temporal%20alignment%2C%0Aour%20model%20generates%20audio%20that%20is%20highly%20synchronized%20with%20the%20input%20video.%0AFurthermore%2C%20through%20reflow%20and%20one-step%20distillation%20with%20guided%20vector%20field%2C%0Aour%20model%20can%20generate%20decent%20audio%20in%20a%20few%2C%20or%20even%20only%20one%20sampling%20step.%0AExperiments%20indicate%20that%20Frieren%20achieves%20state-of-the-art%20performance%20in%20both%0Ageneration%20quality%20and%20temporal%20alignment%20on%20VGGSound%2C%20with%20alignment%20accuracy%0Areaching%2097.22%25%2C%20and%206.2%25%20improvement%20in%20inception%20score%20over%20the%20strong%0Adiffusion-based%20baseline.%20Audio%20samples%20are%20available%20at%0Ahttp%3A//frieren-v2a.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00320v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrieren%253A%2520Efficient%2520Video-to-Audio%2520Generation%2520with%2520Rectified%2520Flow%250A%2520%2520Matching%26entry.906535625%3DYongqi%2520Wang%2520and%2520Wenxiang%2520Guo%2520and%2520Rongjie%2520Huang%2520and%2520Jiawei%2520Huang%2520and%2520Zehan%2520Wang%2520and%2520Fuming%2520You%2520and%2520Ruiqi%2520Li%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Video-to-audio%2520%2528V2A%2529%2520generation%2520aims%2520to%2520synthesize%2520content-matching%2520audio%250Afrom%2520silent%2520video%252C%2520and%2520it%2520remains%2520challenging%2520to%2520build%2520V2A%2520models%2520with%2520high%250Ageneration%2520quality%252C%2520efficiency%252C%2520and%2520visual-audio%2520temporal%2520synchrony.%2520We%2520propose%250AFrieren%252C%2520a%2520V2A%2520model%2520based%2520on%2520rectified%2520flow%2520matching.%2520Frieren%2520regresses%2520the%250Aconditional%2520transport%2520vector%2520field%2520from%2520noise%2520to%2520spectrogram%2520latent%2520with%250Astraight%2520paths%2520and%2520conducts%2520sampling%2520by%2520solving%2520ODE%252C%2520outperforming%250Aautoregressive%2520and%2520score-based%2520models%2520in%2520terms%2520of%2520audio%2520quality.%2520By%2520employing%2520a%250Anon-autoregressive%2520vector%2520field%2520estimator%2520based%2520on%2520a%2520feed-forward%2520transformer%250Aand%2520channel-level%2520cross-modal%2520feature%2520fusion%2520with%2520strong%2520temporal%2520alignment%252C%250Aour%2520model%2520generates%2520audio%2520that%2520is%2520highly%2520synchronized%2520with%2520the%2520input%2520video.%250AFurthermore%252C%2520through%2520reflow%2520and%2520one-step%2520distillation%2520with%2520guided%2520vector%2520field%252C%250Aour%2520model%2520can%2520generate%2520decent%2520audio%2520in%2520a%2520few%252C%2520or%2520even%2520only%2520one%2520sampling%2520step.%250AExperiments%2520indicate%2520that%2520Frieren%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%250Ageneration%2520quality%2520and%2520temporal%2520alignment%2520on%2520VGGSound%252C%2520with%2520alignment%2520accuracy%250Areaching%252097.22%2525%252C%2520and%25206.2%2525%2520improvement%2520in%2520inception%2520score%2520over%2520the%2520strong%250Adiffusion-based%2520baseline.%2520Audio%2520samples%2520are%2520available%2520at%250Ahttp%253A//frieren-v2a.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00320v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frieren%3A%20Efficient%20Video-to-Audio%20Generation%20with%20Rectified%20Flow%0A%20%20Matching&entry.906535625=Yongqi%20Wang%20and%20Wenxiang%20Guo%20and%20Rongjie%20Huang%20and%20Jiawei%20Huang%20and%20Zehan%20Wang%20and%20Fuming%20You%20and%20Ruiqi%20Li%20and%20Zhou%20Zhao&entry.1292438233=%20%20Video-to-audio%20%28V2A%29%20generation%20aims%20to%20synthesize%20content-matching%20audio%0Afrom%20silent%20video%2C%20and%20it%20remains%20challenging%20to%20build%20V2A%20models%20with%20high%0Ageneration%20quality%2C%20efficiency%2C%20and%20visual-audio%20temporal%20synchrony.%20We%20propose%0AFrieren%2C%20a%20V2A%20model%20based%20on%20rectified%20flow%20matching.%20Frieren%20regresses%20the%0Aconditional%20transport%20vector%20field%20from%20noise%20to%20spectrogram%20latent%20with%0Astraight%20paths%20and%20conducts%20sampling%20by%20solving%20ODE%2C%20outperforming%0Aautoregressive%20and%20score-based%20models%20in%20terms%20of%20audio%20quality.%20By%20employing%20a%0Anon-autoregressive%20vector%20field%20estimator%20based%20on%20a%20feed-forward%20transformer%0Aand%20channel-level%20cross-modal%20feature%20fusion%20with%20strong%20temporal%20alignment%2C%0Aour%20model%20generates%20audio%20that%20is%20highly%20synchronized%20with%20the%20input%20video.%0AFurthermore%2C%20through%20reflow%20and%20one-step%20distillation%20with%20guided%20vector%20field%2C%0Aour%20model%20can%20generate%20decent%20audio%20in%20a%20few%2C%20or%20even%20only%20one%20sampling%20step.%0AExperiments%20indicate%20that%20Frieren%20achieves%20state-of-the-art%20performance%20in%20both%0Ageneration%20quality%20and%20temporal%20alignment%20on%20VGGSound%2C%20with%20alignment%20accuracy%0Areaching%2097.22%25%2C%20and%206.2%25%20improvement%20in%20inception%20score%20over%20the%20strong%0Adiffusion-based%20baseline.%20Audio%20samples%20are%20available%20at%0Ahttp%3A//frieren-v2a.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00320v2&entry.124074799=Read"},
{"title": "Can virtual staining for high-throughput screening generalize?", "author": "Samuel Tonks and Cuong Nguyer and Steve Hood and Ryan Musso and Ceridwen Hopely and Steve Titus and Minh Doan and Iain Styles and Alexander Krull", "abstract": "  The large volume and variety of imaging data from high-throughput screening\n(HTS) in the pharmaceutical industry present an excellent resource for training\nvirtual staining models. However, the potential of models trained under one set\nof experimental conditions to generalize to other conditions remains\nunderexplored. This study systematically investigates whether data from three\ncell types (lung, ovarian, and breast) and two phenotypes (toxic and non-toxic\nconditions) commonly found in HTS can effectively train virtual staining models\nto generalize across three typical HTS distribution shifts: unseen phenotypes,\nunseen cell types, and the combination of both. Utilizing a dataset of 772,416\npaired bright-field, cytoplasm, nuclei, and DNA-damage stain images, we\nevaluate the generalization capabilities of models across pixel-based,\ninstance-wise, and biological-feature-based levels. Our findings indicate that\ntraining virtual nuclei and cytoplasm models on non-toxic condition samples not\nonly generalizes to toxic condition samples but leads to improved performance\nacross all evaluation levels compared to training on toxic condition samples.\nGeneralization to unseen cell types shows variability depending on the cell\ntype; models trained on ovarian or lung cell samples often perform well under\nother conditions, while those trained on breast cell samples consistently show\npoor generalization. Generalization to unseen cell types and phenotypes shows\ngood generalization across all levels of evaluation compared to addressing\nunseen cell types alone. This study represents the first large-scale,\ndata-centric analysis of the generalization capability of virtual staining\nmodels trained on diverse HTS datasets, providing valuable strategies for\nexperimental training data generation.\n", "link": "http://arxiv.org/abs/2407.06979v1", "date": "2024-07-09", "relevancy": 1.8858, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4814}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4668}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20virtual%20staining%20for%20high-throughput%20screening%20generalize%3F&body=Title%3A%20Can%20virtual%20staining%20for%20high-throughput%20screening%20generalize%3F%0AAuthor%3A%20Samuel%20Tonks%20and%20Cuong%20Nguyer%20and%20Steve%20Hood%20and%20Ryan%20Musso%20and%20Ceridwen%20Hopely%20and%20Steve%20Titus%20and%20Minh%20Doan%20and%20Iain%20Styles%20and%20Alexander%20Krull%0AAbstract%3A%20%20%20The%20large%20volume%20and%20variety%20of%20imaging%20data%20from%20high-throughput%20screening%0A%28HTS%29%20in%20the%20pharmaceutical%20industry%20present%20an%20excellent%20resource%20for%20training%0Avirtual%20staining%20models.%20However%2C%20the%20potential%20of%20models%20trained%20under%20one%20set%0Aof%20experimental%20conditions%20to%20generalize%20to%20other%20conditions%20remains%0Aunderexplored.%20This%20study%20systematically%20investigates%20whether%20data%20from%20three%0Acell%20types%20%28lung%2C%20ovarian%2C%20and%20breast%29%20and%20two%20phenotypes%20%28toxic%20and%20non-toxic%0Aconditions%29%20commonly%20found%20in%20HTS%20can%20effectively%20train%20virtual%20staining%20models%0Ato%20generalize%20across%20three%20typical%20HTS%20distribution%20shifts%3A%20unseen%20phenotypes%2C%0Aunseen%20cell%20types%2C%20and%20the%20combination%20of%20both.%20Utilizing%20a%20dataset%20of%20772%2C416%0Apaired%20bright-field%2C%20cytoplasm%2C%20nuclei%2C%20and%20DNA-damage%20stain%20images%2C%20we%0Aevaluate%20the%20generalization%20capabilities%20of%20models%20across%20pixel-based%2C%0Ainstance-wise%2C%20and%20biological-feature-based%20levels.%20Our%20findings%20indicate%20that%0Atraining%20virtual%20nuclei%20and%20cytoplasm%20models%20on%20non-toxic%20condition%20samples%20not%0Aonly%20generalizes%20to%20toxic%20condition%20samples%20but%20leads%20to%20improved%20performance%0Aacross%20all%20evaluation%20levels%20compared%20to%20training%20on%20toxic%20condition%20samples.%0AGeneralization%20to%20unseen%20cell%20types%20shows%20variability%20depending%20on%20the%20cell%0Atype%3B%20models%20trained%20on%20ovarian%20or%20lung%20cell%20samples%20often%20perform%20well%20under%0Aother%20conditions%2C%20while%20those%20trained%20on%20breast%20cell%20samples%20consistently%20show%0Apoor%20generalization.%20Generalization%20to%20unseen%20cell%20types%20and%20phenotypes%20shows%0Agood%20generalization%20across%20all%20levels%20of%20evaluation%20compared%20to%20addressing%0Aunseen%20cell%20types%20alone.%20This%20study%20represents%20the%20first%20large-scale%2C%0Adata-centric%20analysis%20of%20the%20generalization%20capability%20of%20virtual%20staining%0Amodels%20trained%20on%20diverse%20HTS%20datasets%2C%20providing%20valuable%20strategies%20for%0Aexperimental%20training%20data%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520virtual%2520staining%2520for%2520high-throughput%2520screening%2520generalize%253F%26entry.906535625%3DSamuel%2520Tonks%2520and%2520Cuong%2520Nguyer%2520and%2520Steve%2520Hood%2520and%2520Ryan%2520Musso%2520and%2520Ceridwen%2520Hopely%2520and%2520Steve%2520Titus%2520and%2520Minh%2520Doan%2520and%2520Iain%2520Styles%2520and%2520Alexander%2520Krull%26entry.1292438233%3D%2520%2520The%2520large%2520volume%2520and%2520variety%2520of%2520imaging%2520data%2520from%2520high-throughput%2520screening%250A%2528HTS%2529%2520in%2520the%2520pharmaceutical%2520industry%2520present%2520an%2520excellent%2520resource%2520for%2520training%250Avirtual%2520staining%2520models.%2520However%252C%2520the%2520potential%2520of%2520models%2520trained%2520under%2520one%2520set%250Aof%2520experimental%2520conditions%2520to%2520generalize%2520to%2520other%2520conditions%2520remains%250Aunderexplored.%2520This%2520study%2520systematically%2520investigates%2520whether%2520data%2520from%2520three%250Acell%2520types%2520%2528lung%252C%2520ovarian%252C%2520and%2520breast%2529%2520and%2520two%2520phenotypes%2520%2528toxic%2520and%2520non-toxic%250Aconditions%2529%2520commonly%2520found%2520in%2520HTS%2520can%2520effectively%2520train%2520virtual%2520staining%2520models%250Ato%2520generalize%2520across%2520three%2520typical%2520HTS%2520distribution%2520shifts%253A%2520unseen%2520phenotypes%252C%250Aunseen%2520cell%2520types%252C%2520and%2520the%2520combination%2520of%2520both.%2520Utilizing%2520a%2520dataset%2520of%2520772%252C416%250Apaired%2520bright-field%252C%2520cytoplasm%252C%2520nuclei%252C%2520and%2520DNA-damage%2520stain%2520images%252C%2520we%250Aevaluate%2520the%2520generalization%2520capabilities%2520of%2520models%2520across%2520pixel-based%252C%250Ainstance-wise%252C%2520and%2520biological-feature-based%2520levels.%2520Our%2520findings%2520indicate%2520that%250Atraining%2520virtual%2520nuclei%2520and%2520cytoplasm%2520models%2520on%2520non-toxic%2520condition%2520samples%2520not%250Aonly%2520generalizes%2520to%2520toxic%2520condition%2520samples%2520but%2520leads%2520to%2520improved%2520performance%250Aacross%2520all%2520evaluation%2520levels%2520compared%2520to%2520training%2520on%2520toxic%2520condition%2520samples.%250AGeneralization%2520to%2520unseen%2520cell%2520types%2520shows%2520variability%2520depending%2520on%2520the%2520cell%250Atype%253B%2520models%2520trained%2520on%2520ovarian%2520or%2520lung%2520cell%2520samples%2520often%2520perform%2520well%2520under%250Aother%2520conditions%252C%2520while%2520those%2520trained%2520on%2520breast%2520cell%2520samples%2520consistently%2520show%250Apoor%2520generalization.%2520Generalization%2520to%2520unseen%2520cell%2520types%2520and%2520phenotypes%2520shows%250Agood%2520generalization%2520across%2520all%2520levels%2520of%2520evaluation%2520compared%2520to%2520addressing%250Aunseen%2520cell%2520types%2520alone.%2520This%2520study%2520represents%2520the%2520first%2520large-scale%252C%250Adata-centric%2520analysis%2520of%2520the%2520generalization%2520capability%2520of%2520virtual%2520staining%250Amodels%2520trained%2520on%2520diverse%2520HTS%2520datasets%252C%2520providing%2520valuable%2520strategies%2520for%250Aexperimental%2520training%2520data%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20virtual%20staining%20for%20high-throughput%20screening%20generalize%3F&entry.906535625=Samuel%20Tonks%20and%20Cuong%20Nguyer%20and%20Steve%20Hood%20and%20Ryan%20Musso%20and%20Ceridwen%20Hopely%20and%20Steve%20Titus%20and%20Minh%20Doan%20and%20Iain%20Styles%20and%20Alexander%20Krull&entry.1292438233=%20%20The%20large%20volume%20and%20variety%20of%20imaging%20data%20from%20high-throughput%20screening%0A%28HTS%29%20in%20the%20pharmaceutical%20industry%20present%20an%20excellent%20resource%20for%20training%0Avirtual%20staining%20models.%20However%2C%20the%20potential%20of%20models%20trained%20under%20one%20set%0Aof%20experimental%20conditions%20to%20generalize%20to%20other%20conditions%20remains%0Aunderexplored.%20This%20study%20systematically%20investigates%20whether%20data%20from%20three%0Acell%20types%20%28lung%2C%20ovarian%2C%20and%20breast%29%20and%20two%20phenotypes%20%28toxic%20and%20non-toxic%0Aconditions%29%20commonly%20found%20in%20HTS%20can%20effectively%20train%20virtual%20staining%20models%0Ato%20generalize%20across%20three%20typical%20HTS%20distribution%20shifts%3A%20unseen%20phenotypes%2C%0Aunseen%20cell%20types%2C%20and%20the%20combination%20of%20both.%20Utilizing%20a%20dataset%20of%20772%2C416%0Apaired%20bright-field%2C%20cytoplasm%2C%20nuclei%2C%20and%20DNA-damage%20stain%20images%2C%20we%0Aevaluate%20the%20generalization%20capabilities%20of%20models%20across%20pixel-based%2C%0Ainstance-wise%2C%20and%20biological-feature-based%20levels.%20Our%20findings%20indicate%20that%0Atraining%20virtual%20nuclei%20and%20cytoplasm%20models%20on%20non-toxic%20condition%20samples%20not%0Aonly%20generalizes%20to%20toxic%20condition%20samples%20but%20leads%20to%20improved%20performance%0Aacross%20all%20evaluation%20levels%20compared%20to%20training%20on%20toxic%20condition%20samples.%0AGeneralization%20to%20unseen%20cell%20types%20shows%20variability%20depending%20on%20the%20cell%0Atype%3B%20models%20trained%20on%20ovarian%20or%20lung%20cell%20samples%20often%20perform%20well%20under%0Aother%20conditions%2C%20while%20those%20trained%20on%20breast%20cell%20samples%20consistently%20show%0Apoor%20generalization.%20Generalization%20to%20unseen%20cell%20types%20and%20phenotypes%20shows%0Agood%20generalization%20across%20all%20levels%20of%20evaluation%20compared%20to%20addressing%0Aunseen%20cell%20types%20alone.%20This%20study%20represents%20the%20first%20large-scale%2C%0Adata-centric%20analysis%20of%20the%20generalization%20capability%20of%20virtual%20staining%0Amodels%20trained%20on%20diverse%20HTS%20datasets%2C%20providing%20valuable%20strategies%20for%0Aexperimental%20training%20data%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06979v1&entry.124074799=Read"},
{"title": "Shape-aware synthesis of pathological lung CT scans using CycleGAN for\n  enhanced semi-supervised lung segmentation", "author": "Rezkellah Noureddine Khiati and Pierre-Yves Brillet and Aur\u00e9lien Justet and Radu Ispas and Catalin Fetita", "abstract": "  This paper addresses the problem of pathological lung segmentation, a\nsignificant challenge in medical image analysis, particularly pronounced in\ncases of peripheral opacities (severe fibrosis and consolidation) because of\nthe textural similarity between lung tissue and surrounding areas. To overcome\nthese challenges, this paper emphasizes the use of CycleGAN for unpaired\nimage-to-image translation, in order to provide an augmentation method able to\ngenerate fake pathological images matching an existing ground truth. Although\nprevious studies have employed CycleGAN, they often neglect the challenge of\nshape deformation, which is crucial for accurate medical image segmentation.\nOur work introduces an innovative strategy that incorporates additional loss\nfunctions. Specifically, it proposes an L1 loss based on the lung surrounding\nwhich shape is constrained to remain unchanged at the transition from the\nhealthy to pathological domains. The lung surrounding is derived based on\nground truth lung masks available in the healthy domain. Furthermore,\npreprocessing steps, such as cropping based on ribs/vertebra locations, are\napplied to refine the input for the CycleGAN, ensuring that the network focus\non the lung region. This is essential to avoid extraneous biases, such as the\nzoom effect bias, which can divert attention from the main task. The method is\napplied to enhance in semi-supervised manner the lung segmentation process by\nemploying a U-Net model trained with on-the-fly data augmentation incorporating\nsynthetic pathological tissues generated by the CycleGAN model. Preliminary\nresults from this research demonstrate significant qualitative and quantitative\nimprovements, setting a new benchmark in the field of pathological lung\nsegmentation. Our code is available at\nhttps://github.com/noureddinekhiati/Semi-supervised-lung-segmentation\n", "link": "http://arxiv.org/abs/2405.08556v2", "date": "2024-07-09", "relevancy": 1.8833, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.485}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4621}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shape-aware%20synthesis%20of%20pathological%20lung%20CT%20scans%20using%20CycleGAN%20for%0A%20%20enhanced%20semi-supervised%20lung%20segmentation&body=Title%3A%20Shape-aware%20synthesis%20of%20pathological%20lung%20CT%20scans%20using%20CycleGAN%20for%0A%20%20enhanced%20semi-supervised%20lung%20segmentation%0AAuthor%3A%20Rezkellah%20Noureddine%20Khiati%20and%20Pierre-Yves%20Brillet%20and%20Aur%C3%A9lien%20Justet%20and%20Radu%20Ispas%20and%20Catalin%20Fetita%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20problem%20of%20pathological%20lung%20segmentation%2C%20a%0Asignificant%20challenge%20in%20medical%20image%20analysis%2C%20particularly%20pronounced%20in%0Acases%20of%20peripheral%20opacities%20%28severe%20fibrosis%20and%20consolidation%29%20because%20of%0Athe%20textural%20similarity%20between%20lung%20tissue%20and%20surrounding%20areas.%20To%20overcome%0Athese%20challenges%2C%20this%20paper%20emphasizes%20the%20use%20of%20CycleGAN%20for%20unpaired%0Aimage-to-image%20translation%2C%20in%20order%20to%20provide%20an%20augmentation%20method%20able%20to%0Agenerate%20fake%20pathological%20images%20matching%20an%20existing%20ground%20truth.%20Although%0Aprevious%20studies%20have%20employed%20CycleGAN%2C%20they%20often%20neglect%20the%20challenge%20of%0Ashape%20deformation%2C%20which%20is%20crucial%20for%20accurate%20medical%20image%20segmentation.%0AOur%20work%20introduces%20an%20innovative%20strategy%20that%20incorporates%20additional%20loss%0Afunctions.%20Specifically%2C%20it%20proposes%20an%20L1%20loss%20based%20on%20the%20lung%20surrounding%0Awhich%20shape%20is%20constrained%20to%20remain%20unchanged%20at%20the%20transition%20from%20the%0Ahealthy%20to%20pathological%20domains.%20The%20lung%20surrounding%20is%20derived%20based%20on%0Aground%20truth%20lung%20masks%20available%20in%20the%20healthy%20domain.%20Furthermore%2C%0Apreprocessing%20steps%2C%20such%20as%20cropping%20based%20on%20ribs/vertebra%20locations%2C%20are%0Aapplied%20to%20refine%20the%20input%20for%20the%20CycleGAN%2C%20ensuring%20that%20the%20network%20focus%0Aon%20the%20lung%20region.%20This%20is%20essential%20to%20avoid%20extraneous%20biases%2C%20such%20as%20the%0Azoom%20effect%20bias%2C%20which%20can%20divert%20attention%20from%20the%20main%20task.%20The%20method%20is%0Aapplied%20to%20enhance%20in%20semi-supervised%20manner%20the%20lung%20segmentation%20process%20by%0Aemploying%20a%20U-Net%20model%20trained%20with%20on-the-fly%20data%20augmentation%20incorporating%0Asynthetic%20pathological%20tissues%20generated%20by%20the%20CycleGAN%20model.%20Preliminary%0Aresults%20from%20this%20research%20demonstrate%20significant%20qualitative%20and%20quantitative%0Aimprovements%2C%20setting%20a%20new%20benchmark%20in%20the%20field%20of%20pathological%20lung%0Asegmentation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/noureddinekhiati/Semi-supervised-lung-segmentation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08556v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShape-aware%2520synthesis%2520of%2520pathological%2520lung%2520CT%2520scans%2520using%2520CycleGAN%2520for%250A%2520%2520enhanced%2520semi-supervised%2520lung%2520segmentation%26entry.906535625%3DRezkellah%2520Noureddine%2520Khiati%2520and%2520Pierre-Yves%2520Brillet%2520and%2520Aur%25C3%25A9lien%2520Justet%2520and%2520Radu%2520Ispas%2520and%2520Catalin%2520Fetita%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520pathological%2520lung%2520segmentation%252C%2520a%250Asignificant%2520challenge%2520in%2520medical%2520image%2520analysis%252C%2520particularly%2520pronounced%2520in%250Acases%2520of%2520peripheral%2520opacities%2520%2528severe%2520fibrosis%2520and%2520consolidation%2529%2520because%2520of%250Athe%2520textural%2520similarity%2520between%2520lung%2520tissue%2520and%2520surrounding%2520areas.%2520To%2520overcome%250Athese%2520challenges%252C%2520this%2520paper%2520emphasizes%2520the%2520use%2520of%2520CycleGAN%2520for%2520unpaired%250Aimage-to-image%2520translation%252C%2520in%2520order%2520to%2520provide%2520an%2520augmentation%2520method%2520able%2520to%250Agenerate%2520fake%2520pathological%2520images%2520matching%2520an%2520existing%2520ground%2520truth.%2520Although%250Aprevious%2520studies%2520have%2520employed%2520CycleGAN%252C%2520they%2520often%2520neglect%2520the%2520challenge%2520of%250Ashape%2520deformation%252C%2520which%2520is%2520crucial%2520for%2520accurate%2520medical%2520image%2520segmentation.%250AOur%2520work%2520introduces%2520an%2520innovative%2520strategy%2520that%2520incorporates%2520additional%2520loss%250Afunctions.%2520Specifically%252C%2520it%2520proposes%2520an%2520L1%2520loss%2520based%2520on%2520the%2520lung%2520surrounding%250Awhich%2520shape%2520is%2520constrained%2520to%2520remain%2520unchanged%2520at%2520the%2520transition%2520from%2520the%250Ahealthy%2520to%2520pathological%2520domains.%2520The%2520lung%2520surrounding%2520is%2520derived%2520based%2520on%250Aground%2520truth%2520lung%2520masks%2520available%2520in%2520the%2520healthy%2520domain.%2520Furthermore%252C%250Apreprocessing%2520steps%252C%2520such%2520as%2520cropping%2520based%2520on%2520ribs/vertebra%2520locations%252C%2520are%250Aapplied%2520to%2520refine%2520the%2520input%2520for%2520the%2520CycleGAN%252C%2520ensuring%2520that%2520the%2520network%2520focus%250Aon%2520the%2520lung%2520region.%2520This%2520is%2520essential%2520to%2520avoid%2520extraneous%2520biases%252C%2520such%2520as%2520the%250Azoom%2520effect%2520bias%252C%2520which%2520can%2520divert%2520attention%2520from%2520the%2520main%2520task.%2520The%2520method%2520is%250Aapplied%2520to%2520enhance%2520in%2520semi-supervised%2520manner%2520the%2520lung%2520segmentation%2520process%2520by%250Aemploying%2520a%2520U-Net%2520model%2520trained%2520with%2520on-the-fly%2520data%2520augmentation%2520incorporating%250Asynthetic%2520pathological%2520tissues%2520generated%2520by%2520the%2520CycleGAN%2520model.%2520Preliminary%250Aresults%2520from%2520this%2520research%2520demonstrate%2520significant%2520qualitative%2520and%2520quantitative%250Aimprovements%252C%2520setting%2520a%2520new%2520benchmark%2520in%2520the%2520field%2520of%2520pathological%2520lung%250Asegmentation.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/noureddinekhiati/Semi-supervised-lung-segmentation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08556v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shape-aware%20synthesis%20of%20pathological%20lung%20CT%20scans%20using%20CycleGAN%20for%0A%20%20enhanced%20semi-supervised%20lung%20segmentation&entry.906535625=Rezkellah%20Noureddine%20Khiati%20and%20Pierre-Yves%20Brillet%20and%20Aur%C3%A9lien%20Justet%20and%20Radu%20Ispas%20and%20Catalin%20Fetita&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20pathological%20lung%20segmentation%2C%20a%0Asignificant%20challenge%20in%20medical%20image%20analysis%2C%20particularly%20pronounced%20in%0Acases%20of%20peripheral%20opacities%20%28severe%20fibrosis%20and%20consolidation%29%20because%20of%0Athe%20textural%20similarity%20between%20lung%20tissue%20and%20surrounding%20areas.%20To%20overcome%0Athese%20challenges%2C%20this%20paper%20emphasizes%20the%20use%20of%20CycleGAN%20for%20unpaired%0Aimage-to-image%20translation%2C%20in%20order%20to%20provide%20an%20augmentation%20method%20able%20to%0Agenerate%20fake%20pathological%20images%20matching%20an%20existing%20ground%20truth.%20Although%0Aprevious%20studies%20have%20employed%20CycleGAN%2C%20they%20often%20neglect%20the%20challenge%20of%0Ashape%20deformation%2C%20which%20is%20crucial%20for%20accurate%20medical%20image%20segmentation.%0AOur%20work%20introduces%20an%20innovative%20strategy%20that%20incorporates%20additional%20loss%0Afunctions.%20Specifically%2C%20it%20proposes%20an%20L1%20loss%20based%20on%20the%20lung%20surrounding%0Awhich%20shape%20is%20constrained%20to%20remain%20unchanged%20at%20the%20transition%20from%20the%0Ahealthy%20to%20pathological%20domains.%20The%20lung%20surrounding%20is%20derived%20based%20on%0Aground%20truth%20lung%20masks%20available%20in%20the%20healthy%20domain.%20Furthermore%2C%0Apreprocessing%20steps%2C%20such%20as%20cropping%20based%20on%20ribs/vertebra%20locations%2C%20are%0Aapplied%20to%20refine%20the%20input%20for%20the%20CycleGAN%2C%20ensuring%20that%20the%20network%20focus%0Aon%20the%20lung%20region.%20This%20is%20essential%20to%20avoid%20extraneous%20biases%2C%20such%20as%20the%0Azoom%20effect%20bias%2C%20which%20can%20divert%20attention%20from%20the%20main%20task.%20The%20method%20is%0Aapplied%20to%20enhance%20in%20semi-supervised%20manner%20the%20lung%20segmentation%20process%20by%0Aemploying%20a%20U-Net%20model%20trained%20with%20on-the-fly%20data%20augmentation%20incorporating%0Asynthetic%20pathological%20tissues%20generated%20by%20the%20CycleGAN%20model.%20Preliminary%0Aresults%20from%20this%20research%20demonstrate%20significant%20qualitative%20and%20quantitative%0Aimprovements%2C%20setting%20a%20new%20benchmark%20in%20the%20field%20of%20pathological%20lung%0Asegmentation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/noureddinekhiati/Semi-supervised-lung-segmentation%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08556v2&entry.124074799=Read"},
{"title": "MADE-for-ASD: A Multi-Atlas Deep Ensemble Network for Diagnosing Autism\n  Spectrum Disorder", "author": "Md Rakibul Hasan and Xuehan Liu and Tom Gedeon and Md Zakir Hossain", "abstract": "  In response to the global need for efficient early diagnosis of Autism\nSpectrum Disorder (ASD), this paper bridges the gap between traditional,\ntime-consuming diagnostic methods and potential automated solutions. We propose\na multi-atlas deep ensemble network, MADE-for-ASD, that integrates multiple\natlases of the brain's functional magnetic resonance imaging (fMRI) data\nthrough a weighted deep ensemble network. Our approach integrates demographic\ninformation into the prediction workflow, which enhances ASD diagnosis\nperformance and offers a more holistic perspective on patient profiling. We\nexperiment with the well-known publicly available ABIDE (Autism Brain Imaging\nData Exchange) I dataset, consisting of resting state fMRI data from 17\ndifferent laboratories around the globe. Our proposed system achieves 75.20%\naccuracy on the entire dataset and 96.40% on a specific subset $-$ both\nsurpassing reported ASD diagnosis accuracy in ABIDE I fMRI studies.\nSpecifically, our model improves by 4.4 percentage points over prior works on\nthe same amount of data. The model exhibits a sensitivity of 82.90% and a\nspecificity of 69.70% on the entire dataset, and 91.00% and 99.50%,\nrespectively, on the specific subset. We leverage the F-score to pinpoint the\ntop 10 ROI in ASD diagnosis, such as \\emph{precuneus} and anterior\n\\emph{cingulate/ventromedial}. The proposed system can potentially pave the way\nfor more cost-effective, efficient and scalable strategies in ASD diagnosis.\nCodes and evaluations are publicly available at TBA.\n", "link": "http://arxiv.org/abs/2407.07076v1", "date": "2024-07-09", "relevancy": 1.4038, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4808}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4648}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MADE-for-ASD%3A%20A%20Multi-Atlas%20Deep%20Ensemble%20Network%20for%20Diagnosing%20Autism%0A%20%20Spectrum%20Disorder&body=Title%3A%20MADE-for-ASD%3A%20A%20Multi-Atlas%20Deep%20Ensemble%20Network%20for%20Diagnosing%20Autism%0A%20%20Spectrum%20Disorder%0AAuthor%3A%20Md%20Rakibul%20Hasan%20and%20Xuehan%20Liu%20and%20Tom%20Gedeon%20and%20Md%20Zakir%20Hossain%0AAbstract%3A%20%20%20In%20response%20to%20the%20global%20need%20for%20efficient%20early%20diagnosis%20of%20Autism%0ASpectrum%20Disorder%20%28ASD%29%2C%20this%20paper%20bridges%20the%20gap%20between%20traditional%2C%0Atime-consuming%20diagnostic%20methods%20and%20potential%20automated%20solutions.%20We%20propose%0Aa%20multi-atlas%20deep%20ensemble%20network%2C%20MADE-for-ASD%2C%20that%20integrates%20multiple%0Aatlases%20of%20the%20brain%27s%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20data%0Athrough%20a%20weighted%20deep%20ensemble%20network.%20Our%20approach%20integrates%20demographic%0Ainformation%20into%20the%20prediction%20workflow%2C%20which%20enhances%20ASD%20diagnosis%0Aperformance%20and%20offers%20a%20more%20holistic%20perspective%20on%20patient%20profiling.%20We%0Aexperiment%20with%20the%20well-known%20publicly%20available%20ABIDE%20%28Autism%20Brain%20Imaging%0AData%20Exchange%29%20I%20dataset%2C%20consisting%20of%20resting%20state%20fMRI%20data%20from%2017%0Adifferent%20laboratories%20around%20the%20globe.%20Our%20proposed%20system%20achieves%2075.20%25%0Aaccuracy%20on%20the%20entire%20dataset%20and%2096.40%25%20on%20a%20specific%20subset%20%24-%24%20both%0Asurpassing%20reported%20ASD%20diagnosis%20accuracy%20in%20ABIDE%20I%20fMRI%20studies.%0ASpecifically%2C%20our%20model%20improves%20by%204.4%20percentage%20points%20over%20prior%20works%20on%0Athe%20same%20amount%20of%20data.%20The%20model%20exhibits%20a%20sensitivity%20of%2082.90%25%20and%20a%0Aspecificity%20of%2069.70%25%20on%20the%20entire%20dataset%2C%20and%2091.00%25%20and%2099.50%25%2C%0Arespectively%2C%20on%20the%20specific%20subset.%20We%20leverage%20the%20F-score%20to%20pinpoint%20the%0Atop%2010%20ROI%20in%20ASD%20diagnosis%2C%20such%20as%20%5Cemph%7Bprecuneus%7D%20and%20anterior%0A%5Cemph%7Bcingulate/ventromedial%7D.%20The%20proposed%20system%20can%20potentially%20pave%20the%20way%0Afor%20more%20cost-effective%2C%20efficient%20and%20scalable%20strategies%20in%20ASD%20diagnosis.%0ACodes%20and%20evaluations%20are%20publicly%20available%20at%20TBA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMADE-for-ASD%253A%2520A%2520Multi-Atlas%2520Deep%2520Ensemble%2520Network%2520for%2520Diagnosing%2520Autism%250A%2520%2520Spectrum%2520Disorder%26entry.906535625%3DMd%2520Rakibul%2520Hasan%2520and%2520Xuehan%2520Liu%2520and%2520Tom%2520Gedeon%2520and%2520Md%2520Zakir%2520Hossain%26entry.1292438233%3D%2520%2520In%2520response%2520to%2520the%2520global%2520need%2520for%2520efficient%2520early%2520diagnosis%2520of%2520Autism%250ASpectrum%2520Disorder%2520%2528ASD%2529%252C%2520this%2520paper%2520bridges%2520the%2520gap%2520between%2520traditional%252C%250Atime-consuming%2520diagnostic%2520methods%2520and%2520potential%2520automated%2520solutions.%2520We%2520propose%250Aa%2520multi-atlas%2520deep%2520ensemble%2520network%252C%2520MADE-for-ASD%252C%2520that%2520integrates%2520multiple%250Aatlases%2520of%2520the%2520brain%2527s%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529%2520data%250Athrough%2520a%2520weighted%2520deep%2520ensemble%2520network.%2520Our%2520approach%2520integrates%2520demographic%250Ainformation%2520into%2520the%2520prediction%2520workflow%252C%2520which%2520enhances%2520ASD%2520diagnosis%250Aperformance%2520and%2520offers%2520a%2520more%2520holistic%2520perspective%2520on%2520patient%2520profiling.%2520We%250Aexperiment%2520with%2520the%2520well-known%2520publicly%2520available%2520ABIDE%2520%2528Autism%2520Brain%2520Imaging%250AData%2520Exchange%2529%2520I%2520dataset%252C%2520consisting%2520of%2520resting%2520state%2520fMRI%2520data%2520from%252017%250Adifferent%2520laboratories%2520around%2520the%2520globe.%2520Our%2520proposed%2520system%2520achieves%252075.20%2525%250Aaccuracy%2520on%2520the%2520entire%2520dataset%2520and%252096.40%2525%2520on%2520a%2520specific%2520subset%2520%2524-%2524%2520both%250Asurpassing%2520reported%2520ASD%2520diagnosis%2520accuracy%2520in%2520ABIDE%2520I%2520fMRI%2520studies.%250ASpecifically%252C%2520our%2520model%2520improves%2520by%25204.4%2520percentage%2520points%2520over%2520prior%2520works%2520on%250Athe%2520same%2520amount%2520of%2520data.%2520The%2520model%2520exhibits%2520a%2520sensitivity%2520of%252082.90%2525%2520and%2520a%250Aspecificity%2520of%252069.70%2525%2520on%2520the%2520entire%2520dataset%252C%2520and%252091.00%2525%2520and%252099.50%2525%252C%250Arespectively%252C%2520on%2520the%2520specific%2520subset.%2520We%2520leverage%2520the%2520F-score%2520to%2520pinpoint%2520the%250Atop%252010%2520ROI%2520in%2520ASD%2520diagnosis%252C%2520such%2520as%2520%255Cemph%257Bprecuneus%257D%2520and%2520anterior%250A%255Cemph%257Bcingulate/ventromedial%257D.%2520The%2520proposed%2520system%2520can%2520potentially%2520pave%2520the%2520way%250Afor%2520more%2520cost-effective%252C%2520efficient%2520and%2520scalable%2520strategies%2520in%2520ASD%2520diagnosis.%250ACodes%2520and%2520evaluations%2520are%2520publicly%2520available%2520at%2520TBA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MADE-for-ASD%3A%20A%20Multi-Atlas%20Deep%20Ensemble%20Network%20for%20Diagnosing%20Autism%0A%20%20Spectrum%20Disorder&entry.906535625=Md%20Rakibul%20Hasan%20and%20Xuehan%20Liu%20and%20Tom%20Gedeon%20and%20Md%20Zakir%20Hossain&entry.1292438233=%20%20In%20response%20to%20the%20global%20need%20for%20efficient%20early%20diagnosis%20of%20Autism%0ASpectrum%20Disorder%20%28ASD%29%2C%20this%20paper%20bridges%20the%20gap%20between%20traditional%2C%0Atime-consuming%20diagnostic%20methods%20and%20potential%20automated%20solutions.%20We%20propose%0Aa%20multi-atlas%20deep%20ensemble%20network%2C%20MADE-for-ASD%2C%20that%20integrates%20multiple%0Aatlases%20of%20the%20brain%27s%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20data%0Athrough%20a%20weighted%20deep%20ensemble%20network.%20Our%20approach%20integrates%20demographic%0Ainformation%20into%20the%20prediction%20workflow%2C%20which%20enhances%20ASD%20diagnosis%0Aperformance%20and%20offers%20a%20more%20holistic%20perspective%20on%20patient%20profiling.%20We%0Aexperiment%20with%20the%20well-known%20publicly%20available%20ABIDE%20%28Autism%20Brain%20Imaging%0AData%20Exchange%29%20I%20dataset%2C%20consisting%20of%20resting%20state%20fMRI%20data%20from%2017%0Adifferent%20laboratories%20around%20the%20globe.%20Our%20proposed%20system%20achieves%2075.20%25%0Aaccuracy%20on%20the%20entire%20dataset%20and%2096.40%25%20on%20a%20specific%20subset%20%24-%24%20both%0Asurpassing%20reported%20ASD%20diagnosis%20accuracy%20in%20ABIDE%20I%20fMRI%20studies.%0ASpecifically%2C%20our%20model%20improves%20by%204.4%20percentage%20points%20over%20prior%20works%20on%0Athe%20same%20amount%20of%20data.%20The%20model%20exhibits%20a%20sensitivity%20of%2082.90%25%20and%20a%0Aspecificity%20of%2069.70%25%20on%20the%20entire%20dataset%2C%20and%2091.00%25%20and%2099.50%25%2C%0Arespectively%2C%20on%20the%20specific%20subset.%20We%20leverage%20the%20F-score%20to%20pinpoint%20the%0Atop%2010%20ROI%20in%20ASD%20diagnosis%2C%20such%20as%20%5Cemph%7Bprecuneus%7D%20and%20anterior%0A%5Cemph%7Bcingulate/ventromedial%7D.%20The%20proposed%20system%20can%20potentially%20pave%20the%20way%0Afor%20more%20cost-effective%2C%20efficient%20and%20scalable%20strategies%20in%20ASD%20diagnosis.%0ACodes%20and%20evaluations%20are%20publicly%20available%20at%20TBA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07076v1&entry.124074799=Read"},
{"title": "Proceedings of The second international workshop on eXplainable AI for\n  the Arts (XAIxArts)", "author": "Nick Bryan-Kinns and Corey Ford and Shuoyang Zheng and Helen Kennedy and Alan Chamberlain and Makayla Lewis and Drew Hemment and Zijin Li and Qiong Wu and Lanxi Xiao and Gus Xia and Jeba Rezwana and Michael Clemens and Gabriel Vigliensoni", "abstract": "  This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.\n", "link": "http://arxiv.org/abs/2406.14485v4", "date": "2024-07-09", "relevancy": 1.4246, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4239}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3429}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.3424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proceedings%20of%20The%20second%20international%20workshop%20on%20eXplainable%20AI%20for%0A%20%20the%20Arts%20%28XAIxArts%29&body=Title%3A%20Proceedings%20of%20The%20second%20international%20workshop%20on%20eXplainable%20AI%20for%0A%20%20the%20Arts%20%28XAIxArts%29%0AAuthor%3A%20Nick%20Bryan-Kinns%20and%20Corey%20Ford%20and%20Shuoyang%20Zheng%20and%20Helen%20Kennedy%20and%20Alan%20Chamberlain%20and%20Makayla%20Lewis%20and%20Drew%20Hemment%20and%20Zijin%20Li%20and%20Qiong%20Wu%20and%20Lanxi%20Xiao%20and%20Gus%20Xia%20and%20Jeba%20Rezwana%20and%20Michael%20Clemens%20and%20Gabriel%20Vigliensoni%0AAbstract%3A%20%20%20This%20second%20international%20workshop%20on%20explainable%20AI%20for%20the%20Arts%20%28XAIxArts%29%0Abrought%20together%20a%20community%20of%20researchers%20in%20HCI%2C%20Interaction%20Design%2C%20AI%2C%0Aexplainable%20AI%20%28XAI%29%2C%20and%20digital%20arts%20to%20explore%20the%20role%20of%20XAI%20for%20the%20Arts.%0AWorkshop%20held%20at%20the%2016th%20ACM%20Conference%20on%20Creativity%20and%20Cognition%20%28C%26C%0A2024%29%2C%20Chicago%2C%20USA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14485v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProceedings%2520of%2520The%2520second%2520international%2520workshop%2520on%2520eXplainable%2520AI%2520for%250A%2520%2520the%2520Arts%2520%2528XAIxArts%2529%26entry.906535625%3DNick%2520Bryan-Kinns%2520and%2520Corey%2520Ford%2520and%2520Shuoyang%2520Zheng%2520and%2520Helen%2520Kennedy%2520and%2520Alan%2520Chamberlain%2520and%2520Makayla%2520Lewis%2520and%2520Drew%2520Hemment%2520and%2520Zijin%2520Li%2520and%2520Qiong%2520Wu%2520and%2520Lanxi%2520Xiao%2520and%2520Gus%2520Xia%2520and%2520Jeba%2520Rezwana%2520and%2520Michael%2520Clemens%2520and%2520Gabriel%2520Vigliensoni%26entry.1292438233%3D%2520%2520This%2520second%2520international%2520workshop%2520on%2520explainable%2520AI%2520for%2520the%2520Arts%2520%2528XAIxArts%2529%250Abrought%2520together%2520a%2520community%2520of%2520researchers%2520in%2520HCI%252C%2520Interaction%2520Design%252C%2520AI%252C%250Aexplainable%2520AI%2520%2528XAI%2529%252C%2520and%2520digital%2520arts%2520to%2520explore%2520the%2520role%2520of%2520XAI%2520for%2520the%2520Arts.%250AWorkshop%2520held%2520at%2520the%252016th%2520ACM%2520Conference%2520on%2520Creativity%2520and%2520Cognition%2520%2528C%2526C%250A2024%2529%252C%2520Chicago%252C%2520USA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14485v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proceedings%20of%20The%20second%20international%20workshop%20on%20eXplainable%20AI%20for%0A%20%20the%20Arts%20%28XAIxArts%29&entry.906535625=Nick%20Bryan-Kinns%20and%20Corey%20Ford%20and%20Shuoyang%20Zheng%20and%20Helen%20Kennedy%20and%20Alan%20Chamberlain%20and%20Makayla%20Lewis%20and%20Drew%20Hemment%20and%20Zijin%20Li%20and%20Qiong%20Wu%20and%20Lanxi%20Xiao%20and%20Gus%20Xia%20and%20Jeba%20Rezwana%20and%20Michael%20Clemens%20and%20Gabriel%20Vigliensoni&entry.1292438233=%20%20This%20second%20international%20workshop%20on%20explainable%20AI%20for%20the%20Arts%20%28XAIxArts%29%0Abrought%20together%20a%20community%20of%20researchers%20in%20HCI%2C%20Interaction%20Design%2C%20AI%2C%0Aexplainable%20AI%20%28XAI%29%2C%20and%20digital%20arts%20to%20explore%20the%20role%20of%20XAI%20for%20the%20Arts.%0AWorkshop%20held%20at%20the%2016th%20ACM%20Conference%20on%20Creativity%20and%20Cognition%20%28C%26C%0A2024%29%2C%20Chicago%2C%20USA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14485v4&entry.124074799=Read"},
{"title": "Trust and Resilience in Federated Learning Through Smart Contracts\n  Enabled Decentralized Systems", "author": "Lorenzo Cassano and Jacopo D'Abramo and Siraj Munir and Stefano Ferretti", "abstract": "  In this paper, we present a study of a Federated Learning (FL) system, based\non the use of decentralized architectures to ensure trust and increase\nreliability. The system is based on the idea that the FL collaborators upload\nthe (ciphered) model parameters on the Inter-Planetary File System (IPFS) and\ninteract with a dedicated smart contract to track their behavior. Thank to this\nsmart contract, the phases of parameter updates are managed efficiently,\nthereby strengthening data security. We have carried out an experimental study\nthat exploits two different methods of weight aggregation, i.e., a classic\naveraging scheme and a federated proximal aggregation. The results confirm the\nfeasibility of the proposal.\n", "link": "http://arxiv.org/abs/2407.06862v1", "date": "2024-07-09", "relevancy": 1.7618, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4671}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4433}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trust%20and%20Resilience%20in%20Federated%20Learning%20Through%20Smart%20Contracts%0A%20%20Enabled%20Decentralized%20Systems&body=Title%3A%20Trust%20and%20Resilience%20in%20Federated%20Learning%20Through%20Smart%20Contracts%0A%20%20Enabled%20Decentralized%20Systems%0AAuthor%3A%20Lorenzo%20Cassano%20and%20Jacopo%20D%27Abramo%20and%20Siraj%20Munir%20and%20Stefano%20Ferretti%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20study%20of%20a%20Federated%20Learning%20%28FL%29%20system%2C%20based%0Aon%20the%20use%20of%20decentralized%20architectures%20to%20ensure%20trust%20and%20increase%0Areliability.%20The%20system%20is%20based%20on%20the%20idea%20that%20the%20FL%20collaborators%20upload%0Athe%20%28ciphered%29%20model%20parameters%20on%20the%20Inter-Planetary%20File%20System%20%28IPFS%29%20and%0Ainteract%20with%20a%20dedicated%20smart%20contract%20to%20track%20their%20behavior.%20Thank%20to%20this%0Asmart%20contract%2C%20the%20phases%20of%20parameter%20updates%20are%20managed%20efficiently%2C%0Athereby%20strengthening%20data%20security.%20We%20have%20carried%20out%20an%20experimental%20study%0Athat%20exploits%20two%20different%20methods%20of%20weight%20aggregation%2C%20i.e.%2C%20a%20classic%0Aaveraging%20scheme%20and%20a%20federated%20proximal%20aggregation.%20The%20results%20confirm%20the%0Afeasibility%20of%20the%20proposal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrust%2520and%2520Resilience%2520in%2520Federated%2520Learning%2520Through%2520Smart%2520Contracts%250A%2520%2520Enabled%2520Decentralized%2520Systems%26entry.906535625%3DLorenzo%2520Cassano%2520and%2520Jacopo%2520D%2527Abramo%2520and%2520Siraj%2520Munir%2520and%2520Stefano%2520Ferretti%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520study%2520of%2520a%2520Federated%2520Learning%2520%2528FL%2529%2520system%252C%2520based%250Aon%2520the%2520use%2520of%2520decentralized%2520architectures%2520to%2520ensure%2520trust%2520and%2520increase%250Areliability.%2520The%2520system%2520is%2520based%2520on%2520the%2520idea%2520that%2520the%2520FL%2520collaborators%2520upload%250Athe%2520%2528ciphered%2529%2520model%2520parameters%2520on%2520the%2520Inter-Planetary%2520File%2520System%2520%2528IPFS%2529%2520and%250Ainteract%2520with%2520a%2520dedicated%2520smart%2520contract%2520to%2520track%2520their%2520behavior.%2520Thank%2520to%2520this%250Asmart%2520contract%252C%2520the%2520phases%2520of%2520parameter%2520updates%2520are%2520managed%2520efficiently%252C%250Athereby%2520strengthening%2520data%2520security.%2520We%2520have%2520carried%2520out%2520an%2520experimental%2520study%250Athat%2520exploits%2520two%2520different%2520methods%2520of%2520weight%2520aggregation%252C%2520i.e.%252C%2520a%2520classic%250Aaveraging%2520scheme%2520and%2520a%2520federated%2520proximal%2520aggregation.%2520The%2520results%2520confirm%2520the%250Afeasibility%2520of%2520the%2520proposal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trust%20and%20Resilience%20in%20Federated%20Learning%20Through%20Smart%20Contracts%0A%20%20Enabled%20Decentralized%20Systems&entry.906535625=Lorenzo%20Cassano%20and%20Jacopo%20D%27Abramo%20and%20Siraj%20Munir%20and%20Stefano%20Ferretti&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20study%20of%20a%20Federated%20Learning%20%28FL%29%20system%2C%20based%0Aon%20the%20use%20of%20decentralized%20architectures%20to%20ensure%20trust%20and%20increase%0Areliability.%20The%20system%20is%20based%20on%20the%20idea%20that%20the%20FL%20collaborators%20upload%0Athe%20%28ciphered%29%20model%20parameters%20on%20the%20Inter-Planetary%20File%20System%20%28IPFS%29%20and%0Ainteract%20with%20a%20dedicated%20smart%20contract%20to%20track%20their%20behavior.%20Thank%20to%20this%0Asmart%20contract%2C%20the%20phases%20of%20parameter%20updates%20are%20managed%20efficiently%2C%0Athereby%20strengthening%20data%20security.%20We%20have%20carried%20out%20an%20experimental%20study%0Athat%20exploits%20two%20different%20methods%20of%20weight%20aggregation%2C%20i.e.%2C%20a%20classic%0Aaveraging%20scheme%20and%20a%20federated%20proximal%20aggregation.%20The%20results%20confirm%20the%0Afeasibility%20of%20the%20proposal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06862v1&entry.124074799=Read"},
{"title": "Non-Asymptotic Performance of Social Machine Learning Under Limited Data", "author": "Ping Hu and Virginia Bordignon and Mert Kayaalp and Ali H. Sayed", "abstract": "  This paper studies the probability of error associated with the social\nmachine learning framework, which involves an independent training phase\nfollowed by a cooperative decision-making phase over a graph. This framework\naddresses the problem of classifying a stream of unlabeled data in a\ndistributed manner. In this work, we examine the classification task with\nlimited observations during the decision-making phase, which requires a\nnon-asymptotic performance analysis. We establish a condition for consistent\ntraining and derive an upper bound on the probability of error for\nclassification. The results clarify the dependence on the statistical\nproperties of the data and the combination policy used over the graph. They\nalso establish the exponential decay of the probability of error with respect\nto the number of unlabeled samples.\n", "link": "http://arxiv.org/abs/2306.09397v2", "date": "2024-07-09", "relevancy": 1.334, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4516}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4436}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Asymptotic%20Performance%20of%20Social%20Machine%20Learning%20Under%20Limited%20Data&body=Title%3A%20Non-Asymptotic%20Performance%20of%20Social%20Machine%20Learning%20Under%20Limited%20Data%0AAuthor%3A%20Ping%20Hu%20and%20Virginia%20Bordignon%20and%20Mert%20Kayaalp%20and%20Ali%20H.%20Sayed%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20probability%20of%20error%20associated%20with%20the%20social%0Amachine%20learning%20framework%2C%20which%20involves%20an%20independent%20training%20phase%0Afollowed%20by%20a%20cooperative%20decision-making%20phase%20over%20a%20graph.%20This%20framework%0Aaddresses%20the%20problem%20of%20classifying%20a%20stream%20of%20unlabeled%20data%20in%20a%0Adistributed%20manner.%20In%20this%20work%2C%20we%20examine%20the%20classification%20task%20with%0Alimited%20observations%20during%20the%20decision-making%20phase%2C%20which%20requires%20a%0Anon-asymptotic%20performance%20analysis.%20We%20establish%20a%20condition%20for%20consistent%0Atraining%20and%20derive%20an%20upper%20bound%20on%20the%20probability%20of%20error%20for%0Aclassification.%20The%20results%20clarify%20the%20dependence%20on%20the%20statistical%0Aproperties%20of%20the%20data%20and%20the%20combination%20policy%20used%20over%20the%20graph.%20They%0Aalso%20establish%20the%20exponential%20decay%20of%20the%20probability%20of%20error%20with%20respect%0Ato%20the%20number%20of%20unlabeled%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.09397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Asymptotic%2520Performance%2520of%2520Social%2520Machine%2520Learning%2520Under%2520Limited%2520Data%26entry.906535625%3DPing%2520Hu%2520and%2520Virginia%2520Bordignon%2520and%2520Mert%2520Kayaalp%2520and%2520Ali%2520H.%2520Sayed%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520probability%2520of%2520error%2520associated%2520with%2520the%2520social%250Amachine%2520learning%2520framework%252C%2520which%2520involves%2520an%2520independent%2520training%2520phase%250Afollowed%2520by%2520a%2520cooperative%2520decision-making%2520phase%2520over%2520a%2520graph.%2520This%2520framework%250Aaddresses%2520the%2520problem%2520of%2520classifying%2520a%2520stream%2520of%2520unlabeled%2520data%2520in%2520a%250Adistributed%2520manner.%2520In%2520this%2520work%252C%2520we%2520examine%2520the%2520classification%2520task%2520with%250Alimited%2520observations%2520during%2520the%2520decision-making%2520phase%252C%2520which%2520requires%2520a%250Anon-asymptotic%2520performance%2520analysis.%2520We%2520establish%2520a%2520condition%2520for%2520consistent%250Atraining%2520and%2520derive%2520an%2520upper%2520bound%2520on%2520the%2520probability%2520of%2520error%2520for%250Aclassification.%2520The%2520results%2520clarify%2520the%2520dependence%2520on%2520the%2520statistical%250Aproperties%2520of%2520the%2520data%2520and%2520the%2520combination%2520policy%2520used%2520over%2520the%2520graph.%2520They%250Aalso%2520establish%2520the%2520exponential%2520decay%2520of%2520the%2520probability%2520of%2520error%2520with%2520respect%250Ato%2520the%2520number%2520of%2520unlabeled%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.09397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Asymptotic%20Performance%20of%20Social%20Machine%20Learning%20Under%20Limited%20Data&entry.906535625=Ping%20Hu%20and%20Virginia%20Bordignon%20and%20Mert%20Kayaalp%20and%20Ali%20H.%20Sayed&entry.1292438233=%20%20This%20paper%20studies%20the%20probability%20of%20error%20associated%20with%20the%20social%0Amachine%20learning%20framework%2C%20which%20involves%20an%20independent%20training%20phase%0Afollowed%20by%20a%20cooperative%20decision-making%20phase%20over%20a%20graph.%20This%20framework%0Aaddresses%20the%20problem%20of%20classifying%20a%20stream%20of%20unlabeled%20data%20in%20a%0Adistributed%20manner.%20In%20this%20work%2C%20we%20examine%20the%20classification%20task%20with%0Alimited%20observations%20during%20the%20decision-making%20phase%2C%20which%20requires%20a%0Anon-asymptotic%20performance%20analysis.%20We%20establish%20a%20condition%20for%20consistent%0Atraining%20and%20derive%20an%20upper%20bound%20on%20the%20probability%20of%20error%20for%0Aclassification.%20The%20results%20clarify%20the%20dependence%20on%20the%20statistical%0Aproperties%20of%20the%20data%20and%20the%20combination%20policy%20used%20over%20the%20graph.%20They%0Aalso%20establish%20the%20exponential%20decay%20of%20the%20probability%20of%20error%20with%20respect%0Ato%20the%20number%20of%20unlabeled%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09397v2&entry.124074799=Read"},
{"title": "Countermeasures Against Adversarial Examples in Radio Signal\n  Classification", "author": "Lu Zhang and Sangarapillai Lambotharan and Gan Zheng and Basil AsSadhan and Fabio Roli", "abstract": "  Deep learning algorithms have been shown to be powerful in many communication\nnetwork design problems, including that in automatic modulation classification.\nHowever, they are vulnerable to carefully crafted attacks called adversarial\nexamples. Hence, the reliance of wireless networks on deep learning algorithms\nposes a serious threat to the security and operation of wireless networks. In\nthis letter, we propose for the first time a countermeasure against adversarial\nexamples in modulation classification. Our countermeasure is based on a neural\nrejection technique, augmented by label smoothing and Gaussian noise injection,\nthat allows to detect and reject adversarial examples with high accuracy. Our\nresults demonstrate that the proposed countermeasure can protect deep-learning\nbased modulation classification systems against adversarial examples.\n", "link": "http://arxiv.org/abs/2407.06796v1", "date": "2024-07-09", "relevancy": 1.7686, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4697}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4231}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Countermeasures%20Against%20Adversarial%20Examples%20in%20Radio%20Signal%0A%20%20Classification&body=Title%3A%20Countermeasures%20Against%20Adversarial%20Examples%20in%20Radio%20Signal%0A%20%20Classification%0AAuthor%3A%20Lu%20Zhang%20and%20Sangarapillai%20Lambotharan%20and%20Gan%20Zheng%20and%20Basil%20AsSadhan%20and%20Fabio%20Roli%0AAbstract%3A%20%20%20Deep%20learning%20algorithms%20have%20been%20shown%20to%20be%20powerful%20in%20many%20communication%0Anetwork%20design%20problems%2C%20including%20that%20in%20automatic%20modulation%20classification.%0AHowever%2C%20they%20are%20vulnerable%20to%20carefully%20crafted%20attacks%20called%20adversarial%0Aexamples.%20Hence%2C%20the%20reliance%20of%20wireless%20networks%20on%20deep%20learning%20algorithms%0Aposes%20a%20serious%20threat%20to%20the%20security%20and%20operation%20of%20wireless%20networks.%20In%0Athis%20letter%2C%20we%20propose%20for%20the%20first%20time%20a%20countermeasure%20against%20adversarial%0Aexamples%20in%20modulation%20classification.%20Our%20countermeasure%20is%20based%20on%20a%20neural%0Arejection%20technique%2C%20augmented%20by%20label%20smoothing%20and%20Gaussian%20noise%20injection%2C%0Athat%20allows%20to%20detect%20and%20reject%20adversarial%20examples%20with%20high%20accuracy.%20Our%0Aresults%20demonstrate%20that%20the%20proposed%20countermeasure%20can%20protect%20deep-learning%0Abased%20modulation%20classification%20systems%20against%20adversarial%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCountermeasures%2520Against%2520Adversarial%2520Examples%2520in%2520Radio%2520Signal%250A%2520%2520Classification%26entry.906535625%3DLu%2520Zhang%2520and%2520Sangarapillai%2520Lambotharan%2520and%2520Gan%2520Zheng%2520and%2520Basil%2520AsSadhan%2520and%2520Fabio%2520Roli%26entry.1292438233%3D%2520%2520Deep%2520learning%2520algorithms%2520have%2520been%2520shown%2520to%2520be%2520powerful%2520in%2520many%2520communication%250Anetwork%2520design%2520problems%252C%2520including%2520that%2520in%2520automatic%2520modulation%2520classification.%250AHowever%252C%2520they%2520are%2520vulnerable%2520to%2520carefully%2520crafted%2520attacks%2520called%2520adversarial%250Aexamples.%2520Hence%252C%2520the%2520reliance%2520of%2520wireless%2520networks%2520on%2520deep%2520learning%2520algorithms%250Aposes%2520a%2520serious%2520threat%2520to%2520the%2520security%2520and%2520operation%2520of%2520wireless%2520networks.%2520In%250Athis%2520letter%252C%2520we%2520propose%2520for%2520the%2520first%2520time%2520a%2520countermeasure%2520against%2520adversarial%250Aexamples%2520in%2520modulation%2520classification.%2520Our%2520countermeasure%2520is%2520based%2520on%2520a%2520neural%250Arejection%2520technique%252C%2520augmented%2520by%2520label%2520smoothing%2520and%2520Gaussian%2520noise%2520injection%252C%250Athat%2520allows%2520to%2520detect%2520and%2520reject%2520adversarial%2520examples%2520with%2520high%2520accuracy.%2520Our%250Aresults%2520demonstrate%2520that%2520the%2520proposed%2520countermeasure%2520can%2520protect%2520deep-learning%250Abased%2520modulation%2520classification%2520systems%2520against%2520adversarial%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Countermeasures%20Against%20Adversarial%20Examples%20in%20Radio%20Signal%0A%20%20Classification&entry.906535625=Lu%20Zhang%20and%20Sangarapillai%20Lambotharan%20and%20Gan%20Zheng%20and%20Basil%20AsSadhan%20and%20Fabio%20Roli&entry.1292438233=%20%20Deep%20learning%20algorithms%20have%20been%20shown%20to%20be%20powerful%20in%20many%20communication%0Anetwork%20design%20problems%2C%20including%20that%20in%20automatic%20modulation%20classification.%0AHowever%2C%20they%20are%20vulnerable%20to%20carefully%20crafted%20attacks%20called%20adversarial%0Aexamples.%20Hence%2C%20the%20reliance%20of%20wireless%20networks%20on%20deep%20learning%20algorithms%0Aposes%20a%20serious%20threat%20to%20the%20security%20and%20operation%20of%20wireless%20networks.%20In%0Athis%20letter%2C%20we%20propose%20for%20the%20first%20time%20a%20countermeasure%20against%20adversarial%0Aexamples%20in%20modulation%20classification.%20Our%20countermeasure%20is%20based%20on%20a%20neural%0Arejection%20technique%2C%20augmented%20by%20label%20smoothing%20and%20Gaussian%20noise%20injection%2C%0Athat%20allows%20to%20detect%20and%20reject%20adversarial%20examples%20with%20high%20accuracy.%20Our%0Aresults%20demonstrate%20that%20the%20proposed%20countermeasure%20can%20protect%20deep-learning%0Abased%20modulation%20classification%20systems%20against%20adversarial%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06796v1&entry.124074799=Read"},
{"title": "EventChat: Implementation and user-centric evaluation of a large\n  language model-driven conversational recommender system for exploring leisure\n  events in an SME context", "author": "Hannes Kunstmann and Joseph Ollier and Joel Persson and Florian von Wangenheim", "abstract": "  Large language models (LLMs) present an enormous evolution in the strategic\npotential of conversational recommender systems (CRS). Yet to date, research\nhas predominantly focused upon technical frameworks to implement LLM-driven\nCRS, rather than end-user evaluations or strategic implications for firms,\nparticularly from the perspective of a small to medium enterprises (SME) that\nmakeup the bedrock of the global economy. In the current paper, we detail the\ndesign of an LLM-driven CRS in an SME setting, and its subsequent performance\nin the field using both objective system metrics and subjective user\nevaluations. While doing so, we additionally outline a short-form revised\nResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly\nevolving field. Our results reveal good system performance from a user\nexperience perspective (85.5% recommendation accuracy) but underscore latency,\ncost, and quality issues challenging business viability. Notably, with a median\ncost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and\nresponse time emerge as crucial areas for achieving a more user-friendly and\neconomically viable LLM-driven CRS for SME settings. One major driver of these\ncosts is the use of an advanced LLM as a ranker within the retrieval-augmented\ngeneration (RAG) technique. Our results additionally indicate that relying\nsolely on approaches such as Prompt-based learning with ChatGPT as the\nunderlying LLM makes it challenging to achieve satisfying quality in a\nproduction environment. Strategic considerations for SMEs deploying an\nLLM-driven CRS are outlined, particularly considering trade-offs in the current\ntechnical landscape.\n", "link": "http://arxiv.org/abs/2407.04472v3", "date": "2024-07-09", "relevancy": 1.2804, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4834}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4247}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EventChat%3A%20Implementation%20and%20user-centric%20evaluation%20of%20a%20large%0A%20%20language%20model-driven%20conversational%20recommender%20system%20for%20exploring%20leisure%0A%20%20events%20in%20an%20SME%20context&body=Title%3A%20EventChat%3A%20Implementation%20and%20user-centric%20evaluation%20of%20a%20large%0A%20%20language%20model-driven%20conversational%20recommender%20system%20for%20exploring%20leisure%0A%20%20events%20in%20an%20SME%20context%0AAuthor%3A%20Hannes%20Kunstmann%20and%20Joseph%20Ollier%20and%20Joel%20Persson%20and%20Florian%20von%20Wangenheim%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20present%20an%20enormous%20evolution%20in%20the%20strategic%0Apotential%20of%20conversational%20recommender%20systems%20%28CRS%29.%20Yet%20to%20date%2C%20research%0Ahas%20predominantly%20focused%20upon%20technical%20frameworks%20to%20implement%20LLM-driven%0ACRS%2C%20rather%20than%20end-user%20evaluations%20or%20strategic%20implications%20for%20firms%2C%0Aparticularly%20from%20the%20perspective%20of%20a%20small%20to%20medium%20enterprises%20%28SME%29%20that%0Amakeup%20the%20bedrock%20of%20the%20global%20economy.%20In%20the%20current%20paper%2C%20we%20detail%20the%0Adesign%20of%20an%20LLM-driven%20CRS%20in%20an%20SME%20setting%2C%20and%20its%20subsequent%20performance%0Ain%20the%20field%20using%20both%20objective%20system%20metrics%20and%20subjective%20user%0Aevaluations.%20While%20doing%20so%2C%20we%20additionally%20outline%20a%20short-form%20revised%0AResQue%20model%20for%20evaluating%20LLM-driven%20CRS%2C%20enabling%20replicability%20in%20a%20rapidly%0Aevolving%20field.%20Our%20results%20reveal%20good%20system%20performance%20from%20a%20user%0Aexperience%20perspective%20%2885.5%25%20recommendation%20accuracy%29%20but%20underscore%20latency%2C%0Acost%2C%20and%20quality%20issues%20challenging%20business%20viability.%20Notably%2C%20with%20a%20median%0Acost%20of%20%240.04%20per%20interaction%20and%20a%20latency%20of%205.7s%2C%20cost-effectiveness%20and%0Aresponse%20time%20emerge%20as%20crucial%20areas%20for%20achieving%20a%20more%20user-friendly%20and%0Aeconomically%20viable%20LLM-driven%20CRS%20for%20SME%20settings.%20One%20major%20driver%20of%20these%0Acosts%20is%20the%20use%20of%20an%20advanced%20LLM%20as%20a%20ranker%20within%20the%20retrieval-augmented%0Ageneration%20%28RAG%29%20technique.%20Our%20results%20additionally%20indicate%20that%20relying%0Asolely%20on%20approaches%20such%20as%20Prompt-based%20learning%20with%20ChatGPT%20as%20the%0Aunderlying%20LLM%20makes%20it%20challenging%20to%20achieve%20satisfying%20quality%20in%20a%0Aproduction%20environment.%20Strategic%20considerations%20for%20SMEs%20deploying%20an%0ALLM-driven%20CRS%20are%20outlined%2C%20particularly%20considering%20trade-offs%20in%20the%20current%0Atechnical%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04472v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEventChat%253A%2520Implementation%2520and%2520user-centric%2520evaluation%2520of%2520a%2520large%250A%2520%2520language%2520model-driven%2520conversational%2520recommender%2520system%2520for%2520exploring%2520leisure%250A%2520%2520events%2520in%2520an%2520SME%2520context%26entry.906535625%3DHannes%2520Kunstmann%2520and%2520Joseph%2520Ollier%2520and%2520Joel%2520Persson%2520and%2520Florian%2520von%2520Wangenheim%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520present%2520an%2520enormous%2520evolution%2520in%2520the%2520strategic%250Apotential%2520of%2520conversational%2520recommender%2520systems%2520%2528CRS%2529.%2520Yet%2520to%2520date%252C%2520research%250Ahas%2520predominantly%2520focused%2520upon%2520technical%2520frameworks%2520to%2520implement%2520LLM-driven%250ACRS%252C%2520rather%2520than%2520end-user%2520evaluations%2520or%2520strategic%2520implications%2520for%2520firms%252C%250Aparticularly%2520from%2520the%2520perspective%2520of%2520a%2520small%2520to%2520medium%2520enterprises%2520%2528SME%2529%2520that%250Amakeup%2520the%2520bedrock%2520of%2520the%2520global%2520economy.%2520In%2520the%2520current%2520paper%252C%2520we%2520detail%2520the%250Adesign%2520of%2520an%2520LLM-driven%2520CRS%2520in%2520an%2520SME%2520setting%252C%2520and%2520its%2520subsequent%2520performance%250Ain%2520the%2520field%2520using%2520both%2520objective%2520system%2520metrics%2520and%2520subjective%2520user%250Aevaluations.%2520While%2520doing%2520so%252C%2520we%2520additionally%2520outline%2520a%2520short-form%2520revised%250AResQue%2520model%2520for%2520evaluating%2520LLM-driven%2520CRS%252C%2520enabling%2520replicability%2520in%2520a%2520rapidly%250Aevolving%2520field.%2520Our%2520results%2520reveal%2520good%2520system%2520performance%2520from%2520a%2520user%250Aexperience%2520perspective%2520%252885.5%2525%2520recommendation%2520accuracy%2529%2520but%2520underscore%2520latency%252C%250Acost%252C%2520and%2520quality%2520issues%2520challenging%2520business%2520viability.%2520Notably%252C%2520with%2520a%2520median%250Acost%2520of%2520%25240.04%2520per%2520interaction%2520and%2520a%2520latency%2520of%25205.7s%252C%2520cost-effectiveness%2520and%250Aresponse%2520time%2520emerge%2520as%2520crucial%2520areas%2520for%2520achieving%2520a%2520more%2520user-friendly%2520and%250Aeconomically%2520viable%2520LLM-driven%2520CRS%2520for%2520SME%2520settings.%2520One%2520major%2520driver%2520of%2520these%250Acosts%2520is%2520the%2520use%2520of%2520an%2520advanced%2520LLM%2520as%2520a%2520ranker%2520within%2520the%2520retrieval-augmented%250Ageneration%2520%2528RAG%2529%2520technique.%2520Our%2520results%2520additionally%2520indicate%2520that%2520relying%250Asolely%2520on%2520approaches%2520such%2520as%2520Prompt-based%2520learning%2520with%2520ChatGPT%2520as%2520the%250Aunderlying%2520LLM%2520makes%2520it%2520challenging%2520to%2520achieve%2520satisfying%2520quality%2520in%2520a%250Aproduction%2520environment.%2520Strategic%2520considerations%2520for%2520SMEs%2520deploying%2520an%250ALLM-driven%2520CRS%2520are%2520outlined%252C%2520particularly%2520considering%2520trade-offs%2520in%2520the%2520current%250Atechnical%2520landscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04472v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventChat%3A%20Implementation%20and%20user-centric%20evaluation%20of%20a%20large%0A%20%20language%20model-driven%20conversational%20recommender%20system%20for%20exploring%20leisure%0A%20%20events%20in%20an%20SME%20context&entry.906535625=Hannes%20Kunstmann%20and%20Joseph%20Ollier%20and%20Joel%20Persson%20and%20Florian%20von%20Wangenheim&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20present%20an%20enormous%20evolution%20in%20the%20strategic%0Apotential%20of%20conversational%20recommender%20systems%20%28CRS%29.%20Yet%20to%20date%2C%20research%0Ahas%20predominantly%20focused%20upon%20technical%20frameworks%20to%20implement%20LLM-driven%0ACRS%2C%20rather%20than%20end-user%20evaluations%20or%20strategic%20implications%20for%20firms%2C%0Aparticularly%20from%20the%20perspective%20of%20a%20small%20to%20medium%20enterprises%20%28SME%29%20that%0Amakeup%20the%20bedrock%20of%20the%20global%20economy.%20In%20the%20current%20paper%2C%20we%20detail%20the%0Adesign%20of%20an%20LLM-driven%20CRS%20in%20an%20SME%20setting%2C%20and%20its%20subsequent%20performance%0Ain%20the%20field%20using%20both%20objective%20system%20metrics%20and%20subjective%20user%0Aevaluations.%20While%20doing%20so%2C%20we%20additionally%20outline%20a%20short-form%20revised%0AResQue%20model%20for%20evaluating%20LLM-driven%20CRS%2C%20enabling%20replicability%20in%20a%20rapidly%0Aevolving%20field.%20Our%20results%20reveal%20good%20system%20performance%20from%20a%20user%0Aexperience%20perspective%20%2885.5%25%20recommendation%20accuracy%29%20but%20underscore%20latency%2C%0Acost%2C%20and%20quality%20issues%20challenging%20business%20viability.%20Notably%2C%20with%20a%20median%0Acost%20of%20%240.04%20per%20interaction%20and%20a%20latency%20of%205.7s%2C%20cost-effectiveness%20and%0Aresponse%20time%20emerge%20as%20crucial%20areas%20for%20achieving%20a%20more%20user-friendly%20and%0Aeconomically%20viable%20LLM-driven%20CRS%20for%20SME%20settings.%20One%20major%20driver%20of%20these%0Acosts%20is%20the%20use%20of%20an%20advanced%20LLM%20as%20a%20ranker%20within%20the%20retrieval-augmented%0Ageneration%20%28RAG%29%20technique.%20Our%20results%20additionally%20indicate%20that%20relying%0Asolely%20on%20approaches%20such%20as%20Prompt-based%20learning%20with%20ChatGPT%20as%20the%0Aunderlying%20LLM%20makes%20it%20challenging%20to%20achieve%20satisfying%20quality%20in%20a%0Aproduction%20environment.%20Strategic%20considerations%20for%20SMEs%20deploying%20an%0ALLM-driven%20CRS%20are%20outlined%2C%20particularly%20considering%20trade-offs%20in%20the%20current%0Atechnical%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04472v3&entry.124074799=Read"},
{"title": "TeVAE: A Variational Autoencoder Approach for Discrete Online Anomaly\n  Detection in Variable-state Multivariate Time-series Data", "author": "Lucas Correia and Jan-Christoph Goos and Philipp Klein and Thomas B\u00e4ck and Anna V. Kononova", "abstract": "  As attention to recorded data grows in the realm of automotive testing and\nmanual evaluation reaches its limits, there is a growing need for automatic\nonline anomaly detection. This real-world data is complex in many ways and\nrequires the modelling of testee behaviour. To address this, we propose a\ntemporal variational autoencoder (TeVAE) that can detect anomalies with minimal\nfalse positives when trained on unlabelled data. Our approach also avoids the\nbypass phenomenon and introduces a new method to remap individual windows to a\ncontinuous time series. Furthermore, we propose metrics to evaluate the\ndetection delay and root-cause capability of our approach and present results\nfrom experiments on a real-world industrial data set. When properly configured,\nTeVAE flags anomalies only 6% of the time wrongly and detects 65% of anomalies\npresent. It also has the potential to perform well with a smaller training and\nvalidation subset but requires a more sophisticated threshold estimation\nmethod.\n", "link": "http://arxiv.org/abs/2407.06849v1", "date": "2024-07-09", "relevancy": 1.5205, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5182}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5066}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeVAE%3A%20A%20Variational%20Autoencoder%20Approach%20for%20Discrete%20Online%20Anomaly%0A%20%20Detection%20in%20Variable-state%20Multivariate%20Time-series%20Data&body=Title%3A%20TeVAE%3A%20A%20Variational%20Autoencoder%20Approach%20for%20Discrete%20Online%20Anomaly%0A%20%20Detection%20in%20Variable-state%20Multivariate%20Time-series%20Data%0AAuthor%3A%20Lucas%20Correia%20and%20Jan-Christoph%20Goos%20and%20Philipp%20Klein%20and%20Thomas%20B%C3%A4ck%20and%20Anna%20V.%20Kononova%0AAbstract%3A%20%20%20As%20attention%20to%20recorded%20data%20grows%20in%20the%20realm%20of%20automotive%20testing%20and%0Amanual%20evaluation%20reaches%20its%20limits%2C%20there%20is%20a%20growing%20need%20for%20automatic%0Aonline%20anomaly%20detection.%20This%20real-world%20data%20is%20complex%20in%20many%20ways%20and%0Arequires%20the%20modelling%20of%20testee%20behaviour.%20To%20address%20this%2C%20we%20propose%20a%0Atemporal%20variational%20autoencoder%20%28TeVAE%29%20that%20can%20detect%20anomalies%20with%20minimal%0Afalse%20positives%20when%20trained%20on%20unlabelled%20data.%20Our%20approach%20also%20avoids%20the%0Abypass%20phenomenon%20and%20introduces%20a%20new%20method%20to%20remap%20individual%20windows%20to%20a%0Acontinuous%20time%20series.%20Furthermore%2C%20we%20propose%20metrics%20to%20evaluate%20the%0Adetection%20delay%20and%20root-cause%20capability%20of%20our%20approach%20and%20present%20results%0Afrom%20experiments%20on%20a%20real-world%20industrial%20data%20set.%20When%20properly%20configured%2C%0ATeVAE%20flags%20anomalies%20only%206%25%20of%20the%20time%20wrongly%20and%20detects%2065%25%20of%20anomalies%0Apresent.%20It%20also%20has%20the%20potential%20to%20perform%20well%20with%20a%20smaller%20training%20and%0Avalidation%20subset%20but%20requires%20a%20more%20sophisticated%20threshold%20estimation%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeVAE%253A%2520A%2520Variational%2520Autoencoder%2520Approach%2520for%2520Discrete%2520Online%2520Anomaly%250A%2520%2520Detection%2520in%2520Variable-state%2520Multivariate%2520Time-series%2520Data%26entry.906535625%3DLucas%2520Correia%2520and%2520Jan-Christoph%2520Goos%2520and%2520Philipp%2520Klein%2520and%2520Thomas%2520B%25C3%25A4ck%2520and%2520Anna%2520V.%2520Kononova%26entry.1292438233%3D%2520%2520As%2520attention%2520to%2520recorded%2520data%2520grows%2520in%2520the%2520realm%2520of%2520automotive%2520testing%2520and%250Amanual%2520evaluation%2520reaches%2520its%2520limits%252C%2520there%2520is%2520a%2520growing%2520need%2520for%2520automatic%250Aonline%2520anomaly%2520detection.%2520This%2520real-world%2520data%2520is%2520complex%2520in%2520many%2520ways%2520and%250Arequires%2520the%2520modelling%2520of%2520testee%2520behaviour.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Atemporal%2520variational%2520autoencoder%2520%2528TeVAE%2529%2520that%2520can%2520detect%2520anomalies%2520with%2520minimal%250Afalse%2520positives%2520when%2520trained%2520on%2520unlabelled%2520data.%2520Our%2520approach%2520also%2520avoids%2520the%250Abypass%2520phenomenon%2520and%2520introduces%2520a%2520new%2520method%2520to%2520remap%2520individual%2520windows%2520to%2520a%250Acontinuous%2520time%2520series.%2520Furthermore%252C%2520we%2520propose%2520metrics%2520to%2520evaluate%2520the%250Adetection%2520delay%2520and%2520root-cause%2520capability%2520of%2520our%2520approach%2520and%2520present%2520results%250Afrom%2520experiments%2520on%2520a%2520real-world%2520industrial%2520data%2520set.%2520When%2520properly%2520configured%252C%250ATeVAE%2520flags%2520anomalies%2520only%25206%2525%2520of%2520the%2520time%2520wrongly%2520and%2520detects%252065%2525%2520of%2520anomalies%250Apresent.%2520It%2520also%2520has%2520the%2520potential%2520to%2520perform%2520well%2520with%2520a%2520smaller%2520training%2520and%250Avalidation%2520subset%2520but%2520requires%2520a%2520more%2520sophisticated%2520threshold%2520estimation%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeVAE%3A%20A%20Variational%20Autoencoder%20Approach%20for%20Discrete%20Online%20Anomaly%0A%20%20Detection%20in%20Variable-state%20Multivariate%20Time-series%20Data&entry.906535625=Lucas%20Correia%20and%20Jan-Christoph%20Goos%20and%20Philipp%20Klein%20and%20Thomas%20B%C3%A4ck%20and%20Anna%20V.%20Kononova&entry.1292438233=%20%20As%20attention%20to%20recorded%20data%20grows%20in%20the%20realm%20of%20automotive%20testing%20and%0Amanual%20evaluation%20reaches%20its%20limits%2C%20there%20is%20a%20growing%20need%20for%20automatic%0Aonline%20anomaly%20detection.%20This%20real-world%20data%20is%20complex%20in%20many%20ways%20and%0Arequires%20the%20modelling%20of%20testee%20behaviour.%20To%20address%20this%2C%20we%20propose%20a%0Atemporal%20variational%20autoencoder%20%28TeVAE%29%20that%20can%20detect%20anomalies%20with%20minimal%0Afalse%20positives%20when%20trained%20on%20unlabelled%20data.%20Our%20approach%20also%20avoids%20the%0Abypass%20phenomenon%20and%20introduces%20a%20new%20method%20to%20remap%20individual%20windows%20to%20a%0Acontinuous%20time%20series.%20Furthermore%2C%20we%20propose%20metrics%20to%20evaluate%20the%0Adetection%20delay%20and%20root-cause%20capability%20of%20our%20approach%20and%20present%20results%0Afrom%20experiments%20on%20a%20real-world%20industrial%20data%20set.%20When%20properly%20configured%2C%0ATeVAE%20flags%20anomalies%20only%206%25%20of%20the%20time%20wrongly%20and%20detects%2065%25%20of%20anomalies%0Apresent.%20It%20also%20has%20the%20potential%20to%20perform%20well%20with%20a%20smaller%20training%20and%0Avalidation%20subset%20but%20requires%20a%20more%20sophisticated%20threshold%20estimation%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06849v1&entry.124074799=Read"},
{"title": "Mining Potentially Explanatory Patterns via Partial Solutions", "author": "GianCarlo Catalano and Alexander E. I. Brownlee and David Cairns and John McCall and Russell Ainslie", "abstract": "  Genetic Algorithms have established their capability for solving many complex\noptimization problems. Even as good solutions are produced, the user's\nunderstanding of a problem is not necessarily improved, which can lead to a\nlack of confidence in the results. To mitigate this issue, explainability aims\nto give insight to the user by presenting them with the knowledge obtained by\nthe algorithm. In this paper we introduce Partial Solutions in order to improve\nthe explainability of solutions to combinatorial optimization problems. Partial\nSolutions represent beneficial traits found by analyzing a population, and are\npresented to the user for explainability, but also provide an explicit model\nfrom which new solutions can be generated. We present an algorithm that\nassembles a collection of Partial Solutions chosen to strike a balance between\nhigh fitness, simplicity and atomicity. Experiments with standard benchmarks\nshow that the proposed algorithm is able to find Partial Solutions which\nimprove explainability at reasonable computational cost without affecting\nsearch performance.\n", "link": "http://arxiv.org/abs/2404.04388v2", "date": "2024-07-09", "relevancy": 1.6549, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.443}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4121}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mining%20Potentially%20Explanatory%20Patterns%20via%20Partial%20Solutions&body=Title%3A%20Mining%20Potentially%20Explanatory%20Patterns%20via%20Partial%20Solutions%0AAuthor%3A%20GianCarlo%20Catalano%20and%20Alexander%20E.%20I.%20Brownlee%20and%20David%20Cairns%20and%20John%20McCall%20and%20Russell%20Ainslie%0AAbstract%3A%20%20%20Genetic%20Algorithms%20have%20established%20their%20capability%20for%20solving%20many%20complex%0Aoptimization%20problems.%20Even%20as%20good%20solutions%20are%20produced%2C%20the%20user%27s%0Aunderstanding%20of%20a%20problem%20is%20not%20necessarily%20improved%2C%20which%20can%20lead%20to%20a%0Alack%20of%20confidence%20in%20the%20results.%20To%20mitigate%20this%20issue%2C%20explainability%20aims%0Ato%20give%20insight%20to%20the%20user%20by%20presenting%20them%20with%20the%20knowledge%20obtained%20by%0Athe%20algorithm.%20In%20this%20paper%20we%20introduce%20Partial%20Solutions%20in%20order%20to%20improve%0Athe%20explainability%20of%20solutions%20to%20combinatorial%20optimization%20problems.%20Partial%0ASolutions%20represent%20beneficial%20traits%20found%20by%20analyzing%20a%20population%2C%20and%20are%0Apresented%20to%20the%20user%20for%20explainability%2C%20but%20also%20provide%20an%20explicit%20model%0Afrom%20which%20new%20solutions%20can%20be%20generated.%20We%20present%20an%20algorithm%20that%0Aassembles%20a%20collection%20of%20Partial%20Solutions%20chosen%20to%20strike%20a%20balance%20between%0Ahigh%20fitness%2C%20simplicity%20and%20atomicity.%20Experiments%20with%20standard%20benchmarks%0Ashow%20that%20the%20proposed%20algorithm%20is%20able%20to%20find%20Partial%20Solutions%20which%0Aimprove%20explainability%20at%20reasonable%20computational%20cost%20without%20affecting%0Asearch%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04388v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMining%2520Potentially%2520Explanatory%2520Patterns%2520via%2520Partial%2520Solutions%26entry.906535625%3DGianCarlo%2520Catalano%2520and%2520Alexander%2520E.%2520I.%2520Brownlee%2520and%2520David%2520Cairns%2520and%2520John%2520McCall%2520and%2520Russell%2520Ainslie%26entry.1292438233%3D%2520%2520Genetic%2520Algorithms%2520have%2520established%2520their%2520capability%2520for%2520solving%2520many%2520complex%250Aoptimization%2520problems.%2520Even%2520as%2520good%2520solutions%2520are%2520produced%252C%2520the%2520user%2527s%250Aunderstanding%2520of%2520a%2520problem%2520is%2520not%2520necessarily%2520improved%252C%2520which%2520can%2520lead%2520to%2520a%250Alack%2520of%2520confidence%2520in%2520the%2520results.%2520To%2520mitigate%2520this%2520issue%252C%2520explainability%2520aims%250Ato%2520give%2520insight%2520to%2520the%2520user%2520by%2520presenting%2520them%2520with%2520the%2520knowledge%2520obtained%2520by%250Athe%2520algorithm.%2520In%2520this%2520paper%2520we%2520introduce%2520Partial%2520Solutions%2520in%2520order%2520to%2520improve%250Athe%2520explainability%2520of%2520solutions%2520to%2520combinatorial%2520optimization%2520problems.%2520Partial%250ASolutions%2520represent%2520beneficial%2520traits%2520found%2520by%2520analyzing%2520a%2520population%252C%2520and%2520are%250Apresented%2520to%2520the%2520user%2520for%2520explainability%252C%2520but%2520also%2520provide%2520an%2520explicit%2520model%250Afrom%2520which%2520new%2520solutions%2520can%2520be%2520generated.%2520We%2520present%2520an%2520algorithm%2520that%250Aassembles%2520a%2520collection%2520of%2520Partial%2520Solutions%2520chosen%2520to%2520strike%2520a%2520balance%2520between%250Ahigh%2520fitness%252C%2520simplicity%2520and%2520atomicity.%2520Experiments%2520with%2520standard%2520benchmarks%250Ashow%2520that%2520the%2520proposed%2520algorithm%2520is%2520able%2520to%2520find%2520Partial%2520Solutions%2520which%250Aimprove%2520explainability%2520at%2520reasonable%2520computational%2520cost%2520without%2520affecting%250Asearch%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04388v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mining%20Potentially%20Explanatory%20Patterns%20via%20Partial%20Solutions&entry.906535625=GianCarlo%20Catalano%20and%20Alexander%20E.%20I.%20Brownlee%20and%20David%20Cairns%20and%20John%20McCall%20and%20Russell%20Ainslie&entry.1292438233=%20%20Genetic%20Algorithms%20have%20established%20their%20capability%20for%20solving%20many%20complex%0Aoptimization%20problems.%20Even%20as%20good%20solutions%20are%20produced%2C%20the%20user%27s%0Aunderstanding%20of%20a%20problem%20is%20not%20necessarily%20improved%2C%20which%20can%20lead%20to%20a%0Alack%20of%20confidence%20in%20the%20results.%20To%20mitigate%20this%20issue%2C%20explainability%20aims%0Ato%20give%20insight%20to%20the%20user%20by%20presenting%20them%20with%20the%20knowledge%20obtained%20by%0Athe%20algorithm.%20In%20this%20paper%20we%20introduce%20Partial%20Solutions%20in%20order%20to%20improve%0Athe%20explainability%20of%20solutions%20to%20combinatorial%20optimization%20problems.%20Partial%0ASolutions%20represent%20beneficial%20traits%20found%20by%20analyzing%20a%20population%2C%20and%20are%0Apresented%20to%20the%20user%20for%20explainability%2C%20but%20also%20provide%20an%20explicit%20model%0Afrom%20which%20new%20solutions%20can%20be%20generated.%20We%20present%20an%20algorithm%20that%0Aassembles%20a%20collection%20of%20Partial%20Solutions%20chosen%20to%20strike%20a%20balance%20between%0Ahigh%20fitness%2C%20simplicity%20and%20atomicity.%20Experiments%20with%20standard%20benchmarks%0Ashow%20that%20the%20proposed%20algorithm%20is%20able%20to%20find%20Partial%20Solutions%20which%0Aimprove%20explainability%20at%20reasonable%20computational%20cost%20without%20affecting%0Asearch%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04388v2&entry.124074799=Read"},
{"title": "ChatTracer: Large Language Model Powered Real-time Bluetooth Device\n  Tracking System", "author": "Qijun Wang and Shichen Zhang and Kunzhe Song and Huacheng Zeng", "abstract": "  Large language models (LLMs) have transformed the way we interact with cyber\ntechnologies. In this paper, we study the possibility of connecting LLM with\nwireless sensor networks (WSN). A successful design will not only extend LLM's\nknowledge landscape to the physical world but also revolutionize human\ninteraction with WSN. To the end, we present ChatTracer, an LLM-powered\nreal-time Bluetooth device tracking system. ChatTracer comprises three key\ncomponents: an array of Bluetooth sniffing nodes, a database, and a fine-tuned\nLLM. ChatTracer was designed based on our experimental observation that\ncommercial Apple/Android devices always broadcast hundreds of BLE packets per\nminute even in their idle status. Its novelties lie in two aspects: i) a\nreliable and efficient BLE packet grouping algorithm; and ii) an LLM\nfine-tuning strategy that combines both supervised fine-tuning (SFT) and\nreinforcement learning with human feedback (RLHF). We have built a prototype of\nChatTracer with four sniffing nodes. Experimental results show that ChatTracer\nnot only outperforms existing localization approaches, but also provides an\nintelligent interface for user interaction.\n", "link": "http://arxiv.org/abs/2403.19833v2", "date": "2024-07-09", "relevancy": 1.7926, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4555}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4538}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatTracer%3A%20Large%20Language%20Model%20Powered%20Real-time%20Bluetooth%20Device%0A%20%20Tracking%20System&body=Title%3A%20ChatTracer%3A%20Large%20Language%20Model%20Powered%20Real-time%20Bluetooth%20Device%0A%20%20Tracking%20System%0AAuthor%3A%20Qijun%20Wang%20and%20Shichen%20Zhang%20and%20Kunzhe%20Song%20and%20Huacheng%20Zeng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20transformed%20the%20way%20we%20interact%20with%20cyber%0Atechnologies.%20In%20this%20paper%2C%20we%20study%20the%20possibility%20of%20connecting%20LLM%20with%0Awireless%20sensor%20networks%20%28WSN%29.%20A%20successful%20design%20will%20not%20only%20extend%20LLM%27s%0Aknowledge%20landscape%20to%20the%20physical%20world%20but%20also%20revolutionize%20human%0Ainteraction%20with%20WSN.%20To%20the%20end%2C%20we%20present%20ChatTracer%2C%20an%20LLM-powered%0Areal-time%20Bluetooth%20device%20tracking%20system.%20ChatTracer%20comprises%20three%20key%0Acomponents%3A%20an%20array%20of%20Bluetooth%20sniffing%20nodes%2C%20a%20database%2C%20and%20a%20fine-tuned%0ALLM.%20ChatTracer%20was%20designed%20based%20on%20our%20experimental%20observation%20that%0Acommercial%20Apple/Android%20devices%20always%20broadcast%20hundreds%20of%20BLE%20packets%20per%0Aminute%20even%20in%20their%20idle%20status.%20Its%20novelties%20lie%20in%20two%20aspects%3A%20i%29%20a%0Areliable%20and%20efficient%20BLE%20packet%20grouping%20algorithm%3B%20and%20ii%29%20an%20LLM%0Afine-tuning%20strategy%20that%20combines%20both%20supervised%20fine-tuning%20%28SFT%29%20and%0Areinforcement%20learning%20with%20human%20feedback%20%28RLHF%29.%20We%20have%20built%20a%20prototype%20of%0AChatTracer%20with%20four%20sniffing%20nodes.%20Experimental%20results%20show%20that%20ChatTracer%0Anot%20only%20outperforms%20existing%20localization%20approaches%2C%20but%20also%20provides%20an%0Aintelligent%20interface%20for%20user%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatTracer%253A%2520Large%2520Language%2520Model%2520Powered%2520Real-time%2520Bluetooth%2520Device%250A%2520%2520Tracking%2520System%26entry.906535625%3DQijun%2520Wang%2520and%2520Shichen%2520Zhang%2520and%2520Kunzhe%2520Song%2520and%2520Huacheng%2520Zeng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520transformed%2520the%2520way%2520we%2520interact%2520with%2520cyber%250Atechnologies.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520possibility%2520of%2520connecting%2520LLM%2520with%250Awireless%2520sensor%2520networks%2520%2528WSN%2529.%2520A%2520successful%2520design%2520will%2520not%2520only%2520extend%2520LLM%2527s%250Aknowledge%2520landscape%2520to%2520the%2520physical%2520world%2520but%2520also%2520revolutionize%2520human%250Ainteraction%2520with%2520WSN.%2520To%2520the%2520end%252C%2520we%2520present%2520ChatTracer%252C%2520an%2520LLM-powered%250Areal-time%2520Bluetooth%2520device%2520tracking%2520system.%2520ChatTracer%2520comprises%2520three%2520key%250Acomponents%253A%2520an%2520array%2520of%2520Bluetooth%2520sniffing%2520nodes%252C%2520a%2520database%252C%2520and%2520a%2520fine-tuned%250ALLM.%2520ChatTracer%2520was%2520designed%2520based%2520on%2520our%2520experimental%2520observation%2520that%250Acommercial%2520Apple/Android%2520devices%2520always%2520broadcast%2520hundreds%2520of%2520BLE%2520packets%2520per%250Aminute%2520even%2520in%2520their%2520idle%2520status.%2520Its%2520novelties%2520lie%2520in%2520two%2520aspects%253A%2520i%2529%2520a%250Areliable%2520and%2520efficient%2520BLE%2520packet%2520grouping%2520algorithm%253B%2520and%2520ii%2529%2520an%2520LLM%250Afine-tuning%2520strategy%2520that%2520combines%2520both%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%250Areinforcement%2520learning%2520with%2520human%2520feedback%2520%2528RLHF%2529.%2520We%2520have%2520built%2520a%2520prototype%2520of%250AChatTracer%2520with%2520four%2520sniffing%2520nodes.%2520Experimental%2520results%2520show%2520that%2520ChatTracer%250Anot%2520only%2520outperforms%2520existing%2520localization%2520approaches%252C%2520but%2520also%2520provides%2520an%250Aintelligent%2520interface%2520for%2520user%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatTracer%3A%20Large%20Language%20Model%20Powered%20Real-time%20Bluetooth%20Device%0A%20%20Tracking%20System&entry.906535625=Qijun%20Wang%20and%20Shichen%20Zhang%20and%20Kunzhe%20Song%20and%20Huacheng%20Zeng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20transformed%20the%20way%20we%20interact%20with%20cyber%0Atechnologies.%20In%20this%20paper%2C%20we%20study%20the%20possibility%20of%20connecting%20LLM%20with%0Awireless%20sensor%20networks%20%28WSN%29.%20A%20successful%20design%20will%20not%20only%20extend%20LLM%27s%0Aknowledge%20landscape%20to%20the%20physical%20world%20but%20also%20revolutionize%20human%0Ainteraction%20with%20WSN.%20To%20the%20end%2C%20we%20present%20ChatTracer%2C%20an%20LLM-powered%0Areal-time%20Bluetooth%20device%20tracking%20system.%20ChatTracer%20comprises%20three%20key%0Acomponents%3A%20an%20array%20of%20Bluetooth%20sniffing%20nodes%2C%20a%20database%2C%20and%20a%20fine-tuned%0ALLM.%20ChatTracer%20was%20designed%20based%20on%20our%20experimental%20observation%20that%0Acommercial%20Apple/Android%20devices%20always%20broadcast%20hundreds%20of%20BLE%20packets%20per%0Aminute%20even%20in%20their%20idle%20status.%20Its%20novelties%20lie%20in%20two%20aspects%3A%20i%29%20a%0Areliable%20and%20efficient%20BLE%20packet%20grouping%20algorithm%3B%20and%20ii%29%20an%20LLM%0Afine-tuning%20strategy%20that%20combines%20both%20supervised%20fine-tuning%20%28SFT%29%20and%0Areinforcement%20learning%20with%20human%20feedback%20%28RLHF%29.%20We%20have%20built%20a%20prototype%20of%0AChatTracer%20with%20four%20sniffing%20nodes.%20Experimental%20results%20show%20that%20ChatTracer%0Anot%20only%20outperforms%20existing%20localization%20approaches%2C%20but%20also%20provides%20an%0Aintelligent%20interface%20for%20user%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19833v2&entry.124074799=Read"},
{"title": "Pretraining-finetuning Framework for Efficient Co-design: A Case Study\n  on Quadruped Robot Parkour", "author": "Ci Chen and Jiyu Yu and Haojian Lu and Hongbo Gao and Rong Xiong and Yue Wang", "abstract": "  In nature, animals with exceptional locomotion abilities, such as cougars,\noften possess asymmetric fore and hind legs, with their powerful hind legs\nacting as reservoirs of energy for leaps. This observation inspired us: could\noptimize the leg length of quadruped robots endow them with similar locomotive\ncapabilities? In this paper, we propose an approach that co-optimizes the\nmechanical structure and control policy to boost the locomotive prowess of\nquadruped robots. Specifically, we introduce a novel pretraining-finetuning\nframework, which not only guarantees optimal control strategies for each\nmechanical candidate but also ensures time efficiency. Additionally, we have\ndevised an innovative training method for our pretraining network, integrating\nspatial domain randomization with regularization methods, markedly improving\nthe network's generalizability. Our experimental results indicate that the\nproposed pretraining-finetuning framework significantly enhances the overall\nco-design performance with less time consumption. Moreover, the co-design\nstrategy substantially exceeds the conventional method of independently\noptimizing control strategies, further improving the robot's locomotive\nperformance and providing an innovative approach to enhancing the extreme\nparkour capabilities of quadruped robots.\n", "link": "http://arxiv.org/abs/2407.06770v1", "date": "2024-07-09", "relevancy": 1.616, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5549}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5382}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretraining-finetuning%20Framework%20for%20Efficient%20Co-design%3A%20A%20Case%20Study%0A%20%20on%20Quadruped%20Robot%20Parkour&body=Title%3A%20Pretraining-finetuning%20Framework%20for%20Efficient%20Co-design%3A%20A%20Case%20Study%0A%20%20on%20Quadruped%20Robot%20Parkour%0AAuthor%3A%20Ci%20Chen%20and%20Jiyu%20Yu%20and%20Haojian%20Lu%20and%20Hongbo%20Gao%20and%20Rong%20Xiong%20and%20Yue%20Wang%0AAbstract%3A%20%20%20In%20nature%2C%20animals%20with%20exceptional%20locomotion%20abilities%2C%20such%20as%20cougars%2C%0Aoften%20possess%20asymmetric%20fore%20and%20hind%20legs%2C%20with%20their%20powerful%20hind%20legs%0Aacting%20as%20reservoirs%20of%20energy%20for%20leaps.%20This%20observation%20inspired%20us%3A%20could%0Aoptimize%20the%20leg%20length%20of%20quadruped%20robots%20endow%20them%20with%20similar%20locomotive%0Acapabilities%3F%20In%20this%20paper%2C%20we%20propose%20an%20approach%20that%20co-optimizes%20the%0Amechanical%20structure%20and%20control%20policy%20to%20boost%20the%20locomotive%20prowess%20of%0Aquadruped%20robots.%20Specifically%2C%20we%20introduce%20a%20novel%20pretraining-finetuning%0Aframework%2C%20which%20not%20only%20guarantees%20optimal%20control%20strategies%20for%20each%0Amechanical%20candidate%20but%20also%20ensures%20time%20efficiency.%20Additionally%2C%20we%20have%0Adevised%20an%20innovative%20training%20method%20for%20our%20pretraining%20network%2C%20integrating%0Aspatial%20domain%20randomization%20with%20regularization%20methods%2C%20markedly%20improving%0Athe%20network%27s%20generalizability.%20Our%20experimental%20results%20indicate%20that%20the%0Aproposed%20pretraining-finetuning%20framework%20significantly%20enhances%20the%20overall%0Aco-design%20performance%20with%20less%20time%20consumption.%20Moreover%2C%20the%20co-design%0Astrategy%20substantially%20exceeds%20the%20conventional%20method%20of%20independently%0Aoptimizing%20control%20strategies%2C%20further%20improving%20the%20robot%27s%20locomotive%0Aperformance%20and%20providing%20an%20innovative%20approach%20to%20enhancing%20the%20extreme%0Aparkour%20capabilities%20of%20quadruped%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretraining-finetuning%2520Framework%2520for%2520Efficient%2520Co-design%253A%2520A%2520Case%2520Study%250A%2520%2520on%2520Quadruped%2520Robot%2520Parkour%26entry.906535625%3DCi%2520Chen%2520and%2520Jiyu%2520Yu%2520and%2520Haojian%2520Lu%2520and%2520Hongbo%2520Gao%2520and%2520Rong%2520Xiong%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520In%2520nature%252C%2520animals%2520with%2520exceptional%2520locomotion%2520abilities%252C%2520such%2520as%2520cougars%252C%250Aoften%2520possess%2520asymmetric%2520fore%2520and%2520hind%2520legs%252C%2520with%2520their%2520powerful%2520hind%2520legs%250Aacting%2520as%2520reservoirs%2520of%2520energy%2520for%2520leaps.%2520This%2520observation%2520inspired%2520us%253A%2520could%250Aoptimize%2520the%2520leg%2520length%2520of%2520quadruped%2520robots%2520endow%2520them%2520with%2520similar%2520locomotive%250Acapabilities%253F%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520approach%2520that%2520co-optimizes%2520the%250Amechanical%2520structure%2520and%2520control%2520policy%2520to%2520boost%2520the%2520locomotive%2520prowess%2520of%250Aquadruped%2520robots.%2520Specifically%252C%2520we%2520introduce%2520a%2520novel%2520pretraining-finetuning%250Aframework%252C%2520which%2520not%2520only%2520guarantees%2520optimal%2520control%2520strategies%2520for%2520each%250Amechanical%2520candidate%2520but%2520also%2520ensures%2520time%2520efficiency.%2520Additionally%252C%2520we%2520have%250Adevised%2520an%2520innovative%2520training%2520method%2520for%2520our%2520pretraining%2520network%252C%2520integrating%250Aspatial%2520domain%2520randomization%2520with%2520regularization%2520methods%252C%2520markedly%2520improving%250Athe%2520network%2527s%2520generalizability.%2520Our%2520experimental%2520results%2520indicate%2520that%2520the%250Aproposed%2520pretraining-finetuning%2520framework%2520significantly%2520enhances%2520the%2520overall%250Aco-design%2520performance%2520with%2520less%2520time%2520consumption.%2520Moreover%252C%2520the%2520co-design%250Astrategy%2520substantially%2520exceeds%2520the%2520conventional%2520method%2520of%2520independently%250Aoptimizing%2520control%2520strategies%252C%2520further%2520improving%2520the%2520robot%2527s%2520locomotive%250Aperformance%2520and%2520providing%2520an%2520innovative%2520approach%2520to%2520enhancing%2520the%2520extreme%250Aparkour%2520capabilities%2520of%2520quadruped%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretraining-finetuning%20Framework%20for%20Efficient%20Co-design%3A%20A%20Case%20Study%0A%20%20on%20Quadruped%20Robot%20Parkour&entry.906535625=Ci%20Chen%20and%20Jiyu%20Yu%20and%20Haojian%20Lu%20and%20Hongbo%20Gao%20and%20Rong%20Xiong%20and%20Yue%20Wang&entry.1292438233=%20%20In%20nature%2C%20animals%20with%20exceptional%20locomotion%20abilities%2C%20such%20as%20cougars%2C%0Aoften%20possess%20asymmetric%20fore%20and%20hind%20legs%2C%20with%20their%20powerful%20hind%20legs%0Aacting%20as%20reservoirs%20of%20energy%20for%20leaps.%20This%20observation%20inspired%20us%3A%20could%0Aoptimize%20the%20leg%20length%20of%20quadruped%20robots%20endow%20them%20with%20similar%20locomotive%0Acapabilities%3F%20In%20this%20paper%2C%20we%20propose%20an%20approach%20that%20co-optimizes%20the%0Amechanical%20structure%20and%20control%20policy%20to%20boost%20the%20locomotive%20prowess%20of%0Aquadruped%20robots.%20Specifically%2C%20we%20introduce%20a%20novel%20pretraining-finetuning%0Aframework%2C%20which%20not%20only%20guarantees%20optimal%20control%20strategies%20for%20each%0Amechanical%20candidate%20but%20also%20ensures%20time%20efficiency.%20Additionally%2C%20we%20have%0Adevised%20an%20innovative%20training%20method%20for%20our%20pretraining%20network%2C%20integrating%0Aspatial%20domain%20randomization%20with%20regularization%20methods%2C%20markedly%20improving%0Athe%20network%27s%20generalizability.%20Our%20experimental%20results%20indicate%20that%20the%0Aproposed%20pretraining-finetuning%20framework%20significantly%20enhances%20the%20overall%0Aco-design%20performance%20with%20less%20time%20consumption.%20Moreover%2C%20the%20co-design%0Astrategy%20substantially%20exceeds%20the%20conventional%20method%20of%20independently%0Aoptimizing%20control%20strategies%2C%20further%20improving%20the%20robot%27s%20locomotive%0Aperformance%20and%20providing%20an%20innovative%20approach%20to%20enhancing%20the%20extreme%0Aparkour%20capabilities%20of%20quadruped%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06770v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


