<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250508.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with\n  Video Diffusion and Data Augmentation", "author": "Yonwoo Choi", "abstract": "  Creating high-quality animatable 3D human avatars from a single image remains\na significant challenge in computer vision due to the inherent difficulty of\nreconstructing complete 3D information from a single viewpoint. Current\napproaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods\nproduce high-quality results but require multiple views or video sequences,\nwhile video diffusion models can generate animations from single images but\nstruggle with consistency and identity preservation. We present SVAD, a novel\napproach that addresses these limitations by leveraging complementary strengths\nof existing techniques. Our method generates synthetic training data through\nvideo diffusion, enhances it with identity preservation and image restoration\nmodules, and utilizes this refined data to train 3DGS avatars. Comprehensive\nevaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)\nsingle-image methods in maintaining identity consistency and fine details\nacross novel poses and viewpoints, while enabling real-time rendering\ncapabilities. Through our data augmentation pipeline, we overcome the\ndependency on dense monocular or multi-view training data typically required by\ntraditional 3DGS approaches. Extensive quantitative, qualitative comparisons\nshow our method achieves superior performance across multiple metrics against\nbaseline models. By effectively combining the generative power of diffusion\nmodels with both the high-quality results and rendering efficiency of 3DGS, our\nwork establishes a new approach for high-fidelity avatar generation from a\nsingle image input.\n", "link": "http://arxiv.org/abs/2505.05475v1", "date": "2025-05-08", "relevancy": 3.397, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6846}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6768}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVAD%3A%20From%20Single%20Image%20to%203D%20Avatar%20via%20Synthetic%20Data%20Generation%20with%0A%20%20Video%20Diffusion%20and%20Data%20Augmentation&body=Title%3A%20SVAD%3A%20From%20Single%20Image%20to%203D%20Avatar%20via%20Synthetic%20Data%20Generation%20with%0A%20%20Video%20Diffusion%20and%20Data%20Augmentation%0AAuthor%3A%20Yonwoo%20Choi%0AAbstract%3A%20%20%20Creating%20high-quality%20animatable%203D%20human%20avatars%20from%20a%20single%20image%20remains%0Aa%20significant%20challenge%20in%20computer%20vision%20due%20to%20the%20inherent%20difficulty%20of%0Areconstructing%20complete%203D%20information%20from%20a%20single%20viewpoint.%20Current%0Aapproaches%20face%20a%20clear%20limitation%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20methods%0Aproduce%20high-quality%20results%20but%20require%20multiple%20views%20or%20video%20sequences%2C%0Awhile%20video%20diffusion%20models%20can%20generate%20animations%20from%20single%20images%20but%0Astruggle%20with%20consistency%20and%20identity%20preservation.%20We%20present%20SVAD%2C%20a%20novel%0Aapproach%20that%20addresses%20these%20limitations%20by%20leveraging%20complementary%20strengths%0Aof%20existing%20techniques.%20Our%20method%20generates%20synthetic%20training%20data%20through%0Avideo%20diffusion%2C%20enhances%20it%20with%20identity%20preservation%20and%20image%20restoration%0Amodules%2C%20and%20utilizes%20this%20refined%20data%20to%20train%203DGS%20avatars.%20Comprehensive%0Aevaluations%20demonstrate%20that%20SVAD%20outperforms%20state-of-the-art%20%28SOTA%29%0Asingle-image%20methods%20in%20maintaining%20identity%20consistency%20and%20fine%20details%0Aacross%20novel%20poses%20and%20viewpoints%2C%20while%20enabling%20real-time%20rendering%0Acapabilities.%20Through%20our%20data%20augmentation%20pipeline%2C%20we%20overcome%20the%0Adependency%20on%20dense%20monocular%20or%20multi-view%20training%20data%20typically%20required%20by%0Atraditional%203DGS%20approaches.%20Extensive%20quantitative%2C%20qualitative%20comparisons%0Ashow%20our%20method%20achieves%20superior%20performance%20across%20multiple%20metrics%20against%0Abaseline%20models.%20By%20effectively%20combining%20the%20generative%20power%20of%20diffusion%0Amodels%20with%20both%20the%20high-quality%20results%20and%20rendering%20efficiency%20of%203DGS%2C%20our%0Awork%20establishes%20a%20new%20approach%20for%20high-fidelity%20avatar%20generation%20from%20a%0Asingle%20image%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVAD%253A%2520From%2520Single%2520Image%2520to%25203D%2520Avatar%2520via%2520Synthetic%2520Data%2520Generation%2520with%250A%2520%2520Video%2520Diffusion%2520and%2520Data%2520Augmentation%26entry.906535625%3DYonwoo%2520Choi%26entry.1292438233%3D%2520%2520Creating%2520high-quality%2520animatable%25203D%2520human%2520avatars%2520from%2520a%2520single%2520image%2520remains%250Aa%2520significant%2520challenge%2520in%2520computer%2520vision%2520due%2520to%2520the%2520inherent%2520difficulty%2520of%250Areconstructing%2520complete%25203D%2520information%2520from%2520a%2520single%2520viewpoint.%2520Current%250Aapproaches%2520face%2520a%2520clear%2520limitation%253A%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520methods%250Aproduce%2520high-quality%2520results%2520but%2520require%2520multiple%2520views%2520or%2520video%2520sequences%252C%250Awhile%2520video%2520diffusion%2520models%2520can%2520generate%2520animations%2520from%2520single%2520images%2520but%250Astruggle%2520with%2520consistency%2520and%2520identity%2520preservation.%2520We%2520present%2520SVAD%252C%2520a%2520novel%250Aapproach%2520that%2520addresses%2520these%2520limitations%2520by%2520leveraging%2520complementary%2520strengths%250Aof%2520existing%2520techniques.%2520Our%2520method%2520generates%2520synthetic%2520training%2520data%2520through%250Avideo%2520diffusion%252C%2520enhances%2520it%2520with%2520identity%2520preservation%2520and%2520image%2520restoration%250Amodules%252C%2520and%2520utilizes%2520this%2520refined%2520data%2520to%2520train%25203DGS%2520avatars.%2520Comprehensive%250Aevaluations%2520demonstrate%2520that%2520SVAD%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%250Asingle-image%2520methods%2520in%2520maintaining%2520identity%2520consistency%2520and%2520fine%2520details%250Aacross%2520novel%2520poses%2520and%2520viewpoints%252C%2520while%2520enabling%2520real-time%2520rendering%250Acapabilities.%2520Through%2520our%2520data%2520augmentation%2520pipeline%252C%2520we%2520overcome%2520the%250Adependency%2520on%2520dense%2520monocular%2520or%2520multi-view%2520training%2520data%2520typically%2520required%2520by%250Atraditional%25203DGS%2520approaches.%2520Extensive%2520quantitative%252C%2520qualitative%2520comparisons%250Ashow%2520our%2520method%2520achieves%2520superior%2520performance%2520across%2520multiple%2520metrics%2520against%250Abaseline%2520models.%2520By%2520effectively%2520combining%2520the%2520generative%2520power%2520of%2520diffusion%250Amodels%2520with%2520both%2520the%2520high-quality%2520results%2520and%2520rendering%2520efficiency%2520of%25203DGS%252C%2520our%250Awork%2520establishes%2520a%2520new%2520approach%2520for%2520high-fidelity%2520avatar%2520generation%2520from%2520a%250Asingle%2520image%2520input.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVAD%3A%20From%20Single%20Image%20to%203D%20Avatar%20via%20Synthetic%20Data%20Generation%20with%0A%20%20Video%20Diffusion%20and%20Data%20Augmentation&entry.906535625=Yonwoo%20Choi&entry.1292438233=%20%20Creating%20high-quality%20animatable%203D%20human%20avatars%20from%20a%20single%20image%20remains%0Aa%20significant%20challenge%20in%20computer%20vision%20due%20to%20the%20inherent%20difficulty%20of%0Areconstructing%20complete%203D%20information%20from%20a%20single%20viewpoint.%20Current%0Aapproaches%20face%20a%20clear%20limitation%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20methods%0Aproduce%20high-quality%20results%20but%20require%20multiple%20views%20or%20video%20sequences%2C%0Awhile%20video%20diffusion%20models%20can%20generate%20animations%20from%20single%20images%20but%0Astruggle%20with%20consistency%20and%20identity%20preservation.%20We%20present%20SVAD%2C%20a%20novel%0Aapproach%20that%20addresses%20these%20limitations%20by%20leveraging%20complementary%20strengths%0Aof%20existing%20techniques.%20Our%20method%20generates%20synthetic%20training%20data%20through%0Avideo%20diffusion%2C%20enhances%20it%20with%20identity%20preservation%20and%20image%20restoration%0Amodules%2C%20and%20utilizes%20this%20refined%20data%20to%20train%203DGS%20avatars.%20Comprehensive%0Aevaluations%20demonstrate%20that%20SVAD%20outperforms%20state-of-the-art%20%28SOTA%29%0Asingle-image%20methods%20in%20maintaining%20identity%20consistency%20and%20fine%20details%0Aacross%20novel%20poses%20and%20viewpoints%2C%20while%20enabling%20real-time%20rendering%0Acapabilities.%20Through%20our%20data%20augmentation%20pipeline%2C%20we%20overcome%20the%0Adependency%20on%20dense%20monocular%20or%20multi-view%20training%20data%20typically%20required%20by%0Atraditional%203DGS%20approaches.%20Extensive%20quantitative%2C%20qualitative%20comparisons%0Ashow%20our%20method%20achieves%20superior%20performance%20across%20multiple%20metrics%20against%0Abaseline%20models.%20By%20effectively%20combining%20the%20generative%20power%20of%20diffusion%0Amodels%20with%20both%20the%20high-quality%20results%20and%20rendering%20efficiency%20of%203DGS%2C%20our%0Awork%20establishes%20a%20new%20approach%20for%20high-fidelity%20avatar%20generation%20from%20a%0Asingle%20image%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05475v1&entry.124074799=Read"},
{"title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in\n  Dynamic Radiance Fields", "author": "Runfeng Li and Mikhail Okunev and Zixuan Guo and Anh Ha Duong and Christian Richardt and Matthew O'Toole and James Tompkin", "abstract": "  We present a method to reconstruct dynamic scenes from monocular\ncontinuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that\nachieves similar or better accuracy than neural volumetric approaches and is\n100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a\nsingle viewpoint is a significant challenge in computer vision. In C-ToF\nradiance field reconstruction, the property of interest-depth-is not directly\nmeasured, causing an additional challenge. This problem has a large and\nunderappreciated impact upon the optimization when using a fast primitive-based\nscene representation like 3D Gaussian splatting, which is commonly used with\nmulti-view data to produce satisfactory results and is brittle in its\noptimization otherwise. We incorporate two heuristics into the optimization to\nimprove the accuracy of scene geometry represented by Gaussians. Experimental\nresults show that our approach produces accurate reconstructions under\nconstrained C-ToF sensing conditions, including for fast motions like swinging\nbaseball bats. https://visual.cs.brown.edu/gftorf\n", "link": "http://arxiv.org/abs/2505.05356v1", "date": "2025-05-08", "relevancy": 3.3629, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7123}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6846}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%20of%20the%20Flight%20of%20the%20Gaussians%3A%20Optimizing%20Depth%20Indirectly%20in%0A%20%20Dynamic%20Radiance%20Fields&body=Title%3A%20Time%20of%20the%20Flight%20of%20the%20Gaussians%3A%20Optimizing%20Depth%20Indirectly%20in%0A%20%20Dynamic%20Radiance%20Fields%0AAuthor%3A%20Runfeng%20Li%20and%20Mikhail%20Okunev%20and%20Zixuan%20Guo%20and%20Anh%20Ha%20Duong%20and%20Christian%20Richardt%20and%20Matthew%20O%27Toole%20and%20James%20Tompkin%0AAbstract%3A%20%20%20We%20present%20a%20method%20to%20reconstruct%20dynamic%20scenes%20from%20monocular%0Acontinuous-wave%20time-of-flight%20%28C-ToF%29%20cameras%20using%20raw%20sensor%20samples%20that%0Aachieves%20similar%20or%20better%20accuracy%20than%20neural%20volumetric%20approaches%20and%20is%0A100x%20faster.%20Quickly%20achieving%20high-fidelity%20dynamic%203D%20reconstruction%20from%20a%0Asingle%20viewpoint%20is%20a%20significant%20challenge%20in%20computer%20vision.%20In%20C-ToF%0Aradiance%20field%20reconstruction%2C%20the%20property%20of%20interest-depth-is%20not%20directly%0Ameasured%2C%20causing%20an%20additional%20challenge.%20This%20problem%20has%20a%20large%20and%0Aunderappreciated%20impact%20upon%20the%20optimization%20when%20using%20a%20fast%20primitive-based%0Ascene%20representation%20like%203D%20Gaussian%20splatting%2C%20which%20is%20commonly%20used%20with%0Amulti-view%20data%20to%20produce%20satisfactory%20results%20and%20is%20brittle%20in%20its%0Aoptimization%20otherwise.%20We%20incorporate%20two%20heuristics%20into%20the%20optimization%20to%0Aimprove%20the%20accuracy%20of%20scene%20geometry%20represented%20by%20Gaussians.%20Experimental%0Aresults%20show%20that%20our%20approach%20produces%20accurate%20reconstructions%20under%0Aconstrained%20C-ToF%20sensing%20conditions%2C%20including%20for%20fast%20motions%20like%20swinging%0Abaseball%20bats.%20https%3A//visual.cs.brown.edu/gftorf%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2520of%2520the%2520Flight%2520of%2520the%2520Gaussians%253A%2520Optimizing%2520Depth%2520Indirectly%2520in%250A%2520%2520Dynamic%2520Radiance%2520Fields%26entry.906535625%3DRunfeng%2520Li%2520and%2520Mikhail%2520Okunev%2520and%2520Zixuan%2520Guo%2520and%2520Anh%2520Ha%2520Duong%2520and%2520Christian%2520Richardt%2520and%2520Matthew%2520O%2527Toole%2520and%2520James%2520Tompkin%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520to%2520reconstruct%2520dynamic%2520scenes%2520from%2520monocular%250Acontinuous-wave%2520time-of-flight%2520%2528C-ToF%2529%2520cameras%2520using%2520raw%2520sensor%2520samples%2520that%250Aachieves%2520similar%2520or%2520better%2520accuracy%2520than%2520neural%2520volumetric%2520approaches%2520and%2520is%250A100x%2520faster.%2520Quickly%2520achieving%2520high-fidelity%2520dynamic%25203D%2520reconstruction%2520from%2520a%250Asingle%2520viewpoint%2520is%2520a%2520significant%2520challenge%2520in%2520computer%2520vision.%2520In%2520C-ToF%250Aradiance%2520field%2520reconstruction%252C%2520the%2520property%2520of%2520interest-depth-is%2520not%2520directly%250Ameasured%252C%2520causing%2520an%2520additional%2520challenge.%2520This%2520problem%2520has%2520a%2520large%2520and%250Aunderappreciated%2520impact%2520upon%2520the%2520optimization%2520when%2520using%2520a%2520fast%2520primitive-based%250Ascene%2520representation%2520like%25203D%2520Gaussian%2520splatting%252C%2520which%2520is%2520commonly%2520used%2520with%250Amulti-view%2520data%2520to%2520produce%2520satisfactory%2520results%2520and%2520is%2520brittle%2520in%2520its%250Aoptimization%2520otherwise.%2520We%2520incorporate%2520two%2520heuristics%2520into%2520the%2520optimization%2520to%250Aimprove%2520the%2520accuracy%2520of%2520scene%2520geometry%2520represented%2520by%2520Gaussians.%2520Experimental%250Aresults%2520show%2520that%2520our%2520approach%2520produces%2520accurate%2520reconstructions%2520under%250Aconstrained%2520C-ToF%2520sensing%2520conditions%252C%2520including%2520for%2520fast%2520motions%2520like%2520swinging%250Abaseball%2520bats.%2520https%253A//visual.cs.brown.edu/gftorf%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20of%20the%20Flight%20of%20the%20Gaussians%3A%20Optimizing%20Depth%20Indirectly%20in%0A%20%20Dynamic%20Radiance%20Fields&entry.906535625=Runfeng%20Li%20and%20Mikhail%20Okunev%20and%20Zixuan%20Guo%20and%20Anh%20Ha%20Duong%20and%20Christian%20Richardt%20and%20Matthew%20O%27Toole%20and%20James%20Tompkin&entry.1292438233=%20%20We%20present%20a%20method%20to%20reconstruct%20dynamic%20scenes%20from%20monocular%0Acontinuous-wave%20time-of-flight%20%28C-ToF%29%20cameras%20using%20raw%20sensor%20samples%20that%0Aachieves%20similar%20or%20better%20accuracy%20than%20neural%20volumetric%20approaches%20and%20is%0A100x%20faster.%20Quickly%20achieving%20high-fidelity%20dynamic%203D%20reconstruction%20from%20a%0Asingle%20viewpoint%20is%20a%20significant%20challenge%20in%20computer%20vision.%20In%20C-ToF%0Aradiance%20field%20reconstruction%2C%20the%20property%20of%20interest-depth-is%20not%20directly%0Ameasured%2C%20causing%20an%20additional%20challenge.%20This%20problem%20has%20a%20large%20and%0Aunderappreciated%20impact%20upon%20the%20optimization%20when%20using%20a%20fast%20primitive-based%0Ascene%20representation%20like%203D%20Gaussian%20splatting%2C%20which%20is%20commonly%20used%20with%0Amulti-view%20data%20to%20produce%20satisfactory%20results%20and%20is%20brittle%20in%20its%0Aoptimization%20otherwise.%20We%20incorporate%20two%20heuristics%20into%20the%20optimization%20to%0Aimprove%20the%20accuracy%20of%20scene%20geometry%20represented%20by%20Gaussians.%20Experimental%0Aresults%20show%20that%20our%20approach%20produces%20accurate%20reconstructions%20under%0Aconstrained%20C-ToF%20sensing%20conditions%2C%20including%20for%20fast%20motions%20like%20swinging%0Abaseball%20bats.%20https%3A//visual.cs.brown.edu/gftorf%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05356v1&entry.124074799=Read"},
{"title": "3D Scene Generation: A Survey", "author": "Beichen Wen and Haozhe Xie and Zhaoxi Chen and Fangzhou Hong and Ziwei Liu", "abstract": "  3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.\n", "link": "http://arxiv.org/abs/2505.05474v1", "date": "2025-05-08", "relevancy": 3.1738, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6425}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6425}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Scene%20Generation%3A%20A%20Survey&body=Title%3A%203D%20Scene%20Generation%3A%20A%20Survey%0AAuthor%3A%20Beichen%20Wen%20and%20Haozhe%20Xie%20and%20Zhaoxi%20Chen%20and%20Fangzhou%20Hong%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%203D%20scene%20generation%20seeks%20to%20synthesize%20spatially%20structured%2C%20semantically%0Ameaningful%2C%20and%20photorealistic%20environments%20for%20applications%20such%20as%20immersive%0Amedia%2C%20robotics%2C%20autonomous%20driving%2C%20and%20embodied%20AI.%20Early%20methods%20based%20on%0Aprocedural%20rules%20offered%20scalability%20but%20limited%20diversity.%20Recent%20advances%20in%0Adeep%20generative%20models%20%28e.g.%2C%20GANs%2C%20diffusion%20models%29%20and%203D%20representations%0A%28e.g.%2C%20NeRF%2C%203D%20Gaussians%29%20have%20enabled%20the%20learning%20of%20real-world%20scene%0Adistributions%2C%20improving%20fidelity%2C%20diversity%2C%20and%20view%20consistency.%20Recent%0Aadvances%20like%20diffusion%20models%20bridge%203D%20scene%20synthesis%20and%20photorealism%20by%0Areframing%20generation%20as%20image%20or%20video%20synthesis%20problems.%20This%20survey%20provides%0Aa%20systematic%20overview%20of%20state-of-the-art%20approaches%2C%20organizing%20them%20into%20four%0Aparadigms%3A%20procedural%20generation%2C%20neural%203D-based%20generation%2C%20image-based%0Ageneration%2C%20and%20video-based%20generation.%20We%20analyze%20their%20technical%20foundations%2C%0Atrade-offs%2C%20and%20representative%20results%2C%20and%20review%20commonly%20used%20datasets%2C%0Aevaluation%20protocols%2C%20and%20downstream%20applications.%20We%20conclude%20by%20discussing%0Akey%20challenges%20in%20generation%20capacity%2C%203D%20representation%2C%20data%20and%20annotations%2C%0Aand%20evaluation%2C%20and%20outline%20promising%20directions%20including%20higher%20fidelity%2C%0Aphysics-aware%20and%20interactive%20generation%2C%20and%20unified%20perception-generation%0Amodels.%20This%20review%20organizes%20recent%20advances%20in%203D%20scene%20generation%20and%0Ahighlights%20promising%20directions%20at%20the%20intersection%20of%20generative%20AI%2C%203D%0Avision%2C%20and%20embodied%20intelligence.%20To%20track%20ongoing%20developments%2C%20we%20maintain%0Aan%20up-to-date%20project%20page%3A%0Ahttps%3A//github.com/hzxie/Awesome-3D-Scene-Generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Scene%2520Generation%253A%2520A%2520Survey%26entry.906535625%3DBeichen%2520Wen%2520and%2520Haozhe%2520Xie%2520and%2520Zhaoxi%2520Chen%2520and%2520Fangzhou%2520Hong%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%25203D%2520scene%2520generation%2520seeks%2520to%2520synthesize%2520spatially%2520structured%252C%2520semantically%250Ameaningful%252C%2520and%2520photorealistic%2520environments%2520for%2520applications%2520such%2520as%2520immersive%250Amedia%252C%2520robotics%252C%2520autonomous%2520driving%252C%2520and%2520embodied%2520AI.%2520Early%2520methods%2520based%2520on%250Aprocedural%2520rules%2520offered%2520scalability%2520but%2520limited%2520diversity.%2520Recent%2520advances%2520in%250Adeep%2520generative%2520models%2520%2528e.g.%252C%2520GANs%252C%2520diffusion%2520models%2529%2520and%25203D%2520representations%250A%2528e.g.%252C%2520NeRF%252C%25203D%2520Gaussians%2529%2520have%2520enabled%2520the%2520learning%2520of%2520real-world%2520scene%250Adistributions%252C%2520improving%2520fidelity%252C%2520diversity%252C%2520and%2520view%2520consistency.%2520Recent%250Aadvances%2520like%2520diffusion%2520models%2520bridge%25203D%2520scene%2520synthesis%2520and%2520photorealism%2520by%250Areframing%2520generation%2520as%2520image%2520or%2520video%2520synthesis%2520problems.%2520This%2520survey%2520provides%250Aa%2520systematic%2520overview%2520of%2520state-of-the-art%2520approaches%252C%2520organizing%2520them%2520into%2520four%250Aparadigms%253A%2520procedural%2520generation%252C%2520neural%25203D-based%2520generation%252C%2520image-based%250Ageneration%252C%2520and%2520video-based%2520generation.%2520We%2520analyze%2520their%2520technical%2520foundations%252C%250Atrade-offs%252C%2520and%2520representative%2520results%252C%2520and%2520review%2520commonly%2520used%2520datasets%252C%250Aevaluation%2520protocols%252C%2520and%2520downstream%2520applications.%2520We%2520conclude%2520by%2520discussing%250Akey%2520challenges%2520in%2520generation%2520capacity%252C%25203D%2520representation%252C%2520data%2520and%2520annotations%252C%250Aand%2520evaluation%252C%2520and%2520outline%2520promising%2520directions%2520including%2520higher%2520fidelity%252C%250Aphysics-aware%2520and%2520interactive%2520generation%252C%2520and%2520unified%2520perception-generation%250Amodels.%2520This%2520review%2520organizes%2520recent%2520advances%2520in%25203D%2520scene%2520generation%2520and%250Ahighlights%2520promising%2520directions%2520at%2520the%2520intersection%2520of%2520generative%2520AI%252C%25203D%250Avision%252C%2520and%2520embodied%2520intelligence.%2520To%2520track%2520ongoing%2520developments%252C%2520we%2520maintain%250Aan%2520up-to-date%2520project%2520page%253A%250Ahttps%253A//github.com/hzxie/Awesome-3D-Scene-Generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Scene%20Generation%3A%20A%20Survey&entry.906535625=Beichen%20Wen%20and%20Haozhe%20Xie%20and%20Zhaoxi%20Chen%20and%20Fangzhou%20Hong%20and%20Ziwei%20Liu&entry.1292438233=%20%203D%20scene%20generation%20seeks%20to%20synthesize%20spatially%20structured%2C%20semantically%0Ameaningful%2C%20and%20photorealistic%20environments%20for%20applications%20such%20as%20immersive%0Amedia%2C%20robotics%2C%20autonomous%20driving%2C%20and%20embodied%20AI.%20Early%20methods%20based%20on%0Aprocedural%20rules%20offered%20scalability%20but%20limited%20diversity.%20Recent%20advances%20in%0Adeep%20generative%20models%20%28e.g.%2C%20GANs%2C%20diffusion%20models%29%20and%203D%20representations%0A%28e.g.%2C%20NeRF%2C%203D%20Gaussians%29%20have%20enabled%20the%20learning%20of%20real-world%20scene%0Adistributions%2C%20improving%20fidelity%2C%20diversity%2C%20and%20view%20consistency.%20Recent%0Aadvances%20like%20diffusion%20models%20bridge%203D%20scene%20synthesis%20and%20photorealism%20by%0Areframing%20generation%20as%20image%20or%20video%20synthesis%20problems.%20This%20survey%20provides%0Aa%20systematic%20overview%20of%20state-of-the-art%20approaches%2C%20organizing%20them%20into%20four%0Aparadigms%3A%20procedural%20generation%2C%20neural%203D-based%20generation%2C%20image-based%0Ageneration%2C%20and%20video-based%20generation.%20We%20analyze%20their%20technical%20foundations%2C%0Atrade-offs%2C%20and%20representative%20results%2C%20and%20review%20commonly%20used%20datasets%2C%0Aevaluation%20protocols%2C%20and%20downstream%20applications.%20We%20conclude%20by%20discussing%0Akey%20challenges%20in%20generation%20capacity%2C%203D%20representation%2C%20data%20and%20annotations%2C%0Aand%20evaluation%2C%20and%20outline%20promising%20directions%20including%20higher%20fidelity%2C%0Aphysics-aware%20and%20interactive%20generation%2C%20and%20unified%20perception-generation%0Amodels.%20This%20review%20organizes%20recent%20advances%20in%203D%20scene%20generation%20and%0Ahighlights%20promising%20directions%20at%20the%20intersection%20of%20generative%20AI%2C%203D%0Avision%2C%20and%20embodied%20intelligence.%20To%20track%20ongoing%20developments%2C%20we%20maintain%0Aan%20up-to-date%20project%20page%3A%0Ahttps%3A//github.com/hzxie/Awesome-3D-Scene-Generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05474v1&entry.124074799=Read"},
{"title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes", "author": "Ahmed Abdelreheem and Filippo Aleotti and Jamie Watson and Zawar Qureshi and Abdelrahman Eldesokey and Peter Wonka and Gabriel Brostow and Sara Vicente and Guillermo Garcia-Hernando", "abstract": "  We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.\n", "link": "http://arxiv.org/abs/2505.05288v1", "date": "2025-05-08", "relevancy": 3.0839, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6325}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PlaceIt3D%3A%20Language-Guided%20Object%20Placement%20in%20Real%203D%20Scenes&body=Title%3A%20PlaceIt3D%3A%20Language-Guided%20Object%20Placement%20in%20Real%203D%20Scenes%0AAuthor%3A%20Ahmed%20Abdelreheem%20and%20Filippo%20Aleotti%20and%20Jamie%20Watson%20and%20Zawar%20Qureshi%20and%20Abdelrahman%20Eldesokey%20and%20Peter%20Wonka%20and%20Gabriel%20Brostow%20and%20Sara%20Vicente%20and%20Guillermo%20Garcia-Hernando%0AAbstract%3A%20%20%20We%20introduce%20the%20novel%20task%20of%20Language-Guided%20Object%20Placement%20in%20Real%203D%0AScenes.%20Our%20model%20is%20given%20a%203D%20scene%27s%20point%20cloud%2C%20a%203D%20asset%2C%20and%20a%20textual%0Aprompt%20broadly%20describing%20where%20the%203D%20asset%20should%20be%20placed.%20The%20task%20here%20is%0Ato%20find%20a%20valid%20placement%20for%20the%203D%20asset%20that%20respects%20the%20prompt.%20Compared%0Awith%20other%20language-guided%20localization%20tasks%20in%203D%20scenes%20such%20as%20grounding%2C%0Athis%20task%20has%20specific%20challenges%3A%20it%20is%20ambiguous%20because%20it%20has%20multiple%0Avalid%20solutions%2C%20and%20it%20requires%20reasoning%20about%203D%20geometric%20relationships%20and%0Afree%20space.%20We%20inaugurate%20this%20task%20by%20proposing%20a%20new%20benchmark%20and%20evaluation%0Aprotocol.%20We%20also%20introduce%20a%20new%20dataset%20for%20training%203D%20LLMs%20on%20this%20task%2C%20as%0Awell%20as%20the%20first%20method%20to%20serve%20as%20a%20non-trivial%20baseline.%20We%20believe%20that%0Athis%20challenging%20task%20and%20our%20new%20benchmark%20could%20become%20part%20of%20the%20suite%20of%0Abenchmarks%20used%20to%20evaluate%20and%20compare%20generalist%203D%20LLM%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlaceIt3D%253A%2520Language-Guided%2520Object%2520Placement%2520in%2520Real%25203D%2520Scenes%26entry.906535625%3DAhmed%2520Abdelreheem%2520and%2520Filippo%2520Aleotti%2520and%2520Jamie%2520Watson%2520and%2520Zawar%2520Qureshi%2520and%2520Abdelrahman%2520Eldesokey%2520and%2520Peter%2520Wonka%2520and%2520Gabriel%2520Brostow%2520and%2520Sara%2520Vicente%2520and%2520Guillermo%2520Garcia-Hernando%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520novel%2520task%2520of%2520Language-Guided%2520Object%2520Placement%2520in%2520Real%25203D%250AScenes.%2520Our%2520model%2520is%2520given%2520a%25203D%2520scene%2527s%2520point%2520cloud%252C%2520a%25203D%2520asset%252C%2520and%2520a%2520textual%250Aprompt%2520broadly%2520describing%2520where%2520the%25203D%2520asset%2520should%2520be%2520placed.%2520The%2520task%2520here%2520is%250Ato%2520find%2520a%2520valid%2520placement%2520for%2520the%25203D%2520asset%2520that%2520respects%2520the%2520prompt.%2520Compared%250Awith%2520other%2520language-guided%2520localization%2520tasks%2520in%25203D%2520scenes%2520such%2520as%2520grounding%252C%250Athis%2520task%2520has%2520specific%2520challenges%253A%2520it%2520is%2520ambiguous%2520because%2520it%2520has%2520multiple%250Avalid%2520solutions%252C%2520and%2520it%2520requires%2520reasoning%2520about%25203D%2520geometric%2520relationships%2520and%250Afree%2520space.%2520We%2520inaugurate%2520this%2520task%2520by%2520proposing%2520a%2520new%2520benchmark%2520and%2520evaluation%250Aprotocol.%2520We%2520also%2520introduce%2520a%2520new%2520dataset%2520for%2520training%25203D%2520LLMs%2520on%2520this%2520task%252C%2520as%250Awell%2520as%2520the%2520first%2520method%2520to%2520serve%2520as%2520a%2520non-trivial%2520baseline.%2520We%2520believe%2520that%250Athis%2520challenging%2520task%2520and%2520our%2520new%2520benchmark%2520could%2520become%2520part%2520of%2520the%2520suite%2520of%250Abenchmarks%2520used%2520to%2520evaluate%2520and%2520compare%2520generalist%25203D%2520LLM%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlaceIt3D%3A%20Language-Guided%20Object%20Placement%20in%20Real%203D%20Scenes&entry.906535625=Ahmed%20Abdelreheem%20and%20Filippo%20Aleotti%20and%20Jamie%20Watson%20and%20Zawar%20Qureshi%20and%20Abdelrahman%20Eldesokey%20and%20Peter%20Wonka%20and%20Gabriel%20Brostow%20and%20Sara%20Vicente%20and%20Guillermo%20Garcia-Hernando&entry.1292438233=%20%20We%20introduce%20the%20novel%20task%20of%20Language-Guided%20Object%20Placement%20in%20Real%203D%0AScenes.%20Our%20model%20is%20given%20a%203D%20scene%27s%20point%20cloud%2C%20a%203D%20asset%2C%20and%20a%20textual%0Aprompt%20broadly%20describing%20where%20the%203D%20asset%20should%20be%20placed.%20The%20task%20here%20is%0Ato%20find%20a%20valid%20placement%20for%20the%203D%20asset%20that%20respects%20the%20prompt.%20Compared%0Awith%20other%20language-guided%20localization%20tasks%20in%203D%20scenes%20such%20as%20grounding%2C%0Athis%20task%20has%20specific%20challenges%3A%20it%20is%20ambiguous%20because%20it%20has%20multiple%0Avalid%20solutions%2C%20and%20it%20requires%20reasoning%20about%203D%20geometric%20relationships%20and%0Afree%20space.%20We%20inaugurate%20this%20task%20by%20proposing%20a%20new%20benchmark%20and%20evaluation%0Aprotocol.%20We%20also%20introduce%20a%20new%20dataset%20for%20training%203D%20LLMs%20on%20this%20task%2C%20as%0Awell%20as%20the%20first%20method%20to%20serve%20as%20a%20non-trivial%20baseline.%20We%20believe%20that%0Athis%20challenging%20task%20and%20our%20new%20benchmark%20could%20become%20part%20of%20the%20suite%20of%0Abenchmarks%20used%20to%20evaluate%20and%20compare%20generalist%203D%20LLM%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05288v1&entry.124074799=Read"},
{"title": "Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound\n  Source Localization", "author": "Sooyoung Park and Arda Senocak and Joon Son Chung", "abstract": "  Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings.\n", "link": "http://arxiv.org/abs/2505.05343v1", "date": "2025-05-08", "relevancy": 2.94, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hearing%20and%20Seeing%20Through%20CLIP%3A%20A%20Framework%20for%20Self-Supervised%20Sound%0A%20%20Source%20Localization&body=Title%3A%20Hearing%20and%20Seeing%20Through%20CLIP%3A%20A%20Framework%20for%20Self-Supervised%20Sound%0A%20%20Source%20Localization%0AAuthor%3A%20Sooyoung%20Park%20and%20Arda%20Senocak%20and%20Joon%20Son%20Chung%0AAbstract%3A%20%20%20Large-scale%20vision-language%20models%20demonstrate%20strong%20multimodal%20alignment%0Aand%20generalization%20across%20diverse%20tasks.%20Among%20them%2C%20CLIP%20stands%20out%20as%20one%20of%0Athe%20most%20successful%20approaches.%20In%20this%20work%2C%20we%20extend%20the%20application%20of%20CLIP%0Ato%20sound%20source%20localization%2C%20proposing%20a%20self-supervised%20method%20operates%0Awithout%20explicit%20text%20input.%20We%20introduce%20a%20framework%20that%20maps%20audios%20into%0Atokens%20compatible%20with%20CLIP%27s%20text%20encoder%2C%20producing%20audio-driven%20embeddings.%0AThese%20embeddings%20are%20used%20to%20generate%20sounding%20region%20masks%2C%20from%20which%20visual%0Afeatures%20are%20extracted%20and%20aligned%20with%20the%20audio%20embeddings%20through%20a%0Acontrastive%20audio-visual%20correspondence%20objective.%20Our%20findings%20show%20that%0Aalignment%20knowledge%20of%20pre-trained%20multimodal%20foundation%20model%20enables%20our%0Amethod%20to%20generate%20more%20complete%20and%20compact%20localization%20for%20sounding%20objects.%0AWe%20further%20propose%20an%20LLM-guided%20extension%20that%20distills%20object-aware%0Aaudio-visual%20scene%20understanding%20into%20the%20model%20during%20training%20to%20enhance%0Aalignment.%20Extensive%20experiments%20across%20five%20diverse%20tasks%20demonstrate%20that%20our%0Amethod%2C%20in%20all%20variants%2C%20outperforms%20state-of-the-art%20approaches%20and%20achieves%0Astrong%20generalization%20in%20zero-shot%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHearing%2520and%2520Seeing%2520Through%2520CLIP%253A%2520A%2520Framework%2520for%2520Self-Supervised%2520Sound%250A%2520%2520Source%2520Localization%26entry.906535625%3DSooyoung%2520Park%2520and%2520Arda%2520Senocak%2520and%2520Joon%2520Son%2520Chung%26entry.1292438233%3D%2520%2520Large-scale%2520vision-language%2520models%2520demonstrate%2520strong%2520multimodal%2520alignment%250Aand%2520generalization%2520across%2520diverse%2520tasks.%2520Among%2520them%252C%2520CLIP%2520stands%2520out%2520as%2520one%2520of%250Athe%2520most%2520successful%2520approaches.%2520In%2520this%2520work%252C%2520we%2520extend%2520the%2520application%2520of%2520CLIP%250Ato%2520sound%2520source%2520localization%252C%2520proposing%2520a%2520self-supervised%2520method%2520operates%250Awithout%2520explicit%2520text%2520input.%2520We%2520introduce%2520a%2520framework%2520that%2520maps%2520audios%2520into%250Atokens%2520compatible%2520with%2520CLIP%2527s%2520text%2520encoder%252C%2520producing%2520audio-driven%2520embeddings.%250AThese%2520embeddings%2520are%2520used%2520to%2520generate%2520sounding%2520region%2520masks%252C%2520from%2520which%2520visual%250Afeatures%2520are%2520extracted%2520and%2520aligned%2520with%2520the%2520audio%2520embeddings%2520through%2520a%250Acontrastive%2520audio-visual%2520correspondence%2520objective.%2520Our%2520findings%2520show%2520that%250Aalignment%2520knowledge%2520of%2520pre-trained%2520multimodal%2520foundation%2520model%2520enables%2520our%250Amethod%2520to%2520generate%2520more%2520complete%2520and%2520compact%2520localization%2520for%2520sounding%2520objects.%250AWe%2520further%2520propose%2520an%2520LLM-guided%2520extension%2520that%2520distills%2520object-aware%250Aaudio-visual%2520scene%2520understanding%2520into%2520the%2520model%2520during%2520training%2520to%2520enhance%250Aalignment.%2520Extensive%2520experiments%2520across%2520five%2520diverse%2520tasks%2520demonstrate%2520that%2520our%250Amethod%252C%2520in%2520all%2520variants%252C%2520outperforms%2520state-of-the-art%2520approaches%2520and%2520achieves%250Astrong%2520generalization%2520in%2520zero-shot%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hearing%20and%20Seeing%20Through%20CLIP%3A%20A%20Framework%20for%20Self-Supervised%20Sound%0A%20%20Source%20Localization&entry.906535625=Sooyoung%20Park%20and%20Arda%20Senocak%20and%20Joon%20Son%20Chung&entry.1292438233=%20%20Large-scale%20vision-language%20models%20demonstrate%20strong%20multimodal%20alignment%0Aand%20generalization%20across%20diverse%20tasks.%20Among%20them%2C%20CLIP%20stands%20out%20as%20one%20of%0Athe%20most%20successful%20approaches.%20In%20this%20work%2C%20we%20extend%20the%20application%20of%20CLIP%0Ato%20sound%20source%20localization%2C%20proposing%20a%20self-supervised%20method%20operates%0Awithout%20explicit%20text%20input.%20We%20introduce%20a%20framework%20that%20maps%20audios%20into%0Atokens%20compatible%20with%20CLIP%27s%20text%20encoder%2C%20producing%20audio-driven%20embeddings.%0AThese%20embeddings%20are%20used%20to%20generate%20sounding%20region%20masks%2C%20from%20which%20visual%0Afeatures%20are%20extracted%20and%20aligned%20with%20the%20audio%20embeddings%20through%20a%0Acontrastive%20audio-visual%20correspondence%20objective.%20Our%20findings%20show%20that%0Aalignment%20knowledge%20of%20pre-trained%20multimodal%20foundation%20model%20enables%20our%0Amethod%20to%20generate%20more%20complete%20and%20compact%20localization%20for%20sounding%20objects.%0AWe%20further%20propose%20an%20LLM-guided%20extension%20that%20distills%20object-aware%0Aaudio-visual%20scene%20understanding%20into%20the%20model%20during%20training%20to%20enhance%0Aalignment.%20Extensive%20experiments%20across%20five%20diverse%20tasks%20demonstrate%20that%20our%0Amethod%2C%20in%20all%20variants%2C%20outperforms%20state-of-the-art%20approaches%20and%20achieves%0Astrong%20generalization%20in%20zero-shot%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05343v1&entry.124074799=Read"},
{"title": "GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans", "author": "Rachmadio Noval Lazuardi and Artem Sevastopolsky and Egor Zakharov and Matthias Niessner and Vanessa Sklyarova", "abstract": "  We propose a novel method that reconstructs hair strands directly from\ncolorless 3D scans by leveraging multi-modal hair orientation extraction. Hair\nstrand reconstruction is a fundamental problem in computer vision and graphics\nthat can be used for high-fidelity digital avatar synthesis, animation, and\nAR/VR applications. However, accurately recovering hair strands from raw scan\ndata remains challenging due to human hair's complex and fine-grained\nstructure. Existing methods typically rely on RGB captures, which can be\nsensitive to the environment and can be a challenging domain for extracting the\norientation of guiding strands, especially in the case of challenging\nhairstyles. To reconstruct the hair purely from the observed geometry, our\nmethod finds sharp surface features directly on the scan and estimates strand\norientation through a neural 2D line detector applied to the renderings of scan\nshading. Additionally, we incorporate a diffusion prior trained on a diverse\nset of synthetic hair scans, refined with an improved noise schedule, and\nadapted to the reconstructed contents via a scan-specific text prompt. We\ndemonstrate that this combination of supervision signals enables accurate\nreconstruction of both simple and intricate hairstyles without relying on color\ninformation. To facilitate further research, we introduce Strands400, the\nlargest publicly available dataset of hair strands with detailed surface\ngeometry extracted from real-world data, which contains reconstructed hair\nstrands from the scans of 400 subjects.\n", "link": "http://arxiv.org/abs/2505.05376v1", "date": "2025-05-08", "relevancy": 2.8842, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5866}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5789}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeomHair%3A%20Reconstruction%20of%20Hair%20Strands%20from%20Colorless%203D%20Scans&body=Title%3A%20GeomHair%3A%20Reconstruction%20of%20Hair%20Strands%20from%20Colorless%203D%20Scans%0AAuthor%3A%20Rachmadio%20Noval%20Lazuardi%20and%20Artem%20Sevastopolsky%20and%20Egor%20Zakharov%20and%20Matthias%20Niessner%20and%20Vanessa%20Sklyarova%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20method%20that%20reconstructs%20hair%20strands%20directly%20from%0Acolorless%203D%20scans%20by%20leveraging%20multi-modal%20hair%20orientation%20extraction.%20Hair%0Astrand%20reconstruction%20is%20a%20fundamental%20problem%20in%20computer%20vision%20and%20graphics%0Athat%20can%20be%20used%20for%20high-fidelity%20digital%20avatar%20synthesis%2C%20animation%2C%20and%0AAR/VR%20applications.%20However%2C%20accurately%20recovering%20hair%20strands%20from%20raw%20scan%0Adata%20remains%20challenging%20due%20to%20human%20hair%27s%20complex%20and%20fine-grained%0Astructure.%20Existing%20methods%20typically%20rely%20on%20RGB%20captures%2C%20which%20can%20be%0Asensitive%20to%20the%20environment%20and%20can%20be%20a%20challenging%20domain%20for%20extracting%20the%0Aorientation%20of%20guiding%20strands%2C%20especially%20in%20the%20case%20of%20challenging%0Ahairstyles.%20To%20reconstruct%20the%20hair%20purely%20from%20the%20observed%20geometry%2C%20our%0Amethod%20finds%20sharp%20surface%20features%20directly%20on%20the%20scan%20and%20estimates%20strand%0Aorientation%20through%20a%20neural%202D%20line%20detector%20applied%20to%20the%20renderings%20of%20scan%0Ashading.%20Additionally%2C%20we%20incorporate%20a%20diffusion%20prior%20trained%20on%20a%20diverse%0Aset%20of%20synthetic%20hair%20scans%2C%20refined%20with%20an%20improved%20noise%20schedule%2C%20and%0Aadapted%20to%20the%20reconstructed%20contents%20via%20a%20scan-specific%20text%20prompt.%20We%0Ademonstrate%20that%20this%20combination%20of%20supervision%20signals%20enables%20accurate%0Areconstruction%20of%20both%20simple%20and%20intricate%20hairstyles%20without%20relying%20on%20color%0Ainformation.%20To%20facilitate%20further%20research%2C%20we%20introduce%20Strands400%2C%20the%0Alargest%20publicly%20available%20dataset%20of%20hair%20strands%20with%20detailed%20surface%0Ageometry%20extracted%20from%20real-world%20data%2C%20which%20contains%20reconstructed%20hair%0Astrands%20from%20the%20scans%20of%20400%20subjects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeomHair%253A%2520Reconstruction%2520of%2520Hair%2520Strands%2520from%2520Colorless%25203D%2520Scans%26entry.906535625%3DRachmadio%2520Noval%2520Lazuardi%2520and%2520Artem%2520Sevastopolsky%2520and%2520Egor%2520Zakharov%2520and%2520Matthias%2520Niessner%2520and%2520Vanessa%2520Sklyarova%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520method%2520that%2520reconstructs%2520hair%2520strands%2520directly%2520from%250Acolorless%25203D%2520scans%2520by%2520leveraging%2520multi-modal%2520hair%2520orientation%2520extraction.%2520Hair%250Astrand%2520reconstruction%2520is%2520a%2520fundamental%2520problem%2520in%2520computer%2520vision%2520and%2520graphics%250Athat%2520can%2520be%2520used%2520for%2520high-fidelity%2520digital%2520avatar%2520synthesis%252C%2520animation%252C%2520and%250AAR/VR%2520applications.%2520However%252C%2520accurately%2520recovering%2520hair%2520strands%2520from%2520raw%2520scan%250Adata%2520remains%2520challenging%2520due%2520to%2520human%2520hair%2527s%2520complex%2520and%2520fine-grained%250Astructure.%2520Existing%2520methods%2520typically%2520rely%2520on%2520RGB%2520captures%252C%2520which%2520can%2520be%250Asensitive%2520to%2520the%2520environment%2520and%2520can%2520be%2520a%2520challenging%2520domain%2520for%2520extracting%2520the%250Aorientation%2520of%2520guiding%2520strands%252C%2520especially%2520in%2520the%2520case%2520of%2520challenging%250Ahairstyles.%2520To%2520reconstruct%2520the%2520hair%2520purely%2520from%2520the%2520observed%2520geometry%252C%2520our%250Amethod%2520finds%2520sharp%2520surface%2520features%2520directly%2520on%2520the%2520scan%2520and%2520estimates%2520strand%250Aorientation%2520through%2520a%2520neural%25202D%2520line%2520detector%2520applied%2520to%2520the%2520renderings%2520of%2520scan%250Ashading.%2520Additionally%252C%2520we%2520incorporate%2520a%2520diffusion%2520prior%2520trained%2520on%2520a%2520diverse%250Aset%2520of%2520synthetic%2520hair%2520scans%252C%2520refined%2520with%2520an%2520improved%2520noise%2520schedule%252C%2520and%250Aadapted%2520to%2520the%2520reconstructed%2520contents%2520via%2520a%2520scan-specific%2520text%2520prompt.%2520We%250Ademonstrate%2520that%2520this%2520combination%2520of%2520supervision%2520signals%2520enables%2520accurate%250Areconstruction%2520of%2520both%2520simple%2520and%2520intricate%2520hairstyles%2520without%2520relying%2520on%2520color%250Ainformation.%2520To%2520facilitate%2520further%2520research%252C%2520we%2520introduce%2520Strands400%252C%2520the%250Alargest%2520publicly%2520available%2520dataset%2520of%2520hair%2520strands%2520with%2520detailed%2520surface%250Ageometry%2520extracted%2520from%2520real-world%2520data%252C%2520which%2520contains%2520reconstructed%2520hair%250Astrands%2520from%2520the%2520scans%2520of%2520400%2520subjects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeomHair%3A%20Reconstruction%20of%20Hair%20Strands%20from%20Colorless%203D%20Scans&entry.906535625=Rachmadio%20Noval%20Lazuardi%20and%20Artem%20Sevastopolsky%20and%20Egor%20Zakharov%20and%20Matthias%20Niessner%20and%20Vanessa%20Sklyarova&entry.1292438233=%20%20We%20propose%20a%20novel%20method%20that%20reconstructs%20hair%20strands%20directly%20from%0Acolorless%203D%20scans%20by%20leveraging%20multi-modal%20hair%20orientation%20extraction.%20Hair%0Astrand%20reconstruction%20is%20a%20fundamental%20problem%20in%20computer%20vision%20and%20graphics%0Athat%20can%20be%20used%20for%20high-fidelity%20digital%20avatar%20synthesis%2C%20animation%2C%20and%0AAR/VR%20applications.%20However%2C%20accurately%20recovering%20hair%20strands%20from%20raw%20scan%0Adata%20remains%20challenging%20due%20to%20human%20hair%27s%20complex%20and%20fine-grained%0Astructure.%20Existing%20methods%20typically%20rely%20on%20RGB%20captures%2C%20which%20can%20be%0Asensitive%20to%20the%20environment%20and%20can%20be%20a%20challenging%20domain%20for%20extracting%20the%0Aorientation%20of%20guiding%20strands%2C%20especially%20in%20the%20case%20of%20challenging%0Ahairstyles.%20To%20reconstruct%20the%20hair%20purely%20from%20the%20observed%20geometry%2C%20our%0Amethod%20finds%20sharp%20surface%20features%20directly%20on%20the%20scan%20and%20estimates%20strand%0Aorientation%20through%20a%20neural%202D%20line%20detector%20applied%20to%20the%20renderings%20of%20scan%0Ashading.%20Additionally%2C%20we%20incorporate%20a%20diffusion%20prior%20trained%20on%20a%20diverse%0Aset%20of%20synthetic%20hair%20scans%2C%20refined%20with%20an%20improved%20noise%20schedule%2C%20and%0Aadapted%20to%20the%20reconstructed%20contents%20via%20a%20scan-specific%20text%20prompt.%20We%0Ademonstrate%20that%20this%20combination%20of%20supervision%20signals%20enables%20accurate%0Areconstruction%20of%20both%20simple%20and%20intricate%20hairstyles%20without%20relying%20on%20color%0Ainformation.%20To%20facilitate%20further%20research%2C%20we%20introduce%20Strands400%2C%20the%0Alargest%20publicly%20available%20dataset%20of%20hair%20strands%20with%20detailed%20surface%0Ageometry%20extracted%20from%20real-world%20data%2C%20which%20contains%20reconstructed%20hair%0Astrands%20from%20the%20scans%20of%20400%20subjects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05376v1&entry.124074799=Read"},
{"title": "Does CLIP perceive art the same way we do?", "author": "Andrea Asperti and Leonardo Dess\u00ec and Maria Chiara Tonetti and Nico Wu", "abstract": "  CLIP has emerged as a powerful multimodal model capable of connecting images\nand text through joint embeddings, but to what extent does it \"see\" the same\nway humans do - especially when interpreting artworks? In this paper, we\ninvestigate CLIP's ability to extract high-level semantic and stylistic\ninformation from paintings, including both human-created and AI-generated\nimagery. We evaluate its perception across multiple dimensions: content, scene\nunderstanding, artistic style, historical period, and the presence of visual\ndeformations or artifacts. By designing targeted probing tasks and comparing\nCLIP's responses to human annotations and expert benchmarks, we explore its\nalignment with human perceptual and contextual understanding. Our findings\nreveal both strengths and limitations in CLIP's visual representations,\nparticularly in relation to aesthetic cues and artistic intent. We further\ndiscuss the implications of these insights for using CLIP as a guidance\nmechanism during generative processes, such as style transfer or prompt-based\nimage synthesis. Our work highlights the need for deeper interpretability in\nmultimodal systems, especially when applied to creative domains where nuance\nand subjectivity play a central role.\n", "link": "http://arxiv.org/abs/2505.05229v1", "date": "2025-05-08", "relevancy": 2.8331, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20CLIP%20perceive%20art%20the%20same%20way%20we%20do%3F&body=Title%3A%20Does%20CLIP%20perceive%20art%20the%20same%20way%20we%20do%3F%0AAuthor%3A%20Andrea%20Asperti%20and%20Leonardo%20Dess%C3%AC%20and%20Maria%20Chiara%20Tonetti%20and%20Nico%20Wu%0AAbstract%3A%20%20%20CLIP%20has%20emerged%20as%20a%20powerful%20multimodal%20model%20capable%20of%20connecting%20images%0Aand%20text%20through%20joint%20embeddings%2C%20but%20to%20what%20extent%20does%20it%20%22see%22%20the%20same%0Away%20humans%20do%20-%20especially%20when%20interpreting%20artworks%3F%20In%20this%20paper%2C%20we%0Ainvestigate%20CLIP%27s%20ability%20to%20extract%20high-level%20semantic%20and%20stylistic%0Ainformation%20from%20paintings%2C%20including%20both%20human-created%20and%20AI-generated%0Aimagery.%20We%20evaluate%20its%20perception%20across%20multiple%20dimensions%3A%20content%2C%20scene%0Aunderstanding%2C%20artistic%20style%2C%20historical%20period%2C%20and%20the%20presence%20of%20visual%0Adeformations%20or%20artifacts.%20By%20designing%20targeted%20probing%20tasks%20and%20comparing%0ACLIP%27s%20responses%20to%20human%20annotations%20and%20expert%20benchmarks%2C%20we%20explore%20its%0Aalignment%20with%20human%20perceptual%20and%20contextual%20understanding.%20Our%20findings%0Areveal%20both%20strengths%20and%20limitations%20in%20CLIP%27s%20visual%20representations%2C%0Aparticularly%20in%20relation%20to%20aesthetic%20cues%20and%20artistic%20intent.%20We%20further%0Adiscuss%20the%20implications%20of%20these%20insights%20for%20using%20CLIP%20as%20a%20guidance%0Amechanism%20during%20generative%20processes%2C%20such%20as%20style%20transfer%20or%20prompt-based%0Aimage%20synthesis.%20Our%20work%20highlights%20the%20need%20for%20deeper%20interpretability%20in%0Amultimodal%20systems%2C%20especially%20when%20applied%20to%20creative%20domains%20where%20nuance%0Aand%20subjectivity%20play%20a%20central%20role.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520CLIP%2520perceive%2520art%2520the%2520same%2520way%2520we%2520do%253F%26entry.906535625%3DAndrea%2520Asperti%2520and%2520Leonardo%2520Dess%25C3%25AC%2520and%2520Maria%2520Chiara%2520Tonetti%2520and%2520Nico%2520Wu%26entry.1292438233%3D%2520%2520CLIP%2520has%2520emerged%2520as%2520a%2520powerful%2520multimodal%2520model%2520capable%2520of%2520connecting%2520images%250Aand%2520text%2520through%2520joint%2520embeddings%252C%2520but%2520to%2520what%2520extent%2520does%2520it%2520%2522see%2522%2520the%2520same%250Away%2520humans%2520do%2520-%2520especially%2520when%2520interpreting%2520artworks%253F%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520CLIP%2527s%2520ability%2520to%2520extract%2520high-level%2520semantic%2520and%2520stylistic%250Ainformation%2520from%2520paintings%252C%2520including%2520both%2520human-created%2520and%2520AI-generated%250Aimagery.%2520We%2520evaluate%2520its%2520perception%2520across%2520multiple%2520dimensions%253A%2520content%252C%2520scene%250Aunderstanding%252C%2520artistic%2520style%252C%2520historical%2520period%252C%2520and%2520the%2520presence%2520of%2520visual%250Adeformations%2520or%2520artifacts.%2520By%2520designing%2520targeted%2520probing%2520tasks%2520and%2520comparing%250ACLIP%2527s%2520responses%2520to%2520human%2520annotations%2520and%2520expert%2520benchmarks%252C%2520we%2520explore%2520its%250Aalignment%2520with%2520human%2520perceptual%2520and%2520contextual%2520understanding.%2520Our%2520findings%250Areveal%2520both%2520strengths%2520and%2520limitations%2520in%2520CLIP%2527s%2520visual%2520representations%252C%250Aparticularly%2520in%2520relation%2520to%2520aesthetic%2520cues%2520and%2520artistic%2520intent.%2520We%2520further%250Adiscuss%2520the%2520implications%2520of%2520these%2520insights%2520for%2520using%2520CLIP%2520as%2520a%2520guidance%250Amechanism%2520during%2520generative%2520processes%252C%2520such%2520as%2520style%2520transfer%2520or%2520prompt-based%250Aimage%2520synthesis.%2520Our%2520work%2520highlights%2520the%2520need%2520for%2520deeper%2520interpretability%2520in%250Amultimodal%2520systems%252C%2520especially%2520when%2520applied%2520to%2520creative%2520domains%2520where%2520nuance%250Aand%2520subjectivity%2520play%2520a%2520central%2520role.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20CLIP%20perceive%20art%20the%20same%20way%20we%20do%3F&entry.906535625=Andrea%20Asperti%20and%20Leonardo%20Dess%C3%AC%20and%20Maria%20Chiara%20Tonetti%20and%20Nico%20Wu&entry.1292438233=%20%20CLIP%20has%20emerged%20as%20a%20powerful%20multimodal%20model%20capable%20of%20connecting%20images%0Aand%20text%20through%20joint%20embeddings%2C%20but%20to%20what%20extent%20does%20it%20%22see%22%20the%20same%0Away%20humans%20do%20-%20especially%20when%20interpreting%20artworks%3F%20In%20this%20paper%2C%20we%0Ainvestigate%20CLIP%27s%20ability%20to%20extract%20high-level%20semantic%20and%20stylistic%0Ainformation%20from%20paintings%2C%20including%20both%20human-created%20and%20AI-generated%0Aimagery.%20We%20evaluate%20its%20perception%20across%20multiple%20dimensions%3A%20content%2C%20scene%0Aunderstanding%2C%20artistic%20style%2C%20historical%20period%2C%20and%20the%20presence%20of%20visual%0Adeformations%20or%20artifacts.%20By%20designing%20targeted%20probing%20tasks%20and%20comparing%0ACLIP%27s%20responses%20to%20human%20annotations%20and%20expert%20benchmarks%2C%20we%20explore%20its%0Aalignment%20with%20human%20perceptual%20and%20contextual%20understanding.%20Our%20findings%0Areveal%20both%20strengths%20and%20limitations%20in%20CLIP%27s%20visual%20representations%2C%0Aparticularly%20in%20relation%20to%20aesthetic%20cues%20and%20artistic%20intent.%20We%20further%0Adiscuss%20the%20implications%20of%20these%20insights%20for%20using%20CLIP%20as%20a%20guidance%0Amechanism%20during%20generative%20processes%2C%20such%20as%20style%20transfer%20or%20prompt-based%0Aimage%20synthesis.%20Our%20work%20highlights%20the%20need%20for%20deeper%20interpretability%20in%0Amultimodal%20systems%2C%20especially%20when%20applied%20to%20creative%20domains%20where%20nuance%0Aand%20subjectivity%20play%20a%20central%20role.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05229v1&entry.124074799=Read"},
{"title": "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and\n  Generation", "author": "Haokun Lin and Teng Wang and Yixiao Ge and Yuying Ge and Zhichao Lu and Ying Wei and Qingfu Zhang and Zhenan Sun and Ying Shan", "abstract": "  Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP.\n", "link": "http://arxiv.org/abs/2505.05422v1", "date": "2025-05-08", "relevancy": 2.7807, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5965}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5399}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokLIP%3A%20Marry%20Visual%20Tokens%20to%20CLIP%20for%20Multimodal%20Comprehension%20and%0A%20%20Generation&body=Title%3A%20TokLIP%3A%20Marry%20Visual%20Tokens%20to%20CLIP%20for%20Multimodal%20Comprehension%20and%0A%20%20Generation%0AAuthor%3A%20Haokun%20Lin%20and%20Teng%20Wang%20and%20Yixiao%20Ge%20and%20Yuying%20Ge%20and%20Zhichao%20Lu%20and%20Ying%20Wei%20and%20Qingfu%20Zhang%20and%20Zhenan%20Sun%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Pioneering%20token-based%20works%20such%20as%20Chameleon%20and%20Emu3%20have%20established%20a%0Afoundation%20for%20multimodal%20unification%20but%20face%20challenges%20of%20high%20training%0Acomputational%20overhead%20and%20limited%20comprehension%20performance%20due%20to%20a%20lack%20of%0Ahigh-level%20semantics.%20In%20this%20paper%2C%20we%20introduce%20TokLIP%2C%20a%20visual%20tokenizer%0Athat%20enhances%20comprehension%20by%20semanticizing%20vector-quantized%20%28VQ%29%20tokens%20and%0Aincorporating%20CLIP-level%20semantics%20while%20enabling%20end-to-end%20multimodal%0Aautoregressive%20training%20with%20standard%20VQ%20tokens.%20TokLIP%20integrates%20a%20low-level%0Adiscrete%20VQ%20tokenizer%20with%20a%20ViT-based%20token%20encoder%20to%20capture%20high-level%0Acontinuous%20semantics.%20Unlike%20previous%20approaches%20%28e.g.%2C%20VILA-U%29%20that%20discretize%0Ahigh-level%20features%2C%20TokLIP%20disentangles%20training%20objectives%20for%20comprehension%0Aand%20generation%2C%20allowing%20the%20direct%20application%20of%20advanced%20VQ%20tokenizers%0Awithout%20the%20need%20for%20tailored%20quantization%20operations.%20Our%20empirical%20results%0Ademonstrate%20that%20TokLIP%20achieves%20exceptional%20data%20efficiency%2C%20empowering%20visual%0Atokens%20with%20high-level%20semantic%20understanding%20while%20enhancing%20low-level%0Agenerative%20capacity%2C%20making%20it%20well-suited%20for%20autoregressive%20Transformers%20in%0Aboth%20comprehension%20and%20generation%20tasks.%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/TencentARC/TokLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokLIP%253A%2520Marry%2520Visual%2520Tokens%2520to%2520CLIP%2520for%2520Multimodal%2520Comprehension%2520and%250A%2520%2520Generation%26entry.906535625%3DHaokun%2520Lin%2520and%2520Teng%2520Wang%2520and%2520Yixiao%2520Ge%2520and%2520Yuying%2520Ge%2520and%2520Zhichao%2520Lu%2520and%2520Ying%2520Wei%2520and%2520Qingfu%2520Zhang%2520and%2520Zhenan%2520Sun%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Pioneering%2520token-based%2520works%2520such%2520as%2520Chameleon%2520and%2520Emu3%2520have%2520established%2520a%250Afoundation%2520for%2520multimodal%2520unification%2520but%2520face%2520challenges%2520of%2520high%2520training%250Acomputational%2520overhead%2520and%2520limited%2520comprehension%2520performance%2520due%2520to%2520a%2520lack%2520of%250Ahigh-level%2520semantics.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520TokLIP%252C%2520a%2520visual%2520tokenizer%250Athat%2520enhances%2520comprehension%2520by%2520semanticizing%2520vector-quantized%2520%2528VQ%2529%2520tokens%2520and%250Aincorporating%2520CLIP-level%2520semantics%2520while%2520enabling%2520end-to-end%2520multimodal%250Aautoregressive%2520training%2520with%2520standard%2520VQ%2520tokens.%2520TokLIP%2520integrates%2520a%2520low-level%250Adiscrete%2520VQ%2520tokenizer%2520with%2520a%2520ViT-based%2520token%2520encoder%2520to%2520capture%2520high-level%250Acontinuous%2520semantics.%2520Unlike%2520previous%2520approaches%2520%2528e.g.%252C%2520VILA-U%2529%2520that%2520discretize%250Ahigh-level%2520features%252C%2520TokLIP%2520disentangles%2520training%2520objectives%2520for%2520comprehension%250Aand%2520generation%252C%2520allowing%2520the%2520direct%2520application%2520of%2520advanced%2520VQ%2520tokenizers%250Awithout%2520the%2520need%2520for%2520tailored%2520quantization%2520operations.%2520Our%2520empirical%2520results%250Ademonstrate%2520that%2520TokLIP%2520achieves%2520exceptional%2520data%2520efficiency%252C%2520empowering%2520visual%250Atokens%2520with%2520high-level%2520semantic%2520understanding%2520while%2520enhancing%2520low-level%250Agenerative%2520capacity%252C%2520making%2520it%2520well-suited%2520for%2520autoregressive%2520Transformers%2520in%250Aboth%2520comprehension%2520and%2520generation%2520tasks.%2520The%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/TencentARC/TokLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokLIP%3A%20Marry%20Visual%20Tokens%20to%20CLIP%20for%20Multimodal%20Comprehension%20and%0A%20%20Generation&entry.906535625=Haokun%20Lin%20and%20Teng%20Wang%20and%20Yixiao%20Ge%20and%20Yuying%20Ge%20and%20Zhichao%20Lu%20and%20Ying%20Wei%20and%20Qingfu%20Zhang%20and%20Zhenan%20Sun%20and%20Ying%20Shan&entry.1292438233=%20%20Pioneering%20token-based%20works%20such%20as%20Chameleon%20and%20Emu3%20have%20established%20a%0Afoundation%20for%20multimodal%20unification%20but%20face%20challenges%20of%20high%20training%0Acomputational%20overhead%20and%20limited%20comprehension%20performance%20due%20to%20a%20lack%20of%0Ahigh-level%20semantics.%20In%20this%20paper%2C%20we%20introduce%20TokLIP%2C%20a%20visual%20tokenizer%0Athat%20enhances%20comprehension%20by%20semanticizing%20vector-quantized%20%28VQ%29%20tokens%20and%0Aincorporating%20CLIP-level%20semantics%20while%20enabling%20end-to-end%20multimodal%0Aautoregressive%20training%20with%20standard%20VQ%20tokens.%20TokLIP%20integrates%20a%20low-level%0Adiscrete%20VQ%20tokenizer%20with%20a%20ViT-based%20token%20encoder%20to%20capture%20high-level%0Acontinuous%20semantics.%20Unlike%20previous%20approaches%20%28e.g.%2C%20VILA-U%29%20that%20discretize%0Ahigh-level%20features%2C%20TokLIP%20disentangles%20training%20objectives%20for%20comprehension%0Aand%20generation%2C%20allowing%20the%20direct%20application%20of%20advanced%20VQ%20tokenizers%0Awithout%20the%20need%20for%20tailored%20quantization%20operations.%20Our%20empirical%20results%0Ademonstrate%20that%20TokLIP%20achieves%20exceptional%20data%20efficiency%2C%20empowering%20visual%0Atokens%20with%20high-level%20semantic%20understanding%20while%20enhancing%20low-level%0Agenerative%20capacity%2C%20making%20it%20well-suited%20for%20autoregressive%20Transformers%20in%0Aboth%20comprehension%20and%20generation%20tasks.%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/TencentARC/TokLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05422v1&entry.124074799=Read"},
{"title": "Generating Physically Stable and Buildable LEGO Designs from Text", "author": "Ava Pun and Kangle Deng and Ruixuan Liu and Deva Ramanan and Changliu Liu and Jun-Yan Zhu", "abstract": "  We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.\n", "link": "http://arxiv.org/abs/2505.05469v1", "date": "2025-05-08", "relevancy": 2.7329, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6062}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5393}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Physically%20Stable%20and%20Buildable%20LEGO%20Designs%20from%20Text&body=Title%3A%20Generating%20Physically%20Stable%20and%20Buildable%20LEGO%20Designs%20from%20Text%0AAuthor%3A%20Ava%20Pun%20and%20Kangle%20Deng%20and%20Ruixuan%20Liu%20and%20Deva%20Ramanan%20and%20Changliu%20Liu%20and%20Jun-Yan%20Zhu%0AAbstract%3A%20%20%20We%20introduce%20LegoGPT%2C%20the%20first%20approach%20for%20generating%20physically%20stable%0ALEGO%20brick%20models%20from%20text%20prompts.%20To%20achieve%20this%2C%20we%20construct%20a%0Alarge-scale%2C%20physically%20stable%20dataset%20of%20LEGO%20designs%2C%20along%20with%20their%0Aassociated%20captions%2C%20and%20train%20an%20autoregressive%20large%20language%20model%20to%0Apredict%20the%20next%20brick%20to%20add%20via%20next-token%20prediction.%20To%20improve%20the%0Astability%20of%20the%20resulting%20designs%2C%20we%20employ%20an%20efficient%20validity%20check%20and%0Aphysics-aware%20rollback%20during%20autoregressive%20inference%2C%20which%20prunes%20infeasible%0Atoken%20predictions%20using%20physics%20laws%20and%20assembly%20constraints.%20Our%20experiments%0Ashow%20that%20LegoGPT%20produces%20stable%2C%20diverse%2C%20and%20aesthetically%20pleasing%20LEGO%0Adesigns%20that%20align%20closely%20with%20the%20input%20text%20prompts.%20We%20also%20develop%20a%0Atext-based%20LEGO%20texturing%20method%20to%20generate%20colored%20and%20textured%20designs.%20We%0Ashow%20that%20our%20designs%20can%20be%20assembled%20manually%20by%20humans%20and%20automatically%20by%0Arobotic%20arms.%20We%20also%20release%20our%20new%20dataset%2C%20StableText2Lego%2C%20containing%20over%0A47%2C000%20LEGO%20structures%20of%20over%2028%2C000%20unique%203D%20objects%20accompanied%20by%20detailed%0Acaptions%2C%20along%20with%20our%20code%20and%20models%20at%20the%20project%20website%3A%0Ahttps%3A//avalovelace1.github.io/LegoGPT/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Physically%2520Stable%2520and%2520Buildable%2520LEGO%2520Designs%2520from%2520Text%26entry.906535625%3DAva%2520Pun%2520and%2520Kangle%2520Deng%2520and%2520Ruixuan%2520Liu%2520and%2520Deva%2520Ramanan%2520and%2520Changliu%2520Liu%2520and%2520Jun-Yan%2520Zhu%26entry.1292438233%3D%2520%2520We%2520introduce%2520LegoGPT%252C%2520the%2520first%2520approach%2520for%2520generating%2520physically%2520stable%250ALEGO%2520brick%2520models%2520from%2520text%2520prompts.%2520To%2520achieve%2520this%252C%2520we%2520construct%2520a%250Alarge-scale%252C%2520physically%2520stable%2520dataset%2520of%2520LEGO%2520designs%252C%2520along%2520with%2520their%250Aassociated%2520captions%252C%2520and%2520train%2520an%2520autoregressive%2520large%2520language%2520model%2520to%250Apredict%2520the%2520next%2520brick%2520to%2520add%2520via%2520next-token%2520prediction.%2520To%2520improve%2520the%250Astability%2520of%2520the%2520resulting%2520designs%252C%2520we%2520employ%2520an%2520efficient%2520validity%2520check%2520and%250Aphysics-aware%2520rollback%2520during%2520autoregressive%2520inference%252C%2520which%2520prunes%2520infeasible%250Atoken%2520predictions%2520using%2520physics%2520laws%2520and%2520assembly%2520constraints.%2520Our%2520experiments%250Ashow%2520that%2520LegoGPT%2520produces%2520stable%252C%2520diverse%252C%2520and%2520aesthetically%2520pleasing%2520LEGO%250Adesigns%2520that%2520align%2520closely%2520with%2520the%2520input%2520text%2520prompts.%2520We%2520also%2520develop%2520a%250Atext-based%2520LEGO%2520texturing%2520method%2520to%2520generate%2520colored%2520and%2520textured%2520designs.%2520We%250Ashow%2520that%2520our%2520designs%2520can%2520be%2520assembled%2520manually%2520by%2520humans%2520and%2520automatically%2520by%250Arobotic%2520arms.%2520We%2520also%2520release%2520our%2520new%2520dataset%252C%2520StableText2Lego%252C%2520containing%2520over%250A47%252C000%2520LEGO%2520structures%2520of%2520over%252028%252C000%2520unique%25203D%2520objects%2520accompanied%2520by%2520detailed%250Acaptions%252C%2520along%2520with%2520our%2520code%2520and%2520models%2520at%2520the%2520project%2520website%253A%250Ahttps%253A//avalovelace1.github.io/LegoGPT/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Physically%20Stable%20and%20Buildable%20LEGO%20Designs%20from%20Text&entry.906535625=Ava%20Pun%20and%20Kangle%20Deng%20and%20Ruixuan%20Liu%20and%20Deva%20Ramanan%20and%20Changliu%20Liu%20and%20Jun-Yan%20Zhu&entry.1292438233=%20%20We%20introduce%20LegoGPT%2C%20the%20first%20approach%20for%20generating%20physically%20stable%0ALEGO%20brick%20models%20from%20text%20prompts.%20To%20achieve%20this%2C%20we%20construct%20a%0Alarge-scale%2C%20physically%20stable%20dataset%20of%20LEGO%20designs%2C%20along%20with%20their%0Aassociated%20captions%2C%20and%20train%20an%20autoregressive%20large%20language%20model%20to%0Apredict%20the%20next%20brick%20to%20add%20via%20next-token%20prediction.%20To%20improve%20the%0Astability%20of%20the%20resulting%20designs%2C%20we%20employ%20an%20efficient%20validity%20check%20and%0Aphysics-aware%20rollback%20during%20autoregressive%20inference%2C%20which%20prunes%20infeasible%0Atoken%20predictions%20using%20physics%20laws%20and%20assembly%20constraints.%20Our%20experiments%0Ashow%20that%20LegoGPT%20produces%20stable%2C%20diverse%2C%20and%20aesthetically%20pleasing%20LEGO%0Adesigns%20that%20align%20closely%20with%20the%20input%20text%20prompts.%20We%20also%20develop%20a%0Atext-based%20LEGO%20texturing%20method%20to%20generate%20colored%20and%20textured%20designs.%20We%0Ashow%20that%20our%20designs%20can%20be%20assembled%20manually%20by%20humans%20and%20automatically%20by%0Arobotic%20arms.%20We%20also%20release%20our%20new%20dataset%2C%20StableText2Lego%2C%20containing%20over%0A47%2C000%20LEGO%20structures%20of%20over%2028%2C000%20unique%203D%20objects%20accompanied%20by%20detailed%0Acaptions%2C%20along%20with%20our%20code%20and%20models%20at%20the%20project%20website%3A%0Ahttps%3A//avalovelace1.github.io/LegoGPT/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05469v1&entry.124074799=Read"},
{"title": "Mapping User Trust in Vision Language Models: Research Landscape,\n  Challenges, and Prospects", "author": "Agnese Chiatti and Sara Bernardini and Lara Shibelski Godoy Piccolo and Viola Schiaffonati and Matteo Matteucci", "abstract": "  The rapid adoption of Vision Language Models (VLMs), pre-trained on large\nimage-text and video-text datasets, calls for protecting and informing users\nabout when to trust these systems. This survey reviews studies on trust\ndynamics in user-VLM interactions, through a multi-disciplinary taxonomy\nencompassing different cognitive science capabilities, collaboration modes, and\nagent behaviours. Literature insights and findings from a workshop with\nprospective VLM users inform preliminary requirements for future VLM trust\nstudies.\n", "link": "http://arxiv.org/abs/2505.05318v1", "date": "2025-05-08", "relevancy": 2.681, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5372}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20User%20Trust%20in%20Vision%20Language%20Models%3A%20Research%20Landscape%2C%0A%20%20Challenges%2C%20and%20Prospects&body=Title%3A%20Mapping%20User%20Trust%20in%20Vision%20Language%20Models%3A%20Research%20Landscape%2C%0A%20%20Challenges%2C%20and%20Prospects%0AAuthor%3A%20Agnese%20Chiatti%20and%20Sara%20Bernardini%20and%20Lara%20Shibelski%20Godoy%20Piccolo%20and%20Viola%20Schiaffonati%20and%20Matteo%20Matteucci%0AAbstract%3A%20%20%20The%20rapid%20adoption%20of%20Vision%20Language%20Models%20%28VLMs%29%2C%20pre-trained%20on%20large%0Aimage-text%20and%20video-text%20datasets%2C%20calls%20for%20protecting%20and%20informing%20users%0Aabout%20when%20to%20trust%20these%20systems.%20This%20survey%20reviews%20studies%20on%20trust%0Adynamics%20in%20user-VLM%20interactions%2C%20through%20a%20multi-disciplinary%20taxonomy%0Aencompassing%20different%20cognitive%20science%20capabilities%2C%20collaboration%20modes%2C%20and%0Aagent%20behaviours.%20Literature%20insights%20and%20findings%20from%20a%20workshop%20with%0Aprospective%20VLM%20users%20inform%20preliminary%20requirements%20for%20future%20VLM%20trust%0Astudies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520User%2520Trust%2520in%2520Vision%2520Language%2520Models%253A%2520Research%2520Landscape%252C%250A%2520%2520Challenges%252C%2520and%2520Prospects%26entry.906535625%3DAgnese%2520Chiatti%2520and%2520Sara%2520Bernardini%2520and%2520Lara%2520Shibelski%2520Godoy%2520Piccolo%2520and%2520Viola%2520Schiaffonati%2520and%2520Matteo%2520Matteucci%26entry.1292438233%3D%2520%2520The%2520rapid%2520adoption%2520of%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520pre-trained%2520on%2520large%250Aimage-text%2520and%2520video-text%2520datasets%252C%2520calls%2520for%2520protecting%2520and%2520informing%2520users%250Aabout%2520when%2520to%2520trust%2520these%2520systems.%2520This%2520survey%2520reviews%2520studies%2520on%2520trust%250Adynamics%2520in%2520user-VLM%2520interactions%252C%2520through%2520a%2520multi-disciplinary%2520taxonomy%250Aencompassing%2520different%2520cognitive%2520science%2520capabilities%252C%2520collaboration%2520modes%252C%2520and%250Aagent%2520behaviours.%2520Literature%2520insights%2520and%2520findings%2520from%2520a%2520workshop%2520with%250Aprospective%2520VLM%2520users%2520inform%2520preliminary%2520requirements%2520for%2520future%2520VLM%2520trust%250Astudies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20User%20Trust%20in%20Vision%20Language%20Models%3A%20Research%20Landscape%2C%0A%20%20Challenges%2C%20and%20Prospects&entry.906535625=Agnese%20Chiatti%20and%20Sara%20Bernardini%20and%20Lara%20Shibelski%20Godoy%20Piccolo%20and%20Viola%20Schiaffonati%20and%20Matteo%20Matteucci&entry.1292438233=%20%20The%20rapid%20adoption%20of%20Vision%20Language%20Models%20%28VLMs%29%2C%20pre-trained%20on%20large%0Aimage-text%20and%20video-text%20datasets%2C%20calls%20for%20protecting%20and%20informing%20users%0Aabout%20when%20to%20trust%20these%20systems.%20This%20survey%20reviews%20studies%20on%20trust%0Adynamics%20in%20user-VLM%20interactions%2C%20through%20a%20multi-disciplinary%20taxonomy%0Aencompassing%20different%20cognitive%20science%20capabilities%2C%20collaboration%20modes%2C%20and%0Aagent%20behaviours.%20Literature%20insights%20and%20findings%20from%20a%20workshop%20with%0Aprospective%20VLM%20users%20inform%20preliminary%20requirements%20for%20future%20VLM%20trust%0Astudies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05318v1&entry.124074799=Read"},
{"title": "Benchmarking Ophthalmology Foundation Models for Clinically Significant\n  Age Macular Degeneration Detection", "author": "Benjamin A. Cohen and Jonathan Fhima and Meishar Meisel and Baskin Meital and Luis Filipe Nakayama and Eran Berkowitz and Joachim A. Behar", "abstract": "  Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to\nlearn robust representations from large-scale natural image datasets, enhancing\ntheir generalization across domains. In retinal imaging, foundation models\npretrained on either natural or ophthalmic data have shown promise, but the\nbenefits of in-domain pretraining remain uncertain. To investigate this, we\nbenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets\ntotaling 70,000 expert-annotated images for the task of moderate-to-late\nage-related macular degeneration (AMD) identification. Our results show that\niBOT pretrained on natural images achieves the highest out-of-distribution\ngeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,\nwhich achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,\nwhich achieved AUROCs of 0.68-0.91. These findings highlight the value of\nfoundation models in improving AMD identification and challenge the assumption\nthat in-domain pretraining is necessary. Furthermore, we release BRAMD, an\nopen-access dataset (n=587) of DFIs with AMD labels from Brazil.\n", "link": "http://arxiv.org/abs/2505.05291v1", "date": "2025-05-08", "relevancy": 2.6649, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Ophthalmology%20Foundation%20Models%20for%20Clinically%20Significant%0A%20%20Age%20Macular%20Degeneration%20Detection&body=Title%3A%20Benchmarking%20Ophthalmology%20Foundation%20Models%20for%20Clinically%20Significant%0A%20%20Age%20Macular%20Degeneration%20Detection%0AAuthor%3A%20Benjamin%20A.%20Cohen%20and%20Jonathan%20Fhima%20and%20Meishar%20Meisel%20and%20Baskin%20Meital%20and%20Luis%20Filipe%20Nakayama%20and%20Eran%20Berkowitz%20and%20Joachim%20A.%20Behar%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20enabled%20Vision%20Transformers%20%28ViTs%29%20to%0Alearn%20robust%20representations%20from%20large-scale%20natural%20image%20datasets%2C%20enhancing%0Atheir%20generalization%20across%20domains.%20In%20retinal%20imaging%2C%20foundation%20models%0Apretrained%20on%20either%20natural%20or%20ophthalmic%20data%20have%20shown%20promise%2C%20but%20the%0Abenefits%20of%20in-domain%20pretraining%20remain%20uncertain.%20To%20investigate%20this%2C%20we%0Abenchmark%20six%20SSL-pretrained%20ViTs%20on%20seven%20digital%20fundus%20image%20%28DFI%29%20datasets%0Atotaling%2070%2C000%20expert-annotated%20images%20for%20the%20task%20of%20moderate-to-late%0Aage-related%20macular%20degeneration%20%28AMD%29%20identification.%20Our%20results%20show%20that%0AiBOT%20pretrained%20on%20natural%20images%20achieves%20the%20highest%20out-of-distribution%0Ageneralization%2C%20with%20AUROCs%20of%200.80-0.97%2C%20outperforming%20domain-specific%20models%2C%0Awhich%20achieved%20AUROCs%20of%200.78-0.96%20and%20a%20baseline%20ViT-L%20with%20no%20pretraining%2C%0Awhich%20achieved%20AUROCs%20of%200.68-0.91.%20These%20findings%20highlight%20the%20value%20of%0Afoundation%20models%20in%20improving%20AMD%20identification%20and%20challenge%20the%20assumption%0Athat%20in-domain%20pretraining%20is%20necessary.%20Furthermore%2C%20we%20release%20BRAMD%2C%20an%0Aopen-access%20dataset%20%28n%3D587%29%20of%20DFIs%20with%20AMD%20labels%20from%20Brazil.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Ophthalmology%2520Foundation%2520Models%2520for%2520Clinically%2520Significant%250A%2520%2520Age%2520Macular%2520Degeneration%2520Detection%26entry.906535625%3DBenjamin%2520A.%2520Cohen%2520and%2520Jonathan%2520Fhima%2520and%2520Meishar%2520Meisel%2520and%2520Baskin%2520Meital%2520and%2520Luis%2520Filipe%2520Nakayama%2520and%2520Eran%2520Berkowitz%2520and%2520Joachim%2520A.%2520Behar%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520enabled%2520Vision%2520Transformers%2520%2528ViTs%2529%2520to%250Alearn%2520robust%2520representations%2520from%2520large-scale%2520natural%2520image%2520datasets%252C%2520enhancing%250Atheir%2520generalization%2520across%2520domains.%2520In%2520retinal%2520imaging%252C%2520foundation%2520models%250Apretrained%2520on%2520either%2520natural%2520or%2520ophthalmic%2520data%2520have%2520shown%2520promise%252C%2520but%2520the%250Abenefits%2520of%2520in-domain%2520pretraining%2520remain%2520uncertain.%2520To%2520investigate%2520this%252C%2520we%250Abenchmark%2520six%2520SSL-pretrained%2520ViTs%2520on%2520seven%2520digital%2520fundus%2520image%2520%2528DFI%2529%2520datasets%250Atotaling%252070%252C000%2520expert-annotated%2520images%2520for%2520the%2520task%2520of%2520moderate-to-late%250Aage-related%2520macular%2520degeneration%2520%2528AMD%2529%2520identification.%2520Our%2520results%2520show%2520that%250AiBOT%2520pretrained%2520on%2520natural%2520images%2520achieves%2520the%2520highest%2520out-of-distribution%250Ageneralization%252C%2520with%2520AUROCs%2520of%25200.80-0.97%252C%2520outperforming%2520domain-specific%2520models%252C%250Awhich%2520achieved%2520AUROCs%2520of%25200.78-0.96%2520and%2520a%2520baseline%2520ViT-L%2520with%2520no%2520pretraining%252C%250Awhich%2520achieved%2520AUROCs%2520of%25200.68-0.91.%2520These%2520findings%2520highlight%2520the%2520value%2520of%250Afoundation%2520models%2520in%2520improving%2520AMD%2520identification%2520and%2520challenge%2520the%2520assumption%250Athat%2520in-domain%2520pretraining%2520is%2520necessary.%2520Furthermore%252C%2520we%2520release%2520BRAMD%252C%2520an%250Aopen-access%2520dataset%2520%2528n%253D587%2529%2520of%2520DFIs%2520with%2520AMD%2520labels%2520from%2520Brazil.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Ophthalmology%20Foundation%20Models%20for%20Clinically%20Significant%0A%20%20Age%20Macular%20Degeneration%20Detection&entry.906535625=Benjamin%20A.%20Cohen%20and%20Jonathan%20Fhima%20and%20Meishar%20Meisel%20and%20Baskin%20Meital%20and%20Luis%20Filipe%20Nakayama%20and%20Eran%20Berkowitz%20and%20Joachim%20A.%20Behar&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20enabled%20Vision%20Transformers%20%28ViTs%29%20to%0Alearn%20robust%20representations%20from%20large-scale%20natural%20image%20datasets%2C%20enhancing%0Atheir%20generalization%20across%20domains.%20In%20retinal%20imaging%2C%20foundation%20models%0Apretrained%20on%20either%20natural%20or%20ophthalmic%20data%20have%20shown%20promise%2C%20but%20the%0Abenefits%20of%20in-domain%20pretraining%20remain%20uncertain.%20To%20investigate%20this%2C%20we%0Abenchmark%20six%20SSL-pretrained%20ViTs%20on%20seven%20digital%20fundus%20image%20%28DFI%29%20datasets%0Atotaling%2070%2C000%20expert-annotated%20images%20for%20the%20task%20of%20moderate-to-late%0Aage-related%20macular%20degeneration%20%28AMD%29%20identification.%20Our%20results%20show%20that%0AiBOT%20pretrained%20on%20natural%20images%20achieves%20the%20highest%20out-of-distribution%0Ageneralization%2C%20with%20AUROCs%20of%200.80-0.97%2C%20outperforming%20domain-specific%20models%2C%0Awhich%20achieved%20AUROCs%20of%200.78-0.96%20and%20a%20baseline%20ViT-L%20with%20no%20pretraining%2C%0Awhich%20achieved%20AUROCs%20of%200.68-0.91.%20These%20findings%20highlight%20the%20value%20of%0Afoundation%20models%20in%20improving%20AMD%20identification%20and%20challenge%20the%20assumption%0Athat%20in-domain%20pretraining%20is%20necessary.%20Furthermore%2C%20we%20release%20BRAMD%2C%20an%0Aopen-access%20dataset%20%28n%3D587%29%20of%20DFIs%20with%20AMD%20labels%20from%20Brazil.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05291v1&entry.124074799=Read"},
{"title": "Understanding In-context Learning of Addition via Activation Subspaces", "author": "Xinyan Hu and Kayo Yin and Michael I. Jordan and Jacob Steinhardt and Lijie Chen", "abstract": "  To perform in-context learning, language models must extract signals from\nindividual few-shot examples, aggregate these into a learned prediction rule,\nand then apply this rule to new examples. How is this implemented in the\nforward pass of modern transformer models? To study this, we consider a\nstructured family of few-shot learning tasks for which the true prediction rule\nis to add an integer $k$ to the input. We find that Llama-3-8B attains high\naccuracy on this task for a range of $k$, and localize its few-shot ability to\njust three attention heads via a novel optimization approach. We further show\nthe extracted signals lie in a six-dimensional subspace, where four of the\ndimensions track the unit digit and the other two dimensions track overall\nmagnitude. We finally examine how these heads extract information from\nindividual few-shot examples, identifying a self-correction mechanism in which\nmistakes from earlier examples are suppressed by later examples. Our results\ndemonstrate how tracking low-dimensional subspaces across a forward pass can\nprovide insight into fine-grained computational structures.\n", "link": "http://arxiv.org/abs/2505.05145v1", "date": "2025-05-08", "relevancy": 2.6289, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20In-context%20Learning%20of%20Addition%20via%20Activation%20Subspaces&body=Title%3A%20Understanding%20In-context%20Learning%20of%20Addition%20via%20Activation%20Subspaces%0AAuthor%3A%20Xinyan%20Hu%20and%20Kayo%20Yin%20and%20Michael%20I.%20Jordan%20and%20Jacob%20Steinhardt%20and%20Lijie%20Chen%0AAbstract%3A%20%20%20To%20perform%20in-context%20learning%2C%20language%20models%20must%20extract%20signals%20from%0Aindividual%20few-shot%20examples%2C%20aggregate%20these%20into%20a%20learned%20prediction%20rule%2C%0Aand%20then%20apply%20this%20rule%20to%20new%20examples.%20How%20is%20this%20implemented%20in%20the%0Aforward%20pass%20of%20modern%20transformer%20models%3F%20To%20study%20this%2C%20we%20consider%20a%0Astructured%20family%20of%20few-shot%20learning%20tasks%20for%20which%20the%20true%20prediction%20rule%0Ais%20to%20add%20an%20integer%20%24k%24%20to%20the%20input.%20We%20find%20that%20Llama-3-8B%20attains%20high%0Aaccuracy%20on%20this%20task%20for%20a%20range%20of%20%24k%24%2C%20and%20localize%20its%20few-shot%20ability%20to%0Ajust%20three%20attention%20heads%20via%20a%20novel%20optimization%20approach.%20We%20further%20show%0Athe%20extracted%20signals%20lie%20in%20a%20six-dimensional%20subspace%2C%20where%20four%20of%20the%0Adimensions%20track%20the%20unit%20digit%20and%20the%20other%20two%20dimensions%20track%20overall%0Amagnitude.%20We%20finally%20examine%20how%20these%20heads%20extract%20information%20from%0Aindividual%20few-shot%20examples%2C%20identifying%20a%20self-correction%20mechanism%20in%20which%0Amistakes%20from%20earlier%20examples%20are%20suppressed%20by%20later%20examples.%20Our%20results%0Ademonstrate%20how%20tracking%20low-dimensional%20subspaces%20across%20a%20forward%20pass%20can%0Aprovide%20insight%20into%20fine-grained%20computational%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520In-context%2520Learning%2520of%2520Addition%2520via%2520Activation%2520Subspaces%26entry.906535625%3DXinyan%2520Hu%2520and%2520Kayo%2520Yin%2520and%2520Michael%2520I.%2520Jordan%2520and%2520Jacob%2520Steinhardt%2520and%2520Lijie%2520Chen%26entry.1292438233%3D%2520%2520To%2520perform%2520in-context%2520learning%252C%2520language%2520models%2520must%2520extract%2520signals%2520from%250Aindividual%2520few-shot%2520examples%252C%2520aggregate%2520these%2520into%2520a%2520learned%2520prediction%2520rule%252C%250Aand%2520then%2520apply%2520this%2520rule%2520to%2520new%2520examples.%2520How%2520is%2520this%2520implemented%2520in%2520the%250Aforward%2520pass%2520of%2520modern%2520transformer%2520models%253F%2520To%2520study%2520this%252C%2520we%2520consider%2520a%250Astructured%2520family%2520of%2520few-shot%2520learning%2520tasks%2520for%2520which%2520the%2520true%2520prediction%2520rule%250Ais%2520to%2520add%2520an%2520integer%2520%2524k%2524%2520to%2520the%2520input.%2520We%2520find%2520that%2520Llama-3-8B%2520attains%2520high%250Aaccuracy%2520on%2520this%2520task%2520for%2520a%2520range%2520of%2520%2524k%2524%252C%2520and%2520localize%2520its%2520few-shot%2520ability%2520to%250Ajust%2520three%2520attention%2520heads%2520via%2520a%2520novel%2520optimization%2520approach.%2520We%2520further%2520show%250Athe%2520extracted%2520signals%2520lie%2520in%2520a%2520six-dimensional%2520subspace%252C%2520where%2520four%2520of%2520the%250Adimensions%2520track%2520the%2520unit%2520digit%2520and%2520the%2520other%2520two%2520dimensions%2520track%2520overall%250Amagnitude.%2520We%2520finally%2520examine%2520how%2520these%2520heads%2520extract%2520information%2520from%250Aindividual%2520few-shot%2520examples%252C%2520identifying%2520a%2520self-correction%2520mechanism%2520in%2520which%250Amistakes%2520from%2520earlier%2520examples%2520are%2520suppressed%2520by%2520later%2520examples.%2520Our%2520results%250Ademonstrate%2520how%2520tracking%2520low-dimensional%2520subspaces%2520across%2520a%2520forward%2520pass%2520can%250Aprovide%2520insight%2520into%2520fine-grained%2520computational%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20In-context%20Learning%20of%20Addition%20via%20Activation%20Subspaces&entry.906535625=Xinyan%20Hu%20and%20Kayo%20Yin%20and%20Michael%20I.%20Jordan%20and%20Jacob%20Steinhardt%20and%20Lijie%20Chen&entry.1292438233=%20%20To%20perform%20in-context%20learning%2C%20language%20models%20must%20extract%20signals%20from%0Aindividual%20few-shot%20examples%2C%20aggregate%20these%20into%20a%20learned%20prediction%20rule%2C%0Aand%20then%20apply%20this%20rule%20to%20new%20examples.%20How%20is%20this%20implemented%20in%20the%0Aforward%20pass%20of%20modern%20transformer%20models%3F%20To%20study%20this%2C%20we%20consider%20a%0Astructured%20family%20of%20few-shot%20learning%20tasks%20for%20which%20the%20true%20prediction%20rule%0Ais%20to%20add%20an%20integer%20%24k%24%20to%20the%20input.%20We%20find%20that%20Llama-3-8B%20attains%20high%0Aaccuracy%20on%20this%20task%20for%20a%20range%20of%20%24k%24%2C%20and%20localize%20its%20few-shot%20ability%20to%0Ajust%20three%20attention%20heads%20via%20a%20novel%20optimization%20approach.%20We%20further%20show%0Athe%20extracted%20signals%20lie%20in%20a%20six-dimensional%20subspace%2C%20where%20four%20of%20the%0Adimensions%20track%20the%20unit%20digit%20and%20the%20other%20two%20dimensions%20track%20overall%0Amagnitude.%20We%20finally%20examine%20how%20these%20heads%20extract%20information%20from%0Aindividual%20few-shot%20examples%2C%20identifying%20a%20self-correction%20mechanism%20in%20which%0Amistakes%20from%20earlier%20examples%20are%20suppressed%20by%20later%20examples.%20Our%20results%0Ademonstrate%20how%20tracking%20low-dimensional%20subspaces%20across%20a%20forward%20pass%20can%0Aprovide%20insight%20into%20fine-grained%20computational%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05145v1&entry.124074799=Read"},
{"title": "Sparse Training from Random Initialization: Aligning Lottery Ticket\n  Masks using Weight Symmetry", "author": "Mohammed Adnan and Rohan Jain and Ekansh Sharma and Rahul Krishnan and Yani Ioannou", "abstract": "  The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask\nand weights that achieve the same generalization performance as the dense model\nwhile using significantly fewer parameters. However, finding a LTH solution is\ncomputationally expensive, and a LTH sparsity mask does not generalize to other\nrandom weight initializations. Recent work has suggested that neural networks\ntrained from random initialization find solutions within the same basin modulo\npermutation, and proposes a method to align trained models within the same loss\nbasin. We hypothesize that misalignment of basins is the reason why LTH masks\ndo not generalize to new random initializations and propose permuting the LTH\nmask to align with the new optimization basin when performing sparse training\nfrom a different random init. We empirically show a significant increase in\ngeneralization when sparse training from random initialization with the\npermuted mask as compared to using the non-permuted LTH mask, on multiple\ndatasets (CIFAR-10, CIFAR-100 and ImageNet) and models (VGG11, ResNet20 and\nResNet50).\n", "link": "http://arxiv.org/abs/2505.05143v1", "date": "2025-05-08", "relevancy": 2.5727, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5445}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5013}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Training%20from%20Random%20Initialization%3A%20Aligning%20Lottery%20Ticket%0A%20%20Masks%20using%20Weight%20Symmetry&body=Title%3A%20Sparse%20Training%20from%20Random%20Initialization%3A%20Aligning%20Lottery%20Ticket%0A%20%20Masks%20using%20Weight%20Symmetry%0AAuthor%3A%20Mohammed%20Adnan%20and%20Rohan%20Jain%20and%20Ekansh%20Sharma%20and%20Rahul%20Krishnan%20and%20Yani%20Ioannou%0AAbstract%3A%20%20%20The%20Lottery%20Ticket%20Hypothesis%20%28LTH%29%20suggests%20there%20exists%20a%20sparse%20LTH%20mask%0Aand%20weights%20that%20achieve%20the%20same%20generalization%20performance%20as%20the%20dense%20model%0Awhile%20using%20significantly%20fewer%20parameters.%20However%2C%20finding%20a%20LTH%20solution%20is%0Acomputationally%20expensive%2C%20and%20a%20LTH%20sparsity%20mask%20does%20not%20generalize%20to%20other%0Arandom%20weight%20initializations.%20Recent%20work%20has%20suggested%20that%20neural%20networks%0Atrained%20from%20random%20initialization%20find%20solutions%20within%20the%20same%20basin%20modulo%0Apermutation%2C%20and%20proposes%20a%20method%20to%20align%20trained%20models%20within%20the%20same%20loss%0Abasin.%20We%20hypothesize%20that%20misalignment%20of%20basins%20is%20the%20reason%20why%20LTH%20masks%0Ado%20not%20generalize%20to%20new%20random%20initializations%20and%20propose%20permuting%20the%20LTH%0Amask%20to%20align%20with%20the%20new%20optimization%20basin%20when%20performing%20sparse%20training%0Afrom%20a%20different%20random%20init.%20We%20empirically%20show%20a%20significant%20increase%20in%0Ageneralization%20when%20sparse%20training%20from%20random%20initialization%20with%20the%0Apermuted%20mask%20as%20compared%20to%20using%20the%20non-permuted%20LTH%20mask%2C%20on%20multiple%0Adatasets%20%28CIFAR-10%2C%20CIFAR-100%20and%20ImageNet%29%20and%20models%20%28VGG11%2C%20ResNet20%20and%0AResNet50%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Training%2520from%2520Random%2520Initialization%253A%2520Aligning%2520Lottery%2520Ticket%250A%2520%2520Masks%2520using%2520Weight%2520Symmetry%26entry.906535625%3DMohammed%2520Adnan%2520and%2520Rohan%2520Jain%2520and%2520Ekansh%2520Sharma%2520and%2520Rahul%2520Krishnan%2520and%2520Yani%2520Ioannou%26entry.1292438233%3D%2520%2520The%2520Lottery%2520Ticket%2520Hypothesis%2520%2528LTH%2529%2520suggests%2520there%2520exists%2520a%2520sparse%2520LTH%2520mask%250Aand%2520weights%2520that%2520achieve%2520the%2520same%2520generalization%2520performance%2520as%2520the%2520dense%2520model%250Awhile%2520using%2520significantly%2520fewer%2520parameters.%2520However%252C%2520finding%2520a%2520LTH%2520solution%2520is%250Acomputationally%2520expensive%252C%2520and%2520a%2520LTH%2520sparsity%2520mask%2520does%2520not%2520generalize%2520to%2520other%250Arandom%2520weight%2520initializations.%2520Recent%2520work%2520has%2520suggested%2520that%2520neural%2520networks%250Atrained%2520from%2520random%2520initialization%2520find%2520solutions%2520within%2520the%2520same%2520basin%2520modulo%250Apermutation%252C%2520and%2520proposes%2520a%2520method%2520to%2520align%2520trained%2520models%2520within%2520the%2520same%2520loss%250Abasin.%2520We%2520hypothesize%2520that%2520misalignment%2520of%2520basins%2520is%2520the%2520reason%2520why%2520LTH%2520masks%250Ado%2520not%2520generalize%2520to%2520new%2520random%2520initializations%2520and%2520propose%2520permuting%2520the%2520LTH%250Amask%2520to%2520align%2520with%2520the%2520new%2520optimization%2520basin%2520when%2520performing%2520sparse%2520training%250Afrom%2520a%2520different%2520random%2520init.%2520We%2520empirically%2520show%2520a%2520significant%2520increase%2520in%250Ageneralization%2520when%2520sparse%2520training%2520from%2520random%2520initialization%2520with%2520the%250Apermuted%2520mask%2520as%2520compared%2520to%2520using%2520the%2520non-permuted%2520LTH%2520mask%252C%2520on%2520multiple%250Adatasets%2520%2528CIFAR-10%252C%2520CIFAR-100%2520and%2520ImageNet%2529%2520and%2520models%2520%2528VGG11%252C%2520ResNet20%2520and%250AResNet50%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Training%20from%20Random%20Initialization%3A%20Aligning%20Lottery%20Ticket%0A%20%20Masks%20using%20Weight%20Symmetry&entry.906535625=Mohammed%20Adnan%20and%20Rohan%20Jain%20and%20Ekansh%20Sharma%20and%20Rahul%20Krishnan%20and%20Yani%20Ioannou&entry.1292438233=%20%20The%20Lottery%20Ticket%20Hypothesis%20%28LTH%29%20suggests%20there%20exists%20a%20sparse%20LTH%20mask%0Aand%20weights%20that%20achieve%20the%20same%20generalization%20performance%20as%20the%20dense%20model%0Awhile%20using%20significantly%20fewer%20parameters.%20However%2C%20finding%20a%20LTH%20solution%20is%0Acomputationally%20expensive%2C%20and%20a%20LTH%20sparsity%20mask%20does%20not%20generalize%20to%20other%0Arandom%20weight%20initializations.%20Recent%20work%20has%20suggested%20that%20neural%20networks%0Atrained%20from%20random%20initialization%20find%20solutions%20within%20the%20same%20basin%20modulo%0Apermutation%2C%20and%20proposes%20a%20method%20to%20align%20trained%20models%20within%20the%20same%20loss%0Abasin.%20We%20hypothesize%20that%20misalignment%20of%20basins%20is%20the%20reason%20why%20LTH%20masks%0Ado%20not%20generalize%20to%20new%20random%20initializations%20and%20propose%20permuting%20the%20LTH%0Amask%20to%20align%20with%20the%20new%20optimization%20basin%20when%20performing%20sparse%20training%0Afrom%20a%20different%20random%20init.%20We%20empirically%20show%20a%20significant%20increase%20in%0Ageneralization%20when%20sparse%20training%20from%20random%20initialization%20with%20the%0Apermuted%20mask%20as%20compared%20to%20using%20the%20non-permuted%20LTH%20mask%2C%20on%20multiple%0Adatasets%20%28CIFAR-10%2C%20CIFAR-100%20and%20ImageNet%29%20and%20models%20%28VGG11%2C%20ResNet20%20and%0AResNet50%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05143v1&entry.124074799=Read"},
{"title": "DiffusionSfM: Predicting Structure and Motion via Ray Origin and\n  Endpoint Diffusion", "author": "Qitao Zhao and Amy Lin and Jeff Tan and Jason Y. Zhang and Deva Ramanan and Shubham Tulsiani", "abstract": "  Current Structure-from-Motion (SfM) methods typically follow a two-stage\npipeline, combining learned or geometric pairwise reasoning with a subsequent\nglobal optimization step. In contrast, we propose a data-driven multi-view\nreasoning approach that directly infers 3D scene geometry and camera poses from\nmulti-view images. Our framework, DiffusionSfM, parameterizes scene geometry\nand cameras as pixel-wise ray origins and endpoints in a global frame and\nemploys a transformer-based denoising diffusion model to predict them from\nmulti-view inputs. To address practical challenges in training diffusion models\nwith missing data and unbounded scene coordinates, we introduce specialized\nmechanisms that ensure robust learning. We empirically validate DiffusionSfM on\nboth synthetic and real datasets, demonstrating that it outperforms classical\nand learning-based approaches while naturally modeling uncertainty.\n", "link": "http://arxiv.org/abs/2505.05473v1", "date": "2025-05-08", "relevancy": 2.547, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6495}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6291}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffusionSfM%3A%20Predicting%20Structure%20and%20Motion%20via%20Ray%20Origin%20and%0A%20%20Endpoint%20Diffusion&body=Title%3A%20DiffusionSfM%3A%20Predicting%20Structure%20and%20Motion%20via%20Ray%20Origin%20and%0A%20%20Endpoint%20Diffusion%0AAuthor%3A%20Qitao%20Zhao%20and%20Amy%20Lin%20and%20Jeff%20Tan%20and%20Jason%20Y.%20Zhang%20and%20Deva%20Ramanan%20and%20Shubham%20Tulsiani%0AAbstract%3A%20%20%20Current%20Structure-from-Motion%20%28SfM%29%20methods%20typically%20follow%20a%20two-stage%0Apipeline%2C%20combining%20learned%20or%20geometric%20pairwise%20reasoning%20with%20a%20subsequent%0Aglobal%20optimization%20step.%20In%20contrast%2C%20we%20propose%20a%20data-driven%20multi-view%0Areasoning%20approach%20that%20directly%20infers%203D%20scene%20geometry%20and%20camera%20poses%20from%0Amulti-view%20images.%20Our%20framework%2C%20DiffusionSfM%2C%20parameterizes%20scene%20geometry%0Aand%20cameras%20as%20pixel-wise%20ray%20origins%20and%20endpoints%20in%20a%20global%20frame%20and%0Aemploys%20a%20transformer-based%20denoising%20diffusion%20model%20to%20predict%20them%20from%0Amulti-view%20inputs.%20To%20address%20practical%20challenges%20in%20training%20diffusion%20models%0Awith%20missing%20data%20and%20unbounded%20scene%20coordinates%2C%20we%20introduce%20specialized%0Amechanisms%20that%20ensure%20robust%20learning.%20We%20empirically%20validate%20DiffusionSfM%20on%0Aboth%20synthetic%20and%20real%20datasets%2C%20demonstrating%20that%20it%20outperforms%20classical%0Aand%20learning-based%20approaches%20while%20naturally%20modeling%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusionSfM%253A%2520Predicting%2520Structure%2520and%2520Motion%2520via%2520Ray%2520Origin%2520and%250A%2520%2520Endpoint%2520Diffusion%26entry.906535625%3DQitao%2520Zhao%2520and%2520Amy%2520Lin%2520and%2520Jeff%2520Tan%2520and%2520Jason%2520Y.%2520Zhang%2520and%2520Deva%2520Ramanan%2520and%2520Shubham%2520Tulsiani%26entry.1292438233%3D%2520%2520Current%2520Structure-from-Motion%2520%2528SfM%2529%2520methods%2520typically%2520follow%2520a%2520two-stage%250Apipeline%252C%2520combining%2520learned%2520or%2520geometric%2520pairwise%2520reasoning%2520with%2520a%2520subsequent%250Aglobal%2520optimization%2520step.%2520In%2520contrast%252C%2520we%2520propose%2520a%2520data-driven%2520multi-view%250Areasoning%2520approach%2520that%2520directly%2520infers%25203D%2520scene%2520geometry%2520and%2520camera%2520poses%2520from%250Amulti-view%2520images.%2520Our%2520framework%252C%2520DiffusionSfM%252C%2520parameterizes%2520scene%2520geometry%250Aand%2520cameras%2520as%2520pixel-wise%2520ray%2520origins%2520and%2520endpoints%2520in%2520a%2520global%2520frame%2520and%250Aemploys%2520a%2520transformer-based%2520denoising%2520diffusion%2520model%2520to%2520predict%2520them%2520from%250Amulti-view%2520inputs.%2520To%2520address%2520practical%2520challenges%2520in%2520training%2520diffusion%2520models%250Awith%2520missing%2520data%2520and%2520unbounded%2520scene%2520coordinates%252C%2520we%2520introduce%2520specialized%250Amechanisms%2520that%2520ensure%2520robust%2520learning.%2520We%2520empirically%2520validate%2520DiffusionSfM%2520on%250Aboth%2520synthetic%2520and%2520real%2520datasets%252C%2520demonstrating%2520that%2520it%2520outperforms%2520classical%250Aand%2520learning-based%2520approaches%2520while%2520naturally%2520modeling%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffusionSfM%3A%20Predicting%20Structure%20and%20Motion%20via%20Ray%20Origin%20and%0A%20%20Endpoint%20Diffusion&entry.906535625=Qitao%20Zhao%20and%20Amy%20Lin%20and%20Jeff%20Tan%20and%20Jason%20Y.%20Zhang%20and%20Deva%20Ramanan%20and%20Shubham%20Tulsiani&entry.1292438233=%20%20Current%20Structure-from-Motion%20%28SfM%29%20methods%20typically%20follow%20a%20two-stage%0Apipeline%2C%20combining%20learned%20or%20geometric%20pairwise%20reasoning%20with%20a%20subsequent%0Aglobal%20optimization%20step.%20In%20contrast%2C%20we%20propose%20a%20data-driven%20multi-view%0Areasoning%20approach%20that%20directly%20infers%203D%20scene%20geometry%20and%20camera%20poses%20from%0Amulti-view%20images.%20Our%20framework%2C%20DiffusionSfM%2C%20parameterizes%20scene%20geometry%0Aand%20cameras%20as%20pixel-wise%20ray%20origins%20and%20endpoints%20in%20a%20global%20frame%20and%0Aemploys%20a%20transformer-based%20denoising%20diffusion%20model%20to%20predict%20them%20from%0Amulti-view%20inputs.%20To%20address%20practical%20challenges%20in%20training%20diffusion%20models%0Awith%20missing%20data%20and%20unbounded%20scene%20coordinates%2C%20we%20introduce%20specialized%0Amechanisms%20that%20ensure%20robust%20learning.%20We%20empirically%20validate%20DiffusionSfM%20on%0Aboth%20synthetic%20and%20real%20datasets%2C%20demonstrating%20that%20it%20outperforms%20classical%0Aand%20learning-based%20approaches%20while%20naturally%20modeling%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05473v1&entry.124074799=Read"},
{"title": "Progressive Inertial Poser: Progressive Real-Time Kinematic Chain\n  Estimation for 3D Full-Body Pose from Three IMU Sensors", "author": "Zunjie Zhu and Yan Zhao and Yihan Hu and Guoxiang Wang and Hai Qiu and Bolun Zheng and Chenggang Yan and Feng Xu", "abstract": "  The motion capture system that supports full-body virtual representation is\nof key significance for virtual reality. Compared to vision-based systems,\nfull-body pose estimation from sparse tracking signals is not limited by\nenvironmental conditions or recording range. However, previous works either\nface the challenge of wearing additional sensors on the pelvis and lower-body\nor rely on external visual sensors to obtain global positions of key joints. To\nimprove the practicality of the technology for virtual reality applications, we\nestimate full-body poses using only inertial data obtained from three Inertial\nMeasurement Unit (IMU) sensors worn on the head and wrists, thereby reducing\nthe complexity of the hardware system. In this work, we propose a method called\nProgressive Inertial Poser (ProgIP) for human pose estimation, which combines\nneural network estimation with a human dynamics model, considers the\nhierarchical structure of the kinematic chain, and employs a multi-stage\nprogressive network estimation with increased depth to reconstruct full-body\nmotion in real time. The encoder combines Transformer Encoder and bidirectional\nLSTM (TE-biLSTM) to flexibly capture the temporal dependencies of the inertial\nsequence, while the decoder based on multi-layer perceptrons (MLPs) transforms\nhigh-dimensional features and accurately projects them onto Skinned\nMulti-Person Linear (SMPL) model parameters. Quantitative and qualitative\nexperimental results on multiple public datasets show that our method\noutperforms state-of-the-art methods with the same inputs, and is comparable to\nrecent works using six IMU sensors.\n", "link": "http://arxiv.org/abs/2505.05336v1", "date": "2025-05-08", "relevancy": 2.4942, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6698}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5992}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Inertial%20Poser%3A%20Progressive%20Real-Time%20Kinematic%20Chain%0A%20%20Estimation%20for%203D%20Full-Body%20Pose%20from%20Three%20IMU%20Sensors&body=Title%3A%20Progressive%20Inertial%20Poser%3A%20Progressive%20Real-Time%20Kinematic%20Chain%0A%20%20Estimation%20for%203D%20Full-Body%20Pose%20from%20Three%20IMU%20Sensors%0AAuthor%3A%20Zunjie%20Zhu%20and%20Yan%20Zhao%20and%20Yihan%20Hu%20and%20Guoxiang%20Wang%20and%20Hai%20Qiu%20and%20Bolun%20Zheng%20and%20Chenggang%20Yan%20and%20Feng%20Xu%0AAbstract%3A%20%20%20The%20motion%20capture%20system%20that%20supports%20full-body%20virtual%20representation%20is%0Aof%20key%20significance%20for%20virtual%20reality.%20Compared%20to%20vision-based%20systems%2C%0Afull-body%20pose%20estimation%20from%20sparse%20tracking%20signals%20is%20not%20limited%20by%0Aenvironmental%20conditions%20or%20recording%20range.%20However%2C%20previous%20works%20either%0Aface%20the%20challenge%20of%20wearing%20additional%20sensors%20on%20the%20pelvis%20and%20lower-body%0Aor%20rely%20on%20external%20visual%20sensors%20to%20obtain%20global%20positions%20of%20key%20joints.%20To%0Aimprove%20the%20practicality%20of%20the%20technology%20for%20virtual%20reality%20applications%2C%20we%0Aestimate%20full-body%20poses%20using%20only%20inertial%20data%20obtained%20from%20three%20Inertial%0AMeasurement%20Unit%20%28IMU%29%20sensors%20worn%20on%20the%20head%20and%20wrists%2C%20thereby%20reducing%0Athe%20complexity%20of%20the%20hardware%20system.%20In%20this%20work%2C%20we%20propose%20a%20method%20called%0AProgressive%20Inertial%20Poser%20%28ProgIP%29%20for%20human%20pose%20estimation%2C%20which%20combines%0Aneural%20network%20estimation%20with%20a%20human%20dynamics%20model%2C%20considers%20the%0Ahierarchical%20structure%20of%20the%20kinematic%20chain%2C%20and%20employs%20a%20multi-stage%0Aprogressive%20network%20estimation%20with%20increased%20depth%20to%20reconstruct%20full-body%0Amotion%20in%20real%20time.%20The%20encoder%20combines%20Transformer%20Encoder%20and%20bidirectional%0ALSTM%20%28TE-biLSTM%29%20to%20flexibly%20capture%20the%20temporal%20dependencies%20of%20the%20inertial%0Asequence%2C%20while%20the%20decoder%20based%20on%20multi-layer%20perceptrons%20%28MLPs%29%20transforms%0Ahigh-dimensional%20features%20and%20accurately%20projects%20them%20onto%20Skinned%0AMulti-Person%20Linear%20%28SMPL%29%20model%20parameters.%20Quantitative%20and%20qualitative%0Aexperimental%20results%20on%20multiple%20public%20datasets%20show%20that%20our%20method%0Aoutperforms%20state-of-the-art%20methods%20with%20the%20same%20inputs%2C%20and%20is%20comparable%20to%0Arecent%20works%20using%20six%20IMU%20sensors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Inertial%2520Poser%253A%2520Progressive%2520Real-Time%2520Kinematic%2520Chain%250A%2520%2520Estimation%2520for%25203D%2520Full-Body%2520Pose%2520from%2520Three%2520IMU%2520Sensors%26entry.906535625%3DZunjie%2520Zhu%2520and%2520Yan%2520Zhao%2520and%2520Yihan%2520Hu%2520and%2520Guoxiang%2520Wang%2520and%2520Hai%2520Qiu%2520and%2520Bolun%2520Zheng%2520and%2520Chenggang%2520Yan%2520and%2520Feng%2520Xu%26entry.1292438233%3D%2520%2520The%2520motion%2520capture%2520system%2520that%2520supports%2520full-body%2520virtual%2520representation%2520is%250Aof%2520key%2520significance%2520for%2520virtual%2520reality.%2520Compared%2520to%2520vision-based%2520systems%252C%250Afull-body%2520pose%2520estimation%2520from%2520sparse%2520tracking%2520signals%2520is%2520not%2520limited%2520by%250Aenvironmental%2520conditions%2520or%2520recording%2520range.%2520However%252C%2520previous%2520works%2520either%250Aface%2520the%2520challenge%2520of%2520wearing%2520additional%2520sensors%2520on%2520the%2520pelvis%2520and%2520lower-body%250Aor%2520rely%2520on%2520external%2520visual%2520sensors%2520to%2520obtain%2520global%2520positions%2520of%2520key%2520joints.%2520To%250Aimprove%2520the%2520practicality%2520of%2520the%2520technology%2520for%2520virtual%2520reality%2520applications%252C%2520we%250Aestimate%2520full-body%2520poses%2520using%2520only%2520inertial%2520data%2520obtained%2520from%2520three%2520Inertial%250AMeasurement%2520Unit%2520%2528IMU%2529%2520sensors%2520worn%2520on%2520the%2520head%2520and%2520wrists%252C%2520thereby%2520reducing%250Athe%2520complexity%2520of%2520the%2520hardware%2520system.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520method%2520called%250AProgressive%2520Inertial%2520Poser%2520%2528ProgIP%2529%2520for%2520human%2520pose%2520estimation%252C%2520which%2520combines%250Aneural%2520network%2520estimation%2520with%2520a%2520human%2520dynamics%2520model%252C%2520considers%2520the%250Ahierarchical%2520structure%2520of%2520the%2520kinematic%2520chain%252C%2520and%2520employs%2520a%2520multi-stage%250Aprogressive%2520network%2520estimation%2520with%2520increased%2520depth%2520to%2520reconstruct%2520full-body%250Amotion%2520in%2520real%2520time.%2520The%2520encoder%2520combines%2520Transformer%2520Encoder%2520and%2520bidirectional%250ALSTM%2520%2528TE-biLSTM%2529%2520to%2520flexibly%2520capture%2520the%2520temporal%2520dependencies%2520of%2520the%2520inertial%250Asequence%252C%2520while%2520the%2520decoder%2520based%2520on%2520multi-layer%2520perceptrons%2520%2528MLPs%2529%2520transforms%250Ahigh-dimensional%2520features%2520and%2520accurately%2520projects%2520them%2520onto%2520Skinned%250AMulti-Person%2520Linear%2520%2528SMPL%2529%2520model%2520parameters.%2520Quantitative%2520and%2520qualitative%250Aexperimental%2520results%2520on%2520multiple%2520public%2520datasets%2520show%2520that%2520our%2520method%250Aoutperforms%2520state-of-the-art%2520methods%2520with%2520the%2520same%2520inputs%252C%2520and%2520is%2520comparable%2520to%250Arecent%2520works%2520using%2520six%2520IMU%2520sensors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Inertial%20Poser%3A%20Progressive%20Real-Time%20Kinematic%20Chain%0A%20%20Estimation%20for%203D%20Full-Body%20Pose%20from%20Three%20IMU%20Sensors&entry.906535625=Zunjie%20Zhu%20and%20Yan%20Zhao%20and%20Yihan%20Hu%20and%20Guoxiang%20Wang%20and%20Hai%20Qiu%20and%20Bolun%20Zheng%20and%20Chenggang%20Yan%20and%20Feng%20Xu&entry.1292438233=%20%20The%20motion%20capture%20system%20that%20supports%20full-body%20virtual%20representation%20is%0Aof%20key%20significance%20for%20virtual%20reality.%20Compared%20to%20vision-based%20systems%2C%0Afull-body%20pose%20estimation%20from%20sparse%20tracking%20signals%20is%20not%20limited%20by%0Aenvironmental%20conditions%20or%20recording%20range.%20However%2C%20previous%20works%20either%0Aface%20the%20challenge%20of%20wearing%20additional%20sensors%20on%20the%20pelvis%20and%20lower-body%0Aor%20rely%20on%20external%20visual%20sensors%20to%20obtain%20global%20positions%20of%20key%20joints.%20To%0Aimprove%20the%20practicality%20of%20the%20technology%20for%20virtual%20reality%20applications%2C%20we%0Aestimate%20full-body%20poses%20using%20only%20inertial%20data%20obtained%20from%20three%20Inertial%0AMeasurement%20Unit%20%28IMU%29%20sensors%20worn%20on%20the%20head%20and%20wrists%2C%20thereby%20reducing%0Athe%20complexity%20of%20the%20hardware%20system.%20In%20this%20work%2C%20we%20propose%20a%20method%20called%0AProgressive%20Inertial%20Poser%20%28ProgIP%29%20for%20human%20pose%20estimation%2C%20which%20combines%0Aneural%20network%20estimation%20with%20a%20human%20dynamics%20model%2C%20considers%20the%0Ahierarchical%20structure%20of%20the%20kinematic%20chain%2C%20and%20employs%20a%20multi-stage%0Aprogressive%20network%20estimation%20with%20increased%20depth%20to%20reconstruct%20full-body%0Amotion%20in%20real%20time.%20The%20encoder%20combines%20Transformer%20Encoder%20and%20bidirectional%0ALSTM%20%28TE-biLSTM%29%20to%20flexibly%20capture%20the%20temporal%20dependencies%20of%20the%20inertial%0Asequence%2C%20while%20the%20decoder%20based%20on%20multi-layer%20perceptrons%20%28MLPs%29%20transforms%0Ahigh-dimensional%20features%20and%20accurately%20projects%20them%20onto%20Skinned%0AMulti-Person%20Linear%20%28SMPL%29%20model%20parameters.%20Quantitative%20and%20qualitative%0Aexperimental%20results%20on%20multiple%20public%20datasets%20show%20that%20our%20method%0Aoutperforms%20state-of-the-art%20methods%20with%20the%20same%20inputs%2C%20and%20is%20comparable%20to%0Arecent%20works%20using%20six%20IMU%20sensors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05336v1&entry.124074799=Read"},
{"title": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining", "author": "Jeffrey Li and Mohammadreza Armandpour and Iman Mirzadeh and Sachin Mehta and Vaishaal Shankar and Raviteja Vemulapalli and Samy Bengio and Oncel Tuzel and Mehrdad Farajtabar and Hadi Pouransari and Fartash Faghri", "abstract": "  Large Language Models (LLMs) trained on historical web data inevitably become\noutdated. We investigate evaluation strategies and update methods for LLMs as\nnew data becomes available. We introduce a web-scale dataset for time-continual\npretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of\nmagnitude larger than previous continual language modeling benchmarks. We also\ndesign time-stratified evaluations across both general CC data and specific\ndomains (Wikipedia, StackExchange, and code documentation) to assess how well\nvarious continual learning methods adapt to new data while retaining past\nknowledge. Our findings demonstrate that, on general CC data, autoregressive\nmeta-schedules combined with a fixed-ratio replay of older data can achieve\ncomparable held-out loss to re-training from scratch, while requiring\nsignificantly less computation (2.6x). However, the optimal balance between\nincorporating new data and replaying old data differs as replay is crucial to\navoid forgetting on generic web data but less so on specific domains.\n", "link": "http://arxiv.org/abs/2504.02107v2", "date": "2025-05-08", "relevancy": 2.4806, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5257}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4896}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiC-LM%3A%20A%20Web-Scale%20Benchmark%20for%20Time-Continual%20LLM%20Pretraining&body=Title%3A%20TiC-LM%3A%20A%20Web-Scale%20Benchmark%20for%20Time-Continual%20LLM%20Pretraining%0AAuthor%3A%20Jeffrey%20Li%20and%20Mohammadreza%20Armandpour%20and%20Iman%20Mirzadeh%20and%20Sachin%20Mehta%20and%20Vaishaal%20Shankar%20and%20Raviteja%20Vemulapalli%20and%20Samy%20Bengio%20and%20Oncel%20Tuzel%20and%20Mehrdad%20Farajtabar%20and%20Hadi%20Pouransari%20and%20Fartash%20Faghri%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20trained%20on%20historical%20web%20data%20inevitably%20become%0Aoutdated.%20We%20investigate%20evaluation%20strategies%20and%20update%20methods%20for%20LLMs%20as%0Anew%20data%20becomes%20available.%20We%20introduce%20a%20web-scale%20dataset%20for%20time-continual%0Apretraining%20of%20LLMs%20derived%20from%20114%20dumps%20of%20Common%20Crawl%20%28CC%29%20-%20orders%20of%0Amagnitude%20larger%20than%20previous%20continual%20language%20modeling%20benchmarks.%20We%20also%0Adesign%20time-stratified%20evaluations%20across%20both%20general%20CC%20data%20and%20specific%0Adomains%20%28Wikipedia%2C%20StackExchange%2C%20and%20code%20documentation%29%20to%20assess%20how%20well%0Avarious%20continual%20learning%20methods%20adapt%20to%20new%20data%20while%20retaining%20past%0Aknowledge.%20Our%20findings%20demonstrate%20that%2C%20on%20general%20CC%20data%2C%20autoregressive%0Ameta-schedules%20combined%20with%20a%20fixed-ratio%20replay%20of%20older%20data%20can%20achieve%0Acomparable%20held-out%20loss%20to%20re-training%20from%20scratch%2C%20while%20requiring%0Asignificantly%20less%20computation%20%282.6x%29.%20However%2C%20the%20optimal%20balance%20between%0Aincorporating%20new%20data%20and%20replaying%20old%20data%20differs%20as%20replay%20is%20crucial%20to%0Aavoid%20forgetting%20on%20generic%20web%20data%20but%20less%20so%20on%20specific%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02107v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiC-LM%253A%2520A%2520Web-Scale%2520Benchmark%2520for%2520Time-Continual%2520LLM%2520Pretraining%26entry.906535625%3DJeffrey%2520Li%2520and%2520Mohammadreza%2520Armandpour%2520and%2520Iman%2520Mirzadeh%2520and%2520Sachin%2520Mehta%2520and%2520Vaishaal%2520Shankar%2520and%2520Raviteja%2520Vemulapalli%2520and%2520Samy%2520Bengio%2520and%2520Oncel%2520Tuzel%2520and%2520Mehrdad%2520Farajtabar%2520and%2520Hadi%2520Pouransari%2520and%2520Fartash%2520Faghri%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520trained%2520on%2520historical%2520web%2520data%2520inevitably%2520become%250Aoutdated.%2520We%2520investigate%2520evaluation%2520strategies%2520and%2520update%2520methods%2520for%2520LLMs%2520as%250Anew%2520data%2520becomes%2520available.%2520We%2520introduce%2520a%2520web-scale%2520dataset%2520for%2520time-continual%250Apretraining%2520of%2520LLMs%2520derived%2520from%2520114%2520dumps%2520of%2520Common%2520Crawl%2520%2528CC%2529%2520-%2520orders%2520of%250Amagnitude%2520larger%2520than%2520previous%2520continual%2520language%2520modeling%2520benchmarks.%2520We%2520also%250Adesign%2520time-stratified%2520evaluations%2520across%2520both%2520general%2520CC%2520data%2520and%2520specific%250Adomains%2520%2528Wikipedia%252C%2520StackExchange%252C%2520and%2520code%2520documentation%2529%2520to%2520assess%2520how%2520well%250Avarious%2520continual%2520learning%2520methods%2520adapt%2520to%2520new%2520data%2520while%2520retaining%2520past%250Aknowledge.%2520Our%2520findings%2520demonstrate%2520that%252C%2520on%2520general%2520CC%2520data%252C%2520autoregressive%250Ameta-schedules%2520combined%2520with%2520a%2520fixed-ratio%2520replay%2520of%2520older%2520data%2520can%2520achieve%250Acomparable%2520held-out%2520loss%2520to%2520re-training%2520from%2520scratch%252C%2520while%2520requiring%250Asignificantly%2520less%2520computation%2520%25282.6x%2529.%2520However%252C%2520the%2520optimal%2520balance%2520between%250Aincorporating%2520new%2520data%2520and%2520replaying%2520old%2520data%2520differs%2520as%2520replay%2520is%2520crucial%2520to%250Aavoid%2520forgetting%2520on%2520generic%2520web%2520data%2520but%2520less%2520so%2520on%2520specific%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02107v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiC-LM%3A%20A%20Web-Scale%20Benchmark%20for%20Time-Continual%20LLM%20Pretraining&entry.906535625=Jeffrey%20Li%20and%20Mohammadreza%20Armandpour%20and%20Iman%20Mirzadeh%20and%20Sachin%20Mehta%20and%20Vaishaal%20Shankar%20and%20Raviteja%20Vemulapalli%20and%20Samy%20Bengio%20and%20Oncel%20Tuzel%20and%20Mehrdad%20Farajtabar%20and%20Hadi%20Pouransari%20and%20Fartash%20Faghri&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20trained%20on%20historical%20web%20data%20inevitably%20become%0Aoutdated.%20We%20investigate%20evaluation%20strategies%20and%20update%20methods%20for%20LLMs%20as%0Anew%20data%20becomes%20available.%20We%20introduce%20a%20web-scale%20dataset%20for%20time-continual%0Apretraining%20of%20LLMs%20derived%20from%20114%20dumps%20of%20Common%20Crawl%20%28CC%29%20-%20orders%20of%0Amagnitude%20larger%20than%20previous%20continual%20language%20modeling%20benchmarks.%20We%20also%0Adesign%20time-stratified%20evaluations%20across%20both%20general%20CC%20data%20and%20specific%0Adomains%20%28Wikipedia%2C%20StackExchange%2C%20and%20code%20documentation%29%20to%20assess%20how%20well%0Avarious%20continual%20learning%20methods%20adapt%20to%20new%20data%20while%20retaining%20past%0Aknowledge.%20Our%20findings%20demonstrate%20that%2C%20on%20general%20CC%20data%2C%20autoregressive%0Ameta-schedules%20combined%20with%20a%20fixed-ratio%20replay%20of%20older%20data%20can%20achieve%0Acomparable%20held-out%20loss%20to%20re-training%20from%20scratch%2C%20while%20requiring%0Asignificantly%20less%20computation%20%282.6x%29.%20However%2C%20the%20optimal%20balance%20between%0Aincorporating%20new%20data%20and%20replaying%20old%20data%20differs%20as%20replay%20is%20crucial%20to%0Aavoid%20forgetting%20on%20generic%20web%20data%20but%20less%20so%20on%20specific%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02107v2&entry.124074799=Read"},
{"title": "Representing spherical tensors with scalar-based machine-learning models", "author": "Michelangelo Domina and Filippo Bigi and Paolo Pegolo and Michele Ceriotti", "abstract": "  Rotational symmetry plays a central role in physics, providing an elegant\nframework to describe how the properties of 3D objects -- from atoms to the\nmacroscopic scale -- transform under the action of rigid rotations. Equivariant\nmodels of 3D point clouds are able to approximate structure-property relations\nin a way that is fully consistent with the structure of the rotation group, by\ncombining intermediate representations that are themselves spherical tensors.\nThe symmetry constraints however make this approach computationally demanding\nand cumbersome to implement, which motivates increasingly popular unconstrained\narchitectures that learn approximate symmetries as part of the training\nprocess. In this work, we explore a third route to tackle this learning\nproblem, where equivariant functions are expressed as the product of a scalar\nfunction of the point cloud coordinates and a small basis of tensors with the\nappropriate symmetry. We also propose approximations of the general expressions\nthat, while lacking universal approximation properties, are fast, simple to\nimplement, and accurate in practical settings.\n", "link": "http://arxiv.org/abs/2505.05404v1", "date": "2025-05-08", "relevancy": 2.4567, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5139}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4809}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representing%20spherical%20tensors%20with%20scalar-based%20machine-learning%20models&body=Title%3A%20Representing%20spherical%20tensors%20with%20scalar-based%20machine-learning%20models%0AAuthor%3A%20Michelangelo%20Domina%20and%20Filippo%20Bigi%20and%20Paolo%20Pegolo%20and%20Michele%20Ceriotti%0AAbstract%3A%20%20%20Rotational%20symmetry%20plays%20a%20central%20role%20in%20physics%2C%20providing%20an%20elegant%0Aframework%20to%20describe%20how%20the%20properties%20of%203D%20objects%20--%20from%20atoms%20to%20the%0Amacroscopic%20scale%20--%20transform%20under%20the%20action%20of%20rigid%20rotations.%20Equivariant%0Amodels%20of%203D%20point%20clouds%20are%20able%20to%20approximate%20structure-property%20relations%0Ain%20a%20way%20that%20is%20fully%20consistent%20with%20the%20structure%20of%20the%20rotation%20group%2C%20by%0Acombining%20intermediate%20representations%20that%20are%20themselves%20spherical%20tensors.%0AThe%20symmetry%20constraints%20however%20make%20this%20approach%20computationally%20demanding%0Aand%20cumbersome%20to%20implement%2C%20which%20motivates%20increasingly%20popular%20unconstrained%0Aarchitectures%20that%20learn%20approximate%20symmetries%20as%20part%20of%20the%20training%0Aprocess.%20In%20this%20work%2C%20we%20explore%20a%20third%20route%20to%20tackle%20this%20learning%0Aproblem%2C%20where%20equivariant%20functions%20are%20expressed%20as%20the%20product%20of%20a%20scalar%0Afunction%20of%20the%20point%20cloud%20coordinates%20and%20a%20small%20basis%20of%20tensors%20with%20the%0Aappropriate%20symmetry.%20We%20also%20propose%20approximations%20of%20the%20general%20expressions%0Athat%2C%20while%20lacking%20universal%20approximation%20properties%2C%20are%20fast%2C%20simple%20to%0Aimplement%2C%20and%20accurate%20in%20practical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresenting%2520spherical%2520tensors%2520with%2520scalar-based%2520machine-learning%2520models%26entry.906535625%3DMichelangelo%2520Domina%2520and%2520Filippo%2520Bigi%2520and%2520Paolo%2520Pegolo%2520and%2520Michele%2520Ceriotti%26entry.1292438233%3D%2520%2520Rotational%2520symmetry%2520plays%2520a%2520central%2520role%2520in%2520physics%252C%2520providing%2520an%2520elegant%250Aframework%2520to%2520describe%2520how%2520the%2520properties%2520of%25203D%2520objects%2520--%2520from%2520atoms%2520to%2520the%250Amacroscopic%2520scale%2520--%2520transform%2520under%2520the%2520action%2520of%2520rigid%2520rotations.%2520Equivariant%250Amodels%2520of%25203D%2520point%2520clouds%2520are%2520able%2520to%2520approximate%2520structure-property%2520relations%250Ain%2520a%2520way%2520that%2520is%2520fully%2520consistent%2520with%2520the%2520structure%2520of%2520the%2520rotation%2520group%252C%2520by%250Acombining%2520intermediate%2520representations%2520that%2520are%2520themselves%2520spherical%2520tensors.%250AThe%2520symmetry%2520constraints%2520however%2520make%2520this%2520approach%2520computationally%2520demanding%250Aand%2520cumbersome%2520to%2520implement%252C%2520which%2520motivates%2520increasingly%2520popular%2520unconstrained%250Aarchitectures%2520that%2520learn%2520approximate%2520symmetries%2520as%2520part%2520of%2520the%2520training%250Aprocess.%2520In%2520this%2520work%252C%2520we%2520explore%2520a%2520third%2520route%2520to%2520tackle%2520this%2520learning%250Aproblem%252C%2520where%2520equivariant%2520functions%2520are%2520expressed%2520as%2520the%2520product%2520of%2520a%2520scalar%250Afunction%2520of%2520the%2520point%2520cloud%2520coordinates%2520and%2520a%2520small%2520basis%2520of%2520tensors%2520with%2520the%250Aappropriate%2520symmetry.%2520We%2520also%2520propose%2520approximations%2520of%2520the%2520general%2520expressions%250Athat%252C%2520while%2520lacking%2520universal%2520approximation%2520properties%252C%2520are%2520fast%252C%2520simple%2520to%250Aimplement%252C%2520and%2520accurate%2520in%2520practical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representing%20spherical%20tensors%20with%20scalar-based%20machine-learning%20models&entry.906535625=Michelangelo%20Domina%20and%20Filippo%20Bigi%20and%20Paolo%20Pegolo%20and%20Michele%20Ceriotti&entry.1292438233=%20%20Rotational%20symmetry%20plays%20a%20central%20role%20in%20physics%2C%20providing%20an%20elegant%0Aframework%20to%20describe%20how%20the%20properties%20of%203D%20objects%20--%20from%20atoms%20to%20the%0Amacroscopic%20scale%20--%20transform%20under%20the%20action%20of%20rigid%20rotations.%20Equivariant%0Amodels%20of%203D%20point%20clouds%20are%20able%20to%20approximate%20structure-property%20relations%0Ain%20a%20way%20that%20is%20fully%20consistent%20with%20the%20structure%20of%20the%20rotation%20group%2C%20by%0Acombining%20intermediate%20representations%20that%20are%20themselves%20spherical%20tensors.%0AThe%20symmetry%20constraints%20however%20make%20this%20approach%20computationally%20demanding%0Aand%20cumbersome%20to%20implement%2C%20which%20motivates%20increasingly%20popular%20unconstrained%0Aarchitectures%20that%20learn%20approximate%20symmetries%20as%20part%20of%20the%20training%0Aprocess.%20In%20this%20work%2C%20we%20explore%20a%20third%20route%20to%20tackle%20this%20learning%0Aproblem%2C%20where%20equivariant%20functions%20are%20expressed%20as%20the%20product%20of%20a%20scalar%0Afunction%20of%20the%20point%20cloud%20coordinates%20and%20a%20small%20basis%20of%20tensors%20with%20the%0Aappropriate%20symmetry.%20We%20also%20propose%20approximations%20of%20the%20general%20expressions%0Athat%2C%20while%20lacking%20universal%20approximation%20properties%2C%20are%20fast%2C%20simple%20to%0Aimplement%2C%20and%20accurate%20in%20practical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05404v1&entry.124074799=Read"},
{"title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning", "author": "Zhengyu Hu and Yichuan Li and Zhengyu Chen and Jingang Wang and Han Liu and Kyumin Lee and Kaize Ding", "abstract": "  Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning.\n", "link": "http://arxiv.org/abs/2410.07074v3", "date": "2025-05-08", "relevancy": 2.4002, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4892}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Ask%20GNN%3A%20Empowering%20Large%20Language%20Model%20for%20Graph%20In-Context%0A%20%20Learning&body=Title%3A%20Let%27s%20Ask%20GNN%3A%20Empowering%20Large%20Language%20Model%20for%20Graph%20In-Context%0A%20%20Learning%0AAuthor%3A%20Zhengyu%20Hu%20and%20Yichuan%20Li%20and%20Zhengyu%20Chen%20and%20Jingang%20Wang%20and%20Han%20Liu%20and%20Kyumin%20Lee%20and%20Kaize%20Ding%0AAbstract%3A%20%20%20Textual%20Attributed%20Graphs%20%28TAGs%29%20are%20crucial%20for%20modeling%20complex%20real-world%0Asystems%2C%20yet%20leveraging%20large%20language%20models%20%28LLMs%29%20for%20TAGs%20presents%20unique%0Achallenges%20due%20to%20the%20gap%20between%20sequential%20text%20processing%20and%0Agraph-structured%20data.%20We%20introduce%20AskGNN%2C%20a%20novel%20approach%20that%20bridges%20this%0Agap%20by%20leveraging%20In-Context%20Learning%20%28ICL%29%20to%20integrate%20graph%20data%20and%0Atask-specific%20information%20into%20LLMs.%20AskGNN%20employs%20a%20Graph%20Neural%20Network%0A%28GNN%29-powered%20structure-enhanced%20retriever%20to%20select%20labeled%20nodes%20across%0Agraphs%2C%20incorporating%20complex%20graph%20structures%20and%20their%20supervision%20signals.%0AOur%20learning-to-retrieve%20algorithm%20optimizes%20the%20retriever%20to%20select%20example%0Anodes%20that%20maximize%20LLM%20performance%20on%20graph.%20Experiments%20across%20three%20tasks%0Aand%20seven%20LLMs%20demonstrate%20AskGNN%27s%20superior%20effectiveness%20in%20graph%20task%0Aperformance%2C%20opening%20new%20avenues%20for%20applying%20LLMs%20to%20graph-structured%20data%0Awithout%20extensive%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07074v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Ask%2520GNN%253A%2520Empowering%2520Large%2520Language%2520Model%2520for%2520Graph%2520In-Context%250A%2520%2520Learning%26entry.906535625%3DZhengyu%2520Hu%2520and%2520Yichuan%2520Li%2520and%2520Zhengyu%2520Chen%2520and%2520Jingang%2520Wang%2520and%2520Han%2520Liu%2520and%2520Kyumin%2520Lee%2520and%2520Kaize%2520Ding%26entry.1292438233%3D%2520%2520Textual%2520Attributed%2520Graphs%2520%2528TAGs%2529%2520are%2520crucial%2520for%2520modeling%2520complex%2520real-world%250Asystems%252C%2520yet%2520leveraging%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520TAGs%2520presents%2520unique%250Achallenges%2520due%2520to%2520the%2520gap%2520between%2520sequential%2520text%2520processing%2520and%250Agraph-structured%2520data.%2520We%2520introduce%2520AskGNN%252C%2520a%2520novel%2520approach%2520that%2520bridges%2520this%250Agap%2520by%2520leveraging%2520In-Context%2520Learning%2520%2528ICL%2529%2520to%2520integrate%2520graph%2520data%2520and%250Atask-specific%2520information%2520into%2520LLMs.%2520AskGNN%2520employs%2520a%2520Graph%2520Neural%2520Network%250A%2528GNN%2529-powered%2520structure-enhanced%2520retriever%2520to%2520select%2520labeled%2520nodes%2520across%250Agraphs%252C%2520incorporating%2520complex%2520graph%2520structures%2520and%2520their%2520supervision%2520signals.%250AOur%2520learning-to-retrieve%2520algorithm%2520optimizes%2520the%2520retriever%2520to%2520select%2520example%250Anodes%2520that%2520maximize%2520LLM%2520performance%2520on%2520graph.%2520Experiments%2520across%2520three%2520tasks%250Aand%2520seven%2520LLMs%2520demonstrate%2520AskGNN%2527s%2520superior%2520effectiveness%2520in%2520graph%2520task%250Aperformance%252C%2520opening%2520new%2520avenues%2520for%2520applying%2520LLMs%2520to%2520graph-structured%2520data%250Awithout%2520extensive%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07074v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Ask%20GNN%3A%20Empowering%20Large%20Language%20Model%20for%20Graph%20In-Context%0A%20%20Learning&entry.906535625=Zhengyu%20Hu%20and%20Yichuan%20Li%20and%20Zhengyu%20Chen%20and%20Jingang%20Wang%20and%20Han%20Liu%20and%20Kyumin%20Lee%20and%20Kaize%20Ding&entry.1292438233=%20%20Textual%20Attributed%20Graphs%20%28TAGs%29%20are%20crucial%20for%20modeling%20complex%20real-world%0Asystems%2C%20yet%20leveraging%20large%20language%20models%20%28LLMs%29%20for%20TAGs%20presents%20unique%0Achallenges%20due%20to%20the%20gap%20between%20sequential%20text%20processing%20and%0Agraph-structured%20data.%20We%20introduce%20AskGNN%2C%20a%20novel%20approach%20that%20bridges%20this%0Agap%20by%20leveraging%20In-Context%20Learning%20%28ICL%29%20to%20integrate%20graph%20data%20and%0Atask-specific%20information%20into%20LLMs.%20AskGNN%20employs%20a%20Graph%20Neural%20Network%0A%28GNN%29-powered%20structure-enhanced%20retriever%20to%20select%20labeled%20nodes%20across%0Agraphs%2C%20incorporating%20complex%20graph%20structures%20and%20their%20supervision%20signals.%0AOur%20learning-to-retrieve%20algorithm%20optimizes%20the%20retriever%20to%20select%20example%0Anodes%20that%20maximize%20LLM%20performance%20on%20graph.%20Experiments%20across%20three%20tasks%0Aand%20seven%20LLMs%20demonstrate%20AskGNN%27s%20superior%20effectiveness%20in%20graph%20task%0Aperformance%2C%20opening%20new%20avenues%20for%20applying%20LLMs%20to%20graph-structured%20data%0Awithout%20extensive%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07074v3&entry.124074799=Read"},
{"title": "Nearly Optimal Sample Complexity for Learning with Label Proportions", "author": "Robert Busa-Fekete and Travis Dick and Claudio Gentile and Haim Kaplan and Tomer Koren and Uri Stemmer", "abstract": "  We investigate Learning from Label Proportions (LLP), a partial information\nsetting where examples in a training set are grouped into bags, and only\naggregate label values in each bag are available. Despite the partial\nobservability, the goal is still to achieve small regret at the level of\nindividual examples. We give results on the sample complexity of LLP under\nsquare loss, showing that our sample complexity is essentially optimal. From an\nalgorithmic viewpoint, we rely on carefully designed variants of Empirical Risk\nMinimization, and Stochastic Gradient Descent algorithms, combined with ad hoc\nvariance reduction techniques. On one hand, our theoretical results improve in\nimportant ways on the existing literature on LLP, specifically in the way the\nsample complexity depends on the bag size. On the other hand, we validate our\nalgorithmic solutions on several datasets, demonstrating improved empirical\nperformance (better accuracy for less samples) against recent baselines.\n", "link": "http://arxiv.org/abs/2505.05355v1", "date": "2025-05-08", "relevancy": 2.4, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4939}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4933}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nearly%20Optimal%20Sample%20Complexity%20for%20Learning%20with%20Label%20Proportions&body=Title%3A%20Nearly%20Optimal%20Sample%20Complexity%20for%20Learning%20with%20Label%20Proportions%0AAuthor%3A%20Robert%20Busa-Fekete%20and%20Travis%20Dick%20and%20Claudio%20Gentile%20and%20Haim%20Kaplan%20and%20Tomer%20Koren%20and%20Uri%20Stemmer%0AAbstract%3A%20%20%20We%20investigate%20Learning%20from%20Label%20Proportions%20%28LLP%29%2C%20a%20partial%20information%0Asetting%20where%20examples%20in%20a%20training%20set%20are%20grouped%20into%20bags%2C%20and%20only%0Aaggregate%20label%20values%20in%20each%20bag%20are%20available.%20Despite%20the%20partial%0Aobservability%2C%20the%20goal%20is%20still%20to%20achieve%20small%20regret%20at%20the%20level%20of%0Aindividual%20examples.%20We%20give%20results%20on%20the%20sample%20complexity%20of%20LLP%20under%0Asquare%20loss%2C%20showing%20that%20our%20sample%20complexity%20is%20essentially%20optimal.%20From%20an%0Aalgorithmic%20viewpoint%2C%20we%20rely%20on%20carefully%20designed%20variants%20of%20Empirical%20Risk%0AMinimization%2C%20and%20Stochastic%20Gradient%20Descent%20algorithms%2C%20combined%20with%20ad%20hoc%0Avariance%20reduction%20techniques.%20On%20one%20hand%2C%20our%20theoretical%20results%20improve%20in%0Aimportant%20ways%20on%20the%20existing%20literature%20on%20LLP%2C%20specifically%20in%20the%20way%20the%0Asample%20complexity%20depends%20on%20the%20bag%20size.%20On%20the%20other%20hand%2C%20we%20validate%20our%0Aalgorithmic%20solutions%20on%20several%20datasets%2C%20demonstrating%20improved%20empirical%0Aperformance%20%28better%20accuracy%20for%20less%20samples%29%20against%20recent%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNearly%2520Optimal%2520Sample%2520Complexity%2520for%2520Learning%2520with%2520Label%2520Proportions%26entry.906535625%3DRobert%2520Busa-Fekete%2520and%2520Travis%2520Dick%2520and%2520Claudio%2520Gentile%2520and%2520Haim%2520Kaplan%2520and%2520Tomer%2520Koren%2520and%2520Uri%2520Stemmer%26entry.1292438233%3D%2520%2520We%2520investigate%2520Learning%2520from%2520Label%2520Proportions%2520%2528LLP%2529%252C%2520a%2520partial%2520information%250Asetting%2520where%2520examples%2520in%2520a%2520training%2520set%2520are%2520grouped%2520into%2520bags%252C%2520and%2520only%250Aaggregate%2520label%2520values%2520in%2520each%2520bag%2520are%2520available.%2520Despite%2520the%2520partial%250Aobservability%252C%2520the%2520goal%2520is%2520still%2520to%2520achieve%2520small%2520regret%2520at%2520the%2520level%2520of%250Aindividual%2520examples.%2520We%2520give%2520results%2520on%2520the%2520sample%2520complexity%2520of%2520LLP%2520under%250Asquare%2520loss%252C%2520showing%2520that%2520our%2520sample%2520complexity%2520is%2520essentially%2520optimal.%2520From%2520an%250Aalgorithmic%2520viewpoint%252C%2520we%2520rely%2520on%2520carefully%2520designed%2520variants%2520of%2520Empirical%2520Risk%250AMinimization%252C%2520and%2520Stochastic%2520Gradient%2520Descent%2520algorithms%252C%2520combined%2520with%2520ad%2520hoc%250Avariance%2520reduction%2520techniques.%2520On%2520one%2520hand%252C%2520our%2520theoretical%2520results%2520improve%2520in%250Aimportant%2520ways%2520on%2520the%2520existing%2520literature%2520on%2520LLP%252C%2520specifically%2520in%2520the%2520way%2520the%250Asample%2520complexity%2520depends%2520on%2520the%2520bag%2520size.%2520On%2520the%2520other%2520hand%252C%2520we%2520validate%2520our%250Aalgorithmic%2520solutions%2520on%2520several%2520datasets%252C%2520demonstrating%2520improved%2520empirical%250Aperformance%2520%2528better%2520accuracy%2520for%2520less%2520samples%2529%2520against%2520recent%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nearly%20Optimal%20Sample%20Complexity%20for%20Learning%20with%20Label%20Proportions&entry.906535625=Robert%20Busa-Fekete%20and%20Travis%20Dick%20and%20Claudio%20Gentile%20and%20Haim%20Kaplan%20and%20Tomer%20Koren%20and%20Uri%20Stemmer&entry.1292438233=%20%20We%20investigate%20Learning%20from%20Label%20Proportions%20%28LLP%29%2C%20a%20partial%20information%0Asetting%20where%20examples%20in%20a%20training%20set%20are%20grouped%20into%20bags%2C%20and%20only%0Aaggregate%20label%20values%20in%20each%20bag%20are%20available.%20Despite%20the%20partial%0Aobservability%2C%20the%20goal%20is%20still%20to%20achieve%20small%20regret%20at%20the%20level%20of%0Aindividual%20examples.%20We%20give%20results%20on%20the%20sample%20complexity%20of%20LLP%20under%0Asquare%20loss%2C%20showing%20that%20our%20sample%20complexity%20is%20essentially%20optimal.%20From%20an%0Aalgorithmic%20viewpoint%2C%20we%20rely%20on%20carefully%20designed%20variants%20of%20Empirical%20Risk%0AMinimization%2C%20and%20Stochastic%20Gradient%20Descent%20algorithms%2C%20combined%20with%20ad%20hoc%0Avariance%20reduction%20techniques.%20On%20one%20hand%2C%20our%20theoretical%20results%20improve%20in%0Aimportant%20ways%20on%20the%20existing%20literature%20on%20LLP%2C%20specifically%20in%20the%20way%20the%0Asample%20complexity%20depends%20on%20the%20bag%20size.%20On%20the%20other%20hand%2C%20we%20validate%20our%0Aalgorithmic%20solutions%20on%20several%20datasets%2C%20demonstrating%20improved%20empirical%0Aperformance%20%28better%20accuracy%20for%20less%20samples%29%20against%20recent%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05355v1&entry.124074799=Read"},
{"title": "Advances in Protein Representation Learning: Methods, Applications, and\n  Future Directions", "author": "Viet Thanh Duy Nguyen and Truong-Son Hy", "abstract": "  Proteins are complex biomolecules that play a central role in various\nbiological processes, making them critical targets for breakthroughs in\nmolecular biology, medical research, and drug discovery. Deciphering their\nintricate, hierarchical structures, and diverse functions is essential for\nadvancing our understanding of life at the molecular level. Protein\nRepresentation Learning (PRL) has emerged as a transformative approach,\nenabling the extraction of meaningful computational representations from\nprotein data to address these challenges. In this paper, we provide a\ncomprehensive review of PRL research, categorizing methodologies into five key\nareas: feature-based, sequence-based, structure-based, multimodal, and\ncomplex-based approaches. To support researchers in this rapidly evolving\nfield, we introduce widely used databases for protein sequences, structures,\nand functions, which serve as essential resources for model development and\nevaluation. We also explore the diverse applications of these approaches in\nmultiple domains, demonstrating their broad impact. Finally, we discuss\npressing technical challenges and outline future directions to advance PRL,\noffering insights to inspire continued innovation in this foundational field.\n", "link": "http://arxiv.org/abs/2503.16659v2", "date": "2025-05-08", "relevancy": 2.3697, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%20Protein%20Representation%20Learning%3A%20Methods%2C%20Applications%2C%20and%0A%20%20Future%20Directions&body=Title%3A%20Advances%20in%20Protein%20Representation%20Learning%3A%20Methods%2C%20Applications%2C%20and%0A%20%20Future%20Directions%0AAuthor%3A%20Viet%20Thanh%20Duy%20Nguyen%20and%20Truong-Son%20Hy%0AAbstract%3A%20%20%20Proteins%20are%20complex%20biomolecules%20that%20play%20a%20central%20role%20in%20various%0Abiological%20processes%2C%20making%20them%20critical%20targets%20for%20breakthroughs%20in%0Amolecular%20biology%2C%20medical%20research%2C%20and%20drug%20discovery.%20Deciphering%20their%0Aintricate%2C%20hierarchical%20structures%2C%20and%20diverse%20functions%20is%20essential%20for%0Aadvancing%20our%20understanding%20of%20life%20at%20the%20molecular%20level.%20Protein%0ARepresentation%20Learning%20%28PRL%29%20has%20emerged%20as%20a%20transformative%20approach%2C%0Aenabling%20the%20extraction%20of%20meaningful%20computational%20representations%20from%0Aprotein%20data%20to%20address%20these%20challenges.%20In%20this%20paper%2C%20we%20provide%20a%0Acomprehensive%20review%20of%20PRL%20research%2C%20categorizing%20methodologies%20into%20five%20key%0Aareas%3A%20feature-based%2C%20sequence-based%2C%20structure-based%2C%20multimodal%2C%20and%0Acomplex-based%20approaches.%20To%20support%20researchers%20in%20this%20rapidly%20evolving%0Afield%2C%20we%20introduce%20widely%20used%20databases%20for%20protein%20sequences%2C%20structures%2C%0Aand%20functions%2C%20which%20serve%20as%20essential%20resources%20for%20model%20development%20and%0Aevaluation.%20We%20also%20explore%20the%20diverse%20applications%20of%20these%20approaches%20in%0Amultiple%20domains%2C%20demonstrating%20their%20broad%20impact.%20Finally%2C%20we%20discuss%0Apressing%20technical%20challenges%20and%20outline%20future%20directions%20to%20advance%20PRL%2C%0Aoffering%20insights%20to%20inspire%20continued%20innovation%20in%20this%20foundational%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.16659v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%2520Protein%2520Representation%2520Learning%253A%2520Methods%252C%2520Applications%252C%2520and%250A%2520%2520Future%2520Directions%26entry.906535625%3DViet%2520Thanh%2520Duy%2520Nguyen%2520and%2520Truong-Son%2520Hy%26entry.1292438233%3D%2520%2520Proteins%2520are%2520complex%2520biomolecules%2520that%2520play%2520a%2520central%2520role%2520in%2520various%250Abiological%2520processes%252C%2520making%2520them%2520critical%2520targets%2520for%2520breakthroughs%2520in%250Amolecular%2520biology%252C%2520medical%2520research%252C%2520and%2520drug%2520discovery.%2520Deciphering%2520their%250Aintricate%252C%2520hierarchical%2520structures%252C%2520and%2520diverse%2520functions%2520is%2520essential%2520for%250Aadvancing%2520our%2520understanding%2520of%2520life%2520at%2520the%2520molecular%2520level.%2520Protein%250ARepresentation%2520Learning%2520%2528PRL%2529%2520has%2520emerged%2520as%2520a%2520transformative%2520approach%252C%250Aenabling%2520the%2520extraction%2520of%2520meaningful%2520computational%2520representations%2520from%250Aprotein%2520data%2520to%2520address%2520these%2520challenges.%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%250Acomprehensive%2520review%2520of%2520PRL%2520research%252C%2520categorizing%2520methodologies%2520into%2520five%2520key%250Aareas%253A%2520feature-based%252C%2520sequence-based%252C%2520structure-based%252C%2520multimodal%252C%2520and%250Acomplex-based%2520approaches.%2520To%2520support%2520researchers%2520in%2520this%2520rapidly%2520evolving%250Afield%252C%2520we%2520introduce%2520widely%2520used%2520databases%2520for%2520protein%2520sequences%252C%2520structures%252C%250Aand%2520functions%252C%2520which%2520serve%2520as%2520essential%2520resources%2520for%2520model%2520development%2520and%250Aevaluation.%2520We%2520also%2520explore%2520the%2520diverse%2520applications%2520of%2520these%2520approaches%2520in%250Amultiple%2520domains%252C%2520demonstrating%2520their%2520broad%2520impact.%2520Finally%252C%2520we%2520discuss%250Apressing%2520technical%2520challenges%2520and%2520outline%2520future%2520directions%2520to%2520advance%2520PRL%252C%250Aoffering%2520insights%2520to%2520inspire%2520continued%2520innovation%2520in%2520this%2520foundational%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16659v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%20Protein%20Representation%20Learning%3A%20Methods%2C%20Applications%2C%20and%0A%20%20Future%20Directions&entry.906535625=Viet%20Thanh%20Duy%20Nguyen%20and%20Truong-Son%20Hy&entry.1292438233=%20%20Proteins%20are%20complex%20biomolecules%20that%20play%20a%20central%20role%20in%20various%0Abiological%20processes%2C%20making%20them%20critical%20targets%20for%20breakthroughs%20in%0Amolecular%20biology%2C%20medical%20research%2C%20and%20drug%20discovery.%20Deciphering%20their%0Aintricate%2C%20hierarchical%20structures%2C%20and%20diverse%20functions%20is%20essential%20for%0Aadvancing%20our%20understanding%20of%20life%20at%20the%20molecular%20level.%20Protein%0ARepresentation%20Learning%20%28PRL%29%20has%20emerged%20as%20a%20transformative%20approach%2C%0Aenabling%20the%20extraction%20of%20meaningful%20computational%20representations%20from%0Aprotein%20data%20to%20address%20these%20challenges.%20In%20this%20paper%2C%20we%20provide%20a%0Acomprehensive%20review%20of%20PRL%20research%2C%20categorizing%20methodologies%20into%20five%20key%0Aareas%3A%20feature-based%2C%20sequence-based%2C%20structure-based%2C%20multimodal%2C%20and%0Acomplex-based%20approaches.%20To%20support%20researchers%20in%20this%20rapidly%20evolving%0Afield%2C%20we%20introduce%20widely%20used%20databases%20for%20protein%20sequences%2C%20structures%2C%0Aand%20functions%2C%20which%20serve%20as%20essential%20resources%20for%20model%20development%20and%0Aevaluation.%20We%20also%20explore%20the%20diverse%20applications%20of%20these%20approaches%20in%0Amultiple%20domains%2C%20demonstrating%20their%20broad%20impact.%20Finally%2C%20we%20discuss%0Apressing%20technical%20challenges%20and%20outline%20future%20directions%20to%20advance%20PRL%2C%0Aoffering%20insights%20to%20inspire%20continued%20innovation%20in%20this%20foundational%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.16659v2&entry.124074799=Read"},
{"title": "Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation", "author": "Chao Liao and Liyang Liu and Xun Wang and Zhengxiong Luo and Xinyu Zhang and Wenliang Zhao and Jie Wu and Liang Li and Zhi Tian and Weilin Huang", "abstract": "  Recent progress in unified models for image understanding and generation has\nbeen impressive, yet most approaches remain limited to single-modal generation\nconditioned on multiple modalities. In this paper, we present Mogao, a unified\nframework that advances this paradigm by enabling interleaved multi-modal\ngeneration through a causal approach. Mogao integrates a set of key technical\nimprovements in architecture design, including a deep-fusion design, dual\nvision encoders, interleaved rotary position embeddings, and multi-modal\nclassifier-free guidance, which allow it to harness the strengths of both\nautoregressive models for text generation and diffusion models for high-quality\nimage synthesis. These practical improvements also make Mogao particularly\neffective to process interleaved sequences of text and images arbitrarily. To\nfurther unlock the potential of unified models, we introduce an efficient\ntraining strategy on a large-scale, in-house dataset specifically curated for\njoint text and image generation. Extensive experiments show that Mogao not only\nachieves state-of-the-art performance in multi-modal understanding and\ntext-to-image generation, but also excels in producing high-quality, coherent\ninterleaved outputs. Its emergent capabilities in zero-shot image editing and\ncompositional generation highlight Mogao as a practical omni-modal foundation\nmodel, paving the way for future development and scaling the unified\nmulti-modal systems.\n", "link": "http://arxiv.org/abs/2505.05472v1", "date": "2025-05-08", "relevancy": 2.3452, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6159}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5846}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mogao%3A%20An%20Omni%20Foundation%20Model%20for%20Interleaved%20Multi-Modal%20Generation&body=Title%3A%20Mogao%3A%20An%20Omni%20Foundation%20Model%20for%20Interleaved%20Multi-Modal%20Generation%0AAuthor%3A%20Chao%20Liao%20and%20Liyang%20Liu%20and%20Xun%20Wang%20and%20Zhengxiong%20Luo%20and%20Xinyu%20Zhang%20and%20Wenliang%20Zhao%20and%20Jie%20Wu%20and%20Liang%20Li%20and%20Zhi%20Tian%20and%20Weilin%20Huang%0AAbstract%3A%20%20%20Recent%20progress%20in%20unified%20models%20for%20image%20understanding%20and%20generation%20has%0Abeen%20impressive%2C%20yet%20most%20approaches%20remain%20limited%20to%20single-modal%20generation%0Aconditioned%20on%20multiple%20modalities.%20In%20this%20paper%2C%20we%20present%20Mogao%2C%20a%20unified%0Aframework%20that%20advances%20this%20paradigm%20by%20enabling%20interleaved%20multi-modal%0Ageneration%20through%20a%20causal%20approach.%20Mogao%20integrates%20a%20set%20of%20key%20technical%0Aimprovements%20in%20architecture%20design%2C%20including%20a%20deep-fusion%20design%2C%20dual%0Avision%20encoders%2C%20interleaved%20rotary%20position%20embeddings%2C%20and%20multi-modal%0Aclassifier-free%20guidance%2C%20which%20allow%20it%20to%20harness%20the%20strengths%20of%20both%0Aautoregressive%20models%20for%20text%20generation%20and%20diffusion%20models%20for%20high-quality%0Aimage%20synthesis.%20These%20practical%20improvements%20also%20make%20Mogao%20particularly%0Aeffective%20to%20process%20interleaved%20sequences%20of%20text%20and%20images%20arbitrarily.%20To%0Afurther%20unlock%20the%20potential%20of%20unified%20models%2C%20we%20introduce%20an%20efficient%0Atraining%20strategy%20on%20a%20large-scale%2C%20in-house%20dataset%20specifically%20curated%20for%0Ajoint%20text%20and%20image%20generation.%20Extensive%20experiments%20show%20that%20Mogao%20not%20only%0Aachieves%20state-of-the-art%20performance%20in%20multi-modal%20understanding%20and%0Atext-to-image%20generation%2C%20but%20also%20excels%20in%20producing%20high-quality%2C%20coherent%0Ainterleaved%20outputs.%20Its%20emergent%20capabilities%20in%20zero-shot%20image%20editing%20and%0Acompositional%20generation%20highlight%20Mogao%20as%20a%20practical%20omni-modal%20foundation%0Amodel%2C%20paving%20the%20way%20for%20future%20development%20and%20scaling%20the%20unified%0Amulti-modal%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMogao%253A%2520An%2520Omni%2520Foundation%2520Model%2520for%2520Interleaved%2520Multi-Modal%2520Generation%26entry.906535625%3DChao%2520Liao%2520and%2520Liyang%2520Liu%2520and%2520Xun%2520Wang%2520and%2520Zhengxiong%2520Luo%2520and%2520Xinyu%2520Zhang%2520and%2520Wenliang%2520Zhao%2520and%2520Jie%2520Wu%2520and%2520Liang%2520Li%2520and%2520Zhi%2520Tian%2520and%2520Weilin%2520Huang%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520unified%2520models%2520for%2520image%2520understanding%2520and%2520generation%2520has%250Abeen%2520impressive%252C%2520yet%2520most%2520approaches%2520remain%2520limited%2520to%2520single-modal%2520generation%250Aconditioned%2520on%2520multiple%2520modalities.%2520In%2520this%2520paper%252C%2520we%2520present%2520Mogao%252C%2520a%2520unified%250Aframework%2520that%2520advances%2520this%2520paradigm%2520by%2520enabling%2520interleaved%2520multi-modal%250Ageneration%2520through%2520a%2520causal%2520approach.%2520Mogao%2520integrates%2520a%2520set%2520of%2520key%2520technical%250Aimprovements%2520in%2520architecture%2520design%252C%2520including%2520a%2520deep-fusion%2520design%252C%2520dual%250Avision%2520encoders%252C%2520interleaved%2520rotary%2520position%2520embeddings%252C%2520and%2520multi-modal%250Aclassifier-free%2520guidance%252C%2520which%2520allow%2520it%2520to%2520harness%2520the%2520strengths%2520of%2520both%250Aautoregressive%2520models%2520for%2520text%2520generation%2520and%2520diffusion%2520models%2520for%2520high-quality%250Aimage%2520synthesis.%2520These%2520practical%2520improvements%2520also%2520make%2520Mogao%2520particularly%250Aeffective%2520to%2520process%2520interleaved%2520sequences%2520of%2520text%2520and%2520images%2520arbitrarily.%2520To%250Afurther%2520unlock%2520the%2520potential%2520of%2520unified%2520models%252C%2520we%2520introduce%2520an%2520efficient%250Atraining%2520strategy%2520on%2520a%2520large-scale%252C%2520in-house%2520dataset%2520specifically%2520curated%2520for%250Ajoint%2520text%2520and%2520image%2520generation.%2520Extensive%2520experiments%2520show%2520that%2520Mogao%2520not%2520only%250Aachieves%2520state-of-the-art%2520performance%2520in%2520multi-modal%2520understanding%2520and%250Atext-to-image%2520generation%252C%2520but%2520also%2520excels%2520in%2520producing%2520high-quality%252C%2520coherent%250Ainterleaved%2520outputs.%2520Its%2520emergent%2520capabilities%2520in%2520zero-shot%2520image%2520editing%2520and%250Acompositional%2520generation%2520highlight%2520Mogao%2520as%2520a%2520practical%2520omni-modal%2520foundation%250Amodel%252C%2520paving%2520the%2520way%2520for%2520future%2520development%2520and%2520scaling%2520the%2520unified%250Amulti-modal%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mogao%3A%20An%20Omni%20Foundation%20Model%20for%20Interleaved%20Multi-Modal%20Generation&entry.906535625=Chao%20Liao%20and%20Liyang%20Liu%20and%20Xun%20Wang%20and%20Zhengxiong%20Luo%20and%20Xinyu%20Zhang%20and%20Wenliang%20Zhao%20and%20Jie%20Wu%20and%20Liang%20Li%20and%20Zhi%20Tian%20and%20Weilin%20Huang&entry.1292438233=%20%20Recent%20progress%20in%20unified%20models%20for%20image%20understanding%20and%20generation%20has%0Abeen%20impressive%2C%20yet%20most%20approaches%20remain%20limited%20to%20single-modal%20generation%0Aconditioned%20on%20multiple%20modalities.%20In%20this%20paper%2C%20we%20present%20Mogao%2C%20a%20unified%0Aframework%20that%20advances%20this%20paradigm%20by%20enabling%20interleaved%20multi-modal%0Ageneration%20through%20a%20causal%20approach.%20Mogao%20integrates%20a%20set%20of%20key%20technical%0Aimprovements%20in%20architecture%20design%2C%20including%20a%20deep-fusion%20design%2C%20dual%0Avision%20encoders%2C%20interleaved%20rotary%20position%20embeddings%2C%20and%20multi-modal%0Aclassifier-free%20guidance%2C%20which%20allow%20it%20to%20harness%20the%20strengths%20of%20both%0Aautoregressive%20models%20for%20text%20generation%20and%20diffusion%20models%20for%20high-quality%0Aimage%20synthesis.%20These%20practical%20improvements%20also%20make%20Mogao%20particularly%0Aeffective%20to%20process%20interleaved%20sequences%20of%20text%20and%20images%20arbitrarily.%20To%0Afurther%20unlock%20the%20potential%20of%20unified%20models%2C%20we%20introduce%20an%20efficient%0Atraining%20strategy%20on%20a%20large-scale%2C%20in-house%20dataset%20specifically%20curated%20for%0Ajoint%20text%20and%20image%20generation.%20Extensive%20experiments%20show%20that%20Mogao%20not%20only%0Aachieves%20state-of-the-art%20performance%20in%20multi-modal%20understanding%20and%0Atext-to-image%20generation%2C%20but%20also%20excels%20in%20producing%20high-quality%2C%20coherent%0Ainterleaved%20outputs.%20Its%20emergent%20capabilities%20in%20zero-shot%20image%20editing%20and%0Acompositional%20generation%20highlight%20Mogao%20as%20a%20practical%20omni-modal%20foundation%0Amodel%2C%20paving%20the%20way%20for%20future%20development%20and%20scaling%20the%20unified%0Amulti-modal%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05472v1&entry.124074799=Read"},
{"title": "DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event\n  Argument Extraction with Slot Querying", "author": "Guanghui Wang and Dexi Liu and Jian-Yun Nie and Qizhi Wan and Rong Hu and Xiping Liu and Wanlong Liu and Jiaming Liu", "abstract": "  Recent advancements in event argument extraction (EAE) involve incorporating\nuseful auxiliary information into models during training and inference, such as\nretrieved instances and event templates. These methods face two challenges: (1)\nthe retrieval results may be irrelevant and (2) templates are developed\nindependently for each event without considering their possible relationship.\nIn this work, we propose DEGAP to address these challenges through a simple yet\neffective components: dual prefixes, i.e. learnable prompt vectors, where the\ninstance-oriented prefix and template-oriented prefix are trained to learn\ninformation from different event instances and templates. Additionally, we\npropose an event-guided adaptive gating mechanism, which can adaptively\nleverage possible connections between different events and thus capture\nrelevant information from the prefix. Finally, these event-guided prefixes\nprovide relevant information as cues to EAE model without retrieval. Extensive\nexperiments demonstrate that our method achieves new state-of-the-art\nperformance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further\nanalysis shows the impact of different components.\n", "link": "http://arxiv.org/abs/2405.13325v3", "date": "2025-05-08", "relevancy": 2.344, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4909}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEGAP%3A%20Dual%20Event-Guided%20Adaptive%20Prefixes%20for%20Templated-Based%20Event%0A%20%20Argument%20Extraction%20with%20Slot%20Querying&body=Title%3A%20DEGAP%3A%20Dual%20Event-Guided%20Adaptive%20Prefixes%20for%20Templated-Based%20Event%0A%20%20Argument%20Extraction%20with%20Slot%20Querying%0AAuthor%3A%20Guanghui%20Wang%20and%20Dexi%20Liu%20and%20Jian-Yun%20Nie%20and%20Qizhi%20Wan%20and%20Rong%20Hu%20and%20Xiping%20Liu%20and%20Wanlong%20Liu%20and%20Jiaming%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20event%20argument%20extraction%20%28EAE%29%20involve%20incorporating%0Auseful%20auxiliary%20information%20into%20models%20during%20training%20and%20inference%2C%20such%20as%0Aretrieved%20instances%20and%20event%20templates.%20These%20methods%20face%20two%20challenges%3A%20%281%29%0Athe%20retrieval%20results%20may%20be%20irrelevant%20and%20%282%29%20templates%20are%20developed%0Aindependently%20for%20each%20event%20without%20considering%20their%20possible%20relationship.%0AIn%20this%20work%2C%20we%20propose%20DEGAP%20to%20address%20these%20challenges%20through%20a%20simple%20yet%0Aeffective%20components%3A%20dual%20prefixes%2C%20i.e.%20learnable%20prompt%20vectors%2C%20where%20the%0Ainstance-oriented%20prefix%20and%20template-oriented%20prefix%20are%20trained%20to%20learn%0Ainformation%20from%20different%20event%20instances%20and%20templates.%20Additionally%2C%20we%0Apropose%20an%20event-guided%20adaptive%20gating%20mechanism%2C%20which%20can%20adaptively%0Aleverage%20possible%20connections%20between%20different%20events%20and%20thus%20capture%0Arelevant%20information%20from%20the%20prefix.%20Finally%2C%20these%20event-guided%20prefixes%0Aprovide%20relevant%20information%20as%20cues%20to%20EAE%20model%20without%20retrieval.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20new%20state-of-the-art%0Aperformance%20on%20four%20datasets%20%28ACE05%2C%20RAMS%2C%20WIKIEVENTS%2C%20and%20MLEE%29.%20Further%0Aanalysis%20shows%20the%20impact%20of%20different%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13325v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEGAP%253A%2520Dual%2520Event-Guided%2520Adaptive%2520Prefixes%2520for%2520Templated-Based%2520Event%250A%2520%2520Argument%2520Extraction%2520with%2520Slot%2520Querying%26entry.906535625%3DGuanghui%2520Wang%2520and%2520Dexi%2520Liu%2520and%2520Jian-Yun%2520Nie%2520and%2520Qizhi%2520Wan%2520and%2520Rong%2520Hu%2520and%2520Xiping%2520Liu%2520and%2520Wanlong%2520Liu%2520and%2520Jiaming%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520event%2520argument%2520extraction%2520%2528EAE%2529%2520involve%2520incorporating%250Auseful%2520auxiliary%2520information%2520into%2520models%2520during%2520training%2520and%2520inference%252C%2520such%2520as%250Aretrieved%2520instances%2520and%2520event%2520templates.%2520These%2520methods%2520face%2520two%2520challenges%253A%2520%25281%2529%250Athe%2520retrieval%2520results%2520may%2520be%2520irrelevant%2520and%2520%25282%2529%2520templates%2520are%2520developed%250Aindependently%2520for%2520each%2520event%2520without%2520considering%2520their%2520possible%2520relationship.%250AIn%2520this%2520work%252C%2520we%2520propose%2520DEGAP%2520to%2520address%2520these%2520challenges%2520through%2520a%2520simple%2520yet%250Aeffective%2520components%253A%2520dual%2520prefixes%252C%2520i.e.%2520learnable%2520prompt%2520vectors%252C%2520where%2520the%250Ainstance-oriented%2520prefix%2520and%2520template-oriented%2520prefix%2520are%2520trained%2520to%2520learn%250Ainformation%2520from%2520different%2520event%2520instances%2520and%2520templates.%2520Additionally%252C%2520we%250Apropose%2520an%2520event-guided%2520adaptive%2520gating%2520mechanism%252C%2520which%2520can%2520adaptively%250Aleverage%2520possible%2520connections%2520between%2520different%2520events%2520and%2520thus%2520capture%250Arelevant%2520information%2520from%2520the%2520prefix.%2520Finally%252C%2520these%2520event-guided%2520prefixes%250Aprovide%2520relevant%2520information%2520as%2520cues%2520to%2520EAE%2520model%2520without%2520retrieval.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520new%2520state-of-the-art%250Aperformance%2520on%2520four%2520datasets%2520%2528ACE05%252C%2520RAMS%252C%2520WIKIEVENTS%252C%2520and%2520MLEE%2529.%2520Further%250Aanalysis%2520shows%2520the%2520impact%2520of%2520different%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13325v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEGAP%3A%20Dual%20Event-Guided%20Adaptive%20Prefixes%20for%20Templated-Based%20Event%0A%20%20Argument%20Extraction%20with%20Slot%20Querying&entry.906535625=Guanghui%20Wang%20and%20Dexi%20Liu%20and%20Jian-Yun%20Nie%20and%20Qizhi%20Wan%20and%20Rong%20Hu%20and%20Xiping%20Liu%20and%20Wanlong%20Liu%20and%20Jiaming%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%20event%20argument%20extraction%20%28EAE%29%20involve%20incorporating%0Auseful%20auxiliary%20information%20into%20models%20during%20training%20and%20inference%2C%20such%20as%0Aretrieved%20instances%20and%20event%20templates.%20These%20methods%20face%20two%20challenges%3A%20%281%29%0Athe%20retrieval%20results%20may%20be%20irrelevant%20and%20%282%29%20templates%20are%20developed%0Aindependently%20for%20each%20event%20without%20considering%20their%20possible%20relationship.%0AIn%20this%20work%2C%20we%20propose%20DEGAP%20to%20address%20these%20challenges%20through%20a%20simple%20yet%0Aeffective%20components%3A%20dual%20prefixes%2C%20i.e.%20learnable%20prompt%20vectors%2C%20where%20the%0Ainstance-oriented%20prefix%20and%20template-oriented%20prefix%20are%20trained%20to%20learn%0Ainformation%20from%20different%20event%20instances%20and%20templates.%20Additionally%2C%20we%0Apropose%20an%20event-guided%20adaptive%20gating%20mechanism%2C%20which%20can%20adaptively%0Aleverage%20possible%20connections%20between%20different%20events%20and%20thus%20capture%0Arelevant%20information%20from%20the%20prefix.%20Finally%2C%20these%20event-guided%20prefixes%0Aprovide%20relevant%20information%20as%20cues%20to%20EAE%20model%20without%20retrieval.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20new%20state-of-the-art%0Aperformance%20on%20four%20datasets%20%28ACE05%2C%20RAMS%2C%20WIKIEVENTS%2C%20and%20MLEE%29.%20Further%0Aanalysis%20shows%20the%20impact%20of%20different%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13325v3&entry.124074799=Read"},
{"title": "MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for\n  PET Denoising", "author": "Xiaolong Niu and Zanting Ye and Xu Han and Yanchao Huang and Hao Sun and Hubing Wu and Lijun Lu", "abstract": "  Acquiring high-quality Positron Emission Tomography (PET) images requires\nadministering high-dose radiotracers, which increases radiation exposure risks.\nGenerating standard-dose PET (SPET) from low-dose PET (LPET) has become a\npotential solution. However, previous studies have primarily focused on single\nlow-dose PET denoising, neglecting two critical factors: discrepancies in dose\nresponse caused by inter-patient variability, and complementary anatomical\nconstraints derived from CT images. In this work, we propose a novel CT-Guided\nMulti-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for\nmulti-dose PET denoising. Our approach integrates anatomical guidance and\ndose-level adaptation to achieve superior denoising performance under low-dose\nconditions. Specifically, this approach incorporates a CT-Guided High-frequency\nWavelet Attention (HWA) module, which uses wavelet transforms to separate\nhigh-frequency anatomical boundary features from CT images. These extracted\nfeatures are then incorporated into PET imaging through an adaptive weighted\nfusion mechanism to enhance edge details. Additionally, we propose the\nDose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism\nthat dynamically integrates dose levels into channel-spatial attention weight\ncalculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets\ndemonstrate that MDAA-Diff outperforms state-of-the-art approaches in\npreserving diagnostic quality under reduced-dose conditions. Our code is\npublicly available.\n", "link": "http://arxiv.org/abs/2505.05112v1", "date": "2025-05-08", "relevancy": 2.3149, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5968}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5922}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDAA-Diff%3A%20CT-Guided%20Multi-Dose%20Adaptive%20Attention%20Diffusion%20Model%20for%0A%20%20PET%20Denoising&body=Title%3A%20MDAA-Diff%3A%20CT-Guided%20Multi-Dose%20Adaptive%20Attention%20Diffusion%20Model%20for%0A%20%20PET%20Denoising%0AAuthor%3A%20Xiaolong%20Niu%20and%20Zanting%20Ye%20and%20Xu%20Han%20and%20Yanchao%20Huang%20and%20Hao%20Sun%20and%20Hubing%20Wu%20and%20Lijun%20Lu%0AAbstract%3A%20%20%20Acquiring%20high-quality%20Positron%20Emission%20Tomography%20%28PET%29%20images%20requires%0Aadministering%20high-dose%20radiotracers%2C%20which%20increases%20radiation%20exposure%20risks.%0AGenerating%20standard-dose%20PET%20%28SPET%29%20from%20low-dose%20PET%20%28LPET%29%20has%20become%20a%0Apotential%20solution.%20However%2C%20previous%20studies%20have%20primarily%20focused%20on%20single%0Alow-dose%20PET%20denoising%2C%20neglecting%20two%20critical%20factors%3A%20discrepancies%20in%20dose%0Aresponse%20caused%20by%20inter-patient%20variability%2C%20and%20complementary%20anatomical%0Aconstraints%20derived%20from%20CT%20images.%20In%20this%20work%2C%20we%20propose%20a%20novel%20CT-Guided%0AMulti-dose%20Adaptive%20Attention%20Denoising%20Diffusion%20Model%20%28MDAA-Diff%29%20for%0Amulti-dose%20PET%20denoising.%20Our%20approach%20integrates%20anatomical%20guidance%20and%0Adose-level%20adaptation%20to%20achieve%20superior%20denoising%20performance%20under%20low-dose%0Aconditions.%20Specifically%2C%20this%20approach%20incorporates%20a%20CT-Guided%20High-frequency%0AWavelet%20Attention%20%28HWA%29%20module%2C%20which%20uses%20wavelet%20transforms%20to%20separate%0Ahigh-frequency%20anatomical%20boundary%20features%20from%20CT%20images.%20These%20extracted%0Afeatures%20are%20then%20incorporated%20into%20PET%20imaging%20through%20an%20adaptive%20weighted%0Afusion%20mechanism%20to%20enhance%20edge%20details.%20Additionally%2C%20we%20propose%20the%0ADose-Adaptive%20Attention%20%28DAA%29%20module%2C%20a%20dose-conditioned%20enhancement%20mechanism%0Athat%20dynamically%20integrates%20dose%20levels%20into%20channel-spatial%20attention%20weight%0Acalculation.%20Extensive%20experiments%20on%2018F-FDG%20and%2068Ga-FAPI%20datasets%0Ademonstrate%20that%20MDAA-Diff%20outperforms%20state-of-the-art%20approaches%20in%0Apreserving%20diagnostic%20quality%20under%20reduced-dose%20conditions.%20Our%20code%20is%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDAA-Diff%253A%2520CT-Guided%2520Multi-Dose%2520Adaptive%2520Attention%2520Diffusion%2520Model%2520for%250A%2520%2520PET%2520Denoising%26entry.906535625%3DXiaolong%2520Niu%2520and%2520Zanting%2520Ye%2520and%2520Xu%2520Han%2520and%2520Yanchao%2520Huang%2520and%2520Hao%2520Sun%2520and%2520Hubing%2520Wu%2520and%2520Lijun%2520Lu%26entry.1292438233%3D%2520%2520Acquiring%2520high-quality%2520Positron%2520Emission%2520Tomography%2520%2528PET%2529%2520images%2520requires%250Aadministering%2520high-dose%2520radiotracers%252C%2520which%2520increases%2520radiation%2520exposure%2520risks.%250AGenerating%2520standard-dose%2520PET%2520%2528SPET%2529%2520from%2520low-dose%2520PET%2520%2528LPET%2529%2520has%2520become%2520a%250Apotential%2520solution.%2520However%252C%2520previous%2520studies%2520have%2520primarily%2520focused%2520on%2520single%250Alow-dose%2520PET%2520denoising%252C%2520neglecting%2520two%2520critical%2520factors%253A%2520discrepancies%2520in%2520dose%250Aresponse%2520caused%2520by%2520inter-patient%2520variability%252C%2520and%2520complementary%2520anatomical%250Aconstraints%2520derived%2520from%2520CT%2520images.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520CT-Guided%250AMulti-dose%2520Adaptive%2520Attention%2520Denoising%2520Diffusion%2520Model%2520%2528MDAA-Diff%2529%2520for%250Amulti-dose%2520PET%2520denoising.%2520Our%2520approach%2520integrates%2520anatomical%2520guidance%2520and%250Adose-level%2520adaptation%2520to%2520achieve%2520superior%2520denoising%2520performance%2520under%2520low-dose%250Aconditions.%2520Specifically%252C%2520this%2520approach%2520incorporates%2520a%2520CT-Guided%2520High-frequency%250AWavelet%2520Attention%2520%2528HWA%2529%2520module%252C%2520which%2520uses%2520wavelet%2520transforms%2520to%2520separate%250Ahigh-frequency%2520anatomical%2520boundary%2520features%2520from%2520CT%2520images.%2520These%2520extracted%250Afeatures%2520are%2520then%2520incorporated%2520into%2520PET%2520imaging%2520through%2520an%2520adaptive%2520weighted%250Afusion%2520mechanism%2520to%2520enhance%2520edge%2520details.%2520Additionally%252C%2520we%2520propose%2520the%250ADose-Adaptive%2520Attention%2520%2528DAA%2529%2520module%252C%2520a%2520dose-conditioned%2520enhancement%2520mechanism%250Athat%2520dynamically%2520integrates%2520dose%2520levels%2520into%2520channel-spatial%2520attention%2520weight%250Acalculation.%2520Extensive%2520experiments%2520on%252018F-FDG%2520and%252068Ga-FAPI%2520datasets%250Ademonstrate%2520that%2520MDAA-Diff%2520outperforms%2520state-of-the-art%2520approaches%2520in%250Apreserving%2520diagnostic%2520quality%2520under%2520reduced-dose%2520conditions.%2520Our%2520code%2520is%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDAA-Diff%3A%20CT-Guided%20Multi-Dose%20Adaptive%20Attention%20Diffusion%20Model%20for%0A%20%20PET%20Denoising&entry.906535625=Xiaolong%20Niu%20and%20Zanting%20Ye%20and%20Xu%20Han%20and%20Yanchao%20Huang%20and%20Hao%20Sun%20and%20Hubing%20Wu%20and%20Lijun%20Lu&entry.1292438233=%20%20Acquiring%20high-quality%20Positron%20Emission%20Tomography%20%28PET%29%20images%20requires%0Aadministering%20high-dose%20radiotracers%2C%20which%20increases%20radiation%20exposure%20risks.%0AGenerating%20standard-dose%20PET%20%28SPET%29%20from%20low-dose%20PET%20%28LPET%29%20has%20become%20a%0Apotential%20solution.%20However%2C%20previous%20studies%20have%20primarily%20focused%20on%20single%0Alow-dose%20PET%20denoising%2C%20neglecting%20two%20critical%20factors%3A%20discrepancies%20in%20dose%0Aresponse%20caused%20by%20inter-patient%20variability%2C%20and%20complementary%20anatomical%0Aconstraints%20derived%20from%20CT%20images.%20In%20this%20work%2C%20we%20propose%20a%20novel%20CT-Guided%0AMulti-dose%20Adaptive%20Attention%20Denoising%20Diffusion%20Model%20%28MDAA-Diff%29%20for%0Amulti-dose%20PET%20denoising.%20Our%20approach%20integrates%20anatomical%20guidance%20and%0Adose-level%20adaptation%20to%20achieve%20superior%20denoising%20performance%20under%20low-dose%0Aconditions.%20Specifically%2C%20this%20approach%20incorporates%20a%20CT-Guided%20High-frequency%0AWavelet%20Attention%20%28HWA%29%20module%2C%20which%20uses%20wavelet%20transforms%20to%20separate%0Ahigh-frequency%20anatomical%20boundary%20features%20from%20CT%20images.%20These%20extracted%0Afeatures%20are%20then%20incorporated%20into%20PET%20imaging%20through%20an%20adaptive%20weighted%0Afusion%20mechanism%20to%20enhance%20edge%20details.%20Additionally%2C%20we%20propose%20the%0ADose-Adaptive%20Attention%20%28DAA%29%20module%2C%20a%20dose-conditioned%20enhancement%20mechanism%0Athat%20dynamically%20integrates%20dose%20levels%20into%20channel-spatial%20attention%20weight%0Acalculation.%20Extensive%20experiments%20on%2018F-FDG%20and%2068Ga-FAPI%20datasets%0Ademonstrate%20that%20MDAA-Diff%20outperforms%20state-of-the-art%20approaches%20in%0Apreserving%20diagnostic%20quality%20under%20reduced-dose%20conditions.%20Our%20code%20is%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05112v1&entry.124074799=Read"},
{"title": "X-Driver: Explainable Autonomous Driving with Vision-Language Models", "author": "Wei Liu and Jiyuan Zhang and Binxiong Zheng and Yufeng Hu and Yingzhan Lin and Zengfeng Zeng", "abstract": "  End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving.\n", "link": "http://arxiv.org/abs/2505.05098v1", "date": "2025-05-08", "relevancy": 2.3121, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5855}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5855}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-Driver%3A%20Explainable%20Autonomous%20Driving%20with%20Vision-Language%20Models&body=Title%3A%20X-Driver%3A%20Explainable%20Autonomous%20Driving%20with%20Vision-Language%20Models%0AAuthor%3A%20Wei%20Liu%20and%20Jiyuan%20Zhang%20and%20Binxiong%20Zheng%20and%20Yufeng%20Hu%20and%20Yingzhan%20Lin%20and%20Zengfeng%20Zeng%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20has%20advanced%20significantly%2C%20offering%20benefits%0Asuch%20as%20system%20simplicity%20and%20stronger%20driving%20performance%20in%20both%20open-loop%0Aand%20closed-loop%20settings%20than%20conventional%20pipelines.%20However%2C%20existing%0Aframeworks%20still%20suffer%20from%20low%20success%20rates%20in%20closed-loop%20evaluations%2C%0Ahighlighting%20their%20limitations%20in%20real-world%20deployment.%20In%20this%20paper%2C%20we%0Aintroduce%20X-Driver%2C%20a%20unified%20multi-modal%20large%20language%20models%28MLLMs%29%0Aframework%20designed%20for%20closed-loop%20autonomous%20driving%2C%20leveraging%0AChain-of-Thought%28CoT%29%20and%20autoregressive%20modeling%20to%20enhance%20perception%20and%0Adecision-making.%20We%20validate%20X-Driver%20across%20multiple%20autonomous%20driving%20tasks%0Ausing%20public%20benchmarks%20in%20CARLA%20simulation%20environment%2C%20including%0ABench2Drive%5B6%5D.%20Our%20experimental%20results%20demonstrate%20superior%20closed-loop%0Aperformance%2C%20surpassing%20the%20current%20state-of-the-art%28SOTA%29%20while%20improving%20the%0Ainterpretability%20of%20driving%20decisions.%20These%20findings%20underscore%20the%20importance%0Aof%20structured%20reasoning%20in%20end-to-end%20driving%20and%20establish%20X-Driver%20as%20a%0Astrong%20baseline%20for%20future%20research%20in%20closed-loop%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-Driver%253A%2520Explainable%2520Autonomous%2520Driving%2520with%2520Vision-Language%2520Models%26entry.906535625%3DWei%2520Liu%2520and%2520Jiyuan%2520Zhang%2520and%2520Binxiong%2520Zheng%2520and%2520Yufeng%2520Hu%2520and%2520Yingzhan%2520Lin%2520and%2520Zengfeng%2520Zeng%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520has%2520advanced%2520significantly%252C%2520offering%2520benefits%250Asuch%2520as%2520system%2520simplicity%2520and%2520stronger%2520driving%2520performance%2520in%2520both%2520open-loop%250Aand%2520closed-loop%2520settings%2520than%2520conventional%2520pipelines.%2520However%252C%2520existing%250Aframeworks%2520still%2520suffer%2520from%2520low%2520success%2520rates%2520in%2520closed-loop%2520evaluations%252C%250Ahighlighting%2520their%2520limitations%2520in%2520real-world%2520deployment.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520X-Driver%252C%2520a%2520unified%2520multi-modal%2520large%2520language%2520models%2528MLLMs%2529%250Aframework%2520designed%2520for%2520closed-loop%2520autonomous%2520driving%252C%2520leveraging%250AChain-of-Thought%2528CoT%2529%2520and%2520autoregressive%2520modeling%2520to%2520enhance%2520perception%2520and%250Adecision-making.%2520We%2520validate%2520X-Driver%2520across%2520multiple%2520autonomous%2520driving%2520tasks%250Ausing%2520public%2520benchmarks%2520in%2520CARLA%2520simulation%2520environment%252C%2520including%250ABench2Drive%255B6%255D.%2520Our%2520experimental%2520results%2520demonstrate%2520superior%2520closed-loop%250Aperformance%252C%2520surpassing%2520the%2520current%2520state-of-the-art%2528SOTA%2529%2520while%2520improving%2520the%250Ainterpretability%2520of%2520driving%2520decisions.%2520These%2520findings%2520underscore%2520the%2520importance%250Aof%2520structured%2520reasoning%2520in%2520end-to-end%2520driving%2520and%2520establish%2520X-Driver%2520as%2520a%250Astrong%2520baseline%2520for%2520future%2520research%2520in%2520closed-loop%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Driver%3A%20Explainable%20Autonomous%20Driving%20with%20Vision-Language%20Models&entry.906535625=Wei%20Liu%20and%20Jiyuan%20Zhang%20and%20Binxiong%20Zheng%20and%20Yufeng%20Hu%20and%20Yingzhan%20Lin%20and%20Zengfeng%20Zeng&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20has%20advanced%20significantly%2C%20offering%20benefits%0Asuch%20as%20system%20simplicity%20and%20stronger%20driving%20performance%20in%20both%20open-loop%0Aand%20closed-loop%20settings%20than%20conventional%20pipelines.%20However%2C%20existing%0Aframeworks%20still%20suffer%20from%20low%20success%20rates%20in%20closed-loop%20evaluations%2C%0Ahighlighting%20their%20limitations%20in%20real-world%20deployment.%20In%20this%20paper%2C%20we%0Aintroduce%20X-Driver%2C%20a%20unified%20multi-modal%20large%20language%20models%28MLLMs%29%0Aframework%20designed%20for%20closed-loop%20autonomous%20driving%2C%20leveraging%0AChain-of-Thought%28CoT%29%20and%20autoregressive%20modeling%20to%20enhance%20perception%20and%0Adecision-making.%20We%20validate%20X-Driver%20across%20multiple%20autonomous%20driving%20tasks%0Ausing%20public%20benchmarks%20in%20CARLA%20simulation%20environment%2C%20including%0ABench2Drive%5B6%5D.%20Our%20experimental%20results%20demonstrate%20superior%20closed-loop%0Aperformance%2C%20surpassing%20the%20current%20state-of-the-art%28SOTA%29%20while%20improving%20the%0Ainterpretability%20of%20driving%20decisions.%20These%20findings%20underscore%20the%20importance%0Aof%20structured%20reasoning%20in%20end-to-end%20driving%20and%20establish%20X-Driver%20as%20a%0Astrong%20baseline%20for%20future%20research%20in%20closed-loop%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05098v1&entry.124074799=Read"},
{"title": "How Do Multimodal Large Language Models Handle Complex Multimodal\n  Reasoning? Placing Them in An Extensible Escape Game", "author": "Ziyue Wang and Yurui Dong and Fuwen Luo and Minyuan Ruan and Zhili Cheng and Chi Chen and Peng Li and Yang Liu", "abstract": "  The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred\ninterest in complex multimodal reasoning tasks in the real-world and virtual\nenvironment, which require coordinating multiple abilities, including visual\nperception, visual reasoning, spatial awareness, and target deduction. However,\nexisting evaluations primarily assess the final task completion, often\ndegrading assessments to isolated abilities such as visual grounding and visual\nquestion answering. Less attention is given to comprehensively and\nquantitatively analyzing reasoning process in multimodal environments, which is\ncrucial for understanding model behaviors and underlying reasoning mechanisms\nbeyond merely task success. To address this, we introduce MM-Escape, an\nextensible benchmark for investigating multimodal reasoning, inspired by\nreal-world escape games. MM-Escape emphasizes intermediate model behaviors\nalongside final task completion. To achieve this, we develop EscapeCraft, a\ncustomizable and open environment that enables models to engage in free-form\nexploration for assessing multimodal reasoning. Extensive experiments show that\nMLLMs, regardless of scale, can successfully complete the simplest room escape\ntasks, with some exhibiting human-like exploration strategies. Yet, performance\ndramatically drops as task difficulty increases. Moreover, we observe that\nperformance bottlenecks vary across models, revealing distinct failure modes\nand limitations in their multimodal reasoning abilities, such as repetitive\ntrajectories without adaptive exploration, getting stuck in corners due to poor\nvisual spatial awareness, and ineffective use of acquired props, such as the\nkey. We hope our work sheds light on new challenges in multimodal reasoning,\nand uncovers potential improvements in MLLMs capabilities.\n", "link": "http://arxiv.org/abs/2503.10042v2", "date": "2025-05-08", "relevancy": 2.2944, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5765}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Do%20Multimodal%20Large%20Language%20Models%20Handle%20Complex%20Multimodal%0A%20%20Reasoning%3F%20Placing%20Them%20in%20An%20Extensible%20Escape%20Game&body=Title%3A%20How%20Do%20Multimodal%20Large%20Language%20Models%20Handle%20Complex%20Multimodal%0A%20%20Reasoning%3F%20Placing%20Them%20in%20An%20Extensible%20Escape%20Game%0AAuthor%3A%20Ziyue%20Wang%20and%20Yurui%20Dong%20and%20Fuwen%20Luo%20and%20Minyuan%20Ruan%20and%20Zhili%20Cheng%20and%20Chi%20Chen%20and%20Peng%20Li%20and%20Yang%20Liu%0AAbstract%3A%20%20%20The%20rapid%20advancing%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20spurred%0Ainterest%20in%20complex%20multimodal%20reasoning%20tasks%20in%20the%20real-world%20and%20virtual%0Aenvironment%2C%20which%20require%20coordinating%20multiple%20abilities%2C%20including%20visual%0Aperception%2C%20visual%20reasoning%2C%20spatial%20awareness%2C%20and%20target%20deduction.%20However%2C%0Aexisting%20evaluations%20primarily%20assess%20the%20final%20task%20completion%2C%20often%0Adegrading%20assessments%20to%20isolated%20abilities%20such%20as%20visual%20grounding%20and%20visual%0Aquestion%20answering.%20Less%20attention%20is%20given%20to%20comprehensively%20and%0Aquantitatively%20analyzing%20reasoning%20process%20in%20multimodal%20environments%2C%20which%20is%0Acrucial%20for%20understanding%20model%20behaviors%20and%20underlying%20reasoning%20mechanisms%0Abeyond%20merely%20task%20success.%20To%20address%20this%2C%20we%20introduce%20MM-Escape%2C%20an%0Aextensible%20benchmark%20for%20investigating%20multimodal%20reasoning%2C%20inspired%20by%0Areal-world%20escape%20games.%20MM-Escape%20emphasizes%20intermediate%20model%20behaviors%0Aalongside%20final%20task%20completion.%20To%20achieve%20this%2C%20we%20develop%20EscapeCraft%2C%20a%0Acustomizable%20and%20open%20environment%20that%20enables%20models%20to%20engage%20in%20free-form%0Aexploration%20for%20assessing%20multimodal%20reasoning.%20Extensive%20experiments%20show%20that%0AMLLMs%2C%20regardless%20of%20scale%2C%20can%20successfully%20complete%20the%20simplest%20room%20escape%0Atasks%2C%20with%20some%20exhibiting%20human-like%20exploration%20strategies.%20Yet%2C%20performance%0Adramatically%20drops%20as%20task%20difficulty%20increases.%20Moreover%2C%20we%20observe%20that%0Aperformance%20bottlenecks%20vary%20across%20models%2C%20revealing%20distinct%20failure%20modes%0Aand%20limitations%20in%20their%20multimodal%20reasoning%20abilities%2C%20such%20as%20repetitive%0Atrajectories%20without%20adaptive%20exploration%2C%20getting%20stuck%20in%20corners%20due%20to%20poor%0Avisual%20spatial%20awareness%2C%20and%20ineffective%20use%20of%20acquired%20props%2C%20such%20as%20the%0Akey.%20We%20hope%20our%20work%20sheds%20light%20on%20new%20challenges%20in%20multimodal%20reasoning%2C%0Aand%20uncovers%20potential%20improvements%20in%20MLLMs%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10042v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Do%2520Multimodal%2520Large%2520Language%2520Models%2520Handle%2520Complex%2520Multimodal%250A%2520%2520Reasoning%253F%2520Placing%2520Them%2520in%2520An%2520Extensible%2520Escape%2520Game%26entry.906535625%3DZiyue%2520Wang%2520and%2520Yurui%2520Dong%2520and%2520Fuwen%2520Luo%2520and%2520Minyuan%2520Ruan%2520and%2520Zhili%2520Cheng%2520and%2520Chi%2520Chen%2520and%2520Peng%2520Li%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancing%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520spurred%250Ainterest%2520in%2520complex%2520multimodal%2520reasoning%2520tasks%2520in%2520the%2520real-world%2520and%2520virtual%250Aenvironment%252C%2520which%2520require%2520coordinating%2520multiple%2520abilities%252C%2520including%2520visual%250Aperception%252C%2520visual%2520reasoning%252C%2520spatial%2520awareness%252C%2520and%2520target%2520deduction.%2520However%252C%250Aexisting%2520evaluations%2520primarily%2520assess%2520the%2520final%2520task%2520completion%252C%2520often%250Adegrading%2520assessments%2520to%2520isolated%2520abilities%2520such%2520as%2520visual%2520grounding%2520and%2520visual%250Aquestion%2520answering.%2520Less%2520attention%2520is%2520given%2520to%2520comprehensively%2520and%250Aquantitatively%2520analyzing%2520reasoning%2520process%2520in%2520multimodal%2520environments%252C%2520which%2520is%250Acrucial%2520for%2520understanding%2520model%2520behaviors%2520and%2520underlying%2520reasoning%2520mechanisms%250Abeyond%2520merely%2520task%2520success.%2520To%2520address%2520this%252C%2520we%2520introduce%2520MM-Escape%252C%2520an%250Aextensible%2520benchmark%2520for%2520investigating%2520multimodal%2520reasoning%252C%2520inspired%2520by%250Areal-world%2520escape%2520games.%2520MM-Escape%2520emphasizes%2520intermediate%2520model%2520behaviors%250Aalongside%2520final%2520task%2520completion.%2520To%2520achieve%2520this%252C%2520we%2520develop%2520EscapeCraft%252C%2520a%250Acustomizable%2520and%2520open%2520environment%2520that%2520enables%2520models%2520to%2520engage%2520in%2520free-form%250Aexploration%2520for%2520assessing%2520multimodal%2520reasoning.%2520Extensive%2520experiments%2520show%2520that%250AMLLMs%252C%2520regardless%2520of%2520scale%252C%2520can%2520successfully%2520complete%2520the%2520simplest%2520room%2520escape%250Atasks%252C%2520with%2520some%2520exhibiting%2520human-like%2520exploration%2520strategies.%2520Yet%252C%2520performance%250Adramatically%2520drops%2520as%2520task%2520difficulty%2520increases.%2520Moreover%252C%2520we%2520observe%2520that%250Aperformance%2520bottlenecks%2520vary%2520across%2520models%252C%2520revealing%2520distinct%2520failure%2520modes%250Aand%2520limitations%2520in%2520their%2520multimodal%2520reasoning%2520abilities%252C%2520such%2520as%2520repetitive%250Atrajectories%2520without%2520adaptive%2520exploration%252C%2520getting%2520stuck%2520in%2520corners%2520due%2520to%2520poor%250Avisual%2520spatial%2520awareness%252C%2520and%2520ineffective%2520use%2520of%2520acquired%2520props%252C%2520such%2520as%2520the%250Akey.%2520We%2520hope%2520our%2520work%2520sheds%2520light%2520on%2520new%2520challenges%2520in%2520multimodal%2520reasoning%252C%250Aand%2520uncovers%2520potential%2520improvements%2520in%2520MLLMs%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10042v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Do%20Multimodal%20Large%20Language%20Models%20Handle%20Complex%20Multimodal%0A%20%20Reasoning%3F%20Placing%20Them%20in%20An%20Extensible%20Escape%20Game&entry.906535625=Ziyue%20Wang%20and%20Yurui%20Dong%20and%20Fuwen%20Luo%20and%20Minyuan%20Ruan%20and%20Zhili%20Cheng%20and%20Chi%20Chen%20and%20Peng%20Li%20and%20Yang%20Liu&entry.1292438233=%20%20The%20rapid%20advancing%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20spurred%0Ainterest%20in%20complex%20multimodal%20reasoning%20tasks%20in%20the%20real-world%20and%20virtual%0Aenvironment%2C%20which%20require%20coordinating%20multiple%20abilities%2C%20including%20visual%0Aperception%2C%20visual%20reasoning%2C%20spatial%20awareness%2C%20and%20target%20deduction.%20However%2C%0Aexisting%20evaluations%20primarily%20assess%20the%20final%20task%20completion%2C%20often%0Adegrading%20assessments%20to%20isolated%20abilities%20such%20as%20visual%20grounding%20and%20visual%0Aquestion%20answering.%20Less%20attention%20is%20given%20to%20comprehensively%20and%0Aquantitatively%20analyzing%20reasoning%20process%20in%20multimodal%20environments%2C%20which%20is%0Acrucial%20for%20understanding%20model%20behaviors%20and%20underlying%20reasoning%20mechanisms%0Abeyond%20merely%20task%20success.%20To%20address%20this%2C%20we%20introduce%20MM-Escape%2C%20an%0Aextensible%20benchmark%20for%20investigating%20multimodal%20reasoning%2C%20inspired%20by%0Areal-world%20escape%20games.%20MM-Escape%20emphasizes%20intermediate%20model%20behaviors%0Aalongside%20final%20task%20completion.%20To%20achieve%20this%2C%20we%20develop%20EscapeCraft%2C%20a%0Acustomizable%20and%20open%20environment%20that%20enables%20models%20to%20engage%20in%20free-form%0Aexploration%20for%20assessing%20multimodal%20reasoning.%20Extensive%20experiments%20show%20that%0AMLLMs%2C%20regardless%20of%20scale%2C%20can%20successfully%20complete%20the%20simplest%20room%20escape%0Atasks%2C%20with%20some%20exhibiting%20human-like%20exploration%20strategies.%20Yet%2C%20performance%0Adramatically%20drops%20as%20task%20difficulty%20increases.%20Moreover%2C%20we%20observe%20that%0Aperformance%20bottlenecks%20vary%20across%20models%2C%20revealing%20distinct%20failure%20modes%0Aand%20limitations%20in%20their%20multimodal%20reasoning%20abilities%2C%20such%20as%20repetitive%0Atrajectories%20without%20adaptive%20exploration%2C%20getting%20stuck%20in%20corners%20due%20to%20poor%0Avisual%20spatial%20awareness%2C%20and%20ineffective%20use%20of%20acquired%20props%2C%20such%20as%20the%0Akey.%20We%20hope%20our%20work%20sheds%20light%20on%20new%20challenges%20in%20multimodal%20reasoning%2C%0Aand%20uncovers%20potential%20improvements%20in%20MLLMs%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10042v2&entry.124074799=Read"},
{"title": "Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty\n  Quantification with Gaussian Process Latent Variable Models", "author": "Aishwarya Venkataramanan and Paul Bodesheim and Joachim Denzler", "abstract": "  Vision-Language Models (VLMs) learn joint representations by mapping images\nand text into a shared latent space. However, recent research highlights that\ndeterministic embeddings from standard VLMs often struggle to capture the\nuncertainties arising from the ambiguities in visual and textual descriptions\nand the multiple possible correspondences between images and texts. Existing\napproaches tackle this by learning probabilistic embeddings during VLM\ntraining, which demands large datasets and does not leverage the powerful\nrepresentations already learned by large-scale VLMs like CLIP. In this paper,\nwe propose GroVE, a post-hoc approach to obtaining probabilistic embeddings\nfrom frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model\n(GPLVM) to learn a shared low-dimensional latent space where image and text\ninputs are mapped to a unified representation, optimized through single-modal\nembedding reconstruction and cross-modal alignment objectives. Once trained,\nthe Gaussian Process model generates uncertainty-aware probabilistic\nembeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty\ncalibration across multiple downstream tasks, including cross-modal retrieval,\nvisual question answering, and active learning.\n", "link": "http://arxiv.org/abs/2505.05163v1", "date": "2025-05-08", "relevancy": 2.2784, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5785}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5681}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Embeddings%20for%20Frozen%20Vision-Language%20Models%3A%20Uncertainty%0A%20%20Quantification%20with%20Gaussian%20Process%20Latent%20Variable%20Models&body=Title%3A%20Probabilistic%20Embeddings%20for%20Frozen%20Vision-Language%20Models%3A%20Uncertainty%0A%20%20Quantification%20with%20Gaussian%20Process%20Latent%20Variable%20Models%0AAuthor%3A%20Aishwarya%20Venkataramanan%20and%20Paul%20Bodesheim%20and%20Joachim%20Denzler%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20learn%20joint%20representations%20by%20mapping%20images%0Aand%20text%20into%20a%20shared%20latent%20space.%20However%2C%20recent%20research%20highlights%20that%0Adeterministic%20embeddings%20from%20standard%20VLMs%20often%20struggle%20to%20capture%20the%0Auncertainties%20arising%20from%20the%20ambiguities%20in%20visual%20and%20textual%20descriptions%0Aand%20the%20multiple%20possible%20correspondences%20between%20images%20and%20texts.%20Existing%0Aapproaches%20tackle%20this%20by%20learning%20probabilistic%20embeddings%20during%20VLM%0Atraining%2C%20which%20demands%20large%20datasets%20and%20does%20not%20leverage%20the%20powerful%0Arepresentations%20already%20learned%20by%20large-scale%20VLMs%20like%20CLIP.%20In%20this%20paper%2C%0Awe%20propose%20GroVE%2C%20a%20post-hoc%20approach%20to%20obtaining%20probabilistic%20embeddings%0Afrom%20frozen%20VLMs.%20GroVE%20builds%20on%20Gaussian%20Process%20Latent%20Variable%20Model%0A%28GPLVM%29%20to%20learn%20a%20shared%20low-dimensional%20latent%20space%20where%20image%20and%20text%0Ainputs%20are%20mapped%20to%20a%20unified%20representation%2C%20optimized%20through%20single-modal%0Aembedding%20reconstruction%20and%20cross-modal%20alignment%20objectives.%20Once%20trained%2C%0Athe%20Gaussian%20Process%20model%20generates%20uncertainty-aware%20probabilistic%0Aembeddings.%20Evaluation%20shows%20that%20GroVE%20achieves%20state-of-the-art%20uncertainty%0Acalibration%20across%20multiple%20downstream%20tasks%2C%20including%20cross-modal%20retrieval%2C%0Avisual%20question%20answering%2C%20and%20active%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Embeddings%2520for%2520Frozen%2520Vision-Language%2520Models%253A%2520Uncertainty%250A%2520%2520Quantification%2520with%2520Gaussian%2520Process%2520Latent%2520Variable%2520Models%26entry.906535625%3DAishwarya%2520Venkataramanan%2520and%2520Paul%2520Bodesheim%2520and%2520Joachim%2520Denzler%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520learn%2520joint%2520representations%2520by%2520mapping%2520images%250Aand%2520text%2520into%2520a%2520shared%2520latent%2520space.%2520However%252C%2520recent%2520research%2520highlights%2520that%250Adeterministic%2520embeddings%2520from%2520standard%2520VLMs%2520often%2520struggle%2520to%2520capture%2520the%250Auncertainties%2520arising%2520from%2520the%2520ambiguities%2520in%2520visual%2520and%2520textual%2520descriptions%250Aand%2520the%2520multiple%2520possible%2520correspondences%2520between%2520images%2520and%2520texts.%2520Existing%250Aapproaches%2520tackle%2520this%2520by%2520learning%2520probabilistic%2520embeddings%2520during%2520VLM%250Atraining%252C%2520which%2520demands%2520large%2520datasets%2520and%2520does%2520not%2520leverage%2520the%2520powerful%250Arepresentations%2520already%2520learned%2520by%2520large-scale%2520VLMs%2520like%2520CLIP.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520GroVE%252C%2520a%2520post-hoc%2520approach%2520to%2520obtaining%2520probabilistic%2520embeddings%250Afrom%2520frozen%2520VLMs.%2520GroVE%2520builds%2520on%2520Gaussian%2520Process%2520Latent%2520Variable%2520Model%250A%2528GPLVM%2529%2520to%2520learn%2520a%2520shared%2520low-dimensional%2520latent%2520space%2520where%2520image%2520and%2520text%250Ainputs%2520are%2520mapped%2520to%2520a%2520unified%2520representation%252C%2520optimized%2520through%2520single-modal%250Aembedding%2520reconstruction%2520and%2520cross-modal%2520alignment%2520objectives.%2520Once%2520trained%252C%250Athe%2520Gaussian%2520Process%2520model%2520generates%2520uncertainty-aware%2520probabilistic%250Aembeddings.%2520Evaluation%2520shows%2520that%2520GroVE%2520achieves%2520state-of-the-art%2520uncertainty%250Acalibration%2520across%2520multiple%2520downstream%2520tasks%252C%2520including%2520cross-modal%2520retrieval%252C%250Avisual%2520question%2520answering%252C%2520and%2520active%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Embeddings%20for%20Frozen%20Vision-Language%20Models%3A%20Uncertainty%0A%20%20Quantification%20with%20Gaussian%20Process%20Latent%20Variable%20Models&entry.906535625=Aishwarya%20Venkataramanan%20and%20Paul%20Bodesheim%20and%20Joachim%20Denzler&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20learn%20joint%20representations%20by%20mapping%20images%0Aand%20text%20into%20a%20shared%20latent%20space.%20However%2C%20recent%20research%20highlights%20that%0Adeterministic%20embeddings%20from%20standard%20VLMs%20often%20struggle%20to%20capture%20the%0Auncertainties%20arising%20from%20the%20ambiguities%20in%20visual%20and%20textual%20descriptions%0Aand%20the%20multiple%20possible%20correspondences%20between%20images%20and%20texts.%20Existing%0Aapproaches%20tackle%20this%20by%20learning%20probabilistic%20embeddings%20during%20VLM%0Atraining%2C%20which%20demands%20large%20datasets%20and%20does%20not%20leverage%20the%20powerful%0Arepresentations%20already%20learned%20by%20large-scale%20VLMs%20like%20CLIP.%20In%20this%20paper%2C%0Awe%20propose%20GroVE%2C%20a%20post-hoc%20approach%20to%20obtaining%20probabilistic%20embeddings%0Afrom%20frozen%20VLMs.%20GroVE%20builds%20on%20Gaussian%20Process%20Latent%20Variable%20Model%0A%28GPLVM%29%20to%20learn%20a%20shared%20low-dimensional%20latent%20space%20where%20image%20and%20text%0Ainputs%20are%20mapped%20to%20a%20unified%20representation%2C%20optimized%20through%20single-modal%0Aembedding%20reconstruction%20and%20cross-modal%20alignment%20objectives.%20Once%20trained%2C%0Athe%20Gaussian%20Process%20model%20generates%20uncertainty-aware%20probabilistic%0Aembeddings.%20Evaluation%20shows%20that%20GroVE%20achieves%20state-of-the-art%20uncertainty%0Acalibration%20across%20multiple%20downstream%20tasks%2C%20including%20cross-modal%20retrieval%2C%0Avisual%20question%20answering%2C%20and%20active%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05163v1&entry.124074799=Read"},
{"title": "Adaptive Markup Language Generation for Contextually-Grounded Visual\n  Document Understanding", "author": "Han Xiao and Yina Xie and Guanxin Tan and Yinghao Chen and Rui Hu and Ke Wang and Aojun Zhou and Hao Li and Hao Shao and Xudong Lu and Peng Gao and Yafei Wen and Xiaoxin Chen and Shuai Ren and Hongsheng Li", "abstract": "  Visual Document Understanding has become essential with the increase of\ntext-rich visual content. This field poses significant challenges due to the\nneed for effective integration of visual perception and textual comprehension,\nparticularly across diverse document types with complex layouts. Moreover,\nexisting fine-tuning datasets for this domain often fall short in providing the\ndetailed contextual information for robust understanding, leading to\nhallucinations and limited comprehension of spatial relationships among visual\nelements. To address these challenges, we propose an innovative pipeline that\nutilizes adaptive generation of markup languages, such as Markdown, JSON, HTML,\nand TiKZ, to build highly structured document representations and deliver\ncontextually-grounded responses. We introduce two fine-grained structured\ndatasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs\nfor document parsing, and DocMark-Instruct, featuring 624k fine-tuning data\nannotations for grounded instruction following. Extensive experiments\ndemonstrate that our proposed model significantly outperforms existing\nstate-of-theart MLLMs across a range of visual document understanding\nbenchmarks, facilitating advanced reasoning and comprehension capabilities in\ncomplex visual scenarios. Our code and models are released at https://github.\ncom/Euphoria16/DocMark.\n", "link": "http://arxiv.org/abs/2505.05446v1", "date": "2025-05-08", "relevancy": 2.2756, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Markup%20Language%20Generation%20for%20Contextually-Grounded%20Visual%0A%20%20Document%20Understanding&body=Title%3A%20Adaptive%20Markup%20Language%20Generation%20for%20Contextually-Grounded%20Visual%0A%20%20Document%20Understanding%0AAuthor%3A%20Han%20Xiao%20and%20Yina%20Xie%20and%20Guanxin%20Tan%20and%20Yinghao%20Chen%20and%20Rui%20Hu%20and%20Ke%20Wang%20and%20Aojun%20Zhou%20and%20Hao%20Li%20and%20Hao%20Shao%20and%20Xudong%20Lu%20and%20Peng%20Gao%20and%20Yafei%20Wen%20and%20Xiaoxin%20Chen%20and%20Shuai%20Ren%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Visual%20Document%20Understanding%20has%20become%20essential%20with%20the%20increase%20of%0Atext-rich%20visual%20content.%20This%20field%20poses%20significant%20challenges%20due%20to%20the%0Aneed%20for%20effective%20integration%20of%20visual%20perception%20and%20textual%20comprehension%2C%0Aparticularly%20across%20diverse%20document%20types%20with%20complex%20layouts.%20Moreover%2C%0Aexisting%20fine-tuning%20datasets%20for%20this%20domain%20often%20fall%20short%20in%20providing%20the%0Adetailed%20contextual%20information%20for%20robust%20understanding%2C%20leading%20to%0Ahallucinations%20and%20limited%20comprehension%20of%20spatial%20relationships%20among%20visual%0Aelements.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20innovative%20pipeline%20that%0Autilizes%20adaptive%20generation%20of%20markup%20languages%2C%20such%20as%20Markdown%2C%20JSON%2C%20HTML%2C%0Aand%20TiKZ%2C%20to%20build%20highly%20structured%20document%20representations%20and%20deliver%0Acontextually-grounded%20responses.%20We%20introduce%20two%20fine-grained%20structured%0Adatasets%3A%20DocMark-Pile%2C%20comprising%20approximately%203.8M%20pretraining%20data%20pairs%0Afor%20document%20parsing%2C%20and%20DocMark-Instruct%2C%20featuring%20624k%20fine-tuning%20data%0Aannotations%20for%20grounded%20instruction%20following.%20Extensive%20experiments%0Ademonstrate%20that%20our%20proposed%20model%20significantly%20outperforms%20existing%0Astate-of-theart%20MLLMs%20across%20a%20range%20of%20visual%20document%20understanding%0Abenchmarks%2C%20facilitating%20advanced%20reasoning%20and%20comprehension%20capabilities%20in%0Acomplex%20visual%20scenarios.%20Our%20code%20and%20models%20are%20released%20at%20https%3A//github.%0Acom/Euphoria16/DocMark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Markup%2520Language%2520Generation%2520for%2520Contextually-Grounded%2520Visual%250A%2520%2520Document%2520Understanding%26entry.906535625%3DHan%2520Xiao%2520and%2520Yina%2520Xie%2520and%2520Guanxin%2520Tan%2520and%2520Yinghao%2520Chen%2520and%2520Rui%2520Hu%2520and%2520Ke%2520Wang%2520and%2520Aojun%2520Zhou%2520and%2520Hao%2520Li%2520and%2520Hao%2520Shao%2520and%2520Xudong%2520Lu%2520and%2520Peng%2520Gao%2520and%2520Yafei%2520Wen%2520and%2520Xiaoxin%2520Chen%2520and%2520Shuai%2520Ren%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Visual%2520Document%2520Understanding%2520has%2520become%2520essential%2520with%2520the%2520increase%2520of%250Atext-rich%2520visual%2520content.%2520This%2520field%2520poses%2520significant%2520challenges%2520due%2520to%2520the%250Aneed%2520for%2520effective%2520integration%2520of%2520visual%2520perception%2520and%2520textual%2520comprehension%252C%250Aparticularly%2520across%2520diverse%2520document%2520types%2520with%2520complex%2520layouts.%2520Moreover%252C%250Aexisting%2520fine-tuning%2520datasets%2520for%2520this%2520domain%2520often%2520fall%2520short%2520in%2520providing%2520the%250Adetailed%2520contextual%2520information%2520for%2520robust%2520understanding%252C%2520leading%2520to%250Ahallucinations%2520and%2520limited%2520comprehension%2520of%2520spatial%2520relationships%2520among%2520visual%250Aelements.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520an%2520innovative%2520pipeline%2520that%250Autilizes%2520adaptive%2520generation%2520of%2520markup%2520languages%252C%2520such%2520as%2520Markdown%252C%2520JSON%252C%2520HTML%252C%250Aand%2520TiKZ%252C%2520to%2520build%2520highly%2520structured%2520document%2520representations%2520and%2520deliver%250Acontextually-grounded%2520responses.%2520We%2520introduce%2520two%2520fine-grained%2520structured%250Adatasets%253A%2520DocMark-Pile%252C%2520comprising%2520approximately%25203.8M%2520pretraining%2520data%2520pairs%250Afor%2520document%2520parsing%252C%2520and%2520DocMark-Instruct%252C%2520featuring%2520624k%2520fine-tuning%2520data%250Aannotations%2520for%2520grounded%2520instruction%2520following.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520proposed%2520model%2520significantly%2520outperforms%2520existing%250Astate-of-theart%2520MLLMs%2520across%2520a%2520range%2520of%2520visual%2520document%2520understanding%250Abenchmarks%252C%2520facilitating%2520advanced%2520reasoning%2520and%2520comprehension%2520capabilities%2520in%250Acomplex%2520visual%2520scenarios.%2520Our%2520code%2520and%2520models%2520are%2520released%2520at%2520https%253A//github.%250Acom/Euphoria16/DocMark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Markup%20Language%20Generation%20for%20Contextually-Grounded%20Visual%0A%20%20Document%20Understanding&entry.906535625=Han%20Xiao%20and%20Yina%20Xie%20and%20Guanxin%20Tan%20and%20Yinghao%20Chen%20and%20Rui%20Hu%20and%20Ke%20Wang%20and%20Aojun%20Zhou%20and%20Hao%20Li%20and%20Hao%20Shao%20and%20Xudong%20Lu%20and%20Peng%20Gao%20and%20Yafei%20Wen%20and%20Xiaoxin%20Chen%20and%20Shuai%20Ren%20and%20Hongsheng%20Li&entry.1292438233=%20%20Visual%20Document%20Understanding%20has%20become%20essential%20with%20the%20increase%20of%0Atext-rich%20visual%20content.%20This%20field%20poses%20significant%20challenges%20due%20to%20the%0Aneed%20for%20effective%20integration%20of%20visual%20perception%20and%20textual%20comprehension%2C%0Aparticularly%20across%20diverse%20document%20types%20with%20complex%20layouts.%20Moreover%2C%0Aexisting%20fine-tuning%20datasets%20for%20this%20domain%20often%20fall%20short%20in%20providing%20the%0Adetailed%20contextual%20information%20for%20robust%20understanding%2C%20leading%20to%0Ahallucinations%20and%20limited%20comprehension%20of%20spatial%20relationships%20among%20visual%0Aelements.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20innovative%20pipeline%20that%0Autilizes%20adaptive%20generation%20of%20markup%20languages%2C%20such%20as%20Markdown%2C%20JSON%2C%20HTML%2C%0Aand%20TiKZ%2C%20to%20build%20highly%20structured%20document%20representations%20and%20deliver%0Acontextually-grounded%20responses.%20We%20introduce%20two%20fine-grained%20structured%0Adatasets%3A%20DocMark-Pile%2C%20comprising%20approximately%203.8M%20pretraining%20data%20pairs%0Afor%20document%20parsing%2C%20and%20DocMark-Instruct%2C%20featuring%20624k%20fine-tuning%20data%0Aannotations%20for%20grounded%20instruction%20following.%20Extensive%20experiments%0Ademonstrate%20that%20our%20proposed%20model%20significantly%20outperforms%20existing%0Astate-of-theart%20MLLMs%20across%20a%20range%20of%20visual%20document%20understanding%0Abenchmarks%2C%20facilitating%20advanced%20reasoning%20and%20comprehension%20capabilities%20in%0Acomplex%20visual%20scenarios.%20Our%20code%20and%20models%20are%20released%20at%20https%3A//github.%0Acom/Euphoria16/DocMark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05446v1&entry.124074799=Read"},
{"title": "Don't Shake the Wheel: Momentum-Aware Planning in End-to-End Autonomous\n  Driving", "author": "Ziying Song and Caiyan Jia and Lin Liu and Hongyu Pan and Yongchang Zhang and Junming Wang and Xingyu Zhang and Shaoqing Xu and Lei Yang and Yadan Luo", "abstract": "  End-to-end autonomous driving frameworks enable seamless integration of\nperception and planning but often rely on one-shot trajectory prediction, which\nmay lead to unstable control and vulnerability to occlusions in single-frame\nperception. To address this, we propose the Momentum-Aware Driving (MomAD)\nframework, which introduces trajectory momentum and perception momentum to\nstabilize and refine trajectory predictions. MomAD comprises two core\ncomponents: (1) Topological Trajectory Matching (TTM) employs Hausdorff\nDistance to select the optimal planning query that aligns with prior paths to\nensure coherence;(2) Momentum Planning Interactor (MPI) cross-attends the\nselected planning query with historical queries to expand static and dynamic\nperception files. This enriched query, in turn, helps regenerate long-horizon\ntrajectory and reduce collision risks. To mitigate noise arising from dynamic\nenvironments and detection errors, we introduce robust instance denoising\nduring training, enabling the planning model to focus on critical signals and\nimprove its robustness. We also propose a novel Trajectory Prediction\nConsistency (TPC) metric to quantitatively assess planning stability.\nExperiments on the nuScenes dataset demonstrate that MomAD achieves superior\nlong-term consistency (>=3s) compared to SOTA methods. Moreover, evaluations on\nthe curated Turning-nuScenes shows that MomAD reduces the collision rate by 26%\nand improves TPC by 0.97m (33.45%) over a 6s prediction horizon, while\nclosedloop on Bench2Drive demonstrates an up to 16.3% improvement in success\nrate.\n", "link": "http://arxiv.org/abs/2503.03125v3", "date": "2025-05-08", "relevancy": 2.2707, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5719}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.565}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Shake%20the%20Wheel%3A%20Momentum-Aware%20Planning%20in%20End-to-End%20Autonomous%0A%20%20Driving&body=Title%3A%20Don%27t%20Shake%20the%20Wheel%3A%20Momentum-Aware%20Planning%20in%20End-to-End%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Ziying%20Song%20and%20Caiyan%20Jia%20and%20Lin%20Liu%20and%20Hongyu%20Pan%20and%20Yongchang%20Zhang%20and%20Junming%20Wang%20and%20Xingyu%20Zhang%20and%20Shaoqing%20Xu%20and%20Lei%20Yang%20and%20Yadan%20Luo%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20frameworks%20enable%20seamless%20integration%20of%0Aperception%20and%20planning%20but%20often%20rely%20on%20one-shot%20trajectory%20prediction%2C%20which%0Amay%20lead%20to%20unstable%20control%20and%20vulnerability%20to%20occlusions%20in%20single-frame%0Aperception.%20To%20address%20this%2C%20we%20propose%20the%20Momentum-Aware%20Driving%20%28MomAD%29%0Aframework%2C%20which%20introduces%20trajectory%20momentum%20and%20perception%20momentum%20to%0Astabilize%20and%20refine%20trajectory%20predictions.%20MomAD%20comprises%20two%20core%0Acomponents%3A%20%281%29%20Topological%20Trajectory%20Matching%20%28TTM%29%20employs%20Hausdorff%0ADistance%20to%20select%20the%20optimal%20planning%20query%20that%20aligns%20with%20prior%20paths%20to%0Aensure%20coherence%3B%282%29%20Momentum%20Planning%20Interactor%20%28MPI%29%20cross-attends%20the%0Aselected%20planning%20query%20with%20historical%20queries%20to%20expand%20static%20and%20dynamic%0Aperception%20files.%20This%20enriched%20query%2C%20in%20turn%2C%20helps%20regenerate%20long-horizon%0Atrajectory%20and%20reduce%20collision%20risks.%20To%20mitigate%20noise%20arising%20from%20dynamic%0Aenvironments%20and%20detection%20errors%2C%20we%20introduce%20robust%20instance%20denoising%0Aduring%20training%2C%20enabling%20the%20planning%20model%20to%20focus%20on%20critical%20signals%20and%0Aimprove%20its%20robustness.%20We%20also%20propose%20a%20novel%20Trajectory%20Prediction%0AConsistency%20%28TPC%29%20metric%20to%20quantitatively%20assess%20planning%20stability.%0AExperiments%20on%20the%20nuScenes%20dataset%20demonstrate%20that%20MomAD%20achieves%20superior%0Along-term%20consistency%20%28%3E%3D3s%29%20compared%20to%20SOTA%20methods.%20Moreover%2C%20evaluations%20on%0Athe%20curated%20Turning-nuScenes%20shows%20that%20MomAD%20reduces%20the%20collision%20rate%20by%2026%25%0Aand%20improves%20TPC%20by%200.97m%20%2833.45%25%29%20over%20a%206s%20prediction%20horizon%2C%20while%0Aclosedloop%20on%20Bench2Drive%20demonstrates%20an%20up%20to%2016.3%25%20improvement%20in%20success%0Arate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03125v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Shake%2520the%2520Wheel%253A%2520Momentum-Aware%2520Planning%2520in%2520End-to-End%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DZiying%2520Song%2520and%2520Caiyan%2520Jia%2520and%2520Lin%2520Liu%2520and%2520Hongyu%2520Pan%2520and%2520Yongchang%2520Zhang%2520and%2520Junming%2520Wang%2520and%2520Xingyu%2520Zhang%2520and%2520Shaoqing%2520Xu%2520and%2520Lei%2520Yang%2520and%2520Yadan%2520Luo%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520frameworks%2520enable%2520seamless%2520integration%2520of%250Aperception%2520and%2520planning%2520but%2520often%2520rely%2520on%2520one-shot%2520trajectory%2520prediction%252C%2520which%250Amay%2520lead%2520to%2520unstable%2520control%2520and%2520vulnerability%2520to%2520occlusions%2520in%2520single-frame%250Aperception.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Momentum-Aware%2520Driving%2520%2528MomAD%2529%250Aframework%252C%2520which%2520introduces%2520trajectory%2520momentum%2520and%2520perception%2520momentum%2520to%250Astabilize%2520and%2520refine%2520trajectory%2520predictions.%2520MomAD%2520comprises%2520two%2520core%250Acomponents%253A%2520%25281%2529%2520Topological%2520Trajectory%2520Matching%2520%2528TTM%2529%2520employs%2520Hausdorff%250ADistance%2520to%2520select%2520the%2520optimal%2520planning%2520query%2520that%2520aligns%2520with%2520prior%2520paths%2520to%250Aensure%2520coherence%253B%25282%2529%2520Momentum%2520Planning%2520Interactor%2520%2528MPI%2529%2520cross-attends%2520the%250Aselected%2520planning%2520query%2520with%2520historical%2520queries%2520to%2520expand%2520static%2520and%2520dynamic%250Aperception%2520files.%2520This%2520enriched%2520query%252C%2520in%2520turn%252C%2520helps%2520regenerate%2520long-horizon%250Atrajectory%2520and%2520reduce%2520collision%2520risks.%2520To%2520mitigate%2520noise%2520arising%2520from%2520dynamic%250Aenvironments%2520and%2520detection%2520errors%252C%2520we%2520introduce%2520robust%2520instance%2520denoising%250Aduring%2520training%252C%2520enabling%2520the%2520planning%2520model%2520to%2520focus%2520on%2520critical%2520signals%2520and%250Aimprove%2520its%2520robustness.%2520We%2520also%2520propose%2520a%2520novel%2520Trajectory%2520Prediction%250AConsistency%2520%2528TPC%2529%2520metric%2520to%2520quantitatively%2520assess%2520planning%2520stability.%250AExperiments%2520on%2520the%2520nuScenes%2520dataset%2520demonstrate%2520that%2520MomAD%2520achieves%2520superior%250Along-term%2520consistency%2520%2528%253E%253D3s%2529%2520compared%2520to%2520SOTA%2520methods.%2520Moreover%252C%2520evaluations%2520on%250Athe%2520curated%2520Turning-nuScenes%2520shows%2520that%2520MomAD%2520reduces%2520the%2520collision%2520rate%2520by%252026%2525%250Aand%2520improves%2520TPC%2520by%25200.97m%2520%252833.45%2525%2529%2520over%2520a%25206s%2520prediction%2520horizon%252C%2520while%250Aclosedloop%2520on%2520Bench2Drive%2520demonstrates%2520an%2520up%2520to%252016.3%2525%2520improvement%2520in%2520success%250Arate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03125v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Shake%20the%20Wheel%3A%20Momentum-Aware%20Planning%20in%20End-to-End%20Autonomous%0A%20%20Driving&entry.906535625=Ziying%20Song%20and%20Caiyan%20Jia%20and%20Lin%20Liu%20and%20Hongyu%20Pan%20and%20Yongchang%20Zhang%20and%20Junming%20Wang%20and%20Xingyu%20Zhang%20and%20Shaoqing%20Xu%20and%20Lei%20Yang%20and%20Yadan%20Luo&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20frameworks%20enable%20seamless%20integration%20of%0Aperception%20and%20planning%20but%20often%20rely%20on%20one-shot%20trajectory%20prediction%2C%20which%0Amay%20lead%20to%20unstable%20control%20and%20vulnerability%20to%20occlusions%20in%20single-frame%0Aperception.%20To%20address%20this%2C%20we%20propose%20the%20Momentum-Aware%20Driving%20%28MomAD%29%0Aframework%2C%20which%20introduces%20trajectory%20momentum%20and%20perception%20momentum%20to%0Astabilize%20and%20refine%20trajectory%20predictions.%20MomAD%20comprises%20two%20core%0Acomponents%3A%20%281%29%20Topological%20Trajectory%20Matching%20%28TTM%29%20employs%20Hausdorff%0ADistance%20to%20select%20the%20optimal%20planning%20query%20that%20aligns%20with%20prior%20paths%20to%0Aensure%20coherence%3B%282%29%20Momentum%20Planning%20Interactor%20%28MPI%29%20cross-attends%20the%0Aselected%20planning%20query%20with%20historical%20queries%20to%20expand%20static%20and%20dynamic%0Aperception%20files.%20This%20enriched%20query%2C%20in%20turn%2C%20helps%20regenerate%20long-horizon%0Atrajectory%20and%20reduce%20collision%20risks.%20To%20mitigate%20noise%20arising%20from%20dynamic%0Aenvironments%20and%20detection%20errors%2C%20we%20introduce%20robust%20instance%20denoising%0Aduring%20training%2C%20enabling%20the%20planning%20model%20to%20focus%20on%20critical%20signals%20and%0Aimprove%20its%20robustness.%20We%20also%20propose%20a%20novel%20Trajectory%20Prediction%0AConsistency%20%28TPC%29%20metric%20to%20quantitatively%20assess%20planning%20stability.%0AExperiments%20on%20the%20nuScenes%20dataset%20demonstrate%20that%20MomAD%20achieves%20superior%0Along-term%20consistency%20%28%3E%3D3s%29%20compared%20to%20SOTA%20methods.%20Moreover%2C%20evaluations%20on%0Athe%20curated%20Turning-nuScenes%20shows%20that%20MomAD%20reduces%20the%20collision%20rate%20by%2026%25%0Aand%20improves%20TPC%20by%200.97m%20%2833.45%25%29%20over%20a%206s%20prediction%20horizon%2C%20while%0Aclosedloop%20on%20Bench2Drive%20demonstrates%20an%20up%20to%2016.3%25%20improvement%20in%20success%0Arate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03125v3&entry.124074799=Read"},
{"title": "Feature-Augmented Deep Networks for Multiscale Building Segmentation in\n  High-Resolution UAV and Satellite Imagery", "author": "Chintan B. Maniyar and Minakshi Kumar and Gengchen Mai", "abstract": "  Accurate building segmentation from high-resolution RGB imagery remains\nchallenging due to spectral similarity with non-building features, shadows, and\nirregular building geometries. In this study, we present a comprehensive deep\nlearning framework for multiscale building segmentation using RGB aerial and\nsatellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate\na diverse, multi-sensor dataset and introduce feature-augmented inputs by\nderiving secondary representations including Principal Component Analysis\n(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index\n(MBI), and Sobel edge filters from RGB channels. These features guide a\nRes-U-Net architecture in learning complex spatial patterns more effectively.\nWe also propose training policies incorporating layer freezing, cyclical\nlearning rates, and SuperConvergence to reduce training time and resource\nusage. Evaluated on a held-out WorldView-3 image, our model achieves an overall\naccuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of\n0.80, outperforming existing RGB-based benchmarks. This study demonstrates the\neffectiveness of combining multi-resolution imagery, feature augmentation, and\noptimized training strategies for robust building segmentation in remote\nsensing applications.\n", "link": "http://arxiv.org/abs/2505.05321v1", "date": "2025-05-08", "relevancy": 2.262, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5843}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5536}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature-Augmented%20Deep%20Networks%20for%20Multiscale%20Building%20Segmentation%20in%0A%20%20High-Resolution%20UAV%20and%20Satellite%20Imagery&body=Title%3A%20Feature-Augmented%20Deep%20Networks%20for%20Multiscale%20Building%20Segmentation%20in%0A%20%20High-Resolution%20UAV%20and%20Satellite%20Imagery%0AAuthor%3A%20Chintan%20B.%20Maniyar%20and%20Minakshi%20Kumar%20and%20Gengchen%20Mai%0AAbstract%3A%20%20%20Accurate%20building%20segmentation%20from%20high-resolution%20RGB%20imagery%20remains%0Achallenging%20due%20to%20spectral%20similarity%20with%20non-building%20features%2C%20shadows%2C%20and%0Airregular%20building%20geometries.%20In%20this%20study%2C%20we%20present%20a%20comprehensive%20deep%0Alearning%20framework%20for%20multiscale%20building%20segmentation%20using%20RGB%20aerial%20and%0Asatellite%20imagery%20with%20spatial%20resolutions%20ranging%20from%200.4m%20to%202.7m.%20We%20curate%0Aa%20diverse%2C%20multi-sensor%20dataset%20and%20introduce%20feature-augmented%20inputs%20by%0Aderiving%20secondary%20representations%20including%20Principal%20Component%20Analysis%0A%28PCA%29%2C%20Visible%20Difference%20Vegetation%20Index%20%28VDVI%29%2C%20Morphological%20Building%20Index%0A%28MBI%29%2C%20and%20Sobel%20edge%20filters%20from%20RGB%20channels.%20These%20features%20guide%20a%0ARes-U-Net%20architecture%20in%20learning%20complex%20spatial%20patterns%20more%20effectively.%0AWe%20also%20propose%20training%20policies%20incorporating%20layer%20freezing%2C%20cyclical%0Alearning%20rates%2C%20and%20SuperConvergence%20to%20reduce%20training%20time%20and%20resource%0Ausage.%20Evaluated%20on%20a%20held-out%20WorldView-3%20image%2C%20our%20model%20achieves%20an%20overall%0Aaccuracy%20of%2096.5%25%2C%20an%20F1-score%20of%200.86%2C%20and%20an%20Intersection%20over%20Union%20%28IoU%29%20of%0A0.80%2C%20outperforming%20existing%20RGB-based%20benchmarks.%20This%20study%20demonstrates%20the%0Aeffectiveness%20of%20combining%20multi-resolution%20imagery%2C%20feature%20augmentation%2C%20and%0Aoptimized%20training%20strategies%20for%20robust%20building%20segmentation%20in%20remote%0Asensing%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature-Augmented%2520Deep%2520Networks%2520for%2520Multiscale%2520Building%2520Segmentation%2520in%250A%2520%2520High-Resolution%2520UAV%2520and%2520Satellite%2520Imagery%26entry.906535625%3DChintan%2520B.%2520Maniyar%2520and%2520Minakshi%2520Kumar%2520and%2520Gengchen%2520Mai%26entry.1292438233%3D%2520%2520Accurate%2520building%2520segmentation%2520from%2520high-resolution%2520RGB%2520imagery%2520remains%250Achallenging%2520due%2520to%2520spectral%2520similarity%2520with%2520non-building%2520features%252C%2520shadows%252C%2520and%250Airregular%2520building%2520geometries.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520comprehensive%2520deep%250Alearning%2520framework%2520for%2520multiscale%2520building%2520segmentation%2520using%2520RGB%2520aerial%2520and%250Asatellite%2520imagery%2520with%2520spatial%2520resolutions%2520ranging%2520from%25200.4m%2520to%25202.7m.%2520We%2520curate%250Aa%2520diverse%252C%2520multi-sensor%2520dataset%2520and%2520introduce%2520feature-augmented%2520inputs%2520by%250Aderiving%2520secondary%2520representations%2520including%2520Principal%2520Component%2520Analysis%250A%2528PCA%2529%252C%2520Visible%2520Difference%2520Vegetation%2520Index%2520%2528VDVI%2529%252C%2520Morphological%2520Building%2520Index%250A%2528MBI%2529%252C%2520and%2520Sobel%2520edge%2520filters%2520from%2520RGB%2520channels.%2520These%2520features%2520guide%2520a%250ARes-U-Net%2520architecture%2520in%2520learning%2520complex%2520spatial%2520patterns%2520more%2520effectively.%250AWe%2520also%2520propose%2520training%2520policies%2520incorporating%2520layer%2520freezing%252C%2520cyclical%250Alearning%2520rates%252C%2520and%2520SuperConvergence%2520to%2520reduce%2520training%2520time%2520and%2520resource%250Ausage.%2520Evaluated%2520on%2520a%2520held-out%2520WorldView-3%2520image%252C%2520our%2520model%2520achieves%2520an%2520overall%250Aaccuracy%2520of%252096.5%2525%252C%2520an%2520F1-score%2520of%25200.86%252C%2520and%2520an%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520of%250A0.80%252C%2520outperforming%2520existing%2520RGB-based%2520benchmarks.%2520This%2520study%2520demonstrates%2520the%250Aeffectiveness%2520of%2520combining%2520multi-resolution%2520imagery%252C%2520feature%2520augmentation%252C%2520and%250Aoptimized%2520training%2520strategies%2520for%2520robust%2520building%2520segmentation%2520in%2520remote%250Asensing%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-Augmented%20Deep%20Networks%20for%20Multiscale%20Building%20Segmentation%20in%0A%20%20High-Resolution%20UAV%20and%20Satellite%20Imagery&entry.906535625=Chintan%20B.%20Maniyar%20and%20Minakshi%20Kumar%20and%20Gengchen%20Mai&entry.1292438233=%20%20Accurate%20building%20segmentation%20from%20high-resolution%20RGB%20imagery%20remains%0Achallenging%20due%20to%20spectral%20similarity%20with%20non-building%20features%2C%20shadows%2C%20and%0Airregular%20building%20geometries.%20In%20this%20study%2C%20we%20present%20a%20comprehensive%20deep%0Alearning%20framework%20for%20multiscale%20building%20segmentation%20using%20RGB%20aerial%20and%0Asatellite%20imagery%20with%20spatial%20resolutions%20ranging%20from%200.4m%20to%202.7m.%20We%20curate%0Aa%20diverse%2C%20multi-sensor%20dataset%20and%20introduce%20feature-augmented%20inputs%20by%0Aderiving%20secondary%20representations%20including%20Principal%20Component%20Analysis%0A%28PCA%29%2C%20Visible%20Difference%20Vegetation%20Index%20%28VDVI%29%2C%20Morphological%20Building%20Index%0A%28MBI%29%2C%20and%20Sobel%20edge%20filters%20from%20RGB%20channels.%20These%20features%20guide%20a%0ARes-U-Net%20architecture%20in%20learning%20complex%20spatial%20patterns%20more%20effectively.%0AWe%20also%20propose%20training%20policies%20incorporating%20layer%20freezing%2C%20cyclical%0Alearning%20rates%2C%20and%20SuperConvergence%20to%20reduce%20training%20time%20and%20resource%0Ausage.%20Evaluated%20on%20a%20held-out%20WorldView-3%20image%2C%20our%20model%20achieves%20an%20overall%0Aaccuracy%20of%2096.5%25%2C%20an%20F1-score%20of%200.86%2C%20and%20an%20Intersection%20over%20Union%20%28IoU%29%20of%0A0.80%2C%20outperforming%20existing%20RGB-based%20benchmarks.%20This%20study%20demonstrates%20the%0Aeffectiveness%20of%20combining%20multi-resolution%20imagery%2C%20feature%20augmentation%2C%20and%0Aoptimized%20training%20strategies%20for%20robust%20building%20segmentation%20in%20remote%0Asensing%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05321v1&entry.124074799=Read"},
{"title": "Balancing Client Participation in Federated Learning Using AoI", "author": "Alireza Javani and Zhiying Wang", "abstract": "  Federated Learning (FL) offers a decentralized framework that preserves data\nprivacy while enabling collaborative model training across distributed clients.\nHowever, FL faces significant challenges due to limited communication\nresources, statistical heterogeneity, and the need for balanced client\nparticipation. This paper proposes an Age of Information (AoI)-based client\nselection policy that addresses these challenges by minimizing load imbalance\nthrough controlled selection intervals. Our method employs a decentralized\nMarkov scheduling policy, allowing clients to independently manage\nparticipation based on age-dependent selection probabilities, which balances\nclient updates across training rounds with minimal central oversight. We\nprovide a convergence proof for our method, demonstrating that it ensures\nstable and efficient model convergence. Specifically, we derive optimal\nparameters for the Markov selection model to achieve balanced and consistent\nclient participation, highlighting the benefits of AoI in enhancing convergence\nstability. Through extensive simulations, we demonstrate that our AoI-based\nmethod, particularly the optimal Markov variant, improves convergence over the\nFedAvg selection approach across both IID and non-IID data settings by $7.5\\%$\nand up to $20\\%$. Our findings underscore the effectiveness of AoI-based\nscheduling for scalable, fair, and efficient FL systems across diverse learning\nenvironments.\n", "link": "http://arxiv.org/abs/2505.05099v1", "date": "2025-05-08", "relevancy": 2.2454, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4603}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4473}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balancing%20Client%20Participation%20in%20Federated%20Learning%20Using%20AoI&body=Title%3A%20Balancing%20Client%20Participation%20in%20Federated%20Learning%20Using%20AoI%0AAuthor%3A%20Alireza%20Javani%20and%20Zhiying%20Wang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20offers%20a%20decentralized%20framework%20that%20preserves%20data%0Aprivacy%20while%20enabling%20collaborative%20model%20training%20across%20distributed%20clients.%0AHowever%2C%20FL%20faces%20significant%20challenges%20due%20to%20limited%20communication%0Aresources%2C%20statistical%20heterogeneity%2C%20and%20the%20need%20for%20balanced%20client%0Aparticipation.%20This%20paper%20proposes%20an%20Age%20of%20Information%20%28AoI%29-based%20client%0Aselection%20policy%20that%20addresses%20these%20challenges%20by%20minimizing%20load%20imbalance%0Athrough%20controlled%20selection%20intervals.%20Our%20method%20employs%20a%20decentralized%0AMarkov%20scheduling%20policy%2C%20allowing%20clients%20to%20independently%20manage%0Aparticipation%20based%20on%20age-dependent%20selection%20probabilities%2C%20which%20balances%0Aclient%20updates%20across%20training%20rounds%20with%20minimal%20central%20oversight.%20We%0Aprovide%20a%20convergence%20proof%20for%20our%20method%2C%20demonstrating%20that%20it%20ensures%0Astable%20and%20efficient%20model%20convergence.%20Specifically%2C%20we%20derive%20optimal%0Aparameters%20for%20the%20Markov%20selection%20model%20to%20achieve%20balanced%20and%20consistent%0Aclient%20participation%2C%20highlighting%20the%20benefits%20of%20AoI%20in%20enhancing%20convergence%0Astability.%20Through%20extensive%20simulations%2C%20we%20demonstrate%20that%20our%20AoI-based%0Amethod%2C%20particularly%20the%20optimal%20Markov%20variant%2C%20improves%20convergence%20over%20the%0AFedAvg%20selection%20approach%20across%20both%20IID%20and%20non-IID%20data%20settings%20by%20%247.5%5C%25%24%0Aand%20up%20to%20%2420%5C%25%24.%20Our%20findings%20underscore%20the%20effectiveness%20of%20AoI-based%0Ascheduling%20for%20scalable%2C%20fair%2C%20and%20efficient%20FL%20systems%20across%20diverse%20learning%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancing%2520Client%2520Participation%2520in%2520Federated%2520Learning%2520Using%2520AoI%26entry.906535625%3DAlireza%2520Javani%2520and%2520Zhiying%2520Wang%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520offers%2520a%2520decentralized%2520framework%2520that%2520preserves%2520data%250Aprivacy%2520while%2520enabling%2520collaborative%2520model%2520training%2520across%2520distributed%2520clients.%250AHowever%252C%2520FL%2520faces%2520significant%2520challenges%2520due%2520to%2520limited%2520communication%250Aresources%252C%2520statistical%2520heterogeneity%252C%2520and%2520the%2520need%2520for%2520balanced%2520client%250Aparticipation.%2520This%2520paper%2520proposes%2520an%2520Age%2520of%2520Information%2520%2528AoI%2529-based%2520client%250Aselection%2520policy%2520that%2520addresses%2520these%2520challenges%2520by%2520minimizing%2520load%2520imbalance%250Athrough%2520controlled%2520selection%2520intervals.%2520Our%2520method%2520employs%2520a%2520decentralized%250AMarkov%2520scheduling%2520policy%252C%2520allowing%2520clients%2520to%2520independently%2520manage%250Aparticipation%2520based%2520on%2520age-dependent%2520selection%2520probabilities%252C%2520which%2520balances%250Aclient%2520updates%2520across%2520training%2520rounds%2520with%2520minimal%2520central%2520oversight.%2520We%250Aprovide%2520a%2520convergence%2520proof%2520for%2520our%2520method%252C%2520demonstrating%2520that%2520it%2520ensures%250Astable%2520and%2520efficient%2520model%2520convergence.%2520Specifically%252C%2520we%2520derive%2520optimal%250Aparameters%2520for%2520the%2520Markov%2520selection%2520model%2520to%2520achieve%2520balanced%2520and%2520consistent%250Aclient%2520participation%252C%2520highlighting%2520the%2520benefits%2520of%2520AoI%2520in%2520enhancing%2520convergence%250Astability.%2520Through%2520extensive%2520simulations%252C%2520we%2520demonstrate%2520that%2520our%2520AoI-based%250Amethod%252C%2520particularly%2520the%2520optimal%2520Markov%2520variant%252C%2520improves%2520convergence%2520over%2520the%250AFedAvg%2520selection%2520approach%2520across%2520both%2520IID%2520and%2520non-IID%2520data%2520settings%2520by%2520%25247.5%255C%2525%2524%250Aand%2520up%2520to%2520%252420%255C%2525%2524.%2520Our%2520findings%2520underscore%2520the%2520effectiveness%2520of%2520AoI-based%250Ascheduling%2520for%2520scalable%252C%2520fair%252C%2520and%2520efficient%2520FL%2520systems%2520across%2520diverse%2520learning%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20Client%20Participation%20in%20Federated%20Learning%20Using%20AoI&entry.906535625=Alireza%20Javani%20and%20Zhiying%20Wang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20offers%20a%20decentralized%20framework%20that%20preserves%20data%0Aprivacy%20while%20enabling%20collaborative%20model%20training%20across%20distributed%20clients.%0AHowever%2C%20FL%20faces%20significant%20challenges%20due%20to%20limited%20communication%0Aresources%2C%20statistical%20heterogeneity%2C%20and%20the%20need%20for%20balanced%20client%0Aparticipation.%20This%20paper%20proposes%20an%20Age%20of%20Information%20%28AoI%29-based%20client%0Aselection%20policy%20that%20addresses%20these%20challenges%20by%20minimizing%20load%20imbalance%0Athrough%20controlled%20selection%20intervals.%20Our%20method%20employs%20a%20decentralized%0AMarkov%20scheduling%20policy%2C%20allowing%20clients%20to%20independently%20manage%0Aparticipation%20based%20on%20age-dependent%20selection%20probabilities%2C%20which%20balances%0Aclient%20updates%20across%20training%20rounds%20with%20minimal%20central%20oversight.%20We%0Aprovide%20a%20convergence%20proof%20for%20our%20method%2C%20demonstrating%20that%20it%20ensures%0Astable%20and%20efficient%20model%20convergence.%20Specifically%2C%20we%20derive%20optimal%0Aparameters%20for%20the%20Markov%20selection%20model%20to%20achieve%20balanced%20and%20consistent%0Aclient%20participation%2C%20highlighting%20the%20benefits%20of%20AoI%20in%20enhancing%20convergence%0Astability.%20Through%20extensive%20simulations%2C%20we%20demonstrate%20that%20our%20AoI-based%0Amethod%2C%20particularly%20the%20optimal%20Markov%20variant%2C%20improves%20convergence%20over%20the%0AFedAvg%20selection%20approach%20across%20both%20IID%20and%20non-IID%20data%20settings%20by%20%247.5%5C%25%24%0Aand%20up%20to%20%2420%5C%25%24.%20Our%20findings%20underscore%20the%20effectiveness%20of%20AoI-based%0Ascheduling%20for%20scalable%2C%20fair%2C%20and%20efficient%20FL%20systems%20across%20diverse%20learning%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05099v1&entry.124074799=Read"},
{"title": "Large Language Models Understanding: an Inherent Ambiguity Barrier", "author": "Daniel N. Nissani", "abstract": "  A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.\n", "link": "http://arxiv.org/abs/2505.00654v3", "date": "2025-05-08", "relevancy": 2.2161, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Understanding%3A%20an%20Inherent%20Ambiguity%20Barrier&body=Title%3A%20Large%20Language%20Models%20Understanding%3A%20an%20Inherent%20Ambiguity%20Barrier%0AAuthor%3A%20Daniel%20N.%20Nissani%0AAbstract%3A%20%20%20A%20lively%20ongoing%20debate%20is%20taking%20place%2C%20since%20the%20extraordinary%20emergence%20of%0ALarge%20Language%20Models%20%28LLMs%29%20with%20regards%20to%20their%20capability%20to%20understand%20the%0Aworld%20and%20capture%20the%20meaning%20of%20the%20dialogues%20in%20which%20they%20are%20involved.%0AArguments%20and%20counter-arguments%20have%20been%20proposed%20based%20upon%20thought%0Aexperiments%2C%20anecdotal%20conversations%20between%20LLMs%20and%20humans%2C%20statistical%0Alinguistic%20analysis%2C%20philosophical%20considerations%2C%20and%20more.%20In%20this%20brief%0Apaper%20we%20present%20a%20counter-argument%20based%20upon%20a%20thought%20experiment%20and%0Asemi-formal%20considerations%20leading%20to%20an%20inherent%20ambiguity%20barrier%20which%0Aprevents%20LLMs%20from%20having%20any%20understanding%20of%20what%20their%20amazingly%20fluent%0Adialogues%20mean.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00654v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Understanding%253A%2520an%2520Inherent%2520Ambiguity%2520Barrier%26entry.906535625%3DDaniel%2520N.%2520Nissani%26entry.1292438233%3D%2520%2520A%2520lively%2520ongoing%2520debate%2520is%2520taking%2520place%252C%2520since%2520the%2520extraordinary%2520emergence%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520regards%2520to%2520their%2520capability%2520to%2520understand%2520the%250Aworld%2520and%2520capture%2520the%2520meaning%2520of%2520the%2520dialogues%2520in%2520which%2520they%2520are%2520involved.%250AArguments%2520and%2520counter-arguments%2520have%2520been%2520proposed%2520based%2520upon%2520thought%250Aexperiments%252C%2520anecdotal%2520conversations%2520between%2520LLMs%2520and%2520humans%252C%2520statistical%250Alinguistic%2520analysis%252C%2520philosophical%2520considerations%252C%2520and%2520more.%2520In%2520this%2520brief%250Apaper%2520we%2520present%2520a%2520counter-argument%2520based%2520upon%2520a%2520thought%2520experiment%2520and%250Asemi-formal%2520considerations%2520leading%2520to%2520an%2520inherent%2520ambiguity%2520barrier%2520which%250Aprevents%2520LLMs%2520from%2520having%2520any%2520understanding%2520of%2520what%2520their%2520amazingly%2520fluent%250Adialogues%2520mean.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00654v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Understanding%3A%20an%20Inherent%20Ambiguity%20Barrier&entry.906535625=Daniel%20N.%20Nissani&entry.1292438233=%20%20A%20lively%20ongoing%20debate%20is%20taking%20place%2C%20since%20the%20extraordinary%20emergence%20of%0ALarge%20Language%20Models%20%28LLMs%29%20with%20regards%20to%20their%20capability%20to%20understand%20the%0Aworld%20and%20capture%20the%20meaning%20of%20the%20dialogues%20in%20which%20they%20are%20involved.%0AArguments%20and%20counter-arguments%20have%20been%20proposed%20based%20upon%20thought%0Aexperiments%2C%20anecdotal%20conversations%20between%20LLMs%20and%20humans%2C%20statistical%0Alinguistic%20analysis%2C%20philosophical%20considerations%2C%20and%20more.%20In%20this%20brief%0Apaper%20we%20present%20a%20counter-argument%20based%20upon%20a%20thought%20experiment%20and%0Asemi-formal%20considerations%20leading%20to%20an%20inherent%20ambiguity%20barrier%20which%0Aprevents%20LLMs%20from%20having%20any%20understanding%20of%20what%20their%20amazingly%20fluent%0Adialogues%20mean.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00654v3&entry.124074799=Read"},
{"title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant", "author": "Haibo Wang and Bo Feng and Zhengfeng Lai and Mingze Xu and Shiyu Li and Weifeng Ge and Afshin Dehghan and Meng Cao and Ping Huang", "abstract": "  We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.\n", "link": "http://arxiv.org/abs/2505.05467v1", "date": "2025-05-08", "relevancy": 2.2154, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamBridge%3A%20Turning%20Your%20Offline%20Video%20Large%20Language%20Model%20into%20a%0A%20%20Proactive%20Streaming%20Assistant&body=Title%3A%20StreamBridge%3A%20Turning%20Your%20Offline%20Video%20Large%20Language%20Model%20into%20a%0A%20%20Proactive%20Streaming%20Assistant%0AAuthor%3A%20Haibo%20Wang%20and%20Bo%20Feng%20and%20Zhengfeng%20Lai%20and%20Mingze%20Xu%20and%20Shiyu%20Li%20and%20Weifeng%20Ge%20and%20Afshin%20Dehghan%20and%20Meng%20Cao%20and%20Ping%20Huang%0AAbstract%3A%20%20%20We%20present%20StreamBridge%2C%20a%20simple%20yet%20effective%20framework%20that%20seamlessly%0Atransforms%20offline%20Video-LLMs%20into%20streaming-capable%20models.%20It%20addresses%20two%0Afundamental%20challenges%20in%20adapting%20existing%20models%20into%20online%20scenarios%3A%20%281%29%0Alimited%20capability%20for%20multi-turn%20real-time%20understanding%2C%20and%20%282%29%20lack%20of%0Aproactive%20response%20mechanisms.%20Specifically%2C%20StreamBridge%20incorporates%20%281%29%20a%0Amemory%20buffer%20combined%20with%20a%20round-decayed%20compression%20strategy%2C%20supporting%0Along-context%20multi-turn%20interactions%2C%20and%20%282%29%20a%20decoupled%2C%20lightweight%0Aactivation%20model%20that%20can%20be%20effortlessly%20integrated%20into%20existing%20Video-LLMs%2C%0Aenabling%20continuous%20proactive%20responses.%20To%20further%20support%20StreamBridge%2C%20we%0Aconstruct%20Stream-IT%2C%20a%20large-scale%20dataset%20tailored%20for%20streaming%20video%0Aunderstanding%2C%20featuring%20interleaved%20video-text%20sequences%20and%20diverse%0Ainstruction%20formats.%20Extensive%20experiments%20show%20that%20StreamBridge%20significantly%0Aimproves%20the%20streaming%20understanding%20capabilities%20of%20offline%20Video-LLMs%20across%0Avarious%20tasks%2C%20outperforming%20even%20proprietary%20models%20such%20as%20GPT-4o%20and%20Gemini%0A1.5%20Pro.%20Simultaneously%2C%20it%20achieves%20competitive%20or%20superior%20performance%20on%0Astandard%20video%20understanding%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamBridge%253A%2520Turning%2520Your%2520Offline%2520Video%2520Large%2520Language%2520Model%2520into%2520a%250A%2520%2520Proactive%2520Streaming%2520Assistant%26entry.906535625%3DHaibo%2520Wang%2520and%2520Bo%2520Feng%2520and%2520Zhengfeng%2520Lai%2520and%2520Mingze%2520Xu%2520and%2520Shiyu%2520Li%2520and%2520Weifeng%2520Ge%2520and%2520Afshin%2520Dehghan%2520and%2520Meng%2520Cao%2520and%2520Ping%2520Huang%26entry.1292438233%3D%2520%2520We%2520present%2520StreamBridge%252C%2520a%2520simple%2520yet%2520effective%2520framework%2520that%2520seamlessly%250Atransforms%2520offline%2520Video-LLMs%2520into%2520streaming-capable%2520models.%2520It%2520addresses%2520two%250Afundamental%2520challenges%2520in%2520adapting%2520existing%2520models%2520into%2520online%2520scenarios%253A%2520%25281%2529%250Alimited%2520capability%2520for%2520multi-turn%2520real-time%2520understanding%252C%2520and%2520%25282%2529%2520lack%2520of%250Aproactive%2520response%2520mechanisms.%2520Specifically%252C%2520StreamBridge%2520incorporates%2520%25281%2529%2520a%250Amemory%2520buffer%2520combined%2520with%2520a%2520round-decayed%2520compression%2520strategy%252C%2520supporting%250Along-context%2520multi-turn%2520interactions%252C%2520and%2520%25282%2529%2520a%2520decoupled%252C%2520lightweight%250Aactivation%2520model%2520that%2520can%2520be%2520effortlessly%2520integrated%2520into%2520existing%2520Video-LLMs%252C%250Aenabling%2520continuous%2520proactive%2520responses.%2520To%2520further%2520support%2520StreamBridge%252C%2520we%250Aconstruct%2520Stream-IT%252C%2520a%2520large-scale%2520dataset%2520tailored%2520for%2520streaming%2520video%250Aunderstanding%252C%2520featuring%2520interleaved%2520video-text%2520sequences%2520and%2520diverse%250Ainstruction%2520formats.%2520Extensive%2520experiments%2520show%2520that%2520StreamBridge%2520significantly%250Aimproves%2520the%2520streaming%2520understanding%2520capabilities%2520of%2520offline%2520Video-LLMs%2520across%250Avarious%2520tasks%252C%2520outperforming%2520even%2520proprietary%2520models%2520such%2520as%2520GPT-4o%2520and%2520Gemini%250A1.5%2520Pro.%2520Simultaneously%252C%2520it%2520achieves%2520competitive%2520or%2520superior%2520performance%2520on%250Astandard%2520video%2520understanding%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamBridge%3A%20Turning%20Your%20Offline%20Video%20Large%20Language%20Model%20into%20a%0A%20%20Proactive%20Streaming%20Assistant&entry.906535625=Haibo%20Wang%20and%20Bo%20Feng%20and%20Zhengfeng%20Lai%20and%20Mingze%20Xu%20and%20Shiyu%20Li%20and%20Weifeng%20Ge%20and%20Afshin%20Dehghan%20and%20Meng%20Cao%20and%20Ping%20Huang&entry.1292438233=%20%20We%20present%20StreamBridge%2C%20a%20simple%20yet%20effective%20framework%20that%20seamlessly%0Atransforms%20offline%20Video-LLMs%20into%20streaming-capable%20models.%20It%20addresses%20two%0Afundamental%20challenges%20in%20adapting%20existing%20models%20into%20online%20scenarios%3A%20%281%29%0Alimited%20capability%20for%20multi-turn%20real-time%20understanding%2C%20and%20%282%29%20lack%20of%0Aproactive%20response%20mechanisms.%20Specifically%2C%20StreamBridge%20incorporates%20%281%29%20a%0Amemory%20buffer%20combined%20with%20a%20round-decayed%20compression%20strategy%2C%20supporting%0Along-context%20multi-turn%20interactions%2C%20and%20%282%29%20a%20decoupled%2C%20lightweight%0Aactivation%20model%20that%20can%20be%20effortlessly%20integrated%20into%20existing%20Video-LLMs%2C%0Aenabling%20continuous%20proactive%20responses.%20To%20further%20support%20StreamBridge%2C%20we%0Aconstruct%20Stream-IT%2C%20a%20large-scale%20dataset%20tailored%20for%20streaming%20video%0Aunderstanding%2C%20featuring%20interleaved%20video-text%20sequences%20and%20diverse%0Ainstruction%20formats.%20Extensive%20experiments%20show%20that%20StreamBridge%20significantly%0Aimproves%20the%20streaming%20understanding%20capabilities%20of%20offline%20Video-LLMs%20across%0Avarious%20tasks%2C%20outperforming%20even%20proprietary%20models%20such%20as%20GPT-4o%20and%20Gemini%0A1.5%20Pro.%20Simultaneously%2C%20it%20achieves%20competitive%20or%20superior%20performance%20on%0Astandard%20video%20understanding%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05467v1&entry.124074799=Read"},
{"title": "SITE: towards Spatial Intelligence Thorough Evaluation", "author": "Wenqi Wang and Reuben Tan and Pengyue Zhu and Jianwei Yang and Zhengyuan Yang and Lijuan Wang and Andrey Kolobov and Jianfeng Gao and Boqing Gong", "abstract": "  Spatial intelligence (SI) represents a cognitive ability encompassing the\nvisualization, manipulation, and reasoning about spatial relationships,\nunderpinning disciplines from neuroscience to robotics. We introduce SITE, a\nbenchmark dataset towards SI Thorough Evaluation in a standardized format of\nmulti-choice visual question-answering, designed to assess large\nvision-language models' spatial intelligence across diverse visual modalities\n(single-image, multi-image, and video) and SI factors (figural to environmental\nscales, spatial visualization and orientation, intrinsic and extrinsic, static\nand dynamic). Our approach to curating the benchmark combines a bottom-up\nsurvey about 31 existing datasets and a top-down strategy drawing upon three\nclassification systems in cognitive science, which prompt us to design two\nnovel types of tasks about view-taking and dynamic scenes. Extensive\nexperiments reveal that leading models fall behind human experts especially in\nspatial orientation, a fundamental SI factor. Moreover, we demonstrate a\npositive correlation between a model's spatial reasoning proficiency and its\nperformance on an embodied AI task.\n", "link": "http://arxiv.org/abs/2505.05456v1", "date": "2025-05-08", "relevancy": 2.2119, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5635}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5635}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SITE%3A%20towards%20Spatial%20Intelligence%20Thorough%20Evaluation&body=Title%3A%20SITE%3A%20towards%20Spatial%20Intelligence%20Thorough%20Evaluation%0AAuthor%3A%20Wenqi%20Wang%20and%20Reuben%20Tan%20and%20Pengyue%20Zhu%20and%20Jianwei%20Yang%20and%20Zhengyuan%20Yang%20and%20Lijuan%20Wang%20and%20Andrey%20Kolobov%20and%20Jianfeng%20Gao%20and%20Boqing%20Gong%0AAbstract%3A%20%20%20Spatial%20intelligence%20%28SI%29%20represents%20a%20cognitive%20ability%20encompassing%20the%0Avisualization%2C%20manipulation%2C%20and%20reasoning%20about%20spatial%20relationships%2C%0Aunderpinning%20disciplines%20from%20neuroscience%20to%20robotics.%20We%20introduce%20SITE%2C%20a%0Abenchmark%20dataset%20towards%20SI%20Thorough%20Evaluation%20in%20a%20standardized%20format%20of%0Amulti-choice%20visual%20question-answering%2C%20designed%20to%20assess%20large%0Avision-language%20models%27%20spatial%20intelligence%20across%20diverse%20visual%20modalities%0A%28single-image%2C%20multi-image%2C%20and%20video%29%20and%20SI%20factors%20%28figural%20to%20environmental%0Ascales%2C%20spatial%20visualization%20and%20orientation%2C%20intrinsic%20and%20extrinsic%2C%20static%0Aand%20dynamic%29.%20Our%20approach%20to%20curating%20the%20benchmark%20combines%20a%20bottom-up%0Asurvey%20about%2031%20existing%20datasets%20and%20a%20top-down%20strategy%20drawing%20upon%20three%0Aclassification%20systems%20in%20cognitive%20science%2C%20which%20prompt%20us%20to%20design%20two%0Anovel%20types%20of%20tasks%20about%20view-taking%20and%20dynamic%20scenes.%20Extensive%0Aexperiments%20reveal%20that%20leading%20models%20fall%20behind%20human%20experts%20especially%20in%0Aspatial%20orientation%2C%20a%20fundamental%20SI%20factor.%20Moreover%2C%20we%20demonstrate%20a%0Apositive%20correlation%20between%20a%20model%27s%20spatial%20reasoning%20proficiency%20and%20its%0Aperformance%20on%20an%20embodied%20AI%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSITE%253A%2520towards%2520Spatial%2520Intelligence%2520Thorough%2520Evaluation%26entry.906535625%3DWenqi%2520Wang%2520and%2520Reuben%2520Tan%2520and%2520Pengyue%2520Zhu%2520and%2520Jianwei%2520Yang%2520and%2520Zhengyuan%2520Yang%2520and%2520Lijuan%2520Wang%2520and%2520Andrey%2520Kolobov%2520and%2520Jianfeng%2520Gao%2520and%2520Boqing%2520Gong%26entry.1292438233%3D%2520%2520Spatial%2520intelligence%2520%2528SI%2529%2520represents%2520a%2520cognitive%2520ability%2520encompassing%2520the%250Avisualization%252C%2520manipulation%252C%2520and%2520reasoning%2520about%2520spatial%2520relationships%252C%250Aunderpinning%2520disciplines%2520from%2520neuroscience%2520to%2520robotics.%2520We%2520introduce%2520SITE%252C%2520a%250Abenchmark%2520dataset%2520towards%2520SI%2520Thorough%2520Evaluation%2520in%2520a%2520standardized%2520format%2520of%250Amulti-choice%2520visual%2520question-answering%252C%2520designed%2520to%2520assess%2520large%250Avision-language%2520models%2527%2520spatial%2520intelligence%2520across%2520diverse%2520visual%2520modalities%250A%2528single-image%252C%2520multi-image%252C%2520and%2520video%2529%2520and%2520SI%2520factors%2520%2528figural%2520to%2520environmental%250Ascales%252C%2520spatial%2520visualization%2520and%2520orientation%252C%2520intrinsic%2520and%2520extrinsic%252C%2520static%250Aand%2520dynamic%2529.%2520Our%2520approach%2520to%2520curating%2520the%2520benchmark%2520combines%2520a%2520bottom-up%250Asurvey%2520about%252031%2520existing%2520datasets%2520and%2520a%2520top-down%2520strategy%2520drawing%2520upon%2520three%250Aclassification%2520systems%2520in%2520cognitive%2520science%252C%2520which%2520prompt%2520us%2520to%2520design%2520two%250Anovel%2520types%2520of%2520tasks%2520about%2520view-taking%2520and%2520dynamic%2520scenes.%2520Extensive%250Aexperiments%2520reveal%2520that%2520leading%2520models%2520fall%2520behind%2520human%2520experts%2520especially%2520in%250Aspatial%2520orientation%252C%2520a%2520fundamental%2520SI%2520factor.%2520Moreover%252C%2520we%2520demonstrate%2520a%250Apositive%2520correlation%2520between%2520a%2520model%2527s%2520spatial%2520reasoning%2520proficiency%2520and%2520its%250Aperformance%2520on%2520an%2520embodied%2520AI%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SITE%3A%20towards%20Spatial%20Intelligence%20Thorough%20Evaluation&entry.906535625=Wenqi%20Wang%20and%20Reuben%20Tan%20and%20Pengyue%20Zhu%20and%20Jianwei%20Yang%20and%20Zhengyuan%20Yang%20and%20Lijuan%20Wang%20and%20Andrey%20Kolobov%20and%20Jianfeng%20Gao%20and%20Boqing%20Gong&entry.1292438233=%20%20Spatial%20intelligence%20%28SI%29%20represents%20a%20cognitive%20ability%20encompassing%20the%0Avisualization%2C%20manipulation%2C%20and%20reasoning%20about%20spatial%20relationships%2C%0Aunderpinning%20disciplines%20from%20neuroscience%20to%20robotics.%20We%20introduce%20SITE%2C%20a%0Abenchmark%20dataset%20towards%20SI%20Thorough%20Evaluation%20in%20a%20standardized%20format%20of%0Amulti-choice%20visual%20question-answering%2C%20designed%20to%20assess%20large%0Avision-language%20models%27%20spatial%20intelligence%20across%20diverse%20visual%20modalities%0A%28single-image%2C%20multi-image%2C%20and%20video%29%20and%20SI%20factors%20%28figural%20to%20environmental%0Ascales%2C%20spatial%20visualization%20and%20orientation%2C%20intrinsic%20and%20extrinsic%2C%20static%0Aand%20dynamic%29.%20Our%20approach%20to%20curating%20the%20benchmark%20combines%20a%20bottom-up%0Asurvey%20about%2031%20existing%20datasets%20and%20a%20top-down%20strategy%20drawing%20upon%20three%0Aclassification%20systems%20in%20cognitive%20science%2C%20which%20prompt%20us%20to%20design%20two%0Anovel%20types%20of%20tasks%20about%20view-taking%20and%20dynamic%20scenes.%20Extensive%0Aexperiments%20reveal%20that%20leading%20models%20fall%20behind%20human%20experts%20especially%20in%0Aspatial%20orientation%2C%20a%20fundamental%20SI%20factor.%20Moreover%2C%20we%20demonstrate%20a%0Apositive%20correlation%20between%20a%20model%27s%20spatial%20reasoning%20proficiency%20and%20its%0Aperformance%20on%20an%20embodied%20AI%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05456v1&entry.124074799=Read"},
{"title": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language\n  Models", "author": "Wei Peng and Kang Liu and Jianchen Hu and Meng Zhang", "abstract": "  Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}.\n", "link": "http://arxiv.org/abs/2505.05189v1", "date": "2025-05-08", "relevancy": 2.2113, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5717}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biomed-DPT%3A%20Dual%20Modality%20Prompt%20Tuning%20for%20Biomedical%20Vision-Language%0A%20%20Models&body=Title%3A%20Biomed-DPT%3A%20Dual%20Modality%20Prompt%20Tuning%20for%20Biomedical%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Wei%20Peng%20and%20Kang%20Liu%20and%20Jianchen%20Hu%20and%20Meng%20Zhang%0AAbstract%3A%20%20%20Prompt%20learning%20is%20one%20of%20the%20most%20effective%20paradigms%20for%20adapting%0Apre-trained%20vision-language%20models%20%28VLMs%29%20to%20the%20biomedical%20image%0Aclassification%20tasks%20in%20few%20shot%20scenarios.%20However%2C%20most%20of%20the%20current%20prompt%0Alearning%20methods%20only%20used%20the%20text%20prompts%20and%20ignored%20the%20particular%0Astructures%20%28such%20as%20the%20complex%20anatomical%20structures%20and%20subtle%20pathological%0Afeatures%29%20in%20the%20biomedical%20images.%20In%20this%20work%2C%20we%20propose%20Biomed-DPT%2C%20a%0Aknowledge-enhanced%20dual%20modality%20prompt%20tuning%20technique.%20In%20designing%20the%20text%0Aprompt%2C%20Biomed-DPT%20constructs%20a%20dual%20prompt%20including%20the%20template-driven%0Aclinical%20prompts%20and%20the%20large%20language%20model%20%28LLM%29-driven%20domain-adapted%0Aprompts%2C%20then%20extracts%20the%20clinical%20knowledge%20from%20the%20domain-adapted%20prompts%0Athrough%20the%20knowledge%20distillation%20technique.%20In%20designing%20the%20vision%20prompt%2C%0ABiomed-DPT%20introduces%20the%20zero%20vector%20as%20a%20soft%20prompt%20to%20leverage%20attention%0Are-weighting%20so%20that%20the%20focus%20on%20non-diagnostic%20regions%20and%20the%20recognition%20of%0Anon-critical%20pathological%20features%20are%20avoided.%20Biomed-DPT%20achieves%20an%20average%0Aclassification%20accuracy%20of%2066.14%5C%25%20across%2011%20biomedical%20image%20datasets%20covering%0A9%20modalities%20and%2010%20organs%2C%20with%20performance%20reaching%2078.06%5C%25%20in%20base%20classes%0Aand%2075.97%5C%25%20in%20novel%20classes%2C%20surpassing%20the%20Context%20Optimization%20%28CoOp%29%20method%0Aby%206.20%5C%25%2C%203.78%5C%25%2C%20and%208.04%5C%25%2C%20respectively.%20Our%20code%20are%20available%20at%0A%5Cunderline%7Bhttps%3A//github.com/Kanyooo/Biomed-DPT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiomed-DPT%253A%2520Dual%2520Modality%2520Prompt%2520Tuning%2520for%2520Biomedical%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DWei%2520Peng%2520and%2520Kang%2520Liu%2520and%2520Jianchen%2520Hu%2520and%2520Meng%2520Zhang%26entry.1292438233%3D%2520%2520Prompt%2520learning%2520is%2520one%2520of%2520the%2520most%2520effective%2520paradigms%2520for%2520adapting%250Apre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520the%2520biomedical%2520image%250Aclassification%2520tasks%2520in%2520few%2520shot%2520scenarios.%2520However%252C%2520most%2520of%2520the%2520current%2520prompt%250Alearning%2520methods%2520only%2520used%2520the%2520text%2520prompts%2520and%2520ignored%2520the%2520particular%250Astructures%2520%2528such%2520as%2520the%2520complex%2520anatomical%2520structures%2520and%2520subtle%2520pathological%250Afeatures%2529%2520in%2520the%2520biomedical%2520images.%2520In%2520this%2520work%252C%2520we%2520propose%2520Biomed-DPT%252C%2520a%250Aknowledge-enhanced%2520dual%2520modality%2520prompt%2520tuning%2520technique.%2520In%2520designing%2520the%2520text%250Aprompt%252C%2520Biomed-DPT%2520constructs%2520a%2520dual%2520prompt%2520including%2520the%2520template-driven%250Aclinical%2520prompts%2520and%2520the%2520large%2520language%2520model%2520%2528LLM%2529-driven%2520domain-adapted%250Aprompts%252C%2520then%2520extracts%2520the%2520clinical%2520knowledge%2520from%2520the%2520domain-adapted%2520prompts%250Athrough%2520the%2520knowledge%2520distillation%2520technique.%2520In%2520designing%2520the%2520vision%2520prompt%252C%250ABiomed-DPT%2520introduces%2520the%2520zero%2520vector%2520as%2520a%2520soft%2520prompt%2520to%2520leverage%2520attention%250Are-weighting%2520so%2520that%2520the%2520focus%2520on%2520non-diagnostic%2520regions%2520and%2520the%2520recognition%2520of%250Anon-critical%2520pathological%2520features%2520are%2520avoided.%2520Biomed-DPT%2520achieves%2520an%2520average%250Aclassification%2520accuracy%2520of%252066.14%255C%2525%2520across%252011%2520biomedical%2520image%2520datasets%2520covering%250A9%2520modalities%2520and%252010%2520organs%252C%2520with%2520performance%2520reaching%252078.06%255C%2525%2520in%2520base%2520classes%250Aand%252075.97%255C%2525%2520in%2520novel%2520classes%252C%2520surpassing%2520the%2520Context%2520Optimization%2520%2528CoOp%2529%2520method%250Aby%25206.20%255C%2525%252C%25203.78%255C%2525%252C%2520and%25208.04%255C%2525%252C%2520respectively.%2520Our%2520code%2520are%2520available%2520at%250A%255Cunderline%257Bhttps%253A//github.com/Kanyooo/Biomed-DPT%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biomed-DPT%3A%20Dual%20Modality%20Prompt%20Tuning%20for%20Biomedical%20Vision-Language%0A%20%20Models&entry.906535625=Wei%20Peng%20and%20Kang%20Liu%20and%20Jianchen%20Hu%20and%20Meng%20Zhang&entry.1292438233=%20%20Prompt%20learning%20is%20one%20of%20the%20most%20effective%20paradigms%20for%20adapting%0Apre-trained%20vision-language%20models%20%28VLMs%29%20to%20the%20biomedical%20image%0Aclassification%20tasks%20in%20few%20shot%20scenarios.%20However%2C%20most%20of%20the%20current%20prompt%0Alearning%20methods%20only%20used%20the%20text%20prompts%20and%20ignored%20the%20particular%0Astructures%20%28such%20as%20the%20complex%20anatomical%20structures%20and%20subtle%20pathological%0Afeatures%29%20in%20the%20biomedical%20images.%20In%20this%20work%2C%20we%20propose%20Biomed-DPT%2C%20a%0Aknowledge-enhanced%20dual%20modality%20prompt%20tuning%20technique.%20In%20designing%20the%20text%0Aprompt%2C%20Biomed-DPT%20constructs%20a%20dual%20prompt%20including%20the%20template-driven%0Aclinical%20prompts%20and%20the%20large%20language%20model%20%28LLM%29-driven%20domain-adapted%0Aprompts%2C%20then%20extracts%20the%20clinical%20knowledge%20from%20the%20domain-adapted%20prompts%0Athrough%20the%20knowledge%20distillation%20technique.%20In%20designing%20the%20vision%20prompt%2C%0ABiomed-DPT%20introduces%20the%20zero%20vector%20as%20a%20soft%20prompt%20to%20leverage%20attention%0Are-weighting%20so%20that%20the%20focus%20on%20non-diagnostic%20regions%20and%20the%20recognition%20of%0Anon-critical%20pathological%20features%20are%20avoided.%20Biomed-DPT%20achieves%20an%20average%0Aclassification%20accuracy%20of%2066.14%5C%25%20across%2011%20biomedical%20image%20datasets%20covering%0A9%20modalities%20and%2010%20organs%2C%20with%20performance%20reaching%2078.06%5C%25%20in%20base%20classes%0Aand%2075.97%5C%25%20in%20novel%20classes%2C%20surpassing%20the%20Context%20Optimization%20%28CoOp%29%20method%0Aby%206.20%5C%25%2C%203.78%5C%25%2C%20and%208.04%5C%25%2C%20respectively.%20Our%20code%20are%20available%20at%0A%5Cunderline%7Bhttps%3A//github.com/Kanyooo/Biomed-DPT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05189v1&entry.124074799=Read"},
{"title": "PADriver: Towards Personalized Autonomous Driving", "author": "Genghua Kou and Fan Jia and Weixin Mao and Yingfei Liu and Yucheng Zhao and Ziheng Zhang and Osamu Yoshie and Tiancai Wang and Ying Li and Xiangyu Zhang", "abstract": "  In this paper, we propose PADriver, a novel closed-loop framework for\npersonalized autonomous driving (PAD). Built upon Multi-modal Large Language\nModel (MLLM), PADriver takes streaming frames and personalized textual prompts\nas inputs. It autoaggressively performs scene understanding, danger level\nestimation and action decision. The predicted danger level reflects the risk of\nthe potential action and provides an explicit reference for the final action,\nwhich corresponds to the preset personalized prompt. Moreover, we construct a\nclosed-loop benchmark named PAD-Highway based on Highway-Env simulator to\ncomprehensively evaluate the decision performance under traffic rules. The\ndataset contains 250 hours videos with high-quality annotation to facilitate\nthe development of PAD behavior analysis. Experimental results on the\nconstructed benchmark show that PADriver outperforms state-of-the-art\napproaches on different evaluation metrics, and enables various driving modes.\n", "link": "http://arxiv.org/abs/2505.05240v1", "date": "2025-05-08", "relevancy": 2.2074, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5733}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5518}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PADriver%3A%20Towards%20Personalized%20Autonomous%20Driving&body=Title%3A%20PADriver%3A%20Towards%20Personalized%20Autonomous%20Driving%0AAuthor%3A%20Genghua%20Kou%20and%20Fan%20Jia%20and%20Weixin%20Mao%20and%20Yingfei%20Liu%20and%20Yucheng%20Zhao%20and%20Ziheng%20Zhang%20and%20Osamu%20Yoshie%20and%20Tiancai%20Wang%20and%20Ying%20Li%20and%20Xiangyu%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20PADriver%2C%20a%20novel%20closed-loop%20framework%20for%0Apersonalized%20autonomous%20driving%20%28PAD%29.%20Built%20upon%20Multi-modal%20Large%20Language%0AModel%20%28MLLM%29%2C%20PADriver%20takes%20streaming%20frames%20and%20personalized%20textual%20prompts%0Aas%20inputs.%20It%20autoaggressively%20performs%20scene%20understanding%2C%20danger%20level%0Aestimation%20and%20action%20decision.%20The%20predicted%20danger%20level%20reflects%20the%20risk%20of%0Athe%20potential%20action%20and%20provides%20an%20explicit%20reference%20for%20the%20final%20action%2C%0Awhich%20corresponds%20to%20the%20preset%20personalized%20prompt.%20Moreover%2C%20we%20construct%20a%0Aclosed-loop%20benchmark%20named%20PAD-Highway%20based%20on%20Highway-Env%20simulator%20to%0Acomprehensively%20evaluate%20the%20decision%20performance%20under%20traffic%20rules.%20The%0Adataset%20contains%20250%20hours%20videos%20with%20high-quality%20annotation%20to%20facilitate%0Athe%20development%20of%20PAD%20behavior%20analysis.%20Experimental%20results%20on%20the%0Aconstructed%20benchmark%20show%20that%20PADriver%20outperforms%20state-of-the-art%0Aapproaches%20on%20different%20evaluation%20metrics%2C%20and%20enables%20various%20driving%20modes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPADriver%253A%2520Towards%2520Personalized%2520Autonomous%2520Driving%26entry.906535625%3DGenghua%2520Kou%2520and%2520Fan%2520Jia%2520and%2520Weixin%2520Mao%2520and%2520Yingfei%2520Liu%2520and%2520Yucheng%2520Zhao%2520and%2520Ziheng%2520Zhang%2520and%2520Osamu%2520Yoshie%2520and%2520Tiancai%2520Wang%2520and%2520Ying%2520Li%2520and%2520Xiangyu%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520PADriver%252C%2520a%2520novel%2520closed-loop%2520framework%2520for%250Apersonalized%2520autonomous%2520driving%2520%2528PAD%2529.%2520Built%2520upon%2520Multi-modal%2520Large%2520Language%250AModel%2520%2528MLLM%2529%252C%2520PADriver%2520takes%2520streaming%2520frames%2520and%2520personalized%2520textual%2520prompts%250Aas%2520inputs.%2520It%2520autoaggressively%2520performs%2520scene%2520understanding%252C%2520danger%2520level%250Aestimation%2520and%2520action%2520decision.%2520The%2520predicted%2520danger%2520level%2520reflects%2520the%2520risk%2520of%250Athe%2520potential%2520action%2520and%2520provides%2520an%2520explicit%2520reference%2520for%2520the%2520final%2520action%252C%250Awhich%2520corresponds%2520to%2520the%2520preset%2520personalized%2520prompt.%2520Moreover%252C%2520we%2520construct%2520a%250Aclosed-loop%2520benchmark%2520named%2520PAD-Highway%2520based%2520on%2520Highway-Env%2520simulator%2520to%250Acomprehensively%2520evaluate%2520the%2520decision%2520performance%2520under%2520traffic%2520rules.%2520The%250Adataset%2520contains%2520250%2520hours%2520videos%2520with%2520high-quality%2520annotation%2520to%2520facilitate%250Athe%2520development%2520of%2520PAD%2520behavior%2520analysis.%2520Experimental%2520results%2520on%2520the%250Aconstructed%2520benchmark%2520show%2520that%2520PADriver%2520outperforms%2520state-of-the-art%250Aapproaches%2520on%2520different%2520evaluation%2520metrics%252C%2520and%2520enables%2520various%2520driving%2520modes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PADriver%3A%20Towards%20Personalized%20Autonomous%20Driving&entry.906535625=Genghua%20Kou%20and%20Fan%20Jia%20and%20Weixin%20Mao%20and%20Yingfei%20Liu%20and%20Yucheng%20Zhao%20and%20Ziheng%20Zhang%20and%20Osamu%20Yoshie%20and%20Tiancai%20Wang%20and%20Ying%20Li%20and%20Xiangyu%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20PADriver%2C%20a%20novel%20closed-loop%20framework%20for%0Apersonalized%20autonomous%20driving%20%28PAD%29.%20Built%20upon%20Multi-modal%20Large%20Language%0AModel%20%28MLLM%29%2C%20PADriver%20takes%20streaming%20frames%20and%20personalized%20textual%20prompts%0Aas%20inputs.%20It%20autoaggressively%20performs%20scene%20understanding%2C%20danger%20level%0Aestimation%20and%20action%20decision.%20The%20predicted%20danger%20level%20reflects%20the%20risk%20of%0Athe%20potential%20action%20and%20provides%20an%20explicit%20reference%20for%20the%20final%20action%2C%0Awhich%20corresponds%20to%20the%20preset%20personalized%20prompt.%20Moreover%2C%20we%20construct%20a%0Aclosed-loop%20benchmark%20named%20PAD-Highway%20based%20on%20Highway-Env%20simulator%20to%0Acomprehensively%20evaluate%20the%20decision%20performance%20under%20traffic%20rules.%20The%0Adataset%20contains%20250%20hours%20videos%20with%20high-quality%20annotation%20to%20facilitate%0Athe%20development%20of%20PAD%20behavior%20analysis.%20Experimental%20results%20on%20the%0Aconstructed%20benchmark%20show%20that%20PADriver%20outperforms%20state-of-the-art%0Aapproaches%20on%20different%20evaluation%20metrics%2C%20and%20enables%20various%20driving%20modes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05240v1&entry.124074799=Read"},
{"title": "DSDrive: Distilling Large Language Model for Lightweight End-to-End\n  Autonomous Driving with Unified Reasoning and Planning", "author": "Wenru Liu and Pei Liu and Jun Ma", "abstract": "  We present DSDrive, a streamlined end-to-end paradigm tailored for\nintegrating the reasoning and planning of autonomous vehicles into a unified\nframework. DSDrive leverages a compact LLM that employs a distillation method\nto preserve the enhanced reasoning capabilities of a larger-sized vision\nlanguage model (VLM). To effectively align the reasoning and planning tasks, a\nwaypoint-driven dual-head coordination module is further developed, which\nsynchronizes dataset structures, optimization objectives, and the learning\nprocess. By integrating these tasks into a unified framework, DSDrive anchors\non the planning results while incorporating detailed reasoning insights,\nthereby enhancing the interpretability and reliability of the end-to-end\npipeline. DSDrive has been thoroughly tested in closed-loop simulations, where\nit performs on par with benchmark models and even outperforms in many key\nmetrics, all while being more compact in size. Additionally, the computational\nefficiency of DSDrive (as reflected in its time and memory requirements during\ninference) has been significantly enhanced. Evidently thus, this work brings\npromising aspects and underscores the potential of lightweight systems in\ndelivering interpretable and efficient solutions for AD.\n", "link": "http://arxiv.org/abs/2505.05360v1", "date": "2025-05-08", "relevancy": 2.204, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSDrive%3A%20Distilling%20Large%20Language%20Model%20for%20Lightweight%20End-to-End%0A%20%20Autonomous%20Driving%20with%20Unified%20Reasoning%20and%20Planning&body=Title%3A%20DSDrive%3A%20Distilling%20Large%20Language%20Model%20for%20Lightweight%20End-to-End%0A%20%20Autonomous%20Driving%20with%20Unified%20Reasoning%20and%20Planning%0AAuthor%3A%20Wenru%20Liu%20and%20Pei%20Liu%20and%20Jun%20Ma%0AAbstract%3A%20%20%20We%20present%20DSDrive%2C%20a%20streamlined%20end-to-end%20paradigm%20tailored%20for%0Aintegrating%20the%20reasoning%20and%20planning%20of%20autonomous%20vehicles%20into%20a%20unified%0Aframework.%20DSDrive%20leverages%20a%20compact%20LLM%20that%20employs%20a%20distillation%20method%0Ato%20preserve%20the%20enhanced%20reasoning%20capabilities%20of%20a%20larger-sized%20vision%0Alanguage%20model%20%28VLM%29.%20To%20effectively%20align%20the%20reasoning%20and%20planning%20tasks%2C%20a%0Awaypoint-driven%20dual-head%20coordination%20module%20is%20further%20developed%2C%20which%0Asynchronizes%20dataset%20structures%2C%20optimization%20objectives%2C%20and%20the%20learning%0Aprocess.%20By%20integrating%20these%20tasks%20into%20a%20unified%20framework%2C%20DSDrive%20anchors%0Aon%20the%20planning%20results%20while%20incorporating%20detailed%20reasoning%20insights%2C%0Athereby%20enhancing%20the%20interpretability%20and%20reliability%20of%20the%20end-to-end%0Apipeline.%20DSDrive%20has%20been%20thoroughly%20tested%20in%20closed-loop%20simulations%2C%20where%0Ait%20performs%20on%20par%20with%20benchmark%20models%20and%20even%20outperforms%20in%20many%20key%0Ametrics%2C%20all%20while%20being%20more%20compact%20in%20size.%20Additionally%2C%20the%20computational%0Aefficiency%20of%20DSDrive%20%28as%20reflected%20in%20its%20time%20and%20memory%20requirements%20during%0Ainference%29%20has%20been%20significantly%20enhanced.%20Evidently%20thus%2C%20this%20work%20brings%0Apromising%20aspects%20and%20underscores%20the%20potential%20of%20lightweight%20systems%20in%0Adelivering%20interpretable%20and%20efficient%20solutions%20for%20AD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSDrive%253A%2520Distilling%2520Large%2520Language%2520Model%2520for%2520Lightweight%2520End-to-End%250A%2520%2520Autonomous%2520Driving%2520with%2520Unified%2520Reasoning%2520and%2520Planning%26entry.906535625%3DWenru%2520Liu%2520and%2520Pei%2520Liu%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520We%2520present%2520DSDrive%252C%2520a%2520streamlined%2520end-to-end%2520paradigm%2520tailored%2520for%250Aintegrating%2520the%2520reasoning%2520and%2520planning%2520of%2520autonomous%2520vehicles%2520into%2520a%2520unified%250Aframework.%2520DSDrive%2520leverages%2520a%2520compact%2520LLM%2520that%2520employs%2520a%2520distillation%2520method%250Ato%2520preserve%2520the%2520enhanced%2520reasoning%2520capabilities%2520of%2520a%2520larger-sized%2520vision%250Alanguage%2520model%2520%2528VLM%2529.%2520To%2520effectively%2520align%2520the%2520reasoning%2520and%2520planning%2520tasks%252C%2520a%250Awaypoint-driven%2520dual-head%2520coordination%2520module%2520is%2520further%2520developed%252C%2520which%250Asynchronizes%2520dataset%2520structures%252C%2520optimization%2520objectives%252C%2520and%2520the%2520learning%250Aprocess.%2520By%2520integrating%2520these%2520tasks%2520into%2520a%2520unified%2520framework%252C%2520DSDrive%2520anchors%250Aon%2520the%2520planning%2520results%2520while%2520incorporating%2520detailed%2520reasoning%2520insights%252C%250Athereby%2520enhancing%2520the%2520interpretability%2520and%2520reliability%2520of%2520the%2520end-to-end%250Apipeline.%2520DSDrive%2520has%2520been%2520thoroughly%2520tested%2520in%2520closed-loop%2520simulations%252C%2520where%250Ait%2520performs%2520on%2520par%2520with%2520benchmark%2520models%2520and%2520even%2520outperforms%2520in%2520many%2520key%250Ametrics%252C%2520all%2520while%2520being%2520more%2520compact%2520in%2520size.%2520Additionally%252C%2520the%2520computational%250Aefficiency%2520of%2520DSDrive%2520%2528as%2520reflected%2520in%2520its%2520time%2520and%2520memory%2520requirements%2520during%250Ainference%2529%2520has%2520been%2520significantly%2520enhanced.%2520Evidently%2520thus%252C%2520this%2520work%2520brings%250Apromising%2520aspects%2520and%2520underscores%2520the%2520potential%2520of%2520lightweight%2520systems%2520in%250Adelivering%2520interpretable%2520and%2520efficient%2520solutions%2520for%2520AD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSDrive%3A%20Distilling%20Large%20Language%20Model%20for%20Lightweight%20End-to-End%0A%20%20Autonomous%20Driving%20with%20Unified%20Reasoning%20and%20Planning&entry.906535625=Wenru%20Liu%20and%20Pei%20Liu%20and%20Jun%20Ma&entry.1292438233=%20%20We%20present%20DSDrive%2C%20a%20streamlined%20end-to-end%20paradigm%20tailored%20for%0Aintegrating%20the%20reasoning%20and%20planning%20of%20autonomous%20vehicles%20into%20a%20unified%0Aframework.%20DSDrive%20leverages%20a%20compact%20LLM%20that%20employs%20a%20distillation%20method%0Ato%20preserve%20the%20enhanced%20reasoning%20capabilities%20of%20a%20larger-sized%20vision%0Alanguage%20model%20%28VLM%29.%20To%20effectively%20align%20the%20reasoning%20and%20planning%20tasks%2C%20a%0Awaypoint-driven%20dual-head%20coordination%20module%20is%20further%20developed%2C%20which%0Asynchronizes%20dataset%20structures%2C%20optimization%20objectives%2C%20and%20the%20learning%0Aprocess.%20By%20integrating%20these%20tasks%20into%20a%20unified%20framework%2C%20DSDrive%20anchors%0Aon%20the%20planning%20results%20while%20incorporating%20detailed%20reasoning%20insights%2C%0Athereby%20enhancing%20the%20interpretability%20and%20reliability%20of%20the%20end-to-end%0Apipeline.%20DSDrive%20has%20been%20thoroughly%20tested%20in%20closed-loop%20simulations%2C%20where%0Ait%20performs%20on%20par%20with%20benchmark%20models%20and%20even%20outperforms%20in%20many%20key%0Ametrics%2C%20all%20while%20being%20more%20compact%20in%20size.%20Additionally%2C%20the%20computational%0Aefficiency%20of%20DSDrive%20%28as%20reflected%20in%20its%20time%20and%20memory%20requirements%20during%0Ainference%29%20has%20been%20significantly%20enhanced.%20Evidently%20thus%2C%20this%20work%20brings%0Apromising%20aspects%20and%20underscores%20the%20potential%20of%20lightweight%20systems%20in%0Adelivering%20interpretable%20and%20efficient%20solutions%20for%20AD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05360v1&entry.124074799=Read"},
{"title": "Federated EndoViT: Pretraining Vision Transformers via Federated\n  Learning on Endoscopic Image Collections", "author": "Max Kirchner and Alexander C. Jenke and Sebastian Bodenstedt and Fiona R. Kolbinger and Oliver L. Saldanha and Jakob N. Kather and Martin Wagner and Stefanie Speidel", "abstract": "  Purpose: In this study, we investigate the training of foundation models\nusing federated learning to address data-sharing limitations and enable\ncollaborative model training without data transfer for minimally invasive\nsurgery. Methods: Inspired by the EndoViT study, we adapt the Masked\nAutoencoder for federated learning, enhancing it with adaptive Sharpness-Aware\nMinimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is\npretrained on the Endo700k dataset collection and later fine-tuned and\nevaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,\nand Surgical Phase Recognition. Results: Our findings demonstrate that\nintegrating adaptive FedSAM into the federated MAE approach improves\npretraining, leading to a reduction in reconstruction loss per patch. The\napplication of FL-EndoViT in surgical downstream tasks results in performance\ncomparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over\nCEN-EndoViT in surgical scene segmentation when data is limited and in action\ntriplet recognition when large datasets are used. Conclusion: These findings\nhighlight the potential of federated learning for privacy-preserving training\nof surgical foundation models, offering a robust and generalizable solution for\nsurgical data science. Effective collaboration requires adapting federated\nlearning methods, such as the integration of FedSAM, which can accommodate the\ninherent data heterogeneity across institutions. In future, exploring FL in\nvideo-based models may enhance these capabilities by incorporating\nspatiotemporal dynamics crucial for real-world surgical environments.\n", "link": "http://arxiv.org/abs/2504.16612v2", "date": "2025-05-08", "relevancy": 2.2037, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.562}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20EndoViT%3A%20Pretraining%20Vision%20Transformers%20via%20Federated%0A%20%20Learning%20on%20Endoscopic%20Image%20Collections&body=Title%3A%20Federated%20EndoViT%3A%20Pretraining%20Vision%20Transformers%20via%20Federated%0A%20%20Learning%20on%20Endoscopic%20Image%20Collections%0AAuthor%3A%20Max%20Kirchner%20and%20Alexander%20C.%20Jenke%20and%20Sebastian%20Bodenstedt%20and%20Fiona%20R.%20Kolbinger%20and%20Oliver%20L.%20Saldanha%20and%20Jakob%20N.%20Kather%20and%20Martin%20Wagner%20and%20Stefanie%20Speidel%0AAbstract%3A%20%20%20Purpose%3A%20In%20this%20study%2C%20we%20investigate%20the%20training%20of%20foundation%20models%0Ausing%20federated%20learning%20to%20address%20data-sharing%20limitations%20and%20enable%0Acollaborative%20model%20training%20without%20data%20transfer%20for%20minimally%20invasive%0Asurgery.%20Methods%3A%20Inspired%20by%20the%20EndoViT%20study%2C%20we%20adapt%20the%20Masked%0AAutoencoder%20for%20federated%20learning%2C%20enhancing%20it%20with%20adaptive%20Sharpness-Aware%0AMinimization%20%28FedSAM%29%20and%20Stochastic%20Weight%20Averaging%20%28SWA%29.%20Our%20model%20is%0Apretrained%20on%20the%20Endo700k%20dataset%20collection%20and%20later%20fine-tuned%20and%0Aevaluated%20for%20tasks%20such%20as%20Semantic%20Segmentation%2C%20Action%20Triplet%20Recognition%2C%0Aand%20Surgical%20Phase%20Recognition.%20Results%3A%20Our%20findings%20demonstrate%20that%0Aintegrating%20adaptive%20FedSAM%20into%20the%20federated%20MAE%20approach%20improves%0Apretraining%2C%20leading%20to%20a%20reduction%20in%20reconstruction%20loss%20per%20patch.%20The%0Aapplication%20of%20FL-EndoViT%20in%20surgical%20downstream%20tasks%20results%20in%20performance%0Acomparable%20to%20CEN-EndoViT.%20Furthermore%2C%20FL-EndoViT%20exhibits%20advantages%20over%0ACEN-EndoViT%20in%20surgical%20scene%20segmentation%20when%20data%20is%20limited%20and%20in%20action%0Atriplet%20recognition%20when%20large%20datasets%20are%20used.%20Conclusion%3A%20These%20findings%0Ahighlight%20the%20potential%20of%20federated%20learning%20for%20privacy-preserving%20training%0Aof%20surgical%20foundation%20models%2C%20offering%20a%20robust%20and%20generalizable%20solution%20for%0Asurgical%20data%20science.%20Effective%20collaboration%20requires%20adapting%20federated%0Alearning%20methods%2C%20such%20as%20the%20integration%20of%20FedSAM%2C%20which%20can%20accommodate%20the%0Ainherent%20data%20heterogeneity%20across%20institutions.%20In%20future%2C%20exploring%20FL%20in%0Avideo-based%20models%20may%20enhance%20these%20capabilities%20by%20incorporating%0Aspatiotemporal%20dynamics%20crucial%20for%20real-world%20surgical%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520EndoViT%253A%2520Pretraining%2520Vision%2520Transformers%2520via%2520Federated%250A%2520%2520Learning%2520on%2520Endoscopic%2520Image%2520Collections%26entry.906535625%3DMax%2520Kirchner%2520and%2520Alexander%2520C.%2520Jenke%2520and%2520Sebastian%2520Bodenstedt%2520and%2520Fiona%2520R.%2520Kolbinger%2520and%2520Oliver%2520L.%2520Saldanha%2520and%2520Jakob%2520N.%2520Kather%2520and%2520Martin%2520Wagner%2520and%2520Stefanie%2520Speidel%26entry.1292438233%3D%2520%2520Purpose%253A%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520training%2520of%2520foundation%2520models%250Ausing%2520federated%2520learning%2520to%2520address%2520data-sharing%2520limitations%2520and%2520enable%250Acollaborative%2520model%2520training%2520without%2520data%2520transfer%2520for%2520minimally%2520invasive%250Asurgery.%2520Methods%253A%2520Inspired%2520by%2520the%2520EndoViT%2520study%252C%2520we%2520adapt%2520the%2520Masked%250AAutoencoder%2520for%2520federated%2520learning%252C%2520enhancing%2520it%2520with%2520adaptive%2520Sharpness-Aware%250AMinimization%2520%2528FedSAM%2529%2520and%2520Stochastic%2520Weight%2520Averaging%2520%2528SWA%2529.%2520Our%2520model%2520is%250Apretrained%2520on%2520the%2520Endo700k%2520dataset%2520collection%2520and%2520later%2520fine-tuned%2520and%250Aevaluated%2520for%2520tasks%2520such%2520as%2520Semantic%2520Segmentation%252C%2520Action%2520Triplet%2520Recognition%252C%250Aand%2520Surgical%2520Phase%2520Recognition.%2520Results%253A%2520Our%2520findings%2520demonstrate%2520that%250Aintegrating%2520adaptive%2520FedSAM%2520into%2520the%2520federated%2520MAE%2520approach%2520improves%250Apretraining%252C%2520leading%2520to%2520a%2520reduction%2520in%2520reconstruction%2520loss%2520per%2520patch.%2520The%250Aapplication%2520of%2520FL-EndoViT%2520in%2520surgical%2520downstream%2520tasks%2520results%2520in%2520performance%250Acomparable%2520to%2520CEN-EndoViT.%2520Furthermore%252C%2520FL-EndoViT%2520exhibits%2520advantages%2520over%250ACEN-EndoViT%2520in%2520surgical%2520scene%2520segmentation%2520when%2520data%2520is%2520limited%2520and%2520in%2520action%250Atriplet%2520recognition%2520when%2520large%2520datasets%2520are%2520used.%2520Conclusion%253A%2520These%2520findings%250Ahighlight%2520the%2520potential%2520of%2520federated%2520learning%2520for%2520privacy-preserving%2520training%250Aof%2520surgical%2520foundation%2520models%252C%2520offering%2520a%2520robust%2520and%2520generalizable%2520solution%2520for%250Asurgical%2520data%2520science.%2520Effective%2520collaboration%2520requires%2520adapting%2520federated%250Alearning%2520methods%252C%2520such%2520as%2520the%2520integration%2520of%2520FedSAM%252C%2520which%2520can%2520accommodate%2520the%250Ainherent%2520data%2520heterogeneity%2520across%2520institutions.%2520In%2520future%252C%2520exploring%2520FL%2520in%250Avideo-based%2520models%2520may%2520enhance%2520these%2520capabilities%2520by%2520incorporating%250Aspatiotemporal%2520dynamics%2520crucial%2520for%2520real-world%2520surgical%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20EndoViT%3A%20Pretraining%20Vision%20Transformers%20via%20Federated%0A%20%20Learning%20on%20Endoscopic%20Image%20Collections&entry.906535625=Max%20Kirchner%20and%20Alexander%20C.%20Jenke%20and%20Sebastian%20Bodenstedt%20and%20Fiona%20R.%20Kolbinger%20and%20Oliver%20L.%20Saldanha%20and%20Jakob%20N.%20Kather%20and%20Martin%20Wagner%20and%20Stefanie%20Speidel&entry.1292438233=%20%20Purpose%3A%20In%20this%20study%2C%20we%20investigate%20the%20training%20of%20foundation%20models%0Ausing%20federated%20learning%20to%20address%20data-sharing%20limitations%20and%20enable%0Acollaborative%20model%20training%20without%20data%20transfer%20for%20minimally%20invasive%0Asurgery.%20Methods%3A%20Inspired%20by%20the%20EndoViT%20study%2C%20we%20adapt%20the%20Masked%0AAutoencoder%20for%20federated%20learning%2C%20enhancing%20it%20with%20adaptive%20Sharpness-Aware%0AMinimization%20%28FedSAM%29%20and%20Stochastic%20Weight%20Averaging%20%28SWA%29.%20Our%20model%20is%0Apretrained%20on%20the%20Endo700k%20dataset%20collection%20and%20later%20fine-tuned%20and%0Aevaluated%20for%20tasks%20such%20as%20Semantic%20Segmentation%2C%20Action%20Triplet%20Recognition%2C%0Aand%20Surgical%20Phase%20Recognition.%20Results%3A%20Our%20findings%20demonstrate%20that%0Aintegrating%20adaptive%20FedSAM%20into%20the%20federated%20MAE%20approach%20improves%0Apretraining%2C%20leading%20to%20a%20reduction%20in%20reconstruction%20loss%20per%20patch.%20The%0Aapplication%20of%20FL-EndoViT%20in%20surgical%20downstream%20tasks%20results%20in%20performance%0Acomparable%20to%20CEN-EndoViT.%20Furthermore%2C%20FL-EndoViT%20exhibits%20advantages%20over%0ACEN-EndoViT%20in%20surgical%20scene%20segmentation%20when%20data%20is%20limited%20and%20in%20action%0Atriplet%20recognition%20when%20large%20datasets%20are%20used.%20Conclusion%3A%20These%20findings%0Ahighlight%20the%20potential%20of%20federated%20learning%20for%20privacy-preserving%20training%0Aof%20surgical%20foundation%20models%2C%20offering%20a%20robust%20and%20generalizable%20solution%20for%0Asurgical%20data%20science.%20Effective%20collaboration%20requires%20adapting%20federated%0Alearning%20methods%2C%20such%20as%20the%20integration%20of%20FedSAM%2C%20which%20can%20accommodate%20the%0Ainherent%20data%20heterogeneity%20across%20institutions.%20In%20future%2C%20exploring%20FL%20in%0Avideo-based%20models%20may%20enhance%20these%20capabilities%20by%20incorporating%0Aspatiotemporal%20dynamics%20crucial%20for%20real-world%20surgical%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16612v2&entry.124074799=Read"},
{"title": "PillarMamba: Learning Local-Global Context for Roadside Point Cloud via\n  Hybrid State Space Model", "author": "Zhang Zhang and Chao Sun and Chao Yue and Da Wen and Tianze Wang and Jianghao Leng", "abstract": "  Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything\n(V2X) tasks, roadside perception has received increasing attention in recent\nyears, as it can extend the perception range of connected vehicles and improve\ntraffic safety. However, roadside point cloud oriented 3D object detection has\nnot been effectively explored. To some extent, the key to the performance of a\npoint cloud detector lies in the receptive field of the network and the ability\nto effectively utilize the scene context. The recent emergence of Mamba, based\non State Space Model (SSM), has shaken up the traditional convolution and\ntransformers that have long been the foundational building blocks, due to its\nefficient global receptive field. In this work, we introduce Mamba to\npillar-based roadside point cloud perception and propose a framework based on\nCross-stage State-space Group (CSG), called PillarMamba. It enhances the\nexpressiveness of the network and achieves efficient computation through\ncross-stage feature fusion. However, due to the limitations of scan directions,\nstate space model faces local connection disrupted and historical relationship\nforgotten. To address this, we propose the Hybrid State-space Block (HSB) to\nobtain the local-global context of roadside point cloud. Specifically, it\nenhances neighborhood connections through local convolution and preserves\nhistorical memory through residual attention. The proposed method outperforms\nthe state-of-the-art methods on the popular large scale roadside benchmark:\nDAIR-V2X-I. The code will be released soon.\n", "link": "http://arxiv.org/abs/2505.05397v1", "date": "2025-05-08", "relevancy": 2.193, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5719}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5322}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PillarMamba%3A%20Learning%20Local-Global%20Context%20for%20Roadside%20Point%20Cloud%20via%0A%20%20Hybrid%20State%20Space%20Model&body=Title%3A%20PillarMamba%3A%20Learning%20Local-Global%20Context%20for%20Roadside%20Point%20Cloud%20via%0A%20%20Hybrid%20State%20Space%20Model%0AAuthor%3A%20Zhang%20Zhang%20and%20Chao%20Sun%20and%20Chao%20Yue%20and%20Da%20Wen%20and%20Tianze%20Wang%20and%20Jianghao%20Leng%0AAbstract%3A%20%20%20Serving%20the%20Intelligent%20Transport%20System%20%28ITS%29%20and%20Vehicle-to-Everything%0A%28V2X%29%20tasks%2C%20roadside%20perception%20has%20received%20increasing%20attention%20in%20recent%0Ayears%2C%20as%20it%20can%20extend%20the%20perception%20range%20of%20connected%20vehicles%20and%20improve%0Atraffic%20safety.%20However%2C%20roadside%20point%20cloud%20oriented%203D%20object%20detection%20has%0Anot%20been%20effectively%20explored.%20To%20some%20extent%2C%20the%20key%20to%20the%20performance%20of%20a%0Apoint%20cloud%20detector%20lies%20in%20the%20receptive%20field%20of%20the%20network%20and%20the%20ability%0Ato%20effectively%20utilize%20the%20scene%20context.%20The%20recent%20emergence%20of%20Mamba%2C%20based%0Aon%20State%20Space%20Model%20%28SSM%29%2C%20has%20shaken%20up%20the%20traditional%20convolution%20and%0Atransformers%20that%20have%20long%20been%20the%20foundational%20building%20blocks%2C%20due%20to%20its%0Aefficient%20global%20receptive%20field.%20In%20this%20work%2C%20we%20introduce%20Mamba%20to%0Apillar-based%20roadside%20point%20cloud%20perception%20and%20propose%20a%20framework%20based%20on%0ACross-stage%20State-space%20Group%20%28CSG%29%2C%20called%20PillarMamba.%20It%20enhances%20the%0Aexpressiveness%20of%20the%20network%20and%20achieves%20efficient%20computation%20through%0Across-stage%20feature%20fusion.%20However%2C%20due%20to%20the%20limitations%20of%20scan%20directions%2C%0Astate%20space%20model%20faces%20local%20connection%20disrupted%20and%20historical%20relationship%0Aforgotten.%20To%20address%20this%2C%20we%20propose%20the%20Hybrid%20State-space%20Block%20%28HSB%29%20to%0Aobtain%20the%20local-global%20context%20of%20roadside%20point%20cloud.%20Specifically%2C%20it%0Aenhances%20neighborhood%20connections%20through%20local%20convolution%20and%20preserves%0Ahistorical%20memory%20through%20residual%20attention.%20The%20proposed%20method%20outperforms%0Athe%20state-of-the-art%20methods%20on%20the%20popular%20large%20scale%20roadside%20benchmark%3A%0ADAIR-V2X-I.%20The%20code%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPillarMamba%253A%2520Learning%2520Local-Global%2520Context%2520for%2520Roadside%2520Point%2520Cloud%2520via%250A%2520%2520Hybrid%2520State%2520Space%2520Model%26entry.906535625%3DZhang%2520Zhang%2520and%2520Chao%2520Sun%2520and%2520Chao%2520Yue%2520and%2520Da%2520Wen%2520and%2520Tianze%2520Wang%2520and%2520Jianghao%2520Leng%26entry.1292438233%3D%2520%2520Serving%2520the%2520Intelligent%2520Transport%2520System%2520%2528ITS%2529%2520and%2520Vehicle-to-Everything%250A%2528V2X%2529%2520tasks%252C%2520roadside%2520perception%2520has%2520received%2520increasing%2520attention%2520in%2520recent%250Ayears%252C%2520as%2520it%2520can%2520extend%2520the%2520perception%2520range%2520of%2520connected%2520vehicles%2520and%2520improve%250Atraffic%2520safety.%2520However%252C%2520roadside%2520point%2520cloud%2520oriented%25203D%2520object%2520detection%2520has%250Anot%2520been%2520effectively%2520explored.%2520To%2520some%2520extent%252C%2520the%2520key%2520to%2520the%2520performance%2520of%2520a%250Apoint%2520cloud%2520detector%2520lies%2520in%2520the%2520receptive%2520field%2520of%2520the%2520network%2520and%2520the%2520ability%250Ato%2520effectively%2520utilize%2520the%2520scene%2520context.%2520The%2520recent%2520emergence%2520of%2520Mamba%252C%2520based%250Aon%2520State%2520Space%2520Model%2520%2528SSM%2529%252C%2520has%2520shaken%2520up%2520the%2520traditional%2520convolution%2520and%250Atransformers%2520that%2520have%2520long%2520been%2520the%2520foundational%2520building%2520blocks%252C%2520due%2520to%2520its%250Aefficient%2520global%2520receptive%2520field.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Mamba%2520to%250Apillar-based%2520roadside%2520point%2520cloud%2520perception%2520and%2520propose%2520a%2520framework%2520based%2520on%250ACross-stage%2520State-space%2520Group%2520%2528CSG%2529%252C%2520called%2520PillarMamba.%2520It%2520enhances%2520the%250Aexpressiveness%2520of%2520the%2520network%2520and%2520achieves%2520efficient%2520computation%2520through%250Across-stage%2520feature%2520fusion.%2520However%252C%2520due%2520to%2520the%2520limitations%2520of%2520scan%2520directions%252C%250Astate%2520space%2520model%2520faces%2520local%2520connection%2520disrupted%2520and%2520historical%2520relationship%250Aforgotten.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Hybrid%2520State-space%2520Block%2520%2528HSB%2529%2520to%250Aobtain%2520the%2520local-global%2520context%2520of%2520roadside%2520point%2520cloud.%2520Specifically%252C%2520it%250Aenhances%2520neighborhood%2520connections%2520through%2520local%2520convolution%2520and%2520preserves%250Ahistorical%2520memory%2520through%2520residual%2520attention.%2520The%2520proposed%2520method%2520outperforms%250Athe%2520state-of-the-art%2520methods%2520on%2520the%2520popular%2520large%2520scale%2520roadside%2520benchmark%253A%250ADAIR-V2X-I.%2520The%2520code%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PillarMamba%3A%20Learning%20Local-Global%20Context%20for%20Roadside%20Point%20Cloud%20via%0A%20%20Hybrid%20State%20Space%20Model&entry.906535625=Zhang%20Zhang%20and%20Chao%20Sun%20and%20Chao%20Yue%20and%20Da%20Wen%20and%20Tianze%20Wang%20and%20Jianghao%20Leng&entry.1292438233=%20%20Serving%20the%20Intelligent%20Transport%20System%20%28ITS%29%20and%20Vehicle-to-Everything%0A%28V2X%29%20tasks%2C%20roadside%20perception%20has%20received%20increasing%20attention%20in%20recent%0Ayears%2C%20as%20it%20can%20extend%20the%20perception%20range%20of%20connected%20vehicles%20and%20improve%0Atraffic%20safety.%20However%2C%20roadside%20point%20cloud%20oriented%203D%20object%20detection%20has%0Anot%20been%20effectively%20explored.%20To%20some%20extent%2C%20the%20key%20to%20the%20performance%20of%20a%0Apoint%20cloud%20detector%20lies%20in%20the%20receptive%20field%20of%20the%20network%20and%20the%20ability%0Ato%20effectively%20utilize%20the%20scene%20context.%20The%20recent%20emergence%20of%20Mamba%2C%20based%0Aon%20State%20Space%20Model%20%28SSM%29%2C%20has%20shaken%20up%20the%20traditional%20convolution%20and%0Atransformers%20that%20have%20long%20been%20the%20foundational%20building%20blocks%2C%20due%20to%20its%0Aefficient%20global%20receptive%20field.%20In%20this%20work%2C%20we%20introduce%20Mamba%20to%0Apillar-based%20roadside%20point%20cloud%20perception%20and%20propose%20a%20framework%20based%20on%0ACross-stage%20State-space%20Group%20%28CSG%29%2C%20called%20PillarMamba.%20It%20enhances%20the%0Aexpressiveness%20of%20the%20network%20and%20achieves%20efficient%20computation%20through%0Across-stage%20feature%20fusion.%20However%2C%20due%20to%20the%20limitations%20of%20scan%20directions%2C%0Astate%20space%20model%20faces%20local%20connection%20disrupted%20and%20historical%20relationship%0Aforgotten.%20To%20address%20this%2C%20we%20propose%20the%20Hybrid%20State-space%20Block%20%28HSB%29%20to%0Aobtain%20the%20local-global%20context%20of%20roadside%20point%20cloud.%20Specifically%2C%20it%0Aenhances%20neighborhood%20connections%20through%20local%20convolution%20and%20preserves%0Ahistorical%20memory%20through%20residual%20attention.%20The%20proposed%20method%20outperforms%0Athe%20state-of-the-art%20methods%20on%20the%20popular%20large%20scale%20roadside%20benchmark%3A%0ADAIR-V2X-I.%20The%20code%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05397v1&entry.124074799=Read"},
{"title": "Transformer-based assignment decision network for multiple object\n  tracking", "author": "Athena Psalta and Vasileios Tsironis and Konstantinos Karantzalos", "abstract": "  Data association is a crucial component for any multiple object tracking\n(MOT) method that follows the tracking-by-detection paradigm. To generate\ncomplete trajectories such methods employ a data association process to\nestablish assignments between detections and existing targets during each\ntimestep. Recent data association approaches try to solve either a\nmulti-dimensional linear assignment task or a network flow minimization problem\nor tackle it via multiple hypotheses tracking. However, during inference an\noptimization step that computes optimal assignments is required for every\nsequence frame inducing additional complexity to any given solution. To this\nend, in the context of this work we introduce Transformer-based Assignment\nDecision Network (TADN) that tackles data association without the need of any\nexplicit optimization during inference. In particular, TADN can directly infer\nassignment pairs between detections and active targets in a single forward pass\nof the network. We have integrated TADN in a rather simple MOT framework,\ndesigned a novel training strategy for efficient end-to-end training and\ndemonstrated the high potential of our approach for online visual\ntracking-by-detection MOT on several popular benchmarks, i.e. MOT17, MOT20 and\nUA-DETRAC. Our proposed approach demonstrates strong performance in most\nevaluation metrics despite its simple nature as a tracker lacking significant\nauxiliary components such as occlusion handling or re-identification. The\nimplementation of our method is publicly available at\nhttps://github.com/psaltaath/tadn-mot.\n", "link": "http://arxiv.org/abs/2208.03571v3", "date": "2025-05-08", "relevancy": 2.1835, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5591}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5369}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer-based%20assignment%20decision%20network%20for%20multiple%20object%0A%20%20tracking&body=Title%3A%20Transformer-based%20assignment%20decision%20network%20for%20multiple%20object%0A%20%20tracking%0AAuthor%3A%20Athena%20Psalta%20and%20Vasileios%20Tsironis%20and%20Konstantinos%20Karantzalos%0AAbstract%3A%20%20%20Data%20association%20is%20a%20crucial%20component%20for%20any%20multiple%20object%20tracking%0A%28MOT%29%20method%20that%20follows%20the%20tracking-by-detection%20paradigm.%20To%20generate%0Acomplete%20trajectories%20such%20methods%20employ%20a%20data%20association%20process%20to%0Aestablish%20assignments%20between%20detections%20and%20existing%20targets%20during%20each%0Atimestep.%20Recent%20data%20association%20approaches%20try%20to%20solve%20either%20a%0Amulti-dimensional%20linear%20assignment%20task%20or%20a%20network%20flow%20minimization%20problem%0Aor%20tackle%20it%20via%20multiple%20hypotheses%20tracking.%20However%2C%20during%20inference%20an%0Aoptimization%20step%20that%20computes%20optimal%20assignments%20is%20required%20for%20every%0Asequence%20frame%20inducing%20additional%20complexity%20to%20any%20given%20solution.%20To%20this%0Aend%2C%20in%20the%20context%20of%20this%20work%20we%20introduce%20Transformer-based%20Assignment%0ADecision%20Network%20%28TADN%29%20that%20tackles%20data%20association%20without%20the%20need%20of%20any%0Aexplicit%20optimization%20during%20inference.%20In%20particular%2C%20TADN%20can%20directly%20infer%0Aassignment%20pairs%20between%20detections%20and%20active%20targets%20in%20a%20single%20forward%20pass%0Aof%20the%20network.%20We%20have%20integrated%20TADN%20in%20a%20rather%20simple%20MOT%20framework%2C%0Adesigned%20a%20novel%20training%20strategy%20for%20efficient%20end-to-end%20training%20and%0Ademonstrated%20the%20high%20potential%20of%20our%20approach%20for%20online%20visual%0Atracking-by-detection%20MOT%20on%20several%20popular%20benchmarks%2C%20i.e.%20MOT17%2C%20MOT20%20and%0AUA-DETRAC.%20Our%20proposed%20approach%20demonstrates%20strong%20performance%20in%20most%0Aevaluation%20metrics%20despite%20its%20simple%20nature%20as%20a%20tracker%20lacking%20significant%0Aauxiliary%20components%20such%20as%20occlusion%20handling%20or%20re-identification.%20The%0Aimplementation%20of%20our%20method%20is%20publicly%20available%20at%0Ahttps%3A//github.com/psaltaath/tadn-mot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.03571v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer-based%2520assignment%2520decision%2520network%2520for%2520multiple%2520object%250A%2520%2520tracking%26entry.906535625%3DAthena%2520Psalta%2520and%2520Vasileios%2520Tsironis%2520and%2520Konstantinos%2520Karantzalos%26entry.1292438233%3D%2520%2520Data%2520association%2520is%2520a%2520crucial%2520component%2520for%2520any%2520multiple%2520object%2520tracking%250A%2528MOT%2529%2520method%2520that%2520follows%2520the%2520tracking-by-detection%2520paradigm.%2520To%2520generate%250Acomplete%2520trajectories%2520such%2520methods%2520employ%2520a%2520data%2520association%2520process%2520to%250Aestablish%2520assignments%2520between%2520detections%2520and%2520existing%2520targets%2520during%2520each%250Atimestep.%2520Recent%2520data%2520association%2520approaches%2520try%2520to%2520solve%2520either%2520a%250Amulti-dimensional%2520linear%2520assignment%2520task%2520or%2520a%2520network%2520flow%2520minimization%2520problem%250Aor%2520tackle%2520it%2520via%2520multiple%2520hypotheses%2520tracking.%2520However%252C%2520during%2520inference%2520an%250Aoptimization%2520step%2520that%2520computes%2520optimal%2520assignments%2520is%2520required%2520for%2520every%250Asequence%2520frame%2520inducing%2520additional%2520complexity%2520to%2520any%2520given%2520solution.%2520To%2520this%250Aend%252C%2520in%2520the%2520context%2520of%2520this%2520work%2520we%2520introduce%2520Transformer-based%2520Assignment%250ADecision%2520Network%2520%2528TADN%2529%2520that%2520tackles%2520data%2520association%2520without%2520the%2520need%2520of%2520any%250Aexplicit%2520optimization%2520during%2520inference.%2520In%2520particular%252C%2520TADN%2520can%2520directly%2520infer%250Aassignment%2520pairs%2520between%2520detections%2520and%2520active%2520targets%2520in%2520a%2520single%2520forward%2520pass%250Aof%2520the%2520network.%2520We%2520have%2520integrated%2520TADN%2520in%2520a%2520rather%2520simple%2520MOT%2520framework%252C%250Adesigned%2520a%2520novel%2520training%2520strategy%2520for%2520efficient%2520end-to-end%2520training%2520and%250Ademonstrated%2520the%2520high%2520potential%2520of%2520our%2520approach%2520for%2520online%2520visual%250Atracking-by-detection%2520MOT%2520on%2520several%2520popular%2520benchmarks%252C%2520i.e.%2520MOT17%252C%2520MOT20%2520and%250AUA-DETRAC.%2520Our%2520proposed%2520approach%2520demonstrates%2520strong%2520performance%2520in%2520most%250Aevaluation%2520metrics%2520despite%2520its%2520simple%2520nature%2520as%2520a%2520tracker%2520lacking%2520significant%250Aauxiliary%2520components%2520such%2520as%2520occlusion%2520handling%2520or%2520re-identification.%2520The%250Aimplementation%2520of%2520our%2520method%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/psaltaath/tadn-mot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.03571v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer-based%20assignment%20decision%20network%20for%20multiple%20object%0A%20%20tracking&entry.906535625=Athena%20Psalta%20and%20Vasileios%20Tsironis%20and%20Konstantinos%20Karantzalos&entry.1292438233=%20%20Data%20association%20is%20a%20crucial%20component%20for%20any%20multiple%20object%20tracking%0A%28MOT%29%20method%20that%20follows%20the%20tracking-by-detection%20paradigm.%20To%20generate%0Acomplete%20trajectories%20such%20methods%20employ%20a%20data%20association%20process%20to%0Aestablish%20assignments%20between%20detections%20and%20existing%20targets%20during%20each%0Atimestep.%20Recent%20data%20association%20approaches%20try%20to%20solve%20either%20a%0Amulti-dimensional%20linear%20assignment%20task%20or%20a%20network%20flow%20minimization%20problem%0Aor%20tackle%20it%20via%20multiple%20hypotheses%20tracking.%20However%2C%20during%20inference%20an%0Aoptimization%20step%20that%20computes%20optimal%20assignments%20is%20required%20for%20every%0Asequence%20frame%20inducing%20additional%20complexity%20to%20any%20given%20solution.%20To%20this%0Aend%2C%20in%20the%20context%20of%20this%20work%20we%20introduce%20Transformer-based%20Assignment%0ADecision%20Network%20%28TADN%29%20that%20tackles%20data%20association%20without%20the%20need%20of%20any%0Aexplicit%20optimization%20during%20inference.%20In%20particular%2C%20TADN%20can%20directly%20infer%0Aassignment%20pairs%20between%20detections%20and%20active%20targets%20in%20a%20single%20forward%20pass%0Aof%20the%20network.%20We%20have%20integrated%20TADN%20in%20a%20rather%20simple%20MOT%20framework%2C%0Adesigned%20a%20novel%20training%20strategy%20for%20efficient%20end-to-end%20training%20and%0Ademonstrated%20the%20high%20potential%20of%20our%20approach%20for%20online%20visual%0Atracking-by-detection%20MOT%20on%20several%20popular%20benchmarks%2C%20i.e.%20MOT17%2C%20MOT20%20and%0AUA-DETRAC.%20Our%20proposed%20approach%20demonstrates%20strong%20performance%20in%20most%0Aevaluation%20metrics%20despite%20its%20simple%20nature%20as%20a%20tracker%20lacking%20significant%0Aauxiliary%20components%20such%20as%20occlusion%20handling%20or%20re-identification.%20The%0Aimplementation%20of%20our%20method%20is%20publicly%20available%20at%0Ahttps%3A//github.com/psaltaath/tadn-mot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.03571v3&entry.124074799=Read"},
{"title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind", "author": "Zheng Zhang and Nuoqian Xiao and Qi Chai and Deheng Ye and Hao Wang", "abstract": "  Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains.\n", "link": "http://arxiv.org/abs/2504.18039v2", "date": "2025-05-08", "relevancy": 2.1763, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiMind%3A%20Enhancing%20Werewolf%20Agents%20with%20Multimodal%20Reasoning%20and%0A%20%20Theory%20of%20Mind&body=Title%3A%20MultiMind%3A%20Enhancing%20Werewolf%20Agents%20with%20Multimodal%20Reasoning%20and%0A%20%20Theory%20of%20Mind%0AAuthor%3A%20Zheng%20Zhang%20and%20Nuoqian%20Xiao%20and%20Qi%20Chai%20and%20Deheng%20Ye%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20demonstrated%20impressive%20capabilities%0Ain%20social%20deduction%20games%20%28SDGs%29%20like%20Werewolf%2C%20where%20strategic%20reasoning%20and%0Asocial%20deception%20are%20essential.%20However%2C%20current%20approaches%20remain%20limited%20to%0Atextual%20information%2C%20ignoring%20crucial%20multimodal%20cues%20such%20as%20facial%0Aexpressions%20and%20tone%20of%20voice%20that%20humans%20naturally%20use%20to%20communicate.%0AMoreover%2C%20existing%20SDG%20agents%20primarily%20focus%20on%20inferring%20other%20players%27%0Aidentities%20without%20modeling%20how%20others%20perceive%20themselves%20or%20fellow%20players.%0ATo%20address%20these%20limitations%2C%20we%20use%20One%20Night%20Ultimate%20Werewolf%20%28ONUW%29%20as%20a%0Atestbed%20and%20present%20MultiMind%2C%20the%20first%20framework%20integrating%20multimodal%0Ainformation%20into%20SDG%20agents.%20MultiMind%20processes%20facial%20expressions%20and%20vocal%0Atones%20alongside%20verbal%20content%2C%20while%20employing%20a%20Theory%20of%20Mind%20%28ToM%29%20model%20to%0Arepresent%20each%20player%27s%20suspicion%20levels%20toward%20others.%20By%20combining%20this%20ToM%0Amodel%20with%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%2C%20our%20agent%20identifies%20communication%0Astrategies%20that%20minimize%20suspicion%20directed%20at%20itself.%20Through%20comprehensive%0Aevaluation%20in%20both%20agent-versus-agent%20simulations%20and%20studies%20with%20human%0Aplayers%2C%20we%20demonstrate%20MultiMind%27s%20superior%20performance%20in%20gameplay.%20Our%20work%0Apresents%20a%20significant%20advancement%20toward%20LLM%20agents%20capable%20of%20human-like%0Asocial%20reasoning%20across%20multimodal%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiMind%253A%2520Enhancing%2520Werewolf%2520Agents%2520with%2520Multimodal%2520Reasoning%2520and%250A%2520%2520Theory%2520of%2520Mind%26entry.906535625%3DZheng%2520Zhang%2520and%2520Nuoqian%2520Xiao%2520and%2520Qi%2520Chai%2520and%2520Deheng%2520Ye%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520have%2520demonstrated%2520impressive%2520capabilities%250Ain%2520social%2520deduction%2520games%2520%2528SDGs%2529%2520like%2520Werewolf%252C%2520where%2520strategic%2520reasoning%2520and%250Asocial%2520deception%2520are%2520essential.%2520However%252C%2520current%2520approaches%2520remain%2520limited%2520to%250Atextual%2520information%252C%2520ignoring%2520crucial%2520multimodal%2520cues%2520such%2520as%2520facial%250Aexpressions%2520and%2520tone%2520of%2520voice%2520that%2520humans%2520naturally%2520use%2520to%2520communicate.%250AMoreover%252C%2520existing%2520SDG%2520agents%2520primarily%2520focus%2520on%2520inferring%2520other%2520players%2527%250Aidentities%2520without%2520modeling%2520how%2520others%2520perceive%2520themselves%2520or%2520fellow%2520players.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520use%2520One%2520Night%2520Ultimate%2520Werewolf%2520%2528ONUW%2529%2520as%2520a%250Atestbed%2520and%2520present%2520MultiMind%252C%2520the%2520first%2520framework%2520integrating%2520multimodal%250Ainformation%2520into%2520SDG%2520agents.%2520MultiMind%2520processes%2520facial%2520expressions%2520and%2520vocal%250Atones%2520alongside%2520verbal%2520content%252C%2520while%2520employing%2520a%2520Theory%2520of%2520Mind%2520%2528ToM%2529%2520model%2520to%250Arepresent%2520each%2520player%2527s%2520suspicion%2520levels%2520toward%2520others.%2520By%2520combining%2520this%2520ToM%250Amodel%2520with%2520Monte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529%252C%2520our%2520agent%2520identifies%2520communication%250Astrategies%2520that%2520minimize%2520suspicion%2520directed%2520at%2520itself.%2520Through%2520comprehensive%250Aevaluation%2520in%2520both%2520agent-versus-agent%2520simulations%2520and%2520studies%2520with%2520human%250Aplayers%252C%2520we%2520demonstrate%2520MultiMind%2527s%2520superior%2520performance%2520in%2520gameplay.%2520Our%2520work%250Apresents%2520a%2520significant%2520advancement%2520toward%2520LLM%2520agents%2520capable%2520of%2520human-like%250Asocial%2520reasoning%2520across%2520multimodal%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiMind%3A%20Enhancing%20Werewolf%20Agents%20with%20Multimodal%20Reasoning%20and%0A%20%20Theory%20of%20Mind&entry.906535625=Zheng%20Zhang%20and%20Nuoqian%20Xiao%20and%20Qi%20Chai%20and%20Deheng%20Ye%20and%20Hao%20Wang&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20demonstrated%20impressive%20capabilities%0Ain%20social%20deduction%20games%20%28SDGs%29%20like%20Werewolf%2C%20where%20strategic%20reasoning%20and%0Asocial%20deception%20are%20essential.%20However%2C%20current%20approaches%20remain%20limited%20to%0Atextual%20information%2C%20ignoring%20crucial%20multimodal%20cues%20such%20as%20facial%0Aexpressions%20and%20tone%20of%20voice%20that%20humans%20naturally%20use%20to%20communicate.%0AMoreover%2C%20existing%20SDG%20agents%20primarily%20focus%20on%20inferring%20other%20players%27%0Aidentities%20without%20modeling%20how%20others%20perceive%20themselves%20or%20fellow%20players.%0ATo%20address%20these%20limitations%2C%20we%20use%20One%20Night%20Ultimate%20Werewolf%20%28ONUW%29%20as%20a%0Atestbed%20and%20present%20MultiMind%2C%20the%20first%20framework%20integrating%20multimodal%0Ainformation%20into%20SDG%20agents.%20MultiMind%20processes%20facial%20expressions%20and%20vocal%0Atones%20alongside%20verbal%20content%2C%20while%20employing%20a%20Theory%20of%20Mind%20%28ToM%29%20model%20to%0Arepresent%20each%20player%27s%20suspicion%20levels%20toward%20others.%20By%20combining%20this%20ToM%0Amodel%20with%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%2C%20our%20agent%20identifies%20communication%0Astrategies%20that%20minimize%20suspicion%20directed%20at%20itself.%20Through%20comprehensive%0Aevaluation%20in%20both%20agent-versus-agent%20simulations%20and%20studies%20with%20human%0Aplayers%2C%20we%20demonstrate%20MultiMind%27s%20superior%20performance%20in%20gameplay.%20Our%20work%0Apresents%20a%20significant%20advancement%20toward%20LLM%20agents%20capable%20of%20human-like%0Asocial%20reasoning%20across%20multimodal%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18039v2&entry.124074799=Read"},
{"title": "Defining and Quantifying Creative Behavior in Popular Image Generators", "author": "Aditi Ramaswamy and Hana Chockler and Melane Navaratnarajah", "abstract": "  Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition.\n", "link": "http://arxiv.org/abs/2505.04497v2", "date": "2025-05-08", "relevancy": 2.1625, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5653}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5403}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defining%20and%20Quantifying%20Creative%20Behavior%20in%20Popular%20Image%20Generators&body=Title%3A%20Defining%20and%20Quantifying%20Creative%20Behavior%20in%20Popular%20Image%20Generators%0AAuthor%3A%20Aditi%20Ramaswamy%20and%20Hana%20Chockler%20and%20Melane%20Navaratnarajah%0AAbstract%3A%20%20%20Creativity%20of%20generative%20AI%20models%20has%20been%20a%20subject%20of%20scientific%20debate%20in%0Athe%20last%20years%2C%20without%20a%20conclusive%20answer.%20In%20this%20paper%2C%20we%20study%20creativity%0Afrom%20a%20practical%20perspective%20and%20introduce%20quantitative%20measures%20that%20help%20the%0Auser%20to%20choose%20a%20suitable%20AI%20model%20for%20a%20given%20task.%20We%20evaluated%20our%20measures%0Aon%20a%20number%20of%20popular%20image-to-image%20generation%20models%2C%20and%20the%20results%20of%0Athis%20suggest%20that%20our%20measures%20conform%20to%20human%20intuition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04497v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefining%2520and%2520Quantifying%2520Creative%2520Behavior%2520in%2520Popular%2520Image%2520Generators%26entry.906535625%3DAditi%2520Ramaswamy%2520and%2520Hana%2520Chockler%2520and%2520Melane%2520Navaratnarajah%26entry.1292438233%3D%2520%2520Creativity%2520of%2520generative%2520AI%2520models%2520has%2520been%2520a%2520subject%2520of%2520scientific%2520debate%2520in%250Athe%2520last%2520years%252C%2520without%2520a%2520conclusive%2520answer.%2520In%2520this%2520paper%252C%2520we%2520study%2520creativity%250Afrom%2520a%2520practical%2520perspective%2520and%2520introduce%2520quantitative%2520measures%2520that%2520help%2520the%250Auser%2520to%2520choose%2520a%2520suitable%2520AI%2520model%2520for%2520a%2520given%2520task.%2520We%2520evaluated%2520our%2520measures%250Aon%2520a%2520number%2520of%2520popular%2520image-to-image%2520generation%2520models%252C%2520and%2520the%2520results%2520of%250Athis%2520suggest%2520that%2520our%2520measures%2520conform%2520to%2520human%2520intuition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04497v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defining%20and%20Quantifying%20Creative%20Behavior%20in%20Popular%20Image%20Generators&entry.906535625=Aditi%20Ramaswamy%20and%20Hana%20Chockler%20and%20Melane%20Navaratnarajah&entry.1292438233=%20%20Creativity%20of%20generative%20AI%20models%20has%20been%20a%20subject%20of%20scientific%20debate%20in%0Athe%20last%20years%2C%20without%20a%20conclusive%20answer.%20In%20this%20paper%2C%20we%20study%20creativity%0Afrom%20a%20practical%20perspective%20and%20introduce%20quantitative%20measures%20that%20help%20the%0Auser%20to%20choose%20a%20suitable%20AI%20model%20for%20a%20given%20task.%20We%20evaluated%20our%20measures%0Aon%20a%20number%20of%20popular%20image-to-image%20generation%20models%2C%20and%20the%20results%20of%0Athis%20suggest%20that%20our%20measures%20conform%20to%20human%20intuition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04497v2&entry.124074799=Read"},
{"title": "USTEP: Spatio-Temporal Predictive Learning under A Unified View", "author": "Cheng Tan and Jue Wang and Zhangyang Gao and Siyuan Li and Stan Z. Li", "abstract": "  Spatio-temporal predictive learning plays a crucial role in self-supervised\nlearning, with wide-ranging applications across a diverse range of fields.\nPrevious approaches for temporal modeling fall into two categories:\nrecurrent-based and recurrent-free methods. The former, while meticulously\nprocessing frames one by one, neglect short-term spatio-temporal information\nredundancies, leading to inefficiencies. The latter naively stack frames\nsequentially, overlooking the inherent temporal dependencies. In this paper, we\nre-examine the two dominant temporal modeling approaches within the realm of\nspatio-temporal predictive learning, offering a unified perspective. Building\nupon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive\nlearning), an innovative framework that reconciles the recurrent-based and\nrecurrent-free methods by integrating both micro-temporal and macro-temporal\nscales. Extensive experiments on a wide range of spatio-temporal predictive\nlearning demonstrate that USTEP achieves significant improvements over existing\ntemporal modeling approaches, thereby establishing it as a robust solution for\na wide range of spatio-temporal applications.\n", "link": "http://arxiv.org/abs/2310.05829v2", "date": "2025-05-08", "relevancy": 2.1455, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5697}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5159}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20USTEP%3A%20Spatio-Temporal%20Predictive%20Learning%20under%20A%20Unified%20View&body=Title%3A%20USTEP%3A%20Spatio-Temporal%20Predictive%20Learning%20under%20A%20Unified%20View%0AAuthor%3A%20Cheng%20Tan%20and%20Jue%20Wang%20and%20Zhangyang%20Gao%20and%20Siyuan%20Li%20and%20Stan%20Z.%20Li%0AAbstract%3A%20%20%20Spatio-temporal%20predictive%20learning%20plays%20a%20crucial%20role%20in%20self-supervised%0Alearning%2C%20with%20wide-ranging%20applications%20across%20a%20diverse%20range%20of%20fields.%0APrevious%20approaches%20for%20temporal%20modeling%20fall%20into%20two%20categories%3A%0Arecurrent-based%20and%20recurrent-free%20methods.%20The%20former%2C%20while%20meticulously%0Aprocessing%20frames%20one%20by%20one%2C%20neglect%20short-term%20spatio-temporal%20information%0Aredundancies%2C%20leading%20to%20inefficiencies.%20The%20latter%20naively%20stack%20frames%0Asequentially%2C%20overlooking%20the%20inherent%20temporal%20dependencies.%20In%20this%20paper%2C%20we%0Are-examine%20the%20two%20dominant%20temporal%20modeling%20approaches%20within%20the%20realm%20of%0Aspatio-temporal%20predictive%20learning%2C%20offering%20a%20unified%20perspective.%20Building%0Aupon%20this%20analysis%2C%20we%20introduce%20USTEP%20%28Unified%20Spatio-TEmporal%20Predictive%0Alearning%29%2C%20an%20innovative%20framework%20that%20reconciles%20the%20recurrent-based%20and%0Arecurrent-free%20methods%20by%20integrating%20both%20micro-temporal%20and%20macro-temporal%0Ascales.%20Extensive%20experiments%20on%20a%20wide%20range%20of%20spatio-temporal%20predictive%0Alearning%20demonstrate%20that%20USTEP%20achieves%20significant%20improvements%20over%20existing%0Atemporal%20modeling%20approaches%2C%20thereby%20establishing%20it%20as%20a%20robust%20solution%20for%0Aa%20wide%20range%20of%20spatio-temporal%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05829v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUSTEP%253A%2520Spatio-Temporal%2520Predictive%2520Learning%2520under%2520A%2520Unified%2520View%26entry.906535625%3DCheng%2520Tan%2520and%2520Jue%2520Wang%2520and%2520Zhangyang%2520Gao%2520and%2520Siyuan%2520Li%2520and%2520Stan%2520Z.%2520Li%26entry.1292438233%3D%2520%2520Spatio-temporal%2520predictive%2520learning%2520plays%2520a%2520crucial%2520role%2520in%2520self-supervised%250Alearning%252C%2520with%2520wide-ranging%2520applications%2520across%2520a%2520diverse%2520range%2520of%2520fields.%250APrevious%2520approaches%2520for%2520temporal%2520modeling%2520fall%2520into%2520two%2520categories%253A%250Arecurrent-based%2520and%2520recurrent-free%2520methods.%2520The%2520former%252C%2520while%2520meticulously%250Aprocessing%2520frames%2520one%2520by%2520one%252C%2520neglect%2520short-term%2520spatio-temporal%2520information%250Aredundancies%252C%2520leading%2520to%2520inefficiencies.%2520The%2520latter%2520naively%2520stack%2520frames%250Asequentially%252C%2520overlooking%2520the%2520inherent%2520temporal%2520dependencies.%2520In%2520this%2520paper%252C%2520we%250Are-examine%2520the%2520two%2520dominant%2520temporal%2520modeling%2520approaches%2520within%2520the%2520realm%2520of%250Aspatio-temporal%2520predictive%2520learning%252C%2520offering%2520a%2520unified%2520perspective.%2520Building%250Aupon%2520this%2520analysis%252C%2520we%2520introduce%2520USTEP%2520%2528Unified%2520Spatio-TEmporal%2520Predictive%250Alearning%2529%252C%2520an%2520innovative%2520framework%2520that%2520reconciles%2520the%2520recurrent-based%2520and%250Arecurrent-free%2520methods%2520by%2520integrating%2520both%2520micro-temporal%2520and%2520macro-temporal%250Ascales.%2520Extensive%2520experiments%2520on%2520a%2520wide%2520range%2520of%2520spatio-temporal%2520predictive%250Alearning%2520demonstrate%2520that%2520USTEP%2520achieves%2520significant%2520improvements%2520over%2520existing%250Atemporal%2520modeling%2520approaches%252C%2520thereby%2520establishing%2520it%2520as%2520a%2520robust%2520solution%2520for%250Aa%2520wide%2520range%2520of%2520spatio-temporal%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05829v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USTEP%3A%20Spatio-Temporal%20Predictive%20Learning%20under%20A%20Unified%20View&entry.906535625=Cheng%20Tan%20and%20Jue%20Wang%20and%20Zhangyang%20Gao%20and%20Siyuan%20Li%20and%20Stan%20Z.%20Li&entry.1292438233=%20%20Spatio-temporal%20predictive%20learning%20plays%20a%20crucial%20role%20in%20self-supervised%0Alearning%2C%20with%20wide-ranging%20applications%20across%20a%20diverse%20range%20of%20fields.%0APrevious%20approaches%20for%20temporal%20modeling%20fall%20into%20two%20categories%3A%0Arecurrent-based%20and%20recurrent-free%20methods.%20The%20former%2C%20while%20meticulously%0Aprocessing%20frames%20one%20by%20one%2C%20neglect%20short-term%20spatio-temporal%20information%0Aredundancies%2C%20leading%20to%20inefficiencies.%20The%20latter%20naively%20stack%20frames%0Asequentially%2C%20overlooking%20the%20inherent%20temporal%20dependencies.%20In%20this%20paper%2C%20we%0Are-examine%20the%20two%20dominant%20temporal%20modeling%20approaches%20within%20the%20realm%20of%0Aspatio-temporal%20predictive%20learning%2C%20offering%20a%20unified%20perspective.%20Building%0Aupon%20this%20analysis%2C%20we%20introduce%20USTEP%20%28Unified%20Spatio-TEmporal%20Predictive%0Alearning%29%2C%20an%20innovative%20framework%20that%20reconciles%20the%20recurrent-based%20and%0Arecurrent-free%20methods%20by%20integrating%20both%20micro-temporal%20and%20macro-temporal%0Ascales.%20Extensive%20experiments%20on%20a%20wide%20range%20of%20spatio-temporal%20predictive%0Alearning%20demonstrate%20that%20USTEP%20achieves%20significant%20improvements%20over%20existing%0Atemporal%20modeling%20approaches%2C%20thereby%20establishing%20it%20as%20a%20robust%20solution%20for%0Aa%20wide%20range%20of%20spatio-temporal%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05829v2&entry.124074799=Read"},
{"title": "PointBA: Towards Backdoor Attacks in 3D Point Cloud", "author": "Xinke Li and Zhirui Chen and Yue Zhao and Zekun Tong and Yabang Zhao and Andrew Lim and Joey Tianyi Zhou", "abstract": "  3D deep learning has been increasingly more popular for a variety of tasks\nincluding many safety-critical applications. However, recently several works\nraise the security issues of 3D deep models. Although most of them consider\nadversarial attacks, we identify that backdoor attack is indeed a more serious\nthreat to 3D deep learning systems but remains unexplored. We present the\nbackdoor attacks in 3D point cloud with a unified framework that exploits the\nunique properties of 3D data and networks. In particular, we design two attack\napproaches on point cloud: the poison-label backdoor attack (PointPBA) and the\nclean-label backdoor attack (PointCBA). The first one is straightforward and\neffective in practice, while the latter is more sophisticated assuming there\nare certain data inspections. The attack algorithms are mainly motivated and\ndeveloped by 1) the recent discovery of 3D adversarial samples suggesting the\nvulnerability of deep models under spatial transformation; 2) the proposed\nfeature disentanglement technique that manipulates the feature of the data\nthrough optimization methods and its potential to embed a new task. Extensive\nexperiments show the efficacy of the PointPBA with over 95% success rate across\nvarious 3D datasets and models, and the more stealthy PointCBA with around 50%\nsuccess rate. Our proposed backdoor attack in 3D point cloud is expected to\nperform as a baseline for improving the robustness of 3D deep models.\n", "link": "http://arxiv.org/abs/2103.16074v4", "date": "2025-05-08", "relevancy": 2.1345, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.55}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5313}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointBA%3A%20Towards%20Backdoor%20Attacks%20in%203D%20Point%20Cloud&body=Title%3A%20PointBA%3A%20Towards%20Backdoor%20Attacks%20in%203D%20Point%20Cloud%0AAuthor%3A%20Xinke%20Li%20and%20Zhirui%20Chen%20and%20Yue%20Zhao%20and%20Zekun%20Tong%20and%20Yabang%20Zhao%20and%20Andrew%20Lim%20and%20Joey%20Tianyi%20Zhou%0AAbstract%3A%20%20%203D%20deep%20learning%20has%20been%20increasingly%20more%20popular%20for%20a%20variety%20of%20tasks%0Aincluding%20many%20safety-critical%20applications.%20However%2C%20recently%20several%20works%0Araise%20the%20security%20issues%20of%203D%20deep%20models.%20Although%20most%20of%20them%20consider%0Aadversarial%20attacks%2C%20we%20identify%20that%20backdoor%20attack%20is%20indeed%20a%20more%20serious%0Athreat%20to%203D%20deep%20learning%20systems%20but%20remains%20unexplored.%20We%20present%20the%0Abackdoor%20attacks%20in%203D%20point%20cloud%20with%20a%20unified%20framework%20that%20exploits%20the%0Aunique%20properties%20of%203D%20data%20and%20networks.%20In%20particular%2C%20we%20design%20two%20attack%0Aapproaches%20on%20point%20cloud%3A%20the%20poison-label%20backdoor%20attack%20%28PointPBA%29%20and%20the%0Aclean-label%20backdoor%20attack%20%28PointCBA%29.%20The%20first%20one%20is%20straightforward%20and%0Aeffective%20in%20practice%2C%20while%20the%20latter%20is%20more%20sophisticated%20assuming%20there%0Aare%20certain%20data%20inspections.%20The%20attack%20algorithms%20are%20mainly%20motivated%20and%0Adeveloped%20by%201%29%20the%20recent%20discovery%20of%203D%20adversarial%20samples%20suggesting%20the%0Avulnerability%20of%20deep%20models%20under%20spatial%20transformation%3B%202%29%20the%20proposed%0Afeature%20disentanglement%20technique%20that%20manipulates%20the%20feature%20of%20the%20data%0Athrough%20optimization%20methods%20and%20its%20potential%20to%20embed%20a%20new%20task.%20Extensive%0Aexperiments%20show%20the%20efficacy%20of%20the%20PointPBA%20with%20over%2095%25%20success%20rate%20across%0Avarious%203D%20datasets%20and%20models%2C%20and%20the%20more%20stealthy%20PointCBA%20with%20around%2050%25%0Asuccess%20rate.%20Our%20proposed%20backdoor%20attack%20in%203D%20point%20cloud%20is%20expected%20to%0Aperform%20as%20a%20baseline%20for%20improving%20the%20robustness%20of%203D%20deep%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2103.16074v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointBA%253A%2520Towards%2520Backdoor%2520Attacks%2520in%25203D%2520Point%2520Cloud%26entry.906535625%3DXinke%2520Li%2520and%2520Zhirui%2520Chen%2520and%2520Yue%2520Zhao%2520and%2520Zekun%2520Tong%2520and%2520Yabang%2520Zhao%2520and%2520Andrew%2520Lim%2520and%2520Joey%2520Tianyi%2520Zhou%26entry.1292438233%3D%2520%25203D%2520deep%2520learning%2520has%2520been%2520increasingly%2520more%2520popular%2520for%2520a%2520variety%2520of%2520tasks%250Aincluding%2520many%2520safety-critical%2520applications.%2520However%252C%2520recently%2520several%2520works%250Araise%2520the%2520security%2520issues%2520of%25203D%2520deep%2520models.%2520Although%2520most%2520of%2520them%2520consider%250Aadversarial%2520attacks%252C%2520we%2520identify%2520that%2520backdoor%2520attack%2520is%2520indeed%2520a%2520more%2520serious%250Athreat%2520to%25203D%2520deep%2520learning%2520systems%2520but%2520remains%2520unexplored.%2520We%2520present%2520the%250Abackdoor%2520attacks%2520in%25203D%2520point%2520cloud%2520with%2520a%2520unified%2520framework%2520that%2520exploits%2520the%250Aunique%2520properties%2520of%25203D%2520data%2520and%2520networks.%2520In%2520particular%252C%2520we%2520design%2520two%2520attack%250Aapproaches%2520on%2520point%2520cloud%253A%2520the%2520poison-label%2520backdoor%2520attack%2520%2528PointPBA%2529%2520and%2520the%250Aclean-label%2520backdoor%2520attack%2520%2528PointCBA%2529.%2520The%2520first%2520one%2520is%2520straightforward%2520and%250Aeffective%2520in%2520practice%252C%2520while%2520the%2520latter%2520is%2520more%2520sophisticated%2520assuming%2520there%250Aare%2520certain%2520data%2520inspections.%2520The%2520attack%2520algorithms%2520are%2520mainly%2520motivated%2520and%250Adeveloped%2520by%25201%2529%2520the%2520recent%2520discovery%2520of%25203D%2520adversarial%2520samples%2520suggesting%2520the%250Avulnerability%2520of%2520deep%2520models%2520under%2520spatial%2520transformation%253B%25202%2529%2520the%2520proposed%250Afeature%2520disentanglement%2520technique%2520that%2520manipulates%2520the%2520feature%2520of%2520the%2520data%250Athrough%2520optimization%2520methods%2520and%2520its%2520potential%2520to%2520embed%2520a%2520new%2520task.%2520Extensive%250Aexperiments%2520show%2520the%2520efficacy%2520of%2520the%2520PointPBA%2520with%2520over%252095%2525%2520success%2520rate%2520across%250Avarious%25203D%2520datasets%2520and%2520models%252C%2520and%2520the%2520more%2520stealthy%2520PointCBA%2520with%2520around%252050%2525%250Asuccess%2520rate.%2520Our%2520proposed%2520backdoor%2520attack%2520in%25203D%2520point%2520cloud%2520is%2520expected%2520to%250Aperform%2520as%2520a%2520baseline%2520for%2520improving%2520the%2520robustness%2520of%25203D%2520deep%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2103.16074v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointBA%3A%20Towards%20Backdoor%20Attacks%20in%203D%20Point%20Cloud&entry.906535625=Xinke%20Li%20and%20Zhirui%20Chen%20and%20Yue%20Zhao%20and%20Zekun%20Tong%20and%20Yabang%20Zhao%20and%20Andrew%20Lim%20and%20Joey%20Tianyi%20Zhou&entry.1292438233=%20%203D%20deep%20learning%20has%20been%20increasingly%20more%20popular%20for%20a%20variety%20of%20tasks%0Aincluding%20many%20safety-critical%20applications.%20However%2C%20recently%20several%20works%0Araise%20the%20security%20issues%20of%203D%20deep%20models.%20Although%20most%20of%20them%20consider%0Aadversarial%20attacks%2C%20we%20identify%20that%20backdoor%20attack%20is%20indeed%20a%20more%20serious%0Athreat%20to%203D%20deep%20learning%20systems%20but%20remains%20unexplored.%20We%20present%20the%0Abackdoor%20attacks%20in%203D%20point%20cloud%20with%20a%20unified%20framework%20that%20exploits%20the%0Aunique%20properties%20of%203D%20data%20and%20networks.%20In%20particular%2C%20we%20design%20two%20attack%0Aapproaches%20on%20point%20cloud%3A%20the%20poison-label%20backdoor%20attack%20%28PointPBA%29%20and%20the%0Aclean-label%20backdoor%20attack%20%28PointCBA%29.%20The%20first%20one%20is%20straightforward%20and%0Aeffective%20in%20practice%2C%20while%20the%20latter%20is%20more%20sophisticated%20assuming%20there%0Aare%20certain%20data%20inspections.%20The%20attack%20algorithms%20are%20mainly%20motivated%20and%0Adeveloped%20by%201%29%20the%20recent%20discovery%20of%203D%20adversarial%20samples%20suggesting%20the%0Avulnerability%20of%20deep%20models%20under%20spatial%20transformation%3B%202%29%20the%20proposed%0Afeature%20disentanglement%20technique%20that%20manipulates%20the%20feature%20of%20the%20data%0Athrough%20optimization%20methods%20and%20its%20potential%20to%20embed%20a%20new%20task.%20Extensive%0Aexperiments%20show%20the%20efficacy%20of%20the%20PointPBA%20with%20over%2095%25%20success%20rate%20across%0Avarious%203D%20datasets%20and%20models%2C%20and%20the%20more%20stealthy%20PointCBA%20with%20around%2050%25%0Asuccess%20rate.%20Our%20proposed%20backdoor%20attack%20in%203D%20point%20cloud%20is%20expected%20to%0Aperform%20as%20a%20baseline%20for%20improving%20the%20robustness%20of%203D%20deep%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2103.16074v4&entry.124074799=Read"},
{"title": "Deep Optimal Transport for Domain Adaptation on SPD Manifolds", "author": "Ce Ju and Cuntai Guan", "abstract": "  Recent progress in geometric deep learning has drawn increasing attention\nfrom the machine learning community toward domain adaptation on symmetric\npositive definite (SPD) manifolds, especially for neuroimaging data that often\nsuffer from distribution shifts across sessions. These data, typically\nrepresented as covariance matrices of brain signals, inherently lie on SPD\nmanifolds due to their symmetry and positive definiteness. However,\nconventional domain adaptation methods often overlook this geometric structure\nwhen applied directly to covariance matrices, which can result in suboptimal\nperformance. To address this issue, we introduce a new geometric deep learning\nframework that combines optimal transport theory with the geometry of SPD\nmanifolds. Our approach aligns data distributions while respecting the manifold\nstructure, effectively reducing both marginal and conditional discrepancies. We\nvalidate our method on three cross-session brain computer interface datasets,\nKU, BNCI2014001, and BNCI2015001, where it consistently outperforms baseline\napproaches while maintaining the intrinsic geometry of the data. We also\nprovide quantitative results and visualizations to better illustrate the\nbehavior of the learned embeddings.\n", "link": "http://arxiv.org/abs/2201.05745v6", "date": "2025-05-08", "relevancy": 2.1219, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5357}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5309}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Optimal%20Transport%20for%20Domain%20Adaptation%20on%20SPD%20Manifolds&body=Title%3A%20Deep%20Optimal%20Transport%20for%20Domain%20Adaptation%20on%20SPD%20Manifolds%0AAuthor%3A%20Ce%20Ju%20and%20Cuntai%20Guan%0AAbstract%3A%20%20%20Recent%20progress%20in%20geometric%20deep%20learning%20has%20drawn%20increasing%20attention%0Afrom%20the%20machine%20learning%20community%20toward%20domain%20adaptation%20on%20symmetric%0Apositive%20definite%20%28SPD%29%20manifolds%2C%20especially%20for%20neuroimaging%20data%20that%20often%0Asuffer%20from%20distribution%20shifts%20across%20sessions.%20These%20data%2C%20typically%0Arepresented%20as%20covariance%20matrices%20of%20brain%20signals%2C%20inherently%20lie%20on%20SPD%0Amanifolds%20due%20to%20their%20symmetry%20and%20positive%20definiteness.%20However%2C%0Aconventional%20domain%20adaptation%20methods%20often%20overlook%20this%20geometric%20structure%0Awhen%20applied%20directly%20to%20covariance%20matrices%2C%20which%20can%20result%20in%20suboptimal%0Aperformance.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20new%20geometric%20deep%20learning%0Aframework%20that%20combines%20optimal%20transport%20theory%20with%20the%20geometry%20of%20SPD%0Amanifolds.%20Our%20approach%20aligns%20data%20distributions%20while%20respecting%20the%20manifold%0Astructure%2C%20effectively%20reducing%20both%20marginal%20and%20conditional%20discrepancies.%20We%0Avalidate%20our%20method%20on%20three%20cross-session%20brain%20computer%20interface%20datasets%2C%0AKU%2C%20BNCI2014001%2C%20and%20BNCI2015001%2C%20where%20it%20consistently%20outperforms%20baseline%0Aapproaches%20while%20maintaining%20the%20intrinsic%20geometry%20of%20the%20data.%20We%20also%0Aprovide%20quantitative%20results%20and%20visualizations%20to%20better%20illustrate%20the%0Abehavior%20of%20the%20learned%20embeddings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.05745v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Optimal%2520Transport%2520for%2520Domain%2520Adaptation%2520on%2520SPD%2520Manifolds%26entry.906535625%3DCe%2520Ju%2520and%2520Cuntai%2520Guan%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520geometric%2520deep%2520learning%2520has%2520drawn%2520increasing%2520attention%250Afrom%2520the%2520machine%2520learning%2520community%2520toward%2520domain%2520adaptation%2520on%2520symmetric%250Apositive%2520definite%2520%2528SPD%2529%2520manifolds%252C%2520especially%2520for%2520neuroimaging%2520data%2520that%2520often%250Asuffer%2520from%2520distribution%2520shifts%2520across%2520sessions.%2520These%2520data%252C%2520typically%250Arepresented%2520as%2520covariance%2520matrices%2520of%2520brain%2520signals%252C%2520inherently%2520lie%2520on%2520SPD%250Amanifolds%2520due%2520to%2520their%2520symmetry%2520and%2520positive%2520definiteness.%2520However%252C%250Aconventional%2520domain%2520adaptation%2520methods%2520often%2520overlook%2520this%2520geometric%2520structure%250Awhen%2520applied%2520directly%2520to%2520covariance%2520matrices%252C%2520which%2520can%2520result%2520in%2520suboptimal%250Aperformance.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520new%2520geometric%2520deep%2520learning%250Aframework%2520that%2520combines%2520optimal%2520transport%2520theory%2520with%2520the%2520geometry%2520of%2520SPD%250Amanifolds.%2520Our%2520approach%2520aligns%2520data%2520distributions%2520while%2520respecting%2520the%2520manifold%250Astructure%252C%2520effectively%2520reducing%2520both%2520marginal%2520and%2520conditional%2520discrepancies.%2520We%250Avalidate%2520our%2520method%2520on%2520three%2520cross-session%2520brain%2520computer%2520interface%2520datasets%252C%250AKU%252C%2520BNCI2014001%252C%2520and%2520BNCI2015001%252C%2520where%2520it%2520consistently%2520outperforms%2520baseline%250Aapproaches%2520while%2520maintaining%2520the%2520intrinsic%2520geometry%2520of%2520the%2520data.%2520We%2520also%250Aprovide%2520quantitative%2520results%2520and%2520visualizations%2520to%2520better%2520illustrate%2520the%250Abehavior%2520of%2520the%2520learned%2520embeddings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.05745v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Optimal%20Transport%20for%20Domain%20Adaptation%20on%20SPD%20Manifolds&entry.906535625=Ce%20Ju%20and%20Cuntai%20Guan&entry.1292438233=%20%20Recent%20progress%20in%20geometric%20deep%20learning%20has%20drawn%20increasing%20attention%0Afrom%20the%20machine%20learning%20community%20toward%20domain%20adaptation%20on%20symmetric%0Apositive%20definite%20%28SPD%29%20manifolds%2C%20especially%20for%20neuroimaging%20data%20that%20often%0Asuffer%20from%20distribution%20shifts%20across%20sessions.%20These%20data%2C%20typically%0Arepresented%20as%20covariance%20matrices%20of%20brain%20signals%2C%20inherently%20lie%20on%20SPD%0Amanifolds%20due%20to%20their%20symmetry%20and%20positive%20definiteness.%20However%2C%0Aconventional%20domain%20adaptation%20methods%20often%20overlook%20this%20geometric%20structure%0Awhen%20applied%20directly%20to%20covariance%20matrices%2C%20which%20can%20result%20in%20suboptimal%0Aperformance.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20new%20geometric%20deep%20learning%0Aframework%20that%20combines%20optimal%20transport%20theory%20with%20the%20geometry%20of%20SPD%0Amanifolds.%20Our%20approach%20aligns%20data%20distributions%20while%20respecting%20the%20manifold%0Astructure%2C%20effectively%20reducing%20both%20marginal%20and%20conditional%20discrepancies.%20We%0Avalidate%20our%20method%20on%20three%20cross-session%20brain%20computer%20interface%20datasets%2C%0AKU%2C%20BNCI2014001%2C%20and%20BNCI2015001%2C%20where%20it%20consistently%20outperforms%20baseline%0Aapproaches%20while%20maintaining%20the%20intrinsic%20geometry%20of%20the%20data.%20We%20also%0Aprovide%20quantitative%20results%20and%20visualizations%20to%20better%20illustrate%20the%0Abehavior%20of%20the%20learned%20embeddings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.05745v6&entry.124074799=Read"},
{"title": "Augmented Deep Contexts for Spatially Embedded Video Coding", "author": "Yifan Bian and Chuanbo Tang and Li Li and Dong Liu", "abstract": "  Most Neural Video Codecs (NVCs) only employ temporal references to generate\ntemporal-only contexts and latent prior. These temporal-only NVCs fail to\nhandle large motions or emerging objects due to limited contexts and misaligned\nlatent prior. To relieve the limitations, we propose a Spatially Embedded Video\nCodec (SEVC), in which the low-resolution video is compressed for spatial\nreferences. Firstly, our SEVC leverages both spatial and temporal references to\ngenerate augmented motion vectors and hybrid spatial-temporal contexts.\nSecondly, to address the misalignment issue in latent prior and enrich the\nprior information, we introduce a spatial-guided latent prior augmented by\nmultiple temporal latent representations. At last, we design a joint\nspatial-temporal optimization to learn quality-adaptive bit allocation for\nspatial references, further boosting rate-distortion performance. Experimental\nresults show that our SEVC effectively alleviates the limitations in handling\nlarge motions or emerging objects, and also reduces 11.9% more bitrate than the\nprevious state-of-the-art NVC while providing an additional low-resolution\nbitstream. Our code and model are available at https://github.com/EsakaK/SEVC.\n", "link": "http://arxiv.org/abs/2505.05309v1", "date": "2025-05-08", "relevancy": 2.1189, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmented%20Deep%20Contexts%20for%20Spatially%20Embedded%20Video%20Coding&body=Title%3A%20Augmented%20Deep%20Contexts%20for%20Spatially%20Embedded%20Video%20Coding%0AAuthor%3A%20Yifan%20Bian%20and%20Chuanbo%20Tang%20and%20Li%20Li%20and%20Dong%20Liu%0AAbstract%3A%20%20%20Most%20Neural%20Video%20Codecs%20%28NVCs%29%20only%20employ%20temporal%20references%20to%20generate%0Atemporal-only%20contexts%20and%20latent%20prior.%20These%20temporal-only%20NVCs%20fail%20to%0Ahandle%20large%20motions%20or%20emerging%20objects%20due%20to%20limited%20contexts%20and%20misaligned%0Alatent%20prior.%20To%20relieve%20the%20limitations%2C%20we%20propose%20a%20Spatially%20Embedded%20Video%0ACodec%20%28SEVC%29%2C%20in%20which%20the%20low-resolution%20video%20is%20compressed%20for%20spatial%0Areferences.%20Firstly%2C%20our%20SEVC%20leverages%20both%20spatial%20and%20temporal%20references%20to%0Agenerate%20augmented%20motion%20vectors%20and%20hybrid%20spatial-temporal%20contexts.%0ASecondly%2C%20to%20address%20the%20misalignment%20issue%20in%20latent%20prior%20and%20enrich%20the%0Aprior%20information%2C%20we%20introduce%20a%20spatial-guided%20latent%20prior%20augmented%20by%0Amultiple%20temporal%20latent%20representations.%20At%20last%2C%20we%20design%20a%20joint%0Aspatial-temporal%20optimization%20to%20learn%20quality-adaptive%20bit%20allocation%20for%0Aspatial%20references%2C%20further%20boosting%20rate-distortion%20performance.%20Experimental%0Aresults%20show%20that%20our%20SEVC%20effectively%20alleviates%20the%20limitations%20in%20handling%0Alarge%20motions%20or%20emerging%20objects%2C%20and%20also%20reduces%2011.9%25%20more%20bitrate%20than%20the%0Aprevious%20state-of-the-art%20NVC%20while%20providing%20an%20additional%20low-resolution%0Abitstream.%20Our%20code%20and%20model%20are%20available%20at%20https%3A//github.com/EsakaK/SEVC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmented%2520Deep%2520Contexts%2520for%2520Spatially%2520Embedded%2520Video%2520Coding%26entry.906535625%3DYifan%2520Bian%2520and%2520Chuanbo%2520Tang%2520and%2520Li%2520Li%2520and%2520Dong%2520Liu%26entry.1292438233%3D%2520%2520Most%2520Neural%2520Video%2520Codecs%2520%2528NVCs%2529%2520only%2520employ%2520temporal%2520references%2520to%2520generate%250Atemporal-only%2520contexts%2520and%2520latent%2520prior.%2520These%2520temporal-only%2520NVCs%2520fail%2520to%250Ahandle%2520large%2520motions%2520or%2520emerging%2520objects%2520due%2520to%2520limited%2520contexts%2520and%2520misaligned%250Alatent%2520prior.%2520To%2520relieve%2520the%2520limitations%252C%2520we%2520propose%2520a%2520Spatially%2520Embedded%2520Video%250ACodec%2520%2528SEVC%2529%252C%2520in%2520which%2520the%2520low-resolution%2520video%2520is%2520compressed%2520for%2520spatial%250Areferences.%2520Firstly%252C%2520our%2520SEVC%2520leverages%2520both%2520spatial%2520and%2520temporal%2520references%2520to%250Agenerate%2520augmented%2520motion%2520vectors%2520and%2520hybrid%2520spatial-temporal%2520contexts.%250ASecondly%252C%2520to%2520address%2520the%2520misalignment%2520issue%2520in%2520latent%2520prior%2520and%2520enrich%2520the%250Aprior%2520information%252C%2520we%2520introduce%2520a%2520spatial-guided%2520latent%2520prior%2520augmented%2520by%250Amultiple%2520temporal%2520latent%2520representations.%2520At%2520last%252C%2520we%2520design%2520a%2520joint%250Aspatial-temporal%2520optimization%2520to%2520learn%2520quality-adaptive%2520bit%2520allocation%2520for%250Aspatial%2520references%252C%2520further%2520boosting%2520rate-distortion%2520performance.%2520Experimental%250Aresults%2520show%2520that%2520our%2520SEVC%2520effectively%2520alleviates%2520the%2520limitations%2520in%2520handling%250Alarge%2520motions%2520or%2520emerging%2520objects%252C%2520and%2520also%2520reduces%252011.9%2525%2520more%2520bitrate%2520than%2520the%250Aprevious%2520state-of-the-art%2520NVC%2520while%2520providing%2520an%2520additional%2520low-resolution%250Abitstream.%2520Our%2520code%2520and%2520model%2520are%2520available%2520at%2520https%253A//github.com/EsakaK/SEVC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmented%20Deep%20Contexts%20for%20Spatially%20Embedded%20Video%20Coding&entry.906535625=Yifan%20Bian%20and%20Chuanbo%20Tang%20and%20Li%20Li%20and%20Dong%20Liu&entry.1292438233=%20%20Most%20Neural%20Video%20Codecs%20%28NVCs%29%20only%20employ%20temporal%20references%20to%20generate%0Atemporal-only%20contexts%20and%20latent%20prior.%20These%20temporal-only%20NVCs%20fail%20to%0Ahandle%20large%20motions%20or%20emerging%20objects%20due%20to%20limited%20contexts%20and%20misaligned%0Alatent%20prior.%20To%20relieve%20the%20limitations%2C%20we%20propose%20a%20Spatially%20Embedded%20Video%0ACodec%20%28SEVC%29%2C%20in%20which%20the%20low-resolution%20video%20is%20compressed%20for%20spatial%0Areferences.%20Firstly%2C%20our%20SEVC%20leverages%20both%20spatial%20and%20temporal%20references%20to%0Agenerate%20augmented%20motion%20vectors%20and%20hybrid%20spatial-temporal%20contexts.%0ASecondly%2C%20to%20address%20the%20misalignment%20issue%20in%20latent%20prior%20and%20enrich%20the%0Aprior%20information%2C%20we%20introduce%20a%20spatial-guided%20latent%20prior%20augmented%20by%0Amultiple%20temporal%20latent%20representations.%20At%20last%2C%20we%20design%20a%20joint%0Aspatial-temporal%20optimization%20to%20learn%20quality-adaptive%20bit%20allocation%20for%0Aspatial%20references%2C%20further%20boosting%20rate-distortion%20performance.%20Experimental%0Aresults%20show%20that%20our%20SEVC%20effectively%20alleviates%20the%20limitations%20in%20handling%0Alarge%20motions%20or%20emerging%20objects%2C%20and%20also%20reduces%2011.9%25%20more%20bitrate%20than%20the%0Aprevious%20state-of-the-art%20NVC%20while%20providing%20an%20additional%20low-resolution%0Abitstream.%20Our%20code%20and%20model%20are%20available%20at%20https%3A//github.com/EsakaK/SEVC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05309v1&entry.124074799=Read"},
{"title": "Localization and path following for an autonomous e-scooter", "author": "David Meister and Robin Str\u00e4sser and Felix Br\u00e4ndle and Marc Seidel and Benno Bassler and Nathan Gerber and Jan Kautz and Elena Rommel and Frank Allg\u00f6wer", "abstract": "  In order to mitigate economical, ecological, and societal challenges in\nelectric scooter (e-scooter) sharing systems, we develop an autonomous\ne-scooter prototype. Our vision is to design a fully autonomous prototype that\ncan find its way to the next parking spot, high-demand area, or charging\nstation. In this work, we propose a path following solution to enable\nlocalization and navigation in an urban environment with a provided path to\nfollow. We design a closed-loop architecture that solves the localization and\npath following problem while allowing the e-scooter to maintain its balance\nwith a previously developed reaction wheel mechanism. Our approach facilitates\nstate and input constraints, e.g., adhering to the path width, while remaining\nexecutable on a Raspberry Pi 5. We demonstrate the efficacy of our approach in\na real-world experiment on our prototype.\n", "link": "http://arxiv.org/abs/2505.05314v1", "date": "2025-05-08", "relevancy": 2.1186, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5854}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4976}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localization%20and%20path%20following%20for%20an%20autonomous%20e-scooter&body=Title%3A%20Localization%20and%20path%20following%20for%20an%20autonomous%20e-scooter%0AAuthor%3A%20David%20Meister%20and%20Robin%20Str%C3%A4sser%20and%20Felix%20Br%C3%A4ndle%20and%20Marc%20Seidel%20and%20Benno%20Bassler%20and%20Nathan%20Gerber%20and%20Jan%20Kautz%20and%20Elena%20Rommel%20and%20Frank%20Allg%C3%B6wer%0AAbstract%3A%20%20%20In%20order%20to%20mitigate%20economical%2C%20ecological%2C%20and%20societal%20challenges%20in%0Aelectric%20scooter%20%28e-scooter%29%20sharing%20systems%2C%20we%20develop%20an%20autonomous%0Ae-scooter%20prototype.%20Our%20vision%20is%20to%20design%20a%20fully%20autonomous%20prototype%20that%0Acan%20find%20its%20way%20to%20the%20next%20parking%20spot%2C%20high-demand%20area%2C%20or%20charging%0Astation.%20In%20this%20work%2C%20we%20propose%20a%20path%20following%20solution%20to%20enable%0Alocalization%20and%20navigation%20in%20an%20urban%20environment%20with%20a%20provided%20path%20to%0Afollow.%20We%20design%20a%20closed-loop%20architecture%20that%20solves%20the%20localization%20and%0Apath%20following%20problem%20while%20allowing%20the%20e-scooter%20to%20maintain%20its%20balance%0Awith%20a%20previously%20developed%20reaction%20wheel%20mechanism.%20Our%20approach%20facilitates%0Astate%20and%20input%20constraints%2C%20e.g.%2C%20adhering%20to%20the%20path%20width%2C%20while%20remaining%0Aexecutable%20on%20a%20Raspberry%20Pi%205.%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20in%0Aa%20real-world%20experiment%20on%20our%20prototype.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalization%2520and%2520path%2520following%2520for%2520an%2520autonomous%2520e-scooter%26entry.906535625%3DDavid%2520Meister%2520and%2520Robin%2520Str%25C3%25A4sser%2520and%2520Felix%2520Br%25C3%25A4ndle%2520and%2520Marc%2520Seidel%2520and%2520Benno%2520Bassler%2520and%2520Nathan%2520Gerber%2520and%2520Jan%2520Kautz%2520and%2520Elena%2520Rommel%2520and%2520Frank%2520Allg%25C3%25B6wer%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520mitigate%2520economical%252C%2520ecological%252C%2520and%2520societal%2520challenges%2520in%250Aelectric%2520scooter%2520%2528e-scooter%2529%2520sharing%2520systems%252C%2520we%2520develop%2520an%2520autonomous%250Ae-scooter%2520prototype.%2520Our%2520vision%2520is%2520to%2520design%2520a%2520fully%2520autonomous%2520prototype%2520that%250Acan%2520find%2520its%2520way%2520to%2520the%2520next%2520parking%2520spot%252C%2520high-demand%2520area%252C%2520or%2520charging%250Astation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520path%2520following%2520solution%2520to%2520enable%250Alocalization%2520and%2520navigation%2520in%2520an%2520urban%2520environment%2520with%2520a%2520provided%2520path%2520to%250Afollow.%2520We%2520design%2520a%2520closed-loop%2520architecture%2520that%2520solves%2520the%2520localization%2520and%250Apath%2520following%2520problem%2520while%2520allowing%2520the%2520e-scooter%2520to%2520maintain%2520its%2520balance%250Awith%2520a%2520previously%2520developed%2520reaction%2520wheel%2520mechanism.%2520Our%2520approach%2520facilitates%250Astate%2520and%2520input%2520constraints%252C%2520e.g.%252C%2520adhering%2520to%2520the%2520path%2520width%252C%2520while%2520remaining%250Aexecutable%2520on%2520a%2520Raspberry%2520Pi%25205.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520approach%2520in%250Aa%2520real-world%2520experiment%2520on%2520our%2520prototype.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localization%20and%20path%20following%20for%20an%20autonomous%20e-scooter&entry.906535625=David%20Meister%20and%20Robin%20Str%C3%A4sser%20and%20Felix%20Br%C3%A4ndle%20and%20Marc%20Seidel%20and%20Benno%20Bassler%20and%20Nathan%20Gerber%20and%20Jan%20Kautz%20and%20Elena%20Rommel%20and%20Frank%20Allg%C3%B6wer&entry.1292438233=%20%20In%20order%20to%20mitigate%20economical%2C%20ecological%2C%20and%20societal%20challenges%20in%0Aelectric%20scooter%20%28e-scooter%29%20sharing%20systems%2C%20we%20develop%20an%20autonomous%0Ae-scooter%20prototype.%20Our%20vision%20is%20to%20design%20a%20fully%20autonomous%20prototype%20that%0Acan%20find%20its%20way%20to%20the%20next%20parking%20spot%2C%20high-demand%20area%2C%20or%20charging%0Astation.%20In%20this%20work%2C%20we%20propose%20a%20path%20following%20solution%20to%20enable%0Alocalization%20and%20navigation%20in%20an%20urban%20environment%20with%20a%20provided%20path%20to%0Afollow.%20We%20design%20a%20closed-loop%20architecture%20that%20solves%20the%20localization%20and%0Apath%20following%20problem%20while%20allowing%20the%20e-scooter%20to%20maintain%20its%20balance%0Awith%20a%20previously%20developed%20reaction%20wheel%20mechanism.%20Our%20approach%20facilitates%0Astate%20and%20input%20constraints%2C%20e.g.%2C%20adhering%20to%20the%20path%20width%2C%20while%20remaining%0Aexecutable%20on%20a%20Raspberry%20Pi%205.%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20in%0Aa%20real-world%20experiment%20on%20our%20prototype.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05314v1&entry.124074799=Read"},
{"title": "MTL-UE: Learning to Learn Nothing for Multi-Task Learning", "author": "Yi Yu and Song Xia and Siyuan Yang and Chenqi Kong and Wenhan Yang and Shijian Lu and Yap-Peng Tan and Alex C. Kot", "abstract": "  Most existing unlearnable strategies focus on preventing unauthorized users\nfrom training single-task learning (STL) models with personal data.\nNevertheless, the paradigm has recently shifted towards multi-task data and\nmulti-task learning (MTL), targeting generalist and foundation models that can\nhandle multiple tasks simultaneously. Despite their growing importance, MTL\ndata and models have been largely neglected while pursuing unlearnable\nstrategies. This paper presents MTL-UE, the first unified framework for\ngenerating unlearnable examples for multi-task data and MTL models. Instead of\noptimizing perturbations for each sample, we design a generator-based structure\nthat introduces label priors and class-wise feature embeddings which leads to\nmuch better attacking performance. In addition, MTL-UE incorporates intra-task\nand inter-task embedding regularization to increase inter-class separation and\nsuppress intra-class variance which enhances the attack robustness greatly.\nFurthermore, MTL-UE is versatile with good supports for dense prediction tasks\nin MTL. It is also plug-and-play allowing integrating existing\nsurrogate-dependent unlearnable methods with little adaptation. Extensive\nexperiments show that MTL-UE achieves superior attacking performance\nconsistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5\nMTL task-weighting strategies.\n", "link": "http://arxiv.org/abs/2505.05279v1", "date": "2025-05-08", "relevancy": 2.118, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5668}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5333}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTL-UE%3A%20Learning%20to%20Learn%20Nothing%20for%20Multi-Task%20Learning&body=Title%3A%20MTL-UE%3A%20Learning%20to%20Learn%20Nothing%20for%20Multi-Task%20Learning%0AAuthor%3A%20Yi%20Yu%20and%20Song%20Xia%20and%20Siyuan%20Yang%20and%20Chenqi%20Kong%20and%20Wenhan%20Yang%20and%20Shijian%20Lu%20and%20Yap-Peng%20Tan%20and%20Alex%20C.%20Kot%0AAbstract%3A%20%20%20Most%20existing%20unlearnable%20strategies%20focus%20on%20preventing%20unauthorized%20users%0Afrom%20training%20single-task%20learning%20%28STL%29%20models%20with%20personal%20data.%0ANevertheless%2C%20the%20paradigm%20has%20recently%20shifted%20towards%20multi-task%20data%20and%0Amulti-task%20learning%20%28MTL%29%2C%20targeting%20generalist%20and%20foundation%20models%20that%20can%0Ahandle%20multiple%20tasks%20simultaneously.%20Despite%20their%20growing%20importance%2C%20MTL%0Adata%20and%20models%20have%20been%20largely%20neglected%20while%20pursuing%20unlearnable%0Astrategies.%20This%20paper%20presents%20MTL-UE%2C%20the%20first%20unified%20framework%20for%0Agenerating%20unlearnable%20examples%20for%20multi-task%20data%20and%20MTL%20models.%20Instead%20of%0Aoptimizing%20perturbations%20for%20each%20sample%2C%20we%20design%20a%20generator-based%20structure%0Athat%20introduces%20label%20priors%20and%20class-wise%20feature%20embeddings%20which%20leads%20to%0Amuch%20better%20attacking%20performance.%20In%20addition%2C%20MTL-UE%20incorporates%20intra-task%0Aand%20inter-task%20embedding%20regularization%20to%20increase%20inter-class%20separation%20and%0Asuppress%20intra-class%20variance%20which%20enhances%20the%20attack%20robustness%20greatly.%0AFurthermore%2C%20MTL-UE%20is%20versatile%20with%20good%20supports%20for%20dense%20prediction%20tasks%0Ain%20MTL.%20It%20is%20also%20plug-and-play%20allowing%20integrating%20existing%0Asurrogate-dependent%20unlearnable%20methods%20with%20little%20adaptation.%20Extensive%0Aexperiments%20show%20that%20MTL-UE%20achieves%20superior%20attacking%20performance%0Aconsistently%20across%204%20MTL%20datasets%2C%203%20base%20UE%20methods%2C%205%20model%20backbones%2C%20and%205%0AMTL%20task-weighting%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTL-UE%253A%2520Learning%2520to%2520Learn%2520Nothing%2520for%2520Multi-Task%2520Learning%26entry.906535625%3DYi%2520Yu%2520and%2520Song%2520Xia%2520and%2520Siyuan%2520Yang%2520and%2520Chenqi%2520Kong%2520and%2520Wenhan%2520Yang%2520and%2520Shijian%2520Lu%2520and%2520Yap-Peng%2520Tan%2520and%2520Alex%2520C.%2520Kot%26entry.1292438233%3D%2520%2520Most%2520existing%2520unlearnable%2520strategies%2520focus%2520on%2520preventing%2520unauthorized%2520users%250Afrom%2520training%2520single-task%2520learning%2520%2528STL%2529%2520models%2520with%2520personal%2520data.%250ANevertheless%252C%2520the%2520paradigm%2520has%2520recently%2520shifted%2520towards%2520multi-task%2520data%2520and%250Amulti-task%2520learning%2520%2528MTL%2529%252C%2520targeting%2520generalist%2520and%2520foundation%2520models%2520that%2520can%250Ahandle%2520multiple%2520tasks%2520simultaneously.%2520Despite%2520their%2520growing%2520importance%252C%2520MTL%250Adata%2520and%2520models%2520have%2520been%2520largely%2520neglected%2520while%2520pursuing%2520unlearnable%250Astrategies.%2520This%2520paper%2520presents%2520MTL-UE%252C%2520the%2520first%2520unified%2520framework%2520for%250Agenerating%2520unlearnable%2520examples%2520for%2520multi-task%2520data%2520and%2520MTL%2520models.%2520Instead%2520of%250Aoptimizing%2520perturbations%2520for%2520each%2520sample%252C%2520we%2520design%2520a%2520generator-based%2520structure%250Athat%2520introduces%2520label%2520priors%2520and%2520class-wise%2520feature%2520embeddings%2520which%2520leads%2520to%250Amuch%2520better%2520attacking%2520performance.%2520In%2520addition%252C%2520MTL-UE%2520incorporates%2520intra-task%250Aand%2520inter-task%2520embedding%2520regularization%2520to%2520increase%2520inter-class%2520separation%2520and%250Asuppress%2520intra-class%2520variance%2520which%2520enhances%2520the%2520attack%2520robustness%2520greatly.%250AFurthermore%252C%2520MTL-UE%2520is%2520versatile%2520with%2520good%2520supports%2520for%2520dense%2520prediction%2520tasks%250Ain%2520MTL.%2520It%2520is%2520also%2520plug-and-play%2520allowing%2520integrating%2520existing%250Asurrogate-dependent%2520unlearnable%2520methods%2520with%2520little%2520adaptation.%2520Extensive%250Aexperiments%2520show%2520that%2520MTL-UE%2520achieves%2520superior%2520attacking%2520performance%250Aconsistently%2520across%25204%2520MTL%2520datasets%252C%25203%2520base%2520UE%2520methods%252C%25205%2520model%2520backbones%252C%2520and%25205%250AMTL%2520task-weighting%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTL-UE%3A%20Learning%20to%20Learn%20Nothing%20for%20Multi-Task%20Learning&entry.906535625=Yi%20Yu%20and%20Song%20Xia%20and%20Siyuan%20Yang%20and%20Chenqi%20Kong%20and%20Wenhan%20Yang%20and%20Shijian%20Lu%20and%20Yap-Peng%20Tan%20and%20Alex%20C.%20Kot&entry.1292438233=%20%20Most%20existing%20unlearnable%20strategies%20focus%20on%20preventing%20unauthorized%20users%0Afrom%20training%20single-task%20learning%20%28STL%29%20models%20with%20personal%20data.%0ANevertheless%2C%20the%20paradigm%20has%20recently%20shifted%20towards%20multi-task%20data%20and%0Amulti-task%20learning%20%28MTL%29%2C%20targeting%20generalist%20and%20foundation%20models%20that%20can%0Ahandle%20multiple%20tasks%20simultaneously.%20Despite%20their%20growing%20importance%2C%20MTL%0Adata%20and%20models%20have%20been%20largely%20neglected%20while%20pursuing%20unlearnable%0Astrategies.%20This%20paper%20presents%20MTL-UE%2C%20the%20first%20unified%20framework%20for%0Agenerating%20unlearnable%20examples%20for%20multi-task%20data%20and%20MTL%20models.%20Instead%20of%0Aoptimizing%20perturbations%20for%20each%20sample%2C%20we%20design%20a%20generator-based%20structure%0Athat%20introduces%20label%20priors%20and%20class-wise%20feature%20embeddings%20which%20leads%20to%0Amuch%20better%20attacking%20performance.%20In%20addition%2C%20MTL-UE%20incorporates%20intra-task%0Aand%20inter-task%20embedding%20regularization%20to%20increase%20inter-class%20separation%20and%0Asuppress%20intra-class%20variance%20which%20enhances%20the%20attack%20robustness%20greatly.%0AFurthermore%2C%20MTL-UE%20is%20versatile%20with%20good%20supports%20for%20dense%20prediction%20tasks%0Ain%20MTL.%20It%20is%20also%20plug-and-play%20allowing%20integrating%20existing%0Asurrogate-dependent%20unlearnable%20methods%20with%20little%20adaptation.%20Extensive%0Aexperiments%20show%20that%20MTL-UE%20achieves%20superior%20attacking%20performance%0Aconsistently%20across%204%20MTL%20datasets%2C%203%20base%20UE%20methods%2C%205%20model%20backbones%2C%20and%205%0AMTL%20task-weighting%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05279v1&entry.124074799=Read"},
{"title": "Proxy Prompt: Endowing SAM and SAM 2 with Auto-Interactive-Prompt for\n  Medical Segmentation", "author": "Wang Xinyi and Kang Hongyu and Wei Peishan and Shuai Li and Yu Sun and Sai Kit Lam and Yongping Zheng", "abstract": "  In this paper, we aim to address the unmet demand for automated prompting and\nenhanced human-model interactions of SAM and SAM2 for the sake of promoting\ntheir widespread clinical adoption. Specifically, we propose Proxy Prompt (PP),\nauto-generated by leveraging non-target data with a pre-annotated mask. We\ndevise a novel 3-step context-selection strategy for adaptively selecting the\nmost representative contextual information from non-target data via vision\nmamba and selective maps, empowering the guiding capability of non-target\nimage-mask pairs for segmentation on target image/video data. To reinforce\nhuman-model interactions in PP, we further propose a contextual colorization\nmodule via a dual-reverse cross-attention to enhance interactions between\ntarget features and contextual-embedding with amplifying distinctive features\nof user-defined object(s). Via extensive evaluations, our method achieves\nstate-of-the-art performance on four public datasets and yields comparable\nresults with fully-trained models, even when trained with only 16 image masks.\n", "link": "http://arxiv.org/abs/2502.03501v3", "date": "2025-05-08", "relevancy": 2.1127, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5378}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5364}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proxy%20Prompt%3A%20Endowing%20SAM%20and%20SAM%202%20with%20Auto-Interactive-Prompt%20for%0A%20%20Medical%20Segmentation&body=Title%3A%20Proxy%20Prompt%3A%20Endowing%20SAM%20and%20SAM%202%20with%20Auto-Interactive-Prompt%20for%0A%20%20Medical%20Segmentation%0AAuthor%3A%20Wang%20Xinyi%20and%20Kang%20Hongyu%20and%20Wei%20Peishan%20and%20Shuai%20Li%20and%20Yu%20Sun%20and%20Sai%20Kit%20Lam%20and%20Yongping%20Zheng%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20aim%20to%20address%20the%20unmet%20demand%20for%20automated%20prompting%20and%0Aenhanced%20human-model%20interactions%20of%20SAM%20and%20SAM2%20for%20the%20sake%20of%20promoting%0Atheir%20widespread%20clinical%20adoption.%20Specifically%2C%20we%20propose%20Proxy%20Prompt%20%28PP%29%2C%0Aauto-generated%20by%20leveraging%20non-target%20data%20with%20a%20pre-annotated%20mask.%20We%0Adevise%20a%20novel%203-step%20context-selection%20strategy%20for%20adaptively%20selecting%20the%0Amost%20representative%20contextual%20information%20from%20non-target%20data%20via%20vision%0Amamba%20and%20selective%20maps%2C%20empowering%20the%20guiding%20capability%20of%20non-target%0Aimage-mask%20pairs%20for%20segmentation%20on%20target%20image/video%20data.%20To%20reinforce%0Ahuman-model%20interactions%20in%20PP%2C%20we%20further%20propose%20a%20contextual%20colorization%0Amodule%20via%20a%20dual-reverse%20cross-attention%20to%20enhance%20interactions%20between%0Atarget%20features%20and%20contextual-embedding%20with%20amplifying%20distinctive%20features%0Aof%20user-defined%20object%28s%29.%20Via%20extensive%20evaluations%2C%20our%20method%20achieves%0Astate-of-the-art%20performance%20on%20four%20public%20datasets%20and%20yields%20comparable%0Aresults%20with%20fully-trained%20models%2C%20even%20when%20trained%20with%20only%2016%20image%20masks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03501v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProxy%2520Prompt%253A%2520Endowing%2520SAM%2520and%2520SAM%25202%2520with%2520Auto-Interactive-Prompt%2520for%250A%2520%2520Medical%2520Segmentation%26entry.906535625%3DWang%2520Xinyi%2520and%2520Kang%2520Hongyu%2520and%2520Wei%2520Peishan%2520and%2520Shuai%2520Li%2520and%2520Yu%2520Sun%2520and%2520Sai%2520Kit%2520Lam%2520and%2520Yongping%2520Zheng%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520address%2520the%2520unmet%2520demand%2520for%2520automated%2520prompting%2520and%250Aenhanced%2520human-model%2520interactions%2520of%2520SAM%2520and%2520SAM2%2520for%2520the%2520sake%2520of%2520promoting%250Atheir%2520widespread%2520clinical%2520adoption.%2520Specifically%252C%2520we%2520propose%2520Proxy%2520Prompt%2520%2528PP%2529%252C%250Aauto-generated%2520by%2520leveraging%2520non-target%2520data%2520with%2520a%2520pre-annotated%2520mask.%2520We%250Adevise%2520a%2520novel%25203-step%2520context-selection%2520strategy%2520for%2520adaptively%2520selecting%2520the%250Amost%2520representative%2520contextual%2520information%2520from%2520non-target%2520data%2520via%2520vision%250Amamba%2520and%2520selective%2520maps%252C%2520empowering%2520the%2520guiding%2520capability%2520of%2520non-target%250Aimage-mask%2520pairs%2520for%2520segmentation%2520on%2520target%2520image/video%2520data.%2520To%2520reinforce%250Ahuman-model%2520interactions%2520in%2520PP%252C%2520we%2520further%2520propose%2520a%2520contextual%2520colorization%250Amodule%2520via%2520a%2520dual-reverse%2520cross-attention%2520to%2520enhance%2520interactions%2520between%250Atarget%2520features%2520and%2520contextual-embedding%2520with%2520amplifying%2520distinctive%2520features%250Aof%2520user-defined%2520object%2528s%2529.%2520Via%2520extensive%2520evaluations%252C%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520on%2520four%2520public%2520datasets%2520and%2520yields%2520comparable%250Aresults%2520with%2520fully-trained%2520models%252C%2520even%2520when%2520trained%2520with%2520only%252016%2520image%2520masks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03501v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proxy%20Prompt%3A%20Endowing%20SAM%20and%20SAM%202%20with%20Auto-Interactive-Prompt%20for%0A%20%20Medical%20Segmentation&entry.906535625=Wang%20Xinyi%20and%20Kang%20Hongyu%20and%20Wei%20Peishan%20and%20Shuai%20Li%20and%20Yu%20Sun%20and%20Sai%20Kit%20Lam%20and%20Yongping%20Zheng&entry.1292438233=%20%20In%20this%20paper%2C%20we%20aim%20to%20address%20the%20unmet%20demand%20for%20automated%20prompting%20and%0Aenhanced%20human-model%20interactions%20of%20SAM%20and%20SAM2%20for%20the%20sake%20of%20promoting%0Atheir%20widespread%20clinical%20adoption.%20Specifically%2C%20we%20propose%20Proxy%20Prompt%20%28PP%29%2C%0Aauto-generated%20by%20leveraging%20non-target%20data%20with%20a%20pre-annotated%20mask.%20We%0Adevise%20a%20novel%203-step%20context-selection%20strategy%20for%20adaptively%20selecting%20the%0Amost%20representative%20contextual%20information%20from%20non-target%20data%20via%20vision%0Amamba%20and%20selective%20maps%2C%20empowering%20the%20guiding%20capability%20of%20non-target%0Aimage-mask%20pairs%20for%20segmentation%20on%20target%20image/video%20data.%20To%20reinforce%0Ahuman-model%20interactions%20in%20PP%2C%20we%20further%20propose%20a%20contextual%20colorization%0Amodule%20via%20a%20dual-reverse%20cross-attention%20to%20enhance%20interactions%20between%0Atarget%20features%20and%20contextual-embedding%20with%20amplifying%20distinctive%20features%0Aof%20user-defined%20object%28s%29.%20Via%20extensive%20evaluations%2C%20our%20method%20achieves%0Astate-of-the-art%20performance%20on%20four%20public%20datasets%20and%20yields%20comparable%0Aresults%20with%20fully-trained%20models%2C%20even%20when%20trained%20with%20only%2016%20image%20masks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03501v3&entry.124074799=Read"},
{"title": "Text2Cypher: Data Pruning using Hard Example Selection", "author": "Makbule Gulcin Ozsoy", "abstract": "  Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution.\n", "link": "http://arxiv.org/abs/2505.05122v1", "date": "2025-05-08", "relevancy": 2.1122, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2Cypher%3A%20Data%20Pruning%20using%20Hard%20Example%20Selection&body=Title%3A%20Text2Cypher%3A%20Data%20Pruning%20using%20Hard%20Example%20Selection%0AAuthor%3A%20Makbule%20Gulcin%20Ozsoy%0AAbstract%3A%20%20%20Database%20query%20languages%20such%20as%20SQL%20for%20relational%20databases%20and%20Cypher%20for%0Agraph%20databases%20have%20been%20widely%20adopted.%20Recent%20advancements%20in%20large%20language%0Amodels%20%28LLMs%29%20enable%20natural%20language%20interactions%20with%20databases%20through%0Amodels%20like%20Text2SQL%20and%20Text2Cypher.%20Fine-tuning%20these%20models%20typically%0Arequires%20large%2C%20diverse%20datasets%20containing%20non-trivial%20examples.%20However%2C%20as%0Adataset%20size%20increases%2C%20the%20cost%20of%20fine-tuning%20also%20rises.%20This%20makes%20smaller%2C%0Ahigh-quality%20datasets%20essential%20for%20reducing%20costs%20for%20the%20same%20or%20better%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20five%20hard-example%20selection%20techniques%0Afor%20pruning%20the%20Text2Cypher%20dataset%2C%20aiming%20to%20preserve%20or%20improve%20performance%0Awhile%20reducing%20resource%20usage.%20Our%20results%20show%20that%20these%20hard-example%0Aselection%20approaches%20can%20halve%20training%20time%20and%20costs%20with%20minimal%20impact%20on%0Aperformance%2C%20and%20demonstrates%20that%20hard-example%20selection%20provides%20a%0Acost-effective%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2Cypher%253A%2520Data%2520Pruning%2520using%2520Hard%2520Example%2520Selection%26entry.906535625%3DMakbule%2520Gulcin%2520Ozsoy%26entry.1292438233%3D%2520%2520Database%2520query%2520languages%2520such%2520as%2520SQL%2520for%2520relational%2520databases%2520and%2520Cypher%2520for%250Agraph%2520databases%2520have%2520been%2520widely%2520adopted.%2520Recent%2520advancements%2520in%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520enable%2520natural%2520language%2520interactions%2520with%2520databases%2520through%250Amodels%2520like%2520Text2SQL%2520and%2520Text2Cypher.%2520Fine-tuning%2520these%2520models%2520typically%250Arequires%2520large%252C%2520diverse%2520datasets%2520containing%2520non-trivial%2520examples.%2520However%252C%2520as%250Adataset%2520size%2520increases%252C%2520the%2520cost%2520of%2520fine-tuning%2520also%2520rises.%2520This%2520makes%2520smaller%252C%250Ahigh-quality%2520datasets%2520essential%2520for%2520reducing%2520costs%2520for%2520the%2520same%2520or%2520better%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520five%2520hard-example%2520selection%2520techniques%250Afor%2520pruning%2520the%2520Text2Cypher%2520dataset%252C%2520aiming%2520to%2520preserve%2520or%2520improve%2520performance%250Awhile%2520reducing%2520resource%2520usage.%2520Our%2520results%2520show%2520that%2520these%2520hard-example%250Aselection%2520approaches%2520can%2520halve%2520training%2520time%2520and%2520costs%2520with%2520minimal%2520impact%2520on%250Aperformance%252C%2520and%2520demonstrates%2520that%2520hard-example%2520selection%2520provides%2520a%250Acost-effective%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2Cypher%3A%20Data%20Pruning%20using%20Hard%20Example%20Selection&entry.906535625=Makbule%20Gulcin%20Ozsoy&entry.1292438233=%20%20Database%20query%20languages%20such%20as%20SQL%20for%20relational%20databases%20and%20Cypher%20for%0Agraph%20databases%20have%20been%20widely%20adopted.%20Recent%20advancements%20in%20large%20language%0Amodels%20%28LLMs%29%20enable%20natural%20language%20interactions%20with%20databases%20through%0Amodels%20like%20Text2SQL%20and%20Text2Cypher.%20Fine-tuning%20these%20models%20typically%0Arequires%20large%2C%20diverse%20datasets%20containing%20non-trivial%20examples.%20However%2C%20as%0Adataset%20size%20increases%2C%20the%20cost%20of%20fine-tuning%20also%20rises.%20This%20makes%20smaller%2C%0Ahigh-quality%20datasets%20essential%20for%20reducing%20costs%20for%20the%20same%20or%20better%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20five%20hard-example%20selection%20techniques%0Afor%20pruning%20the%20Text2Cypher%20dataset%2C%20aiming%20to%20preserve%20or%20improve%20performance%0Awhile%20reducing%20resource%20usage.%20Our%20results%20show%20that%20these%20hard-example%0Aselection%20approaches%20can%20halve%20training%20time%20and%20costs%20with%20minimal%20impact%20on%0Aperformance%2C%20and%20demonstrates%20that%20hard-example%20selection%20provides%20a%0Acost-effective%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05122v1&entry.124074799=Read"},
{"title": "Online Velocity Profile Generation and Tracking for Sampling-Based Local\n  Planning Algorithms in Autonomous Racing Environments", "author": "Alexander Langmann and Levent \u00d6gretmen and Frederik Werner and Johannes Betz", "abstract": "  This work presents an online velocity planner for autonomous racing that\nadapts to changing dynamic constraints, such as grip variations from tire\ntemperature changes and rubber accumulation. The method combines a\nforward-backward solver for online velocity optimization with a novel spatial\nsampling strategy for local trajectory planning, utilizing a three-dimensional\ntrack representation. The computed velocity profile serves as a reference for\nthe local planner, ensuring adaptability to environmental and vehicle dynamics.\nWe demonstrate the approach's robust performance and computational efficiency\nin racing scenarios and discuss its limitations, including sensitivity to\ndeviations from the predefined racing line and high jerk characteristics of the\nvelocity profile.\n", "link": "http://arxiv.org/abs/2505.05157v1", "date": "2025-05-08", "relevancy": 2.0766, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5308}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5241}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Velocity%20Profile%20Generation%20and%20Tracking%20for%20Sampling-Based%20Local%0A%20%20Planning%20Algorithms%20in%20Autonomous%20Racing%20Environments&body=Title%3A%20Online%20Velocity%20Profile%20Generation%20and%20Tracking%20for%20Sampling-Based%20Local%0A%20%20Planning%20Algorithms%20in%20Autonomous%20Racing%20Environments%0AAuthor%3A%20Alexander%20Langmann%20and%20Levent%20%C3%96gretmen%20and%20Frederik%20Werner%20and%20Johannes%20Betz%0AAbstract%3A%20%20%20This%20work%20presents%20an%20online%20velocity%20planner%20for%20autonomous%20racing%20that%0Aadapts%20to%20changing%20dynamic%20constraints%2C%20such%20as%20grip%20variations%20from%20tire%0Atemperature%20changes%20and%20rubber%20accumulation.%20The%20method%20combines%20a%0Aforward-backward%20solver%20for%20online%20velocity%20optimization%20with%20a%20novel%20spatial%0Asampling%20strategy%20for%20local%20trajectory%20planning%2C%20utilizing%20a%20three-dimensional%0Atrack%20representation.%20The%20computed%20velocity%20profile%20serves%20as%20a%20reference%20for%0Athe%20local%20planner%2C%20ensuring%20adaptability%20to%20environmental%20and%20vehicle%20dynamics.%0AWe%20demonstrate%20the%20approach%27s%20robust%20performance%20and%20computational%20efficiency%0Ain%20racing%20scenarios%20and%20discuss%20its%20limitations%2C%20including%20sensitivity%20to%0Adeviations%20from%20the%20predefined%20racing%20line%20and%20high%20jerk%20characteristics%20of%20the%0Avelocity%20profile.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Velocity%2520Profile%2520Generation%2520and%2520Tracking%2520for%2520Sampling-Based%2520Local%250A%2520%2520Planning%2520Algorithms%2520in%2520Autonomous%2520Racing%2520Environments%26entry.906535625%3DAlexander%2520Langmann%2520and%2520Levent%2520%25C3%2596gretmen%2520and%2520Frederik%2520Werner%2520and%2520Johannes%2520Betz%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520an%2520online%2520velocity%2520planner%2520for%2520autonomous%2520racing%2520that%250Aadapts%2520to%2520changing%2520dynamic%2520constraints%252C%2520such%2520as%2520grip%2520variations%2520from%2520tire%250Atemperature%2520changes%2520and%2520rubber%2520accumulation.%2520The%2520method%2520combines%2520a%250Aforward-backward%2520solver%2520for%2520online%2520velocity%2520optimization%2520with%2520a%2520novel%2520spatial%250Asampling%2520strategy%2520for%2520local%2520trajectory%2520planning%252C%2520utilizing%2520a%2520three-dimensional%250Atrack%2520representation.%2520The%2520computed%2520velocity%2520profile%2520serves%2520as%2520a%2520reference%2520for%250Athe%2520local%2520planner%252C%2520ensuring%2520adaptability%2520to%2520environmental%2520and%2520vehicle%2520dynamics.%250AWe%2520demonstrate%2520the%2520approach%2527s%2520robust%2520performance%2520and%2520computational%2520efficiency%250Ain%2520racing%2520scenarios%2520and%2520discuss%2520its%2520limitations%252C%2520including%2520sensitivity%2520to%250Adeviations%2520from%2520the%2520predefined%2520racing%2520line%2520and%2520high%2520jerk%2520characteristics%2520of%2520the%250Avelocity%2520profile.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Velocity%20Profile%20Generation%20and%20Tracking%20for%20Sampling-Based%20Local%0A%20%20Planning%20Algorithms%20in%20Autonomous%20Racing%20Environments&entry.906535625=Alexander%20Langmann%20and%20Levent%20%C3%96gretmen%20and%20Frederik%20Werner%20and%20Johannes%20Betz&entry.1292438233=%20%20This%20work%20presents%20an%20online%20velocity%20planner%20for%20autonomous%20racing%20that%0Aadapts%20to%20changing%20dynamic%20constraints%2C%20such%20as%20grip%20variations%20from%20tire%0Atemperature%20changes%20and%20rubber%20accumulation.%20The%20method%20combines%20a%0Aforward-backward%20solver%20for%20online%20velocity%20optimization%20with%20a%20novel%20spatial%0Asampling%20strategy%20for%20local%20trajectory%20planning%2C%20utilizing%20a%20three-dimensional%0Atrack%20representation.%20The%20computed%20velocity%20profile%20serves%20as%20a%20reference%20for%0Athe%20local%20planner%2C%20ensuring%20adaptability%20to%20environmental%20and%20vehicle%20dynamics.%0AWe%20demonstrate%20the%20approach%27s%20robust%20performance%20and%20computational%20efficiency%0Ain%20racing%20scenarios%20and%20discuss%20its%20limitations%2C%20including%20sensitivity%20to%0Adeviations%20from%20the%20predefined%20racing%20line%20and%20high%20jerk%20characteristics%20of%20the%0Avelocity%20profile.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05157v1&entry.124074799=Read"},
{"title": "GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and\n  Problem Solutions", "author": "Jo-Ku Cheng and Zeren Zhang and Ran Chen and Jingyang Deng and Ziran Qin and Jinwen Ma", "abstract": "  We propose GeoUni, the first unified geometry expert model capable of\ngenerating problem solutions and diagrams within a single framework in a way\nthat enables the creation of unique and individualized geometry problems.\nTraditionally, solving geometry problems and generating diagrams have been\ntreated as separate tasks in machine learning, with no models successfully\nintegrating both to support problem creation. However, we believe that mastery\nin geometry requires frictionless integration of all of these skills, from\nsolving problems to visualizing geometric relationships, and finally, crafting\ntailored problems. Our extensive experiments demonstrate that GeoUni, with only\n1.5B parameters, achieves performance comparable to larger models such as\nDeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also\nexcels in generating precise geometric diagrams, surpassing both text-to-image\nmodels and unified models, including the GPT-4o image generation. Most\nimportantly, GeoUni is the only model capable of successfully generating\ntextual problems with matching diagrams based on specific knowledge points,\nthus offering a wider range of capabilities that extend beyond current models.\n", "link": "http://arxiv.org/abs/2504.10146v2", "date": "2025-05-08", "relevancy": 2.0734, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5242}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5188}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoUni%3A%20A%20Unified%20Model%20for%20Generating%20Geometry%20Diagrams%2C%20Problems%20and%0A%20%20Problem%20Solutions&body=Title%3A%20GeoUni%3A%20A%20Unified%20Model%20for%20Generating%20Geometry%20Diagrams%2C%20Problems%20and%0A%20%20Problem%20Solutions%0AAuthor%3A%20Jo-Ku%20Cheng%20and%20Zeren%20Zhang%20and%20Ran%20Chen%20and%20Jingyang%20Deng%20and%20Ziran%20Qin%20and%20Jinwen%20Ma%0AAbstract%3A%20%20%20We%20propose%20GeoUni%2C%20the%20first%20unified%20geometry%20expert%20model%20capable%20of%0Agenerating%20problem%20solutions%20and%20diagrams%20within%20a%20single%20framework%20in%20a%20way%0Athat%20enables%20the%20creation%20of%20unique%20and%20individualized%20geometry%20problems.%0ATraditionally%2C%20solving%20geometry%20problems%20and%20generating%20diagrams%20have%20been%0Atreated%20as%20separate%20tasks%20in%20machine%20learning%2C%20with%20no%20models%20successfully%0Aintegrating%20both%20to%20support%20problem%20creation.%20However%2C%20we%20believe%20that%20mastery%0Ain%20geometry%20requires%20frictionless%20integration%20of%20all%20of%20these%20skills%2C%20from%0Asolving%20problems%20to%20visualizing%20geometric%20relationships%2C%20and%20finally%2C%20crafting%0Atailored%20problems.%20Our%20extensive%20experiments%20demonstrate%20that%20GeoUni%2C%20with%20only%0A1.5B%20parameters%2C%20achieves%20performance%20comparable%20to%20larger%20models%20such%20as%0ADeepSeek-R1%20with%20671B%20parameters%20in%20geometric%20reasoning%20tasks.%20GeoUni%20also%0Aexcels%20in%20generating%20precise%20geometric%20diagrams%2C%20surpassing%20both%20text-to-image%0Amodels%20and%20unified%20models%2C%20including%20the%20GPT-4o%20image%20generation.%20Most%0Aimportantly%2C%20GeoUni%20is%20the%20only%20model%20capable%20of%20successfully%20generating%0Atextual%20problems%20with%20matching%20diagrams%20based%20on%20specific%20knowledge%20points%2C%0Athus%20offering%20a%20wider%20range%20of%20capabilities%20that%20extend%20beyond%20current%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10146v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoUni%253A%2520A%2520Unified%2520Model%2520for%2520Generating%2520Geometry%2520Diagrams%252C%2520Problems%2520and%250A%2520%2520Problem%2520Solutions%26entry.906535625%3DJo-Ku%2520Cheng%2520and%2520Zeren%2520Zhang%2520and%2520Ran%2520Chen%2520and%2520Jingyang%2520Deng%2520and%2520Ziran%2520Qin%2520and%2520Jinwen%2520Ma%26entry.1292438233%3D%2520%2520We%2520propose%2520GeoUni%252C%2520the%2520first%2520unified%2520geometry%2520expert%2520model%2520capable%2520of%250Agenerating%2520problem%2520solutions%2520and%2520diagrams%2520within%2520a%2520single%2520framework%2520in%2520a%2520way%250Athat%2520enables%2520the%2520creation%2520of%2520unique%2520and%2520individualized%2520geometry%2520problems.%250ATraditionally%252C%2520solving%2520geometry%2520problems%2520and%2520generating%2520diagrams%2520have%2520been%250Atreated%2520as%2520separate%2520tasks%2520in%2520machine%2520learning%252C%2520with%2520no%2520models%2520successfully%250Aintegrating%2520both%2520to%2520support%2520problem%2520creation.%2520However%252C%2520we%2520believe%2520that%2520mastery%250Ain%2520geometry%2520requires%2520frictionless%2520integration%2520of%2520all%2520of%2520these%2520skills%252C%2520from%250Asolving%2520problems%2520to%2520visualizing%2520geometric%2520relationships%252C%2520and%2520finally%252C%2520crafting%250Atailored%2520problems.%2520Our%2520extensive%2520experiments%2520demonstrate%2520that%2520GeoUni%252C%2520with%2520only%250A1.5B%2520parameters%252C%2520achieves%2520performance%2520comparable%2520to%2520larger%2520models%2520such%2520as%250ADeepSeek-R1%2520with%2520671B%2520parameters%2520in%2520geometric%2520reasoning%2520tasks.%2520GeoUni%2520also%250Aexcels%2520in%2520generating%2520precise%2520geometric%2520diagrams%252C%2520surpassing%2520both%2520text-to-image%250Amodels%2520and%2520unified%2520models%252C%2520including%2520the%2520GPT-4o%2520image%2520generation.%2520Most%250Aimportantly%252C%2520GeoUni%2520is%2520the%2520only%2520model%2520capable%2520of%2520successfully%2520generating%250Atextual%2520problems%2520with%2520matching%2520diagrams%2520based%2520on%2520specific%2520knowledge%2520points%252C%250Athus%2520offering%2520a%2520wider%2520range%2520of%2520capabilities%2520that%2520extend%2520beyond%2520current%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10146v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoUni%3A%20A%20Unified%20Model%20for%20Generating%20Geometry%20Diagrams%2C%20Problems%20and%0A%20%20Problem%20Solutions&entry.906535625=Jo-Ku%20Cheng%20and%20Zeren%20Zhang%20and%20Ran%20Chen%20and%20Jingyang%20Deng%20and%20Ziran%20Qin%20and%20Jinwen%20Ma&entry.1292438233=%20%20We%20propose%20GeoUni%2C%20the%20first%20unified%20geometry%20expert%20model%20capable%20of%0Agenerating%20problem%20solutions%20and%20diagrams%20within%20a%20single%20framework%20in%20a%20way%0Athat%20enables%20the%20creation%20of%20unique%20and%20individualized%20geometry%20problems.%0ATraditionally%2C%20solving%20geometry%20problems%20and%20generating%20diagrams%20have%20been%0Atreated%20as%20separate%20tasks%20in%20machine%20learning%2C%20with%20no%20models%20successfully%0Aintegrating%20both%20to%20support%20problem%20creation.%20However%2C%20we%20believe%20that%20mastery%0Ain%20geometry%20requires%20frictionless%20integration%20of%20all%20of%20these%20skills%2C%20from%0Asolving%20problems%20to%20visualizing%20geometric%20relationships%2C%20and%20finally%2C%20crafting%0Atailored%20problems.%20Our%20extensive%20experiments%20demonstrate%20that%20GeoUni%2C%20with%20only%0A1.5B%20parameters%2C%20achieves%20performance%20comparable%20to%20larger%20models%20such%20as%0ADeepSeek-R1%20with%20671B%20parameters%20in%20geometric%20reasoning%20tasks.%20GeoUni%20also%0Aexcels%20in%20generating%20precise%20geometric%20diagrams%2C%20surpassing%20both%20text-to-image%0Amodels%20and%20unified%20models%2C%20including%20the%20GPT-4o%20image%20generation.%20Most%0Aimportantly%2C%20GeoUni%20is%20the%20only%20model%20capable%20of%20successfully%20generating%0Atextual%20problems%20with%20matching%20diagrams%20based%20on%20specific%20knowledge%20points%2C%0Athus%20offering%20a%20wider%20range%20of%20capabilities%20that%20extend%20beyond%20current%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10146v2&entry.124074799=Read"},
{"title": "A Unified Data Representation Learning for Non-parametric Two-sample\n  Testing", "author": "Xunye Tian and Liuhua Peng and Zhijian Zhou and Mingming Gong and Arthur Gretton and Feng Liu", "abstract": "  Learning effective data representations has been crucial in non-parametric\ntwo-sample testing. Common approaches will first split data into training and\ntest sets and then learn data representations purely on the training set.\nHowever, recent theoretical studies have shown that, as long as the sample\nindexes are not used during the learning process, the whole data can be used to\nlearn data representations, meanwhile ensuring control of Type-I errors. The\nabove fact motivates us to use the test set (but without sample indexes) to\nfacilitate the data representation learning in the testing. To this end, we\npropose a representation-learning two-sample testing (RL-TST) framework. RL-TST\nfirst performs purely self-supervised representation learning on the entire\ndataset to capture inherent representations (IRs) that reflect the underlying\ndata manifold. A discriminative model is then trained on these IRs to learn\ndiscriminative representations (DRs), enabling the framework to leverage both\nthe rich structural information from IRs and the discriminative power of DRs.\nExtensive experiments demonstrate that RL-TST outperforms representative\napproaches by simultaneously using data manifold information in the test set\nand enhancing test power via finding the DRs with the training set.\n", "link": "http://arxiv.org/abs/2412.00613v2", "date": "2025-05-08", "relevancy": 2.0638, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5334}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5106}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Data%20Representation%20Learning%20for%20Non-parametric%20Two-sample%0A%20%20Testing&body=Title%3A%20A%20Unified%20Data%20Representation%20Learning%20for%20Non-parametric%20Two-sample%0A%20%20Testing%0AAuthor%3A%20Xunye%20Tian%20and%20Liuhua%20Peng%20and%20Zhijian%20Zhou%20and%20Mingming%20Gong%20and%20Arthur%20Gretton%20and%20Feng%20Liu%0AAbstract%3A%20%20%20Learning%20effective%20data%20representations%20has%20been%20crucial%20in%20non-parametric%0Atwo-sample%20testing.%20Common%20approaches%20will%20first%20split%20data%20into%20training%20and%0Atest%20sets%20and%20then%20learn%20data%20representations%20purely%20on%20the%20training%20set.%0AHowever%2C%20recent%20theoretical%20studies%20have%20shown%20that%2C%20as%20long%20as%20the%20sample%0Aindexes%20are%20not%20used%20during%20the%20learning%20process%2C%20the%20whole%20data%20can%20be%20used%20to%0Alearn%20data%20representations%2C%20meanwhile%20ensuring%20control%20of%20Type-I%20errors.%20The%0Aabove%20fact%20motivates%20us%20to%20use%20the%20test%20set%20%28but%20without%20sample%20indexes%29%20to%0Afacilitate%20the%20data%20representation%20learning%20in%20the%20testing.%20To%20this%20end%2C%20we%0Apropose%20a%20representation-learning%20two-sample%20testing%20%28RL-TST%29%20framework.%20RL-TST%0Afirst%20performs%20purely%20self-supervised%20representation%20learning%20on%20the%20entire%0Adataset%20to%20capture%20inherent%20representations%20%28IRs%29%20that%20reflect%20the%20underlying%0Adata%20manifold.%20A%20discriminative%20model%20is%20then%20trained%20on%20these%20IRs%20to%20learn%0Adiscriminative%20representations%20%28DRs%29%2C%20enabling%20the%20framework%20to%20leverage%20both%0Athe%20rich%20structural%20information%20from%20IRs%20and%20the%20discriminative%20power%20of%20DRs.%0AExtensive%20experiments%20demonstrate%20that%20RL-TST%20outperforms%20representative%0Aapproaches%20by%20simultaneously%20using%20data%20manifold%20information%20in%20the%20test%20set%0Aand%20enhancing%20test%20power%20via%20finding%20the%20DRs%20with%20the%20training%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00613v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Data%2520Representation%2520Learning%2520for%2520Non-parametric%2520Two-sample%250A%2520%2520Testing%26entry.906535625%3DXunye%2520Tian%2520and%2520Liuhua%2520Peng%2520and%2520Zhijian%2520Zhou%2520and%2520Mingming%2520Gong%2520and%2520Arthur%2520Gretton%2520and%2520Feng%2520Liu%26entry.1292438233%3D%2520%2520Learning%2520effective%2520data%2520representations%2520has%2520been%2520crucial%2520in%2520non-parametric%250Atwo-sample%2520testing.%2520Common%2520approaches%2520will%2520first%2520split%2520data%2520into%2520training%2520and%250Atest%2520sets%2520and%2520then%2520learn%2520data%2520representations%2520purely%2520on%2520the%2520training%2520set.%250AHowever%252C%2520recent%2520theoretical%2520studies%2520have%2520shown%2520that%252C%2520as%2520long%2520as%2520the%2520sample%250Aindexes%2520are%2520not%2520used%2520during%2520the%2520learning%2520process%252C%2520the%2520whole%2520data%2520can%2520be%2520used%2520to%250Alearn%2520data%2520representations%252C%2520meanwhile%2520ensuring%2520control%2520of%2520Type-I%2520errors.%2520The%250Aabove%2520fact%2520motivates%2520us%2520to%2520use%2520the%2520test%2520set%2520%2528but%2520without%2520sample%2520indexes%2529%2520to%250Afacilitate%2520the%2520data%2520representation%2520learning%2520in%2520the%2520testing.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520representation-learning%2520two-sample%2520testing%2520%2528RL-TST%2529%2520framework.%2520RL-TST%250Afirst%2520performs%2520purely%2520self-supervised%2520representation%2520learning%2520on%2520the%2520entire%250Adataset%2520to%2520capture%2520inherent%2520representations%2520%2528IRs%2529%2520that%2520reflect%2520the%2520underlying%250Adata%2520manifold.%2520A%2520discriminative%2520model%2520is%2520then%2520trained%2520on%2520these%2520IRs%2520to%2520learn%250Adiscriminative%2520representations%2520%2528DRs%2529%252C%2520enabling%2520the%2520framework%2520to%2520leverage%2520both%250Athe%2520rich%2520structural%2520information%2520from%2520IRs%2520and%2520the%2520discriminative%2520power%2520of%2520DRs.%250AExtensive%2520experiments%2520demonstrate%2520that%2520RL-TST%2520outperforms%2520representative%250Aapproaches%2520by%2520simultaneously%2520using%2520data%2520manifold%2520information%2520in%2520the%2520test%2520set%250Aand%2520enhancing%2520test%2520power%2520via%2520finding%2520the%2520DRs%2520with%2520the%2520training%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00613v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Data%20Representation%20Learning%20for%20Non-parametric%20Two-sample%0A%20%20Testing&entry.906535625=Xunye%20Tian%20and%20Liuhua%20Peng%20and%20Zhijian%20Zhou%20and%20Mingming%20Gong%20and%20Arthur%20Gretton%20and%20Feng%20Liu&entry.1292438233=%20%20Learning%20effective%20data%20representations%20has%20been%20crucial%20in%20non-parametric%0Atwo-sample%20testing.%20Common%20approaches%20will%20first%20split%20data%20into%20training%20and%0Atest%20sets%20and%20then%20learn%20data%20representations%20purely%20on%20the%20training%20set.%0AHowever%2C%20recent%20theoretical%20studies%20have%20shown%20that%2C%20as%20long%20as%20the%20sample%0Aindexes%20are%20not%20used%20during%20the%20learning%20process%2C%20the%20whole%20data%20can%20be%20used%20to%0Alearn%20data%20representations%2C%20meanwhile%20ensuring%20control%20of%20Type-I%20errors.%20The%0Aabove%20fact%20motivates%20us%20to%20use%20the%20test%20set%20%28but%20without%20sample%20indexes%29%20to%0Afacilitate%20the%20data%20representation%20learning%20in%20the%20testing.%20To%20this%20end%2C%20we%0Apropose%20a%20representation-learning%20two-sample%20testing%20%28RL-TST%29%20framework.%20RL-TST%0Afirst%20performs%20purely%20self-supervised%20representation%20learning%20on%20the%20entire%0Adataset%20to%20capture%20inherent%20representations%20%28IRs%29%20that%20reflect%20the%20underlying%0Adata%20manifold.%20A%20discriminative%20model%20is%20then%20trained%20on%20these%20IRs%20to%20learn%0Adiscriminative%20representations%20%28DRs%29%2C%20enabling%20the%20framework%20to%20leverage%20both%0Athe%20rich%20structural%20information%20from%20IRs%20and%20the%20discriminative%20power%20of%20DRs.%0AExtensive%20experiments%20demonstrate%20that%20RL-TST%20outperforms%20representative%0Aapproaches%20by%20simultaneously%20using%20data%20manifold%20information%20in%20the%20test%20set%0Aand%20enhancing%20test%20power%20via%20finding%20the%20DRs%20with%20the%20training%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00613v2&entry.124074799=Read"},
{"title": "Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular\n  Learning", "author": "Ruxue Shi and Hengrui Gu and Hangting Ye and Yiwei Dai and Xu Shen and Xin Wang", "abstract": "  Few-shot tabular learning, in which machine learning models are trained with\na limited amount of labeled data, provides a cost-effective approach to\naddressing real-world challenges. The advent of Large Language Models (LLMs)\nhas sparked interest in leveraging their pre-trained knowledge for few-shot\ntabular learning. Despite promising results, existing approaches either rely on\ntest-time knowledge extraction, which introduces undesirable latency, or\ntext-level knowledge, which leads to unreliable feature engineering. To\novercome these limitations, we propose Latte, a training-time knowledge\nextraction framework that transfers the latent prior knowledge within LLMs to\noptimize a more generalized downstream model. Latte enables general\nknowledge-guided downstream tabular learning, facilitating the weighted fusion\nof information across different feature values while reducing the risk of\noverfitting to limited labeled data. Furthermore, Latte is compatible with\nexisting unsupervised pre-training paradigms and effectively utilizes available\nunlabeled samples to overcome the performance limitations imposed by an\nextremely small labeled dataset. Extensive experiments on various few-shot\ntabular learning benchmarks demonstrate the superior performance of Latte,\nestablishing it as a state-of-the-art approach in this domain\n", "link": "http://arxiv.org/abs/2505.05237v1", "date": "2025-05-08", "relevancy": 2.0554, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5376}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5026}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latte%3A%20Transfering%20LLMs%60%20Latent-level%20Knowledge%20for%20Few-shot%20Tabular%0A%20%20Learning&body=Title%3A%20Latte%3A%20Transfering%20LLMs%60%20Latent-level%20Knowledge%20for%20Few-shot%20Tabular%0A%20%20Learning%0AAuthor%3A%20Ruxue%20Shi%20and%20Hengrui%20Gu%20and%20Hangting%20Ye%20and%20Yiwei%20Dai%20and%20Xu%20Shen%20and%20Xin%20Wang%0AAbstract%3A%20%20%20Few-shot%20tabular%20learning%2C%20in%20which%20machine%20learning%20models%20are%20trained%20with%0Aa%20limited%20amount%20of%20labeled%20data%2C%20provides%20a%20cost-effective%20approach%20to%0Aaddressing%20real-world%20challenges.%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%0Ahas%20sparked%20interest%20in%20leveraging%20their%20pre-trained%20knowledge%20for%20few-shot%0Atabular%20learning.%20Despite%20promising%20results%2C%20existing%20approaches%20either%20rely%20on%0Atest-time%20knowledge%20extraction%2C%20which%20introduces%20undesirable%20latency%2C%20or%0Atext-level%20knowledge%2C%20which%20leads%20to%20unreliable%20feature%20engineering.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20Latte%2C%20a%20training-time%20knowledge%0Aextraction%20framework%20that%20transfers%20the%20latent%20prior%20knowledge%20within%20LLMs%20to%0Aoptimize%20a%20more%20generalized%20downstream%20model.%20Latte%20enables%20general%0Aknowledge-guided%20downstream%20tabular%20learning%2C%20facilitating%20the%20weighted%20fusion%0Aof%20information%20across%20different%20feature%20values%20while%20reducing%20the%20risk%20of%0Aoverfitting%20to%20limited%20labeled%20data.%20Furthermore%2C%20Latte%20is%20compatible%20with%0Aexisting%20unsupervised%20pre-training%20paradigms%20and%20effectively%20utilizes%20available%0Aunlabeled%20samples%20to%20overcome%20the%20performance%20limitations%20imposed%20by%20an%0Aextremely%20small%20labeled%20dataset.%20Extensive%20experiments%20on%20various%20few-shot%0Atabular%20learning%20benchmarks%20demonstrate%20the%20superior%20performance%20of%20Latte%2C%0Aestablishing%20it%20as%20a%20state-of-the-art%20approach%20in%20this%20domain%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatte%253A%2520Transfering%2520LLMs%2560%2520Latent-level%2520Knowledge%2520for%2520Few-shot%2520Tabular%250A%2520%2520Learning%26entry.906535625%3DRuxue%2520Shi%2520and%2520Hengrui%2520Gu%2520and%2520Hangting%2520Ye%2520and%2520Yiwei%2520Dai%2520and%2520Xu%2520Shen%2520and%2520Xin%2520Wang%26entry.1292438233%3D%2520%2520Few-shot%2520tabular%2520learning%252C%2520in%2520which%2520machine%2520learning%2520models%2520are%2520trained%2520with%250Aa%2520limited%2520amount%2520of%2520labeled%2520data%252C%2520provides%2520a%2520cost-effective%2520approach%2520to%250Aaddressing%2520real-world%2520challenges.%2520The%2520advent%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Ahas%2520sparked%2520interest%2520in%2520leveraging%2520their%2520pre-trained%2520knowledge%2520for%2520few-shot%250Atabular%2520learning.%2520Despite%2520promising%2520results%252C%2520existing%2520approaches%2520either%2520rely%2520on%250Atest-time%2520knowledge%2520extraction%252C%2520which%2520introduces%2520undesirable%2520latency%252C%2520or%250Atext-level%2520knowledge%252C%2520which%2520leads%2520to%2520unreliable%2520feature%2520engineering.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520propose%2520Latte%252C%2520a%2520training-time%2520knowledge%250Aextraction%2520framework%2520that%2520transfers%2520the%2520latent%2520prior%2520knowledge%2520within%2520LLMs%2520to%250Aoptimize%2520a%2520more%2520generalized%2520downstream%2520model.%2520Latte%2520enables%2520general%250Aknowledge-guided%2520downstream%2520tabular%2520learning%252C%2520facilitating%2520the%2520weighted%2520fusion%250Aof%2520information%2520across%2520different%2520feature%2520values%2520while%2520reducing%2520the%2520risk%2520of%250Aoverfitting%2520to%2520limited%2520labeled%2520data.%2520Furthermore%252C%2520Latte%2520is%2520compatible%2520with%250Aexisting%2520unsupervised%2520pre-training%2520paradigms%2520and%2520effectively%2520utilizes%2520available%250Aunlabeled%2520samples%2520to%2520overcome%2520the%2520performance%2520limitations%2520imposed%2520by%2520an%250Aextremely%2520small%2520labeled%2520dataset.%2520Extensive%2520experiments%2520on%2520various%2520few-shot%250Atabular%2520learning%2520benchmarks%2520demonstrate%2520the%2520superior%2520performance%2520of%2520Latte%252C%250Aestablishing%2520it%2520as%2520a%2520state-of-the-art%2520approach%2520in%2520this%2520domain%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latte%3A%20Transfering%20LLMs%60%20Latent-level%20Knowledge%20for%20Few-shot%20Tabular%0A%20%20Learning&entry.906535625=Ruxue%20Shi%20and%20Hengrui%20Gu%20and%20Hangting%20Ye%20and%20Yiwei%20Dai%20and%20Xu%20Shen%20and%20Xin%20Wang&entry.1292438233=%20%20Few-shot%20tabular%20learning%2C%20in%20which%20machine%20learning%20models%20are%20trained%20with%0Aa%20limited%20amount%20of%20labeled%20data%2C%20provides%20a%20cost-effective%20approach%20to%0Aaddressing%20real-world%20challenges.%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%0Ahas%20sparked%20interest%20in%20leveraging%20their%20pre-trained%20knowledge%20for%20few-shot%0Atabular%20learning.%20Despite%20promising%20results%2C%20existing%20approaches%20either%20rely%20on%0Atest-time%20knowledge%20extraction%2C%20which%20introduces%20undesirable%20latency%2C%20or%0Atext-level%20knowledge%2C%20which%20leads%20to%20unreliable%20feature%20engineering.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20Latte%2C%20a%20training-time%20knowledge%0Aextraction%20framework%20that%20transfers%20the%20latent%20prior%20knowledge%20within%20LLMs%20to%0Aoptimize%20a%20more%20generalized%20downstream%20model.%20Latte%20enables%20general%0Aknowledge-guided%20downstream%20tabular%20learning%2C%20facilitating%20the%20weighted%20fusion%0Aof%20information%20across%20different%20feature%20values%20while%20reducing%20the%20risk%20of%0Aoverfitting%20to%20limited%20labeled%20data.%20Furthermore%2C%20Latte%20is%20compatible%20with%0Aexisting%20unsupervised%20pre-training%20paradigms%20and%20effectively%20utilizes%20available%0Aunlabeled%20samples%20to%20overcome%20the%20performance%20limitations%20imposed%20by%20an%0Aextremely%20small%20labeled%20dataset.%20Extensive%20experiments%20on%20various%20few-shot%0Atabular%20learning%20benchmarks%20demonstrate%20the%20superior%20performance%20of%20Latte%2C%0Aestablishing%20it%20as%20a%20state-of-the-art%20approach%20in%20this%20domain%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05237v1&entry.124074799=Read"},
{"title": "Re-evaluating Open-ended Evaluation of Large Language Models", "author": "Siqi Liu and Ian Gemp and Luke Marris and Georgios Piliouras and Nicolas Heess and Marc Lanctot", "abstract": "  Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development.\n", "link": "http://arxiv.org/abs/2502.20170v2", "date": "2025-05-08", "relevancy": 2.0552, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-evaluating%20Open-ended%20Evaluation%20of%20Large%20Language%20Models&body=Title%3A%20Re-evaluating%20Open-ended%20Evaluation%20of%20Large%20Language%20Models%0AAuthor%3A%20Siqi%20Liu%20and%20Ian%20Gemp%20and%20Luke%20Marris%20and%20Georgios%20Piliouras%20and%20Nicolas%20Heess%20and%20Marc%20Lanctot%0AAbstract%3A%20%20%20Evaluation%20has%20traditionally%20focused%20on%20ranking%20candidates%20for%20a%20specific%0Askill.%20Modern%20generalist%20models%2C%20such%20as%20Large%20Language%20Models%20%28LLMs%29%2C%0Adecidedly%20outpace%20this%20paradigm.%20Open-ended%20evaluation%20systems%2C%20where%20candidate%0Amodels%20are%20compared%20on%20user-submitted%20prompts%2C%20have%20emerged%20as%20a%20popular%0Asolution.%20Despite%20their%20many%20advantages%2C%20we%20show%20that%20the%20current%20Elo-based%0Arating%20systems%20can%20be%20susceptible%20to%20and%20even%20reinforce%20biases%20in%20data%2C%0Aintentional%20or%20accidental%2C%20due%20to%20their%20sensitivity%20to%20redundancies.%20To%20address%0Athis%20issue%2C%20we%20propose%20evaluation%20as%20a%203-player%20game%2C%20and%20introduce%20novel%0Agame-theoretic%20solution%20concepts%20to%20ensure%20robustness%20to%20redundancy.%20We%20show%0Athat%20our%20method%20leads%20to%20intuitive%20ratings%20and%20provide%20insights%20into%20the%0Acompetitive%20landscape%20of%20LLM%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-evaluating%2520Open-ended%2520Evaluation%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DSiqi%2520Liu%2520and%2520Ian%2520Gemp%2520and%2520Luke%2520Marris%2520and%2520Georgios%2520Piliouras%2520and%2520Nicolas%2520Heess%2520and%2520Marc%2520Lanctot%26entry.1292438233%3D%2520%2520Evaluation%2520has%2520traditionally%2520focused%2520on%2520ranking%2520candidates%2520for%2520a%2520specific%250Askill.%2520Modern%2520generalist%2520models%252C%2520such%2520as%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Adecidedly%2520outpace%2520this%2520paradigm.%2520Open-ended%2520evaluation%2520systems%252C%2520where%2520candidate%250Amodels%2520are%2520compared%2520on%2520user-submitted%2520prompts%252C%2520have%2520emerged%2520as%2520a%2520popular%250Asolution.%2520Despite%2520their%2520many%2520advantages%252C%2520we%2520show%2520that%2520the%2520current%2520Elo-based%250Arating%2520systems%2520can%2520be%2520susceptible%2520to%2520and%2520even%2520reinforce%2520biases%2520in%2520data%252C%250Aintentional%2520or%2520accidental%252C%2520due%2520to%2520their%2520sensitivity%2520to%2520redundancies.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520evaluation%2520as%2520a%25203-player%2520game%252C%2520and%2520introduce%2520novel%250Agame-theoretic%2520solution%2520concepts%2520to%2520ensure%2520robustness%2520to%2520redundancy.%2520We%2520show%250Athat%2520our%2520method%2520leads%2520to%2520intuitive%2520ratings%2520and%2520provide%2520insights%2520into%2520the%250Acompetitive%2520landscape%2520of%2520LLM%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-evaluating%20Open-ended%20Evaluation%20of%20Large%20Language%20Models&entry.906535625=Siqi%20Liu%20and%20Ian%20Gemp%20and%20Luke%20Marris%20and%20Georgios%20Piliouras%20and%20Nicolas%20Heess%20and%20Marc%20Lanctot&entry.1292438233=%20%20Evaluation%20has%20traditionally%20focused%20on%20ranking%20candidates%20for%20a%20specific%0Askill.%20Modern%20generalist%20models%2C%20such%20as%20Large%20Language%20Models%20%28LLMs%29%2C%0Adecidedly%20outpace%20this%20paradigm.%20Open-ended%20evaluation%20systems%2C%20where%20candidate%0Amodels%20are%20compared%20on%20user-submitted%20prompts%2C%20have%20emerged%20as%20a%20popular%0Asolution.%20Despite%20their%20many%20advantages%2C%20we%20show%20that%20the%20current%20Elo-based%0Arating%20systems%20can%20be%20susceptible%20to%20and%20even%20reinforce%20biases%20in%20data%2C%0Aintentional%20or%20accidental%2C%20due%20to%20their%20sensitivity%20to%20redundancies.%20To%20address%0Athis%20issue%2C%20we%20propose%20evaluation%20as%20a%203-player%20game%2C%20and%20introduce%20novel%0Agame-theoretic%20solution%20concepts%20to%20ensure%20robustness%20to%20redundancy.%20We%20show%0Athat%20our%20method%20leads%20to%20intuitive%20ratings%20and%20provide%20insights%20into%20the%0Acompetitive%20landscape%20of%20LLM%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20170v2&entry.124074799=Read"},
{"title": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural\n  Networks", "author": "Kejie Zhao and Wenjia Hua and Aiersi Tuerhong and Luziwei Leng and Yuxin Ma and Qinghua Guo", "abstract": "  Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.\n", "link": "http://arxiv.org/abs/2505.05375v1", "date": "2025-05-08", "relevancy": 2.0541, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5297}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5027}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Threshold%20Modulation%20for%20Online%20Test-Time%20Adaptation%20of%20Spiking%20Neural%0A%20%20Networks&body=Title%3A%20Threshold%20Modulation%20for%20Online%20Test-Time%20Adaptation%20of%20Spiking%20Neural%0A%20%20Networks%0AAuthor%3A%20Kejie%20Zhao%20and%20Wenjia%20Hua%20and%20Aiersi%20Tuerhong%20and%20Luziwei%20Leng%20and%20Yuxin%20Ma%20and%20Qinghua%20Guo%0AAbstract%3A%20%20%20Recently%2C%20spiking%20neural%20networks%20%28SNNs%29%2C%20deployed%20on%20neuromorphic%20chips%2C%0Aprovide%20highly%20efficient%20solutions%20on%20edge%20devices%20in%20different%20scenarios.%0AHowever%2C%20their%20ability%20to%20adapt%20to%20distribution%20shifts%20after%20deployment%20has%0Abecome%20a%20crucial%20challenge.%20Online%20test-time%20adaptation%20%28OTTA%29%20offers%20a%0Apromising%20solution%20by%20enabling%20models%20to%20dynamically%20adjust%20to%20new%20data%0Adistributions%20without%20requiring%20source%20data%20or%20labeled%20target%20samples.%0ANevertheless%2C%20existing%20OTTA%20methods%20are%20largely%20designed%20for%20traditional%0Aartificial%20neural%20networks%20and%20are%20not%20well-suited%20for%20SNNs.%20To%20address%20this%0Agap%2C%20we%20propose%20a%20low-power%2C%20neuromorphic%20chip-friendly%20online%20test-time%0Aadaptation%20framework%2C%20aiming%20to%20enhance%20model%20generalization%20under%20distribution%0Ashifts.%20The%20proposed%20approach%20is%20called%20Threshold%20Modulation%20%28TM%29%2C%20which%0Adynamically%20adjusts%20the%20firing%20threshold%20through%20neuronal%20dynamics-inspired%0Anormalization%2C%20being%20more%20compatible%20with%20neuromorphic%20hardware.%20Experimental%0Aresults%20on%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20this%20method%20in%0Aimproving%20the%20robustness%20of%20SNNs%20against%20distribution%20shifts%20while%20maintaining%0Alow%20computational%20cost.%20The%20proposed%20method%20offers%20a%20practical%20solution%20for%0Aonline%20test-time%20adaptation%20of%20SNNs%2C%20providing%20inspiration%20for%20the%20design%20of%0Afuture%20neuromorphic%20chips.%20The%20demo%20code%20is%20available%20at%0Agithub.com/NneurotransmitterR/TM-OTTA-SNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThreshold%2520Modulation%2520for%2520Online%2520Test-Time%2520Adaptation%2520of%2520Spiking%2520Neural%250A%2520%2520Networks%26entry.906535625%3DKejie%2520Zhao%2520and%2520Wenjia%2520Hua%2520and%2520Aiersi%2520Tuerhong%2520and%2520Luziwei%2520Leng%2520and%2520Yuxin%2520Ma%2520and%2520Qinghua%2520Guo%26entry.1292438233%3D%2520%2520Recently%252C%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%252C%2520deployed%2520on%2520neuromorphic%2520chips%252C%250Aprovide%2520highly%2520efficient%2520solutions%2520on%2520edge%2520devices%2520in%2520different%2520scenarios.%250AHowever%252C%2520their%2520ability%2520to%2520adapt%2520to%2520distribution%2520shifts%2520after%2520deployment%2520has%250Abecome%2520a%2520crucial%2520challenge.%2520Online%2520test-time%2520adaptation%2520%2528OTTA%2529%2520offers%2520a%250Apromising%2520solution%2520by%2520enabling%2520models%2520to%2520dynamically%2520adjust%2520to%2520new%2520data%250Adistributions%2520without%2520requiring%2520source%2520data%2520or%2520labeled%2520target%2520samples.%250ANevertheless%252C%2520existing%2520OTTA%2520methods%2520are%2520largely%2520designed%2520for%2520traditional%250Aartificial%2520neural%2520networks%2520and%2520are%2520not%2520well-suited%2520for%2520SNNs.%2520To%2520address%2520this%250Agap%252C%2520we%2520propose%2520a%2520low-power%252C%2520neuromorphic%2520chip-friendly%2520online%2520test-time%250Aadaptation%2520framework%252C%2520aiming%2520to%2520enhance%2520model%2520generalization%2520under%2520distribution%250Ashifts.%2520The%2520proposed%2520approach%2520is%2520called%2520Threshold%2520Modulation%2520%2528TM%2529%252C%2520which%250Adynamically%2520adjusts%2520the%2520firing%2520threshold%2520through%2520neuronal%2520dynamics-inspired%250Anormalization%252C%2520being%2520more%2520compatible%2520with%2520neuromorphic%2520hardware.%2520Experimental%250Aresults%2520on%2520benchmark%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520method%2520in%250Aimproving%2520the%2520robustness%2520of%2520SNNs%2520against%2520distribution%2520shifts%2520while%2520maintaining%250Alow%2520computational%2520cost.%2520The%2520proposed%2520method%2520offers%2520a%2520practical%2520solution%2520for%250Aonline%2520test-time%2520adaptation%2520of%2520SNNs%252C%2520providing%2520inspiration%2520for%2520the%2520design%2520of%250Afuture%2520neuromorphic%2520chips.%2520The%2520demo%2520code%2520is%2520available%2520at%250Agithub.com/NneurotransmitterR/TM-OTTA-SNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Threshold%20Modulation%20for%20Online%20Test-Time%20Adaptation%20of%20Spiking%20Neural%0A%20%20Networks&entry.906535625=Kejie%20Zhao%20and%20Wenjia%20Hua%20and%20Aiersi%20Tuerhong%20and%20Luziwei%20Leng%20and%20Yuxin%20Ma%20and%20Qinghua%20Guo&entry.1292438233=%20%20Recently%2C%20spiking%20neural%20networks%20%28SNNs%29%2C%20deployed%20on%20neuromorphic%20chips%2C%0Aprovide%20highly%20efficient%20solutions%20on%20edge%20devices%20in%20different%20scenarios.%0AHowever%2C%20their%20ability%20to%20adapt%20to%20distribution%20shifts%20after%20deployment%20has%0Abecome%20a%20crucial%20challenge.%20Online%20test-time%20adaptation%20%28OTTA%29%20offers%20a%0Apromising%20solution%20by%20enabling%20models%20to%20dynamically%20adjust%20to%20new%20data%0Adistributions%20without%20requiring%20source%20data%20or%20labeled%20target%20samples.%0ANevertheless%2C%20existing%20OTTA%20methods%20are%20largely%20designed%20for%20traditional%0Aartificial%20neural%20networks%20and%20are%20not%20well-suited%20for%20SNNs.%20To%20address%20this%0Agap%2C%20we%20propose%20a%20low-power%2C%20neuromorphic%20chip-friendly%20online%20test-time%0Aadaptation%20framework%2C%20aiming%20to%20enhance%20model%20generalization%20under%20distribution%0Ashifts.%20The%20proposed%20approach%20is%20called%20Threshold%20Modulation%20%28TM%29%2C%20which%0Adynamically%20adjusts%20the%20firing%20threshold%20through%20neuronal%20dynamics-inspired%0Anormalization%2C%20being%20more%20compatible%20with%20neuromorphic%20hardware.%20Experimental%0Aresults%20on%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20this%20method%20in%0Aimproving%20the%20robustness%20of%20SNNs%20against%20distribution%20shifts%20while%20maintaining%0Alow%20computational%20cost.%20The%20proposed%20method%20offers%20a%20practical%20solution%20for%0Aonline%20test-time%20adaptation%20of%20SNNs%2C%20providing%20inspiration%20for%20the%20design%20of%0Afuture%20neuromorphic%20chips.%20The%20demo%20code%20is%20available%20at%0Agithub.com/NneurotransmitterR/TM-OTTA-SNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05375v1&entry.124074799=Read"},
{"title": "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models", "author": "Zhouliang Yu and Yuhuan Yuan and Tim Z. Xiao and Fuxiang Frank Xia and Jie Fu and Ge Zhang and Ge Lin and Weiyang Liu", "abstract": "  Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domains, achieving over 50\\%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks.\n", "link": "http://arxiv.org/abs/2502.04728v2", "date": "2025-05-08", "relevancy": 2.0488, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Symbolic%20World%20Models%20via%20Test-time%20Scaling%20of%20Large%20Language%0A%20%20Models&body=Title%3A%20Generating%20Symbolic%20World%20Models%20via%20Test-time%20Scaling%20of%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Zhouliang%20Yu%20and%20Yuhuan%20Yuan%20and%20Tim%20Z.%20Xiao%20and%20Fuxiang%20Frank%20Xia%20and%20Jie%20Fu%20and%20Ge%20Zhang%20and%20Ge%20Lin%20and%20Weiyang%20Liu%0AAbstract%3A%20%20%20Solving%20complex%20planning%20problems%20requires%20Large%20Language%20Models%20%28LLMs%29%20to%0Aexplicitly%20model%20the%20state%20transition%20to%20avoid%20rule%20violations%2C%20comply%20with%0Aconstraints%2C%20and%20ensure%20optimality-a%20task%20hindered%20by%20the%20inherent%20ambiguity%20of%0Anatural%20language.%20To%20overcome%20such%20ambiguity%2C%20Planning%20Domain%20Definition%0ALanguage%20%28PDDL%29%20is%20leveraged%20as%20a%20planning%20abstraction%20that%20enables%20precise%20and%0Aformal%20state%20descriptions.%20With%20PDDL%2C%20we%20can%20generate%20a%20symbolic%20world%20model%0Awhere%20classic%20searching%20algorithms%2C%20such%20as%20A%2A%2C%20can%20be%20seamlessly%20applied%20to%0Afind%20optimal%20plans.%20However%2C%20directly%20generating%20PDDL%20domains%20with%20current%20LLMs%0Aremains%20an%20open%20challenge%20due%20to%20the%20lack%20of%20PDDL%20training%20data.%20To%20address%0Athis%20challenge%2C%20we%20propose%20to%20scale%20up%20the%20test-time%20computation%20of%20LLMs%20to%0Aenhance%20their%20PDDL%20reasoning%20capabilities%2C%20thereby%20enabling%20the%20generation%20of%0Ahigh-quality%20PDDL%20domains.%20Specifically%2C%20we%20introduce%20a%20simple%20yet%20effective%0Aalgorithm%2C%20which%20first%20employs%20a%20Best-of-N%20sampling%20approach%20to%20improve%20the%0Aquality%20of%20the%20initial%20solution%20and%20then%20refines%20the%20solution%20in%20a%20fine-grained%0Amanner%20with%20verbalized%20machine%20learning.%20Our%20method%20outperforms%20o1-mini%20by%20a%0Aconsiderable%20margin%20in%20the%20generation%20of%20PDDL%20domains%2C%20achieving%20over%2050%5C%25%0Asuccess%20rate%20on%20two%20tasks%20%28i.e.%2C%20generating%20PDDL%20domains%20from%20natural%20language%0Adescription%20or%20PDDL%20problems%29.%20This%20is%20done%20without%20requiring%20additional%0Atraining.%20By%20taking%20advantage%20of%20PDDL%20as%20state%20abstraction%2C%20our%20method%20is%20able%0Ato%20outperform%20current%20state-of-the-art%20methods%20on%20almost%20all%20competition-level%0Aplanning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04728v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Symbolic%2520World%2520Models%2520via%2520Test-time%2520Scaling%2520of%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DZhouliang%2520Yu%2520and%2520Yuhuan%2520Yuan%2520and%2520Tim%2520Z.%2520Xiao%2520and%2520Fuxiang%2520Frank%2520Xia%2520and%2520Jie%2520Fu%2520and%2520Ge%2520Zhang%2520and%2520Ge%2520Lin%2520and%2520Weiyang%2520Liu%26entry.1292438233%3D%2520%2520Solving%2520complex%2520planning%2520problems%2520requires%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Aexplicitly%2520model%2520the%2520state%2520transition%2520to%2520avoid%2520rule%2520violations%252C%2520comply%2520with%250Aconstraints%252C%2520and%2520ensure%2520optimality-a%2520task%2520hindered%2520by%2520the%2520inherent%2520ambiguity%2520of%250Anatural%2520language.%2520To%2520overcome%2520such%2520ambiguity%252C%2520Planning%2520Domain%2520Definition%250ALanguage%2520%2528PDDL%2529%2520is%2520leveraged%2520as%2520a%2520planning%2520abstraction%2520that%2520enables%2520precise%2520and%250Aformal%2520state%2520descriptions.%2520With%2520PDDL%252C%2520we%2520can%2520generate%2520a%2520symbolic%2520world%2520model%250Awhere%2520classic%2520searching%2520algorithms%252C%2520such%2520as%2520A%252A%252C%2520can%2520be%2520seamlessly%2520applied%2520to%250Afind%2520optimal%2520plans.%2520However%252C%2520directly%2520generating%2520PDDL%2520domains%2520with%2520current%2520LLMs%250Aremains%2520an%2520open%2520challenge%2520due%2520to%2520the%2520lack%2520of%2520PDDL%2520training%2520data.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520to%2520scale%2520up%2520the%2520test-time%2520computation%2520of%2520LLMs%2520to%250Aenhance%2520their%2520PDDL%2520reasoning%2520capabilities%252C%2520thereby%2520enabling%2520the%2520generation%2520of%250Ahigh-quality%2520PDDL%2520domains.%2520Specifically%252C%2520we%2520introduce%2520a%2520simple%2520yet%2520effective%250Aalgorithm%252C%2520which%2520first%2520employs%2520a%2520Best-of-N%2520sampling%2520approach%2520to%2520improve%2520the%250Aquality%2520of%2520the%2520initial%2520solution%2520and%2520then%2520refines%2520the%2520solution%2520in%2520a%2520fine-grained%250Amanner%2520with%2520verbalized%2520machine%2520learning.%2520Our%2520method%2520outperforms%2520o1-mini%2520by%2520a%250Aconsiderable%2520margin%2520in%2520the%2520generation%2520of%2520PDDL%2520domains%252C%2520achieving%2520over%252050%255C%2525%250Asuccess%2520rate%2520on%2520two%2520tasks%2520%2528i.e.%252C%2520generating%2520PDDL%2520domains%2520from%2520natural%2520language%250Adescription%2520or%2520PDDL%2520problems%2529.%2520This%2520is%2520done%2520without%2520requiring%2520additional%250Atraining.%2520By%2520taking%2520advantage%2520of%2520PDDL%2520as%2520state%2520abstraction%252C%2520our%2520method%2520is%2520able%250Ato%2520outperform%2520current%2520state-of-the-art%2520methods%2520on%2520almost%2520all%2520competition-level%250Aplanning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04728v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Symbolic%20World%20Models%20via%20Test-time%20Scaling%20of%20Large%20Language%0A%20%20Models&entry.906535625=Zhouliang%20Yu%20and%20Yuhuan%20Yuan%20and%20Tim%20Z.%20Xiao%20and%20Fuxiang%20Frank%20Xia%20and%20Jie%20Fu%20and%20Ge%20Zhang%20and%20Ge%20Lin%20and%20Weiyang%20Liu&entry.1292438233=%20%20Solving%20complex%20planning%20problems%20requires%20Large%20Language%20Models%20%28LLMs%29%20to%0Aexplicitly%20model%20the%20state%20transition%20to%20avoid%20rule%20violations%2C%20comply%20with%0Aconstraints%2C%20and%20ensure%20optimality-a%20task%20hindered%20by%20the%20inherent%20ambiguity%20of%0Anatural%20language.%20To%20overcome%20such%20ambiguity%2C%20Planning%20Domain%20Definition%0ALanguage%20%28PDDL%29%20is%20leveraged%20as%20a%20planning%20abstraction%20that%20enables%20precise%20and%0Aformal%20state%20descriptions.%20With%20PDDL%2C%20we%20can%20generate%20a%20symbolic%20world%20model%0Awhere%20classic%20searching%20algorithms%2C%20such%20as%20A%2A%2C%20can%20be%20seamlessly%20applied%20to%0Afind%20optimal%20plans.%20However%2C%20directly%20generating%20PDDL%20domains%20with%20current%20LLMs%0Aremains%20an%20open%20challenge%20due%20to%20the%20lack%20of%20PDDL%20training%20data.%20To%20address%0Athis%20challenge%2C%20we%20propose%20to%20scale%20up%20the%20test-time%20computation%20of%20LLMs%20to%0Aenhance%20their%20PDDL%20reasoning%20capabilities%2C%20thereby%20enabling%20the%20generation%20of%0Ahigh-quality%20PDDL%20domains.%20Specifically%2C%20we%20introduce%20a%20simple%20yet%20effective%0Aalgorithm%2C%20which%20first%20employs%20a%20Best-of-N%20sampling%20approach%20to%20improve%20the%0Aquality%20of%20the%20initial%20solution%20and%20then%20refines%20the%20solution%20in%20a%20fine-grained%0Amanner%20with%20verbalized%20machine%20learning.%20Our%20method%20outperforms%20o1-mini%20by%20a%0Aconsiderable%20margin%20in%20the%20generation%20of%20PDDL%20domains%2C%20achieving%20over%2050%5C%25%0Asuccess%20rate%20on%20two%20tasks%20%28i.e.%2C%20generating%20PDDL%20domains%20from%20natural%20language%0Adescription%20or%20PDDL%20problems%29.%20This%20is%20done%20without%20requiring%20additional%0Atraining.%20By%20taking%20advantage%20of%20PDDL%20as%20state%20abstraction%2C%20our%20method%20is%20able%0Ato%20outperform%20current%20state-of-the-art%20methods%20on%20almost%20all%20competition-level%0Aplanning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04728v2&entry.124074799=Read"},
{"title": "FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data\n  Preparation via Federated Learning", "author": "Zhihao Zeng and Ziquan Fang and Wei Shao and Lu Chen and Yunjun Gao", "abstract": "  Trajectory data, which capture the movement patterns of people and vehicles\nover time and space, are crucial for applications like traffic optimization and\nurban planning. However, issues such as noise and incompleteness often\ncompromise data quality, leading to inaccurate trajectory analyses and limiting\nthe potential of these applications. While Trajectory Data Preparation (TDP)\ncan enhance data quality, existing methods suffer from two key limitations: (i)\nthey do not address data privacy concerns, particularly in federated settings\nwhere trajectory data sharing is prohibited, and (ii) they typically design\ntask-specific models that lack generalizability across diverse TDP scenarios.\nTo overcome these challenges, we propose FedTDP, a privacy-preserving and\nunified framework that leverages the capabilities of Large Language Models\n(LLMs) for TDP in federated environments. Specifically, we: (i) design a\ntrajectory privacy autoencoder to secure data transmission and protect privacy,\n(ii) introduce a trajectory knowledge enhancer to improve model learning of\nTDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)\npropose federated parallel optimization to enhance training efficiency by\nreducing data transmission and enabling parallel model training. Experiments on\n6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP\nconsistently outperforms 13 state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2505.05155v1", "date": "2025-05-08", "relevancy": 2.0381, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5295}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5101}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedTDP%3A%20A%20Privacy-Preserving%20and%20Unified%20Framework%20for%20Trajectory%20Data%0A%20%20Preparation%20via%20Federated%20Learning&body=Title%3A%20FedTDP%3A%20A%20Privacy-Preserving%20and%20Unified%20Framework%20for%20Trajectory%20Data%0A%20%20Preparation%20via%20Federated%20Learning%0AAuthor%3A%20Zhihao%20Zeng%20and%20Ziquan%20Fang%20and%20Wei%20Shao%20and%20Lu%20Chen%20and%20Yunjun%20Gao%0AAbstract%3A%20%20%20Trajectory%20data%2C%20which%20capture%20the%20movement%20patterns%20of%20people%20and%20vehicles%0Aover%20time%20and%20space%2C%20are%20crucial%20for%20applications%20like%20traffic%20optimization%20and%0Aurban%20planning.%20However%2C%20issues%20such%20as%20noise%20and%20incompleteness%20often%0Acompromise%20data%20quality%2C%20leading%20to%20inaccurate%20trajectory%20analyses%20and%20limiting%0Athe%20potential%20of%20these%20applications.%20While%20Trajectory%20Data%20Preparation%20%28TDP%29%0Acan%20enhance%20data%20quality%2C%20existing%20methods%20suffer%20from%20two%20key%20limitations%3A%20%28i%29%0Athey%20do%20not%20address%20data%20privacy%20concerns%2C%20particularly%20in%20federated%20settings%0Awhere%20trajectory%20data%20sharing%20is%20prohibited%2C%20and%20%28ii%29%20they%20typically%20design%0Atask-specific%20models%20that%20lack%20generalizability%20across%20diverse%20TDP%20scenarios.%0ATo%20overcome%20these%20challenges%2C%20we%20propose%20FedTDP%2C%20a%20privacy-preserving%20and%0Aunified%20framework%20that%20leverages%20the%20capabilities%20of%20Large%20Language%20Models%0A%28LLMs%29%20for%20TDP%20in%20federated%20environments.%20Specifically%2C%20we%3A%20%28i%29%20design%20a%0Atrajectory%20privacy%20autoencoder%20to%20secure%20data%20transmission%20and%20protect%20privacy%2C%0A%28ii%29%20introduce%20a%20trajectory%20knowledge%20enhancer%20to%20improve%20model%20learning%20of%0ATDP-related%20knowledge%2C%20enabling%20the%20development%20of%20TDP-oriented%20LLMs%2C%20and%20%28iii%29%0Apropose%20federated%20parallel%20optimization%20to%20enhance%20training%20efficiency%20by%0Areducing%20data%20transmission%20and%20enabling%20parallel%20model%20training.%20Experiments%20on%0A6%20real%20datasets%20and%2010%20mainstream%20TDP%20tasks%20demonstrate%20that%20FedTDP%0Aconsistently%20outperforms%2013%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedTDP%253A%2520A%2520Privacy-Preserving%2520and%2520Unified%2520Framework%2520for%2520Trajectory%2520Data%250A%2520%2520Preparation%2520via%2520Federated%2520Learning%26entry.906535625%3DZhihao%2520Zeng%2520and%2520Ziquan%2520Fang%2520and%2520Wei%2520Shao%2520and%2520Lu%2520Chen%2520and%2520Yunjun%2520Gao%26entry.1292438233%3D%2520%2520Trajectory%2520data%252C%2520which%2520capture%2520the%2520movement%2520patterns%2520of%2520people%2520and%2520vehicles%250Aover%2520time%2520and%2520space%252C%2520are%2520crucial%2520for%2520applications%2520like%2520traffic%2520optimization%2520and%250Aurban%2520planning.%2520However%252C%2520issues%2520such%2520as%2520noise%2520and%2520incompleteness%2520often%250Acompromise%2520data%2520quality%252C%2520leading%2520to%2520inaccurate%2520trajectory%2520analyses%2520and%2520limiting%250Athe%2520potential%2520of%2520these%2520applications.%2520While%2520Trajectory%2520Data%2520Preparation%2520%2528TDP%2529%250Acan%2520enhance%2520data%2520quality%252C%2520existing%2520methods%2520suffer%2520from%2520two%2520key%2520limitations%253A%2520%2528i%2529%250Athey%2520do%2520not%2520address%2520data%2520privacy%2520concerns%252C%2520particularly%2520in%2520federated%2520settings%250Awhere%2520trajectory%2520data%2520sharing%2520is%2520prohibited%252C%2520and%2520%2528ii%2529%2520they%2520typically%2520design%250Atask-specific%2520models%2520that%2520lack%2520generalizability%2520across%2520diverse%2520TDP%2520scenarios.%250ATo%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520FedTDP%252C%2520a%2520privacy-preserving%2520and%250Aunified%2520framework%2520that%2520leverages%2520the%2520capabilities%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520for%2520TDP%2520in%2520federated%2520environments.%2520Specifically%252C%2520we%253A%2520%2528i%2529%2520design%2520a%250Atrajectory%2520privacy%2520autoencoder%2520to%2520secure%2520data%2520transmission%2520and%2520protect%2520privacy%252C%250A%2528ii%2529%2520introduce%2520a%2520trajectory%2520knowledge%2520enhancer%2520to%2520improve%2520model%2520learning%2520of%250ATDP-related%2520knowledge%252C%2520enabling%2520the%2520development%2520of%2520TDP-oriented%2520LLMs%252C%2520and%2520%2528iii%2529%250Apropose%2520federated%2520parallel%2520optimization%2520to%2520enhance%2520training%2520efficiency%2520by%250Areducing%2520data%2520transmission%2520and%2520enabling%2520parallel%2520model%2520training.%2520Experiments%2520on%250A6%2520real%2520datasets%2520and%252010%2520mainstream%2520TDP%2520tasks%2520demonstrate%2520that%2520FedTDP%250Aconsistently%2520outperforms%252013%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedTDP%3A%20A%20Privacy-Preserving%20and%20Unified%20Framework%20for%20Trajectory%20Data%0A%20%20Preparation%20via%20Federated%20Learning&entry.906535625=Zhihao%20Zeng%20and%20Ziquan%20Fang%20and%20Wei%20Shao%20and%20Lu%20Chen%20and%20Yunjun%20Gao&entry.1292438233=%20%20Trajectory%20data%2C%20which%20capture%20the%20movement%20patterns%20of%20people%20and%20vehicles%0Aover%20time%20and%20space%2C%20are%20crucial%20for%20applications%20like%20traffic%20optimization%20and%0Aurban%20planning.%20However%2C%20issues%20such%20as%20noise%20and%20incompleteness%20often%0Acompromise%20data%20quality%2C%20leading%20to%20inaccurate%20trajectory%20analyses%20and%20limiting%0Athe%20potential%20of%20these%20applications.%20While%20Trajectory%20Data%20Preparation%20%28TDP%29%0Acan%20enhance%20data%20quality%2C%20existing%20methods%20suffer%20from%20two%20key%20limitations%3A%20%28i%29%0Athey%20do%20not%20address%20data%20privacy%20concerns%2C%20particularly%20in%20federated%20settings%0Awhere%20trajectory%20data%20sharing%20is%20prohibited%2C%20and%20%28ii%29%20they%20typically%20design%0Atask-specific%20models%20that%20lack%20generalizability%20across%20diverse%20TDP%20scenarios.%0ATo%20overcome%20these%20challenges%2C%20we%20propose%20FedTDP%2C%20a%20privacy-preserving%20and%0Aunified%20framework%20that%20leverages%20the%20capabilities%20of%20Large%20Language%20Models%0A%28LLMs%29%20for%20TDP%20in%20federated%20environments.%20Specifically%2C%20we%3A%20%28i%29%20design%20a%0Atrajectory%20privacy%20autoencoder%20to%20secure%20data%20transmission%20and%20protect%20privacy%2C%0A%28ii%29%20introduce%20a%20trajectory%20knowledge%20enhancer%20to%20improve%20model%20learning%20of%0ATDP-related%20knowledge%2C%20enabling%20the%20development%20of%20TDP-oriented%20LLMs%2C%20and%20%28iii%29%0Apropose%20federated%20parallel%20optimization%20to%20enhance%20training%20efficiency%20by%0Areducing%20data%20transmission%20and%20enabling%20parallel%20model%20training.%20Experiments%20on%0A6%20real%20datasets%20and%2010%20mainstream%20TDP%20tasks%20demonstrate%20that%20FedTDP%0Aconsistently%20outperforms%2013%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05155v1&entry.124074799=Read"},
{"title": "Adaptive Attention-Based Model for 5G Radio-based Outdoor Localization", "author": "Ilayda Yaman and Guoda Tian and Dino Pjanic and Fredrik Tufvesson and Ove Edfors and Zhengya Zhang and Liang Liu", "abstract": "  Radio-based localization in dynamic environments, such as urban and vehicular\nsettings, requires systems that efficiently adapt to varying signal conditions\nand environmental changes. Factors like multipath interference and obstructions\nintroduce different levels of complexity that affect the accuracy of the\nlocalization. Although generalized models offer broad applicability, they often\nstruggle to capture the nuances of specific environments, leading to suboptimal\nperformance in real-world deployments. In contrast, specialized models can be\ntailored to particular conditions, enabling more precise localization by\neffectively handling domain-specific variations, which also results in reduced\nexecution time and smaller model size. However, deploying multiple specialized\nmodels requires an efficient mechanism to select the most appropriate one for a\ngiven scenario. In this work, we develop an adaptive localization framework\nthat combines shallow attention-based models with a router/switching mechanism\nbased on a single-layer perceptron. This enables seamless transitions between\nspecialized localization models optimized for different conditions, balancing\naccuracy and computational complexity. We design three low-complex models\ntailored for distinct scenarios, and a router that dynamically selects the most\nsuitable model based on real-time input characteristics. The proposed framework\nis validated using real-world vehicle localization data collected from a\nmassive MIMO base station and compared to more general models.\n", "link": "http://arxiv.org/abs/2503.23810v2", "date": "2025-05-08", "relevancy": 2.0338, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5171}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5093}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Attention-Based%20Model%20for%205G%20Radio-based%20Outdoor%20Localization&body=Title%3A%20Adaptive%20Attention-Based%20Model%20for%205G%20Radio-based%20Outdoor%20Localization%0AAuthor%3A%20Ilayda%20Yaman%20and%20Guoda%20Tian%20and%20Dino%20Pjanic%20and%20Fredrik%20Tufvesson%20and%20Ove%20Edfors%20and%20Zhengya%20Zhang%20and%20Liang%20Liu%0AAbstract%3A%20%20%20Radio-based%20localization%20in%20dynamic%20environments%2C%20such%20as%20urban%20and%20vehicular%0Asettings%2C%20requires%20systems%20that%20efficiently%20adapt%20to%20varying%20signal%20conditions%0Aand%20environmental%20changes.%20Factors%20like%20multipath%20interference%20and%20obstructions%0Aintroduce%20different%20levels%20of%20complexity%20that%20affect%20the%20accuracy%20of%20the%0Alocalization.%20Although%20generalized%20models%20offer%20broad%20applicability%2C%20they%20often%0Astruggle%20to%20capture%20the%20nuances%20of%20specific%20environments%2C%20leading%20to%20suboptimal%0Aperformance%20in%20real-world%20deployments.%20In%20contrast%2C%20specialized%20models%20can%20be%0Atailored%20to%20particular%20conditions%2C%20enabling%20more%20precise%20localization%20by%0Aeffectively%20handling%20domain-specific%20variations%2C%20which%20also%20results%20in%20reduced%0Aexecution%20time%20and%20smaller%20model%20size.%20However%2C%20deploying%20multiple%20specialized%0Amodels%20requires%20an%20efficient%20mechanism%20to%20select%20the%20most%20appropriate%20one%20for%20a%0Agiven%20scenario.%20In%20this%20work%2C%20we%20develop%20an%20adaptive%20localization%20framework%0Athat%20combines%20shallow%20attention-based%20models%20with%20a%20router/switching%20mechanism%0Abased%20on%20a%20single-layer%20perceptron.%20This%20enables%20seamless%20transitions%20between%0Aspecialized%20localization%20models%20optimized%20for%20different%20conditions%2C%20balancing%0Aaccuracy%20and%20computational%20complexity.%20We%20design%20three%20low-complex%20models%0Atailored%20for%20distinct%20scenarios%2C%20and%20a%20router%20that%20dynamically%20selects%20the%20most%0Asuitable%20model%20based%20on%20real-time%20input%20characteristics.%20The%20proposed%20framework%0Ais%20validated%20using%20real-world%20vehicle%20localization%20data%20collected%20from%20a%0Amassive%20MIMO%20base%20station%20and%20compared%20to%20more%20general%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Attention-Based%2520Model%2520for%25205G%2520Radio-based%2520Outdoor%2520Localization%26entry.906535625%3DIlayda%2520Yaman%2520and%2520Guoda%2520Tian%2520and%2520Dino%2520Pjanic%2520and%2520Fredrik%2520Tufvesson%2520and%2520Ove%2520Edfors%2520and%2520Zhengya%2520Zhang%2520and%2520Liang%2520Liu%26entry.1292438233%3D%2520%2520Radio-based%2520localization%2520in%2520dynamic%2520environments%252C%2520such%2520as%2520urban%2520and%2520vehicular%250Asettings%252C%2520requires%2520systems%2520that%2520efficiently%2520adapt%2520to%2520varying%2520signal%2520conditions%250Aand%2520environmental%2520changes.%2520Factors%2520like%2520multipath%2520interference%2520and%2520obstructions%250Aintroduce%2520different%2520levels%2520of%2520complexity%2520that%2520affect%2520the%2520accuracy%2520of%2520the%250Alocalization.%2520Although%2520generalized%2520models%2520offer%2520broad%2520applicability%252C%2520they%2520often%250Astruggle%2520to%2520capture%2520the%2520nuances%2520of%2520specific%2520environments%252C%2520leading%2520to%2520suboptimal%250Aperformance%2520in%2520real-world%2520deployments.%2520In%2520contrast%252C%2520specialized%2520models%2520can%2520be%250Atailored%2520to%2520particular%2520conditions%252C%2520enabling%2520more%2520precise%2520localization%2520by%250Aeffectively%2520handling%2520domain-specific%2520variations%252C%2520which%2520also%2520results%2520in%2520reduced%250Aexecution%2520time%2520and%2520smaller%2520model%2520size.%2520However%252C%2520deploying%2520multiple%2520specialized%250Amodels%2520requires%2520an%2520efficient%2520mechanism%2520to%2520select%2520the%2520most%2520appropriate%2520one%2520for%2520a%250Agiven%2520scenario.%2520In%2520this%2520work%252C%2520we%2520develop%2520an%2520adaptive%2520localization%2520framework%250Athat%2520combines%2520shallow%2520attention-based%2520models%2520with%2520a%2520router/switching%2520mechanism%250Abased%2520on%2520a%2520single-layer%2520perceptron.%2520This%2520enables%2520seamless%2520transitions%2520between%250Aspecialized%2520localization%2520models%2520optimized%2520for%2520different%2520conditions%252C%2520balancing%250Aaccuracy%2520and%2520computational%2520complexity.%2520We%2520design%2520three%2520low-complex%2520models%250Atailored%2520for%2520distinct%2520scenarios%252C%2520and%2520a%2520router%2520that%2520dynamically%2520selects%2520the%2520most%250Asuitable%2520model%2520based%2520on%2520real-time%2520input%2520characteristics.%2520The%2520proposed%2520framework%250Ais%2520validated%2520using%2520real-world%2520vehicle%2520localization%2520data%2520collected%2520from%2520a%250Amassive%2520MIMO%2520base%2520station%2520and%2520compared%2520to%2520more%2520general%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Attention-Based%20Model%20for%205G%20Radio-based%20Outdoor%20Localization&entry.906535625=Ilayda%20Yaman%20and%20Guoda%20Tian%20and%20Dino%20Pjanic%20and%20Fredrik%20Tufvesson%20and%20Ove%20Edfors%20and%20Zhengya%20Zhang%20and%20Liang%20Liu&entry.1292438233=%20%20Radio-based%20localization%20in%20dynamic%20environments%2C%20such%20as%20urban%20and%20vehicular%0Asettings%2C%20requires%20systems%20that%20efficiently%20adapt%20to%20varying%20signal%20conditions%0Aand%20environmental%20changes.%20Factors%20like%20multipath%20interference%20and%20obstructions%0Aintroduce%20different%20levels%20of%20complexity%20that%20affect%20the%20accuracy%20of%20the%0Alocalization.%20Although%20generalized%20models%20offer%20broad%20applicability%2C%20they%20often%0Astruggle%20to%20capture%20the%20nuances%20of%20specific%20environments%2C%20leading%20to%20suboptimal%0Aperformance%20in%20real-world%20deployments.%20In%20contrast%2C%20specialized%20models%20can%20be%0Atailored%20to%20particular%20conditions%2C%20enabling%20more%20precise%20localization%20by%0Aeffectively%20handling%20domain-specific%20variations%2C%20which%20also%20results%20in%20reduced%0Aexecution%20time%20and%20smaller%20model%20size.%20However%2C%20deploying%20multiple%20specialized%0Amodels%20requires%20an%20efficient%20mechanism%20to%20select%20the%20most%20appropriate%20one%20for%20a%0Agiven%20scenario.%20In%20this%20work%2C%20we%20develop%20an%20adaptive%20localization%20framework%0Athat%20combines%20shallow%20attention-based%20models%20with%20a%20router/switching%20mechanism%0Abased%20on%20a%20single-layer%20perceptron.%20This%20enables%20seamless%20transitions%20between%0Aspecialized%20localization%20models%20optimized%20for%20different%20conditions%2C%20balancing%0Aaccuracy%20and%20computational%20complexity.%20We%20design%20three%20low-complex%20models%0Atailored%20for%20distinct%20scenarios%2C%20and%20a%20router%20that%20dynamically%20selects%20the%20most%0Asuitable%20model%20based%20on%20real-time%20input%20characteristics.%20The%20proposed%20framework%0Ais%20validated%20using%20real-world%20vehicle%20localization%20data%20collected%20from%20a%0Amassive%20MIMO%20base%20station%20and%20compared%20to%20more%20general%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23810v2&entry.124074799=Read"},
{"title": "Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction", "author": "Rafayel Mkrtchyan and Edvard Ghukasyan and Khoren Petrosyan and Hrant Khachatrian and Theofanis P. Raptis", "abstract": "  Indoor pathloss prediction is a fundamental task in wireless network\nplanning, yet it remains challenging due to environmental complexity and data\nscarcity. In this work, we propose a deep learning-based approach utilizing a\nvision transformer (ViT) architecture with DINO-v2 pretrained weights to model\nindoor radio propagation. Our method processes a floor map with additional\nfeatures of the walls to generate indoor pathloss maps. We systematically\nevaluate the effects of architectural choices, data augmentation strategies,\nand feature engineering techniques. Our findings indicate that extensive\naugmentation significantly improves generalization, while feature engineering\nis crucial in low-data regimes. Through comprehensive experiments, we\ndemonstrate the robustness of our model across different generalization\nscenarios.\n", "link": "http://arxiv.org/abs/2412.09507v2", "date": "2025-05-08", "relevancy": 2.0336, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Transformers%20for%20Efficient%20Indoor%20Pathloss%20Radio%20Map%20Prediction&body=Title%3A%20Vision%20Transformers%20for%20Efficient%20Indoor%20Pathloss%20Radio%20Map%20Prediction%0AAuthor%3A%20Rafayel%20Mkrtchyan%20and%20Edvard%20Ghukasyan%20and%20Khoren%20Petrosyan%20and%20Hrant%20Khachatrian%20and%20Theofanis%20P.%20Raptis%0AAbstract%3A%20%20%20Indoor%20pathloss%20prediction%20is%20a%20fundamental%20task%20in%20wireless%20network%0Aplanning%2C%20yet%20it%20remains%20challenging%20due%20to%20environmental%20complexity%20and%20data%0Ascarcity.%20In%20this%20work%2C%20we%20propose%20a%20deep%20learning-based%20approach%20utilizing%20a%0Avision%20transformer%20%28ViT%29%20architecture%20with%20DINO-v2%20pretrained%20weights%20to%20model%0Aindoor%20radio%20propagation.%20Our%20method%20processes%20a%20floor%20map%20with%20additional%0Afeatures%20of%20the%20walls%20to%20generate%20indoor%20pathloss%20maps.%20We%20systematically%0Aevaluate%20the%20effects%20of%20architectural%20choices%2C%20data%20augmentation%20strategies%2C%0Aand%20feature%20engineering%20techniques.%20Our%20findings%20indicate%20that%20extensive%0Aaugmentation%20significantly%20improves%20generalization%2C%20while%20feature%20engineering%0Ais%20crucial%20in%20low-data%20regimes.%20Through%20comprehensive%20experiments%2C%20we%0Ademonstrate%20the%20robustness%20of%20our%20model%20across%20different%20generalization%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09507v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Transformers%2520for%2520Efficient%2520Indoor%2520Pathloss%2520Radio%2520Map%2520Prediction%26entry.906535625%3DRafayel%2520Mkrtchyan%2520and%2520Edvard%2520Ghukasyan%2520and%2520Khoren%2520Petrosyan%2520and%2520Hrant%2520Khachatrian%2520and%2520Theofanis%2520P.%2520Raptis%26entry.1292438233%3D%2520%2520Indoor%2520pathloss%2520prediction%2520is%2520a%2520fundamental%2520task%2520in%2520wireless%2520network%250Aplanning%252C%2520yet%2520it%2520remains%2520challenging%2520due%2520to%2520environmental%2520complexity%2520and%2520data%250Ascarcity.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520deep%2520learning-based%2520approach%2520utilizing%2520a%250Avision%2520transformer%2520%2528ViT%2529%2520architecture%2520with%2520DINO-v2%2520pretrained%2520weights%2520to%2520model%250Aindoor%2520radio%2520propagation.%2520Our%2520method%2520processes%2520a%2520floor%2520map%2520with%2520additional%250Afeatures%2520of%2520the%2520walls%2520to%2520generate%2520indoor%2520pathloss%2520maps.%2520We%2520systematically%250Aevaluate%2520the%2520effects%2520of%2520architectural%2520choices%252C%2520data%2520augmentation%2520strategies%252C%250Aand%2520feature%2520engineering%2520techniques.%2520Our%2520findings%2520indicate%2520that%2520extensive%250Aaugmentation%2520significantly%2520improves%2520generalization%252C%2520while%2520feature%2520engineering%250Ais%2520crucial%2520in%2520low-data%2520regimes.%2520Through%2520comprehensive%2520experiments%252C%2520we%250Ademonstrate%2520the%2520robustness%2520of%2520our%2520model%2520across%2520different%2520generalization%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09507v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Transformers%20for%20Efficient%20Indoor%20Pathloss%20Radio%20Map%20Prediction&entry.906535625=Rafayel%20Mkrtchyan%20and%20Edvard%20Ghukasyan%20and%20Khoren%20Petrosyan%20and%20Hrant%20Khachatrian%20and%20Theofanis%20P.%20Raptis&entry.1292438233=%20%20Indoor%20pathloss%20prediction%20is%20a%20fundamental%20task%20in%20wireless%20network%0Aplanning%2C%20yet%20it%20remains%20challenging%20due%20to%20environmental%20complexity%20and%20data%0Ascarcity.%20In%20this%20work%2C%20we%20propose%20a%20deep%20learning-based%20approach%20utilizing%20a%0Avision%20transformer%20%28ViT%29%20architecture%20with%20DINO-v2%20pretrained%20weights%20to%20model%0Aindoor%20radio%20propagation.%20Our%20method%20processes%20a%20floor%20map%20with%20additional%0Afeatures%20of%20the%20walls%20to%20generate%20indoor%20pathloss%20maps.%20We%20systematically%0Aevaluate%20the%20effects%20of%20architectural%20choices%2C%20data%20augmentation%20strategies%2C%0Aand%20feature%20engineering%20techniques.%20Our%20findings%20indicate%20that%20extensive%0Aaugmentation%20significantly%20improves%20generalization%2C%20while%20feature%20engineering%0Ais%20crucial%20in%20low-data%20regimes.%20Through%20comprehensive%20experiments%2C%20we%0Ademonstrate%20the%20robustness%20of%20our%20model%20across%20different%20generalization%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09507v2&entry.124074799=Read"},
{"title": "A Neuro-Symbolic Framework for Sequence Classification with Relational\n  and Temporal Knowledge", "author": "Luca Salvatore Lorello and Marco Lippi and Stefano Melacci", "abstract": "  One of the goals of neuro-symbolic artificial intelligence is to exploit\nbackground knowledge to improve the performance of learning tasks. However,\nmost of the existing frameworks focus on the simplified scenario where\nknowledge does not change over time and does not cover the temporal dimension.\nIn this work we consider the much more challenging problem of knowledge-driven\nsequence classification where different portions of knowledge must be employed\nat different timesteps, and temporal relations are available. Our experimental\nevaluation compares multi-stage neuro-symbolic and neural-only architectures,\nand it is conducted on a newly-introduced benchmarking framework. Results\ndemonstrate the challenging nature of this novel setting, and also highlight\nunder-explored shortcomings of neuro-symbolic methods, representing a precious\nreference for future research.\n", "link": "http://arxiv.org/abs/2505.05106v1", "date": "2025-05-08", "relevancy": 2.0301, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5044}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Neuro-Symbolic%20Framework%20for%20Sequence%20Classification%20with%20Relational%0A%20%20and%20Temporal%20Knowledge&body=Title%3A%20A%20Neuro-Symbolic%20Framework%20for%20Sequence%20Classification%20with%20Relational%0A%20%20and%20Temporal%20Knowledge%0AAuthor%3A%20Luca%20Salvatore%20Lorello%20and%20Marco%20Lippi%20and%20Stefano%20Melacci%0AAbstract%3A%20%20%20One%20of%20the%20goals%20of%20neuro-symbolic%20artificial%20intelligence%20is%20to%20exploit%0Abackground%20knowledge%20to%20improve%20the%20performance%20of%20learning%20tasks.%20However%2C%0Amost%20of%20the%20existing%20frameworks%20focus%20on%20the%20simplified%20scenario%20where%0Aknowledge%20does%20not%20change%20over%20time%20and%20does%20not%20cover%20the%20temporal%20dimension.%0AIn%20this%20work%20we%20consider%20the%20much%20more%20challenging%20problem%20of%20knowledge-driven%0Asequence%20classification%20where%20different%20portions%20of%20knowledge%20must%20be%20employed%0Aat%20different%20timesteps%2C%20and%20temporal%20relations%20are%20available.%20Our%20experimental%0Aevaluation%20compares%20multi-stage%20neuro-symbolic%20and%20neural-only%20architectures%2C%0Aand%20it%20is%20conducted%20on%20a%20newly-introduced%20benchmarking%20framework.%20Results%0Ademonstrate%20the%20challenging%20nature%20of%20this%20novel%20setting%2C%20and%20also%20highlight%0Aunder-explored%20shortcomings%20of%20neuro-symbolic%20methods%2C%20representing%20a%20precious%0Areference%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Neuro-Symbolic%2520Framework%2520for%2520Sequence%2520Classification%2520with%2520Relational%250A%2520%2520and%2520Temporal%2520Knowledge%26entry.906535625%3DLuca%2520Salvatore%2520Lorello%2520and%2520Marco%2520Lippi%2520and%2520Stefano%2520Melacci%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520goals%2520of%2520neuro-symbolic%2520artificial%2520intelligence%2520is%2520to%2520exploit%250Abackground%2520knowledge%2520to%2520improve%2520the%2520performance%2520of%2520learning%2520tasks.%2520However%252C%250Amost%2520of%2520the%2520existing%2520frameworks%2520focus%2520on%2520the%2520simplified%2520scenario%2520where%250Aknowledge%2520does%2520not%2520change%2520over%2520time%2520and%2520does%2520not%2520cover%2520the%2520temporal%2520dimension.%250AIn%2520this%2520work%2520we%2520consider%2520the%2520much%2520more%2520challenging%2520problem%2520of%2520knowledge-driven%250Asequence%2520classification%2520where%2520different%2520portions%2520of%2520knowledge%2520must%2520be%2520employed%250Aat%2520different%2520timesteps%252C%2520and%2520temporal%2520relations%2520are%2520available.%2520Our%2520experimental%250Aevaluation%2520compares%2520multi-stage%2520neuro-symbolic%2520and%2520neural-only%2520architectures%252C%250Aand%2520it%2520is%2520conducted%2520on%2520a%2520newly-introduced%2520benchmarking%2520framework.%2520Results%250Ademonstrate%2520the%2520challenging%2520nature%2520of%2520this%2520novel%2520setting%252C%2520and%2520also%2520highlight%250Aunder-explored%2520shortcomings%2520of%2520neuro-symbolic%2520methods%252C%2520representing%2520a%2520precious%250Areference%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Neuro-Symbolic%20Framework%20for%20Sequence%20Classification%20with%20Relational%0A%20%20and%20Temporal%20Knowledge&entry.906535625=Luca%20Salvatore%20Lorello%20and%20Marco%20Lippi%20and%20Stefano%20Melacci&entry.1292438233=%20%20One%20of%20the%20goals%20of%20neuro-symbolic%20artificial%20intelligence%20is%20to%20exploit%0Abackground%20knowledge%20to%20improve%20the%20performance%20of%20learning%20tasks.%20However%2C%0Amost%20of%20the%20existing%20frameworks%20focus%20on%20the%20simplified%20scenario%20where%0Aknowledge%20does%20not%20change%20over%20time%20and%20does%20not%20cover%20the%20temporal%20dimension.%0AIn%20this%20work%20we%20consider%20the%20much%20more%20challenging%20problem%20of%20knowledge-driven%0Asequence%20classification%20where%20different%20portions%20of%20knowledge%20must%20be%20employed%0Aat%20different%20timesteps%2C%20and%20temporal%20relations%20are%20available.%20Our%20experimental%0Aevaluation%20compares%20multi-stage%20neuro-symbolic%20and%20neural-only%20architectures%2C%0Aand%20it%20is%20conducted%20on%20a%20newly-introduced%20benchmarking%20framework.%20Results%0Ademonstrate%20the%20challenging%20nature%20of%20this%20novel%20setting%2C%20and%20also%20highlight%0Aunder-explored%20shortcomings%20of%20neuro-symbolic%20methods%2C%20representing%20a%20precious%0Areference%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05106v1&entry.124074799=Read"},
{"title": "AirMorph: Topology-Preserving Deep Learning for Pulmonary Airway\n  Analysis", "author": "Minghui Zhang and Chenyu Li and Fangfang Xie and Yaoyu Liu and Hanxiao Zhang and Junyang Wu and Chunxi Zhang and Jie Yang and Jiayuan Sun and Guang-Zhong Yang and Yun Gu", "abstract": "  Accurate anatomical labeling and analysis of the pulmonary structure and its\nsurrounding anatomy from thoracic CT is getting increasingly important for\nunderstanding the etilogy of abnormalities or supporting targetted therapy and\nearly interventions. Whilst lung and airway cell atlases have been attempted,\nthere is a lack of fine-grained morphological atlases that are clinically\ndeployable. In this work, we introduce AirMorph, a robust, end-to-end deep\nlearning pipeline enabling fully automatic and comprehensive airway anatomical\nlabeling at lobar, segmental, and subsegmental resolutions that can be used to\ncreate digital atlases of the lung. Evaluated across large-scale multi-center\ndatasets comprising diverse pulmonary conditions, the AirMorph consistently\noutperformed existing segmentation and labeling methods in terms of accuracy,\ntopological consistency, and completeness. To simplify clinical interpretation,\nwe further introduce a compact anatomical signature quantifying critical\nmorphological airway features, including stenosis, ectasia, tortuosity,\ndivergence, length, and complexity. When applied to various pulmonary diseases\nsuch as pulmonary fibrosis, emphysema, atelectasis, consolidation, and\nreticular opacities, it demonstrates strong discriminative power, revealing\ndisease-specific morphological patterns with high interpretability and\nexplainability. Additionally, AirMorph supports efficient automated branching\npattern analysis, potentially enhancing bronchoscopic navigation planning and\nprocedural safety, offering a valuable clinical tool for improved diagnosis,\ntargeted treatment, and personalized patient care.\n", "link": "http://arxiv.org/abs/2412.11039v2", "date": "2025-05-08", "relevancy": 2.025, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.503}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AirMorph%3A%20Topology-Preserving%20Deep%20Learning%20for%20Pulmonary%20Airway%0A%20%20Analysis&body=Title%3A%20AirMorph%3A%20Topology-Preserving%20Deep%20Learning%20for%20Pulmonary%20Airway%0A%20%20Analysis%0AAuthor%3A%20Minghui%20Zhang%20and%20Chenyu%20Li%20and%20Fangfang%20Xie%20and%20Yaoyu%20Liu%20and%20Hanxiao%20Zhang%20and%20Junyang%20Wu%20and%20Chunxi%20Zhang%20and%20Jie%20Yang%20and%20Jiayuan%20Sun%20and%20Guang-Zhong%20Yang%20and%20Yun%20Gu%0AAbstract%3A%20%20%20Accurate%20anatomical%20labeling%20and%20analysis%20of%20the%20pulmonary%20structure%20and%20its%0Asurrounding%20anatomy%20from%20thoracic%20CT%20is%20getting%20increasingly%20important%20for%0Aunderstanding%20the%20etilogy%20of%20abnormalities%20or%20supporting%20targetted%20therapy%20and%0Aearly%20interventions.%20Whilst%20lung%20and%20airway%20cell%20atlases%20have%20been%20attempted%2C%0Athere%20is%20a%20lack%20of%20fine-grained%20morphological%20atlases%20that%20are%20clinically%0Adeployable.%20In%20this%20work%2C%20we%20introduce%20AirMorph%2C%20a%20robust%2C%20end-to-end%20deep%0Alearning%20pipeline%20enabling%20fully%20automatic%20and%20comprehensive%20airway%20anatomical%0Alabeling%20at%20lobar%2C%20segmental%2C%20and%20subsegmental%20resolutions%20that%20can%20be%20used%20to%0Acreate%20digital%20atlases%20of%20the%20lung.%20Evaluated%20across%20large-scale%20multi-center%0Adatasets%20comprising%20diverse%20pulmonary%20conditions%2C%20the%20AirMorph%20consistently%0Aoutperformed%20existing%20segmentation%20and%20labeling%20methods%20in%20terms%20of%20accuracy%2C%0Atopological%20consistency%2C%20and%20completeness.%20To%20simplify%20clinical%20interpretation%2C%0Awe%20further%20introduce%20a%20compact%20anatomical%20signature%20quantifying%20critical%0Amorphological%20airway%20features%2C%20including%20stenosis%2C%20ectasia%2C%20tortuosity%2C%0Adivergence%2C%20length%2C%20and%20complexity.%20When%20applied%20to%20various%20pulmonary%20diseases%0Asuch%20as%20pulmonary%20fibrosis%2C%20emphysema%2C%20atelectasis%2C%20consolidation%2C%20and%0Areticular%20opacities%2C%20it%20demonstrates%20strong%20discriminative%20power%2C%20revealing%0Adisease-specific%20morphological%20patterns%20with%20high%20interpretability%20and%0Aexplainability.%20Additionally%2C%20AirMorph%20supports%20efficient%20automated%20branching%0Apattern%20analysis%2C%20potentially%20enhancing%20bronchoscopic%20navigation%20planning%20and%0Aprocedural%20safety%2C%20offering%20a%20valuable%20clinical%20tool%20for%20improved%20diagnosis%2C%0Atargeted%20treatment%2C%20and%20personalized%20patient%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAirMorph%253A%2520Topology-Preserving%2520Deep%2520Learning%2520for%2520Pulmonary%2520Airway%250A%2520%2520Analysis%26entry.906535625%3DMinghui%2520Zhang%2520and%2520Chenyu%2520Li%2520and%2520Fangfang%2520Xie%2520and%2520Yaoyu%2520Liu%2520and%2520Hanxiao%2520Zhang%2520and%2520Junyang%2520Wu%2520and%2520Chunxi%2520Zhang%2520and%2520Jie%2520Yang%2520and%2520Jiayuan%2520Sun%2520and%2520Guang-Zhong%2520Yang%2520and%2520Yun%2520Gu%26entry.1292438233%3D%2520%2520Accurate%2520anatomical%2520labeling%2520and%2520analysis%2520of%2520the%2520pulmonary%2520structure%2520and%2520its%250Asurrounding%2520anatomy%2520from%2520thoracic%2520CT%2520is%2520getting%2520increasingly%2520important%2520for%250Aunderstanding%2520the%2520etilogy%2520of%2520abnormalities%2520or%2520supporting%2520targetted%2520therapy%2520and%250Aearly%2520interventions.%2520Whilst%2520lung%2520and%2520airway%2520cell%2520atlases%2520have%2520been%2520attempted%252C%250Athere%2520is%2520a%2520lack%2520of%2520fine-grained%2520morphological%2520atlases%2520that%2520are%2520clinically%250Adeployable.%2520In%2520this%2520work%252C%2520we%2520introduce%2520AirMorph%252C%2520a%2520robust%252C%2520end-to-end%2520deep%250Alearning%2520pipeline%2520enabling%2520fully%2520automatic%2520and%2520comprehensive%2520airway%2520anatomical%250Alabeling%2520at%2520lobar%252C%2520segmental%252C%2520and%2520subsegmental%2520resolutions%2520that%2520can%2520be%2520used%2520to%250Acreate%2520digital%2520atlases%2520of%2520the%2520lung.%2520Evaluated%2520across%2520large-scale%2520multi-center%250Adatasets%2520comprising%2520diverse%2520pulmonary%2520conditions%252C%2520the%2520AirMorph%2520consistently%250Aoutperformed%2520existing%2520segmentation%2520and%2520labeling%2520methods%2520in%2520terms%2520of%2520accuracy%252C%250Atopological%2520consistency%252C%2520and%2520completeness.%2520To%2520simplify%2520clinical%2520interpretation%252C%250Awe%2520further%2520introduce%2520a%2520compact%2520anatomical%2520signature%2520quantifying%2520critical%250Amorphological%2520airway%2520features%252C%2520including%2520stenosis%252C%2520ectasia%252C%2520tortuosity%252C%250Adivergence%252C%2520length%252C%2520and%2520complexity.%2520When%2520applied%2520to%2520various%2520pulmonary%2520diseases%250Asuch%2520as%2520pulmonary%2520fibrosis%252C%2520emphysema%252C%2520atelectasis%252C%2520consolidation%252C%2520and%250Areticular%2520opacities%252C%2520it%2520demonstrates%2520strong%2520discriminative%2520power%252C%2520revealing%250Adisease-specific%2520morphological%2520patterns%2520with%2520high%2520interpretability%2520and%250Aexplainability.%2520Additionally%252C%2520AirMorph%2520supports%2520efficient%2520automated%2520branching%250Apattern%2520analysis%252C%2520potentially%2520enhancing%2520bronchoscopic%2520navigation%2520planning%2520and%250Aprocedural%2520safety%252C%2520offering%2520a%2520valuable%2520clinical%2520tool%2520for%2520improved%2520diagnosis%252C%250Atargeted%2520treatment%252C%2520and%2520personalized%2520patient%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AirMorph%3A%20Topology-Preserving%20Deep%20Learning%20for%20Pulmonary%20Airway%0A%20%20Analysis&entry.906535625=Minghui%20Zhang%20and%20Chenyu%20Li%20and%20Fangfang%20Xie%20and%20Yaoyu%20Liu%20and%20Hanxiao%20Zhang%20and%20Junyang%20Wu%20and%20Chunxi%20Zhang%20and%20Jie%20Yang%20and%20Jiayuan%20Sun%20and%20Guang-Zhong%20Yang%20and%20Yun%20Gu&entry.1292438233=%20%20Accurate%20anatomical%20labeling%20and%20analysis%20of%20the%20pulmonary%20structure%20and%20its%0Asurrounding%20anatomy%20from%20thoracic%20CT%20is%20getting%20increasingly%20important%20for%0Aunderstanding%20the%20etilogy%20of%20abnormalities%20or%20supporting%20targetted%20therapy%20and%0Aearly%20interventions.%20Whilst%20lung%20and%20airway%20cell%20atlases%20have%20been%20attempted%2C%0Athere%20is%20a%20lack%20of%20fine-grained%20morphological%20atlases%20that%20are%20clinically%0Adeployable.%20In%20this%20work%2C%20we%20introduce%20AirMorph%2C%20a%20robust%2C%20end-to-end%20deep%0Alearning%20pipeline%20enabling%20fully%20automatic%20and%20comprehensive%20airway%20anatomical%0Alabeling%20at%20lobar%2C%20segmental%2C%20and%20subsegmental%20resolutions%20that%20can%20be%20used%20to%0Acreate%20digital%20atlases%20of%20the%20lung.%20Evaluated%20across%20large-scale%20multi-center%0Adatasets%20comprising%20diverse%20pulmonary%20conditions%2C%20the%20AirMorph%20consistently%0Aoutperformed%20existing%20segmentation%20and%20labeling%20methods%20in%20terms%20of%20accuracy%2C%0Atopological%20consistency%2C%20and%20completeness.%20To%20simplify%20clinical%20interpretation%2C%0Awe%20further%20introduce%20a%20compact%20anatomical%20signature%20quantifying%20critical%0Amorphological%20airway%20features%2C%20including%20stenosis%2C%20ectasia%2C%20tortuosity%2C%0Adivergence%2C%20length%2C%20and%20complexity.%20When%20applied%20to%20various%20pulmonary%20diseases%0Asuch%20as%20pulmonary%20fibrosis%2C%20emphysema%2C%20atelectasis%2C%20consolidation%2C%20and%0Areticular%20opacities%2C%20it%20demonstrates%20strong%20discriminative%20power%2C%20revealing%0Adisease-specific%20morphological%20patterns%20with%20high%20interpretability%20and%0Aexplainability.%20Additionally%2C%20AirMorph%20supports%20efficient%20automated%20branching%0Apattern%20analysis%2C%20potentially%20enhancing%20bronchoscopic%20navigation%20planning%20and%0Aprocedural%20safety%2C%20offering%20a%20valuable%20clinical%20tool%20for%20improved%20diagnosis%2C%0Atargeted%20treatment%2C%20and%20personalized%20patient%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11039v2&entry.124074799=Read"},
{"title": "Active learning of neural population dynamics using two-photon\n  holographic optogenetics", "author": "Andrew Wagenmaker and Lu Mi and Marton Rozsa and Matthew S. Bull and Karel Svoboda and Kayvon Daie and Matthew D. Golub and Kevin Jamieson", "abstract": "  Recent advances in techniques for monitoring and perturbing neural\npopulations have greatly enhanced our ability to study circuits in the brain.\nIn particular, two-photon holographic optogenetics now enables precise\nphotostimulation of experimenter-specified groups of individual neurons, while\nsimultaneous two-photon calcium imaging enables the measurement of ongoing and\ninduced activity across the neural population. Despite the enormous space of\npotential photostimulation patterns and the time-consuming nature of\nphotostimulation experiments, very little algorithmic work has been done to\ndetermine the most effective photostimulation patterns for identifying the\nneural population dynamics. Here, we develop methods to efficiently select\nwhich neurons to stimulate such that the resulting neural responses will best\ninform a dynamical model of the neural population activity. Using neural\npopulation responses to photostimulation in mouse motor cortex, we demonstrate\nthe efficacy of a low-rank linear dynamical systems model, and develop an\nactive learning procedure which takes advantage of low-rank structure to\ndetermine informative photostimulation patterns. We demonstrate our approach on\nboth real and synthetic data, obtaining in some cases as much as a two-fold\nreduction in the amount of data required to reach a given predictive power. Our\nactive stimulation design method is based on a novel active learning procedure\nfor low-rank regression, which may be of independent interest.\n", "link": "http://arxiv.org/abs/2412.02529v4", "date": "2025-05-08", "relevancy": 2.0232, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5128}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5067}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20learning%20of%20neural%20population%20dynamics%20using%20two-photon%0A%20%20holographic%20optogenetics&body=Title%3A%20Active%20learning%20of%20neural%20population%20dynamics%20using%20two-photon%0A%20%20holographic%20optogenetics%0AAuthor%3A%20Andrew%20Wagenmaker%20and%20Lu%20Mi%20and%20Marton%20Rozsa%20and%20Matthew%20S.%20Bull%20and%20Karel%20Svoboda%20and%20Kayvon%20Daie%20and%20Matthew%20D.%20Golub%20and%20Kevin%20Jamieson%0AAbstract%3A%20%20%20Recent%20advances%20in%20techniques%20for%20monitoring%20and%20perturbing%20neural%0Apopulations%20have%20greatly%20enhanced%20our%20ability%20to%20study%20circuits%20in%20the%20brain.%0AIn%20particular%2C%20two-photon%20holographic%20optogenetics%20now%20enables%20precise%0Aphotostimulation%20of%20experimenter-specified%20groups%20of%20individual%20neurons%2C%20while%0Asimultaneous%20two-photon%20calcium%20imaging%20enables%20the%20measurement%20of%20ongoing%20and%0Ainduced%20activity%20across%20the%20neural%20population.%20Despite%20the%20enormous%20space%20of%0Apotential%20photostimulation%20patterns%20and%20the%20time-consuming%20nature%20of%0Aphotostimulation%20experiments%2C%20very%20little%20algorithmic%20work%20has%20been%20done%20to%0Adetermine%20the%20most%20effective%20photostimulation%20patterns%20for%20identifying%20the%0Aneural%20population%20dynamics.%20Here%2C%20we%20develop%20methods%20to%20efficiently%20select%0Awhich%20neurons%20to%20stimulate%20such%20that%20the%20resulting%20neural%20responses%20will%20best%0Ainform%20a%20dynamical%20model%20of%20the%20neural%20population%20activity.%20Using%20neural%0Apopulation%20responses%20to%20photostimulation%20in%20mouse%20motor%20cortex%2C%20we%20demonstrate%0Athe%20efficacy%20of%20a%20low-rank%20linear%20dynamical%20systems%20model%2C%20and%20develop%20an%0Aactive%20learning%20procedure%20which%20takes%20advantage%20of%20low-rank%20structure%20to%0Adetermine%20informative%20photostimulation%20patterns.%20We%20demonstrate%20our%20approach%20on%0Aboth%20real%20and%20synthetic%20data%2C%20obtaining%20in%20some%20cases%20as%20much%20as%20a%20two-fold%0Areduction%20in%20the%20amount%20of%20data%20required%20to%20reach%20a%20given%20predictive%20power.%20Our%0Aactive%20stimulation%20design%20method%20is%20based%20on%20a%20novel%20active%20learning%20procedure%0Afor%20low-rank%20regression%2C%20which%20may%20be%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02529v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520learning%2520of%2520neural%2520population%2520dynamics%2520using%2520two-photon%250A%2520%2520holographic%2520optogenetics%26entry.906535625%3DAndrew%2520Wagenmaker%2520and%2520Lu%2520Mi%2520and%2520Marton%2520Rozsa%2520and%2520Matthew%2520S.%2520Bull%2520and%2520Karel%2520Svoboda%2520and%2520Kayvon%2520Daie%2520and%2520Matthew%2520D.%2520Golub%2520and%2520Kevin%2520Jamieson%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520techniques%2520for%2520monitoring%2520and%2520perturbing%2520neural%250Apopulations%2520have%2520greatly%2520enhanced%2520our%2520ability%2520to%2520study%2520circuits%2520in%2520the%2520brain.%250AIn%2520particular%252C%2520two-photon%2520holographic%2520optogenetics%2520now%2520enables%2520precise%250Aphotostimulation%2520of%2520experimenter-specified%2520groups%2520of%2520individual%2520neurons%252C%2520while%250Asimultaneous%2520two-photon%2520calcium%2520imaging%2520enables%2520the%2520measurement%2520of%2520ongoing%2520and%250Ainduced%2520activity%2520across%2520the%2520neural%2520population.%2520Despite%2520the%2520enormous%2520space%2520of%250Apotential%2520photostimulation%2520patterns%2520and%2520the%2520time-consuming%2520nature%2520of%250Aphotostimulation%2520experiments%252C%2520very%2520little%2520algorithmic%2520work%2520has%2520been%2520done%2520to%250Adetermine%2520the%2520most%2520effective%2520photostimulation%2520patterns%2520for%2520identifying%2520the%250Aneural%2520population%2520dynamics.%2520Here%252C%2520we%2520develop%2520methods%2520to%2520efficiently%2520select%250Awhich%2520neurons%2520to%2520stimulate%2520such%2520that%2520the%2520resulting%2520neural%2520responses%2520will%2520best%250Ainform%2520a%2520dynamical%2520model%2520of%2520the%2520neural%2520population%2520activity.%2520Using%2520neural%250Apopulation%2520responses%2520to%2520photostimulation%2520in%2520mouse%2520motor%2520cortex%252C%2520we%2520demonstrate%250Athe%2520efficacy%2520of%2520a%2520low-rank%2520linear%2520dynamical%2520systems%2520model%252C%2520and%2520develop%2520an%250Aactive%2520learning%2520procedure%2520which%2520takes%2520advantage%2520of%2520low-rank%2520structure%2520to%250Adetermine%2520informative%2520photostimulation%2520patterns.%2520We%2520demonstrate%2520our%2520approach%2520on%250Aboth%2520real%2520and%2520synthetic%2520data%252C%2520obtaining%2520in%2520some%2520cases%2520as%2520much%2520as%2520a%2520two-fold%250Areduction%2520in%2520the%2520amount%2520of%2520data%2520required%2520to%2520reach%2520a%2520given%2520predictive%2520power.%2520Our%250Aactive%2520stimulation%2520design%2520method%2520is%2520based%2520on%2520a%2520novel%2520active%2520learning%2520procedure%250Afor%2520low-rank%2520regression%252C%2520which%2520may%2520be%2520of%2520independent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02529v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20learning%20of%20neural%20population%20dynamics%20using%20two-photon%0A%20%20holographic%20optogenetics&entry.906535625=Andrew%20Wagenmaker%20and%20Lu%20Mi%20and%20Marton%20Rozsa%20and%20Matthew%20S.%20Bull%20and%20Karel%20Svoboda%20and%20Kayvon%20Daie%20and%20Matthew%20D.%20Golub%20and%20Kevin%20Jamieson&entry.1292438233=%20%20Recent%20advances%20in%20techniques%20for%20monitoring%20and%20perturbing%20neural%0Apopulations%20have%20greatly%20enhanced%20our%20ability%20to%20study%20circuits%20in%20the%20brain.%0AIn%20particular%2C%20two-photon%20holographic%20optogenetics%20now%20enables%20precise%0Aphotostimulation%20of%20experimenter-specified%20groups%20of%20individual%20neurons%2C%20while%0Asimultaneous%20two-photon%20calcium%20imaging%20enables%20the%20measurement%20of%20ongoing%20and%0Ainduced%20activity%20across%20the%20neural%20population.%20Despite%20the%20enormous%20space%20of%0Apotential%20photostimulation%20patterns%20and%20the%20time-consuming%20nature%20of%0Aphotostimulation%20experiments%2C%20very%20little%20algorithmic%20work%20has%20been%20done%20to%0Adetermine%20the%20most%20effective%20photostimulation%20patterns%20for%20identifying%20the%0Aneural%20population%20dynamics.%20Here%2C%20we%20develop%20methods%20to%20efficiently%20select%0Awhich%20neurons%20to%20stimulate%20such%20that%20the%20resulting%20neural%20responses%20will%20best%0Ainform%20a%20dynamical%20model%20of%20the%20neural%20population%20activity.%20Using%20neural%0Apopulation%20responses%20to%20photostimulation%20in%20mouse%20motor%20cortex%2C%20we%20demonstrate%0Athe%20efficacy%20of%20a%20low-rank%20linear%20dynamical%20systems%20model%2C%20and%20develop%20an%0Aactive%20learning%20procedure%20which%20takes%20advantage%20of%20low-rank%20structure%20to%0Adetermine%20informative%20photostimulation%20patterns.%20We%20demonstrate%20our%20approach%20on%0Aboth%20real%20and%20synthetic%20data%2C%20obtaining%20in%20some%20cases%20as%20much%20as%20a%20two-fold%0Areduction%20in%20the%20amount%20of%20data%20required%20to%20reach%20a%20given%20predictive%20power.%20Our%0Aactive%20stimulation%20design%20method%20is%20based%20on%20a%20novel%20active%20learning%20procedure%0Afor%20low-rank%20regression%2C%20which%20may%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02529v4&entry.124074799=Read"},
{"title": "Improved subsample-and-aggregate via the private modified winsorized\n  mean", "author": "Kelly Ramsay and Dylan Spicker", "abstract": "  We develop a univariate, differentially private mean estimator, called the\nprivate modified winsorized mean, designed to be used as the aggregator in\nsubsample-and-aggregate. We demonstrate, via real data analysis, that common\ndifferentially private multivariate mean estimators may not perform well as the\naggregator, even in large datasets, motivating our developments.We show that\nthe modified winsorized mean is minimax optimal for several, large classes of\ndistributions, even under adversarial contamination. We also demonstrate that,\nempirically, the private modified winsorized mean performs well compared to\nother private mean estimates. We consider the modified winsorized mean as the\naggregator in subsample-and-aggregate, deriving a finite sample deviations\nbound for a subsample-and-aggregate estimate generated with the new aggregator.\nThis result yields two important insights: (i) the optimal choice of subsamples\ndepends on the bias of the estimator computed on the subsamples, and (ii) the\nrate of convergence of the subsample-and-aggregate estimator depends on the\nrobustness of the estimator computed on the subsamples.\n", "link": "http://arxiv.org/abs/2501.14095v3", "date": "2025-05-08", "relevancy": 2.0076, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4074}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4061}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20subsample-and-aggregate%20via%20the%20private%20modified%20winsorized%0A%20%20mean&body=Title%3A%20Improved%20subsample-and-aggregate%20via%20the%20private%20modified%20winsorized%0A%20%20mean%0AAuthor%3A%20Kelly%20Ramsay%20and%20Dylan%20Spicker%0AAbstract%3A%20%20%20We%20develop%20a%20univariate%2C%20differentially%20private%20mean%20estimator%2C%20called%20the%0Aprivate%20modified%20winsorized%20mean%2C%20designed%20to%20be%20used%20as%20the%20aggregator%20in%0Asubsample-and-aggregate.%20We%20demonstrate%2C%20via%20real%20data%20analysis%2C%20that%20common%0Adifferentially%20private%20multivariate%20mean%20estimators%20may%20not%20perform%20well%20as%20the%0Aaggregator%2C%20even%20in%20large%20datasets%2C%20motivating%20our%20developments.We%20show%20that%0Athe%20modified%20winsorized%20mean%20is%20minimax%20optimal%20for%20several%2C%20large%20classes%20of%0Adistributions%2C%20even%20under%20adversarial%20contamination.%20We%20also%20demonstrate%20that%2C%0Aempirically%2C%20the%20private%20modified%20winsorized%20mean%20performs%20well%20compared%20to%0Aother%20private%20mean%20estimates.%20We%20consider%20the%20modified%20winsorized%20mean%20as%20the%0Aaggregator%20in%20subsample-and-aggregate%2C%20deriving%20a%20finite%20sample%20deviations%0Abound%20for%20a%20subsample-and-aggregate%20estimate%20generated%20with%20the%20new%20aggregator.%0AThis%20result%20yields%20two%20important%20insights%3A%20%28i%29%20the%20optimal%20choice%20of%20subsamples%0Adepends%20on%20the%20bias%20of%20the%20estimator%20computed%20on%20the%20subsamples%2C%20and%20%28ii%29%20the%0Arate%20of%20convergence%20of%20the%20subsample-and-aggregate%20estimator%20depends%20on%20the%0Arobustness%20of%20the%20estimator%20computed%20on%20the%20subsamples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14095v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520subsample-and-aggregate%2520via%2520the%2520private%2520modified%2520winsorized%250A%2520%2520mean%26entry.906535625%3DKelly%2520Ramsay%2520and%2520Dylan%2520Spicker%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520univariate%252C%2520differentially%2520private%2520mean%2520estimator%252C%2520called%2520the%250Aprivate%2520modified%2520winsorized%2520mean%252C%2520designed%2520to%2520be%2520used%2520as%2520the%2520aggregator%2520in%250Asubsample-and-aggregate.%2520We%2520demonstrate%252C%2520via%2520real%2520data%2520analysis%252C%2520that%2520common%250Adifferentially%2520private%2520multivariate%2520mean%2520estimators%2520may%2520not%2520perform%2520well%2520as%2520the%250Aaggregator%252C%2520even%2520in%2520large%2520datasets%252C%2520motivating%2520our%2520developments.We%2520show%2520that%250Athe%2520modified%2520winsorized%2520mean%2520is%2520minimax%2520optimal%2520for%2520several%252C%2520large%2520classes%2520of%250Adistributions%252C%2520even%2520under%2520adversarial%2520contamination.%2520We%2520also%2520demonstrate%2520that%252C%250Aempirically%252C%2520the%2520private%2520modified%2520winsorized%2520mean%2520performs%2520well%2520compared%2520to%250Aother%2520private%2520mean%2520estimates.%2520We%2520consider%2520the%2520modified%2520winsorized%2520mean%2520as%2520the%250Aaggregator%2520in%2520subsample-and-aggregate%252C%2520deriving%2520a%2520finite%2520sample%2520deviations%250Abound%2520for%2520a%2520subsample-and-aggregate%2520estimate%2520generated%2520with%2520the%2520new%2520aggregator.%250AThis%2520result%2520yields%2520two%2520important%2520insights%253A%2520%2528i%2529%2520the%2520optimal%2520choice%2520of%2520subsamples%250Adepends%2520on%2520the%2520bias%2520of%2520the%2520estimator%2520computed%2520on%2520the%2520subsamples%252C%2520and%2520%2528ii%2529%2520the%250Arate%2520of%2520convergence%2520of%2520the%2520subsample-and-aggregate%2520estimator%2520depends%2520on%2520the%250Arobustness%2520of%2520the%2520estimator%2520computed%2520on%2520the%2520subsamples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14095v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20subsample-and-aggregate%20via%20the%20private%20modified%20winsorized%0A%20%20mean&entry.906535625=Kelly%20Ramsay%20and%20Dylan%20Spicker&entry.1292438233=%20%20We%20develop%20a%20univariate%2C%20differentially%20private%20mean%20estimator%2C%20called%20the%0Aprivate%20modified%20winsorized%20mean%2C%20designed%20to%20be%20used%20as%20the%20aggregator%20in%0Asubsample-and-aggregate.%20We%20demonstrate%2C%20via%20real%20data%20analysis%2C%20that%20common%0Adifferentially%20private%20multivariate%20mean%20estimators%20may%20not%20perform%20well%20as%20the%0Aaggregator%2C%20even%20in%20large%20datasets%2C%20motivating%20our%20developments.We%20show%20that%0Athe%20modified%20winsorized%20mean%20is%20minimax%20optimal%20for%20several%2C%20large%20classes%20of%0Adistributions%2C%20even%20under%20adversarial%20contamination.%20We%20also%20demonstrate%20that%2C%0Aempirically%2C%20the%20private%20modified%20winsorized%20mean%20performs%20well%20compared%20to%0Aother%20private%20mean%20estimates.%20We%20consider%20the%20modified%20winsorized%20mean%20as%20the%0Aaggregator%20in%20subsample-and-aggregate%2C%20deriving%20a%20finite%20sample%20deviations%0Abound%20for%20a%20subsample-and-aggregate%20estimate%20generated%20with%20the%20new%20aggregator.%0AThis%20result%20yields%20two%20important%20insights%3A%20%28i%29%20the%20optimal%20choice%20of%20subsamples%0Adepends%20on%20the%20bias%20of%20the%20estimator%20computed%20on%20the%20subsamples%2C%20and%20%28ii%29%20the%0Arate%20of%20convergence%20of%20the%20subsample-and-aggregate%20estimator%20depends%20on%20the%0Arobustness%20of%20the%20estimator%20computed%20on%20the%20subsamples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14095v3&entry.124074799=Read"},
{"title": "Evaluating Deep Learning Models for Breast Cancer Classification: A\n  Comparative Study", "author": "Sania Eskandari and Ali Eslamian and Nusrat Munia and Amjad Alqarni and Qiang Cheng", "abstract": "  This study evaluates the effectiveness of deep learning models in classifying\nhistopathological images for early and accurate detection of breast cancer.\nEight advanced models, including ResNet-50, DenseNet-121, ResNeXt-50, Vision\nTransformer (ViT), GoogLeNet (Inception v3), EfficientNet, MobileNet, and\nSqueezeNet, were compared using a dataset of 277,524 image patches. The Vision\nTransformer (ViT) model, with its attention-based mechanisms, achieved the\nhighest validation accuracy of 94%, outperforming conventional CNNs. The study\ndemonstrates the potential of advanced machine learning methods to enhance\nprecision and efficiency in breast cancer diagnosis in clinical settings.\n", "link": "http://arxiv.org/abs/2408.16859v2", "date": "2025-05-08", "relevancy": 1.9841, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5224}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4818}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Deep%20Learning%20Models%20for%20Breast%20Cancer%20Classification%3A%20A%0A%20%20Comparative%20Study&body=Title%3A%20Evaluating%20Deep%20Learning%20Models%20for%20Breast%20Cancer%20Classification%3A%20A%0A%20%20Comparative%20Study%0AAuthor%3A%20Sania%20Eskandari%20and%20Ali%20Eslamian%20and%20Nusrat%20Munia%20and%20Amjad%20Alqarni%20and%20Qiang%20Cheng%0AAbstract%3A%20%20%20This%20study%20evaluates%20the%20effectiveness%20of%20deep%20learning%20models%20in%20classifying%0Ahistopathological%20images%20for%20early%20and%20accurate%20detection%20of%20breast%20cancer.%0AEight%20advanced%20models%2C%20including%20ResNet-50%2C%20DenseNet-121%2C%20ResNeXt-50%2C%20Vision%0ATransformer%20%28ViT%29%2C%20GoogLeNet%20%28Inception%20v3%29%2C%20EfficientNet%2C%20MobileNet%2C%20and%0ASqueezeNet%2C%20were%20compared%20using%20a%20dataset%20of%20277%2C524%20image%20patches.%20The%20Vision%0ATransformer%20%28ViT%29%20model%2C%20with%20its%20attention-based%20mechanisms%2C%20achieved%20the%0Ahighest%20validation%20accuracy%20of%2094%25%2C%20outperforming%20conventional%20CNNs.%20The%20study%0Ademonstrates%20the%20potential%20of%20advanced%20machine%20learning%20methods%20to%20enhance%0Aprecision%20and%20efficiency%20in%20breast%20cancer%20diagnosis%20in%20clinical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16859v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Deep%2520Learning%2520Models%2520for%2520Breast%2520Cancer%2520Classification%253A%2520A%250A%2520%2520Comparative%2520Study%26entry.906535625%3DSania%2520Eskandari%2520and%2520Ali%2520Eslamian%2520and%2520Nusrat%2520Munia%2520and%2520Amjad%2520Alqarni%2520and%2520Qiang%2520Cheng%26entry.1292438233%3D%2520%2520This%2520study%2520evaluates%2520the%2520effectiveness%2520of%2520deep%2520learning%2520models%2520in%2520classifying%250Ahistopathological%2520images%2520for%2520early%2520and%2520accurate%2520detection%2520of%2520breast%2520cancer.%250AEight%2520advanced%2520models%252C%2520including%2520ResNet-50%252C%2520DenseNet-121%252C%2520ResNeXt-50%252C%2520Vision%250ATransformer%2520%2528ViT%2529%252C%2520GoogLeNet%2520%2528Inception%2520v3%2529%252C%2520EfficientNet%252C%2520MobileNet%252C%2520and%250ASqueezeNet%252C%2520were%2520compared%2520using%2520a%2520dataset%2520of%2520277%252C524%2520image%2520patches.%2520The%2520Vision%250ATransformer%2520%2528ViT%2529%2520model%252C%2520with%2520its%2520attention-based%2520mechanisms%252C%2520achieved%2520the%250Ahighest%2520validation%2520accuracy%2520of%252094%2525%252C%2520outperforming%2520conventional%2520CNNs.%2520The%2520study%250Ademonstrates%2520the%2520potential%2520of%2520advanced%2520machine%2520learning%2520methods%2520to%2520enhance%250Aprecision%2520and%2520efficiency%2520in%2520breast%2520cancer%2520diagnosis%2520in%2520clinical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16859v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Deep%20Learning%20Models%20for%20Breast%20Cancer%20Classification%3A%20A%0A%20%20Comparative%20Study&entry.906535625=Sania%20Eskandari%20and%20Ali%20Eslamian%20and%20Nusrat%20Munia%20and%20Amjad%20Alqarni%20and%20Qiang%20Cheng&entry.1292438233=%20%20This%20study%20evaluates%20the%20effectiveness%20of%20deep%20learning%20models%20in%20classifying%0Ahistopathological%20images%20for%20early%20and%20accurate%20detection%20of%20breast%20cancer.%0AEight%20advanced%20models%2C%20including%20ResNet-50%2C%20DenseNet-121%2C%20ResNeXt-50%2C%20Vision%0ATransformer%20%28ViT%29%2C%20GoogLeNet%20%28Inception%20v3%29%2C%20EfficientNet%2C%20MobileNet%2C%20and%0ASqueezeNet%2C%20were%20compared%20using%20a%20dataset%20of%20277%2C524%20image%20patches.%20The%20Vision%0ATransformer%20%28ViT%29%20model%2C%20with%20its%20attention-based%20mechanisms%2C%20achieved%20the%0Ahighest%20validation%20accuracy%20of%2094%25%2C%20outperforming%20conventional%20CNNs.%20The%20study%0Ademonstrates%20the%20potential%20of%20advanced%20machine%20learning%20methods%20to%20enhance%0Aprecision%20and%20efficiency%20in%20breast%20cancer%20diagnosis%20in%20clinical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16859v2&entry.124074799=Read"},
{"title": "MARK: Memory Augmented Refinement of Knowledge", "author": "Anish Ganguli and Prabal Deb and Debleena Banerjee", "abstract": "  Large Language Models (LLMs) assist in specialized tasks but struggle to\nalign with evolving domain knowledge without costly fine-tuning. Domain\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\nand generally accepted principles (e.g., ethical standards); Refined Memory:\nEvolving insights shaped by business needs and real-world changes. However, a\nsignificant gap often exists between a domain expert's deep, nuanced\nunderstanding and the system's domain knowledge, which can hinder accurate\ninformation retrieval and application. Our Memory-Augmented Refinement of\nKnowledge (MARK) framework enables LLMs to continuously learn without\nretraining by leveraging structured refined memory, inspired by the Society of\nMind. MARK operates through specialized agents, each serving a distinct role:\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\nmaintain context over time; User Question Refined Memory Agent: Captures\nuser-provided facts, abbreviations, and terminology for better comprehension;\nLLM Response Refined Memory Agent: Extracts key elements from responses for\nrefinement and personalization. These agents analyse stored refined memory,\ndetect patterns, resolve contradictions, and improve response accuracy.\nTemporal factors like recency and frequency prioritize relevant information\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\nmanufacturing, where proprietary insights are absent from public datasets;\nPersonalized AI Assistants: Improves virtual assistants by remembering user\npreferences, ensuring coherent responses over time.\n", "link": "http://arxiv.org/abs/2505.05177v1", "date": "2025-05-08", "relevancy": 1.9833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4979}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4979}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARK%3A%20Memory%20Augmented%20Refinement%20of%20Knowledge&body=Title%3A%20MARK%3A%20Memory%20Augmented%20Refinement%20of%20Knowledge%0AAuthor%3A%20Anish%20Ganguli%20and%20Prabal%20Deb%20and%20Debleena%20Banerjee%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20assist%20in%20specialized%20tasks%20but%20struggle%20to%0Aalign%20with%20evolving%20domain%20knowledge%20without%20costly%20fine-tuning.%20Domain%0Aknowledge%20consists%20of%3A%20Knowledge%3A%20Immutable%20facts%20%28e.g.%2C%20%27A%20stone%20is%20solid%27%29%0Aand%20generally%20accepted%20principles%20%28e.g.%2C%20ethical%20standards%29%3B%20Refined%20Memory%3A%0AEvolving%20insights%20shaped%20by%20business%20needs%20and%20real-world%20changes.%20However%2C%20a%0Asignificant%20gap%20often%20exists%20between%20a%20domain%20expert%27s%20deep%2C%20nuanced%0Aunderstanding%20and%20the%20system%27s%20domain%20knowledge%2C%20which%20can%20hinder%20accurate%0Ainformation%20retrieval%20and%20application.%20Our%20Memory-Augmented%20Refinement%20of%0AKnowledge%20%28MARK%29%20framework%20enables%20LLMs%20to%20continuously%20learn%20without%0Aretraining%20by%20leveraging%20structured%20refined%20memory%2C%20inspired%20by%20the%20Society%20of%0AMind.%20MARK%20operates%20through%20specialized%20agents%2C%20each%20serving%20a%20distinct%20role%3A%0AResidual%20Refined%20Memory%20Agent%3A%20Stores%20and%20retrieves%20domain-specific%20insights%20to%0Amaintain%20context%20over%20time%3B%20User%20Question%20Refined%20Memory%20Agent%3A%20Captures%0Auser-provided%20facts%2C%20abbreviations%2C%20and%20terminology%20for%20better%20comprehension%3B%0ALLM%20Response%20Refined%20Memory%20Agent%3A%20Extracts%20key%20elements%20from%20responses%20for%0Arefinement%20and%20personalization.%20These%20agents%20analyse%20stored%20refined%20memory%2C%0Adetect%20patterns%2C%20resolve%20contradictions%2C%20and%20improve%20response%20accuracy.%0ATemporal%20factors%20like%20recency%20and%20frequency%20prioritize%20relevant%20information%0Awhile%20discarding%20outdated%20insights.%20MARK%20enhances%20LLMs%20in%20multiple%20ways%3A%20Ground%0ATruth%20Strategy%3A%20Reduces%20hallucinations%20by%20establishing%20a%20structured%20reference%3B%0ADomain-Specific%20Adaptation%3A%20Essential%20for%20fields%20like%20healthcare%2C%20law%2C%20and%0Amanufacturing%2C%20where%20proprietary%20insights%20are%20absent%20from%20public%20datasets%3B%0APersonalized%20AI%20Assistants%3A%20Improves%20virtual%20assistants%20by%20remembering%20user%0Apreferences%2C%20ensuring%20coherent%20responses%20over%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARK%253A%2520Memory%2520Augmented%2520Refinement%2520of%2520Knowledge%26entry.906535625%3DAnish%2520Ganguli%2520and%2520Prabal%2520Deb%2520and%2520Debleena%2520Banerjee%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520assist%2520in%2520specialized%2520tasks%2520but%2520struggle%2520to%250Aalign%2520with%2520evolving%2520domain%2520knowledge%2520without%2520costly%2520fine-tuning.%2520Domain%250Aknowledge%2520consists%2520of%253A%2520Knowledge%253A%2520Immutable%2520facts%2520%2528e.g.%252C%2520%2527A%2520stone%2520is%2520solid%2527%2529%250Aand%2520generally%2520accepted%2520principles%2520%2528e.g.%252C%2520ethical%2520standards%2529%253B%2520Refined%2520Memory%253A%250AEvolving%2520insights%2520shaped%2520by%2520business%2520needs%2520and%2520real-world%2520changes.%2520However%252C%2520a%250Asignificant%2520gap%2520often%2520exists%2520between%2520a%2520domain%2520expert%2527s%2520deep%252C%2520nuanced%250Aunderstanding%2520and%2520the%2520system%2527s%2520domain%2520knowledge%252C%2520which%2520can%2520hinder%2520accurate%250Ainformation%2520retrieval%2520and%2520application.%2520Our%2520Memory-Augmented%2520Refinement%2520of%250AKnowledge%2520%2528MARK%2529%2520framework%2520enables%2520LLMs%2520to%2520continuously%2520learn%2520without%250Aretraining%2520by%2520leveraging%2520structured%2520refined%2520memory%252C%2520inspired%2520by%2520the%2520Society%2520of%250AMind.%2520MARK%2520operates%2520through%2520specialized%2520agents%252C%2520each%2520serving%2520a%2520distinct%2520role%253A%250AResidual%2520Refined%2520Memory%2520Agent%253A%2520Stores%2520and%2520retrieves%2520domain-specific%2520insights%2520to%250Amaintain%2520context%2520over%2520time%253B%2520User%2520Question%2520Refined%2520Memory%2520Agent%253A%2520Captures%250Auser-provided%2520facts%252C%2520abbreviations%252C%2520and%2520terminology%2520for%2520better%2520comprehension%253B%250ALLM%2520Response%2520Refined%2520Memory%2520Agent%253A%2520Extracts%2520key%2520elements%2520from%2520responses%2520for%250Arefinement%2520and%2520personalization.%2520These%2520agents%2520analyse%2520stored%2520refined%2520memory%252C%250Adetect%2520patterns%252C%2520resolve%2520contradictions%252C%2520and%2520improve%2520response%2520accuracy.%250ATemporal%2520factors%2520like%2520recency%2520and%2520frequency%2520prioritize%2520relevant%2520information%250Awhile%2520discarding%2520outdated%2520insights.%2520MARK%2520enhances%2520LLMs%2520in%2520multiple%2520ways%253A%2520Ground%250ATruth%2520Strategy%253A%2520Reduces%2520hallucinations%2520by%2520establishing%2520a%2520structured%2520reference%253B%250ADomain-Specific%2520Adaptation%253A%2520Essential%2520for%2520fields%2520like%2520healthcare%252C%2520law%252C%2520and%250Amanufacturing%252C%2520where%2520proprietary%2520insights%2520are%2520absent%2520from%2520public%2520datasets%253B%250APersonalized%2520AI%2520Assistants%253A%2520Improves%2520virtual%2520assistants%2520by%2520remembering%2520user%250Apreferences%252C%2520ensuring%2520coherent%2520responses%2520over%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARK%3A%20Memory%20Augmented%20Refinement%20of%20Knowledge&entry.906535625=Anish%20Ganguli%20and%20Prabal%20Deb%20and%20Debleena%20Banerjee&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20assist%20in%20specialized%20tasks%20but%20struggle%20to%0Aalign%20with%20evolving%20domain%20knowledge%20without%20costly%20fine-tuning.%20Domain%0Aknowledge%20consists%20of%3A%20Knowledge%3A%20Immutable%20facts%20%28e.g.%2C%20%27A%20stone%20is%20solid%27%29%0Aand%20generally%20accepted%20principles%20%28e.g.%2C%20ethical%20standards%29%3B%20Refined%20Memory%3A%0AEvolving%20insights%20shaped%20by%20business%20needs%20and%20real-world%20changes.%20However%2C%20a%0Asignificant%20gap%20often%20exists%20between%20a%20domain%20expert%27s%20deep%2C%20nuanced%0Aunderstanding%20and%20the%20system%27s%20domain%20knowledge%2C%20which%20can%20hinder%20accurate%0Ainformation%20retrieval%20and%20application.%20Our%20Memory-Augmented%20Refinement%20of%0AKnowledge%20%28MARK%29%20framework%20enables%20LLMs%20to%20continuously%20learn%20without%0Aretraining%20by%20leveraging%20structured%20refined%20memory%2C%20inspired%20by%20the%20Society%20of%0AMind.%20MARK%20operates%20through%20specialized%20agents%2C%20each%20serving%20a%20distinct%20role%3A%0AResidual%20Refined%20Memory%20Agent%3A%20Stores%20and%20retrieves%20domain-specific%20insights%20to%0Amaintain%20context%20over%20time%3B%20User%20Question%20Refined%20Memory%20Agent%3A%20Captures%0Auser-provided%20facts%2C%20abbreviations%2C%20and%20terminology%20for%20better%20comprehension%3B%0ALLM%20Response%20Refined%20Memory%20Agent%3A%20Extracts%20key%20elements%20from%20responses%20for%0Arefinement%20and%20personalization.%20These%20agents%20analyse%20stored%20refined%20memory%2C%0Adetect%20patterns%2C%20resolve%20contradictions%2C%20and%20improve%20response%20accuracy.%0ATemporal%20factors%20like%20recency%20and%20frequency%20prioritize%20relevant%20information%0Awhile%20discarding%20outdated%20insights.%20MARK%20enhances%20LLMs%20in%20multiple%20ways%3A%20Ground%0ATruth%20Strategy%3A%20Reduces%20hallucinations%20by%20establishing%20a%20structured%20reference%3B%0ADomain-Specific%20Adaptation%3A%20Essential%20for%20fields%20like%20healthcare%2C%20law%2C%20and%0Amanufacturing%2C%20where%20proprietary%20insights%20are%20absent%20from%20public%20datasets%3B%0APersonalized%20AI%20Assistants%3A%20Improves%20virtual%20assistants%20by%20remembering%20user%0Apreferences%2C%20ensuring%20coherent%20responses%20over%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05177v1&entry.124074799=Read"},
{"title": "Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics\n  simulations for machine learning", "author": "Fabien Casenave and Xavier Roynard and Brian Staber and William Piat and Michele Alessandro Bucci and Nissrine Akkari and Abbas Kabalan and Xuan Minh Vuong Nguyen and Luca Saverio and Rapha\u00ebl Carpintero Perez and Anthony Kalaydjian and Samy Fouch\u00e9 and Thierry Gonon and Ghassan Najjar and Emmanuel Menier and Matthieu Nastorg and Giovanni Catalani and Christian Rey", "abstract": "  Machine learning-based surrogate models have emerged as a powerful tool to\naccelerate simulation-driven scientific workflows. However, their widespread\nadoption is hindered by the lack of large-scale, diverse, and standardized\ndatasets tailored to physics-based simulations. While existing initiatives\nprovide valuable contributions, many are limited in scope-focusing on specific\nphysics domains, relying on fragmented tooling, or adhering to overly\nsimplistic datamodels that restrict generalization. To address these\nlimitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and\nextensible framework for representing and sharing datasets of physics\nsimulations. PLAID defines a unified standard for describing simulation data\nand is accompanied by a library for creating, reading, and manipulating complex\ndatasets across a wide range of physical use cases (gitlab.com/drti/plaid). We\nrelease six carefully crafted datasets under the PLAID standard, covering\nstructural mechanics and computational fluid dynamics, and provide baseline\nbenchmarks using representative learning methods. Benchmarking tools are made\navailable on Hugging Face, enabling direct participation by the community and\ncontribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions).\n", "link": "http://arxiv.org/abs/2505.02974v2", "date": "2025-05-08", "relevancy": 1.9805, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5131}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5127}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Learning%20AI%20Datamodel%20%28PLAID%29%20datasets%3A%20a%20collection%20of%20physics%0A%20%20simulations%20for%20machine%20learning&body=Title%3A%20Physics-Learning%20AI%20Datamodel%20%28PLAID%29%20datasets%3A%20a%20collection%20of%20physics%0A%20%20simulations%20for%20machine%20learning%0AAuthor%3A%20Fabien%20Casenave%20and%20Xavier%20Roynard%20and%20Brian%20Staber%20and%20William%20Piat%20and%20Michele%20Alessandro%20Bucci%20and%20Nissrine%20Akkari%20and%20Abbas%20Kabalan%20and%20Xuan%20Minh%20Vuong%20Nguyen%20and%20Luca%20Saverio%20and%20Rapha%C3%ABl%20Carpintero%20Perez%20and%20Anthony%20Kalaydjian%20and%20Samy%20Fouch%C3%A9%20and%20Thierry%20Gonon%20and%20Ghassan%20Najjar%20and%20Emmanuel%20Menier%20and%20Matthieu%20Nastorg%20and%20Giovanni%20Catalani%20and%20Christian%20Rey%0AAbstract%3A%20%20%20Machine%20learning-based%20surrogate%20models%20have%20emerged%20as%20a%20powerful%20tool%20to%0Aaccelerate%20simulation-driven%20scientific%20workflows.%20However%2C%20their%20widespread%0Aadoption%20is%20hindered%20by%20the%20lack%20of%20large-scale%2C%20diverse%2C%20and%20standardized%0Adatasets%20tailored%20to%20physics-based%20simulations.%20While%20existing%20initiatives%0Aprovide%20valuable%20contributions%2C%20many%20are%20limited%20in%20scope-focusing%20on%20specific%0Aphysics%20domains%2C%20relying%20on%20fragmented%20tooling%2C%20or%20adhering%20to%20overly%0Asimplistic%20datamodels%20that%20restrict%20generalization.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20PLAID%20%28Physics-Learning%20AI%20Datamodel%29%2C%20a%20flexible%20and%0Aextensible%20framework%20for%20representing%20and%20sharing%20datasets%20of%20physics%0Asimulations.%20PLAID%20defines%20a%20unified%20standard%20for%20describing%20simulation%20data%0Aand%20is%20accompanied%20by%20a%20library%20for%20creating%2C%20reading%2C%20and%20manipulating%20complex%0Adatasets%20across%20a%20wide%20range%20of%20physical%20use%20cases%20%28gitlab.com/drti/plaid%29.%20We%0Arelease%20six%20carefully%20crafted%20datasets%20under%20the%20PLAID%20standard%2C%20covering%0Astructural%20mechanics%20and%20computational%20fluid%20dynamics%2C%20and%20provide%20baseline%0Abenchmarks%20using%20representative%20learning%20methods.%20Benchmarking%20tools%20are%20made%0Aavailable%20on%20Hugging%20Face%2C%20enabling%20direct%20participation%20by%20the%20community%20and%0Acontribution%20to%20ongoing%20evaluation%20efforts%20%28huggingface.co/PLAIDcompetitions%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02974v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Learning%2520AI%2520Datamodel%2520%2528PLAID%2529%2520datasets%253A%2520a%2520collection%2520of%2520physics%250A%2520%2520simulations%2520for%2520machine%2520learning%26entry.906535625%3DFabien%2520Casenave%2520and%2520Xavier%2520Roynard%2520and%2520Brian%2520Staber%2520and%2520William%2520Piat%2520and%2520Michele%2520Alessandro%2520Bucci%2520and%2520Nissrine%2520Akkari%2520and%2520Abbas%2520Kabalan%2520and%2520Xuan%2520Minh%2520Vuong%2520Nguyen%2520and%2520Luca%2520Saverio%2520and%2520Rapha%25C3%25ABl%2520Carpintero%2520Perez%2520and%2520Anthony%2520Kalaydjian%2520and%2520Samy%2520Fouch%25C3%25A9%2520and%2520Thierry%2520Gonon%2520and%2520Ghassan%2520Najjar%2520and%2520Emmanuel%2520Menier%2520and%2520Matthieu%2520Nastorg%2520and%2520Giovanni%2520Catalani%2520and%2520Christian%2520Rey%26entry.1292438233%3D%2520%2520Machine%2520learning-based%2520surrogate%2520models%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520to%250Aaccelerate%2520simulation-driven%2520scientific%2520workflows.%2520However%252C%2520their%2520widespread%250Aadoption%2520is%2520hindered%2520by%2520the%2520lack%2520of%2520large-scale%252C%2520diverse%252C%2520and%2520standardized%250Adatasets%2520tailored%2520to%2520physics-based%2520simulations.%2520While%2520existing%2520initiatives%250Aprovide%2520valuable%2520contributions%252C%2520many%2520are%2520limited%2520in%2520scope-focusing%2520on%2520specific%250Aphysics%2520domains%252C%2520relying%2520on%2520fragmented%2520tooling%252C%2520or%2520adhering%2520to%2520overly%250Asimplistic%2520datamodels%2520that%2520restrict%2520generalization.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520PLAID%2520%2528Physics-Learning%2520AI%2520Datamodel%2529%252C%2520a%2520flexible%2520and%250Aextensible%2520framework%2520for%2520representing%2520and%2520sharing%2520datasets%2520of%2520physics%250Asimulations.%2520PLAID%2520defines%2520a%2520unified%2520standard%2520for%2520describing%2520simulation%2520data%250Aand%2520is%2520accompanied%2520by%2520a%2520library%2520for%2520creating%252C%2520reading%252C%2520and%2520manipulating%2520complex%250Adatasets%2520across%2520a%2520wide%2520range%2520of%2520physical%2520use%2520cases%2520%2528gitlab.com/drti/plaid%2529.%2520We%250Arelease%2520six%2520carefully%2520crafted%2520datasets%2520under%2520the%2520PLAID%2520standard%252C%2520covering%250Astructural%2520mechanics%2520and%2520computational%2520fluid%2520dynamics%252C%2520and%2520provide%2520baseline%250Abenchmarks%2520using%2520representative%2520learning%2520methods.%2520Benchmarking%2520tools%2520are%2520made%250Aavailable%2520on%2520Hugging%2520Face%252C%2520enabling%2520direct%2520participation%2520by%2520the%2520community%2520and%250Acontribution%2520to%2520ongoing%2520evaluation%2520efforts%2520%2528huggingface.co/PLAIDcompetitions%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02974v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Learning%20AI%20Datamodel%20%28PLAID%29%20datasets%3A%20a%20collection%20of%20physics%0A%20%20simulations%20for%20machine%20learning&entry.906535625=Fabien%20Casenave%20and%20Xavier%20Roynard%20and%20Brian%20Staber%20and%20William%20Piat%20and%20Michele%20Alessandro%20Bucci%20and%20Nissrine%20Akkari%20and%20Abbas%20Kabalan%20and%20Xuan%20Minh%20Vuong%20Nguyen%20and%20Luca%20Saverio%20and%20Rapha%C3%ABl%20Carpintero%20Perez%20and%20Anthony%20Kalaydjian%20and%20Samy%20Fouch%C3%A9%20and%20Thierry%20Gonon%20and%20Ghassan%20Najjar%20and%20Emmanuel%20Menier%20and%20Matthieu%20Nastorg%20and%20Giovanni%20Catalani%20and%20Christian%20Rey&entry.1292438233=%20%20Machine%20learning-based%20surrogate%20models%20have%20emerged%20as%20a%20powerful%20tool%20to%0Aaccelerate%20simulation-driven%20scientific%20workflows.%20However%2C%20their%20widespread%0Aadoption%20is%20hindered%20by%20the%20lack%20of%20large-scale%2C%20diverse%2C%20and%20standardized%0Adatasets%20tailored%20to%20physics-based%20simulations.%20While%20existing%20initiatives%0Aprovide%20valuable%20contributions%2C%20many%20are%20limited%20in%20scope-focusing%20on%20specific%0Aphysics%20domains%2C%20relying%20on%20fragmented%20tooling%2C%20or%20adhering%20to%20overly%0Asimplistic%20datamodels%20that%20restrict%20generalization.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20PLAID%20%28Physics-Learning%20AI%20Datamodel%29%2C%20a%20flexible%20and%0Aextensible%20framework%20for%20representing%20and%20sharing%20datasets%20of%20physics%0Asimulations.%20PLAID%20defines%20a%20unified%20standard%20for%20describing%20simulation%20data%0Aand%20is%20accompanied%20by%20a%20library%20for%20creating%2C%20reading%2C%20and%20manipulating%20complex%0Adatasets%20across%20a%20wide%20range%20of%20physical%20use%20cases%20%28gitlab.com/drti/plaid%29.%20We%0Arelease%20six%20carefully%20crafted%20datasets%20under%20the%20PLAID%20standard%2C%20covering%0Astructural%20mechanics%20and%20computational%20fluid%20dynamics%2C%20and%20provide%20baseline%0Abenchmarks%20using%20representative%20learning%20methods.%20Benchmarking%20tools%20are%20made%0Aavailable%20on%20Hugging%20Face%2C%20enabling%20direct%20participation%20by%20the%20community%20and%0Acontribution%20to%20ongoing%20evaluation%20efforts%20%28huggingface.co/PLAIDcompetitions%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02974v2&entry.124074799=Read"},
{"title": "Automated vision-based assistance tools in bronchoscopy: stenosis\n  severity estimation", "author": "Clara Tomasini and Javier Rodriguez-Puigvert and Dinora Polanco and Manuel Vi\u00f1uales and Luis Riazuelo and Ana Cristina Murillo", "abstract": "  Purpose: Subglottic stenosis refers to the narrowing of the subglottis, the\nairway between the vocal cords and the trachea. Its severity is typically\nevaluated by estimating the percentage of obstructed airway. This estimation\ncan be obtained from CT data or through visual inspection by experts exploring\nthe region. However, visual inspections are inherently subjective, leading to\nless consistent and robust diagnoses. No public methods or datasets are\ncurrently available for automated evaluation of this condition from\nbronchoscopy video.\n  Methods: We propose a pipeline for automated subglottic stenosis severity\nestimation during the bronchoscopy exploration, without requiring the physician\nto traverse the stenosed region. Our approach exploits the physical effect of\nillumination decline in endoscopy to segment and track the lumen and obtain a\n3D model of the airway. This 3D model is obtained from a single frame and is\nused to measure the airway narrowing.\n  Results: Our pipeline is the first to enable automated and robust subglottic\nstenosis severity measurement using bronchoscopy images. The results show\nconsistency with ground-truth estimations from CT scans and expert estimations,\nand reliable repeatability across multiple estimations on the same patient. Our\nevaluation is performed on our new Subglottic Stenosis Dataset of real\nbronchoscopy procedures data.\n  Conclusion: We demonstrate how to automate evaluation of subglottic stenosis\nseverity using only bronchoscopy. Our approach can assist with and shorten\ndiagnosis and monitoring procedures, with automated and repeatable estimations\nand less exploration time, and save radiation exposure to patients as no CT is\nrequired. Additionally, we release the first public benchmark for subglottic\nstenosis severity assessment.\n", "link": "http://arxiv.org/abs/2505.05136v1", "date": "2025-05-08", "relevancy": 1.9799, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4977}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4977}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20vision-based%20assistance%20tools%20in%20bronchoscopy%3A%20stenosis%0A%20%20severity%20estimation&body=Title%3A%20Automated%20vision-based%20assistance%20tools%20in%20bronchoscopy%3A%20stenosis%0A%20%20severity%20estimation%0AAuthor%3A%20Clara%20Tomasini%20and%20Javier%20Rodriguez-Puigvert%20and%20Dinora%20Polanco%20and%20Manuel%20Vi%C3%B1uales%20and%20Luis%20Riazuelo%20and%20Ana%20Cristina%20Murillo%0AAbstract%3A%20%20%20Purpose%3A%20Subglottic%20stenosis%20refers%20to%20the%20narrowing%20of%20the%20subglottis%2C%20the%0Aairway%20between%20the%20vocal%20cords%20and%20the%20trachea.%20Its%20severity%20is%20typically%0Aevaluated%20by%20estimating%20the%20percentage%20of%20obstructed%20airway.%20This%20estimation%0Acan%20be%20obtained%20from%20CT%20data%20or%20through%20visual%20inspection%20by%20experts%20exploring%0Athe%20region.%20However%2C%20visual%20inspections%20are%20inherently%20subjective%2C%20leading%20to%0Aless%20consistent%20and%20robust%20diagnoses.%20No%20public%20methods%20or%20datasets%20are%0Acurrently%20available%20for%20automated%20evaluation%20of%20this%20condition%20from%0Abronchoscopy%20video.%0A%20%20Methods%3A%20We%20propose%20a%20pipeline%20for%20automated%20subglottic%20stenosis%20severity%0Aestimation%20during%20the%20bronchoscopy%20exploration%2C%20without%20requiring%20the%20physician%0Ato%20traverse%20the%20stenosed%20region.%20Our%20approach%20exploits%20the%20physical%20effect%20of%0Aillumination%20decline%20in%20endoscopy%20to%20segment%20and%20track%20the%20lumen%20and%20obtain%20a%0A3D%20model%20of%20the%20airway.%20This%203D%20model%20is%20obtained%20from%20a%20single%20frame%20and%20is%0Aused%20to%20measure%20the%20airway%20narrowing.%0A%20%20Results%3A%20Our%20pipeline%20is%20the%20first%20to%20enable%20automated%20and%20robust%20subglottic%0Astenosis%20severity%20measurement%20using%20bronchoscopy%20images.%20The%20results%20show%0Aconsistency%20with%20ground-truth%20estimations%20from%20CT%20scans%20and%20expert%20estimations%2C%0Aand%20reliable%20repeatability%20across%20multiple%20estimations%20on%20the%20same%20patient.%20Our%0Aevaluation%20is%20performed%20on%20our%20new%20Subglottic%20Stenosis%20Dataset%20of%20real%0Abronchoscopy%20procedures%20data.%0A%20%20Conclusion%3A%20We%20demonstrate%20how%20to%20automate%20evaluation%20of%20subglottic%20stenosis%0Aseverity%20using%20only%20bronchoscopy.%20Our%20approach%20can%20assist%20with%20and%20shorten%0Adiagnosis%20and%20monitoring%20procedures%2C%20with%20automated%20and%20repeatable%20estimations%0Aand%20less%20exploration%20time%2C%20and%20save%20radiation%20exposure%20to%20patients%20as%20no%20CT%20is%0Arequired.%20Additionally%2C%20we%20release%20the%20first%20public%20benchmark%20for%20subglottic%0Astenosis%20severity%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520vision-based%2520assistance%2520tools%2520in%2520bronchoscopy%253A%2520stenosis%250A%2520%2520severity%2520estimation%26entry.906535625%3DClara%2520Tomasini%2520and%2520Javier%2520Rodriguez-Puigvert%2520and%2520Dinora%2520Polanco%2520and%2520Manuel%2520Vi%25C3%25B1uales%2520and%2520Luis%2520Riazuelo%2520and%2520Ana%2520Cristina%2520Murillo%26entry.1292438233%3D%2520%2520Purpose%253A%2520Subglottic%2520stenosis%2520refers%2520to%2520the%2520narrowing%2520of%2520the%2520subglottis%252C%2520the%250Aairway%2520between%2520the%2520vocal%2520cords%2520and%2520the%2520trachea.%2520Its%2520severity%2520is%2520typically%250Aevaluated%2520by%2520estimating%2520the%2520percentage%2520of%2520obstructed%2520airway.%2520This%2520estimation%250Acan%2520be%2520obtained%2520from%2520CT%2520data%2520or%2520through%2520visual%2520inspection%2520by%2520experts%2520exploring%250Athe%2520region.%2520However%252C%2520visual%2520inspections%2520are%2520inherently%2520subjective%252C%2520leading%2520to%250Aless%2520consistent%2520and%2520robust%2520diagnoses.%2520No%2520public%2520methods%2520or%2520datasets%2520are%250Acurrently%2520available%2520for%2520automated%2520evaluation%2520of%2520this%2520condition%2520from%250Abronchoscopy%2520video.%250A%2520%2520Methods%253A%2520We%2520propose%2520a%2520pipeline%2520for%2520automated%2520subglottic%2520stenosis%2520severity%250Aestimation%2520during%2520the%2520bronchoscopy%2520exploration%252C%2520without%2520requiring%2520the%2520physician%250Ato%2520traverse%2520the%2520stenosed%2520region.%2520Our%2520approach%2520exploits%2520the%2520physical%2520effect%2520of%250Aillumination%2520decline%2520in%2520endoscopy%2520to%2520segment%2520and%2520track%2520the%2520lumen%2520and%2520obtain%2520a%250A3D%2520model%2520of%2520the%2520airway.%2520This%25203D%2520model%2520is%2520obtained%2520from%2520a%2520single%2520frame%2520and%2520is%250Aused%2520to%2520measure%2520the%2520airway%2520narrowing.%250A%2520%2520Results%253A%2520Our%2520pipeline%2520is%2520the%2520first%2520to%2520enable%2520automated%2520and%2520robust%2520subglottic%250Astenosis%2520severity%2520measurement%2520using%2520bronchoscopy%2520images.%2520The%2520results%2520show%250Aconsistency%2520with%2520ground-truth%2520estimations%2520from%2520CT%2520scans%2520and%2520expert%2520estimations%252C%250Aand%2520reliable%2520repeatability%2520across%2520multiple%2520estimations%2520on%2520the%2520same%2520patient.%2520Our%250Aevaluation%2520is%2520performed%2520on%2520our%2520new%2520Subglottic%2520Stenosis%2520Dataset%2520of%2520real%250Abronchoscopy%2520procedures%2520data.%250A%2520%2520Conclusion%253A%2520We%2520demonstrate%2520how%2520to%2520automate%2520evaluation%2520of%2520subglottic%2520stenosis%250Aseverity%2520using%2520only%2520bronchoscopy.%2520Our%2520approach%2520can%2520assist%2520with%2520and%2520shorten%250Adiagnosis%2520and%2520monitoring%2520procedures%252C%2520with%2520automated%2520and%2520repeatable%2520estimations%250Aand%2520less%2520exploration%2520time%252C%2520and%2520save%2520radiation%2520exposure%2520to%2520patients%2520as%2520no%2520CT%2520is%250Arequired.%2520Additionally%252C%2520we%2520release%2520the%2520first%2520public%2520benchmark%2520for%2520subglottic%250Astenosis%2520severity%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20vision-based%20assistance%20tools%20in%20bronchoscopy%3A%20stenosis%0A%20%20severity%20estimation&entry.906535625=Clara%20Tomasini%20and%20Javier%20Rodriguez-Puigvert%20and%20Dinora%20Polanco%20and%20Manuel%20Vi%C3%B1uales%20and%20Luis%20Riazuelo%20and%20Ana%20Cristina%20Murillo&entry.1292438233=%20%20Purpose%3A%20Subglottic%20stenosis%20refers%20to%20the%20narrowing%20of%20the%20subglottis%2C%20the%0Aairway%20between%20the%20vocal%20cords%20and%20the%20trachea.%20Its%20severity%20is%20typically%0Aevaluated%20by%20estimating%20the%20percentage%20of%20obstructed%20airway.%20This%20estimation%0Acan%20be%20obtained%20from%20CT%20data%20or%20through%20visual%20inspection%20by%20experts%20exploring%0Athe%20region.%20However%2C%20visual%20inspections%20are%20inherently%20subjective%2C%20leading%20to%0Aless%20consistent%20and%20robust%20diagnoses.%20No%20public%20methods%20or%20datasets%20are%0Acurrently%20available%20for%20automated%20evaluation%20of%20this%20condition%20from%0Abronchoscopy%20video.%0A%20%20Methods%3A%20We%20propose%20a%20pipeline%20for%20automated%20subglottic%20stenosis%20severity%0Aestimation%20during%20the%20bronchoscopy%20exploration%2C%20without%20requiring%20the%20physician%0Ato%20traverse%20the%20stenosed%20region.%20Our%20approach%20exploits%20the%20physical%20effect%20of%0Aillumination%20decline%20in%20endoscopy%20to%20segment%20and%20track%20the%20lumen%20and%20obtain%20a%0A3D%20model%20of%20the%20airway.%20This%203D%20model%20is%20obtained%20from%20a%20single%20frame%20and%20is%0Aused%20to%20measure%20the%20airway%20narrowing.%0A%20%20Results%3A%20Our%20pipeline%20is%20the%20first%20to%20enable%20automated%20and%20robust%20subglottic%0Astenosis%20severity%20measurement%20using%20bronchoscopy%20images.%20The%20results%20show%0Aconsistency%20with%20ground-truth%20estimations%20from%20CT%20scans%20and%20expert%20estimations%2C%0Aand%20reliable%20repeatability%20across%20multiple%20estimations%20on%20the%20same%20patient.%20Our%0Aevaluation%20is%20performed%20on%20our%20new%20Subglottic%20Stenosis%20Dataset%20of%20real%0Abronchoscopy%20procedures%20data.%0A%20%20Conclusion%3A%20We%20demonstrate%20how%20to%20automate%20evaluation%20of%20subglottic%20stenosis%0Aseverity%20using%20only%20bronchoscopy.%20Our%20approach%20can%20assist%20with%20and%20shorten%0Adiagnosis%20and%20monitoring%20procedures%2C%20with%20automated%20and%20repeatable%20estimations%0Aand%20less%20exploration%20time%2C%20and%20save%20radiation%20exposure%20to%20patients%20as%20no%20CT%20is%0Arequired.%20Additionally%2C%20we%20release%20the%20first%20public%20benchmark%20for%20subglottic%0Astenosis%20severity%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05136v1&entry.124074799=Read"},
{"title": "Electrocardiogram-Language Model for Few-Shot Question Answering with\n  Meta Learning", "author": "Jialu Tang and Tong Xia and Yuan Lu and Cecilia Mascolo and Aaqib Saeed", "abstract": "  Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves an accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios.\n", "link": "http://arxiv.org/abs/2410.14464v2", "date": "2025-05-08", "relevancy": 1.9741, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5209}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Electrocardiogram-Language%20Model%20for%20Few-Shot%20Question%20Answering%20with%0A%20%20Meta%20Learning&body=Title%3A%20Electrocardiogram-Language%20Model%20for%20Few-Shot%20Question%20Answering%20with%0A%20%20Meta%20Learning%0AAuthor%3A%20Jialu%20Tang%20and%20Tong%20Xia%20and%20Yuan%20Lu%20and%20Cecilia%20Mascolo%20and%20Aaqib%20Saeed%0AAbstract%3A%20%20%20Electrocardiogram%20%28ECG%29%20interpretation%20requires%20specialized%20expertise%2C%20often%0Ainvolving%20synthesizing%20insights%20from%20ECG%20signals%20with%20complex%20clinical%20queries%0Aposed%20in%20natural%20language.%20The%20scarcity%20of%20labeled%20ECG%20data%20coupled%20with%20the%0Adiverse%20nature%20of%20clinical%20inquiries%20presents%20a%20significant%20challenge%20for%0Adeveloping%20robust%20and%20adaptable%20ECG%20diagnostic%20systems.%20This%20work%20introduces%20a%0Anovel%20multimodal%20meta-learning%20method%20for%20few-shot%20ECG%20question%20answering%2C%0Aaddressing%20the%20challenge%20of%20limited%20labeled%20data%20while%20leveraging%20the%20rich%0Aknowledge%20encoded%20within%20large%20language%20models%20%28LLMs%29.%20Our%20LLM-agnostic%0Aapproach%20integrates%20a%20pre-trained%20ECG%20encoder%20with%20a%20frozen%20LLM%20%28e.g.%2C%20LLaMA%0Aand%20Gemma%29%20via%20a%20trainable%20fusion%20module%2C%20enabling%20the%20language%20model%20to%20reason%0Aabout%20ECG%20data%20and%20generate%20clinically%20meaningful%20answers.%20Extensive%0Aexperiments%20demonstrate%20superior%20generalization%20to%20unseen%20diagnostic%20tasks%0Acompared%20to%20supervised%20baselines%2C%20achieving%20notable%20performance%20even%20with%0Alimited%20ECG%20leads.%20For%20instance%2C%20in%20a%205-way%205-shot%20setting%2C%20our%20method%20using%0ALLaMA-3.1-8B%20achieves%20an%20accuracy%20of%2084.6%25%2C%2077.3%25%2C%20and%2069.6%25%20on%20single%20verify%2C%0Achoose%20and%20query%20question%20types%2C%20respectively.%20These%20results%20highlight%20the%0Apotential%20of%20our%20method%20to%20enhance%20clinical%20ECG%20interpretation%20by%20combining%0Asignal%20processing%20with%20the%20nuanced%20language%20understanding%20capabilities%20of%20LLMs%2C%0Aparticularly%20in%20data-constrained%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14464v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElectrocardiogram-Language%2520Model%2520for%2520Few-Shot%2520Question%2520Answering%2520with%250A%2520%2520Meta%2520Learning%26entry.906535625%3DJialu%2520Tang%2520and%2520Tong%2520Xia%2520and%2520Yuan%2520Lu%2520and%2520Cecilia%2520Mascolo%2520and%2520Aaqib%2520Saeed%26entry.1292438233%3D%2520%2520Electrocardiogram%2520%2528ECG%2529%2520interpretation%2520requires%2520specialized%2520expertise%252C%2520often%250Ainvolving%2520synthesizing%2520insights%2520from%2520ECG%2520signals%2520with%2520complex%2520clinical%2520queries%250Aposed%2520in%2520natural%2520language.%2520The%2520scarcity%2520of%2520labeled%2520ECG%2520data%2520coupled%2520with%2520the%250Adiverse%2520nature%2520of%2520clinical%2520inquiries%2520presents%2520a%2520significant%2520challenge%2520for%250Adeveloping%2520robust%2520and%2520adaptable%2520ECG%2520diagnostic%2520systems.%2520This%2520work%2520introduces%2520a%250Anovel%2520multimodal%2520meta-learning%2520method%2520for%2520few-shot%2520ECG%2520question%2520answering%252C%250Aaddressing%2520the%2520challenge%2520of%2520limited%2520labeled%2520data%2520while%2520leveraging%2520the%2520rich%250Aknowledge%2520encoded%2520within%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Our%2520LLM-agnostic%250Aapproach%2520integrates%2520a%2520pre-trained%2520ECG%2520encoder%2520with%2520a%2520frozen%2520LLM%2520%2528e.g.%252C%2520LLaMA%250Aand%2520Gemma%2529%2520via%2520a%2520trainable%2520fusion%2520module%252C%2520enabling%2520the%2520language%2520model%2520to%2520reason%250Aabout%2520ECG%2520data%2520and%2520generate%2520clinically%2520meaningful%2520answers.%2520Extensive%250Aexperiments%2520demonstrate%2520superior%2520generalization%2520to%2520unseen%2520diagnostic%2520tasks%250Acompared%2520to%2520supervised%2520baselines%252C%2520achieving%2520notable%2520performance%2520even%2520with%250Alimited%2520ECG%2520leads.%2520For%2520instance%252C%2520in%2520a%25205-way%25205-shot%2520setting%252C%2520our%2520method%2520using%250ALLaMA-3.1-8B%2520achieves%2520an%2520accuracy%2520of%252084.6%2525%252C%252077.3%2525%252C%2520and%252069.6%2525%2520on%2520single%2520verify%252C%250Achoose%2520and%2520query%2520question%2520types%252C%2520respectively.%2520These%2520results%2520highlight%2520the%250Apotential%2520of%2520our%2520method%2520to%2520enhance%2520clinical%2520ECG%2520interpretation%2520by%2520combining%250Asignal%2520processing%2520with%2520the%2520nuanced%2520language%2520understanding%2520capabilities%2520of%2520LLMs%252C%250Aparticularly%2520in%2520data-constrained%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14464v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Electrocardiogram-Language%20Model%20for%20Few-Shot%20Question%20Answering%20with%0A%20%20Meta%20Learning&entry.906535625=Jialu%20Tang%20and%20Tong%20Xia%20and%20Yuan%20Lu%20and%20Cecilia%20Mascolo%20and%20Aaqib%20Saeed&entry.1292438233=%20%20Electrocardiogram%20%28ECG%29%20interpretation%20requires%20specialized%20expertise%2C%20often%0Ainvolving%20synthesizing%20insights%20from%20ECG%20signals%20with%20complex%20clinical%20queries%0Aposed%20in%20natural%20language.%20The%20scarcity%20of%20labeled%20ECG%20data%20coupled%20with%20the%0Adiverse%20nature%20of%20clinical%20inquiries%20presents%20a%20significant%20challenge%20for%0Adeveloping%20robust%20and%20adaptable%20ECG%20diagnostic%20systems.%20This%20work%20introduces%20a%0Anovel%20multimodal%20meta-learning%20method%20for%20few-shot%20ECG%20question%20answering%2C%0Aaddressing%20the%20challenge%20of%20limited%20labeled%20data%20while%20leveraging%20the%20rich%0Aknowledge%20encoded%20within%20large%20language%20models%20%28LLMs%29.%20Our%20LLM-agnostic%0Aapproach%20integrates%20a%20pre-trained%20ECG%20encoder%20with%20a%20frozen%20LLM%20%28e.g.%2C%20LLaMA%0Aand%20Gemma%29%20via%20a%20trainable%20fusion%20module%2C%20enabling%20the%20language%20model%20to%20reason%0Aabout%20ECG%20data%20and%20generate%20clinically%20meaningful%20answers.%20Extensive%0Aexperiments%20demonstrate%20superior%20generalization%20to%20unseen%20diagnostic%20tasks%0Acompared%20to%20supervised%20baselines%2C%20achieving%20notable%20performance%20even%20with%0Alimited%20ECG%20leads.%20For%20instance%2C%20in%20a%205-way%205-shot%20setting%2C%20our%20method%20using%0ALLaMA-3.1-8B%20achieves%20an%20accuracy%20of%2084.6%25%2C%2077.3%25%2C%20and%2069.6%25%20on%20single%20verify%2C%0Achoose%20and%20query%20question%20types%2C%20respectively.%20These%20results%20highlight%20the%0Apotential%20of%20our%20method%20to%20enhance%20clinical%20ECG%20interpretation%20by%20combining%0Asignal%20processing%20with%20the%20nuanced%20language%20understanding%20capabilities%20of%20LLMs%2C%0Aparticularly%20in%20data-constrained%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14464v2&entry.124074799=Read"},
{"title": "TransProQA: an LLM-based literary Translation evaluation metric with\n  Professional Question Answering", "author": "Ran Zhang and Wei Zhao and Lieve Macken and Steffen Eger", "abstract": "  The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.\n", "link": "http://arxiv.org/abs/2505.05423v1", "date": "2025-05-08", "relevancy": 1.9691, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransProQA%3A%20an%20LLM-based%20literary%20Translation%20evaluation%20metric%20with%0A%20%20Professional%20Question%20Answering&body=Title%3A%20TransProQA%3A%20an%20LLM-based%20literary%20Translation%20evaluation%20metric%20with%0A%20%20Professional%20Question%20Answering%0AAuthor%3A%20Ran%20Zhang%20and%20Wei%20Zhao%20and%20Lieve%20Macken%20and%20Steffen%20Eger%0AAbstract%3A%20%20%20The%20impact%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20extended%20into%20literary%0Adomains.%20However%2C%20existing%20evaluation%20metrics%20prioritize%20mechanical%20accuracy%0Aover%20artistic%20expression%20and%20tend%20to%20overrate%20machine%20translation%20%28MT%29%20as%20being%0Asuperior%20to%20experienced%20professional%20human%20translation.%20In%20the%20long%20run%2C%20this%0Abias%20could%20result%20in%20a%20permanent%20decline%20in%20translation%20quality%20and%20cultural%0Aauthenticity.%20In%20response%20to%20the%20urgent%20need%20for%20a%20specialized%20literary%0Aevaluation%20metric%2C%20we%20introduce%20TransProQA%2C%20a%20novel%2C%20reference-free%2C%20LLM-based%0Aquestion-answering%20%28QA%29%20framework%20designed%20specifically%20for%20literary%0Atranslation%20evaluation.%20TransProQA%20uniquely%20integrates%20insights%20from%0Aprofessional%20literary%20translators%20and%20researchers%2C%20focusing%20on%20critical%0Aelements%20in%20literary%20quality%20assessment%20such%20as%20literary%20devices%2C%20cultural%0Aunderstanding%2C%20and%20authorial%20voice.%20Our%20extensive%20evaluation%20shows%20that%20while%0Aliterary-finetuned%20XCOMET-XL%20yields%20marginal%20gains%2C%20TransProQA%20substantially%0Aoutperforms%20current%20metrics%2C%20achieving%20up%20to%200.07%20gain%20in%20correlation%20%28ACC-EQ%0Aand%20Kendall%27s%20tau%29%20and%20surpassing%20the%20best%20state-of-the-art%20%28SOTA%29%20metrics%20by%0Aover%2015%20points%20in%20adequacy%20assessments.%20Incorporating%20professional%20translator%0Ainsights%20as%20weights%20further%20improves%20performance%2C%20highlighting%20the%20value%20of%0Atranslator%20inputs.%20Notably%2C%20TransProQA%20approaches%20human-level%20evaluation%0Aperformance%20comparable%20to%20trained%20linguistic%20annotators.%20It%20demonstrates%20broad%0Aapplicability%20to%20open-source%20models%20such%20as%20LLaMA3.3-70b%20and%20Qwen2.5-32b%2C%0Aindicating%20its%20potential%20as%20an%20accessible%20and%20training-free%20literary%20evaluation%0Ametric%20and%20a%20valuable%20tool%20for%20evaluating%20texts%20that%20require%20local%20processing%0Adue%20to%20copyright%20or%20ethical%20considerations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransProQA%253A%2520an%2520LLM-based%2520literary%2520Translation%2520evaluation%2520metric%2520with%250A%2520%2520Professional%2520Question%2520Answering%26entry.906535625%3DRan%2520Zhang%2520and%2520Wei%2520Zhao%2520and%2520Lieve%2520Macken%2520and%2520Steffen%2520Eger%26entry.1292438233%3D%2520%2520The%2520impact%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520extended%2520into%2520literary%250Adomains.%2520However%252C%2520existing%2520evaluation%2520metrics%2520prioritize%2520mechanical%2520accuracy%250Aover%2520artistic%2520expression%2520and%2520tend%2520to%2520overrate%2520machine%2520translation%2520%2528MT%2529%2520as%2520being%250Asuperior%2520to%2520experienced%2520professional%2520human%2520translation.%2520In%2520the%2520long%2520run%252C%2520this%250Abias%2520could%2520result%2520in%2520a%2520permanent%2520decline%2520in%2520translation%2520quality%2520and%2520cultural%250Aauthenticity.%2520In%2520response%2520to%2520the%2520urgent%2520need%2520for%2520a%2520specialized%2520literary%250Aevaluation%2520metric%252C%2520we%2520introduce%2520TransProQA%252C%2520a%2520novel%252C%2520reference-free%252C%2520LLM-based%250Aquestion-answering%2520%2528QA%2529%2520framework%2520designed%2520specifically%2520for%2520literary%250Atranslation%2520evaluation.%2520TransProQA%2520uniquely%2520integrates%2520insights%2520from%250Aprofessional%2520literary%2520translators%2520and%2520researchers%252C%2520focusing%2520on%2520critical%250Aelements%2520in%2520literary%2520quality%2520assessment%2520such%2520as%2520literary%2520devices%252C%2520cultural%250Aunderstanding%252C%2520and%2520authorial%2520voice.%2520Our%2520extensive%2520evaluation%2520shows%2520that%2520while%250Aliterary-finetuned%2520XCOMET-XL%2520yields%2520marginal%2520gains%252C%2520TransProQA%2520substantially%250Aoutperforms%2520current%2520metrics%252C%2520achieving%2520up%2520to%25200.07%2520gain%2520in%2520correlation%2520%2528ACC-EQ%250Aand%2520Kendall%2527s%2520tau%2529%2520and%2520surpassing%2520the%2520best%2520state-of-the-art%2520%2528SOTA%2529%2520metrics%2520by%250Aover%252015%2520points%2520in%2520adequacy%2520assessments.%2520Incorporating%2520professional%2520translator%250Ainsights%2520as%2520weights%2520further%2520improves%2520performance%252C%2520highlighting%2520the%2520value%2520of%250Atranslator%2520inputs.%2520Notably%252C%2520TransProQA%2520approaches%2520human-level%2520evaluation%250Aperformance%2520comparable%2520to%2520trained%2520linguistic%2520annotators.%2520It%2520demonstrates%2520broad%250Aapplicability%2520to%2520open-source%2520models%2520such%2520as%2520LLaMA3.3-70b%2520and%2520Qwen2.5-32b%252C%250Aindicating%2520its%2520potential%2520as%2520an%2520accessible%2520and%2520training-free%2520literary%2520evaluation%250Ametric%2520and%2520a%2520valuable%2520tool%2520for%2520evaluating%2520texts%2520that%2520require%2520local%2520processing%250Adue%2520to%2520copyright%2520or%2520ethical%2520considerations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransProQA%3A%20an%20LLM-based%20literary%20Translation%20evaluation%20metric%20with%0A%20%20Professional%20Question%20Answering&entry.906535625=Ran%20Zhang%20and%20Wei%20Zhao%20and%20Lieve%20Macken%20and%20Steffen%20Eger&entry.1292438233=%20%20The%20impact%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20extended%20into%20literary%0Adomains.%20However%2C%20existing%20evaluation%20metrics%20prioritize%20mechanical%20accuracy%0Aover%20artistic%20expression%20and%20tend%20to%20overrate%20machine%20translation%20%28MT%29%20as%20being%0Asuperior%20to%20experienced%20professional%20human%20translation.%20In%20the%20long%20run%2C%20this%0Abias%20could%20result%20in%20a%20permanent%20decline%20in%20translation%20quality%20and%20cultural%0Aauthenticity.%20In%20response%20to%20the%20urgent%20need%20for%20a%20specialized%20literary%0Aevaluation%20metric%2C%20we%20introduce%20TransProQA%2C%20a%20novel%2C%20reference-free%2C%20LLM-based%0Aquestion-answering%20%28QA%29%20framework%20designed%20specifically%20for%20literary%0Atranslation%20evaluation.%20TransProQA%20uniquely%20integrates%20insights%20from%0Aprofessional%20literary%20translators%20and%20researchers%2C%20focusing%20on%20critical%0Aelements%20in%20literary%20quality%20assessment%20such%20as%20literary%20devices%2C%20cultural%0Aunderstanding%2C%20and%20authorial%20voice.%20Our%20extensive%20evaluation%20shows%20that%20while%0Aliterary-finetuned%20XCOMET-XL%20yields%20marginal%20gains%2C%20TransProQA%20substantially%0Aoutperforms%20current%20metrics%2C%20achieving%20up%20to%200.07%20gain%20in%20correlation%20%28ACC-EQ%0Aand%20Kendall%27s%20tau%29%20and%20surpassing%20the%20best%20state-of-the-art%20%28SOTA%29%20metrics%20by%0Aover%2015%20points%20in%20adequacy%20assessments.%20Incorporating%20professional%20translator%0Ainsights%20as%20weights%20further%20improves%20performance%2C%20highlighting%20the%20value%20of%0Atranslator%20inputs.%20Notably%2C%20TransProQA%20approaches%20human-level%20evaluation%0Aperformance%20comparable%20to%20trained%20linguistic%20annotators.%20It%20demonstrates%20broad%0Aapplicability%20to%20open-source%20models%20such%20as%20LLaMA3.3-70b%20and%20Qwen2.5-32b%2C%0Aindicating%20its%20potential%20as%20an%20accessible%20and%20training-free%20literary%20evaluation%0Ametric%20and%20a%20valuable%20tool%20for%20evaluating%20texts%20that%20require%20local%20processing%0Adue%20to%20copyright%20or%20ethical%20considerations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05423v1&entry.124074799=Read"},
{"title": "Crosslingual Reasoning through Test-Time Scaling", "author": "Zheng-Xin Yong and M. Farid Adilazuarda and Jonibek Mansurov and Ruochen Zhang and Niklas Muennighoff and Carsten Eickhoff and Genta Indra Winata and Julia Kreutzer and Stephen H. Bach and Alham Fikri Aji", "abstract": "  Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.\n", "link": "http://arxiv.org/abs/2505.05408v1", "date": "2025-05-08", "relevancy": 1.9649, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Crosslingual%20Reasoning%20through%20Test-Time%20Scaling&body=Title%3A%20Crosslingual%20Reasoning%20through%20Test-Time%20Scaling%0AAuthor%3A%20Zheng-Xin%20Yong%20and%20M.%20Farid%20Adilazuarda%20and%20Jonibek%20Mansurov%20and%20Ruochen%20Zhang%20and%20Niklas%20Muennighoff%20and%20Carsten%20Eickhoff%20and%20Genta%20Indra%20Winata%20and%20Julia%20Kreutzer%20and%20Stephen%20H.%20Bach%20and%20Alham%20Fikri%20Aji%0AAbstract%3A%20%20%20Reasoning%20capabilities%20of%20large%20language%20models%20are%20primarily%20studied%20for%0AEnglish%2C%20even%20when%20pretrained%20models%20are%20multilingual.%20In%20this%20work%2C%20we%0Ainvestigate%20to%20what%20extent%20English%20reasoning%20finetuning%20with%20long%0Achain-of-thoughts%20%28CoTs%29%20can%20generalize%20across%20languages.%20First%2C%20we%20find%20that%0Ascaling%20up%20inference%20compute%20for%20English-centric%20reasoning%20language%20models%0A%28RLMs%29%20improves%20multilingual%20mathematical%20reasoning%20across%20many%20languages%0Aincluding%20low-resource%20languages%2C%20to%20an%20extent%20where%20they%20outperform%20models%0Atwice%20their%20size.%20Second%2C%20we%20reveal%20that%20while%20English-centric%20RLM%27s%20CoTs%20are%0Anaturally%20predominantly%20English%2C%20they%20consistently%20follow%20a%20quote-and-think%0Apattern%20to%20reason%20about%20quoted%20non-English%20inputs.%20Third%2C%20we%20discover%20an%0Aeffective%20strategy%20to%20control%20the%20language%20of%20long%20CoT%20reasoning%2C%20and%20we%0Aobserve%20that%20models%20reason%20better%20and%20more%20efficiently%20in%20high-resource%0Alanguages.%20Finally%2C%20we%20observe%20poor%20out-of-domain%20reasoning%20generalization%2C%20in%0Aparticular%20from%20STEM%20to%20cultural%20commonsense%20knowledge%2C%20even%20for%20English.%0AOverall%2C%20we%20demonstrate%20the%20potentials%2C%20study%20the%20mechanisms%20and%20outline%20the%0Alimitations%20of%20crosslingual%20generalization%20of%20English%20reasoning%20test-time%0Ascaling.%20We%20conclude%20that%20practitioners%20should%20let%20English-centric%20RLMs%20reason%0Ain%20high-resource%20languages%2C%20while%20further%20work%20is%20needed%20to%20improve%20reasoning%0Ain%20low-resource%20languages%20and%20out-of-domain%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrosslingual%2520Reasoning%2520through%2520Test-Time%2520Scaling%26entry.906535625%3DZheng-Xin%2520Yong%2520and%2520M.%2520Farid%2520Adilazuarda%2520and%2520Jonibek%2520Mansurov%2520and%2520Ruochen%2520Zhang%2520and%2520Niklas%2520Muennighoff%2520and%2520Carsten%2520Eickhoff%2520and%2520Genta%2520Indra%2520Winata%2520and%2520Julia%2520Kreutzer%2520and%2520Stephen%2520H.%2520Bach%2520and%2520Alham%2520Fikri%2520Aji%26entry.1292438233%3D%2520%2520Reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520are%2520primarily%2520studied%2520for%250AEnglish%252C%2520even%2520when%2520pretrained%2520models%2520are%2520multilingual.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520to%2520what%2520extent%2520English%2520reasoning%2520finetuning%2520with%2520long%250Achain-of-thoughts%2520%2528CoTs%2529%2520can%2520generalize%2520across%2520languages.%2520First%252C%2520we%2520find%2520that%250Ascaling%2520up%2520inference%2520compute%2520for%2520English-centric%2520reasoning%2520language%2520models%250A%2528RLMs%2529%2520improves%2520multilingual%2520mathematical%2520reasoning%2520across%2520many%2520languages%250Aincluding%2520low-resource%2520languages%252C%2520to%2520an%2520extent%2520where%2520they%2520outperform%2520models%250Atwice%2520their%2520size.%2520Second%252C%2520we%2520reveal%2520that%2520while%2520English-centric%2520RLM%2527s%2520CoTs%2520are%250Anaturally%2520predominantly%2520English%252C%2520they%2520consistently%2520follow%2520a%2520quote-and-think%250Apattern%2520to%2520reason%2520about%2520quoted%2520non-English%2520inputs.%2520Third%252C%2520we%2520discover%2520an%250Aeffective%2520strategy%2520to%2520control%2520the%2520language%2520of%2520long%2520CoT%2520reasoning%252C%2520and%2520we%250Aobserve%2520that%2520models%2520reason%2520better%2520and%2520more%2520efficiently%2520in%2520high-resource%250Alanguages.%2520Finally%252C%2520we%2520observe%2520poor%2520out-of-domain%2520reasoning%2520generalization%252C%2520in%250Aparticular%2520from%2520STEM%2520to%2520cultural%2520commonsense%2520knowledge%252C%2520even%2520for%2520English.%250AOverall%252C%2520we%2520demonstrate%2520the%2520potentials%252C%2520study%2520the%2520mechanisms%2520and%2520outline%2520the%250Alimitations%2520of%2520crosslingual%2520generalization%2520of%2520English%2520reasoning%2520test-time%250Ascaling.%2520We%2520conclude%2520that%2520practitioners%2520should%2520let%2520English-centric%2520RLMs%2520reason%250Ain%2520high-resource%2520languages%252C%2520while%2520further%2520work%2520is%2520needed%2520to%2520improve%2520reasoning%250Ain%2520low-resource%2520languages%2520and%2520out-of-domain%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Crosslingual%20Reasoning%20through%20Test-Time%20Scaling&entry.906535625=Zheng-Xin%20Yong%20and%20M.%20Farid%20Adilazuarda%20and%20Jonibek%20Mansurov%20and%20Ruochen%20Zhang%20and%20Niklas%20Muennighoff%20and%20Carsten%20Eickhoff%20and%20Genta%20Indra%20Winata%20and%20Julia%20Kreutzer%20and%20Stephen%20H.%20Bach%20and%20Alham%20Fikri%20Aji&entry.1292438233=%20%20Reasoning%20capabilities%20of%20large%20language%20models%20are%20primarily%20studied%20for%0AEnglish%2C%20even%20when%20pretrained%20models%20are%20multilingual.%20In%20this%20work%2C%20we%0Ainvestigate%20to%20what%20extent%20English%20reasoning%20finetuning%20with%20long%0Achain-of-thoughts%20%28CoTs%29%20can%20generalize%20across%20languages.%20First%2C%20we%20find%20that%0Ascaling%20up%20inference%20compute%20for%20English-centric%20reasoning%20language%20models%0A%28RLMs%29%20improves%20multilingual%20mathematical%20reasoning%20across%20many%20languages%0Aincluding%20low-resource%20languages%2C%20to%20an%20extent%20where%20they%20outperform%20models%0Atwice%20their%20size.%20Second%2C%20we%20reveal%20that%20while%20English-centric%20RLM%27s%20CoTs%20are%0Anaturally%20predominantly%20English%2C%20they%20consistently%20follow%20a%20quote-and-think%0Apattern%20to%20reason%20about%20quoted%20non-English%20inputs.%20Third%2C%20we%20discover%20an%0Aeffective%20strategy%20to%20control%20the%20language%20of%20long%20CoT%20reasoning%2C%20and%20we%0Aobserve%20that%20models%20reason%20better%20and%20more%20efficiently%20in%20high-resource%0Alanguages.%20Finally%2C%20we%20observe%20poor%20out-of-domain%20reasoning%20generalization%2C%20in%0Aparticular%20from%20STEM%20to%20cultural%20commonsense%20knowledge%2C%20even%20for%20English.%0AOverall%2C%20we%20demonstrate%20the%20potentials%2C%20study%20the%20mechanisms%20and%20outline%20the%0Alimitations%20of%20crosslingual%20generalization%20of%20English%20reasoning%20test-time%0Ascaling.%20We%20conclude%20that%20practitioners%20should%20let%20English-centric%20RLMs%20reason%0Ain%20high-resource%20languages%2C%20while%20further%20work%20is%20needed%20to%20improve%20reasoning%0Ain%20low-resource%20languages%20and%20out-of-domain%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05408v1&entry.124074799=Read"},
{"title": "Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemannian\n  Geometry Finds It", "author": "Marvin F. da Silva and Felix Dangel and Sageev Oore", "abstract": "  The concept of sharpness has been successfully applied to traditional\narchitectures like MLPs and CNNs to predict their generalization. For\ntransformers, however, recent work reported weak correlation between flatness\nand generalization. We argue that existing sharpness measures fail for\ntransformers, because they have much richer symmetries in their attention\nmechanism that induce directions in parameter space along which the network or\nits loss remain identical. We posit that sharpness must account fully for these\nsymmetries, and thus we redefine it on a quotient manifold that results from\nquotienting out the transformer symmetries, thereby removing their ambiguities.\nLeveraging tools from Riemannian geometry, we propose a fully general notion of\nsharpness, in terms of a geodesic ball on the symmetry-corrected quotient\nmanifold. In practice, we need to resort to approximating the geodesics. Doing\nso up to first order yields existing adaptive sharpness measures, and we\ndemonstrate that including higher-order terms is crucial to recover correlation\nwith generalization. We present results on diagonal networks with synthetic\ndata, and show that our geodesic sharpness reveals strong correlation for\nreal-world transformers on both text and image classification tasks.\n", "link": "http://arxiv.org/abs/2505.05409v1", "date": "2025-05-08", "relevancy": 1.9644, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5788}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4789}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hide%20%26%20Seek%3A%20Transformer%20Symmetries%20Obscure%20Sharpness%20%26%20Riemannian%0A%20%20Geometry%20Finds%20It&body=Title%3A%20Hide%20%26%20Seek%3A%20Transformer%20Symmetries%20Obscure%20Sharpness%20%26%20Riemannian%0A%20%20Geometry%20Finds%20It%0AAuthor%3A%20Marvin%20F.%20da%20Silva%20and%20Felix%20Dangel%20and%20Sageev%20Oore%0AAbstract%3A%20%20%20The%20concept%20of%20sharpness%20has%20been%20successfully%20applied%20to%20traditional%0Aarchitectures%20like%20MLPs%20and%20CNNs%20to%20predict%20their%20generalization.%20For%0Atransformers%2C%20however%2C%20recent%20work%20reported%20weak%20correlation%20between%20flatness%0Aand%20generalization.%20We%20argue%20that%20existing%20sharpness%20measures%20fail%20for%0Atransformers%2C%20because%20they%20have%20much%20richer%20symmetries%20in%20their%20attention%0Amechanism%20that%20induce%20directions%20in%20parameter%20space%20along%20which%20the%20network%20or%0Aits%20loss%20remain%20identical.%20We%20posit%20that%20sharpness%20must%20account%20fully%20for%20these%0Asymmetries%2C%20and%20thus%20we%20redefine%20it%20on%20a%20quotient%20manifold%20that%20results%20from%0Aquotienting%20out%20the%20transformer%20symmetries%2C%20thereby%20removing%20their%20ambiguities.%0ALeveraging%20tools%20from%20Riemannian%20geometry%2C%20we%20propose%20a%20fully%20general%20notion%20of%0Asharpness%2C%20in%20terms%20of%20a%20geodesic%20ball%20on%20the%20symmetry-corrected%20quotient%0Amanifold.%20In%20practice%2C%20we%20need%20to%20resort%20to%20approximating%20the%20geodesics.%20Doing%0Aso%20up%20to%20first%20order%20yields%20existing%20adaptive%20sharpness%20measures%2C%20and%20we%0Ademonstrate%20that%20including%20higher-order%20terms%20is%20crucial%20to%20recover%20correlation%0Awith%20generalization.%20We%20present%20results%20on%20diagonal%20networks%20with%20synthetic%0Adata%2C%20and%20show%20that%20our%20geodesic%20sharpness%20reveals%20strong%20correlation%20for%0Areal-world%20transformers%20on%20both%20text%20and%20image%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHide%2520%2526%2520Seek%253A%2520Transformer%2520Symmetries%2520Obscure%2520Sharpness%2520%2526%2520Riemannian%250A%2520%2520Geometry%2520Finds%2520It%26entry.906535625%3DMarvin%2520F.%2520da%2520Silva%2520and%2520Felix%2520Dangel%2520and%2520Sageev%2520Oore%26entry.1292438233%3D%2520%2520The%2520concept%2520of%2520sharpness%2520has%2520been%2520successfully%2520applied%2520to%2520traditional%250Aarchitectures%2520like%2520MLPs%2520and%2520CNNs%2520to%2520predict%2520their%2520generalization.%2520For%250Atransformers%252C%2520however%252C%2520recent%2520work%2520reported%2520weak%2520correlation%2520between%2520flatness%250Aand%2520generalization.%2520We%2520argue%2520that%2520existing%2520sharpness%2520measures%2520fail%2520for%250Atransformers%252C%2520because%2520they%2520have%2520much%2520richer%2520symmetries%2520in%2520their%2520attention%250Amechanism%2520that%2520induce%2520directions%2520in%2520parameter%2520space%2520along%2520which%2520the%2520network%2520or%250Aits%2520loss%2520remain%2520identical.%2520We%2520posit%2520that%2520sharpness%2520must%2520account%2520fully%2520for%2520these%250Asymmetries%252C%2520and%2520thus%2520we%2520redefine%2520it%2520on%2520a%2520quotient%2520manifold%2520that%2520results%2520from%250Aquotienting%2520out%2520the%2520transformer%2520symmetries%252C%2520thereby%2520removing%2520their%2520ambiguities.%250ALeveraging%2520tools%2520from%2520Riemannian%2520geometry%252C%2520we%2520propose%2520a%2520fully%2520general%2520notion%2520of%250Asharpness%252C%2520in%2520terms%2520of%2520a%2520geodesic%2520ball%2520on%2520the%2520symmetry-corrected%2520quotient%250Amanifold.%2520In%2520practice%252C%2520we%2520need%2520to%2520resort%2520to%2520approximating%2520the%2520geodesics.%2520Doing%250Aso%2520up%2520to%2520first%2520order%2520yields%2520existing%2520adaptive%2520sharpness%2520measures%252C%2520and%2520we%250Ademonstrate%2520that%2520including%2520higher-order%2520terms%2520is%2520crucial%2520to%2520recover%2520correlation%250Awith%2520generalization.%2520We%2520present%2520results%2520on%2520diagonal%2520networks%2520with%2520synthetic%250Adata%252C%2520and%2520show%2520that%2520our%2520geodesic%2520sharpness%2520reveals%2520strong%2520correlation%2520for%250Areal-world%2520transformers%2520on%2520both%2520text%2520and%2520image%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hide%20%26%20Seek%3A%20Transformer%20Symmetries%20Obscure%20Sharpness%20%26%20Riemannian%0A%20%20Geometry%20Finds%20It&entry.906535625=Marvin%20F.%20da%20Silva%20and%20Felix%20Dangel%20and%20Sageev%20Oore&entry.1292438233=%20%20The%20concept%20of%20sharpness%20has%20been%20successfully%20applied%20to%20traditional%0Aarchitectures%20like%20MLPs%20and%20CNNs%20to%20predict%20their%20generalization.%20For%0Atransformers%2C%20however%2C%20recent%20work%20reported%20weak%20correlation%20between%20flatness%0Aand%20generalization.%20We%20argue%20that%20existing%20sharpness%20measures%20fail%20for%0Atransformers%2C%20because%20they%20have%20much%20richer%20symmetries%20in%20their%20attention%0Amechanism%20that%20induce%20directions%20in%20parameter%20space%20along%20which%20the%20network%20or%0Aits%20loss%20remain%20identical.%20We%20posit%20that%20sharpness%20must%20account%20fully%20for%20these%0Asymmetries%2C%20and%20thus%20we%20redefine%20it%20on%20a%20quotient%20manifold%20that%20results%20from%0Aquotienting%20out%20the%20transformer%20symmetries%2C%20thereby%20removing%20their%20ambiguities.%0ALeveraging%20tools%20from%20Riemannian%20geometry%2C%20we%20propose%20a%20fully%20general%20notion%20of%0Asharpness%2C%20in%20terms%20of%20a%20geodesic%20ball%20on%20the%20symmetry-corrected%20quotient%0Amanifold.%20In%20practice%2C%20we%20need%20to%20resort%20to%20approximating%20the%20geodesics.%20Doing%0Aso%20up%20to%20first%20order%20yields%20existing%20adaptive%20sharpness%20measures%2C%20and%20we%0Ademonstrate%20that%20including%20higher-order%20terms%20is%20crucial%20to%20recover%20correlation%0Awith%20generalization.%20We%20present%20results%20on%20diagonal%20networks%20with%20synthetic%0Adata%2C%20and%20show%20that%20our%20geodesic%20sharpness%20reveals%20strong%20correlation%20for%0Areal-world%20transformers%20on%20both%20text%20and%20image%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05409v1&entry.124074799=Read"},
{"title": "Taming OOD Actions for Offline Reinforcement Learning: An\n  Advantage-Based Approach", "author": "Xuyang Chen and Keyu Yan and Lin Zhao", "abstract": "  Offline reinforcement learning (RL) aims to learn decision-making policies\nfrom fixed datasets without online interactions, providing a practical solution\nwhere online data collection is expensive or risky. However, offline RL often\nsuffers from distribution shift, resulting in inaccurate evaluation and\nsubstantial overestimation on out-of-distribution (OOD) actions. To address\nthis, existing approaches incorporate conservatism by indiscriminately\ndiscouraging all OOD actions, thereby hindering the agent's ability to\ngeneralize and exploit beneficial ones. In this paper, we propose\nAdvantage-based Diffusion Actor-Critic (ADAC), a novel method that\nsystematically evaluates OOD actions using the batch-optimal value function.\nBased on this evaluation, ADAC defines an advantage function to modulate the\nQ-function update, enabling more precise assessment of OOD action quality. We\ndesign a custom PointMaze environment and collect datasets to visually reveal\nthat advantage modulation can effectively identify and select superior OOD\nactions. Extensive experiments show that ADAC achieves state-of-the-art\nperformance on almost all tasks in the D4RL benchmark, with particularly clear\nmargins on the more challenging tasks.\n", "link": "http://arxiv.org/abs/2505.05126v1", "date": "2025-05-08", "relevancy": 1.9623, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5348}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4833}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20OOD%20Actions%20for%20Offline%20Reinforcement%20Learning%3A%20An%0A%20%20Advantage-Based%20Approach&body=Title%3A%20Taming%20OOD%20Actions%20for%20Offline%20Reinforcement%20Learning%3A%20An%0A%20%20Advantage-Based%20Approach%0AAuthor%3A%20Xuyang%20Chen%20and%20Keyu%20Yan%20and%20Lin%20Zhao%0AAbstract%3A%20%20%20Offline%20reinforcement%20learning%20%28RL%29%20aims%20to%20learn%20decision-making%20policies%0Afrom%20fixed%20datasets%20without%20online%20interactions%2C%20providing%20a%20practical%20solution%0Awhere%20online%20data%20collection%20is%20expensive%20or%20risky.%20However%2C%20offline%20RL%20often%0Asuffers%20from%20distribution%20shift%2C%20resulting%20in%20inaccurate%20evaluation%20and%0Asubstantial%20overestimation%20on%20out-of-distribution%20%28OOD%29%20actions.%20To%20address%0Athis%2C%20existing%20approaches%20incorporate%20conservatism%20by%20indiscriminately%0Adiscouraging%20all%20OOD%20actions%2C%20thereby%20hindering%20the%20agent%27s%20ability%20to%0Ageneralize%20and%20exploit%20beneficial%20ones.%20In%20this%20paper%2C%20we%20propose%0AAdvantage-based%20Diffusion%20Actor-Critic%20%28ADAC%29%2C%20a%20novel%20method%20that%0Asystematically%20evaluates%20OOD%20actions%20using%20the%20batch-optimal%20value%20function.%0ABased%20on%20this%20evaluation%2C%20ADAC%20defines%20an%20advantage%20function%20to%20modulate%20the%0AQ-function%20update%2C%20enabling%20more%20precise%20assessment%20of%20OOD%20action%20quality.%20We%0Adesign%20a%20custom%20PointMaze%20environment%20and%20collect%20datasets%20to%20visually%20reveal%0Athat%20advantage%20modulation%20can%20effectively%20identify%20and%20select%20superior%20OOD%0Aactions.%20Extensive%20experiments%20show%20that%20ADAC%20achieves%20state-of-the-art%0Aperformance%20on%20almost%20all%20tasks%20in%20the%20D4RL%20benchmark%2C%20with%20particularly%20clear%0Amargins%20on%20the%20more%20challenging%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520OOD%2520Actions%2520for%2520Offline%2520Reinforcement%2520Learning%253A%2520An%250A%2520%2520Advantage-Based%2520Approach%26entry.906535625%3DXuyang%2520Chen%2520and%2520Keyu%2520Yan%2520and%2520Lin%2520Zhao%26entry.1292438233%3D%2520%2520Offline%2520reinforcement%2520learning%2520%2528RL%2529%2520aims%2520to%2520learn%2520decision-making%2520policies%250Afrom%2520fixed%2520datasets%2520without%2520online%2520interactions%252C%2520providing%2520a%2520practical%2520solution%250Awhere%2520online%2520data%2520collection%2520is%2520expensive%2520or%2520risky.%2520However%252C%2520offline%2520RL%2520often%250Asuffers%2520from%2520distribution%2520shift%252C%2520resulting%2520in%2520inaccurate%2520evaluation%2520and%250Asubstantial%2520overestimation%2520on%2520out-of-distribution%2520%2528OOD%2529%2520actions.%2520To%2520address%250Athis%252C%2520existing%2520approaches%2520incorporate%2520conservatism%2520by%2520indiscriminately%250Adiscouraging%2520all%2520OOD%2520actions%252C%2520thereby%2520hindering%2520the%2520agent%2527s%2520ability%2520to%250Ageneralize%2520and%2520exploit%2520beneficial%2520ones.%2520In%2520this%2520paper%252C%2520we%2520propose%250AAdvantage-based%2520Diffusion%2520Actor-Critic%2520%2528ADAC%2529%252C%2520a%2520novel%2520method%2520that%250Asystematically%2520evaluates%2520OOD%2520actions%2520using%2520the%2520batch-optimal%2520value%2520function.%250ABased%2520on%2520this%2520evaluation%252C%2520ADAC%2520defines%2520an%2520advantage%2520function%2520to%2520modulate%2520the%250AQ-function%2520update%252C%2520enabling%2520more%2520precise%2520assessment%2520of%2520OOD%2520action%2520quality.%2520We%250Adesign%2520a%2520custom%2520PointMaze%2520environment%2520and%2520collect%2520datasets%2520to%2520visually%2520reveal%250Athat%2520advantage%2520modulation%2520can%2520effectively%2520identify%2520and%2520select%2520superior%2520OOD%250Aactions.%2520Extensive%2520experiments%2520show%2520that%2520ADAC%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520almost%2520all%2520tasks%2520in%2520the%2520D4RL%2520benchmark%252C%2520with%2520particularly%2520clear%250Amargins%2520on%2520the%2520more%2520challenging%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20OOD%20Actions%20for%20Offline%20Reinforcement%20Learning%3A%20An%0A%20%20Advantage-Based%20Approach&entry.906535625=Xuyang%20Chen%20and%20Keyu%20Yan%20and%20Lin%20Zhao&entry.1292438233=%20%20Offline%20reinforcement%20learning%20%28RL%29%20aims%20to%20learn%20decision-making%20policies%0Afrom%20fixed%20datasets%20without%20online%20interactions%2C%20providing%20a%20practical%20solution%0Awhere%20online%20data%20collection%20is%20expensive%20or%20risky.%20However%2C%20offline%20RL%20often%0Asuffers%20from%20distribution%20shift%2C%20resulting%20in%20inaccurate%20evaluation%20and%0Asubstantial%20overestimation%20on%20out-of-distribution%20%28OOD%29%20actions.%20To%20address%0Athis%2C%20existing%20approaches%20incorporate%20conservatism%20by%20indiscriminately%0Adiscouraging%20all%20OOD%20actions%2C%20thereby%20hindering%20the%20agent%27s%20ability%20to%0Ageneralize%20and%20exploit%20beneficial%20ones.%20In%20this%20paper%2C%20we%20propose%0AAdvantage-based%20Diffusion%20Actor-Critic%20%28ADAC%29%2C%20a%20novel%20method%20that%0Asystematically%20evaluates%20OOD%20actions%20using%20the%20batch-optimal%20value%20function.%0ABased%20on%20this%20evaluation%2C%20ADAC%20defines%20an%20advantage%20function%20to%20modulate%20the%0AQ-function%20update%2C%20enabling%20more%20precise%20assessment%20of%20OOD%20action%20quality.%20We%0Adesign%20a%20custom%20PointMaze%20environment%20and%20collect%20datasets%20to%20visually%20reveal%0Athat%20advantage%20modulation%20can%20effectively%20identify%20and%20select%20superior%20OOD%0Aactions.%20Extensive%20experiments%20show%20that%20ADAC%20achieves%20state-of-the-art%0Aperformance%20on%20almost%20all%20tasks%20in%20the%20D4RL%20benchmark%2C%20with%20particularly%20clear%0Amargins%20on%20the%20more%20challenging%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05126v1&entry.124074799=Read"},
{"title": "Stochastic Variational Propagation: Local, Scalable and Efficient\n  Alternative to Backpropagation", "author": "Bojian Yin and Federico Corradi", "abstract": "  Backpropagation (BP) is the cornerstone of deep learning, but its reliance on\nglobal gradient synchronization limits scalability and imposes significant\nmemory overhead. We propose Stochastic Variational Propagation (SVP), a\nscalable alternative that reframes training as hierarchical variational\ninference. SVP treats layer activations as latent variables and optimizes local\nEvidence Lower Bounds (ELBOs), enabling independent, local updates while\npreserving global coherence. However, directly applying KL divergence in\nlayer-wise ELBOs risks inter-layer's representation collapse due to excessive\ncompression. To prevent this, SVP projects activations into low-dimensional\nspaces via fixed random matrices, ensuring information preservation and\nrepresentational diversity. Combined with a feature alignment loss for\ninter-layer consistency, SVP achieves competitive accuracy with BP across\ndiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to\nImageNet), reduces memory usage by up to 4x, and significantly improves\nscalability. More broadly, SVP introduces a probabilistic perspective to deep\nrepresentation learning, opening pathways toward more modular and interpretable\nneural network design.\n", "link": "http://arxiv.org/abs/2505.05181v1", "date": "2025-05-08", "relevancy": 1.9622, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5189}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4857}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Variational%20Propagation%3A%20Local%2C%20Scalable%20and%20Efficient%0A%20%20Alternative%20to%20Backpropagation&body=Title%3A%20Stochastic%20Variational%20Propagation%3A%20Local%2C%20Scalable%20and%20Efficient%0A%20%20Alternative%20to%20Backpropagation%0AAuthor%3A%20Bojian%20Yin%20and%20Federico%20Corradi%0AAbstract%3A%20%20%20Backpropagation%20%28BP%29%20is%20the%20cornerstone%20of%20deep%20learning%2C%20but%20its%20reliance%20on%0Aglobal%20gradient%20synchronization%20limits%20scalability%20and%20imposes%20significant%0Amemory%20overhead.%20We%20propose%20Stochastic%20Variational%20Propagation%20%28SVP%29%2C%20a%0Ascalable%20alternative%20that%20reframes%20training%20as%20hierarchical%20variational%0Ainference.%20SVP%20treats%20layer%20activations%20as%20latent%20variables%20and%20optimizes%20local%0AEvidence%20Lower%20Bounds%20%28ELBOs%29%2C%20enabling%20independent%2C%20local%20updates%20while%0Apreserving%20global%20coherence.%20However%2C%20directly%20applying%20KL%20divergence%20in%0Alayer-wise%20ELBOs%20risks%20inter-layer%27s%20representation%20collapse%20due%20to%20excessive%0Acompression.%20To%20prevent%20this%2C%20SVP%20projects%20activations%20into%20low-dimensional%0Aspaces%20via%20fixed%20random%20matrices%2C%20ensuring%20information%20preservation%20and%0Arepresentational%20diversity.%20Combined%20with%20a%20feature%20alignment%20loss%20for%0Ainter-layer%20consistency%2C%20SVP%20achieves%20competitive%20accuracy%20with%20BP%20across%0Adiverse%20architectures%20%28MLPs%2C%20CNNs%2C%20Transformers%29%20and%20datasets%20%28MNIST%20to%0AImageNet%29%2C%20reduces%20memory%20usage%20by%20up%20to%204x%2C%20and%20significantly%20improves%0Ascalability.%20More%20broadly%2C%20SVP%20introduces%20a%20probabilistic%20perspective%20to%20deep%0Arepresentation%20learning%2C%20opening%20pathways%20toward%20more%20modular%20and%20interpretable%0Aneural%20network%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Variational%2520Propagation%253A%2520Local%252C%2520Scalable%2520and%2520Efficient%250A%2520%2520Alternative%2520to%2520Backpropagation%26entry.906535625%3DBojian%2520Yin%2520and%2520Federico%2520Corradi%26entry.1292438233%3D%2520%2520Backpropagation%2520%2528BP%2529%2520is%2520the%2520cornerstone%2520of%2520deep%2520learning%252C%2520but%2520its%2520reliance%2520on%250Aglobal%2520gradient%2520synchronization%2520limits%2520scalability%2520and%2520imposes%2520significant%250Amemory%2520overhead.%2520We%2520propose%2520Stochastic%2520Variational%2520Propagation%2520%2528SVP%2529%252C%2520a%250Ascalable%2520alternative%2520that%2520reframes%2520training%2520as%2520hierarchical%2520variational%250Ainference.%2520SVP%2520treats%2520layer%2520activations%2520as%2520latent%2520variables%2520and%2520optimizes%2520local%250AEvidence%2520Lower%2520Bounds%2520%2528ELBOs%2529%252C%2520enabling%2520independent%252C%2520local%2520updates%2520while%250Apreserving%2520global%2520coherence.%2520However%252C%2520directly%2520applying%2520KL%2520divergence%2520in%250Alayer-wise%2520ELBOs%2520risks%2520inter-layer%2527s%2520representation%2520collapse%2520due%2520to%2520excessive%250Acompression.%2520To%2520prevent%2520this%252C%2520SVP%2520projects%2520activations%2520into%2520low-dimensional%250Aspaces%2520via%2520fixed%2520random%2520matrices%252C%2520ensuring%2520information%2520preservation%2520and%250Arepresentational%2520diversity.%2520Combined%2520with%2520a%2520feature%2520alignment%2520loss%2520for%250Ainter-layer%2520consistency%252C%2520SVP%2520achieves%2520competitive%2520accuracy%2520with%2520BP%2520across%250Adiverse%2520architectures%2520%2528MLPs%252C%2520CNNs%252C%2520Transformers%2529%2520and%2520datasets%2520%2528MNIST%2520to%250AImageNet%2529%252C%2520reduces%2520memory%2520usage%2520by%2520up%2520to%25204x%252C%2520and%2520significantly%2520improves%250Ascalability.%2520More%2520broadly%252C%2520SVP%2520introduces%2520a%2520probabilistic%2520perspective%2520to%2520deep%250Arepresentation%2520learning%252C%2520opening%2520pathways%2520toward%2520more%2520modular%2520and%2520interpretable%250Aneural%2520network%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Variational%20Propagation%3A%20Local%2C%20Scalable%20and%20Efficient%0A%20%20Alternative%20to%20Backpropagation&entry.906535625=Bojian%20Yin%20and%20Federico%20Corradi&entry.1292438233=%20%20Backpropagation%20%28BP%29%20is%20the%20cornerstone%20of%20deep%20learning%2C%20but%20its%20reliance%20on%0Aglobal%20gradient%20synchronization%20limits%20scalability%20and%20imposes%20significant%0Amemory%20overhead.%20We%20propose%20Stochastic%20Variational%20Propagation%20%28SVP%29%2C%20a%0Ascalable%20alternative%20that%20reframes%20training%20as%20hierarchical%20variational%0Ainference.%20SVP%20treats%20layer%20activations%20as%20latent%20variables%20and%20optimizes%20local%0AEvidence%20Lower%20Bounds%20%28ELBOs%29%2C%20enabling%20independent%2C%20local%20updates%20while%0Apreserving%20global%20coherence.%20However%2C%20directly%20applying%20KL%20divergence%20in%0Alayer-wise%20ELBOs%20risks%20inter-layer%27s%20representation%20collapse%20due%20to%20excessive%0Acompression.%20To%20prevent%20this%2C%20SVP%20projects%20activations%20into%20low-dimensional%0Aspaces%20via%20fixed%20random%20matrices%2C%20ensuring%20information%20preservation%20and%0Arepresentational%20diversity.%20Combined%20with%20a%20feature%20alignment%20loss%20for%0Ainter-layer%20consistency%2C%20SVP%20achieves%20competitive%20accuracy%20with%20BP%20across%0Adiverse%20architectures%20%28MLPs%2C%20CNNs%2C%20Transformers%29%20and%20datasets%20%28MNIST%20to%0AImageNet%29%2C%20reduces%20memory%20usage%20by%20up%20to%204x%2C%20and%20significantly%20improves%0Ascalability.%20More%20broadly%2C%20SVP%20introduces%20a%20probabilistic%20perspective%20to%20deep%0Arepresentation%20learning%2C%20opening%20pathways%20toward%20more%20modular%20and%20interpretable%0Aneural%20network%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05181v1&entry.124074799=Read"},
{"title": "Benchmarking Open-Source Large Language Models on Healthcare Text\n  Classification Tasks", "author": "Yuting Guo and Abeed Sarker", "abstract": "  The application of large language models (LLMs) to healthcare information\nextraction has emerged as a promising approach. This study evaluates the\nclassification performance of five open-source LLMs: GEMMA-3-27B-IT,\nLLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and\nDEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks\ninvolving both social media data (breast cancer, changes in medication regimen,\nadverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma\nlabeling, medication change discussion). We report precision, recall, and F1\nscores with 95% confidence intervals for all model-task combinations. Our\nfindings reveal significant performance variability between LLMs, with\nDeepSeekV3 emerging as the strongest overall performer, achieving the highest\nF1 scores in four tasks. Notably, models generally performed better on social\nmedia tasks compared to clinical data tasks, suggesting potential\ndomain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high\nrecall despite its smaller parameter count, while LLAMA4-109B showed\nsurprisingly underwhelming performance compared to its predecessor LLAMA3-70B,\nindicating that larger parameter counts do not guarantee improved\nclassification results. We observed distinct precision-recall trade-offs across\nmodels, with some favoring sensitivity over specificity and vice versa. These\nfindings highlight the importance of task-specific model selection for\nhealthcare applications, considering the particular data domain and\nprecision-recall requirements rather than model size alone. As healthcare\nincreasingly integrates AI-driven text classification tools, this comprehensive\nbenchmarking provides valuable guidance for model selection and implementation\nwhile underscoring the need for continued evaluation and domain adaptation of\nLLMs in healthcare contexts.\n", "link": "http://arxiv.org/abs/2503.15169v2", "date": "2025-05-08", "relevancy": 1.9606, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4883}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Open-Source%20Large%20Language%20Models%20on%20Healthcare%20Text%0A%20%20Classification%20Tasks&body=Title%3A%20Benchmarking%20Open-Source%20Large%20Language%20Models%20on%20Healthcare%20Text%0A%20%20Classification%20Tasks%0AAuthor%3A%20Yuting%20Guo%20and%20Abeed%20Sarker%0AAbstract%3A%20%20%20The%20application%20of%20large%20language%20models%20%28LLMs%29%20to%20healthcare%20information%0Aextraction%20has%20emerged%20as%20a%20promising%20approach.%20This%20study%20evaluates%20the%0Aclassification%20performance%20of%20five%20open-source%20LLMs%3A%20GEMMA-3-27B-IT%2C%0ALLAMA3-70B%2C%20LLAMA4-109B%2C%20DEEPSEEK-R1-DISTILL-LLAMA-70B%2C%20and%0ADEEPSEEK-V3-0324-UD-Q2_K_XL%2C%20across%20six%20healthcare-related%20classification%20tasks%0Ainvolving%20both%20social%20media%20data%20%28breast%20cancer%2C%20changes%20in%20medication%20regimen%2C%0Aadverse%20pregnancy%20outcomes%2C%20potential%20COVID-19%20cases%29%20and%20clinical%20data%20%28stigma%0Alabeling%2C%20medication%20change%20discussion%29.%20We%20report%20precision%2C%20recall%2C%20and%20F1%0Ascores%20with%2095%25%20confidence%20intervals%20for%20all%20model-task%20combinations.%20Our%0Afindings%20reveal%20significant%20performance%20variability%20between%20LLMs%2C%20with%0ADeepSeekV3%20emerging%20as%20the%20strongest%20overall%20performer%2C%20achieving%20the%20highest%0AF1%20scores%20in%20four%20tasks.%20Notably%2C%20models%20generally%20performed%20better%20on%20social%0Amedia%20tasks%20compared%20to%20clinical%20data%20tasks%2C%20suggesting%20potential%0Adomain-specific%20challenges.%20GEMMA-3-27B-IT%20demonstrated%20exceptionally%20high%0Arecall%20despite%20its%20smaller%20parameter%20count%2C%20while%20LLAMA4-109B%20showed%0Asurprisingly%20underwhelming%20performance%20compared%20to%20its%20predecessor%20LLAMA3-70B%2C%0Aindicating%20that%20larger%20parameter%20counts%20do%20not%20guarantee%20improved%0Aclassification%20results.%20We%20observed%20distinct%20precision-recall%20trade-offs%20across%0Amodels%2C%20with%20some%20favoring%20sensitivity%20over%20specificity%20and%20vice%20versa.%20These%0Afindings%20highlight%20the%20importance%20of%20task-specific%20model%20selection%20for%0Ahealthcare%20applications%2C%20considering%20the%20particular%20data%20domain%20and%0Aprecision-recall%20requirements%20rather%20than%20model%20size%20alone.%20As%20healthcare%0Aincreasingly%20integrates%20AI-driven%20text%20classification%20tools%2C%20this%20comprehensive%0Abenchmarking%20provides%20valuable%20guidance%20for%20model%20selection%20and%20implementation%0Awhile%20underscoring%20the%20need%20for%20continued%20evaluation%20and%20domain%20adaptation%20of%0ALLMs%20in%20healthcare%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.15169v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Open-Source%2520Large%2520Language%2520Models%2520on%2520Healthcare%2520Text%250A%2520%2520Classification%2520Tasks%26entry.906535625%3DYuting%2520Guo%2520and%2520Abeed%2520Sarker%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520healthcare%2520information%250Aextraction%2520has%2520emerged%2520as%2520a%2520promising%2520approach.%2520This%2520study%2520evaluates%2520the%250Aclassification%2520performance%2520of%2520five%2520open-source%2520LLMs%253A%2520GEMMA-3-27B-IT%252C%250ALLAMA3-70B%252C%2520LLAMA4-109B%252C%2520DEEPSEEK-R1-DISTILL-LLAMA-70B%252C%2520and%250ADEEPSEEK-V3-0324-UD-Q2_K_XL%252C%2520across%2520six%2520healthcare-related%2520classification%2520tasks%250Ainvolving%2520both%2520social%2520media%2520data%2520%2528breast%2520cancer%252C%2520changes%2520in%2520medication%2520regimen%252C%250Aadverse%2520pregnancy%2520outcomes%252C%2520potential%2520COVID-19%2520cases%2529%2520and%2520clinical%2520data%2520%2528stigma%250Alabeling%252C%2520medication%2520change%2520discussion%2529.%2520We%2520report%2520precision%252C%2520recall%252C%2520and%2520F1%250Ascores%2520with%252095%2525%2520confidence%2520intervals%2520for%2520all%2520model-task%2520combinations.%2520Our%250Afindings%2520reveal%2520significant%2520performance%2520variability%2520between%2520LLMs%252C%2520with%250ADeepSeekV3%2520emerging%2520as%2520the%2520strongest%2520overall%2520performer%252C%2520achieving%2520the%2520highest%250AF1%2520scores%2520in%2520four%2520tasks.%2520Notably%252C%2520models%2520generally%2520performed%2520better%2520on%2520social%250Amedia%2520tasks%2520compared%2520to%2520clinical%2520data%2520tasks%252C%2520suggesting%2520potential%250Adomain-specific%2520challenges.%2520GEMMA-3-27B-IT%2520demonstrated%2520exceptionally%2520high%250Arecall%2520despite%2520its%2520smaller%2520parameter%2520count%252C%2520while%2520LLAMA4-109B%2520showed%250Asurprisingly%2520underwhelming%2520performance%2520compared%2520to%2520its%2520predecessor%2520LLAMA3-70B%252C%250Aindicating%2520that%2520larger%2520parameter%2520counts%2520do%2520not%2520guarantee%2520improved%250Aclassification%2520results.%2520We%2520observed%2520distinct%2520precision-recall%2520trade-offs%2520across%250Amodels%252C%2520with%2520some%2520favoring%2520sensitivity%2520over%2520specificity%2520and%2520vice%2520versa.%2520These%250Afindings%2520highlight%2520the%2520importance%2520of%2520task-specific%2520model%2520selection%2520for%250Ahealthcare%2520applications%252C%2520considering%2520the%2520particular%2520data%2520domain%2520and%250Aprecision-recall%2520requirements%2520rather%2520than%2520model%2520size%2520alone.%2520As%2520healthcare%250Aincreasingly%2520integrates%2520AI-driven%2520text%2520classification%2520tools%252C%2520this%2520comprehensive%250Abenchmarking%2520provides%2520valuable%2520guidance%2520for%2520model%2520selection%2520and%2520implementation%250Awhile%2520underscoring%2520the%2520need%2520for%2520continued%2520evaluation%2520and%2520domain%2520adaptation%2520of%250ALLMs%2520in%2520healthcare%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15169v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Open-Source%20Large%20Language%20Models%20on%20Healthcare%20Text%0A%20%20Classification%20Tasks&entry.906535625=Yuting%20Guo%20and%20Abeed%20Sarker&entry.1292438233=%20%20The%20application%20of%20large%20language%20models%20%28LLMs%29%20to%20healthcare%20information%0Aextraction%20has%20emerged%20as%20a%20promising%20approach.%20This%20study%20evaluates%20the%0Aclassification%20performance%20of%20five%20open-source%20LLMs%3A%20GEMMA-3-27B-IT%2C%0ALLAMA3-70B%2C%20LLAMA4-109B%2C%20DEEPSEEK-R1-DISTILL-LLAMA-70B%2C%20and%0ADEEPSEEK-V3-0324-UD-Q2_K_XL%2C%20across%20six%20healthcare-related%20classification%20tasks%0Ainvolving%20both%20social%20media%20data%20%28breast%20cancer%2C%20changes%20in%20medication%20regimen%2C%0Aadverse%20pregnancy%20outcomes%2C%20potential%20COVID-19%20cases%29%20and%20clinical%20data%20%28stigma%0Alabeling%2C%20medication%20change%20discussion%29.%20We%20report%20precision%2C%20recall%2C%20and%20F1%0Ascores%20with%2095%25%20confidence%20intervals%20for%20all%20model-task%20combinations.%20Our%0Afindings%20reveal%20significant%20performance%20variability%20between%20LLMs%2C%20with%0ADeepSeekV3%20emerging%20as%20the%20strongest%20overall%20performer%2C%20achieving%20the%20highest%0AF1%20scores%20in%20four%20tasks.%20Notably%2C%20models%20generally%20performed%20better%20on%20social%0Amedia%20tasks%20compared%20to%20clinical%20data%20tasks%2C%20suggesting%20potential%0Adomain-specific%20challenges.%20GEMMA-3-27B-IT%20demonstrated%20exceptionally%20high%0Arecall%20despite%20its%20smaller%20parameter%20count%2C%20while%20LLAMA4-109B%20showed%0Asurprisingly%20underwhelming%20performance%20compared%20to%20its%20predecessor%20LLAMA3-70B%2C%0Aindicating%20that%20larger%20parameter%20counts%20do%20not%20guarantee%20improved%0Aclassification%20results.%20We%20observed%20distinct%20precision-recall%20trade-offs%20across%0Amodels%2C%20with%20some%20favoring%20sensitivity%20over%20specificity%20and%20vice%20versa.%20These%0Afindings%20highlight%20the%20importance%20of%20task-specific%20model%20selection%20for%0Ahealthcare%20applications%2C%20considering%20the%20particular%20data%20domain%20and%0Aprecision-recall%20requirements%20rather%20than%20model%20size%20alone.%20As%20healthcare%0Aincreasingly%20integrates%20AI-driven%20text%20classification%20tools%2C%20this%20comprehensive%0Abenchmarking%20provides%20valuable%20guidance%20for%20model%20selection%20and%20implementation%0Awhile%20underscoring%20the%20need%20for%20continued%20evaluation%20and%20domain%20adaptation%20of%0ALLMs%20in%20healthcare%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.15169v2&entry.124074799=Read"},
{"title": "Concept-Based Unsupervised Domain Adaptation", "author": "Xinyue Xu and Yueying Hu and Hui Tang and Yi Qin and Lu Mi and Hao Wang and Xiaomeng Li", "abstract": "  Concept Bottleneck Models (CBMs) enhance interpretability by explaining\npredictions through human-understandable concepts but typically assume that\ntraining and test data share the same distribution. This assumption often fails\nunder domain shifts, leading to degraded performance and poor generalization.\nTo address these limitations and improve the robustness of CBMs, we propose the\nConcept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed\nto: (1) align concept representations across domains using adversarial\ntraining, (2) introduce a relaxation threshold to allow minor domain-specific\ndifferences in concept distributions, thereby preventing performance drop due\nto over-constraints of these distributions, (3) infer concepts directly in the\ntarget domain without requiring labeled concept data, enabling CBMs to adapt to\ndiverse domains, and (4) integrate concept learning into conventional domain\nadaptation (DA) with theoretical guarantees, improving interpretability and\nestablishing new benchmarks for DA. Experiments demonstrate that our approach\nsignificantly outperforms the state-of-the-art CBM and DA methods on real-world\ndatasets.\n", "link": "http://arxiv.org/abs/2505.05195v1", "date": "2025-05-08", "relevancy": 1.959, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4968}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept-Based%20Unsupervised%20Domain%20Adaptation&body=Title%3A%20Concept-Based%20Unsupervised%20Domain%20Adaptation%0AAuthor%3A%20Xinyue%20Xu%20and%20Yueying%20Hu%20and%20Hui%20Tang%20and%20Yi%20Qin%20and%20Lu%20Mi%20and%20Hao%20Wang%20and%20Xiaomeng%20Li%0AAbstract%3A%20%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20enhance%20interpretability%20by%20explaining%0Apredictions%20through%20human-understandable%20concepts%20but%20typically%20assume%20that%0Atraining%20and%20test%20data%20share%20the%20same%20distribution.%20This%20assumption%20often%20fails%0Aunder%20domain%20shifts%2C%20leading%20to%20degraded%20performance%20and%20poor%20generalization.%0ATo%20address%20these%20limitations%20and%20improve%20the%20robustness%20of%20CBMs%2C%20we%20propose%20the%0AConcept-based%20Unsupervised%20Domain%20Adaptation%20%28CUDA%29%20framework.%20CUDA%20is%20designed%0Ato%3A%20%281%29%20align%20concept%20representations%20across%20domains%20using%20adversarial%0Atraining%2C%20%282%29%20introduce%20a%20relaxation%20threshold%20to%20allow%20minor%20domain-specific%0Adifferences%20in%20concept%20distributions%2C%20thereby%20preventing%20performance%20drop%20due%0Ato%20over-constraints%20of%20these%20distributions%2C%20%283%29%20infer%20concepts%20directly%20in%20the%0Atarget%20domain%20without%20requiring%20labeled%20concept%20data%2C%20enabling%20CBMs%20to%20adapt%20to%0Adiverse%20domains%2C%20and%20%284%29%20integrate%20concept%20learning%20into%20conventional%20domain%0Aadaptation%20%28DA%29%20with%20theoretical%20guarantees%2C%20improving%20interpretability%20and%0Aestablishing%20new%20benchmarks%20for%20DA.%20Experiments%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20the%20state-of-the-art%20CBM%20and%20DA%20methods%20on%20real-world%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept-Based%2520Unsupervised%2520Domain%2520Adaptation%26entry.906535625%3DXinyue%2520Xu%2520and%2520Yueying%2520Hu%2520and%2520Hui%2520Tang%2520and%2520Yi%2520Qin%2520and%2520Lu%2520Mi%2520and%2520Hao%2520Wang%2520and%2520Xiaomeng%2520Li%26entry.1292438233%3D%2520%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520enhance%2520interpretability%2520by%2520explaining%250Apredictions%2520through%2520human-understandable%2520concepts%2520but%2520typically%2520assume%2520that%250Atraining%2520and%2520test%2520data%2520share%2520the%2520same%2520distribution.%2520This%2520assumption%2520often%2520fails%250Aunder%2520domain%2520shifts%252C%2520leading%2520to%2520degraded%2520performance%2520and%2520poor%2520generalization.%250ATo%2520address%2520these%2520limitations%2520and%2520improve%2520the%2520robustness%2520of%2520CBMs%252C%2520we%2520propose%2520the%250AConcept-based%2520Unsupervised%2520Domain%2520Adaptation%2520%2528CUDA%2529%2520framework.%2520CUDA%2520is%2520designed%250Ato%253A%2520%25281%2529%2520align%2520concept%2520representations%2520across%2520domains%2520using%2520adversarial%250Atraining%252C%2520%25282%2529%2520introduce%2520a%2520relaxation%2520threshold%2520to%2520allow%2520minor%2520domain-specific%250Adifferences%2520in%2520concept%2520distributions%252C%2520thereby%2520preventing%2520performance%2520drop%2520due%250Ato%2520over-constraints%2520of%2520these%2520distributions%252C%2520%25283%2529%2520infer%2520concepts%2520directly%2520in%2520the%250Atarget%2520domain%2520without%2520requiring%2520labeled%2520concept%2520data%252C%2520enabling%2520CBMs%2520to%2520adapt%2520to%250Adiverse%2520domains%252C%2520and%2520%25284%2529%2520integrate%2520concept%2520learning%2520into%2520conventional%2520domain%250Aadaptation%2520%2528DA%2529%2520with%2520theoretical%2520guarantees%252C%2520improving%2520interpretability%2520and%250Aestablishing%2520new%2520benchmarks%2520for%2520DA.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520outperforms%2520the%2520state-of-the-art%2520CBM%2520and%2520DA%2520methods%2520on%2520real-world%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept-Based%20Unsupervised%20Domain%20Adaptation&entry.906535625=Xinyue%20Xu%20and%20Yueying%20Hu%20and%20Hui%20Tang%20and%20Yi%20Qin%20and%20Lu%20Mi%20and%20Hao%20Wang%20and%20Xiaomeng%20Li&entry.1292438233=%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20enhance%20interpretability%20by%20explaining%0Apredictions%20through%20human-understandable%20concepts%20but%20typically%20assume%20that%0Atraining%20and%20test%20data%20share%20the%20same%20distribution.%20This%20assumption%20often%20fails%0Aunder%20domain%20shifts%2C%20leading%20to%20degraded%20performance%20and%20poor%20generalization.%0ATo%20address%20these%20limitations%20and%20improve%20the%20robustness%20of%20CBMs%2C%20we%20propose%20the%0AConcept-based%20Unsupervised%20Domain%20Adaptation%20%28CUDA%29%20framework.%20CUDA%20is%20designed%0Ato%3A%20%281%29%20align%20concept%20representations%20across%20domains%20using%20adversarial%0Atraining%2C%20%282%29%20introduce%20a%20relaxation%20threshold%20to%20allow%20minor%20domain-specific%0Adifferences%20in%20concept%20distributions%2C%20thereby%20preventing%20performance%20drop%20due%0Ato%20over-constraints%20of%20these%20distributions%2C%20%283%29%20infer%20concepts%20directly%20in%20the%0Atarget%20domain%20without%20requiring%20labeled%20concept%20data%2C%20enabling%20CBMs%20to%20adapt%20to%0Adiverse%20domains%2C%20and%20%284%29%20integrate%20concept%20learning%20into%20conventional%20domain%0Aadaptation%20%28DA%29%20with%20theoretical%20guarantees%2C%20improving%20interpretability%20and%0Aestablishing%20new%20benchmarks%20for%20DA.%20Experiments%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20the%20state-of-the-art%20CBM%20and%20DA%20methods%20on%20real-world%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05195v1&entry.124074799=Read"},
{"title": "PINN-MEP: Continuous Neural Representations for Minimum-Energy Path\n  Discovery in Molecular Systems", "author": "Magnus Petersen and Roberto Covino", "abstract": "  Characterizing conformational transitions in physical systems remains a\nfundamental challenge in the computational sciences. Traditional sampling\nmethods like molecular dynamics (MD) or MCMC often struggle with the\nhigh-dimensional nature of molecular systems and the high energy barriers of\ntransitions between stable states. While these transitions are rare events in\nsimulation timescales, they often represent the most biologically significant\nprocesses - for example, the conformational change of an ion channel protein\nfrom its closed to open state, which controls cellular ion flow and is crucial\nfor neural signaling. Such transitions in real systems may take milliseconds to\nseconds but could require months or years of continuous simulation to observe\neven once. We present a method that reformulates transition path generation as\na continuous optimization problem solved through physics-informed neural\nnetworks (PINNs) inspired by string methods for minimum-energy path (MEP)\ngeneration. By representing transition paths as implicit neural functions and\nleveraging automatic differentiation with differentiable molecular dynamics\nforce fields, our method enables the efficient discovery of physically\nrealistic transition pathways without requiring expensive path sampling. We\ndemonstrate our method's effectiveness on two proteins, including an explicitly\nhydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300\natoms.\n", "link": "http://arxiv.org/abs/2504.16381v4", "date": "2025-05-08", "relevancy": 1.9351, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5073}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4794}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PINN-MEP%3A%20Continuous%20Neural%20Representations%20for%20Minimum-Energy%20Path%0A%20%20Discovery%20in%20Molecular%20Systems&body=Title%3A%20PINN-MEP%3A%20Continuous%20Neural%20Representations%20for%20Minimum-Energy%20Path%0A%20%20Discovery%20in%20Molecular%20Systems%0AAuthor%3A%20Magnus%20Petersen%20and%20Roberto%20Covino%0AAbstract%3A%20%20%20Characterizing%20conformational%20transitions%20in%20physical%20systems%20remains%20a%0Afundamental%20challenge%20in%20the%20computational%20sciences.%20Traditional%20sampling%0Amethods%20like%20molecular%20dynamics%20%28MD%29%20or%20MCMC%20often%20struggle%20with%20the%0Ahigh-dimensional%20nature%20of%20molecular%20systems%20and%20the%20high%20energy%20barriers%20of%0Atransitions%20between%20stable%20states.%20While%20these%20transitions%20are%20rare%20events%20in%0Asimulation%20timescales%2C%20they%20often%20represent%20the%20most%20biologically%20significant%0Aprocesses%20-%20for%20example%2C%20the%20conformational%20change%20of%20an%20ion%20channel%20protein%0Afrom%20its%20closed%20to%20open%20state%2C%20which%20controls%20cellular%20ion%20flow%20and%20is%20crucial%0Afor%20neural%20signaling.%20Such%20transitions%20in%20real%20systems%20may%20take%20milliseconds%20to%0Aseconds%20but%20could%20require%20months%20or%20years%20of%20continuous%20simulation%20to%20observe%0Aeven%20once.%20We%20present%20a%20method%20that%20reformulates%20transition%20path%20generation%20as%0Aa%20continuous%20optimization%20problem%20solved%20through%20physics-informed%20neural%0Anetworks%20%28PINNs%29%20inspired%20by%20string%20methods%20for%20minimum-energy%20path%20%28MEP%29%0Ageneration.%20By%20representing%20transition%20paths%20as%20implicit%20neural%20functions%20and%0Aleveraging%20automatic%20differentiation%20with%20differentiable%20molecular%20dynamics%0Aforce%20fields%2C%20our%20method%20enables%20the%20efficient%20discovery%20of%20physically%0Arealistic%20transition%20pathways%20without%20requiring%20expensive%20path%20sampling.%20We%0Ademonstrate%20our%20method%27s%20effectiveness%20on%20two%20proteins%2C%20including%20an%20explicitly%0Ahydrated%20bovine%20pancreatic%20trypsin%20inhibitor%20%28BPTI%29%20system%20with%20over%208%2C300%0Aatoms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16381v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPINN-MEP%253A%2520Continuous%2520Neural%2520Representations%2520for%2520Minimum-Energy%2520Path%250A%2520%2520Discovery%2520in%2520Molecular%2520Systems%26entry.906535625%3DMagnus%2520Petersen%2520and%2520Roberto%2520Covino%26entry.1292438233%3D%2520%2520Characterizing%2520conformational%2520transitions%2520in%2520physical%2520systems%2520remains%2520a%250Afundamental%2520challenge%2520in%2520the%2520computational%2520sciences.%2520Traditional%2520sampling%250Amethods%2520like%2520molecular%2520dynamics%2520%2528MD%2529%2520or%2520MCMC%2520often%2520struggle%2520with%2520the%250Ahigh-dimensional%2520nature%2520of%2520molecular%2520systems%2520and%2520the%2520high%2520energy%2520barriers%2520of%250Atransitions%2520between%2520stable%2520states.%2520While%2520these%2520transitions%2520are%2520rare%2520events%2520in%250Asimulation%2520timescales%252C%2520they%2520often%2520represent%2520the%2520most%2520biologically%2520significant%250Aprocesses%2520-%2520for%2520example%252C%2520the%2520conformational%2520change%2520of%2520an%2520ion%2520channel%2520protein%250Afrom%2520its%2520closed%2520to%2520open%2520state%252C%2520which%2520controls%2520cellular%2520ion%2520flow%2520and%2520is%2520crucial%250Afor%2520neural%2520signaling.%2520Such%2520transitions%2520in%2520real%2520systems%2520may%2520take%2520milliseconds%2520to%250Aseconds%2520but%2520could%2520require%2520months%2520or%2520years%2520of%2520continuous%2520simulation%2520to%2520observe%250Aeven%2520once.%2520We%2520present%2520a%2520method%2520that%2520reformulates%2520transition%2520path%2520generation%2520as%250Aa%2520continuous%2520optimization%2520problem%2520solved%2520through%2520physics-informed%2520neural%250Anetworks%2520%2528PINNs%2529%2520inspired%2520by%2520string%2520methods%2520for%2520minimum-energy%2520path%2520%2528MEP%2529%250Ageneration.%2520By%2520representing%2520transition%2520paths%2520as%2520implicit%2520neural%2520functions%2520and%250Aleveraging%2520automatic%2520differentiation%2520with%2520differentiable%2520molecular%2520dynamics%250Aforce%2520fields%252C%2520our%2520method%2520enables%2520the%2520efficient%2520discovery%2520of%2520physically%250Arealistic%2520transition%2520pathways%2520without%2520requiring%2520expensive%2520path%2520sampling.%2520We%250Ademonstrate%2520our%2520method%2527s%2520effectiveness%2520on%2520two%2520proteins%252C%2520including%2520an%2520explicitly%250Ahydrated%2520bovine%2520pancreatic%2520trypsin%2520inhibitor%2520%2528BPTI%2529%2520system%2520with%2520over%25208%252C300%250Aatoms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16381v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PINN-MEP%3A%20Continuous%20Neural%20Representations%20for%20Minimum-Energy%20Path%0A%20%20Discovery%20in%20Molecular%20Systems&entry.906535625=Magnus%20Petersen%20and%20Roberto%20Covino&entry.1292438233=%20%20Characterizing%20conformational%20transitions%20in%20physical%20systems%20remains%20a%0Afundamental%20challenge%20in%20the%20computational%20sciences.%20Traditional%20sampling%0Amethods%20like%20molecular%20dynamics%20%28MD%29%20or%20MCMC%20often%20struggle%20with%20the%0Ahigh-dimensional%20nature%20of%20molecular%20systems%20and%20the%20high%20energy%20barriers%20of%0Atransitions%20between%20stable%20states.%20While%20these%20transitions%20are%20rare%20events%20in%0Asimulation%20timescales%2C%20they%20often%20represent%20the%20most%20biologically%20significant%0Aprocesses%20-%20for%20example%2C%20the%20conformational%20change%20of%20an%20ion%20channel%20protein%0Afrom%20its%20closed%20to%20open%20state%2C%20which%20controls%20cellular%20ion%20flow%20and%20is%20crucial%0Afor%20neural%20signaling.%20Such%20transitions%20in%20real%20systems%20may%20take%20milliseconds%20to%0Aseconds%20but%20could%20require%20months%20or%20years%20of%20continuous%20simulation%20to%20observe%0Aeven%20once.%20We%20present%20a%20method%20that%20reformulates%20transition%20path%20generation%20as%0Aa%20continuous%20optimization%20problem%20solved%20through%20physics-informed%20neural%0Anetworks%20%28PINNs%29%20inspired%20by%20string%20methods%20for%20minimum-energy%20path%20%28MEP%29%0Ageneration.%20By%20representing%20transition%20paths%20as%20implicit%20neural%20functions%20and%0Aleveraging%20automatic%20differentiation%20with%20differentiable%20molecular%20dynamics%0Aforce%20fields%2C%20our%20method%20enables%20the%20efficient%20discovery%20of%20physically%0Arealistic%20transition%20pathways%20without%20requiring%20expensive%20path%20sampling.%20We%0Ademonstrate%20our%20method%27s%20effectiveness%20on%20two%20proteins%2C%20including%20an%20explicitly%0Ahydrated%20bovine%20pancreatic%20trypsin%20inhibitor%20%28BPTI%29%20system%20with%20over%208%2C300%0Aatoms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16381v4&entry.124074799=Read"},
{"title": "LAPSO: A Unified Optimization View for Learning-Augmented Power System\n  Operations", "author": "Wangkun Xu and Zhongda Chu and Fei Teng", "abstract": "  With the high penetration of renewables, traditional model-based power system\noperation is challenged to deliver economic, stable, and robust decisions.\nMachine learning has emerged as a powerful modeling tool for capturing complex\ndynamics to address these challenges. However, its separate design often lacks\nsystematic integration with existing methods. To fill the gap, this paper\nproposes a holistic framework of Learning-Augmented Power System Operations\n(LAPSO, pronounced as Lap-So). Adopting a native optimization perspective,\nLAPSO is centered on the operation stage and aims to break the boundary between\ntemporally siloed power system tasks, such as forecast, operation and control,\nwhile unifying the objectives of machine learning and model-based optimizations\nat both training and inference stages. Systematic analysis and simulations\ndemonstrate the effectiveness of applying LAPSO in designing new integrated\nalgorithms, such as stability-constrained optimization (SCO) and\nobjective-based forecasting (OBF), while enabling end-to-end tracing of\ndifferent sources of uncertainties. In addition, a dedicated Python\npackage-lapso is introduced to automatically augment existing power system\noptimization models with learnable components. All code and data are available\nat https://github.com/xuwkk/lapso_exp.\n", "link": "http://arxiv.org/abs/2505.05203v1", "date": "2025-05-08", "relevancy": 1.934, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4903}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4888}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAPSO%3A%20A%20Unified%20Optimization%20View%20for%20Learning-Augmented%20Power%20System%0A%20%20Operations&body=Title%3A%20LAPSO%3A%20A%20Unified%20Optimization%20View%20for%20Learning-Augmented%20Power%20System%0A%20%20Operations%0AAuthor%3A%20Wangkun%20Xu%20and%20Zhongda%20Chu%20and%20Fei%20Teng%0AAbstract%3A%20%20%20With%20the%20high%20penetration%20of%20renewables%2C%20traditional%20model-based%20power%20system%0Aoperation%20is%20challenged%20to%20deliver%20economic%2C%20stable%2C%20and%20robust%20decisions.%0AMachine%20learning%20has%20emerged%20as%20a%20powerful%20modeling%20tool%20for%20capturing%20complex%0Adynamics%20to%20address%20these%20challenges.%20However%2C%20its%20separate%20design%20often%20lacks%0Asystematic%20integration%20with%20existing%20methods.%20To%20fill%20the%20gap%2C%20this%20paper%0Aproposes%20a%20holistic%20framework%20of%20Learning-Augmented%20Power%20System%20Operations%0A%28LAPSO%2C%20pronounced%20as%20Lap-So%29.%20Adopting%20a%20native%20optimization%20perspective%2C%0ALAPSO%20is%20centered%20on%20the%20operation%20stage%20and%20aims%20to%20break%20the%20boundary%20between%0Atemporally%20siloed%20power%20system%20tasks%2C%20such%20as%20forecast%2C%20operation%20and%20control%2C%0Awhile%20unifying%20the%20objectives%20of%20machine%20learning%20and%20model-based%20optimizations%0Aat%20both%20training%20and%20inference%20stages.%20Systematic%20analysis%20and%20simulations%0Ademonstrate%20the%20effectiveness%20of%20applying%20LAPSO%20in%20designing%20new%20integrated%0Aalgorithms%2C%20such%20as%20stability-constrained%20optimization%20%28SCO%29%20and%0Aobjective-based%20forecasting%20%28OBF%29%2C%20while%20enabling%20end-to-end%20tracing%20of%0Adifferent%20sources%20of%20uncertainties.%20In%20addition%2C%20a%20dedicated%20Python%0Apackage-lapso%20is%20introduced%20to%20automatically%20augment%20existing%20power%20system%0Aoptimization%20models%20with%20learnable%20components.%20All%20code%20and%20data%20are%20available%0Aat%20https%3A//github.com/xuwkk/lapso_exp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAPSO%253A%2520A%2520Unified%2520Optimization%2520View%2520for%2520Learning-Augmented%2520Power%2520System%250A%2520%2520Operations%26entry.906535625%3DWangkun%2520Xu%2520and%2520Zhongda%2520Chu%2520and%2520Fei%2520Teng%26entry.1292438233%3D%2520%2520With%2520the%2520high%2520penetration%2520of%2520renewables%252C%2520traditional%2520model-based%2520power%2520system%250Aoperation%2520is%2520challenged%2520to%2520deliver%2520economic%252C%2520stable%252C%2520and%2520robust%2520decisions.%250AMachine%2520learning%2520has%2520emerged%2520as%2520a%2520powerful%2520modeling%2520tool%2520for%2520capturing%2520complex%250Adynamics%2520to%2520address%2520these%2520challenges.%2520However%252C%2520its%2520separate%2520design%2520often%2520lacks%250Asystematic%2520integration%2520with%2520existing%2520methods.%2520To%2520fill%2520the%2520gap%252C%2520this%2520paper%250Aproposes%2520a%2520holistic%2520framework%2520of%2520Learning-Augmented%2520Power%2520System%2520Operations%250A%2528LAPSO%252C%2520pronounced%2520as%2520Lap-So%2529.%2520Adopting%2520a%2520native%2520optimization%2520perspective%252C%250ALAPSO%2520is%2520centered%2520on%2520the%2520operation%2520stage%2520and%2520aims%2520to%2520break%2520the%2520boundary%2520between%250Atemporally%2520siloed%2520power%2520system%2520tasks%252C%2520such%2520as%2520forecast%252C%2520operation%2520and%2520control%252C%250Awhile%2520unifying%2520the%2520objectives%2520of%2520machine%2520learning%2520and%2520model-based%2520optimizations%250Aat%2520both%2520training%2520and%2520inference%2520stages.%2520Systematic%2520analysis%2520and%2520simulations%250Ademonstrate%2520the%2520effectiveness%2520of%2520applying%2520LAPSO%2520in%2520designing%2520new%2520integrated%250Aalgorithms%252C%2520such%2520as%2520stability-constrained%2520optimization%2520%2528SCO%2529%2520and%250Aobjective-based%2520forecasting%2520%2528OBF%2529%252C%2520while%2520enabling%2520end-to-end%2520tracing%2520of%250Adifferent%2520sources%2520of%2520uncertainties.%2520In%2520addition%252C%2520a%2520dedicated%2520Python%250Apackage-lapso%2520is%2520introduced%2520to%2520automatically%2520augment%2520existing%2520power%2520system%250Aoptimization%2520models%2520with%2520learnable%2520components.%2520All%2520code%2520and%2520data%2520are%2520available%250Aat%2520https%253A//github.com/xuwkk/lapso_exp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAPSO%3A%20A%20Unified%20Optimization%20View%20for%20Learning-Augmented%20Power%20System%0A%20%20Operations&entry.906535625=Wangkun%20Xu%20and%20Zhongda%20Chu%20and%20Fei%20Teng&entry.1292438233=%20%20With%20the%20high%20penetration%20of%20renewables%2C%20traditional%20model-based%20power%20system%0Aoperation%20is%20challenged%20to%20deliver%20economic%2C%20stable%2C%20and%20robust%20decisions.%0AMachine%20learning%20has%20emerged%20as%20a%20powerful%20modeling%20tool%20for%20capturing%20complex%0Adynamics%20to%20address%20these%20challenges.%20However%2C%20its%20separate%20design%20often%20lacks%0Asystematic%20integration%20with%20existing%20methods.%20To%20fill%20the%20gap%2C%20this%20paper%0Aproposes%20a%20holistic%20framework%20of%20Learning-Augmented%20Power%20System%20Operations%0A%28LAPSO%2C%20pronounced%20as%20Lap-So%29.%20Adopting%20a%20native%20optimization%20perspective%2C%0ALAPSO%20is%20centered%20on%20the%20operation%20stage%20and%20aims%20to%20break%20the%20boundary%20between%0Atemporally%20siloed%20power%20system%20tasks%2C%20such%20as%20forecast%2C%20operation%20and%20control%2C%0Awhile%20unifying%20the%20objectives%20of%20machine%20learning%20and%20model-based%20optimizations%0Aat%20both%20training%20and%20inference%20stages.%20Systematic%20analysis%20and%20simulations%0Ademonstrate%20the%20effectiveness%20of%20applying%20LAPSO%20in%20designing%20new%20integrated%0Aalgorithms%2C%20such%20as%20stability-constrained%20optimization%20%28SCO%29%20and%0Aobjective-based%20forecasting%20%28OBF%29%2C%20while%20enabling%20end-to-end%20tracing%20of%0Adifferent%20sources%20of%20uncertainties.%20In%20addition%2C%20a%20dedicated%20Python%0Apackage-lapso%20is%20introduced%20to%20automatically%20augment%20existing%20power%20system%0Aoptimization%20models%20with%20learnable%20components.%20All%20code%20and%20data%20are%20available%0Aat%20https%3A//github.com/xuwkk/lapso_exp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05203v1&entry.124074799=Read"},
{"title": "T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet\n  Extraction", "author": "Kun Peng and Chaodong Tong and Cong Cao and Hao Peng and Qian Li and Guanlin Wu and Lei Jiang and Yanbing Liu and Philip S. Yu", "abstract": "  Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed\nof aspect terms, opinion terms, and sentiment polarities from given sentences.\nThe table tagging method is a popular approach to addressing this task, which\nencodes a sentence into a 2-dimensional table, allowing for the tagging of\nrelations between any two words. Previous efforts have focused on designing\nvarious downstream relation learning modules to better capture interactions\nbetween tokens in the table, revealing that a stronger capability to capture\nrelations can lead to greater improvements in the model. Motivated by this, we\nattempt to directly utilize transformer layers as downstream relation learning\nmodules. Due to the powerful semantic modeling capability of transformers, it\nis foreseeable that this will lead to excellent improvement. However, owing to\nthe quadratic relation between the length of the table and the length of the\ninput sentence sequence, using transformers directly faces two challenges:\noverly long table sequences and unfair local attention interaction. To address\nthese challenges, we propose a novel Table-Transformer (T-T) for the\ntagging-based ASTE method. Specifically, we introduce a stripe attention\nmechanism with a loop-shift strategy to tackle these challenges. The former\nmodifies the global attention mechanism to only attend to a 2-dimensional local\nattention window, while the latter facilitates interaction between different\nattention windows. Extensive and comprehensive experiments demonstrate that the\nT-T, as a downstream relation learning module, achieves state-of-the-art\nperformance with lower computational costs.\n", "link": "http://arxiv.org/abs/2505.05271v1", "date": "2025-05-08", "relevancy": 1.9328, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5278}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4539}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-T%3A%20Table%20Transformer%20for%20Tagging-based%20Aspect%20Sentiment%20Triplet%0A%20%20Extraction&body=Title%3A%20T-T%3A%20Table%20Transformer%20for%20Tagging-based%20Aspect%20Sentiment%20Triplet%0A%20%20Extraction%0AAuthor%3A%20Kun%20Peng%20and%20Chaodong%20Tong%20and%20Cong%20Cao%20and%20Hao%20Peng%20and%20Qian%20Li%20and%20Guanlin%20Wu%20and%20Lei%20Jiang%20and%20Yanbing%20Liu%20and%20Philip%20S.%20Yu%0AAbstract%3A%20%20%20Aspect%20sentiment%20triplet%20extraction%20%28ASTE%29%20aims%20to%20extract%20triplets%20composed%0Aof%20aspect%20terms%2C%20opinion%20terms%2C%20and%20sentiment%20polarities%20from%20given%20sentences.%0AThe%20table%20tagging%20method%20is%20a%20popular%20approach%20to%20addressing%20this%20task%2C%20which%0Aencodes%20a%20sentence%20into%20a%202-dimensional%20table%2C%20allowing%20for%20the%20tagging%20of%0Arelations%20between%20any%20two%20words.%20Previous%20efforts%20have%20focused%20on%20designing%0Avarious%20downstream%20relation%20learning%20modules%20to%20better%20capture%20interactions%0Abetween%20tokens%20in%20the%20table%2C%20revealing%20that%20a%20stronger%20capability%20to%20capture%0Arelations%20can%20lead%20to%20greater%20improvements%20in%20the%20model.%20Motivated%20by%20this%2C%20we%0Aattempt%20to%20directly%20utilize%20transformer%20layers%20as%20downstream%20relation%20learning%0Amodules.%20Due%20to%20the%20powerful%20semantic%20modeling%20capability%20of%20transformers%2C%20it%0Ais%20foreseeable%20that%20this%20will%20lead%20to%20excellent%20improvement.%20However%2C%20owing%20to%0Athe%20quadratic%20relation%20between%20the%20length%20of%20the%20table%20and%20the%20length%20of%20the%0Ainput%20sentence%20sequence%2C%20using%20transformers%20directly%20faces%20two%20challenges%3A%0Aoverly%20long%20table%20sequences%20and%20unfair%20local%20attention%20interaction.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20novel%20Table-Transformer%20%28T-T%29%20for%20the%0Atagging-based%20ASTE%20method.%20Specifically%2C%20we%20introduce%20a%20stripe%20attention%0Amechanism%20with%20a%20loop-shift%20strategy%20to%20tackle%20these%20challenges.%20The%20former%0Amodifies%20the%20global%20attention%20mechanism%20to%20only%20attend%20to%20a%202-dimensional%20local%0Aattention%20window%2C%20while%20the%20latter%20facilitates%20interaction%20between%20different%0Aattention%20windows.%20Extensive%20and%20comprehensive%20experiments%20demonstrate%20that%20the%0AT-T%2C%20as%20a%20downstream%20relation%20learning%20module%2C%20achieves%20state-of-the-art%0Aperformance%20with%20lower%20computational%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-T%253A%2520Table%2520Transformer%2520for%2520Tagging-based%2520Aspect%2520Sentiment%2520Triplet%250A%2520%2520Extraction%26entry.906535625%3DKun%2520Peng%2520and%2520Chaodong%2520Tong%2520and%2520Cong%2520Cao%2520and%2520Hao%2520Peng%2520and%2520Qian%2520Li%2520and%2520Guanlin%2520Wu%2520and%2520Lei%2520Jiang%2520and%2520Yanbing%2520Liu%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3D%2520%2520Aspect%2520sentiment%2520triplet%2520extraction%2520%2528ASTE%2529%2520aims%2520to%2520extract%2520triplets%2520composed%250Aof%2520aspect%2520terms%252C%2520opinion%2520terms%252C%2520and%2520sentiment%2520polarities%2520from%2520given%2520sentences.%250AThe%2520table%2520tagging%2520method%2520is%2520a%2520popular%2520approach%2520to%2520addressing%2520this%2520task%252C%2520which%250Aencodes%2520a%2520sentence%2520into%2520a%25202-dimensional%2520table%252C%2520allowing%2520for%2520the%2520tagging%2520of%250Arelations%2520between%2520any%2520two%2520words.%2520Previous%2520efforts%2520have%2520focused%2520on%2520designing%250Avarious%2520downstream%2520relation%2520learning%2520modules%2520to%2520better%2520capture%2520interactions%250Abetween%2520tokens%2520in%2520the%2520table%252C%2520revealing%2520that%2520a%2520stronger%2520capability%2520to%2520capture%250Arelations%2520can%2520lead%2520to%2520greater%2520improvements%2520in%2520the%2520model.%2520Motivated%2520by%2520this%252C%2520we%250Aattempt%2520to%2520directly%2520utilize%2520transformer%2520layers%2520as%2520downstream%2520relation%2520learning%250Amodules.%2520Due%2520to%2520the%2520powerful%2520semantic%2520modeling%2520capability%2520of%2520transformers%252C%2520it%250Ais%2520foreseeable%2520that%2520this%2520will%2520lead%2520to%2520excellent%2520improvement.%2520However%252C%2520owing%2520to%250Athe%2520quadratic%2520relation%2520between%2520the%2520length%2520of%2520the%2520table%2520and%2520the%2520length%2520of%2520the%250Ainput%2520sentence%2520sequence%252C%2520using%2520transformers%2520directly%2520faces%2520two%2520challenges%253A%250Aoverly%2520long%2520table%2520sequences%2520and%2520unfair%2520local%2520attention%2520interaction.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520Table-Transformer%2520%2528T-T%2529%2520for%2520the%250Atagging-based%2520ASTE%2520method.%2520Specifically%252C%2520we%2520introduce%2520a%2520stripe%2520attention%250Amechanism%2520with%2520a%2520loop-shift%2520strategy%2520to%2520tackle%2520these%2520challenges.%2520The%2520former%250Amodifies%2520the%2520global%2520attention%2520mechanism%2520to%2520only%2520attend%2520to%2520a%25202-dimensional%2520local%250Aattention%2520window%252C%2520while%2520the%2520latter%2520facilitates%2520interaction%2520between%2520different%250Aattention%2520windows.%2520Extensive%2520and%2520comprehensive%2520experiments%2520demonstrate%2520that%2520the%250AT-T%252C%2520as%2520a%2520downstream%2520relation%2520learning%2520module%252C%2520achieves%2520state-of-the-art%250Aperformance%2520with%2520lower%2520computational%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-T%3A%20Table%20Transformer%20for%20Tagging-based%20Aspect%20Sentiment%20Triplet%0A%20%20Extraction&entry.906535625=Kun%20Peng%20and%20Chaodong%20Tong%20and%20Cong%20Cao%20and%20Hao%20Peng%20and%20Qian%20Li%20and%20Guanlin%20Wu%20and%20Lei%20Jiang%20and%20Yanbing%20Liu%20and%20Philip%20S.%20Yu&entry.1292438233=%20%20Aspect%20sentiment%20triplet%20extraction%20%28ASTE%29%20aims%20to%20extract%20triplets%20composed%0Aof%20aspect%20terms%2C%20opinion%20terms%2C%20and%20sentiment%20polarities%20from%20given%20sentences.%0AThe%20table%20tagging%20method%20is%20a%20popular%20approach%20to%20addressing%20this%20task%2C%20which%0Aencodes%20a%20sentence%20into%20a%202-dimensional%20table%2C%20allowing%20for%20the%20tagging%20of%0Arelations%20between%20any%20two%20words.%20Previous%20efforts%20have%20focused%20on%20designing%0Avarious%20downstream%20relation%20learning%20modules%20to%20better%20capture%20interactions%0Abetween%20tokens%20in%20the%20table%2C%20revealing%20that%20a%20stronger%20capability%20to%20capture%0Arelations%20can%20lead%20to%20greater%20improvements%20in%20the%20model.%20Motivated%20by%20this%2C%20we%0Aattempt%20to%20directly%20utilize%20transformer%20layers%20as%20downstream%20relation%20learning%0Amodules.%20Due%20to%20the%20powerful%20semantic%20modeling%20capability%20of%20transformers%2C%20it%0Ais%20foreseeable%20that%20this%20will%20lead%20to%20excellent%20improvement.%20However%2C%20owing%20to%0Athe%20quadratic%20relation%20between%20the%20length%20of%20the%20table%20and%20the%20length%20of%20the%0Ainput%20sentence%20sequence%2C%20using%20transformers%20directly%20faces%20two%20challenges%3A%0Aoverly%20long%20table%20sequences%20and%20unfair%20local%20attention%20interaction.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20novel%20Table-Transformer%20%28T-T%29%20for%20the%0Atagging-based%20ASTE%20method.%20Specifically%2C%20we%20introduce%20a%20stripe%20attention%0Amechanism%20with%20a%20loop-shift%20strategy%20to%20tackle%20these%20challenges.%20The%20former%0Amodifies%20the%20global%20attention%20mechanism%20to%20only%20attend%20to%20a%202-dimensional%20local%0Aattention%20window%2C%20while%20the%20latter%20facilitates%20interaction%20between%20different%0Aattention%20windows.%20Extensive%20and%20comprehensive%20experiments%20demonstrate%20that%20the%0AT-T%2C%20as%20a%20downstream%20relation%20learning%20module%2C%20achieves%20state-of-the-art%0Aperformance%20with%20lower%20computational%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05271v1&entry.124074799=Read"},
{"title": "Scalable Chain of Thoughts via Elastic Reasoning", "author": "Yuhui Xu and Hanze Dong and Lei Wang and Doyen Sahoo and Junnan Li and Caiming Xiong", "abstract": "  Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.\n", "link": "http://arxiv.org/abs/2505.05315v1", "date": "2025-05-08", "relevancy": 1.9276, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Chain%20of%20Thoughts%20via%20Elastic%20Reasoning&body=Title%3A%20Scalable%20Chain%20of%20Thoughts%20via%20Elastic%20Reasoning%0AAuthor%3A%20Yuhui%20Xu%20and%20Hanze%20Dong%20and%20Lei%20Wang%20and%20Doyen%20Sahoo%20and%20Junnan%20Li%20and%20Caiming%20Xiong%0AAbstract%3A%20%20%20Large%20reasoning%20models%20%28LRMs%29%20have%20achieved%20remarkable%20progress%20on%20complex%0Atasks%20by%20generating%20extended%20chains%20of%20thought%20%28CoT%29.%20However%2C%20their%0Auncontrolled%20output%20lengths%20pose%20significant%20challenges%20for%20real-world%0Adeployment%2C%20where%20inference-time%20budgets%20on%20tokens%2C%20latency%2C%20or%20compute%20are%0Astrictly%20constrained.%20We%20propose%20Elastic%20Reasoning%2C%20a%20novel%20framework%20for%0Ascalable%20chain%20of%20thoughts%20that%20explicitly%20separates%20reasoning%20into%20two%0Aphases--thinking%20and%20solution--with%20independently%20allocated%20budgets.%20At%20test%0Atime%2C%20Elastic%20Reasoning%20prioritize%20that%20completeness%20of%20solution%20segments%2C%0Asignificantly%20improving%20reliability%20under%20tight%20resource%20constraints.%20To%20train%0Amodels%20that%20are%20robust%20to%20truncated%20thinking%2C%20we%20introduce%20a%20lightweight%0Abudget-constrained%20rollout%20strategy%2C%20integrated%20into%20GRPO%2C%20which%20teaches%20the%0Amodel%20to%20reason%20adaptively%20when%20the%20thinking%20process%20is%20cut%20short%20and%0Ageneralizes%20effectively%20to%20unseen%20budget%20constraints%20without%20additional%0Atraining.%20Empirical%20results%20on%20mathematical%20%28AIME%2C%20MATH500%29%20and%20programming%0A%28LiveCodeBench%2C%20Codeforces%29%20benchmarks%20demonstrate%20that%20Elastic%20Reasoning%0Aperforms%20robustly%20under%20strict%20budget%20constraints%2C%20while%20incurring%0Asignificantly%20lower%20training%20cost%20than%20baseline%20methods.%20Remarkably%2C%20our%0Aapproach%20also%20produces%20more%20concise%20and%20efficient%20reasoning%20even%20in%0Aunconstrained%20settings.%20Elastic%20Reasoning%20offers%20a%20principled%20and%20practical%0Asolution%20to%20the%20pressing%20challenge%20of%20controllable%20reasoning%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Chain%2520of%2520Thoughts%2520via%2520Elastic%2520Reasoning%26entry.906535625%3DYuhui%2520Xu%2520and%2520Hanze%2520Dong%2520and%2520Lei%2520Wang%2520and%2520Doyen%2520Sahoo%2520and%2520Junnan%2520Li%2520and%2520Caiming%2520Xiong%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520%2528LRMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520on%2520complex%250Atasks%2520by%2520generating%2520extended%2520chains%2520of%2520thought%2520%2528CoT%2529.%2520However%252C%2520their%250Auncontrolled%2520output%2520lengths%2520pose%2520significant%2520challenges%2520for%2520real-world%250Adeployment%252C%2520where%2520inference-time%2520budgets%2520on%2520tokens%252C%2520latency%252C%2520or%2520compute%2520are%250Astrictly%2520constrained.%2520We%2520propose%2520Elastic%2520Reasoning%252C%2520a%2520novel%2520framework%2520for%250Ascalable%2520chain%2520of%2520thoughts%2520that%2520explicitly%2520separates%2520reasoning%2520into%2520two%250Aphases--thinking%2520and%2520solution--with%2520independently%2520allocated%2520budgets.%2520At%2520test%250Atime%252C%2520Elastic%2520Reasoning%2520prioritize%2520that%2520completeness%2520of%2520solution%2520segments%252C%250Asignificantly%2520improving%2520reliability%2520under%2520tight%2520resource%2520constraints.%2520To%2520train%250Amodels%2520that%2520are%2520robust%2520to%2520truncated%2520thinking%252C%2520we%2520introduce%2520a%2520lightweight%250Abudget-constrained%2520rollout%2520strategy%252C%2520integrated%2520into%2520GRPO%252C%2520which%2520teaches%2520the%250Amodel%2520to%2520reason%2520adaptively%2520when%2520the%2520thinking%2520process%2520is%2520cut%2520short%2520and%250Ageneralizes%2520effectively%2520to%2520unseen%2520budget%2520constraints%2520without%2520additional%250Atraining.%2520Empirical%2520results%2520on%2520mathematical%2520%2528AIME%252C%2520MATH500%2529%2520and%2520programming%250A%2528LiveCodeBench%252C%2520Codeforces%2529%2520benchmarks%2520demonstrate%2520that%2520Elastic%2520Reasoning%250Aperforms%2520robustly%2520under%2520strict%2520budget%2520constraints%252C%2520while%2520incurring%250Asignificantly%2520lower%2520training%2520cost%2520than%2520baseline%2520methods.%2520Remarkably%252C%2520our%250Aapproach%2520also%2520produces%2520more%2520concise%2520and%2520efficient%2520reasoning%2520even%2520in%250Aunconstrained%2520settings.%2520Elastic%2520Reasoning%2520offers%2520a%2520principled%2520and%2520practical%250Asolution%2520to%2520the%2520pressing%2520challenge%2520of%2520controllable%2520reasoning%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Chain%20of%20Thoughts%20via%20Elastic%20Reasoning&entry.906535625=Yuhui%20Xu%20and%20Hanze%20Dong%20and%20Lei%20Wang%20and%20Doyen%20Sahoo%20and%20Junnan%20Li%20and%20Caiming%20Xiong&entry.1292438233=%20%20Large%20reasoning%20models%20%28LRMs%29%20have%20achieved%20remarkable%20progress%20on%20complex%0Atasks%20by%20generating%20extended%20chains%20of%20thought%20%28CoT%29.%20However%2C%20their%0Auncontrolled%20output%20lengths%20pose%20significant%20challenges%20for%20real-world%0Adeployment%2C%20where%20inference-time%20budgets%20on%20tokens%2C%20latency%2C%20or%20compute%20are%0Astrictly%20constrained.%20We%20propose%20Elastic%20Reasoning%2C%20a%20novel%20framework%20for%0Ascalable%20chain%20of%20thoughts%20that%20explicitly%20separates%20reasoning%20into%20two%0Aphases--thinking%20and%20solution--with%20independently%20allocated%20budgets.%20At%20test%0Atime%2C%20Elastic%20Reasoning%20prioritize%20that%20completeness%20of%20solution%20segments%2C%0Asignificantly%20improving%20reliability%20under%20tight%20resource%20constraints.%20To%20train%0Amodels%20that%20are%20robust%20to%20truncated%20thinking%2C%20we%20introduce%20a%20lightweight%0Abudget-constrained%20rollout%20strategy%2C%20integrated%20into%20GRPO%2C%20which%20teaches%20the%0Amodel%20to%20reason%20adaptively%20when%20the%20thinking%20process%20is%20cut%20short%20and%0Ageneralizes%20effectively%20to%20unseen%20budget%20constraints%20without%20additional%0Atraining.%20Empirical%20results%20on%20mathematical%20%28AIME%2C%20MATH500%29%20and%20programming%0A%28LiveCodeBench%2C%20Codeforces%29%20benchmarks%20demonstrate%20that%20Elastic%20Reasoning%0Aperforms%20robustly%20under%20strict%20budget%20constraints%2C%20while%20incurring%0Asignificantly%20lower%20training%20cost%20than%20baseline%20methods.%20Remarkably%2C%20our%0Aapproach%20also%20produces%20more%20concise%20and%20efficient%20reasoning%20even%20in%0Aunconstrained%20settings.%20Elastic%20Reasoning%20offers%20a%20principled%20and%20practical%0Asolution%20to%20the%20pressing%20challenge%20of%20controllable%20reasoning%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05315v1&entry.124074799=Read"},
{"title": "OcularAge: A Comparative Study of Iris and Periocular Images for\n  Pediatric Age Estimation", "author": "Naveenkumar G Venkataswamy and Poorna Ravi and Stephanie Schuckers and Masudul H. Imtiaz", "abstract": "  Estimating a child's age from ocular biometric images is challenging due to\nsubtle physiological changes and the limited availability of longitudinal\ndatasets. Although most biometric age estimation studies have focused on facial\nfeatures and adult subjects, pediatric-specific analysis, particularly of the\niris and periocular regions, remains relatively unexplored. This study presents\na comparative evaluation of iris and periocular images for estimating the ages\nof children aged between 4 and 16 years. We utilized a longitudinal dataset\ncomprising more than 21,000 near-infrared (NIR) images, collected from 288\npediatric subjects over eight years using two different imaging sensors. A\nmulti-task deep learning framework was employed to jointly perform age\nprediction and age-group classification, enabling a systematic exploration of\nhow different convolutional neural network (CNN) architectures, particularly\nthose adapted for non-square ocular inputs, capture the complex variability\ninherent in pediatric eye images. The results show that periocular models\nconsistently outperform iris-based models, achieving a mean absolute error\n(MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These\nresults mark the first demonstration that reliable age estimation is feasible\nfrom children's ocular images, enabling privacy-preserving age checks in\nchild-centric applications. This work establishes the first longitudinal\nbenchmark for pediatric ocular age estimation, providing a foundation for\ndesigning robust, child-focused biometric systems. The developed models proved\nresilient across different imaging sensors, confirming their potential for\nreal-world deployment. They also achieved inference speeds of less than 10\nmilliseconds per image on resource-constrained VR headsets, demonstrating their\nsuitability for real-time applications.\n", "link": "http://arxiv.org/abs/2505.05374v1", "date": "2025-05-08", "relevancy": 1.9267, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4876}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4775}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OcularAge%3A%20A%20Comparative%20Study%20of%20Iris%20and%20Periocular%20Images%20for%0A%20%20Pediatric%20Age%20Estimation&body=Title%3A%20OcularAge%3A%20A%20Comparative%20Study%20of%20Iris%20and%20Periocular%20Images%20for%0A%20%20Pediatric%20Age%20Estimation%0AAuthor%3A%20Naveenkumar%20G%20Venkataswamy%20and%20Poorna%20Ravi%20and%20Stephanie%20Schuckers%20and%20Masudul%20H.%20Imtiaz%0AAbstract%3A%20%20%20Estimating%20a%20child%27s%20age%20from%20ocular%20biometric%20images%20is%20challenging%20due%20to%0Asubtle%20physiological%20changes%20and%20the%20limited%20availability%20of%20longitudinal%0Adatasets.%20Although%20most%20biometric%20age%20estimation%20studies%20have%20focused%20on%20facial%0Afeatures%20and%20adult%20subjects%2C%20pediatric-specific%20analysis%2C%20particularly%20of%20the%0Airis%20and%20periocular%20regions%2C%20remains%20relatively%20unexplored.%20This%20study%20presents%0Aa%20comparative%20evaluation%20of%20iris%20and%20periocular%20images%20for%20estimating%20the%20ages%0Aof%20children%20aged%20between%204%20and%2016%20years.%20We%20utilized%20a%20longitudinal%20dataset%0Acomprising%20more%20than%2021%2C000%20near-infrared%20%28NIR%29%20images%2C%20collected%20from%20288%0Apediatric%20subjects%20over%20eight%20years%20using%20two%20different%20imaging%20sensors.%20A%0Amulti-task%20deep%20learning%20framework%20was%20employed%20to%20jointly%20perform%20age%0Aprediction%20and%20age-group%20classification%2C%20enabling%20a%20systematic%20exploration%20of%0Ahow%20different%20convolutional%20neural%20network%20%28CNN%29%20architectures%2C%20particularly%0Athose%20adapted%20for%20non-square%20ocular%20inputs%2C%20capture%20the%20complex%20variability%0Ainherent%20in%20pediatric%20eye%20images.%20The%20results%20show%20that%20periocular%20models%0Aconsistently%20outperform%20iris-based%20models%2C%20achieving%20a%20mean%20absolute%20error%0A%28MAE%29%20of%201.33%20years%20and%20an%20age-group%20classification%20accuracy%20of%2083.82%25.%20These%0Aresults%20mark%20the%20first%20demonstration%20that%20reliable%20age%20estimation%20is%20feasible%0Afrom%20children%27s%20ocular%20images%2C%20enabling%20privacy-preserving%20age%20checks%20in%0Achild-centric%20applications.%20This%20work%20establishes%20the%20first%20longitudinal%0Abenchmark%20for%20pediatric%20ocular%20age%20estimation%2C%20providing%20a%20foundation%20for%0Adesigning%20robust%2C%20child-focused%20biometric%20systems.%20The%20developed%20models%20proved%0Aresilient%20across%20different%20imaging%20sensors%2C%20confirming%20their%20potential%20for%0Areal-world%20deployment.%20They%20also%20achieved%20inference%20speeds%20of%20less%20than%2010%0Amilliseconds%20per%20image%20on%20resource-constrained%20VR%20headsets%2C%20demonstrating%20their%0Asuitability%20for%20real-time%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOcularAge%253A%2520A%2520Comparative%2520Study%2520of%2520Iris%2520and%2520Periocular%2520Images%2520for%250A%2520%2520Pediatric%2520Age%2520Estimation%26entry.906535625%3DNaveenkumar%2520G%2520Venkataswamy%2520and%2520Poorna%2520Ravi%2520and%2520Stephanie%2520Schuckers%2520and%2520Masudul%2520H.%2520Imtiaz%26entry.1292438233%3D%2520%2520Estimating%2520a%2520child%2527s%2520age%2520from%2520ocular%2520biometric%2520images%2520is%2520challenging%2520due%2520to%250Asubtle%2520physiological%2520changes%2520and%2520the%2520limited%2520availability%2520of%2520longitudinal%250Adatasets.%2520Although%2520most%2520biometric%2520age%2520estimation%2520studies%2520have%2520focused%2520on%2520facial%250Afeatures%2520and%2520adult%2520subjects%252C%2520pediatric-specific%2520analysis%252C%2520particularly%2520of%2520the%250Airis%2520and%2520periocular%2520regions%252C%2520remains%2520relatively%2520unexplored.%2520This%2520study%2520presents%250Aa%2520comparative%2520evaluation%2520of%2520iris%2520and%2520periocular%2520images%2520for%2520estimating%2520the%2520ages%250Aof%2520children%2520aged%2520between%25204%2520and%252016%2520years.%2520We%2520utilized%2520a%2520longitudinal%2520dataset%250Acomprising%2520more%2520than%252021%252C000%2520near-infrared%2520%2528NIR%2529%2520images%252C%2520collected%2520from%2520288%250Apediatric%2520subjects%2520over%2520eight%2520years%2520using%2520two%2520different%2520imaging%2520sensors.%2520A%250Amulti-task%2520deep%2520learning%2520framework%2520was%2520employed%2520to%2520jointly%2520perform%2520age%250Aprediction%2520and%2520age-group%2520classification%252C%2520enabling%2520a%2520systematic%2520exploration%2520of%250Ahow%2520different%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520architectures%252C%2520particularly%250Athose%2520adapted%2520for%2520non-square%2520ocular%2520inputs%252C%2520capture%2520the%2520complex%2520variability%250Ainherent%2520in%2520pediatric%2520eye%2520images.%2520The%2520results%2520show%2520that%2520periocular%2520models%250Aconsistently%2520outperform%2520iris-based%2520models%252C%2520achieving%2520a%2520mean%2520absolute%2520error%250A%2528MAE%2529%2520of%25201.33%2520years%2520and%2520an%2520age-group%2520classification%2520accuracy%2520of%252083.82%2525.%2520These%250Aresults%2520mark%2520the%2520first%2520demonstration%2520that%2520reliable%2520age%2520estimation%2520is%2520feasible%250Afrom%2520children%2527s%2520ocular%2520images%252C%2520enabling%2520privacy-preserving%2520age%2520checks%2520in%250Achild-centric%2520applications.%2520This%2520work%2520establishes%2520the%2520first%2520longitudinal%250Abenchmark%2520for%2520pediatric%2520ocular%2520age%2520estimation%252C%2520providing%2520a%2520foundation%2520for%250Adesigning%2520robust%252C%2520child-focused%2520biometric%2520systems.%2520The%2520developed%2520models%2520proved%250Aresilient%2520across%2520different%2520imaging%2520sensors%252C%2520confirming%2520their%2520potential%2520for%250Areal-world%2520deployment.%2520They%2520also%2520achieved%2520inference%2520speeds%2520of%2520less%2520than%252010%250Amilliseconds%2520per%2520image%2520on%2520resource-constrained%2520VR%2520headsets%252C%2520demonstrating%2520their%250Asuitability%2520for%2520real-time%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OcularAge%3A%20A%20Comparative%20Study%20of%20Iris%20and%20Periocular%20Images%20for%0A%20%20Pediatric%20Age%20Estimation&entry.906535625=Naveenkumar%20G%20Venkataswamy%20and%20Poorna%20Ravi%20and%20Stephanie%20Schuckers%20and%20Masudul%20H.%20Imtiaz&entry.1292438233=%20%20Estimating%20a%20child%27s%20age%20from%20ocular%20biometric%20images%20is%20challenging%20due%20to%0Asubtle%20physiological%20changes%20and%20the%20limited%20availability%20of%20longitudinal%0Adatasets.%20Although%20most%20biometric%20age%20estimation%20studies%20have%20focused%20on%20facial%0Afeatures%20and%20adult%20subjects%2C%20pediatric-specific%20analysis%2C%20particularly%20of%20the%0Airis%20and%20periocular%20regions%2C%20remains%20relatively%20unexplored.%20This%20study%20presents%0Aa%20comparative%20evaluation%20of%20iris%20and%20periocular%20images%20for%20estimating%20the%20ages%0Aof%20children%20aged%20between%204%20and%2016%20years.%20We%20utilized%20a%20longitudinal%20dataset%0Acomprising%20more%20than%2021%2C000%20near-infrared%20%28NIR%29%20images%2C%20collected%20from%20288%0Apediatric%20subjects%20over%20eight%20years%20using%20two%20different%20imaging%20sensors.%20A%0Amulti-task%20deep%20learning%20framework%20was%20employed%20to%20jointly%20perform%20age%0Aprediction%20and%20age-group%20classification%2C%20enabling%20a%20systematic%20exploration%20of%0Ahow%20different%20convolutional%20neural%20network%20%28CNN%29%20architectures%2C%20particularly%0Athose%20adapted%20for%20non-square%20ocular%20inputs%2C%20capture%20the%20complex%20variability%0Ainherent%20in%20pediatric%20eye%20images.%20The%20results%20show%20that%20periocular%20models%0Aconsistently%20outperform%20iris-based%20models%2C%20achieving%20a%20mean%20absolute%20error%0A%28MAE%29%20of%201.33%20years%20and%20an%20age-group%20classification%20accuracy%20of%2083.82%25.%20These%0Aresults%20mark%20the%20first%20demonstration%20that%20reliable%20age%20estimation%20is%20feasible%0Afrom%20children%27s%20ocular%20images%2C%20enabling%20privacy-preserving%20age%20checks%20in%0Achild-centric%20applications.%20This%20work%20establishes%20the%20first%20longitudinal%0Abenchmark%20for%20pediatric%20ocular%20age%20estimation%2C%20providing%20a%20foundation%20for%0Adesigning%20robust%2C%20child-focused%20biometric%20systems.%20The%20developed%20models%20proved%0Aresilient%20across%20different%20imaging%20sensors%2C%20confirming%20their%20potential%20for%0Areal-world%20deployment.%20They%20also%20achieved%20inference%20speeds%20of%20less%20than%2010%0Amilliseconds%20per%20image%20on%20resource-constrained%20VR%20headsets%2C%20demonstrating%20their%0Asuitability%20for%20real-time%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05374v1&entry.124074799=Read"},
{"title": "Free Discontinuity Regression: With an Application to the Economic\n  Effects of Internet Shutdowns", "author": "Florian Gunsilius and David Van Dijcke", "abstract": "  Sharp, multidimensional changepoints-abrupt shifts in a regression surface\nwhose locations and magnitudes are unknown-arise in settings as varied as\ngene-expression profiling, financial covariance breaks, climate-regime\ndetection, and urban socioeconomic mapping. Despite their prevalence, there are\nno current approaches that jointly estimate the location and size of the\ndiscontinuity set in a one-shot approach with statistical guarantees. We\ntherefore introduce Free Discontinuity Regression (FDR), a fully nonparametric\nestimator that simultaneously (i) smooths a regression surface, (ii) segments\nit into contiguous regions, and (iii) provably recovers the precise locations\nand sizes of its jumps. By extending a convex relaxation of the Mumford-Shah\nfunctional to random spatial sampling and correlated noise, FDR overcomes the\nfixed-grid and i.i.d. noise assumptions of classical image-segmentation\napproaches, thus enabling its application to real-world data of any dimension.\nThis yields the first identification and uniform consistency results for\nmultivariate jump surfaces: under mild SBV regularity, the estimated function,\nits discontinuity set, and all jump sizes converge to their true population\ncounterparts. Hyperparameters are selected automatically from the data using\nStein's Unbiased Risk Estimate, and large-scale simulations up to three\ndimensions validate the theoretical results and demonstrate good finite-sample\nperformance. Applying FDR to an internet shutdown in India reveals a 25-35%\nreduction in economic activity around the estimated shutdown boundaries-much\nlarger than previous estimates. By unifying smoothing, segmentation, and\neffect-size recovery in a general statistical setting, FDR turns\nfree-discontinuity ideas into a practical tool with formal guarantees for\nmodern multivariate data.\n", "link": "http://arxiv.org/abs/2309.14630v3", "date": "2025-05-08", "relevancy": 1.9267, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4836}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4805}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free%20Discontinuity%20Regression%3A%20With%20an%20Application%20to%20the%20Economic%0A%20%20Effects%20of%20Internet%20Shutdowns&body=Title%3A%20Free%20Discontinuity%20Regression%3A%20With%20an%20Application%20to%20the%20Economic%0A%20%20Effects%20of%20Internet%20Shutdowns%0AAuthor%3A%20Florian%20Gunsilius%20and%20David%20Van%20Dijcke%0AAbstract%3A%20%20%20Sharp%2C%20multidimensional%20changepoints-abrupt%20shifts%20in%20a%20regression%20surface%0Awhose%20locations%20and%20magnitudes%20are%20unknown-arise%20in%20settings%20as%20varied%20as%0Agene-expression%20profiling%2C%20financial%20covariance%20breaks%2C%20climate-regime%0Adetection%2C%20and%20urban%20socioeconomic%20mapping.%20Despite%20their%20prevalence%2C%20there%20are%0Ano%20current%20approaches%20that%20jointly%20estimate%20the%20location%20and%20size%20of%20the%0Adiscontinuity%20set%20in%20a%20one-shot%20approach%20with%20statistical%20guarantees.%20We%0Atherefore%20introduce%20Free%20Discontinuity%20Regression%20%28FDR%29%2C%20a%20fully%20nonparametric%0Aestimator%20that%20simultaneously%20%28i%29%20smooths%20a%20regression%20surface%2C%20%28ii%29%20segments%0Ait%20into%20contiguous%20regions%2C%20and%20%28iii%29%20provably%20recovers%20the%20precise%20locations%0Aand%20sizes%20of%20its%20jumps.%20By%20extending%20a%20convex%20relaxation%20of%20the%20Mumford-Shah%0Afunctional%20to%20random%20spatial%20sampling%20and%20correlated%20noise%2C%20FDR%20overcomes%20the%0Afixed-grid%20and%20i.i.d.%20noise%20assumptions%20of%20classical%20image-segmentation%0Aapproaches%2C%20thus%20enabling%20its%20application%20to%20real-world%20data%20of%20any%20dimension.%0AThis%20yields%20the%20first%20identification%20and%20uniform%20consistency%20results%20for%0Amultivariate%20jump%20surfaces%3A%20under%20mild%20SBV%20regularity%2C%20the%20estimated%20function%2C%0Aits%20discontinuity%20set%2C%20and%20all%20jump%20sizes%20converge%20to%20their%20true%20population%0Acounterparts.%20Hyperparameters%20are%20selected%20automatically%20from%20the%20data%20using%0AStein%27s%20Unbiased%20Risk%20Estimate%2C%20and%20large-scale%20simulations%20up%20to%20three%0Adimensions%20validate%20the%20theoretical%20results%20and%20demonstrate%20good%20finite-sample%0Aperformance.%20Applying%20FDR%20to%20an%20internet%20shutdown%20in%20India%20reveals%20a%2025-35%25%0Areduction%20in%20economic%20activity%20around%20the%20estimated%20shutdown%20boundaries-much%0Alarger%20than%20previous%20estimates.%20By%20unifying%20smoothing%2C%20segmentation%2C%20and%0Aeffect-size%20recovery%20in%20a%20general%20statistical%20setting%2C%20FDR%20turns%0Afree-discontinuity%20ideas%20into%20a%20practical%20tool%20with%20formal%20guarantees%20for%0Amodern%20multivariate%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14630v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree%2520Discontinuity%2520Regression%253A%2520With%2520an%2520Application%2520to%2520the%2520Economic%250A%2520%2520Effects%2520of%2520Internet%2520Shutdowns%26entry.906535625%3DFlorian%2520Gunsilius%2520and%2520David%2520Van%2520Dijcke%26entry.1292438233%3D%2520%2520Sharp%252C%2520multidimensional%2520changepoints-abrupt%2520shifts%2520in%2520a%2520regression%2520surface%250Awhose%2520locations%2520and%2520magnitudes%2520are%2520unknown-arise%2520in%2520settings%2520as%2520varied%2520as%250Agene-expression%2520profiling%252C%2520financial%2520covariance%2520breaks%252C%2520climate-regime%250Adetection%252C%2520and%2520urban%2520socioeconomic%2520mapping.%2520Despite%2520their%2520prevalence%252C%2520there%2520are%250Ano%2520current%2520approaches%2520that%2520jointly%2520estimate%2520the%2520location%2520and%2520size%2520of%2520the%250Adiscontinuity%2520set%2520in%2520a%2520one-shot%2520approach%2520with%2520statistical%2520guarantees.%2520We%250Atherefore%2520introduce%2520Free%2520Discontinuity%2520Regression%2520%2528FDR%2529%252C%2520a%2520fully%2520nonparametric%250Aestimator%2520that%2520simultaneously%2520%2528i%2529%2520smooths%2520a%2520regression%2520surface%252C%2520%2528ii%2529%2520segments%250Ait%2520into%2520contiguous%2520regions%252C%2520and%2520%2528iii%2529%2520provably%2520recovers%2520the%2520precise%2520locations%250Aand%2520sizes%2520of%2520its%2520jumps.%2520By%2520extending%2520a%2520convex%2520relaxation%2520of%2520the%2520Mumford-Shah%250Afunctional%2520to%2520random%2520spatial%2520sampling%2520and%2520correlated%2520noise%252C%2520FDR%2520overcomes%2520the%250Afixed-grid%2520and%2520i.i.d.%2520noise%2520assumptions%2520of%2520classical%2520image-segmentation%250Aapproaches%252C%2520thus%2520enabling%2520its%2520application%2520to%2520real-world%2520data%2520of%2520any%2520dimension.%250AThis%2520yields%2520the%2520first%2520identification%2520and%2520uniform%2520consistency%2520results%2520for%250Amultivariate%2520jump%2520surfaces%253A%2520under%2520mild%2520SBV%2520regularity%252C%2520the%2520estimated%2520function%252C%250Aits%2520discontinuity%2520set%252C%2520and%2520all%2520jump%2520sizes%2520converge%2520to%2520their%2520true%2520population%250Acounterparts.%2520Hyperparameters%2520are%2520selected%2520automatically%2520from%2520the%2520data%2520using%250AStein%2527s%2520Unbiased%2520Risk%2520Estimate%252C%2520and%2520large-scale%2520simulations%2520up%2520to%2520three%250Adimensions%2520validate%2520the%2520theoretical%2520results%2520and%2520demonstrate%2520good%2520finite-sample%250Aperformance.%2520Applying%2520FDR%2520to%2520an%2520internet%2520shutdown%2520in%2520India%2520reveals%2520a%252025-35%2525%250Areduction%2520in%2520economic%2520activity%2520around%2520the%2520estimated%2520shutdown%2520boundaries-much%250Alarger%2520than%2520previous%2520estimates.%2520By%2520unifying%2520smoothing%252C%2520segmentation%252C%2520and%250Aeffect-size%2520recovery%2520in%2520a%2520general%2520statistical%2520setting%252C%2520FDR%2520turns%250Afree-discontinuity%2520ideas%2520into%2520a%2520practical%2520tool%2520with%2520formal%2520guarantees%2520for%250Amodern%2520multivariate%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14630v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free%20Discontinuity%20Regression%3A%20With%20an%20Application%20to%20the%20Economic%0A%20%20Effects%20of%20Internet%20Shutdowns&entry.906535625=Florian%20Gunsilius%20and%20David%20Van%20Dijcke&entry.1292438233=%20%20Sharp%2C%20multidimensional%20changepoints-abrupt%20shifts%20in%20a%20regression%20surface%0Awhose%20locations%20and%20magnitudes%20are%20unknown-arise%20in%20settings%20as%20varied%20as%0Agene-expression%20profiling%2C%20financial%20covariance%20breaks%2C%20climate-regime%0Adetection%2C%20and%20urban%20socioeconomic%20mapping.%20Despite%20their%20prevalence%2C%20there%20are%0Ano%20current%20approaches%20that%20jointly%20estimate%20the%20location%20and%20size%20of%20the%0Adiscontinuity%20set%20in%20a%20one-shot%20approach%20with%20statistical%20guarantees.%20We%0Atherefore%20introduce%20Free%20Discontinuity%20Regression%20%28FDR%29%2C%20a%20fully%20nonparametric%0Aestimator%20that%20simultaneously%20%28i%29%20smooths%20a%20regression%20surface%2C%20%28ii%29%20segments%0Ait%20into%20contiguous%20regions%2C%20and%20%28iii%29%20provably%20recovers%20the%20precise%20locations%0Aand%20sizes%20of%20its%20jumps.%20By%20extending%20a%20convex%20relaxation%20of%20the%20Mumford-Shah%0Afunctional%20to%20random%20spatial%20sampling%20and%20correlated%20noise%2C%20FDR%20overcomes%20the%0Afixed-grid%20and%20i.i.d.%20noise%20assumptions%20of%20classical%20image-segmentation%0Aapproaches%2C%20thus%20enabling%20its%20application%20to%20real-world%20data%20of%20any%20dimension.%0AThis%20yields%20the%20first%20identification%20and%20uniform%20consistency%20results%20for%0Amultivariate%20jump%20surfaces%3A%20under%20mild%20SBV%20regularity%2C%20the%20estimated%20function%2C%0Aits%20discontinuity%20set%2C%20and%20all%20jump%20sizes%20converge%20to%20their%20true%20population%0Acounterparts.%20Hyperparameters%20are%20selected%20automatically%20from%20the%20data%20using%0AStein%27s%20Unbiased%20Risk%20Estimate%2C%20and%20large-scale%20simulations%20up%20to%20three%0Adimensions%20validate%20the%20theoretical%20results%20and%20demonstrate%20good%20finite-sample%0Aperformance.%20Applying%20FDR%20to%20an%20internet%20shutdown%20in%20India%20reveals%20a%2025-35%25%0Areduction%20in%20economic%20activity%20around%20the%20estimated%20shutdown%20boundaries-much%0Alarger%20than%20previous%20estimates.%20By%20unifying%20smoothing%2C%20segmentation%2C%20and%0Aeffect-size%20recovery%20in%20a%20general%20statistical%20setting%2C%20FDR%20turns%0Afree-discontinuity%20ideas%20into%20a%20practical%20tool%20with%20formal%20guarantees%20for%0Amodern%20multivariate%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14630v3&entry.124074799=Read"},
{"title": "Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep\n  Learning", "author": "Muhammad Irfan and Anum Nawaz and Riku Klen and Abdulhamit Subasi and Tomi Westerlund and Wei Chen", "abstract": "  Early detection and accurate diagnosis are essential to improving patient\noutcomes. The use of convolutional neural networks (CNNs) for tumor detection\nhas shown promise, but existing models often suffer from overparameterization,\nwhich limits their performance gains. In this study, fuzzy sigmoid convolution\n(FSC) is introduced along with two additional modules: top-of-the-funnel and\nmiddle-of-the-funnel. The proposed methodology significantly reduces the number\nof trainable parameters without compromising classification accuracy. A novel\nconvolutional operator is central to this approach, effectively dilating the\nreceptive field while preserving input data integrity. This enables efficient\nfeature map reduction and enhances the model's tumor detection capability. In\nthe FSC-based model, fuzzy sigmoid activation functions are incorporated within\nconvolutional layers to improve feature extraction and classification. The\ninclusion of fuzzy logic into the architecture improves its adaptability and\nrobustness. Extensive experiments on three benchmark datasets demonstrate the\nsuperior performance and efficiency of the proposed model. The FSC-based\narchitecture achieved classification accuracies of 99.17%, 99.75%, and 99.89%\non three different datasets. The model employs 100 times fewer parameters than\nlarge-scale transfer learning architectures, highlighting its computational\nefficiency and suitability for detecting brain tumors early. This research\noffers lightweight, high-performance deep-learning models for medical imaging\napplications.\n", "link": "http://arxiv.org/abs/2505.05208v1", "date": "2025-05-08", "relevancy": 1.921, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4955}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4802}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Brain%20Tumor%20Detection%20in%20MRI%3A%20Fuzzy%20Sigmoid%20Convolution%20in%20Deep%0A%20%20Learning&body=Title%3A%20Improved%20Brain%20Tumor%20Detection%20in%20MRI%3A%20Fuzzy%20Sigmoid%20Convolution%20in%20Deep%0A%20%20Learning%0AAuthor%3A%20Muhammad%20Irfan%20and%20Anum%20Nawaz%20and%20Riku%20Klen%20and%20Abdulhamit%20Subasi%20and%20Tomi%20Westerlund%20and%20Wei%20Chen%0AAbstract%3A%20%20%20Early%20detection%20and%20accurate%20diagnosis%20are%20essential%20to%20improving%20patient%0Aoutcomes.%20The%20use%20of%20convolutional%20neural%20networks%20%28CNNs%29%20for%20tumor%20detection%0Ahas%20shown%20promise%2C%20but%20existing%20models%20often%20suffer%20from%20overparameterization%2C%0Awhich%20limits%20their%20performance%20gains.%20In%20this%20study%2C%20fuzzy%20sigmoid%20convolution%0A%28FSC%29%20is%20introduced%20along%20with%20two%20additional%20modules%3A%20top-of-the-funnel%20and%0Amiddle-of-the-funnel.%20The%20proposed%20methodology%20significantly%20reduces%20the%20number%0Aof%20trainable%20parameters%20without%20compromising%20classification%20accuracy.%20A%20novel%0Aconvolutional%20operator%20is%20central%20to%20this%20approach%2C%20effectively%20dilating%20the%0Areceptive%20field%20while%20preserving%20input%20data%20integrity.%20This%20enables%20efficient%0Afeature%20map%20reduction%20and%20enhances%20the%20model%27s%20tumor%20detection%20capability.%20In%0Athe%20FSC-based%20model%2C%20fuzzy%20sigmoid%20activation%20functions%20are%20incorporated%20within%0Aconvolutional%20layers%20to%20improve%20feature%20extraction%20and%20classification.%20The%0Ainclusion%20of%20fuzzy%20logic%20into%20the%20architecture%20improves%20its%20adaptability%20and%0Arobustness.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20demonstrate%20the%0Asuperior%20performance%20and%20efficiency%20of%20the%20proposed%20model.%20The%20FSC-based%0Aarchitecture%20achieved%20classification%20accuracies%20of%2099.17%25%2C%2099.75%25%2C%20and%2099.89%25%0Aon%20three%20different%20datasets.%20The%20model%20employs%20100%20times%20fewer%20parameters%20than%0Alarge-scale%20transfer%20learning%20architectures%2C%20highlighting%20its%20computational%0Aefficiency%20and%20suitability%20for%20detecting%20brain%20tumors%20early.%20This%20research%0Aoffers%20lightweight%2C%20high-performance%20deep-learning%20models%20for%20medical%20imaging%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Brain%2520Tumor%2520Detection%2520in%2520MRI%253A%2520Fuzzy%2520Sigmoid%2520Convolution%2520in%2520Deep%250A%2520%2520Learning%26entry.906535625%3DMuhammad%2520Irfan%2520and%2520Anum%2520Nawaz%2520and%2520Riku%2520Klen%2520and%2520Abdulhamit%2520Subasi%2520and%2520Tomi%2520Westerlund%2520and%2520Wei%2520Chen%26entry.1292438233%3D%2520%2520Early%2520detection%2520and%2520accurate%2520diagnosis%2520are%2520essential%2520to%2520improving%2520patient%250Aoutcomes.%2520The%2520use%2520of%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520for%2520tumor%2520detection%250Ahas%2520shown%2520promise%252C%2520but%2520existing%2520models%2520often%2520suffer%2520from%2520overparameterization%252C%250Awhich%2520limits%2520their%2520performance%2520gains.%2520In%2520this%2520study%252C%2520fuzzy%2520sigmoid%2520convolution%250A%2528FSC%2529%2520is%2520introduced%2520along%2520with%2520two%2520additional%2520modules%253A%2520top-of-the-funnel%2520and%250Amiddle-of-the-funnel.%2520The%2520proposed%2520methodology%2520significantly%2520reduces%2520the%2520number%250Aof%2520trainable%2520parameters%2520without%2520compromising%2520classification%2520accuracy.%2520A%2520novel%250Aconvolutional%2520operator%2520is%2520central%2520to%2520this%2520approach%252C%2520effectively%2520dilating%2520the%250Areceptive%2520field%2520while%2520preserving%2520input%2520data%2520integrity.%2520This%2520enables%2520efficient%250Afeature%2520map%2520reduction%2520and%2520enhances%2520the%2520model%2527s%2520tumor%2520detection%2520capability.%2520In%250Athe%2520FSC-based%2520model%252C%2520fuzzy%2520sigmoid%2520activation%2520functions%2520are%2520incorporated%2520within%250Aconvolutional%2520layers%2520to%2520improve%2520feature%2520extraction%2520and%2520classification.%2520The%250Ainclusion%2520of%2520fuzzy%2520logic%2520into%2520the%2520architecture%2520improves%2520its%2520adaptability%2520and%250Arobustness.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%2520demonstrate%2520the%250Asuperior%2520performance%2520and%2520efficiency%2520of%2520the%2520proposed%2520model.%2520The%2520FSC-based%250Aarchitecture%2520achieved%2520classification%2520accuracies%2520of%252099.17%2525%252C%252099.75%2525%252C%2520and%252099.89%2525%250Aon%2520three%2520different%2520datasets.%2520The%2520model%2520employs%2520100%2520times%2520fewer%2520parameters%2520than%250Alarge-scale%2520transfer%2520learning%2520architectures%252C%2520highlighting%2520its%2520computational%250Aefficiency%2520and%2520suitability%2520for%2520detecting%2520brain%2520tumors%2520early.%2520This%2520research%250Aoffers%2520lightweight%252C%2520high-performance%2520deep-learning%2520models%2520for%2520medical%2520imaging%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Brain%20Tumor%20Detection%20in%20MRI%3A%20Fuzzy%20Sigmoid%20Convolution%20in%20Deep%0A%20%20Learning&entry.906535625=Muhammad%20Irfan%20and%20Anum%20Nawaz%20and%20Riku%20Klen%20and%20Abdulhamit%20Subasi%20and%20Tomi%20Westerlund%20and%20Wei%20Chen&entry.1292438233=%20%20Early%20detection%20and%20accurate%20diagnosis%20are%20essential%20to%20improving%20patient%0Aoutcomes.%20The%20use%20of%20convolutional%20neural%20networks%20%28CNNs%29%20for%20tumor%20detection%0Ahas%20shown%20promise%2C%20but%20existing%20models%20often%20suffer%20from%20overparameterization%2C%0Awhich%20limits%20their%20performance%20gains.%20In%20this%20study%2C%20fuzzy%20sigmoid%20convolution%0A%28FSC%29%20is%20introduced%20along%20with%20two%20additional%20modules%3A%20top-of-the-funnel%20and%0Amiddle-of-the-funnel.%20The%20proposed%20methodology%20significantly%20reduces%20the%20number%0Aof%20trainable%20parameters%20without%20compromising%20classification%20accuracy.%20A%20novel%0Aconvolutional%20operator%20is%20central%20to%20this%20approach%2C%20effectively%20dilating%20the%0Areceptive%20field%20while%20preserving%20input%20data%20integrity.%20This%20enables%20efficient%0Afeature%20map%20reduction%20and%20enhances%20the%20model%27s%20tumor%20detection%20capability.%20In%0Athe%20FSC-based%20model%2C%20fuzzy%20sigmoid%20activation%20functions%20are%20incorporated%20within%0Aconvolutional%20layers%20to%20improve%20feature%20extraction%20and%20classification.%20The%0Ainclusion%20of%20fuzzy%20logic%20into%20the%20architecture%20improves%20its%20adaptability%20and%0Arobustness.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20demonstrate%20the%0Asuperior%20performance%20and%20efficiency%20of%20the%20proposed%20model.%20The%20FSC-based%0Aarchitecture%20achieved%20classification%20accuracies%20of%2099.17%25%2C%2099.75%25%2C%20and%2099.89%25%0Aon%20three%20different%20datasets.%20The%20model%20employs%20100%20times%20fewer%20parameters%20than%0Alarge-scale%20transfer%20learning%20architectures%2C%20highlighting%20its%20computational%0Aefficiency%20and%20suitability%20for%20detecting%20brain%20tumors%20early.%20This%20research%0Aoffers%20lightweight%2C%20high-performance%20deep-learning%20models%20for%20medical%20imaging%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05208v1&entry.124074799=Read"},
{"title": "Approximate Lifted Model Construction", "author": "Malte Luttermann and Jan Speller and Marcel Gehrke and Tanya Braun and Ralf M\u00f6ller and Mattis Hartwig", "abstract": "  Probabilistic relational models such as parametric factor graphs enable\nefficient (lifted) inference by exploiting the indistinguishability of objects.\nIn lifted inference, a representative of indistinguishable objects is used for\ncomputations. To obtain a relational (i.e., lifted) representation, the\nAdvanced Colour Passing (ACP) algorithm is the state of the art. The ACP\nalgorithm, however, requires underlying distributions, encoded as\npotential-based factorisations, to exactly match to identify and exploit\nindistinguishabilities. Hence, ACP is unsuitable for practical applications\nwhere potentials learned from data inevitably deviate even if associated\nobjects are indistinguishable. To mitigate this problem, we introduce the\n$\\varepsilon$-Advanced Colour Passing ($\\varepsilon$-ACP) algorithm, which\nallows for a deviation of potentials depending on a hyperparameter\n$\\varepsilon$. $\\varepsilon$-ACP efficiently uncovers and exploits\nindistinguishabilities that are not exact. We prove that the approximation\nerror induced by $\\varepsilon$-ACP is strictly bounded and our experiments show\nthat the approximation error is close to zero in practice.\n", "link": "http://arxiv.org/abs/2504.20784v2", "date": "2025-05-08", "relevancy": 1.9187, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.506}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4794}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approximate%20Lifted%20Model%20Construction&body=Title%3A%20Approximate%20Lifted%20Model%20Construction%0AAuthor%3A%20Malte%20Luttermann%20and%20Jan%20Speller%20and%20Marcel%20Gehrke%20and%20Tanya%20Braun%20and%20Ralf%20M%C3%B6ller%20and%20Mattis%20Hartwig%0AAbstract%3A%20%20%20Probabilistic%20relational%20models%20such%20as%20parametric%20factor%20graphs%20enable%0Aefficient%20%28lifted%29%20inference%20by%20exploiting%20the%20indistinguishability%20of%20objects.%0AIn%20lifted%20inference%2C%20a%20representative%20of%20indistinguishable%20objects%20is%20used%20for%0Acomputations.%20To%20obtain%20a%20relational%20%28i.e.%2C%20lifted%29%20representation%2C%20the%0AAdvanced%20Colour%20Passing%20%28ACP%29%20algorithm%20is%20the%20state%20of%20the%20art.%20The%20ACP%0Aalgorithm%2C%20however%2C%20requires%20underlying%20distributions%2C%20encoded%20as%0Apotential-based%20factorisations%2C%20to%20exactly%20match%20to%20identify%20and%20exploit%0Aindistinguishabilities.%20Hence%2C%20ACP%20is%20unsuitable%20for%20practical%20applications%0Awhere%20potentials%20learned%20from%20data%20inevitably%20deviate%20even%20if%20associated%0Aobjects%20are%20indistinguishable.%20To%20mitigate%20this%20problem%2C%20we%20introduce%20the%0A%24%5Cvarepsilon%24-Advanced%20Colour%20Passing%20%28%24%5Cvarepsilon%24-ACP%29%20algorithm%2C%20which%0Aallows%20for%20a%20deviation%20of%20potentials%20depending%20on%20a%20hyperparameter%0A%24%5Cvarepsilon%24.%20%24%5Cvarepsilon%24-ACP%20efficiently%20uncovers%20and%20exploits%0Aindistinguishabilities%20that%20are%20not%20exact.%20We%20prove%20that%20the%20approximation%0Aerror%20induced%20by%20%24%5Cvarepsilon%24-ACP%20is%20strictly%20bounded%20and%20our%20experiments%20show%0Athat%20the%20approximation%20error%20is%20close%20to%20zero%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproximate%2520Lifted%2520Model%2520Construction%26entry.906535625%3DMalte%2520Luttermann%2520and%2520Jan%2520Speller%2520and%2520Marcel%2520Gehrke%2520and%2520Tanya%2520Braun%2520and%2520Ralf%2520M%25C3%25B6ller%2520and%2520Mattis%2520Hartwig%26entry.1292438233%3D%2520%2520Probabilistic%2520relational%2520models%2520such%2520as%2520parametric%2520factor%2520graphs%2520enable%250Aefficient%2520%2528lifted%2529%2520inference%2520by%2520exploiting%2520the%2520indistinguishability%2520of%2520objects.%250AIn%2520lifted%2520inference%252C%2520a%2520representative%2520of%2520indistinguishable%2520objects%2520is%2520used%2520for%250Acomputations.%2520To%2520obtain%2520a%2520relational%2520%2528i.e.%252C%2520lifted%2529%2520representation%252C%2520the%250AAdvanced%2520Colour%2520Passing%2520%2528ACP%2529%2520algorithm%2520is%2520the%2520state%2520of%2520the%2520art.%2520The%2520ACP%250Aalgorithm%252C%2520however%252C%2520requires%2520underlying%2520distributions%252C%2520encoded%2520as%250Apotential-based%2520factorisations%252C%2520to%2520exactly%2520match%2520to%2520identify%2520and%2520exploit%250Aindistinguishabilities.%2520Hence%252C%2520ACP%2520is%2520unsuitable%2520for%2520practical%2520applications%250Awhere%2520potentials%2520learned%2520from%2520data%2520inevitably%2520deviate%2520even%2520if%2520associated%250Aobjects%2520are%2520indistinguishable.%2520To%2520mitigate%2520this%2520problem%252C%2520we%2520introduce%2520the%250A%2524%255Cvarepsilon%2524-Advanced%2520Colour%2520Passing%2520%2528%2524%255Cvarepsilon%2524-ACP%2529%2520algorithm%252C%2520which%250Aallows%2520for%2520a%2520deviation%2520of%2520potentials%2520depending%2520on%2520a%2520hyperparameter%250A%2524%255Cvarepsilon%2524.%2520%2524%255Cvarepsilon%2524-ACP%2520efficiently%2520uncovers%2520and%2520exploits%250Aindistinguishabilities%2520that%2520are%2520not%2520exact.%2520We%2520prove%2520that%2520the%2520approximation%250Aerror%2520induced%2520by%2520%2524%255Cvarepsilon%2524-ACP%2520is%2520strictly%2520bounded%2520and%2520our%2520experiments%2520show%250Athat%2520the%2520approximation%2520error%2520is%2520close%2520to%2520zero%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximate%20Lifted%20Model%20Construction&entry.906535625=Malte%20Luttermann%20and%20Jan%20Speller%20and%20Marcel%20Gehrke%20and%20Tanya%20Braun%20and%20Ralf%20M%C3%B6ller%20and%20Mattis%20Hartwig&entry.1292438233=%20%20Probabilistic%20relational%20models%20such%20as%20parametric%20factor%20graphs%20enable%0Aefficient%20%28lifted%29%20inference%20by%20exploiting%20the%20indistinguishability%20of%20objects.%0AIn%20lifted%20inference%2C%20a%20representative%20of%20indistinguishable%20objects%20is%20used%20for%0Acomputations.%20To%20obtain%20a%20relational%20%28i.e.%2C%20lifted%29%20representation%2C%20the%0AAdvanced%20Colour%20Passing%20%28ACP%29%20algorithm%20is%20the%20state%20of%20the%20art.%20The%20ACP%0Aalgorithm%2C%20however%2C%20requires%20underlying%20distributions%2C%20encoded%20as%0Apotential-based%20factorisations%2C%20to%20exactly%20match%20to%20identify%20and%20exploit%0Aindistinguishabilities.%20Hence%2C%20ACP%20is%20unsuitable%20for%20practical%20applications%0Awhere%20potentials%20learned%20from%20data%20inevitably%20deviate%20even%20if%20associated%0Aobjects%20are%20indistinguishable.%20To%20mitigate%20this%20problem%2C%20we%20introduce%20the%0A%24%5Cvarepsilon%24-Advanced%20Colour%20Passing%20%28%24%5Cvarepsilon%24-ACP%29%20algorithm%2C%20which%0Aallows%20for%20a%20deviation%20of%20potentials%20depending%20on%20a%20hyperparameter%0A%24%5Cvarepsilon%24.%20%24%5Cvarepsilon%24-ACP%20efficiently%20uncovers%20and%20exploits%0Aindistinguishabilities%20that%20are%20not%20exact.%20We%20prove%20that%20the%20approximation%0Aerror%20induced%20by%20%24%5Cvarepsilon%24-ACP%20is%20strictly%20bounded%20and%20our%20experiments%20show%0Athat%20the%20approximation%20error%20is%20close%20to%20zero%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20784v2&entry.124074799=Read"},
{"title": "Advancing Neural Network Verification through Hierarchical Safety\n  Abstract Interpretation", "author": "Luca Marzari and Isabella Mastroeni and Alessandro Farinelli", "abstract": "  Traditional methods for formal verification (FV) of deep neural networks\n(DNNs) are constrained by a binary encoding of safety properties, where a model\nis classified as either safe or unsafe (robust or not robust). This binary\nencoding fails to capture the nuanced safety levels within a model, often\nresulting in either overly restrictive or too permissive requirements. In this\npaper, we introduce a novel problem formulation called Abstract\nDNN-Verification, which verifies a hierarchical structure of unsafe outputs,\nproviding a more granular analysis of the safety aspect for a given DNN.\nCrucially, by leveraging abstract interpretation and reasoning about output\nreachable sets, our approach enables assessing multiple safety levels during\nthe FV process, requiring the same (in the worst case) or even potentially less\ncomputational effort than the traditional binary verification approach.\nSpecifically, we demonstrate how this formulation allows rank adversarial\ninputs according to their abstract safety level violation, offering a more\ndetailed evaluation of the model's safety and robustness. Our contributions\ninclude a theoretical exploration of the relationship between our novel\nabstract safety formulation and existing approaches that employ abstract\ninterpretation for robustness verification, complexity analysis of the novel\nproblem introduced, and an empirical evaluation considering both a complex deep\nreinforcement learning task (based on Habitat 3.0) and standard\nDNN-Verification benchmarks.\n", "link": "http://arxiv.org/abs/2505.05235v1", "date": "2025-05-08", "relevancy": 1.9153, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4732}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Neural%20Network%20Verification%20through%20Hierarchical%20Safety%0A%20%20Abstract%20Interpretation&body=Title%3A%20Advancing%20Neural%20Network%20Verification%20through%20Hierarchical%20Safety%0A%20%20Abstract%20Interpretation%0AAuthor%3A%20Luca%20Marzari%20and%20Isabella%20Mastroeni%20and%20Alessandro%20Farinelli%0AAbstract%3A%20%20%20Traditional%20methods%20for%20formal%20verification%20%28FV%29%20of%20deep%20neural%20networks%0A%28DNNs%29%20are%20constrained%20by%20a%20binary%20encoding%20of%20safety%20properties%2C%20where%20a%20model%0Ais%20classified%20as%20either%20safe%20or%20unsafe%20%28robust%20or%20not%20robust%29.%20This%20binary%0Aencoding%20fails%20to%20capture%20the%20nuanced%20safety%20levels%20within%20a%20model%2C%20often%0Aresulting%20in%20either%20overly%20restrictive%20or%20too%20permissive%20requirements.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20problem%20formulation%20called%20Abstract%0ADNN-Verification%2C%20which%20verifies%20a%20hierarchical%20structure%20of%20unsafe%20outputs%2C%0Aproviding%20a%20more%20granular%20analysis%20of%20the%20safety%20aspect%20for%20a%20given%20DNN.%0ACrucially%2C%20by%20leveraging%20abstract%20interpretation%20and%20reasoning%20about%20output%0Areachable%20sets%2C%20our%20approach%20enables%20assessing%20multiple%20safety%20levels%20during%0Athe%20FV%20process%2C%20requiring%20the%20same%20%28in%20the%20worst%20case%29%20or%20even%20potentially%20less%0Acomputational%20effort%20than%20the%20traditional%20binary%20verification%20approach.%0ASpecifically%2C%20we%20demonstrate%20how%20this%20formulation%20allows%20rank%20adversarial%0Ainputs%20according%20to%20their%20abstract%20safety%20level%20violation%2C%20offering%20a%20more%0Adetailed%20evaluation%20of%20the%20model%27s%20safety%20and%20robustness.%20Our%20contributions%0Ainclude%20a%20theoretical%20exploration%20of%20the%20relationship%20between%20our%20novel%0Aabstract%20safety%20formulation%20and%20existing%20approaches%20that%20employ%20abstract%0Ainterpretation%20for%20robustness%20verification%2C%20complexity%20analysis%20of%20the%20novel%0Aproblem%20introduced%2C%20and%20an%20empirical%20evaluation%20considering%20both%20a%20complex%20deep%0Areinforcement%20learning%20task%20%28based%20on%20Habitat%203.0%29%20and%20standard%0ADNN-Verification%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Neural%2520Network%2520Verification%2520through%2520Hierarchical%2520Safety%250A%2520%2520Abstract%2520Interpretation%26entry.906535625%3DLuca%2520Marzari%2520and%2520Isabella%2520Mastroeni%2520and%2520Alessandro%2520Farinelli%26entry.1292438233%3D%2520%2520Traditional%2520methods%2520for%2520formal%2520verification%2520%2528FV%2529%2520of%2520deep%2520neural%2520networks%250A%2528DNNs%2529%2520are%2520constrained%2520by%2520a%2520binary%2520encoding%2520of%2520safety%2520properties%252C%2520where%2520a%2520model%250Ais%2520classified%2520as%2520either%2520safe%2520or%2520unsafe%2520%2528robust%2520or%2520not%2520robust%2529.%2520This%2520binary%250Aencoding%2520fails%2520to%2520capture%2520the%2520nuanced%2520safety%2520levels%2520within%2520a%2520model%252C%2520often%250Aresulting%2520in%2520either%2520overly%2520restrictive%2520or%2520too%2520permissive%2520requirements.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520problem%2520formulation%2520called%2520Abstract%250ADNN-Verification%252C%2520which%2520verifies%2520a%2520hierarchical%2520structure%2520of%2520unsafe%2520outputs%252C%250Aproviding%2520a%2520more%2520granular%2520analysis%2520of%2520the%2520safety%2520aspect%2520for%2520a%2520given%2520DNN.%250ACrucially%252C%2520by%2520leveraging%2520abstract%2520interpretation%2520and%2520reasoning%2520about%2520output%250Areachable%2520sets%252C%2520our%2520approach%2520enables%2520assessing%2520multiple%2520safety%2520levels%2520during%250Athe%2520FV%2520process%252C%2520requiring%2520the%2520same%2520%2528in%2520the%2520worst%2520case%2529%2520or%2520even%2520potentially%2520less%250Acomputational%2520effort%2520than%2520the%2520traditional%2520binary%2520verification%2520approach.%250ASpecifically%252C%2520we%2520demonstrate%2520how%2520this%2520formulation%2520allows%2520rank%2520adversarial%250Ainputs%2520according%2520to%2520their%2520abstract%2520safety%2520level%2520violation%252C%2520offering%2520a%2520more%250Adetailed%2520evaluation%2520of%2520the%2520model%2527s%2520safety%2520and%2520robustness.%2520Our%2520contributions%250Ainclude%2520a%2520theoretical%2520exploration%2520of%2520the%2520relationship%2520between%2520our%2520novel%250Aabstract%2520safety%2520formulation%2520and%2520existing%2520approaches%2520that%2520employ%2520abstract%250Ainterpretation%2520for%2520robustness%2520verification%252C%2520complexity%2520analysis%2520of%2520the%2520novel%250Aproblem%2520introduced%252C%2520and%2520an%2520empirical%2520evaluation%2520considering%2520both%2520a%2520complex%2520deep%250Areinforcement%2520learning%2520task%2520%2528based%2520on%2520Habitat%25203.0%2529%2520and%2520standard%250ADNN-Verification%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Neural%20Network%20Verification%20through%20Hierarchical%20Safety%0A%20%20Abstract%20Interpretation&entry.906535625=Luca%20Marzari%20and%20Isabella%20Mastroeni%20and%20Alessandro%20Farinelli&entry.1292438233=%20%20Traditional%20methods%20for%20formal%20verification%20%28FV%29%20of%20deep%20neural%20networks%0A%28DNNs%29%20are%20constrained%20by%20a%20binary%20encoding%20of%20safety%20properties%2C%20where%20a%20model%0Ais%20classified%20as%20either%20safe%20or%20unsafe%20%28robust%20or%20not%20robust%29.%20This%20binary%0Aencoding%20fails%20to%20capture%20the%20nuanced%20safety%20levels%20within%20a%20model%2C%20often%0Aresulting%20in%20either%20overly%20restrictive%20or%20too%20permissive%20requirements.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20problem%20formulation%20called%20Abstract%0ADNN-Verification%2C%20which%20verifies%20a%20hierarchical%20structure%20of%20unsafe%20outputs%2C%0Aproviding%20a%20more%20granular%20analysis%20of%20the%20safety%20aspect%20for%20a%20given%20DNN.%0ACrucially%2C%20by%20leveraging%20abstract%20interpretation%20and%20reasoning%20about%20output%0Areachable%20sets%2C%20our%20approach%20enables%20assessing%20multiple%20safety%20levels%20during%0Athe%20FV%20process%2C%20requiring%20the%20same%20%28in%20the%20worst%20case%29%20or%20even%20potentially%20less%0Acomputational%20effort%20than%20the%20traditional%20binary%20verification%20approach.%0ASpecifically%2C%20we%20demonstrate%20how%20this%20formulation%20allows%20rank%20adversarial%0Ainputs%20according%20to%20their%20abstract%20safety%20level%20violation%2C%20offering%20a%20more%0Adetailed%20evaluation%20of%20the%20model%27s%20safety%20and%20robustness.%20Our%20contributions%0Ainclude%20a%20theoretical%20exploration%20of%20the%20relationship%20between%20our%20novel%0Aabstract%20safety%20formulation%20and%20existing%20approaches%20that%20employ%20abstract%0Ainterpretation%20for%20robustness%20verification%2C%20complexity%20analysis%20of%20the%20novel%0Aproblem%20introduced%2C%20and%20an%20empirical%20evaluation%20considering%20both%20a%20complex%20deep%0Areinforcement%20learning%20task%20%28based%20on%20Habitat%203.0%29%20and%20standard%0ADNN-Verification%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05235v1&entry.124074799=Read"},
{"title": "Enhancing Text2Cypher with Schema Filtering", "author": "Makbule Gulcin Ozsoy", "abstract": "  Knowledge graphs represent complex data using nodes, relationships, and\nproperties. Cypher, a powerful query language for graph databases, enables\nefficient modeling and querying. Recent advancements in large language models\nallow translation of natural language questions into Cypher queries -\nText2Cypher. A common approach is incorporating database schema into prompts.\nHowever, complex schemas can introduce noise, increase hallucinations, and\nraise computational costs. Schema filtering addresses these challenges by\nincluding only relevant schema elements, improving query generation while\nreducing token costs. This work explores various schema filtering methods for\nText2Cypher task and analyzes their impact on token length, performance, and\ncost. Results show that schema filtering effectively optimizes Text2Cypher,\nespecially for smaller models. Consistent with prior research, we find that\nlarger models benefit less from schema filtering due to their longer context\ncapabilities. However, schema filtering remains valuable for both larger and\nsmaller models in cost reduction.\n", "link": "http://arxiv.org/abs/2505.05118v1", "date": "2025-05-08", "relevancy": 1.9123, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Text2Cypher%20with%20Schema%20Filtering&body=Title%3A%20Enhancing%20Text2Cypher%20with%20Schema%20Filtering%0AAuthor%3A%20Makbule%20Gulcin%20Ozsoy%0AAbstract%3A%20%20%20Knowledge%20graphs%20represent%20complex%20data%20using%20nodes%2C%20relationships%2C%20and%0Aproperties.%20Cypher%2C%20a%20powerful%20query%20language%20for%20graph%20databases%2C%20enables%0Aefficient%20modeling%20and%20querying.%20Recent%20advancements%20in%20large%20language%20models%0Aallow%20translation%20of%20natural%20language%20questions%20into%20Cypher%20queries%20-%0AText2Cypher.%20A%20common%20approach%20is%20incorporating%20database%20schema%20into%20prompts.%0AHowever%2C%20complex%20schemas%20can%20introduce%20noise%2C%20increase%20hallucinations%2C%20and%0Araise%20computational%20costs.%20Schema%20filtering%20addresses%20these%20challenges%20by%0Aincluding%20only%20relevant%20schema%20elements%2C%20improving%20query%20generation%20while%0Areducing%20token%20costs.%20This%20work%20explores%20various%20schema%20filtering%20methods%20for%0AText2Cypher%20task%20and%20analyzes%20their%20impact%20on%20token%20length%2C%20performance%2C%20and%0Acost.%20Results%20show%20that%20schema%20filtering%20effectively%20optimizes%20Text2Cypher%2C%0Aespecially%20for%20smaller%20models.%20Consistent%20with%20prior%20research%2C%20we%20find%20that%0Alarger%20models%20benefit%20less%20from%20schema%20filtering%20due%20to%20their%20longer%20context%0Acapabilities.%20However%2C%20schema%20filtering%20remains%20valuable%20for%20both%20larger%20and%0Asmaller%20models%20in%20cost%20reduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Text2Cypher%2520with%2520Schema%2520Filtering%26entry.906535625%3DMakbule%2520Gulcin%2520Ozsoy%26entry.1292438233%3D%2520%2520Knowledge%2520graphs%2520represent%2520complex%2520data%2520using%2520nodes%252C%2520relationships%252C%2520and%250Aproperties.%2520Cypher%252C%2520a%2520powerful%2520query%2520language%2520for%2520graph%2520databases%252C%2520enables%250Aefficient%2520modeling%2520and%2520querying.%2520Recent%2520advancements%2520in%2520large%2520language%2520models%250Aallow%2520translation%2520of%2520natural%2520language%2520questions%2520into%2520Cypher%2520queries%2520-%250AText2Cypher.%2520A%2520common%2520approach%2520is%2520incorporating%2520database%2520schema%2520into%2520prompts.%250AHowever%252C%2520complex%2520schemas%2520can%2520introduce%2520noise%252C%2520increase%2520hallucinations%252C%2520and%250Araise%2520computational%2520costs.%2520Schema%2520filtering%2520addresses%2520these%2520challenges%2520by%250Aincluding%2520only%2520relevant%2520schema%2520elements%252C%2520improving%2520query%2520generation%2520while%250Areducing%2520token%2520costs.%2520This%2520work%2520explores%2520various%2520schema%2520filtering%2520methods%2520for%250AText2Cypher%2520task%2520and%2520analyzes%2520their%2520impact%2520on%2520token%2520length%252C%2520performance%252C%2520and%250Acost.%2520Results%2520show%2520that%2520schema%2520filtering%2520effectively%2520optimizes%2520Text2Cypher%252C%250Aespecially%2520for%2520smaller%2520models.%2520Consistent%2520with%2520prior%2520research%252C%2520we%2520find%2520that%250Alarger%2520models%2520benefit%2520less%2520from%2520schema%2520filtering%2520due%2520to%2520their%2520longer%2520context%250Acapabilities.%2520However%252C%2520schema%2520filtering%2520remains%2520valuable%2520for%2520both%2520larger%2520and%250Asmaller%2520models%2520in%2520cost%2520reduction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Text2Cypher%20with%20Schema%20Filtering&entry.906535625=Makbule%20Gulcin%20Ozsoy&entry.1292438233=%20%20Knowledge%20graphs%20represent%20complex%20data%20using%20nodes%2C%20relationships%2C%20and%0Aproperties.%20Cypher%2C%20a%20powerful%20query%20language%20for%20graph%20databases%2C%20enables%0Aefficient%20modeling%20and%20querying.%20Recent%20advancements%20in%20large%20language%20models%0Aallow%20translation%20of%20natural%20language%20questions%20into%20Cypher%20queries%20-%0AText2Cypher.%20A%20common%20approach%20is%20incorporating%20database%20schema%20into%20prompts.%0AHowever%2C%20complex%20schemas%20can%20introduce%20noise%2C%20increase%20hallucinations%2C%20and%0Araise%20computational%20costs.%20Schema%20filtering%20addresses%20these%20challenges%20by%0Aincluding%20only%20relevant%20schema%20elements%2C%20improving%20query%20generation%20while%0Areducing%20token%20costs.%20This%20work%20explores%20various%20schema%20filtering%20methods%20for%0AText2Cypher%20task%20and%20analyzes%20their%20impact%20on%20token%20length%2C%20performance%2C%20and%0Acost.%20Results%20show%20that%20schema%20filtering%20effectively%20optimizes%20Text2Cypher%2C%0Aespecially%20for%20smaller%20models.%20Consistent%20with%20prior%20research%2C%20we%20find%20that%0Alarger%20models%20benefit%20less%20from%20schema%20filtering%20due%20to%20their%20longer%20context%0Acapabilities.%20However%2C%20schema%20filtering%20remains%20valuable%20for%20both%20larger%20and%0Asmaller%20models%20in%20cost%20reduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05118v1&entry.124074799=Read"},
{"title": "Diffusion Model Quantization: A Review", "author": "Qian Zeng and Chenggong Hu and Mingli Song and Jie Song", "abstract": "  Recent success of large text-to-image models has empirically underscored the\nexceptional performance of diffusion models in generative tasks. To facilitate\ntheir efficient deployment on resource-constrained edge devices, model\nquantization has emerged as a pivotal technique for both compression and\nacceleration. This survey offers a thorough review of the latest advancements\nin diffusion model quantization, encapsulating and analyzing the current state\nof the art in this rapidly advancing domain. First, we provide an overview of\nthe key challenges encountered in the quantization of diffusion models,\nincluding those based on U-Net architectures and Diffusion Transformers (DiT).\nWe then present a comprehensive taxonomy of prevalent quantization techniques,\nengaging in an in-depth discussion of their underlying principles.\nSubsequently, we perform a meticulous analysis of representative diffusion\nmodel quantization schemes from both qualitative and quantitative perspectives.\nFrom a quantitative standpoint, we rigorously benchmark a variety of methods\nusing widely recognized datasets, delivering an extensive evaluation of the\nmost recent and impactful research in the field. From a qualitative standpoint,\nwe categorize and synthesize the effects of quantization errors, elucidating\nthese impacts through both visual analysis and trajectory examination. In\nconclusion, we outline prospective avenues for future research, proposing novel\ndirections for the quantization of generative models in practical applications.\nThe list of related papers, corresponding codes, pre-trained models and\ncomparison results are publicly available at the survey project homepage\nhttps://github.com/TaylorJocelyn/Diffusion-Model-Quantization.\n", "link": "http://arxiv.org/abs/2505.05215v1", "date": "2025-05-08", "relevancy": 1.8982, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6678}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6462}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Model%20Quantization%3A%20A%20Review&body=Title%3A%20Diffusion%20Model%20Quantization%3A%20A%20Review%0AAuthor%3A%20Qian%20Zeng%20and%20Chenggong%20Hu%20and%20Mingli%20Song%20and%20Jie%20Song%0AAbstract%3A%20%20%20Recent%20success%20of%20large%20text-to-image%20models%20has%20empirically%20underscored%20the%0Aexceptional%20performance%20of%20diffusion%20models%20in%20generative%20tasks.%20To%20facilitate%0Atheir%20efficient%20deployment%20on%20resource-constrained%20edge%20devices%2C%20model%0Aquantization%20has%20emerged%20as%20a%20pivotal%20technique%20for%20both%20compression%20and%0Aacceleration.%20This%20survey%20offers%20a%20thorough%20review%20of%20the%20latest%20advancements%0Ain%20diffusion%20model%20quantization%2C%20encapsulating%20and%20analyzing%20the%20current%20state%0Aof%20the%20art%20in%20this%20rapidly%20advancing%20domain.%20First%2C%20we%20provide%20an%20overview%20of%0Athe%20key%20challenges%20encountered%20in%20the%20quantization%20of%20diffusion%20models%2C%0Aincluding%20those%20based%20on%20U-Net%20architectures%20and%20Diffusion%20Transformers%20%28DiT%29.%0AWe%20then%20present%20a%20comprehensive%20taxonomy%20of%20prevalent%20quantization%20techniques%2C%0Aengaging%20in%20an%20in-depth%20discussion%20of%20their%20underlying%20principles.%0ASubsequently%2C%20we%20perform%20a%20meticulous%20analysis%20of%20representative%20diffusion%0Amodel%20quantization%20schemes%20from%20both%20qualitative%20and%20quantitative%20perspectives.%0AFrom%20a%20quantitative%20standpoint%2C%20we%20rigorously%20benchmark%20a%20variety%20of%20methods%0Ausing%20widely%20recognized%20datasets%2C%20delivering%20an%20extensive%20evaluation%20of%20the%0Amost%20recent%20and%20impactful%20research%20in%20the%20field.%20From%20a%20qualitative%20standpoint%2C%0Awe%20categorize%20and%20synthesize%20the%20effects%20of%20quantization%20errors%2C%20elucidating%0Athese%20impacts%20through%20both%20visual%20analysis%20and%20trajectory%20examination.%20In%0Aconclusion%2C%20we%20outline%20prospective%20avenues%20for%20future%20research%2C%20proposing%20novel%0Adirections%20for%20the%20quantization%20of%20generative%20models%20in%20practical%20applications.%0AThe%20list%20of%20related%20papers%2C%20corresponding%20codes%2C%20pre-trained%20models%20and%0Acomparison%20results%20are%20publicly%20available%20at%20the%20survey%20project%20homepage%0Ahttps%3A//github.com/TaylorJocelyn/Diffusion-Model-Quantization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Model%2520Quantization%253A%2520A%2520Review%26entry.906535625%3DQian%2520Zeng%2520and%2520Chenggong%2520Hu%2520and%2520Mingli%2520Song%2520and%2520Jie%2520Song%26entry.1292438233%3D%2520%2520Recent%2520success%2520of%2520large%2520text-to-image%2520models%2520has%2520empirically%2520underscored%2520the%250Aexceptional%2520performance%2520of%2520diffusion%2520models%2520in%2520generative%2520tasks.%2520To%2520facilitate%250Atheir%2520efficient%2520deployment%2520on%2520resource-constrained%2520edge%2520devices%252C%2520model%250Aquantization%2520has%2520emerged%2520as%2520a%2520pivotal%2520technique%2520for%2520both%2520compression%2520and%250Aacceleration.%2520This%2520survey%2520offers%2520a%2520thorough%2520review%2520of%2520the%2520latest%2520advancements%250Ain%2520diffusion%2520model%2520quantization%252C%2520encapsulating%2520and%2520analyzing%2520the%2520current%2520state%250Aof%2520the%2520art%2520in%2520this%2520rapidly%2520advancing%2520domain.%2520First%252C%2520we%2520provide%2520an%2520overview%2520of%250Athe%2520key%2520challenges%2520encountered%2520in%2520the%2520quantization%2520of%2520diffusion%2520models%252C%250Aincluding%2520those%2520based%2520on%2520U-Net%2520architectures%2520and%2520Diffusion%2520Transformers%2520%2528DiT%2529.%250AWe%2520then%2520present%2520a%2520comprehensive%2520taxonomy%2520of%2520prevalent%2520quantization%2520techniques%252C%250Aengaging%2520in%2520an%2520in-depth%2520discussion%2520of%2520their%2520underlying%2520principles.%250ASubsequently%252C%2520we%2520perform%2520a%2520meticulous%2520analysis%2520of%2520representative%2520diffusion%250Amodel%2520quantization%2520schemes%2520from%2520both%2520qualitative%2520and%2520quantitative%2520perspectives.%250AFrom%2520a%2520quantitative%2520standpoint%252C%2520we%2520rigorously%2520benchmark%2520a%2520variety%2520of%2520methods%250Ausing%2520widely%2520recognized%2520datasets%252C%2520delivering%2520an%2520extensive%2520evaluation%2520of%2520the%250Amost%2520recent%2520and%2520impactful%2520research%2520in%2520the%2520field.%2520From%2520a%2520qualitative%2520standpoint%252C%250Awe%2520categorize%2520and%2520synthesize%2520the%2520effects%2520of%2520quantization%2520errors%252C%2520elucidating%250Athese%2520impacts%2520through%2520both%2520visual%2520analysis%2520and%2520trajectory%2520examination.%2520In%250Aconclusion%252C%2520we%2520outline%2520prospective%2520avenues%2520for%2520future%2520research%252C%2520proposing%2520novel%250Adirections%2520for%2520the%2520quantization%2520of%2520generative%2520models%2520in%2520practical%2520applications.%250AThe%2520list%2520of%2520related%2520papers%252C%2520corresponding%2520codes%252C%2520pre-trained%2520models%2520and%250Acomparison%2520results%2520are%2520publicly%2520available%2520at%2520the%2520survey%2520project%2520homepage%250Ahttps%253A//github.com/TaylorJocelyn/Diffusion-Model-Quantization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Model%20Quantization%3A%20A%20Review&entry.906535625=Qian%20Zeng%20and%20Chenggong%20Hu%20and%20Mingli%20Song%20and%20Jie%20Song&entry.1292438233=%20%20Recent%20success%20of%20large%20text-to-image%20models%20has%20empirically%20underscored%20the%0Aexceptional%20performance%20of%20diffusion%20models%20in%20generative%20tasks.%20To%20facilitate%0Atheir%20efficient%20deployment%20on%20resource-constrained%20edge%20devices%2C%20model%0Aquantization%20has%20emerged%20as%20a%20pivotal%20technique%20for%20both%20compression%20and%0Aacceleration.%20This%20survey%20offers%20a%20thorough%20review%20of%20the%20latest%20advancements%0Ain%20diffusion%20model%20quantization%2C%20encapsulating%20and%20analyzing%20the%20current%20state%0Aof%20the%20art%20in%20this%20rapidly%20advancing%20domain.%20First%2C%20we%20provide%20an%20overview%20of%0Athe%20key%20challenges%20encountered%20in%20the%20quantization%20of%20diffusion%20models%2C%0Aincluding%20those%20based%20on%20U-Net%20architectures%20and%20Diffusion%20Transformers%20%28DiT%29.%0AWe%20then%20present%20a%20comprehensive%20taxonomy%20of%20prevalent%20quantization%20techniques%2C%0Aengaging%20in%20an%20in-depth%20discussion%20of%20their%20underlying%20principles.%0ASubsequently%2C%20we%20perform%20a%20meticulous%20analysis%20of%20representative%20diffusion%0Amodel%20quantization%20schemes%20from%20both%20qualitative%20and%20quantitative%20perspectives.%0AFrom%20a%20quantitative%20standpoint%2C%20we%20rigorously%20benchmark%20a%20variety%20of%20methods%0Ausing%20widely%20recognized%20datasets%2C%20delivering%20an%20extensive%20evaluation%20of%20the%0Amost%20recent%20and%20impactful%20research%20in%20the%20field.%20From%20a%20qualitative%20standpoint%2C%0Awe%20categorize%20and%20synthesize%20the%20effects%20of%20quantization%20errors%2C%20elucidating%0Athese%20impacts%20through%20both%20visual%20analysis%20and%20trajectory%20examination.%20In%0Aconclusion%2C%20we%20outline%20prospective%20avenues%20for%20future%20research%2C%20proposing%20novel%0Adirections%20for%20the%20quantization%20of%20generative%20models%20in%20practical%20applications.%0AThe%20list%20of%20related%20papers%2C%20corresponding%20codes%2C%20pre-trained%20models%20and%0Acomparison%20results%20are%20publicly%20available%20at%20the%20survey%20project%20homepage%0Ahttps%3A//github.com/TaylorJocelyn/Diffusion-Model-Quantization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05215v1&entry.124074799=Read"},
{"title": "Automated detection of underdiagnosed medical conditions via\n  opportunistic imaging", "author": "Asad Aali and Andrew Johnston and Louis Blankemeier and Dave Van Veen and Laura T Derry and David Svec and Jason Hom and Robert D. Boutin and Akshay S. Chaudhari", "abstract": "  Abdominal computed tomography (CT) scans are frequently performed in clinical\nsettings. Opportunistic CT involves repurposing routine CT images to extract\ndiagnostic information and is an emerging tool for detecting underdiagnosed\nconditions such as sarcopenia, hepatic steatosis, and ascites. This study\nutilizes deep learning methods to promote accurate diagnosis and clinical\ndocumentation. We analyze 2,674 inpatient CT scans to identify discrepancies\nbetween imaging phenotypes (characteristics derived from opportunistic CT\nscans) and their corresponding documentation in radiology reports and ICD\ncoding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans\ndiagnosed with sarcopenia, hepatic steatosis, and ascites (respectively)\nthrough either opportunistic imaging or radiology reports were ICD-coded. Our\nfindings demonstrate opportunistic CT's potential to enhance diagnostic\nprecision and accuracy of risk adjustment models, offering advancements in\nprecision medicine.\n", "link": "http://arxiv.org/abs/2409.11686v3", "date": "2025-05-08", "relevancy": 1.8949, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4889}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4727}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20detection%20of%20underdiagnosed%20medical%20conditions%20via%0A%20%20opportunistic%20imaging&body=Title%3A%20Automated%20detection%20of%20underdiagnosed%20medical%20conditions%20via%0A%20%20opportunistic%20imaging%0AAuthor%3A%20Asad%20Aali%20and%20Andrew%20Johnston%20and%20Louis%20Blankemeier%20and%20Dave%20Van%20Veen%20and%20Laura%20T%20Derry%20and%20David%20Svec%20and%20Jason%20Hom%20and%20Robert%20D.%20Boutin%20and%20Akshay%20S.%20Chaudhari%0AAbstract%3A%20%20%20Abdominal%20computed%20tomography%20%28CT%29%20scans%20are%20frequently%20performed%20in%20clinical%0Asettings.%20Opportunistic%20CT%20involves%20repurposing%20routine%20CT%20images%20to%20extract%0Adiagnostic%20information%20and%20is%20an%20emerging%20tool%20for%20detecting%20underdiagnosed%0Aconditions%20such%20as%20sarcopenia%2C%20hepatic%20steatosis%2C%20and%20ascites.%20This%20study%0Autilizes%20deep%20learning%20methods%20to%20promote%20accurate%20diagnosis%20and%20clinical%0Adocumentation.%20We%20analyze%202%2C674%20inpatient%20CT%20scans%20to%20identify%20discrepancies%0Abetween%20imaging%20phenotypes%20%28characteristics%20derived%20from%20opportunistic%20CT%0Ascans%29%20and%20their%20corresponding%20documentation%20in%20radiology%20reports%20and%20ICD%0Acoding.%20Through%20our%20analysis%2C%20we%20find%20that%20only%200.5%25%2C%203.2%25%2C%20and%2030.7%25%20of%20scans%0Adiagnosed%20with%20sarcopenia%2C%20hepatic%20steatosis%2C%20and%20ascites%20%28respectively%29%0Athrough%20either%20opportunistic%20imaging%20or%20radiology%20reports%20were%20ICD-coded.%20Our%0Afindings%20demonstrate%20opportunistic%20CT%27s%20potential%20to%20enhance%20diagnostic%0Aprecision%20and%20accuracy%20of%20risk%20adjustment%20models%2C%20offering%20advancements%20in%0Aprecision%20medicine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11686v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520detection%2520of%2520underdiagnosed%2520medical%2520conditions%2520via%250A%2520%2520opportunistic%2520imaging%26entry.906535625%3DAsad%2520Aali%2520and%2520Andrew%2520Johnston%2520and%2520Louis%2520Blankemeier%2520and%2520Dave%2520Van%2520Veen%2520and%2520Laura%2520T%2520Derry%2520and%2520David%2520Svec%2520and%2520Jason%2520Hom%2520and%2520Robert%2520D.%2520Boutin%2520and%2520Akshay%2520S.%2520Chaudhari%26entry.1292438233%3D%2520%2520Abdominal%2520computed%2520tomography%2520%2528CT%2529%2520scans%2520are%2520frequently%2520performed%2520in%2520clinical%250Asettings.%2520Opportunistic%2520CT%2520involves%2520repurposing%2520routine%2520CT%2520images%2520to%2520extract%250Adiagnostic%2520information%2520and%2520is%2520an%2520emerging%2520tool%2520for%2520detecting%2520underdiagnosed%250Aconditions%2520such%2520as%2520sarcopenia%252C%2520hepatic%2520steatosis%252C%2520and%2520ascites.%2520This%2520study%250Autilizes%2520deep%2520learning%2520methods%2520to%2520promote%2520accurate%2520diagnosis%2520and%2520clinical%250Adocumentation.%2520We%2520analyze%25202%252C674%2520inpatient%2520CT%2520scans%2520to%2520identify%2520discrepancies%250Abetween%2520imaging%2520phenotypes%2520%2528characteristics%2520derived%2520from%2520opportunistic%2520CT%250Ascans%2529%2520and%2520their%2520corresponding%2520documentation%2520in%2520radiology%2520reports%2520and%2520ICD%250Acoding.%2520Through%2520our%2520analysis%252C%2520we%2520find%2520that%2520only%25200.5%2525%252C%25203.2%2525%252C%2520and%252030.7%2525%2520of%2520scans%250Adiagnosed%2520with%2520sarcopenia%252C%2520hepatic%2520steatosis%252C%2520and%2520ascites%2520%2528respectively%2529%250Athrough%2520either%2520opportunistic%2520imaging%2520or%2520radiology%2520reports%2520were%2520ICD-coded.%2520Our%250Afindings%2520demonstrate%2520opportunistic%2520CT%2527s%2520potential%2520to%2520enhance%2520diagnostic%250Aprecision%2520and%2520accuracy%2520of%2520risk%2520adjustment%2520models%252C%2520offering%2520advancements%2520in%250Aprecision%2520medicine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11686v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20detection%20of%20underdiagnosed%20medical%20conditions%20via%0A%20%20opportunistic%20imaging&entry.906535625=Asad%20Aali%20and%20Andrew%20Johnston%20and%20Louis%20Blankemeier%20and%20Dave%20Van%20Veen%20and%20Laura%20T%20Derry%20and%20David%20Svec%20and%20Jason%20Hom%20and%20Robert%20D.%20Boutin%20and%20Akshay%20S.%20Chaudhari&entry.1292438233=%20%20Abdominal%20computed%20tomography%20%28CT%29%20scans%20are%20frequently%20performed%20in%20clinical%0Asettings.%20Opportunistic%20CT%20involves%20repurposing%20routine%20CT%20images%20to%20extract%0Adiagnostic%20information%20and%20is%20an%20emerging%20tool%20for%20detecting%20underdiagnosed%0Aconditions%20such%20as%20sarcopenia%2C%20hepatic%20steatosis%2C%20and%20ascites.%20This%20study%0Autilizes%20deep%20learning%20methods%20to%20promote%20accurate%20diagnosis%20and%20clinical%0Adocumentation.%20We%20analyze%202%2C674%20inpatient%20CT%20scans%20to%20identify%20discrepancies%0Abetween%20imaging%20phenotypes%20%28characteristics%20derived%20from%20opportunistic%20CT%0Ascans%29%20and%20their%20corresponding%20documentation%20in%20radiology%20reports%20and%20ICD%0Acoding.%20Through%20our%20analysis%2C%20we%20find%20that%20only%200.5%25%2C%203.2%25%2C%20and%2030.7%25%20of%20scans%0Adiagnosed%20with%20sarcopenia%2C%20hepatic%20steatosis%2C%20and%20ascites%20%28respectively%29%0Athrough%20either%20opportunistic%20imaging%20or%20radiology%20reports%20were%20ICD-coded.%20Our%0Afindings%20demonstrate%20opportunistic%20CT%27s%20potential%20to%20enhance%20diagnostic%0Aprecision%20and%20accuracy%20of%20risk%20adjustment%20models%2C%20offering%20advancements%20in%0Aprecision%20medicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11686v3&entry.124074799=Read"},
{"title": "Enhancing Treatment Effect Estimation via Active Learning: A\n  Counterfactual Covering Perspective", "author": "Hechuan Wen and Tong Chen and Mingming Gong and Li Kheng Chai and Shazia Sadiq and Hongzhi Yin", "abstract": "  Although numerous complex algorithms for treatment effect estimation have\nbeen developed in recent years, their effectiveness remains limited when\nhandling insufficiently labeled training sets due to the high cost of labeling\nthe effect after treatment, e.g., expensive tumor imaging or biopsy procedures\nneeded to evaluate treatment effects. Therefore, it becomes essential to\nactively incorporate more high-quality labeled data, all while adhering to a\nconstrained labeling budget. To enable data-efficient treatment effect\nestimation, we formalize the problem through rigorous theoretical analysis\nwithin the active learning context, where the derived key measures --\n\\textit{factual} and \\textit{counterfactual covering radius} determine the risk\nupper bound. To reduce the bound, we propose a greedy radius reduction\nalgorithm, which excels under an idealized, balanced data distribution. To\ngeneralize to more realistic data distributions, we further propose FCCM, which\ntransforms the optimization objective into the \\textit{Factual} and\n\\textit{Counterfactual Coverage Maximization} to ensure effective radius\nreduction during data acquisition. Furthermore, benchmarking FCCM against other\nbaselines demonstrates its superiority across both fully synthetic and\nsemi-synthetic datasets.\n", "link": "http://arxiv.org/abs/2505.05242v1", "date": "2025-05-08", "relevancy": 1.891, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4979}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4686}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Treatment%20Effect%20Estimation%20via%20Active%20Learning%3A%20A%0A%20%20Counterfactual%20Covering%20Perspective&body=Title%3A%20Enhancing%20Treatment%20Effect%20Estimation%20via%20Active%20Learning%3A%20A%0A%20%20Counterfactual%20Covering%20Perspective%0AAuthor%3A%20Hechuan%20Wen%20and%20Tong%20Chen%20and%20Mingming%20Gong%20and%20Li%20Kheng%20Chai%20and%20Shazia%20Sadiq%20and%20Hongzhi%20Yin%0AAbstract%3A%20%20%20Although%20numerous%20complex%20algorithms%20for%20treatment%20effect%20estimation%20have%0Abeen%20developed%20in%20recent%20years%2C%20their%20effectiveness%20remains%20limited%20when%0Ahandling%20insufficiently%20labeled%20training%20sets%20due%20to%20the%20high%20cost%20of%20labeling%0Athe%20effect%20after%20treatment%2C%20e.g.%2C%20expensive%20tumor%20imaging%20or%20biopsy%20procedures%0Aneeded%20to%20evaluate%20treatment%20effects.%20Therefore%2C%20it%20becomes%20essential%20to%0Aactively%20incorporate%20more%20high-quality%20labeled%20data%2C%20all%20while%20adhering%20to%20a%0Aconstrained%20labeling%20budget.%20To%20enable%20data-efficient%20treatment%20effect%0Aestimation%2C%20we%20formalize%20the%20problem%20through%20rigorous%20theoretical%20analysis%0Awithin%20the%20active%20learning%20context%2C%20where%20the%20derived%20key%20measures%20--%0A%5Ctextit%7Bfactual%7D%20and%20%5Ctextit%7Bcounterfactual%20covering%20radius%7D%20determine%20the%20risk%0Aupper%20bound.%20To%20reduce%20the%20bound%2C%20we%20propose%20a%20greedy%20radius%20reduction%0Aalgorithm%2C%20which%20excels%20under%20an%20idealized%2C%20balanced%20data%20distribution.%20To%0Ageneralize%20to%20more%20realistic%20data%20distributions%2C%20we%20further%20propose%20FCCM%2C%20which%0Atransforms%20the%20optimization%20objective%20into%20the%20%5Ctextit%7BFactual%7D%20and%0A%5Ctextit%7BCounterfactual%20Coverage%20Maximization%7D%20to%20ensure%20effective%20radius%0Areduction%20during%20data%20acquisition.%20Furthermore%2C%20benchmarking%20FCCM%20against%20other%0Abaselines%20demonstrates%20its%20superiority%20across%20both%20fully%20synthetic%20and%0Asemi-synthetic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Treatment%2520Effect%2520Estimation%2520via%2520Active%2520Learning%253A%2520A%250A%2520%2520Counterfactual%2520Covering%2520Perspective%26entry.906535625%3DHechuan%2520Wen%2520and%2520Tong%2520Chen%2520and%2520Mingming%2520Gong%2520and%2520Li%2520Kheng%2520Chai%2520and%2520Shazia%2520Sadiq%2520and%2520Hongzhi%2520Yin%26entry.1292438233%3D%2520%2520Although%2520numerous%2520complex%2520algorithms%2520for%2520treatment%2520effect%2520estimation%2520have%250Abeen%2520developed%2520in%2520recent%2520years%252C%2520their%2520effectiveness%2520remains%2520limited%2520when%250Ahandling%2520insufficiently%2520labeled%2520training%2520sets%2520due%2520to%2520the%2520high%2520cost%2520of%2520labeling%250Athe%2520effect%2520after%2520treatment%252C%2520e.g.%252C%2520expensive%2520tumor%2520imaging%2520or%2520biopsy%2520procedures%250Aneeded%2520to%2520evaluate%2520treatment%2520effects.%2520Therefore%252C%2520it%2520becomes%2520essential%2520to%250Aactively%2520incorporate%2520more%2520high-quality%2520labeled%2520data%252C%2520all%2520while%2520adhering%2520to%2520a%250Aconstrained%2520labeling%2520budget.%2520To%2520enable%2520data-efficient%2520treatment%2520effect%250Aestimation%252C%2520we%2520formalize%2520the%2520problem%2520through%2520rigorous%2520theoretical%2520analysis%250Awithin%2520the%2520active%2520learning%2520context%252C%2520where%2520the%2520derived%2520key%2520measures%2520--%250A%255Ctextit%257Bfactual%257D%2520and%2520%255Ctextit%257Bcounterfactual%2520covering%2520radius%257D%2520determine%2520the%2520risk%250Aupper%2520bound.%2520To%2520reduce%2520the%2520bound%252C%2520we%2520propose%2520a%2520greedy%2520radius%2520reduction%250Aalgorithm%252C%2520which%2520excels%2520under%2520an%2520idealized%252C%2520balanced%2520data%2520distribution.%2520To%250Ageneralize%2520to%2520more%2520realistic%2520data%2520distributions%252C%2520we%2520further%2520propose%2520FCCM%252C%2520which%250Atransforms%2520the%2520optimization%2520objective%2520into%2520the%2520%255Ctextit%257BFactual%257D%2520and%250A%255Ctextit%257BCounterfactual%2520Coverage%2520Maximization%257D%2520to%2520ensure%2520effective%2520radius%250Areduction%2520during%2520data%2520acquisition.%2520Furthermore%252C%2520benchmarking%2520FCCM%2520against%2520other%250Abaselines%2520demonstrates%2520its%2520superiority%2520across%2520both%2520fully%2520synthetic%2520and%250Asemi-synthetic%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Treatment%20Effect%20Estimation%20via%20Active%20Learning%3A%20A%0A%20%20Counterfactual%20Covering%20Perspective&entry.906535625=Hechuan%20Wen%20and%20Tong%20Chen%20and%20Mingming%20Gong%20and%20Li%20Kheng%20Chai%20and%20Shazia%20Sadiq%20and%20Hongzhi%20Yin&entry.1292438233=%20%20Although%20numerous%20complex%20algorithms%20for%20treatment%20effect%20estimation%20have%0Abeen%20developed%20in%20recent%20years%2C%20their%20effectiveness%20remains%20limited%20when%0Ahandling%20insufficiently%20labeled%20training%20sets%20due%20to%20the%20high%20cost%20of%20labeling%0Athe%20effect%20after%20treatment%2C%20e.g.%2C%20expensive%20tumor%20imaging%20or%20biopsy%20procedures%0Aneeded%20to%20evaluate%20treatment%20effects.%20Therefore%2C%20it%20becomes%20essential%20to%0Aactively%20incorporate%20more%20high-quality%20labeled%20data%2C%20all%20while%20adhering%20to%20a%0Aconstrained%20labeling%20budget.%20To%20enable%20data-efficient%20treatment%20effect%0Aestimation%2C%20we%20formalize%20the%20problem%20through%20rigorous%20theoretical%20analysis%0Awithin%20the%20active%20learning%20context%2C%20where%20the%20derived%20key%20measures%20--%0A%5Ctextit%7Bfactual%7D%20and%20%5Ctextit%7Bcounterfactual%20covering%20radius%7D%20determine%20the%20risk%0Aupper%20bound.%20To%20reduce%20the%20bound%2C%20we%20propose%20a%20greedy%20radius%20reduction%0Aalgorithm%2C%20which%20excels%20under%20an%20idealized%2C%20balanced%20data%20distribution.%20To%0Ageneralize%20to%20more%20realistic%20data%20distributions%2C%20we%20further%20propose%20FCCM%2C%20which%0Atransforms%20the%20optimization%20objective%20into%20the%20%5Ctextit%7BFactual%7D%20and%0A%5Ctextit%7BCounterfactual%20Coverage%20Maximization%7D%20to%20ensure%20effective%20radius%0Areduction%20during%20data%20acquisition.%20Furthermore%2C%20benchmarking%20FCCM%20against%20other%0Abaselines%20demonstrates%20its%20superiority%20across%20both%20fully%20synthetic%20and%0Asemi-synthetic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05242v1&entry.124074799=Read"},
{"title": "Software Development Life Cycle Perspective: A Survey of Benchmarks for\n  CodeLLMs and Agents", "author": "Kaixin Wang and Tianlin Li and Xiaoyu Zhang and Chong Wang and Weisong Sun and Yang Liu and Bin Shi", "abstract": "  Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios.\n", "link": "http://arxiv.org/abs/2505.05283v1", "date": "2025-05-08", "relevancy": 1.8902, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Software%20Development%20Life%20Cycle%20Perspective%3A%20A%20Survey%20of%20Benchmarks%20for%0A%20%20CodeLLMs%20and%20Agents&body=Title%3A%20Software%20Development%20Life%20Cycle%20Perspective%3A%20A%20Survey%20of%20Benchmarks%20for%0A%20%20CodeLLMs%20and%20Agents%0AAuthor%3A%20Kaixin%20Wang%20and%20Tianlin%20Li%20and%20Xiaoyu%20Zhang%20and%20Chong%20Wang%20and%20Weisong%20Sun%20and%20Yang%20Liu%20and%20Bin%20Shi%0AAbstract%3A%20%20%20Code%20large%20language%20models%20%28CodeLLMs%29%20and%20agents%20have%20shown%20great%20promise%20in%0Atackling%20complex%20software%20engineering%20tasks.Compared%20to%20traditional%20software%0Aengineering%20methods%2C%20CodeLLMs%20and%20agents%20offer%20stronger%20abilities%2C%20and%20can%0Aflexibly%20process%20inputs%20and%20outputs%20in%20both%20natural%20and%20code.%20Benchmarking%0Aplays%20a%20crucial%20role%20in%20evaluating%20the%20capabilities%20of%20CodeLLMs%20and%20agents%2C%0Aguiding%20their%20development%20and%20deployment.%20However%2C%20despite%20their%20growing%0Asignificance%2C%20there%20remains%20a%20lack%20of%20comprehensive%20reviews%20of%20benchmarks%20for%0ACodeLLMs%20and%20agents.%20To%20bridge%20this%20gap%2C%20this%20paper%20provides%20a%20comprehensive%0Areview%20of%20existing%20benchmarks%20for%20CodeLLMs%20and%20agents%2C%20studying%20and%20analyzing%0A181%20benchmarks%20from%20461%20relevant%20papers%2C%20covering%20the%20different%20phases%20of%20the%0Asoftware%20development%20life%20cycle%20%28SDLC%29.%20Our%20findings%20reveal%20a%20notable%20imbalance%0Ain%20the%20coverage%20of%20current%20benchmarks%2C%20with%20approximately%2060%25%20focused%20on%20the%0Asoftware%20development%20phase%20in%20SDLC%2C%20while%20requirements%20engineering%20and%20software%0Adesign%20phases%20receive%20minimal%20attention%20at%20only%205%25%20and%203%25%2C%20respectively.%0AAdditionally%2C%20Python%20emerges%20as%20the%20dominant%20programming%20language%20across%20the%0Areviewed%20benchmarks.%20Finally%2C%20this%20paper%20highlights%20the%20challenges%20of%20current%0Aresearch%20and%20proposes%20future%20directions%2C%20aiming%20to%20narrow%20the%20gap%20between%20the%0Atheoretical%20capabilities%20of%20CodeLLMs%20and%20agents%20and%20their%20application%20in%0Areal-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoftware%2520Development%2520Life%2520Cycle%2520Perspective%253A%2520A%2520Survey%2520of%2520Benchmarks%2520for%250A%2520%2520CodeLLMs%2520and%2520Agents%26entry.906535625%3DKaixin%2520Wang%2520and%2520Tianlin%2520Li%2520and%2520Xiaoyu%2520Zhang%2520and%2520Chong%2520Wang%2520and%2520Weisong%2520Sun%2520and%2520Yang%2520Liu%2520and%2520Bin%2520Shi%26entry.1292438233%3D%2520%2520Code%2520large%2520language%2520models%2520%2528CodeLLMs%2529%2520and%2520agents%2520have%2520shown%2520great%2520promise%2520in%250Atackling%2520complex%2520software%2520engineering%2520tasks.Compared%2520to%2520traditional%2520software%250Aengineering%2520methods%252C%2520CodeLLMs%2520and%2520agents%2520offer%2520stronger%2520abilities%252C%2520and%2520can%250Aflexibly%2520process%2520inputs%2520and%2520outputs%2520in%2520both%2520natural%2520and%2520code.%2520Benchmarking%250Aplays%2520a%2520crucial%2520role%2520in%2520evaluating%2520the%2520capabilities%2520of%2520CodeLLMs%2520and%2520agents%252C%250Aguiding%2520their%2520development%2520and%2520deployment.%2520However%252C%2520despite%2520their%2520growing%250Asignificance%252C%2520there%2520remains%2520a%2520lack%2520of%2520comprehensive%2520reviews%2520of%2520benchmarks%2520for%250ACodeLLMs%2520and%2520agents.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520paper%2520provides%2520a%2520comprehensive%250Areview%2520of%2520existing%2520benchmarks%2520for%2520CodeLLMs%2520and%2520agents%252C%2520studying%2520and%2520analyzing%250A181%2520benchmarks%2520from%2520461%2520relevant%2520papers%252C%2520covering%2520the%2520different%2520phases%2520of%2520the%250Asoftware%2520development%2520life%2520cycle%2520%2528SDLC%2529.%2520Our%2520findings%2520reveal%2520a%2520notable%2520imbalance%250Ain%2520the%2520coverage%2520of%2520current%2520benchmarks%252C%2520with%2520approximately%252060%2525%2520focused%2520on%2520the%250Asoftware%2520development%2520phase%2520in%2520SDLC%252C%2520while%2520requirements%2520engineering%2520and%2520software%250Adesign%2520phases%2520receive%2520minimal%2520attention%2520at%2520only%25205%2525%2520and%25203%2525%252C%2520respectively.%250AAdditionally%252C%2520Python%2520emerges%2520as%2520the%2520dominant%2520programming%2520language%2520across%2520the%250Areviewed%2520benchmarks.%2520Finally%252C%2520this%2520paper%2520highlights%2520the%2520challenges%2520of%2520current%250Aresearch%2520and%2520proposes%2520future%2520directions%252C%2520aiming%2520to%2520narrow%2520the%2520gap%2520between%2520the%250Atheoretical%2520capabilities%2520of%2520CodeLLMs%2520and%2520agents%2520and%2520their%2520application%2520in%250Areal-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Software%20Development%20Life%20Cycle%20Perspective%3A%20A%20Survey%20of%20Benchmarks%20for%0A%20%20CodeLLMs%20and%20Agents&entry.906535625=Kaixin%20Wang%20and%20Tianlin%20Li%20and%20Xiaoyu%20Zhang%20and%20Chong%20Wang%20and%20Weisong%20Sun%20and%20Yang%20Liu%20and%20Bin%20Shi&entry.1292438233=%20%20Code%20large%20language%20models%20%28CodeLLMs%29%20and%20agents%20have%20shown%20great%20promise%20in%0Atackling%20complex%20software%20engineering%20tasks.Compared%20to%20traditional%20software%0Aengineering%20methods%2C%20CodeLLMs%20and%20agents%20offer%20stronger%20abilities%2C%20and%20can%0Aflexibly%20process%20inputs%20and%20outputs%20in%20both%20natural%20and%20code.%20Benchmarking%0Aplays%20a%20crucial%20role%20in%20evaluating%20the%20capabilities%20of%20CodeLLMs%20and%20agents%2C%0Aguiding%20their%20development%20and%20deployment.%20However%2C%20despite%20their%20growing%0Asignificance%2C%20there%20remains%20a%20lack%20of%20comprehensive%20reviews%20of%20benchmarks%20for%0ACodeLLMs%20and%20agents.%20To%20bridge%20this%20gap%2C%20this%20paper%20provides%20a%20comprehensive%0Areview%20of%20existing%20benchmarks%20for%20CodeLLMs%20and%20agents%2C%20studying%20and%20analyzing%0A181%20benchmarks%20from%20461%20relevant%20papers%2C%20covering%20the%20different%20phases%20of%20the%0Asoftware%20development%20life%20cycle%20%28SDLC%29.%20Our%20findings%20reveal%20a%20notable%20imbalance%0Ain%20the%20coverage%20of%20current%20benchmarks%2C%20with%20approximately%2060%25%20focused%20on%20the%0Asoftware%20development%20phase%20in%20SDLC%2C%20while%20requirements%20engineering%20and%20software%0Adesign%20phases%20receive%20minimal%20attention%20at%20only%205%25%20and%203%25%2C%20respectively.%0AAdditionally%2C%20Python%20emerges%20as%20the%20dominant%20programming%20language%20across%20the%0Areviewed%20benchmarks.%20Finally%2C%20this%20paper%20highlights%20the%20challenges%20of%20current%0Aresearch%20and%20proposes%20future%20directions%2C%20aiming%20to%20narrow%20the%20gap%20between%20the%0Atheoretical%20capabilities%20of%20CodeLLMs%20and%20agents%20and%20their%20application%20in%0Areal-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05283v1&entry.124074799=Read"},
{"title": "USPR: Learning a Unified Solver for Profiled Routing", "author": "Chuanbo Hua and Federico Berto and Zhikai Zhao and Jiwoo Son and Changhyun Kwon and Jinkyoo Park", "abstract": "  The Profiled Vehicle Routing Problem (PVRP) extends the classical VRP by\nincorporating vehicle-client-specific preferences and constraints, reflecting\nreal-world requirements such as zone restrictions and service-level\npreferences. While recent reinforcement learning (RL) solvers have shown\npromise, they require retraining for each new profile distribution, suffer from\npoor representation ability, and struggle to generalize to out-of-distribution\ninstances. In this paper, we address these limitations by introducing USPR\n(Unified Solver for Profiled Routing), a novel framework that natively handles\narbitrary profile types. USPR introduces three key innovations: (i) Profile\nEmbeddings (PE) to encode any combination of profile types; (ii) Multi-Head\nProfiled Attention (MHPA), an attention mechanism that models rich interactions\nbetween vehicles and clients; (iii) Profile-aware Score Reshaping (PSR), which\ndynamically adjusts decoder logits using profile scores to improve\ngeneralization. Empirical results on diverse PVRP benchmarks demonstrate that\nUSPR achieves state-of-the-art results among learning-based methods while\noffering significant gains in flexibility and computational efficiency. We make\nour source code publicly available to foster future research at\nhttps://github.com/ai4co/uspr.\n", "link": "http://arxiv.org/abs/2505.05119v1", "date": "2025-05-08", "relevancy": 1.8778, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4866}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20USPR%3A%20Learning%20a%20Unified%20Solver%20for%20Profiled%20Routing&body=Title%3A%20USPR%3A%20Learning%20a%20Unified%20Solver%20for%20Profiled%20Routing%0AAuthor%3A%20Chuanbo%20Hua%20and%20Federico%20Berto%20and%20Zhikai%20Zhao%20and%20Jiwoo%20Son%20and%20Changhyun%20Kwon%20and%20Jinkyoo%20Park%0AAbstract%3A%20%20%20The%20Profiled%20Vehicle%20Routing%20Problem%20%28PVRP%29%20extends%20the%20classical%20VRP%20by%0Aincorporating%20vehicle-client-specific%20preferences%20and%20constraints%2C%20reflecting%0Areal-world%20requirements%20such%20as%20zone%20restrictions%20and%20service-level%0Apreferences.%20While%20recent%20reinforcement%20learning%20%28RL%29%20solvers%20have%20shown%0Apromise%2C%20they%20require%20retraining%20for%20each%20new%20profile%20distribution%2C%20suffer%20from%0Apoor%20representation%20ability%2C%20and%20struggle%20to%20generalize%20to%20out-of-distribution%0Ainstances.%20In%20this%20paper%2C%20we%20address%20these%20limitations%20by%20introducing%20USPR%0A%28Unified%20Solver%20for%20Profiled%20Routing%29%2C%20a%20novel%20framework%20that%20natively%20handles%0Aarbitrary%20profile%20types.%20USPR%20introduces%20three%20key%20innovations%3A%20%28i%29%20Profile%0AEmbeddings%20%28PE%29%20to%20encode%20any%20combination%20of%20profile%20types%3B%20%28ii%29%20Multi-Head%0AProfiled%20Attention%20%28MHPA%29%2C%20an%20attention%20mechanism%20that%20models%20rich%20interactions%0Abetween%20vehicles%20and%20clients%3B%20%28iii%29%20Profile-aware%20Score%20Reshaping%20%28PSR%29%2C%20which%0Adynamically%20adjusts%20decoder%20logits%20using%20profile%20scores%20to%20improve%0Ageneralization.%20Empirical%20results%20on%20diverse%20PVRP%20benchmarks%20demonstrate%20that%0AUSPR%20achieves%20state-of-the-art%20results%20among%20learning-based%20methods%20while%0Aoffering%20significant%20gains%20in%20flexibility%20and%20computational%20efficiency.%20We%20make%0Aour%20source%20code%20publicly%20available%20to%20foster%20future%20research%20at%0Ahttps%3A//github.com/ai4co/uspr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUSPR%253A%2520Learning%2520a%2520Unified%2520Solver%2520for%2520Profiled%2520Routing%26entry.906535625%3DChuanbo%2520Hua%2520and%2520Federico%2520Berto%2520and%2520Zhikai%2520Zhao%2520and%2520Jiwoo%2520Son%2520and%2520Changhyun%2520Kwon%2520and%2520Jinkyoo%2520Park%26entry.1292438233%3D%2520%2520The%2520Profiled%2520Vehicle%2520Routing%2520Problem%2520%2528PVRP%2529%2520extends%2520the%2520classical%2520VRP%2520by%250Aincorporating%2520vehicle-client-specific%2520preferences%2520and%2520constraints%252C%2520reflecting%250Areal-world%2520requirements%2520such%2520as%2520zone%2520restrictions%2520and%2520service-level%250Apreferences.%2520While%2520recent%2520reinforcement%2520learning%2520%2528RL%2529%2520solvers%2520have%2520shown%250Apromise%252C%2520they%2520require%2520retraining%2520for%2520each%2520new%2520profile%2520distribution%252C%2520suffer%2520from%250Apoor%2520representation%2520ability%252C%2520and%2520struggle%2520to%2520generalize%2520to%2520out-of-distribution%250Ainstances.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520limitations%2520by%2520introducing%2520USPR%250A%2528Unified%2520Solver%2520for%2520Profiled%2520Routing%2529%252C%2520a%2520novel%2520framework%2520that%2520natively%2520handles%250Aarbitrary%2520profile%2520types.%2520USPR%2520introduces%2520three%2520key%2520innovations%253A%2520%2528i%2529%2520Profile%250AEmbeddings%2520%2528PE%2529%2520to%2520encode%2520any%2520combination%2520of%2520profile%2520types%253B%2520%2528ii%2529%2520Multi-Head%250AProfiled%2520Attention%2520%2528MHPA%2529%252C%2520an%2520attention%2520mechanism%2520that%2520models%2520rich%2520interactions%250Abetween%2520vehicles%2520and%2520clients%253B%2520%2528iii%2529%2520Profile-aware%2520Score%2520Reshaping%2520%2528PSR%2529%252C%2520which%250Adynamically%2520adjusts%2520decoder%2520logits%2520using%2520profile%2520scores%2520to%2520improve%250Ageneralization.%2520Empirical%2520results%2520on%2520diverse%2520PVRP%2520benchmarks%2520demonstrate%2520that%250AUSPR%2520achieves%2520state-of-the-art%2520results%2520among%2520learning-based%2520methods%2520while%250Aoffering%2520significant%2520gains%2520in%2520flexibility%2520and%2520computational%2520efficiency.%2520We%2520make%250Aour%2520source%2520code%2520publicly%2520available%2520to%2520foster%2520future%2520research%2520at%250Ahttps%253A//github.com/ai4co/uspr.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USPR%3A%20Learning%20a%20Unified%20Solver%20for%20Profiled%20Routing&entry.906535625=Chuanbo%20Hua%20and%20Federico%20Berto%20and%20Zhikai%20Zhao%20and%20Jiwoo%20Son%20and%20Changhyun%20Kwon%20and%20Jinkyoo%20Park&entry.1292438233=%20%20The%20Profiled%20Vehicle%20Routing%20Problem%20%28PVRP%29%20extends%20the%20classical%20VRP%20by%0Aincorporating%20vehicle-client-specific%20preferences%20and%20constraints%2C%20reflecting%0Areal-world%20requirements%20such%20as%20zone%20restrictions%20and%20service-level%0Apreferences.%20While%20recent%20reinforcement%20learning%20%28RL%29%20solvers%20have%20shown%0Apromise%2C%20they%20require%20retraining%20for%20each%20new%20profile%20distribution%2C%20suffer%20from%0Apoor%20representation%20ability%2C%20and%20struggle%20to%20generalize%20to%20out-of-distribution%0Ainstances.%20In%20this%20paper%2C%20we%20address%20these%20limitations%20by%20introducing%20USPR%0A%28Unified%20Solver%20for%20Profiled%20Routing%29%2C%20a%20novel%20framework%20that%20natively%20handles%0Aarbitrary%20profile%20types.%20USPR%20introduces%20three%20key%20innovations%3A%20%28i%29%20Profile%0AEmbeddings%20%28PE%29%20to%20encode%20any%20combination%20of%20profile%20types%3B%20%28ii%29%20Multi-Head%0AProfiled%20Attention%20%28MHPA%29%2C%20an%20attention%20mechanism%20that%20models%20rich%20interactions%0Abetween%20vehicles%20and%20clients%3B%20%28iii%29%20Profile-aware%20Score%20Reshaping%20%28PSR%29%2C%20which%0Adynamically%20adjusts%20decoder%20logits%20using%20profile%20scores%20to%20improve%0Ageneralization.%20Empirical%20results%20on%20diverse%20PVRP%20benchmarks%20demonstrate%20that%0AUSPR%20achieves%20state-of-the-art%20results%20among%20learning-based%20methods%20while%0Aoffering%20significant%20gains%20in%20flexibility%20and%20computational%20efficiency.%20We%20make%0Aour%20source%20code%20publicly%20available%20to%20foster%20future%20research%20at%0Ahttps%3A//github.com/ai4co/uspr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05119v1&entry.124074799=Read"},
{"title": "Joint Super-Resolution and Segmentation for 1-m Impervious Surface Area\n  Mapping in China's Yangtze River Economic Belt", "author": "Jie Deng and Danfeng Hong and Chenyu Li and Naoto Yokoya", "abstract": "  We propose a novel joint framework by integrating super-resolution and\nsegmentation, called JointSeg, which enables the generation of 1-meter ISA maps\ndirectly from freely available Sentinel-2 imagery. JointSeg was trained on\nmultimodal cross-resolution inputs, offering a scalable and affordable\nalternative to traditional approaches. This synergistic design enables gradual\nresolution enhancement from 10m to 1m while preserving fine-grained spatial\ntextures, and ensures high classification fidelity through effective\ncross-scale feature fusion. This method has been successfully applied to the\nYangtze River Economic Belt (YREB), a region characterized by complex\nurban-rural patterns and diverse topography. As a result, a comprehensive ISA\nmapping product for 2021, referred to as ISA-1, was generated, covering an area\nof over 2.2 million square kilometers. Quantitative comparisons against the 10m\nESA WorldCover and other benchmark products reveal that ISA-1 achieves an\nF1-score of 85.71%, outperforming bilinear-interpolation-based segmentation by\n9.5%, and surpassing other ISA datasets by 21.43%-61.07%. In densely urbanized\nareas (e.g., Suzhou, Nanjing), ISA-1 reduces ISA overestimation through\nimproved discrimination of green spaces and water bodies. Conversely, in\nmountainous regions (e.g., Ganzi, Zhaotong), it identifies significantly more\nISA due to its enhanced ability to detect fragmented anthropogenic features\nsuch as rural roads and sparse settlements, demonstrating its robustness across\ndiverse landscapes. Moreover, we present biennial ISA maps from 2017 to 2023,\ncapturing spatiotemporal urbanization dynamics across representative cities.\nThe results highlight distinct regional growth patterns: rapid expansion in\nupstream cities, moderate growth in midstream regions, and saturation in\ndownstream metropolitan areas.\n", "link": "http://arxiv.org/abs/2505.05367v1", "date": "2025-05-08", "relevancy": 1.8714, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4717}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4711}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Super-Resolution%20and%20Segmentation%20for%201-m%20Impervious%20Surface%20Area%0A%20%20Mapping%20in%20China%27s%20Yangtze%20River%20Economic%20Belt&body=Title%3A%20Joint%20Super-Resolution%20and%20Segmentation%20for%201-m%20Impervious%20Surface%20Area%0A%20%20Mapping%20in%20China%27s%20Yangtze%20River%20Economic%20Belt%0AAuthor%3A%20Jie%20Deng%20and%20Danfeng%20Hong%20and%20Chenyu%20Li%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20joint%20framework%20by%20integrating%20super-resolution%20and%0Asegmentation%2C%20called%20JointSeg%2C%20which%20enables%20the%20generation%20of%201-meter%20ISA%20maps%0Adirectly%20from%20freely%20available%20Sentinel-2%20imagery.%20JointSeg%20was%20trained%20on%0Amultimodal%20cross-resolution%20inputs%2C%20offering%20a%20scalable%20and%20affordable%0Aalternative%20to%20traditional%20approaches.%20This%20synergistic%20design%20enables%20gradual%0Aresolution%20enhancement%20from%2010m%20to%201m%20while%20preserving%20fine-grained%20spatial%0Atextures%2C%20and%20ensures%20high%20classification%20fidelity%20through%20effective%0Across-scale%20feature%20fusion.%20This%20method%20has%20been%20successfully%20applied%20to%20the%0AYangtze%20River%20Economic%20Belt%20%28YREB%29%2C%20a%20region%20characterized%20by%20complex%0Aurban-rural%20patterns%20and%20diverse%20topography.%20As%20a%20result%2C%20a%20comprehensive%20ISA%0Amapping%20product%20for%202021%2C%20referred%20to%20as%20ISA-1%2C%20was%20generated%2C%20covering%20an%20area%0Aof%20over%202.2%20million%20square%20kilometers.%20Quantitative%20comparisons%20against%20the%2010m%0AESA%20WorldCover%20and%20other%20benchmark%20products%20reveal%20that%20ISA-1%20achieves%20an%0AF1-score%20of%2085.71%25%2C%20outperforming%20bilinear-interpolation-based%20segmentation%20by%0A9.5%25%2C%20and%20surpassing%20other%20ISA%20datasets%20by%2021.43%25-61.07%25.%20In%20densely%20urbanized%0Aareas%20%28e.g.%2C%20Suzhou%2C%20Nanjing%29%2C%20ISA-1%20reduces%20ISA%20overestimation%20through%0Aimproved%20discrimination%20of%20green%20spaces%20and%20water%20bodies.%20Conversely%2C%20in%0Amountainous%20regions%20%28e.g.%2C%20Ganzi%2C%20Zhaotong%29%2C%20it%20identifies%20significantly%20more%0AISA%20due%20to%20its%20enhanced%20ability%20to%20detect%20fragmented%20anthropogenic%20features%0Asuch%20as%20rural%20roads%20and%20sparse%20settlements%2C%20demonstrating%20its%20robustness%20across%0Adiverse%20landscapes.%20Moreover%2C%20we%20present%20biennial%20ISA%20maps%20from%202017%20to%202023%2C%0Acapturing%20spatiotemporal%20urbanization%20dynamics%20across%20representative%20cities.%0AThe%20results%20highlight%20distinct%20regional%20growth%20patterns%3A%20rapid%20expansion%20in%0Aupstream%20cities%2C%20moderate%20growth%20in%20midstream%20regions%2C%20and%20saturation%20in%0Adownstream%20metropolitan%20areas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Super-Resolution%2520and%2520Segmentation%2520for%25201-m%2520Impervious%2520Surface%2520Area%250A%2520%2520Mapping%2520in%2520China%2527s%2520Yangtze%2520River%2520Economic%2520Belt%26entry.906535625%3DJie%2520Deng%2520and%2520Danfeng%2520Hong%2520and%2520Chenyu%2520Li%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520joint%2520framework%2520by%2520integrating%2520super-resolution%2520and%250Asegmentation%252C%2520called%2520JointSeg%252C%2520which%2520enables%2520the%2520generation%2520of%25201-meter%2520ISA%2520maps%250Adirectly%2520from%2520freely%2520available%2520Sentinel-2%2520imagery.%2520JointSeg%2520was%2520trained%2520on%250Amultimodal%2520cross-resolution%2520inputs%252C%2520offering%2520a%2520scalable%2520and%2520affordable%250Aalternative%2520to%2520traditional%2520approaches.%2520This%2520synergistic%2520design%2520enables%2520gradual%250Aresolution%2520enhancement%2520from%252010m%2520to%25201m%2520while%2520preserving%2520fine-grained%2520spatial%250Atextures%252C%2520and%2520ensures%2520high%2520classification%2520fidelity%2520through%2520effective%250Across-scale%2520feature%2520fusion.%2520This%2520method%2520has%2520been%2520successfully%2520applied%2520to%2520the%250AYangtze%2520River%2520Economic%2520Belt%2520%2528YREB%2529%252C%2520a%2520region%2520characterized%2520by%2520complex%250Aurban-rural%2520patterns%2520and%2520diverse%2520topography.%2520As%2520a%2520result%252C%2520a%2520comprehensive%2520ISA%250Amapping%2520product%2520for%25202021%252C%2520referred%2520to%2520as%2520ISA-1%252C%2520was%2520generated%252C%2520covering%2520an%2520area%250Aof%2520over%25202.2%2520million%2520square%2520kilometers.%2520Quantitative%2520comparisons%2520against%2520the%252010m%250AESA%2520WorldCover%2520and%2520other%2520benchmark%2520products%2520reveal%2520that%2520ISA-1%2520achieves%2520an%250AF1-score%2520of%252085.71%2525%252C%2520outperforming%2520bilinear-interpolation-based%2520segmentation%2520by%250A9.5%2525%252C%2520and%2520surpassing%2520other%2520ISA%2520datasets%2520by%252021.43%2525-61.07%2525.%2520In%2520densely%2520urbanized%250Aareas%2520%2528e.g.%252C%2520Suzhou%252C%2520Nanjing%2529%252C%2520ISA-1%2520reduces%2520ISA%2520overestimation%2520through%250Aimproved%2520discrimination%2520of%2520green%2520spaces%2520and%2520water%2520bodies.%2520Conversely%252C%2520in%250Amountainous%2520regions%2520%2528e.g.%252C%2520Ganzi%252C%2520Zhaotong%2529%252C%2520it%2520identifies%2520significantly%2520more%250AISA%2520due%2520to%2520its%2520enhanced%2520ability%2520to%2520detect%2520fragmented%2520anthropogenic%2520features%250Asuch%2520as%2520rural%2520roads%2520and%2520sparse%2520settlements%252C%2520demonstrating%2520its%2520robustness%2520across%250Adiverse%2520landscapes.%2520Moreover%252C%2520we%2520present%2520biennial%2520ISA%2520maps%2520from%25202017%2520to%25202023%252C%250Acapturing%2520spatiotemporal%2520urbanization%2520dynamics%2520across%2520representative%2520cities.%250AThe%2520results%2520highlight%2520distinct%2520regional%2520growth%2520patterns%253A%2520rapid%2520expansion%2520in%250Aupstream%2520cities%252C%2520moderate%2520growth%2520in%2520midstream%2520regions%252C%2520and%2520saturation%2520in%250Adownstream%2520metropolitan%2520areas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Super-Resolution%20and%20Segmentation%20for%201-m%20Impervious%20Surface%20Area%0A%20%20Mapping%20in%20China%27s%20Yangtze%20River%20Economic%20Belt&entry.906535625=Jie%20Deng%20and%20Danfeng%20Hong%20and%20Chenyu%20Li%20and%20Naoto%20Yokoya&entry.1292438233=%20%20We%20propose%20a%20novel%20joint%20framework%20by%20integrating%20super-resolution%20and%0Asegmentation%2C%20called%20JointSeg%2C%20which%20enables%20the%20generation%20of%201-meter%20ISA%20maps%0Adirectly%20from%20freely%20available%20Sentinel-2%20imagery.%20JointSeg%20was%20trained%20on%0Amultimodal%20cross-resolution%20inputs%2C%20offering%20a%20scalable%20and%20affordable%0Aalternative%20to%20traditional%20approaches.%20This%20synergistic%20design%20enables%20gradual%0Aresolution%20enhancement%20from%2010m%20to%201m%20while%20preserving%20fine-grained%20spatial%0Atextures%2C%20and%20ensures%20high%20classification%20fidelity%20through%20effective%0Across-scale%20feature%20fusion.%20This%20method%20has%20been%20successfully%20applied%20to%20the%0AYangtze%20River%20Economic%20Belt%20%28YREB%29%2C%20a%20region%20characterized%20by%20complex%0Aurban-rural%20patterns%20and%20diverse%20topography.%20As%20a%20result%2C%20a%20comprehensive%20ISA%0Amapping%20product%20for%202021%2C%20referred%20to%20as%20ISA-1%2C%20was%20generated%2C%20covering%20an%20area%0Aof%20over%202.2%20million%20square%20kilometers.%20Quantitative%20comparisons%20against%20the%2010m%0AESA%20WorldCover%20and%20other%20benchmark%20products%20reveal%20that%20ISA-1%20achieves%20an%0AF1-score%20of%2085.71%25%2C%20outperforming%20bilinear-interpolation-based%20segmentation%20by%0A9.5%25%2C%20and%20surpassing%20other%20ISA%20datasets%20by%2021.43%25-61.07%25.%20In%20densely%20urbanized%0Aareas%20%28e.g.%2C%20Suzhou%2C%20Nanjing%29%2C%20ISA-1%20reduces%20ISA%20overestimation%20through%0Aimproved%20discrimination%20of%20green%20spaces%20and%20water%20bodies.%20Conversely%2C%20in%0Amountainous%20regions%20%28e.g.%2C%20Ganzi%2C%20Zhaotong%29%2C%20it%20identifies%20significantly%20more%0AISA%20due%20to%20its%20enhanced%20ability%20to%20detect%20fragmented%20anthropogenic%20features%0Asuch%20as%20rural%20roads%20and%20sparse%20settlements%2C%20demonstrating%20its%20robustness%20across%0Adiverse%20landscapes.%20Moreover%2C%20we%20present%20biennial%20ISA%20maps%20from%202017%20to%202023%2C%0Acapturing%20spatiotemporal%20urbanization%20dynamics%20across%20representative%20cities.%0AThe%20results%20highlight%20distinct%20regional%20growth%20patterns%3A%20rapid%20expansion%20in%0Aupstream%20cities%2C%20moderate%20growth%20in%20midstream%20regions%2C%20and%20saturation%20in%0Adownstream%20metropolitan%20areas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05367v1&entry.124074799=Read"},
{"title": "EDmamba: A Simple yet Effective Event Denoising Method with State Space\n  Model", "author": "Ciyu Ruan and Zihang Gong and Ruishan Guo and Jingao Xu and Xinlei Chen", "abstract": "  Event cameras excel in high-speed vision due to their high temporal\nresolution, high dynamic range, and low power consumption. However, as dynamic\nvision sensors, their output is inherently noisy, making efficient denoising\nessential to preserve their ultra-low latency and real-time processing\ncapabilities. Existing event denoising methods struggle with a critical\ndilemma: computationally intensive approaches compromise the sensor's\nhigh-speed advantage, while lightweight methods often lack robustness across\nvarying noise levels. To address this, we propose a novel event denoising\nframework based on State Space Models (SSMs). Our approach represents events as\n4D event clouds and includes a Coarse Feature Extraction (CFE) module that\nextracts embedding features from both geometric and polarity-aware subspaces.\nThe model is further composed of two essential components: A Spatial Mamba\n(S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM)\nthat captures global temporal dynamics, efficiently propagating spatiotemporal\nfeatures across events. Experiments demonstrate that our method achieves\nstate-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per\n100K events inference time, and a 0.982 accuracy score, outperforming\nTransformer-based methods by 2.08% in denoising accuracy and 36X faster.\n", "link": "http://arxiv.org/abs/2505.05391v1", "date": "2025-05-08", "relevancy": 1.5903, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5528}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5371}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EDmamba%3A%20A%20Simple%20yet%20Effective%20Event%20Denoising%20Method%20with%20State%20Space%0A%20%20Model&body=Title%3A%20EDmamba%3A%20A%20Simple%20yet%20Effective%20Event%20Denoising%20Method%20with%20State%20Space%0A%20%20Model%0AAuthor%3A%20Ciyu%20Ruan%20and%20Zihang%20Gong%20and%20Ruishan%20Guo%20and%20Jingao%20Xu%20and%20Xinlei%20Chen%0AAbstract%3A%20%20%20Event%20cameras%20excel%20in%20high-speed%20vision%20due%20to%20their%20high%20temporal%0Aresolution%2C%20high%20dynamic%20range%2C%20and%20low%20power%20consumption.%20However%2C%20as%20dynamic%0Avision%20sensors%2C%20their%20output%20is%20inherently%20noisy%2C%20making%20efficient%20denoising%0Aessential%20to%20preserve%20their%20ultra-low%20latency%20and%20real-time%20processing%0Acapabilities.%20Existing%20event%20denoising%20methods%20struggle%20with%20a%20critical%0Adilemma%3A%20computationally%20intensive%20approaches%20compromise%20the%20sensor%27s%0Ahigh-speed%20advantage%2C%20while%20lightweight%20methods%20often%20lack%20robustness%20across%0Avarying%20noise%20levels.%20To%20address%20this%2C%20we%20propose%20a%20novel%20event%20denoising%0Aframework%20based%20on%20State%20Space%20Models%20%28SSMs%29.%20Our%20approach%20represents%20events%20as%0A4D%20event%20clouds%20and%20includes%20a%20Coarse%20Feature%20Extraction%20%28CFE%29%20module%20that%0Aextracts%20embedding%20features%20from%20both%20geometric%20and%20polarity-aware%20subspaces.%0AThe%20model%20is%20further%20composed%20of%20two%20essential%20components%3A%20A%20Spatial%20Mamba%0A%28S-SSM%29%20that%20models%20local%20geometric%20structures%20and%20a%20Temporal%20Mamba%20%28T-SSM%29%0Athat%20captures%20global%20temporal%20dynamics%2C%20efficiently%20propagating%20spatiotemporal%0Afeatures%20across%20events.%20Experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20accuracy%20and%20efficiency%2C%20with%2088.89K%20parameters%2C%200.0685s%20per%0A100K%20events%20inference%20time%2C%20and%20a%200.982%20accuracy%20score%2C%20outperforming%0ATransformer-based%20methods%20by%202.08%25%20in%20denoising%20accuracy%20and%2036X%20faster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEDmamba%253A%2520A%2520Simple%2520yet%2520Effective%2520Event%2520Denoising%2520Method%2520with%2520State%2520Space%250A%2520%2520Model%26entry.906535625%3DCiyu%2520Ruan%2520and%2520Zihang%2520Gong%2520and%2520Ruishan%2520Guo%2520and%2520Jingao%2520Xu%2520and%2520Xinlei%2520Chen%26entry.1292438233%3D%2520%2520Event%2520cameras%2520excel%2520in%2520high-speed%2520vision%2520due%2520to%2520their%2520high%2520temporal%250Aresolution%252C%2520high%2520dynamic%2520range%252C%2520and%2520low%2520power%2520consumption.%2520However%252C%2520as%2520dynamic%250Avision%2520sensors%252C%2520their%2520output%2520is%2520inherently%2520noisy%252C%2520making%2520efficient%2520denoising%250Aessential%2520to%2520preserve%2520their%2520ultra-low%2520latency%2520and%2520real-time%2520processing%250Acapabilities.%2520Existing%2520event%2520denoising%2520methods%2520struggle%2520with%2520a%2520critical%250Adilemma%253A%2520computationally%2520intensive%2520approaches%2520compromise%2520the%2520sensor%2527s%250Ahigh-speed%2520advantage%252C%2520while%2520lightweight%2520methods%2520often%2520lack%2520robustness%2520across%250Avarying%2520noise%2520levels.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520event%2520denoising%250Aframework%2520based%2520on%2520State%2520Space%2520Models%2520%2528SSMs%2529.%2520Our%2520approach%2520represents%2520events%2520as%250A4D%2520event%2520clouds%2520and%2520includes%2520a%2520Coarse%2520Feature%2520Extraction%2520%2528CFE%2529%2520module%2520that%250Aextracts%2520embedding%2520features%2520from%2520both%2520geometric%2520and%2520polarity-aware%2520subspaces.%250AThe%2520model%2520is%2520further%2520composed%2520of%2520two%2520essential%2520components%253A%2520A%2520Spatial%2520Mamba%250A%2528S-SSM%2529%2520that%2520models%2520local%2520geometric%2520structures%2520and%2520a%2520Temporal%2520Mamba%2520%2528T-SSM%2529%250Athat%2520captures%2520global%2520temporal%2520dynamics%252C%2520efficiently%2520propagating%2520spatiotemporal%250Afeatures%2520across%2520events.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520accuracy%2520and%2520efficiency%252C%2520with%252088.89K%2520parameters%252C%25200.0685s%2520per%250A100K%2520events%2520inference%2520time%252C%2520and%2520a%25200.982%2520accuracy%2520score%252C%2520outperforming%250ATransformer-based%2520methods%2520by%25202.08%2525%2520in%2520denoising%2520accuracy%2520and%252036X%2520faster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EDmamba%3A%20A%20Simple%20yet%20Effective%20Event%20Denoising%20Method%20with%20State%20Space%0A%20%20Model&entry.906535625=Ciyu%20Ruan%20and%20Zihang%20Gong%20and%20Ruishan%20Guo%20and%20Jingao%20Xu%20and%20Xinlei%20Chen&entry.1292438233=%20%20Event%20cameras%20excel%20in%20high-speed%20vision%20due%20to%20their%20high%20temporal%0Aresolution%2C%20high%20dynamic%20range%2C%20and%20low%20power%20consumption.%20However%2C%20as%20dynamic%0Avision%20sensors%2C%20their%20output%20is%20inherently%20noisy%2C%20making%20efficient%20denoising%0Aessential%20to%20preserve%20their%20ultra-low%20latency%20and%20real-time%20processing%0Acapabilities.%20Existing%20event%20denoising%20methods%20struggle%20with%20a%20critical%0Adilemma%3A%20computationally%20intensive%20approaches%20compromise%20the%20sensor%27s%0Ahigh-speed%20advantage%2C%20while%20lightweight%20methods%20often%20lack%20robustness%20across%0Avarying%20noise%20levels.%20To%20address%20this%2C%20we%20propose%20a%20novel%20event%20denoising%0Aframework%20based%20on%20State%20Space%20Models%20%28SSMs%29.%20Our%20approach%20represents%20events%20as%0A4D%20event%20clouds%20and%20includes%20a%20Coarse%20Feature%20Extraction%20%28CFE%29%20module%20that%0Aextracts%20embedding%20features%20from%20both%20geometric%20and%20polarity-aware%20subspaces.%0AThe%20model%20is%20further%20composed%20of%20two%20essential%20components%3A%20A%20Spatial%20Mamba%0A%28S-SSM%29%20that%20models%20local%20geometric%20structures%20and%20a%20Temporal%20Mamba%20%28T-SSM%29%0Athat%20captures%20global%20temporal%20dynamics%2C%20efficiently%20propagating%20spatiotemporal%0Afeatures%20across%20events.%20Experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20accuracy%20and%20efficiency%2C%20with%2088.89K%20parameters%2C%200.0685s%20per%0A100K%20events%20inference%20time%2C%20and%20a%200.982%20accuracy%20score%2C%20outperforming%0ATransformer-based%20methods%20by%202.08%25%20in%20denoising%20accuracy%20and%2036X%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05391v1&entry.124074799=Read"},
{"title": "Societal and technological progress as sewing an ever-growing,\n  ever-changing, patchy, and polychrome quilt", "author": "Joel Z. Leibo and Alexander Sasha Vezhnevets and William A. Cunningham and S\u00e9bastien Krier and Manfred Diaz and Simon Osindero", "abstract": "  Artificial Intelligence (AI) systems are increasingly placed in positions\nwhere their decisions have real consequences, e.g., moderating online spaces,\nconducting research, and advising on policy. Ensuring they operate in a safe\nand ethically acceptable fashion is thus critical. However, most solutions have\nbeen a form of one-size-fits-all \"alignment\". We are worried that such systems,\nwhich overlook enduring moral diversity, will spark resistance, erode trust,\nand destabilize our institutions. This paper traces the underlying problem to\nan often-unstated Axiom of Rational Convergence: the idea that under ideal\nconditions, rational agents will converge in the limit of conversation on a\nsingle ethics. Treating that premise as both optional and doubtful, we propose\nwhat we call the appropriateness framework: an alternative approach grounded in\nconflict theory, cultural evolution, multi-agent systems, and institutional\neconomics. The appropriateness framework treats persistent disagreement as the\nnormal case and designs for it by applying four principles: (1) contextual\ngrounding, (2) community customization, (3) continual adaptation, and (4)\npolycentric governance. We argue here that adopting these design principles is\na good way to shift the main alignment metaphor from moral unification to a\nmore productive metaphor of conflict management, and that taking this step is\nboth desirable and urgent.\n", "link": "http://arxiv.org/abs/2505.05197v1", "date": "2025-05-08", "relevancy": 1.3933, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4941}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4291}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Societal%20and%20technological%20progress%20as%20sewing%20an%20ever-growing%2C%0A%20%20ever-changing%2C%20patchy%2C%20and%20polychrome%20quilt&body=Title%3A%20Societal%20and%20technological%20progress%20as%20sewing%20an%20ever-growing%2C%0A%20%20ever-changing%2C%20patchy%2C%20and%20polychrome%20quilt%0AAuthor%3A%20Joel%20Z.%20Leibo%20and%20Alexander%20Sasha%20Vezhnevets%20and%20William%20A.%20Cunningham%20and%20S%C3%A9bastien%20Krier%20and%20Manfred%20Diaz%20and%20Simon%20Osindero%0AAbstract%3A%20%20%20Artificial%20Intelligence%20%28AI%29%20systems%20are%20increasingly%20placed%20in%20positions%0Awhere%20their%20decisions%20have%20real%20consequences%2C%20e.g.%2C%20moderating%20online%20spaces%2C%0Aconducting%20research%2C%20and%20advising%20on%20policy.%20Ensuring%20they%20operate%20in%20a%20safe%0Aand%20ethically%20acceptable%20fashion%20is%20thus%20critical.%20However%2C%20most%20solutions%20have%0Abeen%20a%20form%20of%20one-size-fits-all%20%22alignment%22.%20We%20are%20worried%20that%20such%20systems%2C%0Awhich%20overlook%20enduring%20moral%20diversity%2C%20will%20spark%20resistance%2C%20erode%20trust%2C%0Aand%20destabilize%20our%20institutions.%20This%20paper%20traces%20the%20underlying%20problem%20to%0Aan%20often-unstated%20Axiom%20of%20Rational%20Convergence%3A%20the%20idea%20that%20under%20ideal%0Aconditions%2C%20rational%20agents%20will%20converge%20in%20the%20limit%20of%20conversation%20on%20a%0Asingle%20ethics.%20Treating%20that%20premise%20as%20both%20optional%20and%20doubtful%2C%20we%20propose%0Awhat%20we%20call%20the%20appropriateness%20framework%3A%20an%20alternative%20approach%20grounded%20in%0Aconflict%20theory%2C%20cultural%20evolution%2C%20multi-agent%20systems%2C%20and%20institutional%0Aeconomics.%20The%20appropriateness%20framework%20treats%20persistent%20disagreement%20as%20the%0Anormal%20case%20and%20designs%20for%20it%20by%20applying%20four%20principles%3A%20%281%29%20contextual%0Agrounding%2C%20%282%29%20community%20customization%2C%20%283%29%20continual%20adaptation%2C%20and%20%284%29%0Apolycentric%20governance.%20We%20argue%20here%20that%20adopting%20these%20design%20principles%20is%0Aa%20good%20way%20to%20shift%20the%20main%20alignment%20metaphor%20from%20moral%20unification%20to%20a%0Amore%20productive%20metaphor%20of%20conflict%20management%2C%20and%20that%20taking%20this%20step%20is%0Aboth%20desirable%20and%20urgent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocietal%2520and%2520technological%2520progress%2520as%2520sewing%2520an%2520ever-growing%252C%250A%2520%2520ever-changing%252C%2520patchy%252C%2520and%2520polychrome%2520quilt%26entry.906535625%3DJoel%2520Z.%2520Leibo%2520and%2520Alexander%2520Sasha%2520Vezhnevets%2520and%2520William%2520A.%2520Cunningham%2520and%2520S%25C3%25A9bastien%2520Krier%2520and%2520Manfred%2520Diaz%2520and%2520Simon%2520Osindero%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520%2528AI%2529%2520systems%2520are%2520increasingly%2520placed%2520in%2520positions%250Awhere%2520their%2520decisions%2520have%2520real%2520consequences%252C%2520e.g.%252C%2520moderating%2520online%2520spaces%252C%250Aconducting%2520research%252C%2520and%2520advising%2520on%2520policy.%2520Ensuring%2520they%2520operate%2520in%2520a%2520safe%250Aand%2520ethically%2520acceptable%2520fashion%2520is%2520thus%2520critical.%2520However%252C%2520most%2520solutions%2520have%250Abeen%2520a%2520form%2520of%2520one-size-fits-all%2520%2522alignment%2522.%2520We%2520are%2520worried%2520that%2520such%2520systems%252C%250Awhich%2520overlook%2520enduring%2520moral%2520diversity%252C%2520will%2520spark%2520resistance%252C%2520erode%2520trust%252C%250Aand%2520destabilize%2520our%2520institutions.%2520This%2520paper%2520traces%2520the%2520underlying%2520problem%2520to%250Aan%2520often-unstated%2520Axiom%2520of%2520Rational%2520Convergence%253A%2520the%2520idea%2520that%2520under%2520ideal%250Aconditions%252C%2520rational%2520agents%2520will%2520converge%2520in%2520the%2520limit%2520of%2520conversation%2520on%2520a%250Asingle%2520ethics.%2520Treating%2520that%2520premise%2520as%2520both%2520optional%2520and%2520doubtful%252C%2520we%2520propose%250Awhat%2520we%2520call%2520the%2520appropriateness%2520framework%253A%2520an%2520alternative%2520approach%2520grounded%2520in%250Aconflict%2520theory%252C%2520cultural%2520evolution%252C%2520multi-agent%2520systems%252C%2520and%2520institutional%250Aeconomics.%2520The%2520appropriateness%2520framework%2520treats%2520persistent%2520disagreement%2520as%2520the%250Anormal%2520case%2520and%2520designs%2520for%2520it%2520by%2520applying%2520four%2520principles%253A%2520%25281%2529%2520contextual%250Agrounding%252C%2520%25282%2529%2520community%2520customization%252C%2520%25283%2529%2520continual%2520adaptation%252C%2520and%2520%25284%2529%250Apolycentric%2520governance.%2520We%2520argue%2520here%2520that%2520adopting%2520these%2520design%2520principles%2520is%250Aa%2520good%2520way%2520to%2520shift%2520the%2520main%2520alignment%2520metaphor%2520from%2520moral%2520unification%2520to%2520a%250Amore%2520productive%2520metaphor%2520of%2520conflict%2520management%252C%2520and%2520that%2520taking%2520this%2520step%2520is%250Aboth%2520desirable%2520and%2520urgent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Societal%20and%20technological%20progress%20as%20sewing%20an%20ever-growing%2C%0A%20%20ever-changing%2C%20patchy%2C%20and%20polychrome%20quilt&entry.906535625=Joel%20Z.%20Leibo%20and%20Alexander%20Sasha%20Vezhnevets%20and%20William%20A.%20Cunningham%20and%20S%C3%A9bastien%20Krier%20and%20Manfred%20Diaz%20and%20Simon%20Osindero&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20systems%20are%20increasingly%20placed%20in%20positions%0Awhere%20their%20decisions%20have%20real%20consequences%2C%20e.g.%2C%20moderating%20online%20spaces%2C%0Aconducting%20research%2C%20and%20advising%20on%20policy.%20Ensuring%20they%20operate%20in%20a%20safe%0Aand%20ethically%20acceptable%20fashion%20is%20thus%20critical.%20However%2C%20most%20solutions%20have%0Abeen%20a%20form%20of%20one-size-fits-all%20%22alignment%22.%20We%20are%20worried%20that%20such%20systems%2C%0Awhich%20overlook%20enduring%20moral%20diversity%2C%20will%20spark%20resistance%2C%20erode%20trust%2C%0Aand%20destabilize%20our%20institutions.%20This%20paper%20traces%20the%20underlying%20problem%20to%0Aan%20often-unstated%20Axiom%20of%20Rational%20Convergence%3A%20the%20idea%20that%20under%20ideal%0Aconditions%2C%20rational%20agents%20will%20converge%20in%20the%20limit%20of%20conversation%20on%20a%0Asingle%20ethics.%20Treating%20that%20premise%20as%20both%20optional%20and%20doubtful%2C%20we%20propose%0Awhat%20we%20call%20the%20appropriateness%20framework%3A%20an%20alternative%20approach%20grounded%20in%0Aconflict%20theory%2C%20cultural%20evolution%2C%20multi-agent%20systems%2C%20and%20institutional%0Aeconomics.%20The%20appropriateness%20framework%20treats%20persistent%20disagreement%20as%20the%0Anormal%20case%20and%20designs%20for%20it%20by%20applying%20four%20principles%3A%20%281%29%20contextual%0Agrounding%2C%20%282%29%20community%20customization%2C%20%283%29%20continual%20adaptation%2C%20and%20%284%29%0Apolycentric%20governance.%20We%20argue%20here%20that%20adopting%20these%20design%20principles%20is%0Aa%20good%20way%20to%20shift%20the%20main%20alignment%20metaphor%20from%20moral%20unification%20to%20a%0Amore%20productive%20metaphor%20of%20conflict%20management%2C%20and%20that%20taking%20this%20step%20is%0Aboth%20desirable%20and%20urgent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05197v1&entry.124074799=Read"},
{"title": "Fast Whole-Body Strain Regulation in Continuum Robots", "author": "Lekan Molu", "abstract": "  We propose reaching steps towards the real-time strain control of\nmultiphysics, multiscale continuum soft robots. To study this problem\nfundamentally, we ground ourselves in a model-based control setting enabled by\nmathematically precise dynamics of a soft robot prototype. Poised to integrate,\nrather than reject, inherent mechanical nonlinearities for embodied compliance,\nwe first separate the original robot dynamics into separate subdynamics --\naided by a perturbing time-scale separation parameter. Second, we prescribe a\nset of stabilizing nonlinear backstepping controllers for regulating the\nresulting subsystems' strain dynamics. Third, we study the interconnected\nsingularly perturbed system by analyzing and establishing its stability.\nFourth, our theories are backed up by fast numerical results on a single arm of\nthe Octopus robot arm. We demonstrate strain regulation to equilibrium, in a\nsignificantly reduced time, of the whole-body reduced-order dynamics of an\ninfinite degrees-of-freedom soft robot. This paper communicates our thinking\nwithin the backdrop of embodied intelligence: it informs our conceptualization,\nformulation, computational setup, and yields improved control performance for\ninfinite degrees-of-freedom soft robots.\n", "link": "http://arxiv.org/abs/2312.06039v3", "date": "2025-05-08", "relevancy": 1.5503, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5351}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5167}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Whole-Body%20Strain%20Regulation%20in%20Continuum%20Robots&body=Title%3A%20Fast%20Whole-Body%20Strain%20Regulation%20in%20Continuum%20Robots%0AAuthor%3A%20Lekan%20Molu%0AAbstract%3A%20%20%20We%20propose%20reaching%20steps%20towards%20the%20real-time%20strain%20control%20of%0Amultiphysics%2C%20multiscale%20continuum%20soft%20robots.%20To%20study%20this%20problem%0Afundamentally%2C%20we%20ground%20ourselves%20in%20a%20model-based%20control%20setting%20enabled%20by%0Amathematically%20precise%20dynamics%20of%20a%20soft%20robot%20prototype.%20Poised%20to%20integrate%2C%0Arather%20than%20reject%2C%20inherent%20mechanical%20nonlinearities%20for%20embodied%20compliance%2C%0Awe%20first%20separate%20the%20original%20robot%20dynamics%20into%20separate%20subdynamics%20--%0Aaided%20by%20a%20perturbing%20time-scale%20separation%20parameter.%20Second%2C%20we%20prescribe%20a%0Aset%20of%20stabilizing%20nonlinear%20backstepping%20controllers%20for%20regulating%20the%0Aresulting%20subsystems%27%20strain%20dynamics.%20Third%2C%20we%20study%20the%20interconnected%0Asingularly%20perturbed%20system%20by%20analyzing%20and%20establishing%20its%20stability.%0AFourth%2C%20our%20theories%20are%20backed%20up%20by%20fast%20numerical%20results%20on%20a%20single%20arm%20of%0Athe%20Octopus%20robot%20arm.%20We%20demonstrate%20strain%20regulation%20to%20equilibrium%2C%20in%20a%0Asignificantly%20reduced%20time%2C%20of%20the%20whole-body%20reduced-order%20dynamics%20of%20an%0Ainfinite%20degrees-of-freedom%20soft%20robot.%20This%20paper%20communicates%20our%20thinking%0Awithin%20the%20backdrop%20of%20embodied%20intelligence%3A%20it%20informs%20our%20conceptualization%2C%0Aformulation%2C%20computational%20setup%2C%20and%20yields%20improved%20control%20performance%20for%0Ainfinite%20degrees-of-freedom%20soft%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06039v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Whole-Body%2520Strain%2520Regulation%2520in%2520Continuum%2520Robots%26entry.906535625%3DLekan%2520Molu%26entry.1292438233%3D%2520%2520We%2520propose%2520reaching%2520steps%2520towards%2520the%2520real-time%2520strain%2520control%2520of%250Amultiphysics%252C%2520multiscale%2520continuum%2520soft%2520robots.%2520To%2520study%2520this%2520problem%250Afundamentally%252C%2520we%2520ground%2520ourselves%2520in%2520a%2520model-based%2520control%2520setting%2520enabled%2520by%250Amathematically%2520precise%2520dynamics%2520of%2520a%2520soft%2520robot%2520prototype.%2520Poised%2520to%2520integrate%252C%250Arather%2520than%2520reject%252C%2520inherent%2520mechanical%2520nonlinearities%2520for%2520embodied%2520compliance%252C%250Awe%2520first%2520separate%2520the%2520original%2520robot%2520dynamics%2520into%2520separate%2520subdynamics%2520--%250Aaided%2520by%2520a%2520perturbing%2520time-scale%2520separation%2520parameter.%2520Second%252C%2520we%2520prescribe%2520a%250Aset%2520of%2520stabilizing%2520nonlinear%2520backstepping%2520controllers%2520for%2520regulating%2520the%250Aresulting%2520subsystems%2527%2520strain%2520dynamics.%2520Third%252C%2520we%2520study%2520the%2520interconnected%250Asingularly%2520perturbed%2520system%2520by%2520analyzing%2520and%2520establishing%2520its%2520stability.%250AFourth%252C%2520our%2520theories%2520are%2520backed%2520up%2520by%2520fast%2520numerical%2520results%2520on%2520a%2520single%2520arm%2520of%250Athe%2520Octopus%2520robot%2520arm.%2520We%2520demonstrate%2520strain%2520regulation%2520to%2520equilibrium%252C%2520in%2520a%250Asignificantly%2520reduced%2520time%252C%2520of%2520the%2520whole-body%2520reduced-order%2520dynamics%2520of%2520an%250Ainfinite%2520degrees-of-freedom%2520soft%2520robot.%2520This%2520paper%2520communicates%2520our%2520thinking%250Awithin%2520the%2520backdrop%2520of%2520embodied%2520intelligence%253A%2520it%2520informs%2520our%2520conceptualization%252C%250Aformulation%252C%2520computational%2520setup%252C%2520and%2520yields%2520improved%2520control%2520performance%2520for%250Ainfinite%2520degrees-of-freedom%2520soft%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06039v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Whole-Body%20Strain%20Regulation%20in%20Continuum%20Robots&entry.906535625=Lekan%20Molu&entry.1292438233=%20%20We%20propose%20reaching%20steps%20towards%20the%20real-time%20strain%20control%20of%0Amultiphysics%2C%20multiscale%20continuum%20soft%20robots.%20To%20study%20this%20problem%0Afundamentally%2C%20we%20ground%20ourselves%20in%20a%20model-based%20control%20setting%20enabled%20by%0Amathematically%20precise%20dynamics%20of%20a%20soft%20robot%20prototype.%20Poised%20to%20integrate%2C%0Arather%20than%20reject%2C%20inherent%20mechanical%20nonlinearities%20for%20embodied%20compliance%2C%0Awe%20first%20separate%20the%20original%20robot%20dynamics%20into%20separate%20subdynamics%20--%0Aaided%20by%20a%20perturbing%20time-scale%20separation%20parameter.%20Second%2C%20we%20prescribe%20a%0Aset%20of%20stabilizing%20nonlinear%20backstepping%20controllers%20for%20regulating%20the%0Aresulting%20subsystems%27%20strain%20dynamics.%20Third%2C%20we%20study%20the%20interconnected%0Asingularly%20perturbed%20system%20by%20analyzing%20and%20establishing%20its%20stability.%0AFourth%2C%20our%20theories%20are%20backed%20up%20by%20fast%20numerical%20results%20on%20a%20single%20arm%20of%0Athe%20Octopus%20robot%20arm.%20We%20demonstrate%20strain%20regulation%20to%20equilibrium%2C%20in%20a%0Asignificantly%20reduced%20time%2C%20of%20the%20whole-body%20reduced-order%20dynamics%20of%20an%0Ainfinite%20degrees-of-freedom%20soft%20robot.%20This%20paper%20communicates%20our%20thinking%0Awithin%20the%20backdrop%20of%20embodied%20intelligence%3A%20it%20informs%20our%20conceptualization%2C%0Aformulation%2C%20computational%20setup%2C%20and%20yields%20improved%20control%20performance%20for%0Ainfinite%20degrees-of-freedom%20soft%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06039v3&entry.124074799=Read"},
{"title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation", "author": "Quang P. M. Pham and Khoi T. N. Nguyen and Nhi H. Doan and Cuong A. Pham and Kentaro Inui and Dezhen Song", "abstract": "  Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics.\n", "link": "http://arxiv.org/abs/2505.00831v3", "date": "2025-05-08", "relevancy": 1.6058, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5661}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SmallPlan%3A%20Leverage%20Small%20Language%20Models%20for%20Sequential%20Path%20Planning%0A%20%20with%20Simulation-Powered%2C%20LLM-Guided%20Distillation&body=Title%3A%20SmallPlan%3A%20Leverage%20Small%20Language%20Models%20for%20Sequential%20Path%20Planning%0A%20%20with%20Simulation-Powered%2C%20LLM-Guided%20Distillation%0AAuthor%3A%20Quang%20P.%20M.%20Pham%20and%20Khoi%20T.%20N.%20Nguyen%20and%20Nhi%20H.%20Doan%20and%20Cuong%20A.%20Pham%20and%20Kentaro%20Inui%20and%20Dezhen%20Song%0AAbstract%3A%20%20%20Efficient%20path%20planning%20in%20robotics%2C%20particularly%20within%20large-scale%2C%20dynamic%0Aenvironments%2C%20remains%20a%20significant%20hurdle.%20While%20Large%20Language%20Models%20%28LLMs%29%0Aoffer%20strong%20reasoning%20capabilities%2C%20their%20high%20computational%20cost%20and%20limited%0Aadaptability%20in%20dynamic%20scenarios%20hinder%20real-time%20deployment%20on%20edge%20devices.%0AWe%20present%20SmallPlan%20--%20a%20novel%20framework%20leveraging%20LLMs%20as%20teacher%20models%20to%0Atrain%20lightweight%20Small%20Language%20Models%20%28SLMs%29%20for%20high-level%20path%20planning%0Atasks.%20In%20SmallPlan%2C%20the%20SLMs%20provide%20optimal%20action%20sequences%20to%20navigate%0Aacross%20scene%20graphs%20that%20compactly%20represent%20full-scaled%203D%20scenes.%20The%20SLMs%0Aare%20trained%20in%20a%20simulation-powered%2C%20interleaved%20manner%20with%20LLM-guided%0Asupervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20This%20strategy%20not%0Aonly%20enables%20SLMs%20to%20successfully%20complete%20navigation%20tasks%20but%20also%20makes%20them%0Aaware%20of%20important%20factors%20like%20travel%20distance%20and%20number%20of%20trials.%20Through%0Aexperiments%2C%20we%20demonstrate%20that%20the%20fine-tuned%20SLMs%20perform%20competitively%20with%0Alarger%20models%20like%20GPT-4o%20on%20sequential%20path%20planning%2C%20without%20suffering%20from%0Ahallucination%20and%20overfitting.%20SmallPlan%20is%20resource-efficient%2C%20making%20it%0Awell-suited%20for%20edge-device%20deployment%20and%20advancing%20practical%20autonomous%0Arobotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00831v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmallPlan%253A%2520Leverage%2520Small%2520Language%2520Models%2520for%2520Sequential%2520Path%2520Planning%250A%2520%2520with%2520Simulation-Powered%252C%2520LLM-Guided%2520Distillation%26entry.906535625%3DQuang%2520P.%2520M.%2520Pham%2520and%2520Khoi%2520T.%2520N.%2520Nguyen%2520and%2520Nhi%2520H.%2520Doan%2520and%2520Cuong%2520A.%2520Pham%2520and%2520Kentaro%2520Inui%2520and%2520Dezhen%2520Song%26entry.1292438233%3D%2520%2520Efficient%2520path%2520planning%2520in%2520robotics%252C%2520particularly%2520within%2520large-scale%252C%2520dynamic%250Aenvironments%252C%2520remains%2520a%2520significant%2520hurdle.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Aoffer%2520strong%2520reasoning%2520capabilities%252C%2520their%2520high%2520computational%2520cost%2520and%2520limited%250Aadaptability%2520in%2520dynamic%2520scenarios%2520hinder%2520real-time%2520deployment%2520on%2520edge%2520devices.%250AWe%2520present%2520SmallPlan%2520--%2520a%2520novel%2520framework%2520leveraging%2520LLMs%2520as%2520teacher%2520models%2520to%250Atrain%2520lightweight%2520Small%2520Language%2520Models%2520%2528SLMs%2529%2520for%2520high-level%2520path%2520planning%250Atasks.%2520In%2520SmallPlan%252C%2520the%2520SLMs%2520provide%2520optimal%2520action%2520sequences%2520to%2520navigate%250Aacross%2520scene%2520graphs%2520that%2520compactly%2520represent%2520full-scaled%25203D%2520scenes.%2520The%2520SLMs%250Aare%2520trained%2520in%2520a%2520simulation-powered%252C%2520interleaved%2520manner%2520with%2520LLM-guided%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520reinforcement%2520learning%2520%2528RL%2529.%2520This%2520strategy%2520not%250Aonly%2520enables%2520SLMs%2520to%2520successfully%2520complete%2520navigation%2520tasks%2520but%2520also%2520makes%2520them%250Aaware%2520of%2520important%2520factors%2520like%2520travel%2520distance%2520and%2520number%2520of%2520trials.%2520Through%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520the%2520fine-tuned%2520SLMs%2520perform%2520competitively%2520with%250Alarger%2520models%2520like%2520GPT-4o%2520on%2520sequential%2520path%2520planning%252C%2520without%2520suffering%2520from%250Ahallucination%2520and%2520overfitting.%2520SmallPlan%2520is%2520resource-efficient%252C%2520making%2520it%250Awell-suited%2520for%2520edge-device%2520deployment%2520and%2520advancing%2520practical%2520autonomous%250Arobotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00831v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SmallPlan%3A%20Leverage%20Small%20Language%20Models%20for%20Sequential%20Path%20Planning%0A%20%20with%20Simulation-Powered%2C%20LLM-Guided%20Distillation&entry.906535625=Quang%20P.%20M.%20Pham%20and%20Khoi%20T.%20N.%20Nguyen%20and%20Nhi%20H.%20Doan%20and%20Cuong%20A.%20Pham%20and%20Kentaro%20Inui%20and%20Dezhen%20Song&entry.1292438233=%20%20Efficient%20path%20planning%20in%20robotics%2C%20particularly%20within%20large-scale%2C%20dynamic%0Aenvironments%2C%20remains%20a%20significant%20hurdle.%20While%20Large%20Language%20Models%20%28LLMs%29%0Aoffer%20strong%20reasoning%20capabilities%2C%20their%20high%20computational%20cost%20and%20limited%0Aadaptability%20in%20dynamic%20scenarios%20hinder%20real-time%20deployment%20on%20edge%20devices.%0AWe%20present%20SmallPlan%20--%20a%20novel%20framework%20leveraging%20LLMs%20as%20teacher%20models%20to%0Atrain%20lightweight%20Small%20Language%20Models%20%28SLMs%29%20for%20high-level%20path%20planning%0Atasks.%20In%20SmallPlan%2C%20the%20SLMs%20provide%20optimal%20action%20sequences%20to%20navigate%0Aacross%20scene%20graphs%20that%20compactly%20represent%20full-scaled%203D%20scenes.%20The%20SLMs%0Aare%20trained%20in%20a%20simulation-powered%2C%20interleaved%20manner%20with%20LLM-guided%0Asupervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20This%20strategy%20not%0Aonly%20enables%20SLMs%20to%20successfully%20complete%20navigation%20tasks%20but%20also%20makes%20them%0Aaware%20of%20important%20factors%20like%20travel%20distance%20and%20number%20of%20trials.%20Through%0Aexperiments%2C%20we%20demonstrate%20that%20the%20fine-tuned%20SLMs%20perform%20competitively%20with%0Alarger%20models%20like%20GPT-4o%20on%20sequential%20path%20planning%2C%20without%20suffering%20from%0Ahallucination%20and%20overfitting.%20SmallPlan%20is%20resource-efficient%2C%20making%20it%0Awell-suited%20for%20edge-device%20deployment%20and%20advancing%20practical%20autonomous%0Arobotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00831v3&entry.124074799=Read"},
{"title": "Is there a half-life for the success rates of AI agents?", "author": "Toby Ord", "abstract": "  Building on the recent empirical work of Kwa et al. (2025), I show that\nwithin their suite of research-engineering tasks the performance of AI agents\non longer-duration tasks can be explained by an extremely simple mathematical\nmodel -- a constant rate of failing during each minute a human would take to do\nthe task. This implies an exponentially declining success rate with the length\nof the task and that each agent could be characterised by its own half-life.\nThis empirical regularity allows us to estimate the success rate for an agent\nat different task lengths. And the fact that this model is a good fit for the\ndata is suggestive of the underlying causes of failure on longer tasks -- that\nthey involve increasingly large sets of subtasks where failing any one fails\nthe task. Whether this model applies more generally on other suites of tasks is\nunknown and an important subject for further work.\n", "link": "http://arxiv.org/abs/2505.05115v1", "date": "2025-05-08", "relevancy": 1.8055, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4913}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4478}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20there%20a%20half-life%20for%20the%20success%20rates%20of%20AI%20agents%3F&body=Title%3A%20Is%20there%20a%20half-life%20for%20the%20success%20rates%20of%20AI%20agents%3F%0AAuthor%3A%20Toby%20Ord%0AAbstract%3A%20%20%20Building%20on%20the%20recent%20empirical%20work%20of%20Kwa%20et%20al.%20%282025%29%2C%20I%20show%20that%0Awithin%20their%20suite%20of%20research-engineering%20tasks%20the%20performance%20of%20AI%20agents%0Aon%20longer-duration%20tasks%20can%20be%20explained%20by%20an%20extremely%20simple%20mathematical%0Amodel%20--%20a%20constant%20rate%20of%20failing%20during%20each%20minute%20a%20human%20would%20take%20to%20do%0Athe%20task.%20This%20implies%20an%20exponentially%20declining%20success%20rate%20with%20the%20length%0Aof%20the%20task%20and%20that%20each%20agent%20could%20be%20characterised%20by%20its%20own%20half-life.%0AThis%20empirical%20regularity%20allows%20us%20to%20estimate%20the%20success%20rate%20for%20an%20agent%0Aat%20different%20task%20lengths.%20And%20the%20fact%20that%20this%20model%20is%20a%20good%20fit%20for%20the%0Adata%20is%20suggestive%20of%20the%20underlying%20causes%20of%20failure%20on%20longer%20tasks%20--%20that%0Athey%20involve%20increasingly%20large%20sets%20of%20subtasks%20where%20failing%20any%20one%20fails%0Athe%20task.%20Whether%20this%20model%20applies%20more%20generally%20on%20other%20suites%20of%20tasks%20is%0Aunknown%20and%20an%20important%20subject%20for%20further%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520there%2520a%2520half-life%2520for%2520the%2520success%2520rates%2520of%2520AI%2520agents%253F%26entry.906535625%3DToby%2520Ord%26entry.1292438233%3D%2520%2520Building%2520on%2520the%2520recent%2520empirical%2520work%2520of%2520Kwa%2520et%2520al.%2520%25282025%2529%252C%2520I%2520show%2520that%250Awithin%2520their%2520suite%2520of%2520research-engineering%2520tasks%2520the%2520performance%2520of%2520AI%2520agents%250Aon%2520longer-duration%2520tasks%2520can%2520be%2520explained%2520by%2520an%2520extremely%2520simple%2520mathematical%250Amodel%2520--%2520a%2520constant%2520rate%2520of%2520failing%2520during%2520each%2520minute%2520a%2520human%2520would%2520take%2520to%2520do%250Athe%2520task.%2520This%2520implies%2520an%2520exponentially%2520declining%2520success%2520rate%2520with%2520the%2520length%250Aof%2520the%2520task%2520and%2520that%2520each%2520agent%2520could%2520be%2520characterised%2520by%2520its%2520own%2520half-life.%250AThis%2520empirical%2520regularity%2520allows%2520us%2520to%2520estimate%2520the%2520success%2520rate%2520for%2520an%2520agent%250Aat%2520different%2520task%2520lengths.%2520And%2520the%2520fact%2520that%2520this%2520model%2520is%2520a%2520good%2520fit%2520for%2520the%250Adata%2520is%2520suggestive%2520of%2520the%2520underlying%2520causes%2520of%2520failure%2520on%2520longer%2520tasks%2520--%2520that%250Athey%2520involve%2520increasingly%2520large%2520sets%2520of%2520subtasks%2520where%2520failing%2520any%2520one%2520fails%250Athe%2520task.%2520Whether%2520this%2520model%2520applies%2520more%2520generally%2520on%2520other%2520suites%2520of%2520tasks%2520is%250Aunknown%2520and%2520an%2520important%2520subject%2520for%2520further%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20there%20a%20half-life%20for%20the%20success%20rates%20of%20AI%20agents%3F&entry.906535625=Toby%20Ord&entry.1292438233=%20%20Building%20on%20the%20recent%20empirical%20work%20of%20Kwa%20et%20al.%20%282025%29%2C%20I%20show%20that%0Awithin%20their%20suite%20of%20research-engineering%20tasks%20the%20performance%20of%20AI%20agents%0Aon%20longer-duration%20tasks%20can%20be%20explained%20by%20an%20extremely%20simple%20mathematical%0Amodel%20--%20a%20constant%20rate%20of%20failing%20during%20each%20minute%20a%20human%20would%20take%20to%20do%0Athe%20task.%20This%20implies%20an%20exponentially%20declining%20success%20rate%20with%20the%20length%0Aof%20the%20task%20and%20that%20each%20agent%20could%20be%20characterised%20by%20its%20own%20half-life.%0AThis%20empirical%20regularity%20allows%20us%20to%20estimate%20the%20success%20rate%20for%20an%20agent%0Aat%20different%20task%20lengths.%20And%20the%20fact%20that%20this%20model%20is%20a%20good%20fit%20for%20the%0Adata%20is%20suggestive%20of%20the%20underlying%20causes%20of%20failure%20on%20longer%20tasks%20--%20that%0Athey%20involve%20increasingly%20large%20sets%20of%20subtasks%20where%20failing%20any%20one%20fails%0Athe%20task.%20Whether%20this%20model%20applies%20more%20generally%20on%20other%20suites%20of%20tasks%20is%0Aunknown%20and%20an%20important%20subject%20for%20further%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05115v1&entry.124074799=Read"},
{"title": "Long-Term Individual Causal Effect Estimation via Identifiable Latent\n  Representation Learning", "author": "Ruichu Cai and Junjie Wan and Weilin Chen and Zeqin Yang and Zijian Li and Peng Zhen and Jiecheng Guo", "abstract": "  Estimating long-term causal effects by combining long-term observational and\nshort-term experimental data is a crucial but challenging problem in many\nreal-world scenarios. In existing methods, several ideal assumptions, e.g.\nlatent unconfoundedness assumption or additive equi-confounding bias\nassumption, are proposed to address the latent confounder problem raised by the\nobservational data. However, in real-world applications, these assumptions are\ntypically violated which limits their practical effectiveness. In this paper,\nwe tackle the problem of estimating the long-term individual causal effects\nwithout the aforementioned assumptions. Specifically, we propose to utilize the\nnatural heterogeneity of data, such as data from multiple sources, to identify\nlatent confounders, thereby significantly avoiding reliance on idealized\nassumptions. Practically, we devise a latent representation learning-based\nestimator of long-term causal effects. Theoretically, we establish the\nidentifiability of latent confounders, with which we further achieve long-term\neffect identification. Extensive experimental studies, conducted on multiple\nsynthetic and semi-synthetic datasets, demonstrate the effectiveness of our\nproposed method.\n", "link": "http://arxiv.org/abs/2505.05192v1", "date": "2025-05-08", "relevancy": 1.4214, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4815}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4678}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Term%20Individual%20Causal%20Effect%20Estimation%20via%20Identifiable%20Latent%0A%20%20Representation%20Learning&body=Title%3A%20Long-Term%20Individual%20Causal%20Effect%20Estimation%20via%20Identifiable%20Latent%0A%20%20Representation%20Learning%0AAuthor%3A%20Ruichu%20Cai%20and%20Junjie%20Wan%20and%20Weilin%20Chen%20and%20Zeqin%20Yang%20and%20Zijian%20Li%20and%20Peng%20Zhen%20and%20Jiecheng%20Guo%0AAbstract%3A%20%20%20Estimating%20long-term%20causal%20effects%20by%20combining%20long-term%20observational%20and%0Ashort-term%20experimental%20data%20is%20a%20crucial%20but%20challenging%20problem%20in%20many%0Areal-world%20scenarios.%20In%20existing%20methods%2C%20several%20ideal%20assumptions%2C%20e.g.%0Alatent%20unconfoundedness%20assumption%20or%20additive%20equi-confounding%20bias%0Aassumption%2C%20are%20proposed%20to%20address%20the%20latent%20confounder%20problem%20raised%20by%20the%0Aobservational%20data.%20However%2C%20in%20real-world%20applications%2C%20these%20assumptions%20are%0Atypically%20violated%20which%20limits%20their%20practical%20effectiveness.%20In%20this%20paper%2C%0Awe%20tackle%20the%20problem%20of%20estimating%20the%20long-term%20individual%20causal%20effects%0Awithout%20the%20aforementioned%20assumptions.%20Specifically%2C%20we%20propose%20to%20utilize%20the%0Anatural%20heterogeneity%20of%20data%2C%20such%20as%20data%20from%20multiple%20sources%2C%20to%20identify%0Alatent%20confounders%2C%20thereby%20significantly%20avoiding%20reliance%20on%20idealized%0Aassumptions.%20Practically%2C%20we%20devise%20a%20latent%20representation%20learning-based%0Aestimator%20of%20long-term%20causal%20effects.%20Theoretically%2C%20we%20establish%20the%0Aidentifiability%20of%20latent%20confounders%2C%20with%20which%20we%20further%20achieve%20long-term%0Aeffect%20identification.%20Extensive%20experimental%20studies%2C%20conducted%20on%20multiple%0Asynthetic%20and%20semi-synthetic%20datasets%2C%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Term%2520Individual%2520Causal%2520Effect%2520Estimation%2520via%2520Identifiable%2520Latent%250A%2520%2520Representation%2520Learning%26entry.906535625%3DRuichu%2520Cai%2520and%2520Junjie%2520Wan%2520and%2520Weilin%2520Chen%2520and%2520Zeqin%2520Yang%2520and%2520Zijian%2520Li%2520and%2520Peng%2520Zhen%2520and%2520Jiecheng%2520Guo%26entry.1292438233%3D%2520%2520Estimating%2520long-term%2520causal%2520effects%2520by%2520combining%2520long-term%2520observational%2520and%250Ashort-term%2520experimental%2520data%2520is%2520a%2520crucial%2520but%2520challenging%2520problem%2520in%2520many%250Areal-world%2520scenarios.%2520In%2520existing%2520methods%252C%2520several%2520ideal%2520assumptions%252C%2520e.g.%250Alatent%2520unconfoundedness%2520assumption%2520or%2520additive%2520equi-confounding%2520bias%250Aassumption%252C%2520are%2520proposed%2520to%2520address%2520the%2520latent%2520confounder%2520problem%2520raised%2520by%2520the%250Aobservational%2520data.%2520However%252C%2520in%2520real-world%2520applications%252C%2520these%2520assumptions%2520are%250Atypically%2520violated%2520which%2520limits%2520their%2520practical%2520effectiveness.%2520In%2520this%2520paper%252C%250Awe%2520tackle%2520the%2520problem%2520of%2520estimating%2520the%2520long-term%2520individual%2520causal%2520effects%250Awithout%2520the%2520aforementioned%2520assumptions.%2520Specifically%252C%2520we%2520propose%2520to%2520utilize%2520the%250Anatural%2520heterogeneity%2520of%2520data%252C%2520such%2520as%2520data%2520from%2520multiple%2520sources%252C%2520to%2520identify%250Alatent%2520confounders%252C%2520thereby%2520significantly%2520avoiding%2520reliance%2520on%2520idealized%250Aassumptions.%2520Practically%252C%2520we%2520devise%2520a%2520latent%2520representation%2520learning-based%250Aestimator%2520of%2520long-term%2520causal%2520effects.%2520Theoretically%252C%2520we%2520establish%2520the%250Aidentifiability%2520of%2520latent%2520confounders%252C%2520with%2520which%2520we%2520further%2520achieve%2520long-term%250Aeffect%2520identification.%2520Extensive%2520experimental%2520studies%252C%2520conducted%2520on%2520multiple%250Asynthetic%2520and%2520semi-synthetic%2520datasets%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Term%20Individual%20Causal%20Effect%20Estimation%20via%20Identifiable%20Latent%0A%20%20Representation%20Learning&entry.906535625=Ruichu%20Cai%20and%20Junjie%20Wan%20and%20Weilin%20Chen%20and%20Zeqin%20Yang%20and%20Zijian%20Li%20and%20Peng%20Zhen%20and%20Jiecheng%20Guo&entry.1292438233=%20%20Estimating%20long-term%20causal%20effects%20by%20combining%20long-term%20observational%20and%0Ashort-term%20experimental%20data%20is%20a%20crucial%20but%20challenging%20problem%20in%20many%0Areal-world%20scenarios.%20In%20existing%20methods%2C%20several%20ideal%20assumptions%2C%20e.g.%0Alatent%20unconfoundedness%20assumption%20or%20additive%20equi-confounding%20bias%0Aassumption%2C%20are%20proposed%20to%20address%20the%20latent%20confounder%20problem%20raised%20by%20the%0Aobservational%20data.%20However%2C%20in%20real-world%20applications%2C%20these%20assumptions%20are%0Atypically%20violated%20which%20limits%20their%20practical%20effectiveness.%20In%20this%20paper%2C%0Awe%20tackle%20the%20problem%20of%20estimating%20the%20long-term%20individual%20causal%20effects%0Awithout%20the%20aforementioned%20assumptions.%20Specifically%2C%20we%20propose%20to%20utilize%20the%0Anatural%20heterogeneity%20of%20data%2C%20such%20as%20data%20from%20multiple%20sources%2C%20to%20identify%0Alatent%20confounders%2C%20thereby%20significantly%20avoiding%20reliance%20on%20idealized%0Aassumptions.%20Practically%2C%20we%20devise%20a%20latent%20representation%20learning-based%0Aestimator%20of%20long-term%20causal%20effects.%20Theoretically%2C%20we%20establish%20the%0Aidentifiability%20of%20latent%20confounders%2C%20with%20which%20we%20further%20achieve%20long-term%0Aeffect%20identification.%20Extensive%20experimental%20studies%2C%20conducted%20on%20multiple%0Asynthetic%20and%20semi-synthetic%20datasets%2C%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05192v1&entry.124074799=Read"},
{"title": "MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via\n  Diffusion Models", "author": "Hongyang Zhu and Haipeng Liu and Bo Fu and Yang Wang", "abstract": "  Multi-object editing aims to modify multiple objects or regions in complex\nscenes while preserving structural coherence. This task faces significant\nchallenges in scenarios involving overlapping or interacting objects: (1)\nInaccurate localization of target objects due to attention misalignment,\nleading to incomplete or misplaced edits; (2) Attribute-object mismatch, where\ncolor or texture changes fail to align with intended regions due to\ncross-attention leakage, creating semantic conflicts (\\textit{e.g.}, color\nbleeding into non-target areas). Existing methods struggle with these\nchallenges: approaches relying on global cross-attention mechanisms suffer from\nattention dilution and spatial interference between objects, while mask-based\nmethods fail to bind attributes to geometrically accurate regions due to\nfeature entanglement in multi-object scenarios. To address these limitations,\nwe propose a training-free, inference-stage optimization approach that enables\nprecise localized image manipulation in complex multi-object scenes, named\nMDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via\ntwo key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention\nwith segmentation masks for precise object positioning, and Color Consistency\nLoss (CCL) amplifies target attribute attention within masks while suppressing\nleakage to adjacent regions. This dual-loss design ensures localized and\ncoherent multi-object edits. Extensive experiments demonstrate that MDE-Edit\noutperforms state-of-the-art methods in editing accuracy and visual quality,\noffering a robust solution for complex multi-object image manipulation tasks.\n", "link": "http://arxiv.org/abs/2505.05101v1", "date": "2025-05-08", "relevancy": 1.249, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6525}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6284}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDE-Edit%3A%20Masked%20Dual-Editing%20for%20Multi-Object%20Image%20Editing%20via%0A%20%20Diffusion%20Models&body=Title%3A%20MDE-Edit%3A%20Masked%20Dual-Editing%20for%20Multi-Object%20Image%20Editing%20via%0A%20%20Diffusion%20Models%0AAuthor%3A%20Hongyang%20Zhu%20and%20Haipeng%20Liu%20and%20Bo%20Fu%20and%20Yang%20Wang%0AAbstract%3A%20%20%20Multi-object%20editing%20aims%20to%20modify%20multiple%20objects%20or%20regions%20in%20complex%0Ascenes%20while%20preserving%20structural%20coherence.%20This%20task%20faces%20significant%0Achallenges%20in%20scenarios%20involving%20overlapping%20or%20interacting%20objects%3A%20%281%29%0AInaccurate%20localization%20of%20target%20objects%20due%20to%20attention%20misalignment%2C%0Aleading%20to%20incomplete%20or%20misplaced%20edits%3B%20%282%29%20Attribute-object%20mismatch%2C%20where%0Acolor%20or%20texture%20changes%20fail%20to%20align%20with%20intended%20regions%20due%20to%0Across-attention%20leakage%2C%20creating%20semantic%20conflicts%20%28%5Ctextit%7Be.g.%7D%2C%20color%0Ableeding%20into%20non-target%20areas%29.%20Existing%20methods%20struggle%20with%20these%0Achallenges%3A%20approaches%20relying%20on%20global%20cross-attention%20mechanisms%20suffer%20from%0Aattention%20dilution%20and%20spatial%20interference%20between%20objects%2C%20while%20mask-based%0Amethods%20fail%20to%20bind%20attributes%20to%20geometrically%20accurate%20regions%20due%20to%0Afeature%20entanglement%20in%20multi-object%20scenarios.%20To%20address%20these%20limitations%2C%0Awe%20propose%20a%20training-free%2C%20inference-stage%20optimization%20approach%20that%20enables%0Aprecise%20localized%20image%20manipulation%20in%20complex%20multi-object%20scenes%2C%20named%0AMDE-Edit.%20MDE-Edit%20optimizes%20the%20noise%20latent%20feature%20in%20diffusion%20models%20via%0Atwo%20key%20losses%3A%20Object%20Alignment%20Loss%20%28OAL%29%20aligns%20multi-layer%20cross-attention%0Awith%20segmentation%20masks%20for%20precise%20object%20positioning%2C%20and%20Color%20Consistency%0ALoss%20%28CCL%29%20amplifies%20target%20attribute%20attention%20within%20masks%20while%20suppressing%0Aleakage%20to%20adjacent%20regions.%20This%20dual-loss%20design%20ensures%20localized%20and%0Acoherent%20multi-object%20edits.%20Extensive%20experiments%20demonstrate%20that%20MDE-Edit%0Aoutperforms%20state-of-the-art%20methods%20in%20editing%20accuracy%20and%20visual%20quality%2C%0Aoffering%20a%20robust%20solution%20for%20complex%20multi-object%20image%20manipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDE-Edit%253A%2520Masked%2520Dual-Editing%2520for%2520Multi-Object%2520Image%2520Editing%2520via%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DHongyang%2520Zhu%2520and%2520Haipeng%2520Liu%2520and%2520Bo%2520Fu%2520and%2520Yang%2520Wang%26entry.1292438233%3D%2520%2520Multi-object%2520editing%2520aims%2520to%2520modify%2520multiple%2520objects%2520or%2520regions%2520in%2520complex%250Ascenes%2520while%2520preserving%2520structural%2520coherence.%2520This%2520task%2520faces%2520significant%250Achallenges%2520in%2520scenarios%2520involving%2520overlapping%2520or%2520interacting%2520objects%253A%2520%25281%2529%250AInaccurate%2520localization%2520of%2520target%2520objects%2520due%2520to%2520attention%2520misalignment%252C%250Aleading%2520to%2520incomplete%2520or%2520misplaced%2520edits%253B%2520%25282%2529%2520Attribute-object%2520mismatch%252C%2520where%250Acolor%2520or%2520texture%2520changes%2520fail%2520to%2520align%2520with%2520intended%2520regions%2520due%2520to%250Across-attention%2520leakage%252C%2520creating%2520semantic%2520conflicts%2520%2528%255Ctextit%257Be.g.%257D%252C%2520color%250Ableeding%2520into%2520non-target%2520areas%2529.%2520Existing%2520methods%2520struggle%2520with%2520these%250Achallenges%253A%2520approaches%2520relying%2520on%2520global%2520cross-attention%2520mechanisms%2520suffer%2520from%250Aattention%2520dilution%2520and%2520spatial%2520interference%2520between%2520objects%252C%2520while%2520mask-based%250Amethods%2520fail%2520to%2520bind%2520attributes%2520to%2520geometrically%2520accurate%2520regions%2520due%2520to%250Afeature%2520entanglement%2520in%2520multi-object%2520scenarios.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520a%2520training-free%252C%2520inference-stage%2520optimization%2520approach%2520that%2520enables%250Aprecise%2520localized%2520image%2520manipulation%2520in%2520complex%2520multi-object%2520scenes%252C%2520named%250AMDE-Edit.%2520MDE-Edit%2520optimizes%2520the%2520noise%2520latent%2520feature%2520in%2520diffusion%2520models%2520via%250Atwo%2520key%2520losses%253A%2520Object%2520Alignment%2520Loss%2520%2528OAL%2529%2520aligns%2520multi-layer%2520cross-attention%250Awith%2520segmentation%2520masks%2520for%2520precise%2520object%2520positioning%252C%2520and%2520Color%2520Consistency%250ALoss%2520%2528CCL%2529%2520amplifies%2520target%2520attribute%2520attention%2520within%2520masks%2520while%2520suppressing%250Aleakage%2520to%2520adjacent%2520regions.%2520This%2520dual-loss%2520design%2520ensures%2520localized%2520and%250Acoherent%2520multi-object%2520edits.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MDE-Edit%250Aoutperforms%2520state-of-the-art%2520methods%2520in%2520editing%2520accuracy%2520and%2520visual%2520quality%252C%250Aoffering%2520a%2520robust%2520solution%2520for%2520complex%2520multi-object%2520image%2520manipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDE-Edit%3A%20Masked%20Dual-Editing%20for%20Multi-Object%20Image%20Editing%20via%0A%20%20Diffusion%20Models&entry.906535625=Hongyang%20Zhu%20and%20Haipeng%20Liu%20and%20Bo%20Fu%20and%20Yang%20Wang&entry.1292438233=%20%20Multi-object%20editing%20aims%20to%20modify%20multiple%20objects%20or%20regions%20in%20complex%0Ascenes%20while%20preserving%20structural%20coherence.%20This%20task%20faces%20significant%0Achallenges%20in%20scenarios%20involving%20overlapping%20or%20interacting%20objects%3A%20%281%29%0AInaccurate%20localization%20of%20target%20objects%20due%20to%20attention%20misalignment%2C%0Aleading%20to%20incomplete%20or%20misplaced%20edits%3B%20%282%29%20Attribute-object%20mismatch%2C%20where%0Acolor%20or%20texture%20changes%20fail%20to%20align%20with%20intended%20regions%20due%20to%0Across-attention%20leakage%2C%20creating%20semantic%20conflicts%20%28%5Ctextit%7Be.g.%7D%2C%20color%0Ableeding%20into%20non-target%20areas%29.%20Existing%20methods%20struggle%20with%20these%0Achallenges%3A%20approaches%20relying%20on%20global%20cross-attention%20mechanisms%20suffer%20from%0Aattention%20dilution%20and%20spatial%20interference%20between%20objects%2C%20while%20mask-based%0Amethods%20fail%20to%20bind%20attributes%20to%20geometrically%20accurate%20regions%20due%20to%0Afeature%20entanglement%20in%20multi-object%20scenarios.%20To%20address%20these%20limitations%2C%0Awe%20propose%20a%20training-free%2C%20inference-stage%20optimization%20approach%20that%20enables%0Aprecise%20localized%20image%20manipulation%20in%20complex%20multi-object%20scenes%2C%20named%0AMDE-Edit.%20MDE-Edit%20optimizes%20the%20noise%20latent%20feature%20in%20diffusion%20models%20via%0Atwo%20key%20losses%3A%20Object%20Alignment%20Loss%20%28OAL%29%20aligns%20multi-layer%20cross-attention%0Awith%20segmentation%20masks%20for%20precise%20object%20positioning%2C%20and%20Color%20Consistency%0ALoss%20%28CCL%29%20amplifies%20target%20attribute%20attention%20within%20masks%20while%20suppressing%0Aleakage%20to%20adjacent%20regions.%20This%20dual-loss%20design%20ensures%20localized%20and%0Acoherent%20multi-object%20edits.%20Extensive%20experiments%20demonstrate%20that%20MDE-Edit%0Aoutperforms%20state-of-the-art%20methods%20in%20editing%20accuracy%20and%20visual%20quality%2C%0Aoffering%20a%20robust%20solution%20for%20complex%20multi-object%20image%20manipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05101v1&entry.124074799=Read"},
{"title": "Denoising Diffusion Probabilistic Models for Coastal Inundation\n  Forecasting", "author": "Kazi Ashik Islam and Zakaria Mehrab and Mahantesh Halappanavar and Henning Mortveit and Sridhar Katragadda and Jon Derek Loftis and Madhav Marathe", "abstract": "  Coastal flooding poses significant risks to communities, necessitating fast\nand accurate forecasting methods to mitigate potential damage. To approach this\nproblem, we present DIFF-FLOOD, a probabilistic spatiotemporal forecasting\nmethod designed based on denoising diffusion models. DIFF-FLOOD predicts\ninundation level at a location by taking both spatial and temporal context into\naccount. It utilizes inundation levels at neighboring locations and digital\nelevation data as spatial context. Inundation history from a context time\nwindow, together with additional co-variates are used as temporal context.\nConvolutional neural networks and cross-attention mechanism are then employed\nto capture the spatiotemporal dynamics in the data. We trained and tested\nDIFF-FLOOD on coastal inundation data from the Eastern Shore of Virginia, a\nregion highly impacted by coastal flooding. Our results show that, DIFF-FLOOD\noutperforms existing forecasting methods in terms of prediction performance (6%\nto 64% improvement in terms of two performance metrics) and scalability.\n", "link": "http://arxiv.org/abs/2505.05381v1", "date": "2025-05-08", "relevancy": 1.0416, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5381}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5254}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Coastal%20Inundation%0A%20%20Forecasting&body=Title%3A%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Coastal%20Inundation%0A%20%20Forecasting%0AAuthor%3A%20Kazi%20Ashik%20Islam%20and%20Zakaria%20Mehrab%20and%20Mahantesh%20Halappanavar%20and%20Henning%20Mortveit%20and%20Sridhar%20Katragadda%20and%20Jon%20Derek%20Loftis%20and%20Madhav%20Marathe%0AAbstract%3A%20%20%20Coastal%20flooding%20poses%20significant%20risks%20to%20communities%2C%20necessitating%20fast%0Aand%20accurate%20forecasting%20methods%20to%20mitigate%20potential%20damage.%20To%20approach%20this%0Aproblem%2C%20we%20present%20DIFF-FLOOD%2C%20a%20probabilistic%20spatiotemporal%20forecasting%0Amethod%20designed%20based%20on%20denoising%20diffusion%20models.%20DIFF-FLOOD%20predicts%0Ainundation%20level%20at%20a%20location%20by%20taking%20both%20spatial%20and%20temporal%20context%20into%0Aaccount.%20It%20utilizes%20inundation%20levels%20at%20neighboring%20locations%20and%20digital%0Aelevation%20data%20as%20spatial%20context.%20Inundation%20history%20from%20a%20context%20time%0Awindow%2C%20together%20with%20additional%20co-variates%20are%20used%20as%20temporal%20context.%0AConvolutional%20neural%20networks%20and%20cross-attention%20mechanism%20are%20then%20employed%0Ato%20capture%20the%20spatiotemporal%20dynamics%20in%20the%20data.%20We%20trained%20and%20tested%0ADIFF-FLOOD%20on%20coastal%20inundation%20data%20from%20the%20Eastern%20Shore%20of%20Virginia%2C%20a%0Aregion%20highly%20impacted%20by%20coastal%20flooding.%20Our%20results%20show%20that%2C%20DIFF-FLOOD%0Aoutperforms%20existing%20forecasting%20methods%20in%20terms%20of%20prediction%20performance%20%286%25%0Ato%2064%25%20improvement%20in%20terms%20of%20two%20performance%20metrics%29%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoising%2520Diffusion%2520Probabilistic%2520Models%2520for%2520Coastal%2520Inundation%250A%2520%2520Forecasting%26entry.906535625%3DKazi%2520Ashik%2520Islam%2520and%2520Zakaria%2520Mehrab%2520and%2520Mahantesh%2520Halappanavar%2520and%2520Henning%2520Mortveit%2520and%2520Sridhar%2520Katragadda%2520and%2520Jon%2520Derek%2520Loftis%2520and%2520Madhav%2520Marathe%26entry.1292438233%3D%2520%2520Coastal%2520flooding%2520poses%2520significant%2520risks%2520to%2520communities%252C%2520necessitating%2520fast%250Aand%2520accurate%2520forecasting%2520methods%2520to%2520mitigate%2520potential%2520damage.%2520To%2520approach%2520this%250Aproblem%252C%2520we%2520present%2520DIFF-FLOOD%252C%2520a%2520probabilistic%2520spatiotemporal%2520forecasting%250Amethod%2520designed%2520based%2520on%2520denoising%2520diffusion%2520models.%2520DIFF-FLOOD%2520predicts%250Ainundation%2520level%2520at%2520a%2520location%2520by%2520taking%2520both%2520spatial%2520and%2520temporal%2520context%2520into%250Aaccount.%2520It%2520utilizes%2520inundation%2520levels%2520at%2520neighboring%2520locations%2520and%2520digital%250Aelevation%2520data%2520as%2520spatial%2520context.%2520Inundation%2520history%2520from%2520a%2520context%2520time%250Awindow%252C%2520together%2520with%2520additional%2520co-variates%2520are%2520used%2520as%2520temporal%2520context.%250AConvolutional%2520neural%2520networks%2520and%2520cross-attention%2520mechanism%2520are%2520then%2520employed%250Ato%2520capture%2520the%2520spatiotemporal%2520dynamics%2520in%2520the%2520data.%2520We%2520trained%2520and%2520tested%250ADIFF-FLOOD%2520on%2520coastal%2520inundation%2520data%2520from%2520the%2520Eastern%2520Shore%2520of%2520Virginia%252C%2520a%250Aregion%2520highly%2520impacted%2520by%2520coastal%2520flooding.%2520Our%2520results%2520show%2520that%252C%2520DIFF-FLOOD%250Aoutperforms%2520existing%2520forecasting%2520methods%2520in%2520terms%2520of%2520prediction%2520performance%2520%25286%2525%250Ato%252064%2525%2520improvement%2520in%2520terms%2520of%2520two%2520performance%2520metrics%2529%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising%20Diffusion%20Probabilistic%20Models%20for%20Coastal%20Inundation%0A%20%20Forecasting&entry.906535625=Kazi%20Ashik%20Islam%20and%20Zakaria%20Mehrab%20and%20Mahantesh%20Halappanavar%20and%20Henning%20Mortveit%20and%20Sridhar%20Katragadda%20and%20Jon%20Derek%20Loftis%20and%20Madhav%20Marathe&entry.1292438233=%20%20Coastal%20flooding%20poses%20significant%20risks%20to%20communities%2C%20necessitating%20fast%0Aand%20accurate%20forecasting%20methods%20to%20mitigate%20potential%20damage.%20To%20approach%20this%0Aproblem%2C%20we%20present%20DIFF-FLOOD%2C%20a%20probabilistic%20spatiotemporal%20forecasting%0Amethod%20designed%20based%20on%20denoising%20diffusion%20models.%20DIFF-FLOOD%20predicts%0Ainundation%20level%20at%20a%20location%20by%20taking%20both%20spatial%20and%20temporal%20context%20into%0Aaccount.%20It%20utilizes%20inundation%20levels%20at%20neighboring%20locations%20and%20digital%0Aelevation%20data%20as%20spatial%20context.%20Inundation%20history%20from%20a%20context%20time%0Awindow%2C%20together%20with%20additional%20co-variates%20are%20used%20as%20temporal%20context.%0AConvolutional%20neural%20networks%20and%20cross-attention%20mechanism%20are%20then%20employed%0Ato%20capture%20the%20spatiotemporal%20dynamics%20in%20the%20data.%20We%20trained%20and%20tested%0ADIFF-FLOOD%20on%20coastal%20inundation%20data%20from%20the%20Eastern%20Shore%20of%20Virginia%2C%20a%0Aregion%20highly%20impacted%20by%20coastal%20flooding.%20Our%20results%20show%20that%2C%20DIFF-FLOOD%0Aoutperforms%20existing%20forecasting%20methods%20in%20terms%20of%20prediction%20performance%20%286%25%0Ato%2064%25%20improvement%20in%20terms%20of%20two%20performance%20metrics%29%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05381v1&entry.124074799=Read"},
{"title": "CloudTrack: Scalable UAV Tracking with Cloud Semantics", "author": "Yannik Blei and Michael Krawez and Nisarga Nilavadi and Tanja Katharina Kaiser and Wolfram Burgard", "abstract": "  Nowadays, unmanned aerial vehicles (UAVs) are commonly used in search and\nrescue scenarios to gather information in the search area. The automatic\nidentification of the person searched for in aerial footage could increase the\nautonomy of such systems, reduce the search time, and thus increase the missed\nperson's chances of survival. In this paper, we present a novel approach to\nperform semantically conditioned open vocabulary object tracking that is\nspecifically designed to cope with the limitations of UAV hardware. Our\napproach has several advantages. It can run with verbal descriptions of the\nmissing person, e.g., the color of the shirt, it does not require dedicated\ntraining to execute the mission and can efficiently track a potentially moving\nperson. Our experimental results demonstrate the versatility and efficacy of\nour approach.\n", "link": "http://arxiv.org/abs/2409.16111v3", "date": "2025-05-08", "relevancy": 1.68, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5834}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5422}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CloudTrack%3A%20Scalable%20UAV%20Tracking%20with%20Cloud%20Semantics&body=Title%3A%20CloudTrack%3A%20Scalable%20UAV%20Tracking%20with%20Cloud%20Semantics%0AAuthor%3A%20Yannik%20Blei%20and%20Michael%20Krawez%20and%20Nisarga%20Nilavadi%20and%20Tanja%20Katharina%20Kaiser%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20Nowadays%2C%20unmanned%20aerial%20vehicles%20%28UAVs%29%20are%20commonly%20used%20in%20search%20and%0Arescue%20scenarios%20to%20gather%20information%20in%20the%20search%20area.%20The%20automatic%0Aidentification%20of%20the%20person%20searched%20for%20in%20aerial%20footage%20could%20increase%20the%0Aautonomy%20of%20such%20systems%2C%20reduce%20the%20search%20time%2C%20and%20thus%20increase%20the%20missed%0Aperson%27s%20chances%20of%20survival.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20to%0Aperform%20semantically%20conditioned%20open%20vocabulary%20object%20tracking%20that%20is%0Aspecifically%20designed%20to%20cope%20with%20the%20limitations%20of%20UAV%20hardware.%20Our%0Aapproach%20has%20several%20advantages.%20It%20can%20run%20with%20verbal%20descriptions%20of%20the%0Amissing%20person%2C%20e.g.%2C%20the%20color%20of%20the%20shirt%2C%20it%20does%20not%20require%20dedicated%0Atraining%20to%20execute%20the%20mission%20and%20can%20efficiently%20track%20a%20potentially%20moving%0Aperson.%20Our%20experimental%20results%20demonstrate%20the%20versatility%20and%20efficacy%20of%0Aour%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16111v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCloudTrack%253A%2520Scalable%2520UAV%2520Tracking%2520with%2520Cloud%2520Semantics%26entry.906535625%3DYannik%2520Blei%2520and%2520Michael%2520Krawez%2520and%2520Nisarga%2520Nilavadi%2520and%2520Tanja%2520Katharina%2520Kaiser%2520and%2520Wolfram%2520Burgard%26entry.1292438233%3D%2520%2520Nowadays%252C%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520are%2520commonly%2520used%2520in%2520search%2520and%250Arescue%2520scenarios%2520to%2520gather%2520information%2520in%2520the%2520search%2520area.%2520The%2520automatic%250Aidentification%2520of%2520the%2520person%2520searched%2520for%2520in%2520aerial%2520footage%2520could%2520increase%2520the%250Aautonomy%2520of%2520such%2520systems%252C%2520reduce%2520the%2520search%2520time%252C%2520and%2520thus%2520increase%2520the%2520missed%250Aperson%2527s%2520chances%2520of%2520survival.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520approach%2520to%250Aperform%2520semantically%2520conditioned%2520open%2520vocabulary%2520object%2520tracking%2520that%2520is%250Aspecifically%2520designed%2520to%2520cope%2520with%2520the%2520limitations%2520of%2520UAV%2520hardware.%2520Our%250Aapproach%2520has%2520several%2520advantages.%2520It%2520can%2520run%2520with%2520verbal%2520descriptions%2520of%2520the%250Amissing%2520person%252C%2520e.g.%252C%2520the%2520color%2520of%2520the%2520shirt%252C%2520it%2520does%2520not%2520require%2520dedicated%250Atraining%2520to%2520execute%2520the%2520mission%2520and%2520can%2520efficiently%2520track%2520a%2520potentially%2520moving%250Aperson.%2520Our%2520experimental%2520results%2520demonstrate%2520the%2520versatility%2520and%2520efficacy%2520of%250Aour%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16111v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CloudTrack%3A%20Scalable%20UAV%20Tracking%20with%20Cloud%20Semantics&entry.906535625=Yannik%20Blei%20and%20Michael%20Krawez%20and%20Nisarga%20Nilavadi%20and%20Tanja%20Katharina%20Kaiser%20and%20Wolfram%20Burgard&entry.1292438233=%20%20Nowadays%2C%20unmanned%20aerial%20vehicles%20%28UAVs%29%20are%20commonly%20used%20in%20search%20and%0Arescue%20scenarios%20to%20gather%20information%20in%20the%20search%20area.%20The%20automatic%0Aidentification%20of%20the%20person%20searched%20for%20in%20aerial%20footage%20could%20increase%20the%0Aautonomy%20of%20such%20systems%2C%20reduce%20the%20search%20time%2C%20and%20thus%20increase%20the%20missed%0Aperson%27s%20chances%20of%20survival.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20to%0Aperform%20semantically%20conditioned%20open%20vocabulary%20object%20tracking%20that%20is%0Aspecifically%20designed%20to%20cope%20with%20the%20limitations%20of%20UAV%20hardware.%20Our%0Aapproach%20has%20several%20advantages.%20It%20can%20run%20with%20verbal%20descriptions%20of%20the%0Amissing%20person%2C%20e.g.%2C%20the%20color%20of%20the%20shirt%2C%20it%20does%20not%20require%20dedicated%0Atraining%20to%20execute%20the%20mission%20and%20can%20efficiently%20track%20a%20potentially%20moving%0Aperson.%20Our%20experimental%20results%20demonstrate%20the%20versatility%20and%20efficacy%20of%0Aour%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16111v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


