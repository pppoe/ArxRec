<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240718.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting", "author": "Richard Shaw and Michal Nazarczuk and Jifei Song and Arthur Moreau and Sibi Catley-Chandar and Helisa Dhamo and Eduardo Perez-Pellitero", "abstract": "  Novel view synthesis has shown rapid progress recently, with methods capable\nof producing increasingly photorealistic results. 3D Gaussian Splatting has\nemerged as a promising method, producing high-quality renderings of scenes and\nenabling interactive viewing at real-time frame rates. However, it is limited\nto static scenes. In this work, we extend 3D Gaussian Splatting to reconstruct\ndynamic scenes. We model a scene's dynamics using dynamic MLPs, learning\ndeformations from temporally-local canonical representations to per-frame 3D\nGaussians. To disentangle static and dynamic regions, tuneable parameters weigh\neach Gaussian's respective MLP parameters, improving the dynamics modelling of\nimbalanced scenes. We introduce a sliding window training strategy that\npartitions the sequence into smaller manageable windows to handle arbitrary\nlength scenes while maintaining high rendering quality. We propose an adaptive\nsampling strategy to determine appropriate window size hyperparameters based on\nthe scene's motion, balancing training overhead with visual quality. Training a\nseparate dynamic 3D Gaussian model for each sliding window allows the canonical\nrepresentation to change, enabling the reconstruction of scenes with\nsignificant geometric changes. Temporal consistency is enforced using a\nfine-tuning step with self-supervising consistency loss on randomly sampled\nnovel views. As a result, our method produces high-quality renderings of\ngeneral dynamic scenes with competitive quantitative performance, which can be\nviewed in real-time in our dynamic interactive viewer.\n", "link": "http://arxiv.org/abs/2312.13308v2", "date": "2024-07-18", "relevancy": 3.2404, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6867}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6696}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWinGS%3A%20Sliding%20Windows%20for%20Dynamic%203D%20Gaussian%20Splatting&body=Title%3A%20SWinGS%3A%20Sliding%20Windows%20for%20Dynamic%203D%20Gaussian%20Splatting%0AAuthor%3A%20Richard%20Shaw%20and%20Michal%20Nazarczuk%20and%20Jifei%20Song%20and%20Arthur%20Moreau%20and%20Sibi%20Catley-Chandar%20and%20Helisa%20Dhamo%20and%20Eduardo%20Perez-Pellitero%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20has%20shown%20rapid%20progress%20recently%2C%20with%20methods%20capable%0Aof%20producing%20increasingly%20photorealistic%20results.%203D%20Gaussian%20Splatting%20has%0Aemerged%20as%20a%20promising%20method%2C%20producing%20high-quality%20renderings%20of%20scenes%20and%0Aenabling%20interactive%20viewing%20at%20real-time%20frame%20rates.%20However%2C%20it%20is%20limited%0Ato%20static%20scenes.%20In%20this%20work%2C%20we%20extend%203D%20Gaussian%20Splatting%20to%20reconstruct%0Adynamic%20scenes.%20We%20model%20a%20scene%27s%20dynamics%20using%20dynamic%20MLPs%2C%20learning%0Adeformations%20from%20temporally-local%20canonical%20representations%20to%20per-frame%203D%0AGaussians.%20To%20disentangle%20static%20and%20dynamic%20regions%2C%20tuneable%20parameters%20weigh%0Aeach%20Gaussian%27s%20respective%20MLP%20parameters%2C%20improving%20the%20dynamics%20modelling%20of%0Aimbalanced%20scenes.%20We%20introduce%20a%20sliding%20window%20training%20strategy%20that%0Apartitions%20the%20sequence%20into%20smaller%20manageable%20windows%20to%20handle%20arbitrary%0Alength%20scenes%20while%20maintaining%20high%20rendering%20quality.%20We%20propose%20an%20adaptive%0Asampling%20strategy%20to%20determine%20appropriate%20window%20size%20hyperparameters%20based%20on%0Athe%20scene%27s%20motion%2C%20balancing%20training%20overhead%20with%20visual%20quality.%20Training%20a%0Aseparate%20dynamic%203D%20Gaussian%20model%20for%20each%20sliding%20window%20allows%20the%20canonical%0Arepresentation%20to%20change%2C%20enabling%20the%20reconstruction%20of%20scenes%20with%0Asignificant%20geometric%20changes.%20Temporal%20consistency%20is%20enforced%20using%20a%0Afine-tuning%20step%20with%20self-supervising%20consistency%20loss%20on%20randomly%20sampled%0Anovel%20views.%20As%20a%20result%2C%20our%20method%20produces%20high-quality%20renderings%20of%0Ageneral%20dynamic%20scenes%20with%20competitive%20quantitative%20performance%2C%20which%20can%20be%0Aviewed%20in%20real-time%20in%20our%20dynamic%20interactive%20viewer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13308v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWinGS%253A%2520Sliding%2520Windows%2520for%2520Dynamic%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DRichard%2520Shaw%2520and%2520Michal%2520Nazarczuk%2520and%2520Jifei%2520Song%2520and%2520Arthur%2520Moreau%2520and%2520Sibi%2520Catley-Chandar%2520and%2520Helisa%2520Dhamo%2520and%2520Eduardo%2520Perez-Pellitero%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520has%2520shown%2520rapid%2520progress%2520recently%252C%2520with%2520methods%2520capable%250Aof%2520producing%2520increasingly%2520photorealistic%2520results.%25203D%2520Gaussian%2520Splatting%2520has%250Aemerged%2520as%2520a%2520promising%2520method%252C%2520producing%2520high-quality%2520renderings%2520of%2520scenes%2520and%250Aenabling%2520interactive%2520viewing%2520at%2520real-time%2520frame%2520rates.%2520However%252C%2520it%2520is%2520limited%250Ato%2520static%2520scenes.%2520In%2520this%2520work%252C%2520we%2520extend%25203D%2520Gaussian%2520Splatting%2520to%2520reconstruct%250Adynamic%2520scenes.%2520We%2520model%2520a%2520scene%2527s%2520dynamics%2520using%2520dynamic%2520MLPs%252C%2520learning%250Adeformations%2520from%2520temporally-local%2520canonical%2520representations%2520to%2520per-frame%25203D%250AGaussians.%2520To%2520disentangle%2520static%2520and%2520dynamic%2520regions%252C%2520tuneable%2520parameters%2520weigh%250Aeach%2520Gaussian%2527s%2520respective%2520MLP%2520parameters%252C%2520improving%2520the%2520dynamics%2520modelling%2520of%250Aimbalanced%2520scenes.%2520We%2520introduce%2520a%2520sliding%2520window%2520training%2520strategy%2520that%250Apartitions%2520the%2520sequence%2520into%2520smaller%2520manageable%2520windows%2520to%2520handle%2520arbitrary%250Alength%2520scenes%2520while%2520maintaining%2520high%2520rendering%2520quality.%2520We%2520propose%2520an%2520adaptive%250Asampling%2520strategy%2520to%2520determine%2520appropriate%2520window%2520size%2520hyperparameters%2520based%2520on%250Athe%2520scene%2527s%2520motion%252C%2520balancing%2520training%2520overhead%2520with%2520visual%2520quality.%2520Training%2520a%250Aseparate%2520dynamic%25203D%2520Gaussian%2520model%2520for%2520each%2520sliding%2520window%2520allows%2520the%2520canonical%250Arepresentation%2520to%2520change%252C%2520enabling%2520the%2520reconstruction%2520of%2520scenes%2520with%250Asignificant%2520geometric%2520changes.%2520Temporal%2520consistency%2520is%2520enforced%2520using%2520a%250Afine-tuning%2520step%2520with%2520self-supervising%2520consistency%2520loss%2520on%2520randomly%2520sampled%250Anovel%2520views.%2520As%2520a%2520result%252C%2520our%2520method%2520produces%2520high-quality%2520renderings%2520of%250Ageneral%2520dynamic%2520scenes%2520with%2520competitive%2520quantitative%2520performance%252C%2520which%2520can%2520be%250Aviewed%2520in%2520real-time%2520in%2520our%2520dynamic%2520interactive%2520viewer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13308v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWinGS%3A%20Sliding%20Windows%20for%20Dynamic%203D%20Gaussian%20Splatting&entry.906535625=Richard%20Shaw%20and%20Michal%20Nazarczuk%20and%20Jifei%20Song%20and%20Arthur%20Moreau%20and%20Sibi%20Catley-Chandar%20and%20Helisa%20Dhamo%20and%20Eduardo%20Perez-Pellitero&entry.1292438233=%20%20Novel%20view%20synthesis%20has%20shown%20rapid%20progress%20recently%2C%20with%20methods%20capable%0Aof%20producing%20increasingly%20photorealistic%20results.%203D%20Gaussian%20Splatting%20has%0Aemerged%20as%20a%20promising%20method%2C%20producing%20high-quality%20renderings%20of%20scenes%20and%0Aenabling%20interactive%20viewing%20at%20real-time%20frame%20rates.%20However%2C%20it%20is%20limited%0Ato%20static%20scenes.%20In%20this%20work%2C%20we%20extend%203D%20Gaussian%20Splatting%20to%20reconstruct%0Adynamic%20scenes.%20We%20model%20a%20scene%27s%20dynamics%20using%20dynamic%20MLPs%2C%20learning%0Adeformations%20from%20temporally-local%20canonical%20representations%20to%20per-frame%203D%0AGaussians.%20To%20disentangle%20static%20and%20dynamic%20regions%2C%20tuneable%20parameters%20weigh%0Aeach%20Gaussian%27s%20respective%20MLP%20parameters%2C%20improving%20the%20dynamics%20modelling%20of%0Aimbalanced%20scenes.%20We%20introduce%20a%20sliding%20window%20training%20strategy%20that%0Apartitions%20the%20sequence%20into%20smaller%20manageable%20windows%20to%20handle%20arbitrary%0Alength%20scenes%20while%20maintaining%20high%20rendering%20quality.%20We%20propose%20an%20adaptive%0Asampling%20strategy%20to%20determine%20appropriate%20window%20size%20hyperparameters%20based%20on%0Athe%20scene%27s%20motion%2C%20balancing%20training%20overhead%20with%20visual%20quality.%20Training%20a%0Aseparate%20dynamic%203D%20Gaussian%20model%20for%20each%20sliding%20window%20allows%20the%20canonical%0Arepresentation%20to%20change%2C%20enabling%20the%20reconstruction%20of%20scenes%20with%0Asignificant%20geometric%20changes.%20Temporal%20consistency%20is%20enforced%20using%20a%0Afine-tuning%20step%20with%20self-supervising%20consistency%20loss%20on%20randomly%20sampled%0Anovel%20views.%20As%20a%20result%2C%20our%20method%20produces%20high-quality%20renderings%20of%0Ageneral%20dynamic%20scenes%20with%20competitive%20quantitative%20performance%2C%20which%20can%20be%0Aviewed%20in%20real-time%20in%20our%20dynamic%20interactive%20viewer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13308v2&entry.124074799=Read"},
{"title": "MVSBoost: An Efficient Point Cloud-based 3D Reconstruction", "author": "Umair Haroon and Ahmad AlMughrabi and Ricardo Marques and Petia Radeva", "abstract": "  Efficient and accurate 3D reconstruction is crucial for various applications,\nincluding augmented and virtual reality, medical imaging, and cinematic special\neffects. While traditional Multi-View Stereo (MVS) systems have been\nfundamental in these applications, using neural implicit fields in implicit 3D\nscene modeling has introduced new possibilities for handling complex topologies\nand continuous surfaces. However, neural implicit fields often suffer from\ncomputational inefficiencies, overfitting, and heavy reliance on data quality,\nlimiting their practical use. This paper presents an enhanced MVS framework\nthat integrates multi-view 360-degree imagery with robust camera pose\nestimation via Structure from Motion (SfM) and advanced image processing for\npoint cloud densification, mesh reconstruction, and texturing. Our approach\nsignificantly improves upon traditional MVS methods, offering superior accuracy\nand precision as validated using Chamfer distance metrics on the Realistic\nSynthetic 360 dataset. The developed MVS technique enhances the detail and\nclarity of 3D reconstructions and demonstrates superior computational\nefficiency and robustness in complex scene reconstruction, effectively handling\nocclusions and varying viewpoints. These improvements suggest that our MVS\nframework can compete with and potentially exceed current state-of-the-art\nneural implicit field methods, especially in scenarios requiring real-time\nprocessing and scalability.\n", "link": "http://arxiv.org/abs/2406.13515v2", "date": "2024-07-18", "relevancy": 3.1632, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6329}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6329}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVSBoost%3A%20An%20Efficient%20Point%20Cloud-based%203D%20Reconstruction&body=Title%3A%20MVSBoost%3A%20An%20Efficient%20Point%20Cloud-based%203D%20Reconstruction%0AAuthor%3A%20Umair%20Haroon%20and%20Ahmad%20AlMughrabi%20and%20Ricardo%20Marques%20and%20Petia%20Radeva%0AAbstract%3A%20%20%20Efficient%20and%20accurate%203D%20reconstruction%20is%20crucial%20for%20various%20applications%2C%0Aincluding%20augmented%20and%20virtual%20reality%2C%20medical%20imaging%2C%20and%20cinematic%20special%0Aeffects.%20While%20traditional%20Multi-View%20Stereo%20%28MVS%29%20systems%20have%20been%0Afundamental%20in%20these%20applications%2C%20using%20neural%20implicit%20fields%20in%20implicit%203D%0Ascene%20modeling%20has%20introduced%20new%20possibilities%20for%20handling%20complex%20topologies%0Aand%20continuous%20surfaces.%20However%2C%20neural%20implicit%20fields%20often%20suffer%20from%0Acomputational%20inefficiencies%2C%20overfitting%2C%20and%20heavy%20reliance%20on%20data%20quality%2C%0Alimiting%20their%20practical%20use.%20This%20paper%20presents%20an%20enhanced%20MVS%20framework%0Athat%20integrates%20multi-view%20360-degree%20imagery%20with%20robust%20camera%20pose%0Aestimation%20via%20Structure%20from%20Motion%20%28SfM%29%20and%20advanced%20image%20processing%20for%0Apoint%20cloud%20densification%2C%20mesh%20reconstruction%2C%20and%20texturing.%20Our%20approach%0Asignificantly%20improves%20upon%20traditional%20MVS%20methods%2C%20offering%20superior%20accuracy%0Aand%20precision%20as%20validated%20using%20Chamfer%20distance%20metrics%20on%20the%20Realistic%0ASynthetic%20360%20dataset.%20The%20developed%20MVS%20technique%20enhances%20the%20detail%20and%0Aclarity%20of%203D%20reconstructions%20and%20demonstrates%20superior%20computational%0Aefficiency%20and%20robustness%20in%20complex%20scene%20reconstruction%2C%20effectively%20handling%0Aocclusions%20and%20varying%20viewpoints.%20These%20improvements%20suggest%20that%20our%20MVS%0Aframework%20can%20compete%20with%20and%20potentially%20exceed%20current%20state-of-the-art%0Aneural%20implicit%20field%20methods%2C%20especially%20in%20scenarios%20requiring%20real-time%0Aprocessing%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13515v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVSBoost%253A%2520An%2520Efficient%2520Point%2520Cloud-based%25203D%2520Reconstruction%26entry.906535625%3DUmair%2520Haroon%2520and%2520Ahmad%2520AlMughrabi%2520and%2520Ricardo%2520Marques%2520and%2520Petia%2520Radeva%26entry.1292438233%3D%2520%2520Efficient%2520and%2520accurate%25203D%2520reconstruction%2520is%2520crucial%2520for%2520various%2520applications%252C%250Aincluding%2520augmented%2520and%2520virtual%2520reality%252C%2520medical%2520imaging%252C%2520and%2520cinematic%2520special%250Aeffects.%2520While%2520traditional%2520Multi-View%2520Stereo%2520%2528MVS%2529%2520systems%2520have%2520been%250Afundamental%2520in%2520these%2520applications%252C%2520using%2520neural%2520implicit%2520fields%2520in%2520implicit%25203D%250Ascene%2520modeling%2520has%2520introduced%2520new%2520possibilities%2520for%2520handling%2520complex%2520topologies%250Aand%2520continuous%2520surfaces.%2520However%252C%2520neural%2520implicit%2520fields%2520often%2520suffer%2520from%250Acomputational%2520inefficiencies%252C%2520overfitting%252C%2520and%2520heavy%2520reliance%2520on%2520data%2520quality%252C%250Alimiting%2520their%2520practical%2520use.%2520This%2520paper%2520presents%2520an%2520enhanced%2520MVS%2520framework%250Athat%2520integrates%2520multi-view%2520360-degree%2520imagery%2520with%2520robust%2520camera%2520pose%250Aestimation%2520via%2520Structure%2520from%2520Motion%2520%2528SfM%2529%2520and%2520advanced%2520image%2520processing%2520for%250Apoint%2520cloud%2520densification%252C%2520mesh%2520reconstruction%252C%2520and%2520texturing.%2520Our%2520approach%250Asignificantly%2520improves%2520upon%2520traditional%2520MVS%2520methods%252C%2520offering%2520superior%2520accuracy%250Aand%2520precision%2520as%2520validated%2520using%2520Chamfer%2520distance%2520metrics%2520on%2520the%2520Realistic%250ASynthetic%2520360%2520dataset.%2520The%2520developed%2520MVS%2520technique%2520enhances%2520the%2520detail%2520and%250Aclarity%2520of%25203D%2520reconstructions%2520and%2520demonstrates%2520superior%2520computational%250Aefficiency%2520and%2520robustness%2520in%2520complex%2520scene%2520reconstruction%252C%2520effectively%2520handling%250Aocclusions%2520and%2520varying%2520viewpoints.%2520These%2520improvements%2520suggest%2520that%2520our%2520MVS%250Aframework%2520can%2520compete%2520with%2520and%2520potentially%2520exceed%2520current%2520state-of-the-art%250Aneural%2520implicit%2520field%2520methods%252C%2520especially%2520in%2520scenarios%2520requiring%2520real-time%250Aprocessing%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13515v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVSBoost%3A%20An%20Efficient%20Point%20Cloud-based%203D%20Reconstruction&entry.906535625=Umair%20Haroon%20and%20Ahmad%20AlMughrabi%20and%20Ricardo%20Marques%20and%20Petia%20Radeva&entry.1292438233=%20%20Efficient%20and%20accurate%203D%20reconstruction%20is%20crucial%20for%20various%20applications%2C%0Aincluding%20augmented%20and%20virtual%20reality%2C%20medical%20imaging%2C%20and%20cinematic%20special%0Aeffects.%20While%20traditional%20Multi-View%20Stereo%20%28MVS%29%20systems%20have%20been%0Afundamental%20in%20these%20applications%2C%20using%20neural%20implicit%20fields%20in%20implicit%203D%0Ascene%20modeling%20has%20introduced%20new%20possibilities%20for%20handling%20complex%20topologies%0Aand%20continuous%20surfaces.%20However%2C%20neural%20implicit%20fields%20often%20suffer%20from%0Acomputational%20inefficiencies%2C%20overfitting%2C%20and%20heavy%20reliance%20on%20data%20quality%2C%0Alimiting%20their%20practical%20use.%20This%20paper%20presents%20an%20enhanced%20MVS%20framework%0Athat%20integrates%20multi-view%20360-degree%20imagery%20with%20robust%20camera%20pose%0Aestimation%20via%20Structure%20from%20Motion%20%28SfM%29%20and%20advanced%20image%20processing%20for%0Apoint%20cloud%20densification%2C%20mesh%20reconstruction%2C%20and%20texturing.%20Our%20approach%0Asignificantly%20improves%20upon%20traditional%20MVS%20methods%2C%20offering%20superior%20accuracy%0Aand%20precision%20as%20validated%20using%20Chamfer%20distance%20metrics%20on%20the%20Realistic%0ASynthetic%20360%20dataset.%20The%20developed%20MVS%20technique%20enhances%20the%20detail%20and%0Aclarity%20of%203D%20reconstructions%20and%20demonstrates%20superior%20computational%0Aefficiency%20and%20robustness%20in%20complex%20scene%20reconstruction%2C%20effectively%20handling%0Aocclusions%20and%20varying%20viewpoints.%20These%20improvements%20suggest%20that%20our%20MVS%0Aframework%20can%20compete%20with%20and%20potentially%20exceed%20current%20state-of-the-art%0Aneural%20implicit%20field%20methods%2C%20especially%20in%20scenarios%20requiring%20real-time%0Aprocessing%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13515v2&entry.124074799=Read"},
{"title": "Streetscapes: Large-scale Consistent Street View Generation Using\n  Autoregressive Video Diffusion", "author": "Boyang Deng and Richard Tucker and Zhengqi Li and Leonidas Guibas and Noah Snavely and Gordon Wetzstein", "abstract": "  We present a method for generating Streetscapes-long sequences of views\nthrough an on-the-fly synthesized city-scale scene. Our generation is\nconditioned by language input (e.g., city name, weather), as well as an\nunderlying map/layout hosting the desired trajectory. Compared to recent models\nfor video generation or 3D view synthesis, our method can scale to much\nlonger-range camera trajectories, spanning several city blocks, while\nmaintaining visual quality and consistency. To achieve this goal, we build on\nrecent work on video diffusion, used within an autoregressive framework that\ncan easily scale to long sequences. In particular, we introduce a new temporal\nimputation method that prevents our autoregressive approach from drifting from\nthe distribution of realistic city imagery. We train our Streetscapes system on\na compelling source of data-posed imagery from Google Street View, along with\ncontextual map data-which allows users to generate city views conditioned on\nany desired city layout, with controllable camera poses. Please see more\nresults at our project page at https://boyangdeng.com/streetscapes.\n", "link": "http://arxiv.org/abs/2407.13759v1", "date": "2024-07-18", "relevancy": 3.1401, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6418}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6418}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streetscapes%3A%20Large-scale%20Consistent%20Street%20View%20Generation%20Using%0A%20%20Autoregressive%20Video%20Diffusion&body=Title%3A%20Streetscapes%3A%20Large-scale%20Consistent%20Street%20View%20Generation%20Using%0A%20%20Autoregressive%20Video%20Diffusion%0AAuthor%3A%20Boyang%20Deng%20and%20Richard%20Tucker%20and%20Zhengqi%20Li%20and%20Leonidas%20Guibas%20and%20Noah%20Snavely%20and%20Gordon%20Wetzstein%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20generating%20Streetscapes-long%20sequences%20of%20views%0Athrough%20an%20on-the-fly%20synthesized%20city-scale%20scene.%20Our%20generation%20is%0Aconditioned%20by%20language%20input%20%28e.g.%2C%20city%20name%2C%20weather%29%2C%20as%20well%20as%20an%0Aunderlying%20map/layout%20hosting%20the%20desired%20trajectory.%20Compared%20to%20recent%20models%0Afor%20video%20generation%20or%203D%20view%20synthesis%2C%20our%20method%20can%20scale%20to%20much%0Alonger-range%20camera%20trajectories%2C%20spanning%20several%20city%20blocks%2C%20while%0Amaintaining%20visual%20quality%20and%20consistency.%20To%20achieve%20this%20goal%2C%20we%20build%20on%0Arecent%20work%20on%20video%20diffusion%2C%20used%20within%20an%20autoregressive%20framework%20that%0Acan%20easily%20scale%20to%20long%20sequences.%20In%20particular%2C%20we%20introduce%20a%20new%20temporal%0Aimputation%20method%20that%20prevents%20our%20autoregressive%20approach%20from%20drifting%20from%0Athe%20distribution%20of%20realistic%20city%20imagery.%20We%20train%20our%20Streetscapes%20system%20on%0Aa%20compelling%20source%20of%20data-posed%20imagery%20from%20Google%20Street%20View%2C%20along%20with%0Acontextual%20map%20data-which%20allows%20users%20to%20generate%20city%20views%20conditioned%20on%0Aany%20desired%20city%20layout%2C%20with%20controllable%20camera%20poses.%20Please%20see%20more%0Aresults%20at%20our%20project%20page%20at%20https%3A//boyangdeng.com/streetscapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreetscapes%253A%2520Large-scale%2520Consistent%2520Street%2520View%2520Generation%2520Using%250A%2520%2520Autoregressive%2520Video%2520Diffusion%26entry.906535625%3DBoyang%2520Deng%2520and%2520Richard%2520Tucker%2520and%2520Zhengqi%2520Li%2520and%2520Leonidas%2520Guibas%2520and%2520Noah%2520Snavely%2520and%2520Gordon%2520Wetzstein%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520for%2520generating%2520Streetscapes-long%2520sequences%2520of%2520views%250Athrough%2520an%2520on-the-fly%2520synthesized%2520city-scale%2520scene.%2520Our%2520generation%2520is%250Aconditioned%2520by%2520language%2520input%2520%2528e.g.%252C%2520city%2520name%252C%2520weather%2529%252C%2520as%2520well%2520as%2520an%250Aunderlying%2520map/layout%2520hosting%2520the%2520desired%2520trajectory.%2520Compared%2520to%2520recent%2520models%250Afor%2520video%2520generation%2520or%25203D%2520view%2520synthesis%252C%2520our%2520method%2520can%2520scale%2520to%2520much%250Alonger-range%2520camera%2520trajectories%252C%2520spanning%2520several%2520city%2520blocks%252C%2520while%250Amaintaining%2520visual%2520quality%2520and%2520consistency.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520build%2520on%250Arecent%2520work%2520on%2520video%2520diffusion%252C%2520used%2520within%2520an%2520autoregressive%2520framework%2520that%250Acan%2520easily%2520scale%2520to%2520long%2520sequences.%2520In%2520particular%252C%2520we%2520introduce%2520a%2520new%2520temporal%250Aimputation%2520method%2520that%2520prevents%2520our%2520autoregressive%2520approach%2520from%2520drifting%2520from%250Athe%2520distribution%2520of%2520realistic%2520city%2520imagery.%2520We%2520train%2520our%2520Streetscapes%2520system%2520on%250Aa%2520compelling%2520source%2520of%2520data-posed%2520imagery%2520from%2520Google%2520Street%2520View%252C%2520along%2520with%250Acontextual%2520map%2520data-which%2520allows%2520users%2520to%2520generate%2520city%2520views%2520conditioned%2520on%250Aany%2520desired%2520city%2520layout%252C%2520with%2520controllable%2520camera%2520poses.%2520Please%2520see%2520more%250Aresults%2520at%2520our%2520project%2520page%2520at%2520https%253A//boyangdeng.com/streetscapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streetscapes%3A%20Large-scale%20Consistent%20Street%20View%20Generation%20Using%0A%20%20Autoregressive%20Video%20Diffusion&entry.906535625=Boyang%20Deng%20and%20Richard%20Tucker%20and%20Zhengqi%20Li%20and%20Leonidas%20Guibas%20and%20Noah%20Snavely%20and%20Gordon%20Wetzstein&entry.1292438233=%20%20We%20present%20a%20method%20for%20generating%20Streetscapes-long%20sequences%20of%20views%0Athrough%20an%20on-the-fly%20synthesized%20city-scale%20scene.%20Our%20generation%20is%0Aconditioned%20by%20language%20input%20%28e.g.%2C%20city%20name%2C%20weather%29%2C%20as%20well%20as%20an%0Aunderlying%20map/layout%20hosting%20the%20desired%20trajectory.%20Compared%20to%20recent%20models%0Afor%20video%20generation%20or%203D%20view%20synthesis%2C%20our%20method%20can%20scale%20to%20much%0Alonger-range%20camera%20trajectories%2C%20spanning%20several%20city%20blocks%2C%20while%0Amaintaining%20visual%20quality%20and%20consistency.%20To%20achieve%20this%20goal%2C%20we%20build%20on%0Arecent%20work%20on%20video%20diffusion%2C%20used%20within%20an%20autoregressive%20framework%20that%0Acan%20easily%20scale%20to%20long%20sequences.%20In%20particular%2C%20we%20introduce%20a%20new%20temporal%0Aimputation%20method%20that%20prevents%20our%20autoregressive%20approach%20from%20drifting%20from%0Athe%20distribution%20of%20realistic%20city%20imagery.%20We%20train%20our%20Streetscapes%20system%20on%0Aa%20compelling%20source%20of%20data-posed%20imagery%20from%20Google%20Street%20View%2C%20along%20with%0Acontextual%20map%20data-which%20allows%20users%20to%20generate%20city%20views%20conditioned%20on%0Aany%20desired%20city%20layout%2C%20with%20controllable%20camera%20poses.%20Please%20see%20more%0Aresults%20at%20our%20project%20page%20at%20https%3A//boyangdeng.com/streetscapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13759v1&entry.124074799=Read"},
{"title": "Shape of Motion: 4D Reconstruction from a Single Video", "author": "Qianqian Wang and Vickie Ye and Hang Gao and Jake Austin and Zhengqi Li and Angjoo Kanazawa", "abstract": "  Monocular dynamic reconstruction is a challenging and long-standing vision\nproblem due to the highly ill-posed nature of the task. Existing approaches are\nlimited in that they either depend on templates, are effective only in\nquasi-static scenes, or fail to model 3D motion explicitly. In this work, we\nintroduce a method capable of reconstructing generic dynamic scenes, featuring\nexplicit, full-sequence-long 3D motion, from casually captured monocular\nvideos. We tackle the under-constrained nature of the problem with two key\ninsights: First, we exploit the low-dimensional structure of 3D motion by\nrepresenting scene motion with a compact set of SE3 motion bases. Each point's\nmotion is expressed as a linear combination of these bases, facilitating soft\ndecomposition of the scene into multiple rigidly-moving groups. Second, we\nutilize a comprehensive set of data-driven priors, including monocular depth\nmaps and long-range 2D tracks, and devise a method to effectively consolidate\nthese noisy supervisory signals, resulting in a globally consistent\nrepresentation of the dynamic scene. Experiments show that our method achieves\nstate-of-the-art performance for both long-range 3D/2D motion estimation and\nnovel view synthesis on dynamic scenes. Project Page:\nhttps://shape-of-motion.github.io/\n", "link": "http://arxiv.org/abs/2407.13764v1", "date": "2024-07-18", "relevancy": 3.1352, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6605}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6103}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shape%20of%20Motion%3A%204D%20Reconstruction%20from%20a%20Single%20Video&body=Title%3A%20Shape%20of%20Motion%3A%204D%20Reconstruction%20from%20a%20Single%20Video%0AAuthor%3A%20Qianqian%20Wang%20and%20Vickie%20Ye%20and%20Hang%20Gao%20and%20Jake%20Austin%20and%20Zhengqi%20Li%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20Monocular%20dynamic%20reconstruction%20is%20a%20challenging%20and%20long-standing%20vision%0Aproblem%20due%20to%20the%20highly%20ill-posed%20nature%20of%20the%20task.%20Existing%20approaches%20are%0Alimited%20in%20that%20they%20either%20depend%20on%20templates%2C%20are%20effective%20only%20in%0Aquasi-static%20scenes%2C%20or%20fail%20to%20model%203D%20motion%20explicitly.%20In%20this%20work%2C%20we%0Aintroduce%20a%20method%20capable%20of%20reconstructing%20generic%20dynamic%20scenes%2C%20featuring%0Aexplicit%2C%20full-sequence-long%203D%20motion%2C%20from%20casually%20captured%20monocular%0Avideos.%20We%20tackle%20the%20under-constrained%20nature%20of%20the%20problem%20with%20two%20key%0Ainsights%3A%20First%2C%20we%20exploit%20the%20low-dimensional%20structure%20of%203D%20motion%20by%0Arepresenting%20scene%20motion%20with%20a%20compact%20set%20of%20SE3%20motion%20bases.%20Each%20point%27s%0Amotion%20is%20expressed%20as%20a%20linear%20combination%20of%20these%20bases%2C%20facilitating%20soft%0Adecomposition%20of%20the%20scene%20into%20multiple%20rigidly-moving%20groups.%20Second%2C%20we%0Autilize%20a%20comprehensive%20set%20of%20data-driven%20priors%2C%20including%20monocular%20depth%0Amaps%20and%20long-range%202D%20tracks%2C%20and%20devise%20a%20method%20to%20effectively%20consolidate%0Athese%20noisy%20supervisory%20signals%2C%20resulting%20in%20a%20globally%20consistent%0Arepresentation%20of%20the%20dynamic%20scene.%20Experiments%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20for%20both%20long-range%203D/2D%20motion%20estimation%20and%0Anovel%20view%20synthesis%20on%20dynamic%20scenes.%20Project%20Page%3A%0Ahttps%3A//shape-of-motion.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShape%2520of%2520Motion%253A%25204D%2520Reconstruction%2520from%2520a%2520Single%2520Video%26entry.906535625%3DQianqian%2520Wang%2520and%2520Vickie%2520Ye%2520and%2520Hang%2520Gao%2520and%2520Jake%2520Austin%2520and%2520Zhengqi%2520Li%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520Monocular%2520dynamic%2520reconstruction%2520is%2520a%2520challenging%2520and%2520long-standing%2520vision%250Aproblem%2520due%2520to%2520the%2520highly%2520ill-posed%2520nature%2520of%2520the%2520task.%2520Existing%2520approaches%2520are%250Alimited%2520in%2520that%2520they%2520either%2520depend%2520on%2520templates%252C%2520are%2520effective%2520only%2520in%250Aquasi-static%2520scenes%252C%2520or%2520fail%2520to%2520model%25203D%2520motion%2520explicitly.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520method%2520capable%2520of%2520reconstructing%2520generic%2520dynamic%2520scenes%252C%2520featuring%250Aexplicit%252C%2520full-sequence-long%25203D%2520motion%252C%2520from%2520casually%2520captured%2520monocular%250Avideos.%2520We%2520tackle%2520the%2520under-constrained%2520nature%2520of%2520the%2520problem%2520with%2520two%2520key%250Ainsights%253A%2520First%252C%2520we%2520exploit%2520the%2520low-dimensional%2520structure%2520of%25203D%2520motion%2520by%250Arepresenting%2520scene%2520motion%2520with%2520a%2520compact%2520set%2520of%2520SE3%2520motion%2520bases.%2520Each%2520point%2527s%250Amotion%2520is%2520expressed%2520as%2520a%2520linear%2520combination%2520of%2520these%2520bases%252C%2520facilitating%2520soft%250Adecomposition%2520of%2520the%2520scene%2520into%2520multiple%2520rigidly-moving%2520groups.%2520Second%252C%2520we%250Autilize%2520a%2520comprehensive%2520set%2520of%2520data-driven%2520priors%252C%2520including%2520monocular%2520depth%250Amaps%2520and%2520long-range%25202D%2520tracks%252C%2520and%2520devise%2520a%2520method%2520to%2520effectively%2520consolidate%250Athese%2520noisy%2520supervisory%2520signals%252C%2520resulting%2520in%2520a%2520globally%2520consistent%250Arepresentation%2520of%2520the%2520dynamic%2520scene.%2520Experiments%2520show%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520for%2520both%2520long-range%25203D/2D%2520motion%2520estimation%2520and%250Anovel%2520view%2520synthesis%2520on%2520dynamic%2520scenes.%2520Project%2520Page%253A%250Ahttps%253A//shape-of-motion.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shape%20of%20Motion%3A%204D%20Reconstruction%20from%20a%20Single%20Video&entry.906535625=Qianqian%20Wang%20and%20Vickie%20Ye%20and%20Hang%20Gao%20and%20Jake%20Austin%20and%20Zhengqi%20Li%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20Monocular%20dynamic%20reconstruction%20is%20a%20challenging%20and%20long-standing%20vision%0Aproblem%20due%20to%20the%20highly%20ill-posed%20nature%20of%20the%20task.%20Existing%20approaches%20are%0Alimited%20in%20that%20they%20either%20depend%20on%20templates%2C%20are%20effective%20only%20in%0Aquasi-static%20scenes%2C%20or%20fail%20to%20model%203D%20motion%20explicitly.%20In%20this%20work%2C%20we%0Aintroduce%20a%20method%20capable%20of%20reconstructing%20generic%20dynamic%20scenes%2C%20featuring%0Aexplicit%2C%20full-sequence-long%203D%20motion%2C%20from%20casually%20captured%20monocular%0Avideos.%20We%20tackle%20the%20under-constrained%20nature%20of%20the%20problem%20with%20two%20key%0Ainsights%3A%20First%2C%20we%20exploit%20the%20low-dimensional%20structure%20of%203D%20motion%20by%0Arepresenting%20scene%20motion%20with%20a%20compact%20set%20of%20SE3%20motion%20bases.%20Each%20point%27s%0Amotion%20is%20expressed%20as%20a%20linear%20combination%20of%20these%20bases%2C%20facilitating%20soft%0Adecomposition%20of%20the%20scene%20into%20multiple%20rigidly-moving%20groups.%20Second%2C%20we%0Autilize%20a%20comprehensive%20set%20of%20data-driven%20priors%2C%20including%20monocular%20depth%0Amaps%20and%20long-range%202D%20tracks%2C%20and%20devise%20a%20method%20to%20effectively%20consolidate%0Athese%20noisy%20supervisory%20signals%2C%20resulting%20in%20a%20globally%20consistent%0Arepresentation%20of%20the%20dynamic%20scene.%20Experiments%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20for%20both%20long-range%203D/2D%20motion%20estimation%20and%0Anovel%20view%20synthesis%20on%20dynamic%20scenes.%20Project%20Page%3A%0Ahttps%3A//shape-of-motion.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13764v1&entry.124074799=Read"},
{"title": "Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian\n  Splatting", "author": "Jeongmin Bae and Seoha Kim and Youngsik Yun and Hahyun Lee and Gun Bang and Youngjung Uh", "abstract": "  As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view\nsynthesis, it is a natural extension to deform a canonical 3DGS to multiple\nframes for representing a dynamic scene. However, previous works fail to\naccurately reconstruct complex dynamic scenes. We attribute the failure to the\ndesign of the deformation field, which is built as a coordinate-based function.\nThis approach is problematic because 3DGS is a mixture of multiple fields\ncentered at the Gaussians, not just a single coordinate-based framework. To\nresolve this problem, we define the deformation as a function of per-Gaussian\nembeddings and temporal embeddings. Moreover, we decompose deformations as\ncoarse and fine deformations to model slow and fast movements, respectively.\nAlso, we introduce a local smoothness regularization for per-Gaussian embedding\nto improve the details in dynamic regions. Project page:\nhttps://jeongminb.github.io/e-d3dgs/\n", "link": "http://arxiv.org/abs/2404.03613v2", "date": "2024-07-18", "relevancy": 3.135, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6837}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6441}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Per-Gaussian%20Embedding-Based%20Deformation%20for%20Deformable%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20Per-Gaussian%20Embedding-Based%20Deformation%20for%20Deformable%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Jeongmin%20Bae%20and%20Seoha%20Kim%20and%20Youngsik%20Yun%20and%20Hahyun%20Lee%20and%20Gun%20Bang%20and%20Youngjung%20Uh%0AAbstract%3A%20%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20provides%20fast%20and%20high-quality%20novel%20view%0Asynthesis%2C%20it%20is%20a%20natural%20extension%20to%20deform%20a%20canonical%203DGS%20to%20multiple%0Aframes%20for%20representing%20a%20dynamic%20scene.%20However%2C%20previous%20works%20fail%20to%0Aaccurately%20reconstruct%20complex%20dynamic%20scenes.%20We%20attribute%20the%20failure%20to%20the%0Adesign%20of%20the%20deformation%20field%2C%20which%20is%20built%20as%20a%20coordinate-based%20function.%0AThis%20approach%20is%20problematic%20because%203DGS%20is%20a%20mixture%20of%20multiple%20fields%0Acentered%20at%20the%20Gaussians%2C%20not%20just%20a%20single%20coordinate-based%20framework.%20To%0Aresolve%20this%20problem%2C%20we%20define%20the%20deformation%20as%20a%20function%20of%20per-Gaussian%0Aembeddings%20and%20temporal%20embeddings.%20Moreover%2C%20we%20decompose%20deformations%20as%0Acoarse%20and%20fine%20deformations%20to%20model%20slow%20and%20fast%20movements%2C%20respectively.%0AAlso%2C%20we%20introduce%20a%20local%20smoothness%20regularization%20for%20per-Gaussian%20embedding%0Ato%20improve%20the%20details%20in%20dynamic%20regions.%20Project%20page%3A%0Ahttps%3A//jeongminb.github.io/e-d3dgs/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03613v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPer-Gaussian%2520Embedding-Based%2520Deformation%2520for%2520Deformable%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DJeongmin%2520Bae%2520and%2520Seoha%2520Kim%2520and%2520Youngsik%2520Yun%2520and%2520Hahyun%2520Lee%2520and%2520Gun%2520Bang%2520and%2520Youngjung%2520Uh%26entry.1292438233%3D%2520%2520As%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520provides%2520fast%2520and%2520high-quality%2520novel%2520view%250Asynthesis%252C%2520it%2520is%2520a%2520natural%2520extension%2520to%2520deform%2520a%2520canonical%25203DGS%2520to%2520multiple%250Aframes%2520for%2520representing%2520a%2520dynamic%2520scene.%2520However%252C%2520previous%2520works%2520fail%2520to%250Aaccurately%2520reconstruct%2520complex%2520dynamic%2520scenes.%2520We%2520attribute%2520the%2520failure%2520to%2520the%250Adesign%2520of%2520the%2520deformation%2520field%252C%2520which%2520is%2520built%2520as%2520a%2520coordinate-based%2520function.%250AThis%2520approach%2520is%2520problematic%2520because%25203DGS%2520is%2520a%2520mixture%2520of%2520multiple%2520fields%250Acentered%2520at%2520the%2520Gaussians%252C%2520not%2520just%2520a%2520single%2520coordinate-based%2520framework.%2520To%250Aresolve%2520this%2520problem%252C%2520we%2520define%2520the%2520deformation%2520as%2520a%2520function%2520of%2520per-Gaussian%250Aembeddings%2520and%2520temporal%2520embeddings.%2520Moreover%252C%2520we%2520decompose%2520deformations%2520as%250Acoarse%2520and%2520fine%2520deformations%2520to%2520model%2520slow%2520and%2520fast%2520movements%252C%2520respectively.%250AAlso%252C%2520we%2520introduce%2520a%2520local%2520smoothness%2520regularization%2520for%2520per-Gaussian%2520embedding%250Ato%2520improve%2520the%2520details%2520in%2520dynamic%2520regions.%2520Project%2520page%253A%250Ahttps%253A//jeongminb.github.io/e-d3dgs/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03613v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Per-Gaussian%20Embedding-Based%20Deformation%20for%20Deformable%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Jeongmin%20Bae%20and%20Seoha%20Kim%20and%20Youngsik%20Yun%20and%20Hahyun%20Lee%20and%20Gun%20Bang%20and%20Youngjung%20Uh&entry.1292438233=%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20provides%20fast%20and%20high-quality%20novel%20view%0Asynthesis%2C%20it%20is%20a%20natural%20extension%20to%20deform%20a%20canonical%203DGS%20to%20multiple%0Aframes%20for%20representing%20a%20dynamic%20scene.%20However%2C%20previous%20works%20fail%20to%0Aaccurately%20reconstruct%20complex%20dynamic%20scenes.%20We%20attribute%20the%20failure%20to%20the%0Adesign%20of%20the%20deformation%20field%2C%20which%20is%20built%20as%20a%20coordinate-based%20function.%0AThis%20approach%20is%20problematic%20because%203DGS%20is%20a%20mixture%20of%20multiple%20fields%0Acentered%20at%20the%20Gaussians%2C%20not%20just%20a%20single%20coordinate-based%20framework.%20To%0Aresolve%20this%20problem%2C%20we%20define%20the%20deformation%20as%20a%20function%20of%20per-Gaussian%0Aembeddings%20and%20temporal%20embeddings.%20Moreover%2C%20we%20decompose%20deformations%20as%0Acoarse%20and%20fine%20deformations%20to%20model%20slow%20and%20fast%20movements%2C%20respectively.%0AAlso%2C%20we%20introduce%20a%20local%20smoothness%20regularization%20for%20per-Gaussian%20embedding%0Ato%20improve%20the%20details%20in%20dynamic%20regions.%20Project%20page%3A%0Ahttps%3A//jeongminb.github.io/e-d3dgs/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03613v2&entry.124074799=Read"},
{"title": "MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images", "author": "Yuedong Chen and Haofei Xu and Chuanxia Zheng and Bohan Zhuang and Marc Pollefeys and Andreas Geiger and Tat-Jen Cham and Jianfei Cai", "abstract": "  We introduce MVSplat, an efficient model that, given sparse multi-view images\nas input, predicts clean feed-forward 3D Gaussians. To accurately localize the\nGaussian centers, we build a cost volume representation via plane sweeping,\nwhere the cross-view feature similarities stored in the cost volume can provide\nvaluable geometry cues to the estimation of depth. We also learn other Gaussian\nprimitives' parameters jointly with the Gaussian centers while only relying on\nphotometric supervision. We demonstrate the importance of the cost volume\nrepresentation in learning feed-forward Gaussians via extensive experimental\nevaluations. On the large-scale RealEstate10K and ACID benchmarks, MVSplat\nachieves state-of-the-art performance with the fastest feed-forward inference\nspeed (22~fps). More impressively, compared to the latest state-of-the-art\nmethod pixelSplat, MVSplat uses $10\\times$ fewer parameters and infers more\nthan $2\\times$ faster while providing higher appearance and geometry quality as\nwell as better cross-dataset generalization.\n", "link": "http://arxiv.org/abs/2403.14627v2", "date": "2024-07-18", "relevancy": 3.0985, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6864}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6256}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVSplat%3A%20Efficient%203D%20Gaussian%20Splatting%20from%20Sparse%20Multi-View%20Images&body=Title%3A%20MVSplat%3A%20Efficient%203D%20Gaussian%20Splatting%20from%20Sparse%20Multi-View%20Images%0AAuthor%3A%20Yuedong%20Chen%20and%20Haofei%20Xu%20and%20Chuanxia%20Zheng%20and%20Bohan%20Zhuang%20and%20Marc%20Pollefeys%20and%20Andreas%20Geiger%20and%20Tat-Jen%20Cham%20and%20Jianfei%20Cai%0AAbstract%3A%20%20%20We%20introduce%20MVSplat%2C%20an%20efficient%20model%20that%2C%20given%20sparse%20multi-view%20images%0Aas%20input%2C%20predicts%20clean%20feed-forward%203D%20Gaussians.%20To%20accurately%20localize%20the%0AGaussian%20centers%2C%20we%20build%20a%20cost%20volume%20representation%20via%20plane%20sweeping%2C%0Awhere%20the%20cross-view%20feature%20similarities%20stored%20in%20the%20cost%20volume%20can%20provide%0Avaluable%20geometry%20cues%20to%20the%20estimation%20of%20depth.%20We%20also%20learn%20other%20Gaussian%0Aprimitives%27%20parameters%20jointly%20with%20the%20Gaussian%20centers%20while%20only%20relying%20on%0Aphotometric%20supervision.%20We%20demonstrate%20the%20importance%20of%20the%20cost%20volume%0Arepresentation%20in%20learning%20feed-forward%20Gaussians%20via%20extensive%20experimental%0Aevaluations.%20On%20the%20large-scale%20RealEstate10K%20and%20ACID%20benchmarks%2C%20MVSplat%0Aachieves%20state-of-the-art%20performance%20with%20the%20fastest%20feed-forward%20inference%0Aspeed%20%2822~fps%29.%20More%20impressively%2C%20compared%20to%20the%20latest%20state-of-the-art%0Amethod%20pixelSplat%2C%20MVSplat%20uses%20%2410%5Ctimes%24%20fewer%20parameters%20and%20infers%20more%0Athan%20%242%5Ctimes%24%20faster%20while%20providing%20higher%20appearance%20and%20geometry%20quality%20as%0Awell%20as%20better%20cross-dataset%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14627v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVSplat%253A%2520Efficient%25203D%2520Gaussian%2520Splatting%2520from%2520Sparse%2520Multi-View%2520Images%26entry.906535625%3DYuedong%2520Chen%2520and%2520Haofei%2520Xu%2520and%2520Chuanxia%2520Zheng%2520and%2520Bohan%2520Zhuang%2520and%2520Marc%2520Pollefeys%2520and%2520Andreas%2520Geiger%2520and%2520Tat-Jen%2520Cham%2520and%2520Jianfei%2520Cai%26entry.1292438233%3D%2520%2520We%2520introduce%2520MVSplat%252C%2520an%2520efficient%2520model%2520that%252C%2520given%2520sparse%2520multi-view%2520images%250Aas%2520input%252C%2520predicts%2520clean%2520feed-forward%25203D%2520Gaussians.%2520To%2520accurately%2520localize%2520the%250AGaussian%2520centers%252C%2520we%2520build%2520a%2520cost%2520volume%2520representation%2520via%2520plane%2520sweeping%252C%250Awhere%2520the%2520cross-view%2520feature%2520similarities%2520stored%2520in%2520the%2520cost%2520volume%2520can%2520provide%250Avaluable%2520geometry%2520cues%2520to%2520the%2520estimation%2520of%2520depth.%2520We%2520also%2520learn%2520other%2520Gaussian%250Aprimitives%2527%2520parameters%2520jointly%2520with%2520the%2520Gaussian%2520centers%2520while%2520only%2520relying%2520on%250Aphotometric%2520supervision.%2520We%2520demonstrate%2520the%2520importance%2520of%2520the%2520cost%2520volume%250Arepresentation%2520in%2520learning%2520feed-forward%2520Gaussians%2520via%2520extensive%2520experimental%250Aevaluations.%2520On%2520the%2520large-scale%2520RealEstate10K%2520and%2520ACID%2520benchmarks%252C%2520MVSplat%250Aachieves%2520state-of-the-art%2520performance%2520with%2520the%2520fastest%2520feed-forward%2520inference%250Aspeed%2520%252822~fps%2529.%2520More%2520impressively%252C%2520compared%2520to%2520the%2520latest%2520state-of-the-art%250Amethod%2520pixelSplat%252C%2520MVSplat%2520uses%2520%252410%255Ctimes%2524%2520fewer%2520parameters%2520and%2520infers%2520more%250Athan%2520%25242%255Ctimes%2524%2520faster%2520while%2520providing%2520higher%2520appearance%2520and%2520geometry%2520quality%2520as%250Awell%2520as%2520better%2520cross-dataset%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14627v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVSplat%3A%20Efficient%203D%20Gaussian%20Splatting%20from%20Sparse%20Multi-View%20Images&entry.906535625=Yuedong%20Chen%20and%20Haofei%20Xu%20and%20Chuanxia%20Zheng%20and%20Bohan%20Zhuang%20and%20Marc%20Pollefeys%20and%20Andreas%20Geiger%20and%20Tat-Jen%20Cham%20and%20Jianfei%20Cai&entry.1292438233=%20%20We%20introduce%20MVSplat%2C%20an%20efficient%20model%20that%2C%20given%20sparse%20multi-view%20images%0Aas%20input%2C%20predicts%20clean%20feed-forward%203D%20Gaussians.%20To%20accurately%20localize%20the%0AGaussian%20centers%2C%20we%20build%20a%20cost%20volume%20representation%20via%20plane%20sweeping%2C%0Awhere%20the%20cross-view%20feature%20similarities%20stored%20in%20the%20cost%20volume%20can%20provide%0Avaluable%20geometry%20cues%20to%20the%20estimation%20of%20depth.%20We%20also%20learn%20other%20Gaussian%0Aprimitives%27%20parameters%20jointly%20with%20the%20Gaussian%20centers%20while%20only%20relying%20on%0Aphotometric%20supervision.%20We%20demonstrate%20the%20importance%20of%20the%20cost%20volume%0Arepresentation%20in%20learning%20feed-forward%20Gaussians%20via%20extensive%20experimental%0Aevaluations.%20On%20the%20large-scale%20RealEstate10K%20and%20ACID%20benchmarks%2C%20MVSplat%0Aachieves%20state-of-the-art%20performance%20with%20the%20fastest%20feed-forward%20inference%0Aspeed%20%2822~fps%29.%20More%20impressively%2C%20compared%20to%20the%20latest%20state-of-the-art%0Amethod%20pixelSplat%2C%20MVSplat%20uses%20%2410%5Ctimes%24%20fewer%20parameters%20and%20infers%20more%0Athan%20%242%5Ctimes%24%20faster%20while%20providing%20higher%20appearance%20and%20geometry%20quality%20as%0Awell%20as%20better%20cross-dataset%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14627v2&entry.124074799=Read"},
{"title": "EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian\n  Splatting", "author": "Yuchen Weng and Zhengwen Shen and Ruofan Chen and Qi Wang and Jun Wang", "abstract": "  3D deblurring reconstruction techniques have recently seen significant\nadvancements with the development of Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). Although these techniques can recover relatively\nclear 3D reconstructions from blurry image inputs, they still face limitations\nin handling severe blurring and complex camera motion. To address these issues,\nwe propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting\n(EaDeblur-GS), which integrates event camera data to enhance the robustness of\n3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE)\nnetwork to estimate Gaussian center deviations and using novel loss functions,\nEaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating\nperformance comparable to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.13520v1", "date": "2024-07-18", "relevancy": 3.0983, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6905}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6287}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EaDeblur-GS%3A%20Event%20assisted%203D%20Deblur%20Reconstruction%20with%20Gaussian%0A%20%20Splatting&body=Title%3A%20EaDeblur-GS%3A%20Event%20assisted%203D%20Deblur%20Reconstruction%20with%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Yuchen%20Weng%20and%20Zhengwen%20Shen%20and%20Ruofan%20Chen%20and%20Qi%20Wang%20and%20Jun%20Wang%0AAbstract%3A%20%20%203D%20deblurring%20reconstruction%20techniques%20have%20recently%20seen%20significant%0Aadvancements%20with%20the%20development%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29.%20Although%20these%20techniques%20can%20recover%20relatively%0Aclear%203D%20reconstructions%20from%20blurry%20image%20inputs%2C%20they%20still%20face%20limitations%0Ain%20handling%20severe%20blurring%20and%20complex%20camera%20motion.%20To%20address%20these%20issues%2C%0Awe%20propose%20Event-assisted%203D%20Deblur%20Reconstruction%20with%20Gaussian%20Splatting%0A%28EaDeblur-GS%29%2C%20which%20integrates%20event%20camera%20data%20to%20enhance%20the%20robustness%20of%0A3DGS%20against%20motion%20blur.%20By%20employing%20an%20Adaptive%20Deviation%20Estimator%20%28ADE%29%0Anetwork%20to%20estimate%20Gaussian%20center%20deviations%20and%20using%20novel%20loss%20functions%2C%0AEaDeblur-GS%20achieves%20sharp%203D%20reconstructions%20in%20real-time%2C%20demonstrating%0Aperformance%20comparable%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEaDeblur-GS%253A%2520Event%2520assisted%25203D%2520Deblur%2520Reconstruction%2520with%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DYuchen%2520Weng%2520and%2520Zhengwen%2520Shen%2520and%2520Ruofan%2520Chen%2520and%2520Qi%2520Wang%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%25203D%2520deblurring%2520reconstruction%2520techniques%2520have%2520recently%2520seen%2520significant%250Aadvancements%2520with%2520the%2520development%2520of%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520and%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529.%2520Although%2520these%2520techniques%2520can%2520recover%2520relatively%250Aclear%25203D%2520reconstructions%2520from%2520blurry%2520image%2520inputs%252C%2520they%2520still%2520face%2520limitations%250Ain%2520handling%2520severe%2520blurring%2520and%2520complex%2520camera%2520motion.%2520To%2520address%2520these%2520issues%252C%250Awe%2520propose%2520Event-assisted%25203D%2520Deblur%2520Reconstruction%2520with%2520Gaussian%2520Splatting%250A%2528EaDeblur-GS%2529%252C%2520which%2520integrates%2520event%2520camera%2520data%2520to%2520enhance%2520the%2520robustness%2520of%250A3DGS%2520against%2520motion%2520blur.%2520By%2520employing%2520an%2520Adaptive%2520Deviation%2520Estimator%2520%2528ADE%2529%250Anetwork%2520to%2520estimate%2520Gaussian%2520center%2520deviations%2520and%2520using%2520novel%2520loss%2520functions%252C%250AEaDeblur-GS%2520achieves%2520sharp%25203D%2520reconstructions%2520in%2520real-time%252C%2520demonstrating%250Aperformance%2520comparable%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EaDeblur-GS%3A%20Event%20assisted%203D%20Deblur%20Reconstruction%20with%20Gaussian%0A%20%20Splatting&entry.906535625=Yuchen%20Weng%20and%20Zhengwen%20Shen%20and%20Ruofan%20Chen%20and%20Qi%20Wang%20and%20Jun%20Wang&entry.1292438233=%20%203D%20deblurring%20reconstruction%20techniques%20have%20recently%20seen%20significant%0Aadvancements%20with%20the%20development%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29.%20Although%20these%20techniques%20can%20recover%20relatively%0Aclear%203D%20reconstructions%20from%20blurry%20image%20inputs%2C%20they%20still%20face%20limitations%0Ain%20handling%20severe%20blurring%20and%20complex%20camera%20motion.%20To%20address%20these%20issues%2C%0Awe%20propose%20Event-assisted%203D%20Deblur%20Reconstruction%20with%20Gaussian%20Splatting%0A%28EaDeblur-GS%29%2C%20which%20integrates%20event%20camera%20data%20to%20enhance%20the%20robustness%20of%0A3DGS%20against%20motion%20blur.%20By%20employing%20an%20Adaptive%20Deviation%20Estimator%20%28ADE%29%0Anetwork%20to%20estimate%20Gaussian%20center%20deviations%20and%20using%20novel%20loss%20functions%2C%0AEaDeblur-GS%20achieves%20sharp%203D%20reconstructions%20in%20real-time%2C%20demonstrating%0Aperformance%20comparable%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13520v1&entry.124074799=Read"},
{"title": "MinD-3D: Reconstruct High-quality 3D objects in Human Brain", "author": "Jianxiong Gao and Yuqian Fu and Yun Wang and Xuelin Qian and Jianfeng Feng and Yanwei Fu", "abstract": "  In this paper, we introduce Recon3DMind, an innovative task aimed at\nreconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI)\nsignals, marking a significant advancement in the fields of cognitive\nneuroscience and computer vision. To support this pioneering task, we present\nthe fMRI-Shape dataset, which includes data from 14 participants and features\n360-degree videos of 3D objects to enable comprehensive fMRI signal capture\nacross various settings, thereby laying a foundation for future research.\nFurthermore, we propose MinD-3D, a novel and effective three-stage framework\nspecifically designed to decode the brain's 3D visual information from fMRI\nsignals, demonstrating the feasibility of this challenging task. The framework\nbegins by extracting and aggregating features from fMRI frames through a\nneuro-fusion encoder, subsequently employs a feature bridge diffusion model to\ngenerate visual features, and ultimately recovers the 3D object via a\ngenerative transformer decoder. We assess the performance of MinD-3D using a\nsuite of semantic and structural metrics and analyze the correlation between\nthe features extracted by our model and the visual regions of interest (ROIs)\nin fMRI signals. Our findings indicate that MinD-3D not only reconstructs 3D\nobjects with high semantic relevance and spatial similarity but also\nsignificantly enhances our understanding of the human brain's capabilities in\nprocessing 3D visual information. Project page at:\nhttps://jianxgao.github.io/MinD-3D.\n", "link": "http://arxiv.org/abs/2312.07485v3", "date": "2024-07-18", "relevancy": 3.0833, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6362}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6362}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MinD-3D%3A%20Reconstruct%20High-quality%203D%20objects%20in%20Human%20Brain&body=Title%3A%20MinD-3D%3A%20Reconstruct%20High-quality%203D%20objects%20in%20Human%20Brain%0AAuthor%3A%20Jianxiong%20Gao%20and%20Yuqian%20Fu%20and%20Yun%20Wang%20and%20Xuelin%20Qian%20and%20Jianfeng%20Feng%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Recon3DMind%2C%20an%20innovative%20task%20aimed%20at%0Areconstructing%203D%20visuals%20from%20Functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%0Asignals%2C%20marking%20a%20significant%20advancement%20in%20the%20fields%20of%20cognitive%0Aneuroscience%20and%20computer%20vision.%20To%20support%20this%20pioneering%20task%2C%20we%20present%0Athe%20fMRI-Shape%20dataset%2C%20which%20includes%20data%20from%2014%20participants%20and%20features%0A360-degree%20videos%20of%203D%20objects%20to%20enable%20comprehensive%20fMRI%20signal%20capture%0Aacross%20various%20settings%2C%20thereby%20laying%20a%20foundation%20for%20future%20research.%0AFurthermore%2C%20we%20propose%20MinD-3D%2C%20a%20novel%20and%20effective%20three-stage%20framework%0Aspecifically%20designed%20to%20decode%20the%20brain%27s%203D%20visual%20information%20from%20fMRI%0Asignals%2C%20demonstrating%20the%20feasibility%20of%20this%20challenging%20task.%20The%20framework%0Abegins%20by%20extracting%20and%20aggregating%20features%20from%20fMRI%20frames%20through%20a%0Aneuro-fusion%20encoder%2C%20subsequently%20employs%20a%20feature%20bridge%20diffusion%20model%20to%0Agenerate%20visual%20features%2C%20and%20ultimately%20recovers%20the%203D%20object%20via%20a%0Agenerative%20transformer%20decoder.%20We%20assess%20the%20performance%20of%20MinD-3D%20using%20a%0Asuite%20of%20semantic%20and%20structural%20metrics%20and%20analyze%20the%20correlation%20between%0Athe%20features%20extracted%20by%20our%20model%20and%20the%20visual%20regions%20of%20interest%20%28ROIs%29%0Ain%20fMRI%20signals.%20Our%20findings%20indicate%20that%20MinD-3D%20not%20only%20reconstructs%203D%0Aobjects%20with%20high%20semantic%20relevance%20and%20spatial%20similarity%20but%20also%0Asignificantly%20enhances%20our%20understanding%20of%20the%20human%20brain%27s%20capabilities%20in%0Aprocessing%203D%20visual%20information.%20Project%20page%20at%3A%0Ahttps%3A//jianxgao.github.io/MinD-3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07485v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinD-3D%253A%2520Reconstruct%2520High-quality%25203D%2520objects%2520in%2520Human%2520Brain%26entry.906535625%3DJianxiong%2520Gao%2520and%2520Yuqian%2520Fu%2520and%2520Yun%2520Wang%2520and%2520Xuelin%2520Qian%2520and%2520Jianfeng%2520Feng%2520and%2520Yanwei%2520Fu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Recon3DMind%252C%2520an%2520innovative%2520task%2520aimed%2520at%250Areconstructing%25203D%2520visuals%2520from%2520Functional%2520Magnetic%2520Resonance%2520Imaging%2520%2528fMRI%2529%250Asignals%252C%2520marking%2520a%2520significant%2520advancement%2520in%2520the%2520fields%2520of%2520cognitive%250Aneuroscience%2520and%2520computer%2520vision.%2520To%2520support%2520this%2520pioneering%2520task%252C%2520we%2520present%250Athe%2520fMRI-Shape%2520dataset%252C%2520which%2520includes%2520data%2520from%252014%2520participants%2520and%2520features%250A360-degree%2520videos%2520of%25203D%2520objects%2520to%2520enable%2520comprehensive%2520fMRI%2520signal%2520capture%250Aacross%2520various%2520settings%252C%2520thereby%2520laying%2520a%2520foundation%2520for%2520future%2520research.%250AFurthermore%252C%2520we%2520propose%2520MinD-3D%252C%2520a%2520novel%2520and%2520effective%2520three-stage%2520framework%250Aspecifically%2520designed%2520to%2520decode%2520the%2520brain%2527s%25203D%2520visual%2520information%2520from%2520fMRI%250Asignals%252C%2520demonstrating%2520the%2520feasibility%2520of%2520this%2520challenging%2520task.%2520The%2520framework%250Abegins%2520by%2520extracting%2520and%2520aggregating%2520features%2520from%2520fMRI%2520frames%2520through%2520a%250Aneuro-fusion%2520encoder%252C%2520subsequently%2520employs%2520a%2520feature%2520bridge%2520diffusion%2520model%2520to%250Agenerate%2520visual%2520features%252C%2520and%2520ultimately%2520recovers%2520the%25203D%2520object%2520via%2520a%250Agenerative%2520transformer%2520decoder.%2520We%2520assess%2520the%2520performance%2520of%2520MinD-3D%2520using%2520a%250Asuite%2520of%2520semantic%2520and%2520structural%2520metrics%2520and%2520analyze%2520the%2520correlation%2520between%250Athe%2520features%2520extracted%2520by%2520our%2520model%2520and%2520the%2520visual%2520regions%2520of%2520interest%2520%2528ROIs%2529%250Ain%2520fMRI%2520signals.%2520Our%2520findings%2520indicate%2520that%2520MinD-3D%2520not%2520only%2520reconstructs%25203D%250Aobjects%2520with%2520high%2520semantic%2520relevance%2520and%2520spatial%2520similarity%2520but%2520also%250Asignificantly%2520enhances%2520our%2520understanding%2520of%2520the%2520human%2520brain%2527s%2520capabilities%2520in%250Aprocessing%25203D%2520visual%2520information.%2520Project%2520page%2520at%253A%250Ahttps%253A//jianxgao.github.io/MinD-3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07485v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MinD-3D%3A%20Reconstruct%20High-quality%203D%20objects%20in%20Human%20Brain&entry.906535625=Jianxiong%20Gao%20and%20Yuqian%20Fu%20and%20Yun%20Wang%20and%20Xuelin%20Qian%20and%20Jianfeng%20Feng%20and%20Yanwei%20Fu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Recon3DMind%2C%20an%20innovative%20task%20aimed%20at%0Areconstructing%203D%20visuals%20from%20Functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%0Asignals%2C%20marking%20a%20significant%20advancement%20in%20the%20fields%20of%20cognitive%0Aneuroscience%20and%20computer%20vision.%20To%20support%20this%20pioneering%20task%2C%20we%20present%0Athe%20fMRI-Shape%20dataset%2C%20which%20includes%20data%20from%2014%20participants%20and%20features%0A360-degree%20videos%20of%203D%20objects%20to%20enable%20comprehensive%20fMRI%20signal%20capture%0Aacross%20various%20settings%2C%20thereby%20laying%20a%20foundation%20for%20future%20research.%0AFurthermore%2C%20we%20propose%20MinD-3D%2C%20a%20novel%20and%20effective%20three-stage%20framework%0Aspecifically%20designed%20to%20decode%20the%20brain%27s%203D%20visual%20information%20from%20fMRI%0Asignals%2C%20demonstrating%20the%20feasibility%20of%20this%20challenging%20task.%20The%20framework%0Abegins%20by%20extracting%20and%20aggregating%20features%20from%20fMRI%20frames%20through%20a%0Aneuro-fusion%20encoder%2C%20subsequently%20employs%20a%20feature%20bridge%20diffusion%20model%20to%0Agenerate%20visual%20features%2C%20and%20ultimately%20recovers%20the%203D%20object%20via%20a%0Agenerative%20transformer%20decoder.%20We%20assess%20the%20performance%20of%20MinD-3D%20using%20a%0Asuite%20of%20semantic%20and%20structural%20metrics%20and%20analyze%20the%20correlation%20between%0Athe%20features%20extracted%20by%20our%20model%20and%20the%20visual%20regions%20of%20interest%20%28ROIs%29%0Ain%20fMRI%20signals.%20Our%20findings%20indicate%20that%20MinD-3D%20not%20only%20reconstructs%203D%0Aobjects%20with%20high%20semantic%20relevance%20and%20spatial%20similarity%20but%20also%0Asignificantly%20enhances%20our%20understanding%20of%20the%20human%20brain%27s%20capabilities%20in%0Aprocessing%203D%20visual%20information.%20Project%20page%20at%3A%0Ahttps%3A//jianxgao.github.io/MinD-3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07485v3&entry.124074799=Read"},
{"title": "Omni-Recon: Harnessing Image-based Rendering for General-Purpose Neural\n  Radiance Fields", "author": "Yonggan Fu and Huaizhi Qu and Zhifan Ye and Chaojian Li and Kevin Zhao and Yingyan Lin", "abstract": "  Recent breakthroughs in Neural Radiance Fields (NeRFs) have sparked\nsignificant demand for their integration into real-world 3D applications.\nHowever, the varied functionalities required by different 3D applications often\nnecessitate diverse NeRF models with various pipelines, leading to tedious NeRF\ntraining for each target task and cumbersome trial-and-error experiments.\nDrawing inspiration from the generalization capability and adaptability of\nemerging foundation models, our work aims to develop one general-purpose NeRF\nfor handling diverse 3D tasks. We achieve this by proposing a framework called\nOmni-Recon, which is capable of (1) generalizable 3D reconstruction and\nzero-shot multitask scene understanding, and (2) adaptability to diverse\ndownstream 3D applications such as real-time rendering and scene editing. Our\nkey insight is that an image-based rendering pipeline, with accurate geometry\nand appearance estimation, can lift 2D image features into their 3D\ncounterparts, thus extending widely explored 2D tasks to the 3D world in a\ngeneralizable manner. Specifically, our Omni-Recon features a general-purpose\nNeRF model using image-based rendering with two decoupled branches: one complex\ntransformer-based branch that progressively fuses geometry and appearance\nfeatures for accurate geometry estimation, and one lightweight branch for\npredicting blending weights of source views. This design achieves\nstate-of-the-art (SOTA) generalizable 3D surface reconstruction quality with\nblending weights reusable across diverse tasks for zero-shot multitask scene\nunderstanding. In addition, it can enable real-time rendering after baking the\ncomplex geometry branch into meshes, swift adaptation to achieve SOTA\ngeneralizable 3D understanding performance, and seamless integration with 2D\ndiffusion models for text-guided 3D editing.\n", "link": "http://arxiv.org/abs/2403.11131v2", "date": "2024-07-18", "relevancy": 3.0271, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6305}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5929}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-Recon%3A%20Harnessing%20Image-based%20Rendering%20for%20General-Purpose%20Neural%0A%20%20Radiance%20Fields&body=Title%3A%20Omni-Recon%3A%20Harnessing%20Image-based%20Rendering%20for%20General-Purpose%20Neural%0A%20%20Radiance%20Fields%0AAuthor%3A%20Yonggan%20Fu%20and%20Huaizhi%20Qu%20and%20Zhifan%20Ye%20and%20Chaojian%20Li%20and%20Kevin%20Zhao%20and%20Yingyan%20Lin%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20sparked%0Asignificant%20demand%20for%20their%20integration%20into%20real-world%203D%20applications.%0AHowever%2C%20the%20varied%20functionalities%20required%20by%20different%203D%20applications%20often%0Anecessitate%20diverse%20NeRF%20models%20with%20various%20pipelines%2C%20leading%20to%20tedious%20NeRF%0Atraining%20for%20each%20target%20task%20and%20cumbersome%20trial-and-error%20experiments.%0ADrawing%20inspiration%20from%20the%20generalization%20capability%20and%20adaptability%20of%0Aemerging%20foundation%20models%2C%20our%20work%20aims%20to%20develop%20one%20general-purpose%20NeRF%0Afor%20handling%20diverse%203D%20tasks.%20We%20achieve%20this%20by%20proposing%20a%20framework%20called%0AOmni-Recon%2C%20which%20is%20capable%20of%20%281%29%20generalizable%203D%20reconstruction%20and%0Azero-shot%20multitask%20scene%20understanding%2C%20and%20%282%29%20adaptability%20to%20diverse%0Adownstream%203D%20applications%20such%20as%20real-time%20rendering%20and%20scene%20editing.%20Our%0Akey%20insight%20is%20that%20an%20image-based%20rendering%20pipeline%2C%20with%20accurate%20geometry%0Aand%20appearance%20estimation%2C%20can%20lift%202D%20image%20features%20into%20their%203D%0Acounterparts%2C%20thus%20extending%20widely%20explored%202D%20tasks%20to%20the%203D%20world%20in%20a%0Ageneralizable%20manner.%20Specifically%2C%20our%20Omni-Recon%20features%20a%20general-purpose%0ANeRF%20model%20using%20image-based%20rendering%20with%20two%20decoupled%20branches%3A%20one%20complex%0Atransformer-based%20branch%20that%20progressively%20fuses%20geometry%20and%20appearance%0Afeatures%20for%20accurate%20geometry%20estimation%2C%20and%20one%20lightweight%20branch%20for%0Apredicting%20blending%20weights%20of%20source%20views.%20This%20design%20achieves%0Astate-of-the-art%20%28SOTA%29%20generalizable%203D%20surface%20reconstruction%20quality%20with%0Ablending%20weights%20reusable%20across%20diverse%20tasks%20for%20zero-shot%20multitask%20scene%0Aunderstanding.%20In%20addition%2C%20it%20can%20enable%20real-time%20rendering%20after%20baking%20the%0Acomplex%20geometry%20branch%20into%20meshes%2C%20swift%20adaptation%20to%20achieve%20SOTA%0Ageneralizable%203D%20understanding%20performance%2C%20and%20seamless%20integration%20with%202D%0Adiffusion%20models%20for%20text-guided%203D%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-Recon%253A%2520Harnessing%2520Image-based%2520Rendering%2520for%2520General-Purpose%2520Neural%250A%2520%2520Radiance%2520Fields%26entry.906535625%3DYonggan%2520Fu%2520and%2520Huaizhi%2520Qu%2520and%2520Zhifan%2520Ye%2520and%2520Chaojian%2520Li%2520and%2520Kevin%2520Zhao%2520and%2520Yingyan%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520breakthroughs%2520in%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520have%2520sparked%250Asignificant%2520demand%2520for%2520their%2520integration%2520into%2520real-world%25203D%2520applications.%250AHowever%252C%2520the%2520varied%2520functionalities%2520required%2520by%2520different%25203D%2520applications%2520often%250Anecessitate%2520diverse%2520NeRF%2520models%2520with%2520various%2520pipelines%252C%2520leading%2520to%2520tedious%2520NeRF%250Atraining%2520for%2520each%2520target%2520task%2520and%2520cumbersome%2520trial-and-error%2520experiments.%250ADrawing%2520inspiration%2520from%2520the%2520generalization%2520capability%2520and%2520adaptability%2520of%250Aemerging%2520foundation%2520models%252C%2520our%2520work%2520aims%2520to%2520develop%2520one%2520general-purpose%2520NeRF%250Afor%2520handling%2520diverse%25203D%2520tasks.%2520We%2520achieve%2520this%2520by%2520proposing%2520a%2520framework%2520called%250AOmni-Recon%252C%2520which%2520is%2520capable%2520of%2520%25281%2529%2520generalizable%25203D%2520reconstruction%2520and%250Azero-shot%2520multitask%2520scene%2520understanding%252C%2520and%2520%25282%2529%2520adaptability%2520to%2520diverse%250Adownstream%25203D%2520applications%2520such%2520as%2520real-time%2520rendering%2520and%2520scene%2520editing.%2520Our%250Akey%2520insight%2520is%2520that%2520an%2520image-based%2520rendering%2520pipeline%252C%2520with%2520accurate%2520geometry%250Aand%2520appearance%2520estimation%252C%2520can%2520lift%25202D%2520image%2520features%2520into%2520their%25203D%250Acounterparts%252C%2520thus%2520extending%2520widely%2520explored%25202D%2520tasks%2520to%2520the%25203D%2520world%2520in%2520a%250Ageneralizable%2520manner.%2520Specifically%252C%2520our%2520Omni-Recon%2520features%2520a%2520general-purpose%250ANeRF%2520model%2520using%2520image-based%2520rendering%2520with%2520two%2520decoupled%2520branches%253A%2520one%2520complex%250Atransformer-based%2520branch%2520that%2520progressively%2520fuses%2520geometry%2520and%2520appearance%250Afeatures%2520for%2520accurate%2520geometry%2520estimation%252C%2520and%2520one%2520lightweight%2520branch%2520for%250Apredicting%2520blending%2520weights%2520of%2520source%2520views.%2520This%2520design%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520generalizable%25203D%2520surface%2520reconstruction%2520quality%2520with%250Ablending%2520weights%2520reusable%2520across%2520diverse%2520tasks%2520for%2520zero-shot%2520multitask%2520scene%250Aunderstanding.%2520In%2520addition%252C%2520it%2520can%2520enable%2520real-time%2520rendering%2520after%2520baking%2520the%250Acomplex%2520geometry%2520branch%2520into%2520meshes%252C%2520swift%2520adaptation%2520to%2520achieve%2520SOTA%250Ageneralizable%25203D%2520understanding%2520performance%252C%2520and%2520seamless%2520integration%2520with%25202D%250Adiffusion%2520models%2520for%2520text-guided%25203D%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-Recon%3A%20Harnessing%20Image-based%20Rendering%20for%20General-Purpose%20Neural%0A%20%20Radiance%20Fields&entry.906535625=Yonggan%20Fu%20and%20Huaizhi%20Qu%20and%20Zhifan%20Ye%20and%20Chaojian%20Li%20and%20Kevin%20Zhao%20and%20Yingyan%20Lin&entry.1292438233=%20%20Recent%20breakthroughs%20in%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20sparked%0Asignificant%20demand%20for%20their%20integration%20into%20real-world%203D%20applications.%0AHowever%2C%20the%20varied%20functionalities%20required%20by%20different%203D%20applications%20often%0Anecessitate%20diverse%20NeRF%20models%20with%20various%20pipelines%2C%20leading%20to%20tedious%20NeRF%0Atraining%20for%20each%20target%20task%20and%20cumbersome%20trial-and-error%20experiments.%0ADrawing%20inspiration%20from%20the%20generalization%20capability%20and%20adaptability%20of%0Aemerging%20foundation%20models%2C%20our%20work%20aims%20to%20develop%20one%20general-purpose%20NeRF%0Afor%20handling%20diverse%203D%20tasks.%20We%20achieve%20this%20by%20proposing%20a%20framework%20called%0AOmni-Recon%2C%20which%20is%20capable%20of%20%281%29%20generalizable%203D%20reconstruction%20and%0Azero-shot%20multitask%20scene%20understanding%2C%20and%20%282%29%20adaptability%20to%20diverse%0Adownstream%203D%20applications%20such%20as%20real-time%20rendering%20and%20scene%20editing.%20Our%0Akey%20insight%20is%20that%20an%20image-based%20rendering%20pipeline%2C%20with%20accurate%20geometry%0Aand%20appearance%20estimation%2C%20can%20lift%202D%20image%20features%20into%20their%203D%0Acounterparts%2C%20thus%20extending%20widely%20explored%202D%20tasks%20to%20the%203D%20world%20in%20a%0Ageneralizable%20manner.%20Specifically%2C%20our%20Omni-Recon%20features%20a%20general-purpose%0ANeRF%20model%20using%20image-based%20rendering%20with%20two%20decoupled%20branches%3A%20one%20complex%0Atransformer-based%20branch%20that%20progressively%20fuses%20geometry%20and%20appearance%0Afeatures%20for%20accurate%20geometry%20estimation%2C%20and%20one%20lightweight%20branch%20for%0Apredicting%20blending%20weights%20of%20source%20views.%20This%20design%20achieves%0Astate-of-the-art%20%28SOTA%29%20generalizable%203D%20surface%20reconstruction%20quality%20with%0Ablending%20weights%20reusable%20across%20diverse%20tasks%20for%20zero-shot%20multitask%20scene%0Aunderstanding.%20In%20addition%2C%20it%20can%20enable%20real-time%20rendering%20after%20baking%20the%0Acomplex%20geometry%20branch%20into%20meshes%2C%20swift%20adaptation%20to%20achieve%20SOTA%0Ageneralizable%203D%20understanding%20performance%2C%20and%20seamless%20integration%20with%202D%0Adiffusion%20models%20for%20text-guided%203D%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11131v2&entry.124074799=Read"},
{"title": "Point-JEPA: A Joint Embedding Predictive Architecture for\n  Self-Supervised Learning on Point Cloud", "author": "Ayumu Saito and Jiju Poovvancheri", "abstract": "  Recent advancements in self-supervised learning in the point cloud domain\nhave demonstrated significant potential. However, these methods often suffer\nfrom drawbacks, including lengthy pre-training time, the necessity of\nreconstruction in the input space, or the necessity of additional modalities.\nIn order to address these issues, we introduce Point-JEPA, a joint embedding\npredictive architecture designed specifically for point cloud data. To this\nend, we introduce a sequencer that orders point cloud tokens to efficiently\ncompute and utilize tokens proximity based on their indices during target and\ncontext selection. The sequencer also allows shared computations of the tokens\nproximity between context and target selection, further improving the\nefficiency. Experimentally, our method achieves competitive results with\nstate-of-the-art methods while avoiding the reconstruction in the input space\nor additional modality.\n", "link": "http://arxiv.org/abs/2404.16432v3", "date": "2024-07-18", "relevancy": 2.9807, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6726}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5677}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point-JEPA%3A%20A%20Joint%20Embedding%20Predictive%20Architecture%20for%0A%20%20Self-Supervised%20Learning%20on%20Point%20Cloud&body=Title%3A%20Point-JEPA%3A%20A%20Joint%20Embedding%20Predictive%20Architecture%20for%0A%20%20Self-Supervised%20Learning%20on%20Point%20Cloud%0AAuthor%3A%20Ayumu%20Saito%20and%20Jiju%20Poovvancheri%0AAbstract%3A%20%20%20Recent%20advancements%20in%20self-supervised%20learning%20in%20the%20point%20cloud%20domain%0Ahave%20demonstrated%20significant%20potential.%20However%2C%20these%20methods%20often%20suffer%0Afrom%20drawbacks%2C%20including%20lengthy%20pre-training%20time%2C%20the%20necessity%20of%0Areconstruction%20in%20the%20input%20space%2C%20or%20the%20necessity%20of%20additional%20modalities.%0AIn%20order%20to%20address%20these%20issues%2C%20we%20introduce%20Point-JEPA%2C%20a%20joint%20embedding%0Apredictive%20architecture%20designed%20specifically%20for%20point%20cloud%20data.%20To%20this%0Aend%2C%20we%20introduce%20a%20sequencer%20that%20orders%20point%20cloud%20tokens%20to%20efficiently%0Acompute%20and%20utilize%20tokens%20proximity%20based%20on%20their%20indices%20during%20target%20and%0Acontext%20selection.%20The%20sequencer%20also%20allows%20shared%20computations%20of%20the%20tokens%0Aproximity%20between%20context%20and%20target%20selection%2C%20further%20improving%20the%0Aefficiency.%20Experimentally%2C%20our%20method%20achieves%20competitive%20results%20with%0Astate-of-the-art%20methods%20while%20avoiding%20the%20reconstruction%20in%20the%20input%20space%0Aor%20additional%20modality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16432v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint-JEPA%253A%2520A%2520Joint%2520Embedding%2520Predictive%2520Architecture%2520for%250A%2520%2520Self-Supervised%2520Learning%2520on%2520Point%2520Cloud%26entry.906535625%3DAyumu%2520Saito%2520and%2520Jiju%2520Poovvancheri%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520self-supervised%2520learning%2520in%2520the%2520point%2520cloud%2520domain%250Ahave%2520demonstrated%2520significant%2520potential.%2520However%252C%2520these%2520methods%2520often%2520suffer%250Afrom%2520drawbacks%252C%2520including%2520lengthy%2520pre-training%2520time%252C%2520the%2520necessity%2520of%250Areconstruction%2520in%2520the%2520input%2520space%252C%2520or%2520the%2520necessity%2520of%2520additional%2520modalities.%250AIn%2520order%2520to%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Point-JEPA%252C%2520a%2520joint%2520embedding%250Apredictive%2520architecture%2520designed%2520specifically%2520for%2520point%2520cloud%2520data.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520a%2520sequencer%2520that%2520orders%2520point%2520cloud%2520tokens%2520to%2520efficiently%250Acompute%2520and%2520utilize%2520tokens%2520proximity%2520based%2520on%2520their%2520indices%2520during%2520target%2520and%250Acontext%2520selection.%2520The%2520sequencer%2520also%2520allows%2520shared%2520computations%2520of%2520the%2520tokens%250Aproximity%2520between%2520context%2520and%2520target%2520selection%252C%2520further%2520improving%2520the%250Aefficiency.%2520Experimentally%252C%2520our%2520method%2520achieves%2520competitive%2520results%2520with%250Astate-of-the-art%2520methods%2520while%2520avoiding%2520the%2520reconstruction%2520in%2520the%2520input%2520space%250Aor%2520additional%2520modality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16432v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point-JEPA%3A%20A%20Joint%20Embedding%20Predictive%20Architecture%20for%0A%20%20Self-Supervised%20Learning%20on%20Point%20Cloud&entry.906535625=Ayumu%20Saito%20and%20Jiju%20Poovvancheri&entry.1292438233=%20%20Recent%20advancements%20in%20self-supervised%20learning%20in%20the%20point%20cloud%20domain%0Ahave%20demonstrated%20significant%20potential.%20However%2C%20these%20methods%20often%20suffer%0Afrom%20drawbacks%2C%20including%20lengthy%20pre-training%20time%2C%20the%20necessity%20of%0Areconstruction%20in%20the%20input%20space%2C%20or%20the%20necessity%20of%20additional%20modalities.%0AIn%20order%20to%20address%20these%20issues%2C%20we%20introduce%20Point-JEPA%2C%20a%20joint%20embedding%0Apredictive%20architecture%20designed%20specifically%20for%20point%20cloud%20data.%20To%20this%0Aend%2C%20we%20introduce%20a%20sequencer%20that%20orders%20point%20cloud%20tokens%20to%20efficiently%0Acompute%20and%20utilize%20tokens%20proximity%20based%20on%20their%20indices%20during%20target%20and%0Acontext%20selection.%20The%20sequencer%20also%20allows%20shared%20computations%20of%20the%20tokens%0Aproximity%20between%20context%20and%20target%20selection%2C%20further%20improving%20the%0Aefficiency.%20Experimentally%2C%20our%20method%20achieves%20competitive%20results%20with%0Astate-of-the-art%20methods%20while%20avoiding%20the%20reconstruction%20in%20the%20input%20space%0Aor%20additional%20modality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16432v3&entry.124074799=Read"},
{"title": "NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation\n  Learning for Neural Radiance Fields", "author": "Muhammad Zubair Irshad and Sergey Zakharov and Vitor Guizilini and Adrien Gaidon and Zsolt Kira and Rares Ambrus", "abstract": "  Neural fields excel in computer vision and robotics due to their ability to\nunderstand the 3D visual world such as inferring semantics, geometry, and\ndynamics. Given the capabilities of neural fields in densely representing a 3D\nscene from 2D images, we ask the question: Can we scale their self-supervised\npretraining, specifically using masked autoencoders, to generate effective 3D\nrepresentations from posed RGB images. Owing to the astounding success of\nextending transformers to novel data modalities, we employ standard 3D Vision\nTransformers to suit the unique formulation of NeRFs. We leverage NeRF's\nvolumetric grid as a dense input to the transformer, contrasting it with other\n3D representations such as pointclouds where the information density can be\nuneven, and the representation is irregular. Due to the difficulty of applying\nmasked autoencoders to an implicit representation, such as NeRF, we opt for\nextracting an explicit representation that canonicalizes scenes across domains\nby employing the camera trajectory for sampling. Our goal is made possible by\nmasking random patches from NeRF's radiance and density grid and employing a\nstandard 3D Swin Transformer to reconstruct the masked patches. In doing so,\nthe model can learn the semantic and spatial structure of complete scenes. We\npretrain this representation at scale on our proposed curated posed-RGB data,\ntotaling over 1.8 million images. Once pretrained, the encoder is used for\neffective 3D transfer learning. Our novel self-supervised pretraining for\nNeRFs, NeRF-MAE, scales remarkably well and improves performance on various\nchallenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,\nNeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF\nscene understanding baselines on Front3D and ScanNet datasets with an absolute\nperformance improvement of over 20% AP50 and 8% AP25 for 3D object detection.\n", "link": "http://arxiv.org/abs/2404.01300v3", "date": "2024-07-18", "relevancy": 2.9451, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6262}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5721}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRF-MAE%3A%20Masked%20AutoEncoders%20for%20Self-Supervised%203D%20Representation%0A%20%20Learning%20for%20Neural%20Radiance%20Fields&body=Title%3A%20NeRF-MAE%3A%20Masked%20AutoEncoders%20for%20Self-Supervised%203D%20Representation%0A%20%20Learning%20for%20Neural%20Radiance%20Fields%0AAuthor%3A%20Muhammad%20Zubair%20Irshad%20and%20Sergey%20Zakharov%20and%20Vitor%20Guizilini%20and%20Adrien%20Gaidon%20and%20Zsolt%20Kira%20and%20Rares%20Ambrus%0AAbstract%3A%20%20%20Neural%20fields%20excel%20in%20computer%20vision%20and%20robotics%20due%20to%20their%20ability%20to%0Aunderstand%20the%203D%20visual%20world%20such%20as%20inferring%20semantics%2C%20geometry%2C%20and%0Adynamics.%20Given%20the%20capabilities%20of%20neural%20fields%20in%20densely%20representing%20a%203D%0Ascene%20from%202D%20images%2C%20we%20ask%20the%20question%3A%20Can%20we%20scale%20their%20self-supervised%0Apretraining%2C%20specifically%20using%20masked%20autoencoders%2C%20to%20generate%20effective%203D%0Arepresentations%20from%20posed%20RGB%20images.%20Owing%20to%20the%20astounding%20success%20of%0Aextending%20transformers%20to%20novel%20data%20modalities%2C%20we%20employ%20standard%203D%20Vision%0ATransformers%20to%20suit%20the%20unique%20formulation%20of%20NeRFs.%20We%20leverage%20NeRF%27s%0Avolumetric%20grid%20as%20a%20dense%20input%20to%20the%20transformer%2C%20contrasting%20it%20with%20other%0A3D%20representations%20such%20as%20pointclouds%20where%20the%20information%20density%20can%20be%0Auneven%2C%20and%20the%20representation%20is%20irregular.%20Due%20to%20the%20difficulty%20of%20applying%0Amasked%20autoencoders%20to%20an%20implicit%20representation%2C%20such%20as%20NeRF%2C%20we%20opt%20for%0Aextracting%20an%20explicit%20representation%20that%20canonicalizes%20scenes%20across%20domains%0Aby%20employing%20the%20camera%20trajectory%20for%20sampling.%20Our%20goal%20is%20made%20possible%20by%0Amasking%20random%20patches%20from%20NeRF%27s%20radiance%20and%20density%20grid%20and%20employing%20a%0Astandard%203D%20Swin%20Transformer%20to%20reconstruct%20the%20masked%20patches.%20In%20doing%20so%2C%0Athe%20model%20can%20learn%20the%20semantic%20and%20spatial%20structure%20of%20complete%20scenes.%20We%0Apretrain%20this%20representation%20at%20scale%20on%20our%20proposed%20curated%20posed-RGB%20data%2C%0Atotaling%20over%201.8%20million%20images.%20Once%20pretrained%2C%20the%20encoder%20is%20used%20for%0Aeffective%203D%20transfer%20learning.%20Our%20novel%20self-supervised%20pretraining%20for%0ANeRFs%2C%20NeRF-MAE%2C%20scales%20remarkably%20well%20and%20improves%20performance%20on%20various%0Achallenging%203D%20tasks.%20Utilizing%20unlabeled%20posed%202D%20data%20for%20pretraining%2C%0ANeRF-MAE%20significantly%20outperforms%20self-supervised%203D%20pretraining%20and%20NeRF%0Ascene%20understanding%20baselines%20on%20Front3D%20and%20ScanNet%20datasets%20with%20an%20absolute%0Aperformance%20improvement%20of%20over%2020%25%20AP50%20and%208%25%20AP25%20for%203D%20object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01300v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRF-MAE%253A%2520Masked%2520AutoEncoders%2520for%2520Self-Supervised%25203D%2520Representation%250A%2520%2520Learning%2520for%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DMuhammad%2520Zubair%2520Irshad%2520and%2520Sergey%2520Zakharov%2520and%2520Vitor%2520Guizilini%2520and%2520Adrien%2520Gaidon%2520and%2520Zsolt%2520Kira%2520and%2520Rares%2520Ambrus%26entry.1292438233%3D%2520%2520Neural%2520fields%2520excel%2520in%2520computer%2520vision%2520and%2520robotics%2520due%2520to%2520their%2520ability%2520to%250Aunderstand%2520the%25203D%2520visual%2520world%2520such%2520as%2520inferring%2520semantics%252C%2520geometry%252C%2520and%250Adynamics.%2520Given%2520the%2520capabilities%2520of%2520neural%2520fields%2520in%2520densely%2520representing%2520a%25203D%250Ascene%2520from%25202D%2520images%252C%2520we%2520ask%2520the%2520question%253A%2520Can%2520we%2520scale%2520their%2520self-supervised%250Apretraining%252C%2520specifically%2520using%2520masked%2520autoencoders%252C%2520to%2520generate%2520effective%25203D%250Arepresentations%2520from%2520posed%2520RGB%2520images.%2520Owing%2520to%2520the%2520astounding%2520success%2520of%250Aextending%2520transformers%2520to%2520novel%2520data%2520modalities%252C%2520we%2520employ%2520standard%25203D%2520Vision%250ATransformers%2520to%2520suit%2520the%2520unique%2520formulation%2520of%2520NeRFs.%2520We%2520leverage%2520NeRF%2527s%250Avolumetric%2520grid%2520as%2520a%2520dense%2520input%2520to%2520the%2520transformer%252C%2520contrasting%2520it%2520with%2520other%250A3D%2520representations%2520such%2520as%2520pointclouds%2520where%2520the%2520information%2520density%2520can%2520be%250Auneven%252C%2520and%2520the%2520representation%2520is%2520irregular.%2520Due%2520to%2520the%2520difficulty%2520of%2520applying%250Amasked%2520autoencoders%2520to%2520an%2520implicit%2520representation%252C%2520such%2520as%2520NeRF%252C%2520we%2520opt%2520for%250Aextracting%2520an%2520explicit%2520representation%2520that%2520canonicalizes%2520scenes%2520across%2520domains%250Aby%2520employing%2520the%2520camera%2520trajectory%2520for%2520sampling.%2520Our%2520goal%2520is%2520made%2520possible%2520by%250Amasking%2520random%2520patches%2520from%2520NeRF%2527s%2520radiance%2520and%2520density%2520grid%2520and%2520employing%2520a%250Astandard%25203D%2520Swin%2520Transformer%2520to%2520reconstruct%2520the%2520masked%2520patches.%2520In%2520doing%2520so%252C%250Athe%2520model%2520can%2520learn%2520the%2520semantic%2520and%2520spatial%2520structure%2520of%2520complete%2520scenes.%2520We%250Apretrain%2520this%2520representation%2520at%2520scale%2520on%2520our%2520proposed%2520curated%2520posed-RGB%2520data%252C%250Atotaling%2520over%25201.8%2520million%2520images.%2520Once%2520pretrained%252C%2520the%2520encoder%2520is%2520used%2520for%250Aeffective%25203D%2520transfer%2520learning.%2520Our%2520novel%2520self-supervised%2520pretraining%2520for%250ANeRFs%252C%2520NeRF-MAE%252C%2520scales%2520remarkably%2520well%2520and%2520improves%2520performance%2520on%2520various%250Achallenging%25203D%2520tasks.%2520Utilizing%2520unlabeled%2520posed%25202D%2520data%2520for%2520pretraining%252C%250ANeRF-MAE%2520significantly%2520outperforms%2520self-supervised%25203D%2520pretraining%2520and%2520NeRF%250Ascene%2520understanding%2520baselines%2520on%2520Front3D%2520and%2520ScanNet%2520datasets%2520with%2520an%2520absolute%250Aperformance%2520improvement%2520of%2520over%252020%2525%2520AP50%2520and%25208%2525%2520AP25%2520for%25203D%2520object%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01300v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF-MAE%3A%20Masked%20AutoEncoders%20for%20Self-Supervised%203D%20Representation%0A%20%20Learning%20for%20Neural%20Radiance%20Fields&entry.906535625=Muhammad%20Zubair%20Irshad%20and%20Sergey%20Zakharov%20and%20Vitor%20Guizilini%20and%20Adrien%20Gaidon%20and%20Zsolt%20Kira%20and%20Rares%20Ambrus&entry.1292438233=%20%20Neural%20fields%20excel%20in%20computer%20vision%20and%20robotics%20due%20to%20their%20ability%20to%0Aunderstand%20the%203D%20visual%20world%20such%20as%20inferring%20semantics%2C%20geometry%2C%20and%0Adynamics.%20Given%20the%20capabilities%20of%20neural%20fields%20in%20densely%20representing%20a%203D%0Ascene%20from%202D%20images%2C%20we%20ask%20the%20question%3A%20Can%20we%20scale%20their%20self-supervised%0Apretraining%2C%20specifically%20using%20masked%20autoencoders%2C%20to%20generate%20effective%203D%0Arepresentations%20from%20posed%20RGB%20images.%20Owing%20to%20the%20astounding%20success%20of%0Aextending%20transformers%20to%20novel%20data%20modalities%2C%20we%20employ%20standard%203D%20Vision%0ATransformers%20to%20suit%20the%20unique%20formulation%20of%20NeRFs.%20We%20leverage%20NeRF%27s%0Avolumetric%20grid%20as%20a%20dense%20input%20to%20the%20transformer%2C%20contrasting%20it%20with%20other%0A3D%20representations%20such%20as%20pointclouds%20where%20the%20information%20density%20can%20be%0Auneven%2C%20and%20the%20representation%20is%20irregular.%20Due%20to%20the%20difficulty%20of%20applying%0Amasked%20autoencoders%20to%20an%20implicit%20representation%2C%20such%20as%20NeRF%2C%20we%20opt%20for%0Aextracting%20an%20explicit%20representation%20that%20canonicalizes%20scenes%20across%20domains%0Aby%20employing%20the%20camera%20trajectory%20for%20sampling.%20Our%20goal%20is%20made%20possible%20by%0Amasking%20random%20patches%20from%20NeRF%27s%20radiance%20and%20density%20grid%20and%20employing%20a%0Astandard%203D%20Swin%20Transformer%20to%20reconstruct%20the%20masked%20patches.%20In%20doing%20so%2C%0Athe%20model%20can%20learn%20the%20semantic%20and%20spatial%20structure%20of%20complete%20scenes.%20We%0Apretrain%20this%20representation%20at%20scale%20on%20our%20proposed%20curated%20posed-RGB%20data%2C%0Atotaling%20over%201.8%20million%20images.%20Once%20pretrained%2C%20the%20encoder%20is%20used%20for%0Aeffective%203D%20transfer%20learning.%20Our%20novel%20self-supervised%20pretraining%20for%0ANeRFs%2C%20NeRF-MAE%2C%20scales%20remarkably%20well%20and%20improves%20performance%20on%20various%0Achallenging%203D%20tasks.%20Utilizing%20unlabeled%20posed%202D%20data%20for%20pretraining%2C%0ANeRF-MAE%20significantly%20outperforms%20self-supervised%203D%20pretraining%20and%20NeRF%0Ascene%20understanding%20baselines%20on%20Front3D%20and%20ScanNet%20datasets%20with%20an%20absolute%0Aperformance%20improvement%20of%20over%2020%25%20AP50%20and%208%25%20AP25%20for%203D%20object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01300v3&entry.124074799=Read"},
{"title": "Open Vocabulary 3D Scene Understanding via Geometry Guided\n  Self-Distillation", "author": "Pengfei Wang and Yuxi Wang and Shuai Li and Zhaoxiang Zhang and Zhen Lei and Lei Zhang", "abstract": "  The scarcity of large-scale 3D-text paired data poses a great challenge on\nopen vocabulary 3D scene understanding, and hence it is popular to leverage\ninternet-scale 2D data and transfer their open vocabulary capabilities to 3D\nmodels through knowledge distillation. However, the existing distillation-based\n3D scene understanding approaches rely on the representation capacity of 2D\nmodels, disregarding the exploration of geometric priors and inherent\nrepresentational advantages offered by 3D data. In this paper, we propose an\neffective approach, namely Geometry Guided Self-Distillation (GGSD), to learn\nsuperior 3D representations from 2D pre-trained models. Specifically, we first\ndesign a geometry guided distillation module to distill knowledge from 2D\nmodels, and then leverage the 3D geometric priors to alleviate the inherent\nnoise in 2D models and enhance the representation learning process. Due to the\nadvantages of 3D representation, the performance of the distilled 3D student\nmodel can significantly surpass that of the 2D teacher model. This motivates us\nto further leverage the representation advantages of 3D data through\nself-distillation. As a result, our proposed GGSD approach outperforms the\nexisting open vocabulary 3D scene understanding methods by a large margin, as\ndemonstrated by our experiments on both indoor and outdoor benchmark datasets.\n", "link": "http://arxiv.org/abs/2407.13362v1", "date": "2024-07-18", "relevancy": 2.9364, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5885}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5867}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Vocabulary%203D%20Scene%20Understanding%20via%20Geometry%20Guided%0A%20%20Self-Distillation&body=Title%3A%20Open%20Vocabulary%203D%20Scene%20Understanding%20via%20Geometry%20Guided%0A%20%20Self-Distillation%0AAuthor%3A%20Pengfei%20Wang%20and%20Yuxi%20Wang%20and%20Shuai%20Li%20and%20Zhaoxiang%20Zhang%20and%20Zhen%20Lei%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20The%20scarcity%20of%20large-scale%203D-text%20paired%20data%20poses%20a%20great%20challenge%20on%0Aopen%20vocabulary%203D%20scene%20understanding%2C%20and%20hence%20it%20is%20popular%20to%20leverage%0Ainternet-scale%202D%20data%20and%20transfer%20their%20open%20vocabulary%20capabilities%20to%203D%0Amodels%20through%20knowledge%20distillation.%20However%2C%20the%20existing%20distillation-based%0A3D%20scene%20understanding%20approaches%20rely%20on%20the%20representation%20capacity%20of%202D%0Amodels%2C%20disregarding%20the%20exploration%20of%20geometric%20priors%20and%20inherent%0Arepresentational%20advantages%20offered%20by%203D%20data.%20In%20this%20paper%2C%20we%20propose%20an%0Aeffective%20approach%2C%20namely%20Geometry%20Guided%20Self-Distillation%20%28GGSD%29%2C%20to%20learn%0Asuperior%203D%20representations%20from%202D%20pre-trained%20models.%20Specifically%2C%20we%20first%0Adesign%20a%20geometry%20guided%20distillation%20module%20to%20distill%20knowledge%20from%202D%0Amodels%2C%20and%20then%20leverage%20the%203D%20geometric%20priors%20to%20alleviate%20the%20inherent%0Anoise%20in%202D%20models%20and%20enhance%20the%20representation%20learning%20process.%20Due%20to%20the%0Aadvantages%20of%203D%20representation%2C%20the%20performance%20of%20the%20distilled%203D%20student%0Amodel%20can%20significantly%20surpass%20that%20of%20the%202D%20teacher%20model.%20This%20motivates%20us%0Ato%20further%20leverage%20the%20representation%20advantages%20of%203D%20data%20through%0Aself-distillation.%20As%20a%20result%2C%20our%20proposed%20GGSD%20approach%20outperforms%20the%0Aexisting%20open%20vocabulary%203D%20scene%20understanding%20methods%20by%20a%20large%20margin%2C%20as%0Ademonstrated%20by%20our%20experiments%20on%20both%20indoor%20and%20outdoor%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Vocabulary%25203D%2520Scene%2520Understanding%2520via%2520Geometry%2520Guided%250A%2520%2520Self-Distillation%26entry.906535625%3DPengfei%2520Wang%2520and%2520Yuxi%2520Wang%2520and%2520Shuai%2520Li%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Zhen%2520Lei%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520scarcity%2520of%2520large-scale%25203D-text%2520paired%2520data%2520poses%2520a%2520great%2520challenge%2520on%250Aopen%2520vocabulary%25203D%2520scene%2520understanding%252C%2520and%2520hence%2520it%2520is%2520popular%2520to%2520leverage%250Ainternet-scale%25202D%2520data%2520and%2520transfer%2520their%2520open%2520vocabulary%2520capabilities%2520to%25203D%250Amodels%2520through%2520knowledge%2520distillation.%2520However%252C%2520the%2520existing%2520distillation-based%250A3D%2520scene%2520understanding%2520approaches%2520rely%2520on%2520the%2520representation%2520capacity%2520of%25202D%250Amodels%252C%2520disregarding%2520the%2520exploration%2520of%2520geometric%2520priors%2520and%2520inherent%250Arepresentational%2520advantages%2520offered%2520by%25203D%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Aeffective%2520approach%252C%2520namely%2520Geometry%2520Guided%2520Self-Distillation%2520%2528GGSD%2529%252C%2520to%2520learn%250Asuperior%25203D%2520representations%2520from%25202D%2520pre-trained%2520models.%2520Specifically%252C%2520we%2520first%250Adesign%2520a%2520geometry%2520guided%2520distillation%2520module%2520to%2520distill%2520knowledge%2520from%25202D%250Amodels%252C%2520and%2520then%2520leverage%2520the%25203D%2520geometric%2520priors%2520to%2520alleviate%2520the%2520inherent%250Anoise%2520in%25202D%2520models%2520and%2520enhance%2520the%2520representation%2520learning%2520process.%2520Due%2520to%2520the%250Aadvantages%2520of%25203D%2520representation%252C%2520the%2520performance%2520of%2520the%2520distilled%25203D%2520student%250Amodel%2520can%2520significantly%2520surpass%2520that%2520of%2520the%25202D%2520teacher%2520model.%2520This%2520motivates%2520us%250Ato%2520further%2520leverage%2520the%2520representation%2520advantages%2520of%25203D%2520data%2520through%250Aself-distillation.%2520As%2520a%2520result%252C%2520our%2520proposed%2520GGSD%2520approach%2520outperforms%2520the%250Aexisting%2520open%2520vocabulary%25203D%2520scene%2520understanding%2520methods%2520by%2520a%2520large%2520margin%252C%2520as%250Ademonstrated%2520by%2520our%2520experiments%2520on%2520both%2520indoor%2520and%2520outdoor%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Vocabulary%203D%20Scene%20Understanding%20via%20Geometry%20Guided%0A%20%20Self-Distillation&entry.906535625=Pengfei%20Wang%20and%20Yuxi%20Wang%20and%20Shuai%20Li%20and%20Zhaoxiang%20Zhang%20and%20Zhen%20Lei%20and%20Lei%20Zhang&entry.1292438233=%20%20The%20scarcity%20of%20large-scale%203D-text%20paired%20data%20poses%20a%20great%20challenge%20on%0Aopen%20vocabulary%203D%20scene%20understanding%2C%20and%20hence%20it%20is%20popular%20to%20leverage%0Ainternet-scale%202D%20data%20and%20transfer%20their%20open%20vocabulary%20capabilities%20to%203D%0Amodels%20through%20knowledge%20distillation.%20However%2C%20the%20existing%20distillation-based%0A3D%20scene%20understanding%20approaches%20rely%20on%20the%20representation%20capacity%20of%202D%0Amodels%2C%20disregarding%20the%20exploration%20of%20geometric%20priors%20and%20inherent%0Arepresentational%20advantages%20offered%20by%203D%20data.%20In%20this%20paper%2C%20we%20propose%20an%0Aeffective%20approach%2C%20namely%20Geometry%20Guided%20Self-Distillation%20%28GGSD%29%2C%20to%20learn%0Asuperior%203D%20representations%20from%202D%20pre-trained%20models.%20Specifically%2C%20we%20first%0Adesign%20a%20geometry%20guided%20distillation%20module%20to%20distill%20knowledge%20from%202D%0Amodels%2C%20and%20then%20leverage%20the%203D%20geometric%20priors%20to%20alleviate%20the%20inherent%0Anoise%20in%202D%20models%20and%20enhance%20the%20representation%20learning%20process.%20Due%20to%20the%0Aadvantages%20of%203D%20representation%2C%20the%20performance%20of%20the%20distilled%203D%20student%0Amodel%20can%20significantly%20surpass%20that%20of%20the%202D%20teacher%20model.%20This%20motivates%20us%0Ato%20further%20leverage%20the%20representation%20advantages%20of%203D%20data%20through%0Aself-distillation.%20As%20a%20result%2C%20our%20proposed%20GGSD%20approach%20outperforms%20the%0Aexisting%20open%20vocabulary%203D%20scene%20understanding%20methods%20by%20a%20large%20margin%2C%20as%0Ademonstrated%20by%20our%20experiments%20on%20both%20indoor%20and%20outdoor%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13362v1&entry.124074799=Read"},
{"title": "TransCAD: A Hierarchical Transformer for CAD Sequence Inference from\n  Point Clouds", "author": "Elona Dupont and Kseniya Cherenkova and Dimitrios Mallis and Gleb Gusev and Anis Kacem and Djamila Aouada", "abstract": "  3D reverse engineering, in which a CAD model is inferred given a 3D scan of a\nphysical object, is a research direction that offers many promising practical\napplications. This paper proposes TransCAD, an end-to-end transformer-based\narchitecture that predicts the CAD sequence from a point cloud. TransCAD\nleverages the structure of CAD sequences by using a hierarchical learning\nstrategy. A loop refiner is also introduced to regress sketch primitive\nparameters. Rigorous experimentation on the DeepCAD and Fusion360 datasets show\nthat TransCAD achieves state-of-the-art results. The result analysis is\nsupported with a proposed metric for CAD sequence, the mean Average Precision\nof CAD Sequence, that addresses the limitations of existing metrics.\n", "link": "http://arxiv.org/abs/2407.12702v2", "date": "2024-07-18", "relevancy": 2.877, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5889}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5889}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransCAD%3A%20A%20Hierarchical%20Transformer%20for%20CAD%20Sequence%20Inference%20from%0A%20%20Point%20Clouds&body=Title%3A%20TransCAD%3A%20A%20Hierarchical%20Transformer%20for%20CAD%20Sequence%20Inference%20from%0A%20%20Point%20Clouds%0AAuthor%3A%20Elona%20Dupont%20and%20Kseniya%20Cherenkova%20and%20Dimitrios%20Mallis%20and%20Gleb%20Gusev%20and%20Anis%20Kacem%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%203D%20reverse%20engineering%2C%20in%20which%20a%20CAD%20model%20is%20inferred%20given%20a%203D%20scan%20of%20a%0Aphysical%20object%2C%20is%20a%20research%20direction%20that%20offers%20many%20promising%20practical%0Aapplications.%20This%20paper%20proposes%20TransCAD%2C%20an%20end-to-end%20transformer-based%0Aarchitecture%20that%20predicts%20the%20CAD%20sequence%20from%20a%20point%20cloud.%20TransCAD%0Aleverages%20the%20structure%20of%20CAD%20sequences%20by%20using%20a%20hierarchical%20learning%0Astrategy.%20A%20loop%20refiner%20is%20also%20introduced%20to%20regress%20sketch%20primitive%0Aparameters.%20Rigorous%20experimentation%20on%20the%20DeepCAD%20and%20Fusion360%20datasets%20show%0Athat%20TransCAD%20achieves%20state-of-the-art%20results.%20The%20result%20analysis%20is%0Asupported%20with%20a%20proposed%20metric%20for%20CAD%20sequence%2C%20the%20mean%20Average%20Precision%0Aof%20CAD%20Sequence%2C%20that%20addresses%20the%20limitations%20of%20existing%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12702v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransCAD%253A%2520A%2520Hierarchical%2520Transformer%2520for%2520CAD%2520Sequence%2520Inference%2520from%250A%2520%2520Point%2520Clouds%26entry.906535625%3DElona%2520Dupont%2520and%2520Kseniya%2520Cherenkova%2520and%2520Dimitrios%2520Mallis%2520and%2520Gleb%2520Gusev%2520and%2520Anis%2520Kacem%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%25203D%2520reverse%2520engineering%252C%2520in%2520which%2520a%2520CAD%2520model%2520is%2520inferred%2520given%2520a%25203D%2520scan%2520of%2520a%250Aphysical%2520object%252C%2520is%2520a%2520research%2520direction%2520that%2520offers%2520many%2520promising%2520practical%250Aapplications.%2520This%2520paper%2520proposes%2520TransCAD%252C%2520an%2520end-to-end%2520transformer-based%250Aarchitecture%2520that%2520predicts%2520the%2520CAD%2520sequence%2520from%2520a%2520point%2520cloud.%2520TransCAD%250Aleverages%2520the%2520structure%2520of%2520CAD%2520sequences%2520by%2520using%2520a%2520hierarchical%2520learning%250Astrategy.%2520A%2520loop%2520refiner%2520is%2520also%2520introduced%2520to%2520regress%2520sketch%2520primitive%250Aparameters.%2520Rigorous%2520experimentation%2520on%2520the%2520DeepCAD%2520and%2520Fusion360%2520datasets%2520show%250Athat%2520TransCAD%2520achieves%2520state-of-the-art%2520results.%2520The%2520result%2520analysis%2520is%250Asupported%2520with%2520a%2520proposed%2520metric%2520for%2520CAD%2520sequence%252C%2520the%2520mean%2520Average%2520Precision%250Aof%2520CAD%2520Sequence%252C%2520that%2520addresses%2520the%2520limitations%2520of%2520existing%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12702v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransCAD%3A%20A%20Hierarchical%20Transformer%20for%20CAD%20Sequence%20Inference%20from%0A%20%20Point%20Clouds&entry.906535625=Elona%20Dupont%20and%20Kseniya%20Cherenkova%20and%20Dimitrios%20Mallis%20and%20Gleb%20Gusev%20and%20Anis%20Kacem%20and%20Djamila%20Aouada&entry.1292438233=%20%203D%20reverse%20engineering%2C%20in%20which%20a%20CAD%20model%20is%20inferred%20given%20a%203D%20scan%20of%20a%0Aphysical%20object%2C%20is%20a%20research%20direction%20that%20offers%20many%20promising%20practical%0Aapplications.%20This%20paper%20proposes%20TransCAD%2C%20an%20end-to-end%20transformer-based%0Aarchitecture%20that%20predicts%20the%20CAD%20sequence%20from%20a%20point%20cloud.%20TransCAD%0Aleverages%20the%20structure%20of%20CAD%20sequences%20by%20using%20a%20hierarchical%20learning%0Astrategy.%20A%20loop%20refiner%20is%20also%20introduced%20to%20regress%20sketch%20primitive%0Aparameters.%20Rigorous%20experimentation%20on%20the%20DeepCAD%20and%20Fusion360%20datasets%20show%0Athat%20TransCAD%20achieves%20state-of-the-art%20results.%20The%20result%20analysis%20is%0Asupported%20with%20a%20proposed%20metric%20for%20CAD%20sequence%2C%20the%20mean%20Average%20Precision%0Aof%20CAD%20Sequence%2C%20that%20addresses%20the%20limitations%20of%20existing%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12702v2&entry.124074799=Read"},
{"title": "MeshSegmenter: Zero-Shot Mesh Semantic Segmentation via Texture\n  Synthesis", "author": "Ziming Zhong and Yanxu Xu and Jing Li and Jiale Xu and Zhengxin Li and Chaohui Yu and Shenghua Gao", "abstract": "  We present MeshSegmenter, a simple yet effective framework designed for\nzero-shot 3D semantic segmentation. This model successfully extends the\npowerful capabilities of 2D segmentation models to 3D meshes, delivering\naccurate 3D segmentation across diverse meshes and segment descriptions.\nSpecifically, our model leverages the Segment Anything Model (SAM) model to\nsegment the target regions from images rendered from the 3D shape. In light of\nthe importance of the texture for segmentation, we also leverage the pretrained\nstable diffusion model to generate images with textures from 3D shape, and\nleverage SAM to segment the target regions from images with textures. Textures\nsupplement the shape for segmentation and facilitate accurate 3D segmentation\neven in geometrically non-prominent areas, such as segmenting a car door within\na car mesh. To achieve the 3D segments, we render 2D images from different\nviews and conduct segmentation for both textured and untextured images. Lastly,\nwe develop a multi-view revoting scheme that integrates 2D segmentation results\nand confidence scores from various views onto the 3D mesh, ensuring the 3D\nconsistency of segmentation results and eliminating inaccuracies from specific\nperspectives. Through these innovations, MeshSegmenter offers stable and\nreliable 3D segmentation results both quantitatively and qualitatively,\nhighlighting its potential as a transformative tool in the field of 3D\nzero-shot segmentation. The code is available at\n\\url{https://github.com/zimingzhong/MeshSegmenter}.\n", "link": "http://arxiv.org/abs/2407.13675v1", "date": "2024-07-18", "relevancy": 2.8725, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5904}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5836}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshSegmenter%3A%20Zero-Shot%20Mesh%20Semantic%20Segmentation%20via%20Texture%0A%20%20Synthesis&body=Title%3A%20MeshSegmenter%3A%20Zero-Shot%20Mesh%20Semantic%20Segmentation%20via%20Texture%0A%20%20Synthesis%0AAuthor%3A%20Ziming%20Zhong%20and%20Yanxu%20Xu%20and%20Jing%20Li%20and%20Jiale%20Xu%20and%20Zhengxin%20Li%20and%20Chaohui%20Yu%20and%20Shenghua%20Gao%0AAbstract%3A%20%20%20We%20present%20MeshSegmenter%2C%20a%20simple%20yet%20effective%20framework%20designed%20for%0Azero-shot%203D%20semantic%20segmentation.%20This%20model%20successfully%20extends%20the%0Apowerful%20capabilities%20of%202D%20segmentation%20models%20to%203D%20meshes%2C%20delivering%0Aaccurate%203D%20segmentation%20across%20diverse%20meshes%20and%20segment%20descriptions.%0ASpecifically%2C%20our%20model%20leverages%20the%20Segment%20Anything%20Model%20%28SAM%29%20model%20to%0Asegment%20the%20target%20regions%20from%20images%20rendered%20from%20the%203D%20shape.%20In%20light%20of%0Athe%20importance%20of%20the%20texture%20for%20segmentation%2C%20we%20also%20leverage%20the%20pretrained%0Astable%20diffusion%20model%20to%20generate%20images%20with%20textures%20from%203D%20shape%2C%20and%0Aleverage%20SAM%20to%20segment%20the%20target%20regions%20from%20images%20with%20textures.%20Textures%0Asupplement%20the%20shape%20for%20segmentation%20and%20facilitate%20accurate%203D%20segmentation%0Aeven%20in%20geometrically%20non-prominent%20areas%2C%20such%20as%20segmenting%20a%20car%20door%20within%0Aa%20car%20mesh.%20To%20achieve%20the%203D%20segments%2C%20we%20render%202D%20images%20from%20different%0Aviews%20and%20conduct%20segmentation%20for%20both%20textured%20and%20untextured%20images.%20Lastly%2C%0Awe%20develop%20a%20multi-view%20revoting%20scheme%20that%20integrates%202D%20segmentation%20results%0Aand%20confidence%20scores%20from%20various%20views%20onto%20the%203D%20mesh%2C%20ensuring%20the%203D%0Aconsistency%20of%20segmentation%20results%20and%20eliminating%20inaccuracies%20from%20specific%0Aperspectives.%20Through%20these%20innovations%2C%20MeshSegmenter%20offers%20stable%20and%0Areliable%203D%20segmentation%20results%20both%20quantitatively%20and%20qualitatively%2C%0Ahighlighting%20its%20potential%20as%20a%20transformative%20tool%20in%20the%20field%20of%203D%0Azero-shot%20segmentation.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zimingzhong/MeshSegmenter%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshSegmenter%253A%2520Zero-Shot%2520Mesh%2520Semantic%2520Segmentation%2520via%2520Texture%250A%2520%2520Synthesis%26entry.906535625%3DZiming%2520Zhong%2520and%2520Yanxu%2520Xu%2520and%2520Jing%2520Li%2520and%2520Jiale%2520Xu%2520and%2520Zhengxin%2520Li%2520and%2520Chaohui%2520Yu%2520and%2520Shenghua%2520Gao%26entry.1292438233%3D%2520%2520We%2520present%2520MeshSegmenter%252C%2520a%2520simple%2520yet%2520effective%2520framework%2520designed%2520for%250Azero-shot%25203D%2520semantic%2520segmentation.%2520This%2520model%2520successfully%2520extends%2520the%250Apowerful%2520capabilities%2520of%25202D%2520segmentation%2520models%2520to%25203D%2520meshes%252C%2520delivering%250Aaccurate%25203D%2520segmentation%2520across%2520diverse%2520meshes%2520and%2520segment%2520descriptions.%250ASpecifically%252C%2520our%2520model%2520leverages%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520model%2520to%250Asegment%2520the%2520target%2520regions%2520from%2520images%2520rendered%2520from%2520the%25203D%2520shape.%2520In%2520light%2520of%250Athe%2520importance%2520of%2520the%2520texture%2520for%2520segmentation%252C%2520we%2520also%2520leverage%2520the%2520pretrained%250Astable%2520diffusion%2520model%2520to%2520generate%2520images%2520with%2520textures%2520from%25203D%2520shape%252C%2520and%250Aleverage%2520SAM%2520to%2520segment%2520the%2520target%2520regions%2520from%2520images%2520with%2520textures.%2520Textures%250Asupplement%2520the%2520shape%2520for%2520segmentation%2520and%2520facilitate%2520accurate%25203D%2520segmentation%250Aeven%2520in%2520geometrically%2520non-prominent%2520areas%252C%2520such%2520as%2520segmenting%2520a%2520car%2520door%2520within%250Aa%2520car%2520mesh.%2520To%2520achieve%2520the%25203D%2520segments%252C%2520we%2520render%25202D%2520images%2520from%2520different%250Aviews%2520and%2520conduct%2520segmentation%2520for%2520both%2520textured%2520and%2520untextured%2520images.%2520Lastly%252C%250Awe%2520develop%2520a%2520multi-view%2520revoting%2520scheme%2520that%2520integrates%25202D%2520segmentation%2520results%250Aand%2520confidence%2520scores%2520from%2520various%2520views%2520onto%2520the%25203D%2520mesh%252C%2520ensuring%2520the%25203D%250Aconsistency%2520of%2520segmentation%2520results%2520and%2520eliminating%2520inaccuracies%2520from%2520specific%250Aperspectives.%2520Through%2520these%2520innovations%252C%2520MeshSegmenter%2520offers%2520stable%2520and%250Areliable%25203D%2520segmentation%2520results%2520both%2520quantitatively%2520and%2520qualitatively%252C%250Ahighlighting%2520its%2520potential%2520as%2520a%2520transformative%2520tool%2520in%2520the%2520field%2520of%25203D%250Azero-shot%2520segmentation.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/zimingzhong/MeshSegmenter%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshSegmenter%3A%20Zero-Shot%20Mesh%20Semantic%20Segmentation%20via%20Texture%0A%20%20Synthesis&entry.906535625=Ziming%20Zhong%20and%20Yanxu%20Xu%20and%20Jing%20Li%20and%20Jiale%20Xu%20and%20Zhengxin%20Li%20and%20Chaohui%20Yu%20and%20Shenghua%20Gao&entry.1292438233=%20%20We%20present%20MeshSegmenter%2C%20a%20simple%20yet%20effective%20framework%20designed%20for%0Azero-shot%203D%20semantic%20segmentation.%20This%20model%20successfully%20extends%20the%0Apowerful%20capabilities%20of%202D%20segmentation%20models%20to%203D%20meshes%2C%20delivering%0Aaccurate%203D%20segmentation%20across%20diverse%20meshes%20and%20segment%20descriptions.%0ASpecifically%2C%20our%20model%20leverages%20the%20Segment%20Anything%20Model%20%28SAM%29%20model%20to%0Asegment%20the%20target%20regions%20from%20images%20rendered%20from%20the%203D%20shape.%20In%20light%20of%0Athe%20importance%20of%20the%20texture%20for%20segmentation%2C%20we%20also%20leverage%20the%20pretrained%0Astable%20diffusion%20model%20to%20generate%20images%20with%20textures%20from%203D%20shape%2C%20and%0Aleverage%20SAM%20to%20segment%20the%20target%20regions%20from%20images%20with%20textures.%20Textures%0Asupplement%20the%20shape%20for%20segmentation%20and%20facilitate%20accurate%203D%20segmentation%0Aeven%20in%20geometrically%20non-prominent%20areas%2C%20such%20as%20segmenting%20a%20car%20door%20within%0Aa%20car%20mesh.%20To%20achieve%20the%203D%20segments%2C%20we%20render%202D%20images%20from%20different%0Aviews%20and%20conduct%20segmentation%20for%20both%20textured%20and%20untextured%20images.%20Lastly%2C%0Awe%20develop%20a%20multi-view%20revoting%20scheme%20that%20integrates%202D%20segmentation%20results%0Aand%20confidence%20scores%20from%20various%20views%20onto%20the%203D%20mesh%2C%20ensuring%20the%203D%0Aconsistency%20of%20segmentation%20results%20and%20eliminating%20inaccuracies%20from%20specific%0Aperspectives.%20Through%20these%20innovations%2C%20MeshSegmenter%20offers%20stable%20and%0Areliable%203D%20segmentation%20results%20both%20quantitatively%20and%20qualitatively%2C%0Ahighlighting%20its%20potential%20as%20a%20transformative%20tool%20in%20the%20field%20of%203D%0Azero-shot%20segmentation.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zimingzhong/MeshSegmenter%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13675v1&entry.124074799=Read"},
{"title": "MaRINeR: Enhancing Novel Views by Matching Rendered Images with Nearby\n  References", "author": "Lukas B\u00f6siger and Mihai Dusmanu and Marc Pollefeys and Zuria Bauer", "abstract": "  Rendering realistic images from 3D reconstruction is an essential task of\nmany Computer Vision and Robotics pipelines, notably for mixed-reality\napplications as well as training autonomous agents in simulated environments.\nHowever, the quality of novel views heavily depends of the source\nreconstruction which is often imperfect due to noisy or missing geometry and\nappearance. Inspired by the recent success of reference-based super-resolution\nnetworks, we propose MaRINeR, a refinement method that leverages information of\na nearby mapping image to improve the rendering of a target viewpoint. We first\nestablish matches between the raw rendered image of the scene geometry from the\ntarget viewpoint and the nearby reference based on deep features, followed by\nhierarchical detail transfer. We show improved renderings in quantitative\nmetrics and qualitative examples from both explicit and implicit scene\nrepresentations. We further employ our method on the downstream tasks of\npseudo-ground-truth validation, synthetic data enhancement and detail recovery\nfor renderings of reduced 3D reconstructions.\n", "link": "http://arxiv.org/abs/2407.13745v1", "date": "2024-07-18", "relevancy": 2.8629, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5769}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5769}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaRINeR%3A%20Enhancing%20Novel%20Views%20by%20Matching%20Rendered%20Images%20with%20Nearby%0A%20%20References&body=Title%3A%20MaRINeR%3A%20Enhancing%20Novel%20Views%20by%20Matching%20Rendered%20Images%20with%20Nearby%0A%20%20References%0AAuthor%3A%20Lukas%20B%C3%B6siger%20and%20Mihai%20Dusmanu%20and%20Marc%20Pollefeys%20and%20Zuria%20Bauer%0AAbstract%3A%20%20%20Rendering%20realistic%20images%20from%203D%20reconstruction%20is%20an%20essential%20task%20of%0Amany%20Computer%20Vision%20and%20Robotics%20pipelines%2C%20notably%20for%20mixed-reality%0Aapplications%20as%20well%20as%20training%20autonomous%20agents%20in%20simulated%20environments.%0AHowever%2C%20the%20quality%20of%20novel%20views%20heavily%20depends%20of%20the%20source%0Areconstruction%20which%20is%20often%20imperfect%20due%20to%20noisy%20or%20missing%20geometry%20and%0Aappearance.%20Inspired%20by%20the%20recent%20success%20of%20reference-based%20super-resolution%0Anetworks%2C%20we%20propose%20MaRINeR%2C%20a%20refinement%20method%20that%20leverages%20information%20of%0Aa%20nearby%20mapping%20image%20to%20improve%20the%20rendering%20of%20a%20target%20viewpoint.%20We%20first%0Aestablish%20matches%20between%20the%20raw%20rendered%20image%20of%20the%20scene%20geometry%20from%20the%0Atarget%20viewpoint%20and%20the%20nearby%20reference%20based%20on%20deep%20features%2C%20followed%20by%0Ahierarchical%20detail%20transfer.%20We%20show%20improved%20renderings%20in%20quantitative%0Ametrics%20and%20qualitative%20examples%20from%20both%20explicit%20and%20implicit%20scene%0Arepresentations.%20We%20further%20employ%20our%20method%20on%20the%20downstream%20tasks%20of%0Apseudo-ground-truth%20validation%2C%20synthetic%20data%20enhancement%20and%20detail%20recovery%0Afor%20renderings%20of%20reduced%203D%20reconstructions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaRINeR%253A%2520Enhancing%2520Novel%2520Views%2520by%2520Matching%2520Rendered%2520Images%2520with%2520Nearby%250A%2520%2520References%26entry.906535625%3DLukas%2520B%25C3%25B6siger%2520and%2520Mihai%2520Dusmanu%2520and%2520Marc%2520Pollefeys%2520and%2520Zuria%2520Bauer%26entry.1292438233%3D%2520%2520Rendering%2520realistic%2520images%2520from%25203D%2520reconstruction%2520is%2520an%2520essential%2520task%2520of%250Amany%2520Computer%2520Vision%2520and%2520Robotics%2520pipelines%252C%2520notably%2520for%2520mixed-reality%250Aapplications%2520as%2520well%2520as%2520training%2520autonomous%2520agents%2520in%2520simulated%2520environments.%250AHowever%252C%2520the%2520quality%2520of%2520novel%2520views%2520heavily%2520depends%2520of%2520the%2520source%250Areconstruction%2520which%2520is%2520often%2520imperfect%2520due%2520to%2520noisy%2520or%2520missing%2520geometry%2520and%250Aappearance.%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520reference-based%2520super-resolution%250Anetworks%252C%2520we%2520propose%2520MaRINeR%252C%2520a%2520refinement%2520method%2520that%2520leverages%2520information%2520of%250Aa%2520nearby%2520mapping%2520image%2520to%2520improve%2520the%2520rendering%2520of%2520a%2520target%2520viewpoint.%2520We%2520first%250Aestablish%2520matches%2520between%2520the%2520raw%2520rendered%2520image%2520of%2520the%2520scene%2520geometry%2520from%2520the%250Atarget%2520viewpoint%2520and%2520the%2520nearby%2520reference%2520based%2520on%2520deep%2520features%252C%2520followed%2520by%250Ahierarchical%2520detail%2520transfer.%2520We%2520show%2520improved%2520renderings%2520in%2520quantitative%250Ametrics%2520and%2520qualitative%2520examples%2520from%2520both%2520explicit%2520and%2520implicit%2520scene%250Arepresentations.%2520We%2520further%2520employ%2520our%2520method%2520on%2520the%2520downstream%2520tasks%2520of%250Apseudo-ground-truth%2520validation%252C%2520synthetic%2520data%2520enhancement%2520and%2520detail%2520recovery%250Afor%2520renderings%2520of%2520reduced%25203D%2520reconstructions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaRINeR%3A%20Enhancing%20Novel%20Views%20by%20Matching%20Rendered%20Images%20with%20Nearby%0A%20%20References&entry.906535625=Lukas%20B%C3%B6siger%20and%20Mihai%20Dusmanu%20and%20Marc%20Pollefeys%20and%20Zuria%20Bauer&entry.1292438233=%20%20Rendering%20realistic%20images%20from%203D%20reconstruction%20is%20an%20essential%20task%20of%0Amany%20Computer%20Vision%20and%20Robotics%20pipelines%2C%20notably%20for%20mixed-reality%0Aapplications%20as%20well%20as%20training%20autonomous%20agents%20in%20simulated%20environments.%0AHowever%2C%20the%20quality%20of%20novel%20views%20heavily%20depends%20of%20the%20source%0Areconstruction%20which%20is%20often%20imperfect%20due%20to%20noisy%20or%20missing%20geometry%20and%0Aappearance.%20Inspired%20by%20the%20recent%20success%20of%20reference-based%20super-resolution%0Anetworks%2C%20we%20propose%20MaRINeR%2C%20a%20refinement%20method%20that%20leverages%20information%20of%0Aa%20nearby%20mapping%20image%20to%20improve%20the%20rendering%20of%20a%20target%20viewpoint.%20We%20first%0Aestablish%20matches%20between%20the%20raw%20rendered%20image%20of%20the%20scene%20geometry%20from%20the%0Atarget%20viewpoint%20and%20the%20nearby%20reference%20based%20on%20deep%20features%2C%20followed%20by%0Ahierarchical%20detail%20transfer.%20We%20show%20improved%20renderings%20in%20quantitative%0Ametrics%20and%20qualitative%20examples%20from%20both%20explicit%20and%20implicit%20scene%0Arepresentations.%20We%20further%20employ%20our%20method%20on%20the%20downstream%20tasks%20of%0Apseudo-ground-truth%20validation%2C%20synthetic%20data%20enhancement%20and%20detail%20recovery%0Afor%20renderings%20of%20reduced%203D%20reconstructions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13745v1&entry.124074799=Read"},
{"title": "Connecting Consistency Distillation to Score Distillation for Text-to-3D\n  Generation", "author": "Zongrui Li and Minghui Hu and Qian Zheng and Xudong Jiang", "abstract": "  Although recent advancements in text-to-3D generation have significantly\nimproved generation quality, issues like limited level of detail and low\nfidelity still persist, which requires further improvement. To understand the\nessence of those issues, we thoroughly analyze current score distillation\nmethods by connecting theories of consistency distillation to score\ndistillation. Based on the insights acquired through analysis, we propose an\noptimization framework, Guided Consistency Sampling (GCS), integrated with 3D\nGaussian Splatting (3DGS) to alleviate those issues. Additionally, we have\nobserved the persistent oversaturation in the rendered views of generated 3D\nassets. From experiments, we find that it is caused by unwanted accumulated\nbrightness in 3DGS during optimization. To mitigate this issue, we introduce a\nBrightness-Equalized Generation (BEG) scheme in 3DGS rendering. Experimental\nresults demonstrate that our approach generates 3D assets with more details and\nhigher fidelity than state-of-the-art methods. The codes are released at\nhttps://github.com/LMozart/ECCV2024-GCS-BEG.\n", "link": "http://arxiv.org/abs/2407.13584v1", "date": "2024-07-18", "relevancy": 2.8374, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5689}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5674}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Connecting%20Consistency%20Distillation%20to%20Score%20Distillation%20for%20Text-to-3D%0A%20%20Generation&body=Title%3A%20Connecting%20Consistency%20Distillation%20to%20Score%20Distillation%20for%20Text-to-3D%0A%20%20Generation%0AAuthor%3A%20Zongrui%20Li%20and%20Minghui%20Hu%20and%20Qian%20Zheng%20and%20Xudong%20Jiang%0AAbstract%3A%20%20%20Although%20recent%20advancements%20in%20text-to-3D%20generation%20have%20significantly%0Aimproved%20generation%20quality%2C%20issues%20like%20limited%20level%20of%20detail%20and%20low%0Afidelity%20still%20persist%2C%20which%20requires%20further%20improvement.%20To%20understand%20the%0Aessence%20of%20those%20issues%2C%20we%20thoroughly%20analyze%20current%20score%20distillation%0Amethods%20by%20connecting%20theories%20of%20consistency%20distillation%20to%20score%0Adistillation.%20Based%20on%20the%20insights%20acquired%20through%20analysis%2C%20we%20propose%20an%0Aoptimization%20framework%2C%20Guided%20Consistency%20Sampling%20%28GCS%29%2C%20integrated%20with%203D%0AGaussian%20Splatting%20%283DGS%29%20to%20alleviate%20those%20issues.%20Additionally%2C%20we%20have%0Aobserved%20the%20persistent%20oversaturation%20in%20the%20rendered%20views%20of%20generated%203D%0Aassets.%20From%20experiments%2C%20we%20find%20that%20it%20is%20caused%20by%20unwanted%20accumulated%0Abrightness%20in%203DGS%20during%20optimization.%20To%20mitigate%20this%20issue%2C%20we%20introduce%20a%0ABrightness-Equalized%20Generation%20%28BEG%29%20scheme%20in%203DGS%20rendering.%20Experimental%0Aresults%20demonstrate%20that%20our%20approach%20generates%203D%20assets%20with%20more%20details%20and%0Ahigher%20fidelity%20than%20state-of-the-art%20methods.%20The%20codes%20are%20released%20at%0Ahttps%3A//github.com/LMozart/ECCV2024-GCS-BEG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConnecting%2520Consistency%2520Distillation%2520to%2520Score%2520Distillation%2520for%2520Text-to-3D%250A%2520%2520Generation%26entry.906535625%3DZongrui%2520Li%2520and%2520Minghui%2520Hu%2520and%2520Qian%2520Zheng%2520and%2520Xudong%2520Jiang%26entry.1292438233%3D%2520%2520Although%2520recent%2520advancements%2520in%2520text-to-3D%2520generation%2520have%2520significantly%250Aimproved%2520generation%2520quality%252C%2520issues%2520like%2520limited%2520level%2520of%2520detail%2520and%2520low%250Afidelity%2520still%2520persist%252C%2520which%2520requires%2520further%2520improvement.%2520To%2520understand%2520the%250Aessence%2520of%2520those%2520issues%252C%2520we%2520thoroughly%2520analyze%2520current%2520score%2520distillation%250Amethods%2520by%2520connecting%2520theories%2520of%2520consistency%2520distillation%2520to%2520score%250Adistillation.%2520Based%2520on%2520the%2520insights%2520acquired%2520through%2520analysis%252C%2520we%2520propose%2520an%250Aoptimization%2520framework%252C%2520Guided%2520Consistency%2520Sampling%2520%2528GCS%2529%252C%2520integrated%2520with%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520to%2520alleviate%2520those%2520issues.%2520Additionally%252C%2520we%2520have%250Aobserved%2520the%2520persistent%2520oversaturation%2520in%2520the%2520rendered%2520views%2520of%2520generated%25203D%250Aassets.%2520From%2520experiments%252C%2520we%2520find%2520that%2520it%2520is%2520caused%2520by%2520unwanted%2520accumulated%250Abrightness%2520in%25203DGS%2520during%2520optimization.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520introduce%2520a%250ABrightness-Equalized%2520Generation%2520%2528BEG%2529%2520scheme%2520in%25203DGS%2520rendering.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520approach%2520generates%25203D%2520assets%2520with%2520more%2520details%2520and%250Ahigher%2520fidelity%2520than%2520state-of-the-art%2520methods.%2520The%2520codes%2520are%2520released%2520at%250Ahttps%253A//github.com/LMozart/ECCV2024-GCS-BEG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Connecting%20Consistency%20Distillation%20to%20Score%20Distillation%20for%20Text-to-3D%0A%20%20Generation&entry.906535625=Zongrui%20Li%20and%20Minghui%20Hu%20and%20Qian%20Zheng%20and%20Xudong%20Jiang&entry.1292438233=%20%20Although%20recent%20advancements%20in%20text-to-3D%20generation%20have%20significantly%0Aimproved%20generation%20quality%2C%20issues%20like%20limited%20level%20of%20detail%20and%20low%0Afidelity%20still%20persist%2C%20which%20requires%20further%20improvement.%20To%20understand%20the%0Aessence%20of%20those%20issues%2C%20we%20thoroughly%20analyze%20current%20score%20distillation%0Amethods%20by%20connecting%20theories%20of%20consistency%20distillation%20to%20score%0Adistillation.%20Based%20on%20the%20insights%20acquired%20through%20analysis%2C%20we%20propose%20an%0Aoptimization%20framework%2C%20Guided%20Consistency%20Sampling%20%28GCS%29%2C%20integrated%20with%203D%0AGaussian%20Splatting%20%283DGS%29%20to%20alleviate%20those%20issues.%20Additionally%2C%20we%20have%0Aobserved%20the%20persistent%20oversaturation%20in%20the%20rendered%20views%20of%20generated%203D%0Aassets.%20From%20experiments%2C%20we%20find%20that%20it%20is%20caused%20by%20unwanted%20accumulated%0Abrightness%20in%203DGS%20during%20optimization.%20To%20mitigate%20this%20issue%2C%20we%20introduce%20a%0ABrightness-Equalized%20Generation%20%28BEG%29%20scheme%20in%203DGS%20rendering.%20Experimental%0Aresults%20demonstrate%20that%20our%20approach%20generates%203D%20assets%20with%20more%20details%20and%0Ahigher%20fidelity%20than%20state-of-the-art%20methods.%20The%20codes%20are%20released%20at%0Ahttps%3A//github.com/LMozart/ECCV2024-GCS-BEG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13584v1&entry.124074799=Read"},
{"title": "Mask2Map: Vectorized HD Map Construction Using Bird's Eye View\n  Segmentation Masks", "author": "Sehwan Choi and Jungho Kim and Hongjae Shin and Jun Won Choi", "abstract": "  In this paper, we introduce Mask2Map, a novel end-to-end online HD map\nconstruction method designed for autonomous driving applications. Our approach\nfocuses on predicting the class and ordered point set of map instances within a\nscene, represented in the bird's eye view (BEV). Mask2Map consists of two\nprimary components: the Instance-Level Mask Prediction Network (IMPNet) and the\nMask-Driven Map Prediction Network (MMPNet). IMPNet generates Mask-Aware\nQueries and BEV Segmentation Masks to capture comprehensive semantic\ninformation globally. Subsequently, MMPNet enhances these query features using\nlocal contextual information through two submodules: the Positional Query\nGenerator (PQG) and the Geometric Feature Extractor (GFE). PQG extracts\ninstance-level positional queries by embedding BEV positional information into\nMask-Aware Queries, while GFE utilizes BEV Segmentation Masks to generate\npoint-level geometric features. However, we observed limited performance in\nMask2Map due to inter-network inconsistency stemming from different predictions\nto Ground Truth (GT) matching between IMPNet and MMPNet. To tackle this\nchallenge, we propose the Inter-network Denoising Training method, which guides\nthe model to denoise the output affected by both noisy GT queries and perturbed\nGT Segmentation Masks. Our evaluation conducted on nuScenes and Argoverse2\nbenchmarks demonstrates that Mask2Map achieves remarkable performance\nimprovements over previous state-of-the-art methods, with gains of 10.1% mAP\nand 4.1 mAP, respectively. Our code can be found at\nhttps://github.com/SehwanChoi0307/Mask2Map.\n", "link": "http://arxiv.org/abs/2407.13517v1", "date": "2024-07-18", "relevancy": 2.8048, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6061}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5574}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mask2Map%3A%20Vectorized%20HD%20Map%20Construction%20Using%20Bird%27s%20Eye%20View%0A%20%20Segmentation%20Masks&body=Title%3A%20Mask2Map%3A%20Vectorized%20HD%20Map%20Construction%20Using%20Bird%27s%20Eye%20View%0A%20%20Segmentation%20Masks%0AAuthor%3A%20Sehwan%20Choi%20and%20Jungho%20Kim%20and%20Hongjae%20Shin%20and%20Jun%20Won%20Choi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Mask2Map%2C%20a%20novel%20end-to-end%20online%20HD%20map%0Aconstruction%20method%20designed%20for%20autonomous%20driving%20applications.%20Our%20approach%0Afocuses%20on%20predicting%20the%20class%20and%20ordered%20point%20set%20of%20map%20instances%20within%20a%0Ascene%2C%20represented%20in%20the%20bird%27s%20eye%20view%20%28BEV%29.%20Mask2Map%20consists%20of%20two%0Aprimary%20components%3A%20the%20Instance-Level%20Mask%20Prediction%20Network%20%28IMPNet%29%20and%20the%0AMask-Driven%20Map%20Prediction%20Network%20%28MMPNet%29.%20IMPNet%20generates%20Mask-Aware%0AQueries%20and%20BEV%20Segmentation%20Masks%20to%20capture%20comprehensive%20semantic%0Ainformation%20globally.%20Subsequently%2C%20MMPNet%20enhances%20these%20query%20features%20using%0Alocal%20contextual%20information%20through%20two%20submodules%3A%20the%20Positional%20Query%0AGenerator%20%28PQG%29%20and%20the%20Geometric%20Feature%20Extractor%20%28GFE%29.%20PQG%20extracts%0Ainstance-level%20positional%20queries%20by%20embedding%20BEV%20positional%20information%20into%0AMask-Aware%20Queries%2C%20while%20GFE%20utilizes%20BEV%20Segmentation%20Masks%20to%20generate%0Apoint-level%20geometric%20features.%20However%2C%20we%20observed%20limited%20performance%20in%0AMask2Map%20due%20to%20inter-network%20inconsistency%20stemming%20from%20different%20predictions%0Ato%20Ground%20Truth%20%28GT%29%20matching%20between%20IMPNet%20and%20MMPNet.%20To%20tackle%20this%0Achallenge%2C%20we%20propose%20the%20Inter-network%20Denoising%20Training%20method%2C%20which%20guides%0Athe%20model%20to%20denoise%20the%20output%20affected%20by%20both%20noisy%20GT%20queries%20and%20perturbed%0AGT%20Segmentation%20Masks.%20Our%20evaluation%20conducted%20on%20nuScenes%20and%20Argoverse2%0Abenchmarks%20demonstrates%20that%20Mask2Map%20achieves%20remarkable%20performance%0Aimprovements%20over%20previous%20state-of-the-art%20methods%2C%20with%20gains%20of%2010.1%25%20mAP%0Aand%204.1%20mAP%2C%20respectively.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/SehwanChoi0307/Mask2Map.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMask2Map%253A%2520Vectorized%2520HD%2520Map%2520Construction%2520Using%2520Bird%2527s%2520Eye%2520View%250A%2520%2520Segmentation%2520Masks%26entry.906535625%3DSehwan%2520Choi%2520and%2520Jungho%2520Kim%2520and%2520Hongjae%2520Shin%2520and%2520Jun%2520Won%2520Choi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Mask2Map%252C%2520a%2520novel%2520end-to-end%2520online%2520HD%2520map%250Aconstruction%2520method%2520designed%2520for%2520autonomous%2520driving%2520applications.%2520Our%2520approach%250Afocuses%2520on%2520predicting%2520the%2520class%2520and%2520ordered%2520point%2520set%2520of%2520map%2520instances%2520within%2520a%250Ascene%252C%2520represented%2520in%2520the%2520bird%2527s%2520eye%2520view%2520%2528BEV%2529.%2520Mask2Map%2520consists%2520of%2520two%250Aprimary%2520components%253A%2520the%2520Instance-Level%2520Mask%2520Prediction%2520Network%2520%2528IMPNet%2529%2520and%2520the%250AMask-Driven%2520Map%2520Prediction%2520Network%2520%2528MMPNet%2529.%2520IMPNet%2520generates%2520Mask-Aware%250AQueries%2520and%2520BEV%2520Segmentation%2520Masks%2520to%2520capture%2520comprehensive%2520semantic%250Ainformation%2520globally.%2520Subsequently%252C%2520MMPNet%2520enhances%2520these%2520query%2520features%2520using%250Alocal%2520contextual%2520information%2520through%2520two%2520submodules%253A%2520the%2520Positional%2520Query%250AGenerator%2520%2528PQG%2529%2520and%2520the%2520Geometric%2520Feature%2520Extractor%2520%2528GFE%2529.%2520PQG%2520extracts%250Ainstance-level%2520positional%2520queries%2520by%2520embedding%2520BEV%2520positional%2520information%2520into%250AMask-Aware%2520Queries%252C%2520while%2520GFE%2520utilizes%2520BEV%2520Segmentation%2520Masks%2520to%2520generate%250Apoint-level%2520geometric%2520features.%2520However%252C%2520we%2520observed%2520limited%2520performance%2520in%250AMask2Map%2520due%2520to%2520inter-network%2520inconsistency%2520stemming%2520from%2520different%2520predictions%250Ato%2520Ground%2520Truth%2520%2528GT%2529%2520matching%2520between%2520IMPNet%2520and%2520MMPNet.%2520To%2520tackle%2520this%250Achallenge%252C%2520we%2520propose%2520the%2520Inter-network%2520Denoising%2520Training%2520method%252C%2520which%2520guides%250Athe%2520model%2520to%2520denoise%2520the%2520output%2520affected%2520by%2520both%2520noisy%2520GT%2520queries%2520and%2520perturbed%250AGT%2520Segmentation%2520Masks.%2520Our%2520evaluation%2520conducted%2520on%2520nuScenes%2520and%2520Argoverse2%250Abenchmarks%2520demonstrates%2520that%2520Mask2Map%2520achieves%2520remarkable%2520performance%250Aimprovements%2520over%2520previous%2520state-of-the-art%2520methods%252C%2520with%2520gains%2520of%252010.1%2525%2520mAP%250Aand%25204.1%2520mAP%252C%2520respectively.%2520Our%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/SehwanChoi0307/Mask2Map.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask2Map%3A%20Vectorized%20HD%20Map%20Construction%20Using%20Bird%27s%20Eye%20View%0A%20%20Segmentation%20Masks&entry.906535625=Sehwan%20Choi%20and%20Jungho%20Kim%20and%20Hongjae%20Shin%20and%20Jun%20Won%20Choi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Mask2Map%2C%20a%20novel%20end-to-end%20online%20HD%20map%0Aconstruction%20method%20designed%20for%20autonomous%20driving%20applications.%20Our%20approach%0Afocuses%20on%20predicting%20the%20class%20and%20ordered%20point%20set%20of%20map%20instances%20within%20a%0Ascene%2C%20represented%20in%20the%20bird%27s%20eye%20view%20%28BEV%29.%20Mask2Map%20consists%20of%20two%0Aprimary%20components%3A%20the%20Instance-Level%20Mask%20Prediction%20Network%20%28IMPNet%29%20and%20the%0AMask-Driven%20Map%20Prediction%20Network%20%28MMPNet%29.%20IMPNet%20generates%20Mask-Aware%0AQueries%20and%20BEV%20Segmentation%20Masks%20to%20capture%20comprehensive%20semantic%0Ainformation%20globally.%20Subsequently%2C%20MMPNet%20enhances%20these%20query%20features%20using%0Alocal%20contextual%20information%20through%20two%20submodules%3A%20the%20Positional%20Query%0AGenerator%20%28PQG%29%20and%20the%20Geometric%20Feature%20Extractor%20%28GFE%29.%20PQG%20extracts%0Ainstance-level%20positional%20queries%20by%20embedding%20BEV%20positional%20information%20into%0AMask-Aware%20Queries%2C%20while%20GFE%20utilizes%20BEV%20Segmentation%20Masks%20to%20generate%0Apoint-level%20geometric%20features.%20However%2C%20we%20observed%20limited%20performance%20in%0AMask2Map%20due%20to%20inter-network%20inconsistency%20stemming%20from%20different%20predictions%0Ato%20Ground%20Truth%20%28GT%29%20matching%20between%20IMPNet%20and%20MMPNet.%20To%20tackle%20this%0Achallenge%2C%20we%20propose%20the%20Inter-network%20Denoising%20Training%20method%2C%20which%20guides%0Athe%20model%20to%20denoise%20the%20output%20affected%20by%20both%20noisy%20GT%20queries%20and%20perturbed%0AGT%20Segmentation%20Masks.%20Our%20evaluation%20conducted%20on%20nuScenes%20and%20Argoverse2%0Abenchmarks%20demonstrates%20that%20Mask2Map%20achieves%20remarkable%20performance%0Aimprovements%20over%20previous%20state-of-the-art%20methods%2C%20with%20gains%20of%2010.1%25%20mAP%0Aand%204.1%20mAP%2C%20respectively.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/SehwanChoi0307/Mask2Map.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13517v1&entry.124074799=Read"},
{"title": "Sparse Beats Dense: Rethinking Supervision in Radar-Camera Depth\n  Completion", "author": "Huadong Li and Minhao Jing and Jiajun Liang and Haoqiang Fan and Renhe Ji", "abstract": "  It is widely believed that sparse supervision is worse than dense supervision\nin the field of depth completion, but the underlying reasons for this are\nrarely discussed. To this end, we revisit the task of radar-camera depth\ncompletion and present a new method with sparse LiDAR supervision to outperform\nprevious dense LiDAR supervision methods in both accuracy and speed.\nSpecifically, when trained by sparse LiDAR supervision, depth completion models\nusually output depth maps containing significant stripe-like artifacts. We find\nthat such a phenomenon is caused by the implicitly learned positional\ndistribution pattern from sparse LiDAR supervision, termed as LiDAR\nDistribution Leakage (LDL) in this paper. Based on such understanding, we\npresent a novel Disruption-Compensation radar-camera depth completion framework\nto address this issue. The Disruption part aims to deliberately disrupt the\nlearning of LiDAR distribution from sparse supervision, while the Compensation\npart aims to leverage 3D spatial and 2D semantic information to compensate for\nthe information loss of previous disruptions. Extensive experimental results\ndemonstrate that by reducing the impact of LDL, our framework with sparse\nsupervision outperforms the state-of-the-art dense supervision methods with\n11.6% improvement in Mean Absolute Error (MAE)} and 1.6x speedup in Frame Per\nSecond (FPS)}. The code is available at\nhttps://github.com/megvii-research/Sparse-Beats-Dense.\n", "link": "http://arxiv.org/abs/2312.00844v3", "date": "2024-07-18", "relevancy": 2.7478, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5601}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5459}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Beats%20Dense%3A%20Rethinking%20Supervision%20in%20Radar-Camera%20Depth%0A%20%20Completion&body=Title%3A%20Sparse%20Beats%20Dense%3A%20Rethinking%20Supervision%20in%20Radar-Camera%20Depth%0A%20%20Completion%0AAuthor%3A%20Huadong%20Li%20and%20Minhao%20Jing%20and%20Jiajun%20Liang%20and%20Haoqiang%20Fan%20and%20Renhe%20Ji%0AAbstract%3A%20%20%20It%20is%20widely%20believed%20that%20sparse%20supervision%20is%20worse%20than%20dense%20supervision%0Ain%20the%20field%20of%20depth%20completion%2C%20but%20the%20underlying%20reasons%20for%20this%20are%0Ararely%20discussed.%20To%20this%20end%2C%20we%20revisit%20the%20task%20of%20radar-camera%20depth%0Acompletion%20and%20present%20a%20new%20method%20with%20sparse%20LiDAR%20supervision%20to%20outperform%0Aprevious%20dense%20LiDAR%20supervision%20methods%20in%20both%20accuracy%20and%20speed.%0ASpecifically%2C%20when%20trained%20by%20sparse%20LiDAR%20supervision%2C%20depth%20completion%20models%0Ausually%20output%20depth%20maps%20containing%20significant%20stripe-like%20artifacts.%20We%20find%0Athat%20such%20a%20phenomenon%20is%20caused%20by%20the%20implicitly%20learned%20positional%0Adistribution%20pattern%20from%20sparse%20LiDAR%20supervision%2C%20termed%20as%20LiDAR%0ADistribution%20Leakage%20%28LDL%29%20in%20this%20paper.%20Based%20on%20such%20understanding%2C%20we%0Apresent%20a%20novel%20Disruption-Compensation%20radar-camera%20depth%20completion%20framework%0Ato%20address%20this%20issue.%20The%20Disruption%20part%20aims%20to%20deliberately%20disrupt%20the%0Alearning%20of%20LiDAR%20distribution%20from%20sparse%20supervision%2C%20while%20the%20Compensation%0Apart%20aims%20to%20leverage%203D%20spatial%20and%202D%20semantic%20information%20to%20compensate%20for%0Athe%20information%20loss%20of%20previous%20disruptions.%20Extensive%20experimental%20results%0Ademonstrate%20that%20by%20reducing%20the%20impact%20of%20LDL%2C%20our%20framework%20with%20sparse%0Asupervision%20outperforms%20the%20state-of-the-art%20dense%20supervision%20methods%20with%0A11.6%25%20improvement%20in%20Mean%20Absolute%20Error%20%28MAE%29%7D%20and%201.6x%20speedup%20in%20Frame%20Per%0ASecond%20%28FPS%29%7D.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/megvii-research/Sparse-Beats-Dense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00844v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Beats%2520Dense%253A%2520Rethinking%2520Supervision%2520in%2520Radar-Camera%2520Depth%250A%2520%2520Completion%26entry.906535625%3DHuadong%2520Li%2520and%2520Minhao%2520Jing%2520and%2520Jiajun%2520Liang%2520and%2520Haoqiang%2520Fan%2520and%2520Renhe%2520Ji%26entry.1292438233%3D%2520%2520It%2520is%2520widely%2520believed%2520that%2520sparse%2520supervision%2520is%2520worse%2520than%2520dense%2520supervision%250Ain%2520the%2520field%2520of%2520depth%2520completion%252C%2520but%2520the%2520underlying%2520reasons%2520for%2520this%2520are%250Ararely%2520discussed.%2520To%2520this%2520end%252C%2520we%2520revisit%2520the%2520task%2520of%2520radar-camera%2520depth%250Acompletion%2520and%2520present%2520a%2520new%2520method%2520with%2520sparse%2520LiDAR%2520supervision%2520to%2520outperform%250Aprevious%2520dense%2520LiDAR%2520supervision%2520methods%2520in%2520both%2520accuracy%2520and%2520speed.%250ASpecifically%252C%2520when%2520trained%2520by%2520sparse%2520LiDAR%2520supervision%252C%2520depth%2520completion%2520models%250Ausually%2520output%2520depth%2520maps%2520containing%2520significant%2520stripe-like%2520artifacts.%2520We%2520find%250Athat%2520such%2520a%2520phenomenon%2520is%2520caused%2520by%2520the%2520implicitly%2520learned%2520positional%250Adistribution%2520pattern%2520from%2520sparse%2520LiDAR%2520supervision%252C%2520termed%2520as%2520LiDAR%250ADistribution%2520Leakage%2520%2528LDL%2529%2520in%2520this%2520paper.%2520Based%2520on%2520such%2520understanding%252C%2520we%250Apresent%2520a%2520novel%2520Disruption-Compensation%2520radar-camera%2520depth%2520completion%2520framework%250Ato%2520address%2520this%2520issue.%2520The%2520Disruption%2520part%2520aims%2520to%2520deliberately%2520disrupt%2520the%250Alearning%2520of%2520LiDAR%2520distribution%2520from%2520sparse%2520supervision%252C%2520while%2520the%2520Compensation%250Apart%2520aims%2520to%2520leverage%25203D%2520spatial%2520and%25202D%2520semantic%2520information%2520to%2520compensate%2520for%250Athe%2520information%2520loss%2520of%2520previous%2520disruptions.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520by%2520reducing%2520the%2520impact%2520of%2520LDL%252C%2520our%2520framework%2520with%2520sparse%250Asupervision%2520outperforms%2520the%2520state-of-the-art%2520dense%2520supervision%2520methods%2520with%250A11.6%2525%2520improvement%2520in%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%257D%2520and%25201.6x%2520speedup%2520in%2520Frame%2520Per%250ASecond%2520%2528FPS%2529%257D.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/megvii-research/Sparse-Beats-Dense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00844v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Beats%20Dense%3A%20Rethinking%20Supervision%20in%20Radar-Camera%20Depth%0A%20%20Completion&entry.906535625=Huadong%20Li%20and%20Minhao%20Jing%20and%20Jiajun%20Liang%20and%20Haoqiang%20Fan%20and%20Renhe%20Ji&entry.1292438233=%20%20It%20is%20widely%20believed%20that%20sparse%20supervision%20is%20worse%20than%20dense%20supervision%0Ain%20the%20field%20of%20depth%20completion%2C%20but%20the%20underlying%20reasons%20for%20this%20are%0Ararely%20discussed.%20To%20this%20end%2C%20we%20revisit%20the%20task%20of%20radar-camera%20depth%0Acompletion%20and%20present%20a%20new%20method%20with%20sparse%20LiDAR%20supervision%20to%20outperform%0Aprevious%20dense%20LiDAR%20supervision%20methods%20in%20both%20accuracy%20and%20speed.%0ASpecifically%2C%20when%20trained%20by%20sparse%20LiDAR%20supervision%2C%20depth%20completion%20models%0Ausually%20output%20depth%20maps%20containing%20significant%20stripe-like%20artifacts.%20We%20find%0Athat%20such%20a%20phenomenon%20is%20caused%20by%20the%20implicitly%20learned%20positional%0Adistribution%20pattern%20from%20sparse%20LiDAR%20supervision%2C%20termed%20as%20LiDAR%0ADistribution%20Leakage%20%28LDL%29%20in%20this%20paper.%20Based%20on%20such%20understanding%2C%20we%0Apresent%20a%20novel%20Disruption-Compensation%20radar-camera%20depth%20completion%20framework%0Ato%20address%20this%20issue.%20The%20Disruption%20part%20aims%20to%20deliberately%20disrupt%20the%0Alearning%20of%20LiDAR%20distribution%20from%20sparse%20supervision%2C%20while%20the%20Compensation%0Apart%20aims%20to%20leverage%203D%20spatial%20and%202D%20semantic%20information%20to%20compensate%20for%0Athe%20information%20loss%20of%20previous%20disruptions.%20Extensive%20experimental%20results%0Ademonstrate%20that%20by%20reducing%20the%20impact%20of%20LDL%2C%20our%20framework%20with%20sparse%0Asupervision%20outperforms%20the%20state-of-the-art%20dense%20supervision%20methods%20with%0A11.6%25%20improvement%20in%20Mean%20Absolute%20Error%20%28MAE%29%7D%20and%201.6x%20speedup%20in%20Frame%20Per%0ASecond%20%28FPS%29%7D.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/megvii-research/Sparse-Beats-Dense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00844v3&entry.124074799=Read"},
{"title": "Which Model Generated This Image? A Model-Agnostic Approach for Origin\n  Attribution", "author": "Fengyuan Liu and Haochen Luo and Yiming Li and Philip Torr and Jindong Gu", "abstract": "  Recent progress in visual generative models enables the generation of\nhigh-quality images. To prevent the misuse of generated images, it is important\nto identify the origin model that generates them. In this work, we study the\norigin attribution of generated images in a practical setting where only a few\nimages generated by a source model are available and the source model cannot be\naccessed. The goal is to check if a given image is generated by the source\nmodel. We first formulate this problem as a few-shot one-class classification\ntask. To solve the task, we propose OCC-CLIP, a CLIP-based framework for\nfew-shot one-class classification, enabling the identification of an image's\nsource model, even among multiple candidates. Extensive experiments\ncorresponding to various generative models verify the effectiveness of our\nOCC-CLIP framework. Furthermore, an experiment based on the recently released\nDALL-E 3 API verifies the real-world applicability of our solution.\n", "link": "http://arxiv.org/abs/2404.02697v2", "date": "2024-07-18", "relevancy": 2.7437, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.577}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5416}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Which%20Model%20Generated%20This%20Image%3F%20A%20Model-Agnostic%20Approach%20for%20Origin%0A%20%20Attribution&body=Title%3A%20Which%20Model%20Generated%20This%20Image%3F%20A%20Model-Agnostic%20Approach%20for%20Origin%0A%20%20Attribution%0AAuthor%3A%20Fengyuan%20Liu%20and%20Haochen%20Luo%20and%20Yiming%20Li%20and%20Philip%20Torr%20and%20Jindong%20Gu%0AAbstract%3A%20%20%20Recent%20progress%20in%20visual%20generative%20models%20enables%20the%20generation%20of%0Ahigh-quality%20images.%20To%20prevent%20the%20misuse%20of%20generated%20images%2C%20it%20is%20important%0Ato%20identify%20the%20origin%20model%20that%20generates%20them.%20In%20this%20work%2C%20we%20study%20the%0Aorigin%20attribution%20of%20generated%20images%20in%20a%20practical%20setting%20where%20only%20a%20few%0Aimages%20generated%20by%20a%20source%20model%20are%20available%20and%20the%20source%20model%20cannot%20be%0Aaccessed.%20The%20goal%20is%20to%20check%20if%20a%20given%20image%20is%20generated%20by%20the%20source%0Amodel.%20We%20first%20formulate%20this%20problem%20as%20a%20few-shot%20one-class%20classification%0Atask.%20To%20solve%20the%20task%2C%20we%20propose%20OCC-CLIP%2C%20a%20CLIP-based%20framework%20for%0Afew-shot%20one-class%20classification%2C%20enabling%20the%20identification%20of%20an%20image%27s%0Asource%20model%2C%20even%20among%20multiple%20candidates.%20Extensive%20experiments%0Acorresponding%20to%20various%20generative%20models%20verify%20the%20effectiveness%20of%20our%0AOCC-CLIP%20framework.%20Furthermore%2C%20an%20experiment%20based%20on%20the%20recently%20released%0ADALL-E%203%20API%20verifies%20the%20real-world%20applicability%20of%20our%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhich%2520Model%2520Generated%2520This%2520Image%253F%2520A%2520Model-Agnostic%2520Approach%2520for%2520Origin%250A%2520%2520Attribution%26entry.906535625%3DFengyuan%2520Liu%2520and%2520Haochen%2520Luo%2520and%2520Yiming%2520Li%2520and%2520Philip%2520Torr%2520and%2520Jindong%2520Gu%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520visual%2520generative%2520models%2520enables%2520the%2520generation%2520of%250Ahigh-quality%2520images.%2520To%2520prevent%2520the%2520misuse%2520of%2520generated%2520images%252C%2520it%2520is%2520important%250Ato%2520identify%2520the%2520origin%2520model%2520that%2520generates%2520them.%2520In%2520this%2520work%252C%2520we%2520study%2520the%250Aorigin%2520attribution%2520of%2520generated%2520images%2520in%2520a%2520practical%2520setting%2520where%2520only%2520a%2520few%250Aimages%2520generated%2520by%2520a%2520source%2520model%2520are%2520available%2520and%2520the%2520source%2520model%2520cannot%2520be%250Aaccessed.%2520The%2520goal%2520is%2520to%2520check%2520if%2520a%2520given%2520image%2520is%2520generated%2520by%2520the%2520source%250Amodel.%2520We%2520first%2520formulate%2520this%2520problem%2520as%2520a%2520few-shot%2520one-class%2520classification%250Atask.%2520To%2520solve%2520the%2520task%252C%2520we%2520propose%2520OCC-CLIP%252C%2520a%2520CLIP-based%2520framework%2520for%250Afew-shot%2520one-class%2520classification%252C%2520enabling%2520the%2520identification%2520of%2520an%2520image%2527s%250Asource%2520model%252C%2520even%2520among%2520multiple%2520candidates.%2520Extensive%2520experiments%250Acorresponding%2520to%2520various%2520generative%2520models%2520verify%2520the%2520effectiveness%2520of%2520our%250AOCC-CLIP%2520framework.%2520Furthermore%252C%2520an%2520experiment%2520based%2520on%2520the%2520recently%2520released%250ADALL-E%25203%2520API%2520verifies%2520the%2520real-world%2520applicability%2520of%2520our%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Which%20Model%20Generated%20This%20Image%3F%20A%20Model-Agnostic%20Approach%20for%20Origin%0A%20%20Attribution&entry.906535625=Fengyuan%20Liu%20and%20Haochen%20Luo%20and%20Yiming%20Li%20and%20Philip%20Torr%20and%20Jindong%20Gu&entry.1292438233=%20%20Recent%20progress%20in%20visual%20generative%20models%20enables%20the%20generation%20of%0Ahigh-quality%20images.%20To%20prevent%20the%20misuse%20of%20generated%20images%2C%20it%20is%20important%0Ato%20identify%20the%20origin%20model%20that%20generates%20them.%20In%20this%20work%2C%20we%20study%20the%0Aorigin%20attribution%20of%20generated%20images%20in%20a%20practical%20setting%20where%20only%20a%20few%0Aimages%20generated%20by%20a%20source%20model%20are%20available%20and%20the%20source%20model%20cannot%20be%0Aaccessed.%20The%20goal%20is%20to%20check%20if%20a%20given%20image%20is%20generated%20by%20the%20source%0Amodel.%20We%20first%20formulate%20this%20problem%20as%20a%20few-shot%20one-class%20classification%0Atask.%20To%20solve%20the%20task%2C%20we%20propose%20OCC-CLIP%2C%20a%20CLIP-based%20framework%20for%0Afew-shot%20one-class%20classification%2C%20enabling%20the%20identification%20of%20an%20image%27s%0Asource%20model%2C%20even%20among%20multiple%20candidates.%20Extensive%20experiments%0Acorresponding%20to%20various%20generative%20models%20verify%20the%20effectiveness%20of%20our%0AOCC-CLIP%20framework.%20Furthermore%2C%20an%20experiment%20based%20on%20the%20recently%20released%0ADALL-E%203%20API%20verifies%20the%20real-world%20applicability%20of%20our%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02697v2&entry.124074799=Read"},
{"title": "SA-DVAE: Improving Zero-Shot Skeleton-Based Action Recognition by\n  Disentangled Variational Autoencoders", "author": "Sheng-Wei Li and Zi-Xiang Wei and Wei-Jie Chen and Yi-Hsin Yu and Chih-Yuan Yang and Jane Yung-jen Hsu", "abstract": "  Existing zero-shot skeleton-based action recognition methods utilize\nprojection networks to learn a shared latent space of skeleton features and\nsemantic embeddings. The inherent imbalance in action recognition datasets,\ncharacterized by variable skeleton sequences yet constant class labels,\npresents significant challenges for alignment. To address the imbalance, we\npropose SA-DVAE -- Semantic Alignment via Disentangled Variational\nAutoencoders, a method that first adopts feature disentanglement to separate\nskeleton features into two independent parts -- one is semantic-related and\nanother is irrelevant -- to better align skeleton and semantic features. We\nimplement this idea via a pair of modality-specific variational autoencoders\ncoupled with a total correction penalty. We conduct experiments on three\nbenchmark datasets: NTU RGB+D, NTU RGB+D 120 and PKU-MMD, and our experimental\nresults show that SA-DAVE produces improved performance over existing methods.\nThe code is available at https://github.com/pha123661/SA-DVAE.\n", "link": "http://arxiv.org/abs/2407.13460v1", "date": "2024-07-18", "relevancy": 2.7421, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5529}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5472}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SA-DVAE%3A%20Improving%20Zero-Shot%20Skeleton-Based%20Action%20Recognition%20by%0A%20%20Disentangled%20Variational%20Autoencoders&body=Title%3A%20SA-DVAE%3A%20Improving%20Zero-Shot%20Skeleton-Based%20Action%20Recognition%20by%0A%20%20Disentangled%20Variational%20Autoencoders%0AAuthor%3A%20Sheng-Wei%20Li%20and%20Zi-Xiang%20Wei%20and%20Wei-Jie%20Chen%20and%20Yi-Hsin%20Yu%20and%20Chih-Yuan%20Yang%20and%20Jane%20Yung-jen%20Hsu%0AAbstract%3A%20%20%20Existing%20zero-shot%20skeleton-based%20action%20recognition%20methods%20utilize%0Aprojection%20networks%20to%20learn%20a%20shared%20latent%20space%20of%20skeleton%20features%20and%0Asemantic%20embeddings.%20The%20inherent%20imbalance%20in%20action%20recognition%20datasets%2C%0Acharacterized%20by%20variable%20skeleton%20sequences%20yet%20constant%20class%20labels%2C%0Apresents%20significant%20challenges%20for%20alignment.%20To%20address%20the%20imbalance%2C%20we%0Apropose%20SA-DVAE%20--%20Semantic%20Alignment%20via%20Disentangled%20Variational%0AAutoencoders%2C%20a%20method%20that%20first%20adopts%20feature%20disentanglement%20to%20separate%0Askeleton%20features%20into%20two%20independent%20parts%20--%20one%20is%20semantic-related%20and%0Aanother%20is%20irrelevant%20--%20to%20better%20align%20skeleton%20and%20semantic%20features.%20We%0Aimplement%20this%20idea%20via%20a%20pair%20of%20modality-specific%20variational%20autoencoders%0Acoupled%20with%20a%20total%20correction%20penalty.%20We%20conduct%20experiments%20on%20three%0Abenchmark%20datasets%3A%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%20and%20PKU-MMD%2C%20and%20our%20experimental%0Aresults%20show%20that%20SA-DAVE%20produces%20improved%20performance%20over%20existing%20methods.%0AThe%20code%20is%20available%20at%20https%3A//github.com/pha123661/SA-DVAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSA-DVAE%253A%2520Improving%2520Zero-Shot%2520Skeleton-Based%2520Action%2520Recognition%2520by%250A%2520%2520Disentangled%2520Variational%2520Autoencoders%26entry.906535625%3DSheng-Wei%2520Li%2520and%2520Zi-Xiang%2520Wei%2520and%2520Wei-Jie%2520Chen%2520and%2520Yi-Hsin%2520Yu%2520and%2520Chih-Yuan%2520Yang%2520and%2520Jane%2520Yung-jen%2520Hsu%26entry.1292438233%3D%2520%2520Existing%2520zero-shot%2520skeleton-based%2520action%2520recognition%2520methods%2520utilize%250Aprojection%2520networks%2520to%2520learn%2520a%2520shared%2520latent%2520space%2520of%2520skeleton%2520features%2520and%250Asemantic%2520embeddings.%2520The%2520inherent%2520imbalance%2520in%2520action%2520recognition%2520datasets%252C%250Acharacterized%2520by%2520variable%2520skeleton%2520sequences%2520yet%2520constant%2520class%2520labels%252C%250Apresents%2520significant%2520challenges%2520for%2520alignment.%2520To%2520address%2520the%2520imbalance%252C%2520we%250Apropose%2520SA-DVAE%2520--%2520Semantic%2520Alignment%2520via%2520Disentangled%2520Variational%250AAutoencoders%252C%2520a%2520method%2520that%2520first%2520adopts%2520feature%2520disentanglement%2520to%2520separate%250Askeleton%2520features%2520into%2520two%2520independent%2520parts%2520--%2520one%2520is%2520semantic-related%2520and%250Aanother%2520is%2520irrelevant%2520--%2520to%2520better%2520align%2520skeleton%2520and%2520semantic%2520features.%2520We%250Aimplement%2520this%2520idea%2520via%2520a%2520pair%2520of%2520modality-specific%2520variational%2520autoencoders%250Acoupled%2520with%2520a%2520total%2520correction%2520penalty.%2520We%2520conduct%2520experiments%2520on%2520three%250Abenchmark%2520datasets%253A%2520NTU%2520RGB%252BD%252C%2520NTU%2520RGB%252BD%2520120%2520and%2520PKU-MMD%252C%2520and%2520our%2520experimental%250Aresults%2520show%2520that%2520SA-DAVE%2520produces%2520improved%2520performance%2520over%2520existing%2520methods.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/pha123661/SA-DVAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SA-DVAE%3A%20Improving%20Zero-Shot%20Skeleton-Based%20Action%20Recognition%20by%0A%20%20Disentangled%20Variational%20Autoencoders&entry.906535625=Sheng-Wei%20Li%20and%20Zi-Xiang%20Wei%20and%20Wei-Jie%20Chen%20and%20Yi-Hsin%20Yu%20and%20Chih-Yuan%20Yang%20and%20Jane%20Yung-jen%20Hsu&entry.1292438233=%20%20Existing%20zero-shot%20skeleton-based%20action%20recognition%20methods%20utilize%0Aprojection%20networks%20to%20learn%20a%20shared%20latent%20space%20of%20skeleton%20features%20and%0Asemantic%20embeddings.%20The%20inherent%20imbalance%20in%20action%20recognition%20datasets%2C%0Acharacterized%20by%20variable%20skeleton%20sequences%20yet%20constant%20class%20labels%2C%0Apresents%20significant%20challenges%20for%20alignment.%20To%20address%20the%20imbalance%2C%20we%0Apropose%20SA-DVAE%20--%20Semantic%20Alignment%20via%20Disentangled%20Variational%0AAutoencoders%2C%20a%20method%20that%20first%20adopts%20feature%20disentanglement%20to%20separate%0Askeleton%20features%20into%20two%20independent%20parts%20--%20one%20is%20semantic-related%20and%0Aanother%20is%20irrelevant%20--%20to%20better%20align%20skeleton%20and%20semantic%20features.%20We%0Aimplement%20this%20idea%20via%20a%20pair%20of%20modality-specific%20variational%20autoencoders%0Acoupled%20with%20a%20total%20correction%20penalty.%20We%20conduct%20experiments%20on%20three%0Abenchmark%20datasets%3A%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%20and%20PKU-MMD%2C%20and%20our%20experimental%0Aresults%20show%20that%20SA-DAVE%20produces%20improved%20performance%20over%20existing%20methods.%0AThe%20code%20is%20available%20at%20https%3A//github.com/pha123661/SA-DVAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13460v1&entry.124074799=Read"},
{"title": "Self-Supervised Learning with Generative Adversarial Networks for\n  Electron Microscopy", "author": "Bashir Kazimi and Karina Ruzaeva and Stefan Sandfeld", "abstract": "  In this work, we explore the potential of self-supervised learning with\nGenerative Adversarial Networks (GANs) for electron microscopy datasets. We\nshow how self-supervised pretraining facilitates efficient fine-tuning for a\nspectrum of downstream tasks, including semantic segmentation, denoising, noise\n\\& background removal, and super-resolution. Experimentation with varying model\ncomplexities and receptive field sizes reveals the remarkable phenomenon that\nfine-tuned models of lower complexity consistently outperform more complex\nmodels with random weight initialization. We demonstrate the versatility of\nself-supervised pretraining across various downstream tasks in the context of\nelectron microscopy, allowing faster convergence and better performance. We\nconclude that self-supervised pretraining serves as a powerful catalyst, being\nespecially advantageous when limited annotated data are available and efficient\nscaling of computational cost is important.\n", "link": "http://arxiv.org/abs/2402.18286v2", "date": "2024-07-18", "relevancy": 2.7193, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5575}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5438}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20with%20Generative%20Adversarial%20Networks%20for%0A%20%20Electron%20Microscopy&body=Title%3A%20Self-Supervised%20Learning%20with%20Generative%20Adversarial%20Networks%20for%0A%20%20Electron%20Microscopy%0AAuthor%3A%20Bashir%20Kazimi%20and%20Karina%20Ruzaeva%20and%20Stefan%20Sandfeld%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%20self-supervised%20learning%20with%0AGenerative%20Adversarial%20Networks%20%28GANs%29%20for%20electron%20microscopy%20datasets.%20We%0Ashow%20how%20self-supervised%20pretraining%20facilitates%20efficient%20fine-tuning%20for%20a%0Aspectrum%20of%20downstream%20tasks%2C%20including%20semantic%20segmentation%2C%20denoising%2C%20noise%0A%5C%26%20background%20removal%2C%20and%20super-resolution.%20Experimentation%20with%20varying%20model%0Acomplexities%20and%20receptive%20field%20sizes%20reveals%20the%20remarkable%20phenomenon%20that%0Afine-tuned%20models%20of%20lower%20complexity%20consistently%20outperform%20more%20complex%0Amodels%20with%20random%20weight%20initialization.%20We%20demonstrate%20the%20versatility%20of%0Aself-supervised%20pretraining%20across%20various%20downstream%20tasks%20in%20the%20context%20of%0Aelectron%20microscopy%2C%20allowing%20faster%20convergence%20and%20better%20performance.%20We%0Aconclude%20that%20self-supervised%20pretraining%20serves%20as%20a%20powerful%20catalyst%2C%20being%0Aespecially%20advantageous%20when%20limited%20annotated%20data%20are%20available%20and%20efficient%0Ascaling%20of%20computational%20cost%20is%20important.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18286v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520with%2520Generative%2520Adversarial%2520Networks%2520for%250A%2520%2520Electron%2520Microscopy%26entry.906535625%3DBashir%2520Kazimi%2520and%2520Karina%2520Ruzaeva%2520and%2520Stefan%2520Sandfeld%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520potential%2520of%2520self-supervised%2520learning%2520with%250AGenerative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520for%2520electron%2520microscopy%2520datasets.%2520We%250Ashow%2520how%2520self-supervised%2520pretraining%2520facilitates%2520efficient%2520fine-tuning%2520for%2520a%250Aspectrum%2520of%2520downstream%2520tasks%252C%2520including%2520semantic%2520segmentation%252C%2520denoising%252C%2520noise%250A%255C%2526%2520background%2520removal%252C%2520and%2520super-resolution.%2520Experimentation%2520with%2520varying%2520model%250Acomplexities%2520and%2520receptive%2520field%2520sizes%2520reveals%2520the%2520remarkable%2520phenomenon%2520that%250Afine-tuned%2520models%2520of%2520lower%2520complexity%2520consistently%2520outperform%2520more%2520complex%250Amodels%2520with%2520random%2520weight%2520initialization.%2520We%2520demonstrate%2520the%2520versatility%2520of%250Aself-supervised%2520pretraining%2520across%2520various%2520downstream%2520tasks%2520in%2520the%2520context%2520of%250Aelectron%2520microscopy%252C%2520allowing%2520faster%2520convergence%2520and%2520better%2520performance.%2520We%250Aconclude%2520that%2520self-supervised%2520pretraining%2520serves%2520as%2520a%2520powerful%2520catalyst%252C%2520being%250Aespecially%2520advantageous%2520when%2520limited%2520annotated%2520data%2520are%2520available%2520and%2520efficient%250Ascaling%2520of%2520computational%2520cost%2520is%2520important.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18286v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20with%20Generative%20Adversarial%20Networks%20for%0A%20%20Electron%20Microscopy&entry.906535625=Bashir%20Kazimi%20and%20Karina%20Ruzaeva%20and%20Stefan%20Sandfeld&entry.1292438233=%20%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%20self-supervised%20learning%20with%0AGenerative%20Adversarial%20Networks%20%28GANs%29%20for%20electron%20microscopy%20datasets.%20We%0Ashow%20how%20self-supervised%20pretraining%20facilitates%20efficient%20fine-tuning%20for%20a%0Aspectrum%20of%20downstream%20tasks%2C%20including%20semantic%20segmentation%2C%20denoising%2C%20noise%0A%5C%26%20background%20removal%2C%20and%20super-resolution.%20Experimentation%20with%20varying%20model%0Acomplexities%20and%20receptive%20field%20sizes%20reveals%20the%20remarkable%20phenomenon%20that%0Afine-tuned%20models%20of%20lower%20complexity%20consistently%20outperform%20more%20complex%0Amodels%20with%20random%20weight%20initialization.%20We%20demonstrate%20the%20versatility%20of%0Aself-supervised%20pretraining%20across%20various%20downstream%20tasks%20in%20the%20context%20of%0Aelectron%20microscopy%2C%20allowing%20faster%20convergence%20and%20better%20performance.%20We%0Aconclude%20that%20self-supervised%20pretraining%20serves%20as%20a%20powerful%20catalyst%2C%20being%0Aespecially%20advantageous%20when%20limited%20annotated%20data%20are%20available%20and%20efficient%0Ascaling%20of%20computational%20cost%20is%20important.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18286v2&entry.124074799=Read"},
{"title": "BAM-DETR: Boundary-Aligned Moment Detection Transformer for Temporal\n  Sentence Grounding in Videos", "author": "Pilhyeon Lee and Hyeran Byun", "abstract": "  Temporal sentence grounding aims to localize moments relevant to a language\ndescription. Recently, DETR-like approaches achieved notable progress by\npredicting the center and length of a target moment. However, they suffer from\nthe issue of center misalignment raised by the inherent ambiguity of moment\ncenters, leading to inaccurate predictions. To remedy this problem, we propose\na novel boundary-oriented moment formulation. In our paradigm, the model no\nlonger needs to find the precise center but instead suffices to predict any\nanchor point within the interval, from which the boundaries are directly\nestimated. Based on this idea, we design a boundary-aligned moment detection\ntransformer, equipped with a dual-pathway decoding process. Specifically, it\nrefines the anchor and boundaries within parallel pathways using global and\nboundary-focused attention, respectively. This separate design allows the model\nto focus on desirable regions, enabling precise refinement of moment\npredictions. Further, we propose a quality-based ranking method, ensuring that\nproposals with high localization qualities are prioritized over incomplete\nones. Experiments on three benchmarks validate the effectiveness of the\nproposed methods. The code is available at\nhttps://github.com/Pilhyeon/BAM-DETR.\n", "link": "http://arxiv.org/abs/2312.00083v2", "date": "2024-07-18", "relevancy": 2.7192, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5817}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5288}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BAM-DETR%3A%20Boundary-Aligned%20Moment%20Detection%20Transformer%20for%20Temporal%0A%20%20Sentence%20Grounding%20in%20Videos&body=Title%3A%20BAM-DETR%3A%20Boundary-Aligned%20Moment%20Detection%20Transformer%20for%20Temporal%0A%20%20Sentence%20Grounding%20in%20Videos%0AAuthor%3A%20Pilhyeon%20Lee%20and%20Hyeran%20Byun%0AAbstract%3A%20%20%20Temporal%20sentence%20grounding%20aims%20to%20localize%20moments%20relevant%20to%20a%20language%0Adescription.%20Recently%2C%20DETR-like%20approaches%20achieved%20notable%20progress%20by%0Apredicting%20the%20center%20and%20length%20of%20a%20target%20moment.%20However%2C%20they%20suffer%20from%0Athe%20issue%20of%20center%20misalignment%20raised%20by%20the%20inherent%20ambiguity%20of%20moment%0Acenters%2C%20leading%20to%20inaccurate%20predictions.%20To%20remedy%20this%20problem%2C%20we%20propose%0Aa%20novel%20boundary-oriented%20moment%20formulation.%20In%20our%20paradigm%2C%20the%20model%20no%0Alonger%20needs%20to%20find%20the%20precise%20center%20but%20instead%20suffices%20to%20predict%20any%0Aanchor%20point%20within%20the%20interval%2C%20from%20which%20the%20boundaries%20are%20directly%0Aestimated.%20Based%20on%20this%20idea%2C%20we%20design%20a%20boundary-aligned%20moment%20detection%0Atransformer%2C%20equipped%20with%20a%20dual-pathway%20decoding%20process.%20Specifically%2C%20it%0Arefines%20the%20anchor%20and%20boundaries%20within%20parallel%20pathways%20using%20global%20and%0Aboundary-focused%20attention%2C%20respectively.%20This%20separate%20design%20allows%20the%20model%0Ato%20focus%20on%20desirable%20regions%2C%20enabling%20precise%20refinement%20of%20moment%0Apredictions.%20Further%2C%20we%20propose%20a%20quality-based%20ranking%20method%2C%20ensuring%20that%0Aproposals%20with%20high%20localization%20qualities%20are%20prioritized%20over%20incomplete%0Aones.%20Experiments%20on%20three%20benchmarks%20validate%20the%20effectiveness%20of%20the%0Aproposed%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Pilhyeon/BAM-DETR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBAM-DETR%253A%2520Boundary-Aligned%2520Moment%2520Detection%2520Transformer%2520for%2520Temporal%250A%2520%2520Sentence%2520Grounding%2520in%2520Videos%26entry.906535625%3DPilhyeon%2520Lee%2520and%2520Hyeran%2520Byun%26entry.1292438233%3D%2520%2520Temporal%2520sentence%2520grounding%2520aims%2520to%2520localize%2520moments%2520relevant%2520to%2520a%2520language%250Adescription.%2520Recently%252C%2520DETR-like%2520approaches%2520achieved%2520notable%2520progress%2520by%250Apredicting%2520the%2520center%2520and%2520length%2520of%2520a%2520target%2520moment.%2520However%252C%2520they%2520suffer%2520from%250Athe%2520issue%2520of%2520center%2520misalignment%2520raised%2520by%2520the%2520inherent%2520ambiguity%2520of%2520moment%250Acenters%252C%2520leading%2520to%2520inaccurate%2520predictions.%2520To%2520remedy%2520this%2520problem%252C%2520we%2520propose%250Aa%2520novel%2520boundary-oriented%2520moment%2520formulation.%2520In%2520our%2520paradigm%252C%2520the%2520model%2520no%250Alonger%2520needs%2520to%2520find%2520the%2520precise%2520center%2520but%2520instead%2520suffices%2520to%2520predict%2520any%250Aanchor%2520point%2520within%2520the%2520interval%252C%2520from%2520which%2520the%2520boundaries%2520are%2520directly%250Aestimated.%2520Based%2520on%2520this%2520idea%252C%2520we%2520design%2520a%2520boundary-aligned%2520moment%2520detection%250Atransformer%252C%2520equipped%2520with%2520a%2520dual-pathway%2520decoding%2520process.%2520Specifically%252C%2520it%250Arefines%2520the%2520anchor%2520and%2520boundaries%2520within%2520parallel%2520pathways%2520using%2520global%2520and%250Aboundary-focused%2520attention%252C%2520respectively.%2520This%2520separate%2520design%2520allows%2520the%2520model%250Ato%2520focus%2520on%2520desirable%2520regions%252C%2520enabling%2520precise%2520refinement%2520of%2520moment%250Apredictions.%2520Further%252C%2520we%2520propose%2520a%2520quality-based%2520ranking%2520method%252C%2520ensuring%2520that%250Aproposals%2520with%2520high%2520localization%2520qualities%2520are%2520prioritized%2520over%2520incomplete%250Aones.%2520Experiments%2520on%2520three%2520benchmarks%2520validate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520methods.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Pilhyeon/BAM-DETR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAM-DETR%3A%20Boundary-Aligned%20Moment%20Detection%20Transformer%20for%20Temporal%0A%20%20Sentence%20Grounding%20in%20Videos&entry.906535625=Pilhyeon%20Lee%20and%20Hyeran%20Byun&entry.1292438233=%20%20Temporal%20sentence%20grounding%20aims%20to%20localize%20moments%20relevant%20to%20a%20language%0Adescription.%20Recently%2C%20DETR-like%20approaches%20achieved%20notable%20progress%20by%0Apredicting%20the%20center%20and%20length%20of%20a%20target%20moment.%20However%2C%20they%20suffer%20from%0Athe%20issue%20of%20center%20misalignment%20raised%20by%20the%20inherent%20ambiguity%20of%20moment%0Acenters%2C%20leading%20to%20inaccurate%20predictions.%20To%20remedy%20this%20problem%2C%20we%20propose%0Aa%20novel%20boundary-oriented%20moment%20formulation.%20In%20our%20paradigm%2C%20the%20model%20no%0Alonger%20needs%20to%20find%20the%20precise%20center%20but%20instead%20suffices%20to%20predict%20any%0Aanchor%20point%20within%20the%20interval%2C%20from%20which%20the%20boundaries%20are%20directly%0Aestimated.%20Based%20on%20this%20idea%2C%20we%20design%20a%20boundary-aligned%20moment%20detection%0Atransformer%2C%20equipped%20with%20a%20dual-pathway%20decoding%20process.%20Specifically%2C%20it%0Arefines%20the%20anchor%20and%20boundaries%20within%20parallel%20pathways%20using%20global%20and%0Aboundary-focused%20attention%2C%20respectively.%20This%20separate%20design%20allows%20the%20model%0Ato%20focus%20on%20desirable%20regions%2C%20enabling%20precise%20refinement%20of%20moment%0Apredictions.%20Further%2C%20we%20propose%20a%20quality-based%20ranking%20method%2C%20ensuring%20that%0Aproposals%20with%20high%20localization%20qualities%20are%20prioritized%20over%20incomplete%0Aones.%20Experiments%20on%20three%20benchmarks%20validate%20the%20effectiveness%20of%20the%0Aproposed%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Pilhyeon/BAM-DETR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00083v2&entry.124074799=Read"},
{"title": "GPSFormer: A Global Perception and Local Structure Fitting-based\n  Transformer for Point Cloud Understanding", "author": "Changshuo Wang and Meiqing Wu and Siew-Kei Lam and Xin Ning and Shangshu Yu and Ruiping Wang and Weijun Li and Thambipillai Srikanthan", "abstract": "  Despite the significant advancements in pre-training methods for point cloud\nunderstanding, directly capturing intricate shape information from irregular\npoint clouds without reliance on external data remains a formidable challenge.\nTo address this problem, we propose GPSFormer, an innovative Global Perception\nand Local Structure Fitting-based Transformer, which learns detailed shape\ninformation from point clouds with remarkable precision. The core of GPSFormer\nis the Global Perception Module (GPM) and the Local Structure Fitting\nConvolution (LSFConv). Specifically, GPM utilizes Adaptive Deformable Graph\nConvolution (ADGConv) to identify short-range dependencies among similar\nfeatures in the feature space and employs Multi-Head Attention (MHA) to learn\nlong-range dependencies across all positions within the feature space,\nultimately enabling flexible learning of contextual representations. Inspired\nby Taylor series, we design LSFConv, which learns both low-order fundamental\nand high-order refinement information from explicitly encoded local geometric\nstructures. Integrating the GPM and LSFConv as fundamental components, we\nconstruct GPSFormer, a cutting-edge Transformer that effectively captures\nglobal and local structures of point clouds. Extensive experiments validate\nGPSFormer's effectiveness in three point cloud tasks: shape classification,\npart segmentation, and few-shot learning. The code of GPSFormer is available at\n\\url{https://github.com/changshuowang/GPSFormer}.\n", "link": "http://arxiv.org/abs/2407.13519v1", "date": "2024-07-18", "relevancy": 2.7155, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5503}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5443}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPSFormer%3A%20A%20Global%20Perception%20and%20Local%20Structure%20Fitting-based%0A%20%20Transformer%20for%20Point%20Cloud%20Understanding&body=Title%3A%20GPSFormer%3A%20A%20Global%20Perception%20and%20Local%20Structure%20Fitting-based%0A%20%20Transformer%20for%20Point%20Cloud%20Understanding%0AAuthor%3A%20Changshuo%20Wang%20and%20Meiqing%20Wu%20and%20Siew-Kei%20Lam%20and%20Xin%20Ning%20and%20Shangshu%20Yu%20and%20Ruiping%20Wang%20and%20Weijun%20Li%20and%20Thambipillai%20Srikanthan%0AAbstract%3A%20%20%20Despite%20the%20significant%20advancements%20in%20pre-training%20methods%20for%20point%20cloud%0Aunderstanding%2C%20directly%20capturing%20intricate%20shape%20information%20from%20irregular%0Apoint%20clouds%20without%20reliance%20on%20external%20data%20remains%20a%20formidable%20challenge.%0ATo%20address%20this%20problem%2C%20we%20propose%20GPSFormer%2C%20an%20innovative%20Global%20Perception%0Aand%20Local%20Structure%20Fitting-based%20Transformer%2C%20which%20learns%20detailed%20shape%0Ainformation%20from%20point%20clouds%20with%20remarkable%20precision.%20The%20core%20of%20GPSFormer%0Ais%20the%20Global%20Perception%20Module%20%28GPM%29%20and%20the%20Local%20Structure%20Fitting%0AConvolution%20%28LSFConv%29.%20Specifically%2C%20GPM%20utilizes%20Adaptive%20Deformable%20Graph%0AConvolution%20%28ADGConv%29%20to%20identify%20short-range%20dependencies%20among%20similar%0Afeatures%20in%20the%20feature%20space%20and%20employs%20Multi-Head%20Attention%20%28MHA%29%20to%20learn%0Along-range%20dependencies%20across%20all%20positions%20within%20the%20feature%20space%2C%0Aultimately%20enabling%20flexible%20learning%20of%20contextual%20representations.%20Inspired%0Aby%20Taylor%20series%2C%20we%20design%20LSFConv%2C%20which%20learns%20both%20low-order%20fundamental%0Aand%20high-order%20refinement%20information%20from%20explicitly%20encoded%20local%20geometric%0Astructures.%20Integrating%20the%20GPM%20and%20LSFConv%20as%20fundamental%20components%2C%20we%0Aconstruct%20GPSFormer%2C%20a%20cutting-edge%20Transformer%20that%20effectively%20captures%0Aglobal%20and%20local%20structures%20of%20point%20clouds.%20Extensive%20experiments%20validate%0AGPSFormer%27s%20effectiveness%20in%20three%20point%20cloud%20tasks%3A%20shape%20classification%2C%0Apart%20segmentation%2C%20and%20few-shot%20learning.%20The%20code%20of%20GPSFormer%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/changshuowang/GPSFormer%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPSFormer%253A%2520A%2520Global%2520Perception%2520and%2520Local%2520Structure%2520Fitting-based%250A%2520%2520Transformer%2520for%2520Point%2520Cloud%2520Understanding%26entry.906535625%3DChangshuo%2520Wang%2520and%2520Meiqing%2520Wu%2520and%2520Siew-Kei%2520Lam%2520and%2520Xin%2520Ning%2520and%2520Shangshu%2520Yu%2520and%2520Ruiping%2520Wang%2520and%2520Weijun%2520Li%2520and%2520Thambipillai%2520Srikanthan%26entry.1292438233%3D%2520%2520Despite%2520the%2520significant%2520advancements%2520in%2520pre-training%2520methods%2520for%2520point%2520cloud%250Aunderstanding%252C%2520directly%2520capturing%2520intricate%2520shape%2520information%2520from%2520irregular%250Apoint%2520clouds%2520without%2520reliance%2520on%2520external%2520data%2520remains%2520a%2520formidable%2520challenge.%250ATo%2520address%2520this%2520problem%252C%2520we%2520propose%2520GPSFormer%252C%2520an%2520innovative%2520Global%2520Perception%250Aand%2520Local%2520Structure%2520Fitting-based%2520Transformer%252C%2520which%2520learns%2520detailed%2520shape%250Ainformation%2520from%2520point%2520clouds%2520with%2520remarkable%2520precision.%2520The%2520core%2520of%2520GPSFormer%250Ais%2520the%2520Global%2520Perception%2520Module%2520%2528GPM%2529%2520and%2520the%2520Local%2520Structure%2520Fitting%250AConvolution%2520%2528LSFConv%2529.%2520Specifically%252C%2520GPM%2520utilizes%2520Adaptive%2520Deformable%2520Graph%250AConvolution%2520%2528ADGConv%2529%2520to%2520identify%2520short-range%2520dependencies%2520among%2520similar%250Afeatures%2520in%2520the%2520feature%2520space%2520and%2520employs%2520Multi-Head%2520Attention%2520%2528MHA%2529%2520to%2520learn%250Along-range%2520dependencies%2520across%2520all%2520positions%2520within%2520the%2520feature%2520space%252C%250Aultimately%2520enabling%2520flexible%2520learning%2520of%2520contextual%2520representations.%2520Inspired%250Aby%2520Taylor%2520series%252C%2520we%2520design%2520LSFConv%252C%2520which%2520learns%2520both%2520low-order%2520fundamental%250Aand%2520high-order%2520refinement%2520information%2520from%2520explicitly%2520encoded%2520local%2520geometric%250Astructures.%2520Integrating%2520the%2520GPM%2520and%2520LSFConv%2520as%2520fundamental%2520components%252C%2520we%250Aconstruct%2520GPSFormer%252C%2520a%2520cutting-edge%2520Transformer%2520that%2520effectively%2520captures%250Aglobal%2520and%2520local%2520structures%2520of%2520point%2520clouds.%2520Extensive%2520experiments%2520validate%250AGPSFormer%2527s%2520effectiveness%2520in%2520three%2520point%2520cloud%2520tasks%253A%2520shape%2520classification%252C%250Apart%2520segmentation%252C%2520and%2520few-shot%2520learning.%2520The%2520code%2520of%2520GPSFormer%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/changshuowang/GPSFormer%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPSFormer%3A%20A%20Global%20Perception%20and%20Local%20Structure%20Fitting-based%0A%20%20Transformer%20for%20Point%20Cloud%20Understanding&entry.906535625=Changshuo%20Wang%20and%20Meiqing%20Wu%20and%20Siew-Kei%20Lam%20and%20Xin%20Ning%20and%20Shangshu%20Yu%20and%20Ruiping%20Wang%20and%20Weijun%20Li%20and%20Thambipillai%20Srikanthan&entry.1292438233=%20%20Despite%20the%20significant%20advancements%20in%20pre-training%20methods%20for%20point%20cloud%0Aunderstanding%2C%20directly%20capturing%20intricate%20shape%20information%20from%20irregular%0Apoint%20clouds%20without%20reliance%20on%20external%20data%20remains%20a%20formidable%20challenge.%0ATo%20address%20this%20problem%2C%20we%20propose%20GPSFormer%2C%20an%20innovative%20Global%20Perception%0Aand%20Local%20Structure%20Fitting-based%20Transformer%2C%20which%20learns%20detailed%20shape%0Ainformation%20from%20point%20clouds%20with%20remarkable%20precision.%20The%20core%20of%20GPSFormer%0Ais%20the%20Global%20Perception%20Module%20%28GPM%29%20and%20the%20Local%20Structure%20Fitting%0AConvolution%20%28LSFConv%29.%20Specifically%2C%20GPM%20utilizes%20Adaptive%20Deformable%20Graph%0AConvolution%20%28ADGConv%29%20to%20identify%20short-range%20dependencies%20among%20similar%0Afeatures%20in%20the%20feature%20space%20and%20employs%20Multi-Head%20Attention%20%28MHA%29%20to%20learn%0Along-range%20dependencies%20across%20all%20positions%20within%20the%20feature%20space%2C%0Aultimately%20enabling%20flexible%20learning%20of%20contextual%20representations.%20Inspired%0Aby%20Taylor%20series%2C%20we%20design%20LSFConv%2C%20which%20learns%20both%20low-order%20fundamental%0Aand%20high-order%20refinement%20information%20from%20explicitly%20encoded%20local%20geometric%0Astructures.%20Integrating%20the%20GPM%20and%20LSFConv%20as%20fundamental%20components%2C%20we%0Aconstruct%20GPSFormer%2C%20a%20cutting-edge%20Transformer%20that%20effectively%20captures%0Aglobal%20and%20local%20structures%20of%20point%20clouds.%20Extensive%20experiments%20validate%0AGPSFormer%27s%20effectiveness%20in%20three%20point%20cloud%20tasks%3A%20shape%20classification%2C%0Apart%20segmentation%2C%20and%20few-shot%20learning.%20The%20code%20of%20GPSFormer%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/changshuowang/GPSFormer%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13519v1&entry.124074799=Read"},
{"title": "Beyond Dropout: Robust Convolutional Neural Networks Based on Local\n  Feature Masking", "author": "Yunpeng Gong and Chuangliang Zhang and Yongjie Hou and Lifei Chen and Min Jiang", "abstract": "  In the contemporary of deep learning, where models often grapple with the\nchallenge of simultaneously achieving robustness against adversarial attacks\nand strong generalization capabilities, this study introduces an innovative\nLocal Feature Masking (LFM) strategy aimed at fortifying the performance of\nConvolutional Neural Networks (CNNs) on both fronts. During the training phase,\nwe strategically incorporate random feature masking in the shallow layers of\nCNNs, effectively alleviating overfitting issues, thereby enhancing the model's\ngeneralization ability and bolstering its resilience to adversarial attacks.\nLFM compels the network to adapt by leveraging remaining features to compensate\nfor the absence of certain semantic features, nurturing a more elastic feature\nlearning mechanism. The efficacy of LFM is substantiated through a series of\nquantitative and qualitative assessments, collectively showcasing a consistent\nand significant improvement in CNN's generalization ability and resistance\nagainst adversarial attacks--a phenomenon not observed in current and prior\nmethodologies. The seamless integration of LFM into established CNN frameworks\nunderscores its potential to advance both generalization and adversarial\nrobustness within the deep learning paradigm. Through comprehensive\nexperiments, including robust person re-identification baseline generalization\nexperiments and adversarial attack experiments, we demonstrate the substantial\nenhancements offered by LFM in addressing the aforementioned challenges. This\ncontribution represents a noteworthy stride in advancing robust neural network\narchitectures.\n", "link": "http://arxiv.org/abs/2407.13646v1", "date": "2024-07-18", "relevancy": 2.6884, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5597}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5383}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Dropout%3A%20Robust%20Convolutional%20Neural%20Networks%20Based%20on%20Local%0A%20%20Feature%20Masking&body=Title%3A%20Beyond%20Dropout%3A%20Robust%20Convolutional%20Neural%20Networks%20Based%20on%20Local%0A%20%20Feature%20Masking%0AAuthor%3A%20Yunpeng%20Gong%20and%20Chuangliang%20Zhang%20and%20Yongjie%20Hou%20and%20Lifei%20Chen%20and%20Min%20Jiang%0AAbstract%3A%20%20%20In%20the%20contemporary%20of%20deep%20learning%2C%20where%20models%20often%20grapple%20with%20the%0Achallenge%20of%20simultaneously%20achieving%20robustness%20against%20adversarial%20attacks%0Aand%20strong%20generalization%20capabilities%2C%20this%20study%20introduces%20an%20innovative%0ALocal%20Feature%20Masking%20%28LFM%29%20strategy%20aimed%20at%20fortifying%20the%20performance%20of%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20on%20both%20fronts.%20During%20the%20training%20phase%2C%0Awe%20strategically%20incorporate%20random%20feature%20masking%20in%20the%20shallow%20layers%20of%0ACNNs%2C%20effectively%20alleviating%20overfitting%20issues%2C%20thereby%20enhancing%20the%20model%27s%0Ageneralization%20ability%20and%20bolstering%20its%20resilience%20to%20adversarial%20attacks.%0ALFM%20compels%20the%20network%20to%20adapt%20by%20leveraging%20remaining%20features%20to%20compensate%0Afor%20the%20absence%20of%20certain%20semantic%20features%2C%20nurturing%20a%20more%20elastic%20feature%0Alearning%20mechanism.%20The%20efficacy%20of%20LFM%20is%20substantiated%20through%20a%20series%20of%0Aquantitative%20and%20qualitative%20assessments%2C%20collectively%20showcasing%20a%20consistent%0Aand%20significant%20improvement%20in%20CNN%27s%20generalization%20ability%20and%20resistance%0Aagainst%20adversarial%20attacks--a%20phenomenon%20not%20observed%20in%20current%20and%20prior%0Amethodologies.%20The%20seamless%20integration%20of%20LFM%20into%20established%20CNN%20frameworks%0Aunderscores%20its%20potential%20to%20advance%20both%20generalization%20and%20adversarial%0Arobustness%20within%20the%20deep%20learning%20paradigm.%20Through%20comprehensive%0Aexperiments%2C%20including%20robust%20person%20re-identification%20baseline%20generalization%0Aexperiments%20and%20adversarial%20attack%20experiments%2C%20we%20demonstrate%20the%20substantial%0Aenhancements%20offered%20by%20LFM%20in%20addressing%20the%20aforementioned%20challenges.%20This%0Acontribution%20represents%20a%20noteworthy%20stride%20in%20advancing%20robust%20neural%20network%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Dropout%253A%2520Robust%2520Convolutional%2520Neural%2520Networks%2520Based%2520on%2520Local%250A%2520%2520Feature%2520Masking%26entry.906535625%3DYunpeng%2520Gong%2520and%2520Chuangliang%2520Zhang%2520and%2520Yongjie%2520Hou%2520and%2520Lifei%2520Chen%2520and%2520Min%2520Jiang%26entry.1292438233%3D%2520%2520In%2520the%2520contemporary%2520of%2520deep%2520learning%252C%2520where%2520models%2520often%2520grapple%2520with%2520the%250Achallenge%2520of%2520simultaneously%2520achieving%2520robustness%2520against%2520adversarial%2520attacks%250Aand%2520strong%2520generalization%2520capabilities%252C%2520this%2520study%2520introduces%2520an%2520innovative%250ALocal%2520Feature%2520Masking%2520%2528LFM%2529%2520strategy%2520aimed%2520at%2520fortifying%2520the%2520performance%2520of%250AConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520on%2520both%2520fronts.%2520During%2520the%2520training%2520phase%252C%250Awe%2520strategically%2520incorporate%2520random%2520feature%2520masking%2520in%2520the%2520shallow%2520layers%2520of%250ACNNs%252C%2520effectively%2520alleviating%2520overfitting%2520issues%252C%2520thereby%2520enhancing%2520the%2520model%2527s%250Ageneralization%2520ability%2520and%2520bolstering%2520its%2520resilience%2520to%2520adversarial%2520attacks.%250ALFM%2520compels%2520the%2520network%2520to%2520adapt%2520by%2520leveraging%2520remaining%2520features%2520to%2520compensate%250Afor%2520the%2520absence%2520of%2520certain%2520semantic%2520features%252C%2520nurturing%2520a%2520more%2520elastic%2520feature%250Alearning%2520mechanism.%2520The%2520efficacy%2520of%2520LFM%2520is%2520substantiated%2520through%2520a%2520series%2520of%250Aquantitative%2520and%2520qualitative%2520assessments%252C%2520collectively%2520showcasing%2520a%2520consistent%250Aand%2520significant%2520improvement%2520in%2520CNN%2527s%2520generalization%2520ability%2520and%2520resistance%250Aagainst%2520adversarial%2520attacks--a%2520phenomenon%2520not%2520observed%2520in%2520current%2520and%2520prior%250Amethodologies.%2520The%2520seamless%2520integration%2520of%2520LFM%2520into%2520established%2520CNN%2520frameworks%250Aunderscores%2520its%2520potential%2520to%2520advance%2520both%2520generalization%2520and%2520adversarial%250Arobustness%2520within%2520the%2520deep%2520learning%2520paradigm.%2520Through%2520comprehensive%250Aexperiments%252C%2520including%2520robust%2520person%2520re-identification%2520baseline%2520generalization%250Aexperiments%2520and%2520adversarial%2520attack%2520experiments%252C%2520we%2520demonstrate%2520the%2520substantial%250Aenhancements%2520offered%2520by%2520LFM%2520in%2520addressing%2520the%2520aforementioned%2520challenges.%2520This%250Acontribution%2520represents%2520a%2520noteworthy%2520stride%2520in%2520advancing%2520robust%2520neural%2520network%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Dropout%3A%20Robust%20Convolutional%20Neural%20Networks%20Based%20on%20Local%0A%20%20Feature%20Masking&entry.906535625=Yunpeng%20Gong%20and%20Chuangliang%20Zhang%20and%20Yongjie%20Hou%20and%20Lifei%20Chen%20and%20Min%20Jiang&entry.1292438233=%20%20In%20the%20contemporary%20of%20deep%20learning%2C%20where%20models%20often%20grapple%20with%20the%0Achallenge%20of%20simultaneously%20achieving%20robustness%20against%20adversarial%20attacks%0Aand%20strong%20generalization%20capabilities%2C%20this%20study%20introduces%20an%20innovative%0ALocal%20Feature%20Masking%20%28LFM%29%20strategy%20aimed%20at%20fortifying%20the%20performance%20of%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20on%20both%20fronts.%20During%20the%20training%20phase%2C%0Awe%20strategically%20incorporate%20random%20feature%20masking%20in%20the%20shallow%20layers%20of%0ACNNs%2C%20effectively%20alleviating%20overfitting%20issues%2C%20thereby%20enhancing%20the%20model%27s%0Ageneralization%20ability%20and%20bolstering%20its%20resilience%20to%20adversarial%20attacks.%0ALFM%20compels%20the%20network%20to%20adapt%20by%20leveraging%20remaining%20features%20to%20compensate%0Afor%20the%20absence%20of%20certain%20semantic%20features%2C%20nurturing%20a%20more%20elastic%20feature%0Alearning%20mechanism.%20The%20efficacy%20of%20LFM%20is%20substantiated%20through%20a%20series%20of%0Aquantitative%20and%20qualitative%20assessments%2C%20collectively%20showcasing%20a%20consistent%0Aand%20significant%20improvement%20in%20CNN%27s%20generalization%20ability%20and%20resistance%0Aagainst%20adversarial%20attacks--a%20phenomenon%20not%20observed%20in%20current%20and%20prior%0Amethodologies.%20The%20seamless%20integration%20of%20LFM%20into%20established%20CNN%20frameworks%0Aunderscores%20its%20potential%20to%20advance%20both%20generalization%20and%20adversarial%0Arobustness%20within%20the%20deep%20learning%20paradigm.%20Through%20comprehensive%0Aexperiments%2C%20including%20robust%20person%20re-identification%20baseline%20generalization%0Aexperiments%20and%20adversarial%20attack%20experiments%2C%20we%20demonstrate%20the%20substantial%0Aenhancements%20offered%20by%20LFM%20in%20addressing%20the%20aforementioned%20challenges.%20This%0Acontribution%20represents%20a%20noteworthy%20stride%20in%20advancing%20robust%20neural%20network%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13646v1&entry.124074799=Read"},
{"title": "On the Discriminability of Self-Supervised Representation Learning", "author": "Zeen Song and Wenwen Qiang and Changwen Zheng and Fuchun Sun and Hui Xiong", "abstract": "  Self-supervised learning (SSL) has recently achieved significant success in\ndownstream visual tasks. However, a notable gap still exists between SSL and\nsupervised learning (SL), especially in complex downstream tasks. In this\npaper, we show that the features learned by SSL methods suffer from the\ncrowding problem, where features of different classes are not distinctly\nseparated, and features within the same class exhibit large intra-class\nvariance. In contrast, SL ensures a clear separation between classes. We\nanalyze this phenomenon and conclude that SSL objectives do not constrain the\nrelationships between different samples and their augmentations. Our\ntheoretical analysis delves into how SSL objectives fail to enforce the\nnecessary constraints between samples and their augmentations, leading to poor\nperformance in complex tasks. We provide a theoretical framework showing that\nthe performance gap between SSL and SL mainly stems from the inability of SSL\nmethods to capture the aggregation of similar augmentations and the separation\nof dissimilar augmentations. To address this issue, we propose a learnable\nregulator called Dynamic Semantic Adjuster (DSA). DSA aggregates and separates\nsamples in the feature space while being robust to outliers. Through extensive\nempirical evaluations on multiple benchmark datasets, we demonstrate the\nsuperiority of DSA in enhancing feature aggregation and separation, ultimately\nclosing the performance gap between SSL and SL.\n", "link": "http://arxiv.org/abs/2407.13541v1", "date": "2024-07-18", "relevancy": 2.6816, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5859}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5175}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Discriminability%20of%20Self-Supervised%20Representation%20Learning&body=Title%3A%20On%20the%20Discriminability%20of%20Self-Supervised%20Representation%20Learning%0AAuthor%3A%20Zeen%20Song%20and%20Wenwen%20Qiang%20and%20Changwen%20Zheng%20and%20Fuchun%20Sun%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20recently%20achieved%20significant%20success%20in%0Adownstream%20visual%20tasks.%20However%2C%20a%20notable%20gap%20still%20exists%20between%20SSL%20and%0Asupervised%20learning%20%28SL%29%2C%20especially%20in%20complex%20downstream%20tasks.%20In%20this%0Apaper%2C%20we%20show%20that%20the%20features%20learned%20by%20SSL%20methods%20suffer%20from%20the%0Acrowding%20problem%2C%20where%20features%20of%20different%20classes%20are%20not%20distinctly%0Aseparated%2C%20and%20features%20within%20the%20same%20class%20exhibit%20large%20intra-class%0Avariance.%20In%20contrast%2C%20SL%20ensures%20a%20clear%20separation%20between%20classes.%20We%0Aanalyze%20this%20phenomenon%20and%20conclude%20that%20SSL%20objectives%20do%20not%20constrain%20the%0Arelationships%20between%20different%20samples%20and%20their%20augmentations.%20Our%0Atheoretical%20analysis%20delves%20into%20how%20SSL%20objectives%20fail%20to%20enforce%20the%0Anecessary%20constraints%20between%20samples%20and%20their%20augmentations%2C%20leading%20to%20poor%0Aperformance%20in%20complex%20tasks.%20We%20provide%20a%20theoretical%20framework%20showing%20that%0Athe%20performance%20gap%20between%20SSL%20and%20SL%20mainly%20stems%20from%20the%20inability%20of%20SSL%0Amethods%20to%20capture%20the%20aggregation%20of%20similar%20augmentations%20and%20the%20separation%0Aof%20dissimilar%20augmentations.%20To%20address%20this%20issue%2C%20we%20propose%20a%20learnable%0Aregulator%20called%20Dynamic%20Semantic%20Adjuster%20%28DSA%29.%20DSA%20aggregates%20and%20separates%0Asamples%20in%20the%20feature%20space%20while%20being%20robust%20to%20outliers.%20Through%20extensive%0Aempirical%20evaluations%20on%20multiple%20benchmark%20datasets%2C%20we%20demonstrate%20the%0Asuperiority%20of%20DSA%20in%20enhancing%20feature%20aggregation%20and%20separation%2C%20ultimately%0Aclosing%20the%20performance%20gap%20between%20SSL%20and%20SL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Discriminability%2520of%2520Self-Supervised%2520Representation%2520Learning%26entry.906535625%3DZeen%2520Song%2520and%2520Wenwen%2520Qiang%2520and%2520Changwen%2520Zheng%2520and%2520Fuchun%2520Sun%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520recently%2520achieved%2520significant%2520success%2520in%250Adownstream%2520visual%2520tasks.%2520However%252C%2520a%2520notable%2520gap%2520still%2520exists%2520between%2520SSL%2520and%250Asupervised%2520learning%2520%2528SL%2529%252C%2520especially%2520in%2520complex%2520downstream%2520tasks.%2520In%2520this%250Apaper%252C%2520we%2520show%2520that%2520the%2520features%2520learned%2520by%2520SSL%2520methods%2520suffer%2520from%2520the%250Acrowding%2520problem%252C%2520where%2520features%2520of%2520different%2520classes%2520are%2520not%2520distinctly%250Aseparated%252C%2520and%2520features%2520within%2520the%2520same%2520class%2520exhibit%2520large%2520intra-class%250Avariance.%2520In%2520contrast%252C%2520SL%2520ensures%2520a%2520clear%2520separation%2520between%2520classes.%2520We%250Aanalyze%2520this%2520phenomenon%2520and%2520conclude%2520that%2520SSL%2520objectives%2520do%2520not%2520constrain%2520the%250Arelationships%2520between%2520different%2520samples%2520and%2520their%2520augmentations.%2520Our%250Atheoretical%2520analysis%2520delves%2520into%2520how%2520SSL%2520objectives%2520fail%2520to%2520enforce%2520the%250Anecessary%2520constraints%2520between%2520samples%2520and%2520their%2520augmentations%252C%2520leading%2520to%2520poor%250Aperformance%2520in%2520complex%2520tasks.%2520We%2520provide%2520a%2520theoretical%2520framework%2520showing%2520that%250Athe%2520performance%2520gap%2520between%2520SSL%2520and%2520SL%2520mainly%2520stems%2520from%2520the%2520inability%2520of%2520SSL%250Amethods%2520to%2520capture%2520the%2520aggregation%2520of%2520similar%2520augmentations%2520and%2520the%2520separation%250Aof%2520dissimilar%2520augmentations.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520learnable%250Aregulator%2520called%2520Dynamic%2520Semantic%2520Adjuster%2520%2528DSA%2529.%2520DSA%2520aggregates%2520and%2520separates%250Asamples%2520in%2520the%2520feature%2520space%2520while%2520being%2520robust%2520to%2520outliers.%2520Through%2520extensive%250Aempirical%2520evaluations%2520on%2520multiple%2520benchmark%2520datasets%252C%2520we%2520demonstrate%2520the%250Asuperiority%2520of%2520DSA%2520in%2520enhancing%2520feature%2520aggregation%2520and%2520separation%252C%2520ultimately%250Aclosing%2520the%2520performance%2520gap%2520between%2520SSL%2520and%2520SL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Discriminability%20of%20Self-Supervised%20Representation%20Learning&entry.906535625=Zeen%20Song%20and%20Wenwen%20Qiang%20and%20Changwen%20Zheng%20and%20Fuchun%20Sun%20and%20Hui%20Xiong&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20recently%20achieved%20significant%20success%20in%0Adownstream%20visual%20tasks.%20However%2C%20a%20notable%20gap%20still%20exists%20between%20SSL%20and%0Asupervised%20learning%20%28SL%29%2C%20especially%20in%20complex%20downstream%20tasks.%20In%20this%0Apaper%2C%20we%20show%20that%20the%20features%20learned%20by%20SSL%20methods%20suffer%20from%20the%0Acrowding%20problem%2C%20where%20features%20of%20different%20classes%20are%20not%20distinctly%0Aseparated%2C%20and%20features%20within%20the%20same%20class%20exhibit%20large%20intra-class%0Avariance.%20In%20contrast%2C%20SL%20ensures%20a%20clear%20separation%20between%20classes.%20We%0Aanalyze%20this%20phenomenon%20and%20conclude%20that%20SSL%20objectives%20do%20not%20constrain%20the%0Arelationships%20between%20different%20samples%20and%20their%20augmentations.%20Our%0Atheoretical%20analysis%20delves%20into%20how%20SSL%20objectives%20fail%20to%20enforce%20the%0Anecessary%20constraints%20between%20samples%20and%20their%20augmentations%2C%20leading%20to%20poor%0Aperformance%20in%20complex%20tasks.%20We%20provide%20a%20theoretical%20framework%20showing%20that%0Athe%20performance%20gap%20between%20SSL%20and%20SL%20mainly%20stems%20from%20the%20inability%20of%20SSL%0Amethods%20to%20capture%20the%20aggregation%20of%20similar%20augmentations%20and%20the%20separation%0Aof%20dissimilar%20augmentations.%20To%20address%20this%20issue%2C%20we%20propose%20a%20learnable%0Aregulator%20called%20Dynamic%20Semantic%20Adjuster%20%28DSA%29.%20DSA%20aggregates%20and%20separates%0Asamples%20in%20the%20feature%20space%20while%20being%20robust%20to%20outliers.%20Through%20extensive%0Aempirical%20evaluations%20on%20multiple%20benchmark%20datasets%2C%20we%20demonstrate%20the%0Asuperiority%20of%20DSA%20in%20enhancing%20feature%20aggregation%20and%20separation%2C%20ultimately%0Aclosing%20the%20performance%20gap%20between%20SSL%20and%20SL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13541v1&entry.124074799=Read"},
{"title": "PICASSO: A Feed-Forward Framework for Parametric Inference of CAD\n  Sketches via Rendering Self-Supervision", "author": "Ahmet Serdar Karadeniz and Dimitrios Mallis and Nesryne Mejri and Kseniya Cherenkova and Anis Kacem and Djamila Aouada", "abstract": "  We propose PICASSO, a novel framework CAD sketch parameterization from\nhand-drawn or precise sketch images via rendering self-supervision. Given a\ndrawing of a CAD sketch, the proposed framework turns it into parametric\nprimitives that can be imported into CAD software. Compared to existing\nmethods, PICASSO enables the learning of parametric CAD sketches from either\nprecise or hand-drawn sketch images, even in cases where annotations at the\nparameter level are scarce or unavailable. This is achieved by leveraging the\ngeometric characteristics of sketches as a learning cue to pre-train a CAD\nparameterization network. Specifically, PICASSO comprises two primary\ncomponents: (1) a Sketch Parameterization Network (SPN) that predicts a series\nof parametric primitives from CAD sketch images, and (2) a Sketch Rendering\nNetwork (SRN) that renders parametric CAD sketches in a differentiable manner.\nSRN facilitates the computation of a image-to-image loss, which can be utilized\nto pre-train SPN, thereby enabling zero- and few-shot learning scenarios for\nthe parameterization of hand-drawn sketches. Extensive evaluation on the widely\nused SketchGraphs dataset validates the effectiveness of the proposed\nframework.\n", "link": "http://arxiv.org/abs/2407.13394v1", "date": "2024-07-18", "relevancy": 2.6662, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5713}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5142}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PICASSO%3A%20A%20Feed-Forward%20Framework%20for%20Parametric%20Inference%20of%20CAD%0A%20%20Sketches%20via%20Rendering%20Self-Supervision&body=Title%3A%20PICASSO%3A%20A%20Feed-Forward%20Framework%20for%20Parametric%20Inference%20of%20CAD%0A%20%20Sketches%20via%20Rendering%20Self-Supervision%0AAuthor%3A%20Ahmet%20Serdar%20Karadeniz%20and%20Dimitrios%20Mallis%20and%20Nesryne%20Mejri%20and%20Kseniya%20Cherenkova%20and%20Anis%20Kacem%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20We%20propose%20PICASSO%2C%20a%20novel%20framework%20CAD%20sketch%20parameterization%20from%0Ahand-drawn%20or%20precise%20sketch%20images%20via%20rendering%20self-supervision.%20Given%20a%0Adrawing%20of%20a%20CAD%20sketch%2C%20the%20proposed%20framework%20turns%20it%20into%20parametric%0Aprimitives%20that%20can%20be%20imported%20into%20CAD%20software.%20Compared%20to%20existing%0Amethods%2C%20PICASSO%20enables%20the%20learning%20of%20parametric%20CAD%20sketches%20from%20either%0Aprecise%20or%20hand-drawn%20sketch%20images%2C%20even%20in%20cases%20where%20annotations%20at%20the%0Aparameter%20level%20are%20scarce%20or%20unavailable.%20This%20is%20achieved%20by%20leveraging%20the%0Ageometric%20characteristics%20of%20sketches%20as%20a%20learning%20cue%20to%20pre-train%20a%20CAD%0Aparameterization%20network.%20Specifically%2C%20PICASSO%20comprises%20two%20primary%0Acomponents%3A%20%281%29%20a%20Sketch%20Parameterization%20Network%20%28SPN%29%20that%20predicts%20a%20series%0Aof%20parametric%20primitives%20from%20CAD%20sketch%20images%2C%20and%20%282%29%20a%20Sketch%20Rendering%0ANetwork%20%28SRN%29%20that%20renders%20parametric%20CAD%20sketches%20in%20a%20differentiable%20manner.%0ASRN%20facilitates%20the%20computation%20of%20a%20image-to-image%20loss%2C%20which%20can%20be%20utilized%0Ato%20pre-train%20SPN%2C%20thereby%20enabling%20zero-%20and%20few-shot%20learning%20scenarios%20for%0Athe%20parameterization%20of%20hand-drawn%20sketches.%20Extensive%20evaluation%20on%20the%20widely%0Aused%20SketchGraphs%20dataset%20validates%20the%20effectiveness%20of%20the%20proposed%0Aframework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPICASSO%253A%2520A%2520Feed-Forward%2520Framework%2520for%2520Parametric%2520Inference%2520of%2520CAD%250A%2520%2520Sketches%2520via%2520Rendering%2520Self-Supervision%26entry.906535625%3DAhmet%2520Serdar%2520Karadeniz%2520and%2520Dimitrios%2520Mallis%2520and%2520Nesryne%2520Mejri%2520and%2520Kseniya%2520Cherenkova%2520and%2520Anis%2520Kacem%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520We%2520propose%2520PICASSO%252C%2520a%2520novel%2520framework%2520CAD%2520sketch%2520parameterization%2520from%250Ahand-drawn%2520or%2520precise%2520sketch%2520images%2520via%2520rendering%2520self-supervision.%2520Given%2520a%250Adrawing%2520of%2520a%2520CAD%2520sketch%252C%2520the%2520proposed%2520framework%2520turns%2520it%2520into%2520parametric%250Aprimitives%2520that%2520can%2520be%2520imported%2520into%2520CAD%2520software.%2520Compared%2520to%2520existing%250Amethods%252C%2520PICASSO%2520enables%2520the%2520learning%2520of%2520parametric%2520CAD%2520sketches%2520from%2520either%250Aprecise%2520or%2520hand-drawn%2520sketch%2520images%252C%2520even%2520in%2520cases%2520where%2520annotations%2520at%2520the%250Aparameter%2520level%2520are%2520scarce%2520or%2520unavailable.%2520This%2520is%2520achieved%2520by%2520leveraging%2520the%250Ageometric%2520characteristics%2520of%2520sketches%2520as%2520a%2520learning%2520cue%2520to%2520pre-train%2520a%2520CAD%250Aparameterization%2520network.%2520Specifically%252C%2520PICASSO%2520comprises%2520two%2520primary%250Acomponents%253A%2520%25281%2529%2520a%2520Sketch%2520Parameterization%2520Network%2520%2528SPN%2529%2520that%2520predicts%2520a%2520series%250Aof%2520parametric%2520primitives%2520from%2520CAD%2520sketch%2520images%252C%2520and%2520%25282%2529%2520a%2520Sketch%2520Rendering%250ANetwork%2520%2528SRN%2529%2520that%2520renders%2520parametric%2520CAD%2520sketches%2520in%2520a%2520differentiable%2520manner.%250ASRN%2520facilitates%2520the%2520computation%2520of%2520a%2520image-to-image%2520loss%252C%2520which%2520can%2520be%2520utilized%250Ato%2520pre-train%2520SPN%252C%2520thereby%2520enabling%2520zero-%2520and%2520few-shot%2520learning%2520scenarios%2520for%250Athe%2520parameterization%2520of%2520hand-drawn%2520sketches.%2520Extensive%2520evaluation%2520on%2520the%2520widely%250Aused%2520SketchGraphs%2520dataset%2520validates%2520the%2520effectiveness%2520of%2520the%2520proposed%250Aframework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PICASSO%3A%20A%20Feed-Forward%20Framework%20for%20Parametric%20Inference%20of%20CAD%0A%20%20Sketches%20via%20Rendering%20Self-Supervision&entry.906535625=Ahmet%20Serdar%20Karadeniz%20and%20Dimitrios%20Mallis%20and%20Nesryne%20Mejri%20and%20Kseniya%20Cherenkova%20and%20Anis%20Kacem%20and%20Djamila%20Aouada&entry.1292438233=%20%20We%20propose%20PICASSO%2C%20a%20novel%20framework%20CAD%20sketch%20parameterization%20from%0Ahand-drawn%20or%20precise%20sketch%20images%20via%20rendering%20self-supervision.%20Given%20a%0Adrawing%20of%20a%20CAD%20sketch%2C%20the%20proposed%20framework%20turns%20it%20into%20parametric%0Aprimitives%20that%20can%20be%20imported%20into%20CAD%20software.%20Compared%20to%20existing%0Amethods%2C%20PICASSO%20enables%20the%20learning%20of%20parametric%20CAD%20sketches%20from%20either%0Aprecise%20or%20hand-drawn%20sketch%20images%2C%20even%20in%20cases%20where%20annotations%20at%20the%0Aparameter%20level%20are%20scarce%20or%20unavailable.%20This%20is%20achieved%20by%20leveraging%20the%0Ageometric%20characteristics%20of%20sketches%20as%20a%20learning%20cue%20to%20pre-train%20a%20CAD%0Aparameterization%20network.%20Specifically%2C%20PICASSO%20comprises%20two%20primary%0Acomponents%3A%20%281%29%20a%20Sketch%20Parameterization%20Network%20%28SPN%29%20that%20predicts%20a%20series%0Aof%20parametric%20primitives%20from%20CAD%20sketch%20images%2C%20and%20%282%29%20a%20Sketch%20Rendering%0ANetwork%20%28SRN%29%20that%20renders%20parametric%20CAD%20sketches%20in%20a%20differentiable%20manner.%0ASRN%20facilitates%20the%20computation%20of%20a%20image-to-image%20loss%2C%20which%20can%20be%20utilized%0Ato%20pre-train%20SPN%2C%20thereby%20enabling%20zero-%20and%20few-shot%20learning%20scenarios%20for%0Athe%20parameterization%20of%20hand-drawn%20sketches.%20Extensive%20evaluation%20on%20the%20widely%0Aused%20SketchGraphs%20dataset%20validates%20the%20effectiveness%20of%20the%20proposed%0Aframework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13394v1&entry.124074799=Read"},
{"title": "SPAMming Labels: Efficient Annotations for the Trackers of Tomorrow", "author": "Orcun Cetintas and Tim Meinhardt and Guillem Bras\u00f3 and Laura Leal-Taix\u00e9", "abstract": "  Increasing the annotation efficiency of trajectory annotations from videos\nhas the potential to enable the next generation of data-hungry tracking\nalgorithms to thrive on large-scale datasets. Despite the importance of this\ntask, there are currently very few works exploring how to efficiently label\ntracking datasets comprehensively. In this work, we introduce SPAM, a video\nlabel engine that provides high-quality labels with minimal human intervention.\nSPAM is built around two key insights: i) most tracking scenarios can be easily\nresolved. To take advantage of this, we utilize a pre-trained model to generate\nhigh-quality pseudo-labels, reserving human involvement for a smaller subset of\nmore difficult instances; ii) handling the spatiotemporal dependencies of track\nannotations across time can be elegantly and efficiently formulated through\ngraphs. Therefore, we use a unified graph formulation to address the annotation\nof both detections and identity association for tracks across time. Based on\nthese insights, SPAM produces high-quality annotations with a fraction of\nground truth labeling cost. We demonstrate that trackers trained on SPAM labels\nachieve comparable performance to those trained on human annotations while\nrequiring only $3-20\\%$ of the human labeling effort. Hence, SPAM paves the way\ntowards highly efficient labeling of large-scale tracking datasets. We release\nall models and code.\n", "link": "http://arxiv.org/abs/2404.11426v2", "date": "2024-07-18", "relevancy": 2.6652, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5337}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5328}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPAMming%20Labels%3A%20Efficient%20Annotations%20for%20the%20Trackers%20of%20Tomorrow&body=Title%3A%20SPAMming%20Labels%3A%20Efficient%20Annotations%20for%20the%20Trackers%20of%20Tomorrow%0AAuthor%3A%20Orcun%20Cetintas%20and%20Tim%20Meinhardt%20and%20Guillem%20Bras%C3%B3%20and%20Laura%20Leal-Taix%C3%A9%0AAbstract%3A%20%20%20Increasing%20the%20annotation%20efficiency%20of%20trajectory%20annotations%20from%20videos%0Ahas%20the%20potential%20to%20enable%20the%20next%20generation%20of%20data-hungry%20tracking%0Aalgorithms%20to%20thrive%20on%20large-scale%20datasets.%20Despite%20the%20importance%20of%20this%0Atask%2C%20there%20are%20currently%20very%20few%20works%20exploring%20how%20to%20efficiently%20label%0Atracking%20datasets%20comprehensively.%20In%20this%20work%2C%20we%20introduce%20SPAM%2C%20a%20video%0Alabel%20engine%20that%20provides%20high-quality%20labels%20with%20minimal%20human%20intervention.%0ASPAM%20is%20built%20around%20two%20key%20insights%3A%20i%29%20most%20tracking%20scenarios%20can%20be%20easily%0Aresolved.%20To%20take%20advantage%20of%20this%2C%20we%20utilize%20a%20pre-trained%20model%20to%20generate%0Ahigh-quality%20pseudo-labels%2C%20reserving%20human%20involvement%20for%20a%20smaller%20subset%20of%0Amore%20difficult%20instances%3B%20ii%29%20handling%20the%20spatiotemporal%20dependencies%20of%20track%0Aannotations%20across%20time%20can%20be%20elegantly%20and%20efficiently%20formulated%20through%0Agraphs.%20Therefore%2C%20we%20use%20a%20unified%20graph%20formulation%20to%20address%20the%20annotation%0Aof%20both%20detections%20and%20identity%20association%20for%20tracks%20across%20time.%20Based%20on%0Athese%20insights%2C%20SPAM%20produces%20high-quality%20annotations%20with%20a%20fraction%20of%0Aground%20truth%20labeling%20cost.%20We%20demonstrate%20that%20trackers%20trained%20on%20SPAM%20labels%0Aachieve%20comparable%20performance%20to%20those%20trained%20on%20human%20annotations%20while%0Arequiring%20only%20%243-20%5C%25%24%20of%20the%20human%20labeling%20effort.%20Hence%2C%20SPAM%20paves%20the%20way%0Atowards%20highly%20efficient%20labeling%20of%20large-scale%20tracking%20datasets.%20We%20release%0Aall%20models%20and%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11426v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPAMming%2520Labels%253A%2520Efficient%2520Annotations%2520for%2520the%2520Trackers%2520of%2520Tomorrow%26entry.906535625%3DOrcun%2520Cetintas%2520and%2520Tim%2520Meinhardt%2520and%2520Guillem%2520Bras%25C3%25B3%2520and%2520Laura%2520Leal-Taix%25C3%25A9%26entry.1292438233%3D%2520%2520Increasing%2520the%2520annotation%2520efficiency%2520of%2520trajectory%2520annotations%2520from%2520videos%250Ahas%2520the%2520potential%2520to%2520enable%2520the%2520next%2520generation%2520of%2520data-hungry%2520tracking%250Aalgorithms%2520to%2520thrive%2520on%2520large-scale%2520datasets.%2520Despite%2520the%2520importance%2520of%2520this%250Atask%252C%2520there%2520are%2520currently%2520very%2520few%2520works%2520exploring%2520how%2520to%2520efficiently%2520label%250Atracking%2520datasets%2520comprehensively.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SPAM%252C%2520a%2520video%250Alabel%2520engine%2520that%2520provides%2520high-quality%2520labels%2520with%2520minimal%2520human%2520intervention.%250ASPAM%2520is%2520built%2520around%2520two%2520key%2520insights%253A%2520i%2529%2520most%2520tracking%2520scenarios%2520can%2520be%2520easily%250Aresolved.%2520To%2520take%2520advantage%2520of%2520this%252C%2520we%2520utilize%2520a%2520pre-trained%2520model%2520to%2520generate%250Ahigh-quality%2520pseudo-labels%252C%2520reserving%2520human%2520involvement%2520for%2520a%2520smaller%2520subset%2520of%250Amore%2520difficult%2520instances%253B%2520ii%2529%2520handling%2520the%2520spatiotemporal%2520dependencies%2520of%2520track%250Aannotations%2520across%2520time%2520can%2520be%2520elegantly%2520and%2520efficiently%2520formulated%2520through%250Agraphs.%2520Therefore%252C%2520we%2520use%2520a%2520unified%2520graph%2520formulation%2520to%2520address%2520the%2520annotation%250Aof%2520both%2520detections%2520and%2520identity%2520association%2520for%2520tracks%2520across%2520time.%2520Based%2520on%250Athese%2520insights%252C%2520SPAM%2520produces%2520high-quality%2520annotations%2520with%2520a%2520fraction%2520of%250Aground%2520truth%2520labeling%2520cost.%2520We%2520demonstrate%2520that%2520trackers%2520trained%2520on%2520SPAM%2520labels%250Aachieve%2520comparable%2520performance%2520to%2520those%2520trained%2520on%2520human%2520annotations%2520while%250Arequiring%2520only%2520%25243-20%255C%2525%2524%2520of%2520the%2520human%2520labeling%2520effort.%2520Hence%252C%2520SPAM%2520paves%2520the%2520way%250Atowards%2520highly%2520efficient%2520labeling%2520of%2520large-scale%2520tracking%2520datasets.%2520We%2520release%250Aall%2520models%2520and%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11426v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPAMming%20Labels%3A%20Efficient%20Annotations%20for%20the%20Trackers%20of%20Tomorrow&entry.906535625=Orcun%20Cetintas%20and%20Tim%20Meinhardt%20and%20Guillem%20Bras%C3%B3%20and%20Laura%20Leal-Taix%C3%A9&entry.1292438233=%20%20Increasing%20the%20annotation%20efficiency%20of%20trajectory%20annotations%20from%20videos%0Ahas%20the%20potential%20to%20enable%20the%20next%20generation%20of%20data-hungry%20tracking%0Aalgorithms%20to%20thrive%20on%20large-scale%20datasets.%20Despite%20the%20importance%20of%20this%0Atask%2C%20there%20are%20currently%20very%20few%20works%20exploring%20how%20to%20efficiently%20label%0Atracking%20datasets%20comprehensively.%20In%20this%20work%2C%20we%20introduce%20SPAM%2C%20a%20video%0Alabel%20engine%20that%20provides%20high-quality%20labels%20with%20minimal%20human%20intervention.%0ASPAM%20is%20built%20around%20two%20key%20insights%3A%20i%29%20most%20tracking%20scenarios%20can%20be%20easily%0Aresolved.%20To%20take%20advantage%20of%20this%2C%20we%20utilize%20a%20pre-trained%20model%20to%20generate%0Ahigh-quality%20pseudo-labels%2C%20reserving%20human%20involvement%20for%20a%20smaller%20subset%20of%0Amore%20difficult%20instances%3B%20ii%29%20handling%20the%20spatiotemporal%20dependencies%20of%20track%0Aannotations%20across%20time%20can%20be%20elegantly%20and%20efficiently%20formulated%20through%0Agraphs.%20Therefore%2C%20we%20use%20a%20unified%20graph%20formulation%20to%20address%20the%20annotation%0Aof%20both%20detections%20and%20identity%20association%20for%20tracks%20across%20time.%20Based%20on%0Athese%20insights%2C%20SPAM%20produces%20high-quality%20annotations%20with%20a%20fraction%20of%0Aground%20truth%20labeling%20cost.%20We%20demonstrate%20that%20trackers%20trained%20on%20SPAM%20labels%0Aachieve%20comparable%20performance%20to%20those%20trained%20on%20human%20annotations%20while%0Arequiring%20only%20%243-20%5C%25%24%20of%20the%20human%20labeling%20effort.%20Hence%2C%20SPAM%20paves%20the%20way%0Atowards%20highly%20efficient%20labeling%20of%20large-scale%20tracking%20datasets.%20We%20release%0Aall%20models%20and%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11426v2&entry.124074799=Read"},
{"title": "GlobalPointer: Large-Scale Plane Adjustment with Bi-Convex Relaxation", "author": "Bangyan Liao and Zhenjun Zhao and Lu Chen and Haoang Li and Daniel Cremers and Peidong Liu", "abstract": "  Plane adjustment (PA) is crucial for many 3D applications, involving\nsimultaneous pose estimation and plane recovery. Despite recent advancements,\nit remains a challenging problem in the realm of multi-view point cloud\nregistration. Current state-of-the-art methods can achieve globally optimal\nconvergence only with good initialization. Furthermore, their high time\ncomplexity renders them impractical for large-scale problems. To address these\nchallenges, we first exploit a novel optimization strategy termed\n\\textit{Bi-Convex Relaxation}, which decouples the original problem into two\nsimpler sub-problems, reformulates each sub-problem using a convex relaxation\ntechnique, and alternately solves each one until the original problem\nconverges. Building on this strategy, we propose two algorithmic variants for\nsolving the plane adjustment problem, namely \\textit{GlobalPointer} and\n\\textit{GlobalPointer++}, based on point-to-plane and plane-to-plane errors,\nrespectively. Extensive experiments on both synthetic and real datasets\ndemonstrate that our method can perform large-scale plane adjustment with\nlinear time complexity, larger convergence region, and robustness to poor\ninitialization, while achieving similar accuracy as prior methods. The code is\navailable at\n\\href{https://github.com/wu-cvgl/GlobalPointer}{github.com/wu-cvgl/GlobalPointer}\n", "link": "http://arxiv.org/abs/2407.13537v1", "date": "2024-07-18", "relevancy": 2.6519, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5524}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.548}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GlobalPointer%3A%20Large-Scale%20Plane%20Adjustment%20with%20Bi-Convex%20Relaxation&body=Title%3A%20GlobalPointer%3A%20Large-Scale%20Plane%20Adjustment%20with%20Bi-Convex%20Relaxation%0AAuthor%3A%20Bangyan%20Liao%20and%20Zhenjun%20Zhao%20and%20Lu%20Chen%20and%20Haoang%20Li%20and%20Daniel%20Cremers%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Plane%20adjustment%20%28PA%29%20is%20crucial%20for%20many%203D%20applications%2C%20involving%0Asimultaneous%20pose%20estimation%20and%20plane%20recovery.%20Despite%20recent%20advancements%2C%0Ait%20remains%20a%20challenging%20problem%20in%20the%20realm%20of%20multi-view%20point%20cloud%0Aregistration.%20Current%20state-of-the-art%20methods%20can%20achieve%20globally%20optimal%0Aconvergence%20only%20with%20good%20initialization.%20Furthermore%2C%20their%20high%20time%0Acomplexity%20renders%20them%20impractical%20for%20large-scale%20problems.%20To%20address%20these%0Achallenges%2C%20we%20first%20exploit%20a%20novel%20optimization%20strategy%20termed%0A%5Ctextit%7BBi-Convex%20Relaxation%7D%2C%20which%20decouples%20the%20original%20problem%20into%20two%0Asimpler%20sub-problems%2C%20reformulates%20each%20sub-problem%20using%20a%20convex%20relaxation%0Atechnique%2C%20and%20alternately%20solves%20each%20one%20until%20the%20original%20problem%0Aconverges.%20Building%20on%20this%20strategy%2C%20we%20propose%20two%20algorithmic%20variants%20for%0Asolving%20the%20plane%20adjustment%20problem%2C%20namely%20%5Ctextit%7BGlobalPointer%7D%20and%0A%5Ctextit%7BGlobalPointer%2B%2B%7D%2C%20based%20on%20point-to-plane%20and%20plane-to-plane%20errors%2C%0Arespectively.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real%20datasets%0Ademonstrate%20that%20our%20method%20can%20perform%20large-scale%20plane%20adjustment%20with%0Alinear%20time%20complexity%2C%20larger%20convergence%20region%2C%20and%20robustness%20to%20poor%0Ainitialization%2C%20while%20achieving%20similar%20accuracy%20as%20prior%20methods.%20The%20code%20is%0Aavailable%20at%0A%5Chref%7Bhttps%3A//github.com/wu-cvgl/GlobalPointer%7D%7Bgithub.com/wu-cvgl/GlobalPointer%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobalPointer%253A%2520Large-Scale%2520Plane%2520Adjustment%2520with%2520Bi-Convex%2520Relaxation%26entry.906535625%3DBangyan%2520Liao%2520and%2520Zhenjun%2520Zhao%2520and%2520Lu%2520Chen%2520and%2520Haoang%2520Li%2520and%2520Daniel%2520Cremers%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Plane%2520adjustment%2520%2528PA%2529%2520is%2520crucial%2520for%2520many%25203D%2520applications%252C%2520involving%250Asimultaneous%2520pose%2520estimation%2520and%2520plane%2520recovery.%2520Despite%2520recent%2520advancements%252C%250Ait%2520remains%2520a%2520challenging%2520problem%2520in%2520the%2520realm%2520of%2520multi-view%2520point%2520cloud%250Aregistration.%2520Current%2520state-of-the-art%2520methods%2520can%2520achieve%2520globally%2520optimal%250Aconvergence%2520only%2520with%2520good%2520initialization.%2520Furthermore%252C%2520their%2520high%2520time%250Acomplexity%2520renders%2520them%2520impractical%2520for%2520large-scale%2520problems.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520first%2520exploit%2520a%2520novel%2520optimization%2520strategy%2520termed%250A%255Ctextit%257BBi-Convex%2520Relaxation%257D%252C%2520which%2520decouples%2520the%2520original%2520problem%2520into%2520two%250Asimpler%2520sub-problems%252C%2520reformulates%2520each%2520sub-problem%2520using%2520a%2520convex%2520relaxation%250Atechnique%252C%2520and%2520alternately%2520solves%2520each%2520one%2520until%2520the%2520original%2520problem%250Aconverges.%2520Building%2520on%2520this%2520strategy%252C%2520we%2520propose%2520two%2520algorithmic%2520variants%2520for%250Asolving%2520the%2520plane%2520adjustment%2520problem%252C%2520namely%2520%255Ctextit%257BGlobalPointer%257D%2520and%250A%255Ctextit%257BGlobalPointer%252B%252B%257D%252C%2520based%2520on%2520point-to-plane%2520and%2520plane-to-plane%2520errors%252C%250Arespectively.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520can%2520perform%2520large-scale%2520plane%2520adjustment%2520with%250Alinear%2520time%2520complexity%252C%2520larger%2520convergence%2520region%252C%2520and%2520robustness%2520to%2520poor%250Ainitialization%252C%2520while%2520achieving%2520similar%2520accuracy%2520as%2520prior%2520methods.%2520The%2520code%2520is%250Aavailable%2520at%250A%255Chref%257Bhttps%253A//github.com/wu-cvgl/GlobalPointer%257D%257Bgithub.com/wu-cvgl/GlobalPointer%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GlobalPointer%3A%20Large-Scale%20Plane%20Adjustment%20with%20Bi-Convex%20Relaxation&entry.906535625=Bangyan%20Liao%20and%20Zhenjun%20Zhao%20and%20Lu%20Chen%20and%20Haoang%20Li%20and%20Daniel%20Cremers%20and%20Peidong%20Liu&entry.1292438233=%20%20Plane%20adjustment%20%28PA%29%20is%20crucial%20for%20many%203D%20applications%2C%20involving%0Asimultaneous%20pose%20estimation%20and%20plane%20recovery.%20Despite%20recent%20advancements%2C%0Ait%20remains%20a%20challenging%20problem%20in%20the%20realm%20of%20multi-view%20point%20cloud%0Aregistration.%20Current%20state-of-the-art%20methods%20can%20achieve%20globally%20optimal%0Aconvergence%20only%20with%20good%20initialization.%20Furthermore%2C%20their%20high%20time%0Acomplexity%20renders%20them%20impractical%20for%20large-scale%20problems.%20To%20address%20these%0Achallenges%2C%20we%20first%20exploit%20a%20novel%20optimization%20strategy%20termed%0A%5Ctextit%7BBi-Convex%20Relaxation%7D%2C%20which%20decouples%20the%20original%20problem%20into%20two%0Asimpler%20sub-problems%2C%20reformulates%20each%20sub-problem%20using%20a%20convex%20relaxation%0Atechnique%2C%20and%20alternately%20solves%20each%20one%20until%20the%20original%20problem%0Aconverges.%20Building%20on%20this%20strategy%2C%20we%20propose%20two%20algorithmic%20variants%20for%0Asolving%20the%20plane%20adjustment%20problem%2C%20namely%20%5Ctextit%7BGlobalPointer%7D%20and%0A%5Ctextit%7BGlobalPointer%2B%2B%7D%2C%20based%20on%20point-to-plane%20and%20plane-to-plane%20errors%2C%0Arespectively.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real%20datasets%0Ademonstrate%20that%20our%20method%20can%20perform%20large-scale%20plane%20adjustment%20with%0Alinear%20time%20complexity%2C%20larger%20convergence%20region%2C%20and%20robustness%20to%20poor%0Ainitialization%2C%20while%20achieving%20similar%20accuracy%20as%20prior%20methods.%20The%20code%20is%0Aavailable%20at%0A%5Chref%7Bhttps%3A//github.com/wu-cvgl/GlobalPointer%7D%7Bgithub.com/wu-cvgl/GlobalPointer%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13537v1&entry.124074799=Read"},
{"title": "Robust Calibration of Large Vision-Language Adapters", "author": "Balamurali Murugesan and Julio Silva-Rodriguez and Ismail Ben Ayed and Jose Dolz", "abstract": "  This paper addresses the critical issue of miscalibration in CLIP-based model\nadaptation, particularly in the challenging scenario of out-of-distribution\n(OOD) samples, which has been overlooked in the existing literature on CLIP\nadaptation. We empirically demonstrate that popular CLIP adaptation approaches,\nsuch as Adapters, Prompt Learning, and Test-Time Adaptation, substantially\ndegrade the calibration capabilities of the zero-shot baseline in the presence\nof distributional drift. We identify the increase in logit ranges as the\nunderlying cause of miscalibration of CLIP adaptation methods, contrasting with\nprevious work on calibrating fully-supervised models. Motivated by these\nobservations, we present a simple and model-agnostic solution to mitigate\nmiscalibration, by scaling the logit range of each sample to its zero-shot\nprediction logits. We explore three different alternatives to achieve this,\nwhich can be either integrated during adaptation or directly used at inference\ntime. Comprehensive experiments on popular OOD classification benchmarks\ndemonstrate the effectiveness of the proposed approaches in mitigating\nmiscalibration while maintaining discriminative performance, whose improvements\nare consistent across the three families of these increasingly popular\napproaches. The code is publicly available at:\nhttps://github.com/Bala93/CLIPCalib\n", "link": "http://arxiv.org/abs/2407.13588v1", "date": "2024-07-18", "relevancy": 2.6341, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5684}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5155}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Calibration%20of%20Large%20Vision-Language%20Adapters&body=Title%3A%20Robust%20Calibration%20of%20Large%20Vision-Language%20Adapters%0AAuthor%3A%20Balamurali%20Murugesan%20and%20Julio%20Silva-Rodriguez%20and%20Ismail%20Ben%20Ayed%20and%20Jose%20Dolz%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20critical%20issue%20of%20miscalibration%20in%20CLIP-based%20model%0Aadaptation%2C%20particularly%20in%20the%20challenging%20scenario%20of%20out-of-distribution%0A%28OOD%29%20samples%2C%20which%20has%20been%20overlooked%20in%20the%20existing%20literature%20on%20CLIP%0Aadaptation.%20We%20empirically%20demonstrate%20that%20popular%20CLIP%20adaptation%20approaches%2C%0Asuch%20as%20Adapters%2C%20Prompt%20Learning%2C%20and%20Test-Time%20Adaptation%2C%20substantially%0Adegrade%20the%20calibration%20capabilities%20of%20the%20zero-shot%20baseline%20in%20the%20presence%0Aof%20distributional%20drift.%20We%20identify%20the%20increase%20in%20logit%20ranges%20as%20the%0Aunderlying%20cause%20of%20miscalibration%20of%20CLIP%20adaptation%20methods%2C%20contrasting%20with%0Aprevious%20work%20on%20calibrating%20fully-supervised%20models.%20Motivated%20by%20these%0Aobservations%2C%20we%20present%20a%20simple%20and%20model-agnostic%20solution%20to%20mitigate%0Amiscalibration%2C%20by%20scaling%20the%20logit%20range%20of%20each%20sample%20to%20its%20zero-shot%0Aprediction%20logits.%20We%20explore%20three%20different%20alternatives%20to%20achieve%20this%2C%0Awhich%20can%20be%20either%20integrated%20during%20adaptation%20or%20directly%20used%20at%20inference%0Atime.%20Comprehensive%20experiments%20on%20popular%20OOD%20classification%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approaches%20in%20mitigating%0Amiscalibration%20while%20maintaining%20discriminative%20performance%2C%20whose%20improvements%0Aare%20consistent%20across%20the%20three%20families%20of%20these%20increasingly%20popular%0Aapproaches.%20The%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Bala93/CLIPCalib%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Calibration%2520of%2520Large%2520Vision-Language%2520Adapters%26entry.906535625%3DBalamurali%2520Murugesan%2520and%2520Julio%2520Silva-Rodriguez%2520and%2520Ismail%2520Ben%2520Ayed%2520and%2520Jose%2520Dolz%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520critical%2520issue%2520of%2520miscalibration%2520in%2520CLIP-based%2520model%250Aadaptation%252C%2520particularly%2520in%2520the%2520challenging%2520scenario%2520of%2520out-of-distribution%250A%2528OOD%2529%2520samples%252C%2520which%2520has%2520been%2520overlooked%2520in%2520the%2520existing%2520literature%2520on%2520CLIP%250Aadaptation.%2520We%2520empirically%2520demonstrate%2520that%2520popular%2520CLIP%2520adaptation%2520approaches%252C%250Asuch%2520as%2520Adapters%252C%2520Prompt%2520Learning%252C%2520and%2520Test-Time%2520Adaptation%252C%2520substantially%250Adegrade%2520the%2520calibration%2520capabilities%2520of%2520the%2520zero-shot%2520baseline%2520in%2520the%2520presence%250Aof%2520distributional%2520drift.%2520We%2520identify%2520the%2520increase%2520in%2520logit%2520ranges%2520as%2520the%250Aunderlying%2520cause%2520of%2520miscalibration%2520of%2520CLIP%2520adaptation%2520methods%252C%2520contrasting%2520with%250Aprevious%2520work%2520on%2520calibrating%2520fully-supervised%2520models.%2520Motivated%2520by%2520these%250Aobservations%252C%2520we%2520present%2520a%2520simple%2520and%2520model-agnostic%2520solution%2520to%2520mitigate%250Amiscalibration%252C%2520by%2520scaling%2520the%2520logit%2520range%2520of%2520each%2520sample%2520to%2520its%2520zero-shot%250Aprediction%2520logits.%2520We%2520explore%2520three%2520different%2520alternatives%2520to%2520achieve%2520this%252C%250Awhich%2520can%2520be%2520either%2520integrated%2520during%2520adaptation%2520or%2520directly%2520used%2520at%2520inference%250Atime.%2520Comprehensive%2520experiments%2520on%2520popular%2520OOD%2520classification%2520benchmarks%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approaches%2520in%2520mitigating%250Amiscalibration%2520while%2520maintaining%2520discriminative%2520performance%252C%2520whose%2520improvements%250Aare%2520consistent%2520across%2520the%2520three%2520families%2520of%2520these%2520increasingly%2520popular%250Aapproaches.%2520The%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/Bala93/CLIPCalib%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Calibration%20of%20Large%20Vision-Language%20Adapters&entry.906535625=Balamurali%20Murugesan%20and%20Julio%20Silva-Rodriguez%20and%20Ismail%20Ben%20Ayed%20and%20Jose%20Dolz&entry.1292438233=%20%20This%20paper%20addresses%20the%20critical%20issue%20of%20miscalibration%20in%20CLIP-based%20model%0Aadaptation%2C%20particularly%20in%20the%20challenging%20scenario%20of%20out-of-distribution%0A%28OOD%29%20samples%2C%20which%20has%20been%20overlooked%20in%20the%20existing%20literature%20on%20CLIP%0Aadaptation.%20We%20empirically%20demonstrate%20that%20popular%20CLIP%20adaptation%20approaches%2C%0Asuch%20as%20Adapters%2C%20Prompt%20Learning%2C%20and%20Test-Time%20Adaptation%2C%20substantially%0Adegrade%20the%20calibration%20capabilities%20of%20the%20zero-shot%20baseline%20in%20the%20presence%0Aof%20distributional%20drift.%20We%20identify%20the%20increase%20in%20logit%20ranges%20as%20the%0Aunderlying%20cause%20of%20miscalibration%20of%20CLIP%20adaptation%20methods%2C%20contrasting%20with%0Aprevious%20work%20on%20calibrating%20fully-supervised%20models.%20Motivated%20by%20these%0Aobservations%2C%20we%20present%20a%20simple%20and%20model-agnostic%20solution%20to%20mitigate%0Amiscalibration%2C%20by%20scaling%20the%20logit%20range%20of%20each%20sample%20to%20its%20zero-shot%0Aprediction%20logits.%20We%20explore%20three%20different%20alternatives%20to%20achieve%20this%2C%0Awhich%20can%20be%20either%20integrated%20during%20adaptation%20or%20directly%20used%20at%20inference%0Atime.%20Comprehensive%20experiments%20on%20popular%20OOD%20classification%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approaches%20in%20mitigating%0Amiscalibration%20while%20maintaining%20discriminative%20performance%2C%20whose%20improvements%0Aare%20consistent%20across%20the%20three%20families%20of%20these%20increasingly%20popular%0Aapproaches.%20The%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Bala93/CLIPCalib%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13588v1&entry.124074799=Read"},
{"title": "Cross-Task Attack: A Self-Supervision Generative Framework Based on\n  Attention Shift", "author": "Qingyuan Zeng and Yunpeng Gong and Min Jiang", "abstract": "  Studying adversarial attacks on artificial intelligence (AI) systems helps\ndiscover model shortcomings, enabling the construction of a more robust system.\nMost existing adversarial attack methods only concentrate on single-task\nsingle-model or single-task cross-model scenarios, overlooking the multi-task\ncharacteristic of artificial intelligence systems. As a result, most of the\nexisting attacks do not pose a practical threat to a comprehensive and\ncollaborative AI system. However, implementing cross-task attacks is highly\ndemanding and challenging due to the difficulty in obtaining the real labels of\ndifferent tasks for the same picture and harmonizing the loss functions across\ndifferent tasks. To address this issue, we propose a self-supervised Cross-Task\nAttack framework (CTA), which utilizes co-attention and anti-attention maps to\ngenerate cross-task adversarial perturbation. Specifically, the co-attention\nmap reflects the area to which different visual task models pay attention,\nwhile the anti-attention map reflects the area that different visual task\nmodels neglect. CTA generates cross-task perturbations by shifting the\nattention area of samples away from the co-attention map and closer to the\nanti-attention map. We conduct extensive experiments on multiple vision tasks\nand the experimental results confirm the effectiveness of the proposed design\nfor adversarial attacks.\n", "link": "http://arxiv.org/abs/2407.13700v1", "date": "2024-07-18", "relevancy": 2.6268, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5272}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Task%20Attack%3A%20A%20Self-Supervision%20Generative%20Framework%20Based%20on%0A%20%20Attention%20Shift&body=Title%3A%20Cross-Task%20Attack%3A%20A%20Self-Supervision%20Generative%20Framework%20Based%20on%0A%20%20Attention%20Shift%0AAuthor%3A%20Qingyuan%20Zeng%20and%20Yunpeng%20Gong%20and%20Min%20Jiang%0AAbstract%3A%20%20%20Studying%20adversarial%20attacks%20on%20artificial%20intelligence%20%28AI%29%20systems%20helps%0Adiscover%20model%20shortcomings%2C%20enabling%20the%20construction%20of%20a%20more%20robust%20system.%0AMost%20existing%20adversarial%20attack%20methods%20only%20concentrate%20on%20single-task%0Asingle-model%20or%20single-task%20cross-model%20scenarios%2C%20overlooking%20the%20multi-task%0Acharacteristic%20of%20artificial%20intelligence%20systems.%20As%20a%20result%2C%20most%20of%20the%0Aexisting%20attacks%20do%20not%20pose%20a%20practical%20threat%20to%20a%20comprehensive%20and%0Acollaborative%20AI%20system.%20However%2C%20implementing%20cross-task%20attacks%20is%20highly%0Ademanding%20and%20challenging%20due%20to%20the%20difficulty%20in%20obtaining%20the%20real%20labels%20of%0Adifferent%20tasks%20for%20the%20same%20picture%20and%20harmonizing%20the%20loss%20functions%20across%0Adifferent%20tasks.%20To%20address%20this%20issue%2C%20we%20propose%20a%20self-supervised%20Cross-Task%0AAttack%20framework%20%28CTA%29%2C%20which%20utilizes%20co-attention%20and%20anti-attention%20maps%20to%0Agenerate%20cross-task%20adversarial%20perturbation.%20Specifically%2C%20the%20co-attention%0Amap%20reflects%20the%20area%20to%20which%20different%20visual%20task%20models%20pay%20attention%2C%0Awhile%20the%20anti-attention%20map%20reflects%20the%20area%20that%20different%20visual%20task%0Amodels%20neglect.%20CTA%20generates%20cross-task%20perturbations%20by%20shifting%20the%0Aattention%20area%20of%20samples%20away%20from%20the%20co-attention%20map%20and%20closer%20to%20the%0Aanti-attention%20map.%20We%20conduct%20extensive%20experiments%20on%20multiple%20vision%20tasks%0Aand%20the%20experimental%20results%20confirm%20the%20effectiveness%20of%20the%20proposed%20design%0Afor%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Task%2520Attack%253A%2520A%2520Self-Supervision%2520Generative%2520Framework%2520Based%2520on%250A%2520%2520Attention%2520Shift%26entry.906535625%3DQingyuan%2520Zeng%2520and%2520Yunpeng%2520Gong%2520and%2520Min%2520Jiang%26entry.1292438233%3D%2520%2520Studying%2520adversarial%2520attacks%2520on%2520artificial%2520intelligence%2520%2528AI%2529%2520systems%2520helps%250Adiscover%2520model%2520shortcomings%252C%2520enabling%2520the%2520construction%2520of%2520a%2520more%2520robust%2520system.%250AMost%2520existing%2520adversarial%2520attack%2520methods%2520only%2520concentrate%2520on%2520single-task%250Asingle-model%2520or%2520single-task%2520cross-model%2520scenarios%252C%2520overlooking%2520the%2520multi-task%250Acharacteristic%2520of%2520artificial%2520intelligence%2520systems.%2520As%2520a%2520result%252C%2520most%2520of%2520the%250Aexisting%2520attacks%2520do%2520not%2520pose%2520a%2520practical%2520threat%2520to%2520a%2520comprehensive%2520and%250Acollaborative%2520AI%2520system.%2520However%252C%2520implementing%2520cross-task%2520attacks%2520is%2520highly%250Ademanding%2520and%2520challenging%2520due%2520to%2520the%2520difficulty%2520in%2520obtaining%2520the%2520real%2520labels%2520of%250Adifferent%2520tasks%2520for%2520the%2520same%2520picture%2520and%2520harmonizing%2520the%2520loss%2520functions%2520across%250Adifferent%2520tasks.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520self-supervised%2520Cross-Task%250AAttack%2520framework%2520%2528CTA%2529%252C%2520which%2520utilizes%2520co-attention%2520and%2520anti-attention%2520maps%2520to%250Agenerate%2520cross-task%2520adversarial%2520perturbation.%2520Specifically%252C%2520the%2520co-attention%250Amap%2520reflects%2520the%2520area%2520to%2520which%2520different%2520visual%2520task%2520models%2520pay%2520attention%252C%250Awhile%2520the%2520anti-attention%2520map%2520reflects%2520the%2520area%2520that%2520different%2520visual%2520task%250Amodels%2520neglect.%2520CTA%2520generates%2520cross-task%2520perturbations%2520by%2520shifting%2520the%250Aattention%2520area%2520of%2520samples%2520away%2520from%2520the%2520co-attention%2520map%2520and%2520closer%2520to%2520the%250Aanti-attention%2520map.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520multiple%2520vision%2520tasks%250Aand%2520the%2520experimental%2520results%2520confirm%2520the%2520effectiveness%2520of%2520the%2520proposed%2520design%250Afor%2520adversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Task%20Attack%3A%20A%20Self-Supervision%20Generative%20Framework%20Based%20on%0A%20%20Attention%20Shift&entry.906535625=Qingyuan%20Zeng%20and%20Yunpeng%20Gong%20and%20Min%20Jiang&entry.1292438233=%20%20Studying%20adversarial%20attacks%20on%20artificial%20intelligence%20%28AI%29%20systems%20helps%0Adiscover%20model%20shortcomings%2C%20enabling%20the%20construction%20of%20a%20more%20robust%20system.%0AMost%20existing%20adversarial%20attack%20methods%20only%20concentrate%20on%20single-task%0Asingle-model%20or%20single-task%20cross-model%20scenarios%2C%20overlooking%20the%20multi-task%0Acharacteristic%20of%20artificial%20intelligence%20systems.%20As%20a%20result%2C%20most%20of%20the%0Aexisting%20attacks%20do%20not%20pose%20a%20practical%20threat%20to%20a%20comprehensive%20and%0Acollaborative%20AI%20system.%20However%2C%20implementing%20cross-task%20attacks%20is%20highly%0Ademanding%20and%20challenging%20due%20to%20the%20difficulty%20in%20obtaining%20the%20real%20labels%20of%0Adifferent%20tasks%20for%20the%20same%20picture%20and%20harmonizing%20the%20loss%20functions%20across%0Adifferent%20tasks.%20To%20address%20this%20issue%2C%20we%20propose%20a%20self-supervised%20Cross-Task%0AAttack%20framework%20%28CTA%29%2C%20which%20utilizes%20co-attention%20and%20anti-attention%20maps%20to%0Agenerate%20cross-task%20adversarial%20perturbation.%20Specifically%2C%20the%20co-attention%0Amap%20reflects%20the%20area%20to%20which%20different%20visual%20task%20models%20pay%20attention%2C%0Awhile%20the%20anti-attention%20map%20reflects%20the%20area%20that%20different%20visual%20task%0Amodels%20neglect.%20CTA%20generates%20cross-task%20perturbations%20by%20shifting%20the%0Aattention%20area%20of%20samples%20away%20from%20the%20co-attention%20map%20and%20closer%20to%20the%0Aanti-attention%20map.%20We%20conduct%20extensive%20experiments%20on%20multiple%20vision%20tasks%0Aand%20the%20experimental%20results%20confirm%20the%20effectiveness%20of%20the%20proposed%20design%0Afor%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13700v1&entry.124074799=Read"},
{"title": "Learning from the Web: Language Drives Weakly-Supervised Incremental\n  Learning for Semantic Segmentation", "author": "Chang Liu and Giulia Rizzoli and Pietro Zanuttigh and Fu Li and Yi Niu", "abstract": "  Current weakly-supervised incremental learning for semantic segmentation\n(WILSS) approaches only consider replacing pixel-level annotations with\nimage-level labels, while the training images are still from well-designed\ndatasets. In this work, we argue that widely available web images can also be\nconsidered for the learning of new classes. To achieve this, firstly we\nintroduce a strategy to select web images which are similar to previously seen\nexamples in the latent space using a Fourier-based domain discriminator. Then,\nan effective caption-driven reharsal strategy is proposed to preserve\npreviously learnt classes. To our knowledge, this is the first work to rely\nsolely on web images for both the learning of new concepts and the preservation\nof the already learned ones in WILSS. Experimental results show that the\nproposed approach can reach state-of-the-art performances without using\nmanually selected and annotated data in the incremental steps.\n", "link": "http://arxiv.org/abs/2407.13363v1", "date": "2024-07-18", "relevancy": 2.5994, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5442}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5085}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20the%20Web%3A%20Language%20Drives%20Weakly-Supervised%20Incremental%0A%20%20Learning%20for%20Semantic%20Segmentation&body=Title%3A%20Learning%20from%20the%20Web%3A%20Language%20Drives%20Weakly-Supervised%20Incremental%0A%20%20Learning%20for%20Semantic%20Segmentation%0AAuthor%3A%20Chang%20Liu%20and%20Giulia%20Rizzoli%20and%20Pietro%20Zanuttigh%20and%20Fu%20Li%20and%20Yi%20Niu%0AAbstract%3A%20%20%20Current%20weakly-supervised%20incremental%20learning%20for%20semantic%20segmentation%0A%28WILSS%29%20approaches%20only%20consider%20replacing%20pixel-level%20annotations%20with%0Aimage-level%20labels%2C%20while%20the%20training%20images%20are%20still%20from%20well-designed%0Adatasets.%20In%20this%20work%2C%20we%20argue%20that%20widely%20available%20web%20images%20can%20also%20be%0Aconsidered%20for%20the%20learning%20of%20new%20classes.%20To%20achieve%20this%2C%20firstly%20we%0Aintroduce%20a%20strategy%20to%20select%20web%20images%20which%20are%20similar%20to%20previously%20seen%0Aexamples%20in%20the%20latent%20space%20using%20a%20Fourier-based%20domain%20discriminator.%20Then%2C%0Aan%20effective%20caption-driven%20reharsal%20strategy%20is%20proposed%20to%20preserve%0Apreviously%20learnt%20classes.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20rely%0Asolely%20on%20web%20images%20for%20both%20the%20learning%20of%20new%20concepts%20and%20the%20preservation%0Aof%20the%20already%20learned%20ones%20in%20WILSS.%20Experimental%20results%20show%20that%20the%0Aproposed%20approach%20can%20reach%20state-of-the-art%20performances%20without%20using%0Amanually%20selected%20and%20annotated%20data%20in%20the%20incremental%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520the%2520Web%253A%2520Language%2520Drives%2520Weakly-Supervised%2520Incremental%250A%2520%2520Learning%2520for%2520Semantic%2520Segmentation%26entry.906535625%3DChang%2520Liu%2520and%2520Giulia%2520Rizzoli%2520and%2520Pietro%2520Zanuttigh%2520and%2520Fu%2520Li%2520and%2520Yi%2520Niu%26entry.1292438233%3D%2520%2520Current%2520weakly-supervised%2520incremental%2520learning%2520for%2520semantic%2520segmentation%250A%2528WILSS%2529%2520approaches%2520only%2520consider%2520replacing%2520pixel-level%2520annotations%2520with%250Aimage-level%2520labels%252C%2520while%2520the%2520training%2520images%2520are%2520still%2520from%2520well-designed%250Adatasets.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520widely%2520available%2520web%2520images%2520can%2520also%2520be%250Aconsidered%2520for%2520the%2520learning%2520of%2520new%2520classes.%2520To%2520achieve%2520this%252C%2520firstly%2520we%250Aintroduce%2520a%2520strategy%2520to%2520select%2520web%2520images%2520which%2520are%2520similar%2520to%2520previously%2520seen%250Aexamples%2520in%2520the%2520latent%2520space%2520using%2520a%2520Fourier-based%2520domain%2520discriminator.%2520Then%252C%250Aan%2520effective%2520caption-driven%2520reharsal%2520strategy%2520is%2520proposed%2520to%2520preserve%250Apreviously%2520learnt%2520classes.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520rely%250Asolely%2520on%2520web%2520images%2520for%2520both%2520the%2520learning%2520of%2520new%2520concepts%2520and%2520the%2520preservation%250Aof%2520the%2520already%2520learned%2520ones%2520in%2520WILSS.%2520Experimental%2520results%2520show%2520that%2520the%250Aproposed%2520approach%2520can%2520reach%2520state-of-the-art%2520performances%2520without%2520using%250Amanually%2520selected%2520and%2520annotated%2520data%2520in%2520the%2520incremental%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20the%20Web%3A%20Language%20Drives%20Weakly-Supervised%20Incremental%0A%20%20Learning%20for%20Semantic%20Segmentation&entry.906535625=Chang%20Liu%20and%20Giulia%20Rizzoli%20and%20Pietro%20Zanuttigh%20and%20Fu%20Li%20and%20Yi%20Niu&entry.1292438233=%20%20Current%20weakly-supervised%20incremental%20learning%20for%20semantic%20segmentation%0A%28WILSS%29%20approaches%20only%20consider%20replacing%20pixel-level%20annotations%20with%0Aimage-level%20labels%2C%20while%20the%20training%20images%20are%20still%20from%20well-designed%0Adatasets.%20In%20this%20work%2C%20we%20argue%20that%20widely%20available%20web%20images%20can%20also%20be%0Aconsidered%20for%20the%20learning%20of%20new%20classes.%20To%20achieve%20this%2C%20firstly%20we%0Aintroduce%20a%20strategy%20to%20select%20web%20images%20which%20are%20similar%20to%20previously%20seen%0Aexamples%20in%20the%20latent%20space%20using%20a%20Fourier-based%20domain%20discriminator.%20Then%2C%0Aan%20effective%20caption-driven%20reharsal%20strategy%20is%20proposed%20to%20preserve%0Apreviously%20learnt%20classes.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20rely%0Asolely%20on%20web%20images%20for%20both%20the%20learning%20of%20new%20concepts%20and%20the%20preservation%0Aof%20the%20already%20learned%20ones%20in%20WILSS.%20Experimental%20results%20show%20that%20the%0Aproposed%20approach%20can%20reach%20state-of-the-art%20performances%20without%20using%0Amanually%20selected%20and%20annotated%20data%20in%20the%20incremental%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13363v1&entry.124074799=Read"},
{"title": "Attention Based Simple Primitives for Open World Compositional Zero-Shot\n  Learning", "author": "Ans Munir and Faisal Z. Qureshi and Muhammad Haris Khan and Mohsen Ali", "abstract": "  Compositional Zero-Shot Learning (CZSL) aims to predict unknown compositions\nmade up of attribute and object pairs. Predicting compositions unseen during\ntraining is a challenging task. We are exploring Open World Compositional\nZero-Shot Learning (OW-CZSL) in this study, where our test space encompasses\nall potential combinations of attributes and objects. Our approach involves\nutilizing the self-attention mechanism between attributes and objects to\nachieve better generalization from seen to unseen compositions. Utilizing a\nself-attention mechanism facilitates the model's ability to identify\nrelationships between attribute and objects. The similarity between the\nself-attended textual and visual features is subsequently calculated to\ngenerate predictions during the inference phase. The potential test space may\nencompass implausible object-attribute combinations arising from unrestricted\nattribute-object pairings. To mitigate this issue, we leverage external\nknowledge from ConceptNet to restrict the test space to realistic compositions.\nOur proposed model, Attention-based Simple Primitives (ASP), demonstrates\ncompetitive performance, achieving results comparable to the state-of-the-art.\n", "link": "http://arxiv.org/abs/2407.13715v1", "date": "2024-07-18", "relevancy": 2.5397, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5467}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4896}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Based%20Simple%20Primitives%20for%20Open%20World%20Compositional%20Zero-Shot%0A%20%20Learning&body=Title%3A%20Attention%20Based%20Simple%20Primitives%20for%20Open%20World%20Compositional%20Zero-Shot%0A%20%20Learning%0AAuthor%3A%20Ans%20Munir%20and%20Faisal%20Z.%20Qureshi%20and%20Muhammad%20Haris%20Khan%20and%20Mohsen%20Ali%0AAbstract%3A%20%20%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20aims%20to%20predict%20unknown%20compositions%0Amade%20up%20of%20attribute%20and%20object%20pairs.%20Predicting%20compositions%20unseen%20during%0Atraining%20is%20a%20challenging%20task.%20We%20are%20exploring%20Open%20World%20Compositional%0AZero-Shot%20Learning%20%28OW-CZSL%29%20in%20this%20study%2C%20where%20our%20test%20space%20encompasses%0Aall%20potential%20combinations%20of%20attributes%20and%20objects.%20Our%20approach%20involves%0Autilizing%20the%20self-attention%20mechanism%20between%20attributes%20and%20objects%20to%0Aachieve%20better%20generalization%20from%20seen%20to%20unseen%20compositions.%20Utilizing%20a%0Aself-attention%20mechanism%20facilitates%20the%20model%27s%20ability%20to%20identify%0Arelationships%20between%20attribute%20and%20objects.%20The%20similarity%20between%20the%0Aself-attended%20textual%20and%20visual%20features%20is%20subsequently%20calculated%20to%0Agenerate%20predictions%20during%20the%20inference%20phase.%20The%20potential%20test%20space%20may%0Aencompass%20implausible%20object-attribute%20combinations%20arising%20from%20unrestricted%0Aattribute-object%20pairings.%20To%20mitigate%20this%20issue%2C%20we%20leverage%20external%0Aknowledge%20from%20ConceptNet%20to%20restrict%20the%20test%20space%20to%20realistic%20compositions.%0AOur%20proposed%20model%2C%20Attention-based%20Simple%20Primitives%20%28ASP%29%2C%20demonstrates%0Acompetitive%20performance%2C%20achieving%20results%20comparable%20to%20the%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Based%2520Simple%2520Primitives%2520for%2520Open%2520World%2520Compositional%2520Zero-Shot%250A%2520%2520Learning%26entry.906535625%3DAns%2520Munir%2520and%2520Faisal%2520Z.%2520Qureshi%2520and%2520Muhammad%2520Haris%2520Khan%2520and%2520Mohsen%2520Ali%26entry.1292438233%3D%2520%2520Compositional%2520Zero-Shot%2520Learning%2520%2528CZSL%2529%2520aims%2520to%2520predict%2520unknown%2520compositions%250Amade%2520up%2520of%2520attribute%2520and%2520object%2520pairs.%2520Predicting%2520compositions%2520unseen%2520during%250Atraining%2520is%2520a%2520challenging%2520task.%2520We%2520are%2520exploring%2520Open%2520World%2520Compositional%250AZero-Shot%2520Learning%2520%2528OW-CZSL%2529%2520in%2520this%2520study%252C%2520where%2520our%2520test%2520space%2520encompasses%250Aall%2520potential%2520combinations%2520of%2520attributes%2520and%2520objects.%2520Our%2520approach%2520involves%250Autilizing%2520the%2520self-attention%2520mechanism%2520between%2520attributes%2520and%2520objects%2520to%250Aachieve%2520better%2520generalization%2520from%2520seen%2520to%2520unseen%2520compositions.%2520Utilizing%2520a%250Aself-attention%2520mechanism%2520facilitates%2520the%2520model%2527s%2520ability%2520to%2520identify%250Arelationships%2520between%2520attribute%2520and%2520objects.%2520The%2520similarity%2520between%2520the%250Aself-attended%2520textual%2520and%2520visual%2520features%2520is%2520subsequently%2520calculated%2520to%250Agenerate%2520predictions%2520during%2520the%2520inference%2520phase.%2520The%2520potential%2520test%2520space%2520may%250Aencompass%2520implausible%2520object-attribute%2520combinations%2520arising%2520from%2520unrestricted%250Aattribute-object%2520pairings.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520leverage%2520external%250Aknowledge%2520from%2520ConceptNet%2520to%2520restrict%2520the%2520test%2520space%2520to%2520realistic%2520compositions.%250AOur%2520proposed%2520model%252C%2520Attention-based%2520Simple%2520Primitives%2520%2528ASP%2529%252C%2520demonstrates%250Acompetitive%2520performance%252C%2520achieving%2520results%2520comparable%2520to%2520the%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Based%20Simple%20Primitives%20for%20Open%20World%20Compositional%20Zero-Shot%0A%20%20Learning&entry.906535625=Ans%20Munir%20and%20Faisal%20Z.%20Qureshi%20and%20Muhammad%20Haris%20Khan%20and%20Mohsen%20Ali&entry.1292438233=%20%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20aims%20to%20predict%20unknown%20compositions%0Amade%20up%20of%20attribute%20and%20object%20pairs.%20Predicting%20compositions%20unseen%20during%0Atraining%20is%20a%20challenging%20task.%20We%20are%20exploring%20Open%20World%20Compositional%0AZero-Shot%20Learning%20%28OW-CZSL%29%20in%20this%20study%2C%20where%20our%20test%20space%20encompasses%0Aall%20potential%20combinations%20of%20attributes%20and%20objects.%20Our%20approach%20involves%0Autilizing%20the%20self-attention%20mechanism%20between%20attributes%20and%20objects%20to%0Aachieve%20better%20generalization%20from%20seen%20to%20unseen%20compositions.%20Utilizing%20a%0Aself-attention%20mechanism%20facilitates%20the%20model%27s%20ability%20to%20identify%0Arelationships%20between%20attribute%20and%20objects.%20The%20similarity%20between%20the%0Aself-attended%20textual%20and%20visual%20features%20is%20subsequently%20calculated%20to%0Agenerate%20predictions%20during%20the%20inference%20phase.%20The%20potential%20test%20space%20may%0Aencompass%20implausible%20object-attribute%20combinations%20arising%20from%20unrestricted%0Aattribute-object%20pairings.%20To%20mitigate%20this%20issue%2C%20we%20leverage%20external%0Aknowledge%20from%20ConceptNet%20to%20restrict%20the%20test%20space%20to%20realistic%20compositions.%0AOur%20proposed%20model%2C%20Attention-based%20Simple%20Primitives%20%28ASP%29%2C%20demonstrates%0Acompetitive%20performance%2C%20achieving%20results%20comparable%20to%20the%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13715v1&entry.124074799=Read"},
{"title": "Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion\n  Models", "author": "Xiaoyu Zhu and Hao Zhou and Pengfei Xing and Long Zhao and Hao Xu and Junwei Liang and Alexander Hauptmann and Ting Liu and Andrew Gallagher", "abstract": "  In this paper, we investigate the use of diffusion models which are\npre-trained on large-scale image-caption pairs for open-vocabulary 3D semantic\nunderstanding. We propose a novel method, namely Diff2Scene, which leverages\nfrozen representations from text-image generative models, along with\nsalient-aware and geometric-aware masks, for open-vocabulary 3D semantic\nsegmentation and visual grounding tasks. Diff2Scene gets rid of any labeled 3D\ndata and effectively identifies objects, appearances, materials, locations and\ntheir compositions in 3D scenes. We show that it outperforms competitive\nbaselines and achieves significant improvements over state-of-the-art methods.\nIn particular, Diff2Scene improves the state-of-the-art method on ScanNet200 by\n12%.\n", "link": "http://arxiv.org/abs/2407.13642v1", "date": "2024-07-18", "relevancy": 2.5378, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6375}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6338}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Vocabulary%203D%20Semantic%20Segmentation%20with%20Text-to-Image%20Diffusion%0A%20%20Models&body=Title%3A%20Open-Vocabulary%203D%20Semantic%20Segmentation%20with%20Text-to-Image%20Diffusion%0A%20%20Models%0AAuthor%3A%20Xiaoyu%20Zhu%20and%20Hao%20Zhou%20and%20Pengfei%20Xing%20and%20Long%20Zhao%20and%20Hao%20Xu%20and%20Junwei%20Liang%20and%20Alexander%20Hauptmann%20and%20Ting%20Liu%20and%20Andrew%20Gallagher%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20use%20of%20diffusion%20models%20which%20are%0Apre-trained%20on%20large-scale%20image-caption%20pairs%20for%20open-vocabulary%203D%20semantic%0Aunderstanding.%20We%20propose%20a%20novel%20method%2C%20namely%20Diff2Scene%2C%20which%20leverages%0Afrozen%20representations%20from%20text-image%20generative%20models%2C%20along%20with%0Asalient-aware%20and%20geometric-aware%20masks%2C%20for%20open-vocabulary%203D%20semantic%0Asegmentation%20and%20visual%20grounding%20tasks.%20Diff2Scene%20gets%20rid%20of%20any%20labeled%203D%0Adata%20and%20effectively%20identifies%20objects%2C%20appearances%2C%20materials%2C%20locations%20and%0Atheir%20compositions%20in%203D%20scenes.%20We%20show%20that%20it%20outperforms%20competitive%0Abaselines%20and%20achieves%20significant%20improvements%20over%20state-of-the-art%20methods.%0AIn%20particular%2C%20Diff2Scene%20improves%20the%20state-of-the-art%20method%20on%20ScanNet200%20by%0A12%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Vocabulary%25203D%2520Semantic%2520Segmentation%2520with%2520Text-to-Image%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DXiaoyu%2520Zhu%2520and%2520Hao%2520Zhou%2520and%2520Pengfei%2520Xing%2520and%2520Long%2520Zhao%2520and%2520Hao%2520Xu%2520and%2520Junwei%2520Liang%2520and%2520Alexander%2520Hauptmann%2520and%2520Ting%2520Liu%2520and%2520Andrew%2520Gallagher%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520use%2520of%2520diffusion%2520models%2520which%2520are%250Apre-trained%2520on%2520large-scale%2520image-caption%2520pairs%2520for%2520open-vocabulary%25203D%2520semantic%250Aunderstanding.%2520We%2520propose%2520a%2520novel%2520method%252C%2520namely%2520Diff2Scene%252C%2520which%2520leverages%250Afrozen%2520representations%2520from%2520text-image%2520generative%2520models%252C%2520along%2520with%250Asalient-aware%2520and%2520geometric-aware%2520masks%252C%2520for%2520open-vocabulary%25203D%2520semantic%250Asegmentation%2520and%2520visual%2520grounding%2520tasks.%2520Diff2Scene%2520gets%2520rid%2520of%2520any%2520labeled%25203D%250Adata%2520and%2520effectively%2520identifies%2520objects%252C%2520appearances%252C%2520materials%252C%2520locations%2520and%250Atheir%2520compositions%2520in%25203D%2520scenes.%2520We%2520show%2520that%2520it%2520outperforms%2520competitive%250Abaselines%2520and%2520achieves%2520significant%2520improvements%2520over%2520state-of-the-art%2520methods.%250AIn%2520particular%252C%2520Diff2Scene%2520improves%2520the%2520state-of-the-art%2520method%2520on%2520ScanNet200%2520by%250A12%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Vocabulary%203D%20Semantic%20Segmentation%20with%20Text-to-Image%20Diffusion%0A%20%20Models&entry.906535625=Xiaoyu%20Zhu%20and%20Hao%20Zhou%20and%20Pengfei%20Xing%20and%20Long%20Zhao%20and%20Hao%20Xu%20and%20Junwei%20Liang%20and%20Alexander%20Hauptmann%20and%20Ting%20Liu%20and%20Andrew%20Gallagher&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20use%20of%20diffusion%20models%20which%20are%0Apre-trained%20on%20large-scale%20image-caption%20pairs%20for%20open-vocabulary%203D%20semantic%0Aunderstanding.%20We%20propose%20a%20novel%20method%2C%20namely%20Diff2Scene%2C%20which%20leverages%0Afrozen%20representations%20from%20text-image%20generative%20models%2C%20along%20with%0Asalient-aware%20and%20geometric-aware%20masks%2C%20for%20open-vocabulary%203D%20semantic%0Asegmentation%20and%20visual%20grounding%20tasks.%20Diff2Scene%20gets%20rid%20of%20any%20labeled%203D%0Adata%20and%20effectively%20identifies%20objects%2C%20appearances%2C%20materials%2C%20locations%20and%0Atheir%20compositions%20in%203D%20scenes.%20We%20show%20that%20it%20outperforms%20competitive%0Abaselines%20and%20achieves%20significant%20improvements%20over%20state-of-the-art%20methods.%0AIn%20particular%2C%20Diff2Scene%20improves%20the%20state-of-the-art%20method%20on%20ScanNet200%20by%0A12%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13642v1&entry.124074799=Read"},
{"title": "HPix: Generating Vector Maps from Satellite Images", "author": "Aditya Taparia and Keshab Nath", "abstract": "  Vector maps find widespread utility across diverse domains due to their\ncapacity to not only store but also represent discrete data boundaries such as\nbuilding footprints, disaster impact analysis, digitization, urban planning,\nlocation points, transport links, and more. Although extensive research exists\non identifying building footprints and road types from satellite imagery, the\ngeneration of vector maps from such imagery remains an area with limited\nexploration. Furthermore, conventional map generation techniques rely on\nlabor-intensive manual feature extraction or rule-based approaches, which\nimpose inherent limitations. To surmount these limitations, we propose a novel\nmethod called HPix, which utilizes modified Generative Adversarial Networks\n(GANs) to generate vector tile map from satellite images. HPix incorporates two\nhierarchical frameworks: one operating at the global level and the other at the\nlocal level, resulting in a comprehensive model. Through empirical evaluations,\nour proposed approach showcases its effectiveness in producing highly accurate\nand visually captivating vector tile maps derived from satellite images. We\nfurther extend our study's application to include mapping of road intersections\nand building footprints cluster based on their area.\n", "link": "http://arxiv.org/abs/2407.13680v1", "date": "2024-07-18", "relevancy": 2.5067, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5227}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4943}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HPix%3A%20Generating%20Vector%20Maps%20from%20Satellite%20Images&body=Title%3A%20HPix%3A%20Generating%20Vector%20Maps%20from%20Satellite%20Images%0AAuthor%3A%20Aditya%20Taparia%20and%20Keshab%20Nath%0AAbstract%3A%20%20%20Vector%20maps%20find%20widespread%20utility%20across%20diverse%20domains%20due%20to%20their%0Acapacity%20to%20not%20only%20store%20but%20also%20represent%20discrete%20data%20boundaries%20such%20as%0Abuilding%20footprints%2C%20disaster%20impact%20analysis%2C%20digitization%2C%20urban%20planning%2C%0Alocation%20points%2C%20transport%20links%2C%20and%20more.%20Although%20extensive%20research%20exists%0Aon%20identifying%20building%20footprints%20and%20road%20types%20from%20satellite%20imagery%2C%20the%0Ageneration%20of%20vector%20maps%20from%20such%20imagery%20remains%20an%20area%20with%20limited%0Aexploration.%20Furthermore%2C%20conventional%20map%20generation%20techniques%20rely%20on%0Alabor-intensive%20manual%20feature%20extraction%20or%20rule-based%20approaches%2C%20which%0Aimpose%20inherent%20limitations.%20To%20surmount%20these%20limitations%2C%20we%20propose%20a%20novel%0Amethod%20called%20HPix%2C%20which%20utilizes%20modified%20Generative%20Adversarial%20Networks%0A%28GANs%29%20to%20generate%20vector%20tile%20map%20from%20satellite%20images.%20HPix%20incorporates%20two%0Ahierarchical%20frameworks%3A%20one%20operating%20at%20the%20global%20level%20and%20the%20other%20at%20the%0Alocal%20level%2C%20resulting%20in%20a%20comprehensive%20model.%20Through%20empirical%20evaluations%2C%0Aour%20proposed%20approach%20showcases%20its%20effectiveness%20in%20producing%20highly%20accurate%0Aand%20visually%20captivating%20vector%20tile%20maps%20derived%20from%20satellite%20images.%20We%0Afurther%20extend%20our%20study%27s%20application%20to%20include%20mapping%20of%20road%20intersections%0Aand%20building%20footprints%20cluster%20based%20on%20their%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHPix%253A%2520Generating%2520Vector%2520Maps%2520from%2520Satellite%2520Images%26entry.906535625%3DAditya%2520Taparia%2520and%2520Keshab%2520Nath%26entry.1292438233%3D%2520%2520Vector%2520maps%2520find%2520widespread%2520utility%2520across%2520diverse%2520domains%2520due%2520to%2520their%250Acapacity%2520to%2520not%2520only%2520store%2520but%2520also%2520represent%2520discrete%2520data%2520boundaries%2520such%2520as%250Abuilding%2520footprints%252C%2520disaster%2520impact%2520analysis%252C%2520digitization%252C%2520urban%2520planning%252C%250Alocation%2520points%252C%2520transport%2520links%252C%2520and%2520more.%2520Although%2520extensive%2520research%2520exists%250Aon%2520identifying%2520building%2520footprints%2520and%2520road%2520types%2520from%2520satellite%2520imagery%252C%2520the%250Ageneration%2520of%2520vector%2520maps%2520from%2520such%2520imagery%2520remains%2520an%2520area%2520with%2520limited%250Aexploration.%2520Furthermore%252C%2520conventional%2520map%2520generation%2520techniques%2520rely%2520on%250Alabor-intensive%2520manual%2520feature%2520extraction%2520or%2520rule-based%2520approaches%252C%2520which%250Aimpose%2520inherent%2520limitations.%2520To%2520surmount%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250Amethod%2520called%2520HPix%252C%2520which%2520utilizes%2520modified%2520Generative%2520Adversarial%2520Networks%250A%2528GANs%2529%2520to%2520generate%2520vector%2520tile%2520map%2520from%2520satellite%2520images.%2520HPix%2520incorporates%2520two%250Ahierarchical%2520frameworks%253A%2520one%2520operating%2520at%2520the%2520global%2520level%2520and%2520the%2520other%2520at%2520the%250Alocal%2520level%252C%2520resulting%2520in%2520a%2520comprehensive%2520model.%2520Through%2520empirical%2520evaluations%252C%250Aour%2520proposed%2520approach%2520showcases%2520its%2520effectiveness%2520in%2520producing%2520highly%2520accurate%250Aand%2520visually%2520captivating%2520vector%2520tile%2520maps%2520derived%2520from%2520satellite%2520images.%2520We%250Afurther%2520extend%2520our%2520study%2527s%2520application%2520to%2520include%2520mapping%2520of%2520road%2520intersections%250Aand%2520building%2520footprints%2520cluster%2520based%2520on%2520their%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HPix%3A%20Generating%20Vector%20Maps%20from%20Satellite%20Images&entry.906535625=Aditya%20Taparia%20and%20Keshab%20Nath&entry.1292438233=%20%20Vector%20maps%20find%20widespread%20utility%20across%20diverse%20domains%20due%20to%20their%0Acapacity%20to%20not%20only%20store%20but%20also%20represent%20discrete%20data%20boundaries%20such%20as%0Abuilding%20footprints%2C%20disaster%20impact%20analysis%2C%20digitization%2C%20urban%20planning%2C%0Alocation%20points%2C%20transport%20links%2C%20and%20more.%20Although%20extensive%20research%20exists%0Aon%20identifying%20building%20footprints%20and%20road%20types%20from%20satellite%20imagery%2C%20the%0Ageneration%20of%20vector%20maps%20from%20such%20imagery%20remains%20an%20area%20with%20limited%0Aexploration.%20Furthermore%2C%20conventional%20map%20generation%20techniques%20rely%20on%0Alabor-intensive%20manual%20feature%20extraction%20or%20rule-based%20approaches%2C%20which%0Aimpose%20inherent%20limitations.%20To%20surmount%20these%20limitations%2C%20we%20propose%20a%20novel%0Amethod%20called%20HPix%2C%20which%20utilizes%20modified%20Generative%20Adversarial%20Networks%0A%28GANs%29%20to%20generate%20vector%20tile%20map%20from%20satellite%20images.%20HPix%20incorporates%20two%0Ahierarchical%20frameworks%3A%20one%20operating%20at%20the%20global%20level%20and%20the%20other%20at%20the%0Alocal%20level%2C%20resulting%20in%20a%20comprehensive%20model.%20Through%20empirical%20evaluations%2C%0Aour%20proposed%20approach%20showcases%20its%20effectiveness%20in%20producing%20highly%20accurate%0Aand%20visually%20captivating%20vector%20tile%20maps%20derived%20from%20satellite%20images.%20We%0Afurther%20extend%20our%20study%27s%20application%20to%20include%20mapping%20of%20road%20intersections%0Aand%20building%20footprints%20cluster%20based%20on%20their%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13680v1&entry.124074799=Read"},
{"title": "Pyramid Diffusion for Fine 3D Large Scene Generation", "author": "Yuheng Liu and Xinke Li and Xueting Li and Lu Qi and Chongshou Li and Ming-Hsuan Yang", "abstract": "  Diffusion models have shown remarkable results in generating 2D images and\nsmall-scale 3D objects. However, their application to the synthesis of\nlarge-scale 3D scenes has been rarely explored. This is mainly due to the\ninherent complexity and bulky size of 3D scenery data, particularly outdoor\nscenes, and the limited availability of comprehensive real-world datasets,\nwhich makes training a stable scene diffusion model challenging. In this work,\nwe explore how to effectively generate large-scale 3D scenes using the\ncoarse-to-fine paradigm. We introduce a framework, the Pyramid Discrete\nDiffusion model (PDD), which employs scale-varied diffusion models to\nprogressively generate high-quality outdoor scenes. Experimental results of PDD\ndemonstrate our successful exploration in generating 3D scenes both\nunconditionally and conditionally. We further showcase the data compatibility\nof the PDD model, due to its multi-scale architecture: a PDD model trained on\none dataset can be easily fine-tuned with another dataset. Code is available at\nhttps://github.com/yuhengliu02/pyramid-discrete-diffusion.\n", "link": "http://arxiv.org/abs/2311.12085v2", "date": "2024-07-18", "relevancy": 2.503, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6356}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6356}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pyramid%20Diffusion%20for%20Fine%203D%20Large%20Scene%20Generation&body=Title%3A%20Pyramid%20Diffusion%20for%20Fine%203D%20Large%20Scene%20Generation%0AAuthor%3A%20Yuheng%20Liu%20and%20Xinke%20Li%20and%20Xueting%20Li%20and%20Lu%20Qi%20and%20Chongshou%20Li%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20shown%20remarkable%20results%20in%20generating%202D%20images%20and%0Asmall-scale%203D%20objects.%20However%2C%20their%20application%20to%20the%20synthesis%20of%0Alarge-scale%203D%20scenes%20has%20been%20rarely%20explored.%20This%20is%20mainly%20due%20to%20the%0Ainherent%20complexity%20and%20bulky%20size%20of%203D%20scenery%20data%2C%20particularly%20outdoor%0Ascenes%2C%20and%20the%20limited%20availability%20of%20comprehensive%20real-world%20datasets%2C%0Awhich%20makes%20training%20a%20stable%20scene%20diffusion%20model%20challenging.%20In%20this%20work%2C%0Awe%20explore%20how%20to%20effectively%20generate%20large-scale%203D%20scenes%20using%20the%0Acoarse-to-fine%20paradigm.%20We%20introduce%20a%20framework%2C%20the%20Pyramid%20Discrete%0ADiffusion%20model%20%28PDD%29%2C%20which%20employs%20scale-varied%20diffusion%20models%20to%0Aprogressively%20generate%20high-quality%20outdoor%20scenes.%20Experimental%20results%20of%20PDD%0Ademonstrate%20our%20successful%20exploration%20in%20generating%203D%20scenes%20both%0Aunconditionally%20and%20conditionally.%20We%20further%20showcase%20the%20data%20compatibility%0Aof%20the%20PDD%20model%2C%20due%20to%20its%20multi-scale%20architecture%3A%20a%20PDD%20model%20trained%20on%0Aone%20dataset%20can%20be%20easily%20fine-tuned%20with%20another%20dataset.%20Code%20is%20available%20at%0Ahttps%3A//github.com/yuhengliu02/pyramid-discrete-diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12085v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPyramid%2520Diffusion%2520for%2520Fine%25203D%2520Large%2520Scene%2520Generation%26entry.906535625%3DYuheng%2520Liu%2520and%2520Xinke%2520Li%2520and%2520Xueting%2520Li%2520and%2520Lu%2520Qi%2520and%2520Chongshou%2520Li%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520shown%2520remarkable%2520results%2520in%2520generating%25202D%2520images%2520and%250Asmall-scale%25203D%2520objects.%2520However%252C%2520their%2520application%2520to%2520the%2520synthesis%2520of%250Alarge-scale%25203D%2520scenes%2520has%2520been%2520rarely%2520explored.%2520This%2520is%2520mainly%2520due%2520to%2520the%250Ainherent%2520complexity%2520and%2520bulky%2520size%2520of%25203D%2520scenery%2520data%252C%2520particularly%2520outdoor%250Ascenes%252C%2520and%2520the%2520limited%2520availability%2520of%2520comprehensive%2520real-world%2520datasets%252C%250Awhich%2520makes%2520training%2520a%2520stable%2520scene%2520diffusion%2520model%2520challenging.%2520In%2520this%2520work%252C%250Awe%2520explore%2520how%2520to%2520effectively%2520generate%2520large-scale%25203D%2520scenes%2520using%2520the%250Acoarse-to-fine%2520paradigm.%2520We%2520introduce%2520a%2520framework%252C%2520the%2520Pyramid%2520Discrete%250ADiffusion%2520model%2520%2528PDD%2529%252C%2520which%2520employs%2520scale-varied%2520diffusion%2520models%2520to%250Aprogressively%2520generate%2520high-quality%2520outdoor%2520scenes.%2520Experimental%2520results%2520of%2520PDD%250Ademonstrate%2520our%2520successful%2520exploration%2520in%2520generating%25203D%2520scenes%2520both%250Aunconditionally%2520and%2520conditionally.%2520We%2520further%2520showcase%2520the%2520data%2520compatibility%250Aof%2520the%2520PDD%2520model%252C%2520due%2520to%2520its%2520multi-scale%2520architecture%253A%2520a%2520PDD%2520model%2520trained%2520on%250Aone%2520dataset%2520can%2520be%2520easily%2520fine-tuned%2520with%2520another%2520dataset.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/yuhengliu02/pyramid-discrete-diffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12085v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pyramid%20Diffusion%20for%20Fine%203D%20Large%20Scene%20Generation&entry.906535625=Yuheng%20Liu%20and%20Xinke%20Li%20and%20Xueting%20Li%20and%20Lu%20Qi%20and%20Chongshou%20Li%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Diffusion%20models%20have%20shown%20remarkable%20results%20in%20generating%202D%20images%20and%0Asmall-scale%203D%20objects.%20However%2C%20their%20application%20to%20the%20synthesis%20of%0Alarge-scale%203D%20scenes%20has%20been%20rarely%20explored.%20This%20is%20mainly%20due%20to%20the%0Ainherent%20complexity%20and%20bulky%20size%20of%203D%20scenery%20data%2C%20particularly%20outdoor%0Ascenes%2C%20and%20the%20limited%20availability%20of%20comprehensive%20real-world%20datasets%2C%0Awhich%20makes%20training%20a%20stable%20scene%20diffusion%20model%20challenging.%20In%20this%20work%2C%0Awe%20explore%20how%20to%20effectively%20generate%20large-scale%203D%20scenes%20using%20the%0Acoarse-to-fine%20paradigm.%20We%20introduce%20a%20framework%2C%20the%20Pyramid%20Discrete%0ADiffusion%20model%20%28PDD%29%2C%20which%20employs%20scale-varied%20diffusion%20models%20to%0Aprogressively%20generate%20high-quality%20outdoor%20scenes.%20Experimental%20results%20of%20PDD%0Ademonstrate%20our%20successful%20exploration%20in%20generating%203D%20scenes%20both%0Aunconditionally%20and%20conditionally.%20We%20further%20showcase%20the%20data%20compatibility%0Aof%20the%20PDD%20model%2C%20due%20to%20its%20multi-scale%20architecture%3A%20a%20PDD%20model%20trained%20on%0Aone%20dataset%20can%20be%20easily%20fine-tuned%20with%20another%20dataset.%20Code%20is%20available%20at%0Ahttps%3A//github.com/yuhengliu02/pyramid-discrete-diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12085v2&entry.124074799=Read"},
{"title": "DiffuX2CT: Diffusion Learning to Reconstruct CT Images from Biplanar\n  X-Rays", "author": "Xuhui Liu and Zhi Qiao and Runkun Liu and Hong Li and Juan Zhang and Xiantong Zhen and Zhen Qian and Baochang Zhang", "abstract": "  Computed tomography (CT) is widely utilized in clinical settings because it\ndelivers detailed 3D images of the human body. However, performing CT scans is\nnot always feasible due to radiation exposure and limitations in certain\nsurgical environments. As an alternative, reconstructing CT images from\nultra-sparse X-rays offers a valuable solution and has gained significant\ninterest in scientific research and medical applications. However, it presents\ngreat challenges as it is inherently an ill-posed problem, often compromised by\nartifacts resulting from overlapping structures in X-ray images. In this paper,\nwe propose DiffuX2CT, which models CT reconstruction from orthogonal biplanar\nX-rays as a conditional diffusion process. DiffuX2CT is established with a 3D\nglobal coherence denoising model with a new, implicit conditioning mechanism.\nWe realize the conditioning mechanism by a newly designed tri-plane decoupling\ngenerator and an implicit neural decoder. By doing so, DiffuX2CT achieves\nstructure-controllable reconstruction, which enables 3D structural information\nto be recovered from 2D X-rays, therefore producing faithful textures in CT\nimages. As an extra contribution, we collect a real-world lumbar CT dataset,\ncalled LumbarV, as a new benchmark to verify the clinical significance and\nperformance of CT reconstruction from X-rays. Extensive experiments on this\ndataset and three more publicly available datasets demonstrate the\neffectiveness of our proposal.\n", "link": "http://arxiv.org/abs/2407.13545v1", "date": "2024-07-18", "relevancy": 2.4767, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6213}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6213}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffuX2CT%3A%20Diffusion%20Learning%20to%20Reconstruct%20CT%20Images%20from%20Biplanar%0A%20%20X-Rays&body=Title%3A%20DiffuX2CT%3A%20Diffusion%20Learning%20to%20Reconstruct%20CT%20Images%20from%20Biplanar%0A%20%20X-Rays%0AAuthor%3A%20Xuhui%20Liu%20and%20Zhi%20Qiao%20and%20Runkun%20Liu%20and%20Hong%20Li%20and%20Juan%20Zhang%20and%20Xiantong%20Zhen%20and%20Zhen%20Qian%20and%20Baochang%20Zhang%0AAbstract%3A%20%20%20Computed%20tomography%20%28CT%29%20is%20widely%20utilized%20in%20clinical%20settings%20because%20it%0Adelivers%20detailed%203D%20images%20of%20the%20human%20body.%20However%2C%20performing%20CT%20scans%20is%0Anot%20always%20feasible%20due%20to%20radiation%20exposure%20and%20limitations%20in%20certain%0Asurgical%20environments.%20As%20an%20alternative%2C%20reconstructing%20CT%20images%20from%0Aultra-sparse%20X-rays%20offers%20a%20valuable%20solution%20and%20has%20gained%20significant%0Ainterest%20in%20scientific%20research%20and%20medical%20applications.%20However%2C%20it%20presents%0Agreat%20challenges%20as%20it%20is%20inherently%20an%20ill-posed%20problem%2C%20often%20compromised%20by%0Aartifacts%20resulting%20from%20overlapping%20structures%20in%20X-ray%20images.%20In%20this%20paper%2C%0Awe%20propose%20DiffuX2CT%2C%20which%20models%20CT%20reconstruction%20from%20orthogonal%20biplanar%0AX-rays%20as%20a%20conditional%20diffusion%20process.%20DiffuX2CT%20is%20established%20with%20a%203D%0Aglobal%20coherence%20denoising%20model%20with%20a%20new%2C%20implicit%20conditioning%20mechanism.%0AWe%20realize%20the%20conditioning%20mechanism%20by%20a%20newly%20designed%20tri-plane%20decoupling%0Agenerator%20and%20an%20implicit%20neural%20decoder.%20By%20doing%20so%2C%20DiffuX2CT%20achieves%0Astructure-controllable%20reconstruction%2C%20which%20enables%203D%20structural%20information%0Ato%20be%20recovered%20from%202D%20X-rays%2C%20therefore%20producing%20faithful%20textures%20in%20CT%0Aimages.%20As%20an%20extra%20contribution%2C%20we%20collect%20a%20real-world%20lumbar%20CT%20dataset%2C%0Acalled%20LumbarV%2C%20as%20a%20new%20benchmark%20to%20verify%20the%20clinical%20significance%20and%0Aperformance%20of%20CT%20reconstruction%20from%20X-rays.%20Extensive%20experiments%20on%20this%0Adataset%20and%20three%20more%20publicly%20available%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffuX2CT%253A%2520Diffusion%2520Learning%2520to%2520Reconstruct%2520CT%2520Images%2520from%2520Biplanar%250A%2520%2520X-Rays%26entry.906535625%3DXuhui%2520Liu%2520and%2520Zhi%2520Qiao%2520and%2520Runkun%2520Liu%2520and%2520Hong%2520Li%2520and%2520Juan%2520Zhang%2520and%2520Xiantong%2520Zhen%2520and%2520Zhen%2520Qian%2520and%2520Baochang%2520Zhang%26entry.1292438233%3D%2520%2520Computed%2520tomography%2520%2528CT%2529%2520is%2520widely%2520utilized%2520in%2520clinical%2520settings%2520because%2520it%250Adelivers%2520detailed%25203D%2520images%2520of%2520the%2520human%2520body.%2520However%252C%2520performing%2520CT%2520scans%2520is%250Anot%2520always%2520feasible%2520due%2520to%2520radiation%2520exposure%2520and%2520limitations%2520in%2520certain%250Asurgical%2520environments.%2520As%2520an%2520alternative%252C%2520reconstructing%2520CT%2520images%2520from%250Aultra-sparse%2520X-rays%2520offers%2520a%2520valuable%2520solution%2520and%2520has%2520gained%2520significant%250Ainterest%2520in%2520scientific%2520research%2520and%2520medical%2520applications.%2520However%252C%2520it%2520presents%250Agreat%2520challenges%2520as%2520it%2520is%2520inherently%2520an%2520ill-posed%2520problem%252C%2520often%2520compromised%2520by%250Aartifacts%2520resulting%2520from%2520overlapping%2520structures%2520in%2520X-ray%2520images.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520DiffuX2CT%252C%2520which%2520models%2520CT%2520reconstruction%2520from%2520orthogonal%2520biplanar%250AX-rays%2520as%2520a%2520conditional%2520diffusion%2520process.%2520DiffuX2CT%2520is%2520established%2520with%2520a%25203D%250Aglobal%2520coherence%2520denoising%2520model%2520with%2520a%2520new%252C%2520implicit%2520conditioning%2520mechanism.%250AWe%2520realize%2520the%2520conditioning%2520mechanism%2520by%2520a%2520newly%2520designed%2520tri-plane%2520decoupling%250Agenerator%2520and%2520an%2520implicit%2520neural%2520decoder.%2520By%2520doing%2520so%252C%2520DiffuX2CT%2520achieves%250Astructure-controllable%2520reconstruction%252C%2520which%2520enables%25203D%2520structural%2520information%250Ato%2520be%2520recovered%2520from%25202D%2520X-rays%252C%2520therefore%2520producing%2520faithful%2520textures%2520in%2520CT%250Aimages.%2520As%2520an%2520extra%2520contribution%252C%2520we%2520collect%2520a%2520real-world%2520lumbar%2520CT%2520dataset%252C%250Acalled%2520LumbarV%252C%2520as%2520a%2520new%2520benchmark%2520to%2520verify%2520the%2520clinical%2520significance%2520and%250Aperformance%2520of%2520CT%2520reconstruction%2520from%2520X-rays.%2520Extensive%2520experiments%2520on%2520this%250Adataset%2520and%2520three%2520more%2520publicly%2520available%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520proposal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffuX2CT%3A%20Diffusion%20Learning%20to%20Reconstruct%20CT%20Images%20from%20Biplanar%0A%20%20X-Rays&entry.906535625=Xuhui%20Liu%20and%20Zhi%20Qiao%20and%20Runkun%20Liu%20and%20Hong%20Li%20and%20Juan%20Zhang%20and%20Xiantong%20Zhen%20and%20Zhen%20Qian%20and%20Baochang%20Zhang&entry.1292438233=%20%20Computed%20tomography%20%28CT%29%20is%20widely%20utilized%20in%20clinical%20settings%20because%20it%0Adelivers%20detailed%203D%20images%20of%20the%20human%20body.%20However%2C%20performing%20CT%20scans%20is%0Anot%20always%20feasible%20due%20to%20radiation%20exposure%20and%20limitations%20in%20certain%0Asurgical%20environments.%20As%20an%20alternative%2C%20reconstructing%20CT%20images%20from%0Aultra-sparse%20X-rays%20offers%20a%20valuable%20solution%20and%20has%20gained%20significant%0Ainterest%20in%20scientific%20research%20and%20medical%20applications.%20However%2C%20it%20presents%0Agreat%20challenges%20as%20it%20is%20inherently%20an%20ill-posed%20problem%2C%20often%20compromised%20by%0Aartifacts%20resulting%20from%20overlapping%20structures%20in%20X-ray%20images.%20In%20this%20paper%2C%0Awe%20propose%20DiffuX2CT%2C%20which%20models%20CT%20reconstruction%20from%20orthogonal%20biplanar%0AX-rays%20as%20a%20conditional%20diffusion%20process.%20DiffuX2CT%20is%20established%20with%20a%203D%0Aglobal%20coherence%20denoising%20model%20with%20a%20new%2C%20implicit%20conditioning%20mechanism.%0AWe%20realize%20the%20conditioning%20mechanism%20by%20a%20newly%20designed%20tri-plane%20decoupling%0Agenerator%20and%20an%20implicit%20neural%20decoder.%20By%20doing%20so%2C%20DiffuX2CT%20achieves%0Astructure-controllable%20reconstruction%2C%20which%20enables%203D%20structural%20information%0Ato%20be%20recovered%20from%202D%20X-rays%2C%20therefore%20producing%20faithful%20textures%20in%20CT%0Aimages.%20As%20an%20extra%20contribution%2C%20we%20collect%20a%20real-world%20lumbar%20CT%20dataset%2C%0Acalled%20LumbarV%2C%20as%20a%20new%20benchmark%20to%20verify%20the%20clinical%20significance%20and%0Aperformance%20of%20CT%20reconstruction%20from%20X-rays.%20Extensive%20experiments%20on%20this%0Adataset%20and%20three%20more%20publicly%20available%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13545v1&entry.124074799=Read"},
{"title": "General Geometry-aware Weakly Supervised 3D Object Detection", "author": "Guowen Zhang and Junsong Fan and Liyi Chen and Zhaoxiang Zhang and Zhen Lei and Lei Zhang", "abstract": "  3D object detection is an indispensable component for scene understanding.\nHowever, the annotation of large-scale 3D datasets requires significant human\neffort. To tackle this problem, many methods adopt weakly supervised 3D object\ndetection that estimates 3D boxes by leveraging 2D boxes and\nscene/class-specific priors. However, these approaches generally depend on\nsophisticated manual priors, which is hard to generalize to novel categories\nand scenes. In this paper, we are motivated to propose a general approach,\nwhich can be easily adapted to new scenes and/or classes. A unified framework\nis developed for learning 3D object detectors from RGB images and associated 2D\nboxes. In specific, we propose three general components: prior injection module\nto obtain general object geometric priors from LLM model, 2D space projection\nconstraint to minimize the discrepancy between the boundaries of projected 3D\nboxes and their corresponding 2D boxes on the image plane, and 3D space\ngeometry constraint to build a Point-to-Box alignment loss to further refine\nthe pose of estimated 3D boxes. Experiments on KITTI and SUN-RGBD datasets\ndemonstrate that our method yields surprisingly high-quality 3D bounding boxes\nwith only 2D annotation. The source code is available at\nhttps://github.com/gwenzhang/GGA.\n", "link": "http://arxiv.org/abs/2407.13748v1", "date": "2024-07-18", "relevancy": 2.4756, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6379}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6169}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General%20Geometry-aware%20Weakly%20Supervised%203D%20Object%20Detection&body=Title%3A%20General%20Geometry-aware%20Weakly%20Supervised%203D%20Object%20Detection%0AAuthor%3A%20Guowen%20Zhang%20and%20Junsong%20Fan%20and%20Liyi%20Chen%20and%20Zhaoxiang%20Zhang%20and%20Zhen%20Lei%20and%20Lei%20Zhang%0AAbstract%3A%20%20%203D%20object%20detection%20is%20an%20indispensable%20component%20for%20scene%20understanding.%0AHowever%2C%20the%20annotation%20of%20large-scale%203D%20datasets%20requires%20significant%20human%0Aeffort.%20To%20tackle%20this%20problem%2C%20many%20methods%20adopt%20weakly%20supervised%203D%20object%0Adetection%20that%20estimates%203D%20boxes%20by%20leveraging%202D%20boxes%20and%0Ascene/class-specific%20priors.%20However%2C%20these%20approaches%20generally%20depend%20on%0Asophisticated%20manual%20priors%2C%20which%20is%20hard%20to%20generalize%20to%20novel%20categories%0Aand%20scenes.%20In%20this%20paper%2C%20we%20are%20motivated%20to%20propose%20a%20general%20approach%2C%0Awhich%20can%20be%20easily%20adapted%20to%20new%20scenes%20and/or%20classes.%20A%20unified%20framework%0Ais%20developed%20for%20learning%203D%20object%20detectors%20from%20RGB%20images%20and%20associated%202D%0Aboxes.%20In%20specific%2C%20we%20propose%20three%20general%20components%3A%20prior%20injection%20module%0Ato%20obtain%20general%20object%20geometric%20priors%20from%20LLM%20model%2C%202D%20space%20projection%0Aconstraint%20to%20minimize%20the%20discrepancy%20between%20the%20boundaries%20of%20projected%203D%0Aboxes%20and%20their%20corresponding%202D%20boxes%20on%20the%20image%20plane%2C%20and%203D%20space%0Ageometry%20constraint%20to%20build%20a%20Point-to-Box%20alignment%20loss%20to%20further%20refine%0Athe%20pose%20of%20estimated%203D%20boxes.%20Experiments%20on%20KITTI%20and%20SUN-RGBD%20datasets%0Ademonstrate%20that%20our%20method%20yields%20surprisingly%20high-quality%203D%20bounding%20boxes%0Awith%20only%202D%20annotation.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/gwenzhang/GGA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral%2520Geometry-aware%2520Weakly%2520Supervised%25203D%2520Object%2520Detection%26entry.906535625%3DGuowen%2520Zhang%2520and%2520Junsong%2520Fan%2520and%2520Liyi%2520Chen%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Zhen%2520Lei%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%25203D%2520object%2520detection%2520is%2520an%2520indispensable%2520component%2520for%2520scene%2520understanding.%250AHowever%252C%2520the%2520annotation%2520of%2520large-scale%25203D%2520datasets%2520requires%2520significant%2520human%250Aeffort.%2520To%2520tackle%2520this%2520problem%252C%2520many%2520methods%2520adopt%2520weakly%2520supervised%25203D%2520object%250Adetection%2520that%2520estimates%25203D%2520boxes%2520by%2520leveraging%25202D%2520boxes%2520and%250Ascene/class-specific%2520priors.%2520However%252C%2520these%2520approaches%2520generally%2520depend%2520on%250Asophisticated%2520manual%2520priors%252C%2520which%2520is%2520hard%2520to%2520generalize%2520to%2520novel%2520categories%250Aand%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520are%2520motivated%2520to%2520propose%2520a%2520general%2520approach%252C%250Awhich%2520can%2520be%2520easily%2520adapted%2520to%2520new%2520scenes%2520and/or%2520classes.%2520A%2520unified%2520framework%250Ais%2520developed%2520for%2520learning%25203D%2520object%2520detectors%2520from%2520RGB%2520images%2520and%2520associated%25202D%250Aboxes.%2520In%2520specific%252C%2520we%2520propose%2520three%2520general%2520components%253A%2520prior%2520injection%2520module%250Ato%2520obtain%2520general%2520object%2520geometric%2520priors%2520from%2520LLM%2520model%252C%25202D%2520space%2520projection%250Aconstraint%2520to%2520minimize%2520the%2520discrepancy%2520between%2520the%2520boundaries%2520of%2520projected%25203D%250Aboxes%2520and%2520their%2520corresponding%25202D%2520boxes%2520on%2520the%2520image%2520plane%252C%2520and%25203D%2520space%250Ageometry%2520constraint%2520to%2520build%2520a%2520Point-to-Box%2520alignment%2520loss%2520to%2520further%2520refine%250Athe%2520pose%2520of%2520estimated%25203D%2520boxes.%2520Experiments%2520on%2520KITTI%2520and%2520SUN-RGBD%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520yields%2520surprisingly%2520high-quality%25203D%2520bounding%2520boxes%250Awith%2520only%25202D%2520annotation.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/gwenzhang/GGA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General%20Geometry-aware%20Weakly%20Supervised%203D%20Object%20Detection&entry.906535625=Guowen%20Zhang%20and%20Junsong%20Fan%20and%20Liyi%20Chen%20and%20Zhaoxiang%20Zhang%20and%20Zhen%20Lei%20and%20Lei%20Zhang&entry.1292438233=%20%203D%20object%20detection%20is%20an%20indispensable%20component%20for%20scene%20understanding.%0AHowever%2C%20the%20annotation%20of%20large-scale%203D%20datasets%20requires%20significant%20human%0Aeffort.%20To%20tackle%20this%20problem%2C%20many%20methods%20adopt%20weakly%20supervised%203D%20object%0Adetection%20that%20estimates%203D%20boxes%20by%20leveraging%202D%20boxes%20and%0Ascene/class-specific%20priors.%20However%2C%20these%20approaches%20generally%20depend%20on%0Asophisticated%20manual%20priors%2C%20which%20is%20hard%20to%20generalize%20to%20novel%20categories%0Aand%20scenes.%20In%20this%20paper%2C%20we%20are%20motivated%20to%20propose%20a%20general%20approach%2C%0Awhich%20can%20be%20easily%20adapted%20to%20new%20scenes%20and/or%20classes.%20A%20unified%20framework%0Ais%20developed%20for%20learning%203D%20object%20detectors%20from%20RGB%20images%20and%20associated%202D%0Aboxes.%20In%20specific%2C%20we%20propose%20three%20general%20components%3A%20prior%20injection%20module%0Ato%20obtain%20general%20object%20geometric%20priors%20from%20LLM%20model%2C%202D%20space%20projection%0Aconstraint%20to%20minimize%20the%20discrepancy%20between%20the%20boundaries%20of%20projected%203D%0Aboxes%20and%20their%20corresponding%202D%20boxes%20on%20the%20image%20plane%2C%20and%203D%20space%0Ageometry%20constraint%20to%20build%20a%20Point-to-Box%20alignment%20loss%20to%20further%20refine%0Athe%20pose%20of%20estimated%203D%20boxes.%20Experiments%20on%20KITTI%20and%20SUN-RGBD%20datasets%0Ademonstrate%20that%20our%20method%20yields%20surprisingly%20high-quality%203D%20bounding%20boxes%0Awith%20only%202D%20annotation.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/gwenzhang/GGA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13748v1&entry.124074799=Read"},
{"title": "Training-free Composite Scene Generation for Layout-to-Image Synthesis", "author": "Jiaqi Liu and Tao Huang and Chang Xu", "abstract": "  Recent breakthroughs in text-to-image diffusion models have significantly\nadvanced the generation of high-fidelity, photo-realistic images from textual\ndescriptions. Yet, these models often struggle with interpreting spatial\narrangements from text, hindering their ability to produce images with precise\nspatial configurations. To bridge this gap, layout-to-image generation has\nemerged as a promising direction. However, training-based approaches are\nlimited by the need for extensively annotated datasets, leading to high data\nacquisition costs and a constrained conceptual scope. Conversely, training-free\nmethods face challenges in accurately locating and generating semantically\nsimilar objects within complex compositions. This paper introduces a novel\ntraining-free approach designed to overcome adversarial semantic intersections\nduring the diffusion conditioning phase. By refining intra-token loss with\nselective sampling and enhancing the diffusion process with attention\nredistribution, we propose two innovative constraints: 1) an inter-token\nconstraint that resolves token conflicts to ensure accurate concept synthesis;\nand 2) a self-attention constraint that improves pixel-to-pixel relationships.\nOur evaluations confirm the effectiveness of leveraging layout information for\nguiding the diffusion process, generating content-rich images with enhanced\nfidelity and complexity. Code is available at\nhttps://github.com/Papple-F/csg.git.\n", "link": "http://arxiv.org/abs/2407.13609v1", "date": "2024-07-18", "relevancy": 2.4631, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6248}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.61}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-free%20Composite%20Scene%20Generation%20for%20Layout-to-Image%20Synthesis&body=Title%3A%20Training-free%20Composite%20Scene%20Generation%20for%20Layout-to-Image%20Synthesis%0AAuthor%3A%20Jiaqi%20Liu%20and%20Tao%20Huang%20and%20Chang%20Xu%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20text-to-image%20diffusion%20models%20have%20significantly%0Aadvanced%20the%20generation%20of%20high-fidelity%2C%20photo-realistic%20images%20from%20textual%0Adescriptions.%20Yet%2C%20these%20models%20often%20struggle%20with%20interpreting%20spatial%0Aarrangements%20from%20text%2C%20hindering%20their%20ability%20to%20produce%20images%20with%20precise%0Aspatial%20configurations.%20To%20bridge%20this%20gap%2C%20layout-to-image%20generation%20has%0Aemerged%20as%20a%20promising%20direction.%20However%2C%20training-based%20approaches%20are%0Alimited%20by%20the%20need%20for%20extensively%20annotated%20datasets%2C%20leading%20to%20high%20data%0Aacquisition%20costs%20and%20a%20constrained%20conceptual%20scope.%20Conversely%2C%20training-free%0Amethods%20face%20challenges%20in%20accurately%20locating%20and%20generating%20semantically%0Asimilar%20objects%20within%20complex%20compositions.%20This%20paper%20introduces%20a%20novel%0Atraining-free%20approach%20designed%20to%20overcome%20adversarial%20semantic%20intersections%0Aduring%20the%20diffusion%20conditioning%20phase.%20By%20refining%20intra-token%20loss%20with%0Aselective%20sampling%20and%20enhancing%20the%20diffusion%20process%20with%20attention%0Aredistribution%2C%20we%20propose%20two%20innovative%20constraints%3A%201%29%20an%20inter-token%0Aconstraint%20that%20resolves%20token%20conflicts%20to%20ensure%20accurate%20concept%20synthesis%3B%0Aand%202%29%20a%20self-attention%20constraint%20that%20improves%20pixel-to-pixel%20relationships.%0AOur%20evaluations%20confirm%20the%20effectiveness%20of%20leveraging%20layout%20information%20for%0Aguiding%20the%20diffusion%20process%2C%20generating%20content-rich%20images%20with%20enhanced%0Afidelity%20and%20complexity.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Papple-F/csg.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-free%2520Composite%2520Scene%2520Generation%2520for%2520Layout-to-Image%2520Synthesis%26entry.906535625%3DJiaqi%2520Liu%2520and%2520Tao%2520Huang%2520and%2520Chang%2520Xu%26entry.1292438233%3D%2520%2520Recent%2520breakthroughs%2520in%2520text-to-image%2520diffusion%2520models%2520have%2520significantly%250Aadvanced%2520the%2520generation%2520of%2520high-fidelity%252C%2520photo-realistic%2520images%2520from%2520textual%250Adescriptions.%2520Yet%252C%2520these%2520models%2520often%2520struggle%2520with%2520interpreting%2520spatial%250Aarrangements%2520from%2520text%252C%2520hindering%2520their%2520ability%2520to%2520produce%2520images%2520with%2520precise%250Aspatial%2520configurations.%2520To%2520bridge%2520this%2520gap%252C%2520layout-to-image%2520generation%2520has%250Aemerged%2520as%2520a%2520promising%2520direction.%2520However%252C%2520training-based%2520approaches%2520are%250Alimited%2520by%2520the%2520need%2520for%2520extensively%2520annotated%2520datasets%252C%2520leading%2520to%2520high%2520data%250Aacquisition%2520costs%2520and%2520a%2520constrained%2520conceptual%2520scope.%2520Conversely%252C%2520training-free%250Amethods%2520face%2520challenges%2520in%2520accurately%2520locating%2520and%2520generating%2520semantically%250Asimilar%2520objects%2520within%2520complex%2520compositions.%2520This%2520paper%2520introduces%2520a%2520novel%250Atraining-free%2520approach%2520designed%2520to%2520overcome%2520adversarial%2520semantic%2520intersections%250Aduring%2520the%2520diffusion%2520conditioning%2520phase.%2520By%2520refining%2520intra-token%2520loss%2520with%250Aselective%2520sampling%2520and%2520enhancing%2520the%2520diffusion%2520process%2520with%2520attention%250Aredistribution%252C%2520we%2520propose%2520two%2520innovative%2520constraints%253A%25201%2529%2520an%2520inter-token%250Aconstraint%2520that%2520resolves%2520token%2520conflicts%2520to%2520ensure%2520accurate%2520concept%2520synthesis%253B%250Aand%25202%2529%2520a%2520self-attention%2520constraint%2520that%2520improves%2520pixel-to-pixel%2520relationships.%250AOur%2520evaluations%2520confirm%2520the%2520effectiveness%2520of%2520leveraging%2520layout%2520information%2520for%250Aguiding%2520the%2520diffusion%2520process%252C%2520generating%2520content-rich%2520images%2520with%2520enhanced%250Afidelity%2520and%2520complexity.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Papple-F/csg.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-free%20Composite%20Scene%20Generation%20for%20Layout-to-Image%20Synthesis&entry.906535625=Jiaqi%20Liu%20and%20Tao%20Huang%20and%20Chang%20Xu&entry.1292438233=%20%20Recent%20breakthroughs%20in%20text-to-image%20diffusion%20models%20have%20significantly%0Aadvanced%20the%20generation%20of%20high-fidelity%2C%20photo-realistic%20images%20from%20textual%0Adescriptions.%20Yet%2C%20these%20models%20often%20struggle%20with%20interpreting%20spatial%0Aarrangements%20from%20text%2C%20hindering%20their%20ability%20to%20produce%20images%20with%20precise%0Aspatial%20configurations.%20To%20bridge%20this%20gap%2C%20layout-to-image%20generation%20has%0Aemerged%20as%20a%20promising%20direction.%20However%2C%20training-based%20approaches%20are%0Alimited%20by%20the%20need%20for%20extensively%20annotated%20datasets%2C%20leading%20to%20high%20data%0Aacquisition%20costs%20and%20a%20constrained%20conceptual%20scope.%20Conversely%2C%20training-free%0Amethods%20face%20challenges%20in%20accurately%20locating%20and%20generating%20semantically%0Asimilar%20objects%20within%20complex%20compositions.%20This%20paper%20introduces%20a%20novel%0Atraining-free%20approach%20designed%20to%20overcome%20adversarial%20semantic%20intersections%0Aduring%20the%20diffusion%20conditioning%20phase.%20By%20refining%20intra-token%20loss%20with%0Aselective%20sampling%20and%20enhancing%20the%20diffusion%20process%20with%20attention%0Aredistribution%2C%20we%20propose%20two%20innovative%20constraints%3A%201%29%20an%20inter-token%0Aconstraint%20that%20resolves%20token%20conflicts%20to%20ensure%20accurate%20concept%20synthesis%3B%0Aand%202%29%20a%20self-attention%20constraint%20that%20improves%20pixel-to-pixel%20relationships.%0AOur%20evaluations%20confirm%20the%20effectiveness%20of%20leveraging%20layout%20information%20for%0Aguiding%20the%20diffusion%20process%2C%20generating%20content-rich%20images%20with%20enhanced%0Afidelity%20and%20complexity.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Papple-F/csg.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13609v1&entry.124074799=Read"},
{"title": "Temporal Representation Learning for Stock Similarities and Its\n  Applications in Investment Management", "author": "Yoontae Hwang and Stefan Zohren and Yongjae Lee", "abstract": "  In the era of rapid globalization and digitalization, accurate identification\nof similar stocks has become increasingly challenging due to the non-stationary\nnature of financial markets and the ambiguity in conventional regional and\nsector classifications. To address these challenges, we examine SimStock, a\nnovel temporal self-supervised learning framework that combines techniques from\nself-supervised learning (SSL) and temporal domain generalization to learn\nrobust and informative representations of financial time series data. The\nprimary focus of our study is to understand the similarities between stocks\nfrom a broader perspective, considering the complex dynamics of the global\nfinancial landscape. We conduct extensive experiments on four real-world\ndatasets with thousands of stocks and demonstrate the effectiveness of SimStock\nin finding similar stocks, outperforming existing methods. The practical\nutility of SimStock is showcased through its application to various investment\nstrategies, such as pairs trading, index tracking, and portfolio optimization,\nwhere it leads to superior performance compared to conventional methods. Our\nfindings empirically examine the potential of data-driven approach to enhance\ninvestment decision-making and risk management practices by leveraging the\npower of temporal self-supervised learning in the face of the ever-changing\nglobal financial landscape.\n", "link": "http://arxiv.org/abs/2407.13751v1", "date": "2024-07-18", "relevancy": 2.3835, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4896}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4791}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Representation%20Learning%20for%20Stock%20Similarities%20and%20Its%0A%20%20Applications%20in%20Investment%20Management&body=Title%3A%20Temporal%20Representation%20Learning%20for%20Stock%20Similarities%20and%20Its%0A%20%20Applications%20in%20Investment%20Management%0AAuthor%3A%20Yoontae%20Hwang%20and%20Stefan%20Zohren%20and%20Yongjae%20Lee%0AAbstract%3A%20%20%20In%20the%20era%20of%20rapid%20globalization%20and%20digitalization%2C%20accurate%20identification%0Aof%20similar%20stocks%20has%20become%20increasingly%20challenging%20due%20to%20the%20non-stationary%0Anature%20of%20financial%20markets%20and%20the%20ambiguity%20in%20conventional%20regional%20and%0Asector%20classifications.%20To%20address%20these%20challenges%2C%20we%20examine%20SimStock%2C%20a%0Anovel%20temporal%20self-supervised%20learning%20framework%20that%20combines%20techniques%20from%0Aself-supervised%20learning%20%28SSL%29%20and%20temporal%20domain%20generalization%20to%20learn%0Arobust%20and%20informative%20representations%20of%20financial%20time%20series%20data.%20The%0Aprimary%20focus%20of%20our%20study%20is%20to%20understand%20the%20similarities%20between%20stocks%0Afrom%20a%20broader%20perspective%2C%20considering%20the%20complex%20dynamics%20of%20the%20global%0Afinancial%20landscape.%20We%20conduct%20extensive%20experiments%20on%20four%20real-world%0Adatasets%20with%20thousands%20of%20stocks%20and%20demonstrate%20the%20effectiveness%20of%20SimStock%0Ain%20finding%20similar%20stocks%2C%20outperforming%20existing%20methods.%20The%20practical%0Autility%20of%20SimStock%20is%20showcased%20through%20its%20application%20to%20various%20investment%0Astrategies%2C%20such%20as%20pairs%20trading%2C%20index%20tracking%2C%20and%20portfolio%20optimization%2C%0Awhere%20it%20leads%20to%20superior%20performance%20compared%20to%20conventional%20methods.%20Our%0Afindings%20empirically%20examine%20the%20potential%20of%20data-driven%20approach%20to%20enhance%0Ainvestment%20decision-making%20and%20risk%20management%20practices%20by%20leveraging%20the%0Apower%20of%20temporal%20self-supervised%20learning%20in%20the%20face%20of%20the%20ever-changing%0Aglobal%20financial%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Representation%2520Learning%2520for%2520Stock%2520Similarities%2520and%2520Its%250A%2520%2520Applications%2520in%2520Investment%2520Management%26entry.906535625%3DYoontae%2520Hwang%2520and%2520Stefan%2520Zohren%2520and%2520Yongjae%2520Lee%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520rapid%2520globalization%2520and%2520digitalization%252C%2520accurate%2520identification%250Aof%2520similar%2520stocks%2520has%2520become%2520increasingly%2520challenging%2520due%2520to%2520the%2520non-stationary%250Anature%2520of%2520financial%2520markets%2520and%2520the%2520ambiguity%2520in%2520conventional%2520regional%2520and%250Asector%2520classifications.%2520To%2520address%2520these%2520challenges%252C%2520we%2520examine%2520SimStock%252C%2520a%250Anovel%2520temporal%2520self-supervised%2520learning%2520framework%2520that%2520combines%2520techniques%2520from%250Aself-supervised%2520learning%2520%2528SSL%2529%2520and%2520temporal%2520domain%2520generalization%2520to%2520learn%250Arobust%2520and%2520informative%2520representations%2520of%2520financial%2520time%2520series%2520data.%2520The%250Aprimary%2520focus%2520of%2520our%2520study%2520is%2520to%2520understand%2520the%2520similarities%2520between%2520stocks%250Afrom%2520a%2520broader%2520perspective%252C%2520considering%2520the%2520complex%2520dynamics%2520of%2520the%2520global%250Afinancial%2520landscape.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520four%2520real-world%250Adatasets%2520with%2520thousands%2520of%2520stocks%2520and%2520demonstrate%2520the%2520effectiveness%2520of%2520SimStock%250Ain%2520finding%2520similar%2520stocks%252C%2520outperforming%2520existing%2520methods.%2520The%2520practical%250Autility%2520of%2520SimStock%2520is%2520showcased%2520through%2520its%2520application%2520to%2520various%2520investment%250Astrategies%252C%2520such%2520as%2520pairs%2520trading%252C%2520index%2520tracking%252C%2520and%2520portfolio%2520optimization%252C%250Awhere%2520it%2520leads%2520to%2520superior%2520performance%2520compared%2520to%2520conventional%2520methods.%2520Our%250Afindings%2520empirically%2520examine%2520the%2520potential%2520of%2520data-driven%2520approach%2520to%2520enhance%250Ainvestment%2520decision-making%2520and%2520risk%2520management%2520practices%2520by%2520leveraging%2520the%250Apower%2520of%2520temporal%2520self-supervised%2520learning%2520in%2520the%2520face%2520of%2520the%2520ever-changing%250Aglobal%2520financial%2520landscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Representation%20Learning%20for%20Stock%20Similarities%20and%20Its%0A%20%20Applications%20in%20Investment%20Management&entry.906535625=Yoontae%20Hwang%20and%20Stefan%20Zohren%20and%20Yongjae%20Lee&entry.1292438233=%20%20In%20the%20era%20of%20rapid%20globalization%20and%20digitalization%2C%20accurate%20identification%0Aof%20similar%20stocks%20has%20become%20increasingly%20challenging%20due%20to%20the%20non-stationary%0Anature%20of%20financial%20markets%20and%20the%20ambiguity%20in%20conventional%20regional%20and%0Asector%20classifications.%20To%20address%20these%20challenges%2C%20we%20examine%20SimStock%2C%20a%0Anovel%20temporal%20self-supervised%20learning%20framework%20that%20combines%20techniques%20from%0Aself-supervised%20learning%20%28SSL%29%20and%20temporal%20domain%20generalization%20to%20learn%0Arobust%20and%20informative%20representations%20of%20financial%20time%20series%20data.%20The%0Aprimary%20focus%20of%20our%20study%20is%20to%20understand%20the%20similarities%20between%20stocks%0Afrom%20a%20broader%20perspective%2C%20considering%20the%20complex%20dynamics%20of%20the%20global%0Afinancial%20landscape.%20We%20conduct%20extensive%20experiments%20on%20four%20real-world%0Adatasets%20with%20thousands%20of%20stocks%20and%20demonstrate%20the%20effectiveness%20of%20SimStock%0Ain%20finding%20similar%20stocks%2C%20outperforming%20existing%20methods.%20The%20practical%0Autility%20of%20SimStock%20is%20showcased%20through%20its%20application%20to%20various%20investment%0Astrategies%2C%20such%20as%20pairs%20trading%2C%20index%20tracking%2C%20and%20portfolio%20optimization%2C%0Awhere%20it%20leads%20to%20superior%20performance%20compared%20to%20conventional%20methods.%20Our%0Afindings%20empirically%20examine%20the%20potential%20of%20data-driven%20approach%20to%20enhance%0Ainvestment%20decision-making%20and%20risk%20management%20practices%20by%20leveraging%20the%0Apower%20of%20temporal%20self-supervised%20learning%20in%20the%20face%20of%20the%20ever-changing%0Aglobal%20financial%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13751v1&entry.124074799=Read"},
{"title": "Qalam : A Multimodal LLM for Arabic Optical Character and Handwriting\n  Recognition", "author": "Gagan Bhatia and El Moatez Billah Nagoudi and Fakhraddin Alwajih and Muhammad Abdul-Mageed", "abstract": "  Arabic Optical Character Recognition (OCR) and Handwriting Recognition (HWR)\npose unique challenges due to the cursive and context-sensitive nature of the\nArabic script. This study introduces Qalam, a novel foundation model designed\nfor Arabic OCR and HWR, built on a SwinV2 encoder and RoBERTa decoder\narchitecture. Our model significantly outperforms existing methods, achieving a\nWord Error Rate (WER) of just 0.80% in HWR tasks and 1.18% in OCR tasks. We\ntrain Qalam on a diverse dataset, including over 4.5 million images from Arabic\nmanuscripts and a synthetic dataset comprising 60k image-text pairs. Notably,\nQalam demonstrates exceptional handling of Arabic diacritics, a critical\nfeature in Arabic scripts. Furthermore, it shows a remarkable ability to\nprocess high-resolution inputs, addressing a common limitation in current OCR\nsystems. These advancements underscore Qalam's potential as a leading solution\nfor Arabic script recognition, offering a significant leap in accuracy and\nefficiency.\n", "link": "http://arxiv.org/abs/2407.13559v1", "date": "2024-07-18", "relevancy": 2.3247, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4912}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4538}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qalam%20%3A%20A%20Multimodal%20LLM%20for%20Arabic%20Optical%20Character%20and%20Handwriting%0A%20%20Recognition&body=Title%3A%20Qalam%20%3A%20A%20Multimodal%20LLM%20for%20Arabic%20Optical%20Character%20and%20Handwriting%0A%20%20Recognition%0AAuthor%3A%20Gagan%20Bhatia%20and%20El%20Moatez%20Billah%20Nagoudi%20and%20Fakhraddin%20Alwajih%20and%20Muhammad%20Abdul-Mageed%0AAbstract%3A%20%20%20Arabic%20Optical%20Character%20Recognition%20%28OCR%29%20and%20Handwriting%20Recognition%20%28HWR%29%0Apose%20unique%20challenges%20due%20to%20the%20cursive%20and%20context-sensitive%20nature%20of%20the%0AArabic%20script.%20This%20study%20introduces%20Qalam%2C%20a%20novel%20foundation%20model%20designed%0Afor%20Arabic%20OCR%20and%20HWR%2C%20built%20on%20a%20SwinV2%20encoder%20and%20RoBERTa%20decoder%0Aarchitecture.%20Our%20model%20significantly%20outperforms%20existing%20methods%2C%20achieving%20a%0AWord%20Error%20Rate%20%28WER%29%20of%20just%200.80%25%20in%20HWR%20tasks%20and%201.18%25%20in%20OCR%20tasks.%20We%0Atrain%20Qalam%20on%20a%20diverse%20dataset%2C%20including%20over%204.5%20million%20images%20from%20Arabic%0Amanuscripts%20and%20a%20synthetic%20dataset%20comprising%2060k%20image-text%20pairs.%20Notably%2C%0AQalam%20demonstrates%20exceptional%20handling%20of%20Arabic%20diacritics%2C%20a%20critical%0Afeature%20in%20Arabic%20scripts.%20Furthermore%2C%20it%20shows%20a%20remarkable%20ability%20to%0Aprocess%20high-resolution%20inputs%2C%20addressing%20a%20common%20limitation%20in%20current%20OCR%0Asystems.%20These%20advancements%20underscore%20Qalam%27s%20potential%20as%20a%20leading%20solution%0Afor%20Arabic%20script%20recognition%2C%20offering%20a%20significant%20leap%20in%20accuracy%20and%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQalam%2520%253A%2520A%2520Multimodal%2520LLM%2520for%2520Arabic%2520Optical%2520Character%2520and%2520Handwriting%250A%2520%2520Recognition%26entry.906535625%3DGagan%2520Bhatia%2520and%2520El%2520Moatez%2520Billah%2520Nagoudi%2520and%2520Fakhraddin%2520Alwajih%2520and%2520Muhammad%2520Abdul-Mageed%26entry.1292438233%3D%2520%2520Arabic%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520and%2520Handwriting%2520Recognition%2520%2528HWR%2529%250Apose%2520unique%2520challenges%2520due%2520to%2520the%2520cursive%2520and%2520context-sensitive%2520nature%2520of%2520the%250AArabic%2520script.%2520This%2520study%2520introduces%2520Qalam%252C%2520a%2520novel%2520foundation%2520model%2520designed%250Afor%2520Arabic%2520OCR%2520and%2520HWR%252C%2520built%2520on%2520a%2520SwinV2%2520encoder%2520and%2520RoBERTa%2520decoder%250Aarchitecture.%2520Our%2520model%2520significantly%2520outperforms%2520existing%2520methods%252C%2520achieving%2520a%250AWord%2520Error%2520Rate%2520%2528WER%2529%2520of%2520just%25200.80%2525%2520in%2520HWR%2520tasks%2520and%25201.18%2525%2520in%2520OCR%2520tasks.%2520We%250Atrain%2520Qalam%2520on%2520a%2520diverse%2520dataset%252C%2520including%2520over%25204.5%2520million%2520images%2520from%2520Arabic%250Amanuscripts%2520and%2520a%2520synthetic%2520dataset%2520comprising%252060k%2520image-text%2520pairs.%2520Notably%252C%250AQalam%2520demonstrates%2520exceptional%2520handling%2520of%2520Arabic%2520diacritics%252C%2520a%2520critical%250Afeature%2520in%2520Arabic%2520scripts.%2520Furthermore%252C%2520it%2520shows%2520a%2520remarkable%2520ability%2520to%250Aprocess%2520high-resolution%2520inputs%252C%2520addressing%2520a%2520common%2520limitation%2520in%2520current%2520OCR%250Asystems.%2520These%2520advancements%2520underscore%2520Qalam%2527s%2520potential%2520as%2520a%2520leading%2520solution%250Afor%2520Arabic%2520script%2520recognition%252C%2520offering%2520a%2520significant%2520leap%2520in%2520accuracy%2520and%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qalam%20%3A%20A%20Multimodal%20LLM%20for%20Arabic%20Optical%20Character%20and%20Handwriting%0A%20%20Recognition&entry.906535625=Gagan%20Bhatia%20and%20El%20Moatez%20Billah%20Nagoudi%20and%20Fakhraddin%20Alwajih%20and%20Muhammad%20Abdul-Mageed&entry.1292438233=%20%20Arabic%20Optical%20Character%20Recognition%20%28OCR%29%20and%20Handwriting%20Recognition%20%28HWR%29%0Apose%20unique%20challenges%20due%20to%20the%20cursive%20and%20context-sensitive%20nature%20of%20the%0AArabic%20script.%20This%20study%20introduces%20Qalam%2C%20a%20novel%20foundation%20model%20designed%0Afor%20Arabic%20OCR%20and%20HWR%2C%20built%20on%20a%20SwinV2%20encoder%20and%20RoBERTa%20decoder%0Aarchitecture.%20Our%20model%20significantly%20outperforms%20existing%20methods%2C%20achieving%20a%0AWord%20Error%20Rate%20%28WER%29%20of%20just%200.80%25%20in%20HWR%20tasks%20and%201.18%25%20in%20OCR%20tasks.%20We%0Atrain%20Qalam%20on%20a%20diverse%20dataset%2C%20including%20over%204.5%20million%20images%20from%20Arabic%0Amanuscripts%20and%20a%20synthetic%20dataset%20comprising%2060k%20image-text%20pairs.%20Notably%2C%0AQalam%20demonstrates%20exceptional%20handling%20of%20Arabic%20diacritics%2C%20a%20critical%0Afeature%20in%20Arabic%20scripts.%20Furthermore%2C%20it%20shows%20a%20remarkable%20ability%20to%0Aprocess%20high-resolution%20inputs%2C%20addressing%20a%20common%20limitation%20in%20current%20OCR%0Asystems.%20These%20advancements%20underscore%20Qalam%27s%20potential%20as%20a%20leading%20solution%0Afor%20Arabic%20script%20recognition%2C%20offering%20a%20significant%20leap%20in%20accuracy%20and%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13559v1&entry.124074799=Read"},
{"title": "LIMT: Language-Informed Multi-Task Visual World Models", "author": "Elie Aljalbout and Nikolaos Sotirakis and Patrick van der Smagt and Maximilian Karl and Nutan Chen", "abstract": "  Most recent successes in robot reinforcement learning involve learning a\nspecialized single-task agent.\n  However, robots capable of performing multiple tasks can be much more\nvaluable in real-world applications.\n  Multi-task reinforcement learning can be very challenging due to the\nincreased sample complexity and the potentially conflicting task objectives.\n  Previous work on this topic is dominated by model-free approaches.\n  The latter can be very sample inefficient even when learning specialized\nsingle-task agents.\n  In this work, we focus on model-based multi-task reinforcement learning.\n  We propose a method for learning multi-task visual world models, leveraging\npre-trained language models to extract semantically meaningful task\nrepresentations.\n  These representations are used by the world model and policy to reason about\ntask similarity in dynamics and behavior.\n  Our results highlight the benefits of using language-driven task\nrepresentations for world models and a clear advantage of model-based\nmulti-task learning over the more common model-free paradigm.\n", "link": "http://arxiv.org/abs/2407.13466v1", "date": "2024-07-18", "relevancy": 2.3198, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.614}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6046}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIMT%3A%20Language-Informed%20Multi-Task%20Visual%20World%20Models&body=Title%3A%20LIMT%3A%20Language-Informed%20Multi-Task%20Visual%20World%20Models%0AAuthor%3A%20Elie%20Aljalbout%20and%20Nikolaos%20Sotirakis%20and%20Patrick%20van%20der%20Smagt%20and%20Maximilian%20Karl%20and%20Nutan%20Chen%0AAbstract%3A%20%20%20Most%20recent%20successes%20in%20robot%20reinforcement%20learning%20involve%20learning%20a%0Aspecialized%20single-task%20agent.%0A%20%20However%2C%20robots%20capable%20of%20performing%20multiple%20tasks%20can%20be%20much%20more%0Avaluable%20in%20real-world%20applications.%0A%20%20Multi-task%20reinforcement%20learning%20can%20be%20very%20challenging%20due%20to%20the%0Aincreased%20sample%20complexity%20and%20the%20potentially%20conflicting%20task%20objectives.%0A%20%20Previous%20work%20on%20this%20topic%20is%20dominated%20by%20model-free%20approaches.%0A%20%20The%20latter%20can%20be%20very%20sample%20inefficient%20even%20when%20learning%20specialized%0Asingle-task%20agents.%0A%20%20In%20this%20work%2C%20we%20focus%20on%20model-based%20multi-task%20reinforcement%20learning.%0A%20%20We%20propose%20a%20method%20for%20learning%20multi-task%20visual%20world%20models%2C%20leveraging%0Apre-trained%20language%20models%20to%20extract%20semantically%20meaningful%20task%0Arepresentations.%0A%20%20These%20representations%20are%20used%20by%20the%20world%20model%20and%20policy%20to%20reason%20about%0Atask%20similarity%20in%20dynamics%20and%20behavior.%0A%20%20Our%20results%20highlight%20the%20benefits%20of%20using%20language-driven%20task%0Arepresentations%20for%20world%20models%20and%20a%20clear%20advantage%20of%20model-based%0Amulti-task%20learning%20over%20the%20more%20common%20model-free%20paradigm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIMT%253A%2520Language-Informed%2520Multi-Task%2520Visual%2520World%2520Models%26entry.906535625%3DElie%2520Aljalbout%2520and%2520Nikolaos%2520Sotirakis%2520and%2520Patrick%2520van%2520der%2520Smagt%2520and%2520Maximilian%2520Karl%2520and%2520Nutan%2520Chen%26entry.1292438233%3D%2520%2520Most%2520recent%2520successes%2520in%2520robot%2520reinforcement%2520learning%2520involve%2520learning%2520a%250Aspecialized%2520single-task%2520agent.%250A%2520%2520However%252C%2520robots%2520capable%2520of%2520performing%2520multiple%2520tasks%2520can%2520be%2520much%2520more%250Avaluable%2520in%2520real-world%2520applications.%250A%2520%2520Multi-task%2520reinforcement%2520learning%2520can%2520be%2520very%2520challenging%2520due%2520to%2520the%250Aincreased%2520sample%2520complexity%2520and%2520the%2520potentially%2520conflicting%2520task%2520objectives.%250A%2520%2520Previous%2520work%2520on%2520this%2520topic%2520is%2520dominated%2520by%2520model-free%2520approaches.%250A%2520%2520The%2520latter%2520can%2520be%2520very%2520sample%2520inefficient%2520even%2520when%2520learning%2520specialized%250Asingle-task%2520agents.%250A%2520%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520model-based%2520multi-task%2520reinforcement%2520learning.%250A%2520%2520We%2520propose%2520a%2520method%2520for%2520learning%2520multi-task%2520visual%2520world%2520models%252C%2520leveraging%250Apre-trained%2520language%2520models%2520to%2520extract%2520semantically%2520meaningful%2520task%250Arepresentations.%250A%2520%2520These%2520representations%2520are%2520used%2520by%2520the%2520world%2520model%2520and%2520policy%2520to%2520reason%2520about%250Atask%2520similarity%2520in%2520dynamics%2520and%2520behavior.%250A%2520%2520Our%2520results%2520highlight%2520the%2520benefits%2520of%2520using%2520language-driven%2520task%250Arepresentations%2520for%2520world%2520models%2520and%2520a%2520clear%2520advantage%2520of%2520model-based%250Amulti-task%2520learning%2520over%2520the%2520more%2520common%2520model-free%2520paradigm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIMT%3A%20Language-Informed%20Multi-Task%20Visual%20World%20Models&entry.906535625=Elie%20Aljalbout%20and%20Nikolaos%20Sotirakis%20and%20Patrick%20van%20der%20Smagt%20and%20Maximilian%20Karl%20and%20Nutan%20Chen&entry.1292438233=%20%20Most%20recent%20successes%20in%20robot%20reinforcement%20learning%20involve%20learning%20a%0Aspecialized%20single-task%20agent.%0A%20%20However%2C%20robots%20capable%20of%20performing%20multiple%20tasks%20can%20be%20much%20more%0Avaluable%20in%20real-world%20applications.%0A%20%20Multi-task%20reinforcement%20learning%20can%20be%20very%20challenging%20due%20to%20the%0Aincreased%20sample%20complexity%20and%20the%20potentially%20conflicting%20task%20objectives.%0A%20%20Previous%20work%20on%20this%20topic%20is%20dominated%20by%20model-free%20approaches.%0A%20%20The%20latter%20can%20be%20very%20sample%20inefficient%20even%20when%20learning%20specialized%0Asingle-task%20agents.%0A%20%20In%20this%20work%2C%20we%20focus%20on%20model-based%20multi-task%20reinforcement%20learning.%0A%20%20We%20propose%20a%20method%20for%20learning%20multi-task%20visual%20world%20models%2C%20leveraging%0Apre-trained%20language%20models%20to%20extract%20semantically%20meaningful%20task%0Arepresentations.%0A%20%20These%20representations%20are%20used%20by%20the%20world%20model%20and%20policy%20to%20reason%20about%0Atask%20similarity%20in%20dynamics%20and%20behavior.%0A%20%20Our%20results%20highlight%20the%20benefits%20of%20using%20language-driven%20task%0Arepresentations%20for%20world%20models%20and%20a%20clear%20advantage%20of%20model-based%0Amulti-task%20learning%20over%20the%20more%20common%20model-free%20paradigm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13466v1&entry.124074799=Read"},
{"title": "PASTA: Controllable Part-Aware Shape Generation with Autoregressive\n  Transformers", "author": "Songlin Li and Despoina Paschalidou and Leonidas Guibas", "abstract": "  The increased demand for tools that automate the 3D content creation process\nled to tremendous progress in deep generative models that can generate diverse\n3D objects of high fidelity. In this paper, we present PASTA, an autoregressive\ntransformer architecture for generating high quality 3D shapes. PASTA comprises\ntwo main components: An autoregressive transformer that generates objects as a\nsequence of cuboidal primitives and a blending network, implemented with a\ntransformer decoder that composes the sequences of cuboids and synthesizes high\nquality meshes for each object. Our model is trained in two stages: First we\ntrain our autoregressive generative model using only annotated cuboidal parts\nas supervision and next, we train our blending network using explicit 3D\nsupervision, in the form of watertight meshes. Evaluations on various ShapeNet\nobjects showcase the ability of our model to perform shape generation from\ndiverse inputs \\eg from scratch, from a partial object, from text and images,\nas well size-guided generation, by explicitly conditioning on a bounding box\nthat defines the object's boundaries. Moreover, as our model considers the\nunderlying part-based structure of a 3D object, we are able to select a\nspecific part and produce shapes with meaningful variations of this part. As\nevidenced by our experiments, our model generates 3D shapes that are both more\nrealistic and diverse than existing part-based and non part-based methods,\nwhile at the same time is simpler to implement and train.\n", "link": "http://arxiv.org/abs/2407.13677v1", "date": "2024-07-18", "relevancy": 2.3166, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5923}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5843}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PASTA%3A%20Controllable%20Part-Aware%20Shape%20Generation%20with%20Autoregressive%0A%20%20Transformers&body=Title%3A%20PASTA%3A%20Controllable%20Part-Aware%20Shape%20Generation%20with%20Autoregressive%0A%20%20Transformers%0AAuthor%3A%20Songlin%20Li%20and%20Despoina%20Paschalidou%20and%20Leonidas%20Guibas%0AAbstract%3A%20%20%20The%20increased%20demand%20for%20tools%20that%20automate%20the%203D%20content%20creation%20process%0Aled%20to%20tremendous%20progress%20in%20deep%20generative%20models%20that%20can%20generate%20diverse%0A3D%20objects%20of%20high%20fidelity.%20In%20this%20paper%2C%20we%20present%20PASTA%2C%20an%20autoregressive%0Atransformer%20architecture%20for%20generating%20high%20quality%203D%20shapes.%20PASTA%20comprises%0Atwo%20main%20components%3A%20An%20autoregressive%20transformer%20that%20generates%20objects%20as%20a%0Asequence%20of%20cuboidal%20primitives%20and%20a%20blending%20network%2C%20implemented%20with%20a%0Atransformer%20decoder%20that%20composes%20the%20sequences%20of%20cuboids%20and%20synthesizes%20high%0Aquality%20meshes%20for%20each%20object.%20Our%20model%20is%20trained%20in%20two%20stages%3A%20First%20we%0Atrain%20our%20autoregressive%20generative%20model%20using%20only%20annotated%20cuboidal%20parts%0Aas%20supervision%20and%20next%2C%20we%20train%20our%20blending%20network%20using%20explicit%203D%0Asupervision%2C%20in%20the%20form%20of%20watertight%20meshes.%20Evaluations%20on%20various%20ShapeNet%0Aobjects%20showcase%20the%20ability%20of%20our%20model%20to%20perform%20shape%20generation%20from%0Adiverse%20inputs%20%5Ceg%20from%20scratch%2C%20from%20a%20partial%20object%2C%20from%20text%20and%20images%2C%0Aas%20well%20size-guided%20generation%2C%20by%20explicitly%20conditioning%20on%20a%20bounding%20box%0Athat%20defines%20the%20object%27s%20boundaries.%20Moreover%2C%20as%20our%20model%20considers%20the%0Aunderlying%20part-based%20structure%20of%20a%203D%20object%2C%20we%20are%20able%20to%20select%20a%0Aspecific%20part%20and%20produce%20shapes%20with%20meaningful%20variations%20of%20this%20part.%20As%0Aevidenced%20by%20our%20experiments%2C%20our%20model%20generates%203D%20shapes%20that%20are%20both%20more%0Arealistic%20and%20diverse%20than%20existing%20part-based%20and%20non%20part-based%20methods%2C%0Awhile%20at%20the%20same%20time%20is%20simpler%20to%20implement%20and%20train.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPASTA%253A%2520Controllable%2520Part-Aware%2520Shape%2520Generation%2520with%2520Autoregressive%250A%2520%2520Transformers%26entry.906535625%3DSonglin%2520Li%2520and%2520Despoina%2520Paschalidou%2520and%2520Leonidas%2520Guibas%26entry.1292438233%3D%2520%2520The%2520increased%2520demand%2520for%2520tools%2520that%2520automate%2520the%25203D%2520content%2520creation%2520process%250Aled%2520to%2520tremendous%2520progress%2520in%2520deep%2520generative%2520models%2520that%2520can%2520generate%2520diverse%250A3D%2520objects%2520of%2520high%2520fidelity.%2520In%2520this%2520paper%252C%2520we%2520present%2520PASTA%252C%2520an%2520autoregressive%250Atransformer%2520architecture%2520for%2520generating%2520high%2520quality%25203D%2520shapes.%2520PASTA%2520comprises%250Atwo%2520main%2520components%253A%2520An%2520autoregressive%2520transformer%2520that%2520generates%2520objects%2520as%2520a%250Asequence%2520of%2520cuboidal%2520primitives%2520and%2520a%2520blending%2520network%252C%2520implemented%2520with%2520a%250Atransformer%2520decoder%2520that%2520composes%2520the%2520sequences%2520of%2520cuboids%2520and%2520synthesizes%2520high%250Aquality%2520meshes%2520for%2520each%2520object.%2520Our%2520model%2520is%2520trained%2520in%2520two%2520stages%253A%2520First%2520we%250Atrain%2520our%2520autoregressive%2520generative%2520model%2520using%2520only%2520annotated%2520cuboidal%2520parts%250Aas%2520supervision%2520and%2520next%252C%2520we%2520train%2520our%2520blending%2520network%2520using%2520explicit%25203D%250Asupervision%252C%2520in%2520the%2520form%2520of%2520watertight%2520meshes.%2520Evaluations%2520on%2520various%2520ShapeNet%250Aobjects%2520showcase%2520the%2520ability%2520of%2520our%2520model%2520to%2520perform%2520shape%2520generation%2520from%250Adiverse%2520inputs%2520%255Ceg%2520from%2520scratch%252C%2520from%2520a%2520partial%2520object%252C%2520from%2520text%2520and%2520images%252C%250Aas%2520well%2520size-guided%2520generation%252C%2520by%2520explicitly%2520conditioning%2520on%2520a%2520bounding%2520box%250Athat%2520defines%2520the%2520object%2527s%2520boundaries.%2520Moreover%252C%2520as%2520our%2520model%2520considers%2520the%250Aunderlying%2520part-based%2520structure%2520of%2520a%25203D%2520object%252C%2520we%2520are%2520able%2520to%2520select%2520a%250Aspecific%2520part%2520and%2520produce%2520shapes%2520with%2520meaningful%2520variations%2520of%2520this%2520part.%2520As%250Aevidenced%2520by%2520our%2520experiments%252C%2520our%2520model%2520generates%25203D%2520shapes%2520that%2520are%2520both%2520more%250Arealistic%2520and%2520diverse%2520than%2520existing%2520part-based%2520and%2520non%2520part-based%2520methods%252C%250Awhile%2520at%2520the%2520same%2520time%2520is%2520simpler%2520to%2520implement%2520and%2520train.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PASTA%3A%20Controllable%20Part-Aware%20Shape%20Generation%20with%20Autoregressive%0A%20%20Transformers&entry.906535625=Songlin%20Li%20and%20Despoina%20Paschalidou%20and%20Leonidas%20Guibas&entry.1292438233=%20%20The%20increased%20demand%20for%20tools%20that%20automate%20the%203D%20content%20creation%20process%0Aled%20to%20tremendous%20progress%20in%20deep%20generative%20models%20that%20can%20generate%20diverse%0A3D%20objects%20of%20high%20fidelity.%20In%20this%20paper%2C%20we%20present%20PASTA%2C%20an%20autoregressive%0Atransformer%20architecture%20for%20generating%20high%20quality%203D%20shapes.%20PASTA%20comprises%0Atwo%20main%20components%3A%20An%20autoregressive%20transformer%20that%20generates%20objects%20as%20a%0Asequence%20of%20cuboidal%20primitives%20and%20a%20blending%20network%2C%20implemented%20with%20a%0Atransformer%20decoder%20that%20composes%20the%20sequences%20of%20cuboids%20and%20synthesizes%20high%0Aquality%20meshes%20for%20each%20object.%20Our%20model%20is%20trained%20in%20two%20stages%3A%20First%20we%0Atrain%20our%20autoregressive%20generative%20model%20using%20only%20annotated%20cuboidal%20parts%0Aas%20supervision%20and%20next%2C%20we%20train%20our%20blending%20network%20using%20explicit%203D%0Asupervision%2C%20in%20the%20form%20of%20watertight%20meshes.%20Evaluations%20on%20various%20ShapeNet%0Aobjects%20showcase%20the%20ability%20of%20our%20model%20to%20perform%20shape%20generation%20from%0Adiverse%20inputs%20%5Ceg%20from%20scratch%2C%20from%20a%20partial%20object%2C%20from%20text%20and%20images%2C%0Aas%20well%20size-guided%20generation%2C%20by%20explicitly%20conditioning%20on%20a%20bounding%20box%0Athat%20defines%20the%20object%27s%20boundaries.%20Moreover%2C%20as%20our%20model%20considers%20the%0Aunderlying%20part-based%20structure%20of%20a%203D%20object%2C%20we%20are%20able%20to%20select%20a%0Aspecific%20part%20and%20produce%20shapes%20with%20meaningful%20variations%20of%20this%20part.%20As%0Aevidenced%20by%20our%20experiments%2C%20our%20model%20generates%203D%20shapes%20that%20are%20both%20more%0Arealistic%20and%20diverse%20than%20existing%20part-based%20and%20non%20part-based%20methods%2C%0Awhile%20at%20the%20same%20time%20is%20simpler%20to%20implement%20and%20train.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13677v1&entry.124074799=Read"},
{"title": "Hyp2Nav: Hyperbolic Planning and Curiosity for Crowd Navigation", "author": "Alessandro Flaborea and Guido Maria D'Amely di Melendugno and Pascal Mettes and Fabio Galasso", "abstract": "  Autonomous robots are increasingly becoming a strong fixture in social\nenvironments. Effective crowd navigation requires not only safe yet fast\nplanning, but should also enable interpretability and computational efficiency\nfor working in real-time on embedded devices. In this work, we advocate for\nhyperbolic learning to enable crowd navigation and we introduce Hyp2Nav.\nDifferent from conventional reinforcement learning-based crowd navigation\nmethods, Hyp2Nav leverages the intrinsic properties of hyperbolic geometry to\nbetter encode the hierarchical nature of decision-making processes in\nnavigation tasks. We propose a hyperbolic policy model and a hyperbolic\ncuriosity module that results in effective social navigation, best success\nrates, and returns across multiple simulation settings, using up to 6 times\nfewer parameters than competitor state-of-the-art models. With our approach, it\nbecomes even possible to obtain policies that work in 2-dimensional embedding\nspaces, opening up new possibilities for low-resource crowd navigation and\nmodel interpretability. Insightfully, the internal hyperbolic representation of\nHyp2Nav correlates with how much attention the robot pays to the surrounding\ncrowds, e.g. due to multiple people occluding its pathway or to a few of them\nshowing colliding plans, rather than to its own planned route.\n", "link": "http://arxiv.org/abs/2407.13567v1", "date": "2024-07-18", "relevancy": 2.2927, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5887}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5723}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyp2Nav%3A%20Hyperbolic%20Planning%20and%20Curiosity%20for%20Crowd%20Navigation&body=Title%3A%20Hyp2Nav%3A%20Hyperbolic%20Planning%20and%20Curiosity%20for%20Crowd%20Navigation%0AAuthor%3A%20Alessandro%20Flaborea%20and%20Guido%20Maria%20D%27Amely%20di%20Melendugno%20and%20Pascal%20Mettes%20and%20Fabio%20Galasso%0AAbstract%3A%20%20%20Autonomous%20robots%20are%20increasingly%20becoming%20a%20strong%20fixture%20in%20social%0Aenvironments.%20Effective%20crowd%20navigation%20requires%20not%20only%20safe%20yet%20fast%0Aplanning%2C%20but%20should%20also%20enable%20interpretability%20and%20computational%20efficiency%0Afor%20working%20in%20real-time%20on%20embedded%20devices.%20In%20this%20work%2C%20we%20advocate%20for%0Ahyperbolic%20learning%20to%20enable%20crowd%20navigation%20and%20we%20introduce%20Hyp2Nav.%0ADifferent%20from%20conventional%20reinforcement%20learning-based%20crowd%20navigation%0Amethods%2C%20Hyp2Nav%20leverages%20the%20intrinsic%20properties%20of%20hyperbolic%20geometry%20to%0Abetter%20encode%20the%20hierarchical%20nature%20of%20decision-making%20processes%20in%0Anavigation%20tasks.%20We%20propose%20a%20hyperbolic%20policy%20model%20and%20a%20hyperbolic%0Acuriosity%20module%20that%20results%20in%20effective%20social%20navigation%2C%20best%20success%0Arates%2C%20and%20returns%20across%20multiple%20simulation%20settings%2C%20using%20up%20to%206%20times%0Afewer%20parameters%20than%20competitor%20state-of-the-art%20models.%20With%20our%20approach%2C%20it%0Abecomes%20even%20possible%20to%20obtain%20policies%20that%20work%20in%202-dimensional%20embedding%0Aspaces%2C%20opening%20up%20new%20possibilities%20for%20low-resource%20crowd%20navigation%20and%0Amodel%20interpretability.%20Insightfully%2C%20the%20internal%20hyperbolic%20representation%20of%0AHyp2Nav%20correlates%20with%20how%20much%20attention%20the%20robot%20pays%20to%20the%20surrounding%0Acrowds%2C%20e.g.%20due%20to%20multiple%20people%20occluding%20its%20pathway%20or%20to%20a%20few%20of%20them%0Ashowing%20colliding%20plans%2C%20rather%20than%20to%20its%20own%20planned%20route.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyp2Nav%253A%2520Hyperbolic%2520Planning%2520and%2520Curiosity%2520for%2520Crowd%2520Navigation%26entry.906535625%3DAlessandro%2520Flaborea%2520and%2520Guido%2520Maria%2520D%2527Amely%2520di%2520Melendugno%2520and%2520Pascal%2520Mettes%2520and%2520Fabio%2520Galasso%26entry.1292438233%3D%2520%2520Autonomous%2520robots%2520are%2520increasingly%2520becoming%2520a%2520strong%2520fixture%2520in%2520social%250Aenvironments.%2520Effective%2520crowd%2520navigation%2520requires%2520not%2520only%2520safe%2520yet%2520fast%250Aplanning%252C%2520but%2520should%2520also%2520enable%2520interpretability%2520and%2520computational%2520efficiency%250Afor%2520working%2520in%2520real-time%2520on%2520embedded%2520devices.%2520In%2520this%2520work%252C%2520we%2520advocate%2520for%250Ahyperbolic%2520learning%2520to%2520enable%2520crowd%2520navigation%2520and%2520we%2520introduce%2520Hyp2Nav.%250ADifferent%2520from%2520conventional%2520reinforcement%2520learning-based%2520crowd%2520navigation%250Amethods%252C%2520Hyp2Nav%2520leverages%2520the%2520intrinsic%2520properties%2520of%2520hyperbolic%2520geometry%2520to%250Abetter%2520encode%2520the%2520hierarchical%2520nature%2520of%2520decision-making%2520processes%2520in%250Anavigation%2520tasks.%2520We%2520propose%2520a%2520hyperbolic%2520policy%2520model%2520and%2520a%2520hyperbolic%250Acuriosity%2520module%2520that%2520results%2520in%2520effective%2520social%2520navigation%252C%2520best%2520success%250Arates%252C%2520and%2520returns%2520across%2520multiple%2520simulation%2520settings%252C%2520using%2520up%2520to%25206%2520times%250Afewer%2520parameters%2520than%2520competitor%2520state-of-the-art%2520models.%2520With%2520our%2520approach%252C%2520it%250Abecomes%2520even%2520possible%2520to%2520obtain%2520policies%2520that%2520work%2520in%25202-dimensional%2520embedding%250Aspaces%252C%2520opening%2520up%2520new%2520possibilities%2520for%2520low-resource%2520crowd%2520navigation%2520and%250Amodel%2520interpretability.%2520Insightfully%252C%2520the%2520internal%2520hyperbolic%2520representation%2520of%250AHyp2Nav%2520correlates%2520with%2520how%2520much%2520attention%2520the%2520robot%2520pays%2520to%2520the%2520surrounding%250Acrowds%252C%2520e.g.%2520due%2520to%2520multiple%2520people%2520occluding%2520its%2520pathway%2520or%2520to%2520a%2520few%2520of%2520them%250Ashowing%2520colliding%2520plans%252C%2520rather%2520than%2520to%2520its%2520own%2520planned%2520route.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyp2Nav%3A%20Hyperbolic%20Planning%20and%20Curiosity%20for%20Crowd%20Navigation&entry.906535625=Alessandro%20Flaborea%20and%20Guido%20Maria%20D%27Amely%20di%20Melendugno%20and%20Pascal%20Mettes%20and%20Fabio%20Galasso&entry.1292438233=%20%20Autonomous%20robots%20are%20increasingly%20becoming%20a%20strong%20fixture%20in%20social%0Aenvironments.%20Effective%20crowd%20navigation%20requires%20not%20only%20safe%20yet%20fast%0Aplanning%2C%20but%20should%20also%20enable%20interpretability%20and%20computational%20efficiency%0Afor%20working%20in%20real-time%20on%20embedded%20devices.%20In%20this%20work%2C%20we%20advocate%20for%0Ahyperbolic%20learning%20to%20enable%20crowd%20navigation%20and%20we%20introduce%20Hyp2Nav.%0ADifferent%20from%20conventional%20reinforcement%20learning-based%20crowd%20navigation%0Amethods%2C%20Hyp2Nav%20leverages%20the%20intrinsic%20properties%20of%20hyperbolic%20geometry%20to%0Abetter%20encode%20the%20hierarchical%20nature%20of%20decision-making%20processes%20in%0Anavigation%20tasks.%20We%20propose%20a%20hyperbolic%20policy%20model%20and%20a%20hyperbolic%0Acuriosity%20module%20that%20results%20in%20effective%20social%20navigation%2C%20best%20success%0Arates%2C%20and%20returns%20across%20multiple%20simulation%20settings%2C%20using%20up%20to%206%20times%0Afewer%20parameters%20than%20competitor%20state-of-the-art%20models.%20With%20our%20approach%2C%20it%0Abecomes%20even%20possible%20to%20obtain%20policies%20that%20work%20in%202-dimensional%20embedding%0Aspaces%2C%20opening%20up%20new%20possibilities%20for%20low-resource%20crowd%20navigation%20and%0Amodel%20interpretability.%20Insightfully%2C%20the%20internal%20hyperbolic%20representation%20of%0AHyp2Nav%20correlates%20with%20how%20much%20attention%20the%20robot%20pays%20to%20the%20surrounding%0Acrowds%2C%20e.g.%20due%20to%20multiple%20people%20occluding%20its%20pathway%20or%20to%20a%20few%20of%20them%0Ashowing%20colliding%20plans%2C%20rather%20than%20to%20its%20own%20planned%20route.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13567v1&entry.124074799=Read"},
{"title": "Exploring AI-enhanced Shared Control for an Assistive Robotic Arm", "author": "Max Pascher and Kirill Kronhardt and Jan Freienstein and Jens Gerken", "abstract": "  Assistive technologies and in particular assistive robotic arms have the\npotential to enable people with motor impairments to live a self-determined\nlife. More and more of these systems have become available for end users in\nrecent years, such as the Kinova Jaco robotic arm. However, they mostly require\ncomplex manual control, which can overwhelm users. As a result, researchers\nhave explored ways to let such robots act autonomously. However, at least for\nthis specific group of users, such an approach has shown to be futile. Here,\nusers want to stay in control to achieve a higher level of personal autonomy,\nto which an autonomous robot runs counter. In our research, we explore how\nArtifical Intelligence (AI) can be integrated into a shared control paradigm.\nIn particular, we focus on the consequential requirements for the interface\nbetween human and robot and how we can keep humans in the loop while still\nsignificantly reducing the mental load and required motor skills.\n", "link": "http://arxiv.org/abs/2306.13509v3", "date": "2024-07-18", "relevancy": 2.2572, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6145}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5857}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20AI-enhanced%20Shared%20Control%20for%20an%20Assistive%20Robotic%20Arm&body=Title%3A%20Exploring%20AI-enhanced%20Shared%20Control%20for%20an%20Assistive%20Robotic%20Arm%0AAuthor%3A%20Max%20Pascher%20and%20Kirill%20Kronhardt%20and%20Jan%20Freienstein%20and%20Jens%20Gerken%0AAbstract%3A%20%20%20Assistive%20technologies%20and%20in%20particular%20assistive%20robotic%20arms%20have%20the%0Apotential%20to%20enable%20people%20with%20motor%20impairments%20to%20live%20a%20self-determined%0Alife.%20More%20and%20more%20of%20these%20systems%20have%20become%20available%20for%20end%20users%20in%0Arecent%20years%2C%20such%20as%20the%20Kinova%20Jaco%20robotic%20arm.%20However%2C%20they%20mostly%20require%0Acomplex%20manual%20control%2C%20which%20can%20overwhelm%20users.%20As%20a%20result%2C%20researchers%0Ahave%20explored%20ways%20to%20let%20such%20robots%20act%20autonomously.%20However%2C%20at%20least%20for%0Athis%20specific%20group%20of%20users%2C%20such%20an%20approach%20has%20shown%20to%20be%20futile.%20Here%2C%0Ausers%20want%20to%20stay%20in%20control%20to%20achieve%20a%20higher%20level%20of%20personal%20autonomy%2C%0Ato%20which%20an%20autonomous%20robot%20runs%20counter.%20In%20our%20research%2C%20we%20explore%20how%0AArtifical%20Intelligence%20%28AI%29%20can%20be%20integrated%20into%20a%20shared%20control%20paradigm.%0AIn%20particular%2C%20we%20focus%20on%20the%20consequential%20requirements%20for%20the%20interface%0Abetween%20human%20and%20robot%20and%20how%20we%20can%20keep%20humans%20in%20the%20loop%20while%20still%0Asignificantly%20reducing%20the%20mental%20load%20and%20required%20motor%20skills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.13509v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520AI-enhanced%2520Shared%2520Control%2520for%2520an%2520Assistive%2520Robotic%2520Arm%26entry.906535625%3DMax%2520Pascher%2520and%2520Kirill%2520Kronhardt%2520and%2520Jan%2520Freienstein%2520and%2520Jens%2520Gerken%26entry.1292438233%3D%2520%2520Assistive%2520technologies%2520and%2520in%2520particular%2520assistive%2520robotic%2520arms%2520have%2520the%250Apotential%2520to%2520enable%2520people%2520with%2520motor%2520impairments%2520to%2520live%2520a%2520self-determined%250Alife.%2520More%2520and%2520more%2520of%2520these%2520systems%2520have%2520become%2520available%2520for%2520end%2520users%2520in%250Arecent%2520years%252C%2520such%2520as%2520the%2520Kinova%2520Jaco%2520robotic%2520arm.%2520However%252C%2520they%2520mostly%2520require%250Acomplex%2520manual%2520control%252C%2520which%2520can%2520overwhelm%2520users.%2520As%2520a%2520result%252C%2520researchers%250Ahave%2520explored%2520ways%2520to%2520let%2520such%2520robots%2520act%2520autonomously.%2520However%252C%2520at%2520least%2520for%250Athis%2520specific%2520group%2520of%2520users%252C%2520such%2520an%2520approach%2520has%2520shown%2520to%2520be%2520futile.%2520Here%252C%250Ausers%2520want%2520to%2520stay%2520in%2520control%2520to%2520achieve%2520a%2520higher%2520level%2520of%2520personal%2520autonomy%252C%250Ato%2520which%2520an%2520autonomous%2520robot%2520runs%2520counter.%2520In%2520our%2520research%252C%2520we%2520explore%2520how%250AArtifical%2520Intelligence%2520%2528AI%2529%2520can%2520be%2520integrated%2520into%2520a%2520shared%2520control%2520paradigm.%250AIn%2520particular%252C%2520we%2520focus%2520on%2520the%2520consequential%2520requirements%2520for%2520the%2520interface%250Abetween%2520human%2520and%2520robot%2520and%2520how%2520we%2520can%2520keep%2520humans%2520in%2520the%2520loop%2520while%2520still%250Asignificantly%2520reducing%2520the%2520mental%2520load%2520and%2520required%2520motor%2520skills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.13509v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20AI-enhanced%20Shared%20Control%20for%20an%20Assistive%20Robotic%20Arm&entry.906535625=Max%20Pascher%20and%20Kirill%20Kronhardt%20and%20Jan%20Freienstein%20and%20Jens%20Gerken&entry.1292438233=%20%20Assistive%20technologies%20and%20in%20particular%20assistive%20robotic%20arms%20have%20the%0Apotential%20to%20enable%20people%20with%20motor%20impairments%20to%20live%20a%20self-determined%0Alife.%20More%20and%20more%20of%20these%20systems%20have%20become%20available%20for%20end%20users%20in%0Arecent%20years%2C%20such%20as%20the%20Kinova%20Jaco%20robotic%20arm.%20However%2C%20they%20mostly%20require%0Acomplex%20manual%20control%2C%20which%20can%20overwhelm%20users.%20As%20a%20result%2C%20researchers%0Ahave%20explored%20ways%20to%20let%20such%20robots%20act%20autonomously.%20However%2C%20at%20least%20for%0Athis%20specific%20group%20of%20users%2C%20such%20an%20approach%20has%20shown%20to%20be%20futile.%20Here%2C%0Ausers%20want%20to%20stay%20in%20control%20to%20achieve%20a%20higher%20level%20of%20personal%20autonomy%2C%0Ato%20which%20an%20autonomous%20robot%20runs%20counter.%20In%20our%20research%2C%20we%20explore%20how%0AArtifical%20Intelligence%20%28AI%29%20can%20be%20integrated%20into%20a%20shared%20control%20paradigm.%0AIn%20particular%2C%20we%20focus%20on%20the%20consequential%20requirements%20for%20the%20interface%0Abetween%20human%20and%20robot%20and%20how%20we%20can%20keep%20humans%20in%20the%20loop%20while%20still%0Asignificantly%20reducing%20the%20mental%20load%20and%20required%20motor%20skills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.13509v3&entry.124074799=Read"},
{"title": "Pose-guided multi-task video transformer for driver action recognition", "author": "Ricardo Pizarro and Roberto Valle and Luis Miguel Bergasa and Jos\u00e9 M. Buenaposada and Luis Baumela", "abstract": "  We investigate the task of identifying situations of distracted driving\nthrough analysis of in-car videos. To tackle this challenge we introduce a\nmulti-task video transformer that predicts both distracted actions and driver\npose. Leveraging VideoMAEv2, a large pre-trained architecture, our approach\nincorporates semantic information from human keypoint locations to enhance\naction recognition and decrease computational overhead by minimizing the number\nof spatio-temporal tokens. By guiding token selection with pose and class\ninformation, we notably reduce the model's computational requirements while\npreserving the baseline accuracy. Our model surpasses existing state-of-the art\nresults in driver action recognition while exhibiting superior efficiency\ncompared to current video transformer-based approaches.\n", "link": "http://arxiv.org/abs/2407.13750v1", "date": "2024-07-18", "relevancy": 2.2326, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5576}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose-guided%20multi-task%20video%20transformer%20for%20driver%20action%20recognition&body=Title%3A%20Pose-guided%20multi-task%20video%20transformer%20for%20driver%20action%20recognition%0AAuthor%3A%20Ricardo%20Pizarro%20and%20Roberto%20Valle%20and%20Luis%20Miguel%20Bergasa%20and%20Jos%C3%A9%20M.%20Buenaposada%20and%20Luis%20Baumela%0AAbstract%3A%20%20%20We%20investigate%20the%20task%20of%20identifying%20situations%20of%20distracted%20driving%0Athrough%20analysis%20of%20in-car%20videos.%20To%20tackle%20this%20challenge%20we%20introduce%20a%0Amulti-task%20video%20transformer%20that%20predicts%20both%20distracted%20actions%20and%20driver%0Apose.%20Leveraging%20VideoMAEv2%2C%20a%20large%20pre-trained%20architecture%2C%20our%20approach%0Aincorporates%20semantic%20information%20from%20human%20keypoint%20locations%20to%20enhance%0Aaction%20recognition%20and%20decrease%20computational%20overhead%20by%20minimizing%20the%20number%0Aof%20spatio-temporal%20tokens.%20By%20guiding%20token%20selection%20with%20pose%20and%20class%0Ainformation%2C%20we%20notably%20reduce%20the%20model%27s%20computational%20requirements%20while%0Apreserving%20the%20baseline%20accuracy.%20Our%20model%20surpasses%20existing%20state-of-the%20art%0Aresults%20in%20driver%20action%20recognition%20while%20exhibiting%20superior%20efficiency%0Acompared%20to%20current%20video%20transformer-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose-guided%2520multi-task%2520video%2520transformer%2520for%2520driver%2520action%2520recognition%26entry.906535625%3DRicardo%2520Pizarro%2520and%2520Roberto%2520Valle%2520and%2520Luis%2520Miguel%2520Bergasa%2520and%2520Jos%25C3%25A9%2520M.%2520Buenaposada%2520and%2520Luis%2520Baumela%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520task%2520of%2520identifying%2520situations%2520of%2520distracted%2520driving%250Athrough%2520analysis%2520of%2520in-car%2520videos.%2520To%2520tackle%2520this%2520challenge%2520we%2520introduce%2520a%250Amulti-task%2520video%2520transformer%2520that%2520predicts%2520both%2520distracted%2520actions%2520and%2520driver%250Apose.%2520Leveraging%2520VideoMAEv2%252C%2520a%2520large%2520pre-trained%2520architecture%252C%2520our%2520approach%250Aincorporates%2520semantic%2520information%2520from%2520human%2520keypoint%2520locations%2520to%2520enhance%250Aaction%2520recognition%2520and%2520decrease%2520computational%2520overhead%2520by%2520minimizing%2520the%2520number%250Aof%2520spatio-temporal%2520tokens.%2520By%2520guiding%2520token%2520selection%2520with%2520pose%2520and%2520class%250Ainformation%252C%2520we%2520notably%2520reduce%2520the%2520model%2527s%2520computational%2520requirements%2520while%250Apreserving%2520the%2520baseline%2520accuracy.%2520Our%2520model%2520surpasses%2520existing%2520state-of-the%2520art%250Aresults%2520in%2520driver%2520action%2520recognition%2520while%2520exhibiting%2520superior%2520efficiency%250Acompared%2520to%2520current%2520video%2520transformer-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose-guided%20multi-task%20video%20transformer%20for%20driver%20action%20recognition&entry.906535625=Ricardo%20Pizarro%20and%20Roberto%20Valle%20and%20Luis%20Miguel%20Bergasa%20and%20Jos%C3%A9%20M.%20Buenaposada%20and%20Luis%20Baumela&entry.1292438233=%20%20We%20investigate%20the%20task%20of%20identifying%20situations%20of%20distracted%20driving%0Athrough%20analysis%20of%20in-car%20videos.%20To%20tackle%20this%20challenge%20we%20introduce%20a%0Amulti-task%20video%20transformer%20that%20predicts%20both%20distracted%20actions%20and%20driver%0Apose.%20Leveraging%20VideoMAEv2%2C%20a%20large%20pre-trained%20architecture%2C%20our%20approach%0Aincorporates%20semantic%20information%20from%20human%20keypoint%20locations%20to%20enhance%0Aaction%20recognition%20and%20decrease%20computational%20overhead%20by%20minimizing%20the%20number%0Aof%20spatio-temporal%20tokens.%20By%20guiding%20token%20selection%20with%20pose%20and%20class%0Ainformation%2C%20we%20notably%20reduce%20the%20model%27s%20computational%20requirements%20while%0Apreserving%20the%20baseline%20accuracy.%20Our%20model%20surpasses%20existing%20state-of-the%20art%0Aresults%20in%20driver%20action%20recognition%20while%20exhibiting%20superior%20efficiency%0Acompared%20to%20current%20video%20transformer-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13750v1&entry.124074799=Read"},
{"title": "Audio-driven Talking Face Generation with Stabilized Synchronization\n  Loss", "author": "Dogucan Yaman and Fevziye Irem Eyiokur and Leonard B\u00e4rmann and Hazim Kemal Ekenel and Alexander Waibel", "abstract": "  Talking face generation aims to create realistic videos with accurate lip\nsynchronization and high visual quality, using given audio and reference video\nwhile preserving identity and visual characteristics. In this paper, we start\nby identifying several issues with existing synchronization learning methods.\nThese involve unstable training, lip synchronization, and visual quality issues\ncaused by lip-sync loss, SyncNet, and lip leaking from the identity reference.\nTo address these issues, we first tackle the lip leaking problem by introducing\na silent-lip generator, which changes the lips of the identity reference to\nalleviate leakage. We then introduce stabilized synchronization loss and\nAVSyncNet to overcome problems caused by lip-sync loss and SyncNet. Experiments\nshow that our model outperforms state-of-the-art methods in both visual quality\nand lip synchronization. Comprehensive ablation studies further validate our\nindividual contributions and their cohesive effects.\n", "link": "http://arxiv.org/abs/2307.09368v3", "date": "2024-07-18", "relevancy": 2.224, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5618}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5544}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-driven%20Talking%20Face%20Generation%20with%20Stabilized%20Synchronization%0A%20%20Loss&body=Title%3A%20Audio-driven%20Talking%20Face%20Generation%20with%20Stabilized%20Synchronization%0A%20%20Loss%0AAuthor%3A%20Dogucan%20Yaman%20and%20Fevziye%20Irem%20Eyiokur%20and%20Leonard%20B%C3%A4rmann%20and%20Hazim%20Kemal%20Ekenel%20and%20Alexander%20Waibel%0AAbstract%3A%20%20%20Talking%20face%20generation%20aims%20to%20create%20realistic%20videos%20with%20accurate%20lip%0Asynchronization%20and%20high%20visual%20quality%2C%20using%20given%20audio%20and%20reference%20video%0Awhile%20preserving%20identity%20and%20visual%20characteristics.%20In%20this%20paper%2C%20we%20start%0Aby%20identifying%20several%20issues%20with%20existing%20synchronization%20learning%20methods.%0AThese%20involve%20unstable%20training%2C%20lip%20synchronization%2C%20and%20visual%20quality%20issues%0Acaused%20by%20lip-sync%20loss%2C%20SyncNet%2C%20and%20lip%20leaking%20from%20the%20identity%20reference.%0ATo%20address%20these%20issues%2C%20we%20first%20tackle%20the%20lip%20leaking%20problem%20by%20introducing%0Aa%20silent-lip%20generator%2C%20which%20changes%20the%20lips%20of%20the%20identity%20reference%20to%0Aalleviate%20leakage.%20We%20then%20introduce%20stabilized%20synchronization%20loss%20and%0AAVSyncNet%20to%20overcome%20problems%20caused%20by%20lip-sync%20loss%20and%20SyncNet.%20Experiments%0Ashow%20that%20our%20model%20outperforms%20state-of-the-art%20methods%20in%20both%20visual%20quality%0Aand%20lip%20synchronization.%20Comprehensive%20ablation%20studies%20further%20validate%20our%0Aindividual%20contributions%20and%20their%20cohesive%20effects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09368v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-driven%2520Talking%2520Face%2520Generation%2520with%2520Stabilized%2520Synchronization%250A%2520%2520Loss%26entry.906535625%3DDogucan%2520Yaman%2520and%2520Fevziye%2520Irem%2520Eyiokur%2520and%2520Leonard%2520B%25C3%25A4rmann%2520and%2520Hazim%2520Kemal%2520Ekenel%2520and%2520Alexander%2520Waibel%26entry.1292438233%3D%2520%2520Talking%2520face%2520generation%2520aims%2520to%2520create%2520realistic%2520videos%2520with%2520accurate%2520lip%250Asynchronization%2520and%2520high%2520visual%2520quality%252C%2520using%2520given%2520audio%2520and%2520reference%2520video%250Awhile%2520preserving%2520identity%2520and%2520visual%2520characteristics.%2520In%2520this%2520paper%252C%2520we%2520start%250Aby%2520identifying%2520several%2520issues%2520with%2520existing%2520synchronization%2520learning%2520methods.%250AThese%2520involve%2520unstable%2520training%252C%2520lip%2520synchronization%252C%2520and%2520visual%2520quality%2520issues%250Acaused%2520by%2520lip-sync%2520loss%252C%2520SyncNet%252C%2520and%2520lip%2520leaking%2520from%2520the%2520identity%2520reference.%250ATo%2520address%2520these%2520issues%252C%2520we%2520first%2520tackle%2520the%2520lip%2520leaking%2520problem%2520by%2520introducing%250Aa%2520silent-lip%2520generator%252C%2520which%2520changes%2520the%2520lips%2520of%2520the%2520identity%2520reference%2520to%250Aalleviate%2520leakage.%2520We%2520then%2520introduce%2520stabilized%2520synchronization%2520loss%2520and%250AAVSyncNet%2520to%2520overcome%2520problems%2520caused%2520by%2520lip-sync%2520loss%2520and%2520SyncNet.%2520Experiments%250Ashow%2520that%2520our%2520model%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520visual%2520quality%250Aand%2520lip%2520synchronization.%2520Comprehensive%2520ablation%2520studies%2520further%2520validate%2520our%250Aindividual%2520contributions%2520and%2520their%2520cohesive%2520effects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.09368v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-driven%20Talking%20Face%20Generation%20with%20Stabilized%20Synchronization%0A%20%20Loss&entry.906535625=Dogucan%20Yaman%20and%20Fevziye%20Irem%20Eyiokur%20and%20Leonard%20B%C3%A4rmann%20and%20Hazim%20Kemal%20Ekenel%20and%20Alexander%20Waibel&entry.1292438233=%20%20Talking%20face%20generation%20aims%20to%20create%20realistic%20videos%20with%20accurate%20lip%0Asynchronization%20and%20high%20visual%20quality%2C%20using%20given%20audio%20and%20reference%20video%0Awhile%20preserving%20identity%20and%20visual%20characteristics.%20In%20this%20paper%2C%20we%20start%0Aby%20identifying%20several%20issues%20with%20existing%20synchronization%20learning%20methods.%0AThese%20involve%20unstable%20training%2C%20lip%20synchronization%2C%20and%20visual%20quality%20issues%0Acaused%20by%20lip-sync%20loss%2C%20SyncNet%2C%20and%20lip%20leaking%20from%20the%20identity%20reference.%0ATo%20address%20these%20issues%2C%20we%20first%20tackle%20the%20lip%20leaking%20problem%20by%20introducing%0Aa%20silent-lip%20generator%2C%20which%20changes%20the%20lips%20of%20the%20identity%20reference%20to%0Aalleviate%20leakage.%20We%20then%20introduce%20stabilized%20synchronization%20loss%20and%0AAVSyncNet%20to%20overcome%20problems%20caused%20by%20lip-sync%20loss%20and%20SyncNet.%20Experiments%0Ashow%20that%20our%20model%20outperforms%20state-of-the-art%20methods%20in%20both%20visual%20quality%0Aand%20lip%20synchronization.%20Comprehensive%20ablation%20studies%20further%20validate%20our%0Aindividual%20contributions%20and%20their%20cohesive%20effects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09368v3&entry.124074799=Read"},
{"title": "UMBRAE: Unified Multimodal Brain Decoding", "author": "Weihao Xia and Raoul de Charette and Cengiz \u00d6ztireli and Jing-Hao Xue", "abstract": "  We address prevailing challenges of the brain-powered research, departing\nfrom the observation that the literature hardly recover accurate spatial\ninformation and require subject-specific models. To address these challenges,\nwe propose UMBRAE, a unified multimodal decoding of brain signals. First, to\nextract instance-level conceptual and spatial details from neural signals, we\nintroduce an efficient universal brain encoder for multimodal-brain alignment\nand recover object descriptions at multiple levels of granularity from\nsubsequent multimodal large language model (MLLM). Second, we introduce a\ncross-subject training strategy mapping subject-specific features to a common\nfeature space. This allows a model to be trained on multiple subjects without\nextra resources, even yielding superior results compared to subject-specific\nmodels. Further, we demonstrate this supports weakly-supervised adaptation to\nnew subjects, with only a fraction of the total training data. Experiments\ndemonstrate that UMBRAE not only achieves superior results in the newly\nintroduced tasks but also outperforms methods in well established tasks. To\nassess our method, we construct and share with the community a comprehensive\nbrain understanding benchmark BrainHub. Our code and benchmark are available at\nhttps://weihaox.github.io/UMBRAE.\n", "link": "http://arxiv.org/abs/2404.07202v2", "date": "2024-07-18", "relevancy": 2.2139, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5545}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UMBRAE%3A%20Unified%20Multimodal%20Brain%20Decoding&body=Title%3A%20UMBRAE%3A%20Unified%20Multimodal%20Brain%20Decoding%0AAuthor%3A%20Weihao%20Xia%20and%20Raoul%20de%20Charette%20and%20Cengiz%20%C3%96ztireli%20and%20Jing-Hao%20Xue%0AAbstract%3A%20%20%20We%20address%20prevailing%20challenges%20of%20the%20brain-powered%20research%2C%20departing%0Afrom%20the%20observation%20that%20the%20literature%20hardly%20recover%20accurate%20spatial%0Ainformation%20and%20require%20subject-specific%20models.%20To%20address%20these%20challenges%2C%0Awe%20propose%20UMBRAE%2C%20a%20unified%20multimodal%20decoding%20of%20brain%20signals.%20First%2C%20to%0Aextract%20instance-level%20conceptual%20and%20spatial%20details%20from%20neural%20signals%2C%20we%0Aintroduce%20an%20efficient%20universal%20brain%20encoder%20for%20multimodal-brain%20alignment%0Aand%20recover%20object%20descriptions%20at%20multiple%20levels%20of%20granularity%20from%0Asubsequent%20multimodal%20large%20language%20model%20%28MLLM%29.%20Second%2C%20we%20introduce%20a%0Across-subject%20training%20strategy%20mapping%20subject-specific%20features%20to%20a%20common%0Afeature%20space.%20This%20allows%20a%20model%20to%20be%20trained%20on%20multiple%20subjects%20without%0Aextra%20resources%2C%20even%20yielding%20superior%20results%20compared%20to%20subject-specific%0Amodels.%20Further%2C%20we%20demonstrate%20this%20supports%20weakly-supervised%20adaptation%20to%0Anew%20subjects%2C%20with%20only%20a%20fraction%20of%20the%20total%20training%20data.%20Experiments%0Ademonstrate%20that%20UMBRAE%20not%20only%20achieves%20superior%20results%20in%20the%20newly%0Aintroduced%20tasks%20but%20also%20outperforms%20methods%20in%20well%20established%20tasks.%20To%0Aassess%20our%20method%2C%20we%20construct%20and%20share%20with%20the%20community%20a%20comprehensive%0Abrain%20understanding%20benchmark%20BrainHub.%20Our%20code%20and%20benchmark%20are%20available%20at%0Ahttps%3A//weihaox.github.io/UMBRAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUMBRAE%253A%2520Unified%2520Multimodal%2520Brain%2520Decoding%26entry.906535625%3DWeihao%2520Xia%2520and%2520Raoul%2520de%2520Charette%2520and%2520Cengiz%2520%25C3%2596ztireli%2520and%2520Jing-Hao%2520Xue%26entry.1292438233%3D%2520%2520We%2520address%2520prevailing%2520challenges%2520of%2520the%2520brain-powered%2520research%252C%2520departing%250Afrom%2520the%2520observation%2520that%2520the%2520literature%2520hardly%2520recover%2520accurate%2520spatial%250Ainformation%2520and%2520require%2520subject-specific%2520models.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520UMBRAE%252C%2520a%2520unified%2520multimodal%2520decoding%2520of%2520brain%2520signals.%2520First%252C%2520to%250Aextract%2520instance-level%2520conceptual%2520and%2520spatial%2520details%2520from%2520neural%2520signals%252C%2520we%250Aintroduce%2520an%2520efficient%2520universal%2520brain%2520encoder%2520for%2520multimodal-brain%2520alignment%250Aand%2520recover%2520object%2520descriptions%2520at%2520multiple%2520levels%2520of%2520granularity%2520from%250Asubsequent%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529.%2520Second%252C%2520we%2520introduce%2520a%250Across-subject%2520training%2520strategy%2520mapping%2520subject-specific%2520features%2520to%2520a%2520common%250Afeature%2520space.%2520This%2520allows%2520a%2520model%2520to%2520be%2520trained%2520on%2520multiple%2520subjects%2520without%250Aextra%2520resources%252C%2520even%2520yielding%2520superior%2520results%2520compared%2520to%2520subject-specific%250Amodels.%2520Further%252C%2520we%2520demonstrate%2520this%2520supports%2520weakly-supervised%2520adaptation%2520to%250Anew%2520subjects%252C%2520with%2520only%2520a%2520fraction%2520of%2520the%2520total%2520training%2520data.%2520Experiments%250Ademonstrate%2520that%2520UMBRAE%2520not%2520only%2520achieves%2520superior%2520results%2520in%2520the%2520newly%250Aintroduced%2520tasks%2520but%2520also%2520outperforms%2520methods%2520in%2520well%2520established%2520tasks.%2520To%250Aassess%2520our%2520method%252C%2520we%2520construct%2520and%2520share%2520with%2520the%2520community%2520a%2520comprehensive%250Abrain%2520understanding%2520benchmark%2520BrainHub.%2520Our%2520code%2520and%2520benchmark%2520are%2520available%2520at%250Ahttps%253A//weihaox.github.io/UMBRAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UMBRAE%3A%20Unified%20Multimodal%20Brain%20Decoding&entry.906535625=Weihao%20Xia%20and%20Raoul%20de%20Charette%20and%20Cengiz%20%C3%96ztireli%20and%20Jing-Hao%20Xue&entry.1292438233=%20%20We%20address%20prevailing%20challenges%20of%20the%20brain-powered%20research%2C%20departing%0Afrom%20the%20observation%20that%20the%20literature%20hardly%20recover%20accurate%20spatial%0Ainformation%20and%20require%20subject-specific%20models.%20To%20address%20these%20challenges%2C%0Awe%20propose%20UMBRAE%2C%20a%20unified%20multimodal%20decoding%20of%20brain%20signals.%20First%2C%20to%0Aextract%20instance-level%20conceptual%20and%20spatial%20details%20from%20neural%20signals%2C%20we%0Aintroduce%20an%20efficient%20universal%20brain%20encoder%20for%20multimodal-brain%20alignment%0Aand%20recover%20object%20descriptions%20at%20multiple%20levels%20of%20granularity%20from%0Asubsequent%20multimodal%20large%20language%20model%20%28MLLM%29.%20Second%2C%20we%20introduce%20a%0Across-subject%20training%20strategy%20mapping%20subject-specific%20features%20to%20a%20common%0Afeature%20space.%20This%20allows%20a%20model%20to%20be%20trained%20on%20multiple%20subjects%20without%0Aextra%20resources%2C%20even%20yielding%20superior%20results%20compared%20to%20subject-specific%0Amodels.%20Further%2C%20we%20demonstrate%20this%20supports%20weakly-supervised%20adaptation%20to%0Anew%20subjects%2C%20with%20only%20a%20fraction%20of%20the%20total%20training%20data.%20Experiments%0Ademonstrate%20that%20UMBRAE%20not%20only%20achieves%20superior%20results%20in%20the%20newly%0Aintroduced%20tasks%20but%20also%20outperforms%20methods%20in%20well%20established%20tasks.%20To%0Aassess%20our%20method%2C%20we%20construct%20and%20share%20with%20the%20community%20a%20comprehensive%0Abrain%20understanding%20benchmark%20BrainHub.%20Our%20code%20and%20benchmark%20are%20available%20at%0Ahttps%3A//weihaox.github.io/UMBRAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07202v2&entry.124074799=Read"},
{"title": "RingID: Rethinking Tree-Ring Watermarking for Enhanced Multi-Key\n  Identification", "author": "Hai Ci and Pei Yang and Yiren Song and Mike Zheng Shou", "abstract": "  We revisit Tree-Ring Watermarking, a recent diffusion model watermarking\nmethod that demonstrates great robustness to various attacks. We conduct an\nin-depth study on it and reveal that the distribution shift unintentionally\nintroduced by the watermarking process, apart from watermark pattern matching,\ncontributes to its exceptional robustness. Our investigation further exposes\ninherent flaws in its original design, particularly in its ability to identify\nmultiple distinct keys, where distribution shift offers no assistance. Based on\nthese findings and analysis, we present RingID for enhanced multi-key\nidentification. It consists of a novel multi-channel heterogeneous watermarking\napproach designed to seamlessly amalgamate distinctive advantages from diverse\nwatermarks. Coupled with a series of suggested enhancements, RingID exhibits\nsubstantial advancements in multi-key identification. Github Page:\nhttps://github.com/showlab/RingID\n", "link": "http://arxiv.org/abs/2404.14055v3", "date": "2024-07-18", "relevancy": 2.213, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4888}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4257}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RingID%3A%20Rethinking%20Tree-Ring%20Watermarking%20for%20Enhanced%20Multi-Key%0A%20%20Identification&body=Title%3A%20RingID%3A%20Rethinking%20Tree-Ring%20Watermarking%20for%20Enhanced%20Multi-Key%0A%20%20Identification%0AAuthor%3A%20Hai%20Ci%20and%20Pei%20Yang%20and%20Yiren%20Song%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20We%20revisit%20Tree-Ring%20Watermarking%2C%20a%20recent%20diffusion%20model%20watermarking%0Amethod%20that%20demonstrates%20great%20robustness%20to%20various%20attacks.%20We%20conduct%20an%0Ain-depth%20study%20on%20it%20and%20reveal%20that%20the%20distribution%20shift%20unintentionally%0Aintroduced%20by%20the%20watermarking%20process%2C%20apart%20from%20watermark%20pattern%20matching%2C%0Acontributes%20to%20its%20exceptional%20robustness.%20Our%20investigation%20further%20exposes%0Ainherent%20flaws%20in%20its%20original%20design%2C%20particularly%20in%20its%20ability%20to%20identify%0Amultiple%20distinct%20keys%2C%20where%20distribution%20shift%20offers%20no%20assistance.%20Based%20on%0Athese%20findings%20and%20analysis%2C%20we%20present%20RingID%20for%20enhanced%20multi-key%0Aidentification.%20It%20consists%20of%20a%20novel%20multi-channel%20heterogeneous%20watermarking%0Aapproach%20designed%20to%20seamlessly%20amalgamate%20distinctive%20advantages%20from%20diverse%0Awatermarks.%20Coupled%20with%20a%20series%20of%20suggested%20enhancements%2C%20RingID%20exhibits%0Asubstantial%20advancements%20in%20multi-key%20identification.%20Github%20Page%3A%0Ahttps%3A//github.com/showlab/RingID%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14055v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRingID%253A%2520Rethinking%2520Tree-Ring%2520Watermarking%2520for%2520Enhanced%2520Multi-Key%250A%2520%2520Identification%26entry.906535625%3DHai%2520Ci%2520and%2520Pei%2520Yang%2520and%2520Yiren%2520Song%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520We%2520revisit%2520Tree-Ring%2520Watermarking%252C%2520a%2520recent%2520diffusion%2520model%2520watermarking%250Amethod%2520that%2520demonstrates%2520great%2520robustness%2520to%2520various%2520attacks.%2520We%2520conduct%2520an%250Ain-depth%2520study%2520on%2520it%2520and%2520reveal%2520that%2520the%2520distribution%2520shift%2520unintentionally%250Aintroduced%2520by%2520the%2520watermarking%2520process%252C%2520apart%2520from%2520watermark%2520pattern%2520matching%252C%250Acontributes%2520to%2520its%2520exceptional%2520robustness.%2520Our%2520investigation%2520further%2520exposes%250Ainherent%2520flaws%2520in%2520its%2520original%2520design%252C%2520particularly%2520in%2520its%2520ability%2520to%2520identify%250Amultiple%2520distinct%2520keys%252C%2520where%2520distribution%2520shift%2520offers%2520no%2520assistance.%2520Based%2520on%250Athese%2520findings%2520and%2520analysis%252C%2520we%2520present%2520RingID%2520for%2520enhanced%2520multi-key%250Aidentification.%2520It%2520consists%2520of%2520a%2520novel%2520multi-channel%2520heterogeneous%2520watermarking%250Aapproach%2520designed%2520to%2520seamlessly%2520amalgamate%2520distinctive%2520advantages%2520from%2520diverse%250Awatermarks.%2520Coupled%2520with%2520a%2520series%2520of%2520suggested%2520enhancements%252C%2520RingID%2520exhibits%250Asubstantial%2520advancements%2520in%2520multi-key%2520identification.%2520Github%2520Page%253A%250Ahttps%253A//github.com/showlab/RingID%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14055v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RingID%3A%20Rethinking%20Tree-Ring%20Watermarking%20for%20Enhanced%20Multi-Key%0A%20%20Identification&entry.906535625=Hai%20Ci%20and%20Pei%20Yang%20and%20Yiren%20Song%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20We%20revisit%20Tree-Ring%20Watermarking%2C%20a%20recent%20diffusion%20model%20watermarking%0Amethod%20that%20demonstrates%20great%20robustness%20to%20various%20attacks.%20We%20conduct%20an%0Ain-depth%20study%20on%20it%20and%20reveal%20that%20the%20distribution%20shift%20unintentionally%0Aintroduced%20by%20the%20watermarking%20process%2C%20apart%20from%20watermark%20pattern%20matching%2C%0Acontributes%20to%20its%20exceptional%20robustness.%20Our%20investigation%20further%20exposes%0Ainherent%20flaws%20in%20its%20original%20design%2C%20particularly%20in%20its%20ability%20to%20identify%0Amultiple%20distinct%20keys%2C%20where%20distribution%20shift%20offers%20no%20assistance.%20Based%20on%0Athese%20findings%20and%20analysis%2C%20we%20present%20RingID%20for%20enhanced%20multi-key%0Aidentification.%20It%20consists%20of%20a%20novel%20multi-channel%20heterogeneous%20watermarking%0Aapproach%20designed%20to%20seamlessly%20amalgamate%20distinctive%20advantages%20from%20diverse%0Awatermarks.%20Coupled%20with%20a%20series%20of%20suggested%20enhancements%2C%20RingID%20exhibits%0Asubstantial%20advancements%20in%20multi-key%20identification.%20Github%20Page%3A%0Ahttps%3A//github.com/showlab/RingID%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14055v3&entry.124074799=Read"},
{"title": "Training-Free Model Merging for Multi-target Domain Adaptation", "author": "Wenyi Li and Huan-ang Gao and Mingju Gao and Beiwen Tian and Rong Zhi and Hao Zhao", "abstract": "  In this paper, we study multi-target domain adaptation of scene understanding\nmodels. While previous methods achieved commendable results through\ninter-domain consistency losses, they often assumed unrealistic simultaneous\naccess to images from all target domains, overlooking constraints such as data\ntransfer bandwidth limitations and data privacy concerns. Given these\nchallenges, we pose the question: How to merge models adapted independently on\ndistinct domains while bypassing the need for direct access to training data?\nOur solution to this problem involves two components, merging model parameters\nand merging model buffers (i.e., normalization layer statistics). For merging\nmodel parameters, empirical analyses of mode connectivity surprisingly reveal\nthat linear merging suffices when employing the same pretrained backbone\nweights for adapting separate models. For merging model buffers, we model the\nreal-world distribution with a Gaussian prior and estimate new statistics from\nthe buffers of separately trained models. Our method is simple yet effective,\nachieving comparable performance with data combination training baselines,\nwhile eliminating the need for accessing training data. Project page:\nhttps://air-discover.github.io/ModelMerging\n", "link": "http://arxiv.org/abs/2407.13771v1", "date": "2024-07-18", "relevancy": 2.2117, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6003}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5238}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Model%20Merging%20for%20Multi-target%20Domain%20Adaptation&body=Title%3A%20Training-Free%20Model%20Merging%20for%20Multi-target%20Domain%20Adaptation%0AAuthor%3A%20Wenyi%20Li%20and%20Huan-ang%20Gao%20and%20Mingju%20Gao%20and%20Beiwen%20Tian%20and%20Rong%20Zhi%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20multi-target%20domain%20adaptation%20of%20scene%20understanding%0Amodels.%20While%20previous%20methods%20achieved%20commendable%20results%20through%0Ainter-domain%20consistency%20losses%2C%20they%20often%20assumed%20unrealistic%20simultaneous%0Aaccess%20to%20images%20from%20all%20target%20domains%2C%20overlooking%20constraints%20such%20as%20data%0Atransfer%20bandwidth%20limitations%20and%20data%20privacy%20concerns.%20Given%20these%0Achallenges%2C%20we%20pose%20the%20question%3A%20How%20to%20merge%20models%20adapted%20independently%20on%0Adistinct%20domains%20while%20bypassing%20the%20need%20for%20direct%20access%20to%20training%20data%3F%0AOur%20solution%20to%20this%20problem%20involves%20two%20components%2C%20merging%20model%20parameters%0Aand%20merging%20model%20buffers%20%28i.e.%2C%20normalization%20layer%20statistics%29.%20For%20merging%0Amodel%20parameters%2C%20empirical%20analyses%20of%20mode%20connectivity%20surprisingly%20reveal%0Athat%20linear%20merging%20suffices%20when%20employing%20the%20same%20pretrained%20backbone%0Aweights%20for%20adapting%20separate%20models.%20For%20merging%20model%20buffers%2C%20we%20model%20the%0Areal-world%20distribution%20with%20a%20Gaussian%20prior%20and%20estimate%20new%20statistics%20from%0Athe%20buffers%20of%20separately%20trained%20models.%20Our%20method%20is%20simple%20yet%20effective%2C%0Aachieving%20comparable%20performance%20with%20data%20combination%20training%20baselines%2C%0Awhile%20eliminating%20the%20need%20for%20accessing%20training%20data.%20Project%20page%3A%0Ahttps%3A//air-discover.github.io/ModelMerging%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Model%2520Merging%2520for%2520Multi-target%2520Domain%2520Adaptation%26entry.906535625%3DWenyi%2520Li%2520and%2520Huan-ang%2520Gao%2520and%2520Mingju%2520Gao%2520and%2520Beiwen%2520Tian%2520and%2520Rong%2520Zhi%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520multi-target%2520domain%2520adaptation%2520of%2520scene%2520understanding%250Amodels.%2520While%2520previous%2520methods%2520achieved%2520commendable%2520results%2520through%250Ainter-domain%2520consistency%2520losses%252C%2520they%2520often%2520assumed%2520unrealistic%2520simultaneous%250Aaccess%2520to%2520images%2520from%2520all%2520target%2520domains%252C%2520overlooking%2520constraints%2520such%2520as%2520data%250Atransfer%2520bandwidth%2520limitations%2520and%2520data%2520privacy%2520concerns.%2520Given%2520these%250Achallenges%252C%2520we%2520pose%2520the%2520question%253A%2520How%2520to%2520merge%2520models%2520adapted%2520independently%2520on%250Adistinct%2520domains%2520while%2520bypassing%2520the%2520need%2520for%2520direct%2520access%2520to%2520training%2520data%253F%250AOur%2520solution%2520to%2520this%2520problem%2520involves%2520two%2520components%252C%2520merging%2520model%2520parameters%250Aand%2520merging%2520model%2520buffers%2520%2528i.e.%252C%2520normalization%2520layer%2520statistics%2529.%2520For%2520merging%250Amodel%2520parameters%252C%2520empirical%2520analyses%2520of%2520mode%2520connectivity%2520surprisingly%2520reveal%250Athat%2520linear%2520merging%2520suffices%2520when%2520employing%2520the%2520same%2520pretrained%2520backbone%250Aweights%2520for%2520adapting%2520separate%2520models.%2520For%2520merging%2520model%2520buffers%252C%2520we%2520model%2520the%250Areal-world%2520distribution%2520with%2520a%2520Gaussian%2520prior%2520and%2520estimate%2520new%2520statistics%2520from%250Athe%2520buffers%2520of%2520separately%2520trained%2520models.%2520Our%2520method%2520is%2520simple%2520yet%2520effective%252C%250Aachieving%2520comparable%2520performance%2520with%2520data%2520combination%2520training%2520baselines%252C%250Awhile%2520eliminating%2520the%2520need%2520for%2520accessing%2520training%2520data.%2520Project%2520page%253A%250Ahttps%253A//air-discover.github.io/ModelMerging%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Model%20Merging%20for%20Multi-target%20Domain%20Adaptation&entry.906535625=Wenyi%20Li%20and%20Huan-ang%20Gao%20and%20Mingju%20Gao%20and%20Beiwen%20Tian%20and%20Rong%20Zhi%20and%20Hao%20Zhao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20multi-target%20domain%20adaptation%20of%20scene%20understanding%0Amodels.%20While%20previous%20methods%20achieved%20commendable%20results%20through%0Ainter-domain%20consistency%20losses%2C%20they%20often%20assumed%20unrealistic%20simultaneous%0Aaccess%20to%20images%20from%20all%20target%20domains%2C%20overlooking%20constraints%20such%20as%20data%0Atransfer%20bandwidth%20limitations%20and%20data%20privacy%20concerns.%20Given%20these%0Achallenges%2C%20we%20pose%20the%20question%3A%20How%20to%20merge%20models%20adapted%20independently%20on%0Adistinct%20domains%20while%20bypassing%20the%20need%20for%20direct%20access%20to%20training%20data%3F%0AOur%20solution%20to%20this%20problem%20involves%20two%20components%2C%20merging%20model%20parameters%0Aand%20merging%20model%20buffers%20%28i.e.%2C%20normalization%20layer%20statistics%29.%20For%20merging%0Amodel%20parameters%2C%20empirical%20analyses%20of%20mode%20connectivity%20surprisingly%20reveal%0Athat%20linear%20merging%20suffices%20when%20employing%20the%20same%20pretrained%20backbone%0Aweights%20for%20adapting%20separate%20models.%20For%20merging%20model%20buffers%2C%20we%20model%20the%0Areal-world%20distribution%20with%20a%20Gaussian%20prior%20and%20estimate%20new%20statistics%20from%0Athe%20buffers%20of%20separately%20trained%20models.%20Our%20method%20is%20simple%20yet%20effective%2C%0Aachieving%20comparable%20performance%20with%20data%20combination%20training%20baselines%2C%0Awhile%20eliminating%20the%20need%20for%20accessing%20training%20data.%20Project%20page%3A%0Ahttps%3A//air-discover.github.io/ModelMerging%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13771v1&entry.124074799=Read"},
{"title": "SegPoint: Segment Any Point Cloud via Large Language Model", "author": "Shuting He and Henghui Ding and Xudong Jiang and Bihan Wen", "abstract": "  Despite significant progress in 3D point cloud segmentation, existing methods\nprimarily address specific tasks and depend on explicit instructions to\nidentify targets, lacking the capability to infer and understand implicit user\nintentions in a unified framework. In this work, we propose a model, called\nSegPoint, that leverages the reasoning capabilities of a multi-modal Large\nLanguage Model (LLM) to produce point-wise segmentation masks across a diverse\nrange of tasks: 1) 3D instruction segmentation, 2) 3D referring segmentation,\n3) 3D semantic segmentation, and 4) 3D open-vocabulary semantic segmentation.\nTo advance 3D instruction research, we introduce a new benchmark, Instruct3D,\ndesigned to evaluate segmentation performance from complex and implicit\ninstructional texts, featuring 2,565 point cloud-instruction pairs. Our\nexperimental results demonstrate that SegPoint achieves competitive performance\non established benchmarks such as ScanRefer for referring segmentation and\nScanNet for semantic segmentation, while delivering outstanding outcomes on the\nInstruct3D dataset. To our knowledge, SegPoint is the first model to address\nthese varied segmentation tasks within a single framework, achieving\nsatisfactory performance.\n", "link": "http://arxiv.org/abs/2407.13761v1", "date": "2024-07-18", "relevancy": 2.2108, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5733}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5407}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegPoint%3A%20Segment%20Any%20Point%20Cloud%20via%20Large%20Language%20Model&body=Title%3A%20SegPoint%3A%20Segment%20Any%20Point%20Cloud%20via%20Large%20Language%20Model%0AAuthor%3A%20Shuting%20He%20and%20Henghui%20Ding%20and%20Xudong%20Jiang%20and%20Bihan%20Wen%0AAbstract%3A%20%20%20Despite%20significant%20progress%20in%203D%20point%20cloud%20segmentation%2C%20existing%20methods%0Aprimarily%20address%20specific%20tasks%20and%20depend%20on%20explicit%20instructions%20to%0Aidentify%20targets%2C%20lacking%20the%20capability%20to%20infer%20and%20understand%20implicit%20user%0Aintentions%20in%20a%20unified%20framework.%20In%20this%20work%2C%20we%20propose%20a%20model%2C%20called%0ASegPoint%2C%20that%20leverages%20the%20reasoning%20capabilities%20of%20a%20multi-modal%20Large%0ALanguage%20Model%20%28LLM%29%20to%20produce%20point-wise%20segmentation%20masks%20across%20a%20diverse%0Arange%20of%20tasks%3A%201%29%203D%20instruction%20segmentation%2C%202%29%203D%20referring%20segmentation%2C%0A3%29%203D%20semantic%20segmentation%2C%20and%204%29%203D%20open-vocabulary%20semantic%20segmentation.%0ATo%20advance%203D%20instruction%20research%2C%20we%20introduce%20a%20new%20benchmark%2C%20Instruct3D%2C%0Adesigned%20to%20evaluate%20segmentation%20performance%20from%20complex%20and%20implicit%0Ainstructional%20texts%2C%20featuring%202%2C565%20point%20cloud-instruction%20pairs.%20Our%0Aexperimental%20results%20demonstrate%20that%20SegPoint%20achieves%20competitive%20performance%0Aon%20established%20benchmarks%20such%20as%20ScanRefer%20for%20referring%20segmentation%20and%0AScanNet%20for%20semantic%20segmentation%2C%20while%20delivering%20outstanding%20outcomes%20on%20the%0AInstruct3D%20dataset.%20To%20our%20knowledge%2C%20SegPoint%20is%20the%20first%20model%20to%20address%0Athese%20varied%20segmentation%20tasks%20within%20a%20single%20framework%2C%20achieving%0Asatisfactory%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegPoint%253A%2520Segment%2520Any%2520Point%2520Cloud%2520via%2520Large%2520Language%2520Model%26entry.906535625%3DShuting%2520He%2520and%2520Henghui%2520Ding%2520and%2520Xudong%2520Jiang%2520and%2520Bihan%2520Wen%26entry.1292438233%3D%2520%2520Despite%2520significant%2520progress%2520in%25203D%2520point%2520cloud%2520segmentation%252C%2520existing%2520methods%250Aprimarily%2520address%2520specific%2520tasks%2520and%2520depend%2520on%2520explicit%2520instructions%2520to%250Aidentify%2520targets%252C%2520lacking%2520the%2520capability%2520to%2520infer%2520and%2520understand%2520implicit%2520user%250Aintentions%2520in%2520a%2520unified%2520framework.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520model%252C%2520called%250ASegPoint%252C%2520that%2520leverages%2520the%2520reasoning%2520capabilities%2520of%2520a%2520multi-modal%2520Large%250ALanguage%2520Model%2520%2528LLM%2529%2520to%2520produce%2520point-wise%2520segmentation%2520masks%2520across%2520a%2520diverse%250Arange%2520of%2520tasks%253A%25201%2529%25203D%2520instruction%2520segmentation%252C%25202%2529%25203D%2520referring%2520segmentation%252C%250A3%2529%25203D%2520semantic%2520segmentation%252C%2520and%25204%2529%25203D%2520open-vocabulary%2520semantic%2520segmentation.%250ATo%2520advance%25203D%2520instruction%2520research%252C%2520we%2520introduce%2520a%2520new%2520benchmark%252C%2520Instruct3D%252C%250Adesigned%2520to%2520evaluate%2520segmentation%2520performance%2520from%2520complex%2520and%2520implicit%250Ainstructional%2520texts%252C%2520featuring%25202%252C565%2520point%2520cloud-instruction%2520pairs.%2520Our%250Aexperimental%2520results%2520demonstrate%2520that%2520SegPoint%2520achieves%2520competitive%2520performance%250Aon%2520established%2520benchmarks%2520such%2520as%2520ScanRefer%2520for%2520referring%2520segmentation%2520and%250AScanNet%2520for%2520semantic%2520segmentation%252C%2520while%2520delivering%2520outstanding%2520outcomes%2520on%2520the%250AInstruct3D%2520dataset.%2520To%2520our%2520knowledge%252C%2520SegPoint%2520is%2520the%2520first%2520model%2520to%2520address%250Athese%2520varied%2520segmentation%2520tasks%2520within%2520a%2520single%2520framework%252C%2520achieving%250Asatisfactory%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegPoint%3A%20Segment%20Any%20Point%20Cloud%20via%20Large%20Language%20Model&entry.906535625=Shuting%20He%20and%20Henghui%20Ding%20and%20Xudong%20Jiang%20and%20Bihan%20Wen&entry.1292438233=%20%20Despite%20significant%20progress%20in%203D%20point%20cloud%20segmentation%2C%20existing%20methods%0Aprimarily%20address%20specific%20tasks%20and%20depend%20on%20explicit%20instructions%20to%0Aidentify%20targets%2C%20lacking%20the%20capability%20to%20infer%20and%20understand%20implicit%20user%0Aintentions%20in%20a%20unified%20framework.%20In%20this%20work%2C%20we%20propose%20a%20model%2C%20called%0ASegPoint%2C%20that%20leverages%20the%20reasoning%20capabilities%20of%20a%20multi-modal%20Large%0ALanguage%20Model%20%28LLM%29%20to%20produce%20point-wise%20segmentation%20masks%20across%20a%20diverse%0Arange%20of%20tasks%3A%201%29%203D%20instruction%20segmentation%2C%202%29%203D%20referring%20segmentation%2C%0A3%29%203D%20semantic%20segmentation%2C%20and%204%29%203D%20open-vocabulary%20semantic%20segmentation.%0ATo%20advance%203D%20instruction%20research%2C%20we%20introduce%20a%20new%20benchmark%2C%20Instruct3D%2C%0Adesigned%20to%20evaluate%20segmentation%20performance%20from%20complex%20and%20implicit%0Ainstructional%20texts%2C%20featuring%202%2C565%20point%20cloud-instruction%20pairs.%20Our%0Aexperimental%20results%20demonstrate%20that%20SegPoint%20achieves%20competitive%20performance%0Aon%20established%20benchmarks%20such%20as%20ScanRefer%20for%20referring%20segmentation%20and%0AScanNet%20for%20semantic%20segmentation%2C%20while%20delivering%20outstanding%20outcomes%20on%20the%0AInstruct3D%20dataset.%20To%20our%20knowledge%2C%20SegPoint%20is%20the%20first%20model%20to%20address%0Athese%20varied%20segmentation%20tasks%20within%20a%20single%20framework%2C%20achieving%0Asatisfactory%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13761v1&entry.124074799=Read"},
{"title": "Beyond Augmentation: Empowering Model Robustness under Extreme Capture\n  Environments", "author": "Yunpeng Gong and Yongjie Hou and Chuangliang Zhang and Min Jiang", "abstract": "  Person Re-identification (re-ID) in computer vision aims to recognize and\ntrack individuals across different cameras. While previous research has mainly\nfocused on challenges like pose variations and lighting changes, the impact of\nextreme capture conditions is often not adequately addressed. These extreme\nconditions, including varied lighting, camera styles, angles, and image\ndistortions, can significantly affect data distribution and re-ID accuracy.\n  Current research typically improves model generalization under normal\nshooting conditions through data augmentation techniques such as adjusting\nbrightness and contrast. However, these methods pay less attention to the\nrobustness of models under extreme shooting conditions. To tackle this, we\npropose a multi-mode synchronization learning (MMSL) strategy . This approach\ninvolves dividing images into grids, randomly selecting grid blocks, and\napplying data augmentation methods like contrast and brightness adjustments.\nThis process introduces diverse transformations without altering the original\nimage structure, helping the model adapt to extreme variations. This method\nimproves the model's generalization under extreme conditions and enables\nlearning diverse features, thus better addressing the challenges in re-ID.\nExtensive experiments on a simulated test set under extreme conditions have\ndemonstrated the effectiveness of our method. This approach is crucial for\nenhancing model robustness and adaptability in real-world scenarios, supporting\nthe future development of person re-identification technology.\n", "link": "http://arxiv.org/abs/2407.13640v1", "date": "2024-07-18", "relevancy": 2.2093, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5587}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5479}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Augmentation%3A%20Empowering%20Model%20Robustness%20under%20Extreme%20Capture%0A%20%20Environments&body=Title%3A%20Beyond%20Augmentation%3A%20Empowering%20Model%20Robustness%20under%20Extreme%20Capture%0A%20%20Environments%0AAuthor%3A%20Yunpeng%20Gong%20and%20Yongjie%20Hou%20and%20Chuangliang%20Zhang%20and%20Min%20Jiang%0AAbstract%3A%20%20%20Person%20Re-identification%20%28re-ID%29%20in%20computer%20vision%20aims%20to%20recognize%20and%0Atrack%20individuals%20across%20different%20cameras.%20While%20previous%20research%20has%20mainly%0Afocused%20on%20challenges%20like%20pose%20variations%20and%20lighting%20changes%2C%20the%20impact%20of%0Aextreme%20capture%20conditions%20is%20often%20not%20adequately%20addressed.%20These%20extreme%0Aconditions%2C%20including%20varied%20lighting%2C%20camera%20styles%2C%20angles%2C%20and%20image%0Adistortions%2C%20can%20significantly%20affect%20data%20distribution%20and%20re-ID%20accuracy.%0A%20%20Current%20research%20typically%20improves%20model%20generalization%20under%20normal%0Ashooting%20conditions%20through%20data%20augmentation%20techniques%20such%20as%20adjusting%0Abrightness%20and%20contrast.%20However%2C%20these%20methods%20pay%20less%20attention%20to%20the%0Arobustness%20of%20models%20under%20extreme%20shooting%20conditions.%20To%20tackle%20this%2C%20we%0Apropose%20a%20multi-mode%20synchronization%20learning%20%28MMSL%29%20strategy%20.%20This%20approach%0Ainvolves%20dividing%20images%20into%20grids%2C%20randomly%20selecting%20grid%20blocks%2C%20and%0Aapplying%20data%20augmentation%20methods%20like%20contrast%20and%20brightness%20adjustments.%0AThis%20process%20introduces%20diverse%20transformations%20without%20altering%20the%20original%0Aimage%20structure%2C%20helping%20the%20model%20adapt%20to%20extreme%20variations.%20This%20method%0Aimproves%20the%20model%27s%20generalization%20under%20extreme%20conditions%20and%20enables%0Alearning%20diverse%20features%2C%20thus%20better%20addressing%20the%20challenges%20in%20re-ID.%0AExtensive%20experiments%20on%20a%20simulated%20test%20set%20under%20extreme%20conditions%20have%0Ademonstrated%20the%20effectiveness%20of%20our%20method.%20This%20approach%20is%20crucial%20for%0Aenhancing%20model%20robustness%20and%20adaptability%20in%20real-world%20scenarios%2C%20supporting%0Athe%20future%20development%20of%20person%20re-identification%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Augmentation%253A%2520Empowering%2520Model%2520Robustness%2520under%2520Extreme%2520Capture%250A%2520%2520Environments%26entry.906535625%3DYunpeng%2520Gong%2520and%2520Yongjie%2520Hou%2520and%2520Chuangliang%2520Zhang%2520and%2520Min%2520Jiang%26entry.1292438233%3D%2520%2520Person%2520Re-identification%2520%2528re-ID%2529%2520in%2520computer%2520vision%2520aims%2520to%2520recognize%2520and%250Atrack%2520individuals%2520across%2520different%2520cameras.%2520While%2520previous%2520research%2520has%2520mainly%250Afocused%2520on%2520challenges%2520like%2520pose%2520variations%2520and%2520lighting%2520changes%252C%2520the%2520impact%2520of%250Aextreme%2520capture%2520conditions%2520is%2520often%2520not%2520adequately%2520addressed.%2520These%2520extreme%250Aconditions%252C%2520including%2520varied%2520lighting%252C%2520camera%2520styles%252C%2520angles%252C%2520and%2520image%250Adistortions%252C%2520can%2520significantly%2520affect%2520data%2520distribution%2520and%2520re-ID%2520accuracy.%250A%2520%2520Current%2520research%2520typically%2520improves%2520model%2520generalization%2520under%2520normal%250Ashooting%2520conditions%2520through%2520data%2520augmentation%2520techniques%2520such%2520as%2520adjusting%250Abrightness%2520and%2520contrast.%2520However%252C%2520these%2520methods%2520pay%2520less%2520attention%2520to%2520the%250Arobustness%2520of%2520models%2520under%2520extreme%2520shooting%2520conditions.%2520To%2520tackle%2520this%252C%2520we%250Apropose%2520a%2520multi-mode%2520synchronization%2520learning%2520%2528MMSL%2529%2520strategy%2520.%2520This%2520approach%250Ainvolves%2520dividing%2520images%2520into%2520grids%252C%2520randomly%2520selecting%2520grid%2520blocks%252C%2520and%250Aapplying%2520data%2520augmentation%2520methods%2520like%2520contrast%2520and%2520brightness%2520adjustments.%250AThis%2520process%2520introduces%2520diverse%2520transformations%2520without%2520altering%2520the%2520original%250Aimage%2520structure%252C%2520helping%2520the%2520model%2520adapt%2520to%2520extreme%2520variations.%2520This%2520method%250Aimproves%2520the%2520model%2527s%2520generalization%2520under%2520extreme%2520conditions%2520and%2520enables%250Alearning%2520diverse%2520features%252C%2520thus%2520better%2520addressing%2520the%2520challenges%2520in%2520re-ID.%250AExtensive%2520experiments%2520on%2520a%2520simulated%2520test%2520set%2520under%2520extreme%2520conditions%2520have%250Ademonstrated%2520the%2520effectiveness%2520of%2520our%2520method.%2520This%2520approach%2520is%2520crucial%2520for%250Aenhancing%2520model%2520robustness%2520and%2520adaptability%2520in%2520real-world%2520scenarios%252C%2520supporting%250Athe%2520future%2520development%2520of%2520person%2520re-identification%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Augmentation%3A%20Empowering%20Model%20Robustness%20under%20Extreme%20Capture%0A%20%20Environments&entry.906535625=Yunpeng%20Gong%20and%20Yongjie%20Hou%20and%20Chuangliang%20Zhang%20and%20Min%20Jiang&entry.1292438233=%20%20Person%20Re-identification%20%28re-ID%29%20in%20computer%20vision%20aims%20to%20recognize%20and%0Atrack%20individuals%20across%20different%20cameras.%20While%20previous%20research%20has%20mainly%0Afocused%20on%20challenges%20like%20pose%20variations%20and%20lighting%20changes%2C%20the%20impact%20of%0Aextreme%20capture%20conditions%20is%20often%20not%20adequately%20addressed.%20These%20extreme%0Aconditions%2C%20including%20varied%20lighting%2C%20camera%20styles%2C%20angles%2C%20and%20image%0Adistortions%2C%20can%20significantly%20affect%20data%20distribution%20and%20re-ID%20accuracy.%0A%20%20Current%20research%20typically%20improves%20model%20generalization%20under%20normal%0Ashooting%20conditions%20through%20data%20augmentation%20techniques%20such%20as%20adjusting%0Abrightness%20and%20contrast.%20However%2C%20these%20methods%20pay%20less%20attention%20to%20the%0Arobustness%20of%20models%20under%20extreme%20shooting%20conditions.%20To%20tackle%20this%2C%20we%0Apropose%20a%20multi-mode%20synchronization%20learning%20%28MMSL%29%20strategy%20.%20This%20approach%0Ainvolves%20dividing%20images%20into%20grids%2C%20randomly%20selecting%20grid%20blocks%2C%20and%0Aapplying%20data%20augmentation%20methods%20like%20contrast%20and%20brightness%20adjustments.%0AThis%20process%20introduces%20diverse%20transformations%20without%20altering%20the%20original%0Aimage%20structure%2C%20helping%20the%20model%20adapt%20to%20extreme%20variations.%20This%20method%0Aimproves%20the%20model%27s%20generalization%20under%20extreme%20conditions%20and%20enables%0Alearning%20diverse%20features%2C%20thus%20better%20addressing%20the%20challenges%20in%20re-ID.%0AExtensive%20experiments%20on%20a%20simulated%20test%20set%20under%20extreme%20conditions%20have%0Ademonstrated%20the%20effectiveness%20of%20our%20method.%20This%20approach%20is%20crucial%20for%0Aenhancing%20model%20robustness%20and%20adaptability%20in%20real-world%20scenarios%2C%20supporting%0Athe%20future%20development%20of%20person%20re-identification%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13640v1&entry.124074799=Read"},
{"title": "Performance Comparison of Session-based Recommendation Algorithms based\n  on GNNs", "author": "Faisal Shehzad and Dietmar Jannach", "abstract": "  In session-based recommendation settings, a recommender system has no access\nto long-term user profiles and thus has to base its suggestions on the user\ninteractions that are observed in an ongoing session. Since such sessions can\nconsist of only a small set of interactions, various approaches based on Graph\nNeural Networks (GNN) were recently proposed, as they allow us to integrate\nvarious types of side information about the items in a natural way.\nUnfortunately, a variety of evaluation settings are used in the literature,\ne.g., in terms of protocols, metrics and baselines, making it difficult to\nassess what represents the state of the art. In this work, we present the\nresults of an evaluation of eight recent GNN-based approaches that were\npublished in high-quality outlets. For a fair comparison, all models are\nsystematically tuned and tested under identical conditions using three common\ndatasets. We furthermore include k-nearest-neighbor and sequential rules-based\nmodels as baselines, as such models have previously exhibited competitive\nperformance results for similar settings. To our surprise, the evaluation\nshowed that the simple models outperform all recent GNN models in terms of the\nMean Reciprocal Rank, which we used as an optimization criterion, and were only\noutperformed in three cases in terms of the Hit Rate. Additional analyses\nfurthermore reveal that several other factors that are often not deeply\ndiscussed in papers, e.g., random seeds, can markedly impact the performance of\nGNN-based models. Our results therefore (a) point to continuing issues in the\ncommunity in terms of research methodology and (b) indicate that there is ample\nroom for improvement in session-based recommendation.\n", "link": "http://arxiv.org/abs/2312.16695v2", "date": "2024-07-18", "relevancy": 2.2011, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4707}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4352}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Performance%20Comparison%20of%20Session-based%20Recommendation%20Algorithms%20based%0A%20%20on%20GNNs&body=Title%3A%20Performance%20Comparison%20of%20Session-based%20Recommendation%20Algorithms%20based%0A%20%20on%20GNNs%0AAuthor%3A%20Faisal%20Shehzad%20and%20Dietmar%20Jannach%0AAbstract%3A%20%20%20In%20session-based%20recommendation%20settings%2C%20a%20recommender%20system%20has%20no%20access%0Ato%20long-term%20user%20profiles%20and%20thus%20has%20to%20base%20its%20suggestions%20on%20the%20user%0Ainteractions%20that%20are%20observed%20in%20an%20ongoing%20session.%20Since%20such%20sessions%20can%0Aconsist%20of%20only%20a%20small%20set%20of%20interactions%2C%20various%20approaches%20based%20on%20Graph%0ANeural%20Networks%20%28GNN%29%20were%20recently%20proposed%2C%20as%20they%20allow%20us%20to%20integrate%0Avarious%20types%20of%20side%20information%20about%20the%20items%20in%20a%20natural%20way.%0AUnfortunately%2C%20a%20variety%20of%20evaluation%20settings%20are%20used%20in%20the%20literature%2C%0Ae.g.%2C%20in%20terms%20of%20protocols%2C%20metrics%20and%20baselines%2C%20making%20it%20difficult%20to%0Aassess%20what%20represents%20the%20state%20of%20the%20art.%20In%20this%20work%2C%20we%20present%20the%0Aresults%20of%20an%20evaluation%20of%20eight%20recent%20GNN-based%20approaches%20that%20were%0Apublished%20in%20high-quality%20outlets.%20For%20a%20fair%20comparison%2C%20all%20models%20are%0Asystematically%20tuned%20and%20tested%20under%20identical%20conditions%20using%20three%20common%0Adatasets.%20We%20furthermore%20include%20k-nearest-neighbor%20and%20sequential%20rules-based%0Amodels%20as%20baselines%2C%20as%20such%20models%20have%20previously%20exhibited%20competitive%0Aperformance%20results%20for%20similar%20settings.%20To%20our%20surprise%2C%20the%20evaluation%0Ashowed%20that%20the%20simple%20models%20outperform%20all%20recent%20GNN%20models%20in%20terms%20of%20the%0AMean%20Reciprocal%20Rank%2C%20which%20we%20used%20as%20an%20optimization%20criterion%2C%20and%20were%20only%0Aoutperformed%20in%20three%20cases%20in%20terms%20of%20the%20Hit%20Rate.%20Additional%20analyses%0Afurthermore%20reveal%20that%20several%20other%20factors%20that%20are%20often%20not%20deeply%0Adiscussed%20in%20papers%2C%20e.g.%2C%20random%20seeds%2C%20can%20markedly%20impact%20the%20performance%20of%0AGNN-based%20models.%20Our%20results%20therefore%20%28a%29%20point%20to%20continuing%20issues%20in%20the%0Acommunity%20in%20terms%20of%20research%20methodology%20and%20%28b%29%20indicate%20that%20there%20is%20ample%0Aroom%20for%20improvement%20in%20session-based%20recommendation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16695v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerformance%2520Comparison%2520of%2520Session-based%2520Recommendation%2520Algorithms%2520based%250A%2520%2520on%2520GNNs%26entry.906535625%3DFaisal%2520Shehzad%2520and%2520Dietmar%2520Jannach%26entry.1292438233%3D%2520%2520In%2520session-based%2520recommendation%2520settings%252C%2520a%2520recommender%2520system%2520has%2520no%2520access%250Ato%2520long-term%2520user%2520profiles%2520and%2520thus%2520has%2520to%2520base%2520its%2520suggestions%2520on%2520the%2520user%250Ainteractions%2520that%2520are%2520observed%2520in%2520an%2520ongoing%2520session.%2520Since%2520such%2520sessions%2520can%250Aconsist%2520of%2520only%2520a%2520small%2520set%2520of%2520interactions%252C%2520various%2520approaches%2520based%2520on%2520Graph%250ANeural%2520Networks%2520%2528GNN%2529%2520were%2520recently%2520proposed%252C%2520as%2520they%2520allow%2520us%2520to%2520integrate%250Avarious%2520types%2520of%2520side%2520information%2520about%2520the%2520items%2520in%2520a%2520natural%2520way.%250AUnfortunately%252C%2520a%2520variety%2520of%2520evaluation%2520settings%2520are%2520used%2520in%2520the%2520literature%252C%250Ae.g.%252C%2520in%2520terms%2520of%2520protocols%252C%2520metrics%2520and%2520baselines%252C%2520making%2520it%2520difficult%2520to%250Aassess%2520what%2520represents%2520the%2520state%2520of%2520the%2520art.%2520In%2520this%2520work%252C%2520we%2520present%2520the%250Aresults%2520of%2520an%2520evaluation%2520of%2520eight%2520recent%2520GNN-based%2520approaches%2520that%2520were%250Apublished%2520in%2520high-quality%2520outlets.%2520For%2520a%2520fair%2520comparison%252C%2520all%2520models%2520are%250Asystematically%2520tuned%2520and%2520tested%2520under%2520identical%2520conditions%2520using%2520three%2520common%250Adatasets.%2520We%2520furthermore%2520include%2520k-nearest-neighbor%2520and%2520sequential%2520rules-based%250Amodels%2520as%2520baselines%252C%2520as%2520such%2520models%2520have%2520previously%2520exhibited%2520competitive%250Aperformance%2520results%2520for%2520similar%2520settings.%2520To%2520our%2520surprise%252C%2520the%2520evaluation%250Ashowed%2520that%2520the%2520simple%2520models%2520outperform%2520all%2520recent%2520GNN%2520models%2520in%2520terms%2520of%2520the%250AMean%2520Reciprocal%2520Rank%252C%2520which%2520we%2520used%2520as%2520an%2520optimization%2520criterion%252C%2520and%2520were%2520only%250Aoutperformed%2520in%2520three%2520cases%2520in%2520terms%2520of%2520the%2520Hit%2520Rate.%2520Additional%2520analyses%250Afurthermore%2520reveal%2520that%2520several%2520other%2520factors%2520that%2520are%2520often%2520not%2520deeply%250Adiscussed%2520in%2520papers%252C%2520e.g.%252C%2520random%2520seeds%252C%2520can%2520markedly%2520impact%2520the%2520performance%2520of%250AGNN-based%2520models.%2520Our%2520results%2520therefore%2520%2528a%2529%2520point%2520to%2520continuing%2520issues%2520in%2520the%250Acommunity%2520in%2520terms%2520of%2520research%2520methodology%2520and%2520%2528b%2529%2520indicate%2520that%2520there%2520is%2520ample%250Aroom%2520for%2520improvement%2520in%2520session-based%2520recommendation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.16695v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20Comparison%20of%20Session-based%20Recommendation%20Algorithms%20based%0A%20%20on%20GNNs&entry.906535625=Faisal%20Shehzad%20and%20Dietmar%20Jannach&entry.1292438233=%20%20In%20session-based%20recommendation%20settings%2C%20a%20recommender%20system%20has%20no%20access%0Ato%20long-term%20user%20profiles%20and%20thus%20has%20to%20base%20its%20suggestions%20on%20the%20user%0Ainteractions%20that%20are%20observed%20in%20an%20ongoing%20session.%20Since%20such%20sessions%20can%0Aconsist%20of%20only%20a%20small%20set%20of%20interactions%2C%20various%20approaches%20based%20on%20Graph%0ANeural%20Networks%20%28GNN%29%20were%20recently%20proposed%2C%20as%20they%20allow%20us%20to%20integrate%0Avarious%20types%20of%20side%20information%20about%20the%20items%20in%20a%20natural%20way.%0AUnfortunately%2C%20a%20variety%20of%20evaluation%20settings%20are%20used%20in%20the%20literature%2C%0Ae.g.%2C%20in%20terms%20of%20protocols%2C%20metrics%20and%20baselines%2C%20making%20it%20difficult%20to%0Aassess%20what%20represents%20the%20state%20of%20the%20art.%20In%20this%20work%2C%20we%20present%20the%0Aresults%20of%20an%20evaluation%20of%20eight%20recent%20GNN-based%20approaches%20that%20were%0Apublished%20in%20high-quality%20outlets.%20For%20a%20fair%20comparison%2C%20all%20models%20are%0Asystematically%20tuned%20and%20tested%20under%20identical%20conditions%20using%20three%20common%0Adatasets.%20We%20furthermore%20include%20k-nearest-neighbor%20and%20sequential%20rules-based%0Amodels%20as%20baselines%2C%20as%20such%20models%20have%20previously%20exhibited%20competitive%0Aperformance%20results%20for%20similar%20settings.%20To%20our%20surprise%2C%20the%20evaluation%0Ashowed%20that%20the%20simple%20models%20outperform%20all%20recent%20GNN%20models%20in%20terms%20of%20the%0AMean%20Reciprocal%20Rank%2C%20which%20we%20used%20as%20an%20optimization%20criterion%2C%20and%20were%20only%0Aoutperformed%20in%20three%20cases%20in%20terms%20of%20the%20Hit%20Rate.%20Additional%20analyses%0Afurthermore%20reveal%20that%20several%20other%20factors%20that%20are%20often%20not%20deeply%0Adiscussed%20in%20papers%2C%20e.g.%2C%20random%20seeds%2C%20can%20markedly%20impact%20the%20performance%20of%0AGNN-based%20models.%20Our%20results%20therefore%20%28a%29%20point%20to%20continuing%20issues%20in%20the%0Acommunity%20in%20terms%20of%20research%20methodology%20and%20%28b%29%20indicate%20that%20there%20is%20ample%0Aroom%20for%20improvement%20in%20session-based%20recommendation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16695v2&entry.124074799=Read"},
{"title": "Masked Autoencoders are Efficient Continual Federated Learners", "author": "Subarnaduti Paul and Lars-Joel Frey and Roshni Kamath and Kristian Kersting and Martin Mundt", "abstract": "  Machine learning is typically framed from a perspective of i.i.d., and more\nimportantly, isolated data. In parts, federated learning lifts this assumption,\nas it sets out to solve the real-world challenge of collaboratively learning a\nshared model from data distributed across clients. However, motivated primarily\nby privacy and computational constraints, the fact that data may change,\ndistributions drift, or even tasks advance individually on clients, is seldom\ntaken into account. The field of continual learning addresses this separate\nchallenge and first steps have recently been taken to leverage synergies in\ndistributed supervised settings, in which several clients learn to solve\nchanging classification tasks over time without forgetting previously seen\nones. Motivated by these prior works, we posit that such federated continual\nlearning should be grounded in unsupervised learning of representations that\nare shared across clients; in the loose spirit of how humans can indirectly\nleverage others' experience without exposure to a specific task. For this\npurpose, we demonstrate that masked autoencoders for distribution estimation\nare particularly amenable to this setup. Specifically, their masking strategy\ncan be seamlessly integrated with task attention mechanisms to enable selective\nknowledge transfer between clients. We empirically corroborate the latter\nstatement through several continual federated scenarios on both image and\nbinary datasets.\n", "link": "http://arxiv.org/abs/2306.03542v2", "date": "2024-07-18", "relevancy": 2.1936, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5909}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5184}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Autoencoders%20are%20Efficient%20Continual%20Federated%20Learners&body=Title%3A%20Masked%20Autoencoders%20are%20Efficient%20Continual%20Federated%20Learners%0AAuthor%3A%20Subarnaduti%20Paul%20and%20Lars-Joel%20Frey%20and%20Roshni%20Kamath%20and%20Kristian%20Kersting%20and%20Martin%20Mundt%0AAbstract%3A%20%20%20Machine%20learning%20is%20typically%20framed%20from%20a%20perspective%20of%20i.i.d.%2C%20and%20more%0Aimportantly%2C%20isolated%20data.%20In%20parts%2C%20federated%20learning%20lifts%20this%20assumption%2C%0Aas%20it%20sets%20out%20to%20solve%20the%20real-world%20challenge%20of%20collaboratively%20learning%20a%0Ashared%20model%20from%20data%20distributed%20across%20clients.%20However%2C%20motivated%20primarily%0Aby%20privacy%20and%20computational%20constraints%2C%20the%20fact%20that%20data%20may%20change%2C%0Adistributions%20drift%2C%20or%20even%20tasks%20advance%20individually%20on%20clients%2C%20is%20seldom%0Ataken%20into%20account.%20The%20field%20of%20continual%20learning%20addresses%20this%20separate%0Achallenge%20and%20first%20steps%20have%20recently%20been%20taken%20to%20leverage%20synergies%20in%0Adistributed%20supervised%20settings%2C%20in%20which%20several%20clients%20learn%20to%20solve%0Achanging%20classification%20tasks%20over%20time%20without%20forgetting%20previously%20seen%0Aones.%20Motivated%20by%20these%20prior%20works%2C%20we%20posit%20that%20such%20federated%20continual%0Alearning%20should%20be%20grounded%20in%20unsupervised%20learning%20of%20representations%20that%0Aare%20shared%20across%20clients%3B%20in%20the%20loose%20spirit%20of%20how%20humans%20can%20indirectly%0Aleverage%20others%27%20experience%20without%20exposure%20to%20a%20specific%20task.%20For%20this%0Apurpose%2C%20we%20demonstrate%20that%20masked%20autoencoders%20for%20distribution%20estimation%0Aare%20particularly%20amenable%20to%20this%20setup.%20Specifically%2C%20their%20masking%20strategy%0Acan%20be%20seamlessly%20integrated%20with%20task%20attention%20mechanisms%20to%20enable%20selective%0Aknowledge%20transfer%20between%20clients.%20We%20empirically%20corroborate%20the%20latter%0Astatement%20through%20several%20continual%20federated%20scenarios%20on%20both%20image%20and%0Abinary%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.03542v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Autoencoders%2520are%2520Efficient%2520Continual%2520Federated%2520Learners%26entry.906535625%3DSubarnaduti%2520Paul%2520and%2520Lars-Joel%2520Frey%2520and%2520Roshni%2520Kamath%2520and%2520Kristian%2520Kersting%2520and%2520Martin%2520Mundt%26entry.1292438233%3D%2520%2520Machine%2520learning%2520is%2520typically%2520framed%2520from%2520a%2520perspective%2520of%2520i.i.d.%252C%2520and%2520more%250Aimportantly%252C%2520isolated%2520data.%2520In%2520parts%252C%2520federated%2520learning%2520lifts%2520this%2520assumption%252C%250Aas%2520it%2520sets%2520out%2520to%2520solve%2520the%2520real-world%2520challenge%2520of%2520collaboratively%2520learning%2520a%250Ashared%2520model%2520from%2520data%2520distributed%2520across%2520clients.%2520However%252C%2520motivated%2520primarily%250Aby%2520privacy%2520and%2520computational%2520constraints%252C%2520the%2520fact%2520that%2520data%2520may%2520change%252C%250Adistributions%2520drift%252C%2520or%2520even%2520tasks%2520advance%2520individually%2520on%2520clients%252C%2520is%2520seldom%250Ataken%2520into%2520account.%2520The%2520field%2520of%2520continual%2520learning%2520addresses%2520this%2520separate%250Achallenge%2520and%2520first%2520steps%2520have%2520recently%2520been%2520taken%2520to%2520leverage%2520synergies%2520in%250Adistributed%2520supervised%2520settings%252C%2520in%2520which%2520several%2520clients%2520learn%2520to%2520solve%250Achanging%2520classification%2520tasks%2520over%2520time%2520without%2520forgetting%2520previously%2520seen%250Aones.%2520Motivated%2520by%2520these%2520prior%2520works%252C%2520we%2520posit%2520that%2520such%2520federated%2520continual%250Alearning%2520should%2520be%2520grounded%2520in%2520unsupervised%2520learning%2520of%2520representations%2520that%250Aare%2520shared%2520across%2520clients%253B%2520in%2520the%2520loose%2520spirit%2520of%2520how%2520humans%2520can%2520indirectly%250Aleverage%2520others%2527%2520experience%2520without%2520exposure%2520to%2520a%2520specific%2520task.%2520For%2520this%250Apurpose%252C%2520we%2520demonstrate%2520that%2520masked%2520autoencoders%2520for%2520distribution%2520estimation%250Aare%2520particularly%2520amenable%2520to%2520this%2520setup.%2520Specifically%252C%2520their%2520masking%2520strategy%250Acan%2520be%2520seamlessly%2520integrated%2520with%2520task%2520attention%2520mechanisms%2520to%2520enable%2520selective%250Aknowledge%2520transfer%2520between%2520clients.%2520We%2520empirically%2520corroborate%2520the%2520latter%250Astatement%2520through%2520several%2520continual%2520federated%2520scenarios%2520on%2520both%2520image%2520and%250Abinary%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.03542v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Autoencoders%20are%20Efficient%20Continual%20Federated%20Learners&entry.906535625=Subarnaduti%20Paul%20and%20Lars-Joel%20Frey%20and%20Roshni%20Kamath%20and%20Kristian%20Kersting%20and%20Martin%20Mundt&entry.1292438233=%20%20Machine%20learning%20is%20typically%20framed%20from%20a%20perspective%20of%20i.i.d.%2C%20and%20more%0Aimportantly%2C%20isolated%20data.%20In%20parts%2C%20federated%20learning%20lifts%20this%20assumption%2C%0Aas%20it%20sets%20out%20to%20solve%20the%20real-world%20challenge%20of%20collaboratively%20learning%20a%0Ashared%20model%20from%20data%20distributed%20across%20clients.%20However%2C%20motivated%20primarily%0Aby%20privacy%20and%20computational%20constraints%2C%20the%20fact%20that%20data%20may%20change%2C%0Adistributions%20drift%2C%20or%20even%20tasks%20advance%20individually%20on%20clients%2C%20is%20seldom%0Ataken%20into%20account.%20The%20field%20of%20continual%20learning%20addresses%20this%20separate%0Achallenge%20and%20first%20steps%20have%20recently%20been%20taken%20to%20leverage%20synergies%20in%0Adistributed%20supervised%20settings%2C%20in%20which%20several%20clients%20learn%20to%20solve%0Achanging%20classification%20tasks%20over%20time%20without%20forgetting%20previously%20seen%0Aones.%20Motivated%20by%20these%20prior%20works%2C%20we%20posit%20that%20such%20federated%20continual%0Alearning%20should%20be%20grounded%20in%20unsupervised%20learning%20of%20representations%20that%0Aare%20shared%20across%20clients%3B%20in%20the%20loose%20spirit%20of%20how%20humans%20can%20indirectly%0Aleverage%20others%27%20experience%20without%20exposure%20to%20a%20specific%20task.%20For%20this%0Apurpose%2C%20we%20demonstrate%20that%20masked%20autoencoders%20for%20distribution%20estimation%0Aare%20particularly%20amenable%20to%20this%20setup.%20Specifically%2C%20their%20masking%20strategy%0Acan%20be%20seamlessly%20integrated%20with%20task%20attention%20mechanisms%20to%20enable%20selective%0Aknowledge%20transfer%20between%20clients.%20We%20empirically%20corroborate%20the%20latter%0Astatement%20through%20several%20continual%20federated%20scenarios%20on%20both%20image%20and%0Abinary%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.03542v2&entry.124074799=Read"},
{"title": "Compressing Structured Tensor Algebra", "author": "Mahdi Ghorbani and Emilien Bauer and Tobias Grosser and Amir Shaikhha", "abstract": "  Tensor algebra is a crucial component for data-intensive workloads such as\nmachine learning and scientific computing. As the complexity of data grows,\nscientists often encounter a dilemma between the highly specialized dense\ntensor algebra and efficient structure-aware algorithms provided by sparse\ntensor algebra. In this paper, we introduce DASTAC, a framework to propagate\nthe tensors's captured high-level structure down to low-level code generation\nby incorporating techniques such as automatic data layout compression,\npolyhedral analysis, and affine code generation. Our methodology reduces memory\nfootprint by automatically detecting the best data layout, heavily benefits\nfrom polyhedral optimizations, leverages further optimizations, and enables\nparallelization through MLIR. Through extensive experimentation, we show that\nDASTAC achieves 1 to 2 orders of magnitude speedup over TACO, a\nstate-of-the-art sparse tensor compiler, and StructTensor, a state-of-the-art\nstructured tensor algebra compiler, with a significantly lower memory\nfootprint.\n", "link": "http://arxiv.org/abs/2407.13726v1", "date": "2024-07-18", "relevancy": 2.1865, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4427}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4423}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressing%20Structured%20Tensor%20Algebra&body=Title%3A%20Compressing%20Structured%20Tensor%20Algebra%0AAuthor%3A%20Mahdi%20Ghorbani%20and%20Emilien%20Bauer%20and%20Tobias%20Grosser%20and%20Amir%20Shaikhha%0AAbstract%3A%20%20%20Tensor%20algebra%20is%20a%20crucial%20component%20for%20data-intensive%20workloads%20such%20as%0Amachine%20learning%20and%20scientific%20computing.%20As%20the%20complexity%20of%20data%20grows%2C%0Ascientists%20often%20encounter%20a%20dilemma%20between%20the%20highly%20specialized%20dense%0Atensor%20algebra%20and%20efficient%20structure-aware%20algorithms%20provided%20by%20sparse%0Atensor%20algebra.%20In%20this%20paper%2C%20we%20introduce%20DASTAC%2C%20a%20framework%20to%20propagate%0Athe%20tensors%27s%20captured%20high-level%20structure%20down%20to%20low-level%20code%20generation%0Aby%20incorporating%20techniques%20such%20as%20automatic%20data%20layout%20compression%2C%0Apolyhedral%20analysis%2C%20and%20affine%20code%20generation.%20Our%20methodology%20reduces%20memory%0Afootprint%20by%20automatically%20detecting%20the%20best%20data%20layout%2C%20heavily%20benefits%0Afrom%20polyhedral%20optimizations%2C%20leverages%20further%20optimizations%2C%20and%20enables%0Aparallelization%20through%20MLIR.%20Through%20extensive%20experimentation%2C%20we%20show%20that%0ADASTAC%20achieves%201%20to%202%20orders%20of%20magnitude%20speedup%20over%20TACO%2C%20a%0Astate-of-the-art%20sparse%20tensor%20compiler%2C%20and%20StructTensor%2C%20a%20state-of-the-art%0Astructured%20tensor%20algebra%20compiler%2C%20with%20a%20significantly%20lower%20memory%0Afootprint.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressing%2520Structured%2520Tensor%2520Algebra%26entry.906535625%3DMahdi%2520Ghorbani%2520and%2520Emilien%2520Bauer%2520and%2520Tobias%2520Grosser%2520and%2520Amir%2520Shaikhha%26entry.1292438233%3D%2520%2520Tensor%2520algebra%2520is%2520a%2520crucial%2520component%2520for%2520data-intensive%2520workloads%2520such%2520as%250Amachine%2520learning%2520and%2520scientific%2520computing.%2520As%2520the%2520complexity%2520of%2520data%2520grows%252C%250Ascientists%2520often%2520encounter%2520a%2520dilemma%2520between%2520the%2520highly%2520specialized%2520dense%250Atensor%2520algebra%2520and%2520efficient%2520structure-aware%2520algorithms%2520provided%2520by%2520sparse%250Atensor%2520algebra.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DASTAC%252C%2520a%2520framework%2520to%2520propagate%250Athe%2520tensors%2527s%2520captured%2520high-level%2520structure%2520down%2520to%2520low-level%2520code%2520generation%250Aby%2520incorporating%2520techniques%2520such%2520as%2520automatic%2520data%2520layout%2520compression%252C%250Apolyhedral%2520analysis%252C%2520and%2520affine%2520code%2520generation.%2520Our%2520methodology%2520reduces%2520memory%250Afootprint%2520by%2520automatically%2520detecting%2520the%2520best%2520data%2520layout%252C%2520heavily%2520benefits%250Afrom%2520polyhedral%2520optimizations%252C%2520leverages%2520further%2520optimizations%252C%2520and%2520enables%250Aparallelization%2520through%2520MLIR.%2520Through%2520extensive%2520experimentation%252C%2520we%2520show%2520that%250ADASTAC%2520achieves%25201%2520to%25202%2520orders%2520of%2520magnitude%2520speedup%2520over%2520TACO%252C%2520a%250Astate-of-the-art%2520sparse%2520tensor%2520compiler%252C%2520and%2520StructTensor%252C%2520a%2520state-of-the-art%250Astructured%2520tensor%2520algebra%2520compiler%252C%2520with%2520a%2520significantly%2520lower%2520memory%250Afootprint.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressing%20Structured%20Tensor%20Algebra&entry.906535625=Mahdi%20Ghorbani%20and%20Emilien%20Bauer%20and%20Tobias%20Grosser%20and%20Amir%20Shaikhha&entry.1292438233=%20%20Tensor%20algebra%20is%20a%20crucial%20component%20for%20data-intensive%20workloads%20such%20as%0Amachine%20learning%20and%20scientific%20computing.%20As%20the%20complexity%20of%20data%20grows%2C%0Ascientists%20often%20encounter%20a%20dilemma%20between%20the%20highly%20specialized%20dense%0Atensor%20algebra%20and%20efficient%20structure-aware%20algorithms%20provided%20by%20sparse%0Atensor%20algebra.%20In%20this%20paper%2C%20we%20introduce%20DASTAC%2C%20a%20framework%20to%20propagate%0Athe%20tensors%27s%20captured%20high-level%20structure%20down%20to%20low-level%20code%20generation%0Aby%20incorporating%20techniques%20such%20as%20automatic%20data%20layout%20compression%2C%0Apolyhedral%20analysis%2C%20and%20affine%20code%20generation.%20Our%20methodology%20reduces%20memory%0Afootprint%20by%20automatically%20detecting%20the%20best%20data%20layout%2C%20heavily%20benefits%0Afrom%20polyhedral%20optimizations%2C%20leverages%20further%20optimizations%2C%20and%20enables%0Aparallelization%20through%20MLIR.%20Through%20extensive%20experimentation%2C%20we%20show%20that%0ADASTAC%20achieves%201%20to%202%20orders%20of%20magnitude%20speedup%20over%20TACO%2C%20a%0Astate-of-the-art%20sparse%20tensor%20compiler%2C%20and%20StructTensor%2C%20a%20state-of-the-art%0Astructured%20tensor%20algebra%20compiler%2C%20with%20a%20significantly%20lower%20memory%0Afootprint.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13726v1&entry.124074799=Read"},
{"title": "SAM-Driven Weakly Supervised Nodule Segmentation with Uncertainty-Aware\n  Cross Teaching", "author": "Xingyue Zhao and Peiqi Li and Xiangde Luo and Meng Yang and Shi Chang and Zhongyu Li", "abstract": "  Automated nodule segmentation is essential for computer-assisted diagnosis in\nultrasound images. Nevertheless, most existing methods depend on precise\npixel-level annotations by medical professionals, a process that is both costly\nand labor-intensive. Recently, segmentation foundation models like SAM have\nshown impressive generalizability on natural images, suggesting their potential\nas pseudo-labelers. However, accurate prompts remain crucial for their success\nin medical images. In this work, we devise a novel weakly supervised framework\nthat effectively utilizes the segmentation foundation model to generate\npseudo-labels from aspect ration annotations for automatic nodule segmentation.\nSpecifically, we develop three types of bounding box prompts based on scalable\nshape priors, followed by an adaptive pseudo-label selection module to fully\nexploit the prediction capabilities of the foundation model for nodules. We\nalso present a SAM-driven uncertainty-aware cross-teaching strategy. This\napproach integrates SAM-based uncertainty estimation and label-space\nperturbations into cross-teaching to mitigate the impact of pseudo-label\ninaccuracies on model training. Extensive experiments on two clinically\ncollected ultrasound datasets demonstrate the superior performance of our\nproposed method.\n", "link": "http://arxiv.org/abs/2407.13553v1", "date": "2024-07-18", "relevancy": 2.1838, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6057}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5518}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM-Driven%20Weakly%20Supervised%20Nodule%20Segmentation%20with%20Uncertainty-Aware%0A%20%20Cross%20Teaching&body=Title%3A%20SAM-Driven%20Weakly%20Supervised%20Nodule%20Segmentation%20with%20Uncertainty-Aware%0A%20%20Cross%20Teaching%0AAuthor%3A%20Xingyue%20Zhao%20and%20Peiqi%20Li%20and%20Xiangde%20Luo%20and%20Meng%20Yang%20and%20Shi%20Chang%20and%20Zhongyu%20Li%0AAbstract%3A%20%20%20Automated%20nodule%20segmentation%20is%20essential%20for%20computer-assisted%20diagnosis%20in%0Aultrasound%20images.%20Nevertheless%2C%20most%20existing%20methods%20depend%20on%20precise%0Apixel-level%20annotations%20by%20medical%20professionals%2C%20a%20process%20that%20is%20both%20costly%0Aand%20labor-intensive.%20Recently%2C%20segmentation%20foundation%20models%20like%20SAM%20have%0Ashown%20impressive%20generalizability%20on%20natural%20images%2C%20suggesting%20their%20potential%0Aas%20pseudo-labelers.%20However%2C%20accurate%20prompts%20remain%20crucial%20for%20their%20success%0Ain%20medical%20images.%20In%20this%20work%2C%20we%20devise%20a%20novel%20weakly%20supervised%20framework%0Athat%20effectively%20utilizes%20the%20segmentation%20foundation%20model%20to%20generate%0Apseudo-labels%20from%20aspect%20ration%20annotations%20for%20automatic%20nodule%20segmentation.%0ASpecifically%2C%20we%20develop%20three%20types%20of%20bounding%20box%20prompts%20based%20on%20scalable%0Ashape%20priors%2C%20followed%20by%20an%20adaptive%20pseudo-label%20selection%20module%20to%20fully%0Aexploit%20the%20prediction%20capabilities%20of%20the%20foundation%20model%20for%20nodules.%20We%0Aalso%20present%20a%20SAM-driven%20uncertainty-aware%20cross-teaching%20strategy.%20This%0Aapproach%20integrates%20SAM-based%20uncertainty%20estimation%20and%20label-space%0Aperturbations%20into%20cross-teaching%20to%20mitigate%20the%20impact%20of%20pseudo-label%0Ainaccuracies%20on%20model%20training.%20Extensive%20experiments%20on%20two%20clinically%0Acollected%20ultrasound%20datasets%20demonstrate%20the%20superior%20performance%20of%20our%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM-Driven%2520Weakly%2520Supervised%2520Nodule%2520Segmentation%2520with%2520Uncertainty-Aware%250A%2520%2520Cross%2520Teaching%26entry.906535625%3DXingyue%2520Zhao%2520and%2520Peiqi%2520Li%2520and%2520Xiangde%2520Luo%2520and%2520Meng%2520Yang%2520and%2520Shi%2520Chang%2520and%2520Zhongyu%2520Li%26entry.1292438233%3D%2520%2520Automated%2520nodule%2520segmentation%2520is%2520essential%2520for%2520computer-assisted%2520diagnosis%2520in%250Aultrasound%2520images.%2520Nevertheless%252C%2520most%2520existing%2520methods%2520depend%2520on%2520precise%250Apixel-level%2520annotations%2520by%2520medical%2520professionals%252C%2520a%2520process%2520that%2520is%2520both%2520costly%250Aand%2520labor-intensive.%2520Recently%252C%2520segmentation%2520foundation%2520models%2520like%2520SAM%2520have%250Ashown%2520impressive%2520generalizability%2520on%2520natural%2520images%252C%2520suggesting%2520their%2520potential%250Aas%2520pseudo-labelers.%2520However%252C%2520accurate%2520prompts%2520remain%2520crucial%2520for%2520their%2520success%250Ain%2520medical%2520images.%2520In%2520this%2520work%252C%2520we%2520devise%2520a%2520novel%2520weakly%2520supervised%2520framework%250Athat%2520effectively%2520utilizes%2520the%2520segmentation%2520foundation%2520model%2520to%2520generate%250Apseudo-labels%2520from%2520aspect%2520ration%2520annotations%2520for%2520automatic%2520nodule%2520segmentation.%250ASpecifically%252C%2520we%2520develop%2520three%2520types%2520of%2520bounding%2520box%2520prompts%2520based%2520on%2520scalable%250Ashape%2520priors%252C%2520followed%2520by%2520an%2520adaptive%2520pseudo-label%2520selection%2520module%2520to%2520fully%250Aexploit%2520the%2520prediction%2520capabilities%2520of%2520the%2520foundation%2520model%2520for%2520nodules.%2520We%250Aalso%2520present%2520a%2520SAM-driven%2520uncertainty-aware%2520cross-teaching%2520strategy.%2520This%250Aapproach%2520integrates%2520SAM-based%2520uncertainty%2520estimation%2520and%2520label-space%250Aperturbations%2520into%2520cross-teaching%2520to%2520mitigate%2520the%2520impact%2520of%2520pseudo-label%250Ainaccuracies%2520on%2520model%2520training.%2520Extensive%2520experiments%2520on%2520two%2520clinically%250Acollected%2520ultrasound%2520datasets%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-Driven%20Weakly%20Supervised%20Nodule%20Segmentation%20with%20Uncertainty-Aware%0A%20%20Cross%20Teaching&entry.906535625=Xingyue%20Zhao%20and%20Peiqi%20Li%20and%20Xiangde%20Luo%20and%20Meng%20Yang%20and%20Shi%20Chang%20and%20Zhongyu%20Li&entry.1292438233=%20%20Automated%20nodule%20segmentation%20is%20essential%20for%20computer-assisted%20diagnosis%20in%0Aultrasound%20images.%20Nevertheless%2C%20most%20existing%20methods%20depend%20on%20precise%0Apixel-level%20annotations%20by%20medical%20professionals%2C%20a%20process%20that%20is%20both%20costly%0Aand%20labor-intensive.%20Recently%2C%20segmentation%20foundation%20models%20like%20SAM%20have%0Ashown%20impressive%20generalizability%20on%20natural%20images%2C%20suggesting%20their%20potential%0Aas%20pseudo-labelers.%20However%2C%20accurate%20prompts%20remain%20crucial%20for%20their%20success%0Ain%20medical%20images.%20In%20this%20work%2C%20we%20devise%20a%20novel%20weakly%20supervised%20framework%0Athat%20effectively%20utilizes%20the%20segmentation%20foundation%20model%20to%20generate%0Apseudo-labels%20from%20aspect%20ration%20annotations%20for%20automatic%20nodule%20segmentation.%0ASpecifically%2C%20we%20develop%20three%20types%20of%20bounding%20box%20prompts%20based%20on%20scalable%0Ashape%20priors%2C%20followed%20by%20an%20adaptive%20pseudo-label%20selection%20module%20to%20fully%0Aexploit%20the%20prediction%20capabilities%20of%20the%20foundation%20model%20for%20nodules.%20We%0Aalso%20present%20a%20SAM-driven%20uncertainty-aware%20cross-teaching%20strategy.%20This%0Aapproach%20integrates%20SAM-based%20uncertainty%20estimation%20and%20label-space%0Aperturbations%20into%20cross-teaching%20to%20mitigate%20the%20impact%20of%20pseudo-label%0Ainaccuracies%20on%20model%20training.%20Extensive%20experiments%20on%20two%20clinically%0Acollected%20ultrasound%20datasets%20demonstrate%20the%20superior%20performance%20of%20our%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13553v1&entry.124074799=Read"},
{"title": "Diffusion-Refined VQA Annotations for Semi-Supervised Gaze Following", "author": "Qiaomu Miao and Alexandros Graikos and Jingwei Zhang and Sounak Mondal and Minh Hoai and Dimitris Samaras", "abstract": "  Training gaze following models requires a large number of images with gaze\ntarget coordinates annotated by human annotators, which is a laborious and\ninherently ambiguous process. We propose the first semi-supervised method for\ngaze following by introducing two novel priors to the task. We obtain the first\nprior using a large pretrained Visual Question Answering (VQA) model, where we\ncompute Grad-CAM heatmaps by `prompting' the VQA model with a gaze following\nquestion. These heatmaps can be noisy and not suited for use in training. The\nneed to refine these noisy annotations leads us to incorporate a second prior.\nWe utilize a diffusion model trained on limited human annotations and modify\nthe reverse sampling process to refine the Grad-CAM heatmaps. By tuning the\ndiffusion process we achieve a trade-off between the human annotation prior and\nthe VQA heatmap prior, which retains the useful VQA prior information while\nexhibiting similar properties to the training data distribution. Our method\noutperforms simple pseudo-annotation generation baselines on the GazeFollow\nimage dataset. More importantly, our pseudo-annotation strategy, applied to a\nwidely used supervised gaze following model (VAT), reduces the annotation need\nby 50%. Our method also performs the best on the VideoAttentionTarget dataset.\n", "link": "http://arxiv.org/abs/2406.02774v2", "date": "2024-07-18", "relevancy": 2.183, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5462}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5457}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-Refined%20VQA%20Annotations%20for%20Semi-Supervised%20Gaze%20Following&body=Title%3A%20Diffusion-Refined%20VQA%20Annotations%20for%20Semi-Supervised%20Gaze%20Following%0AAuthor%3A%20Qiaomu%20Miao%20and%20Alexandros%20Graikos%20and%20Jingwei%20Zhang%20and%20Sounak%20Mondal%20and%20Minh%20Hoai%20and%20Dimitris%20Samaras%0AAbstract%3A%20%20%20Training%20gaze%20following%20models%20requires%20a%20large%20number%20of%20images%20with%20gaze%0Atarget%20coordinates%20annotated%20by%20human%20annotators%2C%20which%20is%20a%20laborious%20and%0Ainherently%20ambiguous%20process.%20We%20propose%20the%20first%20semi-supervised%20method%20for%0Agaze%20following%20by%20introducing%20two%20novel%20priors%20to%20the%20task.%20We%20obtain%20the%20first%0Aprior%20using%20a%20large%20pretrained%20Visual%20Question%20Answering%20%28VQA%29%20model%2C%20where%20we%0Acompute%20Grad-CAM%20heatmaps%20by%20%60prompting%27%20the%20VQA%20model%20with%20a%20gaze%20following%0Aquestion.%20These%20heatmaps%20can%20be%20noisy%20and%20not%20suited%20for%20use%20in%20training.%20The%0Aneed%20to%20refine%20these%20noisy%20annotations%20leads%20us%20to%20incorporate%20a%20second%20prior.%0AWe%20utilize%20a%20diffusion%20model%20trained%20on%20limited%20human%20annotations%20and%20modify%0Athe%20reverse%20sampling%20process%20to%20refine%20the%20Grad-CAM%20heatmaps.%20By%20tuning%20the%0Adiffusion%20process%20we%20achieve%20a%20trade-off%20between%20the%20human%20annotation%20prior%20and%0Athe%20VQA%20heatmap%20prior%2C%20which%20retains%20the%20useful%20VQA%20prior%20information%20while%0Aexhibiting%20similar%20properties%20to%20the%20training%20data%20distribution.%20Our%20method%0Aoutperforms%20simple%20pseudo-annotation%20generation%20baselines%20on%20the%20GazeFollow%0Aimage%20dataset.%20More%20importantly%2C%20our%20pseudo-annotation%20strategy%2C%20applied%20to%20a%0Awidely%20used%20supervised%20gaze%20following%20model%20%28VAT%29%2C%20reduces%20the%20annotation%20need%0Aby%2050%25.%20Our%20method%20also%20performs%20the%20best%20on%20the%20VideoAttentionTarget%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-Refined%2520VQA%2520Annotations%2520for%2520Semi-Supervised%2520Gaze%2520Following%26entry.906535625%3DQiaomu%2520Miao%2520and%2520Alexandros%2520Graikos%2520and%2520Jingwei%2520Zhang%2520and%2520Sounak%2520Mondal%2520and%2520Minh%2520Hoai%2520and%2520Dimitris%2520Samaras%26entry.1292438233%3D%2520%2520Training%2520gaze%2520following%2520models%2520requires%2520a%2520large%2520number%2520of%2520images%2520with%2520gaze%250Atarget%2520coordinates%2520annotated%2520by%2520human%2520annotators%252C%2520which%2520is%2520a%2520laborious%2520and%250Ainherently%2520ambiguous%2520process.%2520We%2520propose%2520the%2520first%2520semi-supervised%2520method%2520for%250Agaze%2520following%2520by%2520introducing%2520two%2520novel%2520priors%2520to%2520the%2520task.%2520We%2520obtain%2520the%2520first%250Aprior%2520using%2520a%2520large%2520pretrained%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520model%252C%2520where%2520we%250Acompute%2520Grad-CAM%2520heatmaps%2520by%2520%2560prompting%2527%2520the%2520VQA%2520model%2520with%2520a%2520gaze%2520following%250Aquestion.%2520These%2520heatmaps%2520can%2520be%2520noisy%2520and%2520not%2520suited%2520for%2520use%2520in%2520training.%2520The%250Aneed%2520to%2520refine%2520these%2520noisy%2520annotations%2520leads%2520us%2520to%2520incorporate%2520a%2520second%2520prior.%250AWe%2520utilize%2520a%2520diffusion%2520model%2520trained%2520on%2520limited%2520human%2520annotations%2520and%2520modify%250Athe%2520reverse%2520sampling%2520process%2520to%2520refine%2520the%2520Grad-CAM%2520heatmaps.%2520By%2520tuning%2520the%250Adiffusion%2520process%2520we%2520achieve%2520a%2520trade-off%2520between%2520the%2520human%2520annotation%2520prior%2520and%250Athe%2520VQA%2520heatmap%2520prior%252C%2520which%2520retains%2520the%2520useful%2520VQA%2520prior%2520information%2520while%250Aexhibiting%2520similar%2520properties%2520to%2520the%2520training%2520data%2520distribution.%2520Our%2520method%250Aoutperforms%2520simple%2520pseudo-annotation%2520generation%2520baselines%2520on%2520the%2520GazeFollow%250Aimage%2520dataset.%2520More%2520importantly%252C%2520our%2520pseudo-annotation%2520strategy%252C%2520applied%2520to%2520a%250Awidely%2520used%2520supervised%2520gaze%2520following%2520model%2520%2528VAT%2529%252C%2520reduces%2520the%2520annotation%2520need%250Aby%252050%2525.%2520Our%2520method%2520also%2520performs%2520the%2520best%2520on%2520the%2520VideoAttentionTarget%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Refined%20VQA%20Annotations%20for%20Semi-Supervised%20Gaze%20Following&entry.906535625=Qiaomu%20Miao%20and%20Alexandros%20Graikos%20and%20Jingwei%20Zhang%20and%20Sounak%20Mondal%20and%20Minh%20Hoai%20and%20Dimitris%20Samaras&entry.1292438233=%20%20Training%20gaze%20following%20models%20requires%20a%20large%20number%20of%20images%20with%20gaze%0Atarget%20coordinates%20annotated%20by%20human%20annotators%2C%20which%20is%20a%20laborious%20and%0Ainherently%20ambiguous%20process.%20We%20propose%20the%20first%20semi-supervised%20method%20for%0Agaze%20following%20by%20introducing%20two%20novel%20priors%20to%20the%20task.%20We%20obtain%20the%20first%0Aprior%20using%20a%20large%20pretrained%20Visual%20Question%20Answering%20%28VQA%29%20model%2C%20where%20we%0Acompute%20Grad-CAM%20heatmaps%20by%20%60prompting%27%20the%20VQA%20model%20with%20a%20gaze%20following%0Aquestion.%20These%20heatmaps%20can%20be%20noisy%20and%20not%20suited%20for%20use%20in%20training.%20The%0Aneed%20to%20refine%20these%20noisy%20annotations%20leads%20us%20to%20incorporate%20a%20second%20prior.%0AWe%20utilize%20a%20diffusion%20model%20trained%20on%20limited%20human%20annotations%20and%20modify%0Athe%20reverse%20sampling%20process%20to%20refine%20the%20Grad-CAM%20heatmaps.%20By%20tuning%20the%0Adiffusion%20process%20we%20achieve%20a%20trade-off%20between%20the%20human%20annotation%20prior%20and%0Athe%20VQA%20heatmap%20prior%2C%20which%20retains%20the%20useful%20VQA%20prior%20information%20while%0Aexhibiting%20similar%20properties%20to%20the%20training%20data%20distribution.%20Our%20method%0Aoutperforms%20simple%20pseudo-annotation%20generation%20baselines%20on%20the%20GazeFollow%0Aimage%20dataset.%20More%20importantly%2C%20our%20pseudo-annotation%20strategy%2C%20applied%20to%20a%0Awidely%20used%20supervised%20gaze%20following%20model%20%28VAT%29%2C%20reduces%20the%20annotation%20need%0Aby%2050%25.%20Our%20method%20also%20performs%20the%20best%20on%20the%20VideoAttentionTarget%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02774v2&entry.124074799=Read"},
{"title": "FADE: A Task-Agnostic Upsampling Operator for Encoder-Decoder\n  Architectures", "author": "Hao Lu and Wenze Liu and Hongtao Fu and Zhiguo Cao", "abstract": "  The goal of this work is to develop a task-agnostic feature upsampling\noperator for dense prediction where the operator is required to facilitate not\nonly region-sensitive tasks like semantic segmentation but also\ndetail-sensitive tasks such as image matting. Prior upsampling operators often\ncan work well in either type of the tasks, but not both. We argue that\ntask-agnostic upsampling should dynamically trade off between semantic\npreservation and detail delineation, instead of having a bias between the two\nproperties. In this paper, we present FADE, a novel, plug-and-play,\nlightweight, and task-agnostic upsampling operator by fusing the assets of\ndecoder and encoder features at three levels: i) considering both the encoder\nand decoder feature in upsampling kernel generation; ii) controlling the\nper-point contribution of the encoder/decoder feature in upsampling kernels\nwith an efficient semi-shift convolutional operator; and iii) enabling the\nselective pass of encoder features with a decoder-dependent gating mechanism\nfor compensating details. To improve the practicality of FADE, we additionally\nstudy parameter- and memory-efficient implementations of semi-shift\nconvolution. We analyze the upsampling behavior of FADE on toy data and show\nthrough large-scale experiments that FADE is task-agnostic with consistent\nperformance improvement on a number of dense prediction tasks with little extra\ncost. For the first time, we demonstrate robust feature upsampling on both\nregion- and detail-sensitive tasks successfully. Code is made available at:\nhttps://github.com/poppinace/fade\n", "link": "http://arxiv.org/abs/2407.13500v1", "date": "2024-07-18", "relevancy": 2.1785, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5818}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5377}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FADE%3A%20A%20Task-Agnostic%20Upsampling%20Operator%20for%20Encoder-Decoder%0A%20%20Architectures&body=Title%3A%20FADE%3A%20A%20Task-Agnostic%20Upsampling%20Operator%20for%20Encoder-Decoder%0A%20%20Architectures%0AAuthor%3A%20Hao%20Lu%20and%20Wenze%20Liu%20and%20Hongtao%20Fu%20and%20Zhiguo%20Cao%0AAbstract%3A%20%20%20The%20goal%20of%20this%20work%20is%20to%20develop%20a%20task-agnostic%20feature%20upsampling%0Aoperator%20for%20dense%20prediction%20where%20the%20operator%20is%20required%20to%20facilitate%20not%0Aonly%20region-sensitive%20tasks%20like%20semantic%20segmentation%20but%20also%0Adetail-sensitive%20tasks%20such%20as%20image%20matting.%20Prior%20upsampling%20operators%20often%0Acan%20work%20well%20in%20either%20type%20of%20the%20tasks%2C%20but%20not%20both.%20We%20argue%20that%0Atask-agnostic%20upsampling%20should%20dynamically%20trade%20off%20between%20semantic%0Apreservation%20and%20detail%20delineation%2C%20instead%20of%20having%20a%20bias%20between%20the%20two%0Aproperties.%20In%20this%20paper%2C%20we%20present%20FADE%2C%20a%20novel%2C%20plug-and-play%2C%0Alightweight%2C%20and%20task-agnostic%20upsampling%20operator%20by%20fusing%20the%20assets%20of%0Adecoder%20and%20encoder%20features%20at%20three%20levels%3A%20i%29%20considering%20both%20the%20encoder%0Aand%20decoder%20feature%20in%20upsampling%20kernel%20generation%3B%20ii%29%20controlling%20the%0Aper-point%20contribution%20of%20the%20encoder/decoder%20feature%20in%20upsampling%20kernels%0Awith%20an%20efficient%20semi-shift%20convolutional%20operator%3B%20and%20iii%29%20enabling%20the%0Aselective%20pass%20of%20encoder%20features%20with%20a%20decoder-dependent%20gating%20mechanism%0Afor%20compensating%20details.%20To%20improve%20the%20practicality%20of%20FADE%2C%20we%20additionally%0Astudy%20parameter-%20and%20memory-efficient%20implementations%20of%20semi-shift%0Aconvolution.%20We%20analyze%20the%20upsampling%20behavior%20of%20FADE%20on%20toy%20data%20and%20show%0Athrough%20large-scale%20experiments%20that%20FADE%20is%20task-agnostic%20with%20consistent%0Aperformance%20improvement%20on%20a%20number%20of%20dense%20prediction%20tasks%20with%20little%20extra%0Acost.%20For%20the%20first%20time%2C%20we%20demonstrate%20robust%20feature%20upsampling%20on%20both%0Aregion-%20and%20detail-sensitive%20tasks%20successfully.%20Code%20is%20made%20available%20at%3A%0Ahttps%3A//github.com/poppinace/fade%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFADE%253A%2520A%2520Task-Agnostic%2520Upsampling%2520Operator%2520for%2520Encoder-Decoder%250A%2520%2520Architectures%26entry.906535625%3DHao%2520Lu%2520and%2520Wenze%2520Liu%2520and%2520Hongtao%2520Fu%2520and%2520Zhiguo%2520Cao%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520this%2520work%2520is%2520to%2520develop%2520a%2520task-agnostic%2520feature%2520upsampling%250Aoperator%2520for%2520dense%2520prediction%2520where%2520the%2520operator%2520is%2520required%2520to%2520facilitate%2520not%250Aonly%2520region-sensitive%2520tasks%2520like%2520semantic%2520segmentation%2520but%2520also%250Adetail-sensitive%2520tasks%2520such%2520as%2520image%2520matting.%2520Prior%2520upsampling%2520operators%2520often%250Acan%2520work%2520well%2520in%2520either%2520type%2520of%2520the%2520tasks%252C%2520but%2520not%2520both.%2520We%2520argue%2520that%250Atask-agnostic%2520upsampling%2520should%2520dynamically%2520trade%2520off%2520between%2520semantic%250Apreservation%2520and%2520detail%2520delineation%252C%2520instead%2520of%2520having%2520a%2520bias%2520between%2520the%2520two%250Aproperties.%2520In%2520this%2520paper%252C%2520we%2520present%2520FADE%252C%2520a%2520novel%252C%2520plug-and-play%252C%250Alightweight%252C%2520and%2520task-agnostic%2520upsampling%2520operator%2520by%2520fusing%2520the%2520assets%2520of%250Adecoder%2520and%2520encoder%2520features%2520at%2520three%2520levels%253A%2520i%2529%2520considering%2520both%2520the%2520encoder%250Aand%2520decoder%2520feature%2520in%2520upsampling%2520kernel%2520generation%253B%2520ii%2529%2520controlling%2520the%250Aper-point%2520contribution%2520of%2520the%2520encoder/decoder%2520feature%2520in%2520upsampling%2520kernels%250Awith%2520an%2520efficient%2520semi-shift%2520convolutional%2520operator%253B%2520and%2520iii%2529%2520enabling%2520the%250Aselective%2520pass%2520of%2520encoder%2520features%2520with%2520a%2520decoder-dependent%2520gating%2520mechanism%250Afor%2520compensating%2520details.%2520To%2520improve%2520the%2520practicality%2520of%2520FADE%252C%2520we%2520additionally%250Astudy%2520parameter-%2520and%2520memory-efficient%2520implementations%2520of%2520semi-shift%250Aconvolution.%2520We%2520analyze%2520the%2520upsampling%2520behavior%2520of%2520FADE%2520on%2520toy%2520data%2520and%2520show%250Athrough%2520large-scale%2520experiments%2520that%2520FADE%2520is%2520task-agnostic%2520with%2520consistent%250Aperformance%2520improvement%2520on%2520a%2520number%2520of%2520dense%2520prediction%2520tasks%2520with%2520little%2520extra%250Acost.%2520For%2520the%2520first%2520time%252C%2520we%2520demonstrate%2520robust%2520feature%2520upsampling%2520on%2520both%250Aregion-%2520and%2520detail-sensitive%2520tasks%2520successfully.%2520Code%2520is%2520made%2520available%2520at%253A%250Ahttps%253A//github.com/poppinace/fade%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FADE%3A%20A%20Task-Agnostic%20Upsampling%20Operator%20for%20Encoder-Decoder%0A%20%20Architectures&entry.906535625=Hao%20Lu%20and%20Wenze%20Liu%20and%20Hongtao%20Fu%20and%20Zhiguo%20Cao&entry.1292438233=%20%20The%20goal%20of%20this%20work%20is%20to%20develop%20a%20task-agnostic%20feature%20upsampling%0Aoperator%20for%20dense%20prediction%20where%20the%20operator%20is%20required%20to%20facilitate%20not%0Aonly%20region-sensitive%20tasks%20like%20semantic%20segmentation%20but%20also%0Adetail-sensitive%20tasks%20such%20as%20image%20matting.%20Prior%20upsampling%20operators%20often%0Acan%20work%20well%20in%20either%20type%20of%20the%20tasks%2C%20but%20not%20both.%20We%20argue%20that%0Atask-agnostic%20upsampling%20should%20dynamically%20trade%20off%20between%20semantic%0Apreservation%20and%20detail%20delineation%2C%20instead%20of%20having%20a%20bias%20between%20the%20two%0Aproperties.%20In%20this%20paper%2C%20we%20present%20FADE%2C%20a%20novel%2C%20plug-and-play%2C%0Alightweight%2C%20and%20task-agnostic%20upsampling%20operator%20by%20fusing%20the%20assets%20of%0Adecoder%20and%20encoder%20features%20at%20three%20levels%3A%20i%29%20considering%20both%20the%20encoder%0Aand%20decoder%20feature%20in%20upsampling%20kernel%20generation%3B%20ii%29%20controlling%20the%0Aper-point%20contribution%20of%20the%20encoder/decoder%20feature%20in%20upsampling%20kernels%0Awith%20an%20efficient%20semi-shift%20convolutional%20operator%3B%20and%20iii%29%20enabling%20the%0Aselective%20pass%20of%20encoder%20features%20with%20a%20decoder-dependent%20gating%20mechanism%0Afor%20compensating%20details.%20To%20improve%20the%20practicality%20of%20FADE%2C%20we%20additionally%0Astudy%20parameter-%20and%20memory-efficient%20implementations%20of%20semi-shift%0Aconvolution.%20We%20analyze%20the%20upsampling%20behavior%20of%20FADE%20on%20toy%20data%20and%20show%0Athrough%20large-scale%20experiments%20that%20FADE%20is%20task-agnostic%20with%20consistent%0Aperformance%20improvement%20on%20a%20number%20of%20dense%20prediction%20tasks%20with%20little%20extra%0Acost.%20For%20the%20first%20time%2C%20we%20demonstrate%20robust%20feature%20upsampling%20on%20both%0Aregion-%20and%20detail-sensitive%20tasks%20successfully.%20Code%20is%20made%20available%20at%3A%0Ahttps%3A//github.com/poppinace/fade%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13500v1&entry.124074799=Read"},
{"title": "SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection", "author": "Yan Gong and Xinyu Zhang and Hao Liu and Xinmin Jiang and Zhiwei Li and Xin Gao and Lei Lin and Dafeng Jin and Jun Li and Huaping Liu", "abstract": "  Multi-modal fusion is increasingly being used for autonomous driving tasks,\nas different modalities provide unique information for feature extraction.\nHowever, the existing two-stream networks are only fused at a specific network\nlayer, which requires a lot of manual attempts to set up. As the CNN goes\ndeeper, the two modal features become more and more advanced and abstract, and\nthe fusion occurs at the feature level with a large gap, which can easily hurt\nthe performance. To reduce the loss of height and depth information during the\nprocess of projecting point clouds into 2D space, we utilize calibration\nparameters to project the point cloud into Altitude Difference Images (ADIs),\nwhich exhibit more distinct road features. In this study, we propose a novel\nfusion architecture called Skip-cross Networks (SkipcrossNets), which combine\nadaptively ADIs and camera images without being bound to a certain fusion\nepoch. Specifically, skip-cross fusion strategy connects each layer to each\nlayer in a feed-forward manner, and for each layer, the feature maps of all\nprevious layers are used as input and its own feature maps are used as input to\nall subsequent layers for the other modality, enhancing feature propagation and\nmulti-modal features fusion. This strategy facilitates selection of the most\nsimilar feature layers from two modalities, enhancing feature reuse and\nproviding complementary effects for sparse point cloud features. The advantages\nof skip-cross fusion strategy is demonstrated through application to the KITTI\nand A2D2 datasets, achieving a MaxF score of 96.85% on KITTI and an F1 score of\n84.84% on A2D2. The model parameters require only 2.33 MB of memory at a speed\nof 68.24 FPS, which can be viable for mobile terminals and embedded devices.\n", "link": "http://arxiv.org/abs/2308.12863v2", "date": "2024-07-18", "relevancy": 2.176, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5527}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5388}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkipcrossNets%3A%20Adaptive%20Skip-cross%20Fusion%20for%20Road%20Detection&body=Title%3A%20SkipcrossNets%3A%20Adaptive%20Skip-cross%20Fusion%20for%20Road%20Detection%0AAuthor%3A%20Yan%20Gong%20and%20Xinyu%20Zhang%20and%20Hao%20Liu%20and%20Xinmin%20Jiang%20and%20Zhiwei%20Li%20and%20Xin%20Gao%20and%20Lei%20Lin%20and%20Dafeng%20Jin%20and%20Jun%20Li%20and%20Huaping%20Liu%0AAbstract%3A%20%20%20Multi-modal%20fusion%20is%20increasingly%20being%20used%20for%20autonomous%20driving%20tasks%2C%0Aas%20different%20modalities%20provide%20unique%20information%20for%20feature%20extraction.%0AHowever%2C%20the%20existing%20two-stream%20networks%20are%20only%20fused%20at%20a%20specific%20network%0Alayer%2C%20which%20requires%20a%20lot%20of%20manual%20attempts%20to%20set%20up.%20As%20the%20CNN%20goes%0Adeeper%2C%20the%20two%20modal%20features%20become%20more%20and%20more%20advanced%20and%20abstract%2C%20and%0Athe%20fusion%20occurs%20at%20the%20feature%20level%20with%20a%20large%20gap%2C%20which%20can%20easily%20hurt%0Athe%20performance.%20To%20reduce%20the%20loss%20of%20height%20and%20depth%20information%20during%20the%0Aprocess%20of%20projecting%20point%20clouds%20into%202D%20space%2C%20we%20utilize%20calibration%0Aparameters%20to%20project%20the%20point%20cloud%20into%20Altitude%20Difference%20Images%20%28ADIs%29%2C%0Awhich%20exhibit%20more%20distinct%20road%20features.%20In%20this%20study%2C%20we%20propose%20a%20novel%0Afusion%20architecture%20called%20Skip-cross%20Networks%20%28SkipcrossNets%29%2C%20which%20combine%0Aadaptively%20ADIs%20and%20camera%20images%20without%20being%20bound%20to%20a%20certain%20fusion%0Aepoch.%20Specifically%2C%20skip-cross%20fusion%20strategy%20connects%20each%20layer%20to%20each%0Alayer%20in%20a%20feed-forward%20manner%2C%20and%20for%20each%20layer%2C%20the%20feature%20maps%20of%20all%0Aprevious%20layers%20are%20used%20as%20input%20and%20its%20own%20feature%20maps%20are%20used%20as%20input%20to%0Aall%20subsequent%20layers%20for%20the%20other%20modality%2C%20enhancing%20feature%20propagation%20and%0Amulti-modal%20features%20fusion.%20This%20strategy%20facilitates%20selection%20of%20the%20most%0Asimilar%20feature%20layers%20from%20two%20modalities%2C%20enhancing%20feature%20reuse%20and%0Aproviding%20complementary%20effects%20for%20sparse%20point%20cloud%20features.%20The%20advantages%0Aof%20skip-cross%20fusion%20strategy%20is%20demonstrated%20through%20application%20to%20the%20KITTI%0Aand%20A2D2%20datasets%2C%20achieving%20a%20MaxF%20score%20of%2096.85%25%20on%20KITTI%20and%20an%20F1%20score%20of%0A84.84%25%20on%20A2D2.%20The%20model%20parameters%20require%20only%202.33%20MB%20of%20memory%20at%20a%20speed%0Aof%2068.24%20FPS%2C%20which%20can%20be%20viable%20for%20mobile%20terminals%20and%20embedded%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkipcrossNets%253A%2520Adaptive%2520Skip-cross%2520Fusion%2520for%2520Road%2520Detection%26entry.906535625%3DYan%2520Gong%2520and%2520Xinyu%2520Zhang%2520and%2520Hao%2520Liu%2520and%2520Xinmin%2520Jiang%2520and%2520Zhiwei%2520Li%2520and%2520Xin%2520Gao%2520and%2520Lei%2520Lin%2520and%2520Dafeng%2520Jin%2520and%2520Jun%2520Li%2520and%2520Huaping%2520Liu%26entry.1292438233%3D%2520%2520Multi-modal%2520fusion%2520is%2520increasingly%2520being%2520used%2520for%2520autonomous%2520driving%2520tasks%252C%250Aas%2520different%2520modalities%2520provide%2520unique%2520information%2520for%2520feature%2520extraction.%250AHowever%252C%2520the%2520existing%2520two-stream%2520networks%2520are%2520only%2520fused%2520at%2520a%2520specific%2520network%250Alayer%252C%2520which%2520requires%2520a%2520lot%2520of%2520manual%2520attempts%2520to%2520set%2520up.%2520As%2520the%2520CNN%2520goes%250Adeeper%252C%2520the%2520two%2520modal%2520features%2520become%2520more%2520and%2520more%2520advanced%2520and%2520abstract%252C%2520and%250Athe%2520fusion%2520occurs%2520at%2520the%2520feature%2520level%2520with%2520a%2520large%2520gap%252C%2520which%2520can%2520easily%2520hurt%250Athe%2520performance.%2520To%2520reduce%2520the%2520loss%2520of%2520height%2520and%2520depth%2520information%2520during%2520the%250Aprocess%2520of%2520projecting%2520point%2520clouds%2520into%25202D%2520space%252C%2520we%2520utilize%2520calibration%250Aparameters%2520to%2520project%2520the%2520point%2520cloud%2520into%2520Altitude%2520Difference%2520Images%2520%2528ADIs%2529%252C%250Awhich%2520exhibit%2520more%2520distinct%2520road%2520features.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%250Afusion%2520architecture%2520called%2520Skip-cross%2520Networks%2520%2528SkipcrossNets%2529%252C%2520which%2520combine%250Aadaptively%2520ADIs%2520and%2520camera%2520images%2520without%2520being%2520bound%2520to%2520a%2520certain%2520fusion%250Aepoch.%2520Specifically%252C%2520skip-cross%2520fusion%2520strategy%2520connects%2520each%2520layer%2520to%2520each%250Alayer%2520in%2520a%2520feed-forward%2520manner%252C%2520and%2520for%2520each%2520layer%252C%2520the%2520feature%2520maps%2520of%2520all%250Aprevious%2520layers%2520are%2520used%2520as%2520input%2520and%2520its%2520own%2520feature%2520maps%2520are%2520used%2520as%2520input%2520to%250Aall%2520subsequent%2520layers%2520for%2520the%2520other%2520modality%252C%2520enhancing%2520feature%2520propagation%2520and%250Amulti-modal%2520features%2520fusion.%2520This%2520strategy%2520facilitates%2520selection%2520of%2520the%2520most%250Asimilar%2520feature%2520layers%2520from%2520two%2520modalities%252C%2520enhancing%2520feature%2520reuse%2520and%250Aproviding%2520complementary%2520effects%2520for%2520sparse%2520point%2520cloud%2520features.%2520The%2520advantages%250Aof%2520skip-cross%2520fusion%2520strategy%2520is%2520demonstrated%2520through%2520application%2520to%2520the%2520KITTI%250Aand%2520A2D2%2520datasets%252C%2520achieving%2520a%2520MaxF%2520score%2520of%252096.85%2525%2520on%2520KITTI%2520and%2520an%2520F1%2520score%2520of%250A84.84%2525%2520on%2520A2D2.%2520The%2520model%2520parameters%2520require%2520only%25202.33%2520MB%2520of%2520memory%2520at%2520a%2520speed%250Aof%252068.24%2520FPS%252C%2520which%2520can%2520be%2520viable%2520for%2520mobile%2520terminals%2520and%2520embedded%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.12863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkipcrossNets%3A%20Adaptive%20Skip-cross%20Fusion%20for%20Road%20Detection&entry.906535625=Yan%20Gong%20and%20Xinyu%20Zhang%20and%20Hao%20Liu%20and%20Xinmin%20Jiang%20and%20Zhiwei%20Li%20and%20Xin%20Gao%20and%20Lei%20Lin%20and%20Dafeng%20Jin%20and%20Jun%20Li%20and%20Huaping%20Liu&entry.1292438233=%20%20Multi-modal%20fusion%20is%20increasingly%20being%20used%20for%20autonomous%20driving%20tasks%2C%0Aas%20different%20modalities%20provide%20unique%20information%20for%20feature%20extraction.%0AHowever%2C%20the%20existing%20two-stream%20networks%20are%20only%20fused%20at%20a%20specific%20network%0Alayer%2C%20which%20requires%20a%20lot%20of%20manual%20attempts%20to%20set%20up.%20As%20the%20CNN%20goes%0Adeeper%2C%20the%20two%20modal%20features%20become%20more%20and%20more%20advanced%20and%20abstract%2C%20and%0Athe%20fusion%20occurs%20at%20the%20feature%20level%20with%20a%20large%20gap%2C%20which%20can%20easily%20hurt%0Athe%20performance.%20To%20reduce%20the%20loss%20of%20height%20and%20depth%20information%20during%20the%0Aprocess%20of%20projecting%20point%20clouds%20into%202D%20space%2C%20we%20utilize%20calibration%0Aparameters%20to%20project%20the%20point%20cloud%20into%20Altitude%20Difference%20Images%20%28ADIs%29%2C%0Awhich%20exhibit%20more%20distinct%20road%20features.%20In%20this%20study%2C%20we%20propose%20a%20novel%0Afusion%20architecture%20called%20Skip-cross%20Networks%20%28SkipcrossNets%29%2C%20which%20combine%0Aadaptively%20ADIs%20and%20camera%20images%20without%20being%20bound%20to%20a%20certain%20fusion%0Aepoch.%20Specifically%2C%20skip-cross%20fusion%20strategy%20connects%20each%20layer%20to%20each%0Alayer%20in%20a%20feed-forward%20manner%2C%20and%20for%20each%20layer%2C%20the%20feature%20maps%20of%20all%0Aprevious%20layers%20are%20used%20as%20input%20and%20its%20own%20feature%20maps%20are%20used%20as%20input%20to%0Aall%20subsequent%20layers%20for%20the%20other%20modality%2C%20enhancing%20feature%20propagation%20and%0Amulti-modal%20features%20fusion.%20This%20strategy%20facilitates%20selection%20of%20the%20most%0Asimilar%20feature%20layers%20from%20two%20modalities%2C%20enhancing%20feature%20reuse%20and%0Aproviding%20complementary%20effects%20for%20sparse%20point%20cloud%20features.%20The%20advantages%0Aof%20skip-cross%20fusion%20strategy%20is%20demonstrated%20through%20application%20to%20the%20KITTI%0Aand%20A2D2%20datasets%2C%20achieving%20a%20MaxF%20score%20of%2096.85%25%20on%20KITTI%20and%20an%20F1%20score%20of%0A84.84%25%20on%20A2D2.%20The%20model%20parameters%20require%20only%202.33%20MB%20of%20memory%20at%20a%20speed%0Aof%2068.24%20FPS%2C%20which%20can%20be%20viable%20for%20mobile%20terminals%20and%20embedded%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12863v2&entry.124074799=Read"},
{"title": "Reducing Barriers to the Use of Marginalised Music Genres in AI", "author": "Nick Bryan-Kinns and Zijin Li", "abstract": "  AI systems for high quality music generation typically rely on extremely\nlarge musical datasets to train the AI models. This creates barriers to\ngenerating music beyond the genres represented in dominant datasets such as\nWestern Classical music or pop music. We undertook a 4 month international\nresearch project summarised in this paper to explore the eXplainable AI (XAI)\nchallenges and opportunities associated with reducing barriers to using\nmarginalised genres of music with AI models. XAI opportunities identified\nincluded topics of improving transparency and control of AI models, explaining\nthe ethics and bias of AI models, fine tuning large models with small datasets\nto reduce bias, and explaining style-transfer opportunities with AI models.\nParticipants in the research emphasised that whilst it is hard to work with\nsmall datasets such as marginalised music and AI, such approaches strengthen\ncultural representation of underrepresented cultures and contribute to\naddressing issues of bias of deep learning models. We are now building on this\nproject to bring together a global International Responsible AI Music community\nand invite people to join our network.\n", "link": "http://arxiv.org/abs/2407.13439v1", "date": "2024-07-18", "relevancy": 2.1689, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4487}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4265}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reducing%20Barriers%20to%20the%20Use%20of%20Marginalised%20Music%20Genres%20in%20AI&body=Title%3A%20Reducing%20Barriers%20to%20the%20Use%20of%20Marginalised%20Music%20Genres%20in%20AI%0AAuthor%3A%20Nick%20Bryan-Kinns%20and%20Zijin%20Li%0AAbstract%3A%20%20%20AI%20systems%20for%20high%20quality%20music%20generation%20typically%20rely%20on%20extremely%0Alarge%20musical%20datasets%20to%20train%20the%20AI%20models.%20This%20creates%20barriers%20to%0Agenerating%20music%20beyond%20the%20genres%20represented%20in%20dominant%20datasets%20such%20as%0AWestern%20Classical%20music%20or%20pop%20music.%20We%20undertook%20a%204%20month%20international%0Aresearch%20project%20summarised%20in%20this%20paper%20to%20explore%20the%20eXplainable%20AI%20%28XAI%29%0Achallenges%20and%20opportunities%20associated%20with%20reducing%20barriers%20to%20using%0Amarginalised%20genres%20of%20music%20with%20AI%20models.%20XAI%20opportunities%20identified%0Aincluded%20topics%20of%20improving%20transparency%20and%20control%20of%20AI%20models%2C%20explaining%0Athe%20ethics%20and%20bias%20of%20AI%20models%2C%20fine%20tuning%20large%20models%20with%20small%20datasets%0Ato%20reduce%20bias%2C%20and%20explaining%20style-transfer%20opportunities%20with%20AI%20models.%0AParticipants%20in%20the%20research%20emphasised%20that%20whilst%20it%20is%20hard%20to%20work%20with%0Asmall%20datasets%20such%20as%20marginalised%20music%20and%20AI%2C%20such%20approaches%20strengthen%0Acultural%20representation%20of%20underrepresented%20cultures%20and%20contribute%20to%0Aaddressing%20issues%20of%20bias%20of%20deep%20learning%20models.%20We%20are%20now%20building%20on%20this%0Aproject%20to%20bring%20together%20a%20global%20International%20Responsible%20AI%20Music%20community%0Aand%20invite%20people%20to%20join%20our%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducing%2520Barriers%2520to%2520the%2520Use%2520of%2520Marginalised%2520Music%2520Genres%2520in%2520AI%26entry.906535625%3DNick%2520Bryan-Kinns%2520and%2520Zijin%2520Li%26entry.1292438233%3D%2520%2520AI%2520systems%2520for%2520high%2520quality%2520music%2520generation%2520typically%2520rely%2520on%2520extremely%250Alarge%2520musical%2520datasets%2520to%2520train%2520the%2520AI%2520models.%2520This%2520creates%2520barriers%2520to%250Agenerating%2520music%2520beyond%2520the%2520genres%2520represented%2520in%2520dominant%2520datasets%2520such%2520as%250AWestern%2520Classical%2520music%2520or%2520pop%2520music.%2520We%2520undertook%2520a%25204%2520month%2520international%250Aresearch%2520project%2520summarised%2520in%2520this%2520paper%2520to%2520explore%2520the%2520eXplainable%2520AI%2520%2528XAI%2529%250Achallenges%2520and%2520opportunities%2520associated%2520with%2520reducing%2520barriers%2520to%2520using%250Amarginalised%2520genres%2520of%2520music%2520with%2520AI%2520models.%2520XAI%2520opportunities%2520identified%250Aincluded%2520topics%2520of%2520improving%2520transparency%2520and%2520control%2520of%2520AI%2520models%252C%2520explaining%250Athe%2520ethics%2520and%2520bias%2520of%2520AI%2520models%252C%2520fine%2520tuning%2520large%2520models%2520with%2520small%2520datasets%250Ato%2520reduce%2520bias%252C%2520and%2520explaining%2520style-transfer%2520opportunities%2520with%2520AI%2520models.%250AParticipants%2520in%2520the%2520research%2520emphasised%2520that%2520whilst%2520it%2520is%2520hard%2520to%2520work%2520with%250Asmall%2520datasets%2520such%2520as%2520marginalised%2520music%2520and%2520AI%252C%2520such%2520approaches%2520strengthen%250Acultural%2520representation%2520of%2520underrepresented%2520cultures%2520and%2520contribute%2520to%250Aaddressing%2520issues%2520of%2520bias%2520of%2520deep%2520learning%2520models.%2520We%2520are%2520now%2520building%2520on%2520this%250Aproject%2520to%2520bring%2520together%2520a%2520global%2520International%2520Responsible%2520AI%2520Music%2520community%250Aand%2520invite%2520people%2520to%2520join%2520our%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20Barriers%20to%20the%20Use%20of%20Marginalised%20Music%20Genres%20in%20AI&entry.906535625=Nick%20Bryan-Kinns%20and%20Zijin%20Li&entry.1292438233=%20%20AI%20systems%20for%20high%20quality%20music%20generation%20typically%20rely%20on%20extremely%0Alarge%20musical%20datasets%20to%20train%20the%20AI%20models.%20This%20creates%20barriers%20to%0Agenerating%20music%20beyond%20the%20genres%20represented%20in%20dominant%20datasets%20such%20as%0AWestern%20Classical%20music%20or%20pop%20music.%20We%20undertook%20a%204%20month%20international%0Aresearch%20project%20summarised%20in%20this%20paper%20to%20explore%20the%20eXplainable%20AI%20%28XAI%29%0Achallenges%20and%20opportunities%20associated%20with%20reducing%20barriers%20to%20using%0Amarginalised%20genres%20of%20music%20with%20AI%20models.%20XAI%20opportunities%20identified%0Aincluded%20topics%20of%20improving%20transparency%20and%20control%20of%20AI%20models%2C%20explaining%0Athe%20ethics%20and%20bias%20of%20AI%20models%2C%20fine%20tuning%20large%20models%20with%20small%20datasets%0Ato%20reduce%20bias%2C%20and%20explaining%20style-transfer%20opportunities%20with%20AI%20models.%0AParticipants%20in%20the%20research%20emphasised%20that%20whilst%20it%20is%20hard%20to%20work%20with%0Asmall%20datasets%20such%20as%20marginalised%20music%20and%20AI%2C%20such%20approaches%20strengthen%0Acultural%20representation%20of%20underrepresented%20cultures%20and%20contribute%20to%0Aaddressing%20issues%20of%20bias%20of%20deep%20learning%20models.%20We%20are%20now%20building%20on%20this%0Aproject%20to%20bring%20together%20a%20global%20International%20Responsible%20AI%20Music%20community%0Aand%20invite%20people%20to%20join%20our%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13439v1&entry.124074799=Read"},
{"title": "Semantic Residual Prompts for Continual Learning", "author": "Martin Menabue and Emanuele Frascaroli and Matteo Boschini and Enver Sangineto and Lorenzo Bonicelli and Angelo Porrello and Simone Calderara", "abstract": "  Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained\nmodel and train a few parameter vectors termed prompts. Most of these methods\norganize these vectors in a pool of key-value pairs and use the input image as\nquery to retrieve the prompts (values). However, as keys are learned while\ntasks progress, the prompting selection strategy is itself subject to\ncatastrophic forgetting, an issue often overlooked by existing approaches. For\ninstance, prompts introduced to accommodate new tasks might end up interfering\nwith previously learned prompts. To make the selection strategy more stable, we\nleverage a foundation model (CLIP) to select our prompts within a two-level\nadaptation mechanism. Specifically, the first level leverages a standard\ntextual prompt pool for the CLIP textual encoder, leading to stable class\nprototypes. The second level, instead, uses these prototypes along with the\nquery image as keys to index a second pool. The retrieved prompts serve to\nadapt a pre-trained ViT, granting plasticity. In doing so, we also propose a\nnovel residual mechanism to transfer CLIP semantics to the ViT layers. Through\nextensive analysis on established CL benchmarks, we show that our method\nsignificantly outperforms both state-of-the-art CL approaches and the zero-shot\nCLIP test. Notably, our findings hold true even for datasets with a substantial\ndomain gap w.r.t. the pre-training knowledge of the backbone model, as\nshowcased by experiments on satellite imagery and medical datasets. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n", "link": "http://arxiv.org/abs/2403.06870v3", "date": "2024-07-18", "relevancy": 2.1672, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5909}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5102}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Residual%20Prompts%20for%20Continual%20Learning&body=Title%3A%20Semantic%20Residual%20Prompts%20for%20Continual%20Learning%0AAuthor%3A%20Martin%20Menabue%20and%20Emanuele%20Frascaroli%20and%20Matteo%20Boschini%20and%20Enver%20Sangineto%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20Prompt-tuning%20methods%20for%20Continual%20Learning%20%28CL%29%20freeze%20a%20large%20pre-trained%0Amodel%20and%20train%20a%20few%20parameter%20vectors%20termed%20prompts.%20Most%20of%20these%20methods%0Aorganize%20these%20vectors%20in%20a%20pool%20of%20key-value%20pairs%20and%20use%20the%20input%20image%20as%0Aquery%20to%20retrieve%20the%20prompts%20%28values%29.%20However%2C%20as%20keys%20are%20learned%20while%0Atasks%20progress%2C%20the%20prompting%20selection%20strategy%20is%20itself%20subject%20to%0Acatastrophic%20forgetting%2C%20an%20issue%20often%20overlooked%20by%20existing%20approaches.%20For%0Ainstance%2C%20prompts%20introduced%20to%20accommodate%20new%20tasks%20might%20end%20up%20interfering%0Awith%20previously%20learned%20prompts.%20To%20make%20the%20selection%20strategy%20more%20stable%2C%20we%0Aleverage%20a%20foundation%20model%20%28CLIP%29%20to%20select%20our%20prompts%20within%20a%20two-level%0Aadaptation%20mechanism.%20Specifically%2C%20the%20first%20level%20leverages%20a%20standard%0Atextual%20prompt%20pool%20for%20the%20CLIP%20textual%20encoder%2C%20leading%20to%20stable%20class%0Aprototypes.%20The%20second%20level%2C%20instead%2C%20uses%20these%20prototypes%20along%20with%20the%0Aquery%20image%20as%20keys%20to%20index%20a%20second%20pool.%20The%20retrieved%20prompts%20serve%20to%0Aadapt%20a%20pre-trained%20ViT%2C%20granting%20plasticity.%20In%20doing%20so%2C%20we%20also%20propose%20a%0Anovel%20residual%20mechanism%20to%20transfer%20CLIP%20semantics%20to%20the%20ViT%20layers.%20Through%0Aextensive%20analysis%20on%20established%20CL%20benchmarks%2C%20we%20show%20that%20our%20method%0Asignificantly%20outperforms%20both%20state-of-the-art%20CL%20approaches%20and%20the%20zero-shot%0ACLIP%20test.%20Notably%2C%20our%20findings%20hold%20true%20even%20for%20datasets%20with%20a%20substantial%0Adomain%20gap%20w.r.t.%20the%20pre-training%20knowledge%20of%20the%20backbone%20model%2C%20as%0Ashowcased%20by%20experiments%20on%20satellite%20imagery%20and%20medical%20datasets.%20The%0Acodebase%20is%20available%20at%20https%3A//github.com/aimagelab/mammoth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06870v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Residual%2520Prompts%2520for%2520Continual%2520Learning%26entry.906535625%3DMartin%2520Menabue%2520and%2520Emanuele%2520Frascaroli%2520and%2520Matteo%2520Boschini%2520and%2520Enver%2520Sangineto%2520and%2520Lorenzo%2520Bonicelli%2520and%2520Angelo%2520Porrello%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520Prompt-tuning%2520methods%2520for%2520Continual%2520Learning%2520%2528CL%2529%2520freeze%2520a%2520large%2520pre-trained%250Amodel%2520and%2520train%2520a%2520few%2520parameter%2520vectors%2520termed%2520prompts.%2520Most%2520of%2520these%2520methods%250Aorganize%2520these%2520vectors%2520in%2520a%2520pool%2520of%2520key-value%2520pairs%2520and%2520use%2520the%2520input%2520image%2520as%250Aquery%2520to%2520retrieve%2520the%2520prompts%2520%2528values%2529.%2520However%252C%2520as%2520keys%2520are%2520learned%2520while%250Atasks%2520progress%252C%2520the%2520prompting%2520selection%2520strategy%2520is%2520itself%2520subject%2520to%250Acatastrophic%2520forgetting%252C%2520an%2520issue%2520often%2520overlooked%2520by%2520existing%2520approaches.%2520For%250Ainstance%252C%2520prompts%2520introduced%2520to%2520accommodate%2520new%2520tasks%2520might%2520end%2520up%2520interfering%250Awith%2520previously%2520learned%2520prompts.%2520To%2520make%2520the%2520selection%2520strategy%2520more%2520stable%252C%2520we%250Aleverage%2520a%2520foundation%2520model%2520%2528CLIP%2529%2520to%2520select%2520our%2520prompts%2520within%2520a%2520two-level%250Aadaptation%2520mechanism.%2520Specifically%252C%2520the%2520first%2520level%2520leverages%2520a%2520standard%250Atextual%2520prompt%2520pool%2520for%2520the%2520CLIP%2520textual%2520encoder%252C%2520leading%2520to%2520stable%2520class%250Aprototypes.%2520The%2520second%2520level%252C%2520instead%252C%2520uses%2520these%2520prototypes%2520along%2520with%2520the%250Aquery%2520image%2520as%2520keys%2520to%2520index%2520a%2520second%2520pool.%2520The%2520retrieved%2520prompts%2520serve%2520to%250Aadapt%2520a%2520pre-trained%2520ViT%252C%2520granting%2520plasticity.%2520In%2520doing%2520so%252C%2520we%2520also%2520propose%2520a%250Anovel%2520residual%2520mechanism%2520to%2520transfer%2520CLIP%2520semantics%2520to%2520the%2520ViT%2520layers.%2520Through%250Aextensive%2520analysis%2520on%2520established%2520CL%2520benchmarks%252C%2520we%2520show%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520both%2520state-of-the-art%2520CL%2520approaches%2520and%2520the%2520zero-shot%250ACLIP%2520test.%2520Notably%252C%2520our%2520findings%2520hold%2520true%2520even%2520for%2520datasets%2520with%2520a%2520substantial%250Adomain%2520gap%2520w.r.t.%2520the%2520pre-training%2520knowledge%2520of%2520the%2520backbone%2520model%252C%2520as%250Ashowcased%2520by%2520experiments%2520on%2520satellite%2520imagery%2520and%2520medical%2520datasets.%2520The%250Acodebase%2520is%2520available%2520at%2520https%253A//github.com/aimagelab/mammoth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06870v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Residual%20Prompts%20for%20Continual%20Learning&entry.906535625=Martin%20Menabue%20and%20Emanuele%20Frascaroli%20and%20Matteo%20Boschini%20and%20Enver%20Sangineto%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara&entry.1292438233=%20%20Prompt-tuning%20methods%20for%20Continual%20Learning%20%28CL%29%20freeze%20a%20large%20pre-trained%0Amodel%20and%20train%20a%20few%20parameter%20vectors%20termed%20prompts.%20Most%20of%20these%20methods%0Aorganize%20these%20vectors%20in%20a%20pool%20of%20key-value%20pairs%20and%20use%20the%20input%20image%20as%0Aquery%20to%20retrieve%20the%20prompts%20%28values%29.%20However%2C%20as%20keys%20are%20learned%20while%0Atasks%20progress%2C%20the%20prompting%20selection%20strategy%20is%20itself%20subject%20to%0Acatastrophic%20forgetting%2C%20an%20issue%20often%20overlooked%20by%20existing%20approaches.%20For%0Ainstance%2C%20prompts%20introduced%20to%20accommodate%20new%20tasks%20might%20end%20up%20interfering%0Awith%20previously%20learned%20prompts.%20To%20make%20the%20selection%20strategy%20more%20stable%2C%20we%0Aleverage%20a%20foundation%20model%20%28CLIP%29%20to%20select%20our%20prompts%20within%20a%20two-level%0Aadaptation%20mechanism.%20Specifically%2C%20the%20first%20level%20leverages%20a%20standard%0Atextual%20prompt%20pool%20for%20the%20CLIP%20textual%20encoder%2C%20leading%20to%20stable%20class%0Aprototypes.%20The%20second%20level%2C%20instead%2C%20uses%20these%20prototypes%20along%20with%20the%0Aquery%20image%20as%20keys%20to%20index%20a%20second%20pool.%20The%20retrieved%20prompts%20serve%20to%0Aadapt%20a%20pre-trained%20ViT%2C%20granting%20plasticity.%20In%20doing%20so%2C%20we%20also%20propose%20a%0Anovel%20residual%20mechanism%20to%20transfer%20CLIP%20semantics%20to%20the%20ViT%20layers.%20Through%0Aextensive%20analysis%20on%20established%20CL%20benchmarks%2C%20we%20show%20that%20our%20method%0Asignificantly%20outperforms%20both%20state-of-the-art%20CL%20approaches%20and%20the%20zero-shot%0ACLIP%20test.%20Notably%2C%20our%20findings%20hold%20true%20even%20for%20datasets%20with%20a%20substantial%0Adomain%20gap%20w.r.t.%20the%20pre-training%20knowledge%20of%20the%20backbone%20model%2C%20as%0Ashowcased%20by%20experiments%20on%20satellite%20imagery%20and%20medical%20datasets.%20The%0Acodebase%20is%20available%20at%20https%3A//github.com/aimagelab/mammoth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06870v3&entry.124074799=Read"},
{"title": "WiNet: Wavelet-based Incremental Learning for Efficient Medical Image\n  Registration", "author": "Xinxing Cheng and Xi Jia and Wenqi Lu and Qiufu Li and Linlin Shen and Alexander Krull and Jinming Duan", "abstract": "  Deep image registration has demonstrated exceptional accuracy and fast\ninference. Recent advances have adopted either multiple cascades or pyramid\narchitectures to estimate dense deformation fields in a coarse-to-fine manner.\nHowever, due to the cascaded nature and repeated composition/warping operations\non feature maps, these methods negatively increase memory usage during training\nand testing. Moreover, such approaches lack explicit constraints on the\nlearning process of small deformations at different scales, thus lacking\nexplainability. In this study, we introduce a model-driven WiNet that\nincrementally estimates scale-wise wavelet coefficients for the\ndisplacement/velocity field across various scales, utilizing the wavelet\ncoefficients derived from the original input image pair. By exploiting the\nproperties of the wavelet transform, these estimated coefficients facilitate\nthe seamless reconstruction of a full-resolution displacement/velocity field\nvia our devised inverse discrete wavelet transform (IDWT) layer. This approach\navoids the complexities of cascading networks or composition operations, making\nour WiNet an explainable and efficient competitor with other coarse-to-fine\nmethods. Extensive experimental results from two 3D datasets show that our\nWiNet is accurate and GPU efficient. The code is available at\nhttps://github.com/x-xc/WiNet .\n", "link": "http://arxiv.org/abs/2407.13426v1", "date": "2024-07-18", "relevancy": 2.1616, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5572}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5559}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WiNet%3A%20Wavelet-based%20Incremental%20Learning%20for%20Efficient%20Medical%20Image%0A%20%20Registration&body=Title%3A%20WiNet%3A%20Wavelet-based%20Incremental%20Learning%20for%20Efficient%20Medical%20Image%0A%20%20Registration%0AAuthor%3A%20Xinxing%20Cheng%20and%20Xi%20Jia%20and%20Wenqi%20Lu%20and%20Qiufu%20Li%20and%20Linlin%20Shen%20and%20Alexander%20Krull%20and%20Jinming%20Duan%0AAbstract%3A%20%20%20Deep%20image%20registration%20has%20demonstrated%20exceptional%20accuracy%20and%20fast%0Ainference.%20Recent%20advances%20have%20adopted%20either%20multiple%20cascades%20or%20pyramid%0Aarchitectures%20to%20estimate%20dense%20deformation%20fields%20in%20a%20coarse-to-fine%20manner.%0AHowever%2C%20due%20to%20the%20cascaded%20nature%20and%20repeated%20composition/warping%20operations%0Aon%20feature%20maps%2C%20these%20methods%20negatively%20increase%20memory%20usage%20during%20training%0Aand%20testing.%20Moreover%2C%20such%20approaches%20lack%20explicit%20constraints%20on%20the%0Alearning%20process%20of%20small%20deformations%20at%20different%20scales%2C%20thus%20lacking%0Aexplainability.%20In%20this%20study%2C%20we%20introduce%20a%20model-driven%20WiNet%20that%0Aincrementally%20estimates%20scale-wise%20wavelet%20coefficients%20for%20the%0Adisplacement/velocity%20field%20across%20various%20scales%2C%20utilizing%20the%20wavelet%0Acoefficients%20derived%20from%20the%20original%20input%20image%20pair.%20By%20exploiting%20the%0Aproperties%20of%20the%20wavelet%20transform%2C%20these%20estimated%20coefficients%20facilitate%0Athe%20seamless%20reconstruction%20of%20a%20full-resolution%20displacement/velocity%20field%0Avia%20our%20devised%20inverse%20discrete%20wavelet%20transform%20%28IDWT%29%20layer.%20This%20approach%0Aavoids%20the%20complexities%20of%20cascading%20networks%20or%20composition%20operations%2C%20making%0Aour%20WiNet%20an%20explainable%20and%20efficient%20competitor%20with%20other%20coarse-to-fine%0Amethods.%20Extensive%20experimental%20results%20from%20two%203D%20datasets%20show%20that%20our%0AWiNet%20is%20accurate%20and%20GPU%20efficient.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/x-xc/WiNet%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWiNet%253A%2520Wavelet-based%2520Incremental%2520Learning%2520for%2520Efficient%2520Medical%2520Image%250A%2520%2520Registration%26entry.906535625%3DXinxing%2520Cheng%2520and%2520Xi%2520Jia%2520and%2520Wenqi%2520Lu%2520and%2520Qiufu%2520Li%2520and%2520Linlin%2520Shen%2520and%2520Alexander%2520Krull%2520and%2520Jinming%2520Duan%26entry.1292438233%3D%2520%2520Deep%2520image%2520registration%2520has%2520demonstrated%2520exceptional%2520accuracy%2520and%2520fast%250Ainference.%2520Recent%2520advances%2520have%2520adopted%2520either%2520multiple%2520cascades%2520or%2520pyramid%250Aarchitectures%2520to%2520estimate%2520dense%2520deformation%2520fields%2520in%2520a%2520coarse-to-fine%2520manner.%250AHowever%252C%2520due%2520to%2520the%2520cascaded%2520nature%2520and%2520repeated%2520composition/warping%2520operations%250Aon%2520feature%2520maps%252C%2520these%2520methods%2520negatively%2520increase%2520memory%2520usage%2520during%2520training%250Aand%2520testing.%2520Moreover%252C%2520such%2520approaches%2520lack%2520explicit%2520constraints%2520on%2520the%250Alearning%2520process%2520of%2520small%2520deformations%2520at%2520different%2520scales%252C%2520thus%2520lacking%250Aexplainability.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520model-driven%2520WiNet%2520that%250Aincrementally%2520estimates%2520scale-wise%2520wavelet%2520coefficients%2520for%2520the%250Adisplacement/velocity%2520field%2520across%2520various%2520scales%252C%2520utilizing%2520the%2520wavelet%250Acoefficients%2520derived%2520from%2520the%2520original%2520input%2520image%2520pair.%2520By%2520exploiting%2520the%250Aproperties%2520of%2520the%2520wavelet%2520transform%252C%2520these%2520estimated%2520coefficients%2520facilitate%250Athe%2520seamless%2520reconstruction%2520of%2520a%2520full-resolution%2520displacement/velocity%2520field%250Avia%2520our%2520devised%2520inverse%2520discrete%2520wavelet%2520transform%2520%2528IDWT%2529%2520layer.%2520This%2520approach%250Aavoids%2520the%2520complexities%2520of%2520cascading%2520networks%2520or%2520composition%2520operations%252C%2520making%250Aour%2520WiNet%2520an%2520explainable%2520and%2520efficient%2520competitor%2520with%2520other%2520coarse-to-fine%250Amethods.%2520Extensive%2520experimental%2520results%2520from%2520two%25203D%2520datasets%2520show%2520that%2520our%250AWiNet%2520is%2520accurate%2520and%2520GPU%2520efficient.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/x-xc/WiNet%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WiNet%3A%20Wavelet-based%20Incremental%20Learning%20for%20Efficient%20Medical%20Image%0A%20%20Registration&entry.906535625=Xinxing%20Cheng%20and%20Xi%20Jia%20and%20Wenqi%20Lu%20and%20Qiufu%20Li%20and%20Linlin%20Shen%20and%20Alexander%20Krull%20and%20Jinming%20Duan&entry.1292438233=%20%20Deep%20image%20registration%20has%20demonstrated%20exceptional%20accuracy%20and%20fast%0Ainference.%20Recent%20advances%20have%20adopted%20either%20multiple%20cascades%20or%20pyramid%0Aarchitectures%20to%20estimate%20dense%20deformation%20fields%20in%20a%20coarse-to-fine%20manner.%0AHowever%2C%20due%20to%20the%20cascaded%20nature%20and%20repeated%20composition/warping%20operations%0Aon%20feature%20maps%2C%20these%20methods%20negatively%20increase%20memory%20usage%20during%20training%0Aand%20testing.%20Moreover%2C%20such%20approaches%20lack%20explicit%20constraints%20on%20the%0Alearning%20process%20of%20small%20deformations%20at%20different%20scales%2C%20thus%20lacking%0Aexplainability.%20In%20this%20study%2C%20we%20introduce%20a%20model-driven%20WiNet%20that%0Aincrementally%20estimates%20scale-wise%20wavelet%20coefficients%20for%20the%0Adisplacement/velocity%20field%20across%20various%20scales%2C%20utilizing%20the%20wavelet%0Acoefficients%20derived%20from%20the%20original%20input%20image%20pair.%20By%20exploiting%20the%0Aproperties%20of%20the%20wavelet%20transform%2C%20these%20estimated%20coefficients%20facilitate%0Athe%20seamless%20reconstruction%20of%20a%20full-resolution%20displacement/velocity%20field%0Avia%20our%20devised%20inverse%20discrete%20wavelet%20transform%20%28IDWT%29%20layer.%20This%20approach%0Aavoids%20the%20complexities%20of%20cascading%20networks%20or%20composition%20operations%2C%20making%0Aour%20WiNet%20an%20explainable%20and%20efficient%20competitor%20with%20other%20coarse-to-fine%0Amethods.%20Extensive%20experimental%20results%20from%20two%203D%20datasets%20show%20that%20our%0AWiNet%20is%20accurate%20and%20GPU%20efficient.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/x-xc/WiNet%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13426v1&entry.124074799=Read"},
{"title": "GDDS: A Single Domain Generalized Defect Detection Frame of Open World\n  Scenario using Gather and Distribute Domain-shift Suppression Network", "author": "Haiyong Chen and Yaxiu Zhang and Yan Zhang and Xin Zhang and Xingwei Yan", "abstract": "  Efficient and intelligent surface defect detection of photovoltaic modules is\ncrucial for improving the quality of photovoltaic modules and ensuring the\nreliable operation of large-scale infrastructure. However, the scenario\ncharacteristics of data distribution deviation make the construction of defect\ndetection models for open world scenarios such as photovoltaic manufacturing\nand power plant inspections a challenge. Therefore, we propose the Gather and\nDistribute Domain shift Suppression Network (GDDS). It adopts a single domain\ngeneralized method that is completely independent of the test samples to\naddress the problem of distribution shift. Using a one-stage network as the\nbaseline network breaks through the limitations of traditional domain\ngeneralization methods that typically use two-stage networks. It not only\nbalances detection accuracy and speed but also simplifies the model deployment\nand application process. The GDDS includes two modules: DeepSpine Module and\nGather and Distribute Module. Specifically, the DeepSpine Module applies a\nwider range of contextual information and suppresses background style shift by\nacquiring and concatenating multi-scale features. The Gather and Distribute\nModule collects and distributes global information to achieve cross layer\ninteractive learning of multi-scale channel features and suppress defect\ninstance shift. Furthermore, the GDDS utilizes normalized Wasserstein distance\nfor similarity measurement, reducing measurement errors caused by bounding box\nposition deviations. We conducted a comprehensive evaluation of GDDS on the EL\nendogenous shift dataset and Photovoltaic inspection infrared image dataset.\nThe experimental results showed that GDDS can adapt to defect detection in open\nworld scenarios faster and better than other state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.13417v1", "date": "2024-07-18", "relevancy": 2.1575, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5631}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5358}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GDDS%3A%20A%20Single%20Domain%20Generalized%20Defect%20Detection%20Frame%20of%20Open%20World%0A%20%20Scenario%20using%20Gather%20and%20Distribute%20Domain-shift%20Suppression%20Network&body=Title%3A%20GDDS%3A%20A%20Single%20Domain%20Generalized%20Defect%20Detection%20Frame%20of%20Open%20World%0A%20%20Scenario%20using%20Gather%20and%20Distribute%20Domain-shift%20Suppression%20Network%0AAuthor%3A%20Haiyong%20Chen%20and%20Yaxiu%20Zhang%20and%20Yan%20Zhang%20and%20Xin%20Zhang%20and%20Xingwei%20Yan%0AAbstract%3A%20%20%20Efficient%20and%20intelligent%20surface%20defect%20detection%20of%20photovoltaic%20modules%20is%0Acrucial%20for%20improving%20the%20quality%20of%20photovoltaic%20modules%20and%20ensuring%20the%0Areliable%20operation%20of%20large-scale%20infrastructure.%20However%2C%20the%20scenario%0Acharacteristics%20of%20data%20distribution%20deviation%20make%20the%20construction%20of%20defect%0Adetection%20models%20for%20open%20world%20scenarios%20such%20as%20photovoltaic%20manufacturing%0Aand%20power%20plant%20inspections%20a%20challenge.%20Therefore%2C%20we%20propose%20the%20Gather%20and%0ADistribute%20Domain%20shift%20Suppression%20Network%20%28GDDS%29.%20It%20adopts%20a%20single%20domain%0Ageneralized%20method%20that%20is%20completely%20independent%20of%20the%20test%20samples%20to%0Aaddress%20the%20problem%20of%20distribution%20shift.%20Using%20a%20one-stage%20network%20as%20the%0Abaseline%20network%20breaks%20through%20the%20limitations%20of%20traditional%20domain%0Ageneralization%20methods%20that%20typically%20use%20two-stage%20networks.%20It%20not%20only%0Abalances%20detection%20accuracy%20and%20speed%20but%20also%20simplifies%20the%20model%20deployment%0Aand%20application%20process.%20The%20GDDS%20includes%20two%20modules%3A%20DeepSpine%20Module%20and%0AGather%20and%20Distribute%20Module.%20Specifically%2C%20the%20DeepSpine%20Module%20applies%20a%0Awider%20range%20of%20contextual%20information%20and%20suppresses%20background%20style%20shift%20by%0Aacquiring%20and%20concatenating%20multi-scale%20features.%20The%20Gather%20and%20Distribute%0AModule%20collects%20and%20distributes%20global%20information%20to%20achieve%20cross%20layer%0Ainteractive%20learning%20of%20multi-scale%20channel%20features%20and%20suppress%20defect%0Ainstance%20shift.%20Furthermore%2C%20the%20GDDS%20utilizes%20normalized%20Wasserstein%20distance%0Afor%20similarity%20measurement%2C%20reducing%20measurement%20errors%20caused%20by%20bounding%20box%0Aposition%20deviations.%20We%20conducted%20a%20comprehensive%20evaluation%20of%20GDDS%20on%20the%20EL%0Aendogenous%20shift%20dataset%20and%20Photovoltaic%20inspection%20infrared%20image%20dataset.%0AThe%20experimental%20results%20showed%20that%20GDDS%20can%20adapt%20to%20defect%20detection%20in%20open%0Aworld%20scenarios%20faster%20and%20better%20than%20other%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGDDS%253A%2520A%2520Single%2520Domain%2520Generalized%2520Defect%2520Detection%2520Frame%2520of%2520Open%2520World%250A%2520%2520Scenario%2520using%2520Gather%2520and%2520Distribute%2520Domain-shift%2520Suppression%2520Network%26entry.906535625%3DHaiyong%2520Chen%2520and%2520Yaxiu%2520Zhang%2520and%2520Yan%2520Zhang%2520and%2520Xin%2520Zhang%2520and%2520Xingwei%2520Yan%26entry.1292438233%3D%2520%2520Efficient%2520and%2520intelligent%2520surface%2520defect%2520detection%2520of%2520photovoltaic%2520modules%2520is%250Acrucial%2520for%2520improving%2520the%2520quality%2520of%2520photovoltaic%2520modules%2520and%2520ensuring%2520the%250Areliable%2520operation%2520of%2520large-scale%2520infrastructure.%2520However%252C%2520the%2520scenario%250Acharacteristics%2520of%2520data%2520distribution%2520deviation%2520make%2520the%2520construction%2520of%2520defect%250Adetection%2520models%2520for%2520open%2520world%2520scenarios%2520such%2520as%2520photovoltaic%2520manufacturing%250Aand%2520power%2520plant%2520inspections%2520a%2520challenge.%2520Therefore%252C%2520we%2520propose%2520the%2520Gather%2520and%250ADistribute%2520Domain%2520shift%2520Suppression%2520Network%2520%2528GDDS%2529.%2520It%2520adopts%2520a%2520single%2520domain%250Ageneralized%2520method%2520that%2520is%2520completely%2520independent%2520of%2520the%2520test%2520samples%2520to%250Aaddress%2520the%2520problem%2520of%2520distribution%2520shift.%2520Using%2520a%2520one-stage%2520network%2520as%2520the%250Abaseline%2520network%2520breaks%2520through%2520the%2520limitations%2520of%2520traditional%2520domain%250Ageneralization%2520methods%2520that%2520typically%2520use%2520two-stage%2520networks.%2520It%2520not%2520only%250Abalances%2520detection%2520accuracy%2520and%2520speed%2520but%2520also%2520simplifies%2520the%2520model%2520deployment%250Aand%2520application%2520process.%2520The%2520GDDS%2520includes%2520two%2520modules%253A%2520DeepSpine%2520Module%2520and%250AGather%2520and%2520Distribute%2520Module.%2520Specifically%252C%2520the%2520DeepSpine%2520Module%2520applies%2520a%250Awider%2520range%2520of%2520contextual%2520information%2520and%2520suppresses%2520background%2520style%2520shift%2520by%250Aacquiring%2520and%2520concatenating%2520multi-scale%2520features.%2520The%2520Gather%2520and%2520Distribute%250AModule%2520collects%2520and%2520distributes%2520global%2520information%2520to%2520achieve%2520cross%2520layer%250Ainteractive%2520learning%2520of%2520multi-scale%2520channel%2520features%2520and%2520suppress%2520defect%250Ainstance%2520shift.%2520Furthermore%252C%2520the%2520GDDS%2520utilizes%2520normalized%2520Wasserstein%2520distance%250Afor%2520similarity%2520measurement%252C%2520reducing%2520measurement%2520errors%2520caused%2520by%2520bounding%2520box%250Aposition%2520deviations.%2520We%2520conducted%2520a%2520comprehensive%2520evaluation%2520of%2520GDDS%2520on%2520the%2520EL%250Aendogenous%2520shift%2520dataset%2520and%2520Photovoltaic%2520inspection%2520infrared%2520image%2520dataset.%250AThe%2520experimental%2520results%2520showed%2520that%2520GDDS%2520can%2520adapt%2520to%2520defect%2520detection%2520in%2520open%250Aworld%2520scenarios%2520faster%2520and%2520better%2520than%2520other%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GDDS%3A%20A%20Single%20Domain%20Generalized%20Defect%20Detection%20Frame%20of%20Open%20World%0A%20%20Scenario%20using%20Gather%20and%20Distribute%20Domain-shift%20Suppression%20Network&entry.906535625=Haiyong%20Chen%20and%20Yaxiu%20Zhang%20and%20Yan%20Zhang%20and%20Xin%20Zhang%20and%20Xingwei%20Yan&entry.1292438233=%20%20Efficient%20and%20intelligent%20surface%20defect%20detection%20of%20photovoltaic%20modules%20is%0Acrucial%20for%20improving%20the%20quality%20of%20photovoltaic%20modules%20and%20ensuring%20the%0Areliable%20operation%20of%20large-scale%20infrastructure.%20However%2C%20the%20scenario%0Acharacteristics%20of%20data%20distribution%20deviation%20make%20the%20construction%20of%20defect%0Adetection%20models%20for%20open%20world%20scenarios%20such%20as%20photovoltaic%20manufacturing%0Aand%20power%20plant%20inspections%20a%20challenge.%20Therefore%2C%20we%20propose%20the%20Gather%20and%0ADistribute%20Domain%20shift%20Suppression%20Network%20%28GDDS%29.%20It%20adopts%20a%20single%20domain%0Ageneralized%20method%20that%20is%20completely%20independent%20of%20the%20test%20samples%20to%0Aaddress%20the%20problem%20of%20distribution%20shift.%20Using%20a%20one-stage%20network%20as%20the%0Abaseline%20network%20breaks%20through%20the%20limitations%20of%20traditional%20domain%0Ageneralization%20methods%20that%20typically%20use%20two-stage%20networks.%20It%20not%20only%0Abalances%20detection%20accuracy%20and%20speed%20but%20also%20simplifies%20the%20model%20deployment%0Aand%20application%20process.%20The%20GDDS%20includes%20two%20modules%3A%20DeepSpine%20Module%20and%0AGather%20and%20Distribute%20Module.%20Specifically%2C%20the%20DeepSpine%20Module%20applies%20a%0Awider%20range%20of%20contextual%20information%20and%20suppresses%20background%20style%20shift%20by%0Aacquiring%20and%20concatenating%20multi-scale%20features.%20The%20Gather%20and%20Distribute%0AModule%20collects%20and%20distributes%20global%20information%20to%20achieve%20cross%20layer%0Ainteractive%20learning%20of%20multi-scale%20channel%20features%20and%20suppress%20defect%0Ainstance%20shift.%20Furthermore%2C%20the%20GDDS%20utilizes%20normalized%20Wasserstein%20distance%0Afor%20similarity%20measurement%2C%20reducing%20measurement%20errors%20caused%20by%20bounding%20box%0Aposition%20deviations.%20We%20conducted%20a%20comprehensive%20evaluation%20of%20GDDS%20on%20the%20EL%0Aendogenous%20shift%20dataset%20and%20Photovoltaic%20inspection%20infrared%20image%20dataset.%0AThe%20experimental%20results%20showed%20that%20GDDS%20can%20adapt%20to%20defect%20detection%20in%20open%0Aworld%20scenarios%20faster%20and%20better%20than%20other%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13417v1&entry.124074799=Read"},
{"title": "HazeCLIP: Towards Language Guided Real-World Image Dehazing", "author": "Ruiyi Wang and Wenhao Li and Xiaohong Liu and Chunyi Li and Zicheng Zhang and Xiongkuo Min and Guangtao Zhai", "abstract": "  Existing methods have achieved remarkable performance in single image\ndehazing, particularly on synthetic datasets. However, they often struggle with\nreal-world hazy images due to domain shift, limiting their practical\napplicability. This paper introduces HazeCLIP, a language-guided adaptation\nframework designed to enhance the real-world performance of pre-trained\ndehazing networks. Inspired by the Contrastive Language-Image Pre-training\n(CLIP) model's ability to distinguish between hazy and clean images, we utilize\nit to evaluate dehazing results. Combined with a region-specific dehazing\ntechnique and tailored prompt sets, CLIP model accurately identifies hazy\nareas, providing a high-quality, human-like prior that guides the fine-tuning\nprocess of pre-trained networks. Extensive experiments demonstrate that\nHazeCLIP achieves the state-of-the-art performance in real-word image dehazing,\nevaluated through both visual quality and no-reference quality assessments. The\ncode is available: https://github.com/Troivyn/HazeCLIP .\n", "link": "http://arxiv.org/abs/2407.13719v1", "date": "2024-07-18", "relevancy": 2.1537, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5458}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5432}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HazeCLIP%3A%20Towards%20Language%20Guided%20Real-World%20Image%20Dehazing&body=Title%3A%20HazeCLIP%3A%20Towards%20Language%20Guided%20Real-World%20Image%20Dehazing%0AAuthor%3A%20Ruiyi%20Wang%20and%20Wenhao%20Li%20and%20Xiaohong%20Liu%20and%20Chunyi%20Li%20and%20Zicheng%20Zhang%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20Existing%20methods%20have%20achieved%20remarkable%20performance%20in%20single%20image%0Adehazing%2C%20particularly%20on%20synthetic%20datasets.%20However%2C%20they%20often%20struggle%20with%0Areal-world%20hazy%20images%20due%20to%20domain%20shift%2C%20limiting%20their%20practical%0Aapplicability.%20This%20paper%20introduces%20HazeCLIP%2C%20a%20language-guided%20adaptation%0Aframework%20designed%20to%20enhance%20the%20real-world%20performance%20of%20pre-trained%0Adehazing%20networks.%20Inspired%20by%20the%20Contrastive%20Language-Image%20Pre-training%0A%28CLIP%29%20model%27s%20ability%20to%20distinguish%20between%20hazy%20and%20clean%20images%2C%20we%20utilize%0Ait%20to%20evaluate%20dehazing%20results.%20Combined%20with%20a%20region-specific%20dehazing%0Atechnique%20and%20tailored%20prompt%20sets%2C%20CLIP%20model%20accurately%20identifies%20hazy%0Aareas%2C%20providing%20a%20high-quality%2C%20human-like%20prior%20that%20guides%20the%20fine-tuning%0Aprocess%20of%20pre-trained%20networks.%20Extensive%20experiments%20demonstrate%20that%0AHazeCLIP%20achieves%20the%20state-of-the-art%20performance%20in%20real-word%20image%20dehazing%2C%0Aevaluated%20through%20both%20visual%20quality%20and%20no-reference%20quality%20assessments.%20The%0Acode%20is%20available%3A%20https%3A//github.com/Troivyn/HazeCLIP%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHazeCLIP%253A%2520Towards%2520Language%2520Guided%2520Real-World%2520Image%2520Dehazing%26entry.906535625%3DRuiyi%2520Wang%2520and%2520Wenhao%2520Li%2520and%2520Xiaohong%2520Liu%2520and%2520Chunyi%2520Li%2520and%2520Zicheng%2520Zhang%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520Existing%2520methods%2520have%2520achieved%2520remarkable%2520performance%2520in%2520single%2520image%250Adehazing%252C%2520particularly%2520on%2520synthetic%2520datasets.%2520However%252C%2520they%2520often%2520struggle%2520with%250Areal-world%2520hazy%2520images%2520due%2520to%2520domain%2520shift%252C%2520limiting%2520their%2520practical%250Aapplicability.%2520This%2520paper%2520introduces%2520HazeCLIP%252C%2520a%2520language-guided%2520adaptation%250Aframework%2520designed%2520to%2520enhance%2520the%2520real-world%2520performance%2520of%2520pre-trained%250Adehazing%2520networks.%2520Inspired%2520by%2520the%2520Contrastive%2520Language-Image%2520Pre-training%250A%2528CLIP%2529%2520model%2527s%2520ability%2520to%2520distinguish%2520between%2520hazy%2520and%2520clean%2520images%252C%2520we%2520utilize%250Ait%2520to%2520evaluate%2520dehazing%2520results.%2520Combined%2520with%2520a%2520region-specific%2520dehazing%250Atechnique%2520and%2520tailored%2520prompt%2520sets%252C%2520CLIP%2520model%2520accurately%2520identifies%2520hazy%250Aareas%252C%2520providing%2520a%2520high-quality%252C%2520human-like%2520prior%2520that%2520guides%2520the%2520fine-tuning%250Aprocess%2520of%2520pre-trained%2520networks.%2520Extensive%2520experiments%2520demonstrate%2520that%250AHazeCLIP%2520achieves%2520the%2520state-of-the-art%2520performance%2520in%2520real-word%2520image%2520dehazing%252C%250Aevaluated%2520through%2520both%2520visual%2520quality%2520and%2520no-reference%2520quality%2520assessments.%2520The%250Acode%2520is%2520available%253A%2520https%253A//github.com/Troivyn/HazeCLIP%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HazeCLIP%3A%20Towards%20Language%20Guided%20Real-World%20Image%20Dehazing&entry.906535625=Ruiyi%20Wang%20and%20Wenhao%20Li%20and%20Xiaohong%20Liu%20and%20Chunyi%20Li%20and%20Zicheng%20Zhang%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=%20%20Existing%20methods%20have%20achieved%20remarkable%20performance%20in%20single%20image%0Adehazing%2C%20particularly%20on%20synthetic%20datasets.%20However%2C%20they%20often%20struggle%20with%0Areal-world%20hazy%20images%20due%20to%20domain%20shift%2C%20limiting%20their%20practical%0Aapplicability.%20This%20paper%20introduces%20HazeCLIP%2C%20a%20language-guided%20adaptation%0Aframework%20designed%20to%20enhance%20the%20real-world%20performance%20of%20pre-trained%0Adehazing%20networks.%20Inspired%20by%20the%20Contrastive%20Language-Image%20Pre-training%0A%28CLIP%29%20model%27s%20ability%20to%20distinguish%20between%20hazy%20and%20clean%20images%2C%20we%20utilize%0Ait%20to%20evaluate%20dehazing%20results.%20Combined%20with%20a%20region-specific%20dehazing%0Atechnique%20and%20tailored%20prompt%20sets%2C%20CLIP%20model%20accurately%20identifies%20hazy%0Aareas%2C%20providing%20a%20high-quality%2C%20human-like%20prior%20that%20guides%20the%20fine-tuning%0Aprocess%20of%20pre-trained%20networks.%20Extensive%20experiments%20demonstrate%20that%0AHazeCLIP%20achieves%20the%20state-of-the-art%20performance%20in%20real-word%20image%20dehazing%2C%0Aevaluated%20through%20both%20visual%20quality%20and%20no-reference%20quality%20assessments.%20The%0Acode%20is%20available%3A%20https%3A//github.com/Troivyn/HazeCLIP%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13719v1&entry.124074799=Read"},
{"title": "Efficient Image Denoising by Low-Rank Singular Vector Approximations of\n  Geodesics' Gramian Matrix", "author": "Kelum Gajamannage and Yonggi Park and S. M. Mallikarjunaiah and Sunil Mathur", "abstract": "  With the advent of sophisticated cameras, the urge to capture high-quality\nimages has grown enormous. However, the noise contamination of the images\nresults in substandard expectations among the people; thus, image denoising is\nan essential pre-processing step. While the algebraic image processing\nframeworks are sometimes inefficient for this denoising task as they may\nrequire processing of matrices of order equivalent to some power of the order\nof the original image, the neural network image processing frameworks are\nsometimes not robust as they require a lot of similar training samples. Thus,\nhere we present a manifold-based noise filtering method that mainly exploits a\nfew prominent singular vectors of the geodesics' Gramian matrix. Especially,\nthe framework partitions an image, say that of size $n \\times n$, into $n^2$\noverlapping patches of known size such that one patch is centered at each\npixel. Then, the prominent singular vectors, of the Gramian matrix of size $n^2\n\\times n^2$ of the geodesic distances computed over the patch space, are\nutilized to denoise the image. Here, the prominent singular vectors are\nrevealed by efficient, but diverse, approximation techniques, rather than\nexplicitly computing them using frameworks like Singular Value Decomposition\n(SVD) which encounters $\\mathcal{O}(n^6)$ operations. Finally, we compare both\ncomputational time and the noise filtration performance of the proposed\ndenoising algorithm with and without singular vector approximation techniques.\n", "link": "http://arxiv.org/abs/2209.13094v4", "date": "2024-07-18", "relevancy": 2.1532, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5702}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5186}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Image%20Denoising%20by%20Low-Rank%20Singular%20Vector%20Approximations%20of%0A%20%20Geodesics%27%20Gramian%20Matrix&body=Title%3A%20Efficient%20Image%20Denoising%20by%20Low-Rank%20Singular%20Vector%20Approximations%20of%0A%20%20Geodesics%27%20Gramian%20Matrix%0AAuthor%3A%20Kelum%20Gajamannage%20and%20Yonggi%20Park%20and%20S.%20M.%20Mallikarjunaiah%20and%20Sunil%20Mathur%0AAbstract%3A%20%20%20With%20the%20advent%20of%20sophisticated%20cameras%2C%20the%20urge%20to%20capture%20high-quality%0Aimages%20has%20grown%20enormous.%20However%2C%20the%20noise%20contamination%20of%20the%20images%0Aresults%20in%20substandard%20expectations%20among%20the%20people%3B%20thus%2C%20image%20denoising%20is%0Aan%20essential%20pre-processing%20step.%20While%20the%20algebraic%20image%20processing%0Aframeworks%20are%20sometimes%20inefficient%20for%20this%20denoising%20task%20as%20they%20may%0Arequire%20processing%20of%20matrices%20of%20order%20equivalent%20to%20some%20power%20of%20the%20order%0Aof%20the%20original%20image%2C%20the%20neural%20network%20image%20processing%20frameworks%20are%0Asometimes%20not%20robust%20as%20they%20require%20a%20lot%20of%20similar%20training%20samples.%20Thus%2C%0Ahere%20we%20present%20a%20manifold-based%20noise%20filtering%20method%20that%20mainly%20exploits%20a%0Afew%20prominent%20singular%20vectors%20of%20the%20geodesics%27%20Gramian%20matrix.%20Especially%2C%0Athe%20framework%20partitions%20an%20image%2C%20say%20that%20of%20size%20%24n%20%5Ctimes%20n%24%2C%20into%20%24n%5E2%24%0Aoverlapping%20patches%20of%20known%20size%20such%20that%20one%20patch%20is%20centered%20at%20each%0Apixel.%20Then%2C%20the%20prominent%20singular%20vectors%2C%20of%20the%20Gramian%20matrix%20of%20size%20%24n%5E2%0A%5Ctimes%20n%5E2%24%20of%20the%20geodesic%20distances%20computed%20over%20the%20patch%20space%2C%20are%0Autilized%20to%20denoise%20the%20image.%20Here%2C%20the%20prominent%20singular%20vectors%20are%0Arevealed%20by%20efficient%2C%20but%20diverse%2C%20approximation%20techniques%2C%20rather%20than%0Aexplicitly%20computing%20them%20using%20frameworks%20like%20Singular%20Value%20Decomposition%0A%28SVD%29%20which%20encounters%20%24%5Cmathcal%7BO%7D%28n%5E6%29%24%20operations.%20Finally%2C%20we%20compare%20both%0Acomputational%20time%20and%20the%20noise%20filtration%20performance%20of%20the%20proposed%0Adenoising%20algorithm%20with%20and%20without%20singular%20vector%20approximation%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.13094v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Image%2520Denoising%2520by%2520Low-Rank%2520Singular%2520Vector%2520Approximations%2520of%250A%2520%2520Geodesics%2527%2520Gramian%2520Matrix%26entry.906535625%3DKelum%2520Gajamannage%2520and%2520Yonggi%2520Park%2520and%2520S.%2520M.%2520Mallikarjunaiah%2520and%2520Sunil%2520Mathur%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520sophisticated%2520cameras%252C%2520the%2520urge%2520to%2520capture%2520high-quality%250Aimages%2520has%2520grown%2520enormous.%2520However%252C%2520the%2520noise%2520contamination%2520of%2520the%2520images%250Aresults%2520in%2520substandard%2520expectations%2520among%2520the%2520people%253B%2520thus%252C%2520image%2520denoising%2520is%250Aan%2520essential%2520pre-processing%2520step.%2520While%2520the%2520algebraic%2520image%2520processing%250Aframeworks%2520are%2520sometimes%2520inefficient%2520for%2520this%2520denoising%2520task%2520as%2520they%2520may%250Arequire%2520processing%2520of%2520matrices%2520of%2520order%2520equivalent%2520to%2520some%2520power%2520of%2520the%2520order%250Aof%2520the%2520original%2520image%252C%2520the%2520neural%2520network%2520image%2520processing%2520frameworks%2520are%250Asometimes%2520not%2520robust%2520as%2520they%2520require%2520a%2520lot%2520of%2520similar%2520training%2520samples.%2520Thus%252C%250Ahere%2520we%2520present%2520a%2520manifold-based%2520noise%2520filtering%2520method%2520that%2520mainly%2520exploits%2520a%250Afew%2520prominent%2520singular%2520vectors%2520of%2520the%2520geodesics%2527%2520Gramian%2520matrix.%2520Especially%252C%250Athe%2520framework%2520partitions%2520an%2520image%252C%2520say%2520that%2520of%2520size%2520%2524n%2520%255Ctimes%2520n%2524%252C%2520into%2520%2524n%255E2%2524%250Aoverlapping%2520patches%2520of%2520known%2520size%2520such%2520that%2520one%2520patch%2520is%2520centered%2520at%2520each%250Apixel.%2520Then%252C%2520the%2520prominent%2520singular%2520vectors%252C%2520of%2520the%2520Gramian%2520matrix%2520of%2520size%2520%2524n%255E2%250A%255Ctimes%2520n%255E2%2524%2520of%2520the%2520geodesic%2520distances%2520computed%2520over%2520the%2520patch%2520space%252C%2520are%250Autilized%2520to%2520denoise%2520the%2520image.%2520Here%252C%2520the%2520prominent%2520singular%2520vectors%2520are%250Arevealed%2520by%2520efficient%252C%2520but%2520diverse%252C%2520approximation%2520techniques%252C%2520rather%2520than%250Aexplicitly%2520computing%2520them%2520using%2520frameworks%2520like%2520Singular%2520Value%2520Decomposition%250A%2528SVD%2529%2520which%2520encounters%2520%2524%255Cmathcal%257BO%257D%2528n%255E6%2529%2524%2520operations.%2520Finally%252C%2520we%2520compare%2520both%250Acomputational%2520time%2520and%2520the%2520noise%2520filtration%2520performance%2520of%2520the%2520proposed%250Adenoising%2520algorithm%2520with%2520and%2520without%2520singular%2520vector%2520approximation%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.13094v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Image%20Denoising%20by%20Low-Rank%20Singular%20Vector%20Approximations%20of%0A%20%20Geodesics%27%20Gramian%20Matrix&entry.906535625=Kelum%20Gajamannage%20and%20Yonggi%20Park%20and%20S.%20M.%20Mallikarjunaiah%20and%20Sunil%20Mathur&entry.1292438233=%20%20With%20the%20advent%20of%20sophisticated%20cameras%2C%20the%20urge%20to%20capture%20high-quality%0Aimages%20has%20grown%20enormous.%20However%2C%20the%20noise%20contamination%20of%20the%20images%0Aresults%20in%20substandard%20expectations%20among%20the%20people%3B%20thus%2C%20image%20denoising%20is%0Aan%20essential%20pre-processing%20step.%20While%20the%20algebraic%20image%20processing%0Aframeworks%20are%20sometimes%20inefficient%20for%20this%20denoising%20task%20as%20they%20may%0Arequire%20processing%20of%20matrices%20of%20order%20equivalent%20to%20some%20power%20of%20the%20order%0Aof%20the%20original%20image%2C%20the%20neural%20network%20image%20processing%20frameworks%20are%0Asometimes%20not%20robust%20as%20they%20require%20a%20lot%20of%20similar%20training%20samples.%20Thus%2C%0Ahere%20we%20present%20a%20manifold-based%20noise%20filtering%20method%20that%20mainly%20exploits%20a%0Afew%20prominent%20singular%20vectors%20of%20the%20geodesics%27%20Gramian%20matrix.%20Especially%2C%0Athe%20framework%20partitions%20an%20image%2C%20say%20that%20of%20size%20%24n%20%5Ctimes%20n%24%2C%20into%20%24n%5E2%24%0Aoverlapping%20patches%20of%20known%20size%20such%20that%20one%20patch%20is%20centered%20at%20each%0Apixel.%20Then%2C%20the%20prominent%20singular%20vectors%2C%20of%20the%20Gramian%20matrix%20of%20size%20%24n%5E2%0A%5Ctimes%20n%5E2%24%20of%20the%20geodesic%20distances%20computed%20over%20the%20patch%20space%2C%20are%0Autilized%20to%20denoise%20the%20image.%20Here%2C%20the%20prominent%20singular%20vectors%20are%0Arevealed%20by%20efficient%2C%20but%20diverse%2C%20approximation%20techniques%2C%20rather%20than%0Aexplicitly%20computing%20them%20using%20frameworks%20like%20Singular%20Value%20Decomposition%0A%28SVD%29%20which%20encounters%20%24%5Cmathcal%7BO%7D%28n%5E6%29%24%20operations.%20Finally%2C%20we%20compare%20both%0Acomputational%20time%20and%20the%20noise%20filtration%20performance%20of%20the%20proposed%0Adenoising%20algorithm%20with%20and%20without%20singular%20vector%20approximation%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.13094v4&entry.124074799=Read"},
{"title": "Exploring End-to-end Differentiable Neural Charged Particle Tracking --\n  A Loss Landscape Perspective", "author": "Tobias Kortus and Ralf Keidel and Nicolas R. Gauger", "abstract": "  Measurement and analysis of high energetic particles for scientific, medical\nor industrial applications is a complex procedure, requiring the design of\nsophisticated detector and data processing systems. The development of adaptive\nand differentiable software pipelines using a combination of conventional and\nmachine learning algorithms is therefore getting ever more important to\noptimize and operate the system efficiently while maintaining end-to-end (E2E)\ndifferentiability. We propose for the application of charged particle tracking\nan E2E differentiable decision-focused learning scheme using graph neural\nnetworks with combinatorial components solving a linear assignment problem for\neach detector layer. We demonstrate empirically that including differentiable\nvariations of discrete assignment operations allows for efficient network\noptimization, working better or on par with approaches that lack E2E\ndifferentiability. In additional studies, we dive deeper into the optimization\nprocess and provide further insights from a loss landscape perspective. We\ndemonstrate that while both methods converge into similar performing, globally\nwell-connected regions, they suffer under substantial predictive instability\nacross initialization and optimization methods, which can have unpredictable\nconsequences on the performance of downstream tasks such as image\nreconstruction. We also point out a dependency between the interpolation factor\nof the gradient estimator and the prediction stability of the model, suggesting\nthe choice of sufficiently small values. Given the strong global connectivity\nof learned solutions and the excellent training performance, we argue that E2E\ndifferentiability provides, besides the general availability of gradient\ninformation, an important tool for robust particle tracking to mitigate\nprediction instabilities by favoring solutions that perform well on downstream\ntasks.\n", "link": "http://arxiv.org/abs/2407.13420v1", "date": "2024-07-18", "relevancy": 2.1495, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5592}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5352}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20End-to-end%20Differentiable%20Neural%20Charged%20Particle%20Tracking%20--%0A%20%20A%20Loss%20Landscape%20Perspective&body=Title%3A%20Exploring%20End-to-end%20Differentiable%20Neural%20Charged%20Particle%20Tracking%20--%0A%20%20A%20Loss%20Landscape%20Perspective%0AAuthor%3A%20Tobias%20Kortus%20and%20Ralf%20Keidel%20and%20Nicolas%20R.%20Gauger%0AAbstract%3A%20%20%20Measurement%20and%20analysis%20of%20high%20energetic%20particles%20for%20scientific%2C%20medical%0Aor%20industrial%20applications%20is%20a%20complex%20procedure%2C%20requiring%20the%20design%20of%0Asophisticated%20detector%20and%20data%20processing%20systems.%20The%20development%20of%20adaptive%0Aand%20differentiable%20software%20pipelines%20using%20a%20combination%20of%20conventional%20and%0Amachine%20learning%20algorithms%20is%20therefore%20getting%20ever%20more%20important%20to%0Aoptimize%20and%20operate%20the%20system%20efficiently%20while%20maintaining%20end-to-end%20%28E2E%29%0Adifferentiability.%20We%20propose%20for%20the%20application%20of%20charged%20particle%20tracking%0Aan%20E2E%20differentiable%20decision-focused%20learning%20scheme%20using%20graph%20neural%0Anetworks%20with%20combinatorial%20components%20solving%20a%20linear%20assignment%20problem%20for%0Aeach%20detector%20layer.%20We%20demonstrate%20empirically%20that%20including%20differentiable%0Avariations%20of%20discrete%20assignment%20operations%20allows%20for%20efficient%20network%0Aoptimization%2C%20working%20better%20or%20on%20par%20with%20approaches%20that%20lack%20E2E%0Adifferentiability.%20In%20additional%20studies%2C%20we%20dive%20deeper%20into%20the%20optimization%0Aprocess%20and%20provide%20further%20insights%20from%20a%20loss%20landscape%20perspective.%20We%0Ademonstrate%20that%20while%20both%20methods%20converge%20into%20similar%20performing%2C%20globally%0Awell-connected%20regions%2C%20they%20suffer%20under%20substantial%20predictive%20instability%0Aacross%20initialization%20and%20optimization%20methods%2C%20which%20can%20have%20unpredictable%0Aconsequences%20on%20the%20performance%20of%20downstream%20tasks%20such%20as%20image%0Areconstruction.%20We%20also%20point%20out%20a%20dependency%20between%20the%20interpolation%20factor%0Aof%20the%20gradient%20estimator%20and%20the%20prediction%20stability%20of%20the%20model%2C%20suggesting%0Athe%20choice%20of%20sufficiently%20small%20values.%20Given%20the%20strong%20global%20connectivity%0Aof%20learned%20solutions%20and%20the%20excellent%20training%20performance%2C%20we%20argue%20that%20E2E%0Adifferentiability%20provides%2C%20besides%20the%20general%20availability%20of%20gradient%0Ainformation%2C%20an%20important%20tool%20for%20robust%20particle%20tracking%20to%20mitigate%0Aprediction%20instabilities%20by%20favoring%20solutions%20that%20perform%20well%20on%20downstream%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520End-to-end%2520Differentiable%2520Neural%2520Charged%2520Particle%2520Tracking%2520--%250A%2520%2520A%2520Loss%2520Landscape%2520Perspective%26entry.906535625%3DTobias%2520Kortus%2520and%2520Ralf%2520Keidel%2520and%2520Nicolas%2520R.%2520Gauger%26entry.1292438233%3D%2520%2520Measurement%2520and%2520analysis%2520of%2520high%2520energetic%2520particles%2520for%2520scientific%252C%2520medical%250Aor%2520industrial%2520applications%2520is%2520a%2520complex%2520procedure%252C%2520requiring%2520the%2520design%2520of%250Asophisticated%2520detector%2520and%2520data%2520processing%2520systems.%2520The%2520development%2520of%2520adaptive%250Aand%2520differentiable%2520software%2520pipelines%2520using%2520a%2520combination%2520of%2520conventional%2520and%250Amachine%2520learning%2520algorithms%2520is%2520therefore%2520getting%2520ever%2520more%2520important%2520to%250Aoptimize%2520and%2520operate%2520the%2520system%2520efficiently%2520while%2520maintaining%2520end-to-end%2520%2528E2E%2529%250Adifferentiability.%2520We%2520propose%2520for%2520the%2520application%2520of%2520charged%2520particle%2520tracking%250Aan%2520E2E%2520differentiable%2520decision-focused%2520learning%2520scheme%2520using%2520graph%2520neural%250Anetworks%2520with%2520combinatorial%2520components%2520solving%2520a%2520linear%2520assignment%2520problem%2520for%250Aeach%2520detector%2520layer.%2520We%2520demonstrate%2520empirically%2520that%2520including%2520differentiable%250Avariations%2520of%2520discrete%2520assignment%2520operations%2520allows%2520for%2520efficient%2520network%250Aoptimization%252C%2520working%2520better%2520or%2520on%2520par%2520with%2520approaches%2520that%2520lack%2520E2E%250Adifferentiability.%2520In%2520additional%2520studies%252C%2520we%2520dive%2520deeper%2520into%2520the%2520optimization%250Aprocess%2520and%2520provide%2520further%2520insights%2520from%2520a%2520loss%2520landscape%2520perspective.%2520We%250Ademonstrate%2520that%2520while%2520both%2520methods%2520converge%2520into%2520similar%2520performing%252C%2520globally%250Awell-connected%2520regions%252C%2520they%2520suffer%2520under%2520substantial%2520predictive%2520instability%250Aacross%2520initialization%2520and%2520optimization%2520methods%252C%2520which%2520can%2520have%2520unpredictable%250Aconsequences%2520on%2520the%2520performance%2520of%2520downstream%2520tasks%2520such%2520as%2520image%250Areconstruction.%2520We%2520also%2520point%2520out%2520a%2520dependency%2520between%2520the%2520interpolation%2520factor%250Aof%2520the%2520gradient%2520estimator%2520and%2520the%2520prediction%2520stability%2520of%2520the%2520model%252C%2520suggesting%250Athe%2520choice%2520of%2520sufficiently%2520small%2520values.%2520Given%2520the%2520strong%2520global%2520connectivity%250Aof%2520learned%2520solutions%2520and%2520the%2520excellent%2520training%2520performance%252C%2520we%2520argue%2520that%2520E2E%250Adifferentiability%2520provides%252C%2520besides%2520the%2520general%2520availability%2520of%2520gradient%250Ainformation%252C%2520an%2520important%2520tool%2520for%2520robust%2520particle%2520tracking%2520to%2520mitigate%250Aprediction%2520instabilities%2520by%2520favoring%2520solutions%2520that%2520perform%2520well%2520on%2520downstream%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20End-to-end%20Differentiable%20Neural%20Charged%20Particle%20Tracking%20--%0A%20%20A%20Loss%20Landscape%20Perspective&entry.906535625=Tobias%20Kortus%20and%20Ralf%20Keidel%20and%20Nicolas%20R.%20Gauger&entry.1292438233=%20%20Measurement%20and%20analysis%20of%20high%20energetic%20particles%20for%20scientific%2C%20medical%0Aor%20industrial%20applications%20is%20a%20complex%20procedure%2C%20requiring%20the%20design%20of%0Asophisticated%20detector%20and%20data%20processing%20systems.%20The%20development%20of%20adaptive%0Aand%20differentiable%20software%20pipelines%20using%20a%20combination%20of%20conventional%20and%0Amachine%20learning%20algorithms%20is%20therefore%20getting%20ever%20more%20important%20to%0Aoptimize%20and%20operate%20the%20system%20efficiently%20while%20maintaining%20end-to-end%20%28E2E%29%0Adifferentiability.%20We%20propose%20for%20the%20application%20of%20charged%20particle%20tracking%0Aan%20E2E%20differentiable%20decision-focused%20learning%20scheme%20using%20graph%20neural%0Anetworks%20with%20combinatorial%20components%20solving%20a%20linear%20assignment%20problem%20for%0Aeach%20detector%20layer.%20We%20demonstrate%20empirically%20that%20including%20differentiable%0Avariations%20of%20discrete%20assignment%20operations%20allows%20for%20efficient%20network%0Aoptimization%2C%20working%20better%20or%20on%20par%20with%20approaches%20that%20lack%20E2E%0Adifferentiability.%20In%20additional%20studies%2C%20we%20dive%20deeper%20into%20the%20optimization%0Aprocess%20and%20provide%20further%20insights%20from%20a%20loss%20landscape%20perspective.%20We%0Ademonstrate%20that%20while%20both%20methods%20converge%20into%20similar%20performing%2C%20globally%0Awell-connected%20regions%2C%20they%20suffer%20under%20substantial%20predictive%20instability%0Aacross%20initialization%20and%20optimization%20methods%2C%20which%20can%20have%20unpredictable%0Aconsequences%20on%20the%20performance%20of%20downstream%20tasks%20such%20as%20image%0Areconstruction.%20We%20also%20point%20out%20a%20dependency%20between%20the%20interpolation%20factor%0Aof%20the%20gradient%20estimator%20and%20the%20prediction%20stability%20of%20the%20model%2C%20suggesting%0Athe%20choice%20of%20sufficiently%20small%20values.%20Given%20the%20strong%20global%20connectivity%0Aof%20learned%20solutions%20and%20the%20excellent%20training%20performance%2C%20we%20argue%20that%20E2E%0Adifferentiability%20provides%2C%20besides%20the%20general%20availability%20of%20gradient%0Ainformation%2C%20an%20important%20tool%20for%20robust%20particle%20tracking%20to%20mitigate%0Aprediction%20instabilities%20by%20favoring%20solutions%20that%20perform%20well%20on%20downstream%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13420v1&entry.124074799=Read"},
{"title": "DISCOVER: A Data-driven Interactive System for Comprehensive\n  Observation, Visualization, and ExploRation of Human Behaviour", "author": "Dominik Schiller and Tobias Hallmen and Daksitha Withanage Don and Elisabeth Andr\u00e9 and Tobias Baur", "abstract": "  Understanding human behavior is a fundamental goal of social sciences, yet\nits analysis presents significant challenges. Conventional methodologies\nemployed for the study of behavior, characterized by labor-intensive data\ncollection processes and intricate analyses, frequently hinder comprehensive\nexploration due to their time and resource demands. In response to these\nchallenges, computational models have proven to be promising tools that help\nresearchers analyze large amounts of data by automatically identifying\nimportant behavioral indicators, such as social signals. However, the\nwidespread adoption of such state-of-the-art computational models is impeded by\ntheir inherent complexity and the substantial computational resources necessary\nto run them, thereby constraining accessibility for researchers without\ntechnical expertise and adequate equipment. To address these barriers, we\nintroduce DISCOVER -- a modular and flexible, yet user-friendly software\nframework specifically developed to streamline computational-driven data\nexploration for human behavior analysis. Our primary objective is to\ndemocratize access to advanced computational methodologies, thereby enabling\nresearchers across disciplines to engage in detailed behavioral analysis\nwithout the need for extensive technical proficiency. In this paper, we\ndemonstrate the capabilities of DISCOVER using four exemplary data exploration\nworkflows that build on each other: Interactive Semantic Content Exploration,\nVisual Inspection, Aided Annotation, and Multimodal Scene Search. By\nillustrating these workflows, we aim to emphasize the versatility and\naccessibility of DISCOVER as a comprehensive framework and propose a set of\nblueprints that can serve as a general starting point for exploratory data\nanalysis.\n", "link": "http://arxiv.org/abs/2407.13408v1", "date": "2024-07-18", "relevancy": 2.1404, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5968}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5228}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DISCOVER%3A%20A%20Data-driven%20Interactive%20System%20for%20Comprehensive%0A%20%20Observation%2C%20Visualization%2C%20and%20ExploRation%20of%20Human%20Behaviour&body=Title%3A%20DISCOVER%3A%20A%20Data-driven%20Interactive%20System%20for%20Comprehensive%0A%20%20Observation%2C%20Visualization%2C%20and%20ExploRation%20of%20Human%20Behaviour%0AAuthor%3A%20Dominik%20Schiller%20and%20Tobias%20Hallmen%20and%20Daksitha%20Withanage%20Don%20and%20Elisabeth%20Andr%C3%A9%20and%20Tobias%20Baur%0AAbstract%3A%20%20%20Understanding%20human%20behavior%20is%20a%20fundamental%20goal%20of%20social%20sciences%2C%20yet%0Aits%20analysis%20presents%20significant%20challenges.%20Conventional%20methodologies%0Aemployed%20for%20the%20study%20of%20behavior%2C%20characterized%20by%20labor-intensive%20data%0Acollection%20processes%20and%20intricate%20analyses%2C%20frequently%20hinder%20comprehensive%0Aexploration%20due%20to%20their%20time%20and%20resource%20demands.%20In%20response%20to%20these%0Achallenges%2C%20computational%20models%20have%20proven%20to%20be%20promising%20tools%20that%20help%0Aresearchers%20analyze%20large%20amounts%20of%20data%20by%20automatically%20identifying%0Aimportant%20behavioral%20indicators%2C%20such%20as%20social%20signals.%20However%2C%20the%0Awidespread%20adoption%20of%20such%20state-of-the-art%20computational%20models%20is%20impeded%20by%0Atheir%20inherent%20complexity%20and%20the%20substantial%20computational%20resources%20necessary%0Ato%20run%20them%2C%20thereby%20constraining%20accessibility%20for%20researchers%20without%0Atechnical%20expertise%20and%20adequate%20equipment.%20To%20address%20these%20barriers%2C%20we%0Aintroduce%20DISCOVER%20--%20a%20modular%20and%20flexible%2C%20yet%20user-friendly%20software%0Aframework%20specifically%20developed%20to%20streamline%20computational-driven%20data%0Aexploration%20for%20human%20behavior%20analysis.%20Our%20primary%20objective%20is%20to%0Ademocratize%20access%20to%20advanced%20computational%20methodologies%2C%20thereby%20enabling%0Aresearchers%20across%20disciplines%20to%20engage%20in%20detailed%20behavioral%20analysis%0Awithout%20the%20need%20for%20extensive%20technical%20proficiency.%20In%20this%20paper%2C%20we%0Ademonstrate%20the%20capabilities%20of%20DISCOVER%20using%20four%20exemplary%20data%20exploration%0Aworkflows%20that%20build%20on%20each%20other%3A%20Interactive%20Semantic%20Content%20Exploration%2C%0AVisual%20Inspection%2C%20Aided%20Annotation%2C%20and%20Multimodal%20Scene%20Search.%20By%0Aillustrating%20these%20workflows%2C%20we%20aim%20to%20emphasize%20the%20versatility%20and%0Aaccessibility%20of%20DISCOVER%20as%20a%20comprehensive%20framework%20and%20propose%20a%20set%20of%0Ablueprints%20that%20can%20serve%20as%20a%20general%20starting%20point%20for%20exploratory%20data%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDISCOVER%253A%2520A%2520Data-driven%2520Interactive%2520System%2520for%2520Comprehensive%250A%2520%2520Observation%252C%2520Visualization%252C%2520and%2520ExploRation%2520of%2520Human%2520Behaviour%26entry.906535625%3DDominik%2520Schiller%2520and%2520Tobias%2520Hallmen%2520and%2520Daksitha%2520Withanage%2520Don%2520and%2520Elisabeth%2520Andr%25C3%25A9%2520and%2520Tobias%2520Baur%26entry.1292438233%3D%2520%2520Understanding%2520human%2520behavior%2520is%2520a%2520fundamental%2520goal%2520of%2520social%2520sciences%252C%2520yet%250Aits%2520analysis%2520presents%2520significant%2520challenges.%2520Conventional%2520methodologies%250Aemployed%2520for%2520the%2520study%2520of%2520behavior%252C%2520characterized%2520by%2520labor-intensive%2520data%250Acollection%2520processes%2520and%2520intricate%2520analyses%252C%2520frequently%2520hinder%2520comprehensive%250Aexploration%2520due%2520to%2520their%2520time%2520and%2520resource%2520demands.%2520In%2520response%2520to%2520these%250Achallenges%252C%2520computational%2520models%2520have%2520proven%2520to%2520be%2520promising%2520tools%2520that%2520help%250Aresearchers%2520analyze%2520large%2520amounts%2520of%2520data%2520by%2520automatically%2520identifying%250Aimportant%2520behavioral%2520indicators%252C%2520such%2520as%2520social%2520signals.%2520However%252C%2520the%250Awidespread%2520adoption%2520of%2520such%2520state-of-the-art%2520computational%2520models%2520is%2520impeded%2520by%250Atheir%2520inherent%2520complexity%2520and%2520the%2520substantial%2520computational%2520resources%2520necessary%250Ato%2520run%2520them%252C%2520thereby%2520constraining%2520accessibility%2520for%2520researchers%2520without%250Atechnical%2520expertise%2520and%2520adequate%2520equipment.%2520To%2520address%2520these%2520barriers%252C%2520we%250Aintroduce%2520DISCOVER%2520--%2520a%2520modular%2520and%2520flexible%252C%2520yet%2520user-friendly%2520software%250Aframework%2520specifically%2520developed%2520to%2520streamline%2520computational-driven%2520data%250Aexploration%2520for%2520human%2520behavior%2520analysis.%2520Our%2520primary%2520objective%2520is%2520to%250Ademocratize%2520access%2520to%2520advanced%2520computational%2520methodologies%252C%2520thereby%2520enabling%250Aresearchers%2520across%2520disciplines%2520to%2520engage%2520in%2520detailed%2520behavioral%2520analysis%250Awithout%2520the%2520need%2520for%2520extensive%2520technical%2520proficiency.%2520In%2520this%2520paper%252C%2520we%250Ademonstrate%2520the%2520capabilities%2520of%2520DISCOVER%2520using%2520four%2520exemplary%2520data%2520exploration%250Aworkflows%2520that%2520build%2520on%2520each%2520other%253A%2520Interactive%2520Semantic%2520Content%2520Exploration%252C%250AVisual%2520Inspection%252C%2520Aided%2520Annotation%252C%2520and%2520Multimodal%2520Scene%2520Search.%2520By%250Aillustrating%2520these%2520workflows%252C%2520we%2520aim%2520to%2520emphasize%2520the%2520versatility%2520and%250Aaccessibility%2520of%2520DISCOVER%2520as%2520a%2520comprehensive%2520framework%2520and%2520propose%2520a%2520set%2520of%250Ablueprints%2520that%2520can%2520serve%2520as%2520a%2520general%2520starting%2520point%2520for%2520exploratory%2520data%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DISCOVER%3A%20A%20Data-driven%20Interactive%20System%20for%20Comprehensive%0A%20%20Observation%2C%20Visualization%2C%20and%20ExploRation%20of%20Human%20Behaviour&entry.906535625=Dominik%20Schiller%20and%20Tobias%20Hallmen%20and%20Daksitha%20Withanage%20Don%20and%20Elisabeth%20Andr%C3%A9%20and%20Tobias%20Baur&entry.1292438233=%20%20Understanding%20human%20behavior%20is%20a%20fundamental%20goal%20of%20social%20sciences%2C%20yet%0Aits%20analysis%20presents%20significant%20challenges.%20Conventional%20methodologies%0Aemployed%20for%20the%20study%20of%20behavior%2C%20characterized%20by%20labor-intensive%20data%0Acollection%20processes%20and%20intricate%20analyses%2C%20frequently%20hinder%20comprehensive%0Aexploration%20due%20to%20their%20time%20and%20resource%20demands.%20In%20response%20to%20these%0Achallenges%2C%20computational%20models%20have%20proven%20to%20be%20promising%20tools%20that%20help%0Aresearchers%20analyze%20large%20amounts%20of%20data%20by%20automatically%20identifying%0Aimportant%20behavioral%20indicators%2C%20such%20as%20social%20signals.%20However%2C%20the%0Awidespread%20adoption%20of%20such%20state-of-the-art%20computational%20models%20is%20impeded%20by%0Atheir%20inherent%20complexity%20and%20the%20substantial%20computational%20resources%20necessary%0Ato%20run%20them%2C%20thereby%20constraining%20accessibility%20for%20researchers%20without%0Atechnical%20expertise%20and%20adequate%20equipment.%20To%20address%20these%20barriers%2C%20we%0Aintroduce%20DISCOVER%20--%20a%20modular%20and%20flexible%2C%20yet%20user-friendly%20software%0Aframework%20specifically%20developed%20to%20streamline%20computational-driven%20data%0Aexploration%20for%20human%20behavior%20analysis.%20Our%20primary%20objective%20is%20to%0Ademocratize%20access%20to%20advanced%20computational%20methodologies%2C%20thereby%20enabling%0Aresearchers%20across%20disciplines%20to%20engage%20in%20detailed%20behavioral%20analysis%0Awithout%20the%20need%20for%20extensive%20technical%20proficiency.%20In%20this%20paper%2C%20we%0Ademonstrate%20the%20capabilities%20of%20DISCOVER%20using%20four%20exemplary%20data%20exploration%0Aworkflows%20that%20build%20on%20each%20other%3A%20Interactive%20Semantic%20Content%20Exploration%2C%0AVisual%20Inspection%2C%20Aided%20Annotation%2C%20and%20Multimodal%20Scene%20Search.%20By%0Aillustrating%20these%20workflows%2C%20we%20aim%20to%20emphasize%20the%20versatility%20and%0Aaccessibility%20of%20DISCOVER%20as%20a%20comprehensive%20framework%20and%20propose%20a%20set%20of%0Ablueprints%20that%20can%20serve%20as%20a%20general%20starting%20point%20for%20exploratory%20data%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13408v1&entry.124074799=Read"},
{"title": "Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark", "author": "Marina Ceccon and Davide Dalle Pezze and Alessandro Fabris and Gian Antonio Susto", "abstract": "  Despite the critical importance of the medical domain in Deep Learning, most\nof the research in this area solely focuses on training models in static\nenvironments. It is only in recent years that research has begun to address\ndynamic environments and tackle the Catastrophic Forgetting problem through\nContinual Learning (CL) techniques. Previous studies have primarily focused on\nscenarios such as Domain Incremental Learning and Class Incremental Learning,\nwhich do not fully capture the complexity of real-world applications.\nTherefore, in this work, we propose a novel benchmark combining the challenges\nof new class arrivals and domain shifts in a single framework, by considering\nthe New Instances and New Classes (NIC) scenario. This benchmark aims to model\na realistic CL setting for the multi-label classification problem in medical\nimaging. Additionally, it encompasses a greater number of tasks compared to\npreviously tested scenarios. Specifically, our benchmark consists of two\ndatasets (NIH and CXP), nineteen classes, and seven tasks, a stream longer than\nthe previously tested ones. To solve common challenges (e.g., the task\ninference problem) found in the CIL and NIC scenarios, we propose a novel\napproach called Replay Consolidation with Label Propagation (RCLP). Our method\nsurpasses existing approaches, exhibiting superior performance with minimal\nforgetting.\n", "link": "http://arxiv.org/abs/2404.06859v3", "date": "2024-07-18", "relevancy": 2.1191, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5642}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Label%20Continual%20Learning%20for%20the%20Medical%20Domain%3A%20A%20Novel%20Benchmark&body=Title%3A%20Multi-Label%20Continual%20Learning%20for%20the%20Medical%20Domain%3A%20A%20Novel%20Benchmark%0AAuthor%3A%20Marina%20Ceccon%20and%20Davide%20Dalle%20Pezze%20and%20Alessandro%20Fabris%20and%20Gian%20Antonio%20Susto%0AAbstract%3A%20%20%20Despite%20the%20critical%20importance%20of%20the%20medical%20domain%20in%20Deep%20Learning%2C%20most%0Aof%20the%20research%20in%20this%20area%20solely%20focuses%20on%20training%20models%20in%20static%0Aenvironments.%20It%20is%20only%20in%20recent%20years%20that%20research%20has%20begun%20to%20address%0Adynamic%20environments%20and%20tackle%20the%20Catastrophic%20Forgetting%20problem%20through%0AContinual%20Learning%20%28CL%29%20techniques.%20Previous%20studies%20have%20primarily%20focused%20on%0Ascenarios%20such%20as%20Domain%20Incremental%20Learning%20and%20Class%20Incremental%20Learning%2C%0Awhich%20do%20not%20fully%20capture%20the%20complexity%20of%20real-world%20applications.%0ATherefore%2C%20in%20this%20work%2C%20we%20propose%20a%20novel%20benchmark%20combining%20the%20challenges%0Aof%20new%20class%20arrivals%20and%20domain%20shifts%20in%20a%20single%20framework%2C%20by%20considering%0Athe%20New%20Instances%20and%20New%20Classes%20%28NIC%29%20scenario.%20This%20benchmark%20aims%20to%20model%0Aa%20realistic%20CL%20setting%20for%20the%20multi-label%20classification%20problem%20in%20medical%0Aimaging.%20Additionally%2C%20it%20encompasses%20a%20greater%20number%20of%20tasks%20compared%20to%0Apreviously%20tested%20scenarios.%20Specifically%2C%20our%20benchmark%20consists%20of%20two%0Adatasets%20%28NIH%20and%20CXP%29%2C%20nineteen%20classes%2C%20and%20seven%20tasks%2C%20a%20stream%20longer%20than%0Athe%20previously%20tested%20ones.%20To%20solve%20common%20challenges%20%28e.g.%2C%20the%20task%0Ainference%20problem%29%20found%20in%20the%20CIL%20and%20NIC%20scenarios%2C%20we%20propose%20a%20novel%0Aapproach%20called%20Replay%20Consolidation%20with%20Label%20Propagation%20%28RCLP%29.%20Our%20method%0Asurpasses%20existing%20approaches%2C%20exhibiting%20superior%20performance%20with%20minimal%0Aforgetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06859v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Label%2520Continual%2520Learning%2520for%2520the%2520Medical%2520Domain%253A%2520A%2520Novel%2520Benchmark%26entry.906535625%3DMarina%2520Ceccon%2520and%2520Davide%2520Dalle%2520Pezze%2520and%2520Alessandro%2520Fabris%2520and%2520Gian%2520Antonio%2520Susto%26entry.1292438233%3D%2520%2520Despite%2520the%2520critical%2520importance%2520of%2520the%2520medical%2520domain%2520in%2520Deep%2520Learning%252C%2520most%250Aof%2520the%2520research%2520in%2520this%2520area%2520solely%2520focuses%2520on%2520training%2520models%2520in%2520static%250Aenvironments.%2520It%2520is%2520only%2520in%2520recent%2520years%2520that%2520research%2520has%2520begun%2520to%2520address%250Adynamic%2520environments%2520and%2520tackle%2520the%2520Catastrophic%2520Forgetting%2520problem%2520through%250AContinual%2520Learning%2520%2528CL%2529%2520techniques.%2520Previous%2520studies%2520have%2520primarily%2520focused%2520on%250Ascenarios%2520such%2520as%2520Domain%2520Incremental%2520Learning%2520and%2520Class%2520Incremental%2520Learning%252C%250Awhich%2520do%2520not%2520fully%2520capture%2520the%2520complexity%2520of%2520real-world%2520applications.%250ATherefore%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520benchmark%2520combining%2520the%2520challenges%250Aof%2520new%2520class%2520arrivals%2520and%2520domain%2520shifts%2520in%2520a%2520single%2520framework%252C%2520by%2520considering%250Athe%2520New%2520Instances%2520and%2520New%2520Classes%2520%2528NIC%2529%2520scenario.%2520This%2520benchmark%2520aims%2520to%2520model%250Aa%2520realistic%2520CL%2520setting%2520for%2520the%2520multi-label%2520classification%2520problem%2520in%2520medical%250Aimaging.%2520Additionally%252C%2520it%2520encompasses%2520a%2520greater%2520number%2520of%2520tasks%2520compared%2520to%250Apreviously%2520tested%2520scenarios.%2520Specifically%252C%2520our%2520benchmark%2520consists%2520of%2520two%250Adatasets%2520%2528NIH%2520and%2520CXP%2529%252C%2520nineteen%2520classes%252C%2520and%2520seven%2520tasks%252C%2520a%2520stream%2520longer%2520than%250Athe%2520previously%2520tested%2520ones.%2520To%2520solve%2520common%2520challenges%2520%2528e.g.%252C%2520the%2520task%250Ainference%2520problem%2529%2520found%2520in%2520the%2520CIL%2520and%2520NIC%2520scenarios%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520called%2520Replay%2520Consolidation%2520with%2520Label%2520Propagation%2520%2528RCLP%2529.%2520Our%2520method%250Asurpasses%2520existing%2520approaches%252C%2520exhibiting%2520superior%2520performance%2520with%2520minimal%250Aforgetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06859v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Label%20Continual%20Learning%20for%20the%20Medical%20Domain%3A%20A%20Novel%20Benchmark&entry.906535625=Marina%20Ceccon%20and%20Davide%20Dalle%20Pezze%20and%20Alessandro%20Fabris%20and%20Gian%20Antonio%20Susto&entry.1292438233=%20%20Despite%20the%20critical%20importance%20of%20the%20medical%20domain%20in%20Deep%20Learning%2C%20most%0Aof%20the%20research%20in%20this%20area%20solely%20focuses%20on%20training%20models%20in%20static%0Aenvironments.%20It%20is%20only%20in%20recent%20years%20that%20research%20has%20begun%20to%20address%0Adynamic%20environments%20and%20tackle%20the%20Catastrophic%20Forgetting%20problem%20through%0AContinual%20Learning%20%28CL%29%20techniques.%20Previous%20studies%20have%20primarily%20focused%20on%0Ascenarios%20such%20as%20Domain%20Incremental%20Learning%20and%20Class%20Incremental%20Learning%2C%0Awhich%20do%20not%20fully%20capture%20the%20complexity%20of%20real-world%20applications.%0ATherefore%2C%20in%20this%20work%2C%20we%20propose%20a%20novel%20benchmark%20combining%20the%20challenges%0Aof%20new%20class%20arrivals%20and%20domain%20shifts%20in%20a%20single%20framework%2C%20by%20considering%0Athe%20New%20Instances%20and%20New%20Classes%20%28NIC%29%20scenario.%20This%20benchmark%20aims%20to%20model%0Aa%20realistic%20CL%20setting%20for%20the%20multi-label%20classification%20problem%20in%20medical%0Aimaging.%20Additionally%2C%20it%20encompasses%20a%20greater%20number%20of%20tasks%20compared%20to%0Apreviously%20tested%20scenarios.%20Specifically%2C%20our%20benchmark%20consists%20of%20two%0Adatasets%20%28NIH%20and%20CXP%29%2C%20nineteen%20classes%2C%20and%20seven%20tasks%2C%20a%20stream%20longer%20than%0Athe%20previously%20tested%20ones.%20To%20solve%20common%20challenges%20%28e.g.%2C%20the%20task%0Ainference%20problem%29%20found%20in%20the%20CIL%20and%20NIC%20scenarios%2C%20we%20propose%20a%20novel%0Aapproach%20called%20Replay%20Consolidation%20with%20Label%20Propagation%20%28RCLP%29.%20Our%20method%0Asurpasses%20existing%20approaches%2C%20exhibiting%20superior%20performance%20with%20minimal%0Aforgetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06859v3&entry.124074799=Read"},
{"title": "Enhancing Source-Free Domain Adaptive Object Detection with\n  Low-confidence Pseudo Label Distillation", "author": "Ilhoon Yoon and Hyeongjun Kwon and Jin Kim and Junyoung Park and Hyunsung Jang and Kwanghoon Sohn", "abstract": "  Source-Free domain adaptive Object Detection (SFOD) is a promising strategy\nfor deploying trained detectors to new, unlabeled domains without accessing\nsource data, addressing significant concerns around data privacy and\nefficiency. Most SFOD methods leverage a Mean-Teacher (MT) self-training\nparadigm relying heavily on High-confidence Pseudo Labels (HPL). However, these\nHPL often overlook small instances that undergo significant appearance changes\nwith domain shifts. Additionally, HPL ignore instances with low confidence due\nto the scarcity of training samples, resulting in biased adaptation toward\nfamiliar instances from the source domain. To address this limitation, we\nintroduce the Low-confidence Pseudo Label Distillation (LPLD) loss within the\nMean-Teacher based SFOD framework. This novel approach is designed to leverage\nthe proposals from Region Proposal Network (RPN), which potentially encompasses\nhard-to-detect objects in unfamiliar domains. Initially, we extract HPL using a\nstandard pseudo-labeling technique and mine a set of Low-confidence Pseudo\nLabels (LPL) from proposals generated by RPN, leaving those that do not overlap\nsignificantly with HPL. These LPL are further refined by leveraging\nclass-relation information and reducing the effect of inherent noise for the\nLPLD loss calculation. Furthermore, we use feature distance to adaptively\nweight the LPLD loss to focus on LPL containing a larger foreground area. Our\nmethod outperforms previous SFOD methods on four cross-domain object detection\nbenchmarks. Extensive experiments demonstrate that our LPLD loss leads to\neffective adaptation by reducing false negatives and facilitating the use of\ndomain-invariant knowledge from the source model. Code is available at\nhttps://github.com/junia3/LPLD.\n", "link": "http://arxiv.org/abs/2407.13524v1", "date": "2024-07-18", "relevancy": 2.1144, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5847}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5188}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Source-Free%20Domain%20Adaptive%20Object%20Detection%20with%0A%20%20Low-confidence%20Pseudo%20Label%20Distillation&body=Title%3A%20Enhancing%20Source-Free%20Domain%20Adaptive%20Object%20Detection%20with%0A%20%20Low-confidence%20Pseudo%20Label%20Distillation%0AAuthor%3A%20Ilhoon%20Yoon%20and%20Hyeongjun%20Kwon%20and%20Jin%20Kim%20and%20Junyoung%20Park%20and%20Hyunsung%20Jang%20and%20Kwanghoon%20Sohn%0AAbstract%3A%20%20%20Source-Free%20domain%20adaptive%20Object%20Detection%20%28SFOD%29%20is%20a%20promising%20strategy%0Afor%20deploying%20trained%20detectors%20to%20new%2C%20unlabeled%20domains%20without%20accessing%0Asource%20data%2C%20addressing%20significant%20concerns%20around%20data%20privacy%20and%0Aefficiency.%20Most%20SFOD%20methods%20leverage%20a%20Mean-Teacher%20%28MT%29%20self-training%0Aparadigm%20relying%20heavily%20on%20High-confidence%20Pseudo%20Labels%20%28HPL%29.%20However%2C%20these%0AHPL%20often%20overlook%20small%20instances%20that%20undergo%20significant%20appearance%20changes%0Awith%20domain%20shifts.%20Additionally%2C%20HPL%20ignore%20instances%20with%20low%20confidence%20due%0Ato%20the%20scarcity%20of%20training%20samples%2C%20resulting%20in%20biased%20adaptation%20toward%0Afamiliar%20instances%20from%20the%20source%20domain.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20the%20Low-confidence%20Pseudo%20Label%20Distillation%20%28LPLD%29%20loss%20within%20the%0AMean-Teacher%20based%20SFOD%20framework.%20This%20novel%20approach%20is%20designed%20to%20leverage%0Athe%20proposals%20from%20Region%20Proposal%20Network%20%28RPN%29%2C%20which%20potentially%20encompasses%0Ahard-to-detect%20objects%20in%20unfamiliar%20domains.%20Initially%2C%20we%20extract%20HPL%20using%20a%0Astandard%20pseudo-labeling%20technique%20and%20mine%20a%20set%20of%20Low-confidence%20Pseudo%0ALabels%20%28LPL%29%20from%20proposals%20generated%20by%20RPN%2C%20leaving%20those%20that%20do%20not%20overlap%0Asignificantly%20with%20HPL.%20These%20LPL%20are%20further%20refined%20by%20leveraging%0Aclass-relation%20information%20and%20reducing%20the%20effect%20of%20inherent%20noise%20for%20the%0ALPLD%20loss%20calculation.%20Furthermore%2C%20we%20use%20feature%20distance%20to%20adaptively%0Aweight%20the%20LPLD%20loss%20to%20focus%20on%20LPL%20containing%20a%20larger%20foreground%20area.%20Our%0Amethod%20outperforms%20previous%20SFOD%20methods%20on%20four%20cross-domain%20object%20detection%0Abenchmarks.%20Extensive%20experiments%20demonstrate%20that%20our%20LPLD%20loss%20leads%20to%0Aeffective%20adaptation%20by%20reducing%20false%20negatives%20and%20facilitating%20the%20use%20of%0Adomain-invariant%20knowledge%20from%20the%20source%20model.%20Code%20is%20available%20at%0Ahttps%3A//github.com/junia3/LPLD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Source-Free%2520Domain%2520Adaptive%2520Object%2520Detection%2520with%250A%2520%2520Low-confidence%2520Pseudo%2520Label%2520Distillation%26entry.906535625%3DIlhoon%2520Yoon%2520and%2520Hyeongjun%2520Kwon%2520and%2520Jin%2520Kim%2520and%2520Junyoung%2520Park%2520and%2520Hyunsung%2520Jang%2520and%2520Kwanghoon%2520Sohn%26entry.1292438233%3D%2520%2520Source-Free%2520domain%2520adaptive%2520Object%2520Detection%2520%2528SFOD%2529%2520is%2520a%2520promising%2520strategy%250Afor%2520deploying%2520trained%2520detectors%2520to%2520new%252C%2520unlabeled%2520domains%2520without%2520accessing%250Asource%2520data%252C%2520addressing%2520significant%2520concerns%2520around%2520data%2520privacy%2520and%250Aefficiency.%2520Most%2520SFOD%2520methods%2520leverage%2520a%2520Mean-Teacher%2520%2528MT%2529%2520self-training%250Aparadigm%2520relying%2520heavily%2520on%2520High-confidence%2520Pseudo%2520Labels%2520%2528HPL%2529.%2520However%252C%2520these%250AHPL%2520often%2520overlook%2520small%2520instances%2520that%2520undergo%2520significant%2520appearance%2520changes%250Awith%2520domain%2520shifts.%2520Additionally%252C%2520HPL%2520ignore%2520instances%2520with%2520low%2520confidence%2520due%250Ato%2520the%2520scarcity%2520of%2520training%2520samples%252C%2520resulting%2520in%2520biased%2520adaptation%2520toward%250Afamiliar%2520instances%2520from%2520the%2520source%2520domain.%2520To%2520address%2520this%2520limitation%252C%2520we%250Aintroduce%2520the%2520Low-confidence%2520Pseudo%2520Label%2520Distillation%2520%2528LPLD%2529%2520loss%2520within%2520the%250AMean-Teacher%2520based%2520SFOD%2520framework.%2520This%2520novel%2520approach%2520is%2520designed%2520to%2520leverage%250Athe%2520proposals%2520from%2520Region%2520Proposal%2520Network%2520%2528RPN%2529%252C%2520which%2520potentially%2520encompasses%250Ahard-to-detect%2520objects%2520in%2520unfamiliar%2520domains.%2520Initially%252C%2520we%2520extract%2520HPL%2520using%2520a%250Astandard%2520pseudo-labeling%2520technique%2520and%2520mine%2520a%2520set%2520of%2520Low-confidence%2520Pseudo%250ALabels%2520%2528LPL%2529%2520from%2520proposals%2520generated%2520by%2520RPN%252C%2520leaving%2520those%2520that%2520do%2520not%2520overlap%250Asignificantly%2520with%2520HPL.%2520These%2520LPL%2520are%2520further%2520refined%2520by%2520leveraging%250Aclass-relation%2520information%2520and%2520reducing%2520the%2520effect%2520of%2520inherent%2520noise%2520for%2520the%250ALPLD%2520loss%2520calculation.%2520Furthermore%252C%2520we%2520use%2520feature%2520distance%2520to%2520adaptively%250Aweight%2520the%2520LPLD%2520loss%2520to%2520focus%2520on%2520LPL%2520containing%2520a%2520larger%2520foreground%2520area.%2520Our%250Amethod%2520outperforms%2520previous%2520SFOD%2520methods%2520on%2520four%2520cross-domain%2520object%2520detection%250Abenchmarks.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520LPLD%2520loss%2520leads%2520to%250Aeffective%2520adaptation%2520by%2520reducing%2520false%2520negatives%2520and%2520facilitating%2520the%2520use%2520of%250Adomain-invariant%2520knowledge%2520from%2520the%2520source%2520model.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/junia3/LPLD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Source-Free%20Domain%20Adaptive%20Object%20Detection%20with%0A%20%20Low-confidence%20Pseudo%20Label%20Distillation&entry.906535625=Ilhoon%20Yoon%20and%20Hyeongjun%20Kwon%20and%20Jin%20Kim%20and%20Junyoung%20Park%20and%20Hyunsung%20Jang%20and%20Kwanghoon%20Sohn&entry.1292438233=%20%20Source-Free%20domain%20adaptive%20Object%20Detection%20%28SFOD%29%20is%20a%20promising%20strategy%0Afor%20deploying%20trained%20detectors%20to%20new%2C%20unlabeled%20domains%20without%20accessing%0Asource%20data%2C%20addressing%20significant%20concerns%20around%20data%20privacy%20and%0Aefficiency.%20Most%20SFOD%20methods%20leverage%20a%20Mean-Teacher%20%28MT%29%20self-training%0Aparadigm%20relying%20heavily%20on%20High-confidence%20Pseudo%20Labels%20%28HPL%29.%20However%2C%20these%0AHPL%20often%20overlook%20small%20instances%20that%20undergo%20significant%20appearance%20changes%0Awith%20domain%20shifts.%20Additionally%2C%20HPL%20ignore%20instances%20with%20low%20confidence%20due%0Ato%20the%20scarcity%20of%20training%20samples%2C%20resulting%20in%20biased%20adaptation%20toward%0Afamiliar%20instances%20from%20the%20source%20domain.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20the%20Low-confidence%20Pseudo%20Label%20Distillation%20%28LPLD%29%20loss%20within%20the%0AMean-Teacher%20based%20SFOD%20framework.%20This%20novel%20approach%20is%20designed%20to%20leverage%0Athe%20proposals%20from%20Region%20Proposal%20Network%20%28RPN%29%2C%20which%20potentially%20encompasses%0Ahard-to-detect%20objects%20in%20unfamiliar%20domains.%20Initially%2C%20we%20extract%20HPL%20using%20a%0Astandard%20pseudo-labeling%20technique%20and%20mine%20a%20set%20of%20Low-confidence%20Pseudo%0ALabels%20%28LPL%29%20from%20proposals%20generated%20by%20RPN%2C%20leaving%20those%20that%20do%20not%20overlap%0Asignificantly%20with%20HPL.%20These%20LPL%20are%20further%20refined%20by%20leveraging%0Aclass-relation%20information%20and%20reducing%20the%20effect%20of%20inherent%20noise%20for%20the%0ALPLD%20loss%20calculation.%20Furthermore%2C%20we%20use%20feature%20distance%20to%20adaptively%0Aweight%20the%20LPLD%20loss%20to%20focus%20on%20LPL%20containing%20a%20larger%20foreground%20area.%20Our%0Amethod%20outperforms%20previous%20SFOD%20methods%20on%20four%20cross-domain%20object%20detection%0Abenchmarks.%20Extensive%20experiments%20demonstrate%20that%20our%20LPLD%20loss%20leads%20to%0Aeffective%20adaptation%20by%20reducing%20false%20negatives%20and%20facilitating%20the%20use%20of%0Adomain-invariant%20knowledge%20from%20the%20source%20model.%20Code%20is%20available%20at%0Ahttps%3A//github.com/junia3/LPLD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13524v1&entry.124074799=Read"},
{"title": "Risk-Aware Vehicle Trajectory Prediction Under Safety-Critical Scenarios", "author": "Qingfan Wang and Dongyang Xu and Gaoyuan Kuang and Chen Lv and Shengbo Eben Li and Bingbing Nie", "abstract": "  Trajectory prediction is significant for intelligent vehicles to achieve\nhigh-level autonomous driving, and a lot of relevant research achievements have\nbeen made recently. Despite the rapid development, most existing studies solely\nfocused on normal safe scenarios while largely neglecting safety-critical\nscenarios, particularly those involving imminent collisions. This oversight may\nresult in autonomous vehicles lacking the essential predictive ability in such\nsituations, posing a significant threat to safety. To tackle these, this paper\nproposes a risk-aware trajectory prediction framework tailored to\nsafety-critical scenarios. Leveraging distinctive hazardous features, we\ndevelop three core risk-aware components. First, we introduce a\nrisk-incorporated scene encoder, which augments conventional encoders with\nquantitative risk information to achieve risk-aware encoding of hazardous scene\ncontexts. Next, we incorporate endpoint-risk-combined intention queries as\nprediction priors in the decoder to ensure that the predicted multimodal\ntrajectories cover both various spatial intentions and risk levels. Lastly, an\nauxiliary risk prediction task is implemented for the ultimate risk-aware\nprediction. Furthermore, to support model training and performance evaluation,\nwe introduce a safety-critical trajectory prediction dataset and tailored\nevaluation metrics. We conduct comprehensive evaluations and compare our model\nwith several SOTA models. Results demonstrate the superior performance of our\nmodel, with a significant improvement in most metrics. This prediction\nadvancement enables autonomous vehicles to execute correct collision avoidance\nmaneuvers under safety-critical scenarios, eventually enhancing road traffic\nsafety.\n", "link": "http://arxiv.org/abs/2407.13480v1", "date": "2024-07-18", "relevancy": 2.111, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5813}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5182}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Risk-Aware%20Vehicle%20Trajectory%20Prediction%20Under%20Safety-Critical%20Scenarios&body=Title%3A%20Risk-Aware%20Vehicle%20Trajectory%20Prediction%20Under%20Safety-Critical%20Scenarios%0AAuthor%3A%20Qingfan%20Wang%20and%20Dongyang%20Xu%20and%20Gaoyuan%20Kuang%20and%20Chen%20Lv%20and%20Shengbo%20Eben%20Li%20and%20Bingbing%20Nie%0AAbstract%3A%20%20%20Trajectory%20prediction%20is%20significant%20for%20intelligent%20vehicles%20to%20achieve%0Ahigh-level%20autonomous%20driving%2C%20and%20a%20lot%20of%20relevant%20research%20achievements%20have%0Abeen%20made%20recently.%20Despite%20the%20rapid%20development%2C%20most%20existing%20studies%20solely%0Afocused%20on%20normal%20safe%20scenarios%20while%20largely%20neglecting%20safety-critical%0Ascenarios%2C%20particularly%20those%20involving%20imminent%20collisions.%20This%20oversight%20may%0Aresult%20in%20autonomous%20vehicles%20lacking%20the%20essential%20predictive%20ability%20in%20such%0Asituations%2C%20posing%20a%20significant%20threat%20to%20safety.%20To%20tackle%20these%2C%20this%20paper%0Aproposes%20a%20risk-aware%20trajectory%20prediction%20framework%20tailored%20to%0Asafety-critical%20scenarios.%20Leveraging%20distinctive%20hazardous%20features%2C%20we%0Adevelop%20three%20core%20risk-aware%20components.%20First%2C%20we%20introduce%20a%0Arisk-incorporated%20scene%20encoder%2C%20which%20augments%20conventional%20encoders%20with%0Aquantitative%20risk%20information%20to%20achieve%20risk-aware%20encoding%20of%20hazardous%20scene%0Acontexts.%20Next%2C%20we%20incorporate%20endpoint-risk-combined%20intention%20queries%20as%0Aprediction%20priors%20in%20the%20decoder%20to%20ensure%20that%20the%20predicted%20multimodal%0Atrajectories%20cover%20both%20various%20spatial%20intentions%20and%20risk%20levels.%20Lastly%2C%20an%0Aauxiliary%20risk%20prediction%20task%20is%20implemented%20for%20the%20ultimate%20risk-aware%0Aprediction.%20Furthermore%2C%20to%20support%20model%20training%20and%20performance%20evaluation%2C%0Awe%20introduce%20a%20safety-critical%20trajectory%20prediction%20dataset%20and%20tailored%0Aevaluation%20metrics.%20We%20conduct%20comprehensive%20evaluations%20and%20compare%20our%20model%0Awith%20several%20SOTA%20models.%20Results%20demonstrate%20the%20superior%20performance%20of%20our%0Amodel%2C%20with%20a%20significant%20improvement%20in%20most%20metrics.%20This%20prediction%0Aadvancement%20enables%20autonomous%20vehicles%20to%20execute%20correct%20collision%20avoidance%0Amaneuvers%20under%20safety-critical%20scenarios%2C%20eventually%20enhancing%20road%20traffic%0Asafety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRisk-Aware%2520Vehicle%2520Trajectory%2520Prediction%2520Under%2520Safety-Critical%2520Scenarios%26entry.906535625%3DQingfan%2520Wang%2520and%2520Dongyang%2520Xu%2520and%2520Gaoyuan%2520Kuang%2520and%2520Chen%2520Lv%2520and%2520Shengbo%2520Eben%2520Li%2520and%2520Bingbing%2520Nie%26entry.1292438233%3D%2520%2520Trajectory%2520prediction%2520is%2520significant%2520for%2520intelligent%2520vehicles%2520to%2520achieve%250Ahigh-level%2520autonomous%2520driving%252C%2520and%2520a%2520lot%2520of%2520relevant%2520research%2520achievements%2520have%250Abeen%2520made%2520recently.%2520Despite%2520the%2520rapid%2520development%252C%2520most%2520existing%2520studies%2520solely%250Afocused%2520on%2520normal%2520safe%2520scenarios%2520while%2520largely%2520neglecting%2520safety-critical%250Ascenarios%252C%2520particularly%2520those%2520involving%2520imminent%2520collisions.%2520This%2520oversight%2520may%250Aresult%2520in%2520autonomous%2520vehicles%2520lacking%2520the%2520essential%2520predictive%2520ability%2520in%2520such%250Asituations%252C%2520posing%2520a%2520significant%2520threat%2520to%2520safety.%2520To%2520tackle%2520these%252C%2520this%2520paper%250Aproposes%2520a%2520risk-aware%2520trajectory%2520prediction%2520framework%2520tailored%2520to%250Asafety-critical%2520scenarios.%2520Leveraging%2520distinctive%2520hazardous%2520features%252C%2520we%250Adevelop%2520three%2520core%2520risk-aware%2520components.%2520First%252C%2520we%2520introduce%2520a%250Arisk-incorporated%2520scene%2520encoder%252C%2520which%2520augments%2520conventional%2520encoders%2520with%250Aquantitative%2520risk%2520information%2520to%2520achieve%2520risk-aware%2520encoding%2520of%2520hazardous%2520scene%250Acontexts.%2520Next%252C%2520we%2520incorporate%2520endpoint-risk-combined%2520intention%2520queries%2520as%250Aprediction%2520priors%2520in%2520the%2520decoder%2520to%2520ensure%2520that%2520the%2520predicted%2520multimodal%250Atrajectories%2520cover%2520both%2520various%2520spatial%2520intentions%2520and%2520risk%2520levels.%2520Lastly%252C%2520an%250Aauxiliary%2520risk%2520prediction%2520task%2520is%2520implemented%2520for%2520the%2520ultimate%2520risk-aware%250Aprediction.%2520Furthermore%252C%2520to%2520support%2520model%2520training%2520and%2520performance%2520evaluation%252C%250Awe%2520introduce%2520a%2520safety-critical%2520trajectory%2520prediction%2520dataset%2520and%2520tailored%250Aevaluation%2520metrics.%2520We%2520conduct%2520comprehensive%2520evaluations%2520and%2520compare%2520our%2520model%250Awith%2520several%2520SOTA%2520models.%2520Results%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%250Amodel%252C%2520with%2520a%2520significant%2520improvement%2520in%2520most%2520metrics.%2520This%2520prediction%250Aadvancement%2520enables%2520autonomous%2520vehicles%2520to%2520execute%2520correct%2520collision%2520avoidance%250Amaneuvers%2520under%2520safety-critical%2520scenarios%252C%2520eventually%2520enhancing%2520road%2520traffic%250Asafety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk-Aware%20Vehicle%20Trajectory%20Prediction%20Under%20Safety-Critical%20Scenarios&entry.906535625=Qingfan%20Wang%20and%20Dongyang%20Xu%20and%20Gaoyuan%20Kuang%20and%20Chen%20Lv%20and%20Shengbo%20Eben%20Li%20and%20Bingbing%20Nie&entry.1292438233=%20%20Trajectory%20prediction%20is%20significant%20for%20intelligent%20vehicles%20to%20achieve%0Ahigh-level%20autonomous%20driving%2C%20and%20a%20lot%20of%20relevant%20research%20achievements%20have%0Abeen%20made%20recently.%20Despite%20the%20rapid%20development%2C%20most%20existing%20studies%20solely%0Afocused%20on%20normal%20safe%20scenarios%20while%20largely%20neglecting%20safety-critical%0Ascenarios%2C%20particularly%20those%20involving%20imminent%20collisions.%20This%20oversight%20may%0Aresult%20in%20autonomous%20vehicles%20lacking%20the%20essential%20predictive%20ability%20in%20such%0Asituations%2C%20posing%20a%20significant%20threat%20to%20safety.%20To%20tackle%20these%2C%20this%20paper%0Aproposes%20a%20risk-aware%20trajectory%20prediction%20framework%20tailored%20to%0Asafety-critical%20scenarios.%20Leveraging%20distinctive%20hazardous%20features%2C%20we%0Adevelop%20three%20core%20risk-aware%20components.%20First%2C%20we%20introduce%20a%0Arisk-incorporated%20scene%20encoder%2C%20which%20augments%20conventional%20encoders%20with%0Aquantitative%20risk%20information%20to%20achieve%20risk-aware%20encoding%20of%20hazardous%20scene%0Acontexts.%20Next%2C%20we%20incorporate%20endpoint-risk-combined%20intention%20queries%20as%0Aprediction%20priors%20in%20the%20decoder%20to%20ensure%20that%20the%20predicted%20multimodal%0Atrajectories%20cover%20both%20various%20spatial%20intentions%20and%20risk%20levels.%20Lastly%2C%20an%0Aauxiliary%20risk%20prediction%20task%20is%20implemented%20for%20the%20ultimate%20risk-aware%0Aprediction.%20Furthermore%2C%20to%20support%20model%20training%20and%20performance%20evaluation%2C%0Awe%20introduce%20a%20safety-critical%20trajectory%20prediction%20dataset%20and%20tailored%0Aevaluation%20metrics.%20We%20conduct%20comprehensive%20evaluations%20and%20compare%20our%20model%0Awith%20several%20SOTA%20models.%20Results%20demonstrate%20the%20superior%20performance%20of%20our%0Amodel%2C%20with%20a%20significant%20improvement%20in%20most%20metrics.%20This%20prediction%0Aadvancement%20enables%20autonomous%20vehicles%20to%20execute%20correct%20collision%20avoidance%0Amaneuvers%20under%20safety-critical%20scenarios%2C%20eventually%20enhancing%20road%20traffic%0Asafety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13480v1&entry.124074799=Read"},
{"title": "MeshFeat: Multi-Resolution Features for Neural Fields on Meshes", "author": "Mihir Mahajan and Florian Hofherr and Daniel Cremers", "abstract": "  Parametric feature grid encodings have gained significant attention as an\nencoding approach for neural fields since they allow for much smaller MLPs,\nwhich significantly decreases the inference time of the models. In this work,\nwe propose MeshFeat, a parametric feature encoding tailored to meshes, for\nwhich we adapt the idea of multi-resolution feature grids from Euclidean space.\nWe start from the structure provided by the given vertex topology and use a\nmesh simplification algorithm to construct a multi-resolution feature\nrepresentation directly on the mesh. The approach allows the usage of small\nMLPs for neural fields on meshes, and we show a significant speed-up compared\nto previous representations while maintaining comparable reconstruction quality\nfor texture reconstruction and BRDF representation. Given its intrinsic\ncoupling to the vertices, the method is particularly well-suited for\nrepresentations on deforming meshes, making it a good fit for object animation.\n", "link": "http://arxiv.org/abs/2407.13592v1", "date": "2024-07-18", "relevancy": 2.1089, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5997}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4754}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshFeat%3A%20Multi-Resolution%20Features%20for%20Neural%20Fields%20on%20Meshes&body=Title%3A%20MeshFeat%3A%20Multi-Resolution%20Features%20for%20Neural%20Fields%20on%20Meshes%0AAuthor%3A%20Mihir%20Mahajan%20and%20Florian%20Hofherr%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Parametric%20feature%20grid%20encodings%20have%20gained%20significant%20attention%20as%20an%0Aencoding%20approach%20for%20neural%20fields%20since%20they%20allow%20for%20much%20smaller%20MLPs%2C%0Awhich%20significantly%20decreases%20the%20inference%20time%20of%20the%20models.%20In%20this%20work%2C%0Awe%20propose%20MeshFeat%2C%20a%20parametric%20feature%20encoding%20tailored%20to%20meshes%2C%20for%0Awhich%20we%20adapt%20the%20idea%20of%20multi-resolution%20feature%20grids%20from%20Euclidean%20space.%0AWe%20start%20from%20the%20structure%20provided%20by%20the%20given%20vertex%20topology%20and%20use%20a%0Amesh%20simplification%20algorithm%20to%20construct%20a%20multi-resolution%20feature%0Arepresentation%20directly%20on%20the%20mesh.%20The%20approach%20allows%20the%20usage%20of%20small%0AMLPs%20for%20neural%20fields%20on%20meshes%2C%20and%20we%20show%20a%20significant%20speed-up%20compared%0Ato%20previous%20representations%20while%20maintaining%20comparable%20reconstruction%20quality%0Afor%20texture%20reconstruction%20and%20BRDF%20representation.%20Given%20its%20intrinsic%0Acoupling%20to%20the%20vertices%2C%20the%20method%20is%20particularly%20well-suited%20for%0Arepresentations%20on%20deforming%20meshes%2C%20making%20it%20a%20good%20fit%20for%20object%20animation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshFeat%253A%2520Multi-Resolution%2520Features%2520for%2520Neural%2520Fields%2520on%2520Meshes%26entry.906535625%3DMihir%2520Mahajan%2520and%2520Florian%2520Hofherr%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Parametric%2520feature%2520grid%2520encodings%2520have%2520gained%2520significant%2520attention%2520as%2520an%250Aencoding%2520approach%2520for%2520neural%2520fields%2520since%2520they%2520allow%2520for%2520much%2520smaller%2520MLPs%252C%250Awhich%2520significantly%2520decreases%2520the%2520inference%2520time%2520of%2520the%2520models.%2520In%2520this%2520work%252C%250Awe%2520propose%2520MeshFeat%252C%2520a%2520parametric%2520feature%2520encoding%2520tailored%2520to%2520meshes%252C%2520for%250Awhich%2520we%2520adapt%2520the%2520idea%2520of%2520multi-resolution%2520feature%2520grids%2520from%2520Euclidean%2520space.%250AWe%2520start%2520from%2520the%2520structure%2520provided%2520by%2520the%2520given%2520vertex%2520topology%2520and%2520use%2520a%250Amesh%2520simplification%2520algorithm%2520to%2520construct%2520a%2520multi-resolution%2520feature%250Arepresentation%2520directly%2520on%2520the%2520mesh.%2520The%2520approach%2520allows%2520the%2520usage%2520of%2520small%250AMLPs%2520for%2520neural%2520fields%2520on%2520meshes%252C%2520and%2520we%2520show%2520a%2520significant%2520speed-up%2520compared%250Ato%2520previous%2520representations%2520while%2520maintaining%2520comparable%2520reconstruction%2520quality%250Afor%2520texture%2520reconstruction%2520and%2520BRDF%2520representation.%2520Given%2520its%2520intrinsic%250Acoupling%2520to%2520the%2520vertices%252C%2520the%2520method%2520is%2520particularly%2520well-suited%2520for%250Arepresentations%2520on%2520deforming%2520meshes%252C%2520making%2520it%2520a%2520good%2520fit%2520for%2520object%2520animation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshFeat%3A%20Multi-Resolution%20Features%20for%20Neural%20Fields%20on%20Meshes&entry.906535625=Mihir%20Mahajan%20and%20Florian%20Hofherr%20and%20Daniel%20Cremers&entry.1292438233=%20%20Parametric%20feature%20grid%20encodings%20have%20gained%20significant%20attention%20as%20an%0Aencoding%20approach%20for%20neural%20fields%20since%20they%20allow%20for%20much%20smaller%20MLPs%2C%0Awhich%20significantly%20decreases%20the%20inference%20time%20of%20the%20models.%20In%20this%20work%2C%0Awe%20propose%20MeshFeat%2C%20a%20parametric%20feature%20encoding%20tailored%20to%20meshes%2C%20for%0Awhich%20we%20adapt%20the%20idea%20of%20multi-resolution%20feature%20grids%20from%20Euclidean%20space.%0AWe%20start%20from%20the%20structure%20provided%20by%20the%20given%20vertex%20topology%20and%20use%20a%0Amesh%20simplification%20algorithm%20to%20construct%20a%20multi-resolution%20feature%0Arepresentation%20directly%20on%20the%20mesh.%20The%20approach%20allows%20the%20usage%20of%20small%0AMLPs%20for%20neural%20fields%20on%20meshes%2C%20and%20we%20show%20a%20significant%20speed-up%20compared%0Ato%20previous%20representations%20while%20maintaining%20comparable%20reconstruction%20quality%0Afor%20texture%20reconstruction%20and%20BRDF%20representation.%20Given%20its%20intrinsic%0Acoupling%20to%20the%20vertices%2C%20the%20method%20is%20particularly%20well-suited%20for%0Arepresentations%20on%20deforming%20meshes%2C%20making%20it%20a%20good%20fit%20for%20object%20animation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13592v1&entry.124074799=Read"},
{"title": "Intention-Aware Planner for Robust and Safe Aerial Tracking", "author": "Qiuyu Ren and Huan Yu and Jiajun Dai and Zhi Zheng and Jun Meng and Li Xu and Chao Xu and Fei Gao and Yanjun Cao", "abstract": "  Autonomous target tracking with quadrotors has wide applications in many\nscenarios, such as cinematographic follow-up shooting or suspect chasing.\nTarget motion prediction is necessary when designing the tracking planner.\nHowever, the widely used constant velocity or constant rotation assumption can\nnot fully capture the dynamics of the target. The tracker may fail when the\ntarget happens to move aggressively, such as sudden turn or deceleration. In\nthis paper, we propose an intention-aware planner by additionally considering\nthe intention of the target to enhance safety and robustness in aerial tracking\napplications. Firstly, a designated intention prediction method is proposed,\nwhich combines a user-defined potential assessment function and a state\nobservation function. A reachable region is generated to specifically evaluate\nthe turning intentions. Then we design an intention-driven hybrid A* method to\npredict the future possible positions for the target. Finally, an\nintention-aware optimization approach is designed to generate a\nspatial-temporal optimal trajectory, allowing the tracker to perceive\nunexpected situations from the target. Benchmark comparisons and real-world\nexperiments are conducted to validate the performance of our method.\n", "link": "http://arxiv.org/abs/2309.08854v3", "date": "2024-07-18", "relevancy": 2.1063, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5424}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5341}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intention-Aware%20Planner%20for%20Robust%20and%20Safe%20Aerial%20Tracking&body=Title%3A%20Intention-Aware%20Planner%20for%20Robust%20and%20Safe%20Aerial%20Tracking%0AAuthor%3A%20Qiuyu%20Ren%20and%20Huan%20Yu%20and%20Jiajun%20Dai%20and%20Zhi%20Zheng%20and%20Jun%20Meng%20and%20Li%20Xu%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao%0AAbstract%3A%20%20%20Autonomous%20target%20tracking%20with%20quadrotors%20has%20wide%20applications%20in%20many%0Ascenarios%2C%20such%20as%20cinematographic%20follow-up%20shooting%20or%20suspect%20chasing.%0ATarget%20motion%20prediction%20is%20necessary%20when%20designing%20the%20tracking%20planner.%0AHowever%2C%20the%20widely%20used%20constant%20velocity%20or%20constant%20rotation%20assumption%20can%0Anot%20fully%20capture%20the%20dynamics%20of%20the%20target.%20The%20tracker%20may%20fail%20when%20the%0Atarget%20happens%20to%20move%20aggressively%2C%20such%20as%20sudden%20turn%20or%20deceleration.%20In%0Athis%20paper%2C%20we%20propose%20an%20intention-aware%20planner%20by%20additionally%20considering%0Athe%20intention%20of%20the%20target%20to%20enhance%20safety%20and%20robustness%20in%20aerial%20tracking%0Aapplications.%20Firstly%2C%20a%20designated%20intention%20prediction%20method%20is%20proposed%2C%0Awhich%20combines%20a%20user-defined%20potential%20assessment%20function%20and%20a%20state%0Aobservation%20function.%20A%20reachable%20region%20is%20generated%20to%20specifically%20evaluate%0Athe%20turning%20intentions.%20Then%20we%20design%20an%20intention-driven%20hybrid%20A%2A%20method%20to%0Apredict%20the%20future%20possible%20positions%20for%20the%20target.%20Finally%2C%20an%0Aintention-aware%20optimization%20approach%20is%20designed%20to%20generate%20a%0Aspatial-temporal%20optimal%20trajectory%2C%20allowing%20the%20tracker%20to%20perceive%0Aunexpected%20situations%20from%20the%20target.%20Benchmark%20comparisons%20and%20real-world%0Aexperiments%20are%20conducted%20to%20validate%20the%20performance%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08854v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntention-Aware%2520Planner%2520for%2520Robust%2520and%2520Safe%2520Aerial%2520Tracking%26entry.906535625%3DQiuyu%2520Ren%2520and%2520Huan%2520Yu%2520and%2520Jiajun%2520Dai%2520and%2520Zhi%2520Zheng%2520and%2520Jun%2520Meng%2520and%2520Li%2520Xu%2520and%2520Chao%2520Xu%2520and%2520Fei%2520Gao%2520and%2520Yanjun%2520Cao%26entry.1292438233%3D%2520%2520Autonomous%2520target%2520tracking%2520with%2520quadrotors%2520has%2520wide%2520applications%2520in%2520many%250Ascenarios%252C%2520such%2520as%2520cinematographic%2520follow-up%2520shooting%2520or%2520suspect%2520chasing.%250ATarget%2520motion%2520prediction%2520is%2520necessary%2520when%2520designing%2520the%2520tracking%2520planner.%250AHowever%252C%2520the%2520widely%2520used%2520constant%2520velocity%2520or%2520constant%2520rotation%2520assumption%2520can%250Anot%2520fully%2520capture%2520the%2520dynamics%2520of%2520the%2520target.%2520The%2520tracker%2520may%2520fail%2520when%2520the%250Atarget%2520happens%2520to%2520move%2520aggressively%252C%2520such%2520as%2520sudden%2520turn%2520or%2520deceleration.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520an%2520intention-aware%2520planner%2520by%2520additionally%2520considering%250Athe%2520intention%2520of%2520the%2520target%2520to%2520enhance%2520safety%2520and%2520robustness%2520in%2520aerial%2520tracking%250Aapplications.%2520Firstly%252C%2520a%2520designated%2520intention%2520prediction%2520method%2520is%2520proposed%252C%250Awhich%2520combines%2520a%2520user-defined%2520potential%2520assessment%2520function%2520and%2520a%2520state%250Aobservation%2520function.%2520A%2520reachable%2520region%2520is%2520generated%2520to%2520specifically%2520evaluate%250Athe%2520turning%2520intentions.%2520Then%2520we%2520design%2520an%2520intention-driven%2520hybrid%2520A%252A%2520method%2520to%250Apredict%2520the%2520future%2520possible%2520positions%2520for%2520the%2520target.%2520Finally%252C%2520an%250Aintention-aware%2520optimization%2520approach%2520is%2520designed%2520to%2520generate%2520a%250Aspatial-temporal%2520optimal%2520trajectory%252C%2520allowing%2520the%2520tracker%2520to%2520perceive%250Aunexpected%2520situations%2520from%2520the%2520target.%2520Benchmark%2520comparisons%2520and%2520real-world%250Aexperiments%2520are%2520conducted%2520to%2520validate%2520the%2520performance%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08854v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intention-Aware%20Planner%20for%20Robust%20and%20Safe%20Aerial%20Tracking&entry.906535625=Qiuyu%20Ren%20and%20Huan%20Yu%20and%20Jiajun%20Dai%20and%20Zhi%20Zheng%20and%20Jun%20Meng%20and%20Li%20Xu%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao&entry.1292438233=%20%20Autonomous%20target%20tracking%20with%20quadrotors%20has%20wide%20applications%20in%20many%0Ascenarios%2C%20such%20as%20cinematographic%20follow-up%20shooting%20or%20suspect%20chasing.%0ATarget%20motion%20prediction%20is%20necessary%20when%20designing%20the%20tracking%20planner.%0AHowever%2C%20the%20widely%20used%20constant%20velocity%20or%20constant%20rotation%20assumption%20can%0Anot%20fully%20capture%20the%20dynamics%20of%20the%20target.%20The%20tracker%20may%20fail%20when%20the%0Atarget%20happens%20to%20move%20aggressively%2C%20such%20as%20sudden%20turn%20or%20deceleration.%20In%0Athis%20paper%2C%20we%20propose%20an%20intention-aware%20planner%20by%20additionally%20considering%0Athe%20intention%20of%20the%20target%20to%20enhance%20safety%20and%20robustness%20in%20aerial%20tracking%0Aapplications.%20Firstly%2C%20a%20designated%20intention%20prediction%20method%20is%20proposed%2C%0Awhich%20combines%20a%20user-defined%20potential%20assessment%20function%20and%20a%20state%0Aobservation%20function.%20A%20reachable%20region%20is%20generated%20to%20specifically%20evaluate%0Athe%20turning%20intentions.%20Then%20we%20design%20an%20intention-driven%20hybrid%20A%2A%20method%20to%0Apredict%20the%20future%20possible%20positions%20for%20the%20target.%20Finally%2C%20an%0Aintention-aware%20optimization%20approach%20is%20designed%20to%20generate%20a%0Aspatial-temporal%20optimal%20trajectory%2C%20allowing%20the%20tracker%20to%20perceive%0Aunexpected%20situations%20from%20the%20target.%20Benchmark%20comparisons%20and%20real-world%0Aexperiments%20are%20conducted%20to%20validate%20the%20performance%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08854v3&entry.124074799=Read"},
{"title": "Localizing Anomalies via Multiscale Score Matching Analysis", "author": "Ahsan Mahmood and Junier Oliva and Martin Styner", "abstract": "  Anomaly detection and localization in medical imaging remain critical\nchallenges in healthcare. This paper introduces Spatial-MSMA (Multiscale Score\nMatching Analysis), a novel unsupervised method for anomaly localization in\nvolumetric brain MRIs. Building upon the MSMA framework, our approach\nincorporates spatial information and conditional likelihoods to enhance anomaly\ndetection capabilities. We employ a flexible normalizing flow model conditioned\non patch positions and global image features to estimate patch-wise anomaly\nscores. The method is evaluated on a dataset of 1,650 T1- and T2-weighted brain\nMRIs from typically developing children, with simulated lesions added to the\ntest set. Spatial-MSMA significantly outperforms existing methods, including\nreconstruction-based, generative-based, and interpretation-based approaches, in\nlesion detection and segmentation tasks. Our model achieves superior\nperformance in both distance-based metrics (99th percentile Hausdorff Distance:\n$7.05 \\pm 0.61$, Mean Surface Distance: $2.10 \\pm 0.43$) and component-wise\nmetrics (True Positive Rate: $0.83 \\pm 0.01$, Positive Predictive Value: $0.96\n\\pm 0.01$). These results demonstrate Spatial-MSMA's potential for accurate and\ninterpretable anomaly localization in medical imaging, with implications for\nimproved diagnosis and treatment planning in clinical settings. Our code is\navailable at~\\url{https://github.com/ahsanMah/sade/}.\n", "link": "http://arxiv.org/abs/2407.00148v2", "date": "2024-07-18", "relevancy": 2.1054, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.547}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5193}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localizing%20Anomalies%20via%20Multiscale%20Score%20Matching%20Analysis&body=Title%3A%20Localizing%20Anomalies%20via%20Multiscale%20Score%20Matching%20Analysis%0AAuthor%3A%20Ahsan%20Mahmood%20and%20Junier%20Oliva%20and%20Martin%20Styner%0AAbstract%3A%20%20%20Anomaly%20detection%20and%20localization%20in%20medical%20imaging%20remain%20critical%0Achallenges%20in%20healthcare.%20This%20paper%20introduces%20Spatial-MSMA%20%28Multiscale%20Score%0AMatching%20Analysis%29%2C%20a%20novel%20unsupervised%20method%20for%20anomaly%20localization%20in%0Avolumetric%20brain%20MRIs.%20Building%20upon%20the%20MSMA%20framework%2C%20our%20approach%0Aincorporates%20spatial%20information%20and%20conditional%20likelihoods%20to%20enhance%20anomaly%0Adetection%20capabilities.%20We%20employ%20a%20flexible%20normalizing%20flow%20model%20conditioned%0Aon%20patch%20positions%20and%20global%20image%20features%20to%20estimate%20patch-wise%20anomaly%0Ascores.%20The%20method%20is%20evaluated%20on%20a%20dataset%20of%201%2C650%20T1-%20and%20T2-weighted%20brain%0AMRIs%20from%20typically%20developing%20children%2C%20with%20simulated%20lesions%20added%20to%20the%0Atest%20set.%20Spatial-MSMA%20significantly%20outperforms%20existing%20methods%2C%20including%0Areconstruction-based%2C%20generative-based%2C%20and%20interpretation-based%20approaches%2C%20in%0Alesion%20detection%20and%20segmentation%20tasks.%20Our%20model%20achieves%20superior%0Aperformance%20in%20both%20distance-based%20metrics%20%2899th%20percentile%20Hausdorff%20Distance%3A%0A%247.05%20%5Cpm%200.61%24%2C%20Mean%20Surface%20Distance%3A%20%242.10%20%5Cpm%200.43%24%29%20and%20component-wise%0Ametrics%20%28True%20Positive%20Rate%3A%20%240.83%20%5Cpm%200.01%24%2C%20Positive%20Predictive%20Value%3A%20%240.96%0A%5Cpm%200.01%24%29.%20These%20results%20demonstrate%20Spatial-MSMA%27s%20potential%20for%20accurate%20and%0Ainterpretable%20anomaly%20localization%20in%20medical%20imaging%2C%20with%20implications%20for%0Aimproved%20diagnosis%20and%20treatment%20planning%20in%20clinical%20settings.%20Our%20code%20is%0Aavailable%20at~%5Curl%7Bhttps%3A//github.com/ahsanMah/sade/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00148v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalizing%2520Anomalies%2520via%2520Multiscale%2520Score%2520Matching%2520Analysis%26entry.906535625%3DAhsan%2520Mahmood%2520and%2520Junier%2520Oliva%2520and%2520Martin%2520Styner%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520and%2520localization%2520in%2520medical%2520imaging%2520remain%2520critical%250Achallenges%2520in%2520healthcare.%2520This%2520paper%2520introduces%2520Spatial-MSMA%2520%2528Multiscale%2520Score%250AMatching%2520Analysis%2529%252C%2520a%2520novel%2520unsupervised%2520method%2520for%2520anomaly%2520localization%2520in%250Avolumetric%2520brain%2520MRIs.%2520Building%2520upon%2520the%2520MSMA%2520framework%252C%2520our%2520approach%250Aincorporates%2520spatial%2520information%2520and%2520conditional%2520likelihoods%2520to%2520enhance%2520anomaly%250Adetection%2520capabilities.%2520We%2520employ%2520a%2520flexible%2520normalizing%2520flow%2520model%2520conditioned%250Aon%2520patch%2520positions%2520and%2520global%2520image%2520features%2520to%2520estimate%2520patch-wise%2520anomaly%250Ascores.%2520The%2520method%2520is%2520evaluated%2520on%2520a%2520dataset%2520of%25201%252C650%2520T1-%2520and%2520T2-weighted%2520brain%250AMRIs%2520from%2520typically%2520developing%2520children%252C%2520with%2520simulated%2520lesions%2520added%2520to%2520the%250Atest%2520set.%2520Spatial-MSMA%2520significantly%2520outperforms%2520existing%2520methods%252C%2520including%250Areconstruction-based%252C%2520generative-based%252C%2520and%2520interpretation-based%2520approaches%252C%2520in%250Alesion%2520detection%2520and%2520segmentation%2520tasks.%2520Our%2520model%2520achieves%2520superior%250Aperformance%2520in%2520both%2520distance-based%2520metrics%2520%252899th%2520percentile%2520Hausdorff%2520Distance%253A%250A%25247.05%2520%255Cpm%25200.61%2524%252C%2520Mean%2520Surface%2520Distance%253A%2520%25242.10%2520%255Cpm%25200.43%2524%2529%2520and%2520component-wise%250Ametrics%2520%2528True%2520Positive%2520Rate%253A%2520%25240.83%2520%255Cpm%25200.01%2524%252C%2520Positive%2520Predictive%2520Value%253A%2520%25240.96%250A%255Cpm%25200.01%2524%2529.%2520These%2520results%2520demonstrate%2520Spatial-MSMA%2527s%2520potential%2520for%2520accurate%2520and%250Ainterpretable%2520anomaly%2520localization%2520in%2520medical%2520imaging%252C%2520with%2520implications%2520for%250Aimproved%2520diagnosis%2520and%2520treatment%2520planning%2520in%2520clinical%2520settings.%2520Our%2520code%2520is%250Aavailable%2520at~%255Curl%257Bhttps%253A//github.com/ahsanMah/sade/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00148v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localizing%20Anomalies%20via%20Multiscale%20Score%20Matching%20Analysis&entry.906535625=Ahsan%20Mahmood%20and%20Junier%20Oliva%20and%20Martin%20Styner&entry.1292438233=%20%20Anomaly%20detection%20and%20localization%20in%20medical%20imaging%20remain%20critical%0Achallenges%20in%20healthcare.%20This%20paper%20introduces%20Spatial-MSMA%20%28Multiscale%20Score%0AMatching%20Analysis%29%2C%20a%20novel%20unsupervised%20method%20for%20anomaly%20localization%20in%0Avolumetric%20brain%20MRIs.%20Building%20upon%20the%20MSMA%20framework%2C%20our%20approach%0Aincorporates%20spatial%20information%20and%20conditional%20likelihoods%20to%20enhance%20anomaly%0Adetection%20capabilities.%20We%20employ%20a%20flexible%20normalizing%20flow%20model%20conditioned%0Aon%20patch%20positions%20and%20global%20image%20features%20to%20estimate%20patch-wise%20anomaly%0Ascores.%20The%20method%20is%20evaluated%20on%20a%20dataset%20of%201%2C650%20T1-%20and%20T2-weighted%20brain%0AMRIs%20from%20typically%20developing%20children%2C%20with%20simulated%20lesions%20added%20to%20the%0Atest%20set.%20Spatial-MSMA%20significantly%20outperforms%20existing%20methods%2C%20including%0Areconstruction-based%2C%20generative-based%2C%20and%20interpretation-based%20approaches%2C%20in%0Alesion%20detection%20and%20segmentation%20tasks.%20Our%20model%20achieves%20superior%0Aperformance%20in%20both%20distance-based%20metrics%20%2899th%20percentile%20Hausdorff%20Distance%3A%0A%247.05%20%5Cpm%200.61%24%2C%20Mean%20Surface%20Distance%3A%20%242.10%20%5Cpm%200.43%24%29%20and%20component-wise%0Ametrics%20%28True%20Positive%20Rate%3A%20%240.83%20%5Cpm%200.01%24%2C%20Positive%20Predictive%20Value%3A%20%240.96%0A%5Cpm%200.01%24%29.%20These%20results%20demonstrate%20Spatial-MSMA%27s%20potential%20for%20accurate%20and%0Ainterpretable%20anomaly%20localization%20in%20medical%20imaging%2C%20with%20implications%20for%0Aimproved%20diagnosis%20and%20treatment%20planning%20in%20clinical%20settings.%20Our%20code%20is%0Aavailable%20at~%5Curl%7Bhttps%3A//github.com/ahsanMah/sade/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00148v2&entry.124074799=Read"},
{"title": "Discussion: Effective and Interpretable Outcome Prediction by Training\n  Sparse Mixtures of Linear Experts", "author": "Francesco Folino and Luigi Pontieri and Pietro Sabatino", "abstract": "  Process Outcome Prediction entails predicting a discrete property of an\nunfinished process instance from its partial trace. High-capacity outcome\npredictors discovered with ensemble and deep learning methods have been shown\nto achieve top accuracy performances, but they suffer from a lack of\ntransparency. Aligning with recent efforts to learn inherently interpretable\noutcome predictors, we propose to train a sparse Mixture-of-Experts where both\nthe ``gate'' and ``expert'' sub-nets are Logistic Regressors. This\nensemble-like model is trained end-to-end while automatically selecting a\nsubset of input features in each sub-net, as an alternative to the common\napproach of performing a global feature selection step prior to model training.\nTest results on benchmark logs confirmed the validity and efficacy of this\napproach.\n", "link": "http://arxiv.org/abs/2407.13526v1", "date": "2024-07-18", "relevancy": 2.0983, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5276}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5268}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discussion%3A%20Effective%20and%20Interpretable%20Outcome%20Prediction%20by%20Training%0A%20%20Sparse%20Mixtures%20of%20Linear%20Experts&body=Title%3A%20Discussion%3A%20Effective%20and%20Interpretable%20Outcome%20Prediction%20by%20Training%0A%20%20Sparse%20Mixtures%20of%20Linear%20Experts%0AAuthor%3A%20Francesco%20Folino%20and%20Luigi%20Pontieri%20and%20Pietro%20Sabatino%0AAbstract%3A%20%20%20Process%20Outcome%20Prediction%20entails%20predicting%20a%20discrete%20property%20of%20an%0Aunfinished%20process%20instance%20from%20its%20partial%20trace.%20High-capacity%20outcome%0Apredictors%20discovered%20with%20ensemble%20and%20deep%20learning%20methods%20have%20been%20shown%0Ato%20achieve%20top%20accuracy%20performances%2C%20but%20they%20suffer%20from%20a%20lack%20of%0Atransparency.%20Aligning%20with%20recent%20efforts%20to%20learn%20inherently%20interpretable%0Aoutcome%20predictors%2C%20we%20propose%20to%20train%20a%20sparse%20Mixture-of-Experts%20where%20both%0Athe%20%60%60gate%27%27%20and%20%60%60expert%27%27%20sub-nets%20are%20Logistic%20Regressors.%20This%0Aensemble-like%20model%20is%20trained%20end-to-end%20while%20automatically%20selecting%20a%0Asubset%20of%20input%20features%20in%20each%20sub-net%2C%20as%20an%20alternative%20to%20the%20common%0Aapproach%20of%20performing%20a%20global%20feature%20selection%20step%20prior%20to%20model%20training.%0ATest%20results%20on%20benchmark%20logs%20confirmed%20the%20validity%20and%20efficacy%20of%20this%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscussion%253A%2520Effective%2520and%2520Interpretable%2520Outcome%2520Prediction%2520by%2520Training%250A%2520%2520Sparse%2520Mixtures%2520of%2520Linear%2520Experts%26entry.906535625%3DFrancesco%2520Folino%2520and%2520Luigi%2520Pontieri%2520and%2520Pietro%2520Sabatino%26entry.1292438233%3D%2520%2520Process%2520Outcome%2520Prediction%2520entails%2520predicting%2520a%2520discrete%2520property%2520of%2520an%250Aunfinished%2520process%2520instance%2520from%2520its%2520partial%2520trace.%2520High-capacity%2520outcome%250Apredictors%2520discovered%2520with%2520ensemble%2520and%2520deep%2520learning%2520methods%2520have%2520been%2520shown%250Ato%2520achieve%2520top%2520accuracy%2520performances%252C%2520but%2520they%2520suffer%2520from%2520a%2520lack%2520of%250Atransparency.%2520Aligning%2520with%2520recent%2520efforts%2520to%2520learn%2520inherently%2520interpretable%250Aoutcome%2520predictors%252C%2520we%2520propose%2520to%2520train%2520a%2520sparse%2520Mixture-of-Experts%2520where%2520both%250Athe%2520%2560%2560gate%2527%2527%2520and%2520%2560%2560expert%2527%2527%2520sub-nets%2520are%2520Logistic%2520Regressors.%2520This%250Aensemble-like%2520model%2520is%2520trained%2520end-to-end%2520while%2520automatically%2520selecting%2520a%250Asubset%2520of%2520input%2520features%2520in%2520each%2520sub-net%252C%2520as%2520an%2520alternative%2520to%2520the%2520common%250Aapproach%2520of%2520performing%2520a%2520global%2520feature%2520selection%2520step%2520prior%2520to%2520model%2520training.%250ATest%2520results%2520on%2520benchmark%2520logs%2520confirmed%2520the%2520validity%2520and%2520efficacy%2520of%2520this%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discussion%3A%20Effective%20and%20Interpretable%20Outcome%20Prediction%20by%20Training%0A%20%20Sparse%20Mixtures%20of%20Linear%20Experts&entry.906535625=Francesco%20Folino%20and%20Luigi%20Pontieri%20and%20Pietro%20Sabatino&entry.1292438233=%20%20Process%20Outcome%20Prediction%20entails%20predicting%20a%20discrete%20property%20of%20an%0Aunfinished%20process%20instance%20from%20its%20partial%20trace.%20High-capacity%20outcome%0Apredictors%20discovered%20with%20ensemble%20and%20deep%20learning%20methods%20have%20been%20shown%0Ato%20achieve%20top%20accuracy%20performances%2C%20but%20they%20suffer%20from%20a%20lack%20of%0Atransparency.%20Aligning%20with%20recent%20efforts%20to%20learn%20inherently%20interpretable%0Aoutcome%20predictors%2C%20we%20propose%20to%20train%20a%20sparse%20Mixture-of-Experts%20where%20both%0Athe%20%60%60gate%27%27%20and%20%60%60expert%27%27%20sub-nets%20are%20Logistic%20Regressors.%20This%0Aensemble-like%20model%20is%20trained%20end-to-end%20while%20automatically%20selecting%20a%0Asubset%20of%20input%20features%20in%20each%20sub-net%2C%20as%20an%20alternative%20to%20the%20common%0Aapproach%20of%20performing%20a%20global%20feature%20selection%20step%20prior%20to%20model%20training.%0ATest%20results%20on%20benchmark%20logs%20confirmed%20the%20validity%20and%20efficacy%20of%20this%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13526v1&entry.124074799=Read"},
{"title": "Visual Haystacks: Answering Harder Questions About Sets of Images", "author": "Tsung-Han Wu and Giscard Biamby and Jerome Quenum and Ritwik Gupta and Joseph E. Gonzalez and Trevor Darrell and David M. Chan", "abstract": "  Recent advancements in Large Multimodal Models (LMMs) have made significant\nprogress in the field of single-image visual question answering. However, these\nmodels face substantial challenges when tasked with queries that span extensive\ncollections of images, similar to real-world scenarios like searching through\nlarge photo albums, finding specific information across the internet, or\nmonitoring environmental changes through satellite imagery. This paper explores\nthe task of Multi-Image Visual Question Answering (MIQA): given a large set of\nimages and a natural language query, the task is to generate a relevant and\ngrounded response. We propose a new public benchmark, dubbed \"Visual Haystacks\n(VHs),\" specifically designed to evaluate LMMs' capabilities in visual\nretrieval and reasoning over sets of unrelated images, where we perform\ncomprehensive evaluations demonstrating that even robust closed-source models\nstruggle significantly. Towards addressing these shortcomings, we introduce\nMIRAGE (Multi-Image Retrieval Augmented Generation), a novel retrieval/QA\nframework tailored for LMMs that confronts the challenges of MIQA with marked\nefficiency and accuracy improvements over baseline methods. Our evaluation\nshows that MIRAGE surpasses closed-source GPT-4o models by up to 11% on the VHs\nbenchmark and offers up to 3.4x improvements in efficiency over text-focused\nmulti-stage approaches.\n", "link": "http://arxiv.org/abs/2407.13766v1", "date": "2024-07-18", "relevancy": 2.0886, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5256}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5251}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Haystacks%3A%20Answering%20Harder%20Questions%20About%20Sets%20of%20Images&body=Title%3A%20Visual%20Haystacks%3A%20Answering%20Harder%20Questions%20About%20Sets%20of%20Images%0AAuthor%3A%20Tsung-Han%20Wu%20and%20Giscard%20Biamby%20and%20Jerome%20Quenum%20and%20Ritwik%20Gupta%20and%20Joseph%20E.%20Gonzalez%20and%20Trevor%20Darrell%20and%20David%20M.%20Chan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20made%20significant%0Aprogress%20in%20the%20field%20of%20single-image%20visual%20question%20answering.%20However%2C%20these%0Amodels%20face%20substantial%20challenges%20when%20tasked%20with%20queries%20that%20span%20extensive%0Acollections%20of%20images%2C%20similar%20to%20real-world%20scenarios%20like%20searching%20through%0Alarge%20photo%20albums%2C%20finding%20specific%20information%20across%20the%20internet%2C%20or%0Amonitoring%20environmental%20changes%20through%20satellite%20imagery.%20This%20paper%20explores%0Athe%20task%20of%20Multi-Image%20Visual%20Question%20Answering%20%28MIQA%29%3A%20given%20a%20large%20set%20of%0Aimages%20and%20a%20natural%20language%20query%2C%20the%20task%20is%20to%20generate%20a%20relevant%20and%0Agrounded%20response.%20We%20propose%20a%20new%20public%20benchmark%2C%20dubbed%20%22Visual%20Haystacks%0A%28VHs%29%2C%22%20specifically%20designed%20to%20evaluate%20LMMs%27%20capabilities%20in%20visual%0Aretrieval%20and%20reasoning%20over%20sets%20of%20unrelated%20images%2C%20where%20we%20perform%0Acomprehensive%20evaluations%20demonstrating%20that%20even%20robust%20closed-source%20models%0Astruggle%20significantly.%20Towards%20addressing%20these%20shortcomings%2C%20we%20introduce%0AMIRAGE%20%28Multi-Image%20Retrieval%20Augmented%20Generation%29%2C%20a%20novel%20retrieval/QA%0Aframework%20tailored%20for%20LMMs%20that%20confronts%20the%20challenges%20of%20MIQA%20with%20marked%0Aefficiency%20and%20accuracy%20improvements%20over%20baseline%20methods.%20Our%20evaluation%0Ashows%20that%20MIRAGE%20surpasses%20closed-source%20GPT-4o%20models%20by%20up%20to%2011%25%20on%20the%20VHs%0Abenchmark%20and%20offers%20up%20to%203.4x%20improvements%20in%20efficiency%20over%20text-focused%0Amulti-stage%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Haystacks%253A%2520Answering%2520Harder%2520Questions%2520About%2520Sets%2520of%2520Images%26entry.906535625%3DTsung-Han%2520Wu%2520and%2520Giscard%2520Biamby%2520and%2520Jerome%2520Quenum%2520and%2520Ritwik%2520Gupta%2520and%2520Joseph%2520E.%2520Gonzalez%2520and%2520Trevor%2520Darrell%2520and%2520David%2520M.%2520Chan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520made%2520significant%250Aprogress%2520in%2520the%2520field%2520of%2520single-image%2520visual%2520question%2520answering.%2520However%252C%2520these%250Amodels%2520face%2520substantial%2520challenges%2520when%2520tasked%2520with%2520queries%2520that%2520span%2520extensive%250Acollections%2520of%2520images%252C%2520similar%2520to%2520real-world%2520scenarios%2520like%2520searching%2520through%250Alarge%2520photo%2520albums%252C%2520finding%2520specific%2520information%2520across%2520the%2520internet%252C%2520or%250Amonitoring%2520environmental%2520changes%2520through%2520satellite%2520imagery.%2520This%2520paper%2520explores%250Athe%2520task%2520of%2520Multi-Image%2520Visual%2520Question%2520Answering%2520%2528MIQA%2529%253A%2520given%2520a%2520large%2520set%2520of%250Aimages%2520and%2520a%2520natural%2520language%2520query%252C%2520the%2520task%2520is%2520to%2520generate%2520a%2520relevant%2520and%250Agrounded%2520response.%2520We%2520propose%2520a%2520new%2520public%2520benchmark%252C%2520dubbed%2520%2522Visual%2520Haystacks%250A%2528VHs%2529%252C%2522%2520specifically%2520designed%2520to%2520evaluate%2520LMMs%2527%2520capabilities%2520in%2520visual%250Aretrieval%2520and%2520reasoning%2520over%2520sets%2520of%2520unrelated%2520images%252C%2520where%2520we%2520perform%250Acomprehensive%2520evaluations%2520demonstrating%2520that%2520even%2520robust%2520closed-source%2520models%250Astruggle%2520significantly.%2520Towards%2520addressing%2520these%2520shortcomings%252C%2520we%2520introduce%250AMIRAGE%2520%2528Multi-Image%2520Retrieval%2520Augmented%2520Generation%2529%252C%2520a%2520novel%2520retrieval/QA%250Aframework%2520tailored%2520for%2520LMMs%2520that%2520confronts%2520the%2520challenges%2520of%2520MIQA%2520with%2520marked%250Aefficiency%2520and%2520accuracy%2520improvements%2520over%2520baseline%2520methods.%2520Our%2520evaluation%250Ashows%2520that%2520MIRAGE%2520surpasses%2520closed-source%2520GPT-4o%2520models%2520by%2520up%2520to%252011%2525%2520on%2520the%2520VHs%250Abenchmark%2520and%2520offers%2520up%2520to%25203.4x%2520improvements%2520in%2520efficiency%2520over%2520text-focused%250Amulti-stage%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Haystacks%3A%20Answering%20Harder%20Questions%20About%20Sets%20of%20Images&entry.906535625=Tsung-Han%20Wu%20and%20Giscard%20Biamby%20and%20Jerome%20Quenum%20and%20Ritwik%20Gupta%20and%20Joseph%20E.%20Gonzalez%20and%20Trevor%20Darrell%20and%20David%20M.%20Chan&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20made%20significant%0Aprogress%20in%20the%20field%20of%20single-image%20visual%20question%20answering.%20However%2C%20these%0Amodels%20face%20substantial%20challenges%20when%20tasked%20with%20queries%20that%20span%20extensive%0Acollections%20of%20images%2C%20similar%20to%20real-world%20scenarios%20like%20searching%20through%0Alarge%20photo%20albums%2C%20finding%20specific%20information%20across%20the%20internet%2C%20or%0Amonitoring%20environmental%20changes%20through%20satellite%20imagery.%20This%20paper%20explores%0Athe%20task%20of%20Multi-Image%20Visual%20Question%20Answering%20%28MIQA%29%3A%20given%20a%20large%20set%20of%0Aimages%20and%20a%20natural%20language%20query%2C%20the%20task%20is%20to%20generate%20a%20relevant%20and%0Agrounded%20response.%20We%20propose%20a%20new%20public%20benchmark%2C%20dubbed%20%22Visual%20Haystacks%0A%28VHs%29%2C%22%20specifically%20designed%20to%20evaluate%20LMMs%27%20capabilities%20in%20visual%0Aretrieval%20and%20reasoning%20over%20sets%20of%20unrelated%20images%2C%20where%20we%20perform%0Acomprehensive%20evaluations%20demonstrating%20that%20even%20robust%20closed-source%20models%0Astruggle%20significantly.%20Towards%20addressing%20these%20shortcomings%2C%20we%20introduce%0AMIRAGE%20%28Multi-Image%20Retrieval%20Augmented%20Generation%29%2C%20a%20novel%20retrieval/QA%0Aframework%20tailored%20for%20LMMs%20that%20confronts%20the%20challenges%20of%20MIQA%20with%20marked%0Aefficiency%20and%20accuracy%20improvements%20over%20baseline%20methods.%20Our%20evaluation%0Ashows%20that%20MIRAGE%20surpasses%20closed-source%20GPT-4o%20models%20by%20up%20to%2011%25%20on%20the%20VHs%0Abenchmark%20and%20offers%20up%20to%203.4x%20improvements%20in%20efficiency%20over%20text-focused%0Amulti-stage%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13766v1&entry.124074799=Read"},
{"title": "Aligning Sight and Sound: Advanced Sound Source Localization Through\n  Audio-Visual Alignment", "author": "Arda Senocak and Hyeonggon Ryu and Junsik Kim and Tae-Hyun Oh and Hanspeter Pfister and Joon Son Chung", "abstract": "  Recent studies on learning-based sound source localization have mainly\nfocused on the localization performance perspective. However, prior work and\nexisting benchmarks overlook a crucial aspect: cross-modal interaction, which\nis essential for interactive sound source localization. Cross-modal interaction\nis vital for understanding semantically matched or mismatched audio-visual\nevents, such as silent objects or off-screen sounds. In this paper, we first\ncomprehensively examine the cross-modal interaction of existing methods,\nbenchmarks, evaluation metrics, and cross-modal understanding tasks. Then, we\nidentify the limitations of previous studies and make several contributions to\novercome the limitations. First, we introduce a new synthetic benchmark for\ninteractive sound source localization. Second, we introduce new evaluation\nmetrics to rigorously assess sound source localization methods, focusing on\naccurately evaluating both localization performance and cross-modal interaction\nability. Third, we propose a learning framework with a cross-modal alignment\nstrategy to enhance cross-modal interaction. Lastly, we evaluate both\ninteractive sound source localization and auxiliary cross-modal retrieval tasks\ntogether to thoroughly assess cross-modal interaction capabilities and\nbenchmark competing methods. Our new benchmarks and evaluation metrics reveal\npreviously overlooked issues in sound source localization studies. Our proposed\nnovel method, with enhanced cross-modal alignment, shows superior sound source\nlocalization performance. This work provides the most comprehensive analysis of\nsound source localization to date, with extensive validation of competing\nmethods on both existing and new benchmarks using new and standard evaluation\nmetrics.\n", "link": "http://arxiv.org/abs/2407.13676v1", "date": "2024-07-18", "relevancy": 2.0791, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5436}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Sight%20and%20Sound%3A%20Advanced%20Sound%20Source%20Localization%20Through%0A%20%20Audio-Visual%20Alignment&body=Title%3A%20Aligning%20Sight%20and%20Sound%3A%20Advanced%20Sound%20Source%20Localization%20Through%0A%20%20Audio-Visual%20Alignment%0AAuthor%3A%20Arda%20Senocak%20and%20Hyeonggon%20Ryu%20and%20Junsik%20Kim%20and%20Tae-Hyun%20Oh%20and%20Hanspeter%20Pfister%20and%20Joon%20Son%20Chung%0AAbstract%3A%20%20%20Recent%20studies%20on%20learning-based%20sound%20source%20localization%20have%20mainly%0Afocused%20on%20the%20localization%20performance%20perspective.%20However%2C%20prior%20work%20and%0Aexisting%20benchmarks%20overlook%20a%20crucial%20aspect%3A%20cross-modal%20interaction%2C%20which%0Ais%20essential%20for%20interactive%20sound%20source%20localization.%20Cross-modal%20interaction%0Ais%20vital%20for%20understanding%20semantically%20matched%20or%20mismatched%20audio-visual%0Aevents%2C%20such%20as%20silent%20objects%20or%20off-screen%20sounds.%20In%20this%20paper%2C%20we%20first%0Acomprehensively%20examine%20the%20cross-modal%20interaction%20of%20existing%20methods%2C%0Abenchmarks%2C%20evaluation%20metrics%2C%20and%20cross-modal%20understanding%20tasks.%20Then%2C%20we%0Aidentify%20the%20limitations%20of%20previous%20studies%20and%20make%20several%20contributions%20to%0Aovercome%20the%20limitations.%20First%2C%20we%20introduce%20a%20new%20synthetic%20benchmark%20for%0Ainteractive%20sound%20source%20localization.%20Second%2C%20we%20introduce%20new%20evaluation%0Ametrics%20to%20rigorously%20assess%20sound%20source%20localization%20methods%2C%20focusing%20on%0Aaccurately%20evaluating%20both%20localization%20performance%20and%20cross-modal%20interaction%0Aability.%20Third%2C%20we%20propose%20a%20learning%20framework%20with%20a%20cross-modal%20alignment%0Astrategy%20to%20enhance%20cross-modal%20interaction.%20Lastly%2C%20we%20evaluate%20both%0Ainteractive%20sound%20source%20localization%20and%20auxiliary%20cross-modal%20retrieval%20tasks%0Atogether%20to%20thoroughly%20assess%20cross-modal%20interaction%20capabilities%20and%0Abenchmark%20competing%20methods.%20Our%20new%20benchmarks%20and%20evaluation%20metrics%20reveal%0Apreviously%20overlooked%20issues%20in%20sound%20source%20localization%20studies.%20Our%20proposed%0Anovel%20method%2C%20with%20enhanced%20cross-modal%20alignment%2C%20shows%20superior%20sound%20source%0Alocalization%20performance.%20This%20work%20provides%20the%20most%20comprehensive%20analysis%20of%0Asound%20source%20localization%20to%20date%2C%20with%20extensive%20validation%20of%20competing%0Amethods%20on%20both%20existing%20and%20new%20benchmarks%20using%20new%20and%20standard%20evaluation%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Sight%2520and%2520Sound%253A%2520Advanced%2520Sound%2520Source%2520Localization%2520Through%250A%2520%2520Audio-Visual%2520Alignment%26entry.906535625%3DArda%2520Senocak%2520and%2520Hyeonggon%2520Ryu%2520and%2520Junsik%2520Kim%2520and%2520Tae-Hyun%2520Oh%2520and%2520Hanspeter%2520Pfister%2520and%2520Joon%2520Son%2520Chung%26entry.1292438233%3D%2520%2520Recent%2520studies%2520on%2520learning-based%2520sound%2520source%2520localization%2520have%2520mainly%250Afocused%2520on%2520the%2520localization%2520performance%2520perspective.%2520However%252C%2520prior%2520work%2520and%250Aexisting%2520benchmarks%2520overlook%2520a%2520crucial%2520aspect%253A%2520cross-modal%2520interaction%252C%2520which%250Ais%2520essential%2520for%2520interactive%2520sound%2520source%2520localization.%2520Cross-modal%2520interaction%250Ais%2520vital%2520for%2520understanding%2520semantically%2520matched%2520or%2520mismatched%2520audio-visual%250Aevents%252C%2520such%2520as%2520silent%2520objects%2520or%2520off-screen%2520sounds.%2520In%2520this%2520paper%252C%2520we%2520first%250Acomprehensively%2520examine%2520the%2520cross-modal%2520interaction%2520of%2520existing%2520methods%252C%250Abenchmarks%252C%2520evaluation%2520metrics%252C%2520and%2520cross-modal%2520understanding%2520tasks.%2520Then%252C%2520we%250Aidentify%2520the%2520limitations%2520of%2520previous%2520studies%2520and%2520make%2520several%2520contributions%2520to%250Aovercome%2520the%2520limitations.%2520First%252C%2520we%2520introduce%2520a%2520new%2520synthetic%2520benchmark%2520for%250Ainteractive%2520sound%2520source%2520localization.%2520Second%252C%2520we%2520introduce%2520new%2520evaluation%250Ametrics%2520to%2520rigorously%2520assess%2520sound%2520source%2520localization%2520methods%252C%2520focusing%2520on%250Aaccurately%2520evaluating%2520both%2520localization%2520performance%2520and%2520cross-modal%2520interaction%250Aability.%2520Third%252C%2520we%2520propose%2520a%2520learning%2520framework%2520with%2520a%2520cross-modal%2520alignment%250Astrategy%2520to%2520enhance%2520cross-modal%2520interaction.%2520Lastly%252C%2520we%2520evaluate%2520both%250Ainteractive%2520sound%2520source%2520localization%2520and%2520auxiliary%2520cross-modal%2520retrieval%2520tasks%250Atogether%2520to%2520thoroughly%2520assess%2520cross-modal%2520interaction%2520capabilities%2520and%250Abenchmark%2520competing%2520methods.%2520Our%2520new%2520benchmarks%2520and%2520evaluation%2520metrics%2520reveal%250Apreviously%2520overlooked%2520issues%2520in%2520sound%2520source%2520localization%2520studies.%2520Our%2520proposed%250Anovel%2520method%252C%2520with%2520enhanced%2520cross-modal%2520alignment%252C%2520shows%2520superior%2520sound%2520source%250Alocalization%2520performance.%2520This%2520work%2520provides%2520the%2520most%2520comprehensive%2520analysis%2520of%250Asound%2520source%2520localization%2520to%2520date%252C%2520with%2520extensive%2520validation%2520of%2520competing%250Amethods%2520on%2520both%2520existing%2520and%2520new%2520benchmarks%2520using%2520new%2520and%2520standard%2520evaluation%250Ametrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Sight%20and%20Sound%3A%20Advanced%20Sound%20Source%20Localization%20Through%0A%20%20Audio-Visual%20Alignment&entry.906535625=Arda%20Senocak%20and%20Hyeonggon%20Ryu%20and%20Junsik%20Kim%20and%20Tae-Hyun%20Oh%20and%20Hanspeter%20Pfister%20and%20Joon%20Son%20Chung&entry.1292438233=%20%20Recent%20studies%20on%20learning-based%20sound%20source%20localization%20have%20mainly%0Afocused%20on%20the%20localization%20performance%20perspective.%20However%2C%20prior%20work%20and%0Aexisting%20benchmarks%20overlook%20a%20crucial%20aspect%3A%20cross-modal%20interaction%2C%20which%0Ais%20essential%20for%20interactive%20sound%20source%20localization.%20Cross-modal%20interaction%0Ais%20vital%20for%20understanding%20semantically%20matched%20or%20mismatched%20audio-visual%0Aevents%2C%20such%20as%20silent%20objects%20or%20off-screen%20sounds.%20In%20this%20paper%2C%20we%20first%0Acomprehensively%20examine%20the%20cross-modal%20interaction%20of%20existing%20methods%2C%0Abenchmarks%2C%20evaluation%20metrics%2C%20and%20cross-modal%20understanding%20tasks.%20Then%2C%20we%0Aidentify%20the%20limitations%20of%20previous%20studies%20and%20make%20several%20contributions%20to%0Aovercome%20the%20limitations.%20First%2C%20we%20introduce%20a%20new%20synthetic%20benchmark%20for%0Ainteractive%20sound%20source%20localization.%20Second%2C%20we%20introduce%20new%20evaluation%0Ametrics%20to%20rigorously%20assess%20sound%20source%20localization%20methods%2C%20focusing%20on%0Aaccurately%20evaluating%20both%20localization%20performance%20and%20cross-modal%20interaction%0Aability.%20Third%2C%20we%20propose%20a%20learning%20framework%20with%20a%20cross-modal%20alignment%0Astrategy%20to%20enhance%20cross-modal%20interaction.%20Lastly%2C%20we%20evaluate%20both%0Ainteractive%20sound%20source%20localization%20and%20auxiliary%20cross-modal%20retrieval%20tasks%0Atogether%20to%20thoroughly%20assess%20cross-modal%20interaction%20capabilities%20and%0Abenchmark%20competing%20methods.%20Our%20new%20benchmarks%20and%20evaluation%20metrics%20reveal%0Apreviously%20overlooked%20issues%20in%20sound%20source%20localization%20studies.%20Our%20proposed%0Anovel%20method%2C%20with%20enhanced%20cross-modal%20alignment%2C%20shows%20superior%20sound%20source%0Alocalization%20performance.%20This%20work%20provides%20the%20most%20comprehensive%20analysis%20of%0Asound%20source%20localization%20to%20date%2C%20with%20extensive%20validation%20of%20competing%0Amethods%20on%20both%20existing%20and%20new%20benchmarks%20using%20new%20and%20standard%20evaluation%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13676v1&entry.124074799=Read"},
{"title": "SCAPE: A Simple and Strong Category-Agnostic Pose Estimator", "author": "Yujia Liang and Zixuan Ye and Wenze Liu and Hao Lu", "abstract": "  Category-Agnostic Pose Estimation (CAPE) aims to localize keypoints on an\nobject of any category given few exemplars in an in-context manner. Prior arts\ninvolve sophisticated designs, e.g., sundry modules for similarity calculation\nand a two-stage framework, or takes in extra heatmap generation and\nsupervision. We notice that CAPE is essentially a task about feature matching,\nwhich can be solved within the attention process. Therefore we first streamline\nthe architecture into a simple baseline consisting of several pure\nself-attention layers and an MLP regression head -- this simplification means\nthat one only needs to consider the attention quality to boost the performance\nof CAPE. Towards an effective attention process for CAPE, we further introduce\ntwo key modules: i) a global keypoint feature perceptor to inject global\nsemantic information into support keypoints, and ii) a keypoint attention\nrefiner to enhance inter-node correlation between keypoints. They jointly form\na Simple and strong Category-Agnostic Pose Estimator (SCAPE). Experimental\nresults show that SCAPE outperforms prior arts by 2.2 and 1.3 PCK under 1-shot\nand 5-shot settings with faster inference speed and lighter model capacity,\nexcelling in both accuracy and efficiency. Code and models are available at\nhttps://github.com/tiny-smart/SCAPE\n", "link": "http://arxiv.org/abs/2407.13483v1", "date": "2024-07-18", "relevancy": 2.0712, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5371}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5095}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCAPE%3A%20A%20Simple%20and%20Strong%20Category-Agnostic%20Pose%20Estimator&body=Title%3A%20SCAPE%3A%20A%20Simple%20and%20Strong%20Category-Agnostic%20Pose%20Estimator%0AAuthor%3A%20Yujia%20Liang%20and%20Zixuan%20Ye%20and%20Wenze%20Liu%20and%20Hao%20Lu%0AAbstract%3A%20%20%20Category-Agnostic%20Pose%20Estimation%20%28CAPE%29%20aims%20to%20localize%20keypoints%20on%20an%0Aobject%20of%20any%20category%20given%20few%20exemplars%20in%20an%20in-context%20manner.%20Prior%20arts%0Ainvolve%20sophisticated%20designs%2C%20e.g.%2C%20sundry%20modules%20for%20similarity%20calculation%0Aand%20a%20two-stage%20framework%2C%20or%20takes%20in%20extra%20heatmap%20generation%20and%0Asupervision.%20We%20notice%20that%20CAPE%20is%20essentially%20a%20task%20about%20feature%20matching%2C%0Awhich%20can%20be%20solved%20within%20the%20attention%20process.%20Therefore%20we%20first%20streamline%0Athe%20architecture%20into%20a%20simple%20baseline%20consisting%20of%20several%20pure%0Aself-attention%20layers%20and%20an%20MLP%20regression%20head%20--%20this%20simplification%20means%0Athat%20one%20only%20needs%20to%20consider%20the%20attention%20quality%20to%20boost%20the%20performance%0Aof%20CAPE.%20Towards%20an%20effective%20attention%20process%20for%20CAPE%2C%20we%20further%20introduce%0Atwo%20key%20modules%3A%20i%29%20a%20global%20keypoint%20feature%20perceptor%20to%20inject%20global%0Asemantic%20information%20into%20support%20keypoints%2C%20and%20ii%29%20a%20keypoint%20attention%0Arefiner%20to%20enhance%20inter-node%20correlation%20between%20keypoints.%20They%20jointly%20form%0Aa%20Simple%20and%20strong%20Category-Agnostic%20Pose%20Estimator%20%28SCAPE%29.%20Experimental%0Aresults%20show%20that%20SCAPE%20outperforms%20prior%20arts%20by%202.2%20and%201.3%20PCK%20under%201-shot%0Aand%205-shot%20settings%20with%20faster%20inference%20speed%20and%20lighter%20model%20capacity%2C%0Aexcelling%20in%20both%20accuracy%20and%20efficiency.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/tiny-smart/SCAPE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCAPE%253A%2520A%2520Simple%2520and%2520Strong%2520Category-Agnostic%2520Pose%2520Estimator%26entry.906535625%3DYujia%2520Liang%2520and%2520Zixuan%2520Ye%2520and%2520Wenze%2520Liu%2520and%2520Hao%2520Lu%26entry.1292438233%3D%2520%2520Category-Agnostic%2520Pose%2520Estimation%2520%2528CAPE%2529%2520aims%2520to%2520localize%2520keypoints%2520on%2520an%250Aobject%2520of%2520any%2520category%2520given%2520few%2520exemplars%2520in%2520an%2520in-context%2520manner.%2520Prior%2520arts%250Ainvolve%2520sophisticated%2520designs%252C%2520e.g.%252C%2520sundry%2520modules%2520for%2520similarity%2520calculation%250Aand%2520a%2520two-stage%2520framework%252C%2520or%2520takes%2520in%2520extra%2520heatmap%2520generation%2520and%250Asupervision.%2520We%2520notice%2520that%2520CAPE%2520is%2520essentially%2520a%2520task%2520about%2520feature%2520matching%252C%250Awhich%2520can%2520be%2520solved%2520within%2520the%2520attention%2520process.%2520Therefore%2520we%2520first%2520streamline%250Athe%2520architecture%2520into%2520a%2520simple%2520baseline%2520consisting%2520of%2520several%2520pure%250Aself-attention%2520layers%2520and%2520an%2520MLP%2520regression%2520head%2520--%2520this%2520simplification%2520means%250Athat%2520one%2520only%2520needs%2520to%2520consider%2520the%2520attention%2520quality%2520to%2520boost%2520the%2520performance%250Aof%2520CAPE.%2520Towards%2520an%2520effective%2520attention%2520process%2520for%2520CAPE%252C%2520we%2520further%2520introduce%250Atwo%2520key%2520modules%253A%2520i%2529%2520a%2520global%2520keypoint%2520feature%2520perceptor%2520to%2520inject%2520global%250Asemantic%2520information%2520into%2520support%2520keypoints%252C%2520and%2520ii%2529%2520a%2520keypoint%2520attention%250Arefiner%2520to%2520enhance%2520inter-node%2520correlation%2520between%2520keypoints.%2520They%2520jointly%2520form%250Aa%2520Simple%2520and%2520strong%2520Category-Agnostic%2520Pose%2520Estimator%2520%2528SCAPE%2529.%2520Experimental%250Aresults%2520show%2520that%2520SCAPE%2520outperforms%2520prior%2520arts%2520by%25202.2%2520and%25201.3%2520PCK%2520under%25201-shot%250Aand%25205-shot%2520settings%2520with%2520faster%2520inference%2520speed%2520and%2520lighter%2520model%2520capacity%252C%250Aexcelling%2520in%2520both%2520accuracy%2520and%2520efficiency.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/tiny-smart/SCAPE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCAPE%3A%20A%20Simple%20and%20Strong%20Category-Agnostic%20Pose%20Estimator&entry.906535625=Yujia%20Liang%20and%20Zixuan%20Ye%20and%20Wenze%20Liu%20and%20Hao%20Lu&entry.1292438233=%20%20Category-Agnostic%20Pose%20Estimation%20%28CAPE%29%20aims%20to%20localize%20keypoints%20on%20an%0Aobject%20of%20any%20category%20given%20few%20exemplars%20in%20an%20in-context%20manner.%20Prior%20arts%0Ainvolve%20sophisticated%20designs%2C%20e.g.%2C%20sundry%20modules%20for%20similarity%20calculation%0Aand%20a%20two-stage%20framework%2C%20or%20takes%20in%20extra%20heatmap%20generation%20and%0Asupervision.%20We%20notice%20that%20CAPE%20is%20essentially%20a%20task%20about%20feature%20matching%2C%0Awhich%20can%20be%20solved%20within%20the%20attention%20process.%20Therefore%20we%20first%20streamline%0Athe%20architecture%20into%20a%20simple%20baseline%20consisting%20of%20several%20pure%0Aself-attention%20layers%20and%20an%20MLP%20regression%20head%20--%20this%20simplification%20means%0Athat%20one%20only%20needs%20to%20consider%20the%20attention%20quality%20to%20boost%20the%20performance%0Aof%20CAPE.%20Towards%20an%20effective%20attention%20process%20for%20CAPE%2C%20we%20further%20introduce%0Atwo%20key%20modules%3A%20i%29%20a%20global%20keypoint%20feature%20perceptor%20to%20inject%20global%0Asemantic%20information%20into%20support%20keypoints%2C%20and%20ii%29%20a%20keypoint%20attention%0Arefiner%20to%20enhance%20inter-node%20correlation%20between%20keypoints.%20They%20jointly%20form%0Aa%20Simple%20and%20strong%20Category-Agnostic%20Pose%20Estimator%20%28SCAPE%29.%20Experimental%0Aresults%20show%20that%20SCAPE%20outperforms%20prior%20arts%20by%202.2%20and%201.3%20PCK%20under%201-shot%0Aand%205-shot%20settings%20with%20faster%20inference%20speed%20and%20lighter%20model%20capacity%2C%0Aexcelling%20in%20both%20accuracy%20and%20efficiency.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/tiny-smart/SCAPE%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13483v1&entry.124074799=Read"},
{"title": "Higher-order Spatio-temporal Physics-incorporated Graph Neural Network\n  for Multivariate Time Series Imputation", "author": "Guojun Liang and Prayag Tiwari and Slawomir Nowaczyk and Stefan Byttner", "abstract": "  Exploring the missing values is an essential but challenging issue due to the\ncomplex latent spatio-temporal correlation and dynamic nature of time series.\nOwing to the outstanding performance in dealing with structure learning\npotentials, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs)\nare often used to capture such complex spatio-temporal features in multivariate\ntime series. However, these data-driven models often fail to capture the\nessential spatio-temporal relationships when significant signal corruption\noccurs. Additionally, calculating the high-order neighbor nodes in these models\nis of high computational complexity. To address these problems, we propose a\nnovel higher-order spatio-temporal physics-incorporated GNN (HSPGNN). Firstly,\nthe dynamic Laplacian matrix can be obtained by the spatial attention\nmechanism. Then, the generic inhomogeneous partial differential equation (PDE)\nof physical dynamic systems is used to construct the dynamic higher-order\nspatio-temporal GNN to obtain the missing time series values. Moreover, we\nestimate the missing impact by Normalizing Flows (NF) to evaluate the\nimportance of each node in the graph for better explainability. Experimental\nresults on four benchmark datasets demonstrate the effectiveness of HSPGNN and\nthe superior performance when combining various order neighbor nodes. Also,\ngraph-like optical flow, dynamic graphs, and missing impact can be obtained\nnaturally by HSPGNN, which provides better dynamic analysis and explanation\nthan traditional data-driven models. Our code is available at\nhttps://github.com/gorgen2020/HSPGNN.\n", "link": "http://arxiv.org/abs/2405.10995v2", "date": "2024-07-18", "relevancy": 2.0686, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5487}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4999}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-order%20Spatio-temporal%20Physics-incorporated%20Graph%20Neural%20Network%0A%20%20for%20Multivariate%20Time%20Series%20Imputation&body=Title%3A%20Higher-order%20Spatio-temporal%20Physics-incorporated%20Graph%20Neural%20Network%0A%20%20for%20Multivariate%20Time%20Series%20Imputation%0AAuthor%3A%20Guojun%20Liang%20and%20Prayag%20Tiwari%20and%20Slawomir%20Nowaczyk%20and%20Stefan%20Byttner%0AAbstract%3A%20%20%20Exploring%20the%20missing%20values%20is%20an%20essential%20but%20challenging%20issue%20due%20to%20the%0Acomplex%20latent%20spatio-temporal%20correlation%20and%20dynamic%20nature%20of%20time%20series.%0AOwing%20to%20the%20outstanding%20performance%20in%20dealing%20with%20structure%20learning%0Apotentials%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20and%20Recurrent%20Neural%20Networks%20%28RNNs%29%0Aare%20often%20used%20to%20capture%20such%20complex%20spatio-temporal%20features%20in%20multivariate%0Atime%20series.%20However%2C%20these%20data-driven%20models%20often%20fail%20to%20capture%20the%0Aessential%20spatio-temporal%20relationships%20when%20significant%20signal%20corruption%0Aoccurs.%20Additionally%2C%20calculating%20the%20high-order%20neighbor%20nodes%20in%20these%20models%0Ais%20of%20high%20computational%20complexity.%20To%20address%20these%20problems%2C%20we%20propose%20a%0Anovel%20higher-order%20spatio-temporal%20physics-incorporated%20GNN%20%28HSPGNN%29.%20Firstly%2C%0Athe%20dynamic%20Laplacian%20matrix%20can%20be%20obtained%20by%20the%20spatial%20attention%0Amechanism.%20Then%2C%20the%20generic%20inhomogeneous%20partial%20differential%20equation%20%28PDE%29%0Aof%20physical%20dynamic%20systems%20is%20used%20to%20construct%20the%20dynamic%20higher-order%0Aspatio-temporal%20GNN%20to%20obtain%20the%20missing%20time%20series%20values.%20Moreover%2C%20we%0Aestimate%20the%20missing%20impact%20by%20Normalizing%20Flows%20%28NF%29%20to%20evaluate%20the%0Aimportance%20of%20each%20node%20in%20the%20graph%20for%20better%20explainability.%20Experimental%0Aresults%20on%20four%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20HSPGNN%20and%0Athe%20superior%20performance%20when%20combining%20various%20order%20neighbor%20nodes.%20Also%2C%0Agraph-like%20optical%20flow%2C%20dynamic%20graphs%2C%20and%20missing%20impact%20can%20be%20obtained%0Anaturally%20by%20HSPGNN%2C%20which%20provides%20better%20dynamic%20analysis%20and%20explanation%0Athan%20traditional%20data-driven%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/gorgen2020/HSPGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10995v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-order%2520Spatio-temporal%2520Physics-incorporated%2520Graph%2520Neural%2520Network%250A%2520%2520for%2520Multivariate%2520Time%2520Series%2520Imputation%26entry.906535625%3DGuojun%2520Liang%2520and%2520Prayag%2520Tiwari%2520and%2520Slawomir%2520Nowaczyk%2520and%2520Stefan%2520Byttner%26entry.1292438233%3D%2520%2520Exploring%2520the%2520missing%2520values%2520is%2520an%2520essential%2520but%2520challenging%2520issue%2520due%2520to%2520the%250Acomplex%2520latent%2520spatio-temporal%2520correlation%2520and%2520dynamic%2520nature%2520of%2520time%2520series.%250AOwing%2520to%2520the%2520outstanding%2520performance%2520in%2520dealing%2520with%2520structure%2520learning%250Apotentials%252C%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520and%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%250Aare%2520often%2520used%2520to%2520capture%2520such%2520complex%2520spatio-temporal%2520features%2520in%2520multivariate%250Atime%2520series.%2520However%252C%2520these%2520data-driven%2520models%2520often%2520fail%2520to%2520capture%2520the%250Aessential%2520spatio-temporal%2520relationships%2520when%2520significant%2520signal%2520corruption%250Aoccurs.%2520Additionally%252C%2520calculating%2520the%2520high-order%2520neighbor%2520nodes%2520in%2520these%2520models%250Ais%2520of%2520high%2520computational%2520complexity.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%250Anovel%2520higher-order%2520spatio-temporal%2520physics-incorporated%2520GNN%2520%2528HSPGNN%2529.%2520Firstly%252C%250Athe%2520dynamic%2520Laplacian%2520matrix%2520can%2520be%2520obtained%2520by%2520the%2520spatial%2520attention%250Amechanism.%2520Then%252C%2520the%2520generic%2520inhomogeneous%2520partial%2520differential%2520equation%2520%2528PDE%2529%250Aof%2520physical%2520dynamic%2520systems%2520is%2520used%2520to%2520construct%2520the%2520dynamic%2520higher-order%250Aspatio-temporal%2520GNN%2520to%2520obtain%2520the%2520missing%2520time%2520series%2520values.%2520Moreover%252C%2520we%250Aestimate%2520the%2520missing%2520impact%2520by%2520Normalizing%2520Flows%2520%2528NF%2529%2520to%2520evaluate%2520the%250Aimportance%2520of%2520each%2520node%2520in%2520the%2520graph%2520for%2520better%2520explainability.%2520Experimental%250Aresults%2520on%2520four%2520benchmark%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520HSPGNN%2520and%250Athe%2520superior%2520performance%2520when%2520combining%2520various%2520order%2520neighbor%2520nodes.%2520Also%252C%250Agraph-like%2520optical%2520flow%252C%2520dynamic%2520graphs%252C%2520and%2520missing%2520impact%2520can%2520be%2520obtained%250Anaturally%2520by%2520HSPGNN%252C%2520which%2520provides%2520better%2520dynamic%2520analysis%2520and%2520explanation%250Athan%2520traditional%2520data-driven%2520models.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/gorgen2020/HSPGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10995v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-order%20Spatio-temporal%20Physics-incorporated%20Graph%20Neural%20Network%0A%20%20for%20Multivariate%20Time%20Series%20Imputation&entry.906535625=Guojun%20Liang%20and%20Prayag%20Tiwari%20and%20Slawomir%20Nowaczyk%20and%20Stefan%20Byttner&entry.1292438233=%20%20Exploring%20the%20missing%20values%20is%20an%20essential%20but%20challenging%20issue%20due%20to%20the%0Acomplex%20latent%20spatio-temporal%20correlation%20and%20dynamic%20nature%20of%20time%20series.%0AOwing%20to%20the%20outstanding%20performance%20in%20dealing%20with%20structure%20learning%0Apotentials%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20and%20Recurrent%20Neural%20Networks%20%28RNNs%29%0Aare%20often%20used%20to%20capture%20such%20complex%20spatio-temporal%20features%20in%20multivariate%0Atime%20series.%20However%2C%20these%20data-driven%20models%20often%20fail%20to%20capture%20the%0Aessential%20spatio-temporal%20relationships%20when%20significant%20signal%20corruption%0Aoccurs.%20Additionally%2C%20calculating%20the%20high-order%20neighbor%20nodes%20in%20these%20models%0Ais%20of%20high%20computational%20complexity.%20To%20address%20these%20problems%2C%20we%20propose%20a%0Anovel%20higher-order%20spatio-temporal%20physics-incorporated%20GNN%20%28HSPGNN%29.%20Firstly%2C%0Athe%20dynamic%20Laplacian%20matrix%20can%20be%20obtained%20by%20the%20spatial%20attention%0Amechanism.%20Then%2C%20the%20generic%20inhomogeneous%20partial%20differential%20equation%20%28PDE%29%0Aof%20physical%20dynamic%20systems%20is%20used%20to%20construct%20the%20dynamic%20higher-order%0Aspatio-temporal%20GNN%20to%20obtain%20the%20missing%20time%20series%20values.%20Moreover%2C%20we%0Aestimate%20the%20missing%20impact%20by%20Normalizing%20Flows%20%28NF%29%20to%20evaluate%20the%0Aimportance%20of%20each%20node%20in%20the%20graph%20for%20better%20explainability.%20Experimental%0Aresults%20on%20four%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20HSPGNN%20and%0Athe%20superior%20performance%20when%20combining%20various%20order%20neighbor%20nodes.%20Also%2C%0Agraph-like%20optical%20flow%2C%20dynamic%20graphs%2C%20and%20missing%20impact%20can%20be%20obtained%0Anaturally%20by%20HSPGNN%2C%20which%20provides%20better%20dynamic%20analysis%20and%20explanation%0Athan%20traditional%20data-driven%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/gorgen2020/HSPGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10995v2&entry.124074799=Read"},
{"title": "The Effects of Selected Object Features on a Pick-and-Place Task: a\n  Human Multimodal Dataset", "author": "Linda Lastrico and Valerio Belcamino and Alessandro Carf\u00ec and Alessia Vignolo and Alessandra Sciutti and Fulvio Mastrogiovanni and Francesco Rea", "abstract": "  We propose a dataset to study the influence of object-specific\ncharacteristics on human pick-and-place movements and compare the quality of\nthe motion kinematics extracted by various sensors. This dataset is also\nsuitable for promoting a broader discussion on general learning problems in the\nhand-object interaction domain, such as intention recognition or motion\ngeneration with applications in the Robotics field. The dataset consists of the\nrecordings of 15 subjects performing 80 repetitions of a pick-and-place action\nunder various experimental conditions, for a total of 1200 pick-and-places. The\ndata has been collected thanks to a multimodal setup composed of multiple\ncameras, observing the actions from different perspectives, a motion capture\nsystem, and a wrist-worn inertial measurement unit. All the objects manipulated\nin the experiments are identical in shape, size, and appearance but differ in\nweight and liquid filling, which influences the carefulness required for their\nhandling.\n", "link": "http://arxiv.org/abs/2407.13425v1", "date": "2024-07-18", "relevancy": 2.0643, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5513}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5154}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Effects%20of%20Selected%20Object%20Features%20on%20a%20Pick-and-Place%20Task%3A%20a%0A%20%20Human%20Multimodal%20Dataset&body=Title%3A%20The%20Effects%20of%20Selected%20Object%20Features%20on%20a%20Pick-and-Place%20Task%3A%20a%0A%20%20Human%20Multimodal%20Dataset%0AAuthor%3A%20Linda%20Lastrico%20and%20Valerio%20Belcamino%20and%20Alessandro%20Carf%C3%AC%20and%20Alessia%20Vignolo%20and%20Alessandra%20Sciutti%20and%20Fulvio%20Mastrogiovanni%20and%20Francesco%20Rea%0AAbstract%3A%20%20%20We%20propose%20a%20dataset%20to%20study%20the%20influence%20of%20object-specific%0Acharacteristics%20on%20human%20pick-and-place%20movements%20and%20compare%20the%20quality%20of%0Athe%20motion%20kinematics%20extracted%20by%20various%20sensors.%20This%20dataset%20is%20also%0Asuitable%20for%20promoting%20a%20broader%20discussion%20on%20general%20learning%20problems%20in%20the%0Ahand-object%20interaction%20domain%2C%20such%20as%20intention%20recognition%20or%20motion%0Ageneration%20with%20applications%20in%20the%20Robotics%20field.%20The%20dataset%20consists%20of%20the%0Arecordings%20of%2015%20subjects%20performing%2080%20repetitions%20of%20a%20pick-and-place%20action%0Aunder%20various%20experimental%20conditions%2C%20for%20a%20total%20of%201200%20pick-and-places.%20The%0Adata%20has%20been%20collected%20thanks%20to%20a%20multimodal%20setup%20composed%20of%20multiple%0Acameras%2C%20observing%20the%20actions%20from%20different%20perspectives%2C%20a%20motion%20capture%0Asystem%2C%20and%20a%20wrist-worn%20inertial%20measurement%20unit.%20All%20the%20objects%20manipulated%0Ain%20the%20experiments%20are%20identical%20in%20shape%2C%20size%2C%20and%20appearance%20but%20differ%20in%0Aweight%20and%20liquid%20filling%2C%20which%20influences%20the%20carefulness%20required%20for%20their%0Ahandling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Effects%2520of%2520Selected%2520Object%2520Features%2520on%2520a%2520Pick-and-Place%2520Task%253A%2520a%250A%2520%2520Human%2520Multimodal%2520Dataset%26entry.906535625%3DLinda%2520Lastrico%2520and%2520Valerio%2520Belcamino%2520and%2520Alessandro%2520Carf%25C3%25AC%2520and%2520Alessia%2520Vignolo%2520and%2520Alessandra%2520Sciutti%2520and%2520Fulvio%2520Mastrogiovanni%2520and%2520Francesco%2520Rea%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520dataset%2520to%2520study%2520the%2520influence%2520of%2520object-specific%250Acharacteristics%2520on%2520human%2520pick-and-place%2520movements%2520and%2520compare%2520the%2520quality%2520of%250Athe%2520motion%2520kinematics%2520extracted%2520by%2520various%2520sensors.%2520This%2520dataset%2520is%2520also%250Asuitable%2520for%2520promoting%2520a%2520broader%2520discussion%2520on%2520general%2520learning%2520problems%2520in%2520the%250Ahand-object%2520interaction%2520domain%252C%2520such%2520as%2520intention%2520recognition%2520or%2520motion%250Ageneration%2520with%2520applications%2520in%2520the%2520Robotics%2520field.%2520The%2520dataset%2520consists%2520of%2520the%250Arecordings%2520of%252015%2520subjects%2520performing%252080%2520repetitions%2520of%2520a%2520pick-and-place%2520action%250Aunder%2520various%2520experimental%2520conditions%252C%2520for%2520a%2520total%2520of%25201200%2520pick-and-places.%2520The%250Adata%2520has%2520been%2520collected%2520thanks%2520to%2520a%2520multimodal%2520setup%2520composed%2520of%2520multiple%250Acameras%252C%2520observing%2520the%2520actions%2520from%2520different%2520perspectives%252C%2520a%2520motion%2520capture%250Asystem%252C%2520and%2520a%2520wrist-worn%2520inertial%2520measurement%2520unit.%2520All%2520the%2520objects%2520manipulated%250Ain%2520the%2520experiments%2520are%2520identical%2520in%2520shape%252C%2520size%252C%2520and%2520appearance%2520but%2520differ%2520in%250Aweight%2520and%2520liquid%2520filling%252C%2520which%2520influences%2520the%2520carefulness%2520required%2520for%2520their%250Ahandling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Effects%20of%20Selected%20Object%20Features%20on%20a%20Pick-and-Place%20Task%3A%20a%0A%20%20Human%20Multimodal%20Dataset&entry.906535625=Linda%20Lastrico%20and%20Valerio%20Belcamino%20and%20Alessandro%20Carf%C3%AC%20and%20Alessia%20Vignolo%20and%20Alessandra%20Sciutti%20and%20Fulvio%20Mastrogiovanni%20and%20Francesco%20Rea&entry.1292438233=%20%20We%20propose%20a%20dataset%20to%20study%20the%20influence%20of%20object-specific%0Acharacteristics%20on%20human%20pick-and-place%20movements%20and%20compare%20the%20quality%20of%0Athe%20motion%20kinematics%20extracted%20by%20various%20sensors.%20This%20dataset%20is%20also%0Asuitable%20for%20promoting%20a%20broader%20discussion%20on%20general%20learning%20problems%20in%20the%0Ahand-object%20interaction%20domain%2C%20such%20as%20intention%20recognition%20or%20motion%0Ageneration%20with%20applications%20in%20the%20Robotics%20field.%20The%20dataset%20consists%20of%20the%0Arecordings%20of%2015%20subjects%20performing%2080%20repetitions%20of%20a%20pick-and-place%20action%0Aunder%20various%20experimental%20conditions%2C%20for%20a%20total%20of%201200%20pick-and-places.%20The%0Adata%20has%20been%20collected%20thanks%20to%20a%20multimodal%20setup%20composed%20of%20multiple%0Acameras%2C%20observing%20the%20actions%20from%20different%20perspectives%2C%20a%20motion%20capture%0Asystem%2C%20and%20a%20wrist-worn%20inertial%20measurement%20unit.%20All%20the%20objects%20manipulated%0Ain%20the%20experiments%20are%20identical%20in%20shape%2C%20size%2C%20and%20appearance%20but%20differ%20in%0Aweight%20and%20liquid%20filling%2C%20which%20influences%20the%20carefulness%20required%20for%20their%0Ahandling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13425v1&entry.124074799=Read"},
{"title": "Efficient Training for Multilingual Visual Speech Recognition:\n  Pre-training with Discretized Visual Speech Representation", "author": "Minsu Kim and Jeong Hun Yeo and Se Jin Park and Hyeongseop Rha and Yong Man Ro", "abstract": "  This paper explores sentence-level multilingual Visual Speech Recognition\n(VSR) that can recognize different languages with a single trained model. As\nthe massive multilingual modeling of visual data requires huge computational\ncosts, we propose a novel training strategy, processing with visual speech\nunits. Motivated by the recent success of the audio speech unit, we propose to\nuse a visual speech unit that can be obtained by discretizing the visual speech\nfeatures extracted from the self-supervised visual speech model. Through\nanalysis, we verify that the visual speech units mainly contain viseme\ninformation while suppressing non-linguistic information. By using the visual\nspeech units as the inputs of our system, we propose to pre-train a VSR model\nto predict corresponding text outputs on multilingual data constructed by\nmerging several VSR databases. As both the inputs (i.e., visual speech units)\nand outputs (i.e., text) are discrete, we can greatly improve the training\nefficiency compared to the standard VSR training. Specifically, the input data\nsize is reduced to 0.016% of the original video inputs. In order to complement\nthe insufficient visual information in speech recognition, we apply curriculum\nlearning where the inputs of the system begin with audio-visual speech units\nand gradually change to visual speech units. After pre-training, the model is\nfinetuned on continuous features. We set new state-of-the-art multilingual VSR\nperformances by achieving comparable performances to the previous\nlanguage-specific VSR models, with a single trained model.\n", "link": "http://arxiv.org/abs/2401.09802v2", "date": "2024-07-18", "relevancy": 2.0541, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5214}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5124}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Training%20for%20Multilingual%20Visual%20Speech%20Recognition%3A%0A%20%20Pre-training%20with%20Discretized%20Visual%20Speech%20Representation&body=Title%3A%20Efficient%20Training%20for%20Multilingual%20Visual%20Speech%20Recognition%3A%0A%20%20Pre-training%20with%20Discretized%20Visual%20Speech%20Representation%0AAuthor%3A%20Minsu%20Kim%20and%20Jeong%20Hun%20Yeo%20and%20Se%20Jin%20Park%20and%20Hyeongseop%20Rha%20and%20Yong%20Man%20Ro%0AAbstract%3A%20%20%20This%20paper%20explores%20sentence-level%20multilingual%20Visual%20Speech%20Recognition%0A%28VSR%29%20that%20can%20recognize%20different%20languages%20with%20a%20single%20trained%20model.%20As%0Athe%20massive%20multilingual%20modeling%20of%20visual%20data%20requires%20huge%20computational%0Acosts%2C%20we%20propose%20a%20novel%20training%20strategy%2C%20processing%20with%20visual%20speech%0Aunits.%20Motivated%20by%20the%20recent%20success%20of%20the%20audio%20speech%20unit%2C%20we%20propose%20to%0Ause%20a%20visual%20speech%20unit%20that%20can%20be%20obtained%20by%20discretizing%20the%20visual%20speech%0Afeatures%20extracted%20from%20the%20self-supervised%20visual%20speech%20model.%20Through%0Aanalysis%2C%20we%20verify%20that%20the%20visual%20speech%20units%20mainly%20contain%20viseme%0Ainformation%20while%20suppressing%20non-linguistic%20information.%20By%20using%20the%20visual%0Aspeech%20units%20as%20the%20inputs%20of%20our%20system%2C%20we%20propose%20to%20pre-train%20a%20VSR%20model%0Ato%20predict%20corresponding%20text%20outputs%20on%20multilingual%20data%20constructed%20by%0Amerging%20several%20VSR%20databases.%20As%20both%20the%20inputs%20%28i.e.%2C%20visual%20speech%20units%29%0Aand%20outputs%20%28i.e.%2C%20text%29%20are%20discrete%2C%20we%20can%20greatly%20improve%20the%20training%0Aefficiency%20compared%20to%20the%20standard%20VSR%20training.%20Specifically%2C%20the%20input%20data%0Asize%20is%20reduced%20to%200.016%25%20of%20the%20original%20video%20inputs.%20In%20order%20to%20complement%0Athe%20insufficient%20visual%20information%20in%20speech%20recognition%2C%20we%20apply%20curriculum%0Alearning%20where%20the%20inputs%20of%20the%20system%20begin%20with%20audio-visual%20speech%20units%0Aand%20gradually%20change%20to%20visual%20speech%20units.%20After%20pre-training%2C%20the%20model%20is%0Afinetuned%20on%20continuous%20features.%20We%20set%20new%20state-of-the-art%20multilingual%20VSR%0Aperformances%20by%20achieving%20comparable%20performances%20to%20the%20previous%0Alanguage-specific%20VSR%20models%2C%20with%20a%20single%20trained%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Training%2520for%2520Multilingual%2520Visual%2520Speech%2520Recognition%253A%250A%2520%2520Pre-training%2520with%2520Discretized%2520Visual%2520Speech%2520Representation%26entry.906535625%3DMinsu%2520Kim%2520and%2520Jeong%2520Hun%2520Yeo%2520and%2520Se%2520Jin%2520Park%2520and%2520Hyeongseop%2520Rha%2520and%2520Yong%2520Man%2520Ro%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520sentence-level%2520multilingual%2520Visual%2520Speech%2520Recognition%250A%2528VSR%2529%2520that%2520can%2520recognize%2520different%2520languages%2520with%2520a%2520single%2520trained%2520model.%2520As%250Athe%2520massive%2520multilingual%2520modeling%2520of%2520visual%2520data%2520requires%2520huge%2520computational%250Acosts%252C%2520we%2520propose%2520a%2520novel%2520training%2520strategy%252C%2520processing%2520with%2520visual%2520speech%250Aunits.%2520Motivated%2520by%2520the%2520recent%2520success%2520of%2520the%2520audio%2520speech%2520unit%252C%2520we%2520propose%2520to%250Ause%2520a%2520visual%2520speech%2520unit%2520that%2520can%2520be%2520obtained%2520by%2520discretizing%2520the%2520visual%2520speech%250Afeatures%2520extracted%2520from%2520the%2520self-supervised%2520visual%2520speech%2520model.%2520Through%250Aanalysis%252C%2520we%2520verify%2520that%2520the%2520visual%2520speech%2520units%2520mainly%2520contain%2520viseme%250Ainformation%2520while%2520suppressing%2520non-linguistic%2520information.%2520By%2520using%2520the%2520visual%250Aspeech%2520units%2520as%2520the%2520inputs%2520of%2520our%2520system%252C%2520we%2520propose%2520to%2520pre-train%2520a%2520VSR%2520model%250Ato%2520predict%2520corresponding%2520text%2520outputs%2520on%2520multilingual%2520data%2520constructed%2520by%250Amerging%2520several%2520VSR%2520databases.%2520As%2520both%2520the%2520inputs%2520%2528i.e.%252C%2520visual%2520speech%2520units%2529%250Aand%2520outputs%2520%2528i.e.%252C%2520text%2529%2520are%2520discrete%252C%2520we%2520can%2520greatly%2520improve%2520the%2520training%250Aefficiency%2520compared%2520to%2520the%2520standard%2520VSR%2520training.%2520Specifically%252C%2520the%2520input%2520data%250Asize%2520is%2520reduced%2520to%25200.016%2525%2520of%2520the%2520original%2520video%2520inputs.%2520In%2520order%2520to%2520complement%250Athe%2520insufficient%2520visual%2520information%2520in%2520speech%2520recognition%252C%2520we%2520apply%2520curriculum%250Alearning%2520where%2520the%2520inputs%2520of%2520the%2520system%2520begin%2520with%2520audio-visual%2520speech%2520units%250Aand%2520gradually%2520change%2520to%2520visual%2520speech%2520units.%2520After%2520pre-training%252C%2520the%2520model%2520is%250Afinetuned%2520on%2520continuous%2520features.%2520We%2520set%2520new%2520state-of-the-art%2520multilingual%2520VSR%250Aperformances%2520by%2520achieving%2520comparable%2520performances%2520to%2520the%2520previous%250Alanguage-specific%2520VSR%2520models%252C%2520with%2520a%2520single%2520trained%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Training%20for%20Multilingual%20Visual%20Speech%20Recognition%3A%0A%20%20Pre-training%20with%20Discretized%20Visual%20Speech%20Representation&entry.906535625=Minsu%20Kim%20and%20Jeong%20Hun%20Yeo%20and%20Se%20Jin%20Park%20and%20Hyeongseop%20Rha%20and%20Yong%20Man%20Ro&entry.1292438233=%20%20This%20paper%20explores%20sentence-level%20multilingual%20Visual%20Speech%20Recognition%0A%28VSR%29%20that%20can%20recognize%20different%20languages%20with%20a%20single%20trained%20model.%20As%0Athe%20massive%20multilingual%20modeling%20of%20visual%20data%20requires%20huge%20computational%0Acosts%2C%20we%20propose%20a%20novel%20training%20strategy%2C%20processing%20with%20visual%20speech%0Aunits.%20Motivated%20by%20the%20recent%20success%20of%20the%20audio%20speech%20unit%2C%20we%20propose%20to%0Ause%20a%20visual%20speech%20unit%20that%20can%20be%20obtained%20by%20discretizing%20the%20visual%20speech%0Afeatures%20extracted%20from%20the%20self-supervised%20visual%20speech%20model.%20Through%0Aanalysis%2C%20we%20verify%20that%20the%20visual%20speech%20units%20mainly%20contain%20viseme%0Ainformation%20while%20suppressing%20non-linguistic%20information.%20By%20using%20the%20visual%0Aspeech%20units%20as%20the%20inputs%20of%20our%20system%2C%20we%20propose%20to%20pre-train%20a%20VSR%20model%0Ato%20predict%20corresponding%20text%20outputs%20on%20multilingual%20data%20constructed%20by%0Amerging%20several%20VSR%20databases.%20As%20both%20the%20inputs%20%28i.e.%2C%20visual%20speech%20units%29%0Aand%20outputs%20%28i.e.%2C%20text%29%20are%20discrete%2C%20we%20can%20greatly%20improve%20the%20training%0Aefficiency%20compared%20to%20the%20standard%20VSR%20training.%20Specifically%2C%20the%20input%20data%0Asize%20is%20reduced%20to%200.016%25%20of%20the%20original%20video%20inputs.%20In%20order%20to%20complement%0Athe%20insufficient%20visual%20information%20in%20speech%20recognition%2C%20we%20apply%20curriculum%0Alearning%20where%20the%20inputs%20of%20the%20system%20begin%20with%20audio-visual%20speech%20units%0Aand%20gradually%20change%20to%20visual%20speech%20units.%20After%20pre-training%2C%20the%20model%20is%0Afinetuned%20on%20continuous%20features.%20We%20set%20new%20state-of-the-art%20multilingual%20VSR%0Aperformances%20by%20achieving%20comparable%20performances%20to%20the%20previous%0Alanguage-specific%20VSR%20models%2C%20with%20a%20single%20trained%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09802v2&entry.124074799=Read"},
{"title": "Not Just Change the Labels, Learn the Features: Watermarking Deep Neural\n  Networks with Multi-View Data", "author": "Yuxuan Li and Sarthak Kumar Maharana and Yunhui Guo", "abstract": "  With the increasing prevalence of Machine Learning as a Service (MLaaS)\nplatforms, there is a growing focus on deep neural network (DNN) watermarking\ntechniques. These methods are used to facilitate the verification of ownership\nfor a target DNN model to protect intellectual property. One of the most widely\nemployed watermarking techniques involves embedding a trigger set into the\nsource model. Unfortunately, existing methodologies based on trigger sets are\nstill susceptible to functionality-stealing attacks, potentially enabling\nadversaries to steal the functionality of the source model without a reliable\nmeans of verifying ownership. In this paper, we first introduce a novel\nperspective on trigger set-based watermarking methods from a feature learning\nperspective. Specifically, we demonstrate that by selecting data exhibiting\nmultiple features, also referred to as \\emph{multi-view data}, it becomes\nfeasible to effectively defend functionality stealing attacks. Based on this\nperspective, we introduce a novel watermarking technique based on Multi-view\ndATa, called MAT, for efficiently embedding watermarks within DNNs. This\napproach involves constructing a trigger set with multi-view data and\nincorporating a simple feature-based regularization method for training the\nsource model. We validate our method across various benchmarks and demonstrate\nits efficacy in defending against model extraction attacks, surpassing relevant\nbaselines by a significant margin. The code is available at:\n\\href{https://github.com/liyuxuan-github/MAT}{https://github.com/liyuxuan-github/MAT}.\n", "link": "http://arxiv.org/abs/2403.10663v2", "date": "2024-07-18", "relevancy": 2.0201, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5125}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5034}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20Just%20Change%20the%20Labels%2C%20Learn%20the%20Features%3A%20Watermarking%20Deep%20Neural%0A%20%20Networks%20with%20Multi-View%20Data&body=Title%3A%20Not%20Just%20Change%20the%20Labels%2C%20Learn%20the%20Features%3A%20Watermarking%20Deep%20Neural%0A%20%20Networks%20with%20Multi-View%20Data%0AAuthor%3A%20Yuxuan%20Li%20and%20Sarthak%20Kumar%20Maharana%20and%20Yunhui%20Guo%0AAbstract%3A%20%20%20With%20the%20increasing%20prevalence%20of%20Machine%20Learning%20as%20a%20Service%20%28MLaaS%29%0Aplatforms%2C%20there%20is%20a%20growing%20focus%20on%20deep%20neural%20network%20%28DNN%29%20watermarking%0Atechniques.%20These%20methods%20are%20used%20to%20facilitate%20the%20verification%20of%20ownership%0Afor%20a%20target%20DNN%20model%20to%20protect%20intellectual%20property.%20One%20of%20the%20most%20widely%0Aemployed%20watermarking%20techniques%20involves%20embedding%20a%20trigger%20set%20into%20the%0Asource%20model.%20Unfortunately%2C%20existing%20methodologies%20based%20on%20trigger%20sets%20are%0Astill%20susceptible%20to%20functionality-stealing%20attacks%2C%20potentially%20enabling%0Aadversaries%20to%20steal%20the%20functionality%20of%20the%20source%20model%20without%20a%20reliable%0Ameans%20of%20verifying%20ownership.%20In%20this%20paper%2C%20we%20first%20introduce%20a%20novel%0Aperspective%20on%20trigger%20set-based%20watermarking%20methods%20from%20a%20feature%20learning%0Aperspective.%20Specifically%2C%20we%20demonstrate%20that%20by%20selecting%20data%20exhibiting%0Amultiple%20features%2C%20also%20referred%20to%20as%20%5Cemph%7Bmulti-view%20data%7D%2C%20it%20becomes%0Afeasible%20to%20effectively%20defend%20functionality%20stealing%20attacks.%20Based%20on%20this%0Aperspective%2C%20we%20introduce%20a%20novel%20watermarking%20technique%20based%20on%20Multi-view%0AdATa%2C%20called%20MAT%2C%20for%20efficiently%20embedding%20watermarks%20within%20DNNs.%20This%0Aapproach%20involves%20constructing%20a%20trigger%20set%20with%20multi-view%20data%20and%0Aincorporating%20a%20simple%20feature-based%20regularization%20method%20for%20training%20the%0Asource%20model.%20We%20validate%20our%20method%20across%20various%20benchmarks%20and%20demonstrate%0Aits%20efficacy%20in%20defending%20against%20model%20extraction%20attacks%2C%20surpassing%20relevant%0Abaselines%20by%20a%20significant%20margin.%20The%20code%20is%20available%20at%3A%0A%5Chref%7Bhttps%3A//github.com/liyuxuan-github/MAT%7D%7Bhttps%3A//github.com/liyuxuan-github/MAT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10663v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520Just%2520Change%2520the%2520Labels%252C%2520Learn%2520the%2520Features%253A%2520Watermarking%2520Deep%2520Neural%250A%2520%2520Networks%2520with%2520Multi-View%2520Data%26entry.906535625%3DYuxuan%2520Li%2520and%2520Sarthak%2520Kumar%2520Maharana%2520and%2520Yunhui%2520Guo%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520prevalence%2520of%2520Machine%2520Learning%2520as%2520a%2520Service%2520%2528MLaaS%2529%250Aplatforms%252C%2520there%2520is%2520a%2520growing%2520focus%2520on%2520deep%2520neural%2520network%2520%2528DNN%2529%2520watermarking%250Atechniques.%2520These%2520methods%2520are%2520used%2520to%2520facilitate%2520the%2520verification%2520of%2520ownership%250Afor%2520a%2520target%2520DNN%2520model%2520to%2520protect%2520intellectual%2520property.%2520One%2520of%2520the%2520most%2520widely%250Aemployed%2520watermarking%2520techniques%2520involves%2520embedding%2520a%2520trigger%2520set%2520into%2520the%250Asource%2520model.%2520Unfortunately%252C%2520existing%2520methodologies%2520based%2520on%2520trigger%2520sets%2520are%250Astill%2520susceptible%2520to%2520functionality-stealing%2520attacks%252C%2520potentially%2520enabling%250Aadversaries%2520to%2520steal%2520the%2520functionality%2520of%2520the%2520source%2520model%2520without%2520a%2520reliable%250Ameans%2520of%2520verifying%2520ownership.%2520In%2520this%2520paper%252C%2520we%2520first%2520introduce%2520a%2520novel%250Aperspective%2520on%2520trigger%2520set-based%2520watermarking%2520methods%2520from%2520a%2520feature%2520learning%250Aperspective.%2520Specifically%252C%2520we%2520demonstrate%2520that%2520by%2520selecting%2520data%2520exhibiting%250Amultiple%2520features%252C%2520also%2520referred%2520to%2520as%2520%255Cemph%257Bmulti-view%2520data%257D%252C%2520it%2520becomes%250Afeasible%2520to%2520effectively%2520defend%2520functionality%2520stealing%2520attacks.%2520Based%2520on%2520this%250Aperspective%252C%2520we%2520introduce%2520a%2520novel%2520watermarking%2520technique%2520based%2520on%2520Multi-view%250AdATa%252C%2520called%2520MAT%252C%2520for%2520efficiently%2520embedding%2520watermarks%2520within%2520DNNs.%2520This%250Aapproach%2520involves%2520constructing%2520a%2520trigger%2520set%2520with%2520multi-view%2520data%2520and%250Aincorporating%2520a%2520simple%2520feature-based%2520regularization%2520method%2520for%2520training%2520the%250Asource%2520model.%2520We%2520validate%2520our%2520method%2520across%2520various%2520benchmarks%2520and%2520demonstrate%250Aits%2520efficacy%2520in%2520defending%2520against%2520model%2520extraction%2520attacks%252C%2520surpassing%2520relevant%250Abaselines%2520by%2520a%2520significant%2520margin.%2520The%2520code%2520is%2520available%2520at%253A%250A%255Chref%257Bhttps%253A//github.com/liyuxuan-github/MAT%257D%257Bhttps%253A//github.com/liyuxuan-github/MAT%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10663v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20Just%20Change%20the%20Labels%2C%20Learn%20the%20Features%3A%20Watermarking%20Deep%20Neural%0A%20%20Networks%20with%20Multi-View%20Data&entry.906535625=Yuxuan%20Li%20and%20Sarthak%20Kumar%20Maharana%20and%20Yunhui%20Guo&entry.1292438233=%20%20With%20the%20increasing%20prevalence%20of%20Machine%20Learning%20as%20a%20Service%20%28MLaaS%29%0Aplatforms%2C%20there%20is%20a%20growing%20focus%20on%20deep%20neural%20network%20%28DNN%29%20watermarking%0Atechniques.%20These%20methods%20are%20used%20to%20facilitate%20the%20verification%20of%20ownership%0Afor%20a%20target%20DNN%20model%20to%20protect%20intellectual%20property.%20One%20of%20the%20most%20widely%0Aemployed%20watermarking%20techniques%20involves%20embedding%20a%20trigger%20set%20into%20the%0Asource%20model.%20Unfortunately%2C%20existing%20methodologies%20based%20on%20trigger%20sets%20are%0Astill%20susceptible%20to%20functionality-stealing%20attacks%2C%20potentially%20enabling%0Aadversaries%20to%20steal%20the%20functionality%20of%20the%20source%20model%20without%20a%20reliable%0Ameans%20of%20verifying%20ownership.%20In%20this%20paper%2C%20we%20first%20introduce%20a%20novel%0Aperspective%20on%20trigger%20set-based%20watermarking%20methods%20from%20a%20feature%20learning%0Aperspective.%20Specifically%2C%20we%20demonstrate%20that%20by%20selecting%20data%20exhibiting%0Amultiple%20features%2C%20also%20referred%20to%20as%20%5Cemph%7Bmulti-view%20data%7D%2C%20it%20becomes%0Afeasible%20to%20effectively%20defend%20functionality%20stealing%20attacks.%20Based%20on%20this%0Aperspective%2C%20we%20introduce%20a%20novel%20watermarking%20technique%20based%20on%20Multi-view%0AdATa%2C%20called%20MAT%2C%20for%20efficiently%20embedding%20watermarks%20within%20DNNs.%20This%0Aapproach%20involves%20constructing%20a%20trigger%20set%20with%20multi-view%20data%20and%0Aincorporating%20a%20simple%20feature-based%20regularization%20method%20for%20training%20the%0Asource%20model.%20We%20validate%20our%20method%20across%20various%20benchmarks%20and%20demonstrate%0Aits%20efficacy%20in%20defending%20against%20model%20extraction%20attacks%2C%20surpassing%20relevant%0Abaselines%20by%20a%20significant%20margin.%20The%20code%20is%20available%20at%3A%0A%5Chref%7Bhttps%3A//github.com/liyuxuan-github/MAT%7D%7Bhttps%3A//github.com/liyuxuan-github/MAT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10663v2&entry.124074799=Read"},
{"title": "Neural Network Tire Force Modeling for Automated Drifting", "author": "Nicholas Drake Broadbent and Trey Weber and Daiki Mori and J. Christian Gerdes", "abstract": "  Automated drifting presents a challenge problem for vehicle control,\nrequiring models and control algorithms that can precisely handle nonlinear,\ncoupled tire forces at the friction limits. We present a neural network\narchitecture for predicting front tire lateral force as a drop-in replacement\nfor physics-based approaches. With a full-scale automated vehicle purpose-built\nfor the drifting application, we deploy these models in a nonlinear model\npredictive controller tuned for tracking a reference drifting trajectory, for\ndirect comparisons of model performance. The neural network tire model exhibits\nsignificantly improved path tracking performance over the brush tire model in\ncases where front-axle braking force is applied, suggesting the neural\nnetwork's ability to express previously unmodeled, latent dynamics in the\ndrifting condition.\n", "link": "http://arxiv.org/abs/2407.13760v1", "date": "2024-07-18", "relevancy": 2.0195, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5383}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5274}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Network%20Tire%20Force%20Modeling%20for%20Automated%20Drifting&body=Title%3A%20Neural%20Network%20Tire%20Force%20Modeling%20for%20Automated%20Drifting%0AAuthor%3A%20Nicholas%20Drake%20Broadbent%20and%20Trey%20Weber%20and%20Daiki%20Mori%20and%20J.%20Christian%20Gerdes%0AAbstract%3A%20%20%20Automated%20drifting%20presents%20a%20challenge%20problem%20for%20vehicle%20control%2C%0Arequiring%20models%20and%20control%20algorithms%20that%20can%20precisely%20handle%20nonlinear%2C%0Acoupled%20tire%20forces%20at%20the%20friction%20limits.%20We%20present%20a%20neural%20network%0Aarchitecture%20for%20predicting%20front%20tire%20lateral%20force%20as%20a%20drop-in%20replacement%0Afor%20physics-based%20approaches.%20With%20a%20full-scale%20automated%20vehicle%20purpose-built%0Afor%20the%20drifting%20application%2C%20we%20deploy%20these%20models%20in%20a%20nonlinear%20model%0Apredictive%20controller%20tuned%20for%20tracking%20a%20reference%20drifting%20trajectory%2C%20for%0Adirect%20comparisons%20of%20model%20performance.%20The%20neural%20network%20tire%20model%20exhibits%0Asignificantly%20improved%20path%20tracking%20performance%20over%20the%20brush%20tire%20model%20in%0Acases%20where%20front-axle%20braking%20force%20is%20applied%2C%20suggesting%20the%20neural%0Anetwork%27s%20ability%20to%20express%20previously%20unmodeled%2C%20latent%20dynamics%20in%20the%0Adrifting%20condition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Network%2520Tire%2520Force%2520Modeling%2520for%2520Automated%2520Drifting%26entry.906535625%3DNicholas%2520Drake%2520Broadbent%2520and%2520Trey%2520Weber%2520and%2520Daiki%2520Mori%2520and%2520J.%2520Christian%2520Gerdes%26entry.1292438233%3D%2520%2520Automated%2520drifting%2520presents%2520a%2520challenge%2520problem%2520for%2520vehicle%2520control%252C%250Arequiring%2520models%2520and%2520control%2520algorithms%2520that%2520can%2520precisely%2520handle%2520nonlinear%252C%250Acoupled%2520tire%2520forces%2520at%2520the%2520friction%2520limits.%2520We%2520present%2520a%2520neural%2520network%250Aarchitecture%2520for%2520predicting%2520front%2520tire%2520lateral%2520force%2520as%2520a%2520drop-in%2520replacement%250Afor%2520physics-based%2520approaches.%2520With%2520a%2520full-scale%2520automated%2520vehicle%2520purpose-built%250Afor%2520the%2520drifting%2520application%252C%2520we%2520deploy%2520these%2520models%2520in%2520a%2520nonlinear%2520model%250Apredictive%2520controller%2520tuned%2520for%2520tracking%2520a%2520reference%2520drifting%2520trajectory%252C%2520for%250Adirect%2520comparisons%2520of%2520model%2520performance.%2520The%2520neural%2520network%2520tire%2520model%2520exhibits%250Asignificantly%2520improved%2520path%2520tracking%2520performance%2520over%2520the%2520brush%2520tire%2520model%2520in%250Acases%2520where%2520front-axle%2520braking%2520force%2520is%2520applied%252C%2520suggesting%2520the%2520neural%250Anetwork%2527s%2520ability%2520to%2520express%2520previously%2520unmodeled%252C%2520latent%2520dynamics%2520in%2520the%250Adrifting%2520condition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Network%20Tire%20Force%20Modeling%20for%20Automated%20Drifting&entry.906535625=Nicholas%20Drake%20Broadbent%20and%20Trey%20Weber%20and%20Daiki%20Mori%20and%20J.%20Christian%20Gerdes&entry.1292438233=%20%20Automated%20drifting%20presents%20a%20challenge%20problem%20for%20vehicle%20control%2C%0Arequiring%20models%20and%20control%20algorithms%20that%20can%20precisely%20handle%20nonlinear%2C%0Acoupled%20tire%20forces%20at%20the%20friction%20limits.%20We%20present%20a%20neural%20network%0Aarchitecture%20for%20predicting%20front%20tire%20lateral%20force%20as%20a%20drop-in%20replacement%0Afor%20physics-based%20approaches.%20With%20a%20full-scale%20automated%20vehicle%20purpose-built%0Afor%20the%20drifting%20application%2C%20we%20deploy%20these%20models%20in%20a%20nonlinear%20model%0Apredictive%20controller%20tuned%20for%20tracking%20a%20reference%20drifting%20trajectory%2C%20for%0Adirect%20comparisons%20of%20model%20performance.%20The%20neural%20network%20tire%20model%20exhibits%0Asignificantly%20improved%20path%20tracking%20performance%20over%20the%20brush%20tire%20model%20in%0Acases%20where%20front-axle%20braking%20force%20is%20applied%2C%20suggesting%20the%20neural%0Anetwork%27s%20ability%20to%20express%20previously%20unmodeled%2C%20latent%20dynamics%20in%20the%0Adrifting%20condition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13760v1&entry.124074799=Read"},
{"title": "Addressing Imbalance for Class Incremental Learning in Medical Image\n  Classification", "author": "Xuze Hao and Wenqian Ni and Xuhao Jiang and Weimin Tan and Bo Yan", "abstract": "  Deep convolutional neural networks have made significant breakthroughs in\nmedical image classification, under the assumption that training samples from\nall classes are simultaneously available. However, in real-world medical\nscenarios, there's a common need to continuously learn about new diseases,\nleading to the emerging field of class incremental learning (CIL) in the\nmedical domain. Typically, CIL suffers from catastrophic forgetting when\ntrained on new classes. This phenomenon is mainly caused by the imbalance\nbetween old and new classes, and it becomes even more challenging with\nimbalanced medical datasets. In this work, we introduce two simple yet\neffective plug-in methods to mitigate the adverse effects of the imbalance.\nFirst, we propose a CIL-balanced classification loss to mitigate the classifier\nbias toward majority classes via logit adjustment. Second, we propose a\ndistribution margin loss that not only alleviates the inter-class overlap in\nembedding space but also enforces the intra-class compactness. We evaluate the\neffectiveness of our method with extensive experiments on three benchmark\ndatasets (CCH5000, HAM10000, and EyePACS). The results demonstrate that our\napproach outperforms state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.13768v1", "date": "2024-07-18", "relevancy": 2.0098, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5057}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5023}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Imbalance%20for%20Class%20Incremental%20Learning%20in%20Medical%20Image%0A%20%20Classification&body=Title%3A%20Addressing%20Imbalance%20for%20Class%20Incremental%20Learning%20in%20Medical%20Image%0A%20%20Classification%0AAuthor%3A%20Xuze%20Hao%20and%20Wenqian%20Ni%20and%20Xuhao%20Jiang%20and%20Weimin%20Tan%20and%20Bo%20Yan%0AAbstract%3A%20%20%20Deep%20convolutional%20neural%20networks%20have%20made%20significant%20breakthroughs%20in%0Amedical%20image%20classification%2C%20under%20the%20assumption%20that%20training%20samples%20from%0Aall%20classes%20are%20simultaneously%20available.%20However%2C%20in%20real-world%20medical%0Ascenarios%2C%20there%27s%20a%20common%20need%20to%20continuously%20learn%20about%20new%20diseases%2C%0Aleading%20to%20the%20emerging%20field%20of%20class%20incremental%20learning%20%28CIL%29%20in%20the%0Amedical%20domain.%20Typically%2C%20CIL%20suffers%20from%20catastrophic%20forgetting%20when%0Atrained%20on%20new%20classes.%20This%20phenomenon%20is%20mainly%20caused%20by%20the%20imbalance%0Abetween%20old%20and%20new%20classes%2C%20and%20it%20becomes%20even%20more%20challenging%20with%0Aimbalanced%20medical%20datasets.%20In%20this%20work%2C%20we%20introduce%20two%20simple%20yet%0Aeffective%20plug-in%20methods%20to%20mitigate%20the%20adverse%20effects%20of%20the%20imbalance.%0AFirst%2C%20we%20propose%20a%20CIL-balanced%20classification%20loss%20to%20mitigate%20the%20classifier%0Abias%20toward%20majority%20classes%20via%20logit%20adjustment.%20Second%2C%20we%20propose%20a%0Adistribution%20margin%20loss%20that%20not%20only%20alleviates%20the%20inter-class%20overlap%20in%0Aembedding%20space%20but%20also%20enforces%20the%20intra-class%20compactness.%20We%20evaluate%20the%0Aeffectiveness%20of%20our%20method%20with%20extensive%20experiments%20on%20three%20benchmark%0Adatasets%20%28CCH5000%2C%20HAM10000%2C%20and%20EyePACS%29.%20The%20results%20demonstrate%20that%20our%0Aapproach%20outperforms%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Imbalance%2520for%2520Class%2520Incremental%2520Learning%2520in%2520Medical%2520Image%250A%2520%2520Classification%26entry.906535625%3DXuze%2520Hao%2520and%2520Wenqian%2520Ni%2520and%2520Xuhao%2520Jiang%2520and%2520Weimin%2520Tan%2520and%2520Bo%2520Yan%26entry.1292438233%3D%2520%2520Deep%2520convolutional%2520neural%2520networks%2520have%2520made%2520significant%2520breakthroughs%2520in%250Amedical%2520image%2520classification%252C%2520under%2520the%2520assumption%2520that%2520training%2520samples%2520from%250Aall%2520classes%2520are%2520simultaneously%2520available.%2520However%252C%2520in%2520real-world%2520medical%250Ascenarios%252C%2520there%2527s%2520a%2520common%2520need%2520to%2520continuously%2520learn%2520about%2520new%2520diseases%252C%250Aleading%2520to%2520the%2520emerging%2520field%2520of%2520class%2520incremental%2520learning%2520%2528CIL%2529%2520in%2520the%250Amedical%2520domain.%2520Typically%252C%2520CIL%2520suffers%2520from%2520catastrophic%2520forgetting%2520when%250Atrained%2520on%2520new%2520classes.%2520This%2520phenomenon%2520is%2520mainly%2520caused%2520by%2520the%2520imbalance%250Abetween%2520old%2520and%2520new%2520classes%252C%2520and%2520it%2520becomes%2520even%2520more%2520challenging%2520with%250Aimbalanced%2520medical%2520datasets.%2520In%2520this%2520work%252C%2520we%2520introduce%2520two%2520simple%2520yet%250Aeffective%2520plug-in%2520methods%2520to%2520mitigate%2520the%2520adverse%2520effects%2520of%2520the%2520imbalance.%250AFirst%252C%2520we%2520propose%2520a%2520CIL-balanced%2520classification%2520loss%2520to%2520mitigate%2520the%2520classifier%250Abias%2520toward%2520majority%2520classes%2520via%2520logit%2520adjustment.%2520Second%252C%2520we%2520propose%2520a%250Adistribution%2520margin%2520loss%2520that%2520not%2520only%2520alleviates%2520the%2520inter-class%2520overlap%2520in%250Aembedding%2520space%2520but%2520also%2520enforces%2520the%2520intra-class%2520compactness.%2520We%2520evaluate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520with%2520extensive%2520experiments%2520on%2520three%2520benchmark%250Adatasets%2520%2528CCH5000%252C%2520HAM10000%252C%2520and%2520EyePACS%2529.%2520The%2520results%2520demonstrate%2520that%2520our%250Aapproach%2520outperforms%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Imbalance%20for%20Class%20Incremental%20Learning%20in%20Medical%20Image%0A%20%20Classification&entry.906535625=Xuze%20Hao%20and%20Wenqian%20Ni%20and%20Xuhao%20Jiang%20and%20Weimin%20Tan%20and%20Bo%20Yan&entry.1292438233=%20%20Deep%20convolutional%20neural%20networks%20have%20made%20significant%20breakthroughs%20in%0Amedical%20image%20classification%2C%20under%20the%20assumption%20that%20training%20samples%20from%0Aall%20classes%20are%20simultaneously%20available.%20However%2C%20in%20real-world%20medical%0Ascenarios%2C%20there%27s%20a%20common%20need%20to%20continuously%20learn%20about%20new%20diseases%2C%0Aleading%20to%20the%20emerging%20field%20of%20class%20incremental%20learning%20%28CIL%29%20in%20the%0Amedical%20domain.%20Typically%2C%20CIL%20suffers%20from%20catastrophic%20forgetting%20when%0Atrained%20on%20new%20classes.%20This%20phenomenon%20is%20mainly%20caused%20by%20the%20imbalance%0Abetween%20old%20and%20new%20classes%2C%20and%20it%20becomes%20even%20more%20challenging%20with%0Aimbalanced%20medical%20datasets.%20In%20this%20work%2C%20we%20introduce%20two%20simple%20yet%0Aeffective%20plug-in%20methods%20to%20mitigate%20the%20adverse%20effects%20of%20the%20imbalance.%0AFirst%2C%20we%20propose%20a%20CIL-balanced%20classification%20loss%20to%20mitigate%20the%20classifier%0Abias%20toward%20majority%20classes%20via%20logit%20adjustment.%20Second%2C%20we%20propose%20a%0Adistribution%20margin%20loss%20that%20not%20only%20alleviates%20the%20inter-class%20overlap%20in%0Aembedding%20space%20but%20also%20enforces%20the%20intra-class%20compactness.%20We%20evaluate%20the%0Aeffectiveness%20of%20our%20method%20with%20extensive%20experiments%20on%20three%20benchmark%0Adatasets%20%28CCH5000%2C%20HAM10000%2C%20and%20EyePACS%29.%20The%20results%20demonstrate%20that%20our%0Aapproach%20outperforms%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13768v1&entry.124074799=Read"},
{"title": "CogniVoice: Multimodal and Multilingual Fusion Networks for Mild\n  Cognitive Impairment Assessment from Spontaneous Speech", "author": "Jiali Cheng and Mohamed Elgaar and Nidhi Vakil and Hadi Amiri", "abstract": "  Mild Cognitive Impairment (MCI) is a medical condition characterized by\nnoticeable declines in memory and cognitive abilities, potentially affecting\nindividual's daily activities. In this paper, we introduce CogniVoice, a novel\nmultilingual and multimodal framework to detect MCI and estimate Mini-Mental\nState Examination (MMSE) scores by analyzing speech data and its textual\ntranscriptions. The key component of CogniVoice is an ensemble multimodal and\nmultilingual network based on ``Product of Experts'' that mitigates reliance on\nshortcut solutions. Using a comprehensive dataset containing both English and\nChinese languages from TAUKADIAL challenge, CogniVoice outperforms the best\nperforming baseline model on MCI classification and MMSE regression tasks by\n2.8 and 4.1 points in F1 and RMSE respectively, and can effectively reduce the\nperformance gap across different language groups by 0.7 points in F1.\n", "link": "http://arxiv.org/abs/2407.13660v1", "date": "2024-07-18", "relevancy": 2.0053, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5082}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4985}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogniVoice%3A%20Multimodal%20and%20Multilingual%20Fusion%20Networks%20for%20Mild%0A%20%20Cognitive%20Impairment%20Assessment%20from%20Spontaneous%20Speech&body=Title%3A%20CogniVoice%3A%20Multimodal%20and%20Multilingual%20Fusion%20Networks%20for%20Mild%0A%20%20Cognitive%20Impairment%20Assessment%20from%20Spontaneous%20Speech%0AAuthor%3A%20Jiali%20Cheng%20and%20Mohamed%20Elgaar%20and%20Nidhi%20Vakil%20and%20Hadi%20Amiri%0AAbstract%3A%20%20%20Mild%20Cognitive%20Impairment%20%28MCI%29%20is%20a%20medical%20condition%20characterized%20by%0Anoticeable%20declines%20in%20memory%20and%20cognitive%20abilities%2C%20potentially%20affecting%0Aindividual%27s%20daily%20activities.%20In%20this%20paper%2C%20we%20introduce%20CogniVoice%2C%20a%20novel%0Amultilingual%20and%20multimodal%20framework%20to%20detect%20MCI%20and%20estimate%20Mini-Mental%0AState%20Examination%20%28MMSE%29%20scores%20by%20analyzing%20speech%20data%20and%20its%20textual%0Atranscriptions.%20The%20key%20component%20of%20CogniVoice%20is%20an%20ensemble%20multimodal%20and%0Amultilingual%20network%20based%20on%20%60%60Product%20of%20Experts%27%27%20that%20mitigates%20reliance%20on%0Ashortcut%20solutions.%20Using%20a%20comprehensive%20dataset%20containing%20both%20English%20and%0AChinese%20languages%20from%20TAUKADIAL%20challenge%2C%20CogniVoice%20outperforms%20the%20best%0Aperforming%20baseline%20model%20on%20MCI%20classification%20and%20MMSE%20regression%20tasks%20by%0A2.8%20and%204.1%20points%20in%20F1%20and%20RMSE%20respectively%2C%20and%20can%20effectively%20reduce%20the%0Aperformance%20gap%20across%20different%20language%20groups%20by%200.7%20points%20in%20F1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogniVoice%253A%2520Multimodal%2520and%2520Multilingual%2520Fusion%2520Networks%2520for%2520Mild%250A%2520%2520Cognitive%2520Impairment%2520Assessment%2520from%2520Spontaneous%2520Speech%26entry.906535625%3DJiali%2520Cheng%2520and%2520Mohamed%2520Elgaar%2520and%2520Nidhi%2520Vakil%2520and%2520Hadi%2520Amiri%26entry.1292438233%3D%2520%2520Mild%2520Cognitive%2520Impairment%2520%2528MCI%2529%2520is%2520a%2520medical%2520condition%2520characterized%2520by%250Anoticeable%2520declines%2520in%2520memory%2520and%2520cognitive%2520abilities%252C%2520potentially%2520affecting%250Aindividual%2527s%2520daily%2520activities.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CogniVoice%252C%2520a%2520novel%250Amultilingual%2520and%2520multimodal%2520framework%2520to%2520detect%2520MCI%2520and%2520estimate%2520Mini-Mental%250AState%2520Examination%2520%2528MMSE%2529%2520scores%2520by%2520analyzing%2520speech%2520data%2520and%2520its%2520textual%250Atranscriptions.%2520The%2520key%2520component%2520of%2520CogniVoice%2520is%2520an%2520ensemble%2520multimodal%2520and%250Amultilingual%2520network%2520based%2520on%2520%2560%2560Product%2520of%2520Experts%2527%2527%2520that%2520mitigates%2520reliance%2520on%250Ashortcut%2520solutions.%2520Using%2520a%2520comprehensive%2520dataset%2520containing%2520both%2520English%2520and%250AChinese%2520languages%2520from%2520TAUKADIAL%2520challenge%252C%2520CogniVoice%2520outperforms%2520the%2520best%250Aperforming%2520baseline%2520model%2520on%2520MCI%2520classification%2520and%2520MMSE%2520regression%2520tasks%2520by%250A2.8%2520and%25204.1%2520points%2520in%2520F1%2520and%2520RMSE%2520respectively%252C%2520and%2520can%2520effectively%2520reduce%2520the%250Aperformance%2520gap%2520across%2520different%2520language%2520groups%2520by%25200.7%2520points%2520in%2520F1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogniVoice%3A%20Multimodal%20and%20Multilingual%20Fusion%20Networks%20for%20Mild%0A%20%20Cognitive%20Impairment%20Assessment%20from%20Spontaneous%20Speech&entry.906535625=Jiali%20Cheng%20and%20Mohamed%20Elgaar%20and%20Nidhi%20Vakil%20and%20Hadi%20Amiri&entry.1292438233=%20%20Mild%20Cognitive%20Impairment%20%28MCI%29%20is%20a%20medical%20condition%20characterized%20by%0Anoticeable%20declines%20in%20memory%20and%20cognitive%20abilities%2C%20potentially%20affecting%0Aindividual%27s%20daily%20activities.%20In%20this%20paper%2C%20we%20introduce%20CogniVoice%2C%20a%20novel%0Amultilingual%20and%20multimodal%20framework%20to%20detect%20MCI%20and%20estimate%20Mini-Mental%0AState%20Examination%20%28MMSE%29%20scores%20by%20analyzing%20speech%20data%20and%20its%20textual%0Atranscriptions.%20The%20key%20component%20of%20CogniVoice%20is%20an%20ensemble%20multimodal%20and%0Amultilingual%20network%20based%20on%20%60%60Product%20of%20Experts%27%27%20that%20mitigates%20reliance%20on%0Ashortcut%20solutions.%20Using%20a%20comprehensive%20dataset%20containing%20both%20English%20and%0AChinese%20languages%20from%20TAUKADIAL%20challenge%2C%20CogniVoice%20outperforms%20the%20best%0Aperforming%20baseline%20model%20on%20MCI%20classification%20and%20MMSE%20regression%20tasks%20by%0A2.8%20and%204.1%20points%20in%20F1%20and%20RMSE%20respectively%2C%20and%20can%20effectively%20reduce%20the%0Aperformance%20gap%20across%20different%20language%20groups%20by%200.7%20points%20in%20F1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13660v1&entry.124074799=Read"},
{"title": "SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision\n  Language Models", "author": "Manav Nitin Kapadnis and Sohan Patnaik and Abhilash Nandy and Sourjyadip Ray and Pawan Goyal and Debdoot Sheet", "abstract": "  Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large\nLanguage Models (MLLMs) can automate the creation of accurate and coherent\nradiological reports. Existing methods often hallucinate details in text-based\nreports that don't accurately reflect the image content. To mitigate this, we\nintroduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort\nGENeraTion using Vision Language Models), which improves the R2Gen task by\nintegrating a self-refining mechanism into the MLLM framework. We employ a\nunique self-supervised loss that leverages similarity between pooled image\nrepresentations and the contextual representations of the generated\nradiological text, alongside the standard Causal Language Modeling objective,\nto refine image-text representations. This allows the model to scrutinize and\nalign the generated text through dynamic interaction between a given image and\nthe generated text, therefore reducing hallucination and continuously enhancing\nnuanced report generation. SERPENT-VLM outperforms existing baselines such as\nLLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and\nRadiology Objects in COntext (ROCO) datasets, and also proves to be robust\nagainst noisy images. A qualitative case study emphasizes the significant\nadvancements towards more sophisticated MLLM frameworks for R2Gen, opening\npaths for further research into self-supervised refinement in the medical\nimaging domain.\n", "link": "http://arxiv.org/abs/2404.17912v2", "date": "2024-07-18", "relevancy": 2.0043, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5125}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5047}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SERPENT-VLM%20%3A%20Self-Refining%20Radiology%20Report%20Generation%20Using%20Vision%0A%20%20Language%20Models&body=Title%3A%20SERPENT-VLM%20%3A%20Self-Refining%20Radiology%20Report%20Generation%20Using%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20Manav%20Nitin%20Kapadnis%20and%20Sohan%20Patnaik%20and%20Abhilash%20Nandy%20and%20Sourjyadip%20Ray%20and%20Pawan%20Goyal%20and%20Debdoot%20Sheet%0AAbstract%3A%20%20%20Radiology%20Report%20Generation%20%28R2Gen%29%20demonstrates%20how%20Multi-modal%20Large%0ALanguage%20Models%20%28MLLMs%29%20can%20automate%20the%20creation%20of%20accurate%20and%20coherent%0Aradiological%20reports.%20Existing%20methods%20often%20hallucinate%20details%20in%20text-based%0Areports%20that%20don%27t%20accurately%20reflect%20the%20image%20content.%20To%20mitigate%20this%2C%20we%0Aintroduce%20a%20novel%20strategy%2C%20SERPENT-VLM%20%28SElf%20Refining%20Radiology%20RePort%0AGENeraTion%20using%20Vision%20Language%20Models%29%2C%20which%20improves%20the%20R2Gen%20task%20by%0Aintegrating%20a%20self-refining%20mechanism%20into%20the%20MLLM%20framework.%20We%20employ%20a%0Aunique%20self-supervised%20loss%20that%20leverages%20similarity%20between%20pooled%20image%0Arepresentations%20and%20the%20contextual%20representations%20of%20the%20generated%0Aradiological%20text%2C%20alongside%20the%20standard%20Causal%20Language%20Modeling%20objective%2C%0Ato%20refine%20image-text%20representations.%20This%20allows%20the%20model%20to%20scrutinize%20and%0Aalign%20the%20generated%20text%20through%20dynamic%20interaction%20between%20a%20given%20image%20and%0Athe%20generated%20text%2C%20therefore%20reducing%20hallucination%20and%20continuously%20enhancing%0Anuanced%20report%20generation.%20SERPENT-VLM%20outperforms%20existing%20baselines%20such%20as%0ALLaVA-Med%2C%20BiomedGPT%2C%20etc.%2C%20achieving%20SoTA%20performance%20on%20the%20IU%20X-ray%20and%0ARadiology%20Objects%20in%20COntext%20%28ROCO%29%20datasets%2C%20and%20also%20proves%20to%20be%20robust%0Aagainst%20noisy%20images.%20A%20qualitative%20case%20study%20emphasizes%20the%20significant%0Aadvancements%20towards%20more%20sophisticated%20MLLM%20frameworks%20for%20R2Gen%2C%20opening%0Apaths%20for%20further%20research%20into%20self-supervised%20refinement%20in%20the%20medical%0Aimaging%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17912v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSERPENT-VLM%2520%253A%2520Self-Refining%2520Radiology%2520Report%2520Generation%2520Using%2520Vision%250A%2520%2520Language%2520Models%26entry.906535625%3DManav%2520Nitin%2520Kapadnis%2520and%2520Sohan%2520Patnaik%2520and%2520Abhilash%2520Nandy%2520and%2520Sourjyadip%2520Ray%2520and%2520Pawan%2520Goyal%2520and%2520Debdoot%2520Sheet%26entry.1292438233%3D%2520%2520Radiology%2520Report%2520Generation%2520%2528R2Gen%2529%2520demonstrates%2520how%2520Multi-modal%2520Large%250ALanguage%2520Models%2520%2528MLLMs%2529%2520can%2520automate%2520the%2520creation%2520of%2520accurate%2520and%2520coherent%250Aradiological%2520reports.%2520Existing%2520methods%2520often%2520hallucinate%2520details%2520in%2520text-based%250Areports%2520that%2520don%2527t%2520accurately%2520reflect%2520the%2520image%2520content.%2520To%2520mitigate%2520this%252C%2520we%250Aintroduce%2520a%2520novel%2520strategy%252C%2520SERPENT-VLM%2520%2528SElf%2520Refining%2520Radiology%2520RePort%250AGENeraTion%2520using%2520Vision%2520Language%2520Models%2529%252C%2520which%2520improves%2520the%2520R2Gen%2520task%2520by%250Aintegrating%2520a%2520self-refining%2520mechanism%2520into%2520the%2520MLLM%2520framework.%2520We%2520employ%2520a%250Aunique%2520self-supervised%2520loss%2520that%2520leverages%2520similarity%2520between%2520pooled%2520image%250Arepresentations%2520and%2520the%2520contextual%2520representations%2520of%2520the%2520generated%250Aradiological%2520text%252C%2520alongside%2520the%2520standard%2520Causal%2520Language%2520Modeling%2520objective%252C%250Ato%2520refine%2520image-text%2520representations.%2520This%2520allows%2520the%2520model%2520to%2520scrutinize%2520and%250Aalign%2520the%2520generated%2520text%2520through%2520dynamic%2520interaction%2520between%2520a%2520given%2520image%2520and%250Athe%2520generated%2520text%252C%2520therefore%2520reducing%2520hallucination%2520and%2520continuously%2520enhancing%250Anuanced%2520report%2520generation.%2520SERPENT-VLM%2520outperforms%2520existing%2520baselines%2520such%2520as%250ALLaVA-Med%252C%2520BiomedGPT%252C%2520etc.%252C%2520achieving%2520SoTA%2520performance%2520on%2520the%2520IU%2520X-ray%2520and%250ARadiology%2520Objects%2520in%2520COntext%2520%2528ROCO%2529%2520datasets%252C%2520and%2520also%2520proves%2520to%2520be%2520robust%250Aagainst%2520noisy%2520images.%2520A%2520qualitative%2520case%2520study%2520emphasizes%2520the%2520significant%250Aadvancements%2520towards%2520more%2520sophisticated%2520MLLM%2520frameworks%2520for%2520R2Gen%252C%2520opening%250Apaths%2520for%2520further%2520research%2520into%2520self-supervised%2520refinement%2520in%2520the%2520medical%250Aimaging%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17912v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SERPENT-VLM%20%3A%20Self-Refining%20Radiology%20Report%20Generation%20Using%20Vision%0A%20%20Language%20Models&entry.906535625=Manav%20Nitin%20Kapadnis%20and%20Sohan%20Patnaik%20and%20Abhilash%20Nandy%20and%20Sourjyadip%20Ray%20and%20Pawan%20Goyal%20and%20Debdoot%20Sheet&entry.1292438233=%20%20Radiology%20Report%20Generation%20%28R2Gen%29%20demonstrates%20how%20Multi-modal%20Large%0ALanguage%20Models%20%28MLLMs%29%20can%20automate%20the%20creation%20of%20accurate%20and%20coherent%0Aradiological%20reports.%20Existing%20methods%20often%20hallucinate%20details%20in%20text-based%0Areports%20that%20don%27t%20accurately%20reflect%20the%20image%20content.%20To%20mitigate%20this%2C%20we%0Aintroduce%20a%20novel%20strategy%2C%20SERPENT-VLM%20%28SElf%20Refining%20Radiology%20RePort%0AGENeraTion%20using%20Vision%20Language%20Models%29%2C%20which%20improves%20the%20R2Gen%20task%20by%0Aintegrating%20a%20self-refining%20mechanism%20into%20the%20MLLM%20framework.%20We%20employ%20a%0Aunique%20self-supervised%20loss%20that%20leverages%20similarity%20between%20pooled%20image%0Arepresentations%20and%20the%20contextual%20representations%20of%20the%20generated%0Aradiological%20text%2C%20alongside%20the%20standard%20Causal%20Language%20Modeling%20objective%2C%0Ato%20refine%20image-text%20representations.%20This%20allows%20the%20model%20to%20scrutinize%20and%0Aalign%20the%20generated%20text%20through%20dynamic%20interaction%20between%20a%20given%20image%20and%0Athe%20generated%20text%2C%20therefore%20reducing%20hallucination%20and%20continuously%20enhancing%0Anuanced%20report%20generation.%20SERPENT-VLM%20outperforms%20existing%20baselines%20such%20as%0ALLaVA-Med%2C%20BiomedGPT%2C%20etc.%2C%20achieving%20SoTA%20performance%20on%20the%20IU%20X-ray%20and%0ARadiology%20Objects%20in%20COntext%20%28ROCO%29%20datasets%2C%20and%20also%20proves%20to%20be%20robust%0Aagainst%20noisy%20images.%20A%20qualitative%20case%20study%20emphasizes%20the%20significant%0Aadvancements%20towards%20more%20sophisticated%20MLLM%20frameworks%20for%20R2Gen%2C%20opening%0Apaths%20for%20further%20research%20into%20self-supervised%20refinement%20in%20the%20medical%0Aimaging%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17912v2&entry.124074799=Read"},
{"title": "Weak-to-Strong Reasoning", "author": "Yuqing Yang and Yan Ma and Pengfei Liu", "abstract": "  When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervisions for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.\n", "link": "http://arxiv.org/abs/2407.13647v1", "date": "2024-07-18", "relevancy": 2.0022, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5209}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4867}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weak-to-Strong%20Reasoning&body=Title%3A%20Weak-to-Strong%20Reasoning%0AAuthor%3A%20Yuqing%20Yang%20and%20Yan%20Ma%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20When%20large%20language%20models%20%28LLMs%29%20exceed%20human-level%20capabilities%2C%20it%20becomes%0Aincreasingly%20challenging%20to%20provide%20full-scale%20and%20accurate%20supervisions%20for%0Athese%20models.%20Weak-to-strong%20learning%2C%20which%20leverages%20a%20less%20capable%20model%20to%0Aunlock%20the%20latent%20abilities%20of%20a%20stronger%20model%2C%20proves%20valuable%20in%20this%0Acontext.%20Yet%2C%20the%20efficacy%20of%20this%20approach%20for%20complex%20reasoning%20tasks%20is%0Astill%20untested.%20Furthermore%2C%20tackling%20reasoning%20tasks%20under%20the%20weak-to-strong%0Asetting%20currently%20lacks%20efficient%20methods%20to%20avoid%20blindly%20imitating%20the%20weak%0Asupervisor%20including%20its%20errors.%20In%20this%20paper%2C%20we%20introduce%20a%20progressive%0Alearning%20framework%20that%20enables%20the%20strong%20model%20to%20autonomously%20refine%20its%0Atraining%20data%2C%20without%20requiring%20input%20from%20either%20a%20more%20advanced%20model%20or%0Ahuman-annotated%20data.%20This%20framework%20begins%20with%20supervised%20fine-tuning%20on%20a%0Aselective%20small%20but%20high-quality%20dataset%2C%20followed%20by%20preference%20optimization%0Aon%20contrastive%20samples%20identified%20by%20the%20strong%20model%20itself.%20Extensive%0Aexperiments%20on%20the%20GSM8K%20and%20MATH%20datasets%20demonstrate%20that%20our%20method%0Asignificantly%20enhances%20the%20reasoning%20capabilities%20of%20Llama2-70b%20using%20three%0Aseparate%20weak%20models.%20This%20method%20is%20further%20validated%20in%20a%20forward-looking%0Aexperimental%20setup%2C%20where%20Llama3-8b-instruct%20effectively%20supervises%20Llama3-70b%0Aon%20the%20highly%20challenging%20OlympicArena%20dataset.%20This%20work%20paves%20the%20way%20for%20a%0Amore%20scalable%20and%20sophisticated%20strategy%20to%20enhance%20AI%20reasoning%20powers.%20All%0Arelevant%20code%20and%20resources%20are%20available%20in%0A%5Curl%7Bhttps%3A//github.com/GAIR-NLP/weak-to-strong-reasoning%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeak-to-Strong%2520Reasoning%26entry.906535625%3DYuqing%2520Yang%2520and%2520Yan%2520Ma%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520When%2520large%2520language%2520models%2520%2528LLMs%2529%2520exceed%2520human-level%2520capabilities%252C%2520it%2520becomes%250Aincreasingly%2520challenging%2520to%2520provide%2520full-scale%2520and%2520accurate%2520supervisions%2520for%250Athese%2520models.%2520Weak-to-strong%2520learning%252C%2520which%2520leverages%2520a%2520less%2520capable%2520model%2520to%250Aunlock%2520the%2520latent%2520abilities%2520of%2520a%2520stronger%2520model%252C%2520proves%2520valuable%2520in%2520this%250Acontext.%2520Yet%252C%2520the%2520efficacy%2520of%2520this%2520approach%2520for%2520complex%2520reasoning%2520tasks%2520is%250Astill%2520untested.%2520Furthermore%252C%2520tackling%2520reasoning%2520tasks%2520under%2520the%2520weak-to-strong%250Asetting%2520currently%2520lacks%2520efficient%2520methods%2520to%2520avoid%2520blindly%2520imitating%2520the%2520weak%250Asupervisor%2520including%2520its%2520errors.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520progressive%250Alearning%2520framework%2520that%2520enables%2520the%2520strong%2520model%2520to%2520autonomously%2520refine%2520its%250Atraining%2520data%252C%2520without%2520requiring%2520input%2520from%2520either%2520a%2520more%2520advanced%2520model%2520or%250Ahuman-annotated%2520data.%2520This%2520framework%2520begins%2520with%2520supervised%2520fine-tuning%2520on%2520a%250Aselective%2520small%2520but%2520high-quality%2520dataset%252C%2520followed%2520by%2520preference%2520optimization%250Aon%2520contrastive%2520samples%2520identified%2520by%2520the%2520strong%2520model%2520itself.%2520Extensive%250Aexperiments%2520on%2520the%2520GSM8K%2520and%2520MATH%2520datasets%2520demonstrate%2520that%2520our%2520method%250Asignificantly%2520enhances%2520the%2520reasoning%2520capabilities%2520of%2520Llama2-70b%2520using%2520three%250Aseparate%2520weak%2520models.%2520This%2520method%2520is%2520further%2520validated%2520in%2520a%2520forward-looking%250Aexperimental%2520setup%252C%2520where%2520Llama3-8b-instruct%2520effectively%2520supervises%2520Llama3-70b%250Aon%2520the%2520highly%2520challenging%2520OlympicArena%2520dataset.%2520This%2520work%2520paves%2520the%2520way%2520for%2520a%250Amore%2520scalable%2520and%2520sophisticated%2520strategy%2520to%2520enhance%2520AI%2520reasoning%2520powers.%2520All%250Arelevant%2520code%2520and%2520resources%2520are%2520available%2520in%250A%255Curl%257Bhttps%253A//github.com/GAIR-NLP/weak-to-strong-reasoning%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weak-to-Strong%20Reasoning&entry.906535625=Yuqing%20Yang%20and%20Yan%20Ma%20and%20Pengfei%20Liu&entry.1292438233=%20%20When%20large%20language%20models%20%28LLMs%29%20exceed%20human-level%20capabilities%2C%20it%20becomes%0Aincreasingly%20challenging%20to%20provide%20full-scale%20and%20accurate%20supervisions%20for%0Athese%20models.%20Weak-to-strong%20learning%2C%20which%20leverages%20a%20less%20capable%20model%20to%0Aunlock%20the%20latent%20abilities%20of%20a%20stronger%20model%2C%20proves%20valuable%20in%20this%0Acontext.%20Yet%2C%20the%20efficacy%20of%20this%20approach%20for%20complex%20reasoning%20tasks%20is%0Astill%20untested.%20Furthermore%2C%20tackling%20reasoning%20tasks%20under%20the%20weak-to-strong%0Asetting%20currently%20lacks%20efficient%20methods%20to%20avoid%20blindly%20imitating%20the%20weak%0Asupervisor%20including%20its%20errors.%20In%20this%20paper%2C%20we%20introduce%20a%20progressive%0Alearning%20framework%20that%20enables%20the%20strong%20model%20to%20autonomously%20refine%20its%0Atraining%20data%2C%20without%20requiring%20input%20from%20either%20a%20more%20advanced%20model%20or%0Ahuman-annotated%20data.%20This%20framework%20begins%20with%20supervised%20fine-tuning%20on%20a%0Aselective%20small%20but%20high-quality%20dataset%2C%20followed%20by%20preference%20optimization%0Aon%20contrastive%20samples%20identified%20by%20the%20strong%20model%20itself.%20Extensive%0Aexperiments%20on%20the%20GSM8K%20and%20MATH%20datasets%20demonstrate%20that%20our%20method%0Asignificantly%20enhances%20the%20reasoning%20capabilities%20of%20Llama2-70b%20using%20three%0Aseparate%20weak%20models.%20This%20method%20is%20further%20validated%20in%20a%20forward-looking%0Aexperimental%20setup%2C%20where%20Llama3-8b-instruct%20effectively%20supervises%20Llama3-70b%0Aon%20the%20highly%20challenging%20OlympicArena%20dataset.%20This%20work%20paves%20the%20way%20for%20a%0Amore%20scalable%20and%20sophisticated%20strategy%20to%20enhance%20AI%20reasoning%20powers.%20All%0Arelevant%20code%20and%20resources%20are%20available%20in%0A%5Curl%7Bhttps%3A//github.com/GAIR-NLP/weak-to-strong-reasoning%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13647v1&entry.124074799=Read"},
{"title": "With or Without Replacement? Improving Confidence in Fourier Imaging", "author": "Frederik Hoppe and Claudio Mayrink Verdun and Felix Krahmer and Marion I. Menzel and Holger Rauhut", "abstract": "  Over the last few years, debiased estimators have been proposed in order to\nestablish rigorous confidence intervals for high-dimensional problems in\nmachine learning and data science. The core argument is that the error of these\nestimators with respect to the ground truth can be expressed as a Gaussian\nvariable plus a remainder term that vanishes as long as the dimension of the\nproblem is sufficiently high. Thus, uncertainty quantification (UQ) can be\nperformed exploiting the Gaussian model. Empirically, however, the remainder\nterm cannot be neglected in many realistic situations of moderately-sized\ndimensions, in particular in certain structured measurement scenarios such as\nMagnetic Resonance Imaging (MRI). This, in turn, can downgrade the advantage of\nthe UQ methods as compared to non-UQ approaches such as the standard LASSO. In\nthis paper, we present a method to improve the debiased estimator by sampling\nwithout replacement. Our approach leverages recent results of ours on the\nstructure of the random nature of certain sampling schemes showing how a\ntransition between sampling with and without replacement can lead to a weighted\nreconstruction scheme with improved performance for the standard LASSO. In this\npaper, we illustrate how this reweighted sampling idea can also improve the\ndebiased estimator and, consequently, provide a better method for UQ in Fourier\nimaging.\n", "link": "http://arxiv.org/abs/2407.13575v1", "date": "2024-07-18", "relevancy": 1.9762, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5068}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4923}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20With%20or%20Without%20Replacement%3F%20Improving%20Confidence%20in%20Fourier%20Imaging&body=Title%3A%20With%20or%20Without%20Replacement%3F%20Improving%20Confidence%20in%20Fourier%20Imaging%0AAuthor%3A%20Frederik%20Hoppe%20and%20Claudio%20Mayrink%20Verdun%20and%20Felix%20Krahmer%20and%20Marion%20I.%20Menzel%20and%20Holger%20Rauhut%0AAbstract%3A%20%20%20Over%20the%20last%20few%20years%2C%20debiased%20estimators%20have%20been%20proposed%20in%20order%20to%0Aestablish%20rigorous%20confidence%20intervals%20for%20high-dimensional%20problems%20in%0Amachine%20learning%20and%20data%20science.%20The%20core%20argument%20is%20that%20the%20error%20of%20these%0Aestimators%20with%20respect%20to%20the%20ground%20truth%20can%20be%20expressed%20as%20a%20Gaussian%0Avariable%20plus%20a%20remainder%20term%20that%20vanishes%20as%20long%20as%20the%20dimension%20of%20the%0Aproblem%20is%20sufficiently%20high.%20Thus%2C%20uncertainty%20quantification%20%28UQ%29%20can%20be%0Aperformed%20exploiting%20the%20Gaussian%20model.%20Empirically%2C%20however%2C%20the%20remainder%0Aterm%20cannot%20be%20neglected%20in%20many%20realistic%20situations%20of%20moderately-sized%0Adimensions%2C%20in%20particular%20in%20certain%20structured%20measurement%20scenarios%20such%20as%0AMagnetic%20Resonance%20Imaging%20%28MRI%29.%20This%2C%20in%20turn%2C%20can%20downgrade%20the%20advantage%20of%0Athe%20UQ%20methods%20as%20compared%20to%20non-UQ%20approaches%20such%20as%20the%20standard%20LASSO.%20In%0Athis%20paper%2C%20we%20present%20a%20method%20to%20improve%20the%20debiased%20estimator%20by%20sampling%0Awithout%20replacement.%20Our%20approach%20leverages%20recent%20results%20of%20ours%20on%20the%0Astructure%20of%20the%20random%20nature%20of%20certain%20sampling%20schemes%20showing%20how%20a%0Atransition%20between%20sampling%20with%20and%20without%20replacement%20can%20lead%20to%20a%20weighted%0Areconstruction%20scheme%20with%20improved%20performance%20for%20the%20standard%20LASSO.%20In%20this%0Apaper%2C%20we%20illustrate%20how%20this%20reweighted%20sampling%20idea%20can%20also%20improve%20the%0Adebiased%20estimator%20and%2C%20consequently%2C%20provide%20a%20better%20method%20for%20UQ%20in%20Fourier%0Aimaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWith%2520or%2520Without%2520Replacement%253F%2520Improving%2520Confidence%2520in%2520Fourier%2520Imaging%26entry.906535625%3DFrederik%2520Hoppe%2520and%2520Claudio%2520Mayrink%2520Verdun%2520and%2520Felix%2520Krahmer%2520and%2520Marion%2520I.%2520Menzel%2520and%2520Holger%2520Rauhut%26entry.1292438233%3D%2520%2520Over%2520the%2520last%2520few%2520years%252C%2520debiased%2520estimators%2520have%2520been%2520proposed%2520in%2520order%2520to%250Aestablish%2520rigorous%2520confidence%2520intervals%2520for%2520high-dimensional%2520problems%2520in%250Amachine%2520learning%2520and%2520data%2520science.%2520The%2520core%2520argument%2520is%2520that%2520the%2520error%2520of%2520these%250Aestimators%2520with%2520respect%2520to%2520the%2520ground%2520truth%2520can%2520be%2520expressed%2520as%2520a%2520Gaussian%250Avariable%2520plus%2520a%2520remainder%2520term%2520that%2520vanishes%2520as%2520long%2520as%2520the%2520dimension%2520of%2520the%250Aproblem%2520is%2520sufficiently%2520high.%2520Thus%252C%2520uncertainty%2520quantification%2520%2528UQ%2529%2520can%2520be%250Aperformed%2520exploiting%2520the%2520Gaussian%2520model.%2520Empirically%252C%2520however%252C%2520the%2520remainder%250Aterm%2520cannot%2520be%2520neglected%2520in%2520many%2520realistic%2520situations%2520of%2520moderately-sized%250Adimensions%252C%2520in%2520particular%2520in%2520certain%2520structured%2520measurement%2520scenarios%2520such%2520as%250AMagnetic%2520Resonance%2520Imaging%2520%2528MRI%2529.%2520This%252C%2520in%2520turn%252C%2520can%2520downgrade%2520the%2520advantage%2520of%250Athe%2520UQ%2520methods%2520as%2520compared%2520to%2520non-UQ%2520approaches%2520such%2520as%2520the%2520standard%2520LASSO.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520method%2520to%2520improve%2520the%2520debiased%2520estimator%2520by%2520sampling%250Awithout%2520replacement.%2520Our%2520approach%2520leverages%2520recent%2520results%2520of%2520ours%2520on%2520the%250Astructure%2520of%2520the%2520random%2520nature%2520of%2520certain%2520sampling%2520schemes%2520showing%2520how%2520a%250Atransition%2520between%2520sampling%2520with%2520and%2520without%2520replacement%2520can%2520lead%2520to%2520a%2520weighted%250Areconstruction%2520scheme%2520with%2520improved%2520performance%2520for%2520the%2520standard%2520LASSO.%2520In%2520this%250Apaper%252C%2520we%2520illustrate%2520how%2520this%2520reweighted%2520sampling%2520idea%2520can%2520also%2520improve%2520the%250Adebiased%2520estimator%2520and%252C%2520consequently%252C%2520provide%2520a%2520better%2520method%2520for%2520UQ%2520in%2520Fourier%250Aimaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=With%20or%20Without%20Replacement%3F%20Improving%20Confidence%20in%20Fourier%20Imaging&entry.906535625=Frederik%20Hoppe%20and%20Claudio%20Mayrink%20Verdun%20and%20Felix%20Krahmer%20and%20Marion%20I.%20Menzel%20and%20Holger%20Rauhut&entry.1292438233=%20%20Over%20the%20last%20few%20years%2C%20debiased%20estimators%20have%20been%20proposed%20in%20order%20to%0Aestablish%20rigorous%20confidence%20intervals%20for%20high-dimensional%20problems%20in%0Amachine%20learning%20and%20data%20science.%20The%20core%20argument%20is%20that%20the%20error%20of%20these%0Aestimators%20with%20respect%20to%20the%20ground%20truth%20can%20be%20expressed%20as%20a%20Gaussian%0Avariable%20plus%20a%20remainder%20term%20that%20vanishes%20as%20long%20as%20the%20dimension%20of%20the%0Aproblem%20is%20sufficiently%20high.%20Thus%2C%20uncertainty%20quantification%20%28UQ%29%20can%20be%0Aperformed%20exploiting%20the%20Gaussian%20model.%20Empirically%2C%20however%2C%20the%20remainder%0Aterm%20cannot%20be%20neglected%20in%20many%20realistic%20situations%20of%20moderately-sized%0Adimensions%2C%20in%20particular%20in%20certain%20structured%20measurement%20scenarios%20such%20as%0AMagnetic%20Resonance%20Imaging%20%28MRI%29.%20This%2C%20in%20turn%2C%20can%20downgrade%20the%20advantage%20of%0Athe%20UQ%20methods%20as%20compared%20to%20non-UQ%20approaches%20such%20as%20the%20standard%20LASSO.%20In%0Athis%20paper%2C%20we%20present%20a%20method%20to%20improve%20the%20debiased%20estimator%20by%20sampling%0Awithout%20replacement.%20Our%20approach%20leverages%20recent%20results%20of%20ours%20on%20the%0Astructure%20of%20the%20random%20nature%20of%20certain%20sampling%20schemes%20showing%20how%20a%0Atransition%20between%20sampling%20with%20and%20without%20replacement%20can%20lead%20to%20a%20weighted%0Areconstruction%20scheme%20with%20improved%20performance%20for%20the%20standard%20LASSO.%20In%20this%0Apaper%2C%20we%20illustrate%20how%20this%20reweighted%20sampling%20idea%20can%20also%20improve%20the%0Adebiased%20estimator%20and%2C%20consequently%2C%20provide%20a%20better%20method%20for%20UQ%20in%20Fourier%0Aimaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13575v1&entry.124074799=Read"},
{"title": "Powerful and Flexible: Personalized Text-to-Image Generation via\n  Reinforcement Learning", "author": "Fanyue Wei and Wei Zeng and Zhenyang Li and Dawei Yin and Lixin Duan and Wen Li", "abstract": "  Personalized text-to-image models allow users to generate varied styles of\nimages (specified with a sentence) for an object (specified with a set of\nreference images). While remarkable results have been achieved using\ndiffusion-based generation models, the visual structure and details of the\nobject are often unexpectedly changed during the diffusion process. One major\nreason is that these diffusion-based approaches typically adopt a simple\nreconstruction objective during training, which can hardly enforce appropriate\nstructural consistency between the generated and the reference images. To this\nend, in this paper, we design a novel reinforcement learning framework by\nutilizing the deterministic policy gradient method for personalized\ntext-to-image generation, with which various objectives, differential or even\nnon-differential, can be easily incorporated to supervise the diffusion models\nto improve the quality of the generated images. Experimental results on\npersonalized text-to-image generation benchmark datasets demonstrate that our\nproposed approach outperforms existing state-of-the-art methods by a large\nmargin on visual fidelity while maintaining text-alignment. Our code is\navailable at: \\url{https://github.com/wfanyue/DPG-T2I-Personalization}.\n", "link": "http://arxiv.org/abs/2407.06642v2", "date": "2024-07-18", "relevancy": 1.9651, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6615}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6565}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Powerful%20and%20Flexible%3A%20Personalized%20Text-to-Image%20Generation%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20Powerful%20and%20Flexible%3A%20Personalized%20Text-to-Image%20Generation%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Fanyue%20Wei%20and%20Wei%20Zeng%20and%20Zhenyang%20Li%20and%20Dawei%20Yin%20and%20Lixin%20Duan%20and%20Wen%20Li%0AAbstract%3A%20%20%20Personalized%20text-to-image%20models%20allow%20users%20to%20generate%20varied%20styles%20of%0Aimages%20%28specified%20with%20a%20sentence%29%20for%20an%20object%20%28specified%20with%20a%20set%20of%0Areference%20images%29.%20While%20remarkable%20results%20have%20been%20achieved%20using%0Adiffusion-based%20generation%20models%2C%20the%20visual%20structure%20and%20details%20of%20the%0Aobject%20are%20often%20unexpectedly%20changed%20during%20the%20diffusion%20process.%20One%20major%0Areason%20is%20that%20these%20diffusion-based%20approaches%20typically%20adopt%20a%20simple%0Areconstruction%20objective%20during%20training%2C%20which%20can%20hardly%20enforce%20appropriate%0Astructural%20consistency%20between%20the%20generated%20and%20the%20reference%20images.%20To%20this%0Aend%2C%20in%20this%20paper%2C%20we%20design%20a%20novel%20reinforcement%20learning%20framework%20by%0Autilizing%20the%20deterministic%20policy%20gradient%20method%20for%20personalized%0Atext-to-image%20generation%2C%20with%20which%20various%20objectives%2C%20differential%20or%20even%0Anon-differential%2C%20can%20be%20easily%20incorporated%20to%20supervise%20the%20diffusion%20models%0Ato%20improve%20the%20quality%20of%20the%20generated%20images.%20Experimental%20results%20on%0Apersonalized%20text-to-image%20generation%20benchmark%20datasets%20demonstrate%20that%20our%0Aproposed%20approach%20outperforms%20existing%20state-of-the-art%20methods%20by%20a%20large%0Amargin%20on%20visual%20fidelity%20while%20maintaining%20text-alignment.%20Our%20code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/wfanyue/DPG-T2I-Personalization%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06642v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPowerful%2520and%2520Flexible%253A%2520Personalized%2520Text-to-Image%2520Generation%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DFanyue%2520Wei%2520and%2520Wei%2520Zeng%2520and%2520Zhenyang%2520Li%2520and%2520Dawei%2520Yin%2520and%2520Lixin%2520Duan%2520and%2520Wen%2520Li%26entry.1292438233%3D%2520%2520Personalized%2520text-to-image%2520models%2520allow%2520users%2520to%2520generate%2520varied%2520styles%2520of%250Aimages%2520%2528specified%2520with%2520a%2520sentence%2529%2520for%2520an%2520object%2520%2528specified%2520with%2520a%2520set%2520of%250Areference%2520images%2529.%2520While%2520remarkable%2520results%2520have%2520been%2520achieved%2520using%250Adiffusion-based%2520generation%2520models%252C%2520the%2520visual%2520structure%2520and%2520details%2520of%2520the%250Aobject%2520are%2520often%2520unexpectedly%2520changed%2520during%2520the%2520diffusion%2520process.%2520One%2520major%250Areason%2520is%2520that%2520these%2520diffusion-based%2520approaches%2520typically%2520adopt%2520a%2520simple%250Areconstruction%2520objective%2520during%2520training%252C%2520which%2520can%2520hardly%2520enforce%2520appropriate%250Astructural%2520consistency%2520between%2520the%2520generated%2520and%2520the%2520reference%2520images.%2520To%2520this%250Aend%252C%2520in%2520this%2520paper%252C%2520we%2520design%2520a%2520novel%2520reinforcement%2520learning%2520framework%2520by%250Autilizing%2520the%2520deterministic%2520policy%2520gradient%2520method%2520for%2520personalized%250Atext-to-image%2520generation%252C%2520with%2520which%2520various%2520objectives%252C%2520differential%2520or%2520even%250Anon-differential%252C%2520can%2520be%2520easily%2520incorporated%2520to%2520supervise%2520the%2520diffusion%2520models%250Ato%2520improve%2520the%2520quality%2520of%2520the%2520generated%2520images.%2520Experimental%2520results%2520on%250Apersonalized%2520text-to-image%2520generation%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%250Aproposed%2520approach%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520by%2520a%2520large%250Amargin%2520on%2520visual%2520fidelity%2520while%2520maintaining%2520text-alignment.%2520Our%2520code%2520is%250Aavailable%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/wfanyue/DPG-T2I-Personalization%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06642v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Powerful%20and%20Flexible%3A%20Personalized%20Text-to-Image%20Generation%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Fanyue%20Wei%20and%20Wei%20Zeng%20and%20Zhenyang%20Li%20and%20Dawei%20Yin%20and%20Lixin%20Duan%20and%20Wen%20Li&entry.1292438233=%20%20Personalized%20text-to-image%20models%20allow%20users%20to%20generate%20varied%20styles%20of%0Aimages%20%28specified%20with%20a%20sentence%29%20for%20an%20object%20%28specified%20with%20a%20set%20of%0Areference%20images%29.%20While%20remarkable%20results%20have%20been%20achieved%20using%0Adiffusion-based%20generation%20models%2C%20the%20visual%20structure%20and%20details%20of%20the%0Aobject%20are%20often%20unexpectedly%20changed%20during%20the%20diffusion%20process.%20One%20major%0Areason%20is%20that%20these%20diffusion-based%20approaches%20typically%20adopt%20a%20simple%0Areconstruction%20objective%20during%20training%2C%20which%20can%20hardly%20enforce%20appropriate%0Astructural%20consistency%20between%20the%20generated%20and%20the%20reference%20images.%20To%20this%0Aend%2C%20in%20this%20paper%2C%20we%20design%20a%20novel%20reinforcement%20learning%20framework%20by%0Autilizing%20the%20deterministic%20policy%20gradient%20method%20for%20personalized%0Atext-to-image%20generation%2C%20with%20which%20various%20objectives%2C%20differential%20or%20even%0Anon-differential%2C%20can%20be%20easily%20incorporated%20to%20supervise%20the%20diffusion%20models%0Ato%20improve%20the%20quality%20of%20the%20generated%20images.%20Experimental%20results%20on%0Apersonalized%20text-to-image%20generation%20benchmark%20datasets%20demonstrate%20that%20our%0Aproposed%20approach%20outperforms%20existing%20state-of-the-art%20methods%20by%20a%20large%0Amargin%20on%20visual%20fidelity%20while%20maintaining%20text-alignment.%20Our%20code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/wfanyue/DPG-T2I-Personalization%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06642v2&entry.124074799=Read"},
{"title": "From Words to Worlds: Compositionality for Cognitive Architectures", "author": "Ruchira Dhar and Anders S\u00f8gaard", "abstract": "  Large language models (LLMs) are very performant connectionist systems, but\ndo they exhibit more compositionality? More importantly, is that part of why\nthey perform so well? We present empirical analyses across four LLM families\n(12 models) and three task categories, including a novel task introduced below.\nOur findings reveal a nuanced relationship in learning of compositional\nstrategies by LLMs -- while scaling enhances compositional abilities,\ninstruction tuning often has a reverse effect. Such disparity brings forth some\nopen issues regarding the development and improvement of large language models\nin alignment with human cognitive capacities.\n", "link": "http://arxiv.org/abs/2407.13419v1", "date": "2024-07-18", "relevancy": 1.9636, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.498}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4913}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Words%20to%20Worlds%3A%20Compositionality%20for%20Cognitive%20Architectures&body=Title%3A%20From%20Words%20to%20Worlds%3A%20Compositionality%20for%20Cognitive%20Architectures%0AAuthor%3A%20Ruchira%20Dhar%20and%20Anders%20S%C3%B8gaard%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20very%20performant%20connectionist%20systems%2C%20but%0Ado%20they%20exhibit%20more%20compositionality%3F%20More%20importantly%2C%20is%20that%20part%20of%20why%0Athey%20perform%20so%20well%3F%20We%20present%20empirical%20analyses%20across%20four%20LLM%20families%0A%2812%20models%29%20and%20three%20task%20categories%2C%20including%20a%20novel%20task%20introduced%20below.%0AOur%20findings%20reveal%20a%20nuanced%20relationship%20in%20learning%20of%20compositional%0Astrategies%20by%20LLMs%20--%20while%20scaling%20enhances%20compositional%20abilities%2C%0Ainstruction%20tuning%20often%20has%20a%20reverse%20effect.%20Such%20disparity%20brings%20forth%20some%0Aopen%20issues%20regarding%20the%20development%20and%20improvement%20of%20large%20language%20models%0Ain%20alignment%20with%20human%20cognitive%20capacities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Words%2520to%2520Worlds%253A%2520Compositionality%2520for%2520Cognitive%2520Architectures%26entry.906535625%3DRuchira%2520Dhar%2520and%2520Anders%2520S%25C3%25B8gaard%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520very%2520performant%2520connectionist%2520systems%252C%2520but%250Ado%2520they%2520exhibit%2520more%2520compositionality%253F%2520More%2520importantly%252C%2520is%2520that%2520part%2520of%2520why%250Athey%2520perform%2520so%2520well%253F%2520We%2520present%2520empirical%2520analyses%2520across%2520four%2520LLM%2520families%250A%252812%2520models%2529%2520and%2520three%2520task%2520categories%252C%2520including%2520a%2520novel%2520task%2520introduced%2520below.%250AOur%2520findings%2520reveal%2520a%2520nuanced%2520relationship%2520in%2520learning%2520of%2520compositional%250Astrategies%2520by%2520LLMs%2520--%2520while%2520scaling%2520enhances%2520compositional%2520abilities%252C%250Ainstruction%2520tuning%2520often%2520has%2520a%2520reverse%2520effect.%2520Such%2520disparity%2520brings%2520forth%2520some%250Aopen%2520issues%2520regarding%2520the%2520development%2520and%2520improvement%2520of%2520large%2520language%2520models%250Ain%2520alignment%2520with%2520human%2520cognitive%2520capacities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Words%20to%20Worlds%3A%20Compositionality%20for%20Cognitive%20Architectures&entry.906535625=Ruchira%20Dhar%20and%20Anders%20S%C3%B8gaard&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20very%20performant%20connectionist%20systems%2C%20but%0Ado%20they%20exhibit%20more%20compositionality%3F%20More%20importantly%2C%20is%20that%20part%20of%20why%0Athey%20perform%20so%20well%3F%20We%20present%20empirical%20analyses%20across%20four%20LLM%20families%0A%2812%20models%29%20and%20three%20task%20categories%2C%20including%20a%20novel%20task%20introduced%20below.%0AOur%20findings%20reveal%20a%20nuanced%20relationship%20in%20learning%20of%20compositional%0Astrategies%20by%20LLMs%20--%20while%20scaling%20enhances%20compositional%20abilities%2C%0Ainstruction%20tuning%20often%20has%20a%20reverse%20effect.%20Such%20disparity%20brings%20forth%20some%0Aopen%20issues%20regarding%20the%20development%20and%20improvement%20of%20large%20language%20models%0Ain%20alignment%20with%20human%20cognitive%20capacities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13419v1&entry.124074799=Read"},
{"title": "IPA-NeRF: Illusory Poisoning Attack Against Neural Radiance Fields", "author": "Wenxiang Jiang and Hanwei Zhang and Shuo Zhao and Zhongwen Guo and Hao Wang", "abstract": "  Neural Radiance Field (NeRF) represents a significant advancement in computer\nvision, offering implicit neural network-based scene representation and novel\nview synthesis capabilities. Its applications span diverse fields including\nrobotics, urban mapping, autonomous navigation, virtual reality/augmented\nreality, etc., some of which are considered high-risk AI applications. However,\ndespite its widespread adoption, the robustness and security of NeRF remain\nlargely unexplored. In this study, we contribute to this area by introducing\nthe Illusory Poisoning Attack against Neural Radiance Fields (IPA-NeRF). This\nattack involves embedding a hidden backdoor view into NeRF, allowing it to\nproduce predetermined outputs, i.e. illusory, when presented with the specified\nbackdoor view while maintaining normal performance with standard inputs. Our\nattack is specifically designed to deceive users or downstream models at a\nparticular position while ensuring that any abnormalities in NeRF remain\nundetectable from other viewpoints. Experimental results demonstrate the\neffectiveness of our Illusory Poisoning Attack, successfully presenting the\ndesired illusory on the specified viewpoint without impacting other views.\nNotably, we achieve this attack by introducing small perturbations solely to\nthe training set. The code can be found at\nhttps://github.com/jiang-wenxiang/IPA-NeRF.\n", "link": "http://arxiv.org/abs/2407.11921v2", "date": "2024-07-18", "relevancy": 1.9588, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4987}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4845}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IPA-NeRF%3A%20Illusory%20Poisoning%20Attack%20Against%20Neural%20Radiance%20Fields&body=Title%3A%20IPA-NeRF%3A%20Illusory%20Poisoning%20Attack%20Against%20Neural%20Radiance%20Fields%0AAuthor%3A%20Wenxiang%20Jiang%20and%20Hanwei%20Zhang%20and%20Shuo%20Zhao%20and%20Zhongwen%20Guo%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Neural%20Radiance%20Field%20%28NeRF%29%20represents%20a%20significant%20advancement%20in%20computer%0Avision%2C%20offering%20implicit%20neural%20network-based%20scene%20representation%20and%20novel%0Aview%20synthesis%20capabilities.%20Its%20applications%20span%20diverse%20fields%20including%0Arobotics%2C%20urban%20mapping%2C%20autonomous%20navigation%2C%20virtual%20reality/augmented%0Areality%2C%20etc.%2C%20some%20of%20which%20are%20considered%20high-risk%20AI%20applications.%20However%2C%0Adespite%20its%20widespread%20adoption%2C%20the%20robustness%20and%20security%20of%20NeRF%20remain%0Alargely%20unexplored.%20In%20this%20study%2C%20we%20contribute%20to%20this%20area%20by%20introducing%0Athe%20Illusory%20Poisoning%20Attack%20against%20Neural%20Radiance%20Fields%20%28IPA-NeRF%29.%20This%0Aattack%20involves%20embedding%20a%20hidden%20backdoor%20view%20into%20NeRF%2C%20allowing%20it%20to%0Aproduce%20predetermined%20outputs%2C%20i.e.%20illusory%2C%20when%20presented%20with%20the%20specified%0Abackdoor%20view%20while%20maintaining%20normal%20performance%20with%20standard%20inputs.%20Our%0Aattack%20is%20specifically%20designed%20to%20deceive%20users%20or%20downstream%20models%20at%20a%0Aparticular%20position%20while%20ensuring%20that%20any%20abnormalities%20in%20NeRF%20remain%0Aundetectable%20from%20other%20viewpoints.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20Illusory%20Poisoning%20Attack%2C%20successfully%20presenting%20the%0Adesired%20illusory%20on%20the%20specified%20viewpoint%20without%20impacting%20other%20views.%0ANotably%2C%20we%20achieve%20this%20attack%20by%20introducing%20small%20perturbations%20solely%20to%0Athe%20training%20set.%20The%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/jiang-wenxiang/IPA-NeRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11921v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIPA-NeRF%253A%2520Illusory%2520Poisoning%2520Attack%2520Against%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DWenxiang%2520Jiang%2520and%2520Hanwei%2520Zhang%2520and%2520Shuo%2520Zhao%2520and%2520Zhongwen%2520Guo%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520represents%2520a%2520significant%2520advancement%2520in%2520computer%250Avision%252C%2520offering%2520implicit%2520neural%2520network-based%2520scene%2520representation%2520and%2520novel%250Aview%2520synthesis%2520capabilities.%2520Its%2520applications%2520span%2520diverse%2520fields%2520including%250Arobotics%252C%2520urban%2520mapping%252C%2520autonomous%2520navigation%252C%2520virtual%2520reality/augmented%250Areality%252C%2520etc.%252C%2520some%2520of%2520which%2520are%2520considered%2520high-risk%2520AI%2520applications.%2520However%252C%250Adespite%2520its%2520widespread%2520adoption%252C%2520the%2520robustness%2520and%2520security%2520of%2520NeRF%2520remain%250Alargely%2520unexplored.%2520In%2520this%2520study%252C%2520we%2520contribute%2520to%2520this%2520area%2520by%2520introducing%250Athe%2520Illusory%2520Poisoning%2520Attack%2520against%2520Neural%2520Radiance%2520Fields%2520%2528IPA-NeRF%2529.%2520This%250Aattack%2520involves%2520embedding%2520a%2520hidden%2520backdoor%2520view%2520into%2520NeRF%252C%2520allowing%2520it%2520to%250Aproduce%2520predetermined%2520outputs%252C%2520i.e.%2520illusory%252C%2520when%2520presented%2520with%2520the%2520specified%250Abackdoor%2520view%2520while%2520maintaining%2520normal%2520performance%2520with%2520standard%2520inputs.%2520Our%250Aattack%2520is%2520specifically%2520designed%2520to%2520deceive%2520users%2520or%2520downstream%2520models%2520at%2520a%250Aparticular%2520position%2520while%2520ensuring%2520that%2520any%2520abnormalities%2520in%2520NeRF%2520remain%250Aundetectable%2520from%2520other%2520viewpoints.%2520Experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520Illusory%2520Poisoning%2520Attack%252C%2520successfully%2520presenting%2520the%250Adesired%2520illusory%2520on%2520the%2520specified%2520viewpoint%2520without%2520impacting%2520other%2520views.%250ANotably%252C%2520we%2520achieve%2520this%2520attack%2520by%2520introducing%2520small%2520perturbations%2520solely%2520to%250Athe%2520training%2520set.%2520The%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/jiang-wenxiang/IPA-NeRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11921v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IPA-NeRF%3A%20Illusory%20Poisoning%20Attack%20Against%20Neural%20Radiance%20Fields&entry.906535625=Wenxiang%20Jiang%20and%20Hanwei%20Zhang%20and%20Shuo%20Zhao%20and%20Zhongwen%20Guo%20and%20Hao%20Wang&entry.1292438233=%20%20Neural%20Radiance%20Field%20%28NeRF%29%20represents%20a%20significant%20advancement%20in%20computer%0Avision%2C%20offering%20implicit%20neural%20network-based%20scene%20representation%20and%20novel%0Aview%20synthesis%20capabilities.%20Its%20applications%20span%20diverse%20fields%20including%0Arobotics%2C%20urban%20mapping%2C%20autonomous%20navigation%2C%20virtual%20reality/augmented%0Areality%2C%20etc.%2C%20some%20of%20which%20are%20considered%20high-risk%20AI%20applications.%20However%2C%0Adespite%20its%20widespread%20adoption%2C%20the%20robustness%20and%20security%20of%20NeRF%20remain%0Alargely%20unexplored.%20In%20this%20study%2C%20we%20contribute%20to%20this%20area%20by%20introducing%0Athe%20Illusory%20Poisoning%20Attack%20against%20Neural%20Radiance%20Fields%20%28IPA-NeRF%29.%20This%0Aattack%20involves%20embedding%20a%20hidden%20backdoor%20view%20into%20NeRF%2C%20allowing%20it%20to%0Aproduce%20predetermined%20outputs%2C%20i.e.%20illusory%2C%20when%20presented%20with%20the%20specified%0Abackdoor%20view%20while%20maintaining%20normal%20performance%20with%20standard%20inputs.%20Our%0Aattack%20is%20specifically%20designed%20to%20deceive%20users%20or%20downstream%20models%20at%20a%0Aparticular%20position%20while%20ensuring%20that%20any%20abnormalities%20in%20NeRF%20remain%0Aundetectable%20from%20other%20viewpoints.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20Illusory%20Poisoning%20Attack%2C%20successfully%20presenting%20the%0Adesired%20illusory%20on%20the%20specified%20viewpoint%20without%20impacting%20other%20views.%0ANotably%2C%20we%20achieve%20this%20attack%20by%20introducing%20small%20perturbations%20solely%20to%0Athe%20training%20set.%20The%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/jiang-wenxiang/IPA-NeRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11921v2&entry.124074799=Read"},
{"title": "Benchmarking Vision Language Models for Cultural Understanding", "author": "Shravan Nayak and Kanishk Jain and Rabiul Awal and Siva Reddy and Sjoerd van Steenkiste and Lisa Anne Hendricks and Karolina Sta\u0144czak and Aishwarya Agrawal", "abstract": "  Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.\n", "link": "http://arxiv.org/abs/2407.10920v2", "date": "2024-07-18", "relevancy": 1.9587, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.501}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4867}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Vision%20Language%20Models%20for%20Cultural%20Understanding&body=Title%3A%20Benchmarking%20Vision%20Language%20Models%20for%20Cultural%20Understanding%0AAuthor%3A%20Shravan%20Nayak%20and%20Kanishk%20Jain%20and%20Rabiul%20Awal%20and%20Siva%20Reddy%20and%20Sjoerd%20van%20Steenkiste%20and%20Lisa%20Anne%20Hendricks%20and%20Karolina%20Sta%C5%84czak%20and%20Aishwarya%20Agrawal%0AAbstract%3A%20%20%20Foundation%20models%20and%20vision-language%20pre-training%20have%20notably%20advanced%0AVision%20Language%20Models%20%28VLMs%29%2C%20enabling%20multimodal%20processing%20of%20visual%20and%0Alinguistic%20data.%20However%2C%20their%20performance%20has%20been%20typically%20assessed%20on%0Ageneral%20scene%20understanding%20-%20recognizing%20objects%2C%20attributes%2C%20and%20actions%20-%0Arather%20than%20cultural%20comprehension.%20This%20study%20introduces%20CulturalVQA%2C%20a%20visual%0Aquestion-answering%20benchmark%20aimed%20at%20assessing%20VLM%27s%20geo-diverse%20cultural%0Aunderstanding.%20We%20curate%20a%20collection%20of%202%2C378%20image-question%20pairs%20with%201-5%0Aanswers%20per%20question%20representing%20cultures%20from%2011%20countries%20across%205%0Acontinents.%20The%20questions%20probe%20understanding%20of%20various%20facets%20of%20culture%20such%0Aas%20clothing%2C%20food%2C%20drinks%2C%20rituals%2C%20and%20traditions.%20Benchmarking%20VLMs%20on%0ACulturalVQA%2C%20including%20GPT-4V%20and%20Gemini%2C%20reveals%20disparity%20in%20their%20level%20of%0Acultural%20understanding%20across%20regions%2C%20with%20strong%20cultural%20understanding%0Acapabilities%20for%20North%20America%20while%20significantly%20lower%20performance%20for%0AAfrica.%20We%20observe%20disparity%20in%20their%20performance%20across%20cultural%20facets%20too%2C%0Awith%20clothing%2C%20rituals%2C%20and%20traditions%20seeing%20higher%20performances%20than%20food%20and%0Adrink.%20These%20disparities%20help%20us%20identify%20areas%20where%20VLMs%20lack%20cultural%0Aunderstanding%20and%20demonstrate%20the%20potential%20of%20CulturalVQA%20as%20a%20comprehensive%0Aevaluation%20set%20for%20gauging%20VLM%20progress%20in%20understanding%20diverse%20cultures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10920v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Vision%2520Language%2520Models%2520for%2520Cultural%2520Understanding%26entry.906535625%3DShravan%2520Nayak%2520and%2520Kanishk%2520Jain%2520and%2520Rabiul%2520Awal%2520and%2520Siva%2520Reddy%2520and%2520Sjoerd%2520van%2520Steenkiste%2520and%2520Lisa%2520Anne%2520Hendricks%2520and%2520Karolina%2520Sta%25C5%2584czak%2520and%2520Aishwarya%2520Agrawal%26entry.1292438233%3D%2520%2520Foundation%2520models%2520and%2520vision-language%2520pre-training%2520have%2520notably%2520advanced%250AVision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520enabling%2520multimodal%2520processing%2520of%2520visual%2520and%250Alinguistic%2520data.%2520However%252C%2520their%2520performance%2520has%2520been%2520typically%2520assessed%2520on%250Ageneral%2520scene%2520understanding%2520-%2520recognizing%2520objects%252C%2520attributes%252C%2520and%2520actions%2520-%250Arather%2520than%2520cultural%2520comprehension.%2520This%2520study%2520introduces%2520CulturalVQA%252C%2520a%2520visual%250Aquestion-answering%2520benchmark%2520aimed%2520at%2520assessing%2520VLM%2527s%2520geo-diverse%2520cultural%250Aunderstanding.%2520We%2520curate%2520a%2520collection%2520of%25202%252C378%2520image-question%2520pairs%2520with%25201-5%250Aanswers%2520per%2520question%2520representing%2520cultures%2520from%252011%2520countries%2520across%25205%250Acontinents.%2520The%2520questions%2520probe%2520understanding%2520of%2520various%2520facets%2520of%2520culture%2520such%250Aas%2520clothing%252C%2520food%252C%2520drinks%252C%2520rituals%252C%2520and%2520traditions.%2520Benchmarking%2520VLMs%2520on%250ACulturalVQA%252C%2520including%2520GPT-4V%2520and%2520Gemini%252C%2520reveals%2520disparity%2520in%2520their%2520level%2520of%250Acultural%2520understanding%2520across%2520regions%252C%2520with%2520strong%2520cultural%2520understanding%250Acapabilities%2520for%2520North%2520America%2520while%2520significantly%2520lower%2520performance%2520for%250AAfrica.%2520We%2520observe%2520disparity%2520in%2520their%2520performance%2520across%2520cultural%2520facets%2520too%252C%250Awith%2520clothing%252C%2520rituals%252C%2520and%2520traditions%2520seeing%2520higher%2520performances%2520than%2520food%2520and%250Adrink.%2520These%2520disparities%2520help%2520us%2520identify%2520areas%2520where%2520VLMs%2520lack%2520cultural%250Aunderstanding%2520and%2520demonstrate%2520the%2520potential%2520of%2520CulturalVQA%2520as%2520a%2520comprehensive%250Aevaluation%2520set%2520for%2520gauging%2520VLM%2520progress%2520in%2520understanding%2520diverse%2520cultures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10920v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Vision%20Language%20Models%20for%20Cultural%20Understanding&entry.906535625=Shravan%20Nayak%20and%20Kanishk%20Jain%20and%20Rabiul%20Awal%20and%20Siva%20Reddy%20and%20Sjoerd%20van%20Steenkiste%20and%20Lisa%20Anne%20Hendricks%20and%20Karolina%20Sta%C5%84czak%20and%20Aishwarya%20Agrawal&entry.1292438233=%20%20Foundation%20models%20and%20vision-language%20pre-training%20have%20notably%20advanced%0AVision%20Language%20Models%20%28VLMs%29%2C%20enabling%20multimodal%20processing%20of%20visual%20and%0Alinguistic%20data.%20However%2C%20their%20performance%20has%20been%20typically%20assessed%20on%0Ageneral%20scene%20understanding%20-%20recognizing%20objects%2C%20attributes%2C%20and%20actions%20-%0Arather%20than%20cultural%20comprehension.%20This%20study%20introduces%20CulturalVQA%2C%20a%20visual%0Aquestion-answering%20benchmark%20aimed%20at%20assessing%20VLM%27s%20geo-diverse%20cultural%0Aunderstanding.%20We%20curate%20a%20collection%20of%202%2C378%20image-question%20pairs%20with%201-5%0Aanswers%20per%20question%20representing%20cultures%20from%2011%20countries%20across%205%0Acontinents.%20The%20questions%20probe%20understanding%20of%20various%20facets%20of%20culture%20such%0Aas%20clothing%2C%20food%2C%20drinks%2C%20rituals%2C%20and%20traditions.%20Benchmarking%20VLMs%20on%0ACulturalVQA%2C%20including%20GPT-4V%20and%20Gemini%2C%20reveals%20disparity%20in%20their%20level%20of%0Acultural%20understanding%20across%20regions%2C%20with%20strong%20cultural%20understanding%0Acapabilities%20for%20North%20America%20while%20significantly%20lower%20performance%20for%0AAfrica.%20We%20observe%20disparity%20in%20their%20performance%20across%20cultural%20facets%20too%2C%0Awith%20clothing%2C%20rituals%2C%20and%20traditions%20seeing%20higher%20performances%20than%20food%20and%0Adrink.%20These%20disparities%20help%20us%20identify%20areas%20where%20VLMs%20lack%20cultural%0Aunderstanding%20and%20demonstrate%20the%20potential%20of%20CulturalVQA%20as%20a%20comprehensive%0Aevaluation%20set%20for%20gauging%20VLM%20progress%20in%20understanding%20diverse%20cultures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10920v2&entry.124074799=Read"},
{"title": "Large Language Models as Reliable Knowledge Bases?", "author": "Danna Zheng and Mirella Lapata and Jeff Z. Pan", "abstract": "  The NLP community has recently shown a growing interest in leveraging Large\nLanguage Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potential\nknowledge bases (KBs). However, the reliability and extent to which LLMs can\nfunction as KBs remain underexplored. While previous studies suggest LLMs can\nencode knowledge within their parameters, the amount of parametric knowledge\nalone is not sufficient to evaluate their effectiveness as KBs. This study\ndefines criteria that a reliable LLM-as-KB should meet, focusing on factuality\nand consistency, and covering both seen and unseen knowledge. We develop\nseveral metrics based on these criteria and use them to evaluate 26 popular\nLLMs, while providing a comprehensive analysis of the effects of model size,\ninstruction tuning, and in-context learning (ICL). Our results paint a worrying\npicture. Even a high-performant model like GPT-3.5-turbo is not factual or\nconsistent, and strategies like ICL and fine-tuning are unsuccessful at making\nLLMs better KBs.\n", "link": "http://arxiv.org/abs/2407.13578v1", "date": "2024-07-18", "relevancy": 1.3698, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4972}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4471}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20as%20Reliable%20Knowledge%20Bases%3F&body=Title%3A%20Large%20Language%20Models%20as%20Reliable%20Knowledge%20Bases%3F%0AAuthor%3A%20Danna%20Zheng%20and%20Mirella%20Lapata%20and%20Jeff%20Z.%20Pan%0AAbstract%3A%20%20%20The%20NLP%20community%20has%20recently%20shown%20a%20growing%20interest%20in%20leveraging%20Large%0ALanguage%20Models%20%28LLMs%29%20for%20knowledge-intensive%20tasks%2C%20viewing%20LLMs%20as%20potential%0Aknowledge%20bases%20%28KBs%29.%20However%2C%20the%20reliability%20and%20extent%20to%20which%20LLMs%20can%0Afunction%20as%20KBs%20remain%20underexplored.%20While%20previous%20studies%20suggest%20LLMs%20can%0Aencode%20knowledge%20within%20their%20parameters%2C%20the%20amount%20of%20parametric%20knowledge%0Aalone%20is%20not%20sufficient%20to%20evaluate%20their%20effectiveness%20as%20KBs.%20This%20study%0Adefines%20criteria%20that%20a%20reliable%20LLM-as-KB%20should%20meet%2C%20focusing%20on%20factuality%0Aand%20consistency%2C%20and%20covering%20both%20seen%20and%20unseen%20knowledge.%20We%20develop%0Aseveral%20metrics%20based%20on%20these%20criteria%20and%20use%20them%20to%20evaluate%2026%20popular%0ALLMs%2C%20while%20providing%20a%20comprehensive%20analysis%20of%20the%20effects%20of%20model%20size%2C%0Ainstruction%20tuning%2C%20and%20in-context%20learning%20%28ICL%29.%20Our%20results%20paint%20a%20worrying%0Apicture.%20Even%20a%20high-performant%20model%20like%20GPT-3.5-turbo%20is%20not%20factual%20or%0Aconsistent%2C%20and%20strategies%20like%20ICL%20and%20fine-tuning%20are%20unsuccessful%20at%20making%0ALLMs%20better%20KBs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520as%2520Reliable%2520Knowledge%2520Bases%253F%26entry.906535625%3DDanna%2520Zheng%2520and%2520Mirella%2520Lapata%2520and%2520Jeff%2520Z.%2520Pan%26entry.1292438233%3D%2520%2520The%2520NLP%2520community%2520has%2520recently%2520shown%2520a%2520growing%2520interest%2520in%2520leveraging%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520for%2520knowledge-intensive%2520tasks%252C%2520viewing%2520LLMs%2520as%2520potential%250Aknowledge%2520bases%2520%2528KBs%2529.%2520However%252C%2520the%2520reliability%2520and%2520extent%2520to%2520which%2520LLMs%2520can%250Afunction%2520as%2520KBs%2520remain%2520underexplored.%2520While%2520previous%2520studies%2520suggest%2520LLMs%2520can%250Aencode%2520knowledge%2520within%2520their%2520parameters%252C%2520the%2520amount%2520of%2520parametric%2520knowledge%250Aalone%2520is%2520not%2520sufficient%2520to%2520evaluate%2520their%2520effectiveness%2520as%2520KBs.%2520This%2520study%250Adefines%2520criteria%2520that%2520a%2520reliable%2520LLM-as-KB%2520should%2520meet%252C%2520focusing%2520on%2520factuality%250Aand%2520consistency%252C%2520and%2520covering%2520both%2520seen%2520and%2520unseen%2520knowledge.%2520We%2520develop%250Aseveral%2520metrics%2520based%2520on%2520these%2520criteria%2520and%2520use%2520them%2520to%2520evaluate%252026%2520popular%250ALLMs%252C%2520while%2520providing%2520a%2520comprehensive%2520analysis%2520of%2520the%2520effects%2520of%2520model%2520size%252C%250Ainstruction%2520tuning%252C%2520and%2520in-context%2520learning%2520%2528ICL%2529.%2520Our%2520results%2520paint%2520a%2520worrying%250Apicture.%2520Even%2520a%2520high-performant%2520model%2520like%2520GPT-3.5-turbo%2520is%2520not%2520factual%2520or%250Aconsistent%252C%2520and%2520strategies%2520like%2520ICL%2520and%2520fine-tuning%2520are%2520unsuccessful%2520at%2520making%250ALLMs%2520better%2520KBs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20as%20Reliable%20Knowledge%20Bases%3F&entry.906535625=Danna%20Zheng%20and%20Mirella%20Lapata%20and%20Jeff%20Z.%20Pan&entry.1292438233=%20%20The%20NLP%20community%20has%20recently%20shown%20a%20growing%20interest%20in%20leveraging%20Large%0ALanguage%20Models%20%28LLMs%29%20for%20knowledge-intensive%20tasks%2C%20viewing%20LLMs%20as%20potential%0Aknowledge%20bases%20%28KBs%29.%20However%2C%20the%20reliability%20and%20extent%20to%20which%20LLMs%20can%0Afunction%20as%20KBs%20remain%20underexplored.%20While%20previous%20studies%20suggest%20LLMs%20can%0Aencode%20knowledge%20within%20their%20parameters%2C%20the%20amount%20of%20parametric%20knowledge%0Aalone%20is%20not%20sufficient%20to%20evaluate%20their%20effectiveness%20as%20KBs.%20This%20study%0Adefines%20criteria%20that%20a%20reliable%20LLM-as-KB%20should%20meet%2C%20focusing%20on%20factuality%0Aand%20consistency%2C%20and%20covering%20both%20seen%20and%20unseen%20knowledge.%20We%20develop%0Aseveral%20metrics%20based%20on%20these%20criteria%20and%20use%20them%20to%20evaluate%2026%20popular%0ALLMs%2C%20while%20providing%20a%20comprehensive%20analysis%20of%20the%20effects%20of%20model%20size%2C%0Ainstruction%20tuning%2C%20and%20in-context%20learning%20%28ICL%29.%20Our%20results%20paint%20a%20worrying%0Apicture.%20Even%20a%20high-performant%20model%20like%20GPT-3.5-turbo%20is%20not%20factual%20or%0Aconsistent%2C%20and%20strategies%20like%20ICL%20and%20fine-tuning%20are%20unsuccessful%20at%20making%0ALLMs%20better%20KBs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13578v1&entry.124074799=Read"},
{"title": "Mechanistically Interpreting a Transformer-based 2-SAT Solver: An\n  Axiomatic Approach", "author": "Nils Palumbo and Ravi Mangal and Zifan Wang and Saranya Vijayakumar and Corina S. Pasareanu and Somesh Jha", "abstract": "  Mechanistic interpretability aims to reverse engineer the computation\nperformed by a neural network in terms of its internal components. Although\nthere is a growing body of research on mechanistic interpretation of neural\nnetworks, the notion of a mechanistic interpretation itself is often ad-hoc.\nInspired by the notion of abstract interpretation from the program analysis\nliterature that aims to develop approximate semantics for programs, we give a\nset of axioms that formally characterize a mechanistic interpretation as a\ndescription that approximately captures the semantics of the neural network\nunder analysis in a compositional manner. We use these axioms to guide the\nmechanistic interpretability analysis of a Transformer-based model trained to\nsolve the well-known 2-SAT problem. We are able to reverse engineer the\nalgorithm learned by the model -- the model first parses the input formulas and\nthen evaluates their satisfiability via enumeration of different possible\nvaluations of the Boolean input variables. We also present evidence to support\nthat the mechanistic interpretation of the analyzed model indeed satisfies the\nstated axioms.\n", "link": "http://arxiv.org/abs/2407.13594v1", "date": "2024-07-18", "relevancy": 0.9213, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4739}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanistically%20Interpreting%20a%20Transformer-based%202-SAT%20Solver%3A%20An%0A%20%20Axiomatic%20Approach&body=Title%3A%20Mechanistically%20Interpreting%20a%20Transformer-based%202-SAT%20Solver%3A%20An%0A%20%20Axiomatic%20Approach%0AAuthor%3A%20Nils%20Palumbo%20and%20Ravi%20Mangal%20and%20Zifan%20Wang%20and%20Saranya%20Vijayakumar%20and%20Corina%20S.%20Pasareanu%20and%20Somesh%20Jha%0AAbstract%3A%20%20%20Mechanistic%20interpretability%20aims%20to%20reverse%20engineer%20the%20computation%0Aperformed%20by%20a%20neural%20network%20in%20terms%20of%20its%20internal%20components.%20Although%0Athere%20is%20a%20growing%20body%20of%20research%20on%20mechanistic%20interpretation%20of%20neural%0Anetworks%2C%20the%20notion%20of%20a%20mechanistic%20interpretation%20itself%20is%20often%20ad-hoc.%0AInspired%20by%20the%20notion%20of%20abstract%20interpretation%20from%20the%20program%20analysis%0Aliterature%20that%20aims%20to%20develop%20approximate%20semantics%20for%20programs%2C%20we%20give%20a%0Aset%20of%20axioms%20that%20formally%20characterize%20a%20mechanistic%20interpretation%20as%20a%0Adescription%20that%20approximately%20captures%20the%20semantics%20of%20the%20neural%20network%0Aunder%20analysis%20in%20a%20compositional%20manner.%20We%20use%20these%20axioms%20to%20guide%20the%0Amechanistic%20interpretability%20analysis%20of%20a%20Transformer-based%20model%20trained%20to%0Asolve%20the%20well-known%202-SAT%20problem.%20We%20are%20able%20to%20reverse%20engineer%20the%0Aalgorithm%20learned%20by%20the%20model%20--%20the%20model%20first%20parses%20the%20input%20formulas%20and%0Athen%20evaluates%20their%20satisfiability%20via%20enumeration%20of%20different%20possible%0Avaluations%20of%20the%20Boolean%20input%20variables.%20We%20also%20present%20evidence%20to%20support%0Athat%20the%20mechanistic%20interpretation%20of%20the%20analyzed%20model%20indeed%20satisfies%20the%0Astated%20axioms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanistically%2520Interpreting%2520a%2520Transformer-based%25202-SAT%2520Solver%253A%2520An%250A%2520%2520Axiomatic%2520Approach%26entry.906535625%3DNils%2520Palumbo%2520and%2520Ravi%2520Mangal%2520and%2520Zifan%2520Wang%2520and%2520Saranya%2520Vijayakumar%2520and%2520Corina%2520S.%2520Pasareanu%2520and%2520Somesh%2520Jha%26entry.1292438233%3D%2520%2520Mechanistic%2520interpretability%2520aims%2520to%2520reverse%2520engineer%2520the%2520computation%250Aperformed%2520by%2520a%2520neural%2520network%2520in%2520terms%2520of%2520its%2520internal%2520components.%2520Although%250Athere%2520is%2520a%2520growing%2520body%2520of%2520research%2520on%2520mechanistic%2520interpretation%2520of%2520neural%250Anetworks%252C%2520the%2520notion%2520of%2520a%2520mechanistic%2520interpretation%2520itself%2520is%2520often%2520ad-hoc.%250AInspired%2520by%2520the%2520notion%2520of%2520abstract%2520interpretation%2520from%2520the%2520program%2520analysis%250Aliterature%2520that%2520aims%2520to%2520develop%2520approximate%2520semantics%2520for%2520programs%252C%2520we%2520give%2520a%250Aset%2520of%2520axioms%2520that%2520formally%2520characterize%2520a%2520mechanistic%2520interpretation%2520as%2520a%250Adescription%2520that%2520approximately%2520captures%2520the%2520semantics%2520of%2520the%2520neural%2520network%250Aunder%2520analysis%2520in%2520a%2520compositional%2520manner.%2520We%2520use%2520these%2520axioms%2520to%2520guide%2520the%250Amechanistic%2520interpretability%2520analysis%2520of%2520a%2520Transformer-based%2520model%2520trained%2520to%250Asolve%2520the%2520well-known%25202-SAT%2520problem.%2520We%2520are%2520able%2520to%2520reverse%2520engineer%2520the%250Aalgorithm%2520learned%2520by%2520the%2520model%2520--%2520the%2520model%2520first%2520parses%2520the%2520input%2520formulas%2520and%250Athen%2520evaluates%2520their%2520satisfiability%2520via%2520enumeration%2520of%2520different%2520possible%250Avaluations%2520of%2520the%2520Boolean%2520input%2520variables.%2520We%2520also%2520present%2520evidence%2520to%2520support%250Athat%2520the%2520mechanistic%2520interpretation%2520of%2520the%2520analyzed%2520model%2520indeed%2520satisfies%2520the%250Astated%2520axioms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanistically%20Interpreting%20a%20Transformer-based%202-SAT%20Solver%3A%20An%0A%20%20Axiomatic%20Approach&entry.906535625=Nils%20Palumbo%20and%20Ravi%20Mangal%20and%20Zifan%20Wang%20and%20Saranya%20Vijayakumar%20and%20Corina%20S.%20Pasareanu%20and%20Somesh%20Jha&entry.1292438233=%20%20Mechanistic%20interpretability%20aims%20to%20reverse%20engineer%20the%20computation%0Aperformed%20by%20a%20neural%20network%20in%20terms%20of%20its%20internal%20components.%20Although%0Athere%20is%20a%20growing%20body%20of%20research%20on%20mechanistic%20interpretation%20of%20neural%0Anetworks%2C%20the%20notion%20of%20a%20mechanistic%20interpretation%20itself%20is%20often%20ad-hoc.%0AInspired%20by%20the%20notion%20of%20abstract%20interpretation%20from%20the%20program%20analysis%0Aliterature%20that%20aims%20to%20develop%20approximate%20semantics%20for%20programs%2C%20we%20give%20a%0Aset%20of%20axioms%20that%20formally%20characterize%20a%20mechanistic%20interpretation%20as%20a%0Adescription%20that%20approximately%20captures%20the%20semantics%20of%20the%20neural%20network%0Aunder%20analysis%20in%20a%20compositional%20manner.%20We%20use%20these%20axioms%20to%20guide%20the%0Amechanistic%20interpretability%20analysis%20of%20a%20Transformer-based%20model%20trained%20to%0Asolve%20the%20well-known%202-SAT%20problem.%20We%20are%20able%20to%20reverse%20engineer%20the%0Aalgorithm%20learned%20by%20the%20model%20--%20the%20model%20first%20parses%20the%20input%20formulas%20and%0Athen%20evaluates%20their%20satisfiability%20via%20enumeration%20of%20different%20possible%0Avaluations%20of%20the%20Boolean%20input%20variables.%20We%20also%20present%20evidence%20to%20support%0Athat%20the%20mechanistic%20interpretation%20of%20the%20analyzed%20model%20indeed%20satisfies%20the%0Astated%20axioms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13594v1&entry.124074799=Read"},
{"title": "An efficient algorithm for solving linear equality-constrained LQR\n  problems", "author": "Jo\u00e3o Sousa-Pinto and Dominique Orban", "abstract": "  We present a new algorithm for solving linear-quadratic regulator (LQR)\nproblems with linear equality constraints, also known as constrained LQR (CLQR)\nproblems.\n  Our method's sequential runtime is linear in the number of stages and\nconstraints, and its parallel runtime is logarithmic in the number of stages.\n  The main technical contribution of this paper is the derivation of\nparallelizable techniques for eliminating the linear equality constraints while\npreserving the standard positive (semi-)definiteness requirements of LQR\nproblems.\n", "link": "http://arxiv.org/abs/2407.05433v2", "date": "2024-07-18", "relevancy": 1.5753, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.397}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20efficient%20algorithm%20for%20solving%20linear%20equality-constrained%20LQR%0A%20%20problems&body=Title%3A%20An%20efficient%20algorithm%20for%20solving%20linear%20equality-constrained%20LQR%0A%20%20problems%0AAuthor%3A%20Jo%C3%A3o%20Sousa-Pinto%20and%20Dominique%20Orban%0AAbstract%3A%20%20%20We%20present%20a%20new%20algorithm%20for%20solving%20linear-quadratic%20regulator%20%28LQR%29%0Aproblems%20with%20linear%20equality%20constraints%2C%20also%20known%20as%20constrained%20LQR%20%28CLQR%29%0Aproblems.%0A%20%20Our%20method%27s%20sequential%20runtime%20is%20linear%20in%20the%20number%20of%20stages%20and%0Aconstraints%2C%20and%20its%20parallel%20runtime%20is%20logarithmic%20in%20the%20number%20of%20stages.%0A%20%20The%20main%20technical%20contribution%20of%20this%20paper%20is%20the%20derivation%20of%0Aparallelizable%20techniques%20for%20eliminating%20the%20linear%20equality%20constraints%20while%0Apreserving%20the%20standard%20positive%20%28semi-%29definiteness%20requirements%20of%20LQR%0Aproblems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05433v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520efficient%2520algorithm%2520for%2520solving%2520linear%2520equality-constrained%2520LQR%250A%2520%2520problems%26entry.906535625%3DJo%25C3%25A3o%2520Sousa-Pinto%2520and%2520Dominique%2520Orban%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520algorithm%2520for%2520solving%2520linear-quadratic%2520regulator%2520%2528LQR%2529%250Aproblems%2520with%2520linear%2520equality%2520constraints%252C%2520also%2520known%2520as%2520constrained%2520LQR%2520%2528CLQR%2529%250Aproblems.%250A%2520%2520Our%2520method%2527s%2520sequential%2520runtime%2520is%2520linear%2520in%2520the%2520number%2520of%2520stages%2520and%250Aconstraints%252C%2520and%2520its%2520parallel%2520runtime%2520is%2520logarithmic%2520in%2520the%2520number%2520of%2520stages.%250A%2520%2520The%2520main%2520technical%2520contribution%2520of%2520this%2520paper%2520is%2520the%2520derivation%2520of%250Aparallelizable%2520techniques%2520for%2520eliminating%2520the%2520linear%2520equality%2520constraints%2520while%250Apreserving%2520the%2520standard%2520positive%2520%2528semi-%2529definiteness%2520requirements%2520of%2520LQR%250Aproblems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05433v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20efficient%20algorithm%20for%20solving%20linear%20equality-constrained%20LQR%0A%20%20problems&entry.906535625=Jo%C3%A3o%20Sousa-Pinto%20and%20Dominique%20Orban&entry.1292438233=%20%20We%20present%20a%20new%20algorithm%20for%20solving%20linear-quadratic%20regulator%20%28LQR%29%0Aproblems%20with%20linear%20equality%20constraints%2C%20also%20known%20as%20constrained%20LQR%20%28CLQR%29%0Aproblems.%0A%20%20Our%20method%27s%20sequential%20runtime%20is%20linear%20in%20the%20number%20of%20stages%20and%0Aconstraints%2C%20and%20its%20parallel%20runtime%20is%20logarithmic%20in%20the%20number%20of%20stages.%0A%20%20The%20main%20technical%20contribution%20of%20this%20paper%20is%20the%20derivation%20of%0Aparallelizable%20techniques%20for%20eliminating%20the%20linear%20equality%20constraints%20while%0Apreserving%20the%20standard%20positive%20%28semi-%29definiteness%20requirements%20of%20LQR%0Aproblems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05433v2&entry.124074799=Read"},
{"title": "Capturing Style in Author and Document Representation", "author": "Enzo Terreau and Antoine Gourru and Julien Velcin", "abstract": "  A wide range of Deep Natural Language Processing (NLP) models integrates\ncontinuous and low dimensional representations of words and documents.\nSurprisingly, very few models study representation learning for authors. These\nrepresentations can be used for many NLP tasks, such as author identification\nand classification, or in recommendation systems. A strong limitation of\nexisting works is that they do not explicitly capture writing style, making\nthem hardly applicable to literary data. We therefore propose a new\narchitecture based on Variational Information Bottleneck (VIB) that learns\nembeddings for both authors and documents with a stylistic constraint. Our\nmodel fine-tunes a pre-trained document encoder. We stimulate the detection of\nwriting style by adding predefined stylistic features making the representation\naxis interpretable with respect to writing style indicators. We evaluate our\nmethod on three datasets: a literary corpus extracted from the Gutenberg\nProject, the Blog Authorship Corpus and IMDb62, for which we show that it\nmatches or outperforms strong/recent baselines in authorship attribution while\ncapturing much more accurately the authors stylistic aspects.\n", "link": "http://arxiv.org/abs/2407.13358v1", "date": "2024-07-18", "relevancy": 1.5371, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5277}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5091}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capturing%20Style%20in%20Author%20and%20Document%20Representation&body=Title%3A%20Capturing%20Style%20in%20Author%20and%20Document%20Representation%0AAuthor%3A%20Enzo%20Terreau%20and%20Antoine%20Gourru%20and%20Julien%20Velcin%0AAbstract%3A%20%20%20A%20wide%20range%20of%20Deep%20Natural%20Language%20Processing%20%28NLP%29%20models%20integrates%0Acontinuous%20and%20low%20dimensional%20representations%20of%20words%20and%20documents.%0ASurprisingly%2C%20very%20few%20models%20study%20representation%20learning%20for%20authors.%20These%0Arepresentations%20can%20be%20used%20for%20many%20NLP%20tasks%2C%20such%20as%20author%20identification%0Aand%20classification%2C%20or%20in%20recommendation%20systems.%20A%20strong%20limitation%20of%0Aexisting%20works%20is%20that%20they%20do%20not%20explicitly%20capture%20writing%20style%2C%20making%0Athem%20hardly%20applicable%20to%20literary%20data.%20We%20therefore%20propose%20a%20new%0Aarchitecture%20based%20on%20Variational%20Information%20Bottleneck%20%28VIB%29%20that%20learns%0Aembeddings%20for%20both%20authors%20and%20documents%20with%20a%20stylistic%20constraint.%20Our%0Amodel%20fine-tunes%20a%20pre-trained%20document%20encoder.%20We%20stimulate%20the%20detection%20of%0Awriting%20style%20by%20adding%20predefined%20stylistic%20features%20making%20the%20representation%0Aaxis%20interpretable%20with%20respect%20to%20writing%20style%20indicators.%20We%20evaluate%20our%0Amethod%20on%20three%20datasets%3A%20a%20literary%20corpus%20extracted%20from%20the%20Gutenberg%0AProject%2C%20the%20Blog%20Authorship%20Corpus%20and%20IMDb62%2C%20for%20which%20we%20show%20that%20it%0Amatches%20or%20outperforms%20strong/recent%20baselines%20in%20authorship%20attribution%20while%0Acapturing%20much%20more%20accurately%20the%20authors%20stylistic%20aspects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapturing%2520Style%2520in%2520Author%2520and%2520Document%2520Representation%26entry.906535625%3DEnzo%2520Terreau%2520and%2520Antoine%2520Gourru%2520and%2520Julien%2520Velcin%26entry.1292438233%3D%2520%2520A%2520wide%2520range%2520of%2520Deep%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520models%2520integrates%250Acontinuous%2520and%2520low%2520dimensional%2520representations%2520of%2520words%2520and%2520documents.%250ASurprisingly%252C%2520very%2520few%2520models%2520study%2520representation%2520learning%2520for%2520authors.%2520These%250Arepresentations%2520can%2520be%2520used%2520for%2520many%2520NLP%2520tasks%252C%2520such%2520as%2520author%2520identification%250Aand%2520classification%252C%2520or%2520in%2520recommendation%2520systems.%2520A%2520strong%2520limitation%2520of%250Aexisting%2520works%2520is%2520that%2520they%2520do%2520not%2520explicitly%2520capture%2520writing%2520style%252C%2520making%250Athem%2520hardly%2520applicable%2520to%2520literary%2520data.%2520We%2520therefore%2520propose%2520a%2520new%250Aarchitecture%2520based%2520on%2520Variational%2520Information%2520Bottleneck%2520%2528VIB%2529%2520that%2520learns%250Aembeddings%2520for%2520both%2520authors%2520and%2520documents%2520with%2520a%2520stylistic%2520constraint.%2520Our%250Amodel%2520fine-tunes%2520a%2520pre-trained%2520document%2520encoder.%2520We%2520stimulate%2520the%2520detection%2520of%250Awriting%2520style%2520by%2520adding%2520predefined%2520stylistic%2520features%2520making%2520the%2520representation%250Aaxis%2520interpretable%2520with%2520respect%2520to%2520writing%2520style%2520indicators.%2520We%2520evaluate%2520our%250Amethod%2520on%2520three%2520datasets%253A%2520a%2520literary%2520corpus%2520extracted%2520from%2520the%2520Gutenberg%250AProject%252C%2520the%2520Blog%2520Authorship%2520Corpus%2520and%2520IMDb62%252C%2520for%2520which%2520we%2520show%2520that%2520it%250Amatches%2520or%2520outperforms%2520strong/recent%2520baselines%2520in%2520authorship%2520attribution%2520while%250Acapturing%2520much%2520more%2520accurately%2520the%2520authors%2520stylistic%2520aspects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capturing%20Style%20in%20Author%20and%20Document%20Representation&entry.906535625=Enzo%20Terreau%20and%20Antoine%20Gourru%20and%20Julien%20Velcin&entry.1292438233=%20%20A%20wide%20range%20of%20Deep%20Natural%20Language%20Processing%20%28NLP%29%20models%20integrates%0Acontinuous%20and%20low%20dimensional%20representations%20of%20words%20and%20documents.%0ASurprisingly%2C%20very%20few%20models%20study%20representation%20learning%20for%20authors.%20These%0Arepresentations%20can%20be%20used%20for%20many%20NLP%20tasks%2C%20such%20as%20author%20identification%0Aand%20classification%2C%20or%20in%20recommendation%20systems.%20A%20strong%20limitation%20of%0Aexisting%20works%20is%20that%20they%20do%20not%20explicitly%20capture%20writing%20style%2C%20making%0Athem%20hardly%20applicable%20to%20literary%20data.%20We%20therefore%20propose%20a%20new%0Aarchitecture%20based%20on%20Variational%20Information%20Bottleneck%20%28VIB%29%20that%20learns%0Aembeddings%20for%20both%20authors%20and%20documents%20with%20a%20stylistic%20constraint.%20Our%0Amodel%20fine-tunes%20a%20pre-trained%20document%20encoder.%20We%20stimulate%20the%20detection%20of%0Awriting%20style%20by%20adding%20predefined%20stylistic%20features%20making%20the%20representation%0Aaxis%20interpretable%20with%20respect%20to%20writing%20style%20indicators.%20We%20evaluate%20our%0Amethod%20on%20three%20datasets%3A%20a%20literary%20corpus%20extracted%20from%20the%20Gutenberg%0AProject%2C%20the%20Blog%20Authorship%20Corpus%20and%20IMDb62%2C%20for%20which%20we%20show%20that%20it%0Amatches%20or%20outperforms%20strong/recent%20baselines%20in%20authorship%20attribution%20while%0Acapturing%20much%20more%20accurately%20the%20authors%20stylistic%20aspects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13358v1&entry.124074799=Read"},
{"title": "Realizable $H$-Consistent and Bayes-Consistent Loss Functions for\n  Learning to Defer", "author": "Anqi Mao and Mehryar Mohri and Yutao Zhong", "abstract": "  We present a comprehensive study of surrogate loss functions for learning to\ndefer. We introduce a broad family of surrogate losses, parameterized by a\nnon-increasing function $\\Psi$, and establish their realizable $H$-consistency\nunder mild conditions. For cost functions based on classification error, we\nfurther show that these losses admit $H$-consistency bounds when the hypothesis\nset is symmetric and complete, a property satisfied by common neural network\nand linear function hypothesis sets. Our results also resolve an open question\nraised in previous work (Mozannar et al., 2023) by proving the realizable\n$H$-consistency and Bayes-consistency of a specific surrogate loss.\nFurthermore, we identify choices of $\\Psi$ that lead to $H$-consistent\nsurrogate losses for any general cost function, thus achieving\nBayes-consistency, realizable $H$-consistency, and $H$-consistency bounds\nsimultaneously. We also investigate the relationship between $H$-consistency\nbounds and realizable $H$-consistency in learning to defer, highlighting key\ndifferences from standard classification. Finally, we empirically evaluate our\nproposed surrogate losses and compare them with existing baselines.\n", "link": "http://arxiv.org/abs/2407.13732v1", "date": "2024-07-18", "relevancy": 1.2946, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4789}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4216}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realizable%20%24H%24-Consistent%20and%20Bayes-Consistent%20Loss%20Functions%20for%0A%20%20Learning%20to%20Defer&body=Title%3A%20Realizable%20%24H%24-Consistent%20and%20Bayes-Consistent%20Loss%20Functions%20for%0A%20%20Learning%20to%20Defer%0AAuthor%3A%20Anqi%20Mao%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong%0AAbstract%3A%20%20%20We%20present%20a%20comprehensive%20study%20of%20surrogate%20loss%20functions%20for%20learning%20to%0Adefer.%20We%20introduce%20a%20broad%20family%20of%20surrogate%20losses%2C%20parameterized%20by%20a%0Anon-increasing%20function%20%24%5CPsi%24%2C%20and%20establish%20their%20realizable%20%24H%24-consistency%0Aunder%20mild%20conditions.%20For%20cost%20functions%20based%20on%20classification%20error%2C%20we%0Afurther%20show%20that%20these%20losses%20admit%20%24H%24-consistency%20bounds%20when%20the%20hypothesis%0Aset%20is%20symmetric%20and%20complete%2C%20a%20property%20satisfied%20by%20common%20neural%20network%0Aand%20linear%20function%20hypothesis%20sets.%20Our%20results%20also%20resolve%20an%20open%20question%0Araised%20in%20previous%20work%20%28Mozannar%20et%20al.%2C%202023%29%20by%20proving%20the%20realizable%0A%24H%24-consistency%20and%20Bayes-consistency%20of%20a%20specific%20surrogate%20loss.%0AFurthermore%2C%20we%20identify%20choices%20of%20%24%5CPsi%24%20that%20lead%20to%20%24H%24-consistent%0Asurrogate%20losses%20for%20any%20general%20cost%20function%2C%20thus%20achieving%0ABayes-consistency%2C%20realizable%20%24H%24-consistency%2C%20and%20%24H%24-consistency%20bounds%0Asimultaneously.%20We%20also%20investigate%20the%20relationship%20between%20%24H%24-consistency%0Abounds%20and%20realizable%20%24H%24-consistency%20in%20learning%20to%20defer%2C%20highlighting%20key%0Adifferences%20from%20standard%20classification.%20Finally%2C%20we%20empirically%20evaluate%20our%0Aproposed%20surrogate%20losses%20and%20compare%20them%20with%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealizable%2520%2524H%2524-Consistent%2520and%2520Bayes-Consistent%2520Loss%2520Functions%2520for%250A%2520%2520Learning%2520to%2520Defer%26entry.906535625%3DAnqi%2520Mao%2520and%2520Mehryar%2520Mohri%2520and%2520Yutao%2520Zhong%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520comprehensive%2520study%2520of%2520surrogate%2520loss%2520functions%2520for%2520learning%2520to%250Adefer.%2520We%2520introduce%2520a%2520broad%2520family%2520of%2520surrogate%2520losses%252C%2520parameterized%2520by%2520a%250Anon-increasing%2520function%2520%2524%255CPsi%2524%252C%2520and%2520establish%2520their%2520realizable%2520%2524H%2524-consistency%250Aunder%2520mild%2520conditions.%2520For%2520cost%2520functions%2520based%2520on%2520classification%2520error%252C%2520we%250Afurther%2520show%2520that%2520these%2520losses%2520admit%2520%2524H%2524-consistency%2520bounds%2520when%2520the%2520hypothesis%250Aset%2520is%2520symmetric%2520and%2520complete%252C%2520a%2520property%2520satisfied%2520by%2520common%2520neural%2520network%250Aand%2520linear%2520function%2520hypothesis%2520sets.%2520Our%2520results%2520also%2520resolve%2520an%2520open%2520question%250Araised%2520in%2520previous%2520work%2520%2528Mozannar%2520et%2520al.%252C%25202023%2529%2520by%2520proving%2520the%2520realizable%250A%2524H%2524-consistency%2520and%2520Bayes-consistency%2520of%2520a%2520specific%2520surrogate%2520loss.%250AFurthermore%252C%2520we%2520identify%2520choices%2520of%2520%2524%255CPsi%2524%2520that%2520lead%2520to%2520%2524H%2524-consistent%250Asurrogate%2520losses%2520for%2520any%2520general%2520cost%2520function%252C%2520thus%2520achieving%250ABayes-consistency%252C%2520realizable%2520%2524H%2524-consistency%252C%2520and%2520%2524H%2524-consistency%2520bounds%250Asimultaneously.%2520We%2520also%2520investigate%2520the%2520relationship%2520between%2520%2524H%2524-consistency%250Abounds%2520and%2520realizable%2520%2524H%2524-consistency%2520in%2520learning%2520to%2520defer%252C%2520highlighting%2520key%250Adifferences%2520from%2520standard%2520classification.%2520Finally%252C%2520we%2520empirically%2520evaluate%2520our%250Aproposed%2520surrogate%2520losses%2520and%2520compare%2520them%2520with%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realizable%20%24H%24-Consistent%20and%20Bayes-Consistent%20Loss%20Functions%20for%0A%20%20Learning%20to%20Defer&entry.906535625=Anqi%20Mao%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong&entry.1292438233=%20%20We%20present%20a%20comprehensive%20study%20of%20surrogate%20loss%20functions%20for%20learning%20to%0Adefer.%20We%20introduce%20a%20broad%20family%20of%20surrogate%20losses%2C%20parameterized%20by%20a%0Anon-increasing%20function%20%24%5CPsi%24%2C%20and%20establish%20their%20realizable%20%24H%24-consistency%0Aunder%20mild%20conditions.%20For%20cost%20functions%20based%20on%20classification%20error%2C%20we%0Afurther%20show%20that%20these%20losses%20admit%20%24H%24-consistency%20bounds%20when%20the%20hypothesis%0Aset%20is%20symmetric%20and%20complete%2C%20a%20property%20satisfied%20by%20common%20neural%20network%0Aand%20linear%20function%20hypothesis%20sets.%20Our%20results%20also%20resolve%20an%20open%20question%0Araised%20in%20previous%20work%20%28Mozannar%20et%20al.%2C%202023%29%20by%20proving%20the%20realizable%0A%24H%24-consistency%20and%20Bayes-consistency%20of%20a%20specific%20surrogate%20loss.%0AFurthermore%2C%20we%20identify%20choices%20of%20%24%5CPsi%24%20that%20lead%20to%20%24H%24-consistent%0Asurrogate%20losses%20for%20any%20general%20cost%20function%2C%20thus%20achieving%0ABayes-consistency%2C%20realizable%20%24H%24-consistency%2C%20and%20%24H%24-consistency%20bounds%0Asimultaneously.%20We%20also%20investigate%20the%20relationship%20between%20%24H%24-consistency%0Abounds%20and%20realizable%20%24H%24-consistency%20in%20learning%20to%20defer%2C%20highlighting%20key%0Adifferences%20from%20standard%20classification.%20Finally%2C%20we%20empirically%20evaluate%20our%0Aproposed%20surrogate%20losses%20and%20compare%20them%20with%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13732v1&entry.124074799=Read"},
{"title": "PetFace: A Large-Scale Dataset and Benchmark for Animal Identification", "author": "Risa Shinoda and Kaede Shiohara", "abstract": "  Automated animal face identification plays a crucial role in the monitoring\nof behaviors, conducting of surveys, and finding of lost animals. Despite the\nadvancements in human face identification, the lack of datasets and benchmarks\nin the animal domain has impeded progress. In this paper, we introduce the\nPetFace dataset, a comprehensive resource for animal face identification\nencompassing 257,484 unique individuals across 13 animal families and 319 breed\ncategories, including both experimental and pet animals. This large-scale\ncollection of individuals facilitates the investigation of unseen animal face\nverification, an area that has not been sufficiently explored in existing\ndatasets due to the limited number of individuals. Moreover, PetFace also has\nfine-grained annotations such as sex, breed, color, and pattern. We provide\nmultiple benchmarks including re-identification for seen individuals and\nverification for unseen individuals. The models trained on our dataset\noutperform those trained on prior datasets, even for detailed breed variations\nand unseen animal families. Our result also indicates that there is some room\nto improve the performance of integrated identification on multiple animal\nfamilies. We hope the PetFace dataset will facilitate animal face\nidentification and encourage the development of non-invasive animal automatic\nidentification methods.\n", "link": "http://arxiv.org/abs/2407.13555v1", "date": "2024-07-18", "relevancy": 1.849, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4727}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.463}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PetFace%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Animal%20Identification&body=Title%3A%20PetFace%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Animal%20Identification%0AAuthor%3A%20Risa%20Shinoda%20and%20Kaede%20Shiohara%0AAbstract%3A%20%20%20Automated%20animal%20face%20identification%20plays%20a%20crucial%20role%20in%20the%20monitoring%0Aof%20behaviors%2C%20conducting%20of%20surveys%2C%20and%20finding%20of%20lost%20animals.%20Despite%20the%0Aadvancements%20in%20human%20face%20identification%2C%20the%20lack%20of%20datasets%20and%20benchmarks%0Ain%20the%20animal%20domain%20has%20impeded%20progress.%20In%20this%20paper%2C%20we%20introduce%20the%0APetFace%20dataset%2C%20a%20comprehensive%20resource%20for%20animal%20face%20identification%0Aencompassing%20257%2C484%20unique%20individuals%20across%2013%20animal%20families%20and%20319%20breed%0Acategories%2C%20including%20both%20experimental%20and%20pet%20animals.%20This%20large-scale%0Acollection%20of%20individuals%20facilitates%20the%20investigation%20of%20unseen%20animal%20face%0Averification%2C%20an%20area%20that%20has%20not%20been%20sufficiently%20explored%20in%20existing%0Adatasets%20due%20to%20the%20limited%20number%20of%20individuals.%20Moreover%2C%20PetFace%20also%20has%0Afine-grained%20annotations%20such%20as%20sex%2C%20breed%2C%20color%2C%20and%20pattern.%20We%20provide%0Amultiple%20benchmarks%20including%20re-identification%20for%20seen%20individuals%20and%0Averification%20for%20unseen%20individuals.%20The%20models%20trained%20on%20our%20dataset%0Aoutperform%20those%20trained%20on%20prior%20datasets%2C%20even%20for%20detailed%20breed%20variations%0Aand%20unseen%20animal%20families.%20Our%20result%20also%20indicates%20that%20there%20is%20some%20room%0Ato%20improve%20the%20performance%20of%20integrated%20identification%20on%20multiple%20animal%0Afamilies.%20We%20hope%20the%20PetFace%20dataset%20will%20facilitate%20animal%20face%0Aidentification%20and%20encourage%20the%20development%20of%20non-invasive%20animal%20automatic%0Aidentification%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPetFace%253A%2520A%2520Large-Scale%2520Dataset%2520and%2520Benchmark%2520for%2520Animal%2520Identification%26entry.906535625%3DRisa%2520Shinoda%2520and%2520Kaede%2520Shiohara%26entry.1292438233%3D%2520%2520Automated%2520animal%2520face%2520identification%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520monitoring%250Aof%2520behaviors%252C%2520conducting%2520of%2520surveys%252C%2520and%2520finding%2520of%2520lost%2520animals.%2520Despite%2520the%250Aadvancements%2520in%2520human%2520face%2520identification%252C%2520the%2520lack%2520of%2520datasets%2520and%2520benchmarks%250Ain%2520the%2520animal%2520domain%2520has%2520impeded%2520progress.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%250APetFace%2520dataset%252C%2520a%2520comprehensive%2520resource%2520for%2520animal%2520face%2520identification%250Aencompassing%2520257%252C484%2520unique%2520individuals%2520across%252013%2520animal%2520families%2520and%2520319%2520breed%250Acategories%252C%2520including%2520both%2520experimental%2520and%2520pet%2520animals.%2520This%2520large-scale%250Acollection%2520of%2520individuals%2520facilitates%2520the%2520investigation%2520of%2520unseen%2520animal%2520face%250Averification%252C%2520an%2520area%2520that%2520has%2520not%2520been%2520sufficiently%2520explored%2520in%2520existing%250Adatasets%2520due%2520to%2520the%2520limited%2520number%2520of%2520individuals.%2520Moreover%252C%2520PetFace%2520also%2520has%250Afine-grained%2520annotations%2520such%2520as%2520sex%252C%2520breed%252C%2520color%252C%2520and%2520pattern.%2520We%2520provide%250Amultiple%2520benchmarks%2520including%2520re-identification%2520for%2520seen%2520individuals%2520and%250Averification%2520for%2520unseen%2520individuals.%2520The%2520models%2520trained%2520on%2520our%2520dataset%250Aoutperform%2520those%2520trained%2520on%2520prior%2520datasets%252C%2520even%2520for%2520detailed%2520breed%2520variations%250Aand%2520unseen%2520animal%2520families.%2520Our%2520result%2520also%2520indicates%2520that%2520there%2520is%2520some%2520room%250Ato%2520improve%2520the%2520performance%2520of%2520integrated%2520identification%2520on%2520multiple%2520animal%250Afamilies.%2520We%2520hope%2520the%2520PetFace%2520dataset%2520will%2520facilitate%2520animal%2520face%250Aidentification%2520and%2520encourage%2520the%2520development%2520of%2520non-invasive%2520animal%2520automatic%250Aidentification%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PetFace%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Animal%20Identification&entry.906535625=Risa%20Shinoda%20and%20Kaede%20Shiohara&entry.1292438233=%20%20Automated%20animal%20face%20identification%20plays%20a%20crucial%20role%20in%20the%20monitoring%0Aof%20behaviors%2C%20conducting%20of%20surveys%2C%20and%20finding%20of%20lost%20animals.%20Despite%20the%0Aadvancements%20in%20human%20face%20identification%2C%20the%20lack%20of%20datasets%20and%20benchmarks%0Ain%20the%20animal%20domain%20has%20impeded%20progress.%20In%20this%20paper%2C%20we%20introduce%20the%0APetFace%20dataset%2C%20a%20comprehensive%20resource%20for%20animal%20face%20identification%0Aencompassing%20257%2C484%20unique%20individuals%20across%2013%20animal%20families%20and%20319%20breed%0Acategories%2C%20including%20both%20experimental%20and%20pet%20animals.%20This%20large-scale%0Acollection%20of%20individuals%20facilitates%20the%20investigation%20of%20unseen%20animal%20face%0Averification%2C%20an%20area%20that%20has%20not%20been%20sufficiently%20explored%20in%20existing%0Adatasets%20due%20to%20the%20limited%20number%20of%20individuals.%20Moreover%2C%20PetFace%20also%20has%0Afine-grained%20annotations%20such%20as%20sex%2C%20breed%2C%20color%2C%20and%20pattern.%20We%20provide%0Amultiple%20benchmarks%20including%20re-identification%20for%20seen%20individuals%20and%0Averification%20for%20unseen%20individuals.%20The%20models%20trained%20on%20our%20dataset%0Aoutperform%20those%20trained%20on%20prior%20datasets%2C%20even%20for%20detailed%20breed%20variations%0Aand%20unseen%20animal%20families.%20Our%20result%20also%20indicates%20that%20there%20is%20some%20room%0Ato%20improve%20the%20performance%20of%20integrated%20identification%20on%20multiple%20animal%0Afamilies.%20We%20hope%20the%20PetFace%20dataset%20will%20facilitate%20animal%20face%0Aidentification%20and%20encourage%20the%20development%20of%20non-invasive%20animal%20automatic%0Aidentification%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13555v1&entry.124074799=Read"},
{"title": "Non-Asymptotic Uncertainty Quantification in High-Dimensional Learning", "author": "Frederik Hoppe and Claudio Mayrink Verdun and Hannah Laus and Felix Krahmer and Holger Rauhut", "abstract": "  Uncertainty quantification (UQ) is a crucial but challenging task in many\nhigh-dimensional regression or learning problems to increase the confidence of\na given predictor. We develop a new data-driven approach for UQ in regression\nthat applies both to classical regression approaches such as the LASSO as well\nas to neural networks. One of the most notable UQ techniques is the debiased\nLASSO, which modifies the LASSO to allow for the construction of asymptotic\nconfidence intervals by decomposing the estimation error into a Gaussian and an\nasymptotically vanishing bias component. However, in real-world problems with\nfinite-dimensional data, the bias term is often too significant to be\nneglected, resulting in overly narrow confidence intervals. Our work rigorously\naddresses this issue and derives a data-driven adjustment that corrects the\nconfidence intervals for a large class of predictors by estimating the means\nand variances of the bias terms from training data, exploiting high-dimensional\nconcentration phenomena. This gives rise to non-asymptotic confidence\nintervals, which can help avoid overestimating uncertainty in critical\napplications such as MRI diagnosis. Importantly, our analysis extends beyond\nsparse regression to data-driven predictors like neural networks, enhancing the\nreliability of model-based deep learning. Our findings bridge the gap between\nestablished theory and the practical applicability of such debiased methods.\n", "link": "http://arxiv.org/abs/2407.13666v1", "date": "2024-07-18", "relevancy": 1.6189, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5772}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5418}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Asymptotic%20Uncertainty%20Quantification%20in%20High-Dimensional%20Learning&body=Title%3A%20Non-Asymptotic%20Uncertainty%20Quantification%20in%20High-Dimensional%20Learning%0AAuthor%3A%20Frederik%20Hoppe%20and%20Claudio%20Mayrink%20Verdun%20and%20Hannah%20Laus%20and%20Felix%20Krahmer%20and%20Holger%20Rauhut%0AAbstract%3A%20%20%20Uncertainty%20quantification%20%28UQ%29%20is%20a%20crucial%20but%20challenging%20task%20in%20many%0Ahigh-dimensional%20regression%20or%20learning%20problems%20to%20increase%20the%20confidence%20of%0Aa%20given%20predictor.%20We%20develop%20a%20new%20data-driven%20approach%20for%20UQ%20in%20regression%0Athat%20applies%20both%20to%20classical%20regression%20approaches%20such%20as%20the%20LASSO%20as%20well%0Aas%20to%20neural%20networks.%20One%20of%20the%20most%20notable%20UQ%20techniques%20is%20the%20debiased%0ALASSO%2C%20which%20modifies%20the%20LASSO%20to%20allow%20for%20the%20construction%20of%20asymptotic%0Aconfidence%20intervals%20by%20decomposing%20the%20estimation%20error%20into%20a%20Gaussian%20and%20an%0Aasymptotically%20vanishing%20bias%20component.%20However%2C%20in%20real-world%20problems%20with%0Afinite-dimensional%20data%2C%20the%20bias%20term%20is%20often%20too%20significant%20to%20be%0Aneglected%2C%20resulting%20in%20overly%20narrow%20confidence%20intervals.%20Our%20work%20rigorously%0Aaddresses%20this%20issue%20and%20derives%20a%20data-driven%20adjustment%20that%20corrects%20the%0Aconfidence%20intervals%20for%20a%20large%20class%20of%20predictors%20by%20estimating%20the%20means%0Aand%20variances%20of%20the%20bias%20terms%20from%20training%20data%2C%20exploiting%20high-dimensional%0Aconcentration%20phenomena.%20This%20gives%20rise%20to%20non-asymptotic%20confidence%0Aintervals%2C%20which%20can%20help%20avoid%20overestimating%20uncertainty%20in%20critical%0Aapplications%20such%20as%20MRI%20diagnosis.%20Importantly%2C%20our%20analysis%20extends%20beyond%0Asparse%20regression%20to%20data-driven%20predictors%20like%20neural%20networks%2C%20enhancing%20the%0Areliability%20of%20model-based%20deep%20learning.%20Our%20findings%20bridge%20the%20gap%20between%0Aestablished%20theory%20and%20the%20practical%20applicability%20of%20such%20debiased%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Asymptotic%2520Uncertainty%2520Quantification%2520in%2520High-Dimensional%2520Learning%26entry.906535625%3DFrederik%2520Hoppe%2520and%2520Claudio%2520Mayrink%2520Verdun%2520and%2520Hannah%2520Laus%2520and%2520Felix%2520Krahmer%2520and%2520Holger%2520Rauhut%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520%2528UQ%2529%2520is%2520a%2520crucial%2520but%2520challenging%2520task%2520in%2520many%250Ahigh-dimensional%2520regression%2520or%2520learning%2520problems%2520to%2520increase%2520the%2520confidence%2520of%250Aa%2520given%2520predictor.%2520We%2520develop%2520a%2520new%2520data-driven%2520approach%2520for%2520UQ%2520in%2520regression%250Athat%2520applies%2520both%2520to%2520classical%2520regression%2520approaches%2520such%2520as%2520the%2520LASSO%2520as%2520well%250Aas%2520to%2520neural%2520networks.%2520One%2520of%2520the%2520most%2520notable%2520UQ%2520techniques%2520is%2520the%2520debiased%250ALASSO%252C%2520which%2520modifies%2520the%2520LASSO%2520to%2520allow%2520for%2520the%2520construction%2520of%2520asymptotic%250Aconfidence%2520intervals%2520by%2520decomposing%2520the%2520estimation%2520error%2520into%2520a%2520Gaussian%2520and%2520an%250Aasymptotically%2520vanishing%2520bias%2520component.%2520However%252C%2520in%2520real-world%2520problems%2520with%250Afinite-dimensional%2520data%252C%2520the%2520bias%2520term%2520is%2520often%2520too%2520significant%2520to%2520be%250Aneglected%252C%2520resulting%2520in%2520overly%2520narrow%2520confidence%2520intervals.%2520Our%2520work%2520rigorously%250Aaddresses%2520this%2520issue%2520and%2520derives%2520a%2520data-driven%2520adjustment%2520that%2520corrects%2520the%250Aconfidence%2520intervals%2520for%2520a%2520large%2520class%2520of%2520predictors%2520by%2520estimating%2520the%2520means%250Aand%2520variances%2520of%2520the%2520bias%2520terms%2520from%2520training%2520data%252C%2520exploiting%2520high-dimensional%250Aconcentration%2520phenomena.%2520This%2520gives%2520rise%2520to%2520non-asymptotic%2520confidence%250Aintervals%252C%2520which%2520can%2520help%2520avoid%2520overestimating%2520uncertainty%2520in%2520critical%250Aapplications%2520such%2520as%2520MRI%2520diagnosis.%2520Importantly%252C%2520our%2520analysis%2520extends%2520beyond%250Asparse%2520regression%2520to%2520data-driven%2520predictors%2520like%2520neural%2520networks%252C%2520enhancing%2520the%250Areliability%2520of%2520model-based%2520deep%2520learning.%2520Our%2520findings%2520bridge%2520the%2520gap%2520between%250Aestablished%2520theory%2520and%2520the%2520practical%2520applicability%2520of%2520such%2520debiased%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Asymptotic%20Uncertainty%20Quantification%20in%20High-Dimensional%20Learning&entry.906535625=Frederik%20Hoppe%20and%20Claudio%20Mayrink%20Verdun%20and%20Hannah%20Laus%20and%20Felix%20Krahmer%20and%20Holger%20Rauhut&entry.1292438233=%20%20Uncertainty%20quantification%20%28UQ%29%20is%20a%20crucial%20but%20challenging%20task%20in%20many%0Ahigh-dimensional%20regression%20or%20learning%20problems%20to%20increase%20the%20confidence%20of%0Aa%20given%20predictor.%20We%20develop%20a%20new%20data-driven%20approach%20for%20UQ%20in%20regression%0Athat%20applies%20both%20to%20classical%20regression%20approaches%20such%20as%20the%20LASSO%20as%20well%0Aas%20to%20neural%20networks.%20One%20of%20the%20most%20notable%20UQ%20techniques%20is%20the%20debiased%0ALASSO%2C%20which%20modifies%20the%20LASSO%20to%20allow%20for%20the%20construction%20of%20asymptotic%0Aconfidence%20intervals%20by%20decomposing%20the%20estimation%20error%20into%20a%20Gaussian%20and%20an%0Aasymptotically%20vanishing%20bias%20component.%20However%2C%20in%20real-world%20problems%20with%0Afinite-dimensional%20data%2C%20the%20bias%20term%20is%20often%20too%20significant%20to%20be%0Aneglected%2C%20resulting%20in%20overly%20narrow%20confidence%20intervals.%20Our%20work%20rigorously%0Aaddresses%20this%20issue%20and%20derives%20a%20data-driven%20adjustment%20that%20corrects%20the%0Aconfidence%20intervals%20for%20a%20large%20class%20of%20predictors%20by%20estimating%20the%20means%0Aand%20variances%20of%20the%20bias%20terms%20from%20training%20data%2C%20exploiting%20high-dimensional%0Aconcentration%20phenomena.%20This%20gives%20rise%20to%20non-asymptotic%20confidence%0Aintervals%2C%20which%20can%20help%20avoid%20overestimating%20uncertainty%20in%20critical%0Aapplications%20such%20as%20MRI%20diagnosis.%20Importantly%2C%20our%20analysis%20extends%20beyond%0Asparse%20regression%20to%20data-driven%20predictors%20like%20neural%20networks%2C%20enhancing%20the%0Areliability%20of%20model-based%20deep%20learning.%20Our%20findings%20bridge%20the%20gap%20between%0Aestablished%20theory%20and%20the%20practical%20applicability%20of%20such%20debiased%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13666v1&entry.124074799=Read"},
{"title": "Model Predictive Path Integral Methods with Reach-Avoid Tasks and\n  Control Barrier Functions", "author": "Hardik Parwana and Mitchell Black and Georgios Fainekos and Bardh Hoxha and Hideki Okamoto and Danil Prokhorov", "abstract": "  The rapid advancement of robotics necessitates robust tools for developing\nand testing safe control architectures in dynamic and uncertain environments.\nEnsuring safety and reliability in robotics, especially in safety-critical\napplications, is crucial, driving substantial industrial and academic efforts.\nIn this context, we extend CBFkit, a Python/ROS2 toolbox, which now\nincorporates a planner using reach-avoid specifications as a cost function.\nThis integration with the Model Predictive Path Integral (MPPI) controllers\nenables the toolbox to satisfy complex tasks while ensuring formal safety\nguarantees under various sources of uncertainty using Control Barrier Functions\n(CBFs). CBFkit is optimized for speed using JAX for automatic differentiation\nand jaxopt for quadratic program solving. The toolbox supports various robotic\napplications, including autonomous navigation, human-robot interaction, and\nmulti-robot coordination. The toolbox also offers a comprehensive library of\nplanner, controller, sensor, and estimator implementations. Through a series of\nexamples, we demonstrate the enhanced capabilities of CBFkit in different\nrobotic scenarios.\n", "link": "http://arxiv.org/abs/2407.13693v1", "date": "2024-07-18", "relevancy": 1.5472, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5834}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5154}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Predictive%20Path%20Integral%20Methods%20with%20Reach-Avoid%20Tasks%20and%0A%20%20Control%20Barrier%20Functions&body=Title%3A%20Model%20Predictive%20Path%20Integral%20Methods%20with%20Reach-Avoid%20Tasks%20and%0A%20%20Control%20Barrier%20Functions%0AAuthor%3A%20Hardik%20Parwana%20and%20Mitchell%20Black%20and%20Georgios%20Fainekos%20and%20Bardh%20Hoxha%20and%20Hideki%20Okamoto%20and%20Danil%20Prokhorov%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20robotics%20necessitates%20robust%20tools%20for%20developing%0Aand%20testing%20safe%20control%20architectures%20in%20dynamic%20and%20uncertain%20environments.%0AEnsuring%20safety%20and%20reliability%20in%20robotics%2C%20especially%20in%20safety-critical%0Aapplications%2C%20is%20crucial%2C%20driving%20substantial%20industrial%20and%20academic%20efforts.%0AIn%20this%20context%2C%20we%20extend%20CBFkit%2C%20a%20Python/ROS2%20toolbox%2C%20which%20now%0Aincorporates%20a%20planner%20using%20reach-avoid%20specifications%20as%20a%20cost%20function.%0AThis%20integration%20with%20the%20Model%20Predictive%20Path%20Integral%20%28MPPI%29%20controllers%0Aenables%20the%20toolbox%20to%20satisfy%20complex%20tasks%20while%20ensuring%20formal%20safety%0Aguarantees%20under%20various%20sources%20of%20uncertainty%20using%20Control%20Barrier%20Functions%0A%28CBFs%29.%20CBFkit%20is%20optimized%20for%20speed%20using%20JAX%20for%20automatic%20differentiation%0Aand%20jaxopt%20for%20quadratic%20program%20solving.%20The%20toolbox%20supports%20various%20robotic%0Aapplications%2C%20including%20autonomous%20navigation%2C%20human-robot%20interaction%2C%20and%0Amulti-robot%20coordination.%20The%20toolbox%20also%20offers%20a%20comprehensive%20library%20of%0Aplanner%2C%20controller%2C%20sensor%2C%20and%20estimator%20implementations.%20Through%20a%20series%20of%0Aexamples%2C%20we%20demonstrate%20the%20enhanced%20capabilities%20of%20CBFkit%20in%20different%0Arobotic%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Predictive%2520Path%2520Integral%2520Methods%2520with%2520Reach-Avoid%2520Tasks%2520and%250A%2520%2520Control%2520Barrier%2520Functions%26entry.906535625%3DHardik%2520Parwana%2520and%2520Mitchell%2520Black%2520and%2520Georgios%2520Fainekos%2520and%2520Bardh%2520Hoxha%2520and%2520Hideki%2520Okamoto%2520and%2520Danil%2520Prokhorov%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520robotics%2520necessitates%2520robust%2520tools%2520for%2520developing%250Aand%2520testing%2520safe%2520control%2520architectures%2520in%2520dynamic%2520and%2520uncertain%2520environments.%250AEnsuring%2520safety%2520and%2520reliability%2520in%2520robotics%252C%2520especially%2520in%2520safety-critical%250Aapplications%252C%2520is%2520crucial%252C%2520driving%2520substantial%2520industrial%2520and%2520academic%2520efforts.%250AIn%2520this%2520context%252C%2520we%2520extend%2520CBFkit%252C%2520a%2520Python/ROS2%2520toolbox%252C%2520which%2520now%250Aincorporates%2520a%2520planner%2520using%2520reach-avoid%2520specifications%2520as%2520a%2520cost%2520function.%250AThis%2520integration%2520with%2520the%2520Model%2520Predictive%2520Path%2520Integral%2520%2528MPPI%2529%2520controllers%250Aenables%2520the%2520toolbox%2520to%2520satisfy%2520complex%2520tasks%2520while%2520ensuring%2520formal%2520safety%250Aguarantees%2520under%2520various%2520sources%2520of%2520uncertainty%2520using%2520Control%2520Barrier%2520Functions%250A%2528CBFs%2529.%2520CBFkit%2520is%2520optimized%2520for%2520speed%2520using%2520JAX%2520for%2520automatic%2520differentiation%250Aand%2520jaxopt%2520for%2520quadratic%2520program%2520solving.%2520The%2520toolbox%2520supports%2520various%2520robotic%250Aapplications%252C%2520including%2520autonomous%2520navigation%252C%2520human-robot%2520interaction%252C%2520and%250Amulti-robot%2520coordination.%2520The%2520toolbox%2520also%2520offers%2520a%2520comprehensive%2520library%2520of%250Aplanner%252C%2520controller%252C%2520sensor%252C%2520and%2520estimator%2520implementations.%2520Through%2520a%2520series%2520of%250Aexamples%252C%2520we%2520demonstrate%2520the%2520enhanced%2520capabilities%2520of%2520CBFkit%2520in%2520different%250Arobotic%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Predictive%20Path%20Integral%20Methods%20with%20Reach-Avoid%20Tasks%20and%0A%20%20Control%20Barrier%20Functions&entry.906535625=Hardik%20Parwana%20and%20Mitchell%20Black%20and%20Georgios%20Fainekos%20and%20Bardh%20Hoxha%20and%20Hideki%20Okamoto%20and%20Danil%20Prokhorov&entry.1292438233=%20%20The%20rapid%20advancement%20of%20robotics%20necessitates%20robust%20tools%20for%20developing%0Aand%20testing%20safe%20control%20architectures%20in%20dynamic%20and%20uncertain%20environments.%0AEnsuring%20safety%20and%20reliability%20in%20robotics%2C%20especially%20in%20safety-critical%0Aapplications%2C%20is%20crucial%2C%20driving%20substantial%20industrial%20and%20academic%20efforts.%0AIn%20this%20context%2C%20we%20extend%20CBFkit%2C%20a%20Python/ROS2%20toolbox%2C%20which%20now%0Aincorporates%20a%20planner%20using%20reach-avoid%20specifications%20as%20a%20cost%20function.%0AThis%20integration%20with%20the%20Model%20Predictive%20Path%20Integral%20%28MPPI%29%20controllers%0Aenables%20the%20toolbox%20to%20satisfy%20complex%20tasks%20while%20ensuring%20formal%20safety%0Aguarantees%20under%20various%20sources%20of%20uncertainty%20using%20Control%20Barrier%20Functions%0A%28CBFs%29.%20CBFkit%20is%20optimized%20for%20speed%20using%20JAX%20for%20automatic%20differentiation%0Aand%20jaxopt%20for%20quadratic%20program%20solving.%20The%20toolbox%20supports%20various%20robotic%0Aapplications%2C%20including%20autonomous%20navigation%2C%20human-robot%20interaction%2C%20and%0Amulti-robot%20coordination.%20The%20toolbox%20also%20offers%20a%20comprehensive%20library%20of%0Aplanner%2C%20controller%2C%20sensor%2C%20and%20estimator%20implementations.%20Through%20a%20series%20of%0Aexamples%2C%20we%20demonstrate%20the%20enhanced%20capabilities%20of%20CBFkit%20in%20different%0Arobotic%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13693v1&entry.124074799=Read"},
{"title": "CellularLint: A Systematic Approach to Identify Inconsistent Behavior in\n  Cellular Network Specifications", "author": "Mirza Masfiqur Rahman and Imtiaz Karim and Elisa Bertino", "abstract": "  In recent years, there has been a growing focus on scrutinizing the security\nof cellular networks, often attributing security vulnerabilities to issues in\nthe underlying protocol design descriptions. These protocol design\nspecifications, typically extensive documents that are thousands of pages long,\ncan harbor inaccuracies, underspecifications, implicit assumptions, and\ninternal inconsistencies. In light of the evolving landscape, we introduce\nCellularLint--a semi-automatic framework for inconsistency detection within the\nstandards of 4G and 5G, capitalizing on a suite of natural language processing\ntechniques. Our proposed method uses a revamped few-shot learning mechanism on\ndomain-adapted large language models. Pre-trained on a vast corpus of cellular\nnetwork protocols, this method enables CellularLint to simultaneously detect\ninconsistencies at various levels of semantics and practical use cases. In\ndoing so, CellularLint significantly advances the automated analysis of\nprotocol specifications in a scalable fashion. In our investigation, we focused\non the Non-Access Stratum (NAS) and the security specifications of 4G and 5G\nnetworks, ultimately uncovering 157 inconsistencies with 82.67% accuracy. After\nverification of these inconsistencies on open-source implementations and 17\ncommercial devices, we confirm that they indeed have a substantial impact on\ndesign decisions, potentially leading to concerns related to privacy,\nintegrity, availability, and interoperability.\n", "link": "http://arxiv.org/abs/2407.13742v1", "date": "2024-07-18", "relevancy": 1.7086, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4542}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4222}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CellularLint%3A%20A%20Systematic%20Approach%20to%20Identify%20Inconsistent%20Behavior%20in%0A%20%20Cellular%20Network%20Specifications&body=Title%3A%20CellularLint%3A%20A%20Systematic%20Approach%20to%20Identify%20Inconsistent%20Behavior%20in%0A%20%20Cellular%20Network%20Specifications%0AAuthor%3A%20Mirza%20Masfiqur%20Rahman%20and%20Imtiaz%20Karim%20and%20Elisa%20Bertino%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20a%20growing%20focus%20on%20scrutinizing%20the%20security%0Aof%20cellular%20networks%2C%20often%20attributing%20security%20vulnerabilities%20to%20issues%20in%0Athe%20underlying%20protocol%20design%20descriptions.%20These%20protocol%20design%0Aspecifications%2C%20typically%20extensive%20documents%20that%20are%20thousands%20of%20pages%20long%2C%0Acan%20harbor%20inaccuracies%2C%20underspecifications%2C%20implicit%20assumptions%2C%20and%0Ainternal%20inconsistencies.%20In%20light%20of%20the%20evolving%20landscape%2C%20we%20introduce%0ACellularLint--a%20semi-automatic%20framework%20for%20inconsistency%20detection%20within%20the%0Astandards%20of%204G%20and%205G%2C%20capitalizing%20on%20a%20suite%20of%20natural%20language%20processing%0Atechniques.%20Our%20proposed%20method%20uses%20a%20revamped%20few-shot%20learning%20mechanism%20on%0Adomain-adapted%20large%20language%20models.%20Pre-trained%20on%20a%20vast%20corpus%20of%20cellular%0Anetwork%20protocols%2C%20this%20method%20enables%20CellularLint%20to%20simultaneously%20detect%0Ainconsistencies%20at%20various%20levels%20of%20semantics%20and%20practical%20use%20cases.%20In%0Adoing%20so%2C%20CellularLint%20significantly%20advances%20the%20automated%20analysis%20of%0Aprotocol%20specifications%20in%20a%20scalable%20fashion.%20In%20our%20investigation%2C%20we%20focused%0Aon%20the%20Non-Access%20Stratum%20%28NAS%29%20and%20the%20security%20specifications%20of%204G%20and%205G%0Anetworks%2C%20ultimately%20uncovering%20157%20inconsistencies%20with%2082.67%25%20accuracy.%20After%0Averification%20of%20these%20inconsistencies%20on%20open-source%20implementations%20and%2017%0Acommercial%20devices%2C%20we%20confirm%20that%20they%20indeed%20have%20a%20substantial%20impact%20on%0Adesign%20decisions%2C%20potentially%20leading%20to%20concerns%20related%20to%20privacy%2C%0Aintegrity%2C%20availability%2C%20and%20interoperability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCellularLint%253A%2520A%2520Systematic%2520Approach%2520to%2520Identify%2520Inconsistent%2520Behavior%2520in%250A%2520%2520Cellular%2520Network%2520Specifications%26entry.906535625%3DMirza%2520Masfiqur%2520Rahman%2520and%2520Imtiaz%2520Karim%2520and%2520Elisa%2520Bertino%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520there%2520has%2520been%2520a%2520growing%2520focus%2520on%2520scrutinizing%2520the%2520security%250Aof%2520cellular%2520networks%252C%2520often%2520attributing%2520security%2520vulnerabilities%2520to%2520issues%2520in%250Athe%2520underlying%2520protocol%2520design%2520descriptions.%2520These%2520protocol%2520design%250Aspecifications%252C%2520typically%2520extensive%2520documents%2520that%2520are%2520thousands%2520of%2520pages%2520long%252C%250Acan%2520harbor%2520inaccuracies%252C%2520underspecifications%252C%2520implicit%2520assumptions%252C%2520and%250Ainternal%2520inconsistencies.%2520In%2520light%2520of%2520the%2520evolving%2520landscape%252C%2520we%2520introduce%250ACellularLint--a%2520semi-automatic%2520framework%2520for%2520inconsistency%2520detection%2520within%2520the%250Astandards%2520of%25204G%2520and%25205G%252C%2520capitalizing%2520on%2520a%2520suite%2520of%2520natural%2520language%2520processing%250Atechniques.%2520Our%2520proposed%2520method%2520uses%2520a%2520revamped%2520few-shot%2520learning%2520mechanism%2520on%250Adomain-adapted%2520large%2520language%2520models.%2520Pre-trained%2520on%2520a%2520vast%2520corpus%2520of%2520cellular%250Anetwork%2520protocols%252C%2520this%2520method%2520enables%2520CellularLint%2520to%2520simultaneously%2520detect%250Ainconsistencies%2520at%2520various%2520levels%2520of%2520semantics%2520and%2520practical%2520use%2520cases.%2520In%250Adoing%2520so%252C%2520CellularLint%2520significantly%2520advances%2520the%2520automated%2520analysis%2520of%250Aprotocol%2520specifications%2520in%2520a%2520scalable%2520fashion.%2520In%2520our%2520investigation%252C%2520we%2520focused%250Aon%2520the%2520Non-Access%2520Stratum%2520%2528NAS%2529%2520and%2520the%2520security%2520specifications%2520of%25204G%2520and%25205G%250Anetworks%252C%2520ultimately%2520uncovering%2520157%2520inconsistencies%2520with%252082.67%2525%2520accuracy.%2520After%250Averification%2520of%2520these%2520inconsistencies%2520on%2520open-source%2520implementations%2520and%252017%250Acommercial%2520devices%252C%2520we%2520confirm%2520that%2520they%2520indeed%2520have%2520a%2520substantial%2520impact%2520on%250Adesign%2520decisions%252C%2520potentially%2520leading%2520to%2520concerns%2520related%2520to%2520privacy%252C%250Aintegrity%252C%2520availability%252C%2520and%2520interoperability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CellularLint%3A%20A%20Systematic%20Approach%20to%20Identify%20Inconsistent%20Behavior%20in%0A%20%20Cellular%20Network%20Specifications&entry.906535625=Mirza%20Masfiqur%20Rahman%20and%20Imtiaz%20Karim%20and%20Elisa%20Bertino&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20a%20growing%20focus%20on%20scrutinizing%20the%20security%0Aof%20cellular%20networks%2C%20often%20attributing%20security%20vulnerabilities%20to%20issues%20in%0Athe%20underlying%20protocol%20design%20descriptions.%20These%20protocol%20design%0Aspecifications%2C%20typically%20extensive%20documents%20that%20are%20thousands%20of%20pages%20long%2C%0Acan%20harbor%20inaccuracies%2C%20underspecifications%2C%20implicit%20assumptions%2C%20and%0Ainternal%20inconsistencies.%20In%20light%20of%20the%20evolving%20landscape%2C%20we%20introduce%0ACellularLint--a%20semi-automatic%20framework%20for%20inconsistency%20detection%20within%20the%0Astandards%20of%204G%20and%205G%2C%20capitalizing%20on%20a%20suite%20of%20natural%20language%20processing%0Atechniques.%20Our%20proposed%20method%20uses%20a%20revamped%20few-shot%20learning%20mechanism%20on%0Adomain-adapted%20large%20language%20models.%20Pre-trained%20on%20a%20vast%20corpus%20of%20cellular%0Anetwork%20protocols%2C%20this%20method%20enables%20CellularLint%20to%20simultaneously%20detect%0Ainconsistencies%20at%20various%20levels%20of%20semantics%20and%20practical%20use%20cases.%20In%0Adoing%20so%2C%20CellularLint%20significantly%20advances%20the%20automated%20analysis%20of%0Aprotocol%20specifications%20in%20a%20scalable%20fashion.%20In%20our%20investigation%2C%20we%20focused%0Aon%20the%20Non-Access%20Stratum%20%28NAS%29%20and%20the%20security%20specifications%20of%204G%20and%205G%0Anetworks%2C%20ultimately%20uncovering%20157%20inconsistencies%20with%2082.67%25%20accuracy.%20After%0Averification%20of%20these%20inconsistencies%20on%20open-source%20implementations%20and%2017%0Acommercial%20devices%2C%20we%20confirm%20that%20they%20indeed%20have%20a%20substantial%20impact%20on%0Adesign%20decisions%2C%20potentially%20leading%20to%20concerns%20related%20to%20privacy%2C%0Aintegrity%2C%20availability%2C%20and%20interoperability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13742v1&entry.124074799=Read"},
{"title": "Queue-based Eco-Driving at Roundabouts with Reinforcement Learning", "author": "Anna-Lena Schlamp and Werner Huber and Stefanie Schmidtner", "abstract": "  We address eco-driving at roundabouts in mixed traffic to enhance traffic\nflow and traffic efficiency in urban areas. The aim is to proactively optimize\nspeed of automated or non-automated connected vehicles (CVs), ensuring both an\nefficient approach and smooth entry into roundabouts. We incorporate the\ntraffic situation ahead, i.e. preceding vehicles and waiting queues. Further,\nwe develop two approaches: a rule-based and an Reinforcement Learning (RL)\nbased eco-driving system, with both using the approach link and information\nfrom conflicting CVs for speed optimization. A fair comparison of rule-based\nand RL-based approaches is performed to explore RL as a viable alternative to\nclassical optimization. Results show that both approaches outperform the\nbaseline. Improvements significantly increase with growing traffic volumes,\nleading to best results on average being obtained at high volumes. Near\ncapacity, performance deteriorates, indicating limited applicability at\ncapacity limits. Examining different CV penetration rates, a decline in\nperformance is observed, but with substantial results still being achieved at\nlower CV rates. RL agents can discover effective policies for speed\noptimization in dynamic roundabout settings, but they do not offer a\nsubstantial advantage over classical approaches, especially at higher traffic\nvolumes or lower CV penetration rates.\n", "link": "http://arxiv.org/abs/2405.00625v2", "date": "2024-07-18", "relevancy": 1.817, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4659}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4554}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Queue-based%20Eco-Driving%20at%20Roundabouts%20with%20Reinforcement%20Learning&body=Title%3A%20Queue-based%20Eco-Driving%20at%20Roundabouts%20with%20Reinforcement%20Learning%0AAuthor%3A%20Anna-Lena%20Schlamp%20and%20Werner%20Huber%20and%20Stefanie%20Schmidtner%0AAbstract%3A%20%20%20We%20address%20eco-driving%20at%20roundabouts%20in%20mixed%20traffic%20to%20enhance%20traffic%0Aflow%20and%20traffic%20efficiency%20in%20urban%20areas.%20The%20aim%20is%20to%20proactively%20optimize%0Aspeed%20of%20automated%20or%20non-automated%20connected%20vehicles%20%28CVs%29%2C%20ensuring%20both%20an%0Aefficient%20approach%20and%20smooth%20entry%20into%20roundabouts.%20We%20incorporate%20the%0Atraffic%20situation%20ahead%2C%20i.e.%20preceding%20vehicles%20and%20waiting%20queues.%20Further%2C%0Awe%20develop%20two%20approaches%3A%20a%20rule-based%20and%20an%20Reinforcement%20Learning%20%28RL%29%0Abased%20eco-driving%20system%2C%20with%20both%20using%20the%20approach%20link%20and%20information%0Afrom%20conflicting%20CVs%20for%20speed%20optimization.%20A%20fair%20comparison%20of%20rule-based%0Aand%20RL-based%20approaches%20is%20performed%20to%20explore%20RL%20as%20a%20viable%20alternative%20to%0Aclassical%20optimization.%20Results%20show%20that%20both%20approaches%20outperform%20the%0Abaseline.%20Improvements%20significantly%20increase%20with%20growing%20traffic%20volumes%2C%0Aleading%20to%20best%20results%20on%20average%20being%20obtained%20at%20high%20volumes.%20Near%0Acapacity%2C%20performance%20deteriorates%2C%20indicating%20limited%20applicability%20at%0Acapacity%20limits.%20Examining%20different%20CV%20penetration%20rates%2C%20a%20decline%20in%0Aperformance%20is%20observed%2C%20but%20with%20substantial%20results%20still%20being%20achieved%20at%0Alower%20CV%20rates.%20RL%20agents%20can%20discover%20effective%20policies%20for%20speed%0Aoptimization%20in%20dynamic%20roundabout%20settings%2C%20but%20they%20do%20not%20offer%20a%0Asubstantial%20advantage%20over%20classical%20approaches%2C%20especially%20at%20higher%20traffic%0Avolumes%20or%20lower%20CV%20penetration%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00625v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQueue-based%2520Eco-Driving%2520at%2520Roundabouts%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DAnna-Lena%2520Schlamp%2520and%2520Werner%2520Huber%2520and%2520Stefanie%2520Schmidtner%26entry.1292438233%3D%2520%2520We%2520address%2520eco-driving%2520at%2520roundabouts%2520in%2520mixed%2520traffic%2520to%2520enhance%2520traffic%250Aflow%2520and%2520traffic%2520efficiency%2520in%2520urban%2520areas.%2520The%2520aim%2520is%2520to%2520proactively%2520optimize%250Aspeed%2520of%2520automated%2520or%2520non-automated%2520connected%2520vehicles%2520%2528CVs%2529%252C%2520ensuring%2520both%2520an%250Aefficient%2520approach%2520and%2520smooth%2520entry%2520into%2520roundabouts.%2520We%2520incorporate%2520the%250Atraffic%2520situation%2520ahead%252C%2520i.e.%2520preceding%2520vehicles%2520and%2520waiting%2520queues.%2520Further%252C%250Awe%2520develop%2520two%2520approaches%253A%2520a%2520rule-based%2520and%2520an%2520Reinforcement%2520Learning%2520%2528RL%2529%250Abased%2520eco-driving%2520system%252C%2520with%2520both%2520using%2520the%2520approach%2520link%2520and%2520information%250Afrom%2520conflicting%2520CVs%2520for%2520speed%2520optimization.%2520A%2520fair%2520comparison%2520of%2520rule-based%250Aand%2520RL-based%2520approaches%2520is%2520performed%2520to%2520explore%2520RL%2520as%2520a%2520viable%2520alternative%2520to%250Aclassical%2520optimization.%2520Results%2520show%2520that%2520both%2520approaches%2520outperform%2520the%250Abaseline.%2520Improvements%2520significantly%2520increase%2520with%2520growing%2520traffic%2520volumes%252C%250Aleading%2520to%2520best%2520results%2520on%2520average%2520being%2520obtained%2520at%2520high%2520volumes.%2520Near%250Acapacity%252C%2520performance%2520deteriorates%252C%2520indicating%2520limited%2520applicability%2520at%250Acapacity%2520limits.%2520Examining%2520different%2520CV%2520penetration%2520rates%252C%2520a%2520decline%2520in%250Aperformance%2520is%2520observed%252C%2520but%2520with%2520substantial%2520results%2520still%2520being%2520achieved%2520at%250Alower%2520CV%2520rates.%2520RL%2520agents%2520can%2520discover%2520effective%2520policies%2520for%2520speed%250Aoptimization%2520in%2520dynamic%2520roundabout%2520settings%252C%2520but%2520they%2520do%2520not%2520offer%2520a%250Asubstantial%2520advantage%2520over%2520classical%2520approaches%252C%2520especially%2520at%2520higher%2520traffic%250Avolumes%2520or%2520lower%2520CV%2520penetration%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00625v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Queue-based%20Eco-Driving%20at%20Roundabouts%20with%20Reinforcement%20Learning&entry.906535625=Anna-Lena%20Schlamp%20and%20Werner%20Huber%20and%20Stefanie%20Schmidtner&entry.1292438233=%20%20We%20address%20eco-driving%20at%20roundabouts%20in%20mixed%20traffic%20to%20enhance%20traffic%0Aflow%20and%20traffic%20efficiency%20in%20urban%20areas.%20The%20aim%20is%20to%20proactively%20optimize%0Aspeed%20of%20automated%20or%20non-automated%20connected%20vehicles%20%28CVs%29%2C%20ensuring%20both%20an%0Aefficient%20approach%20and%20smooth%20entry%20into%20roundabouts.%20We%20incorporate%20the%0Atraffic%20situation%20ahead%2C%20i.e.%20preceding%20vehicles%20and%20waiting%20queues.%20Further%2C%0Awe%20develop%20two%20approaches%3A%20a%20rule-based%20and%20an%20Reinforcement%20Learning%20%28RL%29%0Abased%20eco-driving%20system%2C%20with%20both%20using%20the%20approach%20link%20and%20information%0Afrom%20conflicting%20CVs%20for%20speed%20optimization.%20A%20fair%20comparison%20of%20rule-based%0Aand%20RL-based%20approaches%20is%20performed%20to%20explore%20RL%20as%20a%20viable%20alternative%20to%0Aclassical%20optimization.%20Results%20show%20that%20both%20approaches%20outperform%20the%0Abaseline.%20Improvements%20significantly%20increase%20with%20growing%20traffic%20volumes%2C%0Aleading%20to%20best%20results%20on%20average%20being%20obtained%20at%20high%20volumes.%20Near%0Acapacity%2C%20performance%20deteriorates%2C%20indicating%20limited%20applicability%20at%0Acapacity%20limits.%20Examining%20different%20CV%20penetration%20rates%2C%20a%20decline%20in%0Aperformance%20is%20observed%2C%20but%20with%20substantial%20results%20still%20being%20achieved%20at%0Alower%20CV%20rates.%20RL%20agents%20can%20discover%20effective%20policies%20for%20speed%0Aoptimization%20in%20dynamic%20roundabout%20settings%2C%20but%20they%20do%20not%20offer%20a%0Asubstantial%20advantage%20over%20classical%20approaches%2C%20especially%20at%20higher%20traffic%0Avolumes%20or%20lower%20CV%20penetration%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00625v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


